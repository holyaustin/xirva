[{"id": "1308.1365", "submitter": "Mauricio Gonz\\'alez-Avil\\'es", "authors": "Mauricio Gonz\\'alez Avil\\'es and Jos\\'e Juan Gonz\\'alez Avil\\'es", "title": "Mathematical model of concentrating solar cooker", "comments": "Keywords: Solar cooker, thermal model; mathematical model", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this work is to obtain a mathematical model consistent\nwith the thermal behavior of concentrating solar cookers, such as\nJorhejpataranskua. We also want to simulate different conditions respect to the\nparameters involved of several materials for its construction and efficiency.\nThe model is expressed in terms of a coupled nonlinear system of differential\nequations which are solved using Mathematica 8. The results obtained by our\nmodel are compared with measurements of solar cooker in field testing\noperation. We obtained good results in agreement with experimental data.\nMoreover, the simulation results are used by calculating cooking power and\nstandardized cooking power of solar cooker for different parameters.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 18:11:15 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Avil\u00e9s", "Mauricio Gonz\u00e1lez", ""], ["Avil\u00e9s", "Jos\u00e9 Juan Gonz\u00e1lez", ""]]}, {"id": "1308.1464", "submitter": "Andy Terrel", "authors": "Andy R. Terrel and Kyle T. Mandli", "title": "ManyClaw: Slicing and dicing Riemann solvers for next generation highly\n  parallel architectures", "comments": "TACC-Intel Symposium on Highly Parallel Architectures. 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation computer architectures will include order of magnitude more\nintra-node parallelism; however, many application programmers have a difficult\ntime keeping their codes current with the state-of-the-art machines. In this\ncontext, we analyze Hyperbolic PDE solvers, which are used in the solution of\nmany important applications in science and engineering. We present ManyClaw, a\nproject intended to explore the exploitation of intra-node parallelism in\nhyperbolic PDE solvers via the Clawpack software package for solving hyperbolic\nPDEs. Our goal is to separate the low level parallelism and the physical\nequations thus providing users the capability to leverage intra-node\nparallelism without explicitly writing code to take advantage of newer\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2013 02:24:20 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Terrel", "Andy R.", ""], ["Mandli", "Kyle T.", ""]]}, {"id": "1308.1779", "submitter": "Christoph Lange", "authors": "Marco B. Caminati, Manfred Kerber, Christoph Lange, Colin Rowat", "title": "Proving soundness of combinatorial Vickrey auctions and generating\n  verified executable code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using mechanised reasoning we prove that combinatorial Vickrey auctions are\nsoundly specified in that they associate a unique outcome (allocation and\ntransfers) to any valid input (bids). Having done so, we auto-generate verified\nexecutable code from the formally defined auction. This removes a source of\nerror in implementing the auction design. We intend to use formal methods to\nverify new auction designs. Here, our contribution is to introduce and\ndemonstrate the use of formal methods for auction verification in the familiar\nsetting of a well-known auction.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 08:00:55 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2013 10:47:25 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Caminati", "Marco B.", ""], ["Kerber", "Manfred", ""], ["Lange", "Christoph", ""], ["Rowat", "Colin", ""]]}, {"id": "1308.1860", "submitter": "Alexandru Cioaca Mr", "authors": "Alexandru Cioaca, Adrian Sandu", "title": "An Optimization Framework to Improve 4D-Var Data Assimilation System\n  Performance", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2014.07.005", "report-no": "CSL-TR-4-2013", "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a computational framework for optimizing the parameters\nof data assimilation systems in order to improve their performance. The\napproach formulates a continuous meta-optimization problem for parameters; the\nmeta-optimization is constrained by the original data assimilation problem. The\nnumerical solution process employs adjoint models and iterative solvers. The\nproposed framework is applied to optimize observation values, data weighting\ncoefficients, and the location of sensors for a test problem. The ability to\noptimize a distributed measurement network is crucial for cutting down\noperating costs and detecting malfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 14:16:28 GMT"}, {"version": "v2", "created": "Sat, 9 Nov 2013 12:26:13 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Cioaca", "Alexandru", ""], ["Sandu", "Adrian", ""]]}, {"id": "1308.1975", "submitter": "Zhiyong Wang", "authors": "Zhiyong Wang and Jinbo Xu", "title": "Predicting protein contact map using evolutionary and physical\n  constraints by integer programming (extended version)", "comments": "14 pages, 13 figures, 10 tables", "journal-ref": "Bioinformatics (2013) 29 (13): i266-i273", "doi": "10.1093/bioinformatics/btt211", "report-no": null, "categories": "q-bio.QM cs.CE cs.LG math.OC q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation. Protein contact map describes the pairwise spatial and functional\nrelationship of residues in a protein and contains key information for protein\n3D structure prediction. Although studied extensively, it remains very\nchallenging to predict contact map using only sequence information. Most\nexisting methods predict the contact map matrix element-by-element, ignoring\ncorrelation among contacts and physical feasibility of the whole contact map. A\ncouple of recent methods predict contact map based upon residue co-evolution,\ntaking into consideration contact correlation and enforcing a sparsity\nrestraint, but these methods require a very large number of sequence homologs\nfor the protein under consideration and the resultant contact map may be still\nphysically unfavorable.\n  Results. This paper presents a novel method PhyCMAP for contact map\nprediction, integrating both evolutionary and physical restraints by machine\nlearning and integer linear programming (ILP). The evolutionary restraints\ninclude sequence profile, residue co-evolution and context-specific statistical\npotential. The physical restraints specify more concrete relationship among\ncontacts than the sparsity restraint. As such, our method greatly reduces the\nsolution space of the contact map matrix and thus, significantly improves\nprediction accuracy. Experimental results confirm that PhyCMAP outperforms\ncurrently popular methods no matter how many sequence homologs are available\nfor the protein under consideration. PhyCMAP can predict contacts within\nminutes after PSIBLAST search for sequence homologs is done, much faster than\nthe two recent methods PSICOV and EvFold.\n  See http://raptorx.uchicago.edu for the web server.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 20:44:01 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2013 16:24:06 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Wang", "Zhiyong", ""], ["Xu", "Jinbo", ""]]}, {"id": "1308.2058", "submitter": "Blesson Varghese", "authors": "Ishan Patel, Blesson Varghese, Adam Barker", "title": "RBioCloud: A Light-weight Framework for Bioconductor and R-based Jobs on\n  the Cloud", "comments": "Webpage: http://www.rbiocloud.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale ad hoc analytics of genomic data is popular using the\nR-programming language supported by 671 software packages provided by\nBioconductor. More recently, analytical jobs are benefitting from on-demand\ncomputing and storage, their scalability and their low maintenance cost, all of\nwhich are offered by the cloud. While Biologists and Bioinformaticists can take\nan analytical job and execute it on their personal workstations, it remains\nchallenging to seamlessly execute the job on the cloud infrastructure without\nextensive knowledge of the cloud dashboard. How analytical jobs can not only\nwith minimum effort be executed on the cloud, but also how both the resources\nand data required by the job can be managed is explored in this paper. An\nopen-source light-weight framework for executing R-scripts using Bioconductor\npackages, referred to as `RBioCloud', is designed and developed. RBioCloud\noffers a set of simple command-line tools for managing the cloud resources, the\ndata and the execution of the job. Three biological test cases validate the\nfeasibility of RBioCloud. The framework is publicly available from\nhttp://www.rbiocloud.com.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 09:20:02 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Patel", "Ishan", ""], ["Varghese", "Blesson", ""], ["Barker", "Adam", ""]]}, {"id": "1308.2066", "submitter": "Blesson Varghese", "authors": "Aman Bahl, Oliver Baltzer, Andrew Rau-Chaplin, Blesson Varghese", "title": "Parallel Simulations for Analysing Portfolios of Catastrophic Event Risk", "comments": "Proceedings of the Workshop at the International Conference for High\n  Performance Computing, Networking, Storage and Analysis (SC), 2012, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the heart of the analytical pipeline of a modern quantitative\ninsurance/reinsurance company is a stochastic simulation technique for\nportfolio risk analysis and pricing process referred to as Aggregate Analysis.\nSupport for the computation of risk measures including Probable Maximum Loss\n(PML) and the Tail Value at Risk (TVAR) for a variety of types of complex\nproperty catastrophe insurance contracts including Cat eXcess of Loss (XL), or\nPer-Occurrence XL, and Aggregate XL, and contracts that combine these measures\nis obtained in Aggregate Analysis.\n  In this paper, we explore parallel methods for aggregate risk analysis. A\nparallel aggregate risk analysis algorithm and an engine based on the algorithm\nis proposed. This engine is implemented in C and OpenMP for multi-core CPUs and\nin C and CUDA for many-core GPUs. Performance analysis of the algorithm\nindicates that GPUs offer an alternative HPC solution for aggregate risk\nanalysis that is cost effective. The optimised algorithm on the GPU performs a\n1 million trial aggregate simulation with 1000 catastrophic events per trial on\na typical exposure set and contract structure in just over 20 seconds which is\napproximately 15x times faster than the sequential counterpart. This can\nsufficiently support the real-time pricing scenario in which an underwriter\nanalyses different contractual terms and pricing while discussing a deal with a\nclient over the phone.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 09:43:51 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Bahl", "Aman", ""], ["Baltzer", "Oliver", ""], ["Rau-Chaplin", "Andrew", ""], ["Varghese", "Blesson", ""]]}, {"id": "1308.2307", "submitter": "Tshilidzi Marwala", "authors": "I. Boulkabeit, L. Mthembu, T. Marwala, and F. De Lima Neto", "title": "Finite Element Model Updating Using Fish School Search Optimization\n  Method", "comments": "To appear in the 1st BRICS Countries & 11th CBIC Brazilian Congress\n  on Computational Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent nature inspired optimization algorithm, Fish School Search (FSS) is\napplied to the finite element model (FEM) updating problem. This method is\ntested on a GARTEUR SM-AG19 aeroplane structure. The results of this algorithm\nare compared with two other metaheuristic algorithms; Genetic Algorithm (GA)\nand Particle Swarm Optimization (PSO). It is observed that on average, the FSS\nand PSO algorithms give more accurate results than the GA. A minor modification\nto the FSS is proposed. This modification improves the performance of FSS on\nthe FEM updating problem which has a constrained search space.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2013 13:07:07 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Boulkabeit", "I.", ""], ["Mthembu", "L.", ""], ["Marwala", "T.", ""], ["Neto", "F. De Lima", ""]]}, {"id": "1308.2572", "submitter": "Blesson Varghese", "authors": "A. K. Bahl, O. Baltzer, A. Rau-Chaplin, B. Varghese and A. Whiteway", "title": "Achieving Speedup in Aggregate Risk Analysis using Multiple GPUs", "comments": "Workshop Proceedings of International Conference on Parallel\n  Processing, Lyon, France, 2013, 8 pages. arXiv admin note: text overlap with\n  arXiv:1308.2066", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.DS q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulation techniques employed for the analysis of portfolios of\ninsurance/reinsurance risk, often referred to as `Aggregate Risk Analysis', can\nbenefit from exploiting state-of-the-art high-performance computing platforms.\nIn this paper, parallel methods to speed-up aggregate risk analysis for\nsupporting real-time pricing are explored. An algorithm for analysing aggregate\nrisk is proposed and implemented for multi-core CPUs and for many-core GPUs.\nExperimental studies indicate that GPUs offer a feasible alternative solution\nover traditional high-performance computing systems. A simulation of 1,000,000\ntrials with 1,000 catastrophic events per trial on a typical exposure set and\ncontract structure is performed in less than 5 seconds on a multiple GPU\nplatform. The key result is that the multiple GPU implementation can be used in\nreal-time pricing scenarios as it is approximately 77x times faster than the\nsequential counterpart implemented on a CPU.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 14:09:45 GMT"}], "update_date": "2013-08-19", "authors_parsed": [["Bahl", "A. K.", ""], ["Baltzer", "O.", ""], ["Rau-Chaplin", "A.", ""], ["Varghese", "B.", ""], ["Whiteway", "A.", ""]]}, {"id": "1308.2773", "submitter": "Sabyasachi Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay, Prasanta K Panigrahi", "title": "Wind Speed Data Analysis for Various Seasons during a Decade by Wavelet\n  and S transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appropriate weather prediction is a challenging task and it can be\nfeasible with proper wind speed fluctuation analysis. In this current paper\ndaubechies-4 wavelet is used to analyze the winter wind speed fluctuations due\nto lesser agitated wind data samples of winter. In summer abrupt changes in\nwind speed occurs which creates difficulty for wavelets to keep proper track of\nwind speed fluctuations. So, in that case the concept of the S-transform is\nintroduced.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 07:16:03 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 23:19:56 GMT"}, {"version": "v3", "created": "Mon, 5 Dec 2016 18:07:35 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Panigrahi", "Prasanta K", ""]]}, {"id": "1308.2787", "submitter": "Blesson Varghese", "authors": "Ishan Patel, Andrew Rau-Chaplin, Blesson Varghese", "title": "Accelerating R-based Analytics on the Cloud", "comments": "Concurrency and Computation, 2013", "journal-ref": null, "doi": "10.1002/cpe.3026", "report-no": null, "categories": "cs.DC cs.CE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses how the benefits of cloud-based infrastructure can be\nharnessed for analytical workloads. Often the software handling analytical\nworkloads is not developed by a professional programmer, but on an ad hoc basis\nby Analysts in high-level programming environments such as R or Matlab. The\ngoal of this research is to allow Analysts to take an analytical job that\nexecutes on their personal workstations, and with minimum effort execute it on\ncloud infrastructure and manage both the resources and the data required by the\njob. If this can be facilitated gracefully, then the Analyst benefits from\non-demand resources, low maintenance cost and scalability of computing\nresources, all of which are offered by the cloud. In this paper, a Platform for\nParallel R-based Analytics on the Cloud (P2RAC) that is placed between an\nAnalyst and a cloud infrastructure is proposed and implemented. P2RAC offers a\nset of command-line tools for managing the resources, such as instances and\nclusters, the data and the execution of the software on the Amazon Elastic\nComputing Cloud infrastructure. Experimental studies are pursued using two\nparallel problems and the results obtained confirm the feasibility of employing\nP2RAC for solving large-scale analytical problems on the cloud.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 08:58:24 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Patel", "Ishan", ""], ["Rau-Chaplin", "Andrew", ""], ["Varghese", "Blesson", ""]]}, {"id": "1308.3136", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "Toward the Coevolution of Novel Vertical-Axis Wind Turbines", "comments": "appears in IEEE Transactions on Evolutionary Computation (2014).\n  arXiv admin note: substantial text overlap with arXiv:1212.5271,\n  arXiv:1204.4107", "journal-ref": "IEEE Transactions on Evolutionary Computation (2015),\n  19(2):284-294", "doi": "10.1109/TEVC.2014.2316199", "report-no": null, "categories": "cs.NE cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The production of renewable and sustainable energy is one of the most\nimportant challenges currently facing mankind. Wind has made an increasing\ncontribution to the world's energy supply mix, but still remains a long way\nfrom reaching its full potential. In this paper, we investigate the use of\nartificial evolution to design vertical-axis wind turbine prototypes that are\nphysically instantiated and evaluated under fan generated wind conditions.\nInitially a conventional evolutionary algorithm is used to explore the design\nspace of a single wind turbine and later a cooperative coevolutionary algorithm\nis used to explore the design space of an array of wind turbines. Artificial\nneural networks are used throughout as surrogate models to assist learning and\nfound to reduce the number of fabrications required to reach a higher\naerodynamic efficiency. Unlike in other approaches, such as computational fluid\ndynamics simulations, no mathematical formulations are used and no model\nassumptions are made.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2013 14:02:34 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 16:25:33 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "1308.3565", "submitter": "Scott Field", "authors": "Scott E. Field, Chad R. Galley, Jan S. Hesthaven, Jason Kaye, Manuel\n  Tiglio", "title": "Fast prediction and evaluation of gravitational waveforms using\n  surrogate models", "comments": "20 pages, 17 figures, uses revtex 4.1. Version 2 includes new\n  numerical experiments for longer waveform durations, larger regions of\n  parameter space and multi-mode models", "journal-ref": "Phys. Rev. X 4, 031006 (2014)", "doi": "10.1103/PhysRevX.4.031006", "report-no": null, "categories": "gr-qc cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  [Abridged] We propose a solution to the problem of quickly and accurately\npredicting gravitational waveforms within any given physical model. The method\nis relevant for both real-time applications and in more traditional scenarios\nwhere the generation of waveforms using standard methods can be prohibitively\nexpensive. Our approach is based on three offline steps resulting in an\naccurate reduced-order model that can be used as a surrogate for the\ntrue/fiducial waveform family. First, a set of m parameter values is determined\nusing a greedy algorithm from which a reduced basis representation is\nconstructed. Second, these m parameters induce the selection of m time values\nfor interpolating a waveform time series using an empirical interpolant. Third,\na fit in the parameter dimension is performed for the waveform's value at each\nof these m times. The cost of predicting L waveform time samples for a generic\nparameter choice is of order m L + m c_f online operations where c_f denotes\nthe fitting function operation count and, typically, m << L. We generate\naccurate surrogate models for Effective One Body (EOB) waveforms of\nnon-spinning binary black hole coalescences with durations as long as 10^5 M,\nmass ratios from 1 to 10, and for multiple harmonic modes. We find that these\nsurrogates are three orders of magnitude faster to evaluate as compared to the\ncost of generating EOB waveforms in standard ways. Surrogate model building for\nother waveform models follow the same steps and have the same low online\nscaling cost. For expensive numerical simulations of binary black hole\ncoalescences we thus anticipate large speedups in generating new waveforms with\na surrogate. As waveform generation is one of the dominant costs in parameter\nestimation algorithms and parameter space exploration, surrogate models offer a\nnew and practical way to dramatically accelerate such studies without impacting\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 07:13:44 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2014 03:21:44 GMT"}], "update_date": "2014-07-23", "authors_parsed": [["Field", "Scott E.", ""], ["Galley", "Chad R.", ""], ["Hesthaven", "Jan S.", ""], ["Kaye", "Jason", ""], ["Tiglio", "Manuel", ""]]}, {"id": "1308.3615", "submitter": "Blesson Varghese", "authors": "Andrew Rau-Chaplin, Blesson Varghese, Duane Wilson, Zhimin Yao, and\n  Norbert Zeh", "title": "QuPARA: Query-Driven Large-Scale Portfolio Aggregate Risk Analysis on\n  MapReduce", "comments": "9 pages, IEEE International Conference on Big Data (BigData), Santa\n  Clara, USA, 2013", "journal-ref": null, "doi": "10.1109/BigData.2013.6691640", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulation techniques are used for portfolio risk analysis. Risk\nportfolios may consist of thousands of reinsurance contracts covering millions\nof insured locations. To quantify risk each portfolio must be evaluated in up\nto a million simulation trials, each capturing a different possible sequence of\ncatastrophic events over the course of a contractual year. In this paper, we\nexplore the design of a flexible framework for portfolio risk analysis that\nfacilitates answering a rich variety of catastrophic risk queries. Rather than\naggregating simulation data in order to produce a small set of high-level risk\nmetrics efficiently (as is often done in production risk management systems),\nthe focus here is on allowing the user to pose queries on unaggregated or\npartially aggregated data. The goal is to provide a flexible framework that can\nbe used by analysts to answer a wide variety of unanticipated but natural ad\nhoc queries. Such detailed queries can help actuaries or underwriters to better\nunderstand the multiple dimensions (e.g., spatial correlation, seasonality,\nperil features, construction features, and financial terms) that can impact\nportfolio risk. We implemented a prototype system, called QuPARA (Query-Driven\nLarge-Scale Portfolio Aggregate Risk Analysis), using Hadoop, which is Apache's\nimplementation of the MapReduce paradigm. This allows the user to take\nadvantage of large parallel compute servers in order to answer ad hoc risk\nanalysis queries efficiently even on very large data sets typically encountered\nin practice. We describe the design and implementation of QuPARA and present\nexperimental results that demonstrate its feasibility. A full portfolio risk\nanalysis run consisting of a 1,000,000 trial simulation, with 1,000 events per\ntrial, and 3,200 risk transfer contracts can be completed on a 16-node Hadoop\ncluster in just over 20 minutes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 12:15:14 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Rau-Chaplin", "Andrew", ""], ["Varghese", "Blesson", ""], ["Wilson", "Duane", ""], ["Yao", "Zhimin", ""], ["Zeh", "Norbert", ""]]}, {"id": "1308.3700", "submitter": "Robert Patro", "authors": "Rob Patro (1), Stephen M. Mount (2) and Carl Kingsford (1) ((1) Lane\n  Center for Computational Biology, School of Computer Science, Carnegie Mellon\n  University, (2) Department of Cell Biology and Molecular Genetics and Center\n  for Bioinformatics and Computational Biology, University of Maryland)", "title": "Sailfish: Alignment-free Isoform Quantification from RNA-seq Reads using\n  Lightweight Algorithms", "comments": "28 pages, 2 main figures, 2 algorithm displays, 5 supplementary\n  figures and 2 supplementary notes. Accompanying software available at\n  http://www.cs.cmu.edu/~ckingsf/software/sailfish", "journal-ref": null, "doi": "10.1038/nbt.2862", "report-no": null, "categories": "q-bio.GN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNA-seq has rapidly become the de facto technique to measure gene expression.\nHowever, the time required for analysis has not kept up with the pace of data\ngeneration. Here we introduce Sailfish, a novel computational method for\nquantifying the abundance of previously annotated RNA isoforms from RNA-seq\ndata. Sailfish entirely avoids mapping reads, which is a time-consuming step in\nall current methods. Sailfish provides quantification estimates much faster\nthan existing approaches (typically 20-times faster) without loss of accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2013 19:51:34 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Patro", "Rob", ""], ["Mount", "Stephen M.", ""], ["Kingsford", "Carl", ""]]}, {"id": "1308.3995", "submitter": "Michael Woopen", "authors": "Michael Woopen (1), Aravind Balan (1), Georg May (1) and Jochen\n  Sch\\\"utz (2) ((1) AICES, RWTH Aachen, (2) IGPM, RWTH Aachen)", "title": "A Comparison of Hybridized and Standard DG Methods for Target-Based\n  hp-Adaptive Simulation of Compressible Flow", "comments": null, "journal-ref": "Comp.Fluids 98 (2014) 3-16", "doi": "10.1016/j.compfluid.2014.03.023", "report-no": null, "categories": "cs.CE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comparison between hybridized and non-hybridized discontinuous\nGalerkin methods in the context of target-based hp-adaptation for compressible\nflow problems. The aim is to provide a critical assessment of the computational\nefficiency of hybridized DG methods. Hybridization of finite element\ndiscretizations has the main advantage, that the resulting set of algebraic\nequations has globally coupled degrees of freedom only on the skeleton of the\ncomputational mesh. Consequently, solving for these degrees of freedom involves\nthe solution of a potentially much smaller system. This not only reduces\nstorage requirements, but also allows for a faster solution with iterative\nsolvers. Using a discrete-adjoint approach, sensitivities with respect to\noutput functionals are computed to drive the adaptation. From the error\ndistribution given by the adjoint-based error estimator, h- or p-refinement is\nchosen based on the smoothness of the solution which can be quantified by\nproperly-chosen smoothness indicators. Numerical results are shown for\nsubsonic, transonic, and supersonic flow around the NACA0012 airfoil.\nhp-adaptation proves to be superior to pure h-adaptation if discontinuous or\nsingular flow features are involved. In all cases, a higher polynomial degree\nturns out to be beneficial. We show that for polynomial degree of approximation\np=2 and higher, and for a broad range of test cases, HDG performs better than\nDG in terms of runtime and memory requirements.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 12:06:05 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2013 08:24:28 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2013 19:27:03 GMT"}, {"version": "v4", "created": "Thu, 9 Jan 2014 13:44:40 GMT"}, {"version": "v5", "created": "Wed, 4 Jun 2014 09:31:47 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Woopen", "Michael", "", "AICES, RWTH Aachen"], ["Balan", "Aravind", "", "AICES, RWTH Aachen"], ["May", "Georg", "", "AICES, RWTH Aachen"], ["Sch\u00fctz", "Jochen", "", "IGPM, RWTH Aachen"]]}, {"id": "1308.4618", "submitter": "Michael Bell", "authors": "Michael J. Bell, Matthew Collison, Phillip Lord", "title": "Can inferred provenance and its visualisation be used to detect\n  erroneous annotation? A case study using UniProtKB", "comments": "Paper to shortly appear in PLOS ONE. Composed of 21 pages and 16\n  figures", "journal-ref": null, "doi": "10.1371/journal.pone.0075541", "report-no": null, "categories": "cs.CL cs.CE cs.DL q-bio.QM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A constant influx of new data poses a challenge in keeping the annotation in\nbiological databases current. Most biological databases contain significant\nquantities of textual annotation, which often contains the richest source of\nknowledge. Many databases reuse existing knowledge, during the curation process\nannotations are often propagated between entries. However, this is often not\nmade explicit. Therefore, it can be hard, potentially impossible, for a reader\nto identify where an annotation originated from. Within this work we attempt to\nidentify annotation provenance and track its subsequent propagation.\nSpecifically, we exploit annotation reuse within the UniProt Knowledgebase\n(UniProtKB), at the level of individual sentences. We describe a visualisation\napproach for the provenance and propagation of sentences in UniProtKB which\nenables a large-scale statistical analysis. Initially levels of sentence reuse\nwithin UniProtKB were analysed, showing that reuse is heavily prevalent, which\nenables the tracking of provenance and propagation. By analysing sentences\nthroughout UniProtKB, a number of interesting propagation patterns were\nidentified, covering over 100, 000 sentences. Over 8000 sentences remain in the\ndatabase after they have been removed from the entries where they originally\noccurred. Analysing a subset of these sentences suggest that approximately 30%\nare erroneous, whilst 35% appear to be inconsistent. These results suggest that\nbeing able to visualise sentence propagation and provenance can aid in the\ndetermination of the accuracy and quality of textual annotation. Source code\nand supplementary data are available from the authors website.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 15:49:43 GMT"}], "update_date": "2013-08-22", "authors_parsed": [["Bell", "Michael J.", ""], ["Collison", "Matthew", ""], ["Lord", "Phillip", ""]]}, {"id": "1308.4801", "submitter": "Jos Schijndel van", "authors": "A.W.M. van Schijndel", "title": "The Mapping of Simulated Climate-Dependent Building Innovations", "comments": "Preliminary conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performances of building energy innovations are most of the time dependent on\nthe external climate conditions. This means a high performance of a specific\ninnovation in a certain part of Europe, does not imply the same performances in\nother regions. The mapping of simulated building performances at the EU scale\ncould prevent the waste of potential good ideas by identifying the best region\nfor a specific innovation. This paper presents a methodology for obtaining maps\nof performances of building innovations that are virtually spread over whole\nEurope. It is concluded that these maps are useful for finding regions at the\nEU where innovations have the highest expected performances.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 09:20:15 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["van Schijndel", "A. W. M.", ""]]}, {"id": "1308.4904", "submitter": "EPTCS", "authors": "Luca Bortolussi (University of Trieste), Manuela L. Bujorianu\n  (University of Warwick), Giordano Pola (University of L'Aquila)", "title": "Proceedings Third International Workshop on Hybrid Autonomous Systems", "comments": null, "journal-ref": "EPTCS 124, 2013", "doi": "10.4204/EPTCS.124", "report-no": null, "categories": "cs.SY cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest on autonomous systems is increasing both in industry and\nacademia. Such systems must operate with limited human intervention in a\nchanging environment and must be able to compensate for significant system\nfailures without external intervention. The most appropriate models of\nautonomous systems can be found in the class of hybrid systems (which study\ncontinuous-state dynamic processes via discrete-state controllers) that\ninteract with their environment. This workshop brings together researchers\ninterested in all aspects of autonomy and resilience of hybrid systems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 15:44:08 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Bortolussi", "Luca", "", "University of Trieste"], ["Bujorianu", "Manuela L.", "", "University of Warwick"], ["Pola", "Giordano", "", "University of L'Aquila"]]}, {"id": "1308.5144", "submitter": "Uwe Aickelin", "authors": "Yihui Liu, Uwe Aickelin", "title": "Detect adverse drug reactions for drug Pioglitazone", "comments": "IEEE 11th International Conference on Signal Processing (ICSP),\n  1654-1658, 2012", "journal-ref": null, "doi": "10.1109/ICoSP.2012.6491898", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we propose a novel method to successfully detect the ADRs using\nfeature matrix and feature selection. A feature matrix, which characterizes the\nmedical events before patients take drugs or after patients take drugs, is\ncreated from THIN database. The feature selection method of Student's t-test is\nused to detect the significant features from thousands of medical events. The\nsignificant ADRs, which are corresponding to significant features, are\ndetected. Experiments are performed on the drug Pioglitazone. Compared to other\ncomputerized methods, our proposed method achieves good performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 14:44:29 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Liu", "Yihui", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1308.5338", "submitter": "EPTCS", "authors": "Andrea Ocone (School of Informatics, University of Edinburgh), Guido\n  Sanguinetti (School of Informatics, University of Edinburgh)", "title": "A stochastic hybrid model of a biological filter", "comments": "In Proceedings HAS 2013, arXiv:1308.4904", "journal-ref": "EPTCS 124, 2013, pp. 100-108", "doi": "10.4204/EPTCS.124.10", "report-no": null, "categories": "cs.LG cs.CE q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hybrid model of a biological filter, a genetic circuit which\nremoves fast fluctuations in the cell's internal representation of the extra\ncellular environment. The model takes the classic feed-forward loop (FFL) motif\nand represents it as a network of continuous protein concentrations and binary,\nunobserved gene promoter states. We address the problem of statistical\ninference and parameter learning for this class of models from partial,\ndiscrete time observations. We show that the hybrid representation leads to an\nefficient algorithm for approximate statistical inference in this circuit, and\nshow its effectiveness on a simulated data set.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2013 14:34:38 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Ocone", "Andrea", "", "School of Informatics, University of Edinburgh"], ["Sanguinetti", "Guido", "", "School of Informatics, University of Edinburgh"]]}, {"id": "1308.5724", "submitter": "EPTCS", "authors": "Thao Dang, Carla Piazza", "title": "Proceedings Second International Workshop on Hybrid Systems and Biology", "comments": null, "journal-ref": "EPTCS 125, 2013", "doi": "10.4204/EPTCS.125", "report-no": null, "categories": "cs.CE cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of the Second International Workshop\nHybrid Systems and Biology (HSB 2013) held in Taormina (Italy), on September\n2th, 2013. The workshop is affiliated to the 12th European Conference on\nArtificial Life (ECAL 2013).\n  Systems biology aims at providing a system-level understanding of biological\nsystems by unveiling their structure, dynamics and control methods. Due to the\nintrinsic multi-scale nature of these systems in space, in organization levels\nand in time, it is extremely difficult to model them in a uniform way, e.g., by\nmeans of differential equations or discrete stochastic processes. Furthermore,\nsuch models are often not easily amenable to formal analysis, and their\nsimulations at the organ or even at the cell levels are frequently impractical.\nIndeed, an important open problem is finding appropriate computational models\nthat scale well for both simulation and formal analysis of biological\nprocesses. Hybrid modeling techniques, combining discrete and continuous\nprocesses, are gaining more and more attention in such a context, and they have\nbeen successfully applied to capture the behavior of many biological complex\nsystems, ranging from genetic networks, biochemical reactions, signaling\npathways, cardiac tissues electro-physiology, and tumor genesis. This workshop\naims at bringing together researchers in computer science, mathematics, and\nlife sciences, interested in the opportunities and the challenges of hybrid\nmodeling applied to systems biology.\n  The workshop programme included the keynote presentation of Alessandro\nAstolfi (Imperial College of London, UK) on Immune response enhancement via\nhybrid control. Furthermore, 8 papers were selected out of 13 submissions by\nthe Program Committee of HSB 2013. The papers in this volume address the hybrid\nmodeling of a number important biological processes (iron homeostasis network,\nmammalian cell cycle, vascular endothelial growth factor (VEGF), genetic\nregulatory network in mammalian sclera) and, the formalisms and techniques for\nspecifying and validating properties of biological systems (such as,\nrobustness, oscillations).\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 00:25:04 GMT"}], "update_date": "2013-08-28", "authors_parsed": [["Dang", "Thao", ""], ["Piazza", "Carla", ""]]}, {"id": "1308.5846", "submitter": "Matthew Knepley", "authors": "Brad T. Aagaard and Matthew G. Knepley and Charles A. Williams", "title": "A Domain Decomposition Approach to Implementing Fault Slip in\n  Finite-Element Models of Quasi-static and Dynamic Crustal Deformation", "comments": "14 pages, 15 figures", "journal-ref": "Journal of Geophysical Research, 118(6), pp.3059-3079, 2013", "doi": "10.1002/jgrb.50217", "report-no": null, "categories": "physics.geo-ph cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ a domain decomposition approach with Lagrange multipliers to\nimplement fault slip in a finite-element code, PyLith, for use in both\nquasi-static and dynamic crustal deformation applications. This integrated\napproach to solving both quasi-static and dynamic simulations leverages common\nfinite-element data structures and implementations of various boundary\nconditions, discretization schemes, and bulk and fault rheologies. We have\ndeveloped a custom preconditioner for the Lagrange multiplier portion of the\nsystem of equations that provides excellent scalability with problem size\ncompared to conventional additive Schwarz methods. We demonstrate application\nof this approach using benchmarks for both quasi-static viscoelastic\ndeformation and dynamic spontaneous rupture propagation that verify the\nnumerical implementation in PyLith.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 12:46:37 GMT"}], "update_date": "2013-08-28", "authors_parsed": [["Aagaard", "Brad T.", ""], ["Knepley", "Matthew G.", ""], ["Williams", "Charles A.", ""]]}, {"id": "1308.5906", "submitter": "Cyril Voyant", "authors": "Cyril Voyant (SPE, CHD Castellucio), Daniel Julian (CHD Castellucio),\n  Rudy Roustit (CHD Castellucio), Katia Biffi (CHD Castellucio), Celine\n  Lantieri Marcovici (CHD Castellucio)", "title": "Biological effects and equivalent doses in radiotherapy: a software\n  solution", "comments": null, "journal-ref": "reports of practical oncology and radiotherapy (2013)", "doi": null, "report-no": null, "categories": "cs.CE physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The limits of TDF (time, dose, and fractionation) and linear quadratic models\nhave been known for a long time. Medical physicists and physicians are required\nto provide fast and reliable interpretations regarding the delivered doses or\nany future prescriptions relating to treatment changes. We therefore propose a\ncalculation interface under the GNU license to be used for equivalent doses,\nbiological doses, and normal tumor complication probability (Lyman model). The\nmethodology used draws from several sources: the linear-quadratic-linear model\nof Astrahan, the repopulation effects of Dale, and the prediction of\nmulti-fractionated treatments of Thames. The results are obtained from an\nalgorithm that minimizes an ad-hoc cost function, and then compared to the\nequivalent dose computed using standard calculators in seven French\nradiotherapy centers.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2013 12:08:54 GMT"}], "update_date": "2013-08-28", "authors_parsed": [["Voyant", "Cyril", "", "SPE, CHD Castellucio"], ["Julian", "Daniel", "", "CHD Castellucio"], ["Roustit", "Rudy", "", "CHD Castellucio"], ["Biffi", "Katia", "", "CHD Castellucio"], ["Marcovici", "Celine Lantieri", "", "CHD Castellucio"]]}, {"id": "1308.6074", "submitter": "Sohan Seth", "authors": "Sohan Seth, Niko V\\\"alim\\\"aki, Samuel Kaski, Antti Honkela", "title": "Exploration and retrieval of whole-metagenome sequencing samples", "comments": "16 pages; additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.IR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Over the recent years, the field of whole metagenome shotgun sequencing has\nwitnessed significant growth due to the high-throughput sequencing technologies\nthat allow sequencing genomic samples cheaper, faster, and with better coverage\nthan before. This technical advancement has initiated the trend of sequencing\nmultiple samples in different conditions or environments to explore the\nsimilarities and dissimilarities of the microbial communities. Examples include\nthe human microbiome project and various studies of the human intestinal tract.\nWith the availability of ever larger databases of such measurements, finding\nsamples similar to a given query sample is becoming a central operation. In\nthis paper, we develop a content-based exploration and retrieval method for\nwhole metagenome sequencing samples. We apply a distributed string mining\nframework to efficiently extract all informative sequence $k$-mers from a pool\nof metagenomic samples and use them to measure the dissimilarity between two\nsamples. We evaluate the performance of the proposed approach on two human gut\nmetagenome data sets as well as human microbiome project metagenomic samples.\nWe observe significant enrichment for diseased gut samples in results of\nqueries with another diseased sample and very high accuracy in discriminating\nbetween different body sites even though the method is unsupervised. A software\nimplementation of the DSM framework is available at\nhttps://github.com/HIITMetagenomics/dsm-framework\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 07:28:35 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 09:57:56 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Seth", "Sohan", ""], ["V\u00e4lim\u00e4ki", "Niko", ""], ["Kaski", "Samuel", ""], ["Honkela", "Antti", ""]]}, {"id": "1308.6220", "submitter": "Jiapu Zhang", "authors": "Jiapu Zhang", "title": "Simulated annealing: in mathematical global optimization computation,\n  hybrid with local or global search, and practical applications in\n  crystallography and molecular modelling", "comments": null, "journal-ref": "[Simulated Annealing: Strategies, Potential Uses and Advantages,\n  Editors Prof. Dr. Marcos Tsuzuki & Prof. Dr. Thiago de Castro Martins, NOVA\n  Science Publishers, 2014, ISBN 978-1-63117-268-7], Chapter 1, pp. 1-34", "doi": null, "report-no": null, "categories": "math.OC cs.CE physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Simulated annealing (SA) was inspired from annealing in metallurgy, a\ntechnique involving heating and controlled cooling of a material to increase\nthe size of its crystals and reduce their defects, both are attributes of the\nmaterial that depend on its thermodynamic free energy. In this Paper, firstly\nwe will study SA in details on its practical implementation. Then, hybrid pure\nSA with local (or global) search optimization methods allows us to be able to\ndesign several effective and efficient global search optimization methods. In\norder to keep the original sense of SA, we clarify our understandings of SA in\ncrystallography and molecular modeling field through the studies of prion\namyloid fibrils.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 17:06:35 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Zhang", "Jiapu", ""]]}, {"id": "1308.6697", "submitter": "Uwe Aickelin", "authors": "Yihui Liu, Uwe Aickelin", "title": "Detect adverse drug reactions for drug Atorvastatin", "comments": "Fifth International Symposium on Computational Intelligence and\n  Design (ISCID), 213-216, 2012. arXiv admin note: substantial text overlap\n  with arXiv:1308.5144", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse drug reactions (ADRs) are big concern for public health. ADRs are one\nof most common causes to withdraw some drugs from markets. Now two major\nmethods for detecting ADRs are spontaneous reporting system (SRS), and\nprescription event monitoring (PEM). The World Health Organization (WHO)\ndefines a signal in pharmacovigilance as \"any reported information on a\npossible causal relationship between an adverse event and a drug, the\nrelationship being unknown or incompletely documented previously\". For\nspontaneous reporting systems, many machine learning methods are used to detect\nADRs, such as Bayesian confidence propagation neural network (BCPNN), decision\nsupport methods, genetic algorithms, knowledge based approaches, etc. One\nlimitation is the reporting mechanism to submit ADR reports, which has serious\nunderreporting and is not able to accurately quantify the corresponding risk.\nAnother limitation is hard to detect ADRs with small number of occurrences of\neach drug-event association in the database. In this paper we propose feature\nselection approach to detect ADRs from The Health Improvement Network (THIN)\ndatabase. First a feature matrix, which represents the medical events for the\npatients before and after taking drugs, is created by linking patients'\nprescriptions and corresponding medical events together. Then significant\nfeatures are selected based on feature selection methods, comparing the feature\nmatrix before patients take drugs with one after patients take drugs. Finally\nthe significant ADRs can be detected from thousands of medical events based on\ncorresponding features. Experiments are carried out on the drug Atorvastatin.\nGood performance is achieved.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2013 09:55:56 GMT"}], "update_date": "2013-09-02", "authors_parsed": [["Liu", "Yihui", ""], ["Aickelin", "Uwe", ""]]}]