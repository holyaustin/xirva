[{"id": "1802.00322", "submitter": "Oleksiy Kurganskyy", "authors": "O.Kurganskyy, A.J. Maksimova", "title": "The entropy of a thermodynamic graph", "comments": "13 pages, 2 figures, in russian", "journal-ref": "O.Kurganskyy, A.J.Maksimova. The entropy of a thermodynamic graph.\n  Transactions of IAMM. - Donetsk. - 2016. - Volume 30. - p. 96-108 (in\n  russian)", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an algorithmic model of heat conduction, the thermodynamic\ngraph. The thermodynamic graph is analogous to meshes in the finite difference\nmethod in the sense that the calculation of temperature is carried out at the\nvertices of the graph, and the edges indicate the direct heat exchange between\nthe vertices. Recurrence relations of heat conduction in graph are derived\nwithout using of differential equations and based on the coefficients of\nthermal conductivity and heat capacity. This approach seems to be more direct\nand flexible from the point of view of algorithmic modeling of thermodynamic\nprocess than the derivation of difference schemes from differential equations.\nWe introduce also the notion of entropy of thermodynamic graph. We find the\nmaximum length of the time step at which the entropy does not decrease in the\ngeneral case. As a result, this give us the accurate boundary of the model\nstability.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 10:29:42 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Kurganskyy", "O.", ""], ["Maksimova", "A. J.", ""]]}, {"id": "1802.00617", "submitter": "Roland Ritt", "authors": "Roland Ritt and Paul O'Leary and Christopher Josef Rothschedl and\n  Matthew Harker", "title": "Advanced Symbolic Time Series Analysis in Cyber Physical Systems", "comments": "Conference: International Work-Conference on TImeSEries Analysis\n  (ITISE 2017)", "journal-ref": "Ritt, R. et al., 2017. Advanced Symbolic Time Series Analysis In\n  Cyber Physical Systems. In O. Valenzuela et al., eds. Proceedings -\n  International work-conference on Time Series, ITISE 2017. Granada: University\n  of Granada, pp. 155-160", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents advanced symbolic time series analysis (ASTSA) for large\ndata sets emanating from cyber physical systems (CPS). The definition of CPS\nmost pertinent to this paper is: A CPS is a system with a coupling of the cyber\naspects of computing and communications with the physical aspects of dynamics\nand engineering that must abide by the laws of physics. This includes sensor\nnetworks, real-time and hybrid systems. To ensure that the computation results\nconform to the laws of physics a linear differential operator (LDO) is embedded\nin the processing channel for each sensor. In this manner the dynamics of the\nsystem can be incorporated prior to performing symbolic analysis. A non-linear\nquantization is used for the intervals corresponding to the symbols. The\nintervals are based on observed modes of the system, which can be determined\neither during an exploratory phase or online during operation of the system. A\ncomplete processing channel is called a single channel lexical analyser; one is\nmade available for each sensor on the machine being observed. The\nimplementation of LDO in the system is particularly important since it enables\nthe establishment of a causal link between the observations of the dynamic\nsystem and their cause. Without causality there can be no semantics and without\nsemantics no knowledge acquisition based on the physical background of the\nsystem being observed. Correlation alone is not a guarantee for causality. This\nwork was originally motivated from the observation of large bulk mate- rial\nhandling systems. Typically, there are $n = 150\\dots250$ sensors per machine,\nand data is collected in a multi rate manner; whereby general sensors are\nsampled with $f_s = 1Hz$ and vibration data being sampled in the kilo-hertz\nrange.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 09:48:00 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Ritt", "Roland", ""], ["O'Leary", "Paul", ""], ["Rothschedl", "Christopher Josef", ""], ["Harker", "Matthew", ""]]}, {"id": "1802.01039", "submitter": "Stefan Engblom", "authors": "Stefan Engblom", "title": "Stochastic simulation of pattern formation in growing tissue: a\n  multilevel approach", "comments": null, "journal-ref": "Bull. Math. Biol. (2018)", "doi": "10.1007/s11538-018-0454-y", "report-no": null, "categories": "cs.CE q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take up the challenge of designing realistic computational models of large\ninteracting cell populations. The goal is essentially to bring Gillespie's\ncelebrated stochastic methodology to the level of an interacting population of\ncells. Specifically, we are interested in how the gold standard of single cell\ncomputational modeling, here taken to be spatial stochastic reaction-diffusion\nmodels, may be efficiently coupled with a similar approach at the cell\npopulation level.\n  Concretely, we target a recently proposed set of pathways for pattern\nformation involving Notch-Delta signaling mechanisms. These involve\ncell-to-cell communication as mediated both via direct membrane contact sites\nas well as via cellular protrusions. We explain how to simulate the process in\ngrowing tissue using a multilevel approach and we discuss implications for\nfuture development of the associated computational methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 21:45:00 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 09:08:12 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Engblom", "Stefan", ""]]}, {"id": "1802.01492", "submitter": "Leon Thurner", "authors": "Leon Thurner, Alexander Scheidler, Alexander Probst, Martin Braun", "title": "Analysing the Degree of Meshing in Medium Voltage Target Grids - An\n  Automated Technical and Economical Impact Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are different MV grid concepts with regard to mode of operation and\nprotection system layout. The increasing installation of DG raises the question\nif the currently used concepts are still optimal for future power systems. We\npresent a methodology that allows the automated calculation and comparison of\ntarget grids within different concepts. Specifically, we consider radial grids,\nclosed ring grids and grids with switching stations. A target grid structure is\noptimized for each of those grid concepts based on geographical information. To\nmodel a realistic planning process, compliance with technical constraints for\nnormal operation, contingency behaviour and reliability figures are ensured in\nall grid concepts. We present a multiphase approach to solve the optimization\nproblem based on an iterated local search meta-heuristic. We then economically\ncompare the grids with regards to CAPEX and OPEX for primary and secondary\nequipment, to analyse which concept leads to the most overall cost-efficient\ntarget grids. Since the methodology allows an automated evaluation of a large\nnumber of grids, it can be used to draw general conclusions about the\ncost-efficiency of specific concepts. The methodology is applied to 44 real MV\ngrids spanning about 4800km of lines, for which the results show that a radial\ngrid structure is overall cost efficient compared to grid topologies with\nswitching stations or closed rings even in grid areas with a large DG\npenetration. The contribution of this paper is threefold: first, a\ncomprehensive methodology to compile automated target grid plans under\nrealistic premises is presented. Second, the practical applicability of the\napproach is demonstrated with a large scale case study with a high degree of\nautomation. And third, the results of the case study allow to draw conclusions\nabout the techno-economical differences of different MV grid concepts.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 16:06:26 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Thurner", "Leon", ""], ["Scheidler", "Alexander", ""], ["Probst", "Alexander", ""], ["Braun", "Martin", ""]]}, {"id": "1802.01502", "submitter": "Leon Thurner", "authors": "Leon Thurner, Martin Braun", "title": "Vectorized Calculation of Short Circuit Currents Considering Distributed\n  Generation - An Open Source Implementation of IEC 60909", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in grid planning is to ensure that faults in the grid are\ndetected and cut off without damage in any grid element. Calculating short\ncircuit currents is therefore a vital grid analysis functionality for grid\nplanning applications. The standard IEC 60909 provides guidelines for short\ncircuit calculations and is routinely applied in grid planning applications.\nThis paper presents a method for the vectorized calculation of short circuit\ncurrents according to IEC 60909. Distributed generation units are considered\naccording to the latest revision of the standard. The method is implemented in\nthe python based open source tool pandapower and validated against commercial\nsoftware and examples from literature. The implementation presented in this\npaper is the first comprehensive implementation of the IEC 60909 standard which\nis available under an open source license. It can be used to evaluate fault\ncurrents in grid studies with a high degree of automation and is shown to scale\nwell for large grids. Its practical applicability is shown in a case study with\na real MV grid with a high degree of DG penetration.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 16:18:47 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Thurner", "Leon", ""], ["Braun", "Martin", ""]]}, {"id": "1802.01861", "submitter": "Javier Franco-Pedroso", "authors": "Javier Franco-Pedroso and Joaquin Gonzalez-Rodriguez and Jorge Cubero\n  and Maria Planas and Rafael Cobo and Fernando Pablos", "title": "Generating virtual scenarios of multivariate financial data for\n  quantitative trading applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to the generation of virtual\nscenarios of multivariate financial data of arbitrary length and composition of\nassets. With this approach, decades of realistic time-synchronized data can be\nsimulated for a large number of assets, producing diverse scenarios to test and\nimprove quantitative investment strategies. Our approach is based on the\nanalysis and synthesis of the time-dependent individual and joint\ncharacteristics of real financial time series, using stochastic sequences of\nmarket trends to draw multivariate returns from time-dependent probability\nfunctions preserving both distributional properties of asset returns and\ntime-dependent correlation among time series. Moreover, new time-synchronized\nassets can be arbitrarily generated through a PCA-based procedure to obtain any\nnumber of assets in the final virtual scenario. For the validation of such\nsimulated data, they are tested with an extensive set of measurements showing a\nsignificant degree of agreement with the reference performance of real\nfinancial series, better than that obtained with other classical and\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 09:43:40 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Franco-Pedroso", "Javier", ""], ["Gonzalez-Rodriguez", "Joaquin", ""], ["Cubero", "Jorge", ""], ["Planas", "Maria", ""], ["Cobo", "Rafael", ""], ["Pablos", "Fernando", ""]]}, {"id": "1802.02123", "submitter": "Antonio Huerta", "authors": "Ruben Sevilla, Matteo Giacomini, Alexandros Karkoulias, and Antonio\n  Huerta", "title": "A super--convergent hybridisable discontinuous Galerkin method for\n  linear elasticity", "comments": "37 pages, 18 figures", "journal-ref": "Int. J. Numer. Methods Eng. Vol. 116, Issue 2, pp. 91-116 (2018)", "doi": "10.1002/nme.5916", "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first super-convergent hybridisable discontinuous Galerkin (HDG) method\nfor linear elastic problems capable of using the same degree of approximation\nfor both the primal and mixed variables is presented. The key feature of the\nmethod is the strong imposition of the symmetry of the stress tensor by means\nof the well-known and extensively used Voigt notation, circumventing the use of\ncomplex mathematical concepts to enforce the symmetry of the stress tensor\neither weakly or strongly. A novel procedure to construct element-by-element a\nsuper-convergent post-processed displacement is proposed. Contrary to other HDG\nformulations, the methodology proposed here is able to produce a\nsuper-convergent displacement field for low order approximations. The resulting\nmethod is robust and locking-free in the nearly-incompressible limit. An\nextensive set of numerical examples is utilised to provide evidence of the\noptimality of the method and its super-convergent properties in two and three\ndimensions and for different element types.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 18:39:21 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Sevilla", "Ruben", ""], ["Giacomini", "Matteo", ""], ["Karkoulias", "Alexandros", ""], ["Huerta", "Antonio", ""]]}, {"id": "1802.02258", "submitter": "Vincenzo Gulizzi", "authors": "Vincenzo Gulizzi", "title": "A computational framework for microstructural modelling of\n  polycrystalline materials with damage and failure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present thesis, a computational framework for the analysis of the\ndeformation and damage phenomena occurring at the micro-scale of\npolycrystalline materials is presented.\n  Micro-mechanics studies are commonly performed using the Finite Element\nMethod (FEM) for its versatility and robustness. However, finite element\nformulations usually lead to an extremely high number of degrees of freedom of\nthe considered micro-structures, thus making alternative formulations of great\nengineering interest. Among the others, the Boundary Element Method (BEM)\nrepresents a viable alternative to FEM approaches as it allows to express the\nproblem in terms of boundary values only, thus reducing the total number of\ndegrees of freedom.\n  The computational framework developed in this thesis is based on a non-linear\nmulti-domain BEM approach for generally anisotropic materials and is devoted to\nthe analysis of three-dimensional polycrystalline microstructures. Different\ntheoretical and numerical aspects of the polycrystalline problem using the\nboundary element method are investigated: first, being the formulation based on\na integral representation of the governing equations, a novel and more compact\nexpression of the integration kernels capable of representing the multi-field\nbehaviour of generally anisotropic materials is presented; second, the sources\nof the high computational cost of polycrystalline analyses are identified and\nsuitably treated by means of different strategies including an ad-hoc grain\nboundary meshing technique developed to tackle the large statistical\nvariability of polycrystalline micro-morphologies; third, non-linear\ndeformation and failure mechanisms such as inter-granular and trans-granular\ncracking and generally anisotropic crystal plasticity are studied and the\nnumerical results presented throughout the thesis demonstrate the potential of\nthe developed framework.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 23:19:45 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Gulizzi", "Vincenzo", ""]]}, {"id": "1802.02379", "submitter": "Federico D'Ambrosio", "authors": "Federico D'Ambrosio, Hans L. Bodlaender and Gerard T. Barkema", "title": "Dynamic Sampling from a Discrete Probability Distribution with a Known\n  Distribution of Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a number of efficient data structures for the\nproblem of sampling from a dynamically changing discrete probability\ndistribution, where some prior information is known on the distribution of the\nrates, in particular the maximum and minimum rate, and where the number of\npossible outcomes N is large.\n  We consider three basic data structures, the Acceptance-Rejection method, the\nComplete Binary Tree and the Alias Method. These can be used as building blocks\nin a multi-level data structure, where at each of the levels, one of the basic\ndata structures can be used.\n  Depending on assumptions on the distribution of the rates of outcomes,\ndifferent combinations of the basic structures can be used. We prove that for\nparticular data structures the expected time of sampling and update is\nconstant, when the rates follow a non-decreasing distribution, log-uniform\ndistribution or an inverse polynomial distribution, and show that for any\ndistribution, an expected time of sampling and update of\n$O\\left(\\log\\log{r_{max}}/{r_{min}}\\right)$ is possible, where $r_{max}$ is the\nmaximum rate and $r_{min}$ the minimum rate.\n  We also present an experimental verification, highlighting the limits given\nby the constraints of a real-life setting.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 10:40:05 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 09:20:28 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2020 08:41:59 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2020 13:34:19 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["D'Ambrosio", "Federico", ""], ["Bodlaender", "Hans L.", ""], ["Barkema", "Gerard T.", ""]]}, {"id": "1802.02474", "submitter": "Navjot Kukreja", "authors": "Navjot Kukreja, Jan H\\\"uckelheim, Michael Lange, Mathias Louboutin,\n  Andrea Walther, Simon W. Funke, Gerard Gorman", "title": "High-level python abstractions for optimal checkpointing in inversion\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Inversion and PDE-constrained optimization problems often rely on solving the\nadjoint problem to calculate the gradient of the objec- tive function. This\nrequires storing large amounts of intermediate data, setting a limit to the\nlargest problem that might be solved with a given amount of memory available.\nCheckpointing is an approach that can reduce the amount of memory required by\nredoing parts of the computation instead of storing intermediate results. The\nRevolve checkpointing algorithm o ers an optimal schedule that trades\ncomputational cost for smaller memory footprints. Integrat- ing Revolve into a\nmodern python HPC code and combining it with code generation is not\nstraightforward. We present an API that makes checkpointing accessible from a\nDSL-based code generation environment along with some initial performance gures\nwith a focus on seismic applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 16:02:09 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Kukreja", "Navjot", ""], ["H\u00fcckelheim", "Jan", ""], ["Lange", "Michael", ""], ["Louboutin", "Mathias", ""], ["Walther", "Andrea", ""], ["Funke", "Simon W.", ""], ["Gorman", "Gerard", ""]]}, {"id": "1802.02976", "submitter": "Tobin Isaac", "authors": "Tobin Isaac", "title": "A mixed finite element for weakly-symmetric elasticity", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a finite element discretization for the weakly symmetric equations\nof linear elasticity on tetrahedral meshes. The finite element combines, for $r\n\\geq 0$, discontinuous polynomials of $r$ for the displacement,\n$H(\\mathrm{div})$-conforming polynomials of order $r+1$ for the stress, and\n$H(\\mathrm{curl})$-conforming polynomials of order $r+1$ for the vector\nrepresentation of the multiplier. We prove that this triplet is stable and has\noptimal approximation properties. The lowest order case can be combined with\ninexact quadrature to eliminate the stress and multiplier variables, leaving a\ncompact cell-centered finite volume scheme for the displacement.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 17:38:36 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Isaac", "Tobin", ""]]}, {"id": "1802.02978", "submitter": "Niklas Georg", "authors": "Niklas Georg, Wolfgang Ackermann, Jacopo Corno, Sebastian Sch\\\"ops", "title": "Uncertainty Quantification for Maxwell's Eigenproblem based on\n  Isogeometric Analysis and Mode Tracking", "comments": null, "journal-ref": "Comput. Method Appl. M., 350(15): 228-244, 2019", "doi": "10.1016/j.cma.2019.03.002", "report-no": null, "categories": "cs.CE cs.NA math.NA physics.acc-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electromagnetic field distribution as well as the resonating frequency of\nvarious modes in superconducting cavities used in particle accelerators for\nexample are sensitive to small geometry deformations. The occurring variations\nare motivated by measurements of an available set of resonators from which we\npropose to extract a small number of relevant and independent deformations by\nusing a truncated Karhunen-Lo\\`eve expansion. The random deformations are used\nin an expressive uncertainty quantification workflow to determine the\nsensitivity of the eigenmodes. For the propagation of uncertainty, a stochastic\ncollocation method based on sparse grids is employed. It requires the repeated\nsolution of Maxwell's eigenvalue problem at predefined collocation points,\ni.e., for cavities with perturbed geometry. The main contribution of the paper\nis ensuring the consistency of the solution, i.e., matching the eigenpairs,\namong the various eigenvalue problems at the stochastic collocation points. To\nthis end, a classical eigenvalue tracking technique is proposed that is based\non homotopies between collocation points and a Newton-based eigenvalue solver.\nThe approach can be efficiently parallelized while tracking the eigenpairs. In\nthis paper, we propose the application of isogeometric analysis since it allows\nfor the exact description of the geometrical domains with respect to common\ncomputer-aided design kernels, for a straightforward and convenient way of\nhandling geometrical variations and smooth solutions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 17:40:03 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 20:31:38 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 16:21:35 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Georg", "Niklas", ""], ["Ackermann", "Wolfgang", ""], ["Corno", "Jacopo", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "1802.03064", "submitter": "Dirk Pfl\\\"uger", "authors": "Markus K\\\"oppel and Fabian Franzelin and Ilja Kr\\\"oker and Sergey\n  Oladyshkin and Gabriele Santin and Dominik Wittwar and Andrea Barth and\n  Bernard Haasdonk and Wolfgang Nowak and Dirk Pfl\\\"uger and Christian Rohde", "title": "Comparison of data-driven uncertainty quantification methods for a\n  carbon dioxide storage benchmark scenario", "comments": null, "journal-ref": null, "doi": "10.1007/s10596-018-9785-x", "report-no": null, "categories": "cs.CE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variety of methods is available to quantify uncertainties arising with\\-in\nthe modeling of flow and transport in carbon dioxide storage, but there is a\nlack of thorough comparisons. Usually, raw data from such storage sites can\nhardly be described by theoretical statistical distributions since only very\nlimited data is available. Hence, exact information on distribution shapes for\nall uncertain parameters is very rare in realistic applications. We discuss and\ncompare four different methods tested for data-driven uncertainty\nquantification based on a benchmark scenario of carbon dioxide storage. In the\nbenchmark, for which we provide data and code, carbon dioxide is injected into\na saline aquifer modeled by the nonlinear capillarity-free fractional flow\nformulation for two incompressible fluid phases, namely carbon dioxide and\nbrine. To cover different aspects of uncertainty quantification, we incorporate\nvarious sources of uncertainty such as uncertainty of boundary conditions, of\nconceptual model definitions and of material properties. We consider recent\nversions of the following non-intrusive and intrusive uncertainty\nquantification methods: arbitary polynomial chaos, spatially adaptive sparse\ngrids, kernel-based greedy interpolation and hybrid stochastic Galerkin. The\nperformance of each approach is demonstrated assessing expectation value and\nstandard deviation of the carbon dioxide saturation against a reference\nstatistic based on Monte Carlo sampling. We compare the convergence of all\nmethods reporting on accuracy with respect to the number of model runs and\nresolution. Finally we offer suggestions about the methods' advantages and\ndisadvantages that can guide the modeler for uncertainty quantification in\ncarbon dioxide storage and beyond.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 22:27:38 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["K\u00f6ppel", "Markus", ""], ["Franzelin", "Fabian", ""], ["Kr\u00f6ker", "Ilja", ""], ["Oladyshkin", "Sergey", ""], ["Santin", "Gabriele", ""], ["Wittwar", "Dominik", ""], ["Barth", "Andrea", ""], ["Haasdonk", "Bernard", ""], ["Nowak", "Wolfgang", ""], ["Pfl\u00fcger", "Dirk", ""], ["Rohde", "Christian", ""]]}, {"id": "1802.03211", "submitter": "Miriam Mehl", "authors": "Chris Bradley, Nehzat Emamy, Thomas Ertl, Dominik G\\\"oddeke, Andreas\n  Hessenthaler, Thomas Klotz, Aaron Kr\\\"amer, Michael Krone, Benjamin Maier,\n  Miriam Mehl, Tobias Rau, Oliver R\\\"ohrle", "title": "Towards realistic HPC models of the neuromuscular system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic simulations of detailed, biophysics-based, multi-scale models\nrequire very high resolution and, thus, large-scale compute facilities.\nExisting simulation environments, especially for biomedical applications, are\ndesigned to allow for a high flexibility and generality in model development.\nFlexibility and model development, however, are often a limiting factor for\nlarge-scale simulations. Therefore, new models are typically tested and run on\nsmall-scale compute facilities. By using a detailed biophysics-based,\nchemo-electromechanical skeletal muscle model and the international open-source\nsoftware library OpenCMISS as an example, we present an approach to upgrade an\nexisting muscle simulation framework from a moderately parallel version towards\na massively parallel one that scales both in terms of problem size and in terms\nof the number of parallel processes. For this purpose, we investigate different\nmodeling, algorithmic and implementational aspects. We present improvements\naddressing both numerical and parallel scalability. In addition, our approach\nincludes a novel visualization environment, which is based on the MegaMol\nenvironment capable of handling large amounts of simulated data. It offers a\nplatform for fast visualization prototyping, distributed rendering, and\nadvanced visualization techniques. We present results of a variety of scaling\nstudies at the Tier-1 supercomputer HazelHen at the High Performance Computing\nCenter Stuttgart (HLRS). We improve the overall runtime by a factor of up to\n2.6 and achieved good scalability on up to 768 cores, where the previous\nimplementation used only 4 cores.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 11:44:13 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Bradley", "Chris", ""], ["Emamy", "Nehzat", ""], ["Ertl", "Thomas", ""], ["G\u00f6ddeke", "Dominik", ""], ["Hessenthaler", "Andreas", ""], ["Klotz", "Thomas", ""], ["Kr\u00e4mer", "Aaron", ""], ["Krone", "Michael", ""], ["Maier", "Benjamin", ""], ["Mehl", "Miriam", ""], ["Rau", "Tobias", ""], ["R\u00f6hrle", "Oliver", ""]]}, {"id": "1802.04016", "submitter": "Pierre Baque", "authors": "Pierre Baqu\\'e, Edoardo Remelli, Fran\\c{c}ois Fleuret and Pascal Fua", "title": "Geodesic Convolutional Shape Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerodynamic shape optimization has many industrial applications. Existing\nmethods, however, are so computationally demanding that typical engineering\npractices are to either simply try a limited number of hand-designed shapes or\nrestrict oneself to shapes that can be parameterized using only few degrees of\nfreedom. In this work, we introduce a new way to optimize complex shapes fast\nand accurately. To this end, we train Geodesic Convolutional Neural Networks to\nemulate a fluidynamics simulator. The key to making this approach practical is\nremeshing the original shape using a polycube map, which makes it possible to\nperform the computations on GPUs instead of CPUs. The neural net is then used\nto formulate an objective function that is differentiable with respect to the\nshape parameters, which can then be optimized using a gradient-based technique.\nThis outperforms state- of-the-art methods by 5 to 20% for standard problems\nand, even more importantly, our approach applies to cases that previous methods\ncannot handle.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 12:53:22 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Baqu\u00e9", "Pierre", ""], ["Remelli", "Edoardo", ""], ["Fleuret", "Fran\u00e7ois", ""], ["Fua", "Pascal", ""]]}, {"id": "1802.04243", "submitter": "Kiril Shterev", "authors": "Kiril S. Shterev", "title": "GPU implementation of algorithm SIMPLE-TS for calculation of unsteady,\n  viscous, compressible and heat-conductive gas flows", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent trend of using Graphics Processing Units (GPU's) for high\nperformance computations is driven by the high ratio of price performance for\nthese units, complemented by their cost effectiveness. At first glance,\ncomputational fluid dynamics (CFD) solvers match perfectly to GPU resources\nbecause these solvers make intensive calculations and use relatively little\nmemory. Nevertheless, there are scarce results about the practical use of this\nserious advantage of GPU over CPU, especially for calculations of viscous,\ncompressible, heat-conductive gas flows with double precision accuracy. In this\npaper, two GPU algorithms according to time approximation of convective terms\nwere presented: explicit and implicit scheme. To decrease data transfers\nbetween device memories and increase the arithmetic intensity of a GPU code we\nminimize the number of kernels. The GPU algorithm was implemented in one kernel\nfor the implicit scheme and two kernels for the explicit scheme. The numerical\nequations were put together using macros and optimization, data copy from\nglobal to private memory, and data reuse were left to the compiler. Thus keeps\nthe code simpler with excellent maintenance. As a test case, we model the flow\npast squares in a microchannel at supersonic speed. The tests show that overall\nspeedup of AMD Radeon R9 280X is up to 102x compared to Intel Core i5-4690 core\nand up to 184x compared to Intel Core i7-920 core, while speedup of NVIDIA\nTesla M2090 is up to 11x compared to Intel Core i5-4690 core and up to 20x\ncompared to Intel Core i7-920 core. Memory requirements of GPU code are\nimproved compared to CPU one. It requires 1[GB] global memory for 5.9 million\nfinite volumes that are two times less compared to C++ CPU code. After all the\ncode is simple, portable (written in OpenCL), memory efficient and easily\nmodifiable moreover demonstrates excellent performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:45:13 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Shterev", "Kiril S.", ""]]}, {"id": "1802.04256", "submitter": "Gang Mei", "authors": "Gang Mei, Nengxiong Xu, Liangliang Xu, Yazhe Li", "title": "GeoMFree3D: An Under-Development Meshfree Software Package for\n  Geomechanics", "comments": "5 figures", "journal-ref": "Computers & Mathematics with Applications, Available online 3 June\n  2020", "doi": "10.1016/j.camwa.2020.05.020", "report-no": null, "categories": "physics.comp-ph cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper briefly reports the GeoMFree3D, a meshfree / meshless software\npackage designed for analyzing the problems of large deformations and crack\npropagations of rock and soil masses in geotechnics. The GeoMFree3D is\ndeveloped based on the meshfree RPIM, and accelerated by exploiting the\nparallel computing on multi-core CPU and many-core GPU. The GeoMFree3D is\ncurrently being under intensive developments. To demonstrate the correctness\nand effectiveness of the GeoMFree3D, several simple verification examples are\npresented in this paper. Moreover, future work on the development of the\nGeoMFree3D is introduced.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 15:59:52 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Mei", "Gang", ""], ["Xu", "Nengxiong", ""], ["Xu", "Liangliang", ""], ["Li", "Yazhe", ""]]}, {"id": "1802.04353", "submitter": "Yu Jin", "authors": "Yu Jin, Joseph F. JaJa, Rong Chen, Edward H. Herskovits", "title": "A Data-Driven Approach to Extract Connectivity Structures from Diffusion\n  Tensor Imaging Data", "comments": "Proceedings of 2015 IEEE International Conference on Big Data", "journal-ref": null, "doi": "10.1109/BigData.2015.7363843", "report-no": null, "categories": "cs.CE q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion Tensor Imaging (DTI) is an effective tool for the analysis of\nstructural brain connectivity in normal development and in a broad range of\nbrain disorders. However efforts to derive inherent characteristics of\nstructural brain networks have been hampered by the very high dimensionality of\nthe data, relatively small sample sizes, and the lack of widely acceptable\nconnectivity-based regions of interests (ROIs). Typical approaches have focused\neither on regions defined by standard anatomical atlases that do not\nincorporate anatomical connectivity, or have been based on voxel-wise analysis,\nwhich results in loss of statistical power relative to structure-wise\nconnectivity analysis. In this work, we propose a novel, computationally\nefficient iterative clustering method to generate connectivity-based\nwhole-brain parcellations that converge to a stable parcellation in a few\niterations. Our algorithm is based on a sparse representation of the whole\nbrain connectivity matrix, which reduces the number of edges from around a half\nbillion to a few million while incorporating the necessary spatial constraints.\nWe show that the resulting regions in a sense capture the inherent connectivity\ninformation present in the data, and are stable with respect to initialization\nand the randomization scheme within the algorithm. These parcellations provide\nconsistent structural regions across the subjects of population samples that\nare homogeneous with respect to anatomic connectivity. Our method also derives\nconnectivity structures that can be used to distinguish between population\nsamples with known different structural connectivity. In particular, new\nresults in structural differences for different population samples such as\nFemales vs Males, Normal Controls vs Schizophrenia, and different age groups in\nNormal Controls are also shown.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 20:42:36 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Jin", "Yu", ""], ["JaJa", "Joseph F.", ""], ["Chen", "Rong", ""], ["Herskovits", "Edward H.", ""]]}, {"id": "1802.04750", "submitter": "Denis Jeambrun", "authors": "Denis Jeambrun (CEM2), Yves Moreau (CEM2), Jean-Louis Costaz (Septen),\n  Jean-Pierre Tourret (Septen), Paul Jouanna, Gilles Lecoy (CEM2)", "title": "Simulation of the propagation of a cylindrical shear wave : non linear\n  and dissipative modelling", "comments": null, "journal-ref": "Technical University of BRNO. A.M.S.E. CSS' 96, Sep 1996, BRNO,\n  Czech Republic. Volume 2, pp. 587-591, 1996, A.M.S.E. CSS (Communications,\n  Signals and Systems)", "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CE nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of a wave propagation caused by seismic stimulation allows to\nstudy the behaviour of the environment and to evaluate the consequences. The\nmodel involves the wave equation with a hysteresis loop in the stress-strain\nrelationship. This induces non-linearities and, at the vertices of the loop,\nnon-differentiable mathematical operators. This paper offers a numerical\nprocess which works out this simulation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 09:59:30 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Jeambrun", "Denis", "", "CEM2"], ["Moreau", "Yves", "", "CEM2"], ["Costaz", "Jean-Louis", "", "Septen"], ["Tourret", "Jean-Pierre", "", "Septen"], ["Jouanna", "Paul", "", "CEM2"], ["Lecoy", "Gilles", "", "CEM2"]]}, {"id": "1802.05029", "submitter": "Gabriele Pozzetti", "authors": "Gabriele Pozzetti, Xavier Besseron, Alban Rousset, Bernhard Peters", "title": "A co-located partitions strategy for parallel CFD-DEM couplings", "comments": null, "journal-ref": null, "doi": "10.1016/j.apt.2018.08.025", "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a new partition-collocation strategy for the parallel execution\nof CFD--DEM couplings is investigated. Having a good parallel performance is a\nkey issue for an Eulerian-Lagrangian software that aims to be applied to solve\nindustrially significant problems, as the computational cost of these couplings\nis one of their main drawback. The approach presented here consists in\nco-locating the overlapping parts of the simulation domain of each software on\nthe same MPI process, in order to reduce the cost of the data exchanges. It is\nshown how this strategy allows reducing memory consumption and inter-process\ncommunication between CFD and DEM to a minimum and therefore to overcome an\nimportant parallelization bottleneck identified in the literature. Three\nbenchmarks are proposed to assess the consistency and scalability of this\napproach. A coupled execution on 280 cores shows that less than 0.1% of the\ntime is used to perform inter-physics data exchange.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 10:39:38 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Pozzetti", "Gabriele", ""], ["Besseron", "Xavier", ""], ["Rousset", "Alban", ""], ["Peters", "Bernhard", ""]]}, {"id": "1802.05170", "submitter": "Chuan Zhang", "authors": "Chuan Zhang (1 and 2 and 3), Ziyuan Shen (1 and 2 and 3), Wei Wei (4\n  and 5), Jing Zhao (4 and 5), Zaichen Zhang (2 and 3), Xiaohu You (3) ((1) Lab\n  of Efficient Architectures for Digital-communication and Signal-processing\n  (LEADS), (2) Quantum Information Center of Southeast University, (3) National\n  Mobile Communications Research Laboratory, Southeast University, China, (4)\n  State Key Laboratory of Coordination Chemistry, School of Chemistry and\n  Chemical Engineering, Nanjing University, China, (5) State Key Laboratory of\n  Pharmaceutical Biotechnology, School of Life Sciences, Nanjing)", "title": "Molecular Computing for Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.CC cs.CE cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, it is presented a methodology for implementing arbitrarily\nconstructed time-homogenous Markov chains with biochemical systems. Not only\ndiscrete but also continuous-time Markov chains are allowed to be computed. By\nemploying chemical reaction networks (CRNs) as a programmable language,\nmolecular concentrations serve to denote both input and output values. One\nreaction network is elaborately designed for each chain. The evolution of\nspecies' concentrations over time well matches the transient solutions of the\ntarget continuous-time Markov chain, while equilibrium concentrations can\nindicate the steady state probabilities. Additionally, second-order Markov\nchains are considered for implementation, with bimolecular reactions rather\nthat unary ones. An original scheme is put forward to compile unimolecular\nsystems to DNA strand displacement reactions for the sake of future physical\nimplementations. Deterministic, stochastic and DNA simulations are provided to\nenhance correctness, validity and feasibility.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 15:52:11 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Zhang", "Chuan", "", "1 and 2 and 3"], ["Shen", "Ziyuan", "", "1 and 2 and 3"], ["Wei", "Wei", "", "4\n  and 5"], ["Zhao", "Jing", "", "4 and 5"], ["Zhang", "Zaichen", "", "2 and 3"], ["You", "Xiaohu", ""]]}, {"id": "1802.05541", "submitter": "Mohammed Ishaquddin", "authors": "Md.Ishaquddin, S.Gopalakrishnan", "title": "Novel weak form quadrature elements for non-classical higher order beam\n  and plate theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on Lagrange and Hermite interpolation two novel versions of weak form\nquadrature element are proposed for a non-classical Euler-Bernoulli beam\ntheory. By extending these concept two new plate elements are formulated using\nLagrange-Lagrange and mixed Lagrange-Hermite interpolations for a non-classical\nKirchhoff plate theory. The non-classical theories are governed by sixth order\npartial differential equation and have deflection, slope and curvature as de-\ngrees of freedom. A novel and generalize way is proposed herein to implement\nthese degrees of freedom in a simple and efficient manner. A new procedure to\ncompute the modified weighting coefficient matri- ces for beam and plate\nelements is presented. The proposed elements have displacement as the only\ndegree of freedom in the element do- main and displacement, slope and curvature\nat the boundaries. The Gauss-Lobatto-Legender quadrature points are considered\nas element nodes and also used for numerical integration of the element\nmatrices. The framework for computing the stiffness matrices at the integra-\ntion points is analogous to the conventional finite element method. Numerical\nexamples on free vibration analysis of gradient beams and plates are presented\nto demonstrate the efficiency and accuracy of the proposed elements.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 11:43:05 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Ishaquddin", "Md.", ""], ["Gopalakrishnan", "S.", ""]]}, {"id": "1802.05739", "submitter": "Kristian Jensen", "authors": "Kristian Ejlebjerg Jensen, Peter Szabo, Fridolin Okkels", "title": "A Viscoelastic Catastrophe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cond-mat.soft cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a differential constitutive equation to model the flow of a\nviscoelastic flow in a cross-slot geometry, which is known to exhibit\nbistability above a critical flow rate. The novelty lies in two asymmetric\nmodifications to the geometry, which causes a change in the bifurcation diagram\nsuch that one of the stable solutions becomes disconnected from the solution at\nlow flow speeds. First we show that it is possible to mirror one of the\nmodifications such that the system can be forced to the disconnected solution.\nThen we show that a slow decrease of the flow rate, can cause the system to go\nthrough a drastic change on a short time scale, also known as a catastrophe.\nThe short time scale could lead to a precise and simple experimental\nmeasurement of the flow conditions at which the viscoelastic catastrophe\noccurs. Since the phenomena is intrinsically related to the extensional\nrheology of the fluid, we propose to exploit the phenomena for in-line\nextensional rheometry.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 09:52:42 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Jensen", "Kristian Ejlebjerg", ""], ["Szabo", "Peter", ""], ["Okkels", "Fridolin", ""]]}, {"id": "1802.05856", "submitter": "Hector Zenil", "authors": "Hector Zenil, Narsis A. Kiani, Ming-Mei Shang and Jesper Tegn\\'er", "title": "Algorithmic Complexity and Reprogrammability of Chemical Structure\n  Networks", "comments": "19 pages + Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.CE cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we address the challenge of profiling causal properties and tracking the\ntransformation of chemical compounds from an algorithmic perspective. We\nexplore the potential of applying a computational interventional calculus based\non the principles of algorithmic probability to chemical structure networks. We\nprofile the sensitivity of the elements and covalent bonds in a chemical\nstructure network algorithmically, asking whether reprogrammability affords\ninformation about thermodynamic and chemical processes involved in the\ntransformation of different compound classes. We arrive at numerical results\nsuggesting a correspondence between some physical, structural and functional\nproperties. Our methods are capable of separating chemical classes that reflect\nfunctional and natural differences without considering any information about\natomic and molecular properties. We conclude that these methods, with their\nlinks to chemoinformatics via algorithmic, probability hold promise for future\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 07:52:47 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 14:25:12 GMT"}, {"version": "v3", "created": "Sun, 18 Mar 2018 05:35:52 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Zenil", "Hector", ""], ["Kiani", "Narsis A.", ""], ["Shang", "Ming-Mei", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1802.05982", "submitter": "Chuan Zhang", "authors": "Chuan Zhang (1 and 2 and 3), Yufeng Yang (1 and 2 and 3), Shunqing\n  Zhang (4), Zaichen Zhang (2 and 3), Xiaohu You (2) ((1) Lab of Efficient\n  Architectures for Digital-communication and Signal-processing (LEADS), (2)\n  National Mobile Communications Research Laboratory, (3) Quantum Information\n  Center, Southeast University, China, (4) Shanghai Institute for Advanced\n  Communications and Data Science, Shanghai University, Shanghai, China)", "title": "Residual-Based Detections and Unified Architecture for Massive MIMO\n  Uplink", "comments": "submitted to Journal of Signal Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AR cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive multiple-input multiple-output (M-MIMO) technique brings better\nenergy efficiency and coverage but higher computational complexity than\nsmall-scale MIMO. For linear detections such as minimum mean square error\n(MMSE), prohibitive complexity lies in solving large-scale linear equations.\nFor a better trade-off between bit-error-rate (BER) performance and\ncomputational complexity, iterative linear algorithms like conjugate gradient\n(CG) have been applied and have shown their feasibility in recent years. In\nthis paper, residual-based detection (RBD) algorithms are proposed for M-MIMO\ndetection, including minimal residual (MINRES) algorithm, generalized minimal\nresidual (GMRES) algorithm, and conjugate residual (CR) algorithm. RBD\nalgorithms focus on the minimization of residual norm per iteration, whereas\nmost existing algorithms focus on the approximation of exact signal. Numerical\nresults have shown that, for $64$-QAM $128\\times 8$ MIMO, RBD algorithms are\nonly $0.13$ dB away from the exact matrix inversion method when BER$=10^{-4}$.\nStability of RBD algorithms has also been verified in various correlation\nconditions. Complexity comparison has shown that, CR algorithm require $87\\%$\nless complexity than the traditional method for $128\\times 60$ MIMO. The\nunified hardware architecture is proposed with flexibility, which guarantees a\nlow-complexity implementation for a family of RBD M-MIMO detectors.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 10:54:31 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Zhang", "Chuan", "", "1 and 2 and 3"], ["Yang", "Yufeng", "", "1 and 2 and 3"], ["Zhang", "Shunqing", "", "2 and 3"], ["Zhang", "Zaichen", "", "2 and 3"], ["You", "Xiaohu", ""]]}, {"id": "1802.06013", "submitter": "Maurice S. Fabien", "authors": "Maurice S. Fabien, Matthew G. Knepley, Beatrice M. Riviere", "title": "A hybridizable discontinuous Galerkin method for two-phase flow in\n  heterogeneous porous media", "comments": "20 pages, 39 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for simulating incompressible immiscible two-phase\nflow in porous media. The semi-implicit method decouples the wetting phase\npressure and saturation equations. The equations are discretized using a\nhybridizable discontinuous Galerkin (HDG) method. The proposed method is of\nhigh order, conserves global/local mass balance, and the number of globally\ncoupled degrees of freedom is significantly reduced compared to standard\ninterior penalty discontinuous Galerkin methods. Several numerical examples\nillustrate the accuracy and robustness of the method. These examples include\nverification of convergence rates by manufactured solutions, common 1D\nbenchmarks and realistic discontinuous permeability fields.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 16:25:30 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Fabien", "Maurice S.", ""], ["Knepley", "Matthew G.", ""], ["Riviere", "Beatrice M.", ""]]}, {"id": "1802.06049", "submitter": "Prabhat Kumar", "authors": "Prabhat Kumar, Anupam Saxena and Roger A. Sauer", "title": "Computational synthesis of large deformation compliant mechanisms\n  undergoing self and mutual contact", "comments": "26 pages, 30 figures,", "journal-ref": "Journal of Mechanical Design, 141(1), 012302, 2018", "doi": "10.1115/1.4041054", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topologies of large deformation Contact-aided Compliant Mechanisms (CCMs),\nwith self and mutual contact, exemplified via path generation applications, are\ndesigned using the continuum synthesis approach. Design domains are\nparameterized using honeycomb tessellation. Assignment of material to each\ncell, and generation of rigid contact surfaces, are accomplished via suitably\nsizing and positioning negative circular masks. To facilitate contact analysis,\nboundary smoothing is implemented. Mean value coordinates are employed to\ncompute shape functions, as many regular hexagonal cells get degenerated into\nirregular, concave polygons as a consequence of boundary smoothing. Both,\ngeometric and material nonlinearities are considered in the finite element\nanalysis. The augmented Lagrange multiplier method in association with an\nactive set strategy is employed to incorporate both self and mutual contact.\nCCMs are evolved using the stochastic hill climber search. Synthesized\ncontact-aided compliant continua trace paths with single and importantly,\nmultiple kinks and experience multiple contact interactions pertaining to both\nself and mutual contact modes.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 17:51:43 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 15:19:53 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 21:48:55 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kumar", "Prabhat", ""], ["Saxena", "Anupam", ""], ["Sauer", "Roger A.", ""]]}, {"id": "1802.06517", "submitter": "Ahmed Attia", "authors": "Ahmed Attia, Alen Alexanderian, Arvind K. Saibaba", "title": "Goal-Oriented Optimal Design of Experiments for Large-Scale Bayesian\n  Linear Inverse Problems", "comments": "25 pages, 13 figures", "journal-ref": null, "doi": "10.1088/1361-6420/aad210", "report-no": null, "categories": "cs.CE math.NA math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for goal-oriented optimal design of experiments\n(GOODE) for large-scale Bayesian linear inverse problems governed by PDEs. This\nframework differs from classical Bayesian optimal design of experiments (ODE)\nin the following sense: we seek experimental designs that minimize the\nposterior uncertainty in the experiment end-goal, e.g., a quantity of interest\n(QoI), rather than the estimated parameter itself. This is suitable for\nscenarios in which the solution of an inverse problem is an intermediate step\nand the estimated parameter is then used to compute a QoI. In such problems, a\nGOODE approach has two benefits: the designs can avoid wastage of experimental\nresources by a targeted collection of data, and the resulting design criteria\nare computationally easier to evaluate due to the often low-dimensionality of\nthe QoIs. We present two modified design criteria, A-GOODE and D-GOODE, which\nare natural analogues of classical Bayesian A- and D-optimal criteria. We\nanalyze the connections to other ODE criteria, and provide interpretations for\nthe GOODE criteria by using tools from information theory. Then, we develop an\nefficient gradient-based optimization framework for solving the GOODE\noptimization problems. Additionally, we present comprehensive numerical\nexperiments testing the various aspects of the presented approach. The driving\napplication is the optimal placement of sensors to identify the source of\ncontaminants in a diffusion and transport problem. We enforce sparsity of the\nsensor placements using an $\\ell_1$-norm penalty approach, and propose a\npractical strategy for specifying the associated penalty parameter.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 04:56:59 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 04:40:55 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Attia", "Ahmed", ""], ["Alexanderian", "Alen", ""], ["Saibaba", "Arvind K.", ""]]}, {"id": "1802.07525", "submitter": "Michail-Antisthenis Tsompanas", "authors": "Michail-Antisthenis Tsompanas, Andrew Adamatzky, Ioannis Ieropoulos,\n  Neil Phillips, Georgios Ch. Sirakoulis, John Greenman", "title": "Modelling Microbial Fuel Cells using lattice Boltzmann methods", "comments": null, "journal-ref": null, "doi": "10.1109/TCBB.2018.2831223", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An accurate modelling of bio-electrochemical processes that govern Microbial\nFuel Cells (MFCs) and mapping their behaviour according to several parameters\nwill enhance the development of MFC technology and enable their successful\nimplementation in well defined applications. The geometry of the electrodes is\namong key parameters determining efficiency of MFCs due to the formation of a\nbiofilm of anodophilic bacteria on the anode electrode, which is a decisive\nfactor for the functionality of the device. We simulate the bio-electrochemical\nprocesses in an MFC while taking into account the geometry of the electrodes.\nNamely, lattice Boltzmann methods are used to simulate the fluid dynamics and\nthe advection-diffusion phenomena in the anode compartment. The model is\nverified on voltage and current outputs of a single MFC derived from laboratory\nexperiments under continuous flow.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 11:48:49 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Tsompanas", "Michail-Antisthenis", ""], ["Adamatzky", "Andrew", ""], ["Ieropoulos", "Ioannis", ""], ["Phillips", "Neil", ""], ["Sirakoulis", "Georgios Ch.", ""], ["Greenman", "John", ""]]}, {"id": "1802.07601", "submitter": "Luca Pegolotti", "authors": "Simone Deparis and Luca Pegolotti", "title": "Coupling non-conforming discretizations of PDEs by spectral\n  approximation of the Lagrange multiplier space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the development of a non-conforming domain decomposition\nmethod for the approximation of PDEs based on weakly imposed transmission\nconditions: the continuity of the global solution is enforced by a discrete\nnumber of Lagrange multipliers defined over the interfaces of adjacent\nsubdomains. The method falls into the class of primal hybrid methods, which\nalso include the well-known mortar method. Differently from the mortar method,\nwe discretize the space of basis functions on the interface by spectral\napproximation independently of the discretization of the two adjacent domains;\none of the possible choices is to approximate the interface variational space\nby Fourier basis functions. As we show in the numerical simulations, our\napproach is well-suited for the solution of problems with non-conforming meshes\nor with finite element basis functions with different polynomial degrees in\neach subdomain. Another application of the method that still needs to be\ninvestigated is the coupling of solutions obtained from otherwise incompatible\nmethods, such as the finite element method, the spectral element method or\nisogeometric analysis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:58:21 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Deparis", "Simone", ""], ["Pegolotti", "Luca", ""]]}, {"id": "1802.08040", "submitter": "Jan Vorel", "authors": "Jan Vorel and Petr Kabele", "title": "Inverse analysis of traction-separation relationship based on\n  sequentially linear approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traction-separation relationship is an important material characteristic\ndescribing the fracture behaviour of quasi-brittle solids. A new numerical\nscheme for identification of the traction-separation relation by inverse\nanalysis of data obtained from various types of fracture tests is proposed. Due\nto employing the concept of sequentially linear analysis, the method exhibits a\nsuperior numerical stability and versatility. The applicability and\neffectiveness of the proposed method is demonstrated on examples involving\nidentification of the traction-separation relationship using experimental data\nfrom various test configurations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 13:47:46 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 22:02:25 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Vorel", "Jan", ""], ["Kabele", "Petr", ""]]}, {"id": "1802.08115", "submitter": "Mohammed Ishaquddin", "authors": "Md.Ishaquddin, S.Gopalakrishnan", "title": "Novel differential quadrature element method for higher order strain\n  gradient elasticity theory", "comments": "arXiv admin note: text overlap with arXiv:1802.05541", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel and efficient differential quadrature\nelement based on Lagrange interpolation to solve a sixth order partial\ndifferential equations encountered in non-classical beam theories. These\nnon-classical theories render displacement, slope and curvature as degrees of\nfreedom for an Euler-Bernoulli beam. A generalize scheme is presented herein to\nimplementation the multi-degrees degrees of freedom associated with these\nnon-classical theories in a simplified and efficient way. The proposed element\nhas displacement as the only degree of freedom in the domain, whereas, at the\nboundaries it has displacement, slope and curvature. Further, we extend this\nmethodology and formulate two novel versions of plate element for gradient\nelasticity theory. In the first version, Lagrange interpolation is assumed in\n$x$ and $y$ directions and the second version is based on mixed interpolation,\nwith Lagrange interpolation in $x$ direction and Hermite interpolation in $y$\ndirection. The procedure to compute the modified weighting coefficients by\nincorporating the classical and non-classical boundary conditions is explained.\nThe efficiency of the proposed elements is demonstrated through numerical\nexamples on static analysis of gradient elastic beams and plates for different\nboundary conditions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Feb 2018 10:20:09 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Ishaquddin", "Md.", ""], ["Gopalakrishnan", "S.", ""]]}, {"id": "1802.08157", "submitter": "Abele Simona", "authors": "Abele Simona, Luca Bonaventura, Thomas Pugnat, Barbara Dalena", "title": "High order time integrators for the simulation of charged particle\n  motion in magnetic quadrupoles", "comments": "39 pages, 18 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2019.01.018", "report-no": null, "categories": "cs.CE cs.NA physics.acc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic quadrupoles are essential components of particle accelerators like\nthe Large Hadron Collider. In order to study numerically the stability of the\nparticle beam crossing a quadrupole, a large number of particle revolutions in\nthe accelerator must be simulated, thus leading to the necessity to preserve\nnumerically invariants of motion over a long time interval and to a substantial\ncomputational cost, mostly related to the repeated evaluation of the magnetic\nvector potential. In this paper, in order to reduce this cost, we first\nconsider a specific gauge transformation that allows to reduce significantly\nthe number of vector potential evaluations. We then analyze the sensitivity of\nthe numerical solution to the interpolation procedure required to compute\nmagnetic vector potential data from gridded precomputed values at the locations\nrequired by high order time integration methods. Finally, we compare several\nhigh order integration techniques, in order to assess their accuracy and\nefficiency for these long term simulations. Explicit high order Lie methods are\nconsidered, along with implicit high order symplectic integrators and\nconventional explicit Runge Kutta methods. Among symplectic methods, high order\nLie integrators yield optimal results in terms of cost/accuracy ratios, but non\nsymplectic Runge Kutta methods perform remarkably well even in very long term\nsimulations. Furthermore, the accuracy of the field reconstruction and\ninterpolation techniques are shown to be limiting factors for the accuracy of\nthe particle tracking procedures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 16:47:22 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Simona", "Abele", ""], ["Bonaventura", "Luca", ""], ["Pugnat", "Thomas", ""], ["Dalena", "Barbara", ""]]}, {"id": "1802.08812", "submitter": "Simon Mak", "authors": "Yu-Hung Chang, Liwei Zhang, Xingjian Wang, Shiang-Ting Yeh, Simon Mak,\n  Chih-Li Sung, C. F. Jeff Wu, Vigor Yang", "title": "Kernel-smoothed proper orthogonal decomposition (KSPOD)-based emulation\n  for prediction of spatiotemporally evolving flow dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This interdisciplinary study, which combines machine learning, statistical\nmethodologies, high-fidelity simulations, and flow physics, demonstrates a new\nprocess for building an efficient surrogate model for predicting\nspatiotemporally evolving flow dynamics. In our previous work, a common-grid\nproper-orthogonal-decomposition (CPOD) technique was developed to establish a\nphysics-based surrogate (emulation) model for prediction of mean flowfields and\ndesign exploration over a wide parameter space. The CPOD technique is\nsubstantially improved upon here using a kernel-smoothed POD (KSPOD) technique,\nwhich leverages kriging-based weighted functions from the design matrix. The\nresultant emulation model is then trained using a dataset obtained through\nhigh-fidelity simulations. As an example, the flow evolution in a swirl\ninjector is considered for a wide range of design parameters and operating\nconditions. The KSPOD-based emulation model performs well, and can faithfully\ncapture the spatiotemporal flow dynamics. The model enables effective design\nsurveys utilizing high-fidelity simulation data, achieving a turnaround time\nfor evaluating new design points that is 42,000 times faster than the original\nsimulation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 07:22:27 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Chang", "Yu-Hung", ""], ["Zhang", "Liwei", ""], ["Wang", "Xingjian", ""], ["Yeh", "Shiang-Ting", ""], ["Mak", "Simon", ""], ["Sung", "Chih-Li", ""], ["Wu", "C. F. Jeff", ""], ["Yang", "Vigor", ""]]}, {"id": "1802.09241", "submitter": "Giovanni Stabile", "authors": "Giovanni Stabile and Hermann G. Matthies and Claudio Borri", "title": "A novel reduced order model for vortex induced vibrations of long\n  flexible cylinders", "comments": null, "journal-ref": null, "doi": "10.1016/j.oceaneng.2018.02.064", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript the development of a reduced order model for the analysis\nof long flexible cylinders in an offshore environment is proposed. In\nparticular the focus is on the modelling of the vortex induced vibrations (VIV)\nand the aim is the development of a model capable of capturing both the in-line\nand cross-flow oscillations. The reduced order model is identified starting\nfrom the results of a high fidelity solver developed coupling together a Finite\nElement Solver (FEM) with a Computational Fluid Dynamics (CFD) solver. The high\nfidelity analyses are conducted on a reduced domain size representing a small\nsection of the long cylinder, which is nevertheless, already flexible. The\nsection is forced using a motion which matches the expected motion in full\nscale, and the results are used for the system-parameter identification of the\nreduced order model. The reduced order model is identified by using a system\nand parameter identification approach. The final proposed model consists in the\ncombination of a forced van der Pol oscillator, to model the cross-flow forces,\nand a linear state-space model, to model the in-line forces. The model is\napplied to study a full scale flexible model and the results are validated by\nusing experiments conducted on a flexible riser inside a towing tank.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 10:47:08 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Stabile", "Giovanni", ""], ["Matthies", "Hermann G.", ""], ["Borri", "Claudio", ""]]}, {"id": "1802.09394", "submitter": "Matteo Giacomini", "authors": "Matteo Giacomini, Alexandros Karkoulias, Ruben Sevilla, Antonio Huerta", "title": "A superconvergent HDG method for Stokes flow with strongly enforced\n  symmetry of the stress tensor", "comments": "28 pages, 10 figures, 1 table", "journal-ref": "J. Sci. Comput., 77(3):1679--1702 (2018)", "doi": "10.1007/s10915-018-0855-y", "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a superconvergent hybridizable discontinuous Galerkin\n(HDG) method for the approximation of the Cauchy formulation of the Stokes\nequation using same degree of polynomials for the primal and mixed variables.\nThe novel formulation relies on the well-known Voigt notation to strongly\nenforce the symmetry of the stress tensor. The proposed strategy introduces\nseveral advantages with respect to the existing HDG formulations. First, it\nremedies the suboptimal behavior experienced by the classical HDG method for\nformulations involving the symmetric part of the gradient of the primal\nvariable. The optimal convergence of the mixed variable is retrieved and an\nelement-by-element post-process procedure leads to a superconvergent velocity\nfield, even for low-order approximations. Second, no additional enrichment of\nthe discrete spaces is required and a gain in computational efficiency follows\nfrom reducing the quantity of stored information and the size of the local\nproblems. Eventually, the novel formulation naturally imposes physical\ntractions on the Neumann boundary. Numerical validation of the optimality of\nthe method and its superconvergent properties is performed in 2D and 3D using\nmeshes of different element types.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:24:51 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Giacomini", "Matteo", ""], ["Karkoulias", "Alexandros", ""], ["Sevilla", "Ruben", ""], ["Huerta", "Antonio", ""]]}, {"id": "1802.09825", "submitter": "Reza Ghaffari", "authors": "Reza Ghaffari and Roger A. Sauer", "title": "A new efficient hyperelastic finite element model for graphene and its\n  application to carbon nanotubes and nanocones", "comments": null, "journal-ref": "Finite Elem. Anal. Des. 146(2018):42-61", "doi": "10.1016/j.finel.2018.04.001", "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new hyperelastic material model is proposed for graphene-based structures,\nsuch as graphene, carbon nanotubes (CNTs) and carbon nanocones (CNC). The\nproposed model is based on a set of invariants obtained from the right surface\nCauchy-Green strain tensor and a structural tensor. The model is fully\nnonlinear and can simulate buckling and postbuckling behavior. It is calibrated\nfrom existing quantum data. It is implemented within a rotation-free\nisogeometric shell formulation. The speedup of the model is 1.5 relative to the\nfinite element model of Ghaffari et al. [1], which is based on the logarithmic\nstrain formulation of Kumar and Parks [2]. The material behavior is verified by\ntesting uniaxial tension and pure shear. The performance of the material model\nis illustrated by several numerical examples. The examples include bending,\ntwisting, and wall contact of CNTs and CNCs. The wall contact is modeled with a\ncoarse grained contact model based on the Lennard-Jones potential. The buckling\nand post-buckling behavior is captured in the examples. The results are\ncompared with reference results from the literature and there is good\nagreement.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 11:07:08 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 20:09:50 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Ghaffari", "Reza", ""], ["Sauer", "Roger A.", ""]]}, {"id": "1802.10510", "submitter": "Mohammad Sultan", "authors": "Mohammad M. Sultan, Vijay S. Pande", "title": "Automated design of collective variables using supervised machine\n  learning", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE q-bio.BM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Selection of appropriate collective variables for enhancing sampling of\nmolecular simulations remains an unsolved problem in computational biophysics.\nIn particular, picking initial collective variables (CVs) is particularly\nchallenging in higher dimensions. Which atomic coordinates or transforms there\nof from a list of thousands should one pick for enhanced sampling runs? How\ndoes a modeler even begin to pick starting coordinates for investigation? This\nremains true even in the case of simple two state systems and only increases in\ndifficulty for multi-state systems. In this work, we solve the initial CV\nproblem using a data-driven approach inspired by the filed of supervised\nmachine learning. In particular, we show how the decision functions in\nsupervised machine learning (SML) algorithms can be used as initial CVs\n(SML_cv) for accelerated sampling. Using solvated alanine dipeptide and\nChignolin mini-protein as our test cases, we illustrate how the distance to the\nSupport Vector Machines' decision hyperplane, the output probability estimates\nfrom Logistic Regression, the outputs from deep neural network classifiers, and\nother classifiers may be used to reversibly sample slow structural transitions.\nWe discuss the utility of other SML algorithms that might be useful for\nidentifying CVs for accelerating molecular simulations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:22:14 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 21:52:50 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Sultan", "Mohammad M.", ""], ["Pande", "Vijay S.", ""]]}]