[{"id": "1311.0438", "submitter": "Snehanshu Saha", "authors": "Snehanshu Saha, Swati Routh, Bidisha Goswami", "title": "Modeling Vanilla Option prices: A simulation study by an implicit method", "comments": "An expository report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Option contracts can be valued by using the Black-Scholes equation, a partial\ndifferential equation with initial conditions. An exact solution for European\nstyle options is known. The computation time and the error need to be minimized\nsimultaneously. In this paper, the authors have solved the Black-Scholes\nequation by employing a reasonably accurate implicit method. Options with known\nanalytic solutions have been evaluated. Furthermore, an overall second order\naccurate space and time discretization is proposed in this paper Keywords:\nComputational finance, implicit methods, finite differences, call/put options.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 08:26:38 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 08:42:24 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Saha", "Snehanshu", ""], ["Routh", "Swati", ""], ["Goswami", "Bidisha", ""]]}, {"id": "1311.0534", "submitter": "John Peterson", "authors": "John W. Peterson", "title": "Accurate curve fits of IAPWS data for high-pressure, high-temperature\n  single-phase liquid water based on the stiffened gas equation of state", "comments": "17 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a series of optimal (in the sense of least-squares) curve fits for\nthe stiffened gas equation of state for single-phase liquid water. At high\npressures and (subcritical) temperatures, the parameters produced by these\ncurve fits are found to have very small relative errors: less than $1\\%$ in the\npressure model, and less than $2\\%$ in the temperature model. At low pressures\nand temperatures, especially near the liquid-vapor transition line, the error\nin the curve fits increases rapidly. The smallest pressure value for which\ncurve fits are reported in the present work is 25 MPa, high enough to ensure\nthat the fluid remains a single-phase liquid up to the maximum subcritical\ntemperature of approximately 647K.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 21:42:26 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Peterson", "John W.", ""]]}, {"id": "1311.0790", "submitter": "Nicholas Miller", "authors": "Nicholas C. Miller, Andrew D. Baczewski, John D. Albrecht, and\n  Balasubramaniam Shanker", "title": "A Discontinuous Galerkin Time Domain Framework for Periodic Structures\n  Subject To Oblique Excitation", "comments": "Submitted to IEEE TAP on August 5th, 2013. Revision submitted on\n  February 3rd, 2014", "journal-ref": null, "doi": "10.1109/TAP.2014.2324012", "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A nodal Discontinuous Galerkin (DG) method is derived for the analysis of\ntime-domain (TD) scattering from doubly periodic PEC/dielectric structures\nunder oblique interrogation. Field transformations are employed to elaborate a\nformalism that is free from any issues with causality that are common when\napplying spatial periodic boundary conditions simultaneously with incident\nfields at arbitrary angles of incidence. An upwind numerical flux is derived\nfor the transformed variables, which retains the same form as it does in the\noriginal Maxwell problem for domains without explicitly imposed periodicity.\nThis, in conjunction with the amenability of the DG framework to non-conformal\nmeshes, provides a natural means of accurately solving the first order TD\nMaxwell equations for a number of periodic systems of engineering interest.\nResults are presented that substantiate the accuracy and utility of our method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 17:48:27 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2014 17:29:17 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Miller", "Nicholas C.", ""], ["Baczewski", "Andrew D.", ""], ["Albrecht", "John D.", ""], ["Shanker", "Balasubramaniam", ""]]}, {"id": "1311.1422", "submitter": "Feng Zhao", "authors": "Feng Zhao", "title": "Structural Learning for Template-free Protein Folding", "comments": "138 pages, 7 chapters, 18 figures and 28 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The thesis is aimed to solve the template-free protein folding problem by\ntackling two important components: efficient sampling in vast conformation\nspace, and design of knowledge-based potentials with high accuracy. We have\nproposed the first-order and second-order CRF-Sampler to sample structures from\nthe continuous local dihedral angles space by modeling the lower and higher\norder conditional dependency between neighboring dihedral angles given the\nprimary sequence information. A framework combining the Conditional Random\nFields and the energy function is introduced to guide the local conformation\nsampling using long range constraints with the energy function.\n  The relationship between the sequence profile and the local dihedral angle\ndistribution is nonlinear. Hence we proposed the CNF-Folder to model this\ncomplex relationship by applying a novel machine learning model Conditional\nNeural Fields which utilizes the structural graphical model with the neural\nnetwork. CRF-Samplers and CNF-Folder perform very well in CASP8 and CASP9.\n  Further, a novel pairwise distance statistical potential (EPAD) is designed\nto capture the dependency of the energy profile on the positions of the\ninteracting amino acids as well as the types of those amino acids, opposing the\ncommon assumption that this energy profile depends only on the types of amino\nacids. EPAD has also been successfully applied in the CASP 10 Free Modeling\nexperiment with CNF-Folder, especially outstanding on some uncommon structured\ntargets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 15:37:27 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2013 19:17:57 GMT"}], "update_date": "2013-11-13", "authors_parsed": [["Zhao", "Feng", ""]]}, {"id": "1311.1640", "submitter": "Miguel O. Bernabeu", "authors": "Miguel O. Bernabeu, Martin Jones, Jens H. Nielsen, Timm Kr\\\"uger,\n  Rupert W. Nash, Derek Groen, Sebastian Schmieschek, James Hetherington,\n  Holger Gerhardt, Claudio A. Franco, Peter V. Coveney", "title": "Computer simulations reveal complex distribution of haemodynamic forces\n  in a mouse retina model of angiogenesis", "comments": "34 pages, 12 figures, accepted for publication at the Journal of the\n  Royal Society Interface", "journal-ref": "J. R. Soc. Interface 6 October 2014 vol. 11 no. 99 20140543", "doi": "10.1098/rsif.2014.0543", "report-no": null, "categories": "cs.CE physics.bio-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is currently limited understanding of the role played by haemodynamic\nforces on the processes governing vascular development. One of many obstacles\nto be overcome is being able to measure those forces, at the required\nresolution level, on vessels only a few micrometres thick. In the current\npaper, we present an in silico method for the computation of the haemodynamic\nforces experienced by murine retinal vasculature (a widely used vascular\ndevelopment animal model) beyond what is measurable experimentally. Our results\nshow that it is possible to reconstruct high-resolution three-dimensional\ngeometrical models directly from samples of retinal vasculature and that the\nlattice-Boltzmann algorithm can be used to obtain accurate estimates of the\nhaemodynamics in these domains. We generate flow models from samples obtained\nat postnatal days (P) 5 and 6. Our simulations show important differences\nbetween the flow patterns recovered in both cases, including observations of\nregression occurring in areas where wall shear stress gradients exist. We\npropose two possible mechanisms to account for the observed increase in\nvelocity and wall shear stress between P5 and P6: i) the measured reduction in\ntypical vessel diameter between both time points, ii) the reduction in network\ndensity triggered by the pruning process. The methodology developed herein is\napplicable to other biomedical domains where microvasculature can be imaged but\nexperimental flow measurements are unavailable or difficult to obtain.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 11:14:06 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 11:13:44 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Bernabeu", "Miguel O.", ""], ["Jones", "Martin", ""], ["Nielsen", "Jens H.", ""], ["Kr\u00fcger", "Timm", ""], ["Nash", "Rupert W.", ""], ["Groen", "Derek", ""], ["Schmieschek", "Sebastian", ""], ["Hetherington", "James", ""], ["Gerhardt", "Holger", ""], ["Franco", "Claudio A.", ""], ["Coveney", "Peter V.", ""]]}, {"id": "1311.2167", "submitter": "Kunal  Puri", "authors": "Kunal Puri and Prabhu Ramachandran", "title": "SPH Entropy Errors and the Pressure Blip", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spurious pressure jump at a contact discontinuity, in SPH simulations of\nthe compressible Euler equations is investigated. From the spatiotemporal\nbehaviour of the error, the SPH pressure jump is likened to entropy errors\nobserved for artificial viscosity based finite difference/volume schemes. The\nerror is observed to be generated at start-up and dissipation is the only\nrecourse to mitigate it's effect. We show that similar errors are generated for\nthe Lagrangian plus remap version of the Piecewise Parabolic Method (PPM)\nfinite volume code (PPMLR). Through a comparison with the direct Eulerian\nversion of the PPM code (PPMDE), we argue that a lack of diffusion across the\nmaterial wave (contact discontinuity) is responsible for the error in PPMLR. We\nverify this hypothesis by constructing a more dissipative version of the remap\ncode using a piecewise constant reconstruction. As an application to SPH, we\npropose a hybrid GSPH scheme that adds the requisite dissipation by utilizing a\nmore dissipative Riemann solver for the energy equation. The proposed\nmodification to the GSPH scheme, and it's improved treatment of the anomaly is\nverified for flows with strong shocks in one and two dimensions. The result\nthat dissipation must act across the density and energy equations provides a\nconsistent explanation for many of the hitherto proposed \"cures\" or \"fixes\" for\nthe problem.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2013 12:59:38 GMT"}, {"version": "v2", "created": "Mon, 3 Mar 2014 05:29:15 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Puri", "Kunal", ""], ["Ramachandran", "Prabhu", ""]]}, {"id": "1311.3837", "submitter": "Dalila Hamami", "authors": "Dalila Hamami and Baghdad Atmani", "title": "SBML for optimizing decision support's tools", "comments": null, "journal-ref": "Aircc.Proc. 3.8 (2013) 109-119", "doi": "10.5121/csit.2013.3810", "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Many theoretical works and tools on epidemiological field reflect the\nemphasis on decision-making Tools by both public health and the scientific\ncommunity, which continues to increase. Indeed, in the epidemiological field,\nmodeling tools are proving a very important way in helping to make decision.\nHowever, the variety, the large volume of data and the nature of epidemics lead\nus to seek solutions to alleviate the heavy burden imposed on both experts and\ndevelopers. In this paper, we present a new approach: the passage of an\nepidemic model realized in Bio-PEPA to a narrative language using the basics of\nSBML language. Our goal is to allow on one hand, epidemiologists to verify and\nvalidate the model, and the other hand, developers to optimize the model in\norder to achieve a better model of decision making. We also present some\npreliminary results and some suggestions to improve the simulated model.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 12:58:28 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Hamami", "Dalila", ""], ["Atmani", "Baghdad", ""]]}, {"id": "1311.3840", "submitter": "Mahmood A. Rashid", "authors": "Mahmood A. Rashid, M. A. Hakim Newton, Md. Tamjidul Hoque, and Abdul\n  Sattar", "title": "Mixing Energy Models in Genetic Algorithms for On-Lattice Protein\n  Structure Prediction", "comments": "Volume 2013, 15 pages, BioMed Research International, 2013", "journal-ref": null, "doi": "10.1155/2013/924137", "report-no": "Article ID 924137", "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein structure prediction (PSP) is computationally a very challenging\nproblem. The challenge largely comes from the fact that the energy function\nthat needs to be minimised in order to obtain the native structure of a given\nprotein is not clearly known. A high resolution 20x20 energy model could better\ncapture the behaviour of the actual energy function than a low resolution\nenergy model such as hydrophobic polar. However, the fine grained details of\nthe high resolution interaction energy matrix are often not very informative\nfor guiding the search. In contrast, a low resolution energy model could\neffectively bias the search towards certain promising directions. In this\npaper, we develop a genetic algorithm that mainly uses a high resolution energy\nmodel for protein structure evaluation but uses a low resolution HP energy\nmodel in focussing the search towards exploring structures that have\nhydrophobic cores. We experimentally show that this mixing of energy models\nleads to significant lower energy structures compared to the state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 13:09:19 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Rashid", "Mahmood A.", ""], ["Newton", "M. A. Hakim", ""], ["Hoque", "Md. Tamjidul", ""], ["Sattar", "Abdul", ""]]}, {"id": "1311.4533", "submitter": "Kirana Kumara P", "authors": "Kirana Kumara P", "title": "A Study of Speed of the Boundary Element Method as applied to the\n  Realtime Computational Simulation of Biological Organs", "comments": "preprint, draft, 2 tables, 47 references, 7 files, Codes that can\n  solve three dimensional linear elastostatic problems using constant boundary\n  elements (of triangular shape) while ignoring body forces are provided as\n  supplementary files; codes are distributed under the MIT License in three\n  versions: i) MATLAB version ii) Fortran 90 version (sequential code) iii)\n  Fortran 90 version (parallel code)", "journal-ref": "Electronic Journal of Boundary Elements, Vol. 12, No. 2, pp. 1-25\n  (2014)", "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS physics.comp-ph physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, possibility of simulating biological organs in realtime using\nthe Boundary Element Method (BEM) is investigated. Biological organs are\nassumed to follow linear elastostatic material behavior, and constant boundary\nelement is the element type used. First, a Graphics Processing Unit (GPU) is\nused to speed up the BEM computations to achieve the realtime performance.\nNext, instead of the GPU, a computer cluster is used. Results indicate that BEM\nis fast enough to provide for realtime graphics if biological organs are\nassumed to follow linear elastostatic material behavior. Although the present\nwork does not conduct any simulation using nonlinear material models, results\nfrom using the linear elastostatic material model imply that it would be\ndifficult to obtain realtime performance if highly nonlinear material models\nthat properly characterize biological organs are used. Although the use of BEM\nfor the simulation of biological organs is not new, the results presented in\nthe present study are not found elsewhere in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 20:54:26 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 04:29:47 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2014 18:28:10 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["P", "Kirana Kumara", ""]]}, {"id": "1311.4570", "submitter": "Pedro Neto", "authors": "Diogo Mariano Neto, Pedro Neto", "title": "Numerical modeling of friction stir welding process: a literature review", "comments": "The International Journal of Advanced Manufacturing Technology", "journal-ref": "Volume 65, 2013 , pp 115-126", "doi": "10.1007/s00170-012-4154-8", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey presents a literature review on friction stir welding (FSW)\nmodeling with a special focus on the heat generation due to the contact\nconditions between the FSW tool and the workpiece. The physical process is\ndescribed and the main process parameters that are relevant to its modeling are\nhighlighted. The contact conditions (sliding/sticking) are presented as well as\nan analytical model that allows estimating the associated heat generation. The\nmodeling of the FSW process requires the knowledge of the heat loss mechanisms,\nwhich are discussed mainly considering the more commonly adopted formulations.\nDifferent approaches that have been used to investigate the material flow are\npresented and their advantages/drawbacks are discussed. A reliable FSW process\nmodeling depends on the fine tuning of some process and material parameters.\nUsually, these parameters are achieved with base on experimental data. The\nnumerical modeling of the FSW process can help to achieve such parameters with\nless effort and with economic advantages.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 22:10:14 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Neto", "Diogo Mariano", ""], ["Neto", "Pedro", ""]]}, {"id": "1311.4762", "submitter": "Daniele de Rigo", "authors": "Daniele de Rigo", "title": "Software Uncertainty in Integrated Environmental Modelling: the role of\n  Semantics and Open Science", "comments": "This is the author's version of the work. The definitive version is\n  published in the Vol. 15 of Geophysical Research Abstracts (ISSN 1607-7962)\n  and has been presented at the European Geosciences Union (EGU) General\n  Assembly 2013, Vienna, Austria, 07-12 April 2013 http://www.egu2013.eu/\n  [Updated style and fixed few typos]", "journal-ref": "Geophysical Research Abstracts 15 (2013) 13292+", "doi": "10.6084/m9.figshare.155701", "report-no": null, "categories": "cs.SY cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Computational aspects increasingly shape environmental sciences. Actually,\ntransdisciplinary modelling of complex and uncertain environmental systems is\nchallenging computational science (CS) and also the science-policy interface.\nLarge spatial-scale problems falling within this category - i.e. wide-scale\ntransdisciplinary modelling for environment (WSTMe) - often deal with factors\n(a) for which deep-uncertainty may prevent usual statistical analysis of\nmodelled quantities and need different ways for providing policy-making with\nscience-based support. Here, practical recommendations are proposed for\ntempering a peculiar - not infrequently underestimated - source of uncertainty.\nSoftware errors in complex WSTMe may subtly affect the outcomes with possible\nconsequences even on collective environmental decision-making. Semantic\ntransparency in CS and free software are discussed as possible mitigations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 15:00:26 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 16:44:07 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["de Rigo", "Daniele", ""]]}, {"id": "1311.5550", "submitter": "Fabien Campagne", "authors": "Manuele Simi and Fabien Campagne", "title": "Composable Languages for Bioinformatics: The NYoSh experiment", "comments": "10 pages, 9 figures. Supplementary material: 4 generated source\n  listings. Comment on this manuscript on Twitter or Google+ with this handle:\n  #NYoSh", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language workbenches are software engineering tools that help domain experts\ndevelop solutions to various classes of problems. Some of these tools focus on\nnon-technical users and provide languages to help organize knowledge while\nother workbenches provide means to create new programming languages. A key\nadvantage of language workbenches is that they support the composition of\nindependently developed languages. This capability is useful when developing\nprograms that can benefit from different levels of abstraction. We reasoned\nthat language workbenches could be useful to develop bioinformatics software\nsolutions. In order to evaluate the potential of language workbenches in\nbioinformatics, we tested a prominent workbench by developing an alternative to\nshell scripting. While shell scripts are widely used in bioinformatics to\nautomate computational analysis, existing scripting languages do not provide\nmany of the features present in modern programming languages. We report on our\ndesign of NYoSh (Not Your ordinary Shell). NYoSh was implemented as a\ncollection of languages that can be composed to write programs as expressive\nand concise as shell scripts. NYoSh offers a concrete illustration of the\nadvantages that language workbench technologies can bring to bioinformatics.\nFor instance, NYoSh scripts can be edited with an environment-aware editor that\nprovides semantic error detection and can be compiled interactively with an\nautomatic build and deployment system. In contrast to shell scripts, NYoSh\nscripts can be written in a modern development environment, supporting context\ndependent intentions and can be extended seamlessly with new abstractions and\nlanguage constructs. We demonstrate language extension and composition by\npresenting a tight integration of NYoSh scripts with the GobyWeb system. The\nNYoSh Workbench prototype is distributed at http://nyosh.campagnelab.org\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 20:42:37 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2013 19:53:01 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Simi", "Manuele", ""], ["Campagne", "Fabien", ""]]}, {"id": "1311.5686", "submitter": "Blesson Varghese", "authors": "Zhimin Yao, Blesson Varghese and Andrew Rau-Chaplin", "title": "High Performance Risk Aggregation: Addressing the Data Processing\n  Challenge the Hadoop MapReduce Way", "comments": "ScienceCloud 2013 at HPDC 2013, New York, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo simulations employed for the analysis of portfolios of\ncatastrophic risk process large volumes of data. Often times these simulations\nare not performed in real-time scenarios as they are slow and consume large\ndata. Such simulations can benefit from a framework that exploits parallelism\nfor addressing the computational challenge and facilitates a distributed file\nsystem for addressing the data challenge. To this end, the Apache Hadoop\nframework is chosen for the simulation reported in this paper so that the\ncomputational challenge can be tackled using the MapReduce model and the data\nchallenge can be addressed using the Hadoop Distributed File System. A parallel\nalgorithm for the analysis of aggregate risk is proposed and implemented using\nthe MapReduce model in this paper. An evaluation of the performance of the\nalgorithm indicates that the Hadoop MapReduce model offers a framework for\nprocessing large data in aggregate risk analysis. A simulation of aggregate\nrisk employing 100,000 trials with 1000 catastrophic events per trial on a\ntypical exposure set and contract structure is performed on multiple worker\nnodes in less than 6 minutes. The result indicates the scope and feasibility of\nMapReduce for tackling the computational and data challenge in the analysis of\naggregate risk for real-time use.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 09:35:40 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Yao", "Zhimin", ""], ["Varghese", "Blesson", ""], ["Rau-Chaplin", "Andrew", ""]]}, {"id": "1311.5735", "submitter": "Jose A. Egea", "authors": "Jose A Egea, David Henriques, Thomas Cokelaer, Alejandro F Villaverde,\n  Julio R Banga, Julio Saez-Rodriguez", "title": "MEIGO: an open-source software suite based on metaheuristics for global\n  optimization in systems biology and bioinformatics", "comments": "12 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE cs.MS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization is key to solve many problems in computational biology. Global\noptimization methods provide a robust methodology, and metaheuristics in\nparticular have proven to be the most efficient methods for many applications.\nDespite their utility, there is limited availability of metaheuristic tools. We\npresent MEIGO, an R and Matlab optimization toolbox (also available in Python\nvia a wrapper of the R version), that implements metaheuristics capable of\nsolving diverse problems arising in systems biology and bioinformatics:\nenhanced scatter search method (eSS) for continuous nonlinear programming\n(cNLP) and mixed-integer programming (MINLP) problems, and variable\nneighborhood search (VNS) for Integer Programming (IP) problems. Both methods\ncan be run on a single-thread or in parallel using a cooperative strategy. The\ncode is supplied under GPLv3 and is available at\n\\url{http://www.iim.csic.es/~gingproc/meigo.html}. Documentation and examples\nare included. The R package has been submitted to Bioconductor. We evaluate\nMEIGO against optimization benchmarks, and illustrate its applicability to a\nseries of case studies in bioinformatics and systems biology, outperforming\nother state-of-the-art methods. MEIGO provides a free, open-source platform for\noptimization, that can be applied to multiple domains of systems biology and\nbioinformatics. It includes efficient state of the art metaheuristics, and its\nopen and modular structure allows the addition of further methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 12:39:29 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Egea", "Jose A", ""], ["Henriques", "David", ""], ["Cokelaer", "Thomas", ""], ["Villaverde", "Alejandro F", ""], ["Banga", "Julio R", ""], ["Saez-Rodriguez", "Julio", ""]]}, {"id": "1311.5740", "submitter": "Joris Borgdorff", "authors": "Joris Borgdorff, Mariusz Mamonski, Bartosz Bosak, Krzysztof Kurowski,\n  Mohamed Ben Belgacem, Bastien Chopard, Derek Groen, Peter V. Coveney, Alfons\n  G. Hoekstra", "title": "Distributed Multiscale Computing with MUSCLE 2, the Multiscale Coupling\n  Library and Environment", "comments": "18 pages, 22 figures, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Multiscale Coupling Library and Environment: MUSCLE 2. This\nmultiscale component-based execution environment has a simple to use Java, C++,\nC, Python and Fortran API, compatible with MPI, OpenMP and threading codes. We\ndemonstrate its local and distributed computing capabilities and compare its\nperformance to MUSCLE 1, file copy, MPI, MPWide, and GridFTP. The local\nthroughput of MPI is about two times higher, so very tightly coupled code\nshould use MPI as a single submodel of MUSCLE 2; the distributed performance of\nGridFTP is lower, especially for small messages. We test the performance of a\ncanal system model with MUSCLE 2, where it introduces an overhead as small as\n5% compared to MPI.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 13:02:15 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Borgdorff", "Joris", ""], ["Mamonski", "Mariusz", ""], ["Bosak", "Bartosz", ""], ["Kurowski", "Krzysztof", ""], ["Belgacem", "Mohamed Ben", ""], ["Chopard", "Bastien", ""], ["Groen", "Derek", ""], ["Coveney", "Peter V.", ""], ["Hoekstra", "Alfons G.", ""]]}, {"id": "1311.6215", "submitter": "Valery Wolff", "authors": "Val\\'ery Wolff, Tin Tran Dinh (LaMCoS), Stephane Raynaud (INSA Lyon)", "title": "Using virtual parts to optimize the metrology process", "comments": null, "journal-ref": "Tehnomus journal 19, 1 (2012) 9-16", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the measurement process, there are many parameters affecting the\nmeasurement results: the influence of the probe system, material stiffness of\nmeasured workpiece, the calibration of the probe with a reference sphere, the\nthermal effects. We want to obtain the limits of a measurement methodology to\nbe able to validate a result. The study is applied to a simple part. We observe\nthe dispersion of the position of different drilled holes (XYZ values in a\ncoordinate system) when we change the quality of the part and the method of\ncalculation. We use the Design of Experiment (Taguchi method) to realize our\nstudy. We study the influence of the part quality on a measurement results. We\nconsider two parameters to define the part quality (flatness and\nperpendicularity). We will also study the influence of different methods of\ncalculation to determine the coordinate system. We can use two options in\nMetrolog XG software (tangent plane with or without orientation constraint).\nThe originality of this paper is that we present a method for the design of\nexperiment that uses CATIA (CAD system) to generate the measured parts. In this\nway we can realize a design of experiment with a largest number of experimental\nresults. This is a positive point for a statistical analysis. We are also free\nto define the parts we want to study without manufacturing difficulties.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 05:57:40 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Wolff", "Val\u00e9ry", "", "LaMCoS"], ["Dinh", "Tin Tran", "", "LaMCoS"], ["Raynaud", "Stephane", "", "INSA Lyon"]]}, {"id": "1311.6372", "submitter": "Garth Wells", "authors": "Sander Rhebergen and Garth N. Wells and Richard F. Katz and Andrew J.\n  Wathen", "title": "Analysis of block-preconditioners for models of coupled magma/mantle\n  dynamics", "comments": null, "journal-ref": "SIAM J. Sci. Comput., 36(4), A1960-A1977", "doi": "10.1137/130946678", "report-no": null, "categories": "math.NA cs.CE physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the iterative solution of a finite element\ndiscretisation of the magma dynamics equations. In simplified form, the magma\ndynamics equations share some features of the Stokes equations. We therefore\nformulate, analyse and numerically test a Elman, Silvester and Wathen-type\nblock preconditioner for magma dynamics. We prove analytically and demonstrate\nnumerically the optimality of the preconditioner. The presented analysis\nhighlights the dependence of the preconditioner on parameters in the magma\ndynamics equations that can affect convergence of iterative linear solvers. The\nanalysis is verified through a range of two- and three-dimensional numerical\nexamples on unstructured grids, from simple illustrative problems through to\nlarge problems on subduction zone-like geometries. The computer code to\nreproduce all numerical examples is freely available as supporting material.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 17:22:28 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2014 12:21:27 GMT"}, {"version": "v3", "created": "Thu, 29 May 2014 22:05:09 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Rhebergen", "Sander", ""], ["Wells", "Garth N.", ""], ["Katz", "Richard F.", ""], ["Wathen", "Andrew J.", ""]]}, {"id": "1311.6460", "submitter": "Sabyasachi  Mukhopadhyay", "authors": "Swapnil Barmase, Saurav Das and Sabyasachi Mukhopadhyay", "title": "Wavelet Transform-Based Analysis of QRS complex in ECG Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we have reported a wavelet based time-frequency\nmultiresolution analysis of an ECG signal. The ECG (electrocardiogram), which\nrecords hearts electrical activity, is able to provide with useful information\nabout the type of Cardiac disorders suffered by the patient depending upon the\ndeviations from normal ECG signal pattern. We have plotted the coefficients of\ncontinuous wavelet transform using Morlet wavelet. We used different ECG signal\navailable at MIT-BIH database and performed a comparative study. We\ndemonstrated that the coefficient at a particular scale represents the presence\nof QRS signal very efficiently irrespective of the type or intensity of noise,\npresence of unusually high amplitude of peaks other than QRS peaks and Base\nline drift errors. We believe that the current studies can enlighten the path\ntowards development of very lucid and time efficient algorithms for identifying\nand representing the QRS complexes that can be done with normal computers and\nprocessors.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 20:59:34 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Barmase", "Swapnil", ""], ["Das", "Saurav", ""], ["Mukhopadhyay", "Sabyasachi", ""]]}, {"id": "1311.6799", "submitter": "Sabyasachi  Mukhopadhyay", "authors": "Sabyasachi Mukhopadhyay, Debadatta Dash, Swapnil Barmase, Prasanta K\n  Panigrahi", "title": "Wavelet and Fast Fourier Transform based analysis of Solar Image", "comments": "This paper has been withdrawn by the author due to some modifications\n  are required for this current paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both of Wavelet and Fast Fourier Transform are strong signal processing tools\nin the field of Data Analysis. In this paper fast fourier transform (FFT) and\nWavelet Transform are employed to observe some important features of Solar\nimage (December, 2004). We have tried to find out the periodicity and coherence\nof different sections of the solar image. We plotted the distribution of energy\nin solar surface by analyzing the solar image with scalograms and\n3D-coefficient plots.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 10:37:28 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2013 16:40:09 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Dash", "Debadatta", ""], ["Barmase", "Swapnil", ""], ["Panigrahi", "Prasanta K", ""]]}, {"id": "1311.6868", "submitter": "Alan Veliz-Cuba", "authors": "Alan Veliz-Cuba, Reinhard Laubenbacher, Boris Aguilar", "title": "Dimension Reduction of Large AND-NOT Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.CE cs.SI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean networks have been used successfully in modeling biological networks\nand provide a good framework for theoretical analysis. However, the analysis of\nlarge networks is not trivial. In order to simplify the analysis of such\nnetworks, several model reduction algorithms have been proposed; however, it is\nnot clear if such algorithms scale well with respect to the number of nodes.\nThe goal of this paper is to propose and implement an algorithm for the\nreduction of AND-NOT network models for the purpose of steady state\ncomputation. Our method of network reduction is the use of \"steady state\napproximations\" that do not change the number of steady states. Our algorithm\nis designed to work at the wiring diagram level without the need to evaluate or\nsimplify Boolean functions. Also, our implementation of the algorithm takes\nadvantage of the sparsity typical of discrete models of biological systems. The\nmain features of our algorithm are that it works at the wiring diagram level,\nit runs in polynomial time, and it preserves the number of steady states. We\nused our results to study AND-NOT network models of gene networks and showed\nthat our algorithm greatly simplifies steady state analysis. Furthermore, our\nalgorithm can handle sparse AND-NOT networks with up to 1000000 nodes.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 05:00:07 GMT"}], "update_date": "2013-11-29", "authors_parsed": [["Veliz-Cuba", "Alan", ""], ["Laubenbacher", "Reinhard", ""], ["Aguilar", "Boris", ""]]}, {"id": "1311.7235", "submitter": "Oscar Perpinan", "authors": "F. Antonanzas-Torres and F.J. Mart\\'inez de Pis\\'on and J. Antonanzas\n  and O. Perpi\\~n\\'an", "title": "Downscaling of global solar irradiation in R", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A methodology for downscaling solar irradiation from satellite-derived\ndatabases is described using R software. Different packages such as raster,\nparallel, solaR, gstat, sp and rasterVis are considered in this study for\nimproving solar resource estimation in areas with complex topography, in which\ndownscaling is a very useful tool for reducing inherent deviations in\nsatellite-derived irradiation databases, which lack of high global spatial\nresolution. A topographical analysis of horizon blocking and sky-view is\ndeveloped with a digital elevation model to determine what fraction of hourly\nsolar irradiation reaches the Earth's surface. Eventually, kriging with\nexternal drift is applied for a better estimation of solar irradiation\nthroughout the region analyzed. This methodology has been implemented as an\nexample within the region of La Rioja in northern Spain, and the mean absolute\nerror found is a striking 25.5% lower than with the original database.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 08:13:20 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Antonanzas-Torres", "F.", ""], ["de Pis\u00f3n", "F. J. Mart\u00ednez", ""], ["Antonanzas", "J.", ""], ["Perpi\u00f1\u00e1n", "O.", ""]]}]