[{"id": "1412.1013", "submitter": "Alberto Calderone Dr.", "authors": "Alberto Calderone", "title": "Assembling biological boolean networks using manually curated databases\n  and prediction algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the large quantity of information available, thorough researches in\nvarious biological databases are still needed in order to reconstruct and\nunderstand the steps that lead to known or new phenomena. By using\nprotein-protein interaction networks and algorithms to extract relevant\ninterconnections among proteins of interest, it is possible to assemble\nsubnetworks from global interactomes. Using these extracted networks it is\npossible to use algorithms to predict signal directions while activation and\ninhibition effects can be predicted using RNA interference screenings. The\nresult of this approach is the automatic generation of boolean networks. This\nway of modelling dynamical systems allows the discovery of steady states and\nthe prediction of stimuli response.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 18:36:09 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Calderone", "Alberto", ""]]}, {"id": "1412.1074", "submitter": "Alexandre Drouin", "authors": "Alexandre Drouin, S\\'ebastien Gigu\\`ere, Vladana Sagatovich, Maxime\n  D\\'eraspe, Fran\\c{c}ois Laviolette, Mario Marchand, Jacques Corbeil", "title": "Learning interpretable models of phenotypes from whole genome sequences\n  with the Set Covering Machine", "comments": "Presented at Machine Learning in Computational Biology 2014,\n  Montr\\'eal, Qu\\'ebec, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased affordability of whole genome sequencing has motivated its use\nfor phenotypic studies. We address the problem of learning interpretable models\nfor discrete phenotypes from whole genomes. We propose a general approach that\nrelies on the Set Covering Machine and a k-mer representation of the genomes.\nWe show results for the problem of predicting the resistance of Pseudomonas\nAeruginosa, an important human pathogen, against 4 antibiotics. Our results\ndemonstrate that extremely sparse models which are biologically relevant can be\nlearnt using this approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 13:26:50 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Drouin", "Alexandre", ""], ["Gigu\u00e8re", "S\u00e9bastien", ""], ["Sagatovich", "Vladana", ""], ["D\u00e9raspe", "Maxime", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""], ["Corbeil", "Jacques", ""]]}, {"id": "1412.1463", "submitter": "Am\\'elie Rolland", "authors": "S\\'ebastien Gigu\\`ere, Am\\'elie Rolland, Fran\\c{c}ois Laviolette and\n  Mario Marchand", "title": "On the String Kernel Pre-Image Problem with Applications in Drug\n  Discovery", "comments": "Peer-reviewed and accepted for presentation at Machine Learning in\n  Computational Biology 2014, Montr\\'eal, Qu\\'ebec, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pre-image problem has to be solved during inference by most structured\noutput predictors. For string kernels, this problem corresponds to finding the\nstring associated to a given input. An algorithm capable of solving or finding\ngood approximations to this problem would have many applications in\ncomputational biology and other fields. This work uses a recent result on\ncombinatorial optimization of linear predictors based on string kernels to\ndevelop, for the pre-image, a low complexity upper bound valid for many string\nkernels. This upper bound is used with success in a branch and bound searching\nalgorithm. Applications and results in the discovery of druggable peptides are\npresented and discussed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 20:33:57 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 02:51:56 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Gigu\u00e8re", "S\u00e9bastien", ""], ["Rolland", "Am\u00e9lie", ""], ["Laviolette", "Fran\u00e7ois", ""], ["Marchand", "Mario", ""]]}, {"id": "1412.1929", "submitter": "Sebastian B\\\"ocker", "authors": "Kai D\\\"uhrkop and Sebastian B\\\"ocker", "title": "Fragmentation trees reloaded", "comments": "different dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metabolites, small molecules that are involved in cellular reactions, provide\na direct functional signature of cellular state. Untargeted metabolomics\nexperiments usually relies on tandem mass spectrometry to identify the\nthousands of compounds in a biological sample. Today, the vast majority of\nmetabolites remain unknown. Fragmentation trees have become a powerful tool for\nthe interpretation of tandem mass spectrometry data of small molecules. These\ntrees are found by combinatorial optimization, and aim at explaining the\nexperimental data via fragmentation cascades. To obtain biochemically\nmeaningful results requires an elaborate optimization function. We present a\nnew scoring for computing fragmentation trees, transforming the combinatorial\noptimization into a maximum a posteriori estimator. We demonstrate the\nsuperiority of the new scoring for two tasks: Both for the de novo\nidentification of molecular formulas of unknown compounds, and for searching a\ndatabase for structurally similar compounds, our methods performs significantly\nbetter than the previous scoring, as well as other methods for this task. Our\nmethod can expedite the workflow for untargeted metabolomics, allowing\nresearchers to investigate unknowns using automated computational methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 09:20:07 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 15:04:23 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 14:35:36 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["D\u00fchrkop", "Kai", ""], ["B\u00f6cker", "Sebastian", ""]]}, {"id": "1412.2155", "submitter": "Susan Khor", "authors": "Susan Khor", "title": "Protein residue networks from a local search perspective", "comments": "v5 has 74 pages, a correction in section 2.1, an expansion of\n  Appendix B and the addition of Appendix H. Only materials and results related\n  to sections 3.1 to 3.6 have been published in the journal article which has\n  the same title as this manuscript. Materials and results from section 3.7 and\n  section 4 are each expanded in other manuscripts, Journal of Complex Networks\n  (2015)", "journal-ref": "J Complex Netw (2016) 4 (2): 245-278", "doi": "10.1093/comnet/cnv014", "report-no": null, "categories": "q-bio.MN cs.CE cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We examined protein residue networks (PRNs) from a local search perspective\nto understand why PRNs are highly clustered when having short paths is\nimportant for protein functionality. We found that by adopting a local search\nperspective, this conflict between form and function is resolved as increased\nclustering actually helps to reduce path length in PRNs. Further, the paths\nfound via our EDS local search algorithm are more congruent with the\ncharacteristics of intra-protein communication. EDS identifies a subset of PRN\nedges called short-cuts that are distinct, have high usage, impacts EDS path\nlength, diversity and stretch, and are dominated by short-range contacts. The\nshort-cuts form a network (SCN) that increases in size and transitivity as a\nprotein folds. The structure of a SCN supports its function and formation, and\nthe function of a SCN influences its formation. Several significant differences\nin terms of SCN structure, function and formation is found between successful\nand unsuccessful MD trajectories. By connecting the static and the dynamic\naspects of PRNs, the protein folding process becomes a problem of graph\nformation with the purpose of forming suitable pathways within proteins.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 20:20:14 GMT"}, {"version": "v2", "created": "Wed, 17 Dec 2014 16:37:14 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2015 17:43:47 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2015 19:00:12 GMT"}, {"version": "v5", "created": "Mon, 30 Nov 2015 00:20:33 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Khor", "Susan", ""]]}, {"id": "1412.2417", "submitter": "Hantian Zhang", "authors": "Hantian Zhang", "title": "Non-smooth Approach for Contact Dynamics and Impulse-based Control of\n  Frictional Furuta Pendulum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CE physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis, a non-penetrated and physically consistent non-smooth\nnumerical approach has been proposed, by employing the Prox formulation and\nMoreau's mid-point time-stepping rule, for the contact dynamics with coupled\nand decoupled constraints. Under this circumstance, the robust impulse-based\ncontrol has been successfully implemented and validated on the motion system of\ncontrolled frictional oscillator. Further improvement has been achieved by\nutilizing shooting method in the impulse estimating process instead of robust\nestimation. This non-smooth numerical technique has been applied to the\nunder-actuated friction-coupled mulit-body system, by means of an\nimplementation on the controlled frictional Furuta pendulum. The specifically\ndesigned impulse-based controller has successfully solved the problem of\nstabilization of the inverted frictional Furuta pendulum, which is suffered\nfrom the stiction effect of friction.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 00:17:44 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 00:45:10 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Zhang", "Hantian", ""]]}, {"id": "1412.3138", "submitter": "Yichao Zhou", "authors": "Yichao Zhou, Yuexin Wu, and Jianyang Zeng", "title": "Computational Protein Design Using AND/OR Branch-and-Bound Search", "comments": "RECOMB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of the global minimum energy conformation (GMEC) is an\nimportant and challenging topic in structure-based computational protein\ndesign. In this paper, we propose a new protein design algorithm based on the\nAND/OR branch-and-bound (AOBB) search, which is a variant of the traditional\nbranch-and-bound search algorithm, to solve this combinatorial optimization\nproblem. By integrating with a powerful heuristic function, AOBB is able to\nfully exploit the graph structure of the underlying residue interaction network\nof a backbone template to significantly accelerate the design process. Tests on\nreal protein data show that our new protein design algorithm is able to solve\nmany prob- lems that were previously unsolvable by the traditional exact search\nalgorithms, and for the problems that can be solved with traditional provable\nalgorithms, our new method can provide a large speedup by several orders of\nmagnitude while still guaranteeing to find the global minimum energy\nconformation (GMEC) solution.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 12:23:10 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 14:06:36 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Zhou", "Yichao", ""], ["Wu", "Yuexin", ""], ["Zeng", "Jianyang", ""]]}, {"id": "1412.4076", "submitter": "Mareike Fischer", "authors": "Steven Kelk and Mareike Fischer", "title": "On the complexity of computing MP distance between binary phylogenetic\n  trees", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the field of phylogenetics there is great interest in distance\nmeasures to quantify the dissimilarity of two trees. Recently, a new distance\nmeasure has been proposed: the Maximum Parsimony (MP) distance. This is based\non the difference of the parsimony scores of a single character on both trees\nunder consideration, and the goal is to find the character which maximizes this\ndifference. Here we show that computation of MP distance on two \\emph{binary}\nphylogenetic trees is NP-hard. This is a highly nontrivial extension of an\nearlier NP-hardness proof for two multifurcating phylogenetic trees, and it is\nparticularly relevant given the prominence of binary trees in the phylogenetics\nliterature. As a corollary to the main hardness result we show that computation\nof MP distance is also hard on binary trees if the number of states available\nis bounded. In fact, via a different reduction we show that it is hard even if\nonly two states are available. Finally, as a first response to this hardness we\ngive a simple Integer Linear Program (ILP) formulation which is capable of\ncomputing the MP distance exactly for small trees (and for larger trees when\nonly a small number of character states are available) and which is used to\ncomputationally verify several auxiliary results required by the hardness\nproofs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 18:24:35 GMT"}, {"version": "v2", "created": "Sun, 18 Jan 2015 09:54:03 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Kelk", "Steven", ""], ["Fischer", "Mareike", ""]]}, {"id": "1412.4192", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie, Hamid Abrishami Moghaddam", "title": "Finite Element Method Based Modeling of Cardiac Deformation Estimation\n  under Abnormal Ventricular Muscle Conditions", "comments": "Some grammatical errors are corrected in the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformation modeling of cardiac muscle is an important issue in the field of\ncardiac analysis. Many approaches have been developed to better estimate the\ncardiac muscle deformation, and to obtain a practical model to be used in\ndiagnostic procedures. But there are some conditions, like in case of\nmyocardial infarction, in which the regular modeling approaches are not useful.\nIn this article, using a point-wise approach, we try to estimate the\ndeformation under some abnormal conditions of cardiac muscle. First, the\nendocardial and epicardial contour points are ordered with respect to the\ncenter of gravity of the endocardial contour and displacement vectors of\nboundary points are extracted. Then to solve the governing equations of the\ndeformation, which is an elliptic equation, we apply boundary conditions in\naccordance with the computed displacement vectors and then the Finite Element\nmethod (FEM) will be used to solve the governing equations. Using the obtained\ndisplacement field of the cardiac muscle, strain map is extracted to show the\nmechanical behavior of the cardiac muscle. Several tests are conducted using\nphantom and real cardiac data in order to show the validity of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 04:49:31 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2015 22:24:30 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 04:29:03 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Baghaie", "Ahmadreza", ""], ["Moghaddam", "Hamid Abrishami", ""]]}, {"id": "1412.4388", "submitter": "Lidia Dobrescu", "authors": "Silviu Stanciu, Lidia Dobrescu", "title": "A New System For Recording The Radiological Effective Doses For Patients\n  Investigated by Imaging Methods", "comments": "International Congress Of Radiology ICR 2014, Dubai, United Arab\n  Emirates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.med-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper the project of an integrated system for radiation safety and\nsecurity of the patients investigated by radiological imaging methods is\npresented. The new system is based on smart cards and Public Key\nInfrastructure. The new system allows radiation effective dose data storage and\na more accurate reporting system.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 18:11:18 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Stanciu", "Silviu", ""], ["Dobrescu", "Lidia", ""]]}, {"id": "1412.4556", "submitter": "Blesson Varghese", "authors": "Blesson Varghese and Adam Barker", "title": "Are Clouds Ready to Accelerate Ad hoc Financial Simulations?", "comments": "Best paper nominee at the International Symposium on Big Data\n  Computing (BDC 2014) in conjunction with IEEE/ACM Utility and Cloud Computing\n  (UCC), 2014 London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications employed in the financial services industry to capture and\nestimate a variety of risk metrics are underpinned by stochastic simulations\nwhich are data, memory and computationally intensive. Many of these simulations\nare routinely performed on production-based computing systems. Ad hoc\nsimulations in addition to routine simulations are required to obtain\nup-to-date views of risk metrics. Such simulations are currently not performed\nas they cannot be accommodated on production clusters, which are typically over\ncommitted resources. Scalable, on-demand and pay-as-you go Virtual Machines\n(VMs) offered by the cloud are a potential platform to satisfy the data, memory\nand computational constraints of the simulation. However, \"Are clouds ready to\naccelerate ad hoc financial simulations?\"\n  The research reported in this paper aims to experimentally verify this\nquestion by developing and deploying an important financial simulation,\nreferred to as 'Aggregate Risk Analysis' on the cloud. Parallel techniques to\nimprove efficiency and performance of the simulations are explored. Challenges\nsuch as accommodating large input data on limited memory VMs and rapidly\nprocessing data for real-time use are surmounted. The key result of this\ninvestigation is that Aggregate Risk Analysis can be accommodated on cloud VMs.\nAcceleration of up to 24x using multiple hardware accelerators over the\nimplementation on a single accelerator, 6x over a multiple core implementation\nand approximately 60x over a baseline implementation was achieved on the cloud.\nHowever, computational time is wasted for every dollar spent on the cloud due\nto poor acceleration over multiple virtual cores. Interestingly, private VMs\ncan offer better performance than public VMs on comparable underlying hardware.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 12:03:13 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Varghese", "Blesson", ""], ["Barker", "Adam", ""]]}, {"id": "1412.4726", "submitter": "Rustam Tagiew", "authors": "Rustam Tagiew and Dmitry I. Ignatov and Fadi Amroush", "title": "Experimental economics for web mining", "comments": "3 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers a step towards research infrastructure, which makes data\nfrom experimental economics efficiently usable for analysis of web data. We\nbelieve that regularities of human behavior found in experimental data also\nemerge in real world web data. A format for data from experiments is suggested,\nwhich enables its publication as open data. Once standardized datasets of\nexperiments are available on-line, web mining can take advantages from this\ndata. Further, the questions about the order of causalities arisen from web\ndata analysis can inspire new experiment setups.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 19:09:48 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Tagiew", "Rustam", ""], ["Ignatov", "Dmitry I.", ""], ["Amroush", "Fadi", ""]]}, {"id": "1412.5452", "submitter": "Peter Sarlin", "authors": "Jozsef Mezei and Peter Sarlin", "title": "Aggregation operators for the measurement of systemic risk", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CE q-fin.EC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The policy objective of safeguarding financial stability has stimulated a\nwave of research on systemic risk analytics, yet it still faces challenges in\nmeasurability. This paper models systemic risk by tapping into expert knowledge\nof financial supervisors. We decompose systemic risk into a number of\ninterconnected segments, for which the level of vulnerability is measured. The\nsystem is modeled in the form of a Fuzzy Cognitive Map (FCM), in which nodes\nrepresent vulnerability in segments and links their interconnectedness. A main\nproblem tackled in this paper is the aggregation of values in different\ninterrelated nodes of the network to obtain an estimate systemic risk. To this\nend, the Choquet integral is employed for aggregating expert evaluations of\nmeasures, as it allows for the integration of interrelations among factors in\nthe aggregation process. The approach is illustrated through two applications\nin a European setting. First, we provide an estimation of systemic risk with a\nof pan-European set-up. Second, we estimate country-level risks, allowing for a\nmore granular decomposition. This sets a starting point for the use of the\nrich, oftentimes tacit, knowledge in policy organizations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 15:55:00 GMT"}, {"version": "v2", "created": "Sat, 27 Dec 2014 00:00:26 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Mezei", "Jozsef", ""], ["Sarlin", "Peter", ""]]}, {"id": "1412.5459", "submitter": "Mariam Kiran Dr.", "authors": "Mariam Kiran and Wei Liu", "title": "Converting a Systems Dynamic Model to an Agent-based model for studying\n  the Bicoid morphogen gradient in Drosophila embryo", "comments": "21 pages, 13 figures, technical report, exploratory study", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.CE q-bio.QM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The concentration gradient of the Bicoid morphogen, which is established\nduring the early stages of a Drosophila melanogaster embryonic development,\ndetermines the differential spatial patterns of gene expression and subsequent\ncell fate determination. This is mainly achieved by diffusion elicited by the\ndifferent concentrations of the Bicoid protein in the embryo. Such chemical\ndynamic progress can be simulated by stochastic models, particularly the\nGillespie alogrithm. However, as with various modelling approaches in biology,\neach technique involves drawing assumptions and reducing the model complexity\nsometimes limiting the model capability. This is mainly due to the complexity\nof the software modelling approaches to construct these models. Agent-based\nmodelling is a technique which is becoming increasingly popular for modelling\nthe behaviour of individual molecules or cells in computational biology.\n  This paper attempts to compare these two popular modelling techniques of\nstochastic and agent-based modelling to show how the model can be studied in\ndetail using the different approaches. This paper presents how to use these\ntechniques with the advantages and disadvantages of using either of these.\nThrough various comparisons, such as computation complexity and results\nobtained, we show that although the same model is implemented, both approaches\ncan give varying results. The results of the paper show that the stochastic\nmodel is able to give smoother results compared to the agent-based model which\nmay need further analysis at a later stage. We discuss the reasons for these\nresults and how these could be rectified in systems biology research.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 16:14:55 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Kiran", "Mariam", ""], ["Liu", "Wei", ""]]}, {"id": "1412.5496", "submitter": "Sylvain Lavernhe", "authors": "Shixin X\\'u (LURPA), Nabil Anwer (LURPA), Sylvain Lavernhe (LURPA)", "title": "Conversion of G-code programs for milling into STEP-NC", "comments": "Joint Conference on Mechanical, Design Engineering \\& Advanced\n  Manufacturing, Jun 2014, Toulouse, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  STEP-NC (ISO 14649) is becoming a promising standard to replace or supplement\nthe conventional G-code programs based on ISO 6983 due to its feature based\nmachine independent characteristics and its centric role to enable efficient\nCAD/CAM/CNC interoperability. The re-use of G-code programs is important for\nboth manufacturing and capitalization of machining knowledge, nevertheless the\nconversion is a tedious task when carried out manually and machining knowledge\nis almost hidden in the low level G-code. Mapping G-code into STEP-NC should\nbenefit from more expressiveness of the manufacturing feature-based\ncharacteristics of this new standard. The work presented here proposes an\noverall method for G-code to STEP-NC conversion. First, G-code is converted\ninto canonical machining functions, this can make the method more applicable\nand make subsequent processes easier to implement; then these functions are\nparsed to generate the neutral format of STEP-NC Part21 toolpath file, this\nturns G-code into object instances, and can facilitate company's usage of\nlegacy programs; and finally, also optionally, machining features are extracted\nto generate Part21 CC2 (conformance class) file. The proposed extraction method\nemploys geometric information of cutting area inferred from toolpaths and\nmachining strategies, in addition to cutting tools' data and workpiece's\ndimension data. This comprehensive use of available data makes the extraction\nmore accurate and reliable. The conversion method is holistic, and can be\nextended to process a wide range of G-code programs (e.g. turning or mill-turn\ncodes) with as few user interventions as possible.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 20:34:48 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["X\u00fa", "Shixin", "", "LURPA"], ["Anwer", "Nabil", "", "LURPA"], ["Lavernhe", "Sylvain", "", "LURPA"]]}, {"id": "1412.5517", "submitter": "Engelbert Mephu Nguifo", "authors": "Jocelyn De Goer De Herve, Myoung-Ah Kang, Xavier Bailly, Engelbert\n  Mephu Nguifo", "title": "A perceptual hash function to store and retrieve large scale DNA\n  sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel approach for storing and retrieving massive DNA\nsequences.. The method is based on a perceptual hash function, commonly used to\ndetermine the similarity between digital images, that we adapted for DNA\nsequences. Perceptual hash function presented here is based on a Discrete\nCosine Transform Sign Only (DCT-SO). Each nucleotide is encoded as a fixed gray\nlevel intensity pixel and the hash is calculated from its significant frequency\ncharacteristics. This results to a drastic data reduction between the sequence\nand the perceptual hash. Unlike cryptographic hash functions, perceptual hashes\nare not affected by \"avalanche effect\" and thus can be compared. The similarity\ndistance between two hashes is estimated with the Hamming Distance, which is\nused to retrieve DNA sequences. Experiments that we conducted show that our\napproach is relevant for storing massive DNA sequences, and retrieving them.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 18:47:57 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["De Herve", "Jocelyn De Goer", ""], ["Kang", "Myoung-Ah", ""], ["Bailly", "Xavier", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1412.5627", "submitter": "Fabricio Martins Lopes", "authors": "Bruno Mendes Moro Conque and Andr\\'e Yoshiaki Kashiwabara and\n  Fabr\\'icio Martins Lopes", "title": "Feature extraction from complex networks: A case of study in genomic\n  sequences classification", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new approach for classification of genomic sequences\nfrom measurements of complex networks and information theory. For this, it is\nconsidered the nucleotides, dinucleotides and trinucleotides of a genomic\nsequence. For each of them, the entropy, sum entropy and maximum entropy values\nare calculated.For each of them is also generated a network, in which the nodes\nare the nucleotides, dinucleotides or trinucleotides and its edges are\nestimated by observing the respective adjacency among them in the genomic\nsequence. In this way, it is generated three networks, for which measures of\ncomplex networks are extracted.These measures together with measures of\ninformation theory comprise a feature vector representing a genomic sequence.\nThus, the feature vector is used for classification by methods such as SVM,\nMultiLayer Perceptron, J48, IBK, Naive Bayes and Random Forest in order to\nevaluate the proposed approach.It was adopted coding sequences, intergenic\nsequences and TSS (Transcriptional Starter Sites) as datasets, for which the\nbetter results were obtained by the Random Forest with 91.2%, followed by J48\nwith 89.1% and SVM with 84.8% of accuracy. These results indicate that the new\napproach of feature extraction has its value, reaching good levels of\nclassification even considering only the genomic sequences, i.e., no other a\npriori knowledge about them is considered.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 21:31:51 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Conque", "Bruno Mendes Moro", ""], ["Kashiwabara", "Andr\u00e9 Yoshiaki", ""], ["Lopes", "Fabr\u00edcio Martins", ""]]}, {"id": "1412.6063", "submitter": "Jamal Amani Rad", "authors": "Jamal Amani Rad and Kourosh Parand and Saeid Abbasbandy", "title": "Local weak form meshless techniques based on the radial point\n  interpolation (RPI) method and local boundary integral equation (LBIE) method\n  to evaluate European and American options", "comments": null, "journal-ref": "dx.doi.org/10.1016/j.cnsns.2014.07.015", "doi": null, "report-no": null, "categories": "cs.CE q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the first time in mathematical finance field, we propose the local weak\nform meshless methods for option pricing; especially in this paper we select\nand analysis two schemes of them named local boundary integral equation method\n(LBIE) based on moving least squares approximation (MLS) and local radial point\ninterpolation (LRPI) based on Wu's compactly supported radial basis functions\n(WCS-RBFs). LBIE and LRPI schemes are the truly meshless methods, because, a\ntraditional non-overlapping, continuous mesh is not required, either for the\nconstruction of the shape functions, or for the integration of the local\nsub-domains. In this work, the American option which is a free boundary\nproblem, is reduced to a problem with fixed boundary using a Richardson\nextrapolation technique. Then the $\\theta$-weighted scheme is employed for the\ntime derivative. Stability analysis of the methods is analyzed and performed by\nthe matrix method. In fact, based on an analysis carried out in the present\npaper, the methods are unconditionally stable for implicit Euler (\\theta = 0)\nand Crank-Nicolson (\\theta = 0.5) schemes. It should be noted that LBIE and\nLRPI schemes lead to banded and sparse system matrices. Therefore, we use a\npowerful iterative algorithm named the Bi-conjugate gradient stabilized method\n(BCGSTAB) to get rid of this system. Numerical experiments are presented\nshowing that the LBIE and LRPI approaches are extremely accurate and fast.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 20:05:08 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Rad", "Jamal Amani", ""], ["Parand", "Kourosh", ""], ["Abbasbandy", "Saeid", ""]]}, {"id": "1412.6064", "submitter": "Jamal Amani Rad", "authors": "Jamal Amani Rad and Kourosh Parand", "title": "Numerical pricing of American options under two stochastic factor models\n  with jumps using a meshless local Petrov-Galerkin method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most recent update of financial option models is American options under\nstochastic volatility models with jumps in returns (SVJ) and stochastic\nvolatility models with jumps in returns and volatility (SVCJ). To evaluate\nthese options, mesh-based methods are applied in a number of papers but it is\nwell-known that these methods depend strongly on the mesh properties which is\nthe major disadvantage of them. Therefore, we propose the use of the meshless\nmethods to solve the aforementioned options models, especially in this work we\nselect and analyze one scheme of them, named local radial point interpolation\n(LRPI) based on Wendland's compactly supported radial basis functions\n(WCS-RBFs) with C6, C4 and C2 smoothness degrees. The LRPI method which is a\nspecial type of meshless local Petrov-Galerkin method (MLPG), offers several\nadvantages over the mesh-based methods, nevertheless it has never been applied\nto option pricing, at least to the very best of our knowledge. These schemes\nare the truly meshless methods, because, a traditional non-overlapping\ncontinuous mesh is not required, neither for the construction of the shape\nfunctions, nor for the integration of the local sub-domains. In this work, the\nAmerican option which is a free boundary problem, is reduced to a problem with\nfixed boundary using a Richardson extrapolation technique. Then the\nimplicit-explicit (IMEX) time stepping scheme is employed for the time\nderivative which allows us to smooth the discontinuities of the options'\npayoffs. Stability analysis of the method is analyzed and performed. In fact,\naccording to an analysis carried out in the present paper, the proposed method\nis unconditionally stable. Numerical experiments are presented showing that the\nproposed approaches are extremely accurate and fast.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 19:37:14 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Rad", "Jamal Amani", ""], ["Parand", "Kourosh", ""]]}, {"id": "1412.6306", "submitter": "Lidia Dobrescu", "authors": "Adrian-Ioan Lita, Ioan Plotog, Lidia Dobrescu", "title": "Multiprocessor System Dedicated to Multi-Rotor Mini-UAV Capable of 3D\n  flying", "comments": "International Conference of Scientific Paper AFASES 2014 Brasov,\n  22-24 May 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The paper describes an electronic multiprocessor system that assures\nfunctionality of a miniature UAV capable of 3D flying. The apparatus consists\nof six independently controlled brushless DC motors, each having a propeller\nattached to it. Since the brushless motor requires complex algorithms in order\nto achieve maximum torque, efficiency and response time a DSP must be used. All\nthe motors are then controlled by a main microprocessor which is capable of\nreading sensors (Inertial Measurement Unit (IMU)-orientation and GPS),\nreceiving input commands (remote controller or trajectory plan) and sending\nindependent commands to each of the six motors. The apparatus contains a total\nof eight microcontrollers: the main unit, the IMU mathematical processor and\none microcontroller for each of the six brushless DC motors. Applications for\nsuch an apparatus could include not only military, but also search-and-rescue,\ngeodetics, aerial photography and aerial assistance.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 11:53:38 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Lita", "Adrian-Ioan", ""], ["Plotog", "Ioan", ""], ["Dobrescu", "Lidia", ""]]}, {"id": "1412.6341", "submitter": "Peteris Daugulis", "authors": "Peteris Daugulis", "title": "Connectome graphs and maximum flow problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study maximum flow problems for connectome graphs. We suggest a\nfew computational problems: finding vertex pairs with maximal flow, finding new\nedges which would increase the maximal flow. Initial computation results for\nsome publicly available connectome graphs are described.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 13:45:26 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Daugulis", "Peteris", ""]]}, {"id": "1412.6368", "submitter": "Cl\\'ement Walter", "authors": "Cl\\'ement Walter", "title": "Point Process-based Monte Carlo estimation", "comments": "13 pages + 4 pages of appendix, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue of estimating the expectation of a real-valued\nrandom variable of the form $X = g(\\mathbf{U})$ where $g$ is a deterministic\nfunction and $\\mathbf{U}$ can be a random finite- or infinite-dimensional\nvector. Using recent results on rare event simulation, we propose a unified\nframework for dealing with both probability and mean estimation for such random\nvariables, \\emph{i.e.} linking algorithms such as Tootsie Pop Algorithm (TPA)\nor Last Particle Algorithm with nested sampling. Especially, it extends nested\nsampling as follows: first the random variable $X$ does not need to be bounded\nany more: it gives the principle of an ideal estimator with an infinite number\nof terms that is unbiased and always better than a classical Monte Carlo\nestimator -- in particular it has a finite variance as soon as there exists $k\n\\in \\mathbb{R} > 1$ such that $\\operatorname{E}[X^k] < \\infty$. Moreover we\naddress the issue of nested sampling termination and show that a random\ntruncation of the sum can preserve unbiasedness while increasing the variance\nonly by a factor up to 2 compared to the ideal case. We also build an unbiased\nestimator with fixed computational budget which supports a Central Limit\nTheorem and discuss parallel implementation of nested sampling, which can\ndramatically reduce its computational cost. Finally we extensively study the\ncase where $X$ is heavy-tailed.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:09:59 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 08:54:23 GMT"}, {"version": "v3", "created": "Wed, 21 Jan 2015 15:48:52 GMT"}, {"version": "v4", "created": "Tue, 12 May 2015 11:55:41 GMT"}, {"version": "v5", "created": "Wed, 9 Sep 2015 10:02:52 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Walter", "Cl\u00e9ment", ""]]}, {"id": "1412.6383", "submitter": "Pierre de Buyl", "authors": "Christophe Pouzat, Georgios Is. Detorakis", "title": "SPySort: Neuronal Spike Sorting with Python", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-05", "categories": "cs.CE q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Extracellular recordings with multi-electrode arrays is one of the basic\ntools of contemporary neuroscience. These recordings are mostly used to monitor\nthe activities, understood as sequences of emitted action potentials, of many\nindividual neurons. But the raw data produced by extracellular recordings are\nmost commonly a mixture of activities from several neurons. In order to get the\nactivities of the individual contributing neurons, a pre-processing step called\nspike sorting is required. We present here a pure Python implementation of a\nwell tested spike sorting procedure. The latter was designed in a modular way\nin order to favour a smooth transition from an interactive sorting, for\ninstance with IPython, to an automatic one. Surprisingly enough - or sadly\nenough, depending on one's view point -, recoding our now 15 years old\nprocedure into Python was the occasion of major methodological improvements.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:40:00 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Pouzat", "Christophe", ""], ["Detorakis", "Georgios Is.", ""]]}, {"id": "1412.6386", "submitter": "Pierre de Buyl", "authors": "Thomas Cokelaer, Julio Saez-Rodriguez", "title": "Using Python to Dive into Signalling Data with CellNOpt and BioServices", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-06", "categories": "cs.CE q-bio.MN", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Systems biology is an inter-disciplinary field that studies systems of\nbiological components at different scales, which may be molecules, cells or\nentire organism. In particular, systems biology methods are applied to\nunderstand functional deregulations within human cells (e.g., cancers). In this\ncontext, we present several python packages linked to CellNOptR (R package),\nwhich is used to build predictive logic models of signalling networks by\ntraining networks (derived from literature) to signalling (phospho-proteomic)\ndata. The first package (cellnopt.wrapper) is a wrapper based on RPY2 that\nallows a full access to CellNOptR functionalities within Python. The second one\n(cellnopt.core) was designed to ease the manipulation and visualisation of data\nstructures used in CellNOptR, which was achieved by using Pandas, NetworkX and\nmatplotlib. Systems biology also makes extensive use of web resources and\nservices. We will give an overview and status of BioServices, which allows one\nto access programmatically to web resources used in life science and how it can\nbe combined with CellNOptR.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:43:09 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Cokelaer", "Thomas", ""], ["Saez-Rodriguez", "Julio", ""]]}, {"id": "1412.6391", "submitter": "Pierre de Buyl", "authors": "Davide Monari, Francesco Cenni, Erwin Aertbeli\\\"en, Kaat Desloovere", "title": "Py3DFreeHandUS: a library for voxel-array reconstruction using\n  Ultrasonography and attitude sensors", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-07", "categories": "cs.CV cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In medical imaging, there is a growing interest to provide real-time images\nwith good quality for large anatomical structures. To cope with this issue, we\ndeveloped a library that allows to replace, for some specific clinical\napplications, more robust systems such as Computer Tomography (CT) and Magnetic\nResonance Imaging (MRI). Our python library Py3DFreeHandUS is a package for\nprocessing data acquired simultaneously by ultra-sonographic systems (US) and\nmarker-based optoelectronic systems. In particular, US data enables to\nvisualize subcutaneous body structures, whereas the optoelectronic system is\nable to collect the 3D position in space for reflective objects, that are\ncalled markers. By combining these two measurement devices, it is possible to\nreconstruct the real 3D morphology of body structures such as muscles, for\nrelevant clinical implications. In the present research work, the different\nsteps which allow to obtain a relevant 3D data set as well as the procedures\nfor calibrating the systems and for determining the quality of the\nreconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:47:47 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Monari", "Davide", ""], ["Cenni", "Francesco", ""], ["Aertbeli\u00ebn", "Erwin", ""], ["Desloovere", "Kaat", ""]]}, {"id": "1412.6399", "submitter": "Pierre de Buyl", "authors": "Jamie A. Dean, Liam C. Welsh, Kevin J. Harrington, Christopher M.\n  Nutting, Sarah L. Gulliford", "title": "Predictive Modelling of Toxicity Resulting from Radiotherapy Treatments\n  of Head and Neck Cancer", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-09", "categories": "physics.med-ph cs.CE q-bio.QM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In radiotherapy for head and neck cancer, the radiation dose delivered to the\npharyngeal mucosa (mucosal lining of the throat) is thought to be a major\ncontributing factor to dysphagia (swallowing dysfunction), the most commonly\nreported severe toxicity. There is a variation in the severity of dysphagia\nexperienced by patients. Understanding the role of the dose distribution in\ndysphagia would allow improvements in the radiotherapy technique to be\nexplored. The 3D dose distributions delivered to the pharyngeal mucosa of 249\npatients treated as part of clinical trials were reconstructed. Pydicom was\nused to extract DICOM (digital imaging and communications in medicine) data\n(the standard file formats for medical imaging and radiotherapy data). NumPy\nand SciPy were used to manipulate the data to generate 3D maps of the dose\ndistribution delivered to the pharyngeal mucosa and calculate metrics\ndescribing the dose distribution. Multivariate predictive modelling of severe\ndysphagia, including descriptions of the dose distribution and relevant\nclinical factors, was performed using Pandas and SciKit-Learn. Matplotlib and\nMayavi were used for 2D and 3D data visualisation. A support vector\nclassification model, with feature selection using randomised logistic\nregression, to predict radiation-induced severe dysphagia, was trained. When\nthis model was independently validated, the area under the receiver operating\ncharacteristic curve was 0.54. The model has poor predictive power and work is\nongoing to improve the model through alternative feature engineering and\nstatistical modelling approaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:54:51 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Dean", "Jamie A.", ""], ["Welsh", "Liam C.", ""], ["Harrington", "Kevin J.", ""], ["Nutting", "Christopher M.", ""], ["Gulliford", "Sarah L.", ""]]}, {"id": "1412.6402", "submitter": "Pierre de Buyl", "authors": "Rebecca R. Murphy, Sophie E. Jackson, David Klenerman", "title": "pyFRET: A Python Library for Single Molecule Fluorescence Data Analysis", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-10", "categories": "cs.CE physics.bio-ph q-bio.BM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Single molecule F\\\"orster resonance energy transfer (smFRET) is a powerful\nexperimental technique for studying the properties of individual biological\nmolecules in solution. However, as adoption of smFRET techniques becomes more\nwidespread, the lack of available software, whether open source or commercial,\nfor data analysis, is becoming a significant issue. Here, we present pyFRET, an\nopen source Python package for the analysis of data from single-molecule\nfluorescence experiments from freely diffusing biomolecules. The package\nprovides methods for the complete analysis of a smFRET dataset, from burst\nselection and denoising, through data visualisation and model fitting. We\nprovide support for both continuous excitation and alternating laser excitation\n(ALEX) data analysis. pyFRET is available as a package downloadable from the\nPython Package Index (PyPI) under the open source three-clause BSD licence,\ntogether with links to extensive documentation and tutorials, including example\nusage and test data. Additional documentation including tutorials is hosted\nindependently on ReadTheDocs. The code is available from the free hosting site\nBitbucket. Through distribution of this software, we hope to lower the barrier\nfor the adoption of smFRET experiments by other research groups and we\nencourage others to contribute modules for specific analysis needs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 16:00:31 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Murphy", "Rebecca R.", ""], ["Jackson", "Sophie E.", ""], ["Klenerman", "David", ""]]}, {"id": "1412.6410", "submitter": "Pierre de Buyl", "authors": "Steve Brasier, Fred Pollard", "title": "A Python-based Post-processing Toolset For Seismic Analyses", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-12", "categories": "cs.CE physics.geo-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper discusses the design and implementation of a Python-based toolset\nto aid in assessing the response of the UK's Advanced Gas Reactor nuclear power\nstations to earthquakes. The seismic analyses themselves are carried out with a\ncommercial Finite Element solver, but understanding the raw model output this\nproduces requires customised post-processing and visualisation tools. Extending\nthe existing tools had become increasingly difficult and a decision was made to\ndevelop a new, Python-based toolset. This comprises of a post-processing\nframework (aftershock) which includes an embedded Python interpreter, and a\nplotting package (afterplot) based on numpy and matplotlib. The new toolset had\nto be significantly more flexible and easier to maintain than the existing\ncode-base, while allowing the majority of development to be carried out by\nengineers with little training in software development. The resulting\narchitecture will be described with a focus on exploring how the design drivers\nwere met and the successes and challenges arising from the choices made.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 16:09:16 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Brasier", "Steve", ""], ["Pollard", "Fred", ""]]}, {"id": "1412.6412", "submitter": "Pierre de Buyl", "authors": "Vladim\\'ir Luke\\v{s}, Miroslav Ji\\v{r}\\'ik, Alena Jon\\'a\\v{s}ov\\'a,\n  Eduard Rohan, Ond\\v{r}ej Bubl\\'ik, Robert Cimrman", "title": "Numerical simulation of liver perfusion: from CT scans to FE model", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-13", "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We use a collection of Python programs for numerical simulation of liver\nperfusion. We have an application for semi-automatic generation of a finite\nelement mesh of the human liver from computed tomography scans and for\nreconstruction of the liver vascular structure. When the real vascular trees\ncan not be obtained from the CT data we generate artificial trees using the\nconstructive optimization method. The generated FE mesh and vascular trees are\nimported into SfePy (Simple Finite Elements in Python) and numerical\nsimulations are performed in order to get the pressure distribution and\nperfusion flows in the liver tissue. In the post-processing steps we calculate\ntransport of a contrast fluid through the liver parenchyma.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 16:12:02 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Luke\u0161", "Vladim\u00edr", ""], ["Ji\u0159\u00edk", "Miroslav", ""], ["Jon\u00e1\u0161ov\u00e1", "Alena", ""], ["Rohan", "Eduard", ""], ["Bubl\u00edk", "Ond\u0159ej", ""], ["Cimrman", "Robert", ""]]}, {"id": "1412.6483", "submitter": "Pierre de Buyl", "authors": "Andrew Leonard, Huw Morgan", "title": "Temperature diagnostics of the solar atmosphere using SunPy", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-03", "categories": "astro-ph.SR cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The solar atmosphere is a hot (about 1MK), magnetised plasma of great\ninterest to physicists. There have been many previous studies of the\ntemperature of the Sun's atmosphere (Plowman2012, Wit2012, Hannah2012,\nAschwanden2013, etc.). Almost all of these studies use the SolarSoft software\npackage written in the commercial Interactive Data Language (IDL), which has\nbeen the standard language for solar physics. The SunPy project aims to provide\nan open-source library for solar physics. This work presents (to the authors'\nknowledge) the first study of its type to use SunPy rather than SolarSoft. This\nwork uses SunPy to process multi-wavelength solar observations made by the\nAtmospheric Imaging Assembly (AIA) instrument aboard the Solar Dynamics\nObservatory (SDO) and produce temperature maps of the Sun's atmosphere. The\nmethod uses SunPy's utilities for querying databases of solar events,\ndownloading solar image data, storing and processing images as spatially aware\nMap objects, and tracking solar features as the Sun rotates. An essential\nconsideration in developing this software is computational efficiency due to\nthe large amount of data collected by AIA/SDO, and in anticipating new solar\nmissions which will result in even larger sets of data. An overview of the\nmethod and implementation is given, along with tests involving synthetic data\nand examples of results using real data for various regions in the Sun's\natmosphere.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 18:53:41 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Leonard", "Andrew", ""], ["Morgan", "Huw", ""]]}, {"id": "1412.6850", "submitter": "Francisco Pe\\~nu\\~nuri", "authors": "O. Mendoza-Trejo, Carlos A. Cruz-Villar, R. Pe\\'on-Escalante, M. A.\n  Zambrano-Arjona, F. Penunuri", "title": "Synthesis Method for the Spherical 4R Mechanism with Minimum Center of\n  Mass Acceleration", "comments": "6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the mechanisms area, minimization of the magnitude of the acceleration of\nthe center of mass (ACoM) implies shaking force balancing. For a mechanism\noperating in cycles, the case when the ACoM is zero implies that the\ngravitational potential energy (GPE) is constant. This article shows an\nefficient and effective optimum synthesis method for minimum acceleration of\nthe center of mass of a spherical 4R mechanism by using dual functions and the\ncounterweights balancing method. Once the dual function for ACoM has been\nwritten, one can minimize the shaking forces from a kinematic point of view. We\npresent the synthesis of a spherical 4R mechanism for the case of a path\ngeneration task. The synthesis process involves the optimization of two\nobjective functions, this multiobjective problem is solved by using the\nweighted sum method implemented in the evolutionary algorithm known as\nDifferential Evolution.\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 00:55:39 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2015 22:59:50 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Mendoza-Trejo", "O.", ""], ["Cruz-Villar", "Carlos A.", ""], ["Pe\u00f3n-Escalante", "R.", ""], ["Zambrano-Arjona", "M. A.", ""], ["Penunuri", "F.", ""]]}, {"id": "1412.7030", "submitter": "Pierre de Buyl", "authors": "Pierre de Buyl, Nelle Varoquaux", "title": "Proceedings of the 7th European Conference on Python in Science\n  (EuroSciPy 2014)", "comments": null, "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-00", "categories": "cs.CE cs.MS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  These are the proceedings of the 7th European Conference on Python in\nScience, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014).\n", "versions": [{"version": "v1", "created": "Mon, 22 Dec 2014 15:47:51 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["de Buyl", "Pierre", ""], ["Varoquaux", "Nelle", ""]]}, {"id": "1412.7384", "submitter": "Peng Yang", "authors": "Peng Yang, Xiaoquan Su, Le Ou-Yang, Hon-Nian Chua, Xiao-Li Li, Kang\n  Ning", "title": "Microbial community pattern detection in human body habitats via\n  ensemble clustering framework", "comments": "BMC Systems Biology 2014", "journal-ref": "BMC Systems Biology 2014, 8(Suppl 4):S7", "doi": "10.1186/1752-0509-8-S4-S7", "report-no": null, "categories": "q-bio.QM cs.CE cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human habitat is a host where microbial species evolve, function, and\ncontinue to evolve. Elucidating how microbial communities respond to human\nhabitats is a fundamental and critical task, as establishing baselines of human\nmicrobiome is essential in understanding its role in human disease and health.\nHowever, current studies usually overlook a complex and interconnected\nlandscape of human microbiome and limit the ability in particular body habitats\nwith learning models of specific criterion. Therefore, these methods could not\ncapture the real-world underlying microbial patterns effectively. To obtain a\ncomprehensive view, we propose a novel ensemble clustering framework to mine\nthe structure of microbial community pattern on large-scale metagenomic data.\nParticularly, we first build a microbial similarity network via integrating\n1920 metagenomic samples from three body habitats of healthy adults. Then a\nnovel symmetric Nonnegative Matrix Factorization (NMF) based ensemble model is\nproposed and applied onto the network to detect clustering pattern. Extensive\nexperiments are conducted to evaluate the effectiveness of our model on\nderiving microbial community with respect to body habitat and host gender. From\nclustering results, we observed that body habitat exhibits a strong bound but\nnon-unique microbial structural patterns. Meanwhile, human microbiome reveals\ndifferent degree of structural variations over body habitat and host gender. In\nsummary, our ensemble clustering framework could efficiently explore integrated\nclustering results to accurately identify microbial communities, and provide a\ncomprehensive view for a set of microbial communities. Such trends depict an\nintegrated biography of microbial communities, which offer a new insight\ntowards uncovering pathogenic model of human microbiome.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 12:52:45 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 02:03:08 GMT"}, {"version": "v3", "created": "Sun, 4 Jan 2015 05:37:42 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Yang", "Peng", ""], ["Su", "Xiaoquan", ""], ["Ou-Yang", "Le", ""], ["Chua", "Hon-Nian", ""], ["Li", "Xiao-Li", ""], ["Ning", "Kang", ""]]}, {"id": "1412.7386", "submitter": "Pietro  Hiram Guzzi", "authors": "Mario Cannataro, Pietro Hiram Guzzi, Marianna Milano, Pierangelo\n  Veltri", "title": "A web-based tool to Analyze Semantic Similarity Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational biology, biological entities such as genes or proteins are\nusually annotated with terms extracted from Gene Ontology (GO). The functional\nsimilarity among terms of an ontology is evaluated by using Semantic Similarity\nMeasures (SSM). More recently, the extensive application of SSMs yielded to the\nSemantic Similarity Networks (SSNs). SSNs are edge-weighted graphs where the\nnodes are concepts (e.g. proteins) and each edge has an associated weight that\nrepresents the semantic similarity among related pairs of nodes. The analysis\nof SSNs may reveal biologically meaningful knowledge. For these aims, the need\nfor the introduction of tool able to manage and analyze SSN arises.\nConsequently we developed SSN-Analyzer a web based tool able to build and\npreprocess SSN. As proof of concept we demonstrate that community detection\nalgorithms applied to filtered (thresholded) networks, have better performances\nin terms of biological relevance of the results, with respect to the use of raw\nunfiltered networks.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 13:18:39 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Cannataro", "Mario", ""], ["Guzzi", "Pietro Hiram", ""], ["Milano", "Marianna", ""], ["Veltri", "Pierangelo", ""]]}, {"id": "1412.7811", "submitter": "Roshan Ragel", "authors": "S. M. Vidanagamachchi, S.D. Dewasurendra, R. G. Ragel and M. Niranjan", "title": "A Structured Hardware Software Architecture for Peptide Based Diagnosis\n  of Baylisascaris Procyonis Infection (ICIAfS14)", "comments": "appears in The 7th International Conference on Information and\n  Automation for Sustainability (ICIAfS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inferring proteins from complex peptide cocktails (digestion\nproducts of biological samples) in shotgun proteomic workflow sets extreme\ndemands on computational resources in respect of the required very high\nprocessing throughputs, rapid processing rates and reliability of results. This\nis exacerbated by the fact that, in general, a given protein cannot be defined\nby a fixed sequence of amino acids due to the existence of splice variants and\nisoforms of that protein. Therefore, the problem of protein inference could be\nconsidered as one of identifying sequences of amino acids with some limited\ntolerance. In the current paper a model-based hardware acceleration of a\nstructured and practical inference approach is developed and validated on a\nmass spectrometry experiment of realistic size. We have achieved 10 times\nmaximum speed-up in the co-designed workflow compared to a similar\nsoftware-only workflow run on the processor used for co-design.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 11:33:59 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Vidanagamachchi", "S. M.", ""], ["Dewasurendra", "S. D.", ""], ["Ragel", "R. G.", ""], ["Niranjan", "M.", ""]]}, {"id": "1412.7929", "submitter": "Kyeong Soo (Joseph) Kim", "authors": "Kyeong Soo Kim", "title": "Designing pricing schemes based on progressive tariff and consumer\n  grouping in migration to a future smart grid", "comments": "4 pages, 3 figures, to be presented at International Conference on\n  Information and Convergence Technology for Smart Society (ICICTS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design of pricing schemes for a group of consumers with smart\nmeters (e.g., in a Greenfield area) who are connected through a gateway to a\ntraditional electricity greed with a progressive tariff. Because the\nprogressive tariff cannot take into account the time aspect of electricity\ndemands, we apply it to consumers in both an individual and a group basis over\na shorter time period, which can flatten the overall demand over time and\nthereby reduce peak load. This scenario for the coexistence of traditional and\nsmart girds and the pricing schemes under this scenario can enable smooth\nmigration to a future smart grid.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 11:50:51 GMT"}, {"version": "v2", "created": "Tue, 30 Dec 2014 11:32:32 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Kim", "Kyeong Soo", ""]]}, {"id": "1412.8054", "submitter": "Jakub Mare\\v{c}ek", "authors": "Jakub Marecek, Timothy McCoy, Martin Mevissen", "title": "Power Flow as an Algebraic System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steady states of alternating-current (AC) circuits have been studied in\nconsiderable detail. In 1982, Baillieul and Byrnes derived an upper bound on\nthe number of steady states in a loss-less AC circuit [IEEE TCAS, 29(11):\n724--737] and conjectured that this bound holds for AC circuits in general. We\nprove this is indeed the case, among other results, by studying a certain\nmulti-homogeneous structure in an algebraisation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Dec 2014 14:59:17 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 11:58:45 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Marecek", "Jakub", ""], ["McCoy", "Timothy", ""], ["Mevissen", "Martin", ""]]}, {"id": "1412.8093", "submitter": "Asish  Mukhopadhyay", "authors": "Kaushik Roy, Satish Ch. Panigrahi and Asish Mukhopadhyay", "title": "Multiple alignment of structures using center of proteins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we report on an algorithm for aligning multiple protein\nstructures. The algorithm has been tested on a variety of inputs and it\nperforms well in comparison to well-known algorithms for this problem.\n", "versions": [{"version": "v1", "created": "Sun, 28 Dec 2014 00:52:53 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Roy", "Kaushik", ""], ["Panigrahi", "Satish Ch.", ""], ["Mukhopadhyay", "Asish", ""]]}, {"id": "1412.8467", "submitter": "Roshan Ragel", "authors": "S.M.Vidanagamachchi, S.D. Dewasurendra, R.G. Ragel and M. Niranjan", "title": "A Structured Hardware Software Architecture for Peptide Based Diagnosis\n  - Sub-string Matching Problem with Limited Tolerance (ICIAfS14)", "comments": "appears in The 7th International Conference on Information and\n  Automation for Sustainability (ICIAfS) 2014. arXiv admin note: substantial\n  text overlap with arXiv:1412.7811", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of inferring proteins from complex peptide samples in shotgun\nproteomic workflow sets extreme demands on computational resources. This is\nexacerbated by the fact that, in general, a given protein cannot be defined by\na fixed sequence of amino acids due to the existence of splice variants and\nisoforms of that protein. Therefore, the problem of protein inference could be\nconsidered as one of identifying sequences of amino acids with some limited\ntolerance. Two problems arise from this: a) due to these variations, the\napplicability of exact string matching methodologies could be questioned and b)\nthe difficulty of defining a reference sequence for a particular set of\nproteins that are functionally indistinguishable, but with some variation in\nfeatures. This paper presents a model-based inference approach that is\ndeveloped and validated to solve the inference problem. Our approach starts\nfrom an examination of the known set of splice variants and isoforms of a\ntarget protein to identify the Greatest Common Stable Substring (GCSS) of amino\nacids and the Substrings Subjects to Limited Variation (SSLV) and their\nrespective locations on the GCSS. Then we define and solve the Sub-string\nMatching Problem with Limited Tolerance (SMPLT). This approach is validated on\nidentified peptides in a labelled and clustered data set from UNIPROT.\nIdentification of Baylisascaris Procyonis infection was used as an application\ninstance that achieved up to 70 times speedup compared to a software only\nsystem. This workflow can be generalised to any inexact multiple pattern\nmatching application by replacing the patterns in a clustered and distributed\nenvironment which permits a distance between member strings to account for\npermitted deviations such as substitutions, insertions and deletions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Dec 2014 11:44:51 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Vidanagamachchi", "S. M.", ""], ["Dewasurendra", "S. D.", ""], ["Ragel", "R. G.", ""], ["Niranjan", "M.", ""]]}, {"id": "1412.8520", "submitter": "James P. Crutchfield", "authors": "James P. Crutchfield, Ryan G. James, Sarah Marzen, Dowman P. Varn", "title": "Understanding and Designing Complex Systems: Response to \"A framework\n  for optimal high-level descriptions in science and engineering---preliminary\n  report\"", "comments": "6 pages; http://csc.ucdavis.edu/~cmg/compmech/pubs/ssc_comment.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.AI cs.CE cs.IT math.IT nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We recount recent history behind building compact models of nonlinear,\ncomplex processes and identifying their relevant macroscopic patterns or\n\"macrostates\". We give a synopsis of computational mechanics, predictive\nrate-distortion theory, and the role of information measures in monitoring\nmodel complexity and predictive performance. Computational mechanics provides a\nmethod to extract the optimal minimal predictive model for a given process.\nRate-distortion theory provides methods for systematically approximating such\nmodels. We end by commenting on future prospects for developing a general\nframework that automatically discovers optimal compact models. As a response to\nthe manuscript cited in the title above, this brief commentary corrects\npotentially misleading claims about its state space compression method and\nplaces it in a broader historical setting.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 01:11:40 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Crutchfield", "James P.", ""], ["James", "Ryan G.", ""], ["Marzen", "Sarah", ""], ["Varn", "Dowman P.", ""]]}, {"id": "1412.8574", "submitter": "Victoria Popic", "authors": "Victoria Popic, Raheleh Salari, Iman Hajirasouliha, Dorna\n  Kashef-Haghighi, Robert B. West and Serafim Batzoglou", "title": "Fast and Scalable Inference of Multi-Sample Cancer Lineages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Somatic variants can be used as lineage markers for the phylogenetic\nreconstruction of cancer evolution. Since somatic phylogenetics is complicated\nby sample heterogeneity, novel specialized tree-building methods are required\nfor cancer phylogeny reconstruction. We present LICHeE (Lineage Inference for\nCancer Heterogeneity and Evolution), a novel method that automates the\nphylogenetic inference of cancer progression from multiple somatic samples.\nLICHeE uses variant allele frequencies of SSNVs obtained by deep sequencing to\nreconstruct multi-sample cell lineage trees and infer the subclonal composition\nof the samples. LICHeE is open-sourced and available at\nhttp://viq854.github.io/lichee.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 07:37:52 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Popic", "Victoria", ""], ["Salari", "Raheleh", ""], ["Hajirasouliha", "Iman", ""], ["Kashef-Haghighi", "Dorna", ""], ["West", "Robert B.", ""], ["Batzoglou", "Serafim", ""]]}]