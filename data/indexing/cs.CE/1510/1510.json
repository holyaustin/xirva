[{"id": "1510.00002", "submitter": "Francesco Andriulli", "authors": "Rajendra Mitharwal and Francesco P. Andriulli", "title": "A Regularized Boundary Element Formulation for Contactless SAR\n  Evaluations within Homogeneous and Inhomogeneous Head Phantoms", "comments": null, "journal-ref": null, "doi": "10.1016/j.crhy.2015.10.003", "report-no": null, "categories": "physics.med-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a Boundary Element Method (BEM) formulation for\ncontactless electromagnetic field assessments. The new scheme is based on a\nregularized BEM approach that requires the use of electric measurements only.\nThe regularization is obtained by leveraging on an extension of Calderon\ntechniques to rectangular systems leading to well-conditioned problems\nindependent of the discretization density. This enables the use of highly\ndiscretized Huygens surfaces that can be consequently placed very near to the\nradiating source. In addition, the new regularized scheme is hybridized with\nboth surfacic homogeneous and volumetric inhomogeneous forward BEM solvers\naccelerated with fast matrix-vector multiplication schemes. This allows for\nrapid and effective dosimetric assessments and permits the use of inhomogeneous\nand realistic head phantoms. Numerical results corroborate the theory and\nconfirms the practical effectiveness of all newly proposed formulations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 12:11:52 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Mitharwal", "Rajendra", ""], ["Andriulli", "Francesco P.", ""]]}, {"id": "1510.00073", "submitter": "Dhagash Mehta", "authors": "Dhagash Mehta, Daniel K Molzahn, Konstantin Turitsyn", "title": "Recent Advances in Computational Methods for the Power Flow Equations", "comments": "13 pages, 2 figures. Submitted to the Tutorial Session at IEEE 2016\n  American Control Conference", "journal-ref": null, "doi": "10.1109/ACC.2016.7525170", "report-no": "ADP-15-35/T937", "categories": "math.OC cs.CE cs.SY math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power flow equations are at the core of most of the computations for\ndesigning and operating electric power systems. The power flow equations are a\nsystem of multivariate nonlinear equations which relate the power injections\nand voltages in a power system. A plethora of methods have been devised to\nsolve these equations, starting from Newton-based methods to homotopy\ncontinuation and other optimization-based methods. While many of these methods\noften efficiently find a high-voltage, stable solution due to its large basin\nof attraction, most of the methods struggle to find low-voltage solutions which\nplay significant role in certain stability-related computations. While we do\nnot claim to have exhausted the existing literature on all related methods,\nthis tutorial paper introduces some of the recent advances in methods for\nsolving power flow equations to the wider power systems community as well as\nbringing attention from the computational mathematics and optimization\ncommunities to the power systems problems. After briefly reviewing some of the\ntraditional computational methods used to solve the power flow equations, we\nfocus on three emerging methods: the numerical polynomial homotopy continuation\nmethod, Groebner basis techniques, and moment/sum-of-squares relaxations using\nsemidefinite programming. In passing, we also emphasize the importance of an\nupper bound on the number of solutions of the power flow equations and review\nthe current status of research in this direction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 23:29:18 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Mehta", "Dhagash", ""], ["Molzahn", "Daniel K", ""], ["Turitsyn", "Konstantin", ""]]}, {"id": "1510.01364", "submitter": "Pierre Horgue", "authors": "Pierre Horgue (IMFT), Jacques Franc (IMFT), Romain Guibert (IMFT),\n  G\\'erald Debenest (IMFT)", "title": "An extension of the open-source porousMultiphaseFoam toolbox dedicated\n  to groundwater flows solving the Richards' equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, the existing porousMultiphaseFoam toolbox, developed initially\nfor any two-phase flow in porous media is extended to the specific case of the\nRichards' equation which neglect the pressure gradient of the non-wetting\nphase. This model is typically used for saturated and unsaturated groundwater\nflows. A Picard's algorithm is implemented to linearize and solve the Richards'\nequation developed in the pressure head based form. This new solver of the\nporousMultiphaseFoam toolbox is named groundwaterFoam. The validation of\nthesolver is achieved by a comparison between numerical simulations and results\nobtained from the literature. Finally, a parallel efficiency test is performed\non a large unstructured mesh and exhibits a super-linear behavior as observed\nfor the other solvers of the toolbox.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 12:34:41 GMT"}], "update_date": "2015-10-07", "authors_parsed": [["Horgue", "Pierre", "", "IMFT"], ["Franc", "Jacques", "", "IMFT"], ["Guibert", "Romain", "", "IMFT"], ["Debenest", "G\u00e9rald", "", "IMFT"]]}, {"id": "1510.02163", "submitter": "Vahid Noormofidi", "authors": "Vahid Noormofidi, Susan R. Atlas, Huaiyu Duan", "title": "Performance Analysis of an Astrophysical Simulation Code on the Intel\n  Xeon Phi Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": "CARC-2015-174", "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed the astrophysical simulation code XFLAT to study neutrino\noscillations in supernovae. XFLAT is designed to utilize multiple levels of\nparallelism through MPI, OpenMP, and SIMD instructions (vectorization). It can\nrun on both CPU and Xeon Phi co-processors based on the Intel Many Integrated\nCore Architecture (MIC). We analyze the performance of XFLAT on configurations\nwith CPU only, Xeon Phi only and both CPU and Xeon Phi. We also investigate the\nimpact of I/O and the multi-node performance of XFLAT on the Xeon Phi-equipped\nStampede supercomputer at the Texas Advanced Computing Center (TACC).\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2015 23:02:00 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Noormofidi", "Vahid", ""], ["Atlas", "Susan R.", ""], ["Duan", "Huaiyu", ""]]}, {"id": "1510.02237", "submitter": "Daniel Ruprecht", "authors": "Daniel Ruprecht, Rolf Krause", "title": "Explicit Parallel-in-time Integration of a Linear Acoustic-Advection\n  System", "comments": null, "journal-ref": "Computers & Fluids 59, pp. 72-83, 2012", "doi": "10.1016/j.compfluid.2012.02.015", "report-no": null, "categories": "cs.CE cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The applicability of the Parareal parallel-in-time integration scheme for the\nsolution of a linear, two-dimensional hyperbolic acoustic-advection system,\nwhich is often used as a test case for integration schemes for numerical\nweather prediction (NWP), is addressed. Parallel-in-time schemes are a possible\nway to increase, on the algorithmic level, the amount of parallelism, a\nrequirement arising from the rapidly growing number of CPUs in high performance\ncomputer systems. A recently introduced modification of the \"parallel implicit\ntime-integration algorithm\" could successfully solve hyperbolic problems\narising in structural dynamics. It has later been cast into the framework of\nParareal. The present paper adapts this modified Parareal and employs it for\nthe solution of a hyperbolic flow problem, where the initial value problem\nsolved in parallel arises from the spatial discretization of a partial\ndifferential equation by a finite difference method. It is demonstrated that\nthe modified Parareal is stable and can produce reasonably accurate solutions\nwhile allowing for a noticeable reduction of the time-to-solution. The\nimplementation relies on integration schemes already widely used in NWP (RK-3,\npartially split forward Euler, forward-backward). It is demonstrated that using\nan explicit partially split scheme for the coarse integrator allows to avoid\nthe use of an implicit scheme while still achieving speedup.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2015 08:47:17 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Ruprecht", "Daniel", ""], ["Krause", "Rolf", ""]]}, {"id": "1510.02719", "submitter": "Fehmi Cirak", "authors": "Kosala Bandara and Thomas R\\\"uberg and Fehmi Cirak", "title": "Shape optimisation with multiresolution subdivision surfaces and\n  immersed finite elements", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2015.11.015", "report-no": null, "categories": "math.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new optimisation technique that combines multiresolution\nsubdivision surfaces for boundary description with immersed finite elements for\nthe discretisation of the primal and adjoint problems of optimisation. Similar\nto wavelets multiresolution surfaces represent the domain boundary using a\ncoarse control mesh and a sequence of detail vectors. Based on the\nmultiresolution decomposition efficient and fast algorithms are available for\nreconstructing control meshes of varying fineness. During shape optimisation\nthe vertex coordinates of control meshes are updated using the computed shape\ngradient information. By virtue of the multiresolution editing semantics,\nupdating the coarse control mesh vertex coordinates leads to large-scale\ngeometry changes and, conversely, updating the fine control mesh coordinates\nleads to small-scale geometry changes. In our computations we start by\noptimising the coarsest control mesh and refine it each time the cost function\nreaches a minimum. This approach effectively prevents the appearance of\nnon-physical boundary geometry oscillations and control mesh pathologies, like\ninverted elements. Independent of the fineness of the control mesh used for\noptimisation, on the immersed finite element grid the domain boundary is always\nrepresented with a relatively fine control mesh of fixed resolution. With the\nimmersed finite element method there is no need to maintain an analysis\nsuitable domain mesh. In some of the presented two- and three-dimensional\nelasticity examples the topology derivative is used for creating new holes\ninside the domain.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 16:04:45 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Bandara", "Kosala", ""], ["R\u00fcberg", "Thomas", ""], ["Cirak", "Fehmi", ""]]}, {"id": "1510.02775", "submitter": "Mahmood A. Rashid", "authors": "Mahmood A. Rashid, Firas Khatib, and Abdul Sattar", "title": "Protein preliminaries and structure prediction fundamentals for computer\n  scientists", "comments": "23 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein structure prediction is a challenging and unsolved problem in\ncomputer science. Proteins are the sequence of amino acids connected together\nby single peptide bond. The combinations of the twenty primary amino acids are\nthe constituents of all proteins. In-vitro laboratory methods used in this\nproblem are very time-consuming, cost-intensive, and failure-prone. Thus,\nalternative computational methods come into play. The protein structure\nprediction problem is to find the three-dimensional native structure of a\nprotein, from its amino acid sequence. The native structure of a protein has\nthe minimum free energy possible and arguably determines the function of the\nprotein. In this study, we present the preliminaries of proteins and their\nstructures, protein structure prediction problem, and protein models. We also\ngive a brief overview on experimental and computational methods used in protein\nstructure prediction. This study will provide a fundamental knowledge to the\ncomputer scientists who are intending to pursue their future research on\nprotein structure prediction problem.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2015 19:17:12 GMT"}], "update_date": "2015-10-12", "authors_parsed": [["Rashid", "Mahmood A.", ""], ["Khatib", "Firas", ""], ["Sattar", "Abdul", ""]]}, {"id": "1510.03035", "submitter": "Maria Tirronen", "authors": "Maria Tirronen", "title": "Reliability Analysis of Processes with Moving Cracked Material", "comments": null, "journal-ref": "Applied Mathematical Modelling 40 (2016) 4986-4999", "doi": "10.1016/j.apm.2015.12.010", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliability of processes with moving elastic and isotropic material\ncontaining initial cracks is considered in terms of fracture. The material is\nmodelled as a moving plate which is simply supported from two of its sides and\nsubjected to homogeneous tension acting in the travelling direction. For\ntension, two models are studied: i) tension is constant with respect to time,\nand ii) tension varies temporally according to an Ornstein-Uhlenbeck process.\nCracks of random length are assumed to occur in the material according to a\nstochastic counting process. For a general counting process, a representation\nof the nonfracture probability of the system is obtained that exploits\nconditional Monte Carlo simulation. Explicit formulae are derived for special\ncases. To study the reliability of the system with temporally varying tension,\na known explicit result for the first passage time of an Ornstein-Uhlenbeck\nprocess to a constant boundary is utilized. Numerical examples are provided for\nprinting presses and paper material.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 11:01:30 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Tirronen", "Maria", ""]]}, {"id": "1510.03102", "submitter": "Roberto M\\'inguez", "authors": "R. Garc\\'ia-Bertrand and R. M\\'inguez", "title": "Dynamic Robust Transmission Expansion Planning", "comments": "10 pages, 2 figures. This article has been accepted for publication\n  in a future issue of this journal, but has not been fully edited. Content may\n  change prior to final publication. Citation information: DOI\n  10.1109/TPWRS.2016.2629266, IEEE Transactions on Power Systems 2016", "journal-ref": "IEEE Transactions on Power Systems 2016", "doi": "10.1109/TPWRS.2016.2629266", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent breakthroughs in Transmission Network Expansion Planning (TNEP) have\ndemonstrated that the use of robust optimization, as opposed to stochastic\nprogramming methods, renders the expansion planning problem considering\nuncertainties computationally tractable for real systems. However, there is\nstill a yet unresolved and challenging problem as regards the resolution of the\ndynamic TNEP problem (DTNEP), which considers the year-by-year representation\nof uncertainties and investment decisions in an integrated way. This problem\nhas been considered to be a highly complex and computationally intractable\nproblem, and most research related to this topic focuses on very small case\nstudies or used heuristic methods and has lead most studies about TNEP in the\ntechnical literature to take a wide spectrum of simplifying assumptions. In\nthis paper an adaptive robust transmission network expansion planning\nformulation is proposed for keeping the full dynamic complexity of the problem.\nThe method overcomes the problem size limitations and computational\nintractability associated with dynamic TNEP for realistic cases. Numerical\nresults from an illustrative example and the IEEE 118-bus system are presented\nand discussed, demonstrating the benefits of this dynamic TNEP approach with\nrespect to classical methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2015 21:04:27 GMT"}, {"version": "v2", "created": "Sun, 15 May 2016 14:24:36 GMT"}, {"version": "v3", "created": "Mon, 30 Jan 2017 10:43:56 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Garc\u00eda-Bertrand", "R.", ""], ["M\u00ednguez", "R.", ""]]}, {"id": "1510.04583", "submitter": "Shahin Mohammadi", "authors": "Shahin Mohammadi, Neta Zuckerman, Andrea Goldsmith, and Ananth Grama", "title": "A Critical Survey of Deconvolution Methods for Separating cell-types in\n  Complex Tissues", "comments": "Submitted to the Special Issue of Proceedings of IEEE - Foundations&\n  Applications of Science of Information", "journal-ref": null, "doi": "10.1109/JPROC.2016.2607121", "report-no": null, "categories": "cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying concentrations of components from an observed mixture is a\nfundamental problem in signal processing. It has diverse applications in fields\nranging from hyperspectral imaging to denoising biomedical sensors. This paper\nfocuses on in-silico deconvolution of signals associated with complex tissues\ninto their constitutive cell-type specific components, along with a\nquantitative characterization of the cell-types. Deconvolving mixed\ntissues/cell-types is useful in the removal of contaminants (e.g., surrounding\ncells) from tumor biopsies, as well as in monitoring changes in the cell\npopulation in response to treatment or infection. In these contexts, the\nobserved signal from the mixture of cell-types is assumed to be a linear\ncombination of the expression levels of genes in constitutive cell-types. The\ngoal is to use known signals corresponding to individual cell-types along with\na model of the mixing process to cast the deconvolution problem as a suitable\noptimization problem.\n  In this paper, we present a survey of models, methods, and assumptions\nunderlying deconvolution techniques. We investigate the choice of the different\nloss functions for evaluating estimation error, constraints on solutions,\npreprocessing and data filtering, feature selection, and regularization to\nenhance the quality of solutions, along with the impact of these choices on the\nperformance of regression-based methods for deconvolution. We assess different\ncombinations of these factors and use detailed statistical measures to evaluate\ntheir effectiveness. We identify shortcomings of current methods and avenues\nfor further investigation. For many of the identified shortcomings, such as\nnormalization issues and data filtering, we provide new solutions. We summarize\nour findings in a prescriptive step-by-step process, which can be applied to a\nwide range of deconvolution problems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 15:28:02 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Mohammadi", "Shahin", ""], ["Zuckerman", "Neta", ""], ["Goldsmith", "Andrea", ""], ["Grama", "Ananth", ""]]}, {"id": "1510.04645", "submitter": "Henrik Ronellenfitsch", "authors": "Henrik Ronellenfitsch, Marc Timme, Dirk Witthaut", "title": "A Dual Method for Computing Power Transfer Distribution Factors", "comments": "9 pages, 6 figures; IEEE Transactions on Power Systems 2016\n  (Volume:PP, Issue: 99)", "journal-ref": null, "doi": "10.1109/TPWRS.2016.2589464", "report-no": null, "categories": "cs.SY cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power Transfer Distribution Factors (PTDFs) play a crucial role in power grid\nsecurity analysis, planning, and redispatch. Fast calculation of the PTDFs is\ntherefore of great importance. In this paper, we present a non-approximative\ndual method of computing PTDFs. It uses power flows along topological cycles of\nthe network but still relies on simple matrix algebra. At the core, our method\nchanges the size of the matrix that needs to be inverted to calculate the PTDFs\nfrom $N\\times N$, where $N$ is the number of buses, to $(L-N+1)\\times (L-N+1)$,\nwhere $L$ is the number of lines and $L-N+1$ is the number of independent\ncycles (closed loops) in the network while remaining mathematically fully\nequivalent. For power grids containing a relatively small number of cycles, the\nmethod can offer a speedup of numerical calculations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2015 17:51:06 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 09:38:10 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2016 15:28:20 GMT"}, {"version": "v4", "created": "Mon, 25 Jul 2016 22:49:01 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Ronellenfitsch", "Henrik", ""], ["Timme", "Marc", ""], ["Witthaut", "Dirk", ""]]}, {"id": "1510.05193", "submitter": "Sergio Angel Almada Monter", "authors": "Sergio A. Almada Monter, Amarjit Budhiraja, and Jan Hannig", "title": "Source detection algorithms for dynamic contaminants based on the\n  analysis of a hydrodynamic limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CE cs.MA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose and numerically analyze an algorithm for detection of\na contaminant source using a dynamic sensor network. The algorithm is motivated\nusing a global probabilistic optimization problem and is based on the analysis\nof the hydrodynamic limit of a discrete time evolution equation on the lattice\nunder a suitable scaling of time and space. Numerical results illustrating the\neffectiveness of the algorithm are presented.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 02:34:35 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2015 01:36:34 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Monter", "Sergio A. Almada", ""], ["Budhiraja", "Amarjit", ""], ["Hannig", "Jan", ""]]}, {"id": "1510.05218", "submitter": "Tareq Malas", "authors": "Tareq M. Malas, Julian Hornich, Georg Hager, Hatem Ltaief, Christoph\n  Pflaum and David E. Keyes", "title": "Optimization of an electromagnetics code with multicore wavefront\n  diamond blocking and multi-dimensional intra-tile parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and optimizing the properties of solar cells is becoming a key\nissue in the search for alternatives to nuclear and fossil energy sources. A\ntheoretical analysis via numerical simulations involves solving Maxwell's\nEquations in discretized form and typically requires substantial computing\neffort. We start from a hybrid-parallel (MPI+OpenMP) production code that\nimplements the Time Harmonic Inverse Iteration Method (THIIM) with\nFinite-Difference Frequency Domain (FDFD) discretization. Although this\nalgorithm has the characteristics of a strongly bandwidth-bound stencil update\nscheme, it is significantly different from the popular stencil types that have\nbeen exhaustively studied in the high performance computing literature to date.\nWe apply a recently developed stencil optimization technique, multicore\nwavefront diamond tiling with multi-dimensional cache block sharing, and\ndescribe in detail the peculiarities that need to be considered due to the\nspecial stencil structure. Concurrency in updating the components of the\nelectric and magnetic fields provides an additional level of parallelism. The\ndependence of the cache size requirement of the optimized code on the blocking\nparameters is modeled accurately, and an auto-tuner searches for optimal\nconfigurations in the remaining parameter space. We were able to completely\ndecouple the execution from the memory bandwidth bottleneck, accelerating the\nimplementation by a factor of three to four compared to an optimal\nimplementation with pure spatial blocking on an 18-core Intel Haswell CPU.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2015 10:13:47 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Malas", "Tareq M.", ""], ["Hornich", "Julian", ""], ["Hager", "Georg", ""], ["Ltaief", "Hatem", ""], ["Pflaum", "Christoph", ""], ["Keyes", "David E.", ""]]}, {"id": "1510.05533", "submitter": "Zahra Karimaddini", "authors": "Dagmar Iber, Zahra Karimaddini, Erkan \\\"Unal", "title": "Image-based Modelling of Organogenesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in biology concerns the integration of data\nacross length and time scales into a consistent framework: how do macroscopic\nproperties and functionalities arise from the molecular regulatory networks -\nand how can they change as a result of mutations? Morphogenesis provides an\nexcellent model system to study how simple molecular networks robustly control\ncomplex processes on the macroscopic scale in spite of molecular noise, and how\nimportant functional variants can emerge from small genetic changes. Recent\nadvancements in 3D imaging technologies, computer algorithms, and computer\npower now allow us to develop and analyse increasingly realistic models of\nbiological control. Here we present our pipeline for image-based modeling that\nincludes the segmentation of images, the determination of displacement fields,\nand the solution of systems of partial differential equations (PDEs) on the\ngrowing, embryonic domains. The development of suitable mathematical models,\nthe data-based inference of parameter sets, and the evaluation of competing\nmodels are still challenging, and current approaches are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 15:31:58 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Iber", "Dagmar", ""], ["Karimaddini", "Zahra", ""], ["\u00dcnal", "Erkan", ""]]}, {"id": "1510.05682", "submitter": "Jianzhu Ma", "authors": "Jianzhu Ma", "title": "Protein Structure Prediction by Protein Alignments", "comments": "arXiv admin note: substantial text overlap with arXiv:1306.4420 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proteins are the basic building blocks of life. They usually perform\nfunctions by folding to a particular structure. Understanding the folding\nprocess could help the researchers to understand the functions of proteins and\ncould also help to develop supplemental proteins for people with deficiencies\nand gain more insight into diseases associated with troublesome folding\nproteins. Experimental methods are both expensive and time consuming. In this\nthesis I introduce a new machine learning based method to predict the protein\nstructure. The new method improves the performance from two directions:\ncreating accurate protein alignments and predicting accurate protein contacts.\nFirst, I present an alignment framework MRFalign which goes beyond\nstate-of-the-art methods and uses Markov Random Fields to model a protein\nfamily and align two proteins by aligning two MRFs together. Compared to other\nmethods, that can only model local-range residue correlation, MRFs can model\nlong-range residue interactions and thus, encodes global information in a\nprotein. Secondly, I present a Group Graphical Lasso method for contact\nprediction that integrates joint multi-family Evolutionary Coupling analysis\nand supervised learning to improve accuracy on proteins without many sequence\nhomologs. Different from single-family EC analysis that uses residue\nco-evolution information in only the target protein family, our joint EC\nanalysis uses residue co-evolution in both the target family and its related\nfamilies, which may have divergent sequences but similar folds. Our method can\nalso integrate supervised learning methods to further improve accuracy. We\nevaluate the performance of both methods including each of its components on\nlarge public benchmarks. Experiments show that our methods can achieve better\naccuracy than existing state-of-the-art methods under all the measurements on\nmost of the protein classes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2015 20:46:04 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Ma", "Jianzhu", ""]]}, {"id": "1510.05751", "submitter": "Debojyoti Ghosh", "authors": "Debojyoti Ghosh and Emil M. Constantinescu", "title": "Semi-Implicit Time Integration of Atmospheric Flows with\n  Characteristic-Based Flux Partitioning", "comments": null, "journal-ref": "SIAM Journal on Scientific Computing, 38 (3), 2016", "doi": "10.1137/15M1044369", "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a characteristic-based flux partitioning for the\nsemi-implicit time integration of atmospheric flows. Nonhydrostatic models\nrequire the solution of the compressible Euler equations. The acoustic\ntime-scale is significantly faster than the advective scale, yet it is\ntypically not relevant to atmospheric and weather phenomena. The acoustic and\nadvective components of the hyperbolic flux are separated in the characteristic\nspace. High-order, conservative additive Runge-Kutta methods are applied to the\npartitioned equations so that the acoustic component is integrated in time\nimplicitly with an unconditionally stable method, while the advective component\nis integrated explicitly. The time step of the overall algorithm is thus\ndetermined by the advective scale. Benchmark flow problems are used to\ndemonstrate the accuracy, stability, and convergence of the proposed algorithm.\nThe computational cost of the partitioned semi-implicit approach is compared\nwith that of explicit time integration.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2015 04:36:38 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 18:21:50 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Ghosh", "Debojyoti", ""], ["Constantinescu", "Emil M.", ""]]}, {"id": "1510.06482", "submitter": "Shahin Mohammadi", "authors": "Shahin Mohammadi, David Gleich, Tamara Kolda, Ananth Grama", "title": "Triangular Alignment (TAME): A Tensor-based Approach for Higher-order\n  Network Alignment", "comments": "Submitted to the IEEE/ACM Transactions on Computational Biology and\n  Bioinformatics (TCBB) for review", "journal-ref": "IEEE/ACM Transactions on Computational Biology and Bioinformatics,\n  Vol. 14, No. 6, pp. 1446-1458, December 2017", "doi": "10.1109/tcbb.2016.2595583", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network alignment has extensive applications in comparative interactomics.\nTraditional approaches aim to simultaneously maximize the number of conserved\nedges and the underlying similarity of aligned entities. We propose a novel\nformulation of the network alignment problem that extends topological\nsimilarity to higher-order structures and provides a new objective function\nthat maximizes the number of aligned substructures. This objective function\ncorresponds to an integer programming problem, which is NP-hard. Consequently,\nwe identify a closely related surrogate function whose maximization results in\na tensor eigenvector problem. Based on this formulation, we present an\nalgorithm called Triangular AlignMEnt (TAME), which attempts to maximize the\nnumber of aligned triangles across networks. Using a case study on the\nNAPAbench dataset, we show that triangular alignment is capable of producing\nmappings with high node correctness. We further evaluate our method by aligning\nyeast and human interactomes. Our results indicate that TAME outperforms the\nstate-of-art alignment methods in terms of conserved triangles. In addition, we\nshow that the number of conserved triangles is more significantly correlated,\ncompared to the conserved edge, with node correctness and co-expression of\nedges. Our formulation and resulting algorithms can be easily extended to\narbitrary motifs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 02:55:53 GMT"}, {"version": "v2", "created": "Sun, 17 Jul 2016 18:32:29 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Mohammadi", "Shahin", ""], ["Gleich", "David", ""], ["Kolda", "Tamara", ""], ["Grama", "Ananth", ""]]}, {"id": "1510.07020", "submitter": "Kamal Premaratne", "authors": "Yoangel Torres, Kamal Premaratne, Falk Amelung, Shimon Wdowinski", "title": "An Efficient Polyphase Filter Based Resampling Method for Unifying the\n  PRFs in SAR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable and higher pulse repetition frequencies (PRFs) are increasingly\nbeing used to meet the stricter requirements and complexities of current\nairborne and spaceborne synthetic aperture radar (SAR) systems associated with\nhigher resolution and wider area products. POLYPHASE, the proposed resampling\nscheme, downsamples and unifies variable PRFs within a single look complex\n(SLC) SAR acquisition and across a repeat pass sequence of acquisitions down to\nan effective lower PRF. A sparsity condition of the received SAR data ensures\nthat the uniformly resampled data approximates the spectral properties of a\ndecimated densely sampled version of the received SAR data. While experiments\nconducted with both synthetically generated and real airborne SAR data show\nthat POLYPHASE retains comparable performance to the state-of-the-art BLUI\nscheme in image quality, a polyphase filter-based implementation of POLYPHASE\noffers significant computational savings for arbitrary (not necessarily\nperiodic) input PRF variations, thus allowing fully on-board, in-place, and\nreal-time implementation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2015 12:54:40 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 13:48:23 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Torres", "Yoangel", ""], ["Premaratne", "Kamal", ""], ["Amelung", "Falk", ""], ["Wdowinski", "Shimon", ""]]}, {"id": "1510.07234", "submitter": "Remus Brad", "authors": "Raluca Brad, Eugen H\\u{A}loiu, Remus Brad", "title": "Seam Puckering Objective Evaluation Method for Sewing Process", "comments": null, "journal-ref": "Annals of the University of Oradea, volume XV, no.1, pp.23-28,\n  2014, ISSN 1843-813X", "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents an automated method for the assessment and classification\nof puckering defects detected during the preproduction control stage of the\nsewing machine or product inspection. In this respect, we have presented the\npossible causes and remedies of the wrinkle nonconformities. Subjective factors\nrelated to the control environment and operators during the seams evaluation\ncan be reduced using an automated system whose operation is based on image\nprocessing. Our implementation involves spectral image analysis using Fourier\ntransform and an unsupervised neural network, the Kohonen Map, employed to\nclassify material specimens, the input images, into five discrete degrees of\nquality, from grade 5 (best) to grade 1 (the worst).\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2015 11:37:07 GMT"}], "update_date": "2015-10-27", "authors_parsed": [["Brad", "Raluca", ""], ["H\u0102loiu", "Eugen", ""], ["Brad", "Remus", ""]]}, {"id": "1510.07863", "submitter": "Janine Mergel", "authors": "Janine C. Mergel, Roger A. Sauer and Sina Ober-Bl\\\"obaum", "title": "C1-continuous space-time discretization based on Hamilton's law of\n  varying action", "comments": "slightly condensed the manuscript, added references, numerical\n  results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a class of C1-continuous time integration methods that are\napplicable to conservative problems in elastodynamics. These methods are based\non Hamilton's law of varying action. From the action of the continuous system\nwe derive a spatially and temporally weak form of the governing equilibrium\nequations. This expression is first discretized in space, considering standard\nfinite elements. The resulting system is then discretized in time,\napproximating the displacement by piecewise cubic Hermite shape functions.\nWithin the time domain we thus achieve C1-continuity for the displacement field\nand C0-continuity for the velocity field. From the discrete virtual action we\nfinally construct a class of one-step schemes. These methods are examined both\nanalytically and numerically. Here, we study both linear and nonlinear systems\nas well as inherently continuous and discrete structures. In the numerical\nexamples we focus on one-dimensional applications. The provided theory,\nhowever, is general and valid also for problems in 2D or 3D. We show that the\nmost favorable candidate -- denoted as p2-scheme -- converges with order four.\nThus, especially if high accuracy of the numerical solution is required, this\nscheme can be more efficient than methods of lower order. It further exhibits,\nfor linear simple problems, properties similar to variational integrators, such\nas symplecticity. While it remains to be investigated whether symplecticity\nholds for arbitrary systems, all our numerical results show an excellent\nlong-term energy behavior.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2015 11:12:58 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 13:04:26 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Mergel", "Janine C.", ""], ["Sauer", "Roger A.", ""], ["Ober-Bl\u00f6baum", "Sina", ""]]}, {"id": "1510.08237", "submitter": "Salvatore Alaimo Ph.D.", "authors": "Salvatore Alaimo, Rosalba Giugno, Mario Acunzo, Dario Veneziano,\n  Alfredo Ferro, and Alfredo Pulvirenti", "title": "Post-transcriptional knowledge in pathway analysis increases the\n  accuracy of phenotypes classification", "comments": null, "journal-ref": null, "doi": "10.18632/oncotarget.9788", "report-no": null, "categories": "q-bio.MN cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Motivation: Prediction of phenotypes from high-dimensional data is a crucial\ntask in precision biology and medicine. Many technologies employ genomic\nbiomarkers to characterize phenotypes. However, such elements are not\nsufficient to explain the underlying biology. To improve this, pathway analysis\ntechniques have been proposed. Nevertheless, such methods have shown lack of\naccuracy in phenotypes classification. Results: Here we propose a novel\nmethodology called MITHrIL (Mirna enrIched paTHway Impact anaLysis) for the\nanalysis of signaling pathways, which has built on top of the work of Tarca et\nal., 2009. MITHrIL extends pathways by adding missing regulatory elements, such\nas microRNAs, and their interactions with genes. The method takes as input the\nexpression values of genes and/or microRNAs and returns a list of pathways\nsorted according to their deregulation degree, together with the corresponding\nstatistical significance (p-values). Our analysis shows that MITHrIL\noutperforms its competitors even in the worst case. In addition, our method is\nable to correctly classify sets of tumor samples drawn from TCGA. Availability:\nMITHrIL is freely available at the following URL:\nhttp://alpha.dmi.unict.it/mithril/\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2015 09:33:43 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 20:05:43 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Alaimo", "Salvatore", ""], ["Giugno", "Rosalba", ""], ["Acunzo", "Mario", ""], ["Veneziano", "Dario", ""], ["Ferro", "Alfredo", ""], ["Pulvirenti", "Alfredo", ""]]}, {"id": "1510.08545", "submitter": "Salman Habib", "authors": "Salman Habib, Robert Roser, Tom LeCompte, Zach Marshall, Anders\n  Borgland, Brett Viren, Peter Nugent, Makoto Asai, Lothar Bauerdick, Hal\n  Finkel, Steve Gottlieb, Stefan Hoeche, Paul Sheldon, Jean-Luc Vay, Peter\n  Elmer, Michael Kirby, Simon Patton, Maxim Potekhin, Brian Yanny, Paolo\n  Calafiura, Eli Dart, Oliver Gutsche, Taku Izubuchi, Adam Lyon, Don Petravick", "title": "High Energy Physics Forum for Computational Excellence: Working Group\n  Reports (I. Applications Software II. Software Libraries and Tools III.\n  Systems)", "comments": "72 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.DC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing plays an essential role in all aspects of high energy physics. As\ncomputational technology evolves rapidly in new directions, and data throughput\nand volume continue to follow a steep trend-line, it is important for the HEP\ncommunity to develop an effective response to a series of expected challenges.\nIn order to help shape the desired response, the HEP Forum for Computational\nExcellence (HEP-FCE) initiated a roadmap planning activity with two key\noverlapping drivers -- 1) software effectiveness, and 2) infrastructure and\nexpertise advancement. The HEP-FCE formed three working groups, 1) Applications\nSoftware, 2) Software Libraries and Tools, and 3) Systems (including systems\nsoftware), to provide an overview of the current status of HEP computing and to\npresent findings and opportunities for the desired HEP computational roadmap.\nThe final versions of the reports are combined in this document, and are\npresented along with introductory material.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 02:15:05 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Habib", "Salman", ""], ["Roser", "Robert", ""], ["LeCompte", "Tom", ""], ["Marshall", "Zach", ""], ["Borgland", "Anders", ""], ["Viren", "Brett", ""], ["Nugent", "Peter", ""], ["Asai", "Makoto", ""], ["Bauerdick", "Lothar", ""], ["Finkel", "Hal", ""], ["Gottlieb", "Steve", ""], ["Hoeche", "Stefan", ""], ["Sheldon", "Paul", ""], ["Vay", "Jean-Luc", ""], ["Elmer", "Peter", ""], ["Kirby", "Michael", ""], ["Patton", "Simon", ""], ["Potekhin", "Maxim", ""], ["Yanny", "Brian", ""], ["Calafiura", "Paolo", ""], ["Dart", "Eli", ""], ["Gutsche", "Oliver", ""], ["Izubuchi", "Taku", ""], ["Lyon", "Adam", ""], ["Petravick", "Don", ""]]}, {"id": "1510.08789", "submitter": "Travis Johnston", "authors": "Travis Johnston, Boyu Zhang, Adam Liwo, Silvia Crivelli, Michela\n  Taufer", "title": "In-Situ Data Analysis of Protein Folding Trajectories", "comments": "40 pages, 15 figures, this paper is presently in the format request\n  of the journal to which it was submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The transition from petascale to exascale computers is characterized by\nsubstantial changes in the computer architectures and technologies. The\nresearch community relying on computational simulations is being forced to\nrevisit the algorithms for data generation and analysis due to various\nconcerns, such as higher degrees of concurrency, deeper memory hierarchies,\nsubstantial I/O and communication constraints. Simulations today typically save\nall data to analyze later. Simulations at the exascale will require us to\nanalyze data as it is generated and save only what is really needed for\nanalysis, which must be performed predominately in-situ, i.e., executed\nsufficiently fast locally, limiting memory and disk usage, and avoiding the\nneed to move large data across nodes.\n  In this paper, we present a distributed method that enables in-situ data\nanalysis for large protein folding trajectory datasets. Traditional trajectory\nanalysis methods currently follow a centralized approach that moves the\ntrajectory datasets to a centralized node and processes the data only after\nsimulations have been completed. Our method, on the other hand, captures\nconformational information in-situ using local data only while reducing the\nstorage space needed for the part of the trajectory under consideration. This\nmethod processes the input trajectory data in one pass, breaks from the\ncentralized approach of traditional analysis, avoids the movement of trajectory\ndata, and still builds the global knowledge on the formation of individual\n$\\alpha$-helices or $\\beta$-strands as trajectory frames are generated.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2015 17:34:57 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2015 15:41:25 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Johnston", "Travis", ""], ["Zhang", "Boyu", ""], ["Liwo", "Adam", ""], ["Crivelli", "Silvia", ""], ["Taufer", "Michela", ""]]}]