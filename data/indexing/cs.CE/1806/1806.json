[{"id": "1806.00180", "submitter": "Branko Ristic", "authors": "Branko Ristic, Jeremie Houssineau, Sanjeev Arulampalam", "title": "Robust TMA using the possibility particle filter", "comments": "3 figures, keywords: Target motion analysis; bearings-only tracking;\n  robust stochastic filtering; Monte Carlo estimation; possibility distribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem is target motion analysis (TMA), where the objective is to\nestimate the state of a moving target from noise corrupted bearings-only\nmeasurements. The focus is on recursive TMA, traditionally solved using the\nBayesian filters (e.g. the extended or unscented Kalman filters, particle\nfilters). The TMA is a difficult problem and may cause the algorithms to\ndiverge, especially when the measurement noise model is imperfect or\nmismatched. As a robust alternative to the Bayesian filters for TMA, we propose\nthe recently introduced possibility filter. This filter is implemented in the\nsequential Monte Carlo framework, and referred to as the possibility particle\nfilter. The paper demonstrates its superior performance against the standard\nparticle filter in the presence of a model mismatch, and equal performance in\nthe case of the exact model match.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 03:59:09 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Ristic", "Branko", ""], ["Houssineau", "Jeremie", ""], ["Arulampalam", "Sanjeev", ""]]}, {"id": "1806.00193", "submitter": "Jatin Bedi", "authors": "Jatin Bedi, Durga Toshniwal", "title": "SFA-GTM: Seismic Facies Analysis Based on Generative Topographic Map and\n  RBF", "comments": "11 Pages, 4 figures, 2 Tables, Part of DMG2 2018 proceedings\n  (arXiv:1805.04541)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seismic facies identification plays a significant role in reservoir\ncharacterization. It helps in identifying the various lithological and\nstratigraphical changes in reservoir properties. With the increase in the size\nof seismic data or number of attributes to be analyzed, the manual process for\nfacies identification becomes complicated and time-consuming. Even though\nseismic attributes add multiple dimensions to the data, their role in reservoir\ncharacterization is very crucial. There exist different linear transformation\nmethods that use seismic attributes for identification, characterization, and\nvisualization of seismic facies. These linear transformation methods have been\nwidely used for facies characterization. However, there are some limitations\nassociated with these methods such as deciding the width parameters, number of\nclusters, convergence rate etc. Therefore, the present research work uses\nnon-linear transformation approach that overcomes some of the major limitations\nof linear approaches. The proposed Seismic facies analysis approach based on\nGenerative Topographic Map \\& Radial Basis Function(SFA-GTM) works by\ncalculating the set of four Gray Level Co-occurrence Matrix(GLCM) texture based\nattributes viz. energy, homogeneity, contrast, and dissimilarity. The\nGenerative Topographic Map(GTM) is used for unsupervised classification of\nseismic facies based on the set of calculated texture attributes. Further, the\npresent work uses Radial Basis Function(RBF) for interpolating the missing\nvalues in the data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 04:50:10 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Bedi", "Jatin", ""], ["Toshniwal", "Durga", ""]]}, {"id": "1806.01072", "submitter": "Lorenzo Nespoli", "authors": "Lorenzo Nespoli, Matteo Salani, Vasco Medici", "title": "A rational decentralized generalized Nash equilibrium seeking for energy\n  markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.GT econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to design a decentralized energy market which guarantees\nindividual rationality (IR) in expectation, in the presence of system-level\ngrid constraints. We formulate the market as a welfare maximization problem\nsubject to IR constraints, and we make use of Lagrangian duality to model the\nproblem as a n-person non-cooperative game with a unique generalized Nash\nequilibrium (GNE). We provide a distributed algorithm which converges to the\nGNE. The convergence and properties of the algorithm are investigated by means\nof numerical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 12:33:13 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Nespoli", "Lorenzo", ""], ["Salani", "Matteo", ""], ["Medici", "Vasco", ""]]}, {"id": "1806.01949", "submitter": "Maruti Mudunuru", "authors": "A. Hunter, B. A. Moore, M. K. Mudunuru, V. T. Chau, R. L. Miller, R.\n  B. Tchoua, C. Nyshadham, S. Karra, D. O. Malley, E. Rougier, H. S.\n  Viswanathan, and G. Srinivasan", "title": "Reduced-Order Modeling through Machine Learning Approaches for Brittle\n  Fracture Applications", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, five different approaches for reduced-order modeling of\nbrittle fracture in geomaterials, specifically concrete, are presented and\ncompared. Four of the five methods rely on machine learning (ML) algorithms to\napproximate important aspects of the brittle fracture problem. In addition to\nthe ML algorithms, each method incorporates different physics-based assumptions\nin order to reduce the computational complexity while maintaining the physics\nas much as possible. This work specifically focuses on using the ML approaches\nto model a 2D concrete sample under low strain rate pure tensile loading\nconditions with 20 preexisting cracks present. A high-fidelity finite\nelement-discrete element model is used to both produce a training dataset of\n150 simulations and an additional 35 simulations for validation. Results from\nthe ML approaches are directly compared against the results from the\nhigh-fidelity model. Strengths and weaknesses of each approach are discussed\nand the most important conclusion is that a combination of physics-informed and\ndata-driven features are necessary for emulating the physics of crack\npropagation, interaction and coalescence. All of the models presented here have\nruntimes that are orders of magnitude faster than the original high-fidelity\nmodel and pave the path for developing accurate reduced order models that could\nbe used to inform larger length-scale models with important sub-scale physics\nthat often cannot be accounted for due to computational cost.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 22:08:09 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Hunter", "A.", ""], ["Moore", "B. A.", ""], ["Mudunuru", "M. K.", ""], ["Chau", "V. T.", ""], ["Miller", "R. L.", ""], ["Tchoua", "R. B.", ""], ["Nyshadham", "C.", ""], ["Karra", "S.", ""], ["Malley", "D. O.", ""], ["Rougier", "E.", ""], ["Viswanathan", "H. S.", ""], ["Srinivasan", "G.", ""]]}, {"id": "1806.02538", "submitter": "Rogelio Andrade Mancisidor", "authors": "Rogelio Andrade Mancisidor, Michael Kampffmeyer, Kjersti Aas, Robert\n  Jenssen", "title": "Segment-Based Credit Scoring Using Latent Clusters in the Variational\n  Autoencoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying customer segments in retail banking portfolios with different\nrisk profiles can improve the accuracy of credit scoring. The Variational\nAutoencoder (VAE) has shown promising results in different research domains,\nand it has been documented the powerful information embedded in the latent\nspace of the VAE. We use the VAE and show that transforming the input data into\na meaningful representation, it is possible to steer configurations in the\nlatent space of the VAE. Specifically, the Weight of Evidence (WoE)\ntransformation encapsulates the propensity to fall into financial distress and\nthe latent space in the VAE preserves this characteristic in a well-defined\nclustering structure. These clusters have considerably different risk profiles\nand therefore are suitable not only for credit scoring but also for marketing\nand customer purposes. This new clustering methodology offers solutions to some\nof the challenges in the existing clustering algorithms, e.g., suggests the\nnumber of clusters, assigns cluster labels to new customers, enables cluster\nvisualization, scales to large datasets, captures non-linear relationships\namong others. Finally, for portfolios with a large number of customers in each\ncluster, developing one classifier model per cluster can improve the credit\nscoring assessment.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 07:19:44 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Mancisidor", "Rogelio Andrade", ""], ["Kampffmeyer", "Michael", ""], ["Aas", "Kjersti", ""], ["Jenssen", "Robert", ""]]}, {"id": "1806.03294", "submitter": "Nils Bertschinger", "authors": "Rajbir-Singh Nirwan and Nils Bertschinger", "title": "Applications of Gaussian Process Latent Variable Models in Finance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating covariances between financial assets plays an important role in\nrisk management. In practice, when the sample size is small compared to the\nnumber of variables, the empirical estimate is known to be very unstable. Here,\nwe propose a novel covariance estimator based on the Gaussian Process Latent\nVariable Model (GP-LVM). Our estimator can be considered as a non-linear\nextension of standard factor models with readily interpretable parameters\nreminiscent of market betas. Furthermore, our Bayesian treatment naturally\nshrinks the sample covariance matrix towards a more structured matrix given by\nthe prior and thereby systematically reduces estimation errors. Finally, we\ndiscuss some financial applications of the GP-LVM.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 17:57:26 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 16:20:29 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Nirwan", "Rajbir-Singh", ""], ["Bertschinger", "Nils", ""]]}, {"id": "1806.03798", "submitter": "Varun Shankar", "authors": "Varun Shankar and Aaron L. Fogelson", "title": "Hyperviscosity-Based Stabilization for Radial Basis Function-Finite\n  Difference (RBF-FD) Discretizations of Advection-Diffusion Equations", "comments": "30 pages, 8 figures, accepted to Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2018.06.036", "report-no": null, "categories": "math.NA cs.CE cs.NA math.CA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hyperviscosity formulation for stabilizing RBF-FD\ndiscretizations of the advection-diffusion equation. The amount of\nhyperviscosity is determined quasi-analytically for commonly-used explicit,\nimplicit, and implicit-explicit (IMEX) time integrators by using a simple 1D\nsemi-discrete Von Neumann analysis. The analysis is applied to an analytical\nmodel of spurious growth in RBF-FD solutions that uses auxiliary differential\noperators mimicking the undesirable properties of RBF-FD differentiation\nmatrices. The resulting hyperviscosity formulation is a generalization of\nexisting ones in the literature, but is free of any tuning parameters and can\nbe computed efficiently. To further improve robustness, we introduce a simple\nnew scaling law for polynomial-augmented RBF-FD that relates the degree of\npolyharmonic spline (PHS) RBFs to the degree of the appended polynomial. When\nused in a novel ghost node formulation in conjunction with the\nrecently-developed overlapped RBF-FD method, the resulting method is robust and\nfree of stagnation errors. We validate the high-order convergence rates of our\nmethod on 2D and 3D test cases over a wide range of Peclet numbers (1-1000). We\nthen use our method to solve a 3D coupled problem motivated by models of\nplatelet aggregation and coagulation, again demonstrating high-order\nconvergence rates.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 04:03:57 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Shankar", "Varun", ""], ["Fogelson", "Aaron L.", ""]]}, {"id": "1806.04164", "submitter": "Ozgur Ergul", "authors": "Ugur Meric Gur and Ozgur Ergul", "title": "Solutions of New Potential Integral Equations Using MLFMA Based on the\n  Approximate Stable Diagonalization", "comments": "The paper was completed in August 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present efficient solutions of recently developed potential integral\nequations (PIEs) using a low-frequency implementation of the multilevel fast\nmultipole algorithm (MLFMA). PIEs enable accurate solutions of low-frequency\nproblems involving small objects and/or small discretization elements with\nrespect to wavelength. As the number of unknowns grows, however, PIEs need to\nbe solved via fast algorithms, which are also tolerant to low-frequency\nbreakdowns. Using an approximate diagonalization in MLFMA, we present a new\nimplementation that can provide accurate, stable, and efficient solutions of\nlow-frequency problems involving large numbers of unknowns. The effectiveness\nof the implementation is demonstrated on canonical problems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:08:28 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Gur", "Ugur Meric", ""], ["Ergul", "Ozgur", ""]]}, {"id": "1806.04351", "submitter": "Yingli Wang", "authors": "Yingli Wang, Qingpeng Zhang, and Xiaoguang Yang", "title": "Network Subgraphs of the heterogeneous Chinese credit system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate the evolution of Chinese guarantee networks\nfrom the angle of sub-patterns. First, we find that the mutual, 2-out-stars and\ntriangle sub-patterns are motifs in 2- and 3-node subgraphs. Considering the\nheterogeneous financial characteristics of nodes, we find that small firms tend\nto form a mutual guarantee relationship and large firms are likely to be the\nguarantors in 2-out-stars sub-patterns.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 06:22:44 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 01:25:05 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Wang", "Yingli", ""], ["Zhang", "Qingpeng", ""], ["Yang", "Xiaoguang", ""]]}, {"id": "1806.04482", "submitter": "Andrea Beck", "authors": "Andrea D. Beck, David G. Flad, Claus-Dieter Munz", "title": "Deep Neural Networks for Data-Driven Turbulence Models", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.108910", "report-no": null, "categories": "cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel data-based approach to turbulence modelling\nfor Large Eddy Simulation (LES) by artificial neural networks. We define the\nexact closure terms including the discretization operators and generate\ntraining data from direct numerical simulations of decaying homogeneous\nisotropic turbulence. We design and train artificial neural networks based on\nlocal convolution filters to predict the underlying unknown non-linear mapping\nfrom the coarse grid quantities to the closure terms without a priori\nassumptions. All investigated networks are able to generalize from the data and\nlearn approximations with a cross correlation of up to 47% and even 73% for the\ninner elements, leading to the conclusion that the current training success is\ndata-bound. We further show that selecting both the coarse grid primitive\nvariables as well as the coarse grid LES operator as input features\nsignificantly improves training results. Finally, we construct a stable and\naccurate LES model from the learned closure terms. Therefore, we translate the\nmodel predictions into a data-adaptive, pointwise eddy viscosity closure and\nshow that the resulting LES scheme performs well compared to current state of\nthe art approaches. This work represents the starting point for further\nresearch into data-driven, universal turbulence models.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 17:40:51 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 11:56:38 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 14:24:15 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Beck", "Andrea D.", ""], ["Flad", "David G.", ""], ["Munz", "Claus-Dieter", ""]]}, {"id": "1806.04589", "submitter": "Fuhui Zhou", "authors": "Fuhui Zhou, Yongpeng Wu, Rose Qingyang Hu, and Yi Qian", "title": "Computation Rate Maximization in UAV-Enabled Wireless Powered\n  Mobile-Edge Computing Systems", "comments": "This paper has been accepted by IEEE JSAC", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CE cs.IT cs.NI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile edge computing (MEC) and wireless power transfer (WPT) are two\npromising techniques to enhance the computation capability and to prolong the\noperational time of low-power wireless devices that are ubiquitous in Internet\nof Things. However, the computation performance and the harvested energy are\nsignificantly impacted by the severe propagation loss. In order to address this\nissue, an unmanned aerial vehicle (UAV)-enabled MEC wireless powered system is\nstudied in this paper. The computation rate maximization problems in a\nUAV-enabled MEC wireless powered system are investigated under both partial and\nbinary computation offloading modes, subject to the energy harvesting causal\nconstraint and the UAV's speed constraint. These problems are non-convex and\nchallenging to solve. A two-stage algorithm and a three-stage alternative\nalgorithm are respectively proposed for solving the formulated problems. The\nclosed-form expressions for the optimal central processing unit frequencies,\nuser offloading time, and user transmit power are derived. The optimal\nselection scheme on whether users choose to locally compute or offload\ncomputation tasks is proposed for the binary computation offloading mode.\nSimulation results show that our proposed resource allocation schemes\noutperforms other benchmark schemes. The results also demonstrate that the\nproposed schemes converge fast and have low computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 22:15:22 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 18:48:59 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Zhou", "Fuhui", ""], ["Wu", "Yongpeng", ""], ["Hu", "Rose Qingyang", ""], ["Qian", "Yi", ""]]}, {"id": "1806.05387", "submitter": "Erik Schlogl", "authors": "Karol Gellert and Erik Schl\\\"ogl", "title": "Parameter Learning and Change Detection Using a Particle Filter With\n  Accelerated Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": "QFRC working paper 392", "categories": "stat.ML cs.CE cs.LG cs.NE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the construction of a particle filter, which incorporates\nelements inspired by genetic algorithms, in order to achieve accelerated\nadaptation of the estimated posterior distribution to changes in model\nparameters. Specifically, the filter is designed for the situation where the\nsubsequent data in online sequential filtering does not match the model\nposterior filtered based on data up to a current point in time. The examples\nconsidered encompass parameter regime shifts and stochastic volatility. The\nfilter adapts to regime shifts extremely rapidly and delivers a clear heuristic\nfor distinguishing between regime shifts and stochastic volatility, even though\nthe model dynamics assumed by the filter exhibit neither of those features.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 06:41:10 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Gellert", "Karol", ""], ["Schl\u00f6gl", "Erik", ""]]}, {"id": "1806.05713", "submitter": "Hiroshi Watanabe", "authors": "Hiroshi Watanabe and Koh M. Nakagawa", "title": "SIMD Vectorization for the Lennard-Jones Potential with AVX2 and AVX-512\n  instructions", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2018.10.028", "report-no": null, "categories": "cs.MS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes the SIMD vectorization of the force calculation of the\nLennard-Jones potential with Intel AVX2 and AVX-512 instruction sets. Since the\nforce-calculation kernel of the molecular dynamics method involves indirect\naccess to memory, the data layout is one of the most important factors in\nvectorization. We find that the Array of Structures (AoS) with padding exhibits\nbetter performance than Structure of Arrays (SoA) with appropriate\nvectorization and optimizations. In particular, AoS with 512-bit width exhibits\nthe best performance among the architectures. While the difference in\nperformance between AoS and SoA is significant for the vectorization with AVX2,\nthat with AVX-512 is minor. The effect of other optimization techniques, such\nas software pipelining together with vectorization, is also discussed. We\npresent results for benchmarks on three CPU architectures: Intel Haswell (HSW),\nKnights Landing (KNL), and Skylake (SKL). The performance gains by\nvectorization are about 42\\% on HSW compared with the code optimized without\nvectorization. On KNL, the hand-vectorized codes exhibit 34\\% better\nperformance than the codes vectorized automatically by the Intel compiler. On\nSKL, the code vectorized with AVX2 exhibits slightly better performance than\nthat with vectorized AVX-512.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 08:53:58 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 09:53:05 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Watanabe", "Hiroshi", ""], ["Nakagawa", "Koh M.", ""]]}, {"id": "1806.05799", "submitter": "Hao Liu", "authors": "Hao Liu, Qinyu Cao, Xinru Liao, Guang Qiu, Sheng Li, Jiming Chen", "title": "CIA-Towards a Unified Marketing Optimization Framework for e-Commerce\n  Sponsored Search", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the largest e-commerce platform, Taobao helps advertisers reach billions\nof search queries each day via sponsored search, which has also contributed\nconsiderable revenue to the platform. An efficient bidding strategy to cater to\ndiverse advertiser demands while balancing platform revenue and consumer\nexperience is significant to a healthy and sustainable marketing ecosystem. In\nthis paper we propose \\emph{Customer Intelligent Agent (CIA)}, a bidding\noptimization framework which implements an impression-level bidding to reflect\nadvertisers' conversion willingness and budget control. In this way, CIA is\ncapable of fulfilling various e-commerce advertiser demands on different\nlevels, such as Gross Merchandise Volume optimization, style comparison etc.\nAdditionally, a replay based simulation system is designed to predict the\nperformance of different take-rate. CIA unifies the benefits of three parties\nin the marketing ecosystem without changing the Generalized Second Price\nmechanism. Our extensive offline simulations and large-scale online experiments\non \\emph{Taobao Search Advertising (TSA)} platform verify the high\neffectiveness of the CIA framework. Moreover, CIA has been deployed online as a\nmajor bidding tool in TSA.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 03:57:38 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 07:14:50 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Liu", "Hao", ""], ["Cao", "Qinyu", ""], ["Liao", "Xinru", ""], ["Qiu", "Guang", ""], ["Li", "Sheng", ""], ["Chen", "Jiming", ""]]}, {"id": "1806.05865", "submitter": "Adam Gaier", "authors": "Adam Gaier, Alexander Asteroth, and Jean-Baptiste Mouret", "title": "Data-Efficient Design Exploration through Surrogate-Assisted\n  Illumination", "comments": "ArXiv preprint version, final version published in Evolutionary\n  Computation, doi: 10.1162/evco_a_00231", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design optimization techniques are often used at the beginning of the design\nprocess to explore the space of possible designs. In these domains illumination\nalgorithms, such as MAP-Elites, are promising alternatives to classic\noptimization algorithms because they produce diverse, high-quality solutions in\na single run, instead of only a single near-optimal solution. Unfortunately,\nthese algorithms currently require a large number of function evaluations,\nlimiting their applicability. In this article we introduce a new illumination\nalgorithm, Surrogate-Assisted Illumination (SAIL), that leverages surrogate\nmodeling techniques to create a map of the design space according to\nuser-defined features while minimizing the number of fitness evaluations. On a\n2-dimensional airfoil optimization problem SAIL produces hundreds of diverse\nbut high-performing designs with several orders of magnitude fewer evaluations\nthan MAP-Elites or CMA-ES. We demonstrate that SAIL is also capable of\nproducing maps of high-performing designs in realistic 3-dimensional\naerodynamic tasks with an accurate flow simulation. Data-efficient design\nexploration with SAIL can help designers understand what is possible, beyond\nwhat is optimal, by considering more than pure objective-based optimization.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 09:00:46 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Gaier", "Adam", ""], ["Asteroth", "Alexander", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1806.05876", "submitter": "Carlos Pedro dos Santos Gon\\c{c}alves", "authors": "Carlos Pedro Gon\\c{c}alves", "title": "Financial Risk and Returns Prediction with Modular Networked Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.NE q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An artificial agent for financial risk and returns' prediction is built with\na modular cognitive system comprised of interconnected recurrent neural\nnetworks, such that the agent learns to predict the financial returns, and\nlearns to predict the squared deviation around these predicted returns. These\ntwo expectations are used to build a volatility-sensitive interval prediction\nfor financial returns, which is evaluated on three major financial indices and\nshown to be able to predict financial returns with higher than 80% success rate\nin interval prediction in both training and testing, raising into question the\nEfficient Market Hypothesis. The agent is introduced as an example of a class\nof artificial intelligent systems that are equipped with a Modular Networked\nLearning cognitive system, defined as an integrated networked system of machine\nlearning modules, where each module constitutes a functional unit that is\ntrained for a given specific task that solves a subproblem of a complex main\nproblem expressed as a network of linked subproblems. In the case of neural\nnetworks, these systems function as a form of an \"artificial brain\", where each\nmodule is like a specialized brain region comprised of a neural network with a\nspecific architecture.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 09:49:39 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Gon\u00e7alves", "Carlos Pedro", ""]]}, {"id": "1806.06064", "submitter": "Mark Hallen", "authors": "Mark A. Hallen and Bruce R. Donald", "title": "Protein Design by Algorithm", "comments": null, "journal-ref": "Communications of the ACM, October 2019, Vol. 62 No. 10, Pages\n  76-84", "doi": "10.1145/3338124", "report-no": null, "categories": "cs.CE cs.DS q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review algorithms for protein design in general. Although these algorithms\nhave a rich combinatorial, geometric, and mathematical structure, they are\nalmost never covered in computer science classes. Furthermore, many of these\nalgorithms admit provable guarantees of accuracy, soundness, complexity,\ncompleteness, optimality, and approximation bounds. The algorithms represent a\ndelicate and beautiful balance between discrete and continuous computation and\nmodeling, analogous to that which is seen in robotics, computational geometry,\nand other fields in computational science. Finally, computer scientists may be\nunaware of the almost direct impact of these algorithms for predicting and\nintroducing molecular therapies that have gone in a short time from mathematics\nto algorithms to software to predictions to preclinical testing to clinical\ntrials. Indeed, the overarching goal of these algorithms is to enable the\ndevelopment of new therapeutics that might be impossible or too expensive to\ndiscover using experimental methods. Thus the potential impact of these\nalgorithms on individual, community, and global health has the potential to be\nquite significant.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 04:49:44 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Hallen", "Mark A.", ""], ["Donald", "Bruce R.", ""]]}, {"id": "1806.06387", "submitter": "Marta Nu\\~nez-Garcia", "authors": "Marta Nu\\~nez-Garcia, Oscar Camara, Mark D. O'Neill, Reza Razavi,\n  Henry Chubb, and Constantine Butakoff", "title": "Mind the gap: quantification of incomplete ablation patterns after\n  pulmonary vein isolation using minimum path search", "comments": null, "journal-ref": "Medical Image Analysis, 51, 1-12, (2018)", "doi": "10.1016/j.media.2018.10.001", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary vein isolation (PVI) is a common procedure for the treatment of\natrial fibrillation (AF). A successful isolation produces a continuous lesion\n(scar) completely encircling the veins that stops activation waves from\npropagating to the atrial body. Unfortunately, the encircling lesion is often\nincomplete, becoming a combination of scar and gaps of healthy tissue. These\ngaps are potential causes of AF recurrence, which requires a redo of the\nisolation procedure. Late-gadolinium enhanced cardiac magnetic resonance\n(LGE-CMR) is a non-invasive method that may also be used to detect gaps, but it\nis currently a time-consuming process, prone to high inter-observer\nvariability. In this paper, we present a method to semi-automatically identify\nand quantify ablation gaps. Gap quantification is performed through minimum\npath search in a graph where every node is a scar patch and the edges are the\ngeodesic distances between patches. We propose the Relative Gap Measure (RGM)\nto estimate the percentage of gap around a vein, which is defined as the ratio\nof the overall gap length and the total length of the path that encircles the\nvein. Additionally, an advanced version of the RGM has been developed to\nintegrate gap quantification estimates from different scar segmentation\ntechniques into a single figure-of-merit. Population-based statistical and\nregional analysis of gap distribution was performed using a standardised\nparcellation of the left atrium. We have evaluated our method on synthetic and\nclinical data from 50 AF patients who underwent PVI with radiofrequency\nablation. The population-based analysis concluded that the left superior PV is\nmore prone to lesion gaps while the left inferior PV tends to have less gaps\n(p<0.05 in both cases), in the processed data. This type of information can be\nvery useful for the optimization and objective assessment of PVI interventions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 14:13:14 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Nu\u00f1ez-Garcia", "Marta", ""], ["Camara", "Oscar", ""], ["O'Neill", "Mark D.", ""], ["Razavi", "Reza", ""], ["Chubb", "Henry", ""], ["Butakoff", "Constantine", ""]]}, {"id": "1806.06394", "submitter": "Saeid Hosseini", "authors": "Leila Khalatbari, Mohammad Reza Kangavari, Saeid Hosseini, Hongzhi\n  Yin, Ngai-Man Cheung", "title": "MCP: a Multi-Component learning machine to Predict protein secondary\n  structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gene or DNA sequence in every cell does not control genetic properties on\nits own; Rather, this is done through translation of DNA into protein and\nsubsequent formation of a certain 3D structure. The biological function of a\nprotein is tightly connected to its specific 3D structure. Prediction of the\nprotein secondary structure is a crucial intermediate step towards elucidating\nits 3D structure and function. Traditional experimental methods for prediction\nof protein structure are expensive and time-consuming. Therefore, various\nmachine learning approaches have been proposed to predict the protein secondary\nstructure. Nevertheless, the average accuracy of the suggested solutions has\nhardly reached beyond 80%. The possible underlying reasons are the ambiguous\nsequence-structure relation, noise in input protein data, class imbalance, and\nthe high dimensionality of the encoding schemes that represent the protein\nsequence. In this paper, we propose an accurate multi-component prediction\nmachine to overcome the challenges of protein structure prediction. We devise a\nmulti-component designation to address the high complexity challenge in\nsequence-structure relation. Furthermore, we utilize a compound string\ndissimilarity measure to directly interpret protein sequence content and avoid\ninformation loss. In order to improve the accuracy, we employ two different\nclassifiers including support vector machine and fuzzy nearest neighbor and\ncollectively aggregate the classification outcomes to infer the final protein\nsecondary structures. We conduct comprehensive experiments to compare our model\nwith the current state-of-the-art approaches. The experimental results\ndemonstrate that given a set of input sequences, our multi-component framework\ncan accurately predict the protein structure. Nevertheless, the effectiveness\nof our unified model an be further enhanced through framework configuration.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 15:18:31 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 12:00:37 GMT"}, {"version": "v3", "created": "Thu, 13 Sep 2018 12:53:38 GMT"}, {"version": "v4", "created": "Wed, 29 May 2019 14:41:33 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Khalatbari", "Leila", ""], ["Kangavari", "Mohammad Reza", ""], ["Hosseini", "Saeid", ""], ["Yin", "Hongzhi", ""], ["Cheung", "Ngai-Man", ""]]}, {"id": "1806.06944", "submitter": "Huu Phuoc Bui", "authors": "Michel Duprez, St\\'ephane P.A. Bordas, Marek Bucki, Huu Phuoc Bui,\n  Franz Chouly, Vanessa Lleras, Claudio Lobos, Alexei Lozinski, Pierre-Yves\n  Rohan, Satyendra Tomar", "title": "Quantifying discretization errors for soft-tissue simulation in computer\n  assisted surgery: a preliminary study", "comments": "35 pages, 10 figures", "journal-ref": "Applied Mathematical Modelling Volume 77, Part 1, January 2020,\n  Pages 709-723", "doi": "10.1016/j.apm.2019.07.055", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Errors in biomechanics simulations arise from modeling and discretization.\nModeling errors are due to the choice of the mathematical model whilst\ndiscretization errors measure the impact of the choice of the numerical method\non the accuracy of the approximated solution to this specific mathematical\nmodel. A major source of discretization errors is mesh generation from medical\nimages, that remains one of the major bottlenecks in the development of\nreliable, accurate, automatic and efficient personalized, clinically-relevant\nFinite Element (FE) models in biomechanics. The impact of mesh quality and\ndensity on the accuracy of the FE solution can be quantified with \\emph{a\nposteriori} error estimates. Yet, to our knowledge, the relevance of such error\nestimates for practical biomechanics problems has seldom been addressed, see\n[25]. In this contribution, we propose an implementation of some a posteriori\nerror estimates to quantify the discretization errors and to optimize the mesh.\nMore precisely, we focus on error estimation for a user-defined quantity of\ninterest with the Dual Weighted Residual (DWR) technique. We test its\napplicability and relevance in two situations, corresponding to computations\nfor a tongue and an artery, using a simplified setting, i.e., plane linearized\nelasticity with contractility of the soft-tissue modeled as a pre-stress. Our\nresults demonstrate the feasibility of such methodology to estimate the actual\nsolution errors and to reduce them economically through mesh refinement.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 20:58:49 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Duprez", "Michel", ""], ["Bordas", "St\u00e9phane P. A.", ""], ["Bucki", "Marek", ""], ["Bui", "Huu Phuoc", ""], ["Chouly", "Franz", ""], ["Lleras", "Vanessa", ""], ["Lobos", "Claudio", ""], ["Lozinski", "Alexei", ""], ["Rohan", "Pierre-Yves", ""], ["Tomar", "Satyendra", ""]]}, {"id": "1806.06975", "submitter": "Joseph Paul Cohen", "authors": "Francis Dutil, Joseph Paul Cohen, Martin Weiss, Georgy Derevyanko,\n  Yoshua Bengio", "title": "Towards Gene Expression Convolutions using Gene Interaction Graphs", "comments": "4 pages +1 page references, To appear in the International Conference\n  on Machine Learning Workshop on Computational Biology, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the challenges of applying deep learning to gene expression data. We\nfind experimentally that there exists non-linear signal in the data, however is\nit not discovered automatically given the noise and low numbers of samples used\nin most research. We discuss how gene interaction graphs (same pathway,\nprotein-protein, co-expression, or research paper text association) can be used\nto impose a bias on a deep model similar to the spatial bias imposed by\nconvolutions on an image. We explore the usage of Graph Convolutional Neural\nNetworks coupled with dropout and gene embeddings to utilize the graph\ninformation. We find this approach provides an advantage for particular tasks\nin a low data regime but is very dependent on the quality of the graph used. We\nconclude that more work should be done in this direction. We design experiments\nthat show why existing methods fail to capture signal that is present in the\ndata when features are added which clearly isolates the problem that needs to\nbe addressed.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 22:40:37 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Dutil", "Francis", ""], ["Cohen", "Joseph Paul", ""], ["Weiss", "Martin", ""], ["Derevyanko", "Georgy", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.07125", "submitter": "Marco Palombo", "authors": "Marco Palombo, Daniel C. Alexander, Hui Zhang", "title": "A generative model of realistic brain cells with application to\n  numerical simulation of diffusion-weighted MR signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CE physics.bio-ph physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel computational framework that we developed\nto use numerical simulations to investigate the complexity of brain tissue at a\nmicroscopic level with a detail never realised before. Directly inspired by the\nadvances in computational neuroscience for modelling brain cells, we propose a\ngenerative model that enables us to simulate molecular diffusion within\nrealistic digitalised brain cells, such as neurons and glia, in a completely\ncontrolled and flexible fashion. We validate our new approach by showing an\nexcellent match between the morphology and simulated DW-MR signal of the\ngenerated digital model of brain cells and those of digital reconstruction of\nreal brain cells from available open-access databases. We demonstrate the\nversatility and potentiality of the framework by showing a select set of\nexamples of relevance for the DW-MR community. Further development is ongoing,\nwhich will support even more realistic conditions like dense packing of\nnumerous 3D complex cell structures and varying cell surface permeability.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 09:34:52 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Palombo", "Marco", ""], ["Alexander", "Daniel C.", ""], ["Zhang", "Hui", ""]]}, {"id": "1806.07245", "submitter": "Sepideh Shamsizadeh", "authors": "Sepideh Shamsizadeha, Sama Goliaea, Zahra Razaghi Moghadamb", "title": "CAMIRADA: Cancer microRNA association discovery algorithm, a case study\n  on breast cancer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.GN q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent studies, non-coding protein RNAs have been identified as microRNA\nthat can be used as biomarkers for early diagnosis and treatment of cancer,\nthat decrease mortality in cancer. A microRNA may target hundreds or thousands\nof genes and a gene may regulate several microRNAs, so determining which\nmicroRNA is associated with which cancer is a big challenge. Many computational\nmethods have been performed to detect micoRNAs association with cancer, but\nmore effort is needed with higher accuracy. Increasing research has shown that\nrelationship between microRNAs and TFs play a significant role in the diagnosis\nof cancer. Therefore, we developed a new computational framework (CAMIRADA) to\nidentify cancer-related microRNA based on the relationship between microRNAs\nand disease genes (DG) in the protein network, the functional relationships\nbetween microRNAs and Transcription Factors (TF) on the co-expression network,\nand the relationship between microRNAs and the Differential Expression Gene\n(DEG) on co-expression network. The CAMIRADA was applied to assess breast\ncancer data from two HMDD and miR2Disease databases. In this study, the AUC for\nthe 65 microRNA of the top of the list was 0.95, which was more accurate than\nthe similar methods used to detect microRNA associated with the cancer artery.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 07:41:04 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Shamsizadeha", "Sepideh", ""], ["Goliaea", "Sama", ""], ["Moghadamb", "Zahra Razaghi", ""]]}, {"id": "1806.07500", "submitter": "Matteo Giacomini", "authors": "Ruben Sevilla, Matteo Giacomini, Antonio Huerta", "title": "A locking-free face-centred finite volume (FCFV) method for linear\n  elasticity", "comments": "29 pages, 20 figures", "journal-ref": "Comput. Struct., 212, 43--57 (2019)", "doi": "10.1016/j.compstruc.2018.10.015", "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A face-centred finite volume (FCFV) method is proposed for the linear\nelasticity equation. The FCFV is a mixed hybrid formulation, featuring a system\nof first-order equations, that defines the unknowns on the faces (edges in two\ndimensions) of the mesh elements. The symmetry of the stress tensor is strongly\nenforced using the well-known Voigt notation and the displacement and stress\nfields inside each cell are obtained element-wise by means of explicit\nformulas. The resulting FCFV method is robust and locking-free in the nearly\nincompressible limit. Numerical experiments in two and three dimensions show\noptimal convergence of the displacement and the stress fields without any\nreconstruction. Moreover, the accuracy of the FCFV method is not sensitive to\nmesh distortion and stretching. Classical benchmark tests including Kirch's\nplate and Cook's membrane problems in two dimensions as well as three\ndimensional problems involving shear phenomenons, pressurised thin shells and\ncomplex geometries are presented to show the capability and potential of the\nproposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 23:26:55 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Sevilla", "Ruben", ""], ["Giacomini", "Matteo", ""], ["Huerta", "Antonio", ""]]}, {"id": "1806.07884", "submitter": "Zuzana Majdisova", "authors": "Zuzana Majdisova and Vaclav Skala", "title": "Big Geo Data Surface Approximation using Radial Basis Functions: A\n  Comparative Study", "comments": "arXiv admin note: text overlap with arXiv:1806.04243", "journal-ref": "Computers and Geosciences, Vol.109, pp.51-58, ISSN 0098-3004,\n  Elsevier, December 2017", "doi": "10.1016/j.cageo.2017.08.007", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation of scattered data is often a task in many engineering problems.\nThe Radial Basis Function (RBF) approximation is appropriate for big scattered\ndatasets in $n-$dimensional space. It is a non-separable approximation, as it\nis based on the distance between two points. This method leads to the solution\nof an overdetermined linear system of equations.\n  In this paper the RBF approximation methods are briefly described, a new\napproach to the RBF approximation of big datasets is presented, and a\ncomparison for different Compactly Supported RBFs (CS-RBFs) is made with\nrespect to the accuracy of the computation. The proposed approach uses symmetry\nof a matrix, partitioning the matrix into blocks and data structures for\nstorage of the sparse matrix. The experiments are performed for synthetic and\nreal datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 13:13:41 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Majdisova", "Zuzana", ""], ["Skala", "Vaclav", ""]]}, {"id": "1806.07952", "submitter": "Gabriel Spadon", "authors": "Gabriel Spadon, Gabriel Gimenes, Jose F. Rodrigues-Jr", "title": "Topological street-network characterization through feature-vector and\n  cluster analysis", "comments": "Paper to be published on the International Conference on\n  Computational Science (ICCS), 2018", "journal-ref": null, "doi": "10.1007/978-3-319-93698-7_21", "report-no": null, "categories": "cs.SI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks provide a means to describe cities through their street\nmesh, expressing characteristics that refer to the structure and organization\nof an urban zone. Although other studies have used complex networks to model\nstreet meshes, we observed a lack of methods to characterize the relationship\nbetween cities by using their topological features. Accordingly, this paper\naims to describe interactions between cities by using vectors of topological\nfeatures extracted from their street meshes represented as complex networks.\nThe methodology of this study is based on the use of digital maps. Over the\ncomputational representation of such maps, we extract global complex-network\nfeatures that embody the characteristics of the cities. These vectors allow for\nthe use of multidimensional projection and clustering techniques, enabling a\nsimilarity-based comparison of the street meshes. We experiment with 645 cities\nfrom the Brazilian state of Sao Paulo. Our results show how the joint of global\nfeatures describes urban indicators that are deep-rooted in the network's\ntopology and how they reveal characteristics and similarities among sets of\ncities that are separated from each other.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 12:36:21 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Spadon", "Gabriel", ""], ["Gimenes", "Gabriel", ""], ["Rodrigues-Jr", "Jose F.", ""]]}, {"id": "1806.08114", "submitter": "Mehdi Baniasadi", "authors": "Mehdi Baniasadi, Maryam Baniasadi, Gabriele Pozzetti, Bernhard Peters", "title": "A numerical study on the softening process of iron ore particles in the\n  cohesive zone of an experimental blast furnace using a coupled CFD-DEM method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reduced iron-bearing materials start softening in the cohesive zone of a\nblast furnace due to the high temperature and the weight of the burden above.\nSoftening process causes a reduction of void space between particles. As a\nresult, the pressure drop and gas flow change remarkably in this particular\nzone. As a consequence, it has a significant influence on the performance of a\nblast furnace and is needed to be fully characterized. For this reason, the gas\nrheology along with the deformation of the particles and the heat transfer\nbetween particle-particle and particle-gas should be adequately described. In\nthis paper, the eXtended Discrete Element Method (XDEM), as a CFD-DEM approach\ncoupled with the heat transfer, is applied to model complex gas- solid flow\nduring the softening process of pre-reduced iron ore pellets in an Experimental\nBlast Furnace (EBF). The particle deformation, displacement, temperature, and\ngas pressure drop and flow under conditions relevant to the EBF operations are\nexamined. Moreover, to accurately capture the high gas velocity inlet, a\ndual-grid multi-scale approach is applied. The approach and findings are\nhelpful to understand the effect of the softening process on the pressure drop\nand gas flow in the cohesive zone of the blast furnace.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 08:52:44 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Baniasadi", "Mehdi", ""], ["Baniasadi", "Maryam", ""], ["Pozzetti", "Gabriele", ""], ["Peters", "Bernhard", ""]]}, {"id": "1806.08117", "submitter": "Constantin Grigo", "authors": "Constantin Grigo, Phaedon-Stelios Koutsourelakis", "title": "A data-driven model order reduction approach for Stokes flow through\n  random porous media", "comments": "2 pages, 2 figures", "journal-ref": "PAMM Proc. Appl. Math. Mech.2018;18:e201800314", "doi": "10.1002/pamm.201800314", "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct numerical simulation of Stokes flow through an impermeable, rigid body\nmatrix by finite elements requires meshes fine enough to resolve the pore-size\nscale and is thus a computationally expensive task. The cost is significantly\namplified when randomness in the pore microstructure is present and therefore\nmultiple simulations need to be carried out. It is well known that in the limit\nof scale-separation, Stokes flow can be accurately approximated by Darcy's law\nwith an effective diffusivity field depending on viscosity and the pore-matrix\ntopology. We propose a fully probabilistic, Darcy-type, reduced-order model\nwhich, based on only a few tens of full-order Stokes model runs, is capable of\nlearning a map from the fine-scale topology to the effective diffusivity and is\nmaximally predictive of the fine-scale response. The reduced-order model\nlearned can significantly accelerate uncertainty quantification tasks as well\nas provide quantitative confidence metrics of the predictive estimates\nproduced.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 08:53:42 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Grigo", "Constantin", ""], ["Koutsourelakis", "Phaedon-Stelios", ""]]}, {"id": "1806.09548", "submitter": "Andreas Lindholm", "authors": "Andreas Lindholm and Fredrik Lindsten", "title": "Learning dynamical systems with particle stochastic approximation EM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the particle stochastic approximation EM (PSAEM) algorithm for\nlearning of dynamical systems. The method builds on the EM algorithm, an\niterative procedure for maximum likelihood inference in latent variable models.\nBy combining stochastic approximation EM and particle Gibbs with ancestor\nsampling (PGAS), PSAEM obtains superior computational performance and\nconvergence properties compared to plain particle-smoothing-based\napproximations of the EM algorithm. PSAEM can be used for plain maximum\nlikelihood inference as well as for empirical Bayes learning of\nhyperparameters. Specifically, the latter point means that existing PGAS\nimplementations easily can be extended with PSAEM to estimate hyperparameters\nat almost no extra computational cost. We discuss the convergence properties of\nthe algorithm, and demonstrate it on several signal processing applications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 16:12:35 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 10:38:50 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Lindholm", "Andreas", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1806.10655", "submitter": "Ahmed Attia", "authors": "Ahmed Attia and Emil Constantinescu", "title": "An Optimal Experimental Design Framework for Adaptive Inflation and\n  Covariance Localization for Ensemble Filters", "comments": "31 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an optimal experimental design framework for adapting the\ncovariance inflation and localization in data assimilation problems. Covariance\ninflation and localization are ubiquitously employed to alleviate the effect of\nusing ensembles of finite sizes in all practical data assimilation systems. The\nchoice of both the inflation factor and the localization radius can have a\nsignificant impact on the performance of the assimilation scheme. These\nparameters are generally tuned by trial and error, rendering them expensive to\noptimize in practice. Spatially and temporally varying inflation parameter and\nlocalization radii have been recently proposed and have been empirically proven\nto enhance the performance of the employed assimilation filter. In this study,\nwe present a variational framework for adaptive tuning of the inflation and\nlocalization parameters. Each of these parameters is optimized independently,\nwith an objective to minimize the uncertainty in the posterior state. The\nproposed framework does not assume uncorrelated observations or prior errors\nand can in principle be applied without expert knowledge about the model and\nthe observations. Thus, it is adequate for handling dense as well as sparse\nobservational networks. We present the mathematical formulation, algorithmic\ndescription of the approach, and numerical experiments using the two-layer\nLorenz-96 model.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 19:36:44 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 22:43:48 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Attia", "Ahmed", ""], ["Constantinescu", "Emil", ""]]}, {"id": "1806.10925", "submitter": "Matthew England Dr", "authors": "C. Mulligan, J.H. Davenport, M.England", "title": "TheoryGuru: A Mathematica Package to apply Quantifier Elimination", "comments": "To appear in Proc ICMS 2018", "journal-ref": "In: J.H. Davenport, M. Kauers, G. Labahn and J. Urban, eds.\n  Mathematical Software - ICMS 2018, pp. 369-378. (Lecture Notes in Computer\n  Science 10931). Springer, 2018", "doi": "10.1007/978-3-319-96418-8_44", "report-no": null, "categories": "cs.SC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the use of Quantifier Elimination (QE) technology for automated\nreasoning in economics. There is a great body of work considering QE\napplications in science and engineering but we demonstrate here that it also\nhas use in the social sciences. We explain how many suggested theorems in\neconomics could either be proven, or even have their hypotheses shown to be\ninconsistent, automatically via QE.\n  However, economists who this technology could benefit are usually unfamiliar\nwith QE, and the use of mathematical software generally. This motivated the\ndevelopment of a Mathematica Package TheoryGuru, whose purpose is to lower the\ncosts of applying QE to economics. We describe the package's functionality and\ngive examples of its use.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 12:53:08 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Mulligan", "C.", ""], ["Davenport", "J. H.", ""], ["England", "M.", ""]]}, {"id": "1806.11277", "submitter": "Eran Treister", "authors": "Eran Treister", "title": "Shifted Laplacian multigrid for the elastic Helmholtz equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shifted Laplacian multigrid method is a well known approach for\npreconditioning the indefinite linear system arising from the discretization of\nthe acoustic Helmholtz equation. This equation is used to model wave\npropagation in the frequency domain. However, in some cases the acoustic\nequation is not sufficient for modeling the physics of the wave propagation,\nand one has to consider the elastic Helmholtz equation. Such a case arises in\ngeophysical seismic imaging applications, where the earth's subsurface is the\nelastic medium. The elastic Helmholtz equation is much harder to solve than its\nacoustic counterpart, partially because it is three times larger, and partially\nbecause it models more complicated physics. Despite this, there are very few\nsolvers available for the elastic equation compared to the array of solvers\nthat are available for the acoustic one. In this work we extend the shifted\nLaplacian approach to the elastic Helmholtz equation, by combining the complex\nshift idea with approaches for linear elasticity. We demonstrate the efficiency\nand properties of our solver using numerical experiments for problems with\nheterogeneous media in two and three dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 06:21:37 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Treister", "Eran", ""]]}]