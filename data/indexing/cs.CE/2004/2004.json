[{"id": "2004.00707", "submitter": "Fangxin Fang", "authors": "M. Cheng, F. Fang, C.C. Pain, I.M. Navon", "title": "Data-driven modelling of nonlinear spatio-temporal fluid flows using a\n  deep convolutional generative adversarial network", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.113000", "report-no": null, "categories": "physics.ao-ph cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning techniques for improving fluid flow modelling have gained\nsignificant attention in recent years. Advanced deep learning techniques\nachieve great progress in rapidly predicting fluid flows without prior\nknowledge of the underlying physical relationships. Advanced deep learning\ntechniques achieve great progress in rapidly predicting fluid flows without\nprior knowledge of the underlying physical relationships. However, most of\nexisting researches focused mainly on either sequence learning or spatial\nlearning, rarely on both spatial and temporal dynamics of fluid flows\n(Reichstein et al., 2019). In this work, an Artificial Intelligence (AI) fluid\nmodel based on a general deep convolutional generative adversarial network\n(DCGAN) has been developed for predicting spatio-temporal flow distributions.\nIn deep convolutional networks, the high-dimensional flows can be converted\ninto the low-dimensional \"latent\" representations. The complex features of flow\ndynamics can be captured by the adversarial networks. The above DCGAN fluid\nmodel enables us to provide reasonable predictive accuracy of flow fields while\nmaintaining a high computational efficiency. The performance of the DCGAN is\nillustrated for two test cases of Hokkaido tsunami with different incoming\nwaves along the coastal line. It is demonstrated that the results from the\nDCGAN are comparable with those from the original high fidelity model\n(Fluidity). The spatio-temporal flow features have been represented as the flow\nevolves, especially, the wave phases and flow peaks can be captured accurately.\nIn addition, the results illustrate that the online CPU cost is reduced by five\norders of magnitude compared to the original high fidelity model simulations.\nThe promising results show that the DCGAN can provide rapid and reliable\nspatio-temporal prediction for nonlinear fluid flows.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2020 00:53:57 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Cheng", "M.", ""], ["Fang", "F.", ""], ["Pain", "C. C.", ""], ["Navon", "I. M.", ""]]}, {"id": "2004.01612", "submitter": "Iryna Kulchytska-Ruchka", "authors": "Iryna Kulchytska-Ruchka and Sebastian Sch\\\"ops", "title": "Towards a Parallel-in-Time Calculation of Time-Periodic Solutions with\n  Unknown Period", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel parallel-in-time algorithm able to compute\ntime-periodic solutions of problems where the period is not given. Exploiting\nthe idea of the multiple shooting method, the proposed approach calculates the\ninitial values at each subinterval as well as the corresponding period\niteratively. As in the Parareal method, parallelization in the time domain is\nperformed using discretization on a two-level grid. A special linearization of\nthe time-periodic system on the coarse grid is introduced to speed up the\ncomputations. The iterative algorithm is verified via its application to the\nColpitt oscillator model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 15:00:28 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 12:23:57 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kulchytska-Ruchka", "Iryna", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "2004.02003", "submitter": "Sudhanshu Sane", "authors": "Sudhanshu Sane, Abhishek Yenpure, Roxana Bujack, Matthew Larsen,\n  Kenneth Moreland, Christoph Garth and Hank Childs", "title": "Scalable In Situ Lagrangian Flow Map Extraction: Demonstrating the\n  Viability of a Communication-Free Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and evaluate a new algorithm for the in situ extraction of\nLagrangian flow maps, which we call Boundary Termination Optimization (BTO).\nOur approach is a communication-free model, requiring no message passing or\nsynchronization between processes, improving scalability, thereby reducing\noverall execution time and alleviating the encumbrance placed on simulation\ncodes from in situ processing. We terminate particle integration at node\nboundaries and store only a subset of the flow map that would have been\nextracted by communicating particles across nodes, thus introducing an\naccuracy-performance tradeoff. We run experiments with as many as 2048 GPUs and\nwith multiple simulation data sets. For the experiment configurations we\nconsider, our findings demonstrate that our communication-free technique saves\nas much as 2x to 4x in execution time in situ, while staying nearly as accurate\nquantitatively and qualitatively as previous work. Most significantly, this\nstudy establishes the viability of approaching in situ Lagrangian flow map\nextraction using communication-free models in the future.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 19:21:28 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Sane", "Sudhanshu", ""], ["Yenpure", "Abhishek", ""], ["Bujack", "Roxana", ""], ["Larsen", "Matthew", ""], ["Moreland", "Kenneth", ""], ["Garth", "Christoph", ""], ["Childs", "Hank", ""]]}, {"id": "2004.02037", "submitter": "Elif Ecem Bas", "authors": "Elif Ecem Bas, Mohamed A. Moustafa, David Feil-Seifer, Janelle\n  Blankenburg", "title": "Using Machine Learning Approach for Computational Substructure in\n  Real-Time Hybrid Simulation", "comments": "10 Pages, 12 Figures, IMAC 38i Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid simulation (HS) is a widely used structural testing method that\ncombines a computational substructure with a numerical model for\nwell-understood components and an experimental substructure for other parts of\nthe structure that are physically tested. One challenge for fast HS or\nreal-time HS (RTHS) is associated with the analytical substructures of\nrelatively complex structures, which could have large number of degrees of\nfreedoms (DOFs), for instance. These large DOFs computations could be hard to\nperform in real-time, even with the all current hardware capacities. In this\nstudy, a metamodeling technique is proposed to represent the structural dynamic\nbehavior of the analytical substructure. A preliminary study is conducted where\na one-bay one-story concentrically braced frame (CBF) is tested under\nearthquake loading by using a compact HS setup at the University of Nevada,\nReno. The experimental setup allows for using a small-scale brace as the\nexperimental substructure combined with a steel frame at the prototype\nfull-scale for the analytical substructure. Two different machine learning\nalgorithms are evaluated to provide a valid and useful metamodeling solution\nfor analytical substructure. The metamodels are trained with the available data\nthat is obtained from the pure analytical solution of the prototype steel\nframe. The two algorithms used for developing the metamodels are: (1) linear\nregression (LR) model, and (2) basic recurrent neural network (RNN). The\nmetamodels are first validated against the pure analytical response of the\nstructure. Next, RTHS experiments are conducted by using metamodels. RTHS test\nresults using both LR and RNN models are evaluated, and the advantages and\ndisadvantages of these models are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 22:22:40 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Bas", "Elif Ecem", ""], ["Moustafa", "Mohamed A.", ""], ["Feil-Seifer", "David", ""], ["Blankenburg", "Janelle", ""]]}, {"id": "2004.02503", "submitter": "Robert Eggersmann", "authors": "Robert Eggersmann, Laurent Stainier, Michael Ortiz, Stefanie Reese", "title": "Model-free Data-Driven Computational Mechanics Enhanced by Tensor Voting", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.113499", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data-driven computing paradigm initially introduced by Kirchdoerfer &\nOrtiz (2016) is extended by incorporating locally linear tangent spaces into\nthe data set. These tangent spaces are constructed by means of the tensor\nvoting method introduced by Mordohai & Medioni (2010) which improves the\nlearning of the underlying structure of a data set. Tensor voting is an\ninstance-based machine learning technique which accumulates votes from the\nnearest neighbors to build up second-order tensors encoding tangents and\nnormals to the underlying data structure. The here proposed second-order\ndata-driven paradigm is a plug-in method for distance-minimizing as well as\nentropy-maximizing data-driven schemes. Like its predecessor, the resulting\nmethod aims to minimize a suitably defined free energy over phase space subject\nto compatibility and equilibrium constraints. The method's implementation is\nstraightforward and numerically efficient since the data structure analysis is\nperformed in an offline step. Selected numerical examples are presented that\nestablish the higher-order convergence properties of the data-driven solvers\nenhanced by tensor voting for ideal and noisy data sets.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 09:18:24 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 09:59:36 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Eggersmann", "Robert", ""], ["Stainier", "Laurent", ""], ["Ortiz", "Michael", ""], ["Reese", "Stefanie", ""]]}, {"id": "2004.02586", "submitter": "Pablo Hern\\'andez-Becerro", "authors": "Pablo Hern\\'andez-Becerro, Daniel Spescha and Konrad Wegener", "title": "Model order reduction of thermo-mechanical models with parametric\n  convective boundary conditions: focus on machine tool", "comments": null, "journal-ref": null, "doi": "10.1007/s00466-020-01926-x", "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a parametric Model Order Reduction (MOR) method for\nweakly coupled thermo-mechanical Finite Element (FE) models of machine tools\nand other similar mechatronic systems. This work proposes a reduction method,\nKrylov Modal Subspace (KMS), and a theoretical bound of the reduction error.\nThe developed method addresses the parametric dependency of the convective\nboundary conditions using the concept of system bilinearization. Additionally,\nthis paper investigates the coupling between the reduced-order thermal system\nand the mechanical response. A numerical example shows that the reduced-order\nmodel captures the response of the original system in the frequency range of\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 11:00:26 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Hern\u00e1ndez-Becerro", "Pablo", ""], ["Spescha", "Daniel", ""], ["Wegener", "Konrad", ""]]}, {"id": "2004.02587", "submitter": "Ryszard Piasecki dr hab. prof. UO", "authors": "R. Piasecki, W. Olchawa, D. Fr\\k{a}czek, A. Bartecka", "title": "A Two-Stage Reconstruction of Microstructures with Arbitrarily Shaped\n  Inclusions", "comments": "extended, published version with 7 figures, Supplementary Materials\n  are at the end of the main manuscript", "journal-ref": "Materials 2020, 13, 2748", "doi": "10.3390/ma13122748", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of our research is to develop an effective method with a wide\nrange of applications for the statistical reconstruction of heterogeneous\nmicrostructures with compact inclusions of any shape, such as highly irregular\ngrains. The devised approach uses multi-scale extended entropic descriptors\n(ED) that quantify the degree of spatial non-uniformity of configurations of\nfinite-sized objects. This technique is an innovative development of previously\nelaborated entropy methods for statistical reconstruction. Here, we discuss the\ntwo-dimensional case, but this method can be generalized into three dimensions.\nAt the first stage, the developed procedure creates a set of black synthetic\nclusters that serve as surrogate inclusions. The clusters have the same\nindividual areas and interfaces as their target counterparts, but random\nshapes. Then, from a given number of easy-to-generate synthetic cluster\nconfigurations, we choose the one with the lowest value of the cost function\ndefined by us using extended ED. At the second stage, we make a significant\nchange in the standard technique of simulated annealing (SA). Instead of\nswapping pixels of different phases, we randomly move each of the selected\nsynthetic clusters. To demonstrate the accuracy of the method, we reconstruct\nand analyze two-phase microstructures with irregular inclusions of silica in\nrubber matrix as well as stones in cement paste. The results show that the\ntwo-stage reconstruction (TSR) method provides convincing realizations for\nthese complex microstructures. The advantages of TSR include the ease of\nobtaining synthetic microstructures, very low computational costs, and\nsatisfactory mapping in the statistical context of inclusion shapes. Finally,\nits simplicity should greatly facilitate independent applications.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 19:16:27 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 22:21:39 GMT"}, {"version": "v3", "created": "Thu, 9 Jul 2020 23:31:22 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Piasecki", "R.", ""], ["Olchawa", "W.", ""], ["Fr\u0105czek", "D.", ""], ["Bartecka", "A.", ""]]}, {"id": "2004.02609", "submitter": "Mingyu Wang", "authors": "Mingyu Wang, Cheng Qian, Jacob K. White, and Abdulkadir C. Yucel", "title": "VoxCap: FFT-Accelerated and Tucker-Enhanced Capacitance Extraction\n  Simulator for Voxelized Structures", "comments": null, "journal-ref": null, "doi": "10.1109/TMTT.2020.3022091", "report-no": null, "categories": "cs.CE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VoxCap, a fast Fourier transform (FFT)-accelerated and Tucker-enhanced\nintegral equation simulator for capacitance extraction of voxelized structures,\nis proposed. The VoxCap solves the surface integral equations (SIEs) for\nconductor and dielectric surfaces with three key attributes that make the\nVoxCap highly CPU and memory efficient for the capacitance extraction of the\nvoxelized structures: (i) VoxCap exploits the FFTs for accelerating the\nmatrix-vector multiplications during the iterative solution of linear system of\nequations arising due to the discretization of SIEs. (ii) During the iterative\nsolution, VoxCap uses a highly effective and memory-efficient preconditioner\nthat reduces the number of iterations significantly. (iii) VoxCap employs\nTucker decompositions to compress the block Toeplitz and circulant tensors,\nrequiring the largest memory in the simulator. By doing so, it reduces the\nmemory requirement of these tensors from hundreds of gigabytes to a few\nmegabytes and the CPU time required to obtain Toeplitz tensors from tens of\nminutes (even hours) to a few seconds for very large scale problems. VoxCap is\ncapable of accurately computing capacitance of arbitrarily shaped and\nlarge-scale voxelized structures on a desktop computer.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 05:57:34 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 00:46:30 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Wang", "Mingyu", ""], ["Qian", "Cheng", ""], ["White", "Jacob K.", ""], ["Yucel", "Abdulkadir C.", ""]]}, {"id": "2004.02975", "submitter": "Ramiro dell'Erba", "authors": "R. dell'Erba", "title": "Swarm robotics and complex behaviour of continuum material", "comments": "arXiv admin note: text overlap with arXiv:2003.11908", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In swarm robotics, just as for an animal swarm in Nature, one of the aims is\nto reach and maintain a desired configuration. One of the possibilities for the\nteam, to reach this aim, is to see what its neighbours are doing. This approach\ngenerates a rules system governing the movement of the single robot just by\nreference to neighbour's motion. The same approach is used in position based\ndynamics to simulate behaviour of complex continuum materials under\ndeformation. Therefore, in some previous works, we have considered a\ntwo-dimensional lattice of particles and calculated its time evolution by using\na rules system derived from our experience in swarm robotics. The new position\nof a particle, like the element of a swarm, is determined by the spatial\nposition of the other particles. No dynamic is considered, but it can be\nthought as being hidden in the behaviour rules. This method has given good\nresults in some simple situations reproducing the behaviour of deformable\nbodies under imposed strain. In this paper we try to stress our model to\nhighlight its limits and how they can be improved. Some other, more complex,\nexamples are computed and discussed. Shear test, different lattice, different\nfracture mechanism and ASTM shape sample behaviour have been investigated by\nthe software tool we have developed.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 08:36:34 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["dell'Erba", "R.", ""]]}, {"id": "2004.03348", "submitter": "Andrea Bacigalupo Dr", "authors": "Andrea Bacigalupo, Luigi Gambarotta", "title": "Identification of non-local continua for lattice-like materials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.app-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is focused on the dynamic homogenization of lattice-like materials\nwith lumped mass at the nodes to obtain energetically consistent models\nproviding accurate descriptions of the acoustic behavior of the discrete\nsystem. The equation of motion of the lattice is transformed according to a\nunitary approach aimed to identify equivalent non-local continuum models of\nintegral-differential and gradient type, the latter obtained through standard\nor enhanced continualization. The bilateral Z-transform of the difference\nequation of motion is matched to the governing integral-differential equation\nof the equivalent continuum in the transformed Fourier space, which has the\nsame frequency band structure as the Lagrangian one. Firstly, it is shown that\nthe approximation of the kernels via Taylor polynomials leads to the\ndifferential field equations of higher order continua endowed with non-local\nconstitutive terms. The field equations derived from such approach corresponds\nto the ones obtained through the so called standard continualization. However,\nthe differential problem turns out to be ill-posed because the non-positive\ndefiniteness of the potential energy density of the higher order continuum.\nEnergetically consistent equivalent continua have been identified through a\nproper mapping correlating the transformed macro-displacements in the Fourier\nspace with a new auxiliary regularizing continuum macro-displacement field in\nthe same space. Specifically, the mapping here introduced has zeros at the edge\nof the first Brillouin zone. The integral-differential governing equation and\nthe corresponding differential one has been reformulated through an enhanced\ncontinualization that is characterized by energetically consistent differential\nequations. The constitutive and inertial kernels of the integral-differential\nequation exhibit polar singularities at the edge of the first Brillouin zone.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 12:55:45 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bacigalupo", "Andrea", ""], ["Gambarotta", "Luigi", ""]]}, {"id": "2004.03442", "submitter": "Nicolo Pollini", "authors": "Nicol\\`o Pollini", "title": "Fail-safe optimization of viscous dampers for seismic retrofitting", "comments": null, "journal-ref": "Earthquake Engineering & Structural Dynamics 2020", "doi": "10.1002/eqe.3319", "report-no": null, "categories": "cs.CE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new optimization approach for designing minimum-cost\nfail-safe distributions of fluid viscous dampers for seismic retrofitting.\nFailure is modeled as either complete damage of the dampers or partial\ndegradation of the dampers' properties. In general, this leads to optimization\nproblems with large number of constraints. Thus, the use of a working-set\noptimization algorithm is proposed. The main idea is to solve a sequence of\nrelaxed optimization sub-problems with a small sub-set of all constraints. The\nalgorithm terminates once a solution of a sub-problem is found that satisfies\nall the constraints of the problem. The retrofitting cost is minimized with\nconstraints on the inter-story drifts at the peripheries of frame structures.\nThe structures considered are subjected to a realistic ensemble of ground\nmotions, and their response is evaluated with time-history analyses. The\ntransient optimization problem is efficiently solved with a gradient-based\nsequential linear programming algorithm. The gradients of the response\nfunctions are calculated with a consistent adjoint sensitivity analysis\nprocedure. Promising results attained for 3-D irregular frames are presented\nand discussed. The numerical results highlight the fact that the optimized\nlayout and size of the dampers can change significantly even for moderate\nlevels of damage.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:46:04 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Pollini", "Nicol\u00f2", ""]]}, {"id": "2004.03454", "submitter": "Maxwell Xu Cai", "authors": "Caspar van Leeuwen, Damian Podareanu, Valeriu Codreanu, Maxwell X.\n  Cai, Axel Berg, Simon Portegies Zwart, Robin Stoffer, Menno Veerman, Chiel\n  van Heerwaarden, Sydney Otten, Sascha Caron, Cunliang Geng, Francesco\n  Ambrosetti, Alexandre M.J.J. Bonvin", "title": "Deep-learning enhancement of large scale numerical simulations", "comments": "White paper consists of 36 pages and 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional simulations on High-Performance Computing (HPC) systems typically\ninvolve modeling very large domains and/or very complex equations. HPC systems\nallow running large models, but limits in performance increase that have become\nmore prominent in the last 5-10 years will likely be experienced. Therefore new\napproaches are needed to increase application performance. Deep learning\nappears to be a promising way to achieve this. Recently deep learning has been\nemployed to enhance solving problems that traditionally are solved with\nlarge-scale numerical simulations using HPC. This type of application, deep\nlearning for high-performance computing, is the theme of this whitepaper. Our\ngoal is to provide concrete guidelines to scientists and others that would like\nto explore opportunities for applying deep learning approaches in their own\nlarge-scale numerical simulations. These guidelines have been extracted from a\nnumber of experiments that have been undertaken in various scientific domains\nover the last two years, and which are described in more detail in the\nAppendix. Additionally, we share the most important lessons that we have\nlearned.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 13:12:02 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["van Leeuwen", "Caspar", ""], ["Podareanu", "Damian", ""], ["Codreanu", "Valeriu", ""], ["Cai", "Maxwell X.", ""], ["Berg", "Axel", ""], ["Zwart", "Simon Portegies", ""], ["Stoffer", "Robin", ""], ["Veerman", "Menno", ""], ["van Heerwaarden", "Chiel", ""], ["Otten", "Sydney", ""], ["Caron", "Sascha", ""], ["Geng", "Cunliang", ""], ["Ambrosetti", "Francesco", ""], ["Bonvin", "Alexandre M. J. J.", ""]]}, {"id": "2004.04663", "submitter": "Bertrand Iooss", "authors": "A. Marrel (CEA-DES (ex-DEN)), Bertrand Iooss (IMT, EDF R&D PRISME, GdR\n  MASCOT-NUM), V Chabridon (EDF R&D PRISME)", "title": "Statistical identification of penalizing configurations in\n  high-dimensional thermalhydraulic numerical experiments: The ICSCREAM\n  methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of risk assessment in nuclear accident analysis,\nbest-estimate computer codes are used to estimate safety margins. Several\ninputs of the code can be uncertain, due to a lack of knowledge but also to the\nparticular choice of accidental scenario being considered. The objective of\nthis work is to identify the most penalizing (or critical) configurations\n(corresponding to extreme values of the code output) of several input\nparameters (called \"scenario inputs\"), independently of the uncertainty of the\nother input parameters. However, complex computer codes, as the ones used in\nthermal-hydraulic accident scenario simulations, are often too CPU-time\nexpensive to be directly used to perform these studies. A solution consists in\nfitting the code output by a metamodel, built from a reduced number of code\nsimulations. When the number of input parameters is very large (e.g., around a\nhundred here), the metamodel building remains a challenge. To overcome this, we\npropose a methodology, called ICSCREAM (Identification of penalizing\nConfigurations using SCREening And Metamodel), based on screening techniques\nand Gaussian process (Gp) metamodeling. The efficiency of this methodology is\nillustrated on a thermal-hydraulic industrial case simulating an accident of\nprimary coolant loss in a Pressurized Water Reactor. This use-case includes 97\nuncertain inputs, two scenario inputs to be penalized and 500 code simulations\nfor the learning database. The study focuses on the peak cladding temperature\n(PCT) and critical configurations are defined by exceeding the 90%-quantile of\nPCT.For the screening step, statistical tests of independence based on the\nHilbert-Schmidt independence criterion are used for global and target\nsensitivity analyses. They allow a significant reduction of inputs (from 97 to\n20) and a ranking of these influential inputs by order of influence. Then, a Gp\nmetamodel is sequentially built to reach a satisfactory predictivity of 82% of\nexplained PTC variance, and a high capacity of identifying PTC critical areas\n(94% of good ranking rate above the threshold). Finally, the Gp is used to\nestimate, within a Bayesian framework, the conditional probabilities of\nexceeding the threshold, according to the two scenario inputs. The analysis\nreveals the strong interaction of the two scenario inputs in the occurrence of\ncritical configurations, worst cases corresponding to medium values of both\ninputs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 07:01:34 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 08:56:14 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Marrel", "A.", "", "CEA-DES"], ["Iooss", "Bertrand", "", "IMT, EDF R&D PRISME, GdR\n  MASCOT-NUM"], ["Chabridon", "V", "", "EDF R&D PRISME"]]}, {"id": "2004.04844", "submitter": "Hidekazu Yoshioka", "authors": "Hidekazu Yoshioka, Yuta Yaegashi, and Motoh Tsujimura", "title": "A random observation-based management model of population dynamics and\n  its ecological application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new stochastic control problem of population dynamics under partial\nobservation is formulated and analyzed both mathematically and numerically,\nwith an emphasis on environmental and ecological problems. The decision-maker\ncan only randomly and time-discretely observe and impulsively intervene the\npopulation dynamics governed by a regime-switching stochastic differential\nequation. The hybrid nature of the problem leads to an optimality equation\ncontaining an integro-differential equation and a static optimization problem.\nIt is therefore different from the conventional Hamilton-Jacobi-Bellman\nequations. Existence and solvability issues of this optimality equation are\nanalyzed in a viscosity sense. Its exact solution to a reduced but still\nnontrivial model is derived as well. The model is finally applied to a\nrealistic environmental management problem in a river using a finite difference\nscheme.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 23:05:28 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Yoshioka", "Hidekazu", ""], ["Yaegashi", "Yuta", ""], ["Tsujimura", "Motoh", ""]]}, {"id": "2004.05448", "submitter": "Marco Swierstra", "authors": "Marco K. Swierstra, Deepak K. Gupta, Matthijs Langelaar", "title": "Automated and Accurate Geometry Extraction and Shape Optimization of 3D\n  Topology Optimization Results", "comments": "A previous version of this work was presented at the NAFEMS European\n  Conference: Simulation-Based Optimisation (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designs generated by density-based topology optimization (TO) exhibit jagged\nand/or smeared boundaries, which forms an obstacle to their integration with\nexisting CAD tools. Addressing this problem by smoothing or manual design\nadjustments is time-consuming and affects the optimality of TO designs. This\npaper proposes a fully automated procedure to obtain unambiguous, accurate and\noptimized geometries from arbitrary 3D TO results. It consists of a geometry\nextraction stage using a level-set-based design description involving radial\nbasis functions, followed by a shape optimization stage involving local\nanalysis refinements near the structural boundary using the Finite Cell Method.\nWell-defined bounds on basis function weights ensure that sufficient\nsensitivity information is available throughout the shape optimization process.\nOur approach results in highly smooth and accurate optimized geometries, and\nits effectiveness is illustrated by 2D and 3D examples.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 17:29:28 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Swierstra", "Marco K.", ""], ["Gupta", "Deepak K.", ""], ["Langelaar", "Matthijs", ""]]}, {"id": "2004.05461", "submitter": "Keigo Nakamura", "authors": "Keigo Nakamura and Yoshiro Suzuki", "title": "Deep learning-based topological optimization for representing a\n  user-specified design area", "comments": "12 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presently, topology optimization requires multiple iterations to create an\noptimized structure for given conditions. Among the conditions for topology\noptimization,the design area is one of the most important for structural\ndesign. In this study, we propose a new deep learning model to generate an\noptimized structure for a given design domain and other boundary conditions\nwithout iteration. For this purpose, we used open-source topology optimization\nMATLAB code to generate a pair of optimized structures under various design\nconditions. The resolution of the optimized structure is 32 * 32 pixels, and\nthe design conditions are design area, volume fraction, distribution of\nexternal forces, and load value. Our deep learning model is primarily composed\nof a convolutional neural network (CNN)-based encoder and decoder, trained with\ndatasets generated with MATLAB code. In the encoder, we use batch normalization\n(BN) to increase the stability of the CNN model. In the decoder, we use SPADE\n(spatially adaptive denormalization) to reinforce the design area information.\nComparing the performance of our proposed model with a CNN model that does not\nuse BN and SPADE, values for mean absolute error (MAE), mean compliance error,\nand volume error with the optimized topology structure generated in MAT-LAB\ncode were smaller, and the proposed model was able to represent the design area\nmore precisely. The proposed method generates near-optimal structures\nreflecting the design area in less computational time, compared with the\nopen-source topology optimization MATLAB code.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 18:54:07 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 13:44:20 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Nakamura", "Keigo", ""], ["Suzuki", "Yoshiro", ""]]}, {"id": "2004.06220", "submitter": "Rebecca E. Morrison", "authors": "Rebecca E. Morrison, Americo Cunha Jr", "title": "Embedded model discrepancy: A case study of Zika modeling", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": "10.1063/5.0005204", "report-no": null, "categories": "q-bio.PE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical models of epidemiological systems enable investigation of and\npredictions about potential disease outbreaks. However, commonly used models\nare often highly simplified representations of incredibly complex systems.\nBecause of these simplifications, the model output, of say new cases of a\ndisease over time, or when an epidemic will occur, may be inconsistent with\navailable data. In this case, we must improve the model, especially if we plan\nto make decisions based on it that could affect human health and safety, but\ndirect improvements are often beyond our reach. In this work, we explore this\nproblem through a case study of the Zika outbreak in Brazil in 2016. We propose\nan embedded discrepancy operator---a modification to the model equations that\nrequires modest information about the system and is calibrated by all relevant\ndata. We show that the new enriched model demonstrates greatly increased\nconsistency with real data. Moreover, the method is general enough to easily\napply to many other mathematical models in epidemiology.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 22:12:10 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Morrison", "Rebecca E.", ""], ["Cunha", "Americo", "Jr"]]}, {"id": "2004.06487", "submitter": "Anh Tran", "authors": "Anh Tran, Jing Sun, Dehao Liu, Tim Wildey, Yan Wang", "title": "Multiscale stochastic reduced-order model for uncertainty propagation\n  using Fokker-Planck equation with microstructure evolution applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty involved in computational materials modeling needs to be\nquantified to enhance the credibility of predictions. Tracking the propagation\nof model-form and parameter uncertainty for each simulation step, however, is\ncomputationally expensive. In this paper, a multiscale stochastic reduced-order\nmodel (ROM) is proposed to propagate the uncertainty as a stochastic process\nwith Gaussian noise. The quantity of interest (QoI) is modeled by a non-linear\nLangevin equation, where its associated probability density function is\npropagated using Fokker-Planck equation. The drift and diffusion coefficients\nof the Fokker-Planck equation are trained and tested from the time-series\ndataset obtained from direct numerical simulations. Considering microstructure\ndescriptors in the microstructure evolution as QoIs, we demonstrate our\nproposed methodology in three integrated computational materials engineering\n(ICME) models: kinetic Monte Carlo, phase field, and molecular dynamics\nsimulations. It is demonstrated that once calibrated correctly using the\navailable time-series datasets from these ICME models, the proposed ROM is\ncapable of propagating the microstructure descriptors dynamically, and the\nresults agree well with the ICME models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 23:13:35 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Tran", "Anh", ""], ["Sun", "Jing", ""], ["Liu", "Dehao", ""], ["Wildey", "Tim", ""], ["Wang", "Yan", ""]]}, {"id": "2004.06803", "submitter": "Xihaier Luo", "authors": "Chao Yin and Xihaier Luo and Ahsan Kareem", "title": "Probabilistic Evolution of Stochastic Dynamical Systems: A Meso-scale\n  Perspective", "comments": "33 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic dynamical systems arise naturally across nearly all areas of\nscience and engineering. Typically, a dynamical system model is based on some\nprior knowledge about the underlying dynamics of interest in which\nprobabilistic features are used to quantify and propagate uncertainties\nassociated with the initial conditions, external excitations, etc. From a\nprobabilistic modeling standing point, two broad classes of methods exist, i.e.\nmacro-scale methods and micro-scale methods. Classically, macro-scale methods\nsuch as statistical moments-based strategies are usually too coarse to capture\nthe multi-mode shape or tails of a non-Gaussian distribution. Micro-scale\nmethods such as random samples-based approaches, on the other hand, become\ncomputationally very challenging in dealing with high-dimensional stochastic\nsystems. In view of these potential limitations, a meso-scale scheme is\nproposed here that utilizes a meso-scale statistical structure to describe the\ndynamical evolution from a probabilistic perspective. The significance of this\nstatistical structure is two-fold. First, it can be tailored to any arbitrary\nrandom space. Second, it not only maintains the probability evolution around\nsample trajectories but also requires fewer meso-scale components than the\nmicro-scale samples. To demonstrate the efficacy of the proposed meso-scale\nscheme, a set of examples of increasing complexity are provided. Connections to\nthe benchmark stochastic models as conservative and Markov models along with\npractical implementation guidelines are presented.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 19:17:22 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Yin", "Chao", ""], ["Luo", "Xihaier", ""], ["Kareem", "Ahsan", ""]]}, {"id": "2004.07002", "submitter": "Marek Tyburec", "authors": "Marek Tyburec, Jan Zeman, Martin Do\\v{s}k\\'a\\v{r}, Martin\n  Kru\\v{z}\\'ik, and Mat\\v{e}j Lep\\v{s}", "title": "Modular-topology optimization with Wang tilings: An application to truss\n  structures", "comments": "17 pages, 16 figures", "journal-ref": "Structural and Multidisciplinary Optimization volume 63,\n  pages1099-1117 (2021)", "doi": "10.1007/s00158-020-02744-8", "report-no": null, "categories": "cs.CE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity is appealing for solving many problems in optimization. It brings\nthe benefits of manufacturability and reconfigurability to structural\noptimization, and enables a trade-off between the computational performance of\na Periodic Unit Cell (PUC) and the efficacy of non-uniform designs in\nmulti-scale material optimization. Here, we introduce a novel strategy for\nconcurrent minimum-compliance design of truss modules topologies and their\nmacroscopic assembly encoded using Wang tiling, a formalism providing\nindependent control over the number of modules and their interfaces. We tackle\nthe emerging bilevel optimization problem with a combination of meta-heuristics\nand mathematical programming. At the upper level, we employ a genetic algorithm\nto optimize module assemblies. For each assembly, we obtain optimal module\ntopologies as a solution to a convex second-order conic program that exploits\nthe underlying modularity, incorporating stress constraints, multiple load\ncases, and reuse of module(s) for various structures. Merits of the proposed\nstrategy are illustrated with three representative examples, clearly\ndemonstrating that the best designs obtained by our method exhibited decreased\ncompliance: from 56% to 69% compared to the PUC designs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 11:06:37 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Tyburec", "Marek", ""], ["Zeman", "Jan", ""], ["Do\u0161k\u00e1\u0159", "Martin", ""], ["Kru\u017e\u00edk", "Martin", ""], ["Lep\u0161", "Mat\u011bj", ""]]}, {"id": "2004.07389", "submitter": "Rafael Orozco", "authors": "Ziyi Yin, Rafael Orozco, Philipp Witte, Mathias Louboutin, Gabrio\n  Rizzuti, and Felix J. Herrmann", "title": "Extended source imaging, a unifying framework for seismic & medical\n  imaging", "comments": "Submitted to the Society of Exploration Geophysicists Annual Meeting\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CE eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present three imaging modalities that live on the crossroads of seismic\nand medical imaging. Through the lens of extended source imaging, we can draw\ndeep connections among the fields of wave-equation based seismic and medical\nimaging, despite first appearances. From the seismic perspective, we underline\nthe importance to work with the correct physics and spatially varying velocity\nfields. Medical imaging, on the other hand, opens the possibility for new\nimaging modalities where outside stimuli, such as laser or radar pulses, can\nnot only be used to identify endogenous optical or thermal contrasts but that\nthese sources can also be used to insonify the medium so that images of the\nwhole specimen can in principle be created.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 23:16:29 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Yin", "Ziyi", ""], ["Orozco", "Rafael", ""], ["Witte", "Philipp", ""], ["Louboutin", "Mathias", ""], ["Rizzuti", "Gabrio", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "2004.07444", "submitter": "Oliver Serang", "authors": "Patrick Kreitzberg, Jake Pennington, Kyle Lucke, Oliver Serang", "title": "Fast exact computation of the $k$ most abundant isotope peaks with\n  layer-ordered heaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical computation of isotopic distribution of compounds is crucial\nin many important applications of mass spectrometry, especially as machine\nprecision grows. A considerable amount of good tools have been created in the\nlast decade for doing so. In this paper we present a novel algorithm for\ncalculating the top $k$ peaks of a given compound. The algorithm takes\nadvantage of layer-ordered heaps used in an optimal method of selection on\n$X+Y$ and is able to efficiently calculate the top $k$ peaks on very large\nmolecules. Among its peers, this algorithm shows a significant speedup on\nmolecules whose elements have many isotopes. The algorithm obtains a speedup of\nmore than 31x when compared to $\\textsc{IsoSpec}$ on \\ch{Au2Ca10Ga10Pd76} when\ncomputing 47409787 peaks, which covers 0.999 of the total abundance.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 03:55:30 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Kreitzberg", "Patrick", ""], ["Pennington", "Jake", ""], ["Lucke", "Kyle", ""], ["Serang", "Oliver", ""]]}, {"id": "2004.08417", "submitter": "Arpan Mukherjee", "authors": "Arpan Mukherjee, Anna Kuechle Szweda, Andrew Alegria, Rahul Rai and\n  Tarunraj Singh", "title": "Identifying Weakly Connected Subsystems in Building Energy Model for\n  Effective Load Estimation in Presence of Parametric Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is necessary to estimate the expected energy usage of a building to\ndetermine how to reduce energy usage. The expected energy usage of a building\ncan be reliably simulated using a Building Energy Model (BEM). Many of the\nnumerous input parameters in a BEM are uncertain. To ensure that the building\nsimulation is sufficiently accurate, and to better understand the impact of\nimprecisions in the input parameters and calculation methods, it is desirable\nto quantify uncertainty in the BEM throughout the modeling process. Uncertainty\nquantification (UQ) typically requires a large number of simulations to produce\nmeaningful data, which, due to the vast number of input parameters and the\ndynamic nature of building simulation, is computationally expensive.\nUncertainty Quantification (UQ) in BEM domain is thus intractable due to the\nsize of the problem and parameters involved and hence it needs an advanced\nmethodology for analysis. The current paper outlines a novel\nWeakly-Connected-Systems (WCSs) identification-based UQ framework developed to\npropagate the quantifiable uncertainty in the BEM. The overall approach is\ndemonstrated on the physics-based thermal model of an actual building in\nCentral New York.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 18:16:36 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Mukherjee", "Arpan", ""], ["Szweda", "Anna Kuechle", ""], ["Alegria", "Andrew", ""], ["Rai", "Rahul", ""], ["Singh", "Tarunraj", ""]]}, {"id": "2004.08466", "submitter": "Joaquim Dias Garcia", "authors": "Igor Carvalho, Tiago Andrade, Joaquim Dias Garcia and Maria de Lujan\n  Latorre", "title": "Application of Progressive Hedging to Var Expansion Planning Under\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the application of a Progressive Hedging (PH) algorithm\nto the least-cost var planning under uncertainty. The method PH is a\nscenario-based decomposition technique for solving stochastic programs, i.e.,\nit decomposes a large scale stochastic problem into s deterministic subproblems\nand couples the decision from the s subproblems to form a solution for the\noriginal stochastic problem. The effectiveness and computational performance of\nthe proposed methodology will be illustrated with var planning studies for the\nIEEE 24-bus system (5 operating scenarios), the 200-bus Bolivian system (1,152\noperating scenarios) and the 1,600-bus Colombian system (180 scenarios).\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 21:54:36 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Carvalho", "Igor", ""], ["Andrade", "Tiago", ""], ["Garcia", "Joaquim Dias", ""], ["Latorre", "Maria de Lujan", ""]]}, {"id": "2004.09211", "submitter": "Yoann Altmann", "authors": "Quentin Legros and Julian Tachella and Rachael Tobin and Aongus\n  McCarthy and Sylvain Meignen and Gerald S. Buller and Yoann Altmann and\n  Stephen McLaughlin and Michael E. Davies", "title": "Robust 3D reconstruction of dynamic scenes from single-photon lidar\n  using Beta-divergences", "comments": "12 pages", "journal-ref": null, "doi": "10.1109/TIP.2020.3046882", "report-no": null, "categories": "eess.IV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new algorithm for fast, online 3D reconstruction\nof dynamic scenes using times of arrival of photons recorded by single-photon\ndetector arrays. One of the main challenges in 3D imaging using single-photon\nlidar in practical applications is the presence of strong ambient illumination\nwhich corrupts the data and can jeopardize the detection of peaks/surface in\nthe signals. This background noise not only complicates the observation model\nclassically used for 3D reconstruction but also the estimation procedure which\nrequires iterative methods. In this work, we consider a new similarity measure\nfor robust depth estimation, which allows us to use a simple observation model\nand a non-iterative estimation procedure while being robust to\nmis-specification of the background illumination model. This choice leads to a\ncomputationally attractive depth estimation procedure without significant\ndegradation of the reconstruction performance. This new depth estimation\nprocedure is coupled with a spatio-temporal model to capture the natural\ncorrelation between neighboring pixels and successive frames for dynamic scene\nanalysis. The resulting online inference process is scalable and well suited\nfor parallel implementation. The benefits of the proposed method are\ndemonstrated through a series of experiments conducted with simulated and real\nsingle-photon lidar videos, allowing the analysis of dynamic scenes at 325 m\nobserved under extreme ambient illumination conditions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 11:24:31 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 22:08:52 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 10:20:23 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Legros", "Quentin", ""], ["Tachella", "Julian", ""], ["Tobin", "Rachael", ""], ["McCarthy", "Aongus", ""], ["Meignen", "Sylvain", ""], ["Buller", "Gerald S.", ""], ["Altmann", "Yoann", ""], ["McLaughlin", "Stephen", ""], ["Davies", "Michael E.", ""]]}, {"id": "2004.09668", "submitter": "Ren\\'e Schenkendorf", "authors": "Andrea Pozzi and Xiangzhong Xie and Davide M Raimondo and Ren\\'e\n  Schenkendorf", "title": "Global Sensitivity Methods for Design of Experiments in Lithium-ion\n  Battery Context", "comments": "Accepted for 21st IFAC World Congress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Battery management systems may rely on mathematical models to provide higher\nperformance than standard charging protocols. Electrochemical models allow us\nto capture the phenomena occurring inside a lithium-ion cell and therefore,\ncould be the best model choice. However, to be of practical value, they require\nreliable model parameters. Uncertainty quantification and optimal experimental\ndesign concepts are essential tools for identifying systems and estimating\nparameters precisely. Approximation errors in uncertainty quantification result\nin sub-optimal experimental designs and consequently, less-informative data,\nand higher parameter unreliability. In this work, we propose a highly efficient\ndesign of experiment method based on global parameter sensitivities. This novel\nconcept is applied to the single-particle model with electrolyte and thermal\ndynamics (SPMeT), a well-known electrochemical model for lithium-ion cells. The\nproposed method avoids the simplifying assumption of output-parameter\nlinearization (i.e., local parameter sensitivities) used in conventional Fisher\ninformation matrix-based experimental design strategies. Thus, the optimized\ncurrent input profile results in experimental data of higher information\ncontent and in turn, in more precise parameter estimates.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 23:05:43 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 13:52:50 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Pozzi", "Andrea", ""], ["Xie", "Xiangzhong", ""], ["Raimondo", "Davide M", ""], ["Schenkendorf", "Ren\u00e9", ""]]}, {"id": "2004.09963", "submitter": "Nick James", "authors": "Arjun Prakash, Nick James, Max Menzies, Gilad Francis", "title": "Structural clustering of volatility regimes for dynamic trading\n  strategies", "comments": "Expression edits and small methodological changes relative to v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new method to find the number of volatility regimes in a\nnonstationary financial time series by applying unsupervised learning to its\nvolatility structure. We use change point detection to partition a time series\ninto locally stationary segments and then compute a distance matrix between\nsegment distributions. The segments are clustered into a learned number of\ndiscrete volatility regimes via an optimization routine. Using this framework,\nwe determine a volatility clustering structure for financial indices, large-cap\nequities, exchange-traded funds and currency pairs. Our method overcomes the\nrigid assumptions necessary to implement many parametric regime-switching\nmodels, while effectively distilling a time series into several characteristic\nbehaviours. Our results provide significant simplification of these time series\nand a strong descriptive analysis of prior behaviours of volatility. This\nempirical analysis could be used with other regime-switching implementations,\njustifying the parametric structure encoded in any candidate model. Finally, we\ncreate and validate a dynamic trading strategy that learns the optimal match\nbetween the current distribution of a time series and its past regimes, thereby\nmaking online risk-avoidance decisions in the present.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 12:54:23 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 10:54:34 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Prakash", "Arjun", ""], ["James", "Nick", ""], ["Menzies", "Max", ""], ["Francis", "Gilad", ""]]}, {"id": "2004.10081", "submitter": "David Fobes", "authors": "David M Fobes, Sander Claeys, Frederik Geth, Carleton Coffrin", "title": "PowerModelsDistribution.jl: An Open-Source Framework for Exploring\n  Distribution Power Flow Formulations", "comments": null, "journal-ref": "Electric Power Systems Research, 189, 106664 (2020)", "doi": "10.1016/j.epsr.2020.106664", "report-no": null, "categories": "cs.CE cs.SY eess.SP eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce PowerModelsDistribution, a free, open-source\ntoolkit for distribution power network optimization, whose primary focus is\nestablishing a baseline implementation of steady-state multi-conductor\nunbalanced distribution network optimization problems, which includes\nimplementations of Power Flow and Optimal Power Flow problem types. Currently\nimplemented power flow formulations for these problem types include AC (polar\nand rectangular), a second-order conic relaxation of the Branch Flow Model\n(BFM) and Bus Injection Model (BIM), a semi-definite relaxation of BFM, and\nseveral linear approximations, such as the simplified unbalanced BFM. The\nresults of AC power flow have been validated against OpenDSS, an open-source\n\"electric power distribution system simulator\", using IEEE distribution test\nfeeders (13, 34, 123 bus and LVTestCase), all parsed using a built-in OpenDSS\nparser. This includes support for standard distribution system components as\nwell as novel resource models such as generic energy storage (multi-period) and\nphotovoltaic systems, with the intention to add support for additional\ncomponents in the future.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 13:35:38 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Fobes", "David M", ""], ["Claeys", "Sander", ""], ["Geth", "Frederik", ""], ["Coffrin", "Carleton", ""]]}, {"id": "2004.10560", "submitter": "Kartikay Gupta Mr", "authors": "Kartikay Gupta and Niladri Chatterjee", "title": "Examining Lead-Lag Relationships In-Depth, With Focus On FX Market As\n  Covid-19 Crises Unfolds", "comments": "Suggestions are welcome. In the second version, a citation has been\n  updated on request from the corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lead-lag relationship plays a vital role in financial markets. It is the\nphenomenon where a certain price-series lags behind and partially replicates\nthe movement of leading time-series. The present research proposes a new\ntechnique which helps better identify the lead-lag relationship empirically.\nApart from better identifying the lead-lag path, the technique also gives a\nmeasure for adjudging closeness between financial time-series. Also, the\nproposed measure is closely related to correlation, and it uses Dynamic\nProgramming technique for finding the optimal lead-lag path. Further, it\nretains most of the properties of a metric, so much so, it is termed as loose\nmetric. Tests are performed on Synthetic Time Series (STS) with known lead-lag\nrelationship and comparisons are done with other state-of-the-art models on the\nbasis of significance and forecastability. The proposed technique gives the\nbest results in both the tests. It finds paths which are all statistically\nsignificant, and its forecasts are closest to the target values. Then, we use\nthe measure to study the topology evolution of the Foreign Exchange market, as\nthe COVID-19 pandemic unfolds. Here, we study the FX currency prices of 29\nprominent countries of the world. It is observed that as the crises unfold, all\nthe currencies become strongly interlinked to each other. Also, USA Dollar\nstarts playing even more central role in the FX market. Finally, we mention\nseveral other application areas of the proposed technique for designing\nintelligent systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 13:24:31 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 17:41:00 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Gupta", "Kartikay", ""], ["Chatterjee", "Niladri", ""]]}, {"id": "2004.10584", "submitter": "Guglielmo Scovazzi", "authors": "Nabil M. Atallah, Claudio Canuto, Guglielmo Scovazzi", "title": "The Second-Generation Shifted Boundary Method and Its Numerical Analysis", "comments": "28 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the Shifted Boundary Method (SBM) was proposed within the class of\nunfitted (or immersed, or embedded) finite element methods. By reformulating\nthe original boundary value problem over a surrogate (approximate)\ncomputational domain, the SBM avoids integration over cut cells and the\nassociated problematic issues regarding numerical stability and matrix\nconditioning. Accuracy is maintained by modifying the original boundary\nconditions using Taylor expansions. Hence the name of the method, that {\\it\nshifts} the location and values of the boundary conditions. In this article, we\npresent enhanced variational SBM formulations for the Poisson and Stokes\nproblems with improved flexibility and robustness. These simplified variational\nforms allow to relax some of the assumptions required by the mathematical\nproofs of stability and convergence of earlier implementations. First, we show\nthat these new SBM implementations can be proved asymptotically stable and\nconvergent even without the rather restrictive assumption that the inner\nproduct between the normals to the true and surrogate boundaries is positive.\nSecond, we show that it is not necessary to introduce a stabilization term\ninvolving the tangential derivatives of the solution at Dirichlet boundaries,\ntherefore avoiding the calibration of an additional stabilization parameter.\nFinally, we prove enhanced $L^{2}$-estimates without the cumbersome assumption\n- of earlier proofs - that the surrogate domain is convex. Instead we rely on a\nconventional assumption that the boundary of the true domain is smooth, which\ncan also be replaced by requiring convexity of the true domain. The\naforementioned improvements open the way to a more general and efficient\nimplementation of the Shifted Boundary Method, particularly in complex\nthree-dimensional geometries. We present numerical experiments in two and three\ndimensions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 14:03:25 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Atallah", "Nabil M.", ""], ["Canuto", "Claudio", ""], ["Scovazzi", "Guglielmo", ""]]}, {"id": "2004.10712", "submitter": "Alain Bastide", "authors": "Rakotobe Micha\\\"el, Ramalingom Delphine, Cocquet Pierre-Henri, and\n  Bastide Alain", "title": "Modelling of flow through spatially varying porous media with\n  application to topology optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.CE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this study is to highlight the effect of porosity variation\nin a topology optimization process in the field of fluid dynamics. Usually a\npenalization term added to momentum equation provides to get material\ndistribution. Every time material is added inside the computational domain,\nthere is creation of new fluid-solid interfaces and apparition of gradient of\nporosity. However, at present, porosity variation is not taken account in\ntopology optimization and the penalization term used to locate the solid is\nanalogous to a Darcy term used for flows in porous media. With that in mind, in\nthis paper, we first develop an original one-domain macroscopic model for the\nmodelling of flow through spatially varying porous media that goes beyond the\nscope of Darcy regime. Next, we numerically solve a topology optimization\nproblem and compare the results obtained with the standard model that does not\ninclude effect of porosity variation with those obtained with our model. Among\nour results, we show for instance that the designs obtained are different but\npercentages of reduction of objective functional remain quite close (below 4\\%\nof difference). In addition, we illustrate effects of porosity and particle\ndiameter values on final optimized designs.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:16:32 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Micha\u00ebl", "Rakotobe", ""], ["Delphine", "Ramalingom", ""], ["Pierre-Henri", "Cocquet", ""], ["Alain", "Bastide", ""]]}, {"id": "2004.11121", "submitter": "Takahiro Yabe", "authors": "Takahiro Yabe, Yunchang Zhang, Satish Ukkusuri", "title": "Quantifying the Economic Impact of Extreme Shocks on Businesses using\n  Human Mobility Data: a Bayesian Causal Inference Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, extreme shocks, such as natural disasters, are increasing in\nboth frequency and intensity, causing significant economic loss to many cities\naround the world. Quantifying the economic cost of local businesses after\nextreme shocks is important for post-disaster assessment and pre-disaster\nplanning. Conventionally, surveys have been the primary source of data used to\nquantify damages inflicted on businesses by disasters. However, surveys often\nsuffer from high cost and long time for implementation, spatio-temporal\nsparsity in observations, and limitations in scalability. Recently, large scale\nhuman mobility data (e.g. mobile phone GPS) have been used to observe and\nanalyze human mobility patterns in an unprecedented spatio-temporal granularity\nand scale. In this work, we use location data collected from mobile phones to\nestimate and analyze the causal impact of hurricanes on business performance.\nTo quantify the causal impact of the disaster, we use a Bayesian structural\ntime series model to predict the counterfactual performances of affected\nbusinesses (what if the disaster did not occur?), which may use performances of\nother businesses outside the disaster areas as covariates. The method is tested\nto quantify the resilience of 635 businesses across 9 categories in Puerto Rico\nafter Hurricane Maria. Furthermore, hierarchical Bayesian models are used to\nreveal the effect of business characteristics such as location and category on\nthe long-term resilience of businesses. The study presents a novel and more\nefficient method to quantify business resilience, which could assist policy\nmakers in disaster preparation and relief processes.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 01:44:56 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Yabe", "Takahiro", ""], ["Zhang", "Yunchang", ""], ["Ukkusuri", "Satish", ""]]}, {"id": "2004.11321", "submitter": "Gareth Molyneux", "authors": "Gareth W. Molyneux, Viraj B. Wijesuriya, and Alessandro Abate", "title": "Bayesian Verification of Chemical Reaction Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven verification approach that determines whether or not\na given chemical reaction network (CRN) satisfies a given property, expressed\nas a formula in a modal logic. Our approach consists of three phases,\nintegrating formal verification over models with learning from data. First, we\nconsider a parametric set of possible models based on a known stoichiometry and\nclassify them against the property of interest. Secondly, we utilise Bayesian\ninference to update a probability distribution of the parameters within a\nparametric model with data gathered from the underlying CRN. In the third and\nfinal stage, we combine the results of both steps to compute the probability\nthat the underlying CRN satisfies the given property. We apply the new approach\nto a case study and compare it to Bayesian statistical model checking.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:08:57 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Molyneux", "Gareth W.", ""], ["Wijesuriya", "Viraj B.", ""], ["Abate", "Alessandro", ""]]}, {"id": "2004.11356", "submitter": "Michael Kapteyn", "authors": "Michael G. Kapteyn and Karen E. Willcox", "title": "From Physics-Based Models to Predictive Digital Twins via Interpretable\n  Machine Learning", "comments": "20 pages, 13 figures, submitted to AIAA Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops a methodology for creating a data-driven digital twin from\na library of physics-based models representing various asset states. The\ndigital twin is updated using interpretable machine learning. Specifically, we\nuse optimal trees---a recently developed scalable machine learning method---to\ntrain an interpretable data-driven classifier. Training data for the classifier\nare generated offline using simulated scenarios solved by the library of\nphysics-based models. These data can be further augmented using experimental or\nother historical data. In operation, the classifier uses observational data\nfrom the asset to infer which physics-based models in the model library are the\nbest candidates for the updated digital twin. The approach is demonstrated\nthrough the development of a structural digital twin for a 12ft wingspan\nunmanned aerial vehicle. This digital twin is built from a library of\nreduced-order models of the vehicle in a range of structural states. The\ndata-driven digital twin dynamically updates in response to structural damage\nor degradation and enables the aircraft to replan a safe mission accordingly.\nWithin this context, we study the performance of the optimal tree classifiers\nand demonstrate how their interpretability enables explainable structural\nassessments from sparse sensor measurements, and also informs optimal sensor\nplacement.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:55:04 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 00:41:35 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 21:08:37 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Kapteyn", "Michael G.", ""], ["Willcox", "Karen E.", ""]]}, {"id": "2004.11521", "submitter": "Seiji Takeda Dr", "authors": "Seiji Takeda, Toshiyuki Hama, Hsiang-Han Hsu, Victoria A. Piunova,\n  Dmitry Zubarev, Daniel P. Sanders, Jed W. Pitera, Makoto Kogoh, Takumi Hongo,\n  Yenwei Cheng, Wolf Bocanett, Hideaki Nakashika, Akihiro Fujita, Yuta\n  Tsuchiya, Katsuhiko Hino, Kentaro Yano, Shuichi Hirose, Hiroki Toda,\n  Yasumitsu Orii, Daiju Nakano", "title": "Molecular Inverse-Design Platform for Material Industries", "comments": "9 pages, 7 figures, Accepted to KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of new materials has been the essential force which brings a\ndiscontinuous improvement to industrial products' performance. However, the\nextra-vast combinatorial design space of material structures exceeds human\nexperts' capability to explore all, thereby hampering material development. In\nthis paper, we present a material industry-oriented web platform of an\nAI-driven molecular inverse-design system, which automatically designs brand\nnew molecular structures rapidly and diversely. Different from existing\ninverse-design solutions, in this system, the combination of substructure-based\nfeature encoding and molecular graph generation algorithms allows a user to\ngain high-speed, interpretable, and customizable design process. Also, a\nhierarchical data structure and user-oriented UI provide a flexible and\nintuitive workflow. The system is deployed on IBM's and our client's cloud\nservers and has been used by 5 partner companies. To illustrate actual\nindustrial use cases, we exhibit inverse-design of sugar and dye molecules,\nthat were carried out by experimental chemists in those client companies.\nCompared to general human chemist's standard performance, the molecular design\nspeed was accelerated more than 10 times, and greatly increased variety was\nobserved in the inverse-designed molecules without loss of chemical realism.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 03:42:26 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 03:21:04 GMT"}, {"version": "v3", "created": "Sat, 16 May 2020 06:48:27 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Takeda", "Seiji", ""], ["Hama", "Toshiyuki", ""], ["Hsu", "Hsiang-Han", ""], ["Piunova", "Victoria A.", ""], ["Zubarev", "Dmitry", ""], ["Sanders", "Daniel P.", ""], ["Pitera", "Jed W.", ""], ["Kogoh", "Makoto", ""], ["Hongo", "Takumi", ""], ["Cheng", "Yenwei", ""], ["Bocanett", "Wolf", ""], ["Nakashika", "Hideaki", ""], ["Fujita", "Akihiro", ""], ["Tsuchiya", "Yuta", ""], ["Hino", "Katsuhiko", ""], ["Yano", "Kentaro", ""], ["Hirose", "Shuichi", ""], ["Toda", "Hiroki", ""], ["Orii", "Yasumitsu", ""], ["Nakano", "Daiju", ""]]}, {"id": "2004.11526", "submitter": "Johannes Hendriks", "authors": "Johannes Hendriks, Nicholas O'Dell, Adrian Wills, Anton Tremsin,\n  Christopher Wensrich and Takenao Shinohara", "title": "Bayesian Non-parametric Bragg-edge Fitting for Neutron Transmission\n  Strain Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy resolved neutron transmission techniques can provide high-resolution\nimages of strain within polycrystalline samples allowing the study of residual\nstrain and stress in engineered components. Strain is estimated from such data\nby analysing features known as Bragg-edges for which several methods exist. It\nis important for these methods to provide both accurate estimates of strain and\nan accurate quantification the associated uncertainty. Our contribution is\ntwofold. First, we present a numerical simulation analysis of these existing\nmethods, which shows that the most accurate estimates of strain are provided by\na method that provides inaccurate estimates of certainty. Second, a novel\nBayesian non-parametric method for estimating strain from Bragg-edges is\npresented. The numerical simulation analysis indicates that this method\nprovides both competitive estimates of strain and accurate quantification of\ncertainty, two demonstrations on experimental data are then presented.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 04:18:11 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 03:28:36 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Hendriks", "Johannes", ""], ["O'Dell", "Nicholas", ""], ["Wills", "Adrian", ""], ["Tremsin", "Anton", ""], ["Wensrich", "Christopher", ""], ["Shinohara", "Takenao", ""]]}, {"id": "2004.11658", "submitter": "Denghui Lu", "authors": "Denghui Lu, Han Wang, Mohan Chen, Jiduan Liu, Lin Lin, Roberto Car,\n  Weinan E, Weile Jia and Linfeng Zhang", "title": "86 PFLOPS Deep Potential Molecular Dynamics simulation of 100 million\n  atoms with ab initio accuracy", "comments": "29 pages, 11 figures", "journal-ref": null, "doi": "10.1016/j.cpc.2020.107624", "report-no": null, "categories": "physics.comp-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the GPU version of DeePMD-kit, which, upon training a deep neural\nnetwork model using ab initio data, can drive extremely large-scale molecular\ndynamics (MD) simulation with ab initio accuracy. Our tests show that the GPU\nversion is 7 times faster than the CPU version with the same power consumption.\nThe code can scale up to the entire Summit supercomputer. For a copper system\nof 113, 246, 208 atoms, the code can perform one nanosecond MD simulation per\nday, reaching a peak performance of 86 PFLOPS (43% of the peak). Such\nunprecedented ability to perform MD simulation with ab initio accuracy opens up\nthe possibility of studying many important issues in materials and molecules,\nsuch as heterogeneous catalysis, electrochemical cells, irradiation damage,\ncrack propagation, and biochemical reactions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 11:16:39 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 06:53:11 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 06:05:31 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Lu", "Denghui", ""], ["Wang", "Han", ""], ["Chen", "Mohan", ""], ["Liu", "Jiduan", ""], ["Lin", "Lin", ""], ["Car", "Roberto", ""], ["E", "Weinan", ""], ["Jia", "Weile", ""], ["Zhang", "Linfeng", ""]]}, {"id": "2004.11689", "submitter": "Yared Bekele", "authors": "Yared W. Bekele", "title": "Deep Learning for One-dimensional Consolidation", "comments": "19 pages, 13 figures", "journal-ref": "JRMGE-374, 2020", "doi": null, "report-no": null, "categories": "cs.CE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks with physical governing equations as constraints have\nrecently created a new trend in machine learning research. In line with such\nefforts, a deep learning model for one-dimensional consolidation where the\ngoverning equation is applied as a constraint in the neural network is\npresented here. A review of related research is first presented and discussed.\nThe deep learning model relies on automatic differentiation for applying the\ngoverning equation as a constraint. The total loss is measured as a combination\nof the training loss (based on analytical and model predicted solutions) and\nthe constraint loss (a requirement to satisfy the governing equation). Two\nclasses of problems are considered: forward and inverse problems. The forward\nproblems demonstrate the performance of a physically constrained neural network\nmodel in predicting solutions for one-dimensional consolidation problems.\nInverse problems show prediction of the coefficient of consolidation.\nTerzaghi's problem with varying boundary conditions are used as example and the\ndeep learning model shows a remarkable performance in both the forward and\ninverse problems. While the application demonstrated here is a simple\none-dimensional consolidation problem, such a deep learning model integrated\nwith a physical law has huge implications for use in, such as, faster real-time\nnumerical prediction for digital twins, numerical model reproducibility and\nconstitutive model parameter optimization.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 06:58:15 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Bekele", "Yared W.", ""]]}, {"id": "2004.11698", "submitter": "Kai Zhou", "authors": "Kai Zhou and Jiong Tang", "title": "Structural Model Updating Using Adaptive Multi-Response Gaussian Process\n  Meta-modeling", "comments": null, "journal-ref": null, "doi": "10.1016/j.ymssp.2020.107121", "report-no": null, "categories": "cs.CE eess.SP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite element model updating utilizing frequency response functions as\ninputs is an important procedure in structural analysis, design and control.\nThis paper presents a highly efficient framework that is built upon Gaussian\nprocess emulation to inversely identify model parameters through sampling. In\nparticular, a multi-response Gaussian process (MRGP) meta-modeling approach is\nformulated that can accurately construct the error response surface, i.e., the\ndiscrepancies between the frequency response predictions and actual\nmeasurement. In order to reduce the computational cost of repeated finite\nelement simulations, an adaptive sampling strategy is established, where the\nsearch of unknown parameters is guided by the response surface features.\nMeanwhile, the information of previously sampled model parameters and the\ncorresponding errors is utilized as additional training data to refine the MRGP\nmeta-model. Two stochastic optimization techniques, i.e., particle swarm and\nsimulated annealing, are employed to train the MRGP meta-model for comparison.\nSystematic case studies are conducted to examine the accuracy and robustness of\nthe new framework of model updating.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:26:33 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Zhou", "Kai", ""], ["Tang", "Jiong", ""]]}, {"id": "2004.11700", "submitter": "Rasmus Bj{\\o}rk", "authors": "Kaspar K. Nielsen, Andrea R. Insinga, Rasmus Bj{\\o}rk", "title": "The stray- and demagnetizing field from a homogeneously magnetized\n  tetrahedron", "comments": "5 pages, 4 figures", "journal-ref": "IEEE Magnetics Letters, Vol. 10, 2110205, 2019", "doi": "10.1109/LMAG.2019.2956895", "report-no": null, "categories": "cs.CE physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stray- and demagnetization tensor field for a homogeneously magnetized\ntetrahedron is found analytically. The tetrahedron is a special case of four\ntriangular faces with constant magnetization-charge surface density, for which\nwe also determine the tensor field. The tensor field is implemented in the open\nsource micromagnetic and magnetostatic simulation framework MagTense and\ncompared with the obtained magnetic field from an FEM solution, showing\nexcellent agreement. This result is important for modeling magnetostatics in\ngeneral and for micromagnetism in particular as the demagnetizing field of an\narbitrary body discretized using conventional meshing techniques is\nsignificantly simplified with this approach.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:34:11 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Nielsen", "Kaspar K.", ""], ["Insinga", "Andrea R.", ""], ["Bj\u00f8rk", "Rasmus", ""]]}, {"id": "2004.11701", "submitter": "Rasmus Bj{\\o}rk", "authors": "K. K. Nielsen and R. Bj{\\o}rk", "title": "The magnetic field from a homogeneously magnetized cylindrical tile", "comments": "8 pages, 3 figures", "journal-ref": "Journal of Magnetism and Magnetic Materials, Vol. 507, 166799,\n  2020", "doi": "10.1016/j.jmmm.2020.166799", "report-no": null, "categories": "cs.CE physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The magnetic field of a homogeneously magnetized cylindrical tile geometry,\ni.e. an angular section of a finite hollow cylinder, is found. The field is\nexpressed as the product between a tensor field describing the geometrical part\nof the problem and a column vector holding the magnetization of the tile.\nOutside the tile, the tensor is identical to the demagnetization tensor. We\nfind that four components of the tensor, $N_{xy},N_{xz},N_{yz}$ and $N_{zy}$,\ncan be expressed fully analytically, while the five remaining components,\n$N_{xx},N_{yx},N_{yy},N_{zx}$ and $N_{zz}$, contain integrals that have to be\nevaluated numerically. When evaluated numerically the tensor is symmetric. A\ncomparison between the found solution, implemented in the open source magnetic\nframework MagTense, and a finite element calculation of the magnetic flux\ndensity of a cylindrical tile shows excellent agreement.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 17:31:10 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Nielsen", "K. K.", ""], ["Bj\u00f8rk", "R.", ""]]}, {"id": "2004.11797", "submitter": "Ioannis Papadopoulos", "authors": "Ioannis P. A. Papadopoulos, Patrick E. Farrell, Thomas M. Surowiec", "title": "Computing multiple solutions of topology optimization problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topology optimization problems often support multiple local minima due to a\nlack of convexity. Typically, gradient-based techniques combined with\ncontinuation in model parameters are used to promote convergence to more\noptimal solutions; however, these methods can fail even in the simplest cases.\nIn this paper, we present an algorithm to perform a systematic exploratory\nsearch for the solutions of the optimization problem via second-order methods\nwithout a good initial guess. The algorithm combines the techniques of\ndeflation, barrier methods and primal-dual active set solvers in a novel way.\nWe demonstrate this approach on several numerical examples, observe\nmesh-independence in certain cases and show that multiple distinct local minima\ncan be recovered.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 15:28:34 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 10:27:49 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Papadopoulos", "Ioannis P. A.", ""], ["Farrell", "Patrick E.", ""], ["Surowiec", "Thomas M.", ""]]}, {"id": "2004.11948", "submitter": "Anh Tran", "authors": "Anh Tran, John A. Mitchell, Laura P. Swiler, Tim Wildey", "title": "An active learning high-throughput microstructure calibration framework\n  for solving inverse structure-process problems in materials informatics", "comments": "Acta Materialia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining a process-structure-property relationship is the holy grail of\nmaterials science, where both computational prediction in the forward direction\nand materials design in the inverse direction are essential. Problems in\nmaterials design are often considered in the context of process-property\nlinkage by bypassing the materials structure, or in the context of\nstructure-property linkage as in microstructure-sensitive design problems.\nHowever, there is a lack of research effort in studying materials design\nproblems in the context of process-structure linkage, which has a great\nimplication in reverse engineering. In this work, given a target\nmicrostructure, we propose an active learning high-throughput microstructure\ncalibration framework to derive a set of processing parameters, which can\nproduce an optimal microstructure that is statistically equivalent to the\ntarget microstructure. The proposed framework is formulated as a noisy\nmulti-objective optimization problem, where each objective function measures a\ndeterministic or statistical difference of the same microstructure descriptor\nbetween a candidate microstructure and a target microstructure. Furthermore, to\nsignificantly reduce the physical waiting wall-time, we enable the\nhigh-throughput feature of the microstructure calibration framework by adopting\nan asynchronously parallel Bayesian optimization by exploiting high-performance\ncomputing resources. Case studies in additive manufacturing and grain growth\nare used to demonstrate the applicability of the proposed framework, where\nkinetic Monte Carlo (kMC) simulation is used as a forward predictive model,\nsuch that for a given target microstructure, the target processing parameters\nthat produced this microstructure are successfully recovered.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 19:16:22 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Tran", "Anh", ""], ["Mitchell", "John A.", ""], ["Swiler", "Laura P.", ""], ["Wildey", "Tim", ""]]}, {"id": "2004.12554", "submitter": "Frederico Gadelha Guimaraes", "authors": "Petr\\^onio C\\^andido de Lima e Silva, Carlos Alberto Severiano Junior,\n  Marcos Antonio Alves, Rodrigo Silva, Miri Weiss Cohen, Frederico Gadelha\n  Guimar\\~aes", "title": "Forecasting in Non-stationary Environments with Fuzzy Time Series", "comments": "21 pages, 7 figures, submitted to Applied Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a Non-Stationary Fuzzy Time Series (NSFTS) method\nwith time varying parameters adapted from the distribution of the data. In this\napproach, we employ Non-Stationary Fuzzy Sets, in which perturbation functions\nare used to adapt the membership function parameters in the knowledge base in\nresponse to statistical changes in the time series. The proposed method is\ncapable of dynamically adapting its fuzzy sets to reflect the changes in the\nstochastic process based on the residual errors, without the need to retraining\nthe model. This method can handle non-stationary and heteroskedastic data as\nwell as scenarios with concept-drift. The proposed approach allows the model to\nbe trained only once and remain useful long after while keeping reasonable\naccuracy. The flexibility of the method by means of computational experiments\nwas tested with eight synthetic non-stationary time series data with several\nkinds of concept drifts, four real market indices (Dow Jones, NASDAQ, SP500 and\nTAIEX), three real FOREX pairs (EUR-USD, EUR-GBP, GBP-USD), and two real\ncryptocoins exchange rates (Bitcoin-USD and Ethereum-USD). As competitor models\nthe Time Variant fuzzy time series and the Incremental Ensemble were used,\nthese are two of the major approaches for handling non-stationary data sets.\nNon-parametric tests are employed to check the significance of the results. The\nproposed method shows resilience to concept drift, by adapting parameters of\nthe model, while preserving the symbolic structure of the knowledge base.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 02:35:46 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Silva", "Petr\u00f4nio C\u00e2ndido de Lima e", ""], ["Junior", "Carlos Alberto Severiano", ""], ["Alves", "Marcos Antonio", ""], ["Silva", "Rodrigo", ""], ["Cohen", "Miri Weiss", ""], ["Guimar\u00e3es", "Frederico Gadelha", ""]]}, {"id": "2004.13201", "submitter": "Chennakesava Kadapa", "authors": "Chennakesava Kadapa and Mokarram Hossain", "title": "A linearised consistent mixed displacement-pressure formulation for\n  hyperelasticity", "comments": "Accepted for publication in Mechanics of Advanced Materials and\n  Structures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel mixed displacement-pressure formulation based on an energy\nfunctional that takes into account the relation between the pressure and the\nvolumetric energy function. We demonstrate that the proposed two-field mixed\ndisplacement-pressure formulation is not only applicable for nearly and truly\nincompressible cases but also is consistent in the compressible regime.\nFurthermore, we prove with analytical derivation and numerical results that the\nproposed two-field formulation is a simplified and efficient alternative for\nthe three-field displacement-pressure-Jacobian formulation for hyperelastic\nmaterials whose strain energy density functions are decomposed into deviatoric\nand volumetric parts.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 22:44:33 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Kadapa", "Chennakesava", ""], ["Hossain", "Mokarram", ""]]}, {"id": "2004.13283", "submitter": "EPTCS", "authors": "Marc Bouissou (EDF (\\'Electricit\\'e de France)), Shahid Khan (RWTH\n  Aachen University), Joost-Pieter Katoen (RWTH Aachen University), Pavel Krcal\n  (Lloyd's Register)", "title": "Various Ways to Quantify BDMPs", "comments": "In Proceedings MARS 2020, arXiv:2004.12403", "journal-ref": "EPTCS 316, 2020, pp. 1-14", "doi": "10.4204/EPTCS.316.1", "report-no": null, "categories": "cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Boolean logic driven Markov process (BDMP) is a dependability analysis\nmodel that defines a continuous-time Markov chain (CTMC). This formalism has\nhigh expressive power, yet it remains readable because its graphical\nrepresentation stays close to standard fault trees. The size of a BDMP is\nroughly speaking proportional to the size of the system it models, whereas the\nsize of the CTMC specified by this BDMP suffers from exponential growth. Thus\nquantifying large BDMPs can be a challenging task. The most general method to\nquantify them is Monte Carlo simulation, but this may be intractable for highly\nreliable systems. On the other hand, some subcategories of BDMPs can be\nprocessed with much more efficient methods. For example, BDMPs without repairs\ncan be translated into dynamic fault trees, a formalism accepted as an input of\nthe STORM model checker, that performs numerical calculations on sparse\nmatrices, or they can be processed with the tool FIGSEQ that explores paths\ngoing to a failure state and calculates their probabilities. BDMPs with repairs\ncan be quantified by FIGSEQ (BDMPs capturing quickly and completely repairable\nbehaviors are solved by a different algorithm), and by the I&AB (Initiator and\nAll Barriers) method, recently published and implemented in a prototype version\nof RISKSPECTRUM PSA. This tool, based exclusively on Boolean representations\nlooks for and quantifies minimal cut sets of the system, i.e., minimal\ncombinations of component failures that induce the loss of the system. This\nallows a quick quantification of large models with repairable components,\nstandby redundancies and some other types of dependencies between omponents.\nAll these quantification methods have been tried on a benchmark whose\ndefinition was published at the MARS 2017 workshop: the model of emergency\npower supplies of a nuclear power plant. In this paper, after a recall of the\ntheoretical principles of the various quantification methods, we compare their\nperformances on that benchmark.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:21:21 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Bouissou", "Marc", "", "EDF"], ["Khan", "Shahid", "", "RWTH\n  Aachen University"], ["Katoen", "Joost-Pieter", "", "RWTH Aachen University"], ["Krcal", "Pavel", "", "Lloyd's Register"]]}, {"id": "2004.13571", "submitter": "Peter Dunning", "authors": "Ewan Fong, Sadik L. Omairey and Peter D. Dunning", "title": "Design of multifunctional metamaterials using optimization", "comments": "15 pages, 6 figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA math.NA math.OC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper explores the use of optimization to design multifunctional\nmetamaterials, and proposes a methodology for constructing a design envelope of\npotential properties. A thermal-mechanical metamaterial, proposed by Ai and Gao\n(2017), is used as the subject of the study. The properties of the metamaterial\nare computed using finite element-based periodic homogenization, which is\nimplemented in Abaqus utilizing an open-source plugin (EasyPBC). Several\noptimization problems are solved using a particle swarm-based optimization\nmethod from the pyOpt package. A series of constrained optimization problems\nare used to construct a design envelop of potential properties. The design\nenvelope more fully captures the potential of the metamaterial, compared with\nthe current practice of using parametric studies. This is because the optimizer\ncan change all parameters simultaneously to find the optimal design. This\ndemonstrates the potential of using an optimization-based approach for\ndesigning and exploring multifunctional metamaterial properties. This proposed\napproach is general and can be applied to any metamaterial design, assuming an\naccurate numerical model exists to evaluate its properties.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 14:55:15 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Fong", "Ewan", ""], ["Omairey", "Sadik L.", ""], ["Dunning", "Peter D.", ""]]}, {"id": "2004.13909", "submitter": "Xin-Long Luo", "authors": "Yiyan Yao and Xin-long Luo", "title": "Improving Vertical Positioning Accuracy with the Weighted Multinomial\n  Logistic Regression Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CE cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, a method of improving vertical positioning accuracy with the\nGlobal Positioning System (GPS) information and barometric pressure values is\nproposed. Firstly, we clear null values for the raw data collected in various\nenvironments, and use the 3$\\sigma$-rule to identify outliers. Secondly, the\nWeighted Multinomial Logistic Regression (WMLR) classifier is trained to obtain\nthe predicted altitude of outliers. Finally, in order to verify its effect, we\ncompare the MLR method, the WMLR method, and the Support Vector Machine (SVM)\nmethod for the cleaned dataset which is regarded as the test baseline. The\nnumerical results show that the vertical positioning accuracy is improved from\n5.9 meters (the MLR method), 5.4 meters (the SVM method) to 5 meters (the WMLR\nmethod) for 67% test points.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 01:13:51 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 11:18:27 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yao", "Yiyan", ""], ["Luo", "Xin-long", ""]]}, {"id": "2004.14223", "submitter": "Ali Javili", "authors": "A. Javili, S. Firooz, A. T. McBride, P. Steinmann", "title": "The computational framework for continuum-kinematics-inspired\n  peridynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peridynamics (PD) is a non-local continuum formulation. The original version\nof PD was restricted to bond-based interactions. Bond-based PD is geometrically\nexact and its kinematics are similar to classical continuum mechanics (CCM).\nHowever, it cannot capture the Poisson effect correctly. This shortcoming was\naddressed via state-based PD, but the kinematics are not accurately preserved.\nContinuum-kinematics-inspired peridynamics (CPD) provides a geometrically exact\nframework whose underlying kinematics coincide with that of CCM and captures\nthe Poisson effect correctly. In CPD, one distinguishes between one-, two- and\nthree-neighbour interactions. One-neighbour interactions are equivalent to the\nbond-based interactions of the original PD formalism. However, two- and\nthree-neighbour interactions are fundamentally different from state-based\ninteractions as the basic elements of continuum kinematics are preserved\nprecisely. The objective of this contribution is to elaborate on computational\naspects of CPD and present detailed derivations that are essential for its\nimplementation. Key features of the resulting computational CPD are elucidated\nvia a series of numerical examples. These include three-dimensional problems at\nlarge deformations. The proposed strategy is robust and the quadratic rate of\nconvergence associated with the Newton--Raphson scheme is observed.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 18:46:10 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Javili", "A.", ""], ["Firooz", "S.", ""], ["McBride", "A. T.", ""], ["Steinmann", "P.", ""]]}, {"id": "2004.14461", "submitter": "Daniel Sch\\\"ollhammer", "authors": "D. Sch\\\"ollhammer and T.P. Fries", "title": "A Higher-order Trace Finite Element Method for Shells", "comments": "Research article has been submitted to International Journal for\n  Numerical Methods in Engineering V1: Initial submission. V2: Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A higher-order fictitious domain method (FDM) for Reissner-Mindlin shells is\nproposed which uses a three-dimensional background mesh for the discretization.\nThe midsurface of the shell is immersed into the higher-order background mesh\nand the geometry is implied by level-set functions. The mechanical model is\nbased on the Tangential Differential Calculus (TDC) which extends the classical\nmodels based on curvilinear coordinates to implicit geometries. The shell model\nis described by PDEs on manifolds and the resulting FDM may typically be called\nTrace FEM. The three standard key aspects of FDMs have to be addressed in the\nTrace FEM as well to allow for a higher-order accurate method: (i) numerical\nintegration in the cut background elements, (ii) stabilization of awkward cut\nsituations and elimination of linear dependencies, and (iii) enforcement of\nboundary conditions using Nitsche's method. The numerical results confirm that\nhigher-order accurate results are enabled by the proposed method provided that\nthe solutions are sufficiently smooth.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 20:22:00 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 12:14:11 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Sch\u00f6llhammer", "D.", ""], ["Fries", "T. P.", ""]]}, {"id": "2004.14768", "submitter": "David Fobes", "authors": "Frederik Geth, Carleton Coffrin, David M Fobes", "title": "A Flexible Storage Model for Power Network Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-19-25958", "categories": "eess.SY cs.CE cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a simple and flexible storage model for use in a variety\nof multi-period optimal power flow problems. The proposed model is designed for\nresearch use in a broad assortment of contexts enabled by the following key\nfeatures: (i) the model can represent the dynamics of an energy buffer at a\nwide range of scales, from residential battery storage to grid-scale pumped\nhydro; (ii) it is compatible with both balanced and unbalanced formulations of\nthe power flow equations; (iii) convex relaxations and linear approximations to\nallow seamless integration of the proposed model into applications where\nconvexity or linearity is required are developed; (iv) a minimalist and\nstandardized data model is presented, to facilitate easy of use by the research\ncommunity. The proposed model is validated using a proof-of-concept twenty-four\nhour storage scheduling task that demonstrates the value of the model's key\nfeatures. An open-source implementation of the model is provided as part of the\nPowerModels and PowerModelsDistribution optimization toolboxes.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 12:56:29 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Geth", "Frederik", ""], ["Coffrin", "Carleton", ""], ["Fobes", "David M", ""]]}, {"id": "2004.14803", "submitter": "Saideep Nannapaneni", "authors": "Sima E. Borujeni, Saideep Nannapaneni, Nam H. Nguyen, Elizabeth C.\n  Behrman, James E. Steck", "title": "Quantum circuit representation of Bayesian networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic graphical models such as Bayesian networks are widely used to\nmodel stochastic systems to perform various types of analysis such as\nprobabilistic prediction, risk analysis, and system health monitoring, which\ncan become computationally expensive in large-scale systems. While\ndemonstrations of true quantum supremacy remain rare, quantum computing\napplications managing to exploit the advantages of amplitude amplification have\nshown significant computational benefits when compared against their classical\ncounterparts. We develop a systematic method for designing a quantum circuit to\nrepresent a generic discrete Bayesian network with nodes that may have two or\nmore states, where nodes with more than two states are mapped to multiple\nqubits. The marginal probabilities associated with root nodes (nodes without\nany parent nodes) are represented using rotation gates, and the conditional\nprobability tables associated with non-root nodes are represented using\ncontrolled rotation gates. The controlled rotation gates with more than one\ncontrol qubit are represented using ancilla qubits. The proposed approach is\ndemonstrated for three examples: a 4-node oil company stock prediction, a\n10-node network for liquidity risk assessment, and a 9-node naive Bayes\nclassifier for bankruptcy prediction. The circuits were designed and simulated\nusing Qiskit, a quantum computing platform that enables simulations and also\nhas the capability to run on real quantum hardware. The results were validated\nagainst those obtained from classical Bayesian network implementations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 04:58:54 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 15:38:03 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Borujeni", "Sima E.", ""], ["Nannapaneni", "Saideep", ""], ["Nguyen", "Nam H.", ""], ["Behrman", "Elizabeth C.", ""], ["Steck", "James E.", ""]]}, {"id": "2004.14832", "submitter": "Sarah Verhulst", "authors": "Deepak Baby, Arthur Van Den Broucke, Sarah Verhulst", "title": "A convolutional neural-network model of human cochlear mechanics and\n  filter tuning for real-time applications", "comments": null, "journal-ref": null, "doi": "10.1038/s42256-020-00286-8", "report-no": null, "categories": "eess.AS cs.CE cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auditory models are commonly used as feature extractors for automatic\nspeech-recognition systems or as front-ends for robotics, machine-hearing and\nhearing-aid applications. Although auditory models can capture the biophysical\nand nonlinear properties of human hearing in great detail, these biophysical\nmodels are computationally expensive and cannot be used in real-time\napplications. We present a hybrid approach where convolutional neural networks\nare combined with computational neuroscience to yield a real-time end-to-end\nmodel for human cochlear mechanics, including level-dependent filter tuning\n(CoNNear). The CoNNear model was trained on acoustic speech material and its\nperformance and applicability were evaluated using (unseen) sound stimuli\ncommonly employed in cochlear mechanics research. The CoNNear model accurately\nsimulates human cochlear frequency selectivity and its dependence on sound\nintensity, an essential quality for robust speech intelligibility at negative\nspeech-to-background-noise ratios. The CoNNear architecture is based on\nparallel and differentiable computations and has the power to achieve real-time\nhuman performance. These unique CoNNear features will enable the next\ngeneration of human-like machine-hearing applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:43:03 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 20:38:38 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 12:05:56 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2020 20:08:14 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Baby", "Deepak", ""], ["Broucke", "Arthur Van Den", ""], ["Verhulst", "Sarah", ""]]}]