[{"id": "0908.0567", "submitter": "Oktie Hassanzadeh", "authors": "Oktie Hassanzadeh, Anastasios Kementsietsidis, Lipyeow Lim, Renee J.\n  Miller, Min Wang", "title": "LinkedCT: A Linked Data Space for Clinical Trials", "comments": "5 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Linked Clinical Trials (LinkedCT) project aims at publishing the first\nopen semantic web data source for clinical trials data. The database exposed by\nLinkedCT is generated by (1) transforming existing data sources of clinical\ntrials into RDF, and (2) discovering semantic links between the records in the\ntrials data and several other data sources. In this paper, we discuss several\nchallenges involved in these two steps and present the methodology used in\nLinkedCT to overcome these challenges. Our approach for semantic link discovery\ninvolves using state-of-the-art approximate string matching techniques combined\nwith ontology-based semantic matching of the records, all performed in a\ndeclarative and easy-to-use framework. We present an evaluation of the\nperformance of our proposed techniques in several link discovery scenarios in\nLinkedCT.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2009 23:30:46 GMT"}], "update_date": "2009-08-06", "authors_parsed": [["Hassanzadeh", "Oktie", ""], ["Kementsietsidis", "Anastasios", ""], ["Lim", "Lipyeow", ""], ["Miller", "Renee J.", ""], ["Wang", "Min", ""]]}, {"id": "0908.0833", "submitter": "Petr Ivankov", "authors": "Petr R. Ivankov", "title": "Top-down Paradigm in Engineering Software Integration", "comments": "29 pages, 44 figures, 5 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The top-down approach of engineering software integration is considered in\nthis parer. A set of advantages of this approach are presented, by examples.\nAll examples are supplied by open source code.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2009 12:21:22 GMT"}], "update_date": "2009-08-07", "authors_parsed": [["Ivankov", "Petr R.", ""]]}, {"id": "0908.2061", "submitter": "Sebastian Roch", "authors": "Sebastien Roch", "title": "Sequence-Length Requirement of Distance-Based Phylogeny Reconstruction:\n  Breaking the Polynomial Barrier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CE cs.DS math.ST q-bio.PE q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new distance-based phylogeny reconstruction technique which\nprovably achieves, at sufficiently short branch lengths, a polylogarithmic\nsequence-length requirement -- improving significantly over previous polynomial\nbounds for distance-based methods. The technique is based on an averaging\nprocedure that implicitly reconstructs ancestral sequences.\n  In the same token, we extend previous results on phase transitions in\nphylogeny reconstruction to general time-reversible models. More precisely, we\nshow that in the so-called Kesten-Stigum zone (roughly, a region of the\nparameter space where ancestral sequences are well approximated by ``linear\ncombinations'' of the observed sequences) sequences of length $\\poly(\\log n)$\nsuffice for reconstruction when branch lengths are discretized. Here $n$ is the\nnumber of extant species.\n  Our results challenge, to some extent, the conventional wisdom that estimates\nof evolutionary distances alone carry significantly less information about\nphylogenies than full sequence datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2009 13:20:44 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Roch", "Sebastien", ""]]}, {"id": "0908.2578", "submitter": "Alain Gerard", "authors": "Claudiu-Florinel Bisu (MPS), Jean-Yves K'Nevez (LMP), Philippe Darnis\n  (LGM2B), Raynald Laheurte (LMP), Alain G\\'erard (LMP)", "title": "New method to characterize a machining system: application in turning", "comments": null, "journal-ref": "Int. J. of Mat. Form. 2, 2 (2009) 93-105", "doi": "10.1007/s12289-009-0395-y", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies simulates the machining process by using a single degree of\nfreedom spring-mass sytem to model the tool stiffness, or the workpiece\nstiffness, or the unit tool-workpiece stiffness in modelings 2D. Others impose\nthe tool action, or use more or less complex modelings of the efforts applied\nby the tool taking account the tool geometry. Thus, all these models remain\ntwo-dimensional or sometimes partially three-dimensional. This paper aims at\ndeveloping an experimental method allowing to determine accurately the real\nthree-dimensional behaviour of a machining system (machine tool, cutting tool,\ntool-holder and associated system of force metrology six-component\ndynamometer). In the work-space model of machining, a new experimental\nprocedure is implemented to determine the machining system elastic behaviour.\nAn experimental study of machining system is presented. We propose a machining\nsystem static characterization. A decomposition in two distinct blocks of the\nsystem \"Workpiece-Tool-Machine\" is realized. The block Tool and the block\nWorkpiece are studied and characterized separately by matrix stiffness and\ndisplacement (three translations and three rotations). The Castigliano's theory\nallows us to calculate the total stiffness matrix and the total displacement\nmatrix. A stiffness center point and a plan of tool tip static displacement are\npresented in agreement with the turning machining dynamic model and especially\nduring the self induced vibration. These results are necessary to have a good\nthree-dimensional machining system dynamic characterization.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2009 14:29:42 GMT"}], "update_date": "2009-08-19", "authors_parsed": [["Bisu", "Claudiu-Florinel", "", "MPS"], ["K'Nevez", "Jean-Yves", "", "LMP"], ["Darnis", "Philippe", "", "LGM2B"], ["Laheurte", "Raynald", "", "LMP"], ["G\u00e9rard", "Alain", "", "LMP"]]}, {"id": "0908.3252", "submitter": "Jean-Fran\\c{c}ois Giovannelli", "authors": "R. Boubertakh, J.-F. Giovannelli, A. Herment, A. De Cesare", "title": "Non-quadratic convex regularized reconstruction of MR images from spiral\n  acquisitions", "comments": null, "journal-ref": "Signal Processing, vol. 86, pp. 2479-2494, 2006", "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining fast MR acquisition sequences and high resolution imaging is a\nmajor issue in dynamic imaging. Reducing the acquisition time can be achieved\nby using non-Cartesian and sparse acquisitions. The reconstruction of MR images\nfrom these measurements is generally carried out using gridding that\ninterpolates the missing data to obtain a dense Cartesian k-space filling. The\nMR image is then reconstructed using a conventional Fast Fourier Transform. The\nestimation of the missing data unavoidably introduces artifacts in the image\nthat remain difficult to quantify.\n  A general reconstruction method is proposed to take into account these\nlimitations. It can be applied to any sampling trajectory in k-space, Cartesian\nor not, and specifically takes into account the exact location of the measured\ndata, without making any interpolation of the missing data in k-space.\nInformation about the expected characteristics of the imaged object is\nintroduced to preserve the spatial resolution and improve the signal to noise\nratio in a regularization framework. The reconstructed image is obtained by\nminimizing a non-quadratic convex objective function. An original rewriting of\nthis criterion is shown to strongly improve the reconstruction efficiency.\nResults on simulated data and on a real spiral acquisition are presented and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2009 14:03:03 GMT"}], "update_date": "2009-08-25", "authors_parsed": [["Boubertakh", "R.", ""], ["Giovannelli", "J. -F.", ""], ["Herment", "A.", ""], ["De Cesare", "A.", ""]]}, {"id": "0908.3280", "submitter": "Andri Mirzal M.Sc.", "authors": "Andri Mirzal", "title": "On the Relationship between Trading Network and WWW Network: A\n  Preferential Attachment Perspective", "comments": "21 pages, 6 figures, submitted to International Journal of Business\n  Intelligence and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper describes the relationship between trading network and WWW network\nfrom preferential attachment mechanism perspective. This mechanism is known to\nbe the underlying principle in the network evolution and has been incorporated\nto formulate two famous web pages ranking algorithms, PageRank and HITS. We\npoint out the differences between trading network and WWW network in this\nmechanism, derive the formulation of HITS-based ranking algorithm for trading\nnetwork as a direct consequence of the differences, and apply the same\nframework when deriving the formulation back to the HITS formulation that turns\nto become a technique to accelerate its convergences.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2009 02:06:35 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2009 15:32:28 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Mirzal", "Andri", ""]]}, {"id": "0908.4427", "submitter": "Matthew Knepley", "authors": "Matthew G. Knepley, Dmitry A. Karpeev", "title": "Mesh Algorithms for PDE with Sieve I: Mesh Distribution", "comments": "36 pages, 22 figures", "journal-ref": "Scientific Programming, 17(3), 215-230, 2009", "doi": "10.3233/SPR-2009-0249", "report-no": null, "categories": "cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed a new programming framework, called Sieve, to support\nparallel numerical PDE algorithms operating over distributed meshes. We have\nalso developed a reference implementation of Sieve in C++ as a library of\ngeneric algorithms operating on distributed containers conforming to the Sieve\ninterface. Sieve makes instances of the incidence relation, or \\emph{arrows},\nthe conceptual first-class objects represented in the containers. Further,\ngeneric algorithms acting on this arrow container are systematically used to\nprovide natural geometric operations on the topology and also, through duality,\non the data. Finally, coverings and duality are used to encode not only\nindividual meshes, but all types of hierarchies underlying PDE data structures,\nincluding multigrid and mesh partitions.\n  In order to demonstrate the usefulness of the framework, we show how the mesh\npartition data can be represented and manipulated using the same fundamental\nmechanisms used to represent meshes. We present the complete description of an\nalgorithm to encode a mesh partition and then distribute a mesh, which is\nindependent of the mesh dimension, element shape, or embedding. Moreover, data\nassociated with the mesh can be similarly distributed with exactly the same\nalgorithm. The use of a high level of abstraction within the Sieve leads to\nseveral benefits in terms of code reuse, simplicity, and extensibility. We\ndiscuss these benefits and compare our approach to other existing mesh\nlibraries.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2009 21:53:01 GMT"}], "update_date": "2010-09-02", "authors_parsed": [["Knepley", "Matthew G.", ""], ["Karpeev", "Dmitry A.", ""]]}, {"id": "0908.4580", "submitter": "Emanuele Viola", "authors": "Jasmina Hasanhodzic, Andrew W. Lo, Emanuele Viola", "title": "A Computational View of Market Efficiency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CC q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to study market efficiency from a computational viewpoint.\nBorrowing from theoretical computer science, we define a market to be\n\\emph{efficient with respect to resources $S$} (e.g., time, memory) if no\nstrategy using resources $S$ can make a profit. As a first step, we consider\nmemory-$m$ strategies whose action at time $t$ depends only on the $m$ previous\nobservations at times $t-m,...,t-1$. We introduce and study a simple model of\nmarket evolution, where strategies impact the market by their decision to buy\nor sell. We show that the effect of optimal strategies using memory $m$ can\nlead to \"market conditions\" that were not present initially, such as (1) market\nbubbles and (2) the possibility for a strategy using memory $m' > m$ to make a\nbigger profit than was initially possible. We suggest ours as a framework to\nrationalize the technological arms race of quantitative trading firms.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2009 16:57:35 GMT"}], "update_date": "2009-09-01", "authors_parsed": [["Hasanhodzic", "Jasmina", ""], ["Lo", "Andrew W.", ""], ["Viola", "Emanuele", ""]]}]