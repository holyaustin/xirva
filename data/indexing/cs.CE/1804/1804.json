[{"id": "1804.00250", "submitter": "Yugandhar Sarkale", "authors": "Saeed Nozhati, Yugandhar Sarkale, Bruce R. Ellingwood, Edwin K. P.\n  Chong, Hussam Mahmoud", "title": "A Modified Approximate Dynamic Programming Algorithm for Community-level\n  Food Security Following Disasters", "comments": "The material presented here might have some overlap with\n  arXiv:1803.04144 and arXiv:1803.01451. Nonetheless, this material contains\n  substantial additions", "journal-ref": "Proceedings of the 9th International Congress on Environmental\n  Modelling and Software (iEMSs 2018), Fort Collins, CO, June 24--28, 2018", "doi": null, "report-no": null, "categories": "cs.CE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the aftermath of an extreme natural hazard, community residents must have\naccess to functioning food retailers to maintain food security. Food security\nis dependent on supporting critical infrastructure systems, including\nelectricity, potable water, and transportation. An understanding of the\nresponse of such interdependent networks and the process of post-disaster\nrecovery is the cornerstone of an efficient emergency management plan. In this\nstudy, the interconnectedness among different critical facilities, such as\nelectrical power networks, water networks, highway bridges, and food retailers,\nis modeled. The study considers various sources of uncertainty and complexity\nin the recovery process of a community to capture the stochastic behavior of\nthe spatially distributed infrastructure systems. The study utilizes an\napproximate dynamic programming (ADP) framework to allocate resources to\nrestore infrastructure components efficiently. The proposed ADP scheme enables\nus to identify near-optimal restoration decisions at the community level.\nFurthermore, we employ a simulated annealing (SA) algorithm to complement the\nproposed ADP framework and to identify near-optimal actions accurately. In the\nsequel, we use the City of Gilroy, California, USA to illustrate the\napplicability of the proposed methodology following a severe earthquake. The\napproach can be implemented efficiently to identify practical policy\ninterventions to hasten recovery of food systems and to reduce adverse\nfood-insecurity impacts for other hazards and communities.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 02:23:02 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 07:57:34 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Nozhati", "Saeed", ""], ["Sarkale", "Yugandhar", ""], ["Ellingwood", "Bruce R.", ""], ["Chong", "Edwin K. P.", ""], ["Mahmoud", "Hussam", ""]]}, {"id": "1804.02502", "submitter": "Cesar H Comin Prof.", "authors": "Felipe L. Gewers, Gustavo R. Ferreira, Henrique F. de Arruda, Filipi\n  N. Silva, Cesar H. Comin, Diego R. Amancio, Luciano da F. Costa", "title": "Principal Component Analysis: A Natural Approach to Data Exploration", "comments": null, "journal-ref": "ACM Computing Surveys (CSUR), 54(4), pp.1-34 (2021)", "doi": "10.1145/3447755", "report-no": null, "categories": "cs.CE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is often used for analyzing data in the\nmost diverse areas. In this work, we report an integrated approach to several\ntheoretical and practical aspects of PCA. We start by providing, in an\nintuitive and accessible manner, the basic principles underlying PCA and its\napplications. Next, we present a systematic, though no exclusive, survey of\nsome representative works illustrating the potential of PCA applications to a\nwide range of areas. An experimental investigation of the ability of PCA for\nvariance explanation and dimensionality reduction is also developed, which\nconfirms the efficacy of PCA and also shows that standardizing or not the\noriginal data can have important effects on the obtained results. Overall, we\nbelieve the several covered issues can assist researchers from the most diverse\nareas in using and interpreting PCA.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 03:48:49 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 13:20:08 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gewers", "Felipe L.", ""], ["Ferreira", "Gustavo R.", ""], ["de Arruda", "Henrique F.", ""], ["Silva", "Filipi N.", ""], ["Comin", "Cesar H.", ""], ["Amancio", "Diego R.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1804.03198", "submitter": "Casimiro Adays Curbelo Monta\\~nez", "authors": "Casimiro Adays Curbelo Monta\\~nez, Paul Fergus, Almudena Curbelo\n  Monta\\~nez and Carl Chalmers", "title": "Deep Learning Classification of Polygenic Obesity using Genome Wide\n  Association Study SNPs", "comments": "8 pages, 2 figures, 4 tables, 9 equations, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CE cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, association results from genome-wide association studies\n(GWAS) are combined with a deep learning framework to test the predictive\ncapacity of statistically significant single nucleotide polymorphism (SNPs)\nassociated with obesity phenotype. Our approach demonstrates the potential of\ndeep learning as a powerful framework for GWAS analysis that can capture\ninformation about SNPs and the important interactions between them. Basic\nstatistical methods and techniques for the analysis of genetic SNP data from\npopulation-based genome-wide studies have been considered. Statistical\nassociation testing between individual SNPs and obesity was conducted under an\nadditive model using logistic regression. Four subsets of loci after\nquality-control (QC) and association analysis were selected: P-values lower\nthan 1x10-5 (5 SNPs), 1x10-4 (32 SNPs), 1x10-3 (248 SNPs) and 1x10-2 (2465\nSNPs). A deep learning classifier is initialised using these sets of SNPs and\nfine-tuned to classify obese and non-obese observations. Using a deep learning\nclassifier model and genetic variants with P-value < 1x10-2 (2465 SNPs) it was\npossible to obtain results (SE=0.9604, SP=0.9712, Gini=0.9817, LogLoss=0.1150,\nAUC=0.9908 and MSE=0.0300). As the P-value increased, an evident deterioration\nin performance was observed. Results demonstrate that single SNP analysis fails\nto capture the cumulative effect of less significant variants and their overall\ncontribution to the outcome in disease prediction, which is captured using a\ndeep learning framework.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:37:37 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 10:18:40 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Monta\u00f1ez", "Casimiro Adays Curbelo", ""], ["Fergus", "Paul", ""], ["Monta\u00f1ez", "Almudena Curbelo", ""], ["Chalmers", "Carl", ""]]}, {"id": "1804.03458", "submitter": "Fabian Key", "authors": "Fabian Key and Lutz Pauli and Stefanie Elgeti", "title": "The Virtual Ring Shear-Slip Mesh Update Method", "comments": null, "journal-ref": null, "doi": "10.1016/j.compfluid.2018.04.006", "report-no": null, "categories": "math.NA cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method - the Virtual Ring Shear-Slip Mesh Update Method (VR-SSMUM) -\nfor the efficient and accurate modeling of moving boundary or interface\nproblems in the context of the numerical analysis of fluid flow is presented.\nWe focus on cases with periodic straight-line translation including object\nentry and exit. The periodic character of the motion is reflected in the method\nvia a mapping of the physical domain onto a closed virtual ring. Therefore, we\nuse an extended mesh, where unneeded portions are deactivated to control the\ncomputational overhead. We provide a validation case as well as examples for\nthe applicability of the method to 2D and 3D models of packaging machines.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 11:22:48 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Key", "Fabian", ""], ["Pauli", "Lutz", ""], ["Elgeti", "Stefanie", ""]]}, {"id": "1804.03731", "submitter": "Marc Benoit", "authors": "Marc Benoit, Siva Nadarajah", "title": "On the Geometric Conservation Law for the Non Linear Frequency Domain\n  and Time-Spectral Methods", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2019.04.002", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present and validate two new procedures to\nenforce the Geometric Conservation Law (GCL) on a moving grid for an Arbitrary\nLagrangian Eulerian (ALE) formulation of the Euler equations discretized in\ntime for either the Non Linear Frequency Domain (NLFD) or Time-Spectral (TS)\nmethods. The equations are spatially discretized by a structured finite-volume\nscheme on a hexahedral mesh. The derived methodologies follow a general\napproach where the positions and the velocities of the grid points are known at\neach time step. The integrated face mesh velocities are derived either from the\nApproximation of the Exact Volumetric Increments (AEVI) relative to the\nundeformed mesh or exactly computed based on a Trilinear Mapping (TRI-MAP)\nbetween the physical space and the computational domain. The accuracy of the\nAEVI method highly depends on the computation of the volumetric increments and\nlimits the temporal-order of accuracy of the deduced integrated face mesh\nvelocities to between one and two. Thus defeating the purpose of the NLFD\nmethod which possesses spectral rate of convergence. However, the TRI-MAP\nmethod has proven to be more computationally efficient, ensuring the\nsatisfaction of the GCL once the convergence of the time derivative of the cell\nvolume is reached in Fourier space. The methods are validated numerically by\nverifying the conservation of uniform flow and by comparing the integrated face\nmesh velocities to the exact values derived from the mapping.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 21:53:23 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Benoit", "Marc", ""], ["Nadarajah", "Siva", ""]]}, {"id": "1804.03995", "submitter": "Jan Mandel", "authors": "\\`Angel Farguell Caus, James Haley, Adam K. Kochanski, Ana Cort\\'es\n  Fit\\'e, Jan Mandel", "title": "Assimilation of fire perimeters and satellite detections by minimization\n  of the residual in a fire spread model", "comments": "13 pages, 6 figures, ICCS 2018. Minor update", "journal-ref": "Lecture Notes in Computer Science 10861, pp. 711-723, 2018", "doi": "10.1007/978-3-319-93701-4_56", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assimilation of data into a fire-spread model is formulated as an\noptimization problem. The level set equation, which relates the fire arrival\ntime and the rate of spread, is allowed to be satisfied only approximately, and\nwe minimize a norm of the residual. Previous methods based on modification of\nthe fire arrival time either used an additive correction to the fire arrival\ntime, or made a position correction. Unlike additive fire arrival time\ncorrections, the new method respects the dependence of the fire rate of spread\non diurnal changes of fuel moisture and on weather changes, and, unlike\nposition corrections, it respects the dependence of the fire spread on fuels\nand terrain as well. The method is used to interpolate the fire arrival time\nbetween two perimeters by imposing the fire arrival time at the perimeters as\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 03:36:16 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 21:26:12 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Caus", "\u00c0ngel Farguell", ""], ["Haley", "James", ""], ["Kochanski", "Adam K.", ""], ["Fit\u00e9", "Ana Cort\u00e9s", ""], ["Mandel", "Jan", ""]]}, {"id": "1804.04457", "submitter": "Claire Heaney", "authors": "C. E. Heaney (1, 2), P. Salinas (1, 2), F. Fang (1, 2, 3), C. C. Pain\n  (1, 2, 3) and I. M. Navon (4) ((1) Applied Modelling and Computation Group,\n  Department of Earth Science and Engineering, Imperial College London, UK (2)\n  Novel Reservoir Modelling and Simulation Group, Department of Earth Science\n  and Engineering, Imperial College London, UK (3) Data Assimilation\n  Laboratory, Data Science Institute, Imperial College London, UK (4)\n  Department of Scientific Computing, Florida State University, USA)", "title": "Goal-based sensitivity maps using time windows and ensemble\n  perturbations", "comments": "35 pages, 13 figures. Submitted to JCP in September 2018 Changes:\n  additional context given in the introduction, additional explanation given in\n  section 2.2, some changes to equations. Results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for forming sensitivity maps (or sensitivites) using\nensembles. The method is an alternative to using an adjoint, which can be very\nchallenging to formulate and also computationally expensive to solve. The main\nnovelties of the presented approach are: 1) the use of goals, weighting the\nperturbation to help resolve the most important sensitivities, 2) the use of\ntime windows, which enable the perturbations to be optimised independently for\neach window and 3) re-orthogonalisation of the solution through time, which\nhelps optimise each perturbation when calculating sensitivity maps. These novel\nmethods greatly reduce the number of ensembles required to form the sensitivity\nmaps as demonstrated in this paper. As the presented method relies solely on\nensembles obtained from the forward model, it can therefore be applied directly\nto forward models of arbitrary complexity arising from, for example,\nmulti-physics coupling, legacy codes or model chains. It can also be applied to\ncompute sensitivities for optimisation of sensor placement, optimisation for\ndesign or control, goal-based mesh adaptivity, assessment of goals (e.g. hazard\nassessment and mitigation in the natural environment), determining the worth of\ncurrent data and data assimilation.\n  We analyse and demonstrate the efficiency of the approach by applying the\nmethod to advection problems and also a non-linear heterogeneous multi-phase\nporous media problem, showing, in all cases, that the number of ensembles\nrequired to obtain accurate sensitivity maps is relatively low, in the order of\n10s.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 12:13:34 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 12:07:44 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Heaney", "C. E.", ""], ["Salinas", "P.", ""], ["Fang", "F.", ""], ["Pain", "C. C.", ""], ["Navon", "I. M.", ""]]}, {"id": "1804.04550", "submitter": "Calum Edmunds", "authors": "Calum Edmunds, Waqquas Bukhsh, Simon Gill, Stuart Galloway", "title": "Locational Marginal Price Variability at Distribution Level: A Regional\n  Study", "comments": "5 pages", "journal-ref": null, "doi": "10.1109/ISGTEurope.2018.8571664", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As distribution systems move towards being more actively managed there is\nincreased potential for regional markets and the application of locational\nmarginal prices (LMPs) to capture spatial variation in the marginal cost of\nelectricity at distribution level. However, with this increased network\nvisibility can come increased price volatility and uncertainty to investors.\nThis paper studies the variation in LMPs in a section of the south west of\nEngland distribution network for current and future installed capacity of\ndistributed generation. It has been shown that in an unconstrained network,\nspatial LMP variation (due to losses) is minimal compared to the temporal\nvariation. In a constrained network, a significant increase in LMP volatility\nwas observed, both spatially and temporally. This could bring risk for\ngenerators particularly if they become stranded in low price areas, or flexible\ndemands facing a drop-off in return when constraints are removed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 14:47:33 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 13:48:19 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Edmunds", "Calum", ""], ["Bukhsh", "Waqquas", ""], ["Gill", "Simon", ""], ["Galloway", "Stuart", ""]]}, {"id": "1804.04736", "submitter": "Vivekanandan Balasubramanian", "authors": "Vivek Balasubramanian, Travis Jensen, Matteo Turilli, Peter Kasson,\n  Michael Shirts, Shantenu Jha", "title": "Adaptive Ensemble Biomolecular Simulations at Scale", "comments": null, "journal-ref": "SN Comput. Sci. 1, 104 (2020)", "doi": "10.1007/s42979-020-0081-1", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in both theory and methods have created opportunities to\nsimulate biomolecular processes more efficiently using adaptive ensemble\nsimulations. Ensemble-based simulations are used widely to compute a number of\nindividual simulation trajectories and analyze statistics across them. Adaptive\nensemble simulations offer a further level of sophistication and flexibility by\nenabling high-level algorithms to control simulations based on intermediate\nresults. Novel high-level algorithms require sophisticated approaches to\nutilize the intermediate data during runtime. Thus, there is a need for\nscalable software systems to support adaptive ensemble-based applications. We\ndescribe the operations in executing adaptive workflows, classify different\ntypes of adaptations, and describe challenges in implementing them in software\ntools. We enhance Ensemble Toolkit (EnTK) -- an ensemble execution system -- to\nsupport the scalable execution of adaptive workflows on HPC systems, and\ncharacterize the adaptation overhead in EnTK. We implement two high-level\nadaptive ensemble algorithms -- expanded ensemble and Markov state modeling,\nand execute upto $2^{12}$ ensemble members, on thousands of cores on three\ndistinct HPC platforms. We highlight scientific advantages enabled by the novel\ncapabilities of our approach. To the best of our knowledge, this is the first\nattempt at describing and implementing multiple adaptive ensemble workflows\nusing a common conceptual and implementation framework.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 21:56:55 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 11:29:17 GMT"}, {"version": "v3", "created": "Sat, 27 Oct 2018 19:00:00 GMT"}, {"version": "v4", "created": "Mon, 25 Feb 2019 16:40:36 GMT"}, {"version": "v5", "created": "Mon, 3 Jun 2019 19:11:00 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Balasubramanian", "Vivek", ""], ["Jensen", "Travis", ""], ["Turilli", "Matteo", ""], ["Kasson", "Peter", ""], ["Shirts", "Michael", ""], ["Jha", "Shantenu", ""]]}, {"id": "1804.04839", "submitter": "Emanuel Weitschek", "authors": "Fabrizio Celli, Fabio Cumbo, and Emanuel Weitschek", "title": "Classification of large DNA methylation datasets for identifying cancer\n  drivers", "comments": null, "journal-ref": "F. Celli, F. Cumbo, E. Weitschek: Classification of Large DNA\n  Methylation Datasets for Identifying Cancer Drivers. Big Data Research,\n  10.1016/j.bdr.2018.02.005, 2018", "doi": "10.1016/j.bdr.2018.02.005", "report-no": null, "categories": "q-bio.GN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA methylation is a well-studied genetic modification crucial to regulate\nthe functioning of the genome. Its alterations play an important role in\ntumorigenesis and tumor-suppression. Thus, studying DNA methylation data may\nhelp biomarker discovery in cancer. Since public data on DNA methylation become\nabundant, and considering the high number of methylated sites (features)\npresent in the genome, it is important to have a method for efficiently\nprocessing such large datasets. Relying on big data technologies, we propose\nBIGBIOCL an algorithm that can apply supervised classification methods to\ndatasets with hundreds of thousands of features. It is designed for the\nextraction of alternative and equivalent classification models through\niterative deletion of selected features. We run experiments on DNA methylation\ndatasets extracted from The Cancer Genome Atlas, focusing on three tumor types:\nbreast, kidney, and thyroid carcinomas. We perform classifications extracting\nseveral methylated sites and their associated genes with accurate performance.\nResults suggest that BIGBIOCL can perform hundreds of classification iterations\non hundreds of thousands of features in few hours. Moreover, we compare the\nperformance of our method with other state-of-the-art classifiers and with a\nwide-spread DNA methylation analysis method based on network analysis. Finally,\nwe are able to efficiently compute multiple alternative classification models\nand extract, from DNA-methylation large datasets, a set of candidate genes to\nbe further investigated to determine their active role in cancer. BIGBIOCL,\nresults of experiments, and a guide to carry on new experiments are freely\navailable on GitHub.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 08:53:39 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Celli", "Fabrizio", ""], ["Cumbo", "Fabio", ""], ["Weitschek", "Emanuel", ""]]}, {"id": "1804.05665", "submitter": "Chinweike Agbachi", "authors": "C. P. E. Agbachi", "title": "Optimisation of Least Squares Algorithm: A Study of Frame Based\n  Programming Techniques in Horizontal Networks", "comments": "10 pages", "journal-ref": "IJMTT. V37(3):190-198 September 2016. ISSN:2231-5373", "doi": "10.14445/22315373/IJMTT-V37P526", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Least squares estimation, a regression technique based on minimisation of\nresiduals, has been invaluable in bringing the best fit solutions to parameters\nin science and engineering. However, in dynamic environments such as in\nGeomatics Engineering, formation of these equations can be very challenging.\nAnd these constraints are ported and apparent in most program models, requiring\nusers at ease with the subject matter. This paper reviews the methods of least\nsquares approximation and examines a one-step automated approach, with error\nanalysis, through the instrumentality of frames, object oriented programming.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 22:06:25 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Agbachi", "C. P. E.", ""]]}, {"id": "1804.05667", "submitter": "Yingli Wang", "authors": "Yingli Wang, Qingpeng Zhang, and Xiaoguang Yang", "title": "Evolution of the Chinese Guarantee Network under Financial Crisis and\n  Stimulus Program", "comments": "30pages, 8 figures, 1 table", "journal-ref": "Nature Communications volume 11, Article number: 2693 (2020)", "doi": "10.1038/s41467-020-16535-8", "report-no": null, "categories": "q-fin.RM cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our knowledge about the evolution of guarantee network in downturn period is\nlimited due to the lack of comprehensive data of the whole credit system. Here\nwe analyze the dynamic Chinese guarantee network constructed from a\ncomprehensive bank loan dataset that accounts for nearly 80% total loans in\nChina, during 01/2007-03/2012. The results show that, first, during the\n2007-2008 global financial crisis, the guarantee network became smaller, less\nconnected and more stable because of many bankruptcies; second, the stimulus\nprogram encouraged mutual guarantee behaviors, resulting in highly reciprocal\nand fragile network structure; third, the following monetary policy adjustment\nenhanced the resilience of the guarantee network by reducing mutual guarantees.\nInterestingly, our work reveals that the financial crisis made the network more\nresilient, and conversely, the government bailout degenerated network\nresilience. These counterintuitive findings can provide new insight into the\nresilience of real-world credit system under external shocks or rescues.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 09:27:09 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 01:25:24 GMT"}, {"version": "v3", "created": "Sat, 9 May 2020 10:41:14 GMT"}, {"version": "v4", "created": "Mon, 1 Jun 2020 10:32:45 GMT"}, {"version": "v5", "created": "Tue, 2 Jun 2020 13:06:36 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Wang", "Yingli", ""], ["Zhang", "Qingpeng", ""], ["Yang", "Xiaoguang", ""]]}, {"id": "1804.05916", "submitter": "Ludovico Minati", "authors": "Stanis{\\l}aw Dro\\.zd\\.z, Robert G\\k{e}barowski, Ludovico Minati,\n  Pawe{\\l} O\\'swi\\k{e}cimka, Marcin W\\k{a}torek", "title": "Bitcoin market route to maturity? Evidence from return fluctuations,\n  temporal correlations and multiscaling effects", "comments": null, "journal-ref": "S. Drozdz, R. Gebarowski, L. Minati, P. Oswiecimka, and M.\n  Watorek, Chaos 28, 071101 (2018)", "doi": "10.1063/1.5036517", "report-no": null, "categories": "q-fin.ST cs.CE econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on 1-minute price changes recorded since year 2012, the fluctuation\nproperties of the rapidly-emerging Bitcoin (BTC) market are assessed over\nchosen sub-periods, in terms of return distributions, volatility\nautocorrelation, Hurst exponents and multiscaling effects. The findings are\ncompared to the stylized facts of mature world markets. While early trading was\naffected by system-specific irregularities, it is found that over the months\npreceding Apr 2018 all these statistical indicators approach the features\nhallmarking maturity. This can be taken as an indication that the Bitcoin\nmarket, and possibly other cryptocurrencies, carry concrete potential of\nimminently becoming a regular market, alternative to the foreign exchange\n(Forex). Since high-frequency price data are available since the beginning of\ntrading, the Bitcoin offers a unique window into the statistical\ncharacteristics of a market maturation trajectory.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 20:00:01 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 06:12:39 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Dro\u017cd\u017c", "Stanis\u0142aw", ""], ["G\u0119barowski", "Robert", ""], ["Minati", "Ludovico", ""], ["O\u015bwi\u0119cimka", "Pawe\u0142", ""], ["W\u0105torek", "Marcin", ""]]}, {"id": "1804.06129", "submitter": "Nicolas Pignet", "authors": "Micka\\\"el Abbas and Alexandre Ern and Nicolas Pignet", "title": "A Hybrid High-Order method for incremental associative plasticity with\n  small deformations", "comments": "28 pages, 26 figures, preprint", "journal-ref": null, "doi": "10.1016/j.cma.2018.08.037", "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise and evaluate numerically a Hybrid High-Order (HHO) method for\nincremental associative plasticity with small deformations. The HHO method uses\nas discrete unknowns piecewise polynomials of order $k\\ge1$ on the mesh\nskeleton, together with cell-based polynomials that can be eliminated locally\nby static condensation. The HHO method supports polyhedral meshes with\nnon-matching interfaces, is free of volumetric-locking and the integration of\nthe behavior law is performed only at cell-based quadrature nodes. Moreover,\nthe principle of virtual work is satisfied locally with equilibrated tractions.\nVarious two- and three-dimensional test cases from the literature are presented\nincluding comparison against known solutions and against results obtained with\nan industrial software using conforming and mixed finite elements.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 09:37:48 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 08:23:41 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 12:39:28 GMT"}, {"version": "v4", "created": "Sun, 23 Sep 2018 07:50:52 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Abbas", "Micka\u00ebl", ""], ["Ern", "Alexandre", ""], ["Pignet", "Nicolas", ""]]}, {"id": "1804.06203", "submitter": "Qidi Li", "authors": "Qidi Li, Hu Wang, Yang Zeng, Zhiwei Lv", "title": "A D-vine copula-based coupling uncertainty analysis for stiffness\n  predication of variable-stiffness composite", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study suggests a coupling uncertainty analysis method to investigate the\nstiffness characteristics of variable stiffness (VS) composite. The D-vine\ncopula function is used to address the coupling of random variables. To\nidentify the copula relation between random variables, a novel one-step\nBayesian copula model selection (OBCS) method is proposed to obtain a suitable\ncopula function as well as the marginal CDF of random variables. The entire\nprocess is Monte Carlo simulation (MCS). However, due to the expensive\ncomputational cost of complete finite element analysis (FEA) in MCS, a fast\nsolver, reanalysis method is introduced. To further improve the efficiency of\nentire procedure, a back propagation neural network (BPNN) model is also\nintroduced based on the reanalysis method. Compared with the reanalysis method,\nBPNN shows a higher efficiency as well as sufficient accuracy. Finally, the\nfiber angle deviation of VS composite is investigated by the suggested\nstrategy. Two numerical examples are presented to verify the feasibility of\nthis method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 12:41:20 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 03:12:58 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Li", "Qidi", ""], ["Wang", "Hu", ""], ["Zeng", "Yang", ""], ["Lv", "Zhiwei", ""]]}, {"id": "1804.06219", "submitter": "Elnura Irmatova", "authors": "Anwar Irmatov and Elnura Irmatova", "title": "Application of the Ranking Relative Principal Component Attributes\n  Network Model (REL-PCANet) for the Inclusive Development Index Estimation", "comments": "13 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2018, at the World Economic Forum in Davos it was presented a new\ncountries' economic performance metric named the Inclusive Development Index\n(IDI) composed of 12 indicators. The new metric implies that countries might\nneed to realize structural reforms for improving both economic expansion and\nsocial inclusion performance. That is why, it is vital for the IDI calculation\nmethod to have strong statistical and mathematical basis, so that results are\naccurate and transparent for public purposes. In the current work, we propose a\nnovel approach for the IDI estimation - the Ranking Relative Principal\nComponent Attributes Network Model (REL-PCANet). The model is based on RELARM\nand RankNet principles and combines elements of PCA, techniques applied in\nimage recognition and learning to rank mechanisms. Also, we define a new\napproach for estimation of target probabilities matrix to reflect dynamic\nchanges in countries' inclusive development. Empirical study proved that\nREL-PCANet ensures reliable and robust scores and rankings, thus is recommended\nfor practical implementation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 14:44:28 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Irmatov", "Anwar", ""], ["Irmatova", "Elnura", ""]]}, {"id": "1804.06262", "submitter": "Casimiro Adays Curbelo Monta\\~nez", "authors": "Casimiro A. Curbelo Monta\\~nez, Paul Fergus, Carl Chalmers and Jade\n  Hind", "title": "Analysis of Extremely Obese Individuals Using Deep Learning Stacked\n  Autoencoders and Genome-Wide Genetic Data", "comments": "13 pages, 4 figures, 13 equations, 2 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aetiology of polygenic obesity is multifactorial, which indicates that\nlife-style and environmental factors may influence multiples genes to aggravate\nthis disorder. Several low-risk single nucleotide polymorphisms (SNPs) have\nbeen associated with BMI. However, identified loci only explain a small\nproportion of the variation ob-served for this phenotype. The linear nature of\ngenome wide association studies (GWAS) used to identify associations between\ngenetic variants and the phenotype have had limited success in explaining the\nheritability variation of BMI and shown low predictive capacity in\nclassification studies. GWAS ignores the epistatic interactions that less\nsignificant variants have on the phenotypic outcome. In this paper we utilise a\nnovel deep learning-based methodology to reduce the high dimensional space in\nGWAS and find epistatic interactions between SNPs for classification purposes.\nSNPs were filtered based on the effects associations have with BMI. Since\nBonferroni adjustment for multiple testing is highly conservative, an important\nproportion of SNPs involved in SNP-SNP interactions are ignored. Therefore,\nonly SNPs with p-values < 1x10-2 were considered for subsequent epistasis\nanalysis using stacked auto encoders (SAE). This allows the nonlinearity\npresent in SNP-SNP interactions to be discovered through progressively smaller\nhidden layer units and to initialise a multi-layer feedforward artificial\nneural network (ANN) classifier. The classifier is fine-tuned to classify\nextremely obese and non-obese individuals. The best results were obtained with\n2000 compressed units (SE=0.949153, SP=0.933014, Gini=0.949936,\nLo-gloss=0.1956, AUC=0.97497 and MSE=0.054057). Using 50 compressed units it\nwas possible to achieve (SE=0.785311, SP=0.799043, Gini=0.703566,\nLogloss=0.476864, AUC=0.85178 and MSE=0.156315).\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 15:25:14 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 10:24:59 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Monta\u00f1ez", "Casimiro A. Curbelo", ""], ["Fergus", "Paul", ""], ["Chalmers", "Carl", ""], ["Hind", "Jade", ""]]}, {"id": "1804.06742", "submitter": "Florian Sch\\\"afer", "authors": "Florian Sch\\\"afer, Martin Braun", "title": "An efficient open-source implementation to compute the Jacobian matrix\n  for the Newton-Raphson power flow algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power flow calculations for systems with a large number of buses, e.g. grids\nwith multiple voltage levels, or time series based calculations result in a\nhigh computational effort. A common power flow solver for the efficient\nanalysis of power systems is the Newton-Raphson algorithm. The main\ncomputational effort of this method results from the linearization of the\nnonlinear power flow problem and solving the resulting linear equation. This\npaper presents an algorithm for the fast linearization of the power flow\nproblem by creating the Jacobian matrix directly in CRS format. The increase in\nspeed is achieved by reducing the number of iterations over the nonzero\nelements of the sparse Jacobian matrix. This allows to efficiently create the\nJacobian matrix without having to approximate the problem. A comparison of the\ncalculation time of three power grids shows that comparable open-source\nimplementations need 3-14x the time to create the Jacobian matrix.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 14:07:41 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Sch\u00e4fer", "Florian", ""], ["Braun", "Martin", ""]]}, {"id": "1804.06816", "submitter": "Christoph Meier", "authors": "Christoph Meier, Reimar Weissbach, Johannes Weinberg, Wolfgang A.\n  Wall, A. John Hart", "title": "Modeling and Characterization of Cohesion in Fine Metal Powders with a\n  Focus on Additive Manufacturing Process Simulations", "comments": null, "journal-ref": null, "doi": "10.1016/j.powtec.2018.11.072", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cohesive interactions between fine metal powder particles crucially\ninfluence their flow behavior, which is in turn important to many powder-based\nmanufacturing processes including emerging methods for powder-based metal\nadditive manufacturing (AM). The present work proposes a novel modeling and\ncharacterization approach for micron-scale metal powders, with a special focus\non characteristics of importance to powder-bed AM. The model is based on the\ndiscrete element method (DEM), and the considered particle-to-particle and\nparticle-to-wall interactions involve frictional contact, rolling resistance\nand cohesive forces. Special emphasis lies on the modeling of cohesion. The\nproposed adhesion force law is defined by the pull-off force resulting from the\nsurface energy of powder particles in combination with a van-der-Waals force\ncurve regularization. The model is applied to predict the angle of repose (AOR)\nof exemplary spherical Ti-6Al-4V powders, and the surface energy value\nunderlying the adhesion force law is calibrated by fitting the corresponding\nangle of repose values from numerical and experimental funnel tests. To the\nbest of the authors' knowledge, this is the first work providing an\nexperimental estimate for the effective surface energy of the considered class\nof metal powders. By this approach, an effective surface energy of $0.1mJ/m^2$\nis found for the investigated Ti-6Al-4V powder. This value is considerably\nlower than typical experimental values for flat metal contact surfaces in the\nrange of $30-50 mJ/m^2$, indicating the crucial influence of factors such as\nsurface roughness and chemical surface contamination on fine metal powders.\nMore importantly, the present study demonstrates that a neglect of the related\ncohesive forces leads to a drastical underestimation of the AOR and,\nconsequently, to an insufficient representation of the bulk powder behavior.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 17:08:47 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 08:44:27 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 07:30:16 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Meier", "Christoph", ""], ["Weissbach", "Reimar", ""], ["Weinberg", "Johannes", ""], ["Wall", "Wolfgang A.", ""], ["Hart", "A. John", ""]]}, {"id": "1804.06822", "submitter": "Christoph Meier", "authors": "Christoph Meier, Reimbar Weissbach, Johannes Weinberg, Wolfgang A.\n  Wall, A. John Hart", "title": "Critical Influences of Particle Size and Adhesion on the Powder Layer\n  Uniformity in Metal Additive Manufacturing", "comments": "Changed reference / bibliography style as compared to version 1", "journal-ref": null, "doi": "10.1016/j.jmatprotec.2018.10.037", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quality of powder layers, specifically their packing density and surface\nuniformity, is a critical factor influencing the quality of components produced\nby powder bed metal additive manufacturing (AM) processes, including selective\nlaser melting, electron beam melting and binder jetting. The present work\nemploys a computational model to study the critical influence of powder\ncohesiveness on the powder recoating process in AM. The model is based on the\ndiscrete element method (DEM) with particle-to-particle and particle-to-wall\ninteractions involving frictional contact, rolling resistance and cohesive\nforces. Quantitative metrics, namely the spatial mean values and standard\ndeviations of the packing fraction and surface profile field, are defined in\norder to evaluate powder layer quality. Based on these metrics, the\nsize-dependent behavior of exemplary plasma-atomized Ti-6Al-4V powders during\nthe recoating process is studied. It is found that decreased particle size /\nincreased cohesiveness leads to considerably decreased powder layer quality in\nterms of low, strongly varying packing fractions and highly non-uniform surface\nprofiles. For relatively fine-grained powders (mean particle diameter $17 \\mu\nm$), it is shown that cohesive forces dominate gravity forces by two orders of\nmagnitude leading to low quality powder layers not suitable for subsequent\nlaser melting without additional layer / surface finishing steps. Besides\nparticle-to-particle adhesion, this contribution quantifies the influence of\nmechanical bulk powder material parameters, nominal layer thickness, blade\nvelocity as well as particle-to-wall adhesion. Finally, the implications of the\nresulting powder layer characteristics on the subsequent melting process are\ndiscussed and practical recommendations are given for the choice of powder\nrecoating process parameters.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 17:16:18 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 07:37:25 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Meier", "Christoph", ""], ["Weissbach", "Reimbar", ""], ["Weinberg", "Johannes", ""], ["Wall", "Wolfgang A.", ""], ["Hart", "A. John", ""]]}, {"id": "1804.07112", "submitter": "Deepak Gupta", "authors": "Debjani Bhowmick, Deepak K. Gupta, Saumen Maiti and Uma Shankar", "title": "Velocity-Porosity Supermodel: A Deep Neural Networks based concept", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rock physics models (RPMs) are used to estimate the elastic properties (e.g.\nvelocity, moduli) from the rock properties (e.g. porosity, lithology, fluid\nsaturation). However, the rock properties drastically vary for different\ngeological conditions, and it is not easy to find a model that is applicable\nunder all scenarios. There exist several empirical velocity-porosity transforms\nas well as first-principle-based models, however, each of these has its own\nlimitations. It is not very straight-forward to choose the correct RPM, and\ntemplates exist, which are overlapped with the log data to decide on the\ncorrect model. In this work, we use deep machine learning and explore the\nconcept of designing a supermodel that can be used for several different\nlithological conditions without any parameter tuning. In this paper, this test\nis restricted to only empirical velocity-porosity transforms, however, the\nfuture goal is to design a rock physics supermodel that can be used on a\nvariety of rock properties. The goal of this paper to is to combine the\nadvantages of several existing empirical velocity-porosity transforms under a\nsingle framework, and design a velocity-porosity supermodel (VPS) using\nartificial neural networks (ANN) based deep learning. Two test cases are used\nand based on the results presented in this paper, it is clear that deep neural\nnetworks can be a potential tool to develop a supermodel for lithological\nmodeling and characterization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 12:29:23 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Bhowmick", "Debjani", ""], ["Gupta", "Deepak K.", ""], ["Maiti", "Saumen", ""], ["Shankar", "Uma", ""]]}, {"id": "1804.07682", "submitter": "Maxim Gonchar", "authors": "Anna Fatkina, Maxim Gonchar, Liudmila Kolupaeva, Dmitry Naumov,\n  Konstantin Treskov", "title": "CUDA Support in GNA Data Analysis Framework", "comments": "12 pages, 7 figures, ICCSA 2018, submitted to Lecture Notes in\n  Computer Science (Springer Verlag)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usage of GPUs as co-processors is a well-established approach to accelerate\ncostly algorithms operating on matrices and vectors.\n  We aim to further improve the performance of the Global Neutrino Analysis\nframework (GNA) by adding GPU support in a way that is transparent to the end\nuser. To achieve our goal we use CUDA, a state of the art technology providing\nGPGPU programming methods.\n  In this paper we describe new features of GNA related to CUDA support. Some\nspecific framework features that influence GPGPU integration are also\nexplained. The paper investigates the feasibility of GPU technology application\nand shows an example of the achieved acceleration of an algorithm implemented\nwithin framework. Benchmarks show a significant performance increase when using\nGPU transformations.\n  The project is currently in the developmental phase. Our plans include\nimplementation of the set of transformations necessary for the data analysis in\nthe GNA framework and tests of the GPU expediency in the complete analysis\nchain.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 15:42:21 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Fatkina", "Anna", ""], ["Gonchar", "Maxim", ""], ["Kolupaeva", "Liudmila", ""], ["Naumov", "Dmitry", ""], ["Treskov", "Konstantin", ""]]}, {"id": "1804.08380", "submitter": "Sindhu Nagaraja", "authors": "Sindhu Nagaraja, Mohamed Elhaddad, Marreddy Ambati, Stefan\n  Kollmannsberger, Laura De Lorenzis, Ernst Rank", "title": "Phase-field modeling of brittle fracture with multi-level hp-FEM and the\n  finite cell method", "comments": null, "journal-ref": "Computational Mechanics, 2018 : ISS 0178-7675, Springer", "doi": "10.1007/s00466-018-1649-7", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The difficulties in dealing with discontinuities related to a sharp crack are\novercome in the phase-field approach for fracture by modeling the crack as a\ndiffusive object being described by a continuous field having high gradients.\nThe discrete crack limit case is approached for a small length-scale parameter\nthat controls the width of the transition region between the fully broken and\nthe undamaged phases. From a computational standpoint, this necessitates fine\nmeshes, at least locally, in order to accurately resolve the phase-field\nprofile. In the classical approach, phase-field models are computed on a fixed\nmesh that is a priori refined in the areas where the crack is expected to\npropagate. This on the other hand curbs the convenience of using phase-field\nmodels for unknown crack paths and its ability to handle complex crack\npropagation patterns. In this work, we overcome this issue by employing the\nmulti-level hp-refinement technique that enables a dynamically changing mesh\nwhich in turn allows the refinement to remain local at singularities and high\ngradients without problems of hanging nodes. Yet, in case of complex\ngeometries, mesh generation and in particular local refinement becomes\nnon-trivial. We address this issue by integrating a two-dimensional phase-field\nframework for brittle fracture with the finite cell method (FCM). The FCM based\non high-order finite elements is a non-geometry-conforming discretization\ntechnique wherein the physical domain is embedded into a larger fictitious\ndomain of simple geometry that can be easily discretized. This facilitates mesh\ngeneration for complex geometries and supports local refinement. Numerical\nexamples including a comparison to a validation experiment illustrate the\napplicability of the multi-level hp-refinement and the FCM in the context of\nphase-field simulations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 12:50:12 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Nagaraja", "Sindhu", ""], ["Elhaddad", "Mohamed", ""], ["Ambati", "Marreddy", ""], ["Kollmannsberger", "Stefan", ""], ["De Lorenzis", "Laura", ""], ["Rank", "Ernst", ""]]}, {"id": "1804.08935", "submitter": "Daisuke Namekata", "authors": "Daisuke Namekata, Masaki Iwasawa, Keigo Nitadori, Ataru Tanikawa,\n  Takayuki Muranushi, Long Wang, Natsuki Hosono, Kentaro Nomura, Junichiro\n  Makino", "title": "Fortran interface layer of the framework for developing particle\n  simulator FDPS", "comments": "10 pages, 10 figures; accepted for publication in PASJ; a typo in\n  author name is corrected", "journal-ref": null, "doi": "10.1093/pasj/psy062", "report-no": null, "categories": "astro-ph.IM astro-ph.EP cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical simulations based on particle methods have been widely used in\nvarious fields including astrophysics. To date, simulation softwares have been\ndeveloped by individual researchers or research groups in each field, with a\nhuge amount of time and effort, even though numerical algorithms used are very\nsimilar. To improve the situation, we have developed a framework, called FDPS,\nwhich enables researchers to easily develop massively parallel particle\nsimulation codes for arbitrary particle methods. Until version 3.0, FDPS have\nprovided API only for C++ programing language. This limitation comes from the\nfact that FDPS is developed using the template feature in C++, which is\nessential to support arbitrary data types of particle. However, there are many\nresearchers who use Fortran to develop their codes. Thus, the previous versions\nof FDPS require such people to invest much time to learn C++. This is\ninefficient. To cope with this problem, we newly developed a Fortran interface\nlayer in FDPS, which provides API for Fortran. In order to support arbitrary\ndata types of particle in Fortran, we design the Fortran interface layer as\nfollows. Based on a given derived data type in Fortran representing particle, a\nPython script provided by us automatically generates a library that manipulates\nthe C++ core part of FDPS. This library is seen as a Fortran module providing\nAPI of FDPS from the Fortran side and uses C programs internally to\ninteroperate Fortran with C++. In this way, we have overcome several technical\nissues when emulating `template' in Fortran. By using the Fortran interface,\nusers can develop all parts of their codes in Fortran. We show that the\noverhead of the Fortran interface part is sufficiently small and a code written\nin Fortran shows a performance practically identical to the one written in C++.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 09:54:22 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 06:31:07 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Namekata", "Daisuke", ""], ["Iwasawa", "Masaki", ""], ["Nitadori", "Keigo", ""], ["Tanikawa", "Ataru", ""], ["Muranushi", "Takayuki", ""], ["Wang", "Long", ""], ["Hosono", "Natsuki", ""], ["Nomura", "Kentaro", ""], ["Makino", "Junichiro", ""]]}, {"id": "1804.09250", "submitter": "Xin-She Yang", "authors": "Asma Chakri, Xin-She Yang, Rabia Khelif, Mohamed Benouaret", "title": "Reliability based-design optimization using the directional bat\n  algorithm", "comments": "Neural Computing and Applications, 2017", "journal-ref": null, "doi": "10.1007/s00521-016-2797-3", "report-no": null, "categories": "math.OC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability based design optimization (RBDO) problems are important in\nengineering applications, but it is challenging to solve such problems. In this\nstudy, a new resolution method based on the directional Bat Algorithm (dBA) is\npresented. To overcome the difficulties in the evaluations of probabilistic\nconstraints, the reliable design space concept has been applied to convert the\nyielded stochastic constrained optimization problem from the RBDO formulation\ninto a deterministic constrained optimization problem. In addition, the\nconstraint handling technique has also been introduced to the dBA so that the\nalgorithm can solve constrained optimization problem effectively. The new\nmethod has been applied to several engineering problems and the results show\nthat the new method can solve different varieties of RBDO problems efficiently.\nIn fact, the obtained solutions are consistent with the best results in the\nliterature.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 11:51:09 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Chakri", "Asma", ""], ["Yang", "Xin-She", ""], ["Khelif", "Rabia", ""], ["Benouaret", "Mohamed", ""]]}, {"id": "1804.09601", "submitter": "Conor O Malley", "authors": "Conor O'Malley, Drosos Kourounis, Gabriela Hug and Olaf Schenk", "title": "Optimizing gas networks using adjoint gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing amount of gas-fired power plants are currently being installed\nin modern power grids worldwide. This is due to their low cost and the inherent\nflexibility offered to the electrical network, particularly in the face of\nincreasing renewable generation. However, the integration and operation of gas\ngenerators poses additional challenges to gas network operators, mainly because\nthey can induce rapid changes in the demand. This paper presents an efficient\nminimization scheme of gas compression costs under dynamic conditions where\ndeliveries to customers are described by time-dependent mass flows. The\noptimization scheme is comprised of a set of transient nonlinear partial\ndifferential equations that model the isothermal gas flow in pipes, an adjoint\nproblem for efficient calculation of the objective gradients and constraint\nJacobians, and state-of-the-art optimal control methods for solving nonlinear\nprograms. As the evaluation of constraint Jacobians can become computationally\ncostly as the number of constraints increases, efficient constraint lumping\nschemes are proposed and investigated with respect to accuracy and performance.\nThe resulting optimal control problems are solved using both interior-point and\nsequential quadratic programming methods. The proposed optimization framework\nis validated through several benchmark cases of increasing complexity.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 14:40:54 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["O'Malley", "Conor", ""], ["Kourounis", "Drosos", ""], ["Hug", "Gabriela", ""], ["Schenk", "Olaf", ""]]}, {"id": "1804.10029", "submitter": "Ehsan Kazemi", "authors": "Ehsan Kazemi and Matthias Grossglauser", "title": "MPGM: Scalable and Accurate Multiple Network Alignment", "comments": null, "journal-ref": "IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS,\n  2019", "doi": "10.1109/TCBB.2019.2914050", "report-no": null, "categories": "q-bio.MN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein-protein interaction (PPI) network alignment is a canonical operation\nto transfer biological knowledge among species. The alignment of PPI-networks\nhas many applications, such as the prediction of protein function, detection of\nconserved network motifs, and the reconstruction of species' phylogenetic\nrelationships. A good multiple-network alignment (MNA), by considering the data\nrelated to several species, provides a deep understanding of biological\nnetworks and system-level cellular processes. With the massive amounts of\navailable PPI data and the increasing number of known PPI networks, the problem\nof MNA is gaining more attention in the systems-biology studies.\n  In this paper, we introduce a new scalable and accurate algorithm, called\nMPGM, for aligning multiple networks. The MPGM algorithm has two main steps:\n(i) SEEDGENERATION and (ii) MULTIPLEPERCOLATION. In the first step, to generate\nan initial set of seed tuples, the SEEDGENERATION algorithm uses only protein\nsequence similarities. In the second step, to align remaining unmatched nodes,\nthe MULTIPLEPERCOLATION algorithm uses network structures and the seed tuples\ngenerated from the first step. We show that, with respect to different\nevaluation criteria, MPGM outperforms the other state-of-the-art algorithms. In\naddition, we guarantee the performance of MPGM under certain classes of network\nmodels. We introduce a sampling-based stochastic model for generating k\ncorrelated networks. We prove that for this model, if a sufficient number of\nseed tuples are available, the MULTIPLEPERCOLATION algorithm correctly aligns\nalmost all the nodes. Our theoretical results are supported by experimental\nevaluations over synthetic networks.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 12:55:05 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 23:57:56 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Kazemi", "Ehsan", ""], ["Grossglauser", "Matthias", ""]]}, {"id": "1804.10060", "submitter": "Garth Wells", "authors": "Chris N. Richardson, Nathan Sime, Garth N. Wells", "title": "Scalable computation of thermomechanical turbomachinery problems", "comments": null, "journal-ref": null, "doi": "10.1016/j.finel.2018.11.002", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly held view in the turbomachinery community is that finite element\nmethods are not well-suited for very large-scale thermomechanical simulations.\nWe seek to dispel this notion by presenting performance data for a collection\nof realistic, large-scale thermomechanical simulations. We describe the\nnecessary technology to compute problems with $O(10^7)$ to $O(10^9)$\ndegrees-of-freedom, and emphasise what is required to achieve near linear\ncomputational complexity with good parallel scaling. Performance data is\npresented for turbomachinery components with up to 3.3 billion\ndegrees-of-freedom. The software libraries used to perform the simulations are\nfreely available under open source licenses. The performance demonstrated in\nthis work opens up the possibility of system-level thermomechanical modelling,\nand lays the foundation for further research into high-performance formulations\nfor even larger problems and for other physical processes, such as contact,\nthat are important in turbomachinery analysis.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 13:49:43 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 14:44:04 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Richardson", "Chris N.", ""], ["Sime", "Nathan", ""], ["Wells", "Garth N.", ""]]}, {"id": "1804.10775", "submitter": "Khalid Raza", "authors": "Khalid Raza", "title": "Fuzzy logic based approaches for gene regulatory network inference", "comments": "29 pages, 12 figures and 1 table", "journal-ref": "Artificial Intelligence in Medicine, (2020) Elsevier, 97: 189-203", "doi": "10.1016/j.artmed.2018.12.004", "report-no": null, "categories": "cs.CE q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advancement in high-throughput techniques has fueled the generation\nof large volume of biological data rapidly with low cost. Some of these\ntechniques are microarray and next generation sequencing which provides genome\nlevel insight of living cells. As a result, the size of most of the biological\ndatabases, such as NCBI-GEO, NCBI-SRA, is exponentially growing. These\nbiological data are analyzed using computational techniques for knowledge\ndiscovery - which is one of the objectives of bioinformatics research. Gene\nregulatory network (GRN) is a gene-gene interaction network which plays pivotal\nrole in understanding gene regulation process and disease studies. From the\nlast couple of decades, the researchers are interested in developing\ncomputational algorithms for GRN inference (GRNI) using high-throughput\nexperimental data. Several computational approaches have been applied for\ninferring GRN from gene expression data including statistical techniques\n(correlation coefficient), information theory (mutual information), regression\nbased approaches, probabilistic approaches (Bayesian networks, naive byes),\nartificial neural networks, and fuzzy logic. The fuzzy logic, along with its\nhybridization with other intelligent approach, is well studied in GRNI due to\nits several advantages. In this paper, we present a consolidated review on\nfuzzy logic and its hybrid approaches for GRNI developed during last two\ndecades.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 09:49:08 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Raza", "Khalid", ""]]}, {"id": "1804.10796", "submitter": "Anahita Namvar", "authors": "Anahita Namvar, Mohsen Naderpour", "title": "Handling Uncertainty in Social Lending Credit Risk Prediction with a\n  Choquet Fuzzy Integral Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the main business models in the financial technology field,\npeer-to-peer (P2P) lending has disrupted traditional financial services by\nproviding an online platform for lending money that has remarkably reduced\nfinancial costs. However, the inherent uncertainty in P2P loans can result in\nhuge financial losses for P2P platforms. Therefore, accurate risk prediction is\ncritical to the success of P2P lending platforms. Indeed, even a small\nimprovement in credit risk prediction would be of benefit to P2P lending\nplatforms. This paper proposes an innovative credit risk prediction framework\nthat fuses base classifiers based on a Choquet fuzzy integral. Choquet integral\nfusion improves creditworthiness evaluations by synthesizing the prediction\nresults of multiple classifiers and finding the largest consistency between\noutcomes among conflicting and consistent results. The proposed model was\nvalidated through experimental analysis on a real- world dataset from a\nwell-known P2P lending marketplace. The empirical results indicate that the\ncombination of multiple classifiers based on fuzzy Choquet integrals\noutperforms the best base classifiers used in credit risk prediction to date.\nIn addition, the proposed methodology is superior to some conventional\ncombination techniques.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2018 12:54:51 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Namvar", "Anahita", ""], ["Naderpour", "Mohsen", ""]]}, {"id": "1804.10922", "submitter": "Fatima Zohra Smaili", "authors": "Fatima Zohra Smaili, Xin Gao and Robert Hoehndorf", "title": "OPA2Vec: combining formal and informal content of biomedical ontologies\n  to improve similarity-based prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Ontologies are widely used in biology for data annotation,\nintegration, and analysis. In addition to formally structured axioms,\nontologies contain meta-data in the form of annotation axioms which provide\nvaluable pieces of information that characterize ontology classes. Annotations\ncommonly used in ontologies include class labels, descriptions, or synonyms.\nDespite being a rich source of semantic information, the ontology meta-data are\ngenerally unexploited by ontology-based analysis methods such as semantic\nsimilarity measures. Results: We propose a novel method, OPA2Vec, to generate\nvector representations of biological entities in ontologies by combining formal\nontology axioms and annotation axioms from the ontology meta-data. We apply a\nWord2Vec model that has been pre-trained on PubMed abstracts to produce feature\nvectors from our collected data. We validate our method in two different ways:\nfirst, we use the obtained vector representations of proteins as a similarity\nmeasure to predict protein-protein interaction (PPI) on two different datasets.\nSecond, we evaluate our method on predicting gene-disease associations based on\nphenotype similarity by generating vector representations of genes and diseases\nusing a phenotype ontology, and applying the obtained vectors to predict\ngene-disease associations. These two experiments are just an illustration of\nthe possible applications of our method. OPA2Vec can be used to produce vector\nrepresentations of any biomedical entity given any type of biomedical ontology.\nAvailability: https://github.com/bio-ontology-research-group/opa2vec Contact:\nrobert.hoehndorf@kaust.edu.sa and xin.gao@kaust.edu.sa.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 12:49:29 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Smaili", "Fatima Zohra", ""], ["Gao", "Xin", ""], ["Hoehndorf", "Robert", ""]]}, {"id": "1804.11049", "submitter": "Ming Dong", "authors": "M. Dong, P. C. M. Meira, W. Xu and C. Y. Chung", "title": "Non-Intrusive Signature Extraction for Major Residential Loads", "comments": "10 pages, 10 figures", "journal-ref": "IEEE Transactions on Smart Grid, vol. 4, no. 3, pp. 1421-1430,\n  Sept. 2013", "doi": "10.1109/TSG.2013.2245926", "report-no": null, "categories": "eess.SP cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data collected by smart meters contain a lot of useful information. One\npotential use of the data is to track the energy consumptions and operating\nstatuses of major home appliances.The results will enable homeowners to make\nsound decisions on how to save energy and how to participate in demand response\nprograms. This paper presents a new method to breakdown the total power demand\nmeasured by a smart meter to those used by individual appliances. A unique\nfeature of the proposed method is that it utilizes diverse signatures\nassociated with the entire operating window of an appliance for identification.\nAs a result, appliances with complicated middle process can be tracked. A novel\nappliance registration device and scheme is also proposed to automate the\ncreation of appliance signature database and to eliminate the need of massive\ntraining before identification. The software and system have been developed and\ndeployed to real houses in order to verify the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 05:04:01 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Dong", "M.", ""], ["Meira", "P. C. M.", ""], ["Xu", "W.", ""], ["Chung", "C. Y.", ""]]}]