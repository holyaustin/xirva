[{"id": "1404.0027", "submitter": "Denis Mestivier", "authors": "Neri Mickael and Denis Mestivier", "title": "An efficient GPU acceptance-rejection algorithm for the selection of the\n  next reaction to occur for Stochastic Simulation Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The Stochastic Simulation Algorithm (SSA) has largely diffused in\nthe field of systems biology. This approach needs many realizations for\nestablishing statistical results on the system under study. It is very\ncomputationnally demanding, and with the advent of large models this burden is\nincreasing. Hence parallel implementation of SSA are needed to address these\nneeds.\n  At the very heart of the SSA is the selection of the next reaction to occur\nat each time step, and to the best of our knowledge all implementations are\nbased on an inverse transformation method. However, this method involves a\nrandom number of steps to select this next reaction and is poorly amenable to a\nparallel implementation.\n  Results: Here, we introduce a parallel acceptance-rejection algorithm to\nselect the K next reactions to occur. This algorithm uses a deterministic\nnumber of steps, a property well suited to a parallel implementation. It is\nsimple and small, accurate and scalable. We propose a Graphics Processing Unit\n(GPU) implementation and validate our algorithm with simulated propensity\ndistributions and the propensity distribution of a large model of yeast iron\nmetabolism. We show that our algorithm can handle thousands of selections of\nnext reaction to occur in parallel on the GPU, paving the way to massive SSA.\n  Availability: We present our GPU-AR algorithm that focuses on the very heart\nof the SSA. We do not embed our algorithm within a full implementation in order\nto stay pedagogical and allows its rapid implementation in existing software.\nWe hope that it will enable stochastic modelers to implement our algorithm with\nthe benefits of their own optimizations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 11:11:20 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Mickael", "Neri", ""], ["Mestivier", "Denis", ""]]}, {"id": "1404.0084", "submitter": "EPTCS", "authors": "Adriana Compagnoni (Stevens Institute of Technology), Paola Giannini\n  (Universit\\`a del Piemonte Orientale), Catherine Kim (Stevens Institute of\n  Technology), Matthew Milideo (Stevens Institute of Technology), Vishakha\n  Sharma (Stevens Institute of Technology)", "title": "A Calculus of Located Entities", "comments": "In Proceedings DCM 2013, arXiv:1403.7685", "journal-ref": "EPTCS 144, 2014, pp. 41-56", "doi": "10.4204/EPTCS.144.4", "report-no": null, "categories": "cs.PL cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define BioScapeL, a stochastic pi-calculus in 3D-space. A novel aspect of\nBioScapeL is that entities have programmable locations. The programmer can\nspecify a particular location where to place an entity, or a location relative\nto the current location of the entity. The motivation for the extension comes\nfrom the need to describe the evolution of populations of biochemical species\nin space, while keeping a sufficiently high level description, so that\nphenomena like diffusion, collision, and confinement can remain part of the\nsemantics of the calculus. Combined with the random diffusion movement\ninherited from BioScape, programmable locations allow us to capture the\nassemblies of configurations of polymers, oligomers, and complexes such as\nmicrotubules or actin filaments.\n  Further new aspects of BioScapeL include random translation and scaling.\nRandom translation is instrumental in describing the location of new entities\nrelative to the old ones. For example, when a cell secretes a hydronium ion,\nthe ion should be placed at a given distance from the originating cell, but in\na random direction. Additionally, scaling allows us to capture at a high level\nevents such as division and growth; for example, daughter cells after mitosis\nhave half the size of the mother cell.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 00:39:01 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Compagnoni", "Adriana", "", "Stevens Institute of Technology"], ["Giannini", "Paola", "", "Universit\u00e0 del Piemonte Orientale"], ["Kim", "Catherine", "", "Stevens Institute of\n  Technology"], ["Milideo", "Matthew", "", "Stevens Institute of Technology"], ["Sharma", "Vishakha", "", "Stevens Institute of Technology"]]}, {"id": "1404.0444", "submitter": "Stefano Schivo", "authors": "Stefano Schivo (1), Jetse Scholma (2), Marcel Karperien (2), Janine N.\n  Post (2), Jaco van de Pol (1) and Rom Langerak (1) ((1) Formal Methods and\n  Tools, Faculty of EEMCS, University of Twente, Enschede, The Netherlands, (2)\n  Developmental BioEngineering, MIRA Institute for Biomedical Technology and\n  Technical Medicine, University of Twente, Enschede, The Netherlands)", "title": "Setting Parameters for Biological Models With ANIMO", "comments": null, "journal-ref": "EPTCS 145, 2014, pp. 35-47", "doi": "10.4204/EPTCS.145.5", "report-no": null, "categories": "q-bio.MN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ANIMO (Analysis of Networks with Interactive MOdeling) is a software for\nmodeling biological networks, such as e.g. signaling, metabolic or gene\nnetworks. An ANIMO model is essentially the sum of a network topology and a\nnumber of interaction parameters. The topology describes the interactions\nbetween biological entities in form of a graph, while the parameters determine\nthe speed of occurrence of such interactions. When a mismatch is observed\nbetween the behavior of an ANIMO model and experimental data, we want to update\nthe model so that it explains the new data. In general, the topology of a model\ncan be expanded with new (known or hypothetical) nodes, and enables it to match\nexperimental data. However, the unrestrained addition of new parts to a model\ncauses two problems: models can become too complex too fast, to the point of\nbeing intractable, and too many parts marked as \"hypothetical\" or \"not known\"\nmake a model unrealistic. Even if changing the topology is normally the easier\ntask, these problems push us to try a better parameter fit as a first step, and\nresort to modifying the model topology only as a last resource. In this paper\nwe show the support added in ANIMO to ease the task of expanding the knowledge\non biological networks, concentrating in particular on the parameter settings.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 03:38:20 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Schivo", "Stefano", ""], ["Scholma", "Jetse", ""], ["Karperien", "Marcel", ""], ["Post", "Janine N.", ""], ["van de Pol", "Jaco", ""], ["Langerak", "Rom", ""]]}, {"id": "1404.0453", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N", "title": "Cellular Automata and Its Applications in Bioinformatics: A Review", "comments": null, "journal-ref": "Global Perspectives on Artificial Intelligence (GPAI) Volume 2\n  Issue 2, Pages 16-22 April 2014", "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at providing a survey on the problems that can be easily\naddressed by cellular automata in bioinformatics. Some of the authors have\nproposed algorithms for addressing some problems in bioinformatics but the\napplication of cellular automata in bioinformatics is a virgin field in\nresearch. None of the researchers has tried to relate the major problems in\nbioinformatics and find a common solution. Extensive literature surveys were\nconducted. We have considered some papers in various journals and conferences\nfor conduct of our research. This paper provides intuition towards relating\nvarious problems in bioinformatics logically and tries to attain a common frame\nwork for addressing the same.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 04:18:06 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""], ["N", "SSSN Usha Devi", ""]]}, {"id": "1404.0540", "submitter": "Xinyang Deng", "authors": "Li Gou, Yong Deng, Rehan Sadiq, Sankaran Mahadevan", "title": "Modeling contaminant intrusion in water distribution networks based on D\n  numbers", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient modeling on uncertain information plays an important role in\nestimating the risk of contaminant intrusion in water distribution networks.\nDempster-Shafer evidence theory is one of the most commonly used methods.\nHowever, the Dempster-Shafer evidence theory has some hypotheses including the\nexclusive property of the elements in the frame of discernment, which may not\nbe consistent with the real world. In this paper, based on a more effective\nrepresentation of uncertainty, called D numbers, a new method that allows the\nelements in the frame of discernment to be non-exclusive is proposed. To\ndemonstrate the efficiency of the proposed method, we apply it to the water\ndistribution networks to estimate the risk of contaminant intrusion.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 13:07:06 GMT"}], "update_date": "2014-04-03", "authors_parsed": [["Gou", "Li", ""], ["Deng", "Yong", ""], ["Sadiq", "Rehan", ""], ["Mahadevan", "Sankaran", ""]]}, {"id": "1404.0672", "submitter": "Hammurabi Mendes", "authors": "Hammurabi Mendes, Sorin Istrail", "title": "Thermodynamic Hypothesis as Social Choice: An Impossibility Theorem for\n  Protein Folding", "comments": "Submitted for peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein Folding is concerned with the reasons and mechanism behind a\nprotein's tertiary structure. The thermodynamic hypothesis of Anfinsen\npostulates an universal energy function (UEF) characterizing the tertiary\nstructure, defined consistently across proteins, in terms of their aminoacid\nsequence.\n  We consider the approach of examining multiple protein structure descriptors\nin the PDB (Protein Data Bank), and infer individual preferences, biases\nfavoring particular classes of aminoacid interactions in each of them, later\naggregating these individual preferences into a global preference. This 2-step\nprocess would ideally expose intrinsic biases on classes of aminoacid\ninteractions in the UEF itself. The intuition is that any intrinsic biases in\nthe UEF are expressed within each protein in a specific manner consistent with\nits specific aminoacid sequence, size, and fold (consistently with Anfinsen's\nthermodynamic hypothesis), making a 1-step, holistic aggregation less\ndesirable.\n  Our intention is to illustrate how some impossibility results from voting\ntheory would apply in this setting, being possibly applicable to other protein\nfolding problems as well. We consider concepts and results from voting theory\nand unveil methodological difficulties for the approach mentioned above. With\nour observations, we intend to highlight how key theoretical barriers, already\nexposed by economists, can be relevant for the development of new methods, new\nalgorithms, for problems related to protein folding.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 05:40:39 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Mendes", "Hammurabi", ""], ["Istrail", "Sorin", ""]]}, {"id": "1404.1144", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N", "title": "AIS-MACA- Z: MACA based Clonal Classifier for Splicing Site, Protein\n  Coding and Promoter Region Identification in Eukaryotes", "comments": "6,1-6 Pages, Journal of Artificial Intelligence Research & Advances\n  Volume 1, Issue 1,2014. arXiv admin note: text overlap with arXiv:1403.5933,\n  arXiv:1404.0453", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bioinformatics incorporates information regarding biological data storage,\naccessing mechanisms and presentation of characteristics within this data. Most\nof the problems in bioinformatics and be addressed efficiently by computer\ntechniques. This paper aims at building a classifier based on Multiple\nAttractor Cellular Automata (MACA) which uses fuzzy logic with version Z to\npredict splicing site, protein coding and promoter region identification in\neukaryotes. It is strengthened with an artificial immune system technique\n(AIS), Clonal algorithm for choosing rules of best fitness. The proposed\nclassifier can handle DNA sequences of lengths 54,108,162,252,354. This\nclassifier gives the exact boundaries of both protein and promoter regions with\nan average accuracy of 90.6%. This classifier can predict the splicing site\nwith 97% accuracy. This classifier was tested with 1, 97,000 data components\nwhich were taken from Fickett & Toung , EPDnew, and other sequences from a\nrenowned medical university.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 03:31:29 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""], ["N", "SSSN Usha Devi", ""]]}, {"id": "1404.1990", "submitter": "Alexei Botchkarev", "authors": "Alexei Botchkarev", "title": "Estimating the Accuracy of the Return on Investment (ROI) Performance\n  Evaluations", "comments": null, "journal-ref": "Interdisciplinary Journal of Information, Knowledge, and\n  Management, 2015, 10, 217-233", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Return on Investment (ROI) is one of the most popular performance measurement\nand evaluation metrics. ROI analysis (when applied correctly) is a powerful\ntool in comparing solutions and making informed decisions on the acquisitions\nof information systems. The ROI sensitivity to error is a natural thought, and\ncommon sense suggests that ROI evaluations cannot be absolutely accurate.\nHowever, literature review revealed that in most publications and analyst firms\nreports, this issue is just overlooked. On the one hand, the results of the ROI\ncalculations are implied to be produced with a mathematical rigor, possibility\nof errors is not mentioned and amount of errors is not estimated. On the\ncontrary, another approach claims ROI evaluations to be absolutely inaccurate\nbecause, in view of their authors, future benefits (especially, intangible)\ncannot be estimated within any reasonable boundaries. The purpose of this study\nis to provide a systematic research of the accuracy of the ROI evaluations in\nthe context of the information systems implementations. The main contribution\nof the study is that this is the first systematic effort to evaluate ROI\naccuracy. Analytical expressions have been derived for estimating errors of the\nROI evaluations. Results of the Monte Carlo simulation will help practitioners\nin making informed decisions based on explicitly stated factors influencing the\nROI uncertainties. The results of this research are intended for researchers in\ninformation systems, technology solutions and business management, and also for\ninformation specialists, project managers, program managers, technology\ndirectors, and information systems evaluators. Most results are applicable to\nROI evaluations in a wider subject area.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 01:50:15 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 03:15:24 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Botchkarev", "Alexei", ""]]}, {"id": "1404.2343", "submitter": "Timur Mirzoev", "authors": "Dr. Timur Mirzoev", "title": "Wireless Transmission of Video for Biomechanical Analysis", "comments": null, "journal-ref": "Information systems. Problems, perspectives, innovation approaches\n  Volume 2. 2007. Saint Petersburg State University of Aerospace\n  Instrumentation. ISBN 978-5-80888-0244-5", "doi": null, "report-no": null, "categories": "cs.CE cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there is a possibility to wirelessly stream video over a network, a\nsophisticated computer analysis of the transmitted video is possible. Such\nprocess is used in biomechanics when it is important to analyze athletes\nperformance via streaming digital uncompressed video to a computer and then\nanalyzing it using specific software such as Arial Performance Analysis Systems\nor Dartfish. This manuscript presents some approaches and challenges in\nstreaming video as well as some applications of Information Technology in\nbiomechanics. An example of how scientists from Indiana State University\napproached the wireless transmission of video is also introduced.\n", "versions": [{"version": "v1", "created": "Wed, 9 Apr 2014 01:19:29 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Mirzoev", "Dr. Timur", ""]]}, {"id": "1404.2872", "submitter": "Md Pavel Mahmud", "authors": "Md Pavel Mahmud and Alexander Schliep", "title": "TreQ-CG: Clustering Accelerates High-Throughput Sequencing Read Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As high-throughput sequencers become standard equipment outside of sequencing\ncenters, there is an increasing need for efficient methods for pre-processing\nand primary analysis. While a vast literature proposes methods for HTS data\nanalysis, we argue that significant improvements can still be gained by\nexploiting expensive pre-processing steps which can be amortized with savings\nfrom later stages. We propose a method to accelerate and improve read mapping\nbased on an initial clustering of possibly billions of high-throughput\nsequencing reads, yielding clusters of high stringency and a high degree of\noverlap. This clustering improves on the state-of-the-art in running time for\nsmall datasets and, for the first time, makes clustering high-coverage human\nlibraries feasible. Given the efficiently computed clusters, only one\nrepresentative read from each cluster needs to be mapped using a traditional\nreadmapper such as BWA, instead of individually mapping all reads. On human\nreads, all processing steps, including clustering and mapping, only require\n11%-59% of the time for individually mapping all reads, achieving speed-ups for\nall readmappers, while minimally affecting mapping quality. This accelerates a\nhighly sensitive readmapper such as Stampy to be competitive with a fast\nreadmapper such as BWA on unclustered reads.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 16:29:09 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Mahmud", "Md Pavel", ""], ["Schliep", "Alexander", ""]]}, {"id": "1404.3286", "submitter": "Mahdi Moeini", "authors": "Mahdi Moeini", "title": "A Continuous Optimization Approach for the Financial Portfolio Selection\n  under Discrete Asset Choice Constraints", "comments": "Proceedings of the 12th International Symposium on Operational\n  Research (SOR'2013), Slovenia, September 2013, pp. 89-95, (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a generalization of the Markowitz's Mean-Variance\nmodel under linear transaction costs and cardinality constraints. The\ncardinality constraints are used to limit the number of assets in the optimal\nportfolio. The generalized model is formulated as a mixed integer quadratic\nprogramming (MIP) problem. The purpose of this paper is to investigate a\ncontinuous approach based on difference of convex functions (DC) programming\nfor solving the MIP model. The preliminary comparative results of the proposed\napproach versus CPLEX are presented.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 12:30:06 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Moeini", "Mahdi", ""]]}, {"id": "1404.3329", "submitter": "Mahdi Moeini", "authors": "Hoai An Le Thi, Mahdi Moeini", "title": "Portfolio Selection Under Buy-In Threshold Constraints Using DC\n  Programming and DCA", "comments": "Proceedings of third International Conference on Service Systems and\n  Service Management (SSSM'06/IEEE), Troyes, Oct. 2006, pp. 296-300 (2006).\n  arXiv admin note: text overlap with arXiv:cs/0501005 by other authors", "journal-ref": null, "doi": "10.1109/ICSSSM.2006.320630", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In matter of Portfolio selection, we consider a generalization of the\nMarkowitz Mean-Variance model which includes buy-in threshold constraints.\nThese constraints limit the amount of capital to be invested in each asset and\nprevent very small investments in any asset. The new model can be converted\ninto a NP-hard mixed integer quadratic programming problem. The purpose of this\npaper is to investigate a continuous approach based on DC programming and DCA\nfor solving this new model. DCA is a local continuous approach to solve a wide\nvariety of nonconvex programs for which it provided quite often a global\nsolution and proved to be more robust and efficient than standard methods.\nPreliminary comparative results of DCA and a classical Branch-and-Bound\nalgorithm will be presented. These results show that DCA is an efficient and\npromising approach for the considered portfolio selection problem.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 23:50:21 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Thi", "Hoai An Le", ""], ["Moeini", "Mahdi", ""]]}, {"id": "1404.3330", "submitter": "Mahdi Moeini", "authors": "Mahdi Moeini, Hoai An Le Thi", "title": "A DC programming approach for constrained two-dimensional non-guillotine\n  cutting problem", "comments": "Proceedings of the International Conference on Industrial Engineering\n  and Systems Management (IESM 2011), Metz, May 2011, pp. 212-221, (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a new application of Difference of Convex functions\nprogramming and DCA in solving the constrained two-dimensional non-guillotine\ncutting problem. This problem consists of cutting a number of rectangular\npieces from a large rectangular object. The cuts are done under some\nconstraints and the objective is to maximize the total value of the pieces cut.\nWe reformulate this problem as a DC program and solve it by DCA. The\nperformance of the approach is compared with the standard solver CPLEX.\n", "versions": [{"version": "v1", "created": "Sat, 12 Apr 2014 23:58:20 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Moeini", "Mahdi", ""], ["Thi", "Hoai An Le", ""]]}, {"id": "1404.3448", "submitter": "YuKun Zhong", "authors": "YuKun Zhong, BaoQiu Wang, JianBiao Lin, Chen Tao, Che Nian, Xie Wen", "title": "Solving The Longest Overlap Region Problem for Noncoding DNA Sequences\n  with GPU", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early hardware limitations of GPU (lack of synchronization primitives and\nlimited memory caching mechanisms) can make GPU-based computation inefficient.\nNow Bio-technologies bring more chances to Bioinformatics and Biological\nEngineering. Our paper introduces a way to solve the longest overlap region of\nnon-coding DNA sequences on using the Compute Unified Device Architecture\n(CUDA) platform Intel(R) Core(TM) i3- 3110m quad-core. Compared to standard CPU\nimplementation, CUDA performance proves the method of the longest overlap\nregion recognition of noncoding DNA is an efficient approach to\nhigh-performance bioinformatics applications. Studies show the fact that\nefficiency of GPU performance is more than 20 times speedup than that of CPU\nserial implementation. We believe our method gives a cost-efficient solution to\nthe bioinformatics community for solving longest overlap region recognition\nproblem and other related fields.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 02:00:37 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 00:58:29 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Zhong", "YuKun", ""], ["Wang", "BaoQiu", ""], ["Lin", "JianBiao", ""], ["Tao", "Chen", ""], ["Nian", "Che", ""], ["Wen", "Xie", ""]]}, {"id": "1404.3456", "submitter": "YuKun Zhong", "authors": "Yukun Zhong, ZhiWei He, XianHong Wang, XiongBin Cao", "title": "A Way For Accelerating The DNA Sequence Reconstruction Problem By CUDA", "comments": "7 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, we usually utilize the method of shotgun to cut a DNA sequence\ninto pieces and we have to reconstruct the original DNA sequence from the\npieces, those are widely used method for DNA assembly. Emerging DNA sequence\ntechnologies open up more opportunities for molecular biology. This paper\nintroduce a new method to improve the efficiency of reconstructing DNA sequence\nusing suffix array based on CUDA programming model. The experimental result\nshow the construction of suffix array using GPU is an more efficient approach\non Intel(R) Core(TM) i3-3110K quad-core and NVIDIA GeForce 610M GPU, and study\nshow the performance of our method is more than 20 times than that of CPU\nserial implementation. We believe our method give a cost-efficient solution to\nthe bioinformatics community.\n", "versions": [{"version": "v1", "created": "Mon, 14 Apr 2014 02:57:44 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Zhong", "Yukun", ""], ["He", "ZhiWei", ""], ["Wang", "XianHong", ""], ["Cao", "XiongBin", ""]]}, {"id": "1404.4275", "submitter": "Qian Xiaochao", "authors": "Xiaochao Qian", "title": "A Bitcoin system with no mining and no history transactions: Build a\n  compact Bitcoin system", "comments": "Call for collaborators", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CR q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an explicit definition of decentralization and show you that\ndecentralization is almost impossible for the current stage and Bitcoin is the\nfirst truly noncentralized currency in the currency history. We propose a new\nframework of noncentralized cryptocurrency system with an assumption of the\nexistence of a weak adversary for a bank alliance. It abandons the mining\nprocess and blockchain, and removes history transactions from data\nsynchronization. We propose a consensus algorithm named Converged Consensus for\na noncentralized cryptocurrency system.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 04:13:37 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 05:24:00 GMT"}, {"version": "v3", "created": "Sat, 3 May 2014 09:04:25 GMT"}, {"version": "v4", "created": "Sun, 11 May 2014 10:59:36 GMT"}, {"version": "v5", "created": "Sat, 17 May 2014 08:04:23 GMT"}, {"version": "v6", "created": "Mon, 2 Jun 2014 05:28:23 GMT"}, {"version": "v7", "created": "Fri, 20 Jun 2014 09:21:53 GMT"}, {"version": "v8", "created": "Wed, 27 Apr 2016 03:03:11 GMT"}, {"version": "v9", "created": "Fri, 9 Sep 2016 00:09:25 GMT"}], "update_date": "2016-09-12", "authors_parsed": [["Qian", "Xiaochao", ""]]}, {"id": "1404.4282", "submitter": "Mireille Bossy", "authors": "Mireille Bossy and Jose Espina and Jacques Morice and Cristian Paris\n  and Antoine Rousseau", "title": "Modeling the wind circulation around mills with a Lagrangian stochastic\n  approach", "comments": null, "journal-ref": "SMAI-Journal of computational mathematics, 2 (2016), p. 177-214\n  ISSN 2426-8399", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims at introducing model methodology and numerical studies related\nto a Lagrangian stochastic approach applied to the computation of the wind\ncirculation around mills. We adapt the Lagrangian stochastic downscaling method\nthat we have introduced in [3] and [4] to the atmospheric boundary layer and we\nintroduce here a Lagrangian version of the actuator disc methods to take\naccount of the mills. We present our numerical method and numerical experiments\nin the case of non rotating and rotating actuator disc models. We also present\nsome features of our numerical method, in particular the computation of the\nprobability distribution of the wind in the wake zone, as a byproduct of the\nfluid particle model and the associated PDF method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 15:23:49 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 22:27:37 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2016 14:37:28 GMT"}, {"version": "v4", "created": "Tue, 6 Sep 2016 08:30:03 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Bossy", "Mireille", ""], ["Espina", "Jose", ""], ["Morice", "Jacques", ""], ["Paris", "Cristian", ""], ["Rousseau", "Antoine", ""]]}, {"id": "1404.4304", "submitter": "Ronald Hochreiter", "authors": "Christoph Waldhauser and Ronald Hochreiter and Johannes Otepka and\n  Norbert Pfeifer and Sajid Ghuffar and Karolina Korzeniowska and Gerald Wagner", "title": "Automated Classification of Airborne Laser Scanning Point Clouds", "comments": null, "journal-ref": "In: Solving Computationally Expensive Engineering Problems.\n  Springer Proceedings in Mathematics & Statistics Volume 97: 269-292. 2014", "doi": "10.1007/978-3-319-08985-0_12", "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making sense of the physical world has always been at the core of mapping. Up\nuntil recently, this has always dependent on using the human eye. Using\nairborne lasers, it has become possible to quickly \"see\" more of the world in\nmany more dimensions. The resulting enormous point clouds serve as data sources\nfor applications far beyond the original mapping purposes ranging from flooding\nprotection and forestry to threat mitigation. In order to process these large\nquantities of data, novel methods are required. In this contribution, we\ndevelop models to automatically classify ground cover and soil types. Using the\nlogic of machine learning, we critically review the advantages of supervised\nand unsupervised methods. Focusing on decision trees, we improve accuracy by\nincluding beam vector components and using a genetic algorithm. We find that\nour approach delivers consistently high quality classifications, surpassing\nclassical methods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 16:27:53 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Waldhauser", "Christoph", ""], ["Hochreiter", "Ronald", ""], ["Otepka", "Johannes", ""], ["Pfeifer", "Norbert", ""], ["Ghuffar", "Sajid", ""], ["Korzeniowska", "Karolina", ""], ["Wagner", "Gerald", ""]]}, {"id": "1404.4820", "submitter": "Xu Guo", "authors": "Xu Guo, Weisheng Zhang and Wenliang Zhong", "title": "Topology optimization based on moving deformable components: A new\n  computational framework", "comments": "46 pages, 22 figures", "journal-ref": "Journal of Applied Mechanics Vol.81 (2014) 081009", "doi": "10.1115/1.4027609", "report-no": "DLUT-DEM-2014-18", "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work, a new computational framework for structural topology\noptimization based on the concept of moving deformable components is proposed.\nCompared with the traditional pixel or node point-based solution framework, the\nproposed solution paradigm can incorporate more geometry and mechanical\ninformation into topology optimization directly and therefore render the\nsolution process more flexible. It also has the great potential to reduce the\ncomputational burden associated with topology optimization substantially. Some\nrepresentative examples are presented to illustrate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 17 Apr 2014 14:19:19 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Guo", "Xu", ""], ["Zhang", "Weisheng", ""], ["Zhong", "Wenliang", ""]]}, {"id": "1404.4888", "submitter": "Isadora Nun Ms", "authors": "Isadora Nun, Karim Pichara, Pavlos Protopapas, Dae-Won Kim", "title": "Supervised detection of anomalous light-curves in massive astronomical\n  catalogs", "comments": "16 pages, 18 figures, published in The Astrophysical Journal", "journal-ref": "2014, ApJ, 793, 23", "doi": "10.1088/0004-637X/793/1/23", "report-no": null, "categories": "cs.CE astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of synoptic sky surveys has led to a massive amount of data\nfor which resources needed for analysis are beyond human capabilities. To\nprocess this information and to extract all possible knowledge, machine\nlearning techniques become necessary. Here we present a new method to\nautomatically discover unknown variable objects in large astronomical catalogs.\nWith the aim of taking full advantage of all the information we have about\nknown objects, our method is based on a supervised algorithm. In particular, we\ntrain a random forest classifier using known variability classes of objects and\nobtain votes for each of the objects in the training set. We then model this\nvoting distribution with a Bayesian network and obtain the joint voting\ndistribution among the training objects. Consequently, an unknown object is\nconsidered as an outlier insofar it has a low joint probability. Our method is\nsuitable for exploring massive datasets given that the training process is\nperformed offline. We tested our algorithm on 20 millions light-curves from the\nMACHO catalog and generated a list of anomalous candidates. We divided the\ncandidates into two main classes of outliers: artifacts and intrinsic outliers.\nArtifacts were principally due to air mass variation, seasonal variation, bad\ncalibration or instrumental errors and were consequently removed from our\noutlier list and added to the training set. After retraining, we selected about\n4000 objects, which we passed to a post analysis stage by perfoming a\ncross-match with all publicly available catalogs. Within these candidates we\nidentified certain known but rare objects such as eclipsing Cepheids, blue\nvariables, cataclysmic variables and X-ray sources. For some outliers there\nwere no additional information. Among them we identified three unknown\nvariability types and few individual outliers that will be followed up for a\ndeeper analysis.\n", "versions": [{"version": "v1", "created": "Fri, 18 Apr 2014 21:12:13 GMT"}, {"version": "v2", "created": "Wed, 3 Sep 2014 15:50:49 GMT"}, {"version": "v3", "created": "Wed, 27 May 2015 21:27:11 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Nun", "Isadora", ""], ["Pichara", "Karim", ""], ["Protopapas", "Pavlos", ""], ["Kim", "Dae-Won", ""]]}, {"id": "1404.4944", "submitter": "Jo\\~ao Pedro Pedroso", "authors": "Jo\\~ao Pedro Pedroso and Mikio Kubo and Ana Viana", "title": "Unit commitment with valve-point loading effect", "comments": null, "journal-ref": null, "doi": null, "report-no": "DCC-2014-05", "categories": "math.OC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valve-point loading affects the input-output characteristics of generating\nunits, bringing the fuel costs nonlinear and nonsmooth. This has been\nconsidered in the solution of load dispatch problems, but not in the planning\nphase of unit commitment. This paper presents a mathematical optimization model\nfor the thermal unit commitment problem considering valve-point loading. The\nformulation is based on a careful linearization of the fuel cost function,\nwhich is modeled with great detail on power regions being used in the current\nsolution, and roughly on other regions. A set of benchmark instances for this\nproblem is used for analyzing the method, with recourse to a general-purpose\nmixed-integer optimization solver.\n", "versions": [{"version": "v1", "created": "Sat, 19 Apr 2014 10:50:16 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Pedroso", "Jo\u00e3o Pedro", ""], ["Kubo", "Mikio", ""], ["Viana", "Ana", ""]]}, {"id": "1404.5062", "submitter": "Samir Lemes", "authors": "Nermina Zaimovic-Uzunovic, Samir Lemes, Damir Curic, Alan Topcic", "title": "Rapid prototyping for sling design optimization", "comments": "ISSN 1854-6250", "journal-ref": "Advances in Production, Engineering & Management 6(2011)4, 271-280", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with combination of two modern engineering methods in order\nto optimise the shape of a representative casting product. The product being\nanalysed is a sling, which is used to attach pulling rope in timber\ntransportation. The first step was 3D modelling and static stress/strain\nanalysis using CAD/CAE software NX4. The slinger shape optimization was\nperformed using Traction method, by means of software Optishape-TS. To define\nconstraints for shape optimization, FEA software FEMAP was used. The mould\npattern with optimized 3D shape was then prepared using Fused Deposition\nModelling (FDM) Rapid prototyping method. The sling mass decreased by 20%,\nwhile signifficantly better stress distribution was achieved, with maximum\nstress 3.5 times less than initial value. The future researches should use 3D\nscanning technology in order to provide more accurate 3D model of initial part.\nResults of this research can be used by toolmakers in order to engage FEA/RP\ntechnology to design and manufacture lighter products with acceptable stress\ndistribution.\n", "versions": [{"version": "v1", "created": "Sun, 20 Apr 2014 18:31:52 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Zaimovic-Uzunovic", "Nermina", ""], ["Lemes", "Samir", ""], ["Curic", "Damir", ""], ["Topcic", "Alan", ""]]}, {"id": "1404.5254", "submitter": "Eldad Haber", "authors": "Eldad Haber and Mathias Chung", "title": "Simultaneous Source for non-uniform data variance and missing data", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of simultaneous sources in geophysical inverse problems has\nrevolutionized the ability to deal with large scale data sets that are obtained\nfrom multiple source experiments. However, the technique breaks when the data\nhas non-uniform standard deviation or when some data are missing. In this paper\nwe develop, study, and compare a number of techniques that enable to utilize\nadvantages of the simultaneous source framework for these cases. We show that\nthe inverse problem can still be solved efficiently by using these new\ntechniques. We demonstrate our new approaches on the Direct Current Resistivity\ninverse problem.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 17:55:34 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Haber", "Eldad", ""], ["Chung", "Mathias", ""]]}, {"id": "1404.5458", "submitter": "Yuri Gordienko G.", "authors": "Yuri Gordienko, Lev Bekenov, Olexandr Gatsenko, Elena Zasimchuk,\n  Valentin Tatarenko", "title": "Complex Workflow Management and Integration of Distributed Computing\n  Resources by Science Gateway Portal for Molecular Dynamics Simulations in\n  Materials Science", "comments": "8 pages, 8 figures; Proc. of Third International Conference \"High\n  Performance Computing\" HPC-UA 2013 (Ukraine, Kyiv, October 7-11, 2013)\n  (http://hpc-ua.org/hpc-ua-13/proceedings). ISBN 978-966-7690-16-8 (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The \"IMP Science Gateway Portal\" (http://scigate.imp.kiev.ua) for complex\nworkflow management and integration of distributed computing resources (like\nclusters, service grids, desktop grids, clouds) is presented. It is created on\nthe basis of WS-PGRADE and gUSE technologies, where WS-PGRADE is designed for\nscience workflow operation and gUSE - for smooth integration of available\nresources for parallel and distributed computing in various heterogeneous\ndistributed computing infrastructures (DCI). The typical scientific workflow\nwith possible scenarios of its preparation and usage is considered. Several\ntypical science applications (scientific workflows) are considered for\nmolecular dynamics (MD) simulations of complex behavior of various\nnanostructures (nanoindentation of graphene layers, defect system relaxation in\nmetal nanocrystals, thermal stability of boron nitride nanotubes, etc.). The\nadvantages and drawbacks of the solution are shortly analyzed in the context of\nits practical applications for MD simulations in materials science, physics and\nnanotechnologies with available heterogeneous DCIs.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 11:34:04 GMT"}], "update_date": "2014-04-23", "authors_parsed": [["Gordienko", "Yuri", ""], ["Bekenov", "Lev", ""], ["Gatsenko", "Olexandr", ""], ["Zasimchuk", "Elena", ""], ["Tatarenko", "Valentin", ""]]}, {"id": "1404.5611", "submitter": "Yuri Gordienko G.", "authors": "Yuri Gordienko, Lev Bekenev, Olexandra Baskova, Olexander Gatsenko,\n  Elena Zasimchuk, Sergii Stirenko", "title": "IMP Science Gateway: from the Portal to the Hub of Virtual Experimental\n  Labs in Materials Science", "comments": "6 pages, 5 figures, 3 tables; 6th International Workshop on Science\n  Gateways, IWSG-2014 (Dublin, Ireland, 3-5 June, 2014). arXiv admin note:\n  substantial text overlap with arXiv:1404.5458", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Science gateway\" (SG) ideology means a user-friendly intuitive interface\nbetween scientists (or scientific communities) and different software\ncomponents + various distributed computing infrastructures (DCIs) (like grids,\nclouds, clusters), where researchers can focus on their scientific goals and\nless on peculiarities of software/DCI. \"IMP Science Gateway Portal\"\n(http://scigate.imp.kiev.ua) for complex workflow management and integration of\ndistributed computing resources (like clusters, service grids, desktop grids,\nclouds) is presented. It is created on the basis of WS-PGRADE and gUSE\ntechnologies, where WS-PGRADE is designed for science workflow operation and\ngUSE - for smooth integration of available resources for parallel and\ndistributed computing in various heterogeneous distributed computing\ninfrastructures (DCI). The typical scientific workflows with possible scenarios\nof its preparation and usage are presented. Several typical use cases for these\nscience applications (scientific workflows) are considered for molecular\ndynamics (MD) simulations of complex behavior of various nanostructures\n(nanoindentation of graphene layers, defect system relaxation in metal\nnanocrystals, thermal stability of boron nitride nanotubes, etc.). The user\nexperience is analyzed in the context of its practical applications for MD\nsimulations in materials science, physics and nanotechnologies with available\nheterogeneous DCIs. In conclusion, the \"science gateway\" approach - workflow\nmanager (like WS-PGRADE) + DCI resources manager (like gUSE)- gives opportunity\nto use the SG portal (like \"IMP Science Gateway Portal\") in a very promising\nway, namely, as a hub of various virtual experimental labs (different software\ncomponents + various requirements to resources) in the context of its practical\nMD applications in materials science, physics, chemistry, biology, and\nnanotechnologies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 11:55:17 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Gordienko", "Yuri", ""], ["Bekenev", "Lev", ""], ["Baskova", "Olexandra", ""], ["Gatsenko", "Olexander", ""], ["Zasimchuk", "Elena", ""], ["Stirenko", "Sergii", ""]]}, {"id": "1404.5756", "submitter": "Salvatore Cuomo", "authors": "R. Farina, S. Dobricic, A. Storto, S. Masina and S. Cuomo", "title": "A Revised Scheme to Compute Horizontal Covariances in an Oceanographic\n  3D-VAR Assimilation System", "comments": "26 pages", "journal-ref": null, "doi": "10.1016/j.jcp.2015.01.003", "report-no": null, "categories": "cs.NA cs.CE cs.DC math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improvement of an oceanographic three dimensional variational\nassimilation scheme (3D-VAR), named OceanVar, by introducing a recursive filter\n(RF) with the third order of accuracy (3rd-RF), instead of a RF with first\norder of accuracy (1st-RF), to approximate horizontal Gaussian covariances. An\nadvantage of the proposed scheme is that the CPU's time can be substantially\nreduced with benefits on the large scale applications. Experiments estimating\nthe impact of 3rd-RF are performed by assimilating oceanographic data in two\nrealistic oceanographic applications. The results evince benefits in terms of\nassimilation process computational time, accuracy of the Gaussian correlation\nmodeling, and show that the 3rd-RF is a suitable tool for operational data\nassimilation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 09:22:17 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Farina", "R.", ""], ["Dobricic", "S.", ""], ["Storto", "A.", ""], ["Masina", "S.", ""], ["Cuomo", "S.", ""]]}, {"id": "1404.5764", "submitter": "Yuri Gordienko G.", "authors": "Olexander Gatsenko, Lev Bekenev, Evgen Pavlov, Yuri G. Gordienko", "title": "From Quantity to Quality: Massive Molecular Dynamics Simulation of\n  Nanostructures under Plastic Deformation in Desktop and Service Grid\n  Distributed Computing Infrastructure", "comments": "13 pages, 11 pages (http://journals.agh.edu.pl/csci/article/view/106)", "journal-ref": "Computer Science 14 (1), 27-44 (2013)", "doi": "10.7494/csci.2013.14.1.27", "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed computing infrastructure (DCI) on the basis of BOINC and\nEDGeS-bridge technologies for high-performance distributed computing is used\nfor porting the sequential molecular dynamics (MD) application to its parallel\nversion for DCI with Desktop Grids (DGs) and Service Grids (SGs). The actual\nmetrics of the working DG-SG DCI were measured, and the normal distribution of\nhost performances, and signs of log-normal distributions of other\ncharacteristics (CPUs, RAM, and HDD per host) were found. The practical\nfeasibility and high efficiency of the MD simulations on the basis of DG-SG DCI\nwere demonstrated during the experiment with the massive MD simulations for the\nlarge quantity of aluminum nanocrystals ($\\sim10^2$-$10^3$). Statistical\nanalysis (Kolmogorov-Smirnov test, moment analysis, and bootstrapping analysis)\nof the defect density distribution over the ensemble of nanocrystals had shown\nthat change of plastic deformation mode is followed by the qualitative change\nof defect density distribution type over ensemble of nanocrystals. Some\nlimitations (fluctuating performance, unpredictable availability of resources,\netc.) of the typical DG-SG DCI were outlined, and some advantages (high\nefficiency, high speedup, and low cost) were demonstrated. Deploying on DG DCI\nallows to get new scientific $\\it{quality}$ from the simulated $\\it{quantity}$\nof numerous configurations by harnessing sufficient computational power to\nundertake MD simulations in a wider range of physical parameters\n(configurations) in a much shorter timeframe.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 09:42:50 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Gatsenko", "Olexander", ""], ["Bekenev", "Lev", ""], ["Pavlov", "Evgen", ""], ["Gordienko", "Yuri G.", ""]]}, {"id": "1404.6020", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu, SSSN Usha Devi N", "title": "A Fast Multiple Attractor Cellular Automata with Modified Clonal\n  Classifier for Splicing Site Prediction in Human Genome", "comments": "Four Pages, Global Perspectives on Artificial Intelligence (GPAI)\n  Volume 2, April 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bioinformatics encompass storing, analyzing and interpreting the biological\ndata. Most of the challenges for Machine Learning methods like Cellular\nAutomata is to furnish the functional information with the corresponding\nbiological sequences. In eukaryotes DNA is divided into introns and exons. The\nintrons will be removed to make the coding region by a process called splicing.\nBy indentifying a splice site we can easily specify the DNA sequence category\n(Donor/Accepter/Neither).Splicing sites play an important role in understanding\nthe genes. A class of CA which can handle fuzzy logic is employed with modified\nclonal algorithm is proposed to identify the splicing site. This classifier is\ntested with Irvine Primate Splice Junction Database. It is compared with\nNNspIICE, GENIO, HSPL and SPIICE VIEW. The reported accuracy and efficiency of\nprediction is quite promising.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 03:52:24 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""], ["N", "SSSN Usha Devi", ""]]}, {"id": "1404.6384", "submitter": "Pierre de Buyl", "authors": "Jinook Oh", "title": "CATOS: Computer Aided Training/Observing System", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-03", "categories": "cs.CE cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In animal behavioral biology, there are several cases in which an autonomous\nobserving/training system would be useful. 1) Observation of certain species\ncontinuously, or for documenting specific events, which happen irregularly; 2)\nLongterm intensive training of animals in preparation for behavioral\nexperiments; and 3) Training and testing of animals without human interference,\nto eliminate potential cues and biases induced by humans. The primary goal of\nthis study is to build a system named CATOS (Computer Aided Training/Observing\nSystem) that could be used in the above situations. As a proof of concept, the\nsystem was built and tested in a pilot experiment, in which cats were trained\nto press three buttons differently in response to three different sounds (human\nspeech) to receive food rewards. The system was built in use for about 6\nmonths, successfully training two cats. One cat learned to press a particular\nbutton, out of three buttons, to obtain the food reward with over 70 percent\ncorrectness.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:53:33 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Oh", "Jinook", ""]]}, {"id": "1404.6385", "submitter": "Pierre de Buyl", "authors": "Fabrice Salvaire", "title": "High-Content Digital Microscopy with Python", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-05", "categories": "cs.CE cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  High-Content Digital Microscopy enhances user comfort, data storage and\nanalysis throughput, paving the way to new researches and medical diagnostics.\nA digital microscopy platform aims at capturing an image of a cover slip, at\nstoring information on a file server and a database, at visualising the image\nand analysing its content. We will discuss how the Python ecosystem can provide\nsuch software framework efficiently. Moreover this paper will give an\nillustration of the data chunking approach to manage the huge amount of data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:54:26 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 14:19:35 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Salvaire", "Fabrice", ""]]}, {"id": "1404.6391", "submitter": "Pierre de Buyl", "authors": "Robert Cimrman", "title": "SfePy - Write Your Own FE Application", "comments": "Part of the Proceedings of the 6th European Conference on Python in\n  Science (EuroSciPy 2013), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2013-10", "categories": "cs.CE cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  SfePy (Simple Finite Elements in Python) is a framework for solving various\nkinds of problems (mechanics, physics, biology, ...) described by partial\ndifferential equations in two or three space dimensions by the finite element\nmethod. The paper illustrates its use in an interactive environment or as a\nframework for building custom finite-element based solvers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 10:56:35 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 14:28:34 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Cimrman", "Robert", ""]]}, {"id": "1404.6866", "submitter": "Ming Wang", "authors": "Wang Liang, Zhao KaiYong", "title": "Detecting \"protein words\" through unsupervised word segmentation", "comments": "11 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised word segmentation methods were applied to analyze the protein\nsequence. Protein sequences, such as 'MTMDKSELVQKA...', were used as input to\nthese methods. Segmented 'protein word' sequences, such as 'MTM DKSE LVQKA',\nwere then obtained. We compare the 'protein words' produced by unsupervised\nsegmentation and the protein secondary structure segmentation. An interesting\nfinding is that the unsupervised word segmentation is more efficient than\nsecondary structure segmentation in expressing information. Our experiment also\nsuggests there may be some 'protein ruins' in current noncoding regions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 05:03:21 GMT"}, {"version": "v2", "created": "Fri, 6 Jun 2014 16:17:20 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 14:47:37 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2015 15:13:43 GMT"}, {"version": "v5", "created": "Fri, 16 Oct 2015 03:04:45 GMT"}, {"version": "v6", "created": "Wed, 28 Oct 2015 10:24:30 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Liang", "Wang", ""], ["KaiYong", "Zhao", ""]]}, {"id": "1404.7055", "submitter": "Gautam Dasarathy", "authors": "Gautam Dasarathy, Robert Nowak, and Sebastien Roch", "title": "Data Requirement for Phylogenetic Inference from Multiple Loci: A New\n  Distance Method", "comments": "19 pages, 2 figures. Preliminary version to appear in IEEE ISIT 2014.\n  Added acknowledgements and made the proof of the \"equality\" part of Theorem 3\n  explicit in Appendix C", "journal-ref": null, "doi": "10.1109/TCBB.2014.2361685", "report-no": null, "categories": "q-bio.PE cs.CE cs.DS math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the evolutionary history of a set of\nspecies (phylogeny or species tree) from several genes. It is known that the\nevolutionary history of individual genes (gene trees) might be topologically\ndistinct from each other and from the underlying species tree, possibly\nconfounding phylogenetic analysis. A further complication in practice is that\none has to estimate gene trees from molecular sequences of finite length. We\nprovide the first full data-requirement analysis of a species tree\nreconstruction method that takes into account estimation errors at the gene\nlevel. Under that criterion, we also devise a novel reconstruction algorithm\nthat provably improves over all previous methods in a regime of interest.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 16:54:20 GMT"}, {"version": "v2", "created": "Mon, 30 Jun 2014 09:53:06 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Dasarathy", "Gautam", ""], ["Nowak", "Robert", ""], ["Roch", "Sebastien", ""]]}, {"id": "1404.7550", "submitter": "Gaurav Thakur", "authors": "Gaurav Thakur", "title": "The Synchrosqueezing transform for instantaneous spectral analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Synchrosqueezing transform is a time-frequency analysis method that can\ndecompose complex signals into time-varying oscillatory components. It is a\nform of time-frequency reassignment that is both sparse and invertible,\nallowing for the recovery of the signal. This article presents an overview of\nthe theory and stability properties of Synchrosqueezing, as well as\napplications of the technique to topics in cardiology, climate science and\neconomics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 22:31:21 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Thakur", "Gaurav", ""]]}, {"id": "1404.7646", "submitter": "Yuri Gordienko G.", "authors": "Yuri G. Gordienko", "title": "Migration-Driven Hierarchical Crystal Defect Aggregation - Symmetry and\n  Scaling Analysis", "comments": "8 pages, 5 figures; Proc. 17th European Conference on Fracture -\n  ECF17 (Czech Republic, Brno, 2-5 September 2008), 249-256. $D_2(n)$ was\n  corrected later in https://arxiv.org/abs/1104.5381", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the hierarchical defect substructures were observed experimentally\nin several metals and alloys before and after fracture. The general models of\ncrystal defect aggregation with appearance hierarchical defect substructures\nare proposed and considered in the wide range of scales. Their general group\nanalysis is performed, and symmetries of the governing equations are\nidentified. The models of defect aggregate growth are considered for several\npartial cases and compared with classical Lifshitz-Slyozov-Wagner theory of\ncoarsening, Leyvraz-Redner scaling theory of aggregate growth, etc. The reduced\nequations of new models are generated and solved, and the general scaling\nsolutions are given. The results obtained are illustrated by preliminary\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 09:24:19 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Gordienko", "Yuri G.", ""]]}]