[{"id": "1611.00127", "submitter": "Quan Bui", "authors": "Quan M. Bui, Howard C. Elman, J.D. Moulton", "title": "Algebraic Multigrid Preconditioners for Multiphase Flow in Porous Media", "comments": null, "journal-ref": null, "doi": "10.1137/16M1082652", "report-no": "LA-UR-16-28063", "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiphase flow is a critical process in a wide range of applications,\nincluding carbon sequestration, contaminant remediation, and groundwater\nmanagement. Typically, this process is modeled by a nonlinear system of partial\ndifferential equations derived by considering the mass conservation of each\nphase (e.g., oil, water), along with constitutive laws for the relationship of\nphase velocity to phase pressure. In this study, we develop and study efficient\nsolution algorithms for solving the algebraic systems of equations derived from\na fully coupled and time-implicit treatment of models of multiphase flow. We\nexplore the performance of several preconditioners based on algebraic multigrid\n(AMG) for solving the linearized problem, including \"black-box\" AMG applied\ndirectly to the system, a new version of constrained pressure residual\nmultigrid (CPR-AMG) preconditioning, and a new preconditioner derived using an\napproximate Schur complement arising from the block factorization of the\nJacobian. We show that the new methods are the most robust with respect to\nproblem character as determined by varying effects of capillary pressures, and\nwe show that the block factorization preconditioner is both efficient and\nscales optimally with problem size.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 04:52:47 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Bui", "Quan M.", ""], ["Elman", "Howard C.", ""], ["Moulton", "J. D.", ""]]}, {"id": "1611.00531", "submitter": "Maria Girardi", "authors": "Maria Girardi, Cristina Padovani, Daniele Pellegrini", "title": "Modal Analysis of Masonry Structures", "comments": "50 pages, 23 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new numerical procedure for evaluating the vibration\nfrequencies and mode shapes of masonry buildings in the presence of cracks. The\nalgorithm has been implemented within the NOSA-ITACA code, which models masonry\nas a nonlinear elastic material with zero tensile strength. Some case studies\nare reported, and the differences between linear and nonlinear behavior\nhighlighted.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 10:18:13 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Girardi", "Maria", ""], ["Padovani", "Cristina", ""], ["Pellegrini", "Daniele", ""]]}, {"id": "1611.00606", "submitter": "Diego Fabregat-Traver", "authors": "Diego Fabregat-Traver (1), Davor Davidovi\\'c (2), Markus H\\\"ohnerbach\n  (1), Edoardo Di Napoli (3 and 4) ((1) AICES, RWTH Aachen University, (2) RBI,\n  Zagreb, Croatia, (3) J\\\"ulich Supercomputing Centre, (4) J\\\"ulich Aachen\n  Research Alliance -- High-performance Computing)", "title": "Hybrid CPU-GPU generation of the Hamiltonian and Overlap matrices in\n  FLAPW methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the integration of high-performance numerical\nlibraries in ab initio codes and the portability of performance and\nscalability. The target of our work is FLEUR, a software for electronic\nstructure calculations developed in the Forschungszentrum J\\\"ulich over the\ncourse of two decades. The presented work follows up on a previous effort to\nmodernize legacy code by re-engineering and rewriting it in terms of highly\noptimized libraries. We illustrate how this initial effort to get efficient and\nportable shared-memory code enables fast porting of the code to emerging\nheterogeneous architectures. More specifically, we port the code to nodes\nequipped with multiple GPUs. We divide our study in two parts. First, we show\nconsiderable speedups attained by minor and relatively straightforward code\nchanges to off-load parts of the computation to the GPUs. Then, we identify\nfurther possible improvements to achieve even higher performance and\nscalability. On a system consisting of 16-cores and 2 GPUs, we observe speedups\nof up to 5x with respect to our optimized shared-memory code, which in turn\nmeans between 7.5x and 12.5x speedup with respect to the original FLEUR code.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 13:48:29 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Fabregat-Traver", "Diego", "", "3 and 4"], ["Davidovi\u0107", "Davor", "", "3 and 4"], ["H\u00f6hnerbach", "Markus", "", "3 and 4"], ["Di Napoli", "Edoardo", "", "3 and 4"]]}, {"id": "1611.00616", "submitter": "Jiafeng Xu", "authors": "Jiafeng Xu, Karl Henning Halse", "title": "Dual Quaternion Variational Integrator for Rigid Body Dynamic Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a symplectic dual quaternion variational integrator(DQVI) for\nsimulating single rigid body motion in all six degrees of freedom. Dual\nquaternion is used to represent rigid body kinematics and one-step Lie group\nvariational integrator is used to conserve the geometric structure, energy and\nmomentum of the system during the simulation. The combination of these two\nbecomes the first Lie group variational integrator for rigid body simulation\nwithout decoupling translations and rotations. Newton-Raphson method is used to\nsolve the recursive dynamic equation. This method is suitable for real-time\nrigid body simulations with high precision under large time step. DQVI respects\nthe symplectic structure of the system with excellent long-term conservation of\ngeometry structure, momentum and energy. It also allows the reference point and\n6-by-6 inertia matrix to be arbitrarily defined, which is very convenient for a\nvariety of engineering problems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 14:02:16 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 15:49:09 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Xu", "Jiafeng", ""], ["Halse", "Karl Henning", ""]]}, {"id": "1611.00880", "submitter": "Angxiu Ni", "authors": "Angxiu Ni, Qiqi Wang", "title": "Sensitivity analysis on chaotic dynamical system by Non-Intrusive Least\n  Square Shadowing (NILSS)", "comments": "26 pages, 10 figures", "journal-ref": "Journal of Computational Physics, Volume 347, Page 56-77, 2017", "doi": "10.1016/j.jcp.2017.06.033", "report-no": null, "categories": "physics.comp-ph cs.CE cs.NA math.NA nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops the non-intrusive formulation of the Least-squares\nshadowing (LSS) method, for computing the sensitivity of long-time averaged\nobjectives in chaotic dynamical systems. This non-intrusive formulation\nconstrains the computation to only the unstable subspace, greatly reducing the\ncost of LSS for many problems; moreover, it reparametrizes the LSS problem,\nrequiring only minor modifications to existing tangent solvers. NILSS is\ndemonstrated on a chaotic flow over a backward-facing step simulated with a\nmesh of 12e3 cells.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 04:22:07 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 04:38:41 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 16:24:11 GMT"}, {"version": "v4", "created": "Sat, 26 Nov 2016 03:53:50 GMT"}, {"version": "v5", "created": "Wed, 11 Jan 2017 15:11:07 GMT"}, {"version": "v6", "created": "Tue, 31 Jan 2017 22:00:10 GMT"}, {"version": "v7", "created": "Sat, 13 May 2017 14:51:10 GMT"}, {"version": "v8", "created": "Wed, 12 Jul 2017 23:50:45 GMT"}, {"version": "v9", "created": "Fri, 21 Jun 2019 18:03:27 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ni", "Angxiu", ""], ["Wang", "Qiqi", ""]]}, {"id": "1611.01463", "submitter": "Nonthachote Chatsanga", "authors": "Nonthachote Chatsanga, Andrew J. Parkes", "title": "International Portfolio Optimisation with Integrated Currency Overlay\n  Costs and Constraints", "comments": null, "journal-ref": null, "doi": "10.1016/j.eswa.2017.04.009", "report-no": null, "categories": "q-fin.PM cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio optimisation typically aims to provide an optimal allocation that\nminimises risk, at a given return target, by diversifying over different\ninvestments. However, the potential scope of such risk diversification can be\nlimited if investments are concentrated in only one country, or more\nspecifically one currency. Multi-currency portfolio is an alternative to\nachieve higher returns and more diversified portfolios but it requires a\ncareful management of the entailed risks from changes in exchange rates.\n  The deviation between asset and currency exposures in a portfolio is defined\nas the \"currency overlay\". This paper addresses risk mitigation by allowing\ncurrency overlay and asset allocation be optimised together. We propose a model\nof the international portfolio optimisation problem in which the currency\noverlay is constructed by holding foreign exchange rate forward contracts.\nCrucially, the cost of carry, transaction costs, and margin requirement of\nforward contracts are also taken into account in portfolio return calculation.\nThis novel extension of previous overlay models improves the accuracy of risk\nand return calculation of portfolios; furthermore, our experimental results\nshow that inclusion of such costs significantly changes the optimal decisions.\nEffects of constraints imposed to reduce transaction costs associated are\nexamined and the empirical results show that risk-return compensation of\nportfolios varies significantly with different return targets.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 17:47:05 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Chatsanga", "Nonthachote", ""], ["Parkes", "Andrew J.", ""]]}, {"id": "1611.01598", "submitter": "Natalie  Perlin", "authors": "Natalie Perlin, Joel P. Zysman, Ben P. Kirtman", "title": "Practical scalability assesment for parallel scientific numerical\n  applications", "comments": "9 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of scalability analysis of numerical parallel applications has\nbeen revisited, with the specific goals defined for the performance estimation\nof research applications. A series of Community Climate Model System (CCSM)\nnumerical simulations were used to test the several MPI implementations,\ndetermine optimal use of the system resources, and their scalability. The\nscaling capacity and model throughput performance metrics for $N$ cores showed\na log-linear behavior approximated by a power fit in the form of $C(N)=bN^a$,\nwhere $a$ and $b$ are two empirical constants. Different metrics yielded\nidentical power coefficients ($a$), but different dimensionality coefficients\n($b$). This model was consistent except for the large numbers of N. The power\nfit approach appears to be very useful for scalability estimates, especially\nwhen no serial testing is possible. Scalability analysis of additional\nscientific application has been conducted in the similar way to validate the\nrobustness of the power fit approach.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 03:55:18 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Perlin", "Natalie", ""], ["Zysman", "Joel P.", ""], ["Kirtman", "Ben P.", ""]]}, {"id": "1611.02256", "submitter": "Zheng Zhang", "authors": "Zheng Zhang and Tsui-Wei Weng and Luca Daniel", "title": "A Big-Data Approach to Handle Many Process Variations: Tensor Recovery\n  and Applications", "comments": "8 figures", "journal-ref": "IEEE Transactions on Component, Packaging and Manufacturing\n  Technology, 2017", "doi": null, "report-no": null, "categories": "cs.CE math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fabrication process variations are a major source of yield degradation in the\nnano-scale design of integrated circuits (IC), microelectromechanical systems\n(MEMS) and photonic circuits. Stochastic spectral methods are a promising\ntechnique to quantify the uncertainties caused by process variations. Despite\ntheir superior efficiency over Monte Carlo for many design cases, these\nalgorithms suffer from the curse of dimensionality; i.e., their computational\ncost grows very fast as the number of random parameters increases. In order to\nsolve this challenging problem, this paper presents a high-dimensional\nuncertainty quantification algorithm from a big-data perspective. Specifically,\nwe show that the huge number of (e.g., $1.5 \\times 10^{27}$) simulation samples\nin standard stochastic collocation can be reduced to a very small one (e.g.,\n$500$) by exploiting some hidden structures of a high-dimensional data array.\nThis idea is formulated as a tensor recovery problem with sparse and low-rank\nconstraints; and it is solved with an alternating minimization approach.\nNumerical results show that our approach can simulate efficiently some ICs, as\nwell as MEMS and photonic problems with over 50 independent random parameters,\nwhereas the traditional algorithm can only handle several random parameters.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:35:35 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Zhang", "Zheng", ""], ["Weng", "Tsui-Wei", ""], ["Daniel", "Luca", ""]]}, {"id": "1611.03725", "submitter": "Shweta Sagari", "authors": "Shweta Sagari, Larry Greenstein, Wade Trappe", "title": "Practical Interpolation for Spectrum Cartography through Local Path Loss\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental building block for supporting better utilization of radio\nspectrum involves predicting the impact that an emitter will have at different\ngeographic locations. To this end, fixed sensors can be deployed to spatially\nsample the RF environment over an area of interest, with interpolation methods\nused to infer received power at locations between sensors. This paper describes\na radio map interpolation method that exploits the known properties of most\npath loss models, with the aim of minimizing the RMS errors in predicted\ndB-power. We show that the results come very close to those for ideal Simple\nKriging. Moreover, the method is simpler in terms of real-time computation by\nthe network and it requires no knowledge of the spatial correlation of shadow\nfading. Our analysis of the method is general, but we exemplify it for a\nspecific network geometry, comprising a grid-like pattern of sensors. We also\nprovide comparisons to other widely used interpolation methods.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 07:23:43 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Sagari", "Shweta", ""], ["Greenstein", "Larry", ""], ["Trappe", "Wade", ""]]}, {"id": "1611.06271", "submitter": "Utkarsh R. Patel", "authors": "Utkarsh R. Patel, Piero Triverio, and Sean V. Hum", "title": "A Novel Single-Source Surface Integral Method to Compute Scattering from\n  Dielectric Objects", "comments": "Submitted to IEEE Antennas and Wireless Propagation Letters on\n  November 18, 2016", "journal-ref": null, "doi": "10.1109/LAWP.2017.2669183", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the traditional surface integral methods, the computation of scattering\nfrom a dielectric object requires two equivalent current densities on the\nboundary of the dielectric. In this paper, we present an approach that requires\nonly a single current density. Our method is based on a surface admittance\noperator and is applicable to dielectric bodies of arbitrary shape. The\nformulation results in four times lower memory consumption and up to eight\ntimes lower time to solve the linear system than the traditional PMCHWT\nformulation. Numerical results demonstrate that the proposed technique is as\naccurate as the PMCHWT formulation.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 23:02:35 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Patel", "Utkarsh R.", ""], ["Triverio", "Piero", ""], ["Hum", "Sean V.", ""]]}, {"id": "1611.06396", "submitter": "Huu Phuoc Bui", "authors": "Huu Phuoc Bui, Vincent Richefeu and Fr\\'ed\\'eric Dufour", "title": "Studying the influence of inclusion characteristics on the\n  characteristic length involved in quasi-brittle materials using the lattice\n  element method", "comments": "16 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike nonlocal models, there is no need to introduce an internal length in\nthe constitutive law for lattice model at the mesoscopic scale. Actually, the\ninternal length is not explicitly introduced but rather governed by the\nmesostructure characteristics themselves. The influence of the mesostructure on\nthe width of the fracture process zone which is assumed to be correlated to the\ncharacteristic length of the homogenized quasi-brittle material is studied. The\ninfluence of the ligament size (a structural parameter) is also investigated.\nThis analysis provides recommendations/warnings when extracting an internal\nlength required for nonlocal damage models from the material mesostructure\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 16:17:12 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Bui", "Huu Phuoc", ""], ["Richefeu", "Vincent", ""], ["Dufour", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1611.06436", "submitter": "Christoph Meier", "authors": "Christoph Meier and Maximilian J. Grill and Wolfgang A. Wall and\n  Alexander Popp", "title": "Geometrically exact beam elements and smooth contact schemes for the\n  modeling of fiber-based materials and structures", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijsolstr.2017.07.020", "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the authors have proposed a novel all-angle beam contact (ABC)\nformulation that combines the advantages of existing point and line contact\nmodels in a variationally consistent manner. However, the ABC formulation has\nso far only been applied in combination with a special torsion-free beam model,\nwhich yields a very simple and efficient finite element formulation, but which\nis restricted to initially straight beams with isotropic cross-sections. In\norder to abstain from these restrictions, the current work combines the ABC\nformulation with a geometrically exact Kirchhoff-Love beam element formulation\nthat is capable of treating even the most general cases of slender beam\nproblems in terms of initial geometry and external loads. While the neglect of\nshear deformation that is inherent to this formulation has been shown to\nprovide considerable numerical advantages in the range of high beam slenderness\nratios, alternative shear-deformable beam models are required for examples with\nthick beams. The current contribution additionally proposes a novel\ngeometrically exact beam element based on the Simo-Reissner theory. Similar to\nthe torsion-free and the Kirchhoff-Love beam elements, also this Simo-Reissner\nelement is based on a C1-continuous Hermite interpolation of the beam\ncenterline, which will allow for smooth contact kinematics. For this Hermitian\nSimo-Reissner element, a consistent spatial convergence behavior as well as the\nsuccessful avoidance of membrane and shear locking will be demonstrated\nnumerically. All in all, the combination of the ABC formulation with these\ndifferent beam element variants (i.e.~the torsion-free element, the\nKirchhoff-Love element and the Simo-Reissner element) results in a very\nflexible and modular simulation framework that allows to choose the optimal\nelement formulation for any given application in terms of accuracy, efficiency\nand robustness.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 22:11:12 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Meier", "Christoph", ""], ["Grill", "Maximilian J.", ""], ["Wall", "Wolfgang A.", ""], ["Popp", "Alexander", ""]]}, {"id": "1611.06721", "submitter": "Sebastian Sch\\\"ops", "authors": "Jennifer Dutin\\'e and Markus Clemens and Sebastian Sch\\\"ops", "title": "Multiple Right-Hand Side Techniques in Semi-Explicit Time Integration\n  Methods for Transient Eddy Current Problems", "comments": "4 pages, 5 figures", "journal-ref": "IEEE Trans. Magn., Volume: 53, Issue: 6, June 2017", "doi": "10.1109/TMAG.2017.2682558", "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatially discretized magnetic vector potential formulation of\nmagnetoquasistatic field problems is transformed from an infinitely stiff\ndifferential algebraic equation system into a finitely stiff ordinary\ndifferential equation (ODE) system by application of a generalized Schur\ncomplement for nonconducting parts. The ODE can be integrated in time using\nexplicit time integration schemes, e.g. the explicit Euler method. This\nrequires the repeated evaluation of a pseudo-inverse of the discrete curl-curl\nmatrix in nonconducting material by the preconditioned conjugate gradient (PCG)\nmethod which forms a multiple right-hand side problem. The subspace projection\nextrapolation method and proper orthogonal decomposition are compared for the\ncomputation of suitable start vectors in each time step for the PCG method\nwhich reduce the number of iterations and the overall computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 11:04:26 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 09:02:33 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Dutin\u00e9", "Jennifer", ""], ["Clemens", "Markus", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "1611.06868", "submitter": "Bjorn Gustavsen", "authors": "Bjorn Gustavsen, Alvaro Portillo, Rodrigo Ronchi, Asgeir Mjelve", "title": "High-Frequency Modeling and Simulation of a Single-Phase Three-Winding\n  Transformer Including Taps in Regulating Winding", "comments": "8 pages, 21 figures, to be submitted to IEEE Transactions on Power\n  Delivery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer terminal equivalents obtained via admittance measurements are\nsuitable for simulating high-frequency transient interaction between the\ntransformer and the network. This paper augments the terminal equivalent\napproach with a measurement-based voltage transfer function model which permits\ncalculation of voltages at internal points in the regulating winding. The\napproach is demonstrated for a single-phase three-winding transformer in tap\nposition Nom+ with inclusion of three internal points in the regulating winding\nthat represent the mid-point and the two extreme ends. The terminal equivalent\nmodeling makes use of additional common-mode measurements to avoid error\nmagnifications to result from the ungrounded tertiary winding. The final model\nis used in a time domain simulation where ground-fault initiation results in a\nresonant voltage build-up in the winding. It is shown that that the peak value\nof the resonant overvoltage can be higher than during the lightning impulse\ntest, with unfavorable network conditions. Additional measurements show that\nthe selected tap position affects the terminal behavior of the transformer,\nchanging the frequency and peak value of the lower resonance point in the\nvoltage transfer between windings.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 14:27:39 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Gustavsen", "Bjorn", ""], ["Portillo", "Alvaro", ""], ["Ronchi", "Rodrigo", ""], ["Mjelve", "Asgeir", ""]]}, {"id": "1611.07297", "submitter": "Corneliu Arsene Dr", "authors": "Corneliu T.C. Arsene", "title": "High-Performance and Distributed Computing in a Probabilistic Finite\n  Element Comparison Study of the Human Lower Leg Model with Total Knee\n  Replacement", "comments": "An extended version of this paper is available from IEEE organisation\n  and it was published in 2018 IEEE Conference on Computational Intelligence in\n  Bioinformatics and Computational Biology (CIBCB), DOI:\n  10.1109/CIBCB.2018.8404959 and with the same title and pages 1-9", "journal-ref": null, "doi": "10.5281/zenodo.154259", "report-no": null, "categories": "cs.CE physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reliability theory is used to assess the sensitivity of a passive flexion and\nactive flexion of the human lower leg Finite Element (FE) models with Total\nKnee Replacement (TKR) to the variability in the input parameters of the\nrespective FE models. The sensitivity of the active flexion simulating the\nstair ascent of the human lower leg FE model with TKR was presented before in\n[1,2] whereas now in this paper a comparison is made with the passive flexion\nof the human lower leg FE model with TKR. First, with the Monte Carlo\nSimulation Technique (MCST), a number of randomly generated input data of the\nFE model(s) are obtained based on the normal standard deviations of the\nrespective input parameters. Then a series of FE simulations are done and the\noutput kinematics and peak contact pressures are obtained for the respective FE\nmodels (passive flexion and/or active flexion models). Seven output performance\nmeasures are reported for the passive flexion model and one more parameter was\nreported for the active flexion FE model (patello-femoral peak contact\npressure) in [1]. A sensitivity study will be performed based on the Response\nSurface Method (RSM) to identify the key parameters that influence the\nkinematics and peak contact pressures of the passive flexion FE model. Another\ntwo MCST and RSM-based probabilistic FE analyses will be performed based on a\nreduced list of 19 key input parameters. In total 4 probabilistic FE analyses\nwill be performed: 2 probabilistic FE analyses (MCST and RSM) based on an\nextended set of 78 input variables and another 2 probabilistic FE analyses\n(MCST and RSM) based on a reduced set of 19 input variables. Due to the likely\ncomputation cost in order to make hundreds of FE simulations with MCST, a\nhigh-performance and distributed computing system will be used for the passive\nflexion FE model the same as it was used for the active flexion FE model in\n[1].\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 13:39:04 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 13:27:27 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 04:33:49 GMT"}, {"version": "v4", "created": "Wed, 18 Jul 2018 21:54:39 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Arsene", "Corneliu T. C.", ""]]}, {"id": "1611.07368", "submitter": "Mariusz Klimek", "authors": "Mariusz Klimek, Stefan Kurz, Sebastian Schoeps and Thomas Weiland", "title": "Discretization of Maxwell's Equations for Non-inertial Observers Using\n  Space-Time Algebra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We employ Maxwell's equations formulated in Space-Time Algebra to perform\ndiscretization of moving geometries directly in space-time. All the derivations\nare carried out without any non-relativistic assumptions, thus the application\narea of the scheme is not restricted to low velocities. The 4D mesh\nconstruction is based on a 3D mesh stemming from a conventional 3D mesh\ngenerator. The movement of the system is encoded in the 4D mesh geometry,\nenabling an easy extension of well-known 3D approaches to the space-time\nsetting. As a research example, we study a manifestation of Sagnac's effect in\na rotating ring resonator. In case of constant rotation, the space-time\napproach enhances the efficiency of the scheme, as the material matrices are\nconstant for every time step, without abandoning the relativistic framework.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 13:11:29 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Klimek", "Mariusz", ""], ["Kurz", "Stefan", ""], ["Schoeps", "Sebastian", ""], ["Weiland", "Thomas", ""]]}, {"id": "1611.07403", "submitter": "Sebastian Sch\\\"ops", "authors": "Ulrich R\\\"omer and Christian Schmidt and Ursula van Rienen and\n  Sebastian Sch\\\"ops", "title": "Low-Dimensional Stochastic Modeling of the Electrical Properties of\n  Biological Tissues", "comments": "4 pages, 5 figures", "journal-ref": "IEEE Trans. Magn, Volume: 53, Issue: 6, June 2017", "doi": "10.1109/TMAG.2017.2668841", "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification plays an important role in biomedical engineering\nas measurement data is often unavailable and literature data shows a wide\nvariability. Using state-of-the-art methods one encounters difficulties when\nthe number of random inputs is large. This is the case, e.g., when using\ncomposite Cole-Cole equations to model random electrical properties. It is\nshown how the number of parameters can be significantly reduced by the\nKarhunen-Loeve expansion. The low-dimensional random model is used to quantify\nuncertainties in the axon activation during deep brain stimulation. Numerical\nresults for a Medtronic 3387 electrode design are given.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 16:38:28 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 08:55:33 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["R\u00f6mer", "Ulrich", ""], ["Schmidt", "Christian", ""], ["van Rienen", "Ursula", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "1611.07910", "submitter": "Isaac Skog", "authors": "Johan Wahlstr\\\"om, Isaac Skog, Jo\\~ao G. P. Rodrigues, Peter H\\\"andel,\n  Ana Aguiar", "title": "Map-aided Dead-reckoning --- A Study on Locational Privacy in Insurance\n  Telematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a particle-based framework for estimating the position of a\nvehicle using map information and measurements of speed. Two measurement\nfunctions are considered. The first is based on the assumption that the lateral\nforce on the vehicle does not exceed critical limits derived from physical\nconstraints. The second is based on the assumption that the driver approaches a\ntarget speed derived from the speed limits along the upcoming trajectory.\nPerformance evaluations of the proposed method indicate that end destinations\noften can be estimated with an accuracy in the order of $100\\,[m]$. These\nresults expose the sensitivity and commercial value of data collected in many\nof today's insurance telematics programs, and thereby have privacy implications\nfor millions of policyholders. We end by discussing the strengths and\nweaknesses of different methods for anonymization and privacy preservation in\ntelematics programs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 10:04:45 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Wahlstr\u00f6m", "Johan", ""], ["Skog", "Isaac", ""], ["Rodrigues", "Jo\u00e3o G. P.", ""], ["H\u00e4ndel", "Peter", ""], ["Aguiar", "Ana", ""]]}, {"id": "1611.08266", "submitter": "Hui Liu Mr", "authors": "Hui Liu", "title": "Researches on Dynamic Load Balancing Algorithms and hp-Adaptivity in 3-D\n  Parallel Adaptive Finite Element Computations", "comments": "in Chinese", "journal-ref": null, "doi": "10.13140/RG.2.2.16168.98566", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is related to PHG (Parallel Hierarchical Grid). PHG is a toolbox\nfor developing parallel adaptive finite element programs, which is under active\ndevelopment at the State Key Laboratory of Scientific and Engineering\nComputing. The main results of this work are as follows.\n  1) For the tetrahedral meshes used in PHG, under reasonable assumptions, we\nproved the existence of through-vertex Hamiltonian paths between arbitrary two\nvertices, as well as the existence of through-vertex Hamiltonian cycles, and\ndesigned an efficient algorithm with linear complexity for constructing\nthrough-vertex Hamiltonian paths. The resulting algorithm has been implemented\nin PHG, and is used for ordering elements in the coarsest mesh for the\nrefinement tree mesh partitioning algorithm.\n  2) We designed encoding and decoding algorithms for high dimensional Hilbert\norder. Hilbert order has good locality, and it has wide applications in various\nfields in computer science, such as memory management, database, and dynamic\nload balancing.\n  3) We implemented refinement tree and space filling curve based mesh\npartitioning algorithms in PHG, and designed the dynamic load balancing module\nof PHG. The refinement tree based partitioning algorithm was originally\nproposed by Mitchell, the one implemented in PHG was improved in several\naspects. The space filling curve based mesh partitioning function in PHG can\nuse either Hilbert or Morton space filling curve.\n  4) We studied existing hp-adaptive strategies in the literature, and proposed\na new strategy. Numerical experiments show that our new strategy achieves\nexponential convergence, and is superior, in both precision of the solutions\nand computation time, to the strategy compared. This part of the work also\nserves to validate the hp-adaptivity module of PHG.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 17:31:41 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Liu", "Hui", ""]]}, {"id": "1611.08438", "submitter": "Sebastian Sch\\\"ops", "authors": "Ulrich R\\\"omer and Sebastian Sch\\\"ops and Herbert De Gersem", "title": "A Defect Corrected Finite Element Approach for the Accurate Evaluation\n  of Magnetic Fields on Unstructured Grids", "comments": "19 pages, 10 figures", "journal-ref": "Journal of Computational Physics, 335 (2017), 688-699", "doi": "10.1016/j.jcp.2017.01.041", "report-no": null, "categories": "math.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In electromagnetic simulations of magnets and machines one is often\ninterested in a highly accurate and local evaluation of the magnetic field\nuniformity. Based on local post-processing of the solution, a defect correction\nscheme is proposed as an easy to realize alternative to higher order finite\nelement or hybrid approaches. Radial basis functions (RBF)s are key for the\ngenerality of the method, which in particular can handle unstructured grids.\nAlso, contrary to conventional finite element basis functions, higher\nderivatives of the solution can be evaluated, as required, e.g., for deflection\nmagnets. Defect correction is applied to obtain a solution with improved\naccuracy and adjoint techniques are used to estimate the remaining error for a\nspecific quantity of interest. Significantly improved (local) convergence\norders are obtained. The scheme is also applied to the simulation of a\nStern-Gerlach magnet currently in operation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 12:33:30 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["R\u00f6mer", "Ulrich", ""], ["Sch\u00f6ps", "Sebastian", ""], ["De Gersem", "Herbert", ""]]}, {"id": "1611.08758", "submitter": "Kalyana Babu Nakshatrala", "authors": "J. Chang and K. B. Nakshatrala", "title": "Variational inequality approach to enforce the non-negative constraint\n  for advection-diffusion equations", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2017.03.022", "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive simulations are crucial for the success of many subsurface\napplications, and it is highly desirable to obtain accurate non-negative\nsolutions for transport equations in these numerical simulations. In this\npaper, we propose a computational framework based on the variational inequality\n(VI) which can also be used to enforce important mathematical properties (e.g.,\nmaximum principles) and physical constraints (e.g., the non-negative\nconstraint). We demonstrate that this framework is not only applicable to\ndiffusion equations but also to non-symmetric advection-diffusion equations. An\nattractive feature of the proposed framework is that it works with with any\nweak formulation for the advection-diffusion equations, including single-field\nformulations, which are computationally attractive. A particular emphasis is\nplaced on the parallel and algorithmic performance of the VI approach across\nlarge-scale and heterogeneous problems. It is also shown that QP and VI are\nequivalent under certain conditions. State-of-the-art QP and VI solvers\navailable from the PETSc library are used on a variety of steady-state 2D and\n3D benchmarks, and a comparative study on the scalability between the QP and VI\nsolvers is presented. We then extend the proposed framework to transient\nproblems by simulating the miscible displacement of fluids in a heterogeneous\nporous medium and illustrate the importance of enforcing maximum principles for\nthese types of coupled problems. Our numerical experiments indicate that VIs\nare indeed a viable approach for enforcing the maximum principles and the\nnon-negative constraint in a large-scale computing environment. Also provided\nare Firedrake project files as well as a discussion on the computer\nimplementation to help facilitate readers in understanding the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 23:02:31 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 00:10:42 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Chang", "J.", ""], ["Nakshatrala", "K. B.", ""]]}, {"id": "1611.09131", "submitter": "Scott Haag", "authors": "Scott Haag, Ali Shokoufandeh", "title": "Development of a data model to facilitate rapid Watershed Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data model to store and retrieve surface watershed boundaries using graph\ntheoretic approaches is proposed. This data model integrates output from a\nstandard digital elevation models (DEM) derived stream catchment boundaries,\nand vector representation of stream centerlines then applies them to three\nnovel algorithms. The first is called Modified Nested Set (MNS), which is a\ndepth first graph traversal algorithm that searches across stream reaches\n(vertices) and stream junctions (edges) labeling vertices by their discovery\ntime, finish time, and distance from the root. The second is called Log Reduced\nGraphs (LRG), which creates a set S of logarithmically reduced graphs from the\noriginal data, to store the watershed boundaries. The final algorithm is called\nStitching Watershed, which provides a technique to merge watershed boundaries\nacross the set of graphs created in the LRG algorithm.\n  This technique was applied to the ~ 30,600 km2 Delaware River Watershed and\ncompared to hypothetical data storage models in terms of prep-processing, data\nstorage, and query complexity costs. Results show that the proposed technique\nprovides significant benefits vs. the hypothetical methods with a 99-98%\nreduction in prepossessing, 96-80% reduction in query complexity and a 76%\nreduction in storage costs. The increasing availability of high resolution\nelevation data within the United States and the internationally provides an\nopportunity to extend these results to other watersheds through the world.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 15:31:00 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Haag", "Scott", ""], ["Shokoufandeh", "Ali", ""]]}, {"id": "1611.09835", "submitter": "Saptarshi Das", "authors": "Indranil Pan, Saptarshi Das and Shantanu Das", "title": "Multi-objective Active Control Policy Design for Commensurate and\n  Incommensurate Fractional Order Chaotic Financial Systems", "comments": "26 pages, 8 figures, 2 tables", "journal-ref": "Applied Mathematical Modelling, Volume 39, Issue 2, 15 January\n  2015, Pages 500-514", "doi": "10.1016/j.apm.2014.06.005", "report-no": null, "categories": "math.OC cs.CE cs.NE cs.SY nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an active control policy design for a fractional order (FO)\nfinancial system is attempted, considering multiple conflicting objectives. An\nactive control template as a nonlinear state feedback mechanism is developed\nand the controller gains are chosen within a multi-objective optimization (MOO)\nframework to satisfy the conditions of asymptotic stability, derived\nanalytically. The MOO gives a set of solutions on the Pareto optimal front for\nthe multiple conflicting objectives that are considered. It is shown that there\nis a trade-off between the multiple design objectives and a better performance\nin one objective can only be obtained at the cost of performance deterioration\nin the other objectives. The multi-objective controller design has been\ncompared using three different MOO techniques viz. Non Dominated Sorting\nGenetic Algorithm-II (NSGA-II), epsilon variable Multi-Objective Genetic\nAlgorithm (ev-MOGA), and Multi Objective Evolutionary Algorithm with\nDecomposition (MOEA/D). The robustness of the same control policy designed with\nthe nominal system settings have been investigated also for gradual decrease in\nthe commensurate and incommensurate fractional orders of the financial system.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:47:25 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Pan", "Indranil", ""], ["Das", "Saptarshi", ""], ["Das", "Shantanu", ""]]}]