[{"id": "1707.00516", "submitter": "Siddharth Samsi", "authors": "Siddharth Samsi, Brian Helfer, Jeremy Kepner, Albert Reuther and\n  Darrell O. Ricke", "title": "A Linear Algebra Approach to Fast DNA Mixture Analysis Using GPUs", "comments": "Accepted for publication at the 2017 IEEE High Performance Extreme\n  Computing conference", "journal-ref": null, "doi": "10.1109/HPEC.2017.8091027", "report-no": null, "categories": "cs.PF cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of DNA samples is an important step in forensics, and the speed of\nanalysis can impact investigations. Comparison of DNA sequences is based on the\nanalysis of short tandem repeats (STRs), which are short DNA sequences of 2-5\nbase pairs. Current forensics approaches use 20 STR loci for analysis. The use\nof single nucleotide polymorphisms (SNPs) has utility for analysis of complex\nDNA mixtures. The use of tens of thousands of SNPs loci for analysis poses\nsignificant computational challenges because the forensic analysis scales by\nthe product of the loci count and number of DNA samples to be analyzed. In this\npaper, we discuss the implementation of a DNA sequence comparison algorithm by\nre-casting the algorithm in terms of linear algebra primitives. By developing\nan overloaded matrix multiplication approach to DNA comparisons, we can\nleverage advances in GPU hardware and algoithms for Dense Generalized\nMatrix-Multiply (DGEMM) to speed up DNA sample comparisons. We show that it is\npossible to compare 2048 unknown DNA samples with 20 million known samples in\nunder 6 seconds using a NVIDIA K80 GPU.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 12:57:54 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Samsi", "Siddharth", ""], ["Helfer", "Brian", ""], ["Kepner", "Jeremy", ""], ["Reuther", "Albert", ""], ["Ricke", "Darrell O.", ""]]}, {"id": "1707.00953", "submitter": "Rodrigo de Lamare", "authors": "H. Ruan and R. C. de Lamare", "title": "Study of Joint MMSE Consensus and Relay Selection Algorithms for\n  Distributed Beamforming", "comments": "2 tables, 3 figures, 11 pages. arXiv admin note: text overlap with\n  arXiv:1310.7282", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CE cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents joint minimum mean-square error (MMSE) consensus algorithm\nand relay selection algorithms for distributed beamforming. We propose joint\nMMSE consensus relay and selection schemes with a total power constraint and\nlocal communications among the relays for a network with cooperating sensors.\nWe also devise greedy relay selection algorithms based on the MMSE consensus\napproach that optimize the network performance. Simulation results show that\nthe proposed scheme and algorithms outperform existing techniques for\ndistributed beamforming.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 16:56:40 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Ruan", "H.", ""], ["de Lamare", "R. C.", ""]]}, {"id": "1707.00994", "submitter": "Swakkhar Shatabda", "authors": "Farshid Rayhan, Sajid Ahmed, Swakkhar Shatabda, Dewan Md Farid, Zaynab\n  Mousavian, Abdollah Dehzangi, M Sohel Rahman", "title": "iDTI-ESBoost: Identification of Drug Target Interaction Using\n  Evolutionary and Structural Features with Boosting", "comments": "pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of new drug-target interactions is extremely important as it can\nlead the researchers to find new uses for old drugs and to realize the\ntherapeutic profiles or side effects thereof. However, experimental prediction\nof drug-target interactions is expensive and time-consuming. As a result,\ncomputational methods for prediction of new drug-target interactions have\ngained much interest in recent times. We present iDTI-ESBoost, a prediction\nmodel for identification of drug-target interactions using evolutionary and\nstructural features. Our proposed method uses a novel balancing technique and a\nboosting technique for the binary classification problem of drug-target\ninteraction. On four benchmark datasets taken from a gold standard data,\niDTI-ESBoost outperforms the state-of-the-art methods in terms of area under\nReceiver operating characteristic (auROC) curve. iDTI-ESBoost also outperforms\nthe latest and the best-performing method in the literature to-date in terms of\narea under precision recall (auPR) curve. This is significant as auPR curves\nare argued to be more appropriate as a metric for comparison for imbalanced\ndatasets, like the one studied in this research. In the sequel, our experiments\nestablish the effectiveness of the classifier, balancing methods and the novel\nfeatures incorporated in iDTI-ESBoost. iDTI-ESBoost is a novel prediction\nmethod that has for the first time exploited the structural features along with\nthe evolutionary features to predict drug-protein interactions. We believe the\nexcellent performance of iDTI-ESBoost both in terms of auROC and auPR would\nmotivate the researchers and practitioners to use it to predict drug-target\ninteractions. To facilitate that, iDTI-ESBoost is readily available for use at:\nhttp://farshidrayhan.pythonanywhere.com/iDTI-ESBoost/\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 13:55:51 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Rayhan", "Farshid", ""], ["Ahmed", "Sajid", ""], ["Shatabda", "Swakkhar", ""], ["Farid", "Dewan Md", ""], ["Mousavian", "Zaynab", ""], ["Dehzangi", "Abdollah", ""], ["Rahman", "M Sohel", ""]]}, {"id": "1707.01301", "submitter": "Denys Dutykh", "authors": "Gayaz Khakimzyanov, Denys Dutykh (LAMA), Oleg Gusev, Nina Shokina", "title": "Dispersive shallow water wave modelling. Part II: Numerical simulation\n  on a globally flat space", "comments": "67 pages, 17 figures, 3 tables, 149 references. Other author's papers\n  can be downloaded at http://www.denys-dutykh.com/", "journal-ref": "Commun. Comput. Phys. (2018), Vol. 23, Issue 1, pp. 30-92", "doi": "10.4208/cicp.OA-2016-0179b", "report-no": null, "categories": "physics.flu-dyn cs.CE physics.ao-ph physics.class-ph physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we describe a numerical method to solve numerically the weakly\ndispersive fully nonlinear Serre-Green-Naghdi (SGN) celebrated model. Namely,\nour scheme is based on reliable finite volume methods, proven to be very\neffective for the hyperbolic part of equations. The particularity of our study\nis that we develop an adaptive numerical model using moving grids. Moreover, we\nuse a special form of the SGN equations where non-hydrostatic part of pressure\nis found by solving a nonlinear elliptic equation. Moreover, this form of\ngoverning equations allows determining the natural form of boundary conditions\nto obtain a well-posed (numerical) problem.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 10:29:05 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 12:40:50 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 12:50:03 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Khakimzyanov", "Gayaz", "", "LAMA"], ["Dutykh", "Denys", "", "LAMA"], ["Gusev", "Oleg", ""], ["Shokina", "Nina", ""]]}, {"id": "1707.01556", "submitter": "Jean-Baptiste Chapelier", "authors": "Jean-Baptiste Chapelier, Bono Wasistho and Carlo Scalo", "title": "A Coherent vorticity preserving eddy viscosity correction for Large-Eddy\n  Simulation", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2018.01.012", "report-no": null, "categories": "cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new approach to Large-Eddy Simulation (LES) where\nsubgrid-scale (SGS) dissipation is applied proportionally to the degree of\nlocal spectral broadening, hence mitigated or deactivated in regions dominated\nby large-scale and/or laminar vortical motion. The proposed Coherent vorticity\npreserving (CvP) LES methodology is based on the evaluation of the ratio of the\ntest-filtered to resolved (or grid-filtered) enstrophy $\\sigma$. Values of\n$\\sigma$ close to 1 indicate low sub-test-filter turbulent activity, justifying\nlocal deactivation of the SGS dissipation. The intensity of the SGS dissipation\nis progressively increased for $\\sigma < 1$ which corresponds to a small-scale\nspectral broadening. The SGS dissipation is then fully activated in developed\nturbulence characterized by $\\sigma \\le \\sigma_{eq}$, where the value\n$\\sigma_{eq}$ is derived assuming a Kolmogorov spectrum. The proposed approach\ncan be applied to any eddy-viscosity model, is algorithmically simple and\ncomputationally inexpensive. LES of Taylor-Green vortex breakdown demonstrates\nthat the CvP methodology improves the performance of traditional, non-dynamic\ndissipative SGS models, capturing the peak of total turbulent kinetic energy\ndissipation during transition. Similar accuracy is obtained by adopting\nGermano's dynamic procedure albeit at more than twice the computational\noverhead. A CvP-LES of a pair of unstable periodic helical vortices is shown to\npredict accurately the experimentally observed growth rate using coarse\nresolutions. The ability of the CvP methodology to dynamically sort the\ncoherent, large-scale motion from the smaller, broadband scales during\ntransition is demonstrated via flow visualizations. LES of compressible channel\nare carried out and show a good match with a reference DNS.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 19:37:50 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 18:06:35 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Chapelier", "Jean-Baptiste", ""], ["Wasistho", "Bono", ""], ["Scalo", "Carlo", ""]]}, {"id": "1707.01947", "submitter": "Andreas Pels", "authors": "Andreas Pels, Johan Gyselinck, Ruth V. Sabariego, Sebastian Sch\\\"ops", "title": "Efficient simulation of DC-DC switch-mode power converters by multirate\n  partial differential equations", "comments": null, "journal-ref": "IEEE Journal on Multiscale and Multiphysics Computational\n  Techniques, vol. 4, pp. 64-75, 2019", "doi": "10.1109/JMMCT.2018.2888900", "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, Multirate Partial Differential Equations (MPDEs) are used for\nthe efficient simulation of problems with 2-level pulsed excitations as they\noften occur in power electronics, e.g., DC-DC switch-mode converters. The\ndifferential equations describing the problem are reformulated as MPDEs which\nare solved by a Galerkin approach and time discretization. For the solution\nexpansion two types of basis functions are proposed, namely classical Finite\nElement (FE) nodal functions and the recently introduced excitation-specific\npulse width modulation (PWM) basis functions. The new method is applied to the\nexample of a buck converter. Convergence, accuracy of the solution and\ncomputational efficiency of the method are numerically analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 19:49:58 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 19:38:17 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Pels", "Andreas", ""], ["Gyselinck", "Johan", ""], ["Sabariego", "Ruth V.", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "1707.03437", "submitter": "Xin Wang", "authors": "Xin Wang, Lejun Zou, Yupeng Ren, Yi Qin, Zhonghao Guo, Xiaohua Shen", "title": "Outcrop fracture characterization on suppositional planes cutting\n  through digital outcrop models (DOMs)", "comments": "Research article, 18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional fracture data collection methods are usually implemented on\nplanar surfaces or assuming they are planar; these methods may introduce\nsampling errors on uneven outcrop surfaces. Consequently, data collected on\nlimited types of outcrop surfaces (mainly bedding surfaces) may not be a\nsufficient representation of fracture network characteristic in outcrops.\nRecent development of techniques that obtain DOMs from outcrops and extract the\nfull extent of individual fractures offers the opportunity to address the\nproblem of performing the conventional sampling methods on uneven outcrop\nsurfaces. In this study, we propose a new method that performs outcrop fracture\ncharacterization on suppositional planes cutting through DOMs. The\nsuppositional plane is the best fit plane of the outcrop surface, and the\nfracture trace map is extracted on the suppositional plane so that the fracture\nnetwork can be further characterized. The amount of sampling errors introduced\nby the conventional methods and avoided by the new method on 16 uneven outcrop\nsurfaces with different roughnesses are estimated. The results show that the\nconventional sampling methods don't apply to outcrops other than bedding\nsurfaces or outcrops whose roughness > 0.04 m, and that the proposed method can\ngreatly extend the types of outcrop surfaces for outcrop fracture\ncharacterization with the suppositional plane cutting through DOMs.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 06:34:59 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Wang", "Xin", ""], ["Zou", "Lejun", ""], ["Ren", "Yupeng", ""], ["Qin", "Yi", ""], ["Guo", "Zhonghao", ""], ["Shen", "Xiaohua", ""]]}, {"id": "1707.03581", "submitter": "Daniel Ruprecht", "authors": "Andreas Naumann, Daniel Ruprecht and Joerg Wensch", "title": "Toward transient finite element simulation of thermal deformation of\n  machine tools in real-time", "comments": null, "journal-ref": "Computational Mechanics 62(5), pp. 929 - 942, 2018", "doi": "10.1007/s00466-018-1540-6", "report-no": null, "categories": "cs.CE cs.NA cs.SY math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite element models without simplifying assumptions can accurately describe\nthe spatial and temporal distribution of heat in machine tools as well as the\nresulting deformation. In principle, this allows to correct for displacements\nof the Tool Centre Point and enables high precision manufacturing. However, the\ncomputational cost of FEM models and restriction to generic algorithms in\ncommercial tools like ANSYS prevents their operational use since simulations\nhave to run faster than real-time. For the case where heat diffusion is slow\ncompared to machine movement, we introduce a tailored implicit-explicit\nmulti-rate time stepping method of higher order based on spectral deferred\ncorrections. Using the open-source FEM library DUNE, we show that fully coupled\nsimulations of the temperature field are possible in real-time for a machine\nconsisting of a stock sliding up and down on rails attached to a stand.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 07:48:46 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Naumann", "Andreas", ""], ["Ruprecht", "Daniel", ""], ["Wensch", "Joerg", ""]]}, {"id": "1707.03686", "submitter": "Vladimir Frolov", "authors": "Vladimir Frolov, Michael Chertkov", "title": "Methodology for Multi-stage, Operations- and Uncertainty-Aware Placement\n  and Sizing of FACTS Devices in a Large Power Transmission System", "comments": "paper is accepted for presentation at IREP 2017. arXiv admin note:\n  text overlap with arXiv:1608.04467", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.OC physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new optimization methodology for planning installation of Flexible\nAlternating Current Transmission System (FACTS) devices of the parallel and\nshunt types into large power transmission systems, which allows to delay or\navoid installations of generally much more expensive power lines. Methodology\ntakes as an input projected economic development, expressed through a paced\ngrowth of the system loads, as well as uncertainties, expressed through\nmultiple scenarios of the growth. We price new devices according to their\ncapacities. Installation cost contributes to the optimization objective in\ncombination with the cost of operations integrated over time and averaged over\nthe scenarios. The multi-stage (-time-frame) optimization aims to achieve a\ngradual distribution of new resources in space and time. Constraints on the\ninvestment budget, or equivalently constraint on building capacity, is\nintroduced at each time frame. Our approach adjusts operationally not only\nnewly installed FACTS devices but also other already existing flexible degrees\nof freedom. This complex optimization problem is stated using the most general\nAC Power Flows. Non-linear, non-convex, multiple-scenario and multi-time-frame\noptimization is resolved via efficient heuristics, consisting of a sequence of\nalternating Linear Programmings or Quadratic Programmings (depending on the\ngeneration cost) and AC-PF solution steps designed to maintain operational\nfeasibility for all scenarios. Computational scalability and application of the\nnewly developed approach is illustrated on the example of the 2736-nodes large\nPolish system. One most important advantage of the framework is that the\noptimal capacity of FACTS is build up gradually at each time frame in a limited\nnumber of locations, thus allowing to prepare the system better for possible\ncongestion due to future economic and other uncertainties.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 16:19:07 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Frolov", "Vladimir", ""], ["Chertkov", "Michael", ""]]}, {"id": "1707.03871", "submitter": "Omar Knio", "authors": "S. Allouch, M. Lucchesi, O.P. Le Ma\\^itre, K.A. Mustapha, O.M. Knio", "title": "Particle Simulation of Fractional Diffusion Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores different particle-based approaches to the simulation of\none-dimensional fractional subdiffusion equations in unbounded domains. We rely\non smooth particle approximations, and consider four methods for estimating the\nfractional diffusion term. The first method is based on direct differentiation\nof the particle representation, it follows the Riesz definition of the\nfractional derivative and results in a non-conservative scheme. The other three\nmethods follow the particle strength exchange (PSE) methodology and are by\nconstruction conservative, in the sense that the total particle strength is\ntime invariant. The first PSE algorithm is based on using direct\ndifferentiation to estimate the fractional diffusion flux, and exploiting the\nresulting estimates in an integral representation of the divergence operator.\nMeanwhile, the second one relies on the regularized Riesz representation of the\nfractional diffusion term to derive a suitable interaction formula acting\ndirectly on the particle representation of the diffusing field. A third PSE\nconstruction is considered that exploits the Green's function of the fractional\ndiffusion equation. The performance of all four approaches is assessed for the\ncase of a one-dimensional diffusion equation with constant diffusivity. This\nenables us to take advantage of known analytical solutions, and consequently\nconduct a detailed analysis of the performance of the methods. This includes a\nquantitative study of the various sources of error, namely filtering,\nquadrature, domain truncation, and time integration, as well as a space and\ntime self-convergence analysis. These analyses are conducted for different\nvalues of the order of the fractional derivatives, and computational\nexperiences are used to gain insight that can be used for generalization of the\npresent constructions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 19:06:05 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Allouch", "S.", ""], ["Lucchesi", "M.", ""], ["Ma\u00eetre", "O. P. Le", ""], ["Mustapha", "K. A.", ""], ["Knio", "O. M.", ""]]}, {"id": "1707.04038", "submitter": "Daniel Anderson", "authors": "Daniel Anderson and Jerome Droniou", "title": "An arbitrary order scheme on generic meshes for miscible displacements\n  in porous media", "comments": null, "journal-ref": "SIAM J. Sci. Comput. 40 (4), B1020-B1054, 2018", "doi": "10.1137/17M1138807", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design, analyse and implement an arbitrary order scheme applicable to\ngeneric meshes for a coupled elliptic-parabolic PDE system describing miscible\ndisplacement in porous media. The discretisation is based on several\nadaptations of the Hybrid-High-Order (HHO) method due to Di Pietro et al.\n[Computational Methods in Applied Mathematics, 14(4), (2014)]. The equation\ngoverning the pressure is discretised using an adaptation of the HHO method for\nvariable diffusion, while the discrete concentration equation is based on the\nHHO method for advection-diffusion-reaction problems combined with numerically\nstable flux reconstructions for the advective velocity that we have derived\nusing the results of Cockburn et al. [ESAIM: Mathematical Modelling and\nNumerical Analysis, 50(3), (2016)]. We perform some rigorous analysis of the\nmethod to demonstrate its $L^2$ stability under the irregular data often\npresented by reservoir engineering problems and present several numerical tests\nto demonstrate the quality of the results that are produced by the proposed\nscheme.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 09:31:26 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 12:12:26 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2018 02:55:06 GMT"}, {"version": "v4", "created": "Mon, 14 Jan 2019 19:11:07 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Anderson", "Daniel", ""], ["Droniou", "Jerome", ""]]}, {"id": "1707.04304", "submitter": "Charilaos Mylonas Mr.", "authors": "Charilaos Mylonas, Valentin Bemetz and Eleni Chatzi", "title": "Multiscale Surrogate Modeling and Uncertainty Quantification for\n  Periodic Composite Structures", "comments": "Appeared in UNCECOMP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational modeling of the structural behavior of continuous fiber\ncomposite materials often takes into account the periodicity of the underlying\nmicro-structure. A well established method dealing with the structural behavior\nof periodic micro-structures is the so- called Asymptotic Expansion\nHomogenization (AEH). By considering a periodic perturbation of the material\ndisplacement, scale bridging functions, also referred to as elastic correctors,\ncan be derived in order to connect the strains at the level of the\nmacro-structure with micro- structural strains. For complicated inhomogeneous\nmicro-structures, the derivation of such functions is usually performed by the\nnumerical solution of a PDE problem - typically with the Finite Element Method.\nMoreover, when dealing with uncertain micro-structural geometry and material\nparameters, there is considerable uncertainty introduced in the actual stresses\nexperienced by the materials. Due to the high computational cost of computing\nthe elastic correctors, the choice of a pure Monte-Carlo approach for dealing\nwith the inevitable material and geometric uncertainties is clearly\ncomputationally intractable. This problem is even more pronounced when the\neffect of damage in the micro-scale is considered, where re-evaluation of the\nmicro-structural representative volume element is necessary for every occurring\ndamage. The novelty in this paper is that a non-intrusive surrogate modeling\napproach is employed with the purpose of directly bridging the macro-scale\nbehavior of the structure with the material behavior in the micro-scale,\ntherefore reducing the number of costly evaluations of corrector functions,\nallowing for future developments on the incorporation of fatigue or static\ndamage in the analysis of composite structural components.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 17:45:22 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Mylonas", "Charilaos", ""], ["Bemetz", "Valentin", ""], ["Chatzi", "Eleni", ""]]}, {"id": "1707.04826", "submitter": "Xiaojiao Yu", "authors": "Xiaojiao Yu", "title": "Machine learning application in the life time of materials", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Materials design and development typically takes several decades from the\ninitial discovery to commercialization with the traditional trial and error\ndevelopment approach. With the accumulation of data from both experimental and\ncomputational results, data based machine learning becomes an emerging field in\nmaterials discovery, design and property prediction. This manuscript reviews\nthe history of materials science as a disciplinary the most common machine\nlearning method used in materials science, and specifically how they are used\nin materials discovery, design, synthesis and even failure detection and\nanalysis after materials are deployed in real application. Finally, the\nlimitations of machine learning for application in materials science and\nchallenges in this emerging field is discussed.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 05:58:40 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Yu", "Xiaojiao", ""]]}, {"id": "1707.05050", "submitter": "Nicolai Mallig", "authors": "Nicolai Mallig and Peter Vortisch", "title": "Modeling travel demand over a period of one week: The mobiTopp model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When mobiTopp was initially designed, more than 10 years ago, it has been the\nfirst travel demand simulation model intended for an analysis period of one\nweek. However, the first version supported only an analysis period of one day.\nThis paper describes the lessons learned while extending the simulation period\nfrom one day to one week. One important issue is ensuring realistic start times\nof activities. Due to differences between the realized trip durations during\nthe simulation and the trip durations assumed when creating the activity\nschedule, the realized activity schedule and the planned activity schedule may\ndeviate from each other at some point in time during simulation. A suitable\nrescheduling strategy is needed to prevent this. Another issue is the different\nbehavior at weekends, when more joint activities take place than on weekdays,\nresulting in an increased share of trips made using the mode car as passenger.\nIf a mode choice model that takes availability of ride-sharing opportunities\ninto account is used, it can be difficult to reproduce the correct modal split\nwithout modeling explicitly these joint activities. When modeling travel demand\nfor a week, it is also important to account for infrequent long-distance trips.\nWhile the share of these trips is low, the total number is not negligible. It\nseems that these long-distance trips are not well covered by the destination\nchoice model used for the day-to-day trips, indicating the need for a\nlong-distance trip model of infrequent events.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 09:21:36 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Mallig", "Nicolai", ""], ["Vortisch", "Peter", ""]]}, {"id": "1707.05163", "submitter": "Michael Lange", "authors": "Michael Lange and Erik van Sebille", "title": "Parcels v0.9: prototyping a Lagrangian Ocean Analysis framework for the\n  petascale age", "comments": "Submitted to Geoscientific Model Development (GMD)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Ocean General Circulation Models (OGCMs) move into the petascale age,\nwhere the output from global high-resolution model runs can be of the order of\nhundreds of terabytes in size, tools to analyse the output of these models will\nneed to scale up too. Lagrangian Ocean Analysis, where virtual particles are\ntracked through hydrodynamic fields, is an increasingly popular way to analyse\nOGCM output, by mapping pathways and connectivity of biotic and abiotic\nparticulates. However, the current software stack of Lagrangian Ocean Analysis\ncodes is not dynamic enough to cope with the increasing complexity, scale and\nneed for customisation of use-cases. Furthermore, most community codes are\ndeveloped for stand-alone use, making it a nontrivial task to integrate virtual\nparticles at runtime of the OGCM. Here, we introduce the new Parcels code,\nwhich was designed from the ground up to be sufficiently scalable to cope with\npetascale computing. We highlight its API design that combines flexibility and\ncustomisation with the ability to optimise for HPC workflows, following the\nparadigm of domain-specific languages. Parcels is primarily written in Python,\nutilising the wide range of tools available in the scientific Python ecosystem,\nwhile generating low-level C-code and using Just-In-Time compilation for\nperformance-critical computation. We show a worked-out example of its API, and\nvalidate the accuracy of the code against seven idealised test cases. This\nversion~0.9 of Parcels is focussed on laying out the API, with future work\nconcentrating on optimisation, efficiency and at-runtime coupling with OGCMs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 18:37:42 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 15:45:01 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Lange", "Michael", ""], ["van Sebille", "Erik", ""]]}, {"id": "1707.05707", "submitter": "Giovanni Nastasi", "authors": "Giovanni Nastasi", "title": "Monte Carlo Simulation of Charge Transport in Graphene (Simulazione\n  Monte Carlo per il trasporto di cariche nel grafene)", "comments": "Master Thesis in Mathematics. Supervisor: Prof. Vittorio Romano.\n  Language: italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulations of charge transport in graphene are presented by implementing a\nrecent method published on the paper: V. Romano, A. Majorana, M. Coco, \"DSMC\nmethod consistent with the Pauli exclusion principle and comparison with\ndeterministic solutions for charge transport in graphene\", Journal of\nComputational Physics 302 (2015) 267-284. After an overview of the most\nimportant aspects of the semiclassical transport model for the dynamics of\nelectrons in monolayer graphene, it is made a comparison in computational time\nbetween MATLAB and Fortran implementations of the algorithms. Therefore it is\nstudied the case of graphene on substrates which it is produced original\nresults by introducing models for the distribution of distances between\ngraphene's atoms and impurities. Finally simulations, by choosing different\nkind of substrates, are done.\n  -----\n  Le simulazioni per il trasporto di cariche nel grafene sono presentate\nimplementando un recente metodo pubblicato nell'articolo: V. Romano, A.\nMajorana, M. Coco, \"DSMC method consistent with the Pauli exclusion principle\nand comparison with deterministic solutions for charge transport in graphene\",\nJournal of Computational Physics 302 (2015) 267-284. Dopo una panoramica sugli\naspetti pi\\`u importanti del modello di trasporto semiclassico per la dinamica\ndegli elettroni nel grafene sospeso, \\`e stato effettuato un confronto del\ntempo computazionale tra le implementazioni MATLAB e Fortran dell'algoritmo.\nInoltre \\`e stato anche studiato il caso del grafene su substrato su cui sono\nstati prodotti dei risultati originali considerando dei modelli per la\ndistribuzione delle distanze tra gli atomi del grafene e le impurezze. Infine\nsono state effettuate delle simulazioni scegliendo substrati di diversa natura.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 13:35:05 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Nastasi", "Giovanni", ""]]}, {"id": "1707.05711", "submitter": "Wai-Tong Louis Fan", "authors": "Wai-Tong Louis Fan, Sebastien Roch", "title": "Statistically consistent and computationally efficient inference of\n  ancestral DNA sequences in the TKF91 model under dense taxon sampling", "comments": "Title modified, 31 pages, 2 Figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In evolutionary biology, the speciation history of living organisms is\nrepresented graphically by a phylogeny, that is, a rooted tree whose leaves\ncorrespond to current species and branchings indicate past speciation events.\nPhylogenies are commonly estimated from molecular sequences, such as DNA\nsequences, collected from the species of interest. At a high level, the idea\nbehind this inference is simple: the further apart in the Tree of Life are two\nspecies, the greater is the number of mutations to have accumulated in their\ngenomes since their most recent common ancestor. In order to obtain accurate\nestimates in phylogenetic analyses, it is standard practice to employ\nstatistical approaches based on stochastic models of sequence evolution on a\ntree. For tractability, such models necessarily make simplifying assumptions\nabout the evolutionary mechanisms involved. In particular, commonly omitted are\ninsertions and deletions of nucleotides -- also known as indels.\n  Properly accounting for indels in statistical phylogenetic analyses remains a\nmajor challenge in computational evolutionary biology. Here we consider the\nproblem of reconstructing ancestral sequences on a known phylogeny in a model\nof sequence evolution incorporating nucleotide substitutions, insertions and\ndeletions, specifically the classical TKF91 process. We focus on the case of\ndense phylogenies of bounded height, which we refer to as the taxon-rich\nsetting, where statistical consistency is achievable. We give the first\npolynomial-time ancestral reconstruction algorithm with provable guarantees\nunder constant rates of mutation. Our algorithm succeeds when the phylogeny\nsatisfies the \"big bang\" condition, a necessary and sufficient condition for\nstatistical consistency in this context.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 15:55:57 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 14:50:49 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 01:42:25 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Fan", "Wai-Tong Louis", ""], ["Roch", "Sebastien", ""]]}, {"id": "1707.07189", "submitter": "Shahzad Ahmed Mr.", "authors": "M. Usman Ali, Shahzad Ahmed, Javed Ferzund, Atif Mehmood, Abbas Rehman", "title": "Using PCA and Factor Analysis for Dimensionality Reduction of\n  Bio-informatics Data", "comments": "12 pages, 11 figures, 2 tables", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications(IJACSA), Volume 8 Issue 5, 2017", "doi": "10.14569/IJACSA.2017.080551", "report-no": null, "categories": "q-bio.OT cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large volume of Genomics data is produced on daily basis due to the\nadvancement in sequencing technology. This data is of no value if it is not\nproperly analysed. Different kinds of analytics are required to extract useful\ninformation from this raw data. Classification, Prediction, Clustering and\nPattern Extraction are useful techniques of data mining. These techniques\nrequire appropriate selection of attributes of data for getting accurate\nresults. However, Bioinformatics data is high dimensional, usually having\nhundreds of attributes. Such large a number of attributes affect the\nperformance of machine learning algorithms used for classification/prediction.\nSo, dimensionality reduction techniques are required to reduce the number of\nattributes that can be further used for analysis. In this paper, Principal\nComponent Analysis and Factor Analysis are used for dimensionality reduction of\nBioinformatics data. These techniques were applied on Leukaemia data set and\nthe number of attributes was reduced from to.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 16:21:12 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Ali", "M. Usman", ""], ["Ahmed", "Shahzad", ""], ["Ferzund", "Javed", ""], ["Mehmood", "Atif", ""], ["Rehman", "Abbas", ""]]}, {"id": "1707.07823", "submitter": "Gal Oren", "authors": "Gal Oren, Nerya Y. Stroh", "title": "Mathematical Model for Detection of Leakage in Domestic Water Supply\n  Systems by Reading Consumption from an Analogue Water Meter", "comments": null, "journal-ref": "International Journal of Environmental Science and Development\n  (IJESD), Vol. 4, No. 4, International Association of Computer Science and\n  Information Technology Press, ISSN: 2010-0264, 2013", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we introduce the principles to detect leakage using a\nmathematical model based on machine learning and domestic water consumption\nmonitoring in real time. The model uses data which is measured from a water\nmeter, analyzes the water consumption, and uses two criteria simultaneously:\ndeviation from the average consumption, and comparison of steady water\nconsumptions over a period of time. Simulation of the model on a regular\nhousehold consumer was implemented on Antileaks - device that we have built\nthat designed to transfer consumption information from an analogue water meter\nto a digital form in real time.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 06:08:18 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Oren", "Gal", ""], ["Stroh", "Nerya Y.", ""]]}, {"id": "1707.08247", "submitter": "Debojyoti Ghosh", "authors": "Debojyoti Ghosh, Mikhail A. Dorf, Milo R. Dorr, Jeffrey A. F.\n  Hittinger", "title": "Kinetic Simulation of Collisional Magnetized Plasmas with Semi-Implicit\n  Time Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plasmas with varying collisionalities occur in many applications, such as\ntokamak edge regions, where the flows are characterized by significant\nvariations in density and temperature. While a kinetic model is necessary for\nweakly-collisional high-temperature plasmas, high collisionality in colder\nregions render the equations numerically stiff due to disparate time scales. In\nthis paper, we propose an implicit-explicit algorithm for such cases, where the\ncollisional term is integrated implicitly in time, while the advective term is\nintegrated explicitly in time, thus allowing time step sizes that are\ncomparable to the advective time scales. This partitioning results in a more\nefficient algorithm than those using explicit time integrators, where the time\nstep sizes are constrained by the stiff collisional time scales. We implement\nsemi-implicit additive Runge-Kutta methods in COGENT, a finite-volume\ngyrokinetic code for mapped, multiblock grids and test the accuracy,\nconvergence, and computational cost of these semi-implicit methods for test\ncases with highly-collisional plasmas.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 23:19:20 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Ghosh", "Debojyoti", ""], ["Dorf", "Mikhail A.", ""], ["Dorr", "Milo R.", ""], ["Hittinger", "Jeffrey A. F.", ""]]}, {"id": "1707.09320", "submitter": "Dingwen Tao", "authors": "Dingwen Tao, Sheng Di, Hanqi Guo, Zizhong Chen, Franck Cappello", "title": "Z-checker: A Framework for Assessing Lossy Compression of Scientific\n  Data", "comments": "Accepted by The International Journal of High Performance Computing\n  Application", "journal-ref": null, "doi": "10.1177/1094342017737147", "report-no": null, "categories": "cs.OH astro-ph.IM cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of vast volume of data being produced by today's scientific\nsimulations and experiments, lossy data compressor allowing user-controlled\nloss of accuracy during the compression is a relevant solution for\nsignificantly reducing the data size. However, lossy compressor developers and\nusers are missing a tool to explore the features of scientific datasets and\nunderstand the data alteration after compression in a systematic and reliable\nway. To address this gap, we have designed and implemented a generic framework\ncalled Z-checker. On the one hand, Z-checker combines a battery of data\nanalysis components for data compression. On the other hand, Z-checker is\nimplemented as an open-source community tool to which users and developers can\ncontribute and add new analysis components based on their additional analysis\ndemands. In this paper, we present a survey of existing lossy compressors. Then\nwe describe the design framework of Z-checker, in which we integrated\nevaluation metrics proposed in prior work as well as other analysis tools.\nSpecifically, for lossy compressor developers, Z-checker can be used to\ncharacterize critical properties of any dataset to improve compression\nstrategies. For lossy compression users, Z-checker can detect the compression\nquality, provide various global distortion analysis comparing the original data\nwith the decompressed data and statistical analysis of the compression error.\nZ-checker can perform the analysis with either coarse granularity or fine\ngranularity, such that the users and developers can select the best-fit,\nadaptive compressors for different parts of the dataset. Z-checker features a\nvisualization interface displaying all analysis results in addition to some\nbasic views of the datasets such as time series. To the best of our knowledge,\nZ-checker is the first tool designed to assess lossy compression\ncomprehensively for scientific datasets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 18:09:33 GMT"}, {"version": "v2", "created": "Fri, 10 Nov 2017 20:16:10 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Tao", "Dingwen", ""], ["Di", "Sheng", ""], ["Guo", "Hanqi", ""], ["Chen", "Zizhong", ""], ["Cappello", "Franck", ""]]}, {"id": "1707.09377", "submitter": "Doncescu Andrei", "authors": "Iuliana Marin, Virgil Tudose, Anton Hadar, Nicolae Goga, Andrei\n  Doncescu (LAAS-DISCO)", "title": "Improved Adaptive Resolution Molecular Dynamics Simulation", "comments": "International Conference on Engineering,Technology and Innovation,\n  Jun 2017, Madeira, Portugal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  -Molecular simulations allow the study of properties and interactions of\nmolecular systems. This article presents an improved version of the Adaptive\nResolution Scheme that links two systems having atomistic (also called\nfine-grained) and coarse-grained resolutions using a force interpolation\nscheme. Interactions forces are obtained based on the Hamiltonian derivation\nfor a given molecular system. The new algorithm was implemented in GROMACS\nmolecular dynamics software package and tested on a butane system. The MARTINI\ncoarse-grained force field is applied between the coarse-grained particles of\nthe butane system. The molecular dynamics package GROMACS and the Message\nPassing Interface allow the simulation of such a system in a reasonable amount\nof time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:30:08 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Marin", "Iuliana", "", "LAAS-DISCO"], ["Tudose", "Virgil", "", "LAAS-DISCO"], ["Hadar", "Anton", "", "LAAS-DISCO"], ["Goga", "Nicolae", "", "LAAS-DISCO"], ["Doncescu", "Andrei", "", "LAAS-DISCO"]]}, {"id": "1707.09683", "submitter": "Hanyu Jiang", "authors": "Hanyu Jiang, Narayan Ganesan and Yu-Dong Yao", "title": "CUDAMPF++: A Proactive Resource Exhaustion Scheme for Accelerating\n  Homologous Sequence Search on CUDA-enabled GPU", "comments": "15 pages, submitted to academic journal", "journal-ref": "IEEE.Trans.Parallel.Distibuted.Sys. (2018)", "doi": "10.1109/TPDS.2018.2830393", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic sequence alignment is an important research topic in bioinformatics\nand continues to attract significant efforts. As genomic data grow\nexponentially, however, most of alignment methods face challenges due to their\nhuge computational costs. HMMER, a suite of bioinformatics tools, is widely\nused for the analysis of homologous protein and nucleotide sequences with high\nsensitivity, based on profile hidden Markov models (HMMs). Its latest version,\nHMMER3, introdues a heuristic pipeline to accelerate the alignment process,\nwhich is carried out on central processing units (CPUs) with the support of\nstreaming SIMD extensions (SSE) instructions. Few acceleration results have\nsince been reported based on HMMER3. In this paper, we propose a five-tiered\nparallel framework, CUDAMPF++, to accelerate the most computationally intensive\nstages of HMMER3's pipeline, multiple/single segment Viterbi (MSV/SSV), on a\nsingle graphics processing unit (GPU). As an architecture-aware design, the\nproposed framework aims to fully utilize hardware resources via exploiting\nfiner-grained parallelism (multi-sequence alignment) compared with its\npredecessor (CUDAMPF). In addition, we propose a novel method that proactively\nsacrifices L1 Cache Hit Ratio (CHR) to get improved performance and scalability\nin return. A comprehensive evaluation shows that the proposed framework\noutperfroms all existig work and exhibits good consistency in performance\nregardless of the variation of query models or protein sequence datasets. For\nMSV (SSV) kernels, the peak performance of the CUDAMPF++ is 283.9 (471.7) GCUPS\non a single K40 GPU, and impressive speedups ranging from 1.x (1.7x) to 168.3x\n(160.7x) are achieved over the CPU-based implementation (16 cores, 32 threads).\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 23:58:45 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Jiang", "Hanyu", ""], ["Ganesan", "Narayan", ""], ["Yao", "Yu-Dong", ""]]}, {"id": "1707.09783", "submitter": "Marc Olm", "authors": "Marc Olm, Santiago Badia and Alberto F. Mart\\'in", "title": "Simulation of high temperature superconductors and experimental\n  validation", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a parallel, fully-distributed finite element\nnumerical framework to simulate the low-frequency electromagnetic response of\nsuperconducting devices, which allows to efficiently exploit HPC platforms. We\nselect the so-called H-formulation, which uses the magnetic field as a state\nvariable. N\\'ed\\'elec elements (of arbitrary order) are required for an\naccurate approximation of the H-formulation for modelling electromagnetic\nfields along interfaces between regions with high contrast medium properties.\nAn h-adaptive mesh refinement technique customized for N\\'ed\\'elec elements\nleads to a structured fine mesh in areas of interest whereas a smart coarsening\nis obtained in other regions. The composition of a tailored, robust, parallel\nnonlinear solver completes the exposition of the developed tools to tackle the\nproblem. First, a comparison against experimental data is performed to show the\navailability of the finite element approximation to model the physical\nphenomena. Then, a selected state-of-the-art 3D benchmark is reproduced,\nfocusing on the parallel performance of the algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 09:48:34 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 11:49:29 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Olm", "Marc", ""], ["Badia", "Santiago", ""], ["Mart\u00edn", "Alberto F.", ""]]}, {"id": "1707.09837", "submitter": "Artur Lugmayr", "authors": "Irina Kuznetsova, Yuliya V Karpievitch, Aleksandra Filipovska, Artur\n  Lugmayr, Andreas Holzinger", "title": "Review of Machine Learning Algorithms in Differential Expression\n  Analysis", "comments": null, "journal-ref": "Proc. of the 9th Workshop on Semantic Ambient Media Experiences\n  (SAME'2016/2), Visualisation - Emerging Media - and User-Experience, Int.\n  Series on Information Systems and Management in Creative eMedia (CreMedia),\n  No. 2016/2, 2016", "doi": null, "report-no": "CreMedia/2016/02/01/02", "categories": "stat.ML cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In biological research machine learning algorithms are part of nearly every\nanalytical process. They are used to identify new insights into biological\nphenomena, interpret data, provide molecular diagnosis for diseases and develop\npersonalized medicine that will enable future treatments of diseases. In this\npaper we (1) illustrate the importance of machine learning in the analysis of\nlarge scale sequencing data, (2) present an illustrative standardized workflow\nof the analysis process, (3) perform a Differential Expression (DE) analysis of\na publicly available RNA sequencing (RNASeq) data set to demonstrate the\ncapabilities of various algorithms at each step of the workflow, and (4) show a\nmachine learning solution in improving the computing time, storage\nrequirements, and minimize utilization of computer memory in analyses of\nRNA-Seq datasets. The source code of the analysis pipeline and associated\nscripts are presented in the paper appendix to allow replication of\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 14:56:45 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Kuznetsova", "Irina", ""], ["Karpievitch", "Yuliya V", ""], ["Filipovska", "Aleksandra", ""], ["Lugmayr", "Artur", ""], ["Holzinger", "Andreas", ""]]}, {"id": "1707.09865", "submitter": "Hamid Hamraz", "authors": "Hamid Hamraz and Marco A. Contreras", "title": "Remote sensing of forests using discrete return airborne LiDAR", "comments": "This manuscript is a book chapter that has provisionally been\n  accepted to be published in \"Recent Advances and Applications in Remote\n  Sensing\", ISBN 978-953-51-5564-5. Ed.: Hung, Ming Cheh. InTechOpen. The\n  chapter summarizes novel methods from four recently published journal ppapers\n  by the authors in a concise and cohesive manner", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Airborne discrete return light detection and ranging (LiDAR) point clouds\ncovering forested areas can be processed to segment individual trees and\nretrieve their morphological attributes. Segmenting individual trees in natural\ndeciduous forests however remained a challenge because of the complex and\nmulti-layered canopy. In this chapter, we present (i) a robust segmentation\nmethod that avoids a priori assumptions about the canopy structure, (ii) a\nvertical canopy stratification procedure that improves segmentation of\nunderstory trees, (iii) an occlusion model for estimating the point density of\neach canopy stratum, and (iv) a distributed computing approach for efficient\nprocessing at the forest level. When applied to the University of Kentucky\nRobinson Forest, the segmentation method detected about 90% of overstory and\n47% of understory trees with over-segmentation rates of 14% and 2%. Stratifying\nthe canopy improved the detection rate of understory trees to 68% at the cost\nof increasing their over-segmentations to 16%. According to our occlusion\nmodel, a point density of ~170 pt/m-sqr is needed to segment understory trees\nas accurately as overstory trees. Lastly, using the distributed approach, we\nsegmented about two million trees in the 7,440-ha forest in 2.5 hours using 192\nprocessors, which is 167 times faster than using a single processor. Keywords:\nindividual tree segmentation, multi-layered stand, vertical canopy\nstratification, segmentation evaluation, point density, canopy occlusion\neffect, big data, distributed computing.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 19:06:18 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Hamraz", "Hamid", ""], ["Contreras", "Marco A.", ""]]}]