[{"id": "1711.00001", "submitter": "Haoze Wu", "authors": "Haoze Wu, Yangyu Zhou", "title": "Gene Ontology (GO) Prediction using Machine Learning Methods", "comments": "The results in this paper result from a biased test set, and is\n  therefore not reliable", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE q-bio.QM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We applied machine learning to predict whether a gene is involved in axon\nregeneration. We extracted 31 features from different databases and trained\nfive machine learning models. Our optimal model, a Random Forest Classifier\nwith 50 submodels, yielded a test score of 85.71%, which is 4.1% higher than\nthe baseline score. We concluded that our models have some predictive\ncapability. Similar methodology and features could be applied to predict other\nGene Ontology (GO) terms.\n", "versions": [{"version": "v1", "created": "Mon, 30 Oct 2017 19:02:13 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 16:47:43 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Wu", "Haoze", ""], ["Zhou", "Yangyu", ""]]}, {"id": "1711.00005", "submitter": "Yong-Xian Wang", "authors": "Min Xu, Yongxian Wang, Anthony Theodore Chronopoulos, Hao Yue", "title": "Performance Optimization and Parallelization of a Parabolic Equation\n  Solver in Computational Ocean Acoustics on Modern Many-core Computer", "comments": "9 pages, 8 figures, 3 tables. preprint for the International\n  Conference on Computer Science and Application Engineering (CSAE2017).\n  2017.10.21-23, Shanghai, China", "journal-ref": null, "doi": "10.12783/dtcse/csae2017/17546", "report-no": null, "categories": "cs.MS cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of open-source codes widely used in computational ocean acoustics,\nFOR3D can provide a very good estimate for underwater acoustic propagation. In\nthis paper, we propose a performance optimization and parallelization to speed\nup the running of FOR3D. We utilized a variety of methods to enhance the entire\nperformance, such as using a multi-threaded programming model to exploit the\npotential capability of the many-core node of high-performance computing (HPC)\nsystem, tuning compile options, using efficient tuned mathematical library and\nutilizing vectorization optimization instruction. In addition, we extended the\napplication from single-frequency calculation to multi-frequency calculation\nsuccessfully by using OpenMP+MPI hybrid programming techniques on the\nmainstream HPC platform. A detailed performance evaluation was performed and\nthe results showed that the proposed parallelization obtained good accelerated\neffect of 25.77X when testing a typical three-dimensional medium-sized case on\nTianhe-2 supercomputer. It also showed that the tuned parallel version has a\nweak-scalability. The speed of calculation of underwater sound field can be\ngreatly improved by the strategy mentioned in this paper. The method used in\nthis paper is not only applicable to other similar computing models in\ncomputational ocean acoustics but also a guideline of performance enhancement\nfor scientific and engineering application running on modern\nmany-core-computing platform.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 13:58:48 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 08:52:52 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Xu", "Min", ""], ["Wang", "Yongxian", ""], ["Chronopoulos", "Anthony Theodore", ""], ["Yue", "Hao", ""]]}, {"id": "1711.00110", "submitter": "Yong-Xian Wang", "authors": "Wang Yong-Xian and Zhang Li-Lun and Che Yong-Gang and Xu Chuan-Fu and\n  Liu Wei and Liu Hua-Yong and Wang Zheng-Hua", "title": "Improved Algorithm for Reconstructing Singular Connection in Multi-Block\n  CFD Applications", "comments": "10 pages, 3 figures, 2 tables. Pre-print version", "journal-ref": "Transaction of Nanjing University of Aeronautics & Astronautics.\n  2013, 30(S):51-57", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An improved algorithm is proposed for the reconstruction of singular\nconnectivity from the available pairwise connections during preprocessing\nphase. To evaluate the performance of the algorithm, an in-house computational\nfluid dynamics (CFD) code is used in which high-order finite-difference method\nfor spatial discretization running on the Tianhe-1A supercomputer is employed.\nTest cases with a varied amount of mesh points are chosen, and the test results\nindicate that the improved singular connection reconstruction algorithm can\nachieve a speedup factor of 1000X or more when compared with the naive search\nmethod adopted in the former version of our code. Moreover, the parallel\nefficiency can benefit from the strategy of local communication based on the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 29 Oct 2017 15:07:08 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Yong-Xian", "Wang", ""], ["Li-Lun", "Zhang", ""], ["Yong-Gang", "Che", ""], ["Chuan-Fu", "Xu", ""], ["Wei", "Liu", ""], ["Hua-Yong", "Liu", ""], ["Zheng-Hua", "Wang", ""]]}, {"id": "1711.00336", "submitter": "Christoph Rettinger", "authors": "Christoph Rettinger and Ulrich R\\\"ude", "title": "A Coupled Lattice Boltzmann Method and Discrete Element Method for\n  Discrete Particle Simulations of Particulate Flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete particle simulations are widely used to study large-scale\nparticulate flows in complex geometries where particle-particle and\nparticle-fluid interactions require an adequate representation but the\ncomputational cost has to be kept low. In this work, we present a novel\ncoupling approach for such simulations. A lattice Boltzmann formulation of the\ngeneralized Navier-Stokes equations is used to describe the fluid motion. This\npromises efficient simulations suitable for high performance computing and,\nsince volume displacement effects by the solid phase are considered, our\napproach is also applicable to non-dilute particulate systems. The discrete\nelement method is combined with an explicit evaluation of interparticle\nlubrication forces to simulate the motion of individual submerged particles.\nDrag, pressure and added mass forces determine the momentum transfer by\nfluid-particle interactions. A stable coupling algorithm is presented and\ndiscussed in detail. We demonstrate the validity of our approach for dilute as\nwell as dense systems by predicting the settling velocity of spheres over a\nbroad range of solid volume fractions in good agreement with semi-empirical\ncorrelations. Additionally, the accuracy of particle-wall interactions in a\nviscous fluid is thoroughly tested and established. Our approach can thus be\nreadily used for various particulate systems and can be extended\nstraightforward to e.g. non-spherical particles.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 13:34:12 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Rettinger", "Christoph", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1711.01177", "submitter": "Jeremie Kim", "authors": "Jeremie S. Kim, Damla Senol Cali, Hongyi Xin, Donghyuk Lee, Saugata\n  Ghose, Mohammed Alser, Hasan Hassan, Oguz Ergin, Can Alkan and Onur Mutlu", "title": "GRIM-Filter: Fast Seed Location Filtering in DNA Read Mapping Using\n  Processing-in-Memory Technologies", "comments": "arXiv admin note: text overlap with arXiv:1708.04329", "journal-ref": "BMC Genomics, 19 (Suppl 2):89, 2018", "doi": "10.1186/s12864-018-4460-0", "report-no": null, "categories": "q-bio.GN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Seed location filtering is critical in DNA read mapping, a\nprocess where billions of DNA fragments (reads) sampled from a donor are mapped\nonto a reference genome to identify genomic variants of the donor.\nState-of-the-art read mappers 1) quickly generate possible mapping locations\nfor seeds (i.e., smaller segments) within each read, 2) extract reference\nsequences at each of the mapping locations, and 3) check similarity between\neach read and its associated reference sequences with a\ncomputationally-expensive algorithm (i.e., sequence alignment) to determine the\norigin of the read. A seed location filter comes into play before alignment,\ndiscarding seed locations that alignment would deem a poor match. The ideal\nseed location filter would discard all poor match locations prior to alignment\nsuch that there is no wasted computation on unnecessary alignments.\n  Results: We propose a novel seed location filtering algorithm, GRIM-Filter,\noptimized to exploit 3D-stacked memory systems that integrate computation\nwithin a logic layer stacked under memory layers, to perform\nprocessing-in-memory (PIM). GRIM-Filter quickly filters seed locations by 1)\nintroducing a new representation of coarse-grained segments of the reference\ngenome, and 2) using massively-parallel in-memory operations to identify read\npresence within each coarse-grained segment. Our evaluations show that for a\nsequence alignment error tolerance of 0.05, GRIM-Filter 1) reduces the false\nnegative rate of filtering by 5.59x--6.41x, and 2) provides an end-to-end read\nmapper speedup of 1.81x--3.65x, compared to a state-of-the-art read mapper\nemploying the best previous seed location filtering algorithm.\n  Availability: The code is available online at:\nhttps://github.com/CMU-SAFARI/GRIM\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 16:03:46 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Kim", "Jeremie S.", ""], ["Cali", "Damla Senol", ""], ["Xin", "Hongyi", ""], ["Lee", "Donghyuk", ""], ["Ghose", "Saugata", ""], ["Alser", "Mohammed", ""], ["Hassan", "Hasan", ""], ["Ergin", "Oguz", ""], ["Alkan", "Can", ""], ["Mutlu", "Onur", ""]]}, {"id": "1711.01410", "submitter": "Jonas \\v{S}ukys", "authors": "Jonas \\v{S}ukys and Mira Kattwinkel", "title": "SPUX: Scalable Particle Markov Chain Monte Carlo for uncertainty\n  quantification in stochastic ecological models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of individual based models (IBMs), successful in modeling complex\necological dynamical systems, is often performed only ad-hoc. Bayesian\ninference can be used for both parameter estimation and uncertainty\nquantification, but its successful application to realistic scenarios has been\nhindered by the complex stochastic nature of IBMs. Computationally expensive\ntechniques such as Particle Filter (PF) provide marginal likelihood estimates,\nwhere multiple model simulations (particles) are required to get a sample from\nthe state distribution conditional on the observed data. Particle ensembles are\nre-sampled at each data observation time, requiring particle destruction and\nreplication, which lead to an increase in algorithmic complexity. We present\nSPUX, a Python implementation of parallel Particle Markov Chain Monte Carlo\n(PMCMC) algorithm, which mitigates high computational costs by distributing\nparticles over multiple computational units. Adaptive load re-balancing\ntechniques are used to mitigate computational work imbalances introduced by\nre-sampling. Framework performance is investigated and significant speed-ups\nare observed for a simple predator-prey IBM model.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 07:34:31 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["\u0160ukys", "Jonas", ""], ["Kattwinkel", "Mira", ""]]}, {"id": "1711.01728", "submitter": "Carleton Coffrin", "authors": "Carleton Coffrin, Russell Bent, Kaarthik Sundar, Yeesian Ng, Miles\n  Lubin", "title": "PowerModels.jl: An Open-Source Framework for Exploring Power Flow\n  Formulations", "comments": null, "journal-ref": null, "doi": null, "report-no": "LA-UR-17-29326", "categories": "math.OC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the power system research community has seen an explosion of\nnovel methods for formulating and solving power network optimization problems.\nThese emerging methods range from new power flow approximations, which go\nbeyond the traditional DC power flow by capturing reactive power, to convex\nrelaxations, which provide solution quality and runtime performance guarantees.\nUnfortunately, the sophistication of these emerging methods often presents a\nsignificant barrier to evaluating them on a wide variety of power system\noptimization applications. To address this issue, this work proposes\nPowerModels, an open-source platform for comparing power flow formulations.\nFrom its inception, PowerModels was designed to streamline the process of\nevaluating different power flow formulations on shared optimization problem\nspecifications. This work provides a brief introduction to the design of\nPowerModels, validates its implementation, and demonstrates its effectiveness\nwith a proof-of-concept study analyzing five different formulations of the\nOptimal Power Flow problem.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 04:58:07 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 19:59:02 GMT"}, {"version": "v3", "created": "Tue, 13 Mar 2018 03:34:56 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Coffrin", "Carleton", ""], ["Bent", "Russell", ""], ["Sundar", "Kaarthik", ""], ["Ng", "Yeesian", ""], ["Lubin", "Miles", ""]]}, {"id": "1711.01828", "submitter": "Sebastian Sch\\\"ops", "authors": "Jacopo Corno and Carlo de Falco and Herbert De Gersem and Sebastian\n  Sch\\\"ops", "title": "Isogeometric Analysis Simulation of TESLA Cavities Under Uncertainty", "comments": null, "journal-ref": "Proceedings of the International Conference on Electromagnetics in\n  Advanced Applications (ICEAA) 2015. IEEE, Sept. 2015, pp. 1508-1511", "doi": "10.1109/ICEAA.2015.7297375", "report-no": null, "categories": "physics.comp-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the design of electromagnetic devices the accurate representation of the\ngeometry plays a crucial role in determining the device performance. For\naccelerator cavities, in particular, controlling the frequencies of the\neigenmodes is important in order to guarantee the synchronization between the\nelectromagnetic field and the accelerated particles. The main interest of this\nwork is in the evaluation of eigenmode sensitivities with respect to\ngeometrical changes using Monte Carlo simulations and stochastic collocation.\nThe choice of an IGA approach for the spatial discretization allows for an\nexact handling of the domains and their deformations, guaranteeing, at the same\ntime, accurate and highly regular solutions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 10:49:29 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Corno", "Jacopo", ""], ["de Falco", "Carlo", ""], ["De Gersem", "Herbert", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "1711.01996", "submitter": "Brendan Keith", "authors": "Brendan Keith, Ali Vaziri Astaneh, Leszek Demkowicz", "title": "Goal-oriented adaptive mesh refinement for non-symmetric functional\n  settings", "comments": "32 pages", "journal-ref": "SIAM J. Numer. Anal. 57(4):1649-1676 (2019)", "doi": "10.1137/18M1181754", "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a new unified duality theory is developed for\nPetrov-Galerkin finite element methods. This novel theory is then used to\nmotivate goal-oriented adaptive mesh refinement strategies for use with\ndiscontinuous Petrov-Galerkin (DPG) methods. The focus of this article is\nmainly on broken ultraweak variational formulations of stationary boundary\nvalue problems, however, many of the ideas presented within are general enough\nthat they be extended to any such well-posed variational formulation. The\nproposed goal-oriented adaptive mesh refinement procedures require the\nconstruction of refinement indicators for both a primal problem and a dual\nproblem. In the DPG context, the primal problem is simply the system of linear\nequations coming from a standard DPG method and the dual problem is a similar\nsystem of equations, coming from a new method which is dual to DPG. This new\nmethod has the same coefficient matrix as the associated DPG method but has a\ndifferent load. We refer to this new finite element method as a DPG* method. A\nthorough analysis of DPG* methods, as stand-alone finite element methods, is\nnot given here but will be provided in subsequent articles. For DPG methods,\nthe current theory of a posteriori error estimation is reviewed and the\nreliability estimate in [13, Theorem 2.1] is improved on. For DPG* methods,\nthree different classes of refinement indicators are derived and several\ncontributions are made towards rigorous a posteriori error estimation. At the\nclosure of the article, results of numerical experiments with Poisson's\nboundary value problem in a three-dimensional domain are provided. These\nresults clearly demonstrate the utility of the goal-oriented adaptive mesh\nrefinement strategies for quantities of interest with either interior or\nboundary terms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 16:32:16 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 16:46:14 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 15:45:08 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Keith", "Brendan", ""], ["Vaziri Astaneh", "Ali", ""], ["Demkowicz", "Leszek", ""]]}, {"id": "1711.02597", "submitter": "Felix Engelmann", "authors": "Felix Engelmann, Florian Glaser, Henning Kopp, Frank Kargl, Christof\n  Weinhardt", "title": "Towards an Economic Analysis of Routing in Payment Channel Networks", "comments": "6 pages, 3 figures, SERIAL '17 Workshop", "journal-ref": null, "doi": "10.1145/3152824.3152826", "report-no": null, "categories": "cs.CE cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Payment channel networks are supposed to overcome technical scalability\nlimitations of blockchain infrastructure by employing a special overlay network\nwith fast payment confirmation and only sporadic settlement of netted\ntransactions on the blockchain. However, they introduce economic routing\nconstraints that limit decentralized scalability and are currently not well\nunderstood. In this paper, we model the economic incentives for participants in\npayment channel networks. We provide the first formal model of payment channel\neconomics and analyze how the cheapest path can be found. Additionally, our\nsimulation assesses the long-term evolution of a payment channel network. We\nfind that even for small routing fees, sometimes it is cheaper to settle the\ntransaction directly on the blockchain.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 16:37:22 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Engelmann", "Felix", ""], ["Glaser", "Florian", ""], ["Kopp", "Henning", ""], ["Kargl", "Frank", ""], ["Weinhardt", "Christof", ""]]}, {"id": "1711.02661", "submitter": "Pierluigi Gallo", "authors": "Pierluigi Gallo, Francesco Randazzo, and Ignazio Gallo", "title": "e-Fair: Aggregation in e-Commerce for Exploiting Economies of Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many new and interesting models of successful online\nbusiness have been developed, including competitive models such as auctions,\nwhere the product price tends to rise, and group-buying, where users cooperate\nobtaining a dynamic price that tends to go down. We propose the e-fair as a\nbusiness model for social commerce, where both sellers and buyers are grouped\nto maximize benefits. e-Fairs extend the group-buying model aggregating demand\nand supply for price optimization as well as consolidating shipments and\noptimize withdrawals for guaranteeing additional savings. e-Fairs work upon\nmultiple dimensions: time to aggregate buyers, their geographical distribution,\nprice/quantity curves provided by sellers, and location of withdrawal points.\nWe provide an analytical model for time and spatial optimization and simulate\nrealistic scenarios using both real purchase data from an Italian marketplace\nand simulated ones. Experimental results demonstrate the potentials offered by\ne-fairs and show benefits for all the involved actors.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 18:58:50 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Gallo", "Pierluigi", ""], ["Randazzo", "Francesco", ""], ["Gallo", "Ignazio", ""]]}, {"id": "1711.02751", "submitter": "Shinhoo Kang", "authors": "Shinhoo Kang, Francis X. Giraldo, Tan Bui-Thanh", "title": "IMEX HDG-DG: a coupled implicit hybridized discontinuous Galerkin (HDG)\n  and explicit discontinuous Galerkin (DG) approach for shallow water systems", "comments": "27 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose IMEX HDG-DG schemes for planar and spherical shallow water\nsystems. Of interest is subcritical flow, where the speed of the gravity wave\nis faster than that of nonlinear advection. In order to simulate these flows\nefficiently, we split the governing system into a stiff part describing the\ngravity wave and a non-stiff part associated with nonlinear advection. The\nformer is discretized implicitly with the HDG method while an explicit\nRunge-Kutta DG discretization is employed for the latter. The proposed IMEX\nHDG-DG framework: 1) facilitates high-order solutions both in time and space;\n  2) avoids overly small time-step sizes;\n  3) requires only one linear system solve per time stage;\n  4) relative to DG generates smaller and sparser linear systems while\npromoting further parallelism. Numerical results of various test cases\ndemonstrate that our methods are comparable to explicit Runge-Kutta DG schemes\nin terms of accuracy while allowing for much larger time step sizes.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 22:28:10 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Kang", "Shinhoo", ""], ["Giraldo", "Francis X.", ""], ["Bui-Thanh", "Tan", ""]]}, {"id": "1711.02810", "submitter": "Biswarup Bhattacharya", "authors": "Biswarup Bhattacharya, Abhishek Sinha", "title": "Deep Fault Analysis and Subset Selection in Solar Power Grids", "comments": "Presented at NIPS 2017 Workshop on Machine Learning for the\n  Developing World", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-availability of reliable and sustainable electric power is a major\nproblem in the developing world. Renewable energy sources like solar are not\nvery lucrative in the current stage due to various uncertainties like weather,\nstorage, land use among others. There also exists various other issues like\nmis-commitment of power, absence of intelligent fault analysis, congestion,\netc. In this paper, we propose a novel deep learning-based system for\npredicting faults and selecting power generators optimally so as to reduce\ncosts and ensure higher reliability in solar power systems. The results are\nhighly encouraging and they suggest that the approaches proposed in this paper\nhave the potential to be applied successfully in the developing world.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 03:09:51 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Bhattacharya", "Biswarup", ""], ["Sinha", "Abhishek", ""]]}, {"id": "1711.03156", "submitter": "Jiachen Yang", "authors": "Jiachen Yang, Xiaojing Ye, Rakshit Trivedi, Huan Xu, Hongyuan Zha", "title": "Learning Deep Mean Field Games for Modeling Large Population Behavior", "comments": "Accepted to International Conference on Learning Representations\n  (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of representing collective behavior of large\npopulations and predicting the evolution of a population distribution over a\ndiscrete state space. A discrete time mean field game (MFG) is motivated as an\ninterpretable model founded on game theory for understanding the aggregate\neffect of individual actions and predicting the temporal evolution of\npopulation distributions. We achieve a synthesis of MFG and Markov decision\nprocesses (MDP) by showing that a special MFG is reducible to an MDP. This\nenables us to broaden the scope of mean field game theory and infer MFG models\nof large real-world systems via deep inverse reinforcement learning. Our method\nlearns both the reward function and forward dynamics of an MFG from real data,\nand we report the first empirical test of a mean field game model of a\nreal-world social media population.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:43:39 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 06:33:04 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Yang", "Jiachen", ""], ["Ye", "Xiaojing", ""], ["Trivedi", "Rakshit", ""], ["Xu", "Huan", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1711.03232", "submitter": "Eric Mason", "authors": "Eric Mason, Birsen Yazici", "title": "Performance Analysis of Convex LRMR based Passive SAR Imaging", "comments": "Submitted to IEEE Transactions on Computational Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passive synthetic aperture radar (SAR) uses existing signals of opportunity\nsuch as communication and broadcasting signals. In our prior work, we have\ndeveloped a low-rank matrix recovery (LRMR) method that can reconstruct scenes\nwith extended and densely distributed point targets, overcoming shortcomings of\nconventional methods. The approach is based on correlating two sets of bistatic\nmeasurements, which results in a linear mapping of the tensor product of the\nscene reflectivity with itself. Recognizing this tensor product as a rank-one\npositive semi-definite (PSD) operator, we pose passive SAR image reconstruction\nas a LRMR problem with convex relaxation. In this paper, we present a\nperformance analysis of the convex LRMR-based passive SAR image reconstruction\nmethod. We use the restricted isometry property (RIP) and show that exact\nreconstruction is guaranteed under the condition that the pixel spacing or\nresolution satisfies a certain lower bound. We show that for sufficiently large\ncenter frequencies, our method provides superior resolution than that of\nFourier based methods, making it a super-resolution technique. Additionally, we\nshow that phaseless imaging is a special case of our passive SAR imaging\nmethod. We present extensive numerical simulation to validate our analysis.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 02:21:33 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Mason", "Eric", ""], ["Yazici", "Birsen", ""]]}, {"id": "1711.03331", "submitter": "Alexander Scheidler", "authors": "Alexander Scheidler and Leon Thurner and Martin Braun", "title": "Heuristic Optimization for Automated Distribution System Planning in\n  Network Integration Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network integration studies try to assess the impact of future developments,\nsuch as the increase of Renewable Energy Sources or the introduction of Smart\nGrid Technologies, on large-scale network areas. Goals can be to support\nstrategic alignment in the regulatory framework or to adapt the network\nplanning principles of Distribution System Operators. This study outlines an\napproach for the automated distribution system planning that can calculate\nnetwork reconfiguration, reinforcement and extension plans in a fully automated\nfashion. This allows the estimation of the expected cost in massive\nprobabilistic simulations of large numbers of real networks and constitutes a\ncore component of a framework for large-scale network integration studies.\nExemplary case study results are presented that were performed in cooperation\nwith different major distribution system operators. The case studies cover the\nestimation of expected network reinforcement costs, technical and economical\nassessment of smart grid technologies and structural network optimisation.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 11:25:21 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 12:30:26 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Scheidler", "Alexander", ""], ["Thurner", "Leon", ""], ["Braun", "Martin", ""]]}, {"id": "1711.03589", "submitter": "Dmitry Kulyabov PhD", "authors": "M. N. Gevorkyan, A. V. Demidova, Robert A. Sobolewski, I. S. Zaryadov,\n  A. V. Korolkova, D. S. Kulyabov and L. A. Sevastianov", "title": "Approaches to Stochastic Modeling of Wind Turbines", "comments": "in English; in Russian", "journal-ref": null, "doi": "10.7148/2017-0622", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. This paper study statistical data gathered from wind turbines\nlocated on the territory of the Republic of Poland. The research is aimed to\nconstruct the stochastic model that predicts the change of wind speed with\ntime. Purpose. The purpose of this work is to find the optimal distribution for\nthe approximation of available statistical data on wind speed. Methods. We\nconsider four distributions of a random variable: Log-Normal, Weibull, Gamma\nand Beta. In order to evaluate the parameters of distributions we use method of\nmaximum likelihood. To assess the the results of approximation we use a\nquantile-quantile plot. Results. All the considered distributions properly\napproximate the available data. The Weibull distribution shows the best results\nfor the extreme values of the wind speed. Conclusions. The results of the\nanalysis are consistent with the common practice of using the Weibull\ndistribution for wind speed modeling. In the future we plan to compare the\nresults obtained with a much larger data set as well as to build a stochastic\nmodel of the evolution of the wind speed depending on time.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 06:38:50 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Gevorkyan", "M. N.", ""], ["Demidova", "A. V.", ""], ["Sobolewski", "Robert A.", ""], ["Zaryadov", "I. S.", ""], ["Korolkova", "A. V.", ""], ["Kulyabov", "D. S.", ""], ["Sevastianov", "L. A.", ""]]}, {"id": "1711.06231", "submitter": "Sam Ma", "authors": "Zhanshan Ma", "title": "Extending species-area relationships (SAR) to diversity-area\n  relationships (DAR)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I extend the traditional SAR, which has achieved status of ecological law and\nplays a critical role in global biodiversity assessment, to the general (alpha-\nor beta-diversity in Hill numbers) diversity area relationship (DAR). The\nextension was motivated to remedy the limitation of traditional SAR that only\naddress one aspect of biodiversity scaling, i.e., species richness scaling over\nspace. The extension was made possible by the fact that all Hill numbers are in\nunits of species (referred to as the effective number of species or as species\nequivalents), and I postulated that Hill numbers should follow the same or\nsimilar pattern of SAR. I selected three DAR models, the traditional power law\n(PL), PLEC (PL with exponential cutoff) and PLIEC (PL with inverse exponential\ncutoff). I defined three new concepts and derived their quantifications: (i)DAR\nprofile: z-q series where z is the PL scaling parameter at different diversity\norder (q); (ii)PDO (pair-wise diversity overlap) profile: g-q series where g is\nthe PDO corresponding to q; (iii) MAD (maximal accrual diversity) profile:\nDmax-q series where Dmax is the MAD corresponding to q. Furthermore, the PDO-g\nis quantified based on the self-similarity property of the PL model, and Dmax\ncan be estimated from the PLEC parameters. The three profiles constitute a\nnovel DAR approach to biodiversity scaling. I verified the postulation with the\nAmerican gut microbiome project (AGP) dataset of 1473 healthy North American\nindividuals (the largest human dataset from a single project to date). The PL\nmodel was preferred due to its simplicity and established ecological properties\nsuch as self-similarity (necessary for establishing PDO profile), and PLEC has\nan advantage in establishing the MAD profile. All three profiles for the AGP\ndataset were successfully quantified and compared with existing SAR parameters\nin the literature whenever possible.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 18:20:25 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Ma", "Zhanshan", ""]]}, {"id": "1711.06276", "submitter": "Eliu Huerta", "authors": "E. A. Huerta, C. J. Moore, Prayush Kumar, Daniel George, Alvin J. K.\n  Chua, Roland Haas, Erik Wessel, Daniel Johnson, Derek Glennon, Adam Rebei, A.\n  Miguel Holgado, Jonathan R. Gair and Harald P. Pfeiffer", "title": "Eccentric, nonspinning, inspiral, Gaussian-process merger approximant\n  for the detection and characterization of eccentric binary black hole mergers", "comments": "19 pages, 10 figures, 1 Appendix. v2: we use numerical relativity\n  simulations to quantify the importance of including higher-order waveform\n  multipoles for the detection of eccentric binary black hole mergers,\n  references added. Accepted to Phys. Rev. D", "journal-ref": "Phys. Rev. D 97, 024031 (2018)", "doi": "10.1103/PhysRevD.97.024031", "report-no": null, "categories": "gr-qc astro-ph.CO astro-ph.HE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present $\\texttt{ENIGMA}$, a time domain, inspiral-merger-ringdown\nwaveform model that describes non-spinning binary black holes systems that\nevolve on moderately eccentric orbits. The inspiral evolution is described\nusing a consistent combination of post-Newtonian theory, self-force and black\nhole perturbation theory. Assuming eccentric binaries that circularize prior to\ncoalescence, we smoothly match the eccentric inspiral with a stand-alone,\nquasi-circular merger, which is constructed using machine learning algorithms\nthat are trained with quasi-circular numerical relativity waveforms. We show\nthat $\\texttt{ENIGMA}$ reproduces with excellent accuracy the dynamics of\nquasi-circular compact binaries. We validate $\\texttt{ENIGMA}$ using a set of\n$\\texttt{Einstein Toolkit}$ eccentric numerical relativity waveforms, which\ndescribe eccentric binary black hole mergers with mass-ratios between $1 \\leq q\n\\leq 5.5$, and eccentricities $e_0 \\lesssim 0.2$ ten orbits before merger. We\nuse this model to explore in detail the physics that can be extracted with\nmoderately eccentric, non-spinning binary black hole mergers. We use\n$\\texttt{ENIGMA}$ to show that GW150914, GW151226, GW170104, GW170814 and\nGW170608 can be effectively recovered with spinning, quasi-circular templates\nif the eccentricity of these events at a gravitational wave frequency of 10Hz\nsatisfies $e_0\\leq \\{0.175,\\, 0.125,\\,0.175,\\,0.175,\\, 0.125\\}$, respectively.\nWe show that if these systems have eccentricities $e_0\\sim 0.1$ at a\ngravitational wave frequency of 10Hz, they can be misclassified as\nquasi-circular binaries due to parameter space degeneracies between\neccentricity and spin corrections. Using our catalog of eccentric numerical\nrelativity simulations, we discuss the importance of including higher-order\nwaveform multipoles in gravitational wave searches of eccentric binary black\nhole mergers.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 19:00:02 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 20:59:45 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Huerta", "E. A.", ""], ["Moore", "C. J.", ""], ["Kumar", "Prayush", ""], ["George", "Daniel", ""], ["Chua", "Alvin J. K.", ""], ["Haas", "Roland", ""], ["Wessel", "Erik", ""], ["Johnson", "Daniel", ""], ["Glennon", "Derek", ""], ["Rebei", "Adam", ""], ["Holgado", "A. Miguel", ""], ["Gair", "Jonathan R.", ""], ["Pfeiffer", "Harald P.", ""]]}, {"id": "1711.06333", "submitter": "Jason Phipps Morgan", "authors": "J. M. Taram\\'on, J. P. Morgan, C. Shi, and J. Hasenclever", "title": "Generation of unstructured meshes in 2-D, 3-D, and spherical geometries\n  with embedded high resolution sub-regions", "comments": "20 pages + supplement, submitted to SIAM J. Sci. Comp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present 2-D, 3-D, and spherical mesh generators for the Finite Element\nMethod (FEM) using triangular and tetrahedral elements. The mesh nodes are\ntreated as if they were linked by virtual springs that obey Hooke's law. Given\nthe desired length for the springs, the FEM is used to solve for the optimal\nnodal positions for the static equilibrium of this spring system. A\n'guide-mesh' approach allows the user to create embedded high resolution\nsub-regions within a coarser mesh. The method converges rapidly. For example,\nin 3-D, the algorithm is able to refine a specific region within an\nunstructured tetrahedral spherical shell so that the edge-length factor\n$l_{0r}/l_{0c} = 1/33$ within a few iterations, where $l_{0r}$ and $l_{0c}$ are\nthe desired spring length for elements inside the refined and coarse regions\nrespectively. One use for this type of mesh is to model regional problems as a\nfine region within a global mesh that has no fictitious boundaries, at only a\nsmall additional computational cost. The algorithm also includes routines to\nlocally improve the quality of the mesh and to avoid badly shaped\n'slivers-like' tetrahedra.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 20:01:38 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Taram\u00f3n", "J. M.", ""], ["Morgan", "J. P.", ""], ["Shi", "C.", ""], ["Hasenclever", "J.", ""]]}, {"id": "1711.07325", "submitter": "Wiktor Daszczuk", "authors": "W{\\l}odzimierz Choroma\\'nski, Wiktor Daszczuk, Jaros{\\l}aw Dyduch,\n  Mariusz Maciejewski, Pawe{\\l} Brach, Waldemar Grabski", "title": "PRT (Personal Rapid Transit) network simulation", "comments": "17 pages, 6 figures", "journal-ref": "Proceedings of the 13th World Conference on Transportation\n  Research, Rio de Janeiro, Brasil, 7-10 July 2013, Joao Victor (ed.), 2014,\n  Federal University of Rio de Janeiro, ISBN 978-85-285-0232-9", "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation problems of large urban conurbations inspire search for new\ntransportation systems, that meet high environmental standards, are relatively\ncheap and user friendly. The latter element also includes the needs of disabled\nand elderly people. This article concerns a new transportation system PRT -\nPersonal Rapid Transit. In this article the attention is focused on the\nanalysis of the efficiency of the PRT transport network. The simulator of\nvehicle movement in PRT network as well as algorithms for traffic management\nand control will be presented. The proposal of its physical implementation will\nbe also included.\n", "versions": [{"version": "v1", "created": "Wed, 18 Oct 2017 19:09:48 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Choroma\u0144ski", "W\u0142odzimierz", ""], ["Daszczuk", "Wiktor", ""], ["Dyduch", "Jaros\u0142aw", ""], ["Maciejewski", "Mariusz", ""], ["Brach", "Pawe\u0142", ""], ["Grabski", "Waldemar", ""]]}, {"id": "1711.07370", "submitter": "Jaroslaw Zola", "authors": "Steven Y. Ko, Lauren Sassoubre, Jaroslaw Zola", "title": "Applications and Challenges of Real-time Mobile DNA Analysis", "comments": null, "journal-ref": null, "doi": "10.1145/3177102.3177114", "report-no": null, "categories": "cs.CE cs.DC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DNA sequencing is the process of identifying the exact order of\nnucleotides within a given DNA molecule. The new portable and relatively\ninexpensive DNA sequencers, such as Oxford Nanopore MinION, have the potential\nto move DNA sequencing outside of laboratory, leading to faster and more\naccessible DNA-based diagnostics. However, portable DNA sequencing and analysis\nare challenging for mobile systems, owing to high data throughputs and\ncomputationally intensive processing performed in environments with unreliable\nconnectivity and power.\n  In this paper, we provide an analysis of the challenges that mobile systems\nand mobile computing must address to maximize the potential of portable DNA\nsequencing, and in situ DNA analysis. We explain the DNA sequencing process and\nhighlight the main differences between traditional and portable DNA sequencing\nin the context of the actual and envisioned applications. We look at the\nidentified challenges from the perspective of both algorithms and systems\ndesign, showing the need for careful co-design.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 15:17:31 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Ko", "Steven Y.", ""], ["Sassoubre", "Lauren", ""], ["Zola", "Jaroslaw", ""]]}, {"id": "1711.07651", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu, Albert Y.S. Lam, David J. Hill, Victor O.K. Li", "title": "Delay Aware Intelligent Transient Stability Assessment System", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2017.2746093", "report-no": null, "categories": "cs.SY cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transient stability assessment is a critical tool for power system design and\noperation. With the emerging advanced synchrophasor measurement techniques,\nmachine learning methods are playing an increasingly important role in power\nsystem stability assessment. However, most existing research makes a strong\nassumption that the measurement data transmission delay is negligible. In this\npaper, we focus on investigating the influence of communication delay on\nsynchrophasor-based transient stability assessment. In particular, we develop a\ndelay aware intelligent system to address this issue. By utilizing an ensemble\nof multiple long short-term memory networks, the proposed system can make early\nassessments to achieve a much shorter response time by utilizing incomplete\nsystem variable measurements. Compared with existing work, our system is able\nto make accurate assessments with a significantly improved efficiency. We\nperform numerous case studies to demonstrate the superiority of the proposed\nintelligent system, in which accurate assessments can be developed with time\none third less than state-of-the-art methodologies. Moreover, the simulations\nindicate that noise in the measurements has trivial impact on the assessment\nperformance, demonstrating the robustness of the proposed system.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:21:33 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Yu", "James J. Q.", ""], ["Lam", "Albert Y. S.", ""], ["Hill", "David J.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1711.07652", "submitter": "James J.Q. Yu", "authors": "James J.Q. Yu, Albert Y.S. Lam, David J. Hill, Victor O.K. Li", "title": "A Unified Framework for Wide Area Measurement System Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide area measurement system (WAMS) is one of the essential components in the\nfuture power system. To make WAMS construction plans, practical models of the\npower network observability, reliability, and underlying communication\ninfrastructures need to be considered. To address this challenging problem, in\nthis paper we propose a unified framework for WAMS planning to cover most\nrealistic concerns in the construction process. The framework jointly optimizes\nthe system construction cost, measurement reliability, and volume of\nsynchrophasor data traffic resulting in a multi-objective optimization problem,\nwhich provides multiple Pareto optimal solutions to suit different requirements\nby the utilities. The framework is verified on two IEEE test systems. The\nsimulation results demonstrate the trade-off relationships among the proposed\nobjectives. Moreover, the proposed framework can develop optimal WAMS plans for\nfull observability with minimal cost. This work develops a comprehensive\nframework for most practical WAMS construction designs.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 07:21:44 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Yu", "James J. Q.", ""], ["Lam", "Albert Y. S.", ""], ["Hill", "David J.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1711.09013", "submitter": "Arnold Kalmbach", "authors": "Arnold Kalmbach, Heidi M. Sosik, Gregory Dudek, and Yogesh Girdhar", "title": "Learning Seasonal Phytoplankton Communities with Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop and demonstrate a probabilistic generative model for\nphytoplankton communities. The proposed model takes counts of a set of\nphytoplankton taxa in a timeseries as its training data, and models communities\nby learning sparse co-occurrence structure between the taxa. Our model is\nprobabilistic, where communities are represented by probability distributions\nover the species, and each time-step is represented by a probability\ndistribution over the communities. The proposed approach uses a non-parametric,\nspatiotemporal topic model to encourage the communities to form an\ninterpretable representation of the data, without making strong assumptions\nabout the communities. We demonstrate the quality and interpretability of our\nmethod by its ability to improve performance of a simplistic regression model.\nWe show that simple linear regression is sufficient to predict the community\ndistribution learned by our method, and therefore the taxon distributions, from\na set of naively chosen environment variables. In contrast, a similar\nregression model is insufficient to predict the taxon distributions directly or\nthrough PCA with the same level of accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 21:20:26 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 15:35:30 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Kalmbach", "Arnold", ""], ["Sosik", "Heidi M.", ""], ["Dudek", "Gregory", ""], ["Girdhar", "Yogesh", ""]]}, {"id": "1711.09307", "submitter": "Elia Merzari", "authors": "Elia Merzari, Aleksandr Obabko, and Paul Fischer", "title": "Spectral Element Methods for Liquid Metal Reactors Applications", "comments": "23 pages and 15 figures", "journal-ref": null, "doi": null, "report-no": "ANL/MCS-P9028-1117", "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Funded by the U.S. Department of Energy, the Nuclear Energy Advanced Modeling\nand Simulation (NEAMS) program aims to develop an integrated multiphysics\nsimulation capability for the design and analysis of future generations of\nnuclear power plants. NEAMS embraces a multiresolution hierarchy designing the\ncode suite structure to ultimately span the full range of length and time\nscales present in relevant reactor design and safety analyses. Advanced\nreactors, such as liquid metal reactors, rely on innovative component designs\nto meet cost and safety targets. In order to span a wider design range,\nadvanced modeling and simulation capabilities that rely on minimal assumptions\nplay an important role in optimizing the design. Over the past several years\nthe NEAMS program has developed the integrated multiphysics code suite\n(thermal-hydraulics, structural analysis and neutronics) SHARP aimed at\nstreamlining the prototyping of such components. For the simulation of fluid\nflow and heat transfer, SHARP focuses on the high-fidelity end, aiming\nprimarily at turbulence-resolving techniques such large eddy simulation (LES)\nand direct numerical simulation (DNS). The computational fluid dynamics code\n(CFD) selected for SHARP is Nek5000, a state-of-the-art highly scalable tool\nemploying the spectral element method (SEM). In this manuscript, to be\npublished in a Von karman institute lecture series monograph on liquid metal\nreactors, we review the method and its implementation in Nek5000. We also\nexamine several applications. We note that Nek5000 is also regularly employed\nfor intermediate-fidelity approaches such as Reynolds-averaged Navier-Stokes\n(RANS) and for reduced-order models employing momentum sources or porous media,\nespecially when coupled to neutronics modeling.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 23:29:43 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Merzari", "Elia", ""], ["Obabko", "Aleksandr", ""], ["Fischer", "Paul", ""]]}, {"id": "1711.09490", "submitter": "Hyemin Han", "authors": "Hyemin Han, Kangwook Lee, Firat Soylu", "title": "Simulating outcomes of interventions using a multipurpose simulation\n  program based on the Evolutionary Causal Matrices and Markov Chain", "comments": null, "journal-ref": null, "doi": "10.1007/s10115-017-1151-0", "report-no": null, "categories": "stat.AP cs.CE cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Predicting long-term outcomes of interventions is necessary for educational\nand social policy-making processes that might widely influence our society for\nthe long-term. However, performing such predictions based on data from\nlarge-scale experiments might be challenging due to the lack of time and\nresources. In order to address this issue, computer simulations based on\nEvolutionary Causal Matrices and Markov Chain can be used to predict long-term\noutcomes with relatively small-scale lab data. In this paper, we introduce\nPython classes implementing a computer simulation model and presented some\npilots implementations demonstrating how the model can be utilized for\npredicting outcomes of diverse interventions. We also introduce the\nclass-structured simulation module both with real experimental data and with\nhypothetical data formulated based on social psychological theories. Classes\ndeveloped and tested in the present study provide researchers and practitioners\nwith a feasible and practical method to simulate intervention outcomes\nprospectively.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 23:24:58 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Han", "Hyemin", ""], ["Lee", "Kangwook", ""], ["Soylu", "Firat", ""]]}, {"id": "1711.09852", "submitter": "Slobodan Milovanovi\\'c", "authors": "Slobodan Milovanovi\\'c and Victor Shcherbakov", "title": "Pricing Derivatives under Multiple Stochastic Factors by Localized\n  Radial Basis Function Methods", "comments": "The authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two localized Radial Basis Function (RBF) methods, the Radial\nBasis Function Partition of Unity method (RBF-PUM) and the Radial Basis\nFunction generated Finite Differences method (RBF-FD), for solving financial\nderivative pricing problems arising from market models with multiple stochastic\nfactors. We demonstrate the useful features of the proposed methods, such as\nhigh accuracy, sparsity of the differentiation matrices, mesh-free nature and\nmulti-dimensional extendability, and show how to apply these methods for\nsolving time-dependent higher-dimensional PDEs in finance. We test these\nmethods on several problems that incorporate stochastic asset, volatility, and\ninterest rate dynamics by conducting numerical experiments. The results\nillustrate the capability of both methods to solve the problems to a sufficient\naccuracy within reasonable time. Both methods exhibit similar orders of\nconvergence, which can be further improved by a more elaborate choice of the\nmethod parameters. Finally, we discuss the parallelization potentials of the\nproposed methods and report the speedup on the example of RBF-FD.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 17:47:14 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 17:16:49 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Milovanovi\u0107", "Slobodan", ""], ["Shcherbakov", "Victor", ""]]}, {"id": "1711.10579", "submitter": "Bin Wang", "authors": "Bin Wang, John Bachan, Cy Chan", "title": "ExaGridPF: A Parallel Power Flow Solver for Transmission and Unbalanced\n  Distribution Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates parallelization strategies for solving power flow\nproblems in both transmission and unbalanced, three-phase distribution systems\nby developing a scalable power flow solver, ExaGridPF, which is compatible with\nexisting high-performance computing platforms. Newton-Raphson (NR) and\nNewton-Krylov (NK) algorithms have been implemented to verify the performance\nimprovement over both standard IEEE test cases and synthesized grid topologies.\nFor three-phase, unbalanced system, we adapt the current injection method (CIM)\nto model the power flow and utilize SuperLU to parallelize the computing load\nacross multiple threads. The experimental results indicate that more than 5\ntimes speedup ratio can be achieved for synthesized large-scale transmission\ntopologies, and significant efficiency improvements are observed over existing\nmethods for the distribution networks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 21:57:14 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Wang", "Bin", ""], ["Bachan", "John", ""], ["Chan", "Cy", ""]]}, {"id": "1711.10782", "submitter": "Ali Farokhi Nejad", "authors": "A.Farokhi Nejad, M. pourasghar, S.Peirovi, M.N.Tamin", "title": "Computational Aided Design for Generating a Modular, Lightweight Car\n  Concept", "comments": "9 pages, 14 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing an appropriate design process for a conceptual model is a stepping\nstone toward designing car bodies. This paper presents a methodology to design\na lightweight and modular space frame chassis for a sedan electric car. The\ndual phase high strength steel with improved mechanical properties is employed\nto reduce the weight of the car body. Utilizing the finite element analysis\nyields two models in order to predict the performance of each component. The\nfirst model is a beam structure with a rapid response in structural stiffness\nsimulation. This model is used for performing the static tests including modal\nfrequency, bending stiffens and torsional stiffness evaluation. Whereas the\nsecond model, i.e., a shell model, is proposed to illustrate every module's\nmechanical behavior as well as its crashworthiness efficiency. In order to\nperform the crashworthiness analysis, the explicit nonlinear dynamic solver\nprovided by ABAQUS, a commercial finite element software, is used. The results\nof finite element beam and shell models are in line with the concept design\nspecifications. Implementation of this procedure leads to generate a\nlightweight and modular concept for an electric car.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 11:30:50 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Nejad", "A. Farokhi", ""], ["pourasghar", "M.", ""], ["Peirovi", "S.", ""], ["Tamin", "M. N.", ""]]}, {"id": "1711.11230", "submitter": "Joseph Chow", "authors": "Saeid Rasulkhani, Joseph Y. J. Chow", "title": "Route-cost-assignment with joint user and operator behavior as a\n  many-to-one stable matching assignment game", "comments": null, "journal-ref": "Transportation Research Part B 124 (2019) 60-81", "doi": "10.1016/j.trb.2019.04.008", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalized market equilibrium model using assignment game\ncriteria for evaluating transportation systems that consist of both operators'\nand users' decisions. The model finds stable pricing, in terms of generalized\ncosts, and matches between user populations in a network to set of routes with\nline capacities. The proposed model gives a set of stable outcomes instead of\nsingle point pricing that allows operators to design ticket pricing,\nroutes/schedules that impact access/egress, shared policies that impact\nwait/transfer costs, etc., based on a desired mechanism or policy. The set of\nstable outcomes is proven to be convex from which assignment-dependent unique\nuser-optimal and operator-optimal outcomes can be obtained. Different user\ngroups can benefit from using this model in a prescriptive manner or within a\nsequential design process. We look at several different examples to test our\nmodel: small examples of fixed transit routes and a case study using a small\nsubset of taxi data in NYC. The case study illustrates how one can use the\nmodel to evaluate a policy that can require passengers to walk up to 1 block\naway to meet with a shared taxi without turning away passengers.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 05:06:51 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 16:16:51 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 05:04:36 GMT"}, {"version": "v4", "created": "Mon, 22 Oct 2018 16:33:01 GMT"}, {"version": "v5", "created": "Wed, 5 Dec 2018 00:42:25 GMT"}, {"version": "v6", "created": "Thu, 11 Apr 2019 02:42:18 GMT"}, {"version": "v7", "created": "Sat, 13 Apr 2019 03:08:48 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Rasulkhani", "Saeid", ""], ["Chow", "Joseph Y. J.", ""]]}, {"id": "1711.11519", "submitter": "Yusen He", "authors": "Tinghui Ouyang, Yusen He, Huajin Li, Zhiyu Sun, Stephen Baek", "title": "A Deep Learning Framework for Short-term Power Load Forecasting", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TETCI.2018.2880511", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scheduling and operation of power system becomes prominently complex and\nuncertain, especially with the penetration of distributed power. Load\nforecasting matters to the effective operation of power system. This paper\nproposes a novel deep learning framework to forecast the short-term grid load.\nFirst, the load data is processed by Box-Cox transformation, and two parameters\n(electricity price and temperature) are investigated. Then, to quantify the\ntail-dependence of power load on the two parameters, parametric Copula models\nare fitted and the threshold of peak load are computed. Next, a deep belief\nnetwork is built to forecast the hourly load of the power grid. One year grid\nload data collected from an urbanized area in Texas, United States is utilized\nin the case studies. Short-term load forecasting are examined in four seasons\nindependently. Day-ahead and week-ahead load forecasting experiments are\nconducted in each season using the proposed framework. The proposed framework\nis compared with classical neural networks, support vector regression machine,\nextreme learning machine, and classical deep belief networks. The load\nforecasting performances are assessed by mean absolute percentage error, root\nmean square error, and hit rate. Computational results confirm the\neffectiveness of the proposed data-driven deep learning framework. The\nprediction accuracies of both day-ahead forecasting and week-ahead forecasting\ndemonstrate that the proposed framework outperforms the tested algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 17:12:54 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 22:39:28 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Ouyang", "Tinghui", ""], ["He", "Yusen", ""], ["Li", "Huajin", ""], ["Sun", "Zhiyu", ""], ["Baek", "Stephen", ""]]}]