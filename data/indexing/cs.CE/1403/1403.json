[{"id": "1403.0135", "submitter": "Sergio Consoli", "authors": "Sergio Consoli, Diego Reforgiato Recupero, Vanni Zavarella", "title": "A survey on tidal analysis and forecasting methods for Tsunami detection", "comments": "Review paper", "journal-ref": "Science of Tsunami Hazards, 33(1):1-56; Feb. 2014", "doi": null, "report-no": null, "categories": "cs.CE math.OC physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate analysis and forecasting of tidal level are very important tasks for\nhuman activities in oceanic and coastal areas. They can be crucial in\ncatastrophic situations like occurrences of Tsunamis in order to provide a\nrapid alerting to the human population involved and to save lives. Conventional\ntidal forecasting methods are based on harmonic analysis using the least\nsquares method to determine harmonic parameters. However, a large number of\nparameters and long-term measured data are required for precise tidal level\npredictions with harmonic analysis. Furthermore, traditional harmonic methods\nrely on models based on the analysis of astronomical components and they can be\ninadequate when the contribution of non-astronomical components, such as the\nweather, is significant. Other alternative approaches have been developed in\nthe literature in order to deal with these situations and provide predictions\nwith the desired accuracy, with respect also to the length of the available\ntidal record. These methods include standard high or band pass filtering\ntechniques, although the relatively deterministic character and large amplitude\nof tidal signals make special techniques, like artificial neural networks and\nwavelets transform analysis methods, more effective. This paper is intended to\nprovide the communities of both researchers and practitioners with a broadly\napplicable, up to date coverage of tidal analysis and forecasting methodologies\nthat have proven to be successful in a variety of circumstances, and that hold\nparticular promise for success in the future. Classical and novel methods are\nreviewed in a systematic and consistent way, outlining their main concepts and\ncomponents, similarities and differences, advantages and disadvantages.\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2014 23:13:57 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Consoli", "Sergio", ""], ["Recupero", "Diego Reforgiato", ""], ["Zavarella", "Vanni", ""]]}, {"id": "1403.0240", "submitter": "Ivo Sbalzarini", "authors": "Ivo F. Sbalzarini, Sophie Schneider, Janick Cardinale", "title": "Particle methods enable fast and simple approximation of Sobolev\n  gradients in image segmentation", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.NA q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bio-image analysis is challenging due to inhomogeneous intensity\ndistributions and high levels of noise in the images. Bayesian inference\nprovides a principled way for regularizing the problem using prior knowledge. A\nfundamental choice is how one measures \"distances\" between shapes in an image.\nIt has been shown that the straightforward geometric L2 distance is degenerate\nand leads to pathological situations. This is avoided when using Sobolev\ngradients, rendering the segmentation problem less ill-posed. The high\ncomputational cost and implementation overhead of Sobolev gradients, however,\nhave hampered practical applications. We show how particle methods as applied\nto image segmentation allow for a simple and computationally efficient\nimplementation of Sobolev gradients. We show that the evaluation of Sobolev\ngradients amounts to particle-particle interactions along the contour in an\nimage. We extend an existing particle-based segmentation algorithm to using\nSobolev gradients. Using synthetic and real-world images, we benchmark the\nresults for both 2D and 3D images using piecewise smooth and piecewise constant\nregion models. The present particle approximation of Sobolev gradients is 2.8\nto 10 times faster than the previous reference implementation, but retains the\nknown favorable properties of Sobolev gradients. This speedup is achieved by\nusing local particle-particle interactions instead of solving a global Poisson\nequation at each iteration. The computational time per iteration is higher for\nSobolev gradients than for L2 gradients. Since Sobolev gradients precondition\nthe optimization problem, however, a smaller number of overall iterations may\nbe necessary for the algorithm to converge, which can in some cases amortize\nthe higher per-iteration cost.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 16:58:29 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Sbalzarini", "Ivo F.", ""], ["Schneider", "Sophie", ""], ["Cardinale", "Janick", ""]]}, {"id": "1403.0306", "submitter": "Loc Tran Vinh", "authors": "Loc V. Tran, Vinh Phu Nguyen, M. Abdel Wahab, H. Nguyen-Xuan", "title": "An extended isogeometric analysis for vibration of cracked FGM plates\n  using higher-order shear deformation theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel and effective formulation that combines the eXtended IsoGeometric\nApproach (XIGA) and Higher-order Shear Deformation Theory (HSDT) is proposed to\nstudy the free vibration of cracked Functionally Graded Material (FGM) plates.\nHerein, the general HSDT model with five unknown variables per node is applied\nfor calculating the stiffness matrix without needing Shear Correction Factor\n(SCF). In order to model the discontinuous and singular phenomena in the\ncracked plates, IsoGeometric Analysis (IGA) utilizing the Non-Uniform Rational\nB-Spline (NURBS) functions is incorporated with enrichment functions through\nthe partition of unity method. NURBS basis functions with their inherent\narbitrary high order smoothness permit the C1 requirement of the HSDT model.\nThe material properties of the FGM plates vary continuously through the plate\nthickness according to an exponent function. The effects of gradient index,\ncrack length, crack location, length to thickness on the natural frequencies\nand mode shapes of simply supported and clamped FGM plate are studied.\nNumerical examples are provided to show excellent performance of the proposed\nmethod compared with other published solutions in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 04:09:30 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Tran", "Loc V.", ""], ["Nguyen", "Vinh Phu", ""], ["Wahab", "M. Abdel", ""], ["Nguyen-Xuan", "H.", ""]]}, {"id": "1403.0307", "submitter": "Loc Tran Vinh", "authors": "Loc V. Tran, Chien H. Thai, Buntara S. Gan and H. Nguyen-Xuan", "title": "Isogeometric finite element analysis of laminated composite plates based\n  on a four variable refined plate theory", "comments": "arXiv admin note: substantial text overlap with arXiv:1310.1847", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a novel and effective formulation based on isogeometric\napproach (IGA) and Refined Plate Theory (RPT) is proposed to study the behavior\nof laminated composite plates. Using many kinds of higher-order distributed\nfunctions, RPT model naturally satisfies the traction-free boundary conditions\nat plate surfaces and describes the non-linear distribution of shear stresses\nwithout requiring shear correction factor (SCF). IGA utilizes the basis\nfunctions, namely B-splines or non-uniform rational B-splines (NURBS), which\nachieve easily the smoothness of any arbitrary order. It hence satisfies the C1\nrequirement of the RPT model. The static, dynamic and buckling analysis of\nrectangular plates is investigated for different boundary conditions. Numerical\nresults show high effectiveness of the present formulation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 04:10:48 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Tran", "Loc V.", ""], ["Thai", "Chien H.", ""], ["Gan", "Buntara S.", ""], ["Nguyen-Xuan", "H.", ""]]}, {"id": "1403.0468", "submitter": "Evgeny Nikulchev", "authors": "Evgeny Nikulchev, Oleg Kozlov", "title": "Identification of Structural Model for Chaotic Systems", "comments": null, "journal-ref": "Journal of Modern Physics, 2013, 4, 1381-1392", "doi": "10.4236/jmp.2013.410166", "report-no": null, "categories": "math.DS cs.CE cs.SY nlin.CD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This article is talking about the study constructive method of structural\nidentification systems with chaotic dynamics. It is shown that the\nreconstructed attractors are a source of information not only about the\ndynamics but also on the basis of the attractors which can be identified and\nthe mere sight of models. It is known that the knowledge of the symmetry group\nallows you to specify the form of a minimal system. Forming a group\ntransformation can be found in the recon-structed attractor. The affine system\nas the basic model is selected. Type of a nonlinear system is the subject of\ncalcula-tions. A theoretical analysis is performed and proof of the possibility\nof constructing models in the central invariant manifold reduced. This\ndeveloped algorithm for determining the observed symmetry in the attractor. The\nresults of identification used in real systems are an application.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 17:19:46 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Nikulchev", "Evgeny", ""], ["Kozlov", "Oleg", ""]]}, {"id": "1403.0500", "submitter": "Blesson Varghese", "authors": "Blesson Varghese and Gerard McKee and Vassil Alexandrov", "title": "Automating Fault Tolerance in High-Performance Computational Biological\n  Jobs Using Multi-Agent Approaches", "comments": "Computers in Biology and Medicine", "journal-ref": null, "doi": "10.1016/j.compbiomed.2014.02.005", "report-no": null, "categories": "cs.DC cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Large-scale biological jobs on high-performance computing systems\nrequire manual intervention if one or more computing cores on which they\nexecute fail. This places not only a cost on the maintenance of the job, but\nalso a cost on the time taken for reinstating the job and the risk of losing\ndata and execution accomplished by the job before it failed. Approaches which\ncan proactively detect computing core failures and take action to relocate the\ncomputing core's job onto reliable cores can make a significant step towards\nautomating fault tolerance.\n  Method: This paper describes an experimental investigation into the use of\nmulti-agent approaches for fault tolerance. Two approaches are studied, the\nfirst at the job level and the second at the core level. The approaches are\ninvestigated for single core failure scenarios that can occur in the execution\nof parallel reduction algorithms on computer clusters. A third approach is\nproposed that incorporates multi-agent technology both at the job and core\nlevel. Experiments are pursued in the context of genome searching, a popular\ncomputational biology application.\n  Result: The key conclusion is that the approaches proposed are feasible for\nautomating fault tolerance in high-performance computing systems with minimal\nhuman intervention. In a typical experiment in which the fault tolerance is\nstudied, centralised and decentralised checkpointing approaches on an average\nadd 90% to the actual time for executing the job. On the other hand, in the\nsame experiment the multi-agent approaches add only 10% to the overall\nexecution time.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 17:42:24 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Varghese", "Blesson", ""], ["McKee", "Gerard", ""], ["Alexandrov", "Vassil", ""]]}, {"id": "1403.0541", "submitter": "Saadat Anwar", "authors": "Saadat Anwar", "title": "Representing, reasoning and answering questions about biological\n  pathways - various applications", "comments": "thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological organisms are composed of numerous interconnected biochemical\nprocesses. Diseases occur when normal functionality of these processes is\ndisrupted. Thus, understanding these biochemical processes and their\ninterrelationships is a primary task in biomedical research and a prerequisite\nfor diagnosing diseases, and drug development. Scientists studying these\nprocesses have identified various pathways responsible for drug metabolism, and\nsignal transduction, etc.\n  Newer techniques and speed improvements have resulted in deeper knowledge\nabout these pathways, resulting in refined models that tend to be large and\ncomplex, making it difficult for a person to remember all aspects of it. Thus,\ncomputer models are needed to analyze them. We want to build such a system that\nallows modeling of biological systems and pathways in such a way that we can\nanswer questions about them.\n  Many existing models focus on structural and/or factoid questions, using\nsurface-level knowledge that does not require understanding the underlying\nmodel. We believe these are not the kind of questions that a biologist may ask\nsomeone to test their understanding of the biological processes. We want our\nsystem to answer the kind of questions a biologist may ask. Such questions\nappear in early college level text books.\n  Thus the main goal of our thesis is to develop a system that allows us to\nencode knowledge about biological pathways and answer such questions about them\ndemonstrating understanding of the pathway. To that end, we develop a language\nthat will allow posing such questions and illustrate the utility of our\nframework with various applications in the biological domain. We use some\nexisting tools with modifications to accomplish our goal.\n  Finally, we apply our system to real world applications by extracting pathway\nknowledge from text and answering questions related to drug development.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 19:45:41 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Anwar", "Saadat", ""]]}, {"id": "1403.0623", "submitter": "Saptarshi Das", "authors": "Indranil Pan, Daya Shankar Pandey, Saptarshi Das", "title": "Global solar irradiation prediction using a multi-gene genetic\n  programming approach", "comments": "31 pages, 16 figures, 5 tables", "journal-ref": "Journal of Renewable and Sustainable Energy, vol. 5, no. 6, pp.\n  063129, 2013", "doi": "10.1063/1.4850495", "report-no": null, "categories": "cs.NE cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a nonlinear symbolic regression technique using an\nevolutionary algorithm known as multi-gene genetic programming (MGGP) is\napplied for a data-driven modelling between the dependent and the independent\nvariables. The technique is applied for modelling the measured global solar\nirradiation and validated through numerical simulations. The proposed modelling\ntechnique shows improved results over the fuzzy logic and artificial neural\nnetwork (ANN) based approaches as attempted by contemporary researchers. The\nmethod proposed here results in nonlinear analytical expressions, unlike those\nwith neural networks which is essentially a black box modelling approach. This\nadditional flexibility is an advantage from the modelling perspective and helps\nto discern the important variables which affect the prediction. Due to the\nevolutionary nature of the algorithm, it is able to get out of local minima and\nconverge to a global optimum unlike the back-propagation (BP) algorithm used\nfor training neural networks. This results in a better percentage fit than the\nones obtained using neural networks by contemporary researchers. Also a\nhold-out cross validation is done on the obtained genetic programming (GP)\nresults which show that the results generalize well to new data and do not\nover-fit the training samples. The multi-gene GP results are compared with\nthose, obtained using its single-gene version and also the same with four\nclassical regression models in order to show the effectiveness of the adopted\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 22:34:28 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Pan", "Indranil", ""], ["Pandey", "Daya Shankar", ""], ["Das", "Saptarshi", ""]]}, {"id": "1403.1313", "submitter": "Roshan Ragel", "authors": "P. Perera and R. G. Ragel", "title": "Accelerating motif finding in DNA sequences with multicore CPUs", "comments": null, "journal-ref": "Industrial and Information Systems (ICIIS), 2013 8th IEEE\n  International Conference on, pp. 242-247, 17-20 Dec. 2013", "doi": "10.1109/ICIInfS.2013.6731989", "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motif discovery in DNA sequences is a challenging task in molecular biology.\nIn computational motif discovery, Planted (l, d) motif finding is a widely\nstudied problem and numerous algorithms are available to solve it. Both\nhardware and software accelerators have been introduced to accelerate the motif\nfinding algorithms. However, the use of hardware accelerators such as FPGAs\nneeds hardware specialists to design such systems. Software based acceleration\nmethods on the other hand are easier to implement than hardware acceleration\ntechniques. Grid computing is one such software based acceleration technique\nwhich has been used in acceleration of motif finding. However, drawbacks such\nas network communication delays and the need of fast interconnection between\nnodes in the grid can limit its usage and scalability. As using multicore CPUs\nto accelerate CPU intensive tasks are becoming increasingly popular and common\nnowadays, we can employ it to accelerate motif finding and it can be a faster\nmethod than grid based acceleration. In this paper, we have explored the use of\nmulticore CPUs to accelerate motif finding. We have accelerated the Skip-Brute\nForce algorithm on multicore CPUs parallelizing it using the POSIX thread\nlibrary. Our method yielded an average speed up of 34x on a 32-core processor\ncompared to a speed up of 21x on a grid based implementation of 32 nodes.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 01:24:13 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Perera", "P.", ""], ["Ragel", "R. G.", ""]]}, {"id": "1403.1317", "submitter": "Roshan Ragel", "authors": "S.M. Vidanagamachchi, S.D. Dewasurendra and R.G. Ragel", "title": "Hardware software co-design of the Aho-Corasick algorithm: Scalable for\n  protein identification?", "comments": null, "journal-ref": "Industrial and Information Systems (ICIIS), 2013 8th IEEE\n  International Conference on, pp. 321-325, 17-20 Dec. 2013", "doi": "10.1109/ICIInfS.2013.6732003", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern matching is commonly required in many application areas and\nbioinformatics is a major area of interest that requires both exact and\napproximate pattern matching. Much work has been done in this area, yet there\nis still a significant space for improvement in efficiency, flexibility, and\nthroughput. This paper presents a hardware software co-design of Aho-Corasick\nalgorithm in Nios II soft-processor and a study on its scalability for a\npattern matching application. A software only approach is used to compare the\nthroughput and the scalability of the hardware software co-design approach.\nAccording to the results we obtained, we conclude that the hardware software\nco-design implementation shows a maximum of 10 times speed up for pattern size\nof 1200 peptides compared to the software only implementation. The results also\nshow that the hardware software co-design approach scales well for increasing\ndata size compared to the software only approach.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 01:42:41 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Vidanagamachchi", "S. M.", ""], ["Dewasurendra", "S. D.", ""], ["Ragel", "R. G.", ""]]}, {"id": "1403.1319", "submitter": "Roshan Ragel", "authors": "S.M. Vidanagamachchi, S.D. Dewasurendra and R.G. Ragel", "title": "Hardware accelerated protein inference framework", "comments": null, "journal-ref": "Industrial and Information Systems (ICIIS), 2013 8th IEEE\n  International Conference on, pp. 649-653, 17-20 Dec. 2013", "doi": "10.1109/ICIInfS.2013.6732061", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protein inference plays a vital role in the proteomics study. Two major\napproaches could be used to handle the problem of protein inference; top-down\nand bottom-up. This paper presents a framework for protein inference, which\nuses hardware accelerated protein inference framework for handling the most\nimportant step in a bottom-up approach, viz. peptide identification during the\nassembling process. In our framework, identified peptides and their\nprobabilities are used to predict the most suitable reference protein cluster\nfor a given input amino acid sequence with the probability of identified\npeptides. The framework is developed on an FPGA where hardware software\nco-design techniques are used to accelerate the computationally intensive parts\nof the protein inference process. In the paper we have measured, compared and\nreported the time taken for the protein inference process in our framework\nagainst a pure software implementation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 01:46:05 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Vidanagamachchi", "S. M.", ""], ["Dewasurendra", "S. D.", ""], ["Ragel", "R. G.", ""]]}, {"id": "1403.1336", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu", "title": "An Extensive Repot on the Efficiency of AIS-INMACA (A Novel Integrated\n  MACA based Clonal Classifier for Protein Coding and Promoter Region\n  Prediction)", "comments": "5 Pages, Review of Bioinformatics and Biometrics (RBB) Volume 3 Issue\n  1, March 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper exclusively reports the efficiency of AIS-INMACA. AIS-INMACA has\ncreated good impact on solving major problems in bioinformatics like protein\nregion identification and promoter region prediction with less time (Pokkuluri\nKiran Sree, 2014). This AIS-INMACA is now came with several variations\n(Pokkuluri Kiran Sree, 2014) towards projecting it as a tool in bioinformatics\nfor solving many problems in bioinformatics. So this paper will be very much\nuseful for so many researchers who are working in the domain of bioinformatics\nwith cellular automata.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 03:46:38 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""]]}, {"id": "1403.1347", "submitter": "Jian Zhou Zhou", "authors": "Jian Zhou and Olga G. Troyanskaya", "title": "Deep Supervised and Convolutional Generative Stochastic Network for\n  Protein Secondary Structure Prediction", "comments": "Accepted by ICML 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting protein secondary structure is a fundamental problem in protein\nstructure prediction. Here we present a new supervised generative stochastic\nnetwork (GSN) based method to predict local secondary structure with deep\nhierarchical representations. GSN is a recently proposed deep learning\ntechnique (Bengio & Thibodeau-Laufer, 2013) to globally train deep generative\nmodel. We present the supervised extension of GSN, which learns a Markov chain\nto sample from a conditional distribution, and applied it to protein structure\nprediction. To scale the model to full-sized, high-dimensional data, like\nprotein sequences with hundreds of amino acids, we introduce a convolutional\narchitecture, which allows efficient learning across multiple layers of\nhierarchical representations. Our architecture uniquely focuses on predicting\nstructured low-level labels informed with both low and high-level\nrepresentations learned by the model. In our application this corresponds to\nlabeling the secondary structure state of each amino-acid residue. We trained\nand tested the model on separate sets of non-homologous proteins sharing less\nthan 30% sequence identity. Our model achieves 66.4% Q8 accuracy on the CB513\ndataset, better than the previously reported best performance 64.9% (Wang et\nal., 2011) for this challenging secondary structure prediction problem.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 05:18:26 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Zhou", "Jian", ""], ["Troyanskaya", "Olga G.", ""]]}, {"id": "1403.1523", "submitter": "Changchuan Yin Dr.", "authors": "Changchuan Yin, Xuemeng E. Yin, Jiasong Wang", "title": "A Novel Method for Comparative Analysis of DNA Sequences by\n  Ramanujan-Fourier Transform", "comments": "Ramanujan-Fourier transform and DNA sequences", "journal-ref": null, "doi": "10.1089/cmb.2014.0120", "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alignment-free sequence analysis approaches provide important alternatives\nover multiple sequence alignment (MSA) in biological sequence analysis because\nalignment-free approaches have low computation complexity and are not dependent\non high level of sequence identity, however, most of the existing\nalignment-free methods do not employ true full information content of sequences\nand thus can not accurately reveal similarities and differences among DNA\nsequences. We present a novel alignment-free computational method for sequence\nanalysis based on Ramanujan-Fourier transform (RFT), in which complete\ninformation of DNA sequences is retained. We represent DNA sequences as four\nbinary indicator sequences and apply RFT on the indicator sequences to convert\nthem into frequency domain. The Euclidean distance of the complete RFT\ncoefficients of DNA sequences are used as similarity measure. To address the\ndifferent lengths in Euclidean space of RFT coefficients, we pad zeros to short\nDNA binary sequences so that the binary sequences equal the longest length in\nthe comparison sequence data. Thus, the DNA sequences are compared in the same\ndimensional frequency space without information loss. We demonstrate the\nusefulness of the proposed method by presenting experimental results on\nhierarchical clustering of genes and genomes. The proposed method opens a new\nchannel to biological sequence analysis, classification, and structural module\nidentification.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 18:29:52 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 18:35:05 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Yin", "Changchuan", ""], ["Yin", "Xuemeng E.", ""], ["Wang", "Jiasong", ""]]}, {"id": "1403.1949", "submitter": "Mehdi Naseriparsa", "authors": "Mehdi Naseriparsa, Mohammad Mansour Riahi Kashani", "title": "Combination of PCA with SMOTE Resampling to Boost the Prediction Rate in\n  Lung Cancer Dataset", "comments": "6 pages. arXiv admin note: text overlap with arXiv:1106.1813,\n  arXiv:1001.1446 by other authors", "journal-ref": "International Journal of Computer Applications,Vol 77,No 3,pp\n  33-38,2013", "doi": "10.5120/13376-0987", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification algorithms are unable to make reliable models on the datasets\nwith huge sizes. These datasets contain many irrelevant and redundant features\nthat mislead the classifiers. Furthermore, many huge datasets have imbalanced\nclass distribution which leads to bias over majority class in the\nclassification process. In this paper combination of unsupervised\ndimensionality reduction methods with resampling is proposed and the results\nare tested on Lung-Cancer dataset. In the first step PCA is applied on\nLung-Cancer dataset to compact the dataset and eliminate irrelevant features\nand in the second step SMOTE resampling is carried out to balance the class\ndistribution and increase the variety of sample domain. Finally, Naive Bayes\nclassifier is applied on the resulting dataset and the results are compared and\nevaluation metrics are calculated. The experiments show the effectiveness of\nthe proposed method across four evaluation metrics: Overall accuracy, False\nPositive Rate, Precision, Recall.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 08:12:54 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Naseriparsa", "Mehdi", ""], ["Kashani", "Mohammad Mansour Riahi", ""]]}, {"id": "1403.2000", "submitter": "Simon Kramer", "authors": "Simon Kramer", "title": "A Galois-Connection between Myers-Briggs' Type Indicators and Szondi's\n  Personality Profiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We propose a computable Galois-connection between Myers-Briggs' Type\nIndicators (MBTIs), the most widely-used personality measure for\nnon-psychiatric populations (based on C.G. Jung's personality types), and\nSzondi's personality profiles (SPPs), a less well-known but, as we show, finer\npersonality measure for psychiatric as well as non-psychiatric populations\n(conceived as a unification of the depth psychology of S. Freud, C.G. Jung, and\nA. Adler). The practical significance of our result is that our\nGalois-connection provides a pair of computable, interpreting translations\nbetween the two personality spaces of MBTIs and SPPs: one concrete from\nMBTI-space to SPP-space (because SPPs are finer) and one abstract from\nSPP-space to MBTI-space (because MBTIs are coarser). Thus Myers-Briggs' and\nSzondi's personality-test results are mutually interpretable and\ninter-translatable, even automatically by computers.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 19:25:36 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Kramer", "Simon", ""]]}, {"id": "1403.2002", "submitter": "Sadi Seker E", "authors": "Sadi Evren Seker, Cihan Mert, Khaled Al-Naami, Nuri Ozalp, Ugur Ayan", "title": "Time Series Analysis on Stock Market for Text Mining Correlation of\n  Economy News", "comments": "23 pages", "journal-ref": "International Journal of Social Sciences and Humanity Studies Vol\n  6, No 1, 2014 ISSN: 1309-8063 (Online)", "doi": null, "report-no": null, "categories": "cs.CE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an information retrieval method for the economy news. The\neffect of economy news, are researched in the word level and stock market\nvalues are considered as the ground proof. The correlation between stock market\nprices and economy news is an already addressed problem for most of the\ncountries. The most well-known approach is applying the text mining approaches\nto the news and some time series analysis techniques over stock market closing\nvalues in order to apply classification or clustering algorithms over the\nfeatures extracted. This study goes further and tries to ask the question what\nare the available time series analysis techniques for the stock market closing\nvalues and which one is the most suitable? In this study, the news and their\ndates are collected into a database and text mining is applied over the news,\nthe text mining part has been kept simple with only term frequency-inverse\ndocument frequency method. For the time series analysis part, we have studied\n10 different methods such as random walk, moving average, acceleration,\nBollinger band, price rate of change, periodic average, difference, momentum or\nrelative strength index and their variation. In this study we have also\nexplained these techniques in a comparative way and we have applied the methods\nover Turkish Stock Market closing values for more than a 2 year period. On the\nother hand, we have applied the term frequency-inverse document frequency\nmethod on the economy news of one of the high-circulating newspapers in Turkey.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 19:50:15 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Seker", "Sadi Evren", ""], ["Mert", "Cihan", ""], ["Al-Naami", "Khaled", ""], ["Ozalp", "Nuri", ""], ["Ayan", "Ugur", ""]]}, {"id": "1403.2654", "submitter": "Yanping Chen", "authors": "Yanping Chen, Adena Why, Gustavo Batista, Agenor Mafra-Neto, Eamonn\n  Keogh", "title": "Flying Insect Classification with Inexpensive Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The ability to use inexpensive, noninvasive sensors to accurately classify\nflying insects would have significant implications for entomological research,\nand allow for the development of many useful applications in vector control for\nboth medical and agricultural entomology. Given this, the last sixty years have\nseen many research efforts on this task. To date, however, none of this\nresearch has had a lasting impact. In this work, we explain this lack of\nprogress. We attribute the stagnation on this problem to several factors,\nincluding the use of acoustic sensing devices, the over-reliance on the single\nfeature of wingbeat frequency, and the attempts to learn complex models with\nrelatively little data. In contrast, we show that pseudo-acoustic optical\nsensors can produce vastly superior data, that we can exploit additional\nfeatures, both intrinsic and extrinsic to the insect's flight behavior, and\nthat a Bayesian classification approach allows us to efficiently learn\nclassification models that are very robust to over-fitting. We demonstrate our\nfindings with large scale experiments that dwarf all previous works combined,\nas measured by the number of insects and the number of species considered.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 18:36:39 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Chen", "Yanping", ""], ["Why", "Adena", ""], ["Batista", "Gustavo", ""], ["Mafra-Neto", "Agenor", ""], ["Keogh", "Eamonn", ""]]}, {"id": "1403.2740", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie, Hamid Abrishami Moghaddam", "title": "A consistent model for cardiac deformation estimation under abnormal\n  ventricular muscle conditions", "comments": "Published in the IFMBE Proceedings of World Congress on Medical\n  Physics and Biomedical Engineering, September 7 - 12, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformation modeling of cardiac muscle is an important issue in the field of\ncardiac analysis. For this reason, many approaches have been developed to best\nestimate the cardiac muscle deformation, and to obtain a practical model to use\nin diagnostic procedures. But there are some conditions, like in case of\nmyocardial infarction, in which the regular modeling approaches are not useful.\nIn this section, using a point-wise approach in deformation estimation, we try\nto estimate the deformation under some abnormal conditions of cardiac muscle.\nFirst, the endocardial and epicardial contour points are ordered with respect\nto the center of gravity of endocardial contour and boundary point displacement\nvectors are extracted. Then to solve the governing equation of deformation,\nwhich is an elliptic equation, we apply boundary conditions in accordance with\nthe computed displacement vectors and then the Finite Element method (FEM) will\nbe used to solve the governing equation. Using obtained displacement field\nthrough the cardiac muscle, strain map is extracted to show the mechanical\nbehavior of cardiac muscle. To validate the proposed algorithm in case of\ninfracted muscle, a non-homogeneous ring is modeled using ANSYS under a uniform\ntime varying internal pressure, which is the case in real cardiac muscle\ndeformation and then the proposed algorithm implemented in MATLAB and the\nresults for such problem are extracted.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 00:14:19 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2014 01:27:30 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Baghaie", "Ahmadreza", ""], ["Moghaddam", "Hamid Abrishami", ""]]}, {"id": "1403.2848", "submitter": "Suprativ Saha", "authors": "Ananya Bose and Suprativ Saha", "title": "Delineation of Techniques to implement on the enhanced proposed model\n  using data mining for protein sequence classification", "comments": "8 pages, 1 figures", "journal-ref": "International Journal of Database Management Systems ( IJDMS )\n  Vol.6, No.1, February 2014", "doi": "10.5121/ijdms.2014.6105", "report-no": null, "categories": "cs.DB cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In post genomic era with the advent of new technologies a huge amount of\ncomplex molecular data are generated with high throughput. The management of\nthis biological data is definitely a challenging task due to complexity and\nheterogeneity of data for discovering new knowledge. Issues like managing noisy\nand incomplete data are needed to be dealt with. Use of data mining in\nbiological domain has made its inventory success. Discovering new knowledge\nfrom the biological data is a major challenge in data mining technique. The\nnovelty of the proposed model is its combined use of intelligent techniques to\nclassify the protein sequence faster and efficiently. Use of FFT, fuzzy\nclassifier, String weighted algorithm, gram encoding method, neural network\nmodel and rough set classifier in a single model and in an appropriate place\ncan enhance the quality of the classification system.Thus the primary challenge\nis to identify and classify the large protein sequences in a very fast and easy\nbut intellectual way to decrease the time complexity and space complexity.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 08:44:08 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Bose", "Ananya", ""], ["Saha", "Suprativ", ""]]}, {"id": "1403.3060", "submitter": "Chanabasayya Vastrad M", "authors": "Doreswamy, Chanabasayya M. Vastrad", "title": "Non linear Prediction of Antitubercular Activity Of Oxazolines and\n  Oxazoles derivatives Making Use of Compact TS-Fuzzy models Through Clustering\n  with orthogonal least sqaure technique and Fuzzy identification system", "comments": "Published 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The prediction of uncertain and predictive nonlinear systems is an important\nand challenging problem. Fuzzy logic models are often a good choice to describe\nsuch systems however in many cases these become complex soon. commonlly, too\nless effort is put into descriptor selection and in the creation of suitable\nlocal rules. Moreover, in common no model reduction is applied, while this may\nanalyze the model by removing redundant data. This paper suggests a combined\nmethod that deal with these issues in order to create compact Takagi Sugeno\n(TS) models that can be effectively used to represent complex predictive\nsystems. A new fuzzy clustering method is come up with for the identification\nof compact TS-fuzzy models. The best relevant consequent variables of the TS\nmodel are choosen by an orthogonal least squares technique based on the\nobtained clusters.For the selection of the relevant antecedent (scheduling)\nvariables a new method has been developed based on Fisher's interclass\nseparability basis. This complete approach is demonstrated by means of the\nOxazolines and Oxazoles derivatives as antituberculosis agent for nonlinear\nregression benchmark. The results are compared with results obtained by\nneuro-fuzzy i.e. ANFIS algorithm and advanced fuzzyy clustering techniques i.e\nFMID toolbox .\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 02:40:40 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Doreswamy", "", ""], ["Vastrad", "Chanabasayya M.", ""]]}, {"id": "1403.3251", "submitter": "Matthias Markl", "authors": "Matthias Markl and Regina Ammer and Ulrich R\\\"ude and Carolin K\\\"orner", "title": "Numerical Investigations on Hatching Process Strategies for Powder Bed\n  Based Additive Manufacturing using an Electron Beam", "comments": null, "journal-ref": "The International Journal of Advanced Manufacturing Technology:\n  Volume 78, Issue 1 (2015), Page 239-247", "doi": "10.1007/s00170-014-6594-9", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates in hatching process strategies for additive\nmanufacturing using an electron beam by numerical simulations. The underlying\nphysical model and the corresponding three dimensional thermal free surface\nlattice Boltzmann method of the simulation software are briefly presented. The\nsimulation software has already been validated on the basis of experiments up\nto 1.2 kW beam power by hatching a cuboid with a basic process strategy,\nwhereby the results are classified into `porous', `good' and `uneven',\ndepending on their relative density and top surface smoothness. In this paper\nwe study the limitations of this basic process strategy in terms of higher beam\npowers and scan velocities to exploit the future potential of high power\nelectron beam guns up to 10 kW. Subsequently, we introduce modified process\nstrategies, which circumvent these restrictions, to build the part as fast as\npossible under the restriction of a fully dense part with a smooth top surface.\nThese process strategies are suitable to reduce the build time and costs,\nmaximize the beam power usage and therefore use the potential of high power\nelectron beam guns.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 12:53:12 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 08:39:22 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Markl", "Matthias", ""], ["Ammer", "Regina", ""], ["R\u00fcde", "Ulrich", ""], ["K\u00f6rner", "Carolin", ""]]}, {"id": "1403.3495", "submitter": "Shuliang Wang", "authors": "Shuliang Wang, Yiping Zhao", "title": "Analyzing Large Biological Datasets with an Improved Algorithm for MIC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computational framework utilizes the traditional similarity measures for\nmining the significant relationships in biological annotations is recently\nproposed by Tatiana V. Karpinets et al. [2]. In this paper, an improved\napproximation algorithm for MIC (maximal information coefficient) named IAMIC\nis suggested to perfect this framework for discovering the hidden regularities\nbetween biological annotations. Further, IAMIC is the enhanced algorithm for\napproximating a novel similarity coefficient MIC with generality and\nequitability, which makes it more appropriate for data exploration. Here it is\nshown that IAMIC is also applicable for identify the associations between\nbiological annotations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 07:26:33 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Wang", "Shuliang", ""], ["Zhao", "Yiping", ""]]}, {"id": "1403.3724", "submitter": "William Gray Roncal", "authors": "William Gray Roncal, Michael Pekala, Verena Kaynig-Fittkau, Dean M.\n  Kleissas, Joshua T. Vogelstein, Hanspeter Pfister, Randal Burns, R. Jacob\n  Vogelstein, Mark A. Chevillet, Gregory D. Hager", "title": "VESICLE: Volumetric Evaluation of Synaptic Interfaces using Computer\n  vision at Large Scale", "comments": "v4: added clarifying figures and updates for readability. v3: fixed\n  metadata. 11 pp v2: Added CNN classifier, significant changes to improve\n  performance and generalization", "journal-ref": "Proceedings of the British Machine Vision Conference (BMVC), pages\n  81.1-81.13. BMVA Press, September 2015", "doi": null, "report-no": null, "categories": "cs.CV cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An open challenge problem at the forefront of modern neuroscience is to\nobtain a comprehensive mapping of the neural pathways that underlie human brain\nfunction; an enhanced understanding of the wiring diagram of the brain promises\nto lead to new breakthroughs in diagnosing and treating neurological disorders.\nInferring brain structure from image data, such as that obtained via electron\nmicroscopy (EM), entails solving the problem of identifying biological\nstructures in large data volumes. Synapses, which are a key communication\nstructure in the brain, are particularly difficult to detect due to their small\nsize and limited contrast. Prior work in automated synapse detection has relied\nupon time-intensive biological preparations (post-staining, isotropic slice\nthicknesses) in order to simplify the problem.\n  This paper presents VESICLE, the first known approach designed for mammalian\nsynapse detection in anisotropic, non-post-stained data. Our methods explicitly\nleverage biological context, and the results exceed existing synapse detection\nmethods in terms of accuracy and scalability. We provide two different\napproaches - one a deep learning classifier (VESICLE-CNN) and one a lightweight\nRandom Forest approach (VESICLE-RF) to offer alternatives in the\nperformance-scalability space. Addressing this synapse detection challenge\nenables the analysis of high-throughput imaging data soon expected to reach\npetabytes of data, and provide tools for more rapid estimation of brain-graphs.\nFinally, to facilitate community efforts, we developed tools for large-scale\nobject detection, and demonstrated this framework to find $\\approx$ 50,000\nsynapses in 60,000 $\\mu m ^3$ (220 GB on disk) of electron microscopy data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 23:16:36 GMT"}, {"version": "v2", "created": "Wed, 13 May 2015 16:53:05 GMT"}, {"version": "v3", "created": "Thu, 14 May 2015 01:01:16 GMT"}, {"version": "v4", "created": "Mon, 7 Sep 2015 21:41:20 GMT"}], "update_date": "2015-09-09", "authors_parsed": [["Roncal", "William Gray", ""], ["Pekala", "Michael", ""], ["Kaynig-Fittkau", "Verena", ""], ["Kleissas", "Dean M.", ""], ["Vogelstein", "Joshua T.", ""], ["Pfister", "Hanspeter", ""], ["Burns", "Randal", ""], ["Vogelstein", "R. Jacob", ""], ["Chevillet", "Mark A.", ""], ["Hager", "Gregory D.", ""]]}, {"id": "1403.4508", "submitter": "Cornelia Victoria Anghel Drugarin", "authors": "Lenuta Suciu, Florentina Cziple, Cornelia Anghel", "title": "Research on Study Mechanical Vibrations with Data Acquisition Systems", "comments": "AGIR Bucuresti Press, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a new study method of mechanic vibrations with the help of\nthe data acquisition systems. The study of vibrations with the help of data\nacquisition systems allows the solving of some engineering problems connected\nto the measurement of some parameters which are difficult to measure having in\nview the improvement of the technical performances of the industrial equipment\nor devices\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 13:06:20 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Suciu", "Lenuta", ""], ["Cziple", "Florentina", ""], ["Anghel", "Cornelia", ""]]}, {"id": "1403.4871", "submitter": "Mark Shackelford", "authors": "Mark Shackelford", "title": "Evolutionary Algorithm for Drug Discovery Interim Design Report", "comments": "7 Pages, Interim Design Document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A software program which aims to provide an exploration capability over the\nSearch Space of potential drug molecules. The program explores the search space\nby generating random molecules, determining their fitness and then breeding a\nnew generation from the fittest individuals. The search space, in theory any\ncombination of any elements in any order, is constrained by the use of a subset\nof elements and a list of fragments, molecular parts that are known to be\nuseful in drug development. The resultant molecules from each generation are\nstored in a searchable database, so that the user can browse through previous\ngenerations looking for interesting molecules.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 16:24:13 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Shackelford", "Mark", ""]]}, {"id": "1403.4881", "submitter": "Martin Treiber", "authors": "Martin Treiber and Venkatesan Kanagaraj", "title": "Comparing Numerical Integration Schemes for Time-Continuous\n  Car-Following Models", "comments": "Submitted to Transportation Research Part B: Methodological", "journal-ref": "Physica A: Statistical Mechanics and its Applications 419C,\n  pp.183-195 (2015)", "doi": "10.1016/j.physa.2014.09.061", "report-no": null, "categories": "cs.CE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When simulating trajectories by integrating time-continuous car-following\nmodels, standard integration schemes such as the forth-order Runge-Kutta method\n(RK4) are rarely used while the simple Euler's method is popular among\nresearchers. We compare four explicit methods: Euler's method, ballistic\nupdate, Heun's method (trapezoidal rule), and the standard forth-order RK4. As\nperformance metrics, we plot the global discretization error as a function of\nthe numerical complexity. We tested the methods on several time-continuous\ncar-following models in several multi-vehicle simulation scenarios with and\nwithout discontinuities such as stops or a discontinuous behavior of an\nexternal leader. We find that the theoretical advantage of RK4 (consistency\norder~4) only plays a role if both the acceleration function of the model and\nthe external data of the simulation scenario are sufficiently often\ndifferentiable. Otherwise, we obtain lower (and often fractional) consistency\norders. Although, to our knowledge, Heun's method has never been used for\nintegrating car-following models, it turns out to be the best scheme for many\npractical situations. The ballistic update always prevails Euler's method\nalthough both are of first order.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 16:54:23 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Treiber", "Martin", ""], ["Kanagaraj", "Venkatesan", ""]]}, {"id": "1403.5029", "submitter": "Wei Zhang", "authors": "Wei Zhang, Jae-Woong Chang, Lilong Lin, Kay Minn, Baolin Wu, Jeremy\n  Chien, Jeongsik Yong, Hui Zheng, Rui Kuang", "title": "Network-based Isoform Quantification with RNA-Seq Data for Cancer\n  Transcriptome Analysis", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1004465", "report-no": null, "categories": "cs.CE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  High-throughput mRNA sequencing (RNA-Seq) is widely used for transcript\nquantification of gene isoforms. Since RNA-Seq data alone is often not\nsufficient to accurately identify the read origins from the isoforms for\nquantification, we propose to explore protein domain-domain interactions as\nprior knowledge for integrative analysis with RNA-seq data. We introduce a\nNetwork-based method for RNA-Seq-based Transcript Quantification (Net-RSTQ) to\nintegrate protein domain-domain interaction network with short read alignments\nfor transcript abundance estimation. Based on our observation that the\nabundances of the neighboring isoforms by domain-domain interactions in the\nnetwork are positively correlated, Net-RSTQ models the expression of the\nneighboring transcripts as Dirichlet priors on the likelihood of the observed\nread alignments against the transcripts in one gene. The transcript abundances\nof all the genes are then jointly estimated with alternating optimization of\nmultiple EM problems. In simulation Net-RSTQ effectively improved isoform\ntranscript quantifications when isoform co-expressions correlate with their\ninteractions. qRT-PCR results on 25 multi-isoform genes in a stem cell line, an\novarian cancer cell line, and a breast cancer cell line also showed that\nNet-RSTQ estimated more consistent isoform proportions with RNA-Seq data. In\nthe experiments on the RNA-Seq data in The Cancer Genome Atlas (TCGA), the\ntranscript abundances estimated by Net-RSTQ are more informative for patient\nsample classification of ovarian cancer, breast cancer and lung cancer. All\nexperimental results collectively support that Net-RSTQ is a promising approach\nfor isoform quantification.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 02:35:15 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2015 16:06:01 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2015 15:48:46 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Zhang", "Wei", ""], ["Chang", "Jae-Woong", ""], ["Lin", "Lilong", ""], ["Minn", "Kay", ""], ["Wu", "Baolin", ""], ["Chien", "Jeremy", ""], ["Yong", "Jeongsik", ""], ["Zheng", "Hui", ""], ["Kuang", "Rui", ""]]}, {"id": "1403.5686", "submitter": "Haris Vikalo", "authors": "Xiaohu Shen, Manohar Shamaiah, and Haris Vikalo", "title": "Iterative Learning for Reference-Guided DNA Sequence Assembly from Short\n  Reads: Algorithms and Limits of Performance", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2014.2333564", "report-no": null, "categories": "q-bio.GN cs.CE cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent emergence of next-generation DNA sequencing technology has enabled\nacquisition of genetic information at unprecedented scales. In order to\ndetermine the genetic blueprint of an organism, sequencing platforms typically\nemploy so-called shotgun sequencing strategy to oversample the target genome\nwith a library of relatively short overlapping reads. The order of nucleotides\nin the reads is determined by processing the acquired noisy signals generated\nby the sequencing instrument. Assembly of a genome from potentially erroneous\nshort reads is a computationally daunting task even in the scenario where a\nreference genome exists. Errors and gaps in the reference, and perfect repeat\nregions in the target, further render the assembly challenging and cause\ninaccuracies. In this paper, we formulate the reference-guided sequence\nassembly problem as the inference of the genome sequence on a bipartite graph\nand solve it using a message-passing algorithm. The proposed algorithm can be\ninterpreted as the well-known classical belief propagation scheme under a\ncertain prior. Unlike existing state-of-the-art methods, the proposed algorithm\ncombines the information provided by the reads without needing to know\nreliability of the short reads (so-called quality scores). Relation of the\nmessage-passing algorithm to a provably convergent power iteration scheme is\ndiscussed. To evaluate and benchmark the performance of the proposed technique,\nwe find an analytical expression for the probability of error of a genie-aided\nmaximum a posteriori (MAP) decision scheme. Results on both simulated and\nexperimental data demonstrate that the proposed message-passing algorithm\noutperforms commonly used state-of-the-art tools, and it nearly achieves the\nperformance of the aforementioned MAP decision scheme.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 16:59:53 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Shen", "Xiaohu", ""], ["Shamaiah", "Manohar", ""], ["Vikalo", "Haris", ""]]}, {"id": "1403.5933", "submitter": "Kiran Sree Pokkuluri Prof", "authors": "Pokkuluri Kiran Sree, Inampudi Ramesh Babu", "title": "AIS-INMACA: A Novel Integrated MACA Based Clonal Classifier for Protein\n  Coding and Promoter Region Prediction", "comments": "7 Pages", "journal-ref": "Journal of Bioinformatics and Comparative Genomics,2014", "doi": null, "report-no": "Pokkuluri Kiran Sree, et al. (2014) AIS-INMACA: A Novel Integrated\n  MACA Based Clonal Classifier for Protein Coding and Promoter Region\n  Prediction. J Bioinfo Comp Genom 1: 1-7", "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the problems in bioinformatics are now the challenges in computing.\nThis paper aims at building a classifier based on Multiple Attractor Cellular\nAutomata (MACA) which uses fuzzy logic. It is strengthened with an artificial\nImmune System Technique (AIS), Clonal algorithm for identifying a protein\ncoding and promoter region in a given DNA sequence. The proposed classifier is\nnamed as AIS-INMACA introduces a novel concept to combine CA with artificial\nimmune system to produce a better classifier which can address major problems\nin bioinformatics. This will be the first integrated algorithm which can\npredict both promoter and protein coding regions. To obtain good fitness rules\nthe basic concept of Clonal selection algorithm was used. The proposed\nclassifier can handle DNA sequences of lengths 54,108,162,252,354. This\nclassifier gives the exact boundaries of both protein and promoter regions with\nan average accuracy of 89.6%. This classifier was tested with 97,000 data\ncomponents which were taken from Fickett & Toung, MPromDb, and other sequences\nfrom a renowned medical university. This proposed classifier can handle huge\ndata sets and can find protein and promoter regions even in mixed and\noverlapped DNA sequences. This work also aims at identifying the logicality\nbetween the major problems in bioinformatics and tries to obtaining a common\nframe work for addressing major problems in bioinformatics like protein\nstructure prediction, RNA structure prediction, predicting the splicing pattern\nof any primary transcript and analysis of information content in DNA, RNA,\nprotein sequences and structure. This work will attract more researchers\ntowards application of CA as a potential pattern classifier to many important\nproblems in bioinformatics\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 12:37:11 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Sree", "Pokkuluri Kiran", ""], ["Babu", "Inampudi Ramesh", ""]]}, {"id": "1403.6025", "submitter": "Ruven Pillay", "authors": "Emmanuel Bertin, Ruven Pillay, Chiara Marmo", "title": "Web-Based Visualization of Very Large Scientific Astronomy Imagery", "comments": "Published in Astronomy & Computing. IIPImage server available from\n  http://iipimage.sourceforge.net . Visiomatic code and demos available from\n  http://www.visiomatic.org/", "journal-ref": "Astronomy and Computing, vol. 10, pp. 43-53, Apr. 2015", "doi": "10.1016/j.ascom.2014.12.006", "report-no": null, "categories": "astro-ph.IM cs.CE cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing and navigating through large astronomy images from a remote\nlocation with current astronomy display tools can be a frustrating experience\nin terms of speed and ergonomics, especially on mobile devices. In this paper,\nwe present a high performance, versatile and robust client-server system for\nremote visualization and analysis of extremely large scientific images.\nApplications of this work include survey image quality control, interactive\ndata query and exploration, citizen science, as well as public outreach. The\nproposed software is entirely open source and is designed to be generic and\napplicable to a variety of datasets. It provides access to floating point data\nat terabyte scales, with the ability to precisely adjust image settings in\nreal-time. The proposed clients are light-weight, platform-independent web\napplications built on standard HTML5 web technologies and compatible with both\ntouch and mouse-based devices. We put the system to the test and assess the\nperformance of the system and show that a single server can comfortably handle\nmore than a hundred simultaneous users accessing full precision 32 bit\nastronomy data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 16:24:57 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 14:21:56 GMT"}, {"version": "v3", "created": "Sun, 1 Feb 2015 22:29:40 GMT"}, {"version": "v4", "created": "Thu, 5 Feb 2015 10:40:31 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Bertin", "Emmanuel", ""], ["Pillay", "Ruven", ""], ["Marmo", "Chiara", ""]]}, {"id": "1403.6048", "submitter": "Simon Kramer", "authors": "Simon Kramer", "title": "Computer-Aided Discovery and Categorisation of Personality Axioms", "comments": "related to arXiv:1403.2000", "journal-ref": "IfCoLog Journal of Logics and their Applications, 1(2), 2014,\n  Pages 107-133", "doi": null, "report-no": null, "categories": "cs.CE cs.CY cs.LO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We propose a computer-algebraic, order-theoretic framework based on\nintuitionistic logic for the computer-aided discovery of personality axioms\nfrom personality-test data and their mathematical categorisation into formal\npersonality theories in the spirit of F.~Klein's Erlanger Programm for\ngeometrical theories. As a result, formal personality theories can be\nautomatically generated, diagrammatically visualised, and mathematically\ncharacterised in terms of categories of invariant-preserving transformations in\nthe sense of Klein and category theory. Our personality theories and categories\nare induced by implicational invariants that are ground instances of\nintuitionistic implication, which we postulate as axioms. In our mindset, the\nessence of personality, and thus mental health and illness, is its invariance.\nThe truth of these axioms is algorithmically extracted from histories of\npartially-ordered, symbolic data of observed behaviour. The personality-test\ndata and the personality theories are related by a Galois-connection in our\nframework. As data format, we adopt the format of the symbolic values generated\nby the Szondi-test, a personality test based on L.~Szondi's unifying,\ndepth-psychological theory of fate analysis.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 17:27:59 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Kramer", "Simon", ""]]}, {"id": "1403.6167", "submitter": "Utkarsh R. Patel", "authors": "Utkarsh R. Patel and Piero Triverio", "title": "MoM-SO: a Complete Method for Computing the Impedance of Cable Systems\n  Including Skin, Proximity, and Ground Return Effects", "comments": "This paper has now been published in the IEEE Trans. on Power\n  Delivery in Oct. 2015, vol. 30, no. 5, pp. 2110-2118. DOI:\n  10.1109/TPWRD.2014.2378594", "journal-ref": "IEEE Trans. on Power Delivery in Oct. 2015, vol. 30, no. 5, pp.\n  2110-2118", "doi": "10.1109/TPWRD.2014.2378594", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of accurate and broadband models for underground and\nsubmarine cable systems is of paramount importance for the correct prediction\nof electromagnetic transients in power grids. Recently, we proposed the MoM-SO\nmethod for extracting the series impedance of power cables while accounting for\nskin and proximity effect in the conductors. In this paper, we extend the\nmethod to include ground return effects and to handle cables placed inside a\ntunnel. Numerical tests show that the proposed method is more accurate than\nwidely-used analytic formulas, and is much faster than existing proximity-aware\napproaches like finite elements. For a three-phase cable system in a tunnel,\nthe proposed method requires only 0.3 seconds of CPU time per frequency point,\nagainst the 8.3 minutes taken by finite elements, for a speed up beyond 1000 X.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 22:03:07 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 18:26:42 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 15:51:35 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Patel", "Utkarsh R.", ""], ["Triverio", "Piero", ""]]}, {"id": "1403.6358", "submitter": "Ariful Azad", "authors": "Ariful Azad, Bartek Rajwa, Alex Pothen", "title": "Immunophenotypes of Acute Myeloid Leukemia From Flow Cytometry Data\n  Using Templates", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Motivation: We investigate whether a template-based classification pipeline\ncould be used to identify immunophenotypes in (and thereby classify) a\nheterogeneous disease with many subtypes. The disease we consider here is Acute\nMyeloid Leukemia, which is heterogeneous at the morphologic, cytogenetic and\nmolecular levels, with several known subtypes. The prognosis and treatment for\nAML depends on the subtype.\n  Results: We apply flowMatch, an algorithmic pipeline for flow cytometry data\ncreated in earlier work, to compute templates succinctly summarizing classes of\nAML and healthy samples. We develop a scoring function that accounts for\nfeatures of the AML data such as heterogeneity to identify immunophenotypes\ncorresponding to various AML subtypes, including APL. All of the AML samples in\nthe test set are classified correctly with high confidence.\n  Availability: flowMatch is available at\nwww.bioconductor.org/packages/devel/bioc/html/flowMatch.html; programs specific\nto immunophenotyping AML are at www.cs.purdue.edu/homes/aazad/software.html.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 02:23:28 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Azad", "Ariful", ""], ["Rajwa", "Bartek", ""], ["Pothen", "Alex", ""]]}, {"id": "1403.6426", "submitter": "Elmar Peise", "authors": "Elmar Peise (1), Diego Fabregat-Traver (1), Paolo Bientinesi (1) ((1)\n  AICES, RWTH Aachen)", "title": "High Performance Solutions for Big-data GWAS", "comments": "Submitted to Parallel Computing. arXiv admin note: substantial text\n  overlap with arXiv:1304.2272", "journal-ref": null, "doi": null, "report-no": "AICES-2013/12-01", "categories": "q-bio.GN cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to associate complex traits with genetic polymorphisms, genome-wide\nassociation studies process huge datasets involving tens of thousands of\nindividuals genotyped for millions of polymorphisms. When handling these\ndatasets, which exceed the main memory of contemporary computers, one faces two\ndistinct challenges: 1) Millions of polymorphisms and thousands of phenotypes\ncome at the cost of hundreds of gigabytes of data, which can only be kept in\nsecondary storage; 2) the relatedness of the test population is represented by\na relationship matrix, which, for large populations, can only fit in the\ncombined main memory of a distributed architecture. In this paper, by using\ndistributed resources such as Cloud or clusters, we address both challenges:\nThe genotype and phenotype data is streamed from secondary storage using a\ndouble buffer- ing technique, while the relationship matrix is kept across the\nmain memory of a distributed memory system. With the help of these solutions,\nwe develop separate algorithms for studies involving only one or a multitude of\ntraits. We show that these algorithms sustain high-performance and allow the\nanalysis of enormous datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 17:21:55 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Peise", "Elmar", ""], ["Fabregat-Traver", "Diego", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1403.6676", "submitter": "Christian Decker", "authors": "Christian Decker and Roger Wattenhofer", "title": "Bitcoin Transaction Malleability and MtGox", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-11212-1_18", "report-no": null, "categories": "cs.CR cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bitcoin, transaction malleability describes the fact that the signatures\nthat prove the ownership of bitcoins being transferred in a transaction do not\nprovide any integrity guarantee for the signatures themselves. This allows an\nattacker to mount a malleability attack in which it intercepts, modifies, and\nrebroadcasts a transaction, causing the transaction issuer to believe that the\noriginal transaction was not confirmed. In February 2014 MtGox, once the\nlargest Bitcoin exchange, closed and filed for bankruptcy claiming that\nattackers used malleability attacks to drain its accounts. In this work we use\ntraces of the Bitcoin network for over a year preceding the filing to show\nthat, while the problem is real, there was no widespread use of malleability\nattacks before the closure of MtGox.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 14:01:13 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Decker", "Christian", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "1403.7137", "submitter": "Ahmed Attia", "authors": "Ahmed Attia, Adrian Sandu", "title": "A Sampling Filter for Non-Gaussian Data Assimilation", "comments": "52 pages, 24 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": "CSTR-4/2014", "categories": "cs.CE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation combines information from models, measurements, and priors\nto estimate the state of a dynamical system such as the atmosphere. The\nEnsemble Kalman filter (EnKF) is a family of ensemble-based data assimilation\napproaches that has gained wide popularity due its simple formulation, ease of\nimplementation, and good practical results. Most EnKF algorithms assume that\nthe underlying probability distributions are Gaussian. Although this assumption\nis well accepted, it is too restrictive when applied to large nonlinear models,\nnonlinear observation operators, and large levels of uncertainty. Several\napproaches have been proposed in order to avoid the Gaussianity assumption. One\nof the most successful strategies is the maximum likelihood ensemble filter\n(MLEF) which computes a maximum a posteriori estimate of the state assuming the\nposterior distribution is Gaussian. MLEF is designed to work with nonlinear and\neven non-differentiable observation operators, and shows good practical\nperformance. However, there are limits to the degree of nonlinearity that MLEF\ncan handle. This paper proposes a new ensemble-based data assimilation method,\nnamed the \"sampling filter\", which obtains the analysis by sampling directly\nfrom the posterior distribution. The sampling strategy is based on a Hybrid\nMonte Carlo (HMC) approach that can handle non-Gaussian probability\ndistributions. Numerical experiments are carried out using the Lorenz-96 model\nand observation operators with different levels of non-linearity and\ndifferentiability. The proposed filter is also tested with shallow water model\non a sphere with linear observation operator. The results show that the\nsampling filter can perform well even in highly nonlinear situations were EnKF\nand MLEF filters diverge.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 17:21:08 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 22:06:20 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Attia", "Ahmed", ""], ["Sandu", "Adrian", ""]]}, {"id": "1403.7209", "submitter": "Istv\\'an Z Reguly", "authors": "Istv\\'an Z. Reguly and Gihan R. Mudalige and Carlo Bertolli and\n  Michael B. Giles and Adam Betts and Paul H. J. Kelly and David Radford", "title": "Acceleration of a Full-scale Industrial CFD Application with OP2", "comments": "Submitted to ACM Transactions on Parallel Computing", "journal-ref": "IEEE Transactions on Parallel and Distributed Systems, vol. 27,\n  no. 5, pp. 1265-1278, May 1 2016. doi: 10.1109/TPDS.2015.2453972", "doi": "10.1109/TPDS.2015.2453972", "report-no": null, "categories": "cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hydra is a full-scale industrial CFD application used for the design of\nturbomachinery at Rolls Royce plc. It consists of over 300 parallel loops with\na code base exceeding 50K lines and is capable of performing complex\nsimulations over highly detailed unstructured mesh geometries. Unlike simpler\nstructured-mesh applications, which feature high speed-ups when accelerated by\nmodern processor architectures, such as multi-core and many-core processor\nsystems, Hydra presents major challenges in data organization and movement that\nneed to be overcome for continued high performance on emerging platforms. We\npresent research in achieving this goal through the OP2 domain-specific\nhigh-level framework. OP2 targets the domain of unstructured mesh problems and\nfollows the design of an active library using source-to-source translation and\ncompilation to generate multiple parallel implementations from a single\nhigh-level application source for execution on a range of back-end hardware\nplatforms. We chart the conversion of Hydra from its original hand-tuned\nproduction version to one that utilizes OP2, and map out the key difficulties\nencountered in the process. To our knowledge this research presents the first\napplication of such a high-level framework to a full scale production code.\nSpecifically we show (1) how different parallel implementations can be achieved\nwith an active library framework, even for a highly complicated industrial\napplication such as Hydra, and (2) how different optimizations targeting\ncontrasting parallel architectures can be applied to the whole application,\nseamlessly, reducing developer effort and increasing code longevity.\nPerformance results demonstrate that not only the same runtime performance as\nthat of the hand-tuned original production code could be achieved, but it can\nbe significantly improved on conventional processor systems. Additionally, we\nachieve further...\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 20:14:24 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Reguly", "Istv\u00e1n Z.", ""], ["Mudalige", "Gihan R.", ""], ["Bertolli", "Carlo", ""], ["Giles", "Michael B.", ""], ["Betts", "Adam", ""], ["Kelly", "Paul H. J.", ""], ["Radford", "David", ""]]}, {"id": "1403.7296", "submitter": "Roshan Ragel", "authors": "S. M. Vidanagamachchi, S.D. Dewasurendra, R. G. Ragel and M. Niranjan", "title": "Tile optimization for area in FPGA based hardware acceleration of\n  peptide identification", "comments": null, "journal-ref": "Industrial and Information Systems (ICIIS), 2011 6th IEEE\n  International Conference on, 16-19 Aug. 2011, pp. 140 - 145, Kandy", "doi": "10.1109/ICIINFS.2011.6038056", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in life sciences over the last few decades have lead to the\ngeneration of a huge amount of biological data. Computing research has become a\nvital part in driving biological discovery where analysis and categorization of\nbiological data are involved. String matching algorithms can be applied for\nprotein/gene sequence matching and with the phenomenal increase in the size of\nstring databases to be analyzed, software implementations of these algorithms\nseems to have hit a hard limit and hardware acceleration is increasingly being\nsought. Several hardware platforms such as Field Programmable Gate Arrays\n(FPGA), Graphics Processing Units (GPU) and Chip Multi Processors (CMP) are\nbeing explored as hardware platforms. In this paper, we give a comprehensive\noverview of the literature on hardware acceleration of string matching\nalgorithms, we take an FPGA hardware exploration and expedite the design time\nby a design automation technique. Further, our design automation is also\noptimized for better hardware utilization through optimizing the number of\npeptides that can be represented in an FPGA tile. The results indicate\nsignificant improvements in design time and hardware utilization which are\nreported in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 08:04:43 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Vidanagamachchi", "S. M.", ""], ["Dewasurendra", "S. D.", ""], ["Ragel", "R. G.", ""], ["Niranjan", "M.", ""]]}, {"id": "1403.7481", "submitter": "Sebastian Deorowicz", "authors": "Agnieszka Danek, Sebastian Deorowicz, Szymon Grabowski", "title": "Indexing large genome collections on a PC", "comments": null, "journal-ref": "PLOS ONE, Article no.0109384 (2014)", "doi": "10.1371/journal.pone.0109384", "report-no": null, "categories": "cs.CE q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The availability of thousands of invidual genomes of one species\nshould boost rapid progress in personalized medicine or understanding of the\ninteraction between genotype and phenotype, to name a few applications. A key\noperation useful in such analyses is aligning sequencing reads against a\ncollection of genomes, which is costly with the use of existing algorithms due\nto their large memory requirements.\n  Results: We present MuGI, Multiple Genome Index, which reports all\noccurrences of a given pattern, in exact and approximate matching model,\nagainst a collection of thousand(s) genomes. Its unique feature is the small\nindex size fitting in a standard computer with 16--32\\,GB, or even 8\\,GB, of\nRAM, for the 1000GP collection of 1092 diploid human genomes. The solution is\nalso fast. For example, the exact matching queries are handled in average time\nof 39\\,$\\mu$s and with up to 3 mismatches in 373\\,$\\mu$s on the test PC with\nthe index size of 13.4\\,GB. For a smaller index, occupying 7.4\\,GB in memory,\nthe respective times grow to 76\\,$\\mu$s and 917\\,$\\mu$s.\n  Availability: Software and Suuplementary material:\n\\url{http://sun.aei.polsl.pl/mugi}.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 18:41:39 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Danek", "Agnieszka", ""], ["Deorowicz", "Sebastian", ""], ["Grabowski", "Szymon", ""]]}, {"id": "1403.7948", "submitter": "Marc Demange", "authors": "Ferhat Alkan, T\\\"urker B{\\i}y{\\i}ko\\u{g}lu, Marc Demange and Cesim\n  Erten", "title": "Structure of conflict graphs in constraint alignment problems and\n  algorithms", "comments": "22 pages, 6 figures", "journal-ref": "Discrete Mathematics & Theoretical Computer Science, vol. 21 no.\n  4, Discrete Algorithms (September 11, 2019) dmtcs:5755", "doi": "10.23638/DMTCS-21-4-10", "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the constrained graph alignment problem which has applications in\nbiological network analysis. Given two input graphs $G_1=(V_1,E_1),\nG_2=(V_2,E_2)$, a pair of vertex mappings induces an {\\it edge conservation} if\nthe vertex pairs are adjacent in their respective graphs. %In general terms The\ngoal is to provide a one-to-one mapping between the vertices of the input\ngraphs in order to maximize edge conservation. However the allowed mappings are\nrestricted since each vertex from $V_1$ (resp. $V_2$) is allowed to be mapped\nto at most $m_1$ (resp. $m_2$) specified vertices in $V_2$ (resp. $V_1$). Most\nof results in this paper deal with the case $m_2=1$ which attracted most\nattention in the related literature. We formulate the problem as a maximum\nindependent set problem in a related {\\em conflict graph} and investigate\nstructural properties of this graph in terms of forbidden subgraphs. We are\ninterested, in particular, in excluding certain wheals, fans, cliques or claws\n(all terms are defined in the paper), which corresponds in excluding certain\ncycles, paths, cliques or independent sets in the neighborhood of each vertex.\nThen, we investigate algorithmic consequences of some of these properties,\nwhich illustrates the potential of this approach and raises new horizons for\nfurther works. In particular this approach allows us to reinterpret a known\npolynomial case in terms of conflict graph and to improve known approximation\nand fixed-parameter tractability results through efficiently solving the\nmaximum independent set problem in conflict graphs. Some of our new\napproximation results involve approximation ratios that are function of the\noptimal value, in particular its square root; this kind of results cannot be\nachieved for maximum independent set in general graphs.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 11:18:32 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 17:23:39 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 03:55:36 GMT"}, {"version": "v4", "created": "Wed, 10 Jul 2019 13:10:50 GMT"}, {"version": "v5", "created": "Mon, 9 Sep 2019 05:33:12 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Alkan", "Ferhat", ""], ["B\u0131y\u0131ko\u011flu", "T\u00fcrker", ""], ["Demange", "Marc", ""], ["Erten", "Cesim", ""]]}]