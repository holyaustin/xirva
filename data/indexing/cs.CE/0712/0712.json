[{"id": "0712.2255", "submitter": "Ian T Foster", "authors": "Ian Foster", "title": "Human-Machine Symbiosis, 50 Years On", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE cs.HC", "license": null, "abstract": "  Licklider advocated in 1960 the construction of computers capable of working\nsymbiotically with humans to address problems not easily addressed by humans\nworking alone. Since that time, many of the advances that he envisioned have\nbeen achieved, yet the time spent by human problem solvers in mundane\nactivities remains large. I propose here four areas in which improved tools can\nfurther advance the goal of enhancing human intellect: services, provenance,\nknowledge communities, and automation of problem-solving protocols.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2007 23:00:37 GMT"}], "update_date": "2007-12-17", "authors_parsed": [["Foster", "Ian", ""]]}, {"id": "0712.2262", "submitter": "Ian T Foster", "authors": "David Bernholdt, Shishir Bharathi, David Brown, Kasidit Chanchio,\n  Meili Chen, Ann Chervenak, Luca Cinquini, Bob Drach, Ian Foster, Peter Fox,\n  Jose Garcia, Carl Kesselman, Rob Markel, Don Middleton, Veronika Nefedova,\n  Line Pouchard, Arie Shoshani, Alex Sim, Gary Strand, Dean Williams", "title": "The Earth System Grid: Supporting the Next Generation of Climate\n  Modeling Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.NI", "license": null, "abstract": "  Understanding the earth's climate system and how it might be changing is a\npreeminent scientific challenge. Global climate models are used to simulate\npast, present, and future climates, and experiments are executed continuously\non an array of distributed supercomputers. The resulting data archive, spread\nover several sites, currently contains upwards of 100 TB of simulation data and\nis growing rapidly. Looking toward mid-decade and beyond, we must anticipate\nand prepare for distributed climate research data holdings of many petabytes.\nThe Earth System Grid (ESG) is a collaborative interdisciplinary project aimed\nat addressing the challenge of enabling management, discovery, access, and\nanalysis of these critically important datasets in a distributed and\nheterogeneous computational environment. The problem is fundamentally a Grid\nproblem. Building upon the Globus toolkit and a variety of other technologies,\nESG is developing an environment that addresses authentication, authorization\nfor data access, large-scale data transport and management, services and\nabstractions for high-performance remote data access, mechanisms for scalable\ndata replication, cataloging with rich semantic and syntactic information, data\ndiscovery, distributed monitoring, and Web-based portals for using the system.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2007 23:39:04 GMT"}], "update_date": "2007-12-17", "authors_parsed": [["Bernholdt", "David", ""], ["Bharathi", "Shishir", ""], ["Brown", "David", ""], ["Chanchio", "Kasidit", ""], ["Chen", "Meili", ""], ["Chervenak", "Ann", ""], ["Cinquini", "Luca", ""], ["Drach", "Bob", ""], ["Foster", "Ian", ""], ["Fox", "Peter", ""], ["Garcia", "Jose", ""], ["Kesselman", "Carl", ""], ["Markel", "Rob", ""], ["Middleton", "Don", ""], ["Nefedova", "Veronika", ""], ["Pouchard", "Line", ""], ["Shoshani", "Arie", ""], ["Sim", "Alex", ""], ["Strand", "Gary", ""], ["Williams", "Dean", ""]]}, {"id": "0712.2643", "submitter": "Cyrille Bertelle", "authors": "Pierrick Tranouez (LITIS), Cyrille Bertelle (LITIS), Damien Olivier\n  (LITIS)", "title": "Changing Levels of Description in a Fluid Flow Simulation", "comments": null, "journal-ref": "Emergent Properties in Natural and Artificial Dynamical Systems,\n  Springer (Ed.) (2006) 87-99", "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.CE", "license": null, "abstract": "  We describe here our perception of complex systems, of how we feel the\ndifferent layers of description are important part of a correct complex system\nsimulation. We describe a rough models categorization between rules based and\nlaw based, of how these categories handled the levels of descriptions or\nscales. We then describe our fluid flow simulation, which combines different\nfineness of grain in a mixed approach of these categories. This simulation is\nbuilt keeping in mind an ulterior use inside a more general aquatic ecosystem.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2007 07:07:06 GMT"}], "update_date": "2007-12-18", "authors_parsed": [["Tranouez", "Pierrick", "", "LITIS"], ["Bertelle", "Cyrille", "", "LITIS"], ["Olivier", "Damien", "", "LITIS"]]}, {"id": "0712.2789", "submitter": "Lester Ingber", "authors": "Lester Ingber", "title": "Trading in Risk Dimensions (TRD)", "comments": "This 2005 report has been withdrawn by the author as requested by the\n  publisher of \"Handbook of Technical Trading Analysis\" (Wiley, 2009) in which\n  an updated version appears", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work, mostly published, developed two-shell recursive trading\nsystems. An inner-shell of Canonical Momenta Indicators (CMI) is adaptively fit\nto incoming market data. A parameterized trading-rule outer-shell uses the\nglobal optimization code Adaptive Simulated Annealing (ASA) to fit the trading\nsystem to historical data. A simple fitting algorithm, usually not requiring\nASA, is used for the inner-shell fit. An additional risk-management\nmiddle-shell has been added to create a three-shell recursive\noptimization/sampling/fitting algorithm. Portfolio-level distributions of\ncopula-transformed multivariate distributions (with constituent markets\npossessing different marginal distributions in returns space) are generated by\nMonte Carlo samplings. ASA is used to importance-sample weightings of these\nmarkets.\n  The core code, Trading in Risk Dimensions (TRD), processes Training and\nTesting trading systems on historical data, and consistently interacts with\nRealTime trading platforms at minute resolutions, but this scale can be\nmodified. This approach transforms constituent probability distributions into a\ncommon space where it makes sense to develop correlations to further develop\nprobability distributions and risk/uncertainty analyses of the full portfolio.\nASA is used for importance-sampling these distributions and for optimizing\nsystem parameters.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2007 18:11:52 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2009 03:32:16 GMT"}], "update_date": "2009-11-04", "authors_parsed": [["Ingber", "Lester", ""]]}, {"id": "0712.3423", "submitter": "Alban Ponse", "authors": "J.A. Bergstra, A. Ponse, M.B. van der Zwaag", "title": "Tuplix Calculus", "comments": "22 pages", "journal-ref": "Scientific Annals of Computer Science, 18:35--61, 2008", "doi": null, "report-no": "PRG0713", "categories": "cs.LO cs.CE", "license": null, "abstract": "  We introduce a calculus for tuplices, which are expressions that generalize\nmatrices and vectors. Tuplices have an underlying data type for quantities that\nare taken from a zero-totalized field. We start with the core tuplix calculus\nCTC for entries and tests, which are combined using conjunctive composition. We\ndefine a standard model and prove that CTC is relatively complete with respect\nto it. The core calculus is extended with operators for choice, information\nhiding, scalar multiplication, clearing and encapsulation. We provide two\nexamples of applications; one on incremental financial budgeting, and one on\nmodular financial budget design.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2007 13:58:14 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Bergstra", "J. A.", ""], ["Ponse", "A.", ""], ["van der Zwaag", "M. B.", ""]]}, {"id": "0712.3617", "submitter": "Erhan Bayraktar", "authors": "Erhan Bayraktar, Bo Yang", "title": "A Unified Framework for Pricing Credit and Equity Derivatives", "comments": "Keywords: Credit Default Swap, Defaultable Bond, Defaultable Stock,\n  Equity Options, Stochastic Interest Rate, Implied Volatility, Multiscale\n  Perturbation Method", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model which can be jointly calibrated to the corporate bond term\nstructure and equity option volatility surface of the same company. Our purpose\nis to obtain explicit bond and equity option pricing formulas that can be\ncalibrated to find a risk neutral model that matches a set of observed market\nprices. This risk neutral model can then be used to price more exotic, illiquid\nor over-the-counter derivatives. We observe that the model implied credit\ndefault swap (CDS) spread matches the market CDS spread and that our model\nproduces a very desirable CDS spread term structure. This is observation is\nworth noticing since without calibrating any parameter to the CDS spread data,\nit is matched by the CDS spread that our model generates using the available\ninformation from the equity options and corporate bond markets. We also observe\nthat our model matches the equity option implied volatility surface well since\nwe properly account for the default risk premium in the implied volatility\nsurface. We demonstrate the importance of accounting for the default risk and\nstochastic interest rate in equity option pricing by comparing our results to\nFouque, Papanicolaou, Sircar and Solna (2003), which only accounts for\nstochastic volatility.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2007 02:53:38 GMT"}, {"version": "v2", "created": "Sat, 20 Sep 2008 21:44:00 GMT"}], "update_date": "2008-09-21", "authors_parsed": [["Bayraktar", "Erhan", ""], ["Yang", "Bo", ""]]}, {"id": "0712.4126", "submitter": "Chandan Reddy", "authors": "Chandan K. Reddy", "title": "TRUST-TECH based Methods for Optimization and Learning", "comments": "PHD Thesis", "journal-ref": "Chandan K. Reddy, TRUST-TECH based Methods for Optimization and\n  Learning, PHD Thesis, Cornell University, February 2007", "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.MS cs.NA cs.NE", "license": null, "abstract": "  Many problems that arise in machine learning domain deal with nonlinearity\nand quite often demand users to obtain global optimal solutions rather than\nlocal optimal ones. Optimization problems are inherent in machine learning\nalgorithms and hence many methods in machine learning were inherited from the\noptimization literature. Popularly known as the initialization problem, the\nideal set of parameters required will significantly depend on the given\ninitialization values. The recently developed TRUST-TECH (TRansformation Under\nSTability-reTaining Equilibria CHaracterization) methodology systematically\nexplores the subspace of the parameters to obtain a complete set of local\noptimal solutions. In this thesis work, we propose TRUST-TECH based methods for\nsolving several optimization and machine learning problems. Two stages namely,\nthe local stage and the neighborhood-search stage, are repeated alternatively\nin the solution space to achieve improvements in the quality of the solutions.\nOur methods were tested on both synthetic and real datasets and the advantages\nof using this novel framework are clearly manifested. This framework not only\nreduces the sensitivity to initialization, but also allows the flexibility for\nthe practitioners to use various global and local methods that work well for a\nparticular problem of interest. Other hierarchical stochastic algorithms like\nevolutionary algorithms and smoothing algorithms are also studied and\nframeworks for combining these methods with TRUST-TECH have been proposed and\nevaluated on several test systems.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2007 03:14:32 GMT"}], "update_date": "2007-12-27", "authors_parsed": [["Reddy", "Chandan K.", ""]]}]