[{"id": "1705.00033", "submitter": "Mohamed Abuella", "authors": "Mohamed Abuella and Badrul Chowdhury", "title": "Random Forest Ensemble of Support Vector Regression Models for Solar\n  Power Forecasting", "comments": "This is a preprint of the full paper that published in Innovative\n  Smart Grid Technologies, North America Conference, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the uncertainty of variable renewable resources, two\noff-the-shelf machine learning tools are deployed to forecast the solar power\noutput of a solar photovoltaic system. The support vector machines generate the\nforecasts and the random forest acts as an ensemble learning method to combine\nthe forecasts. The common ensemble technique in wind and solar power\nforecasting is the blending of meteorological data from several sources. In\nthis study though, the present and the past solar power forecasts from several\nmodels, as well as the associated meteorological data, are incorporated into\nthe random forest to combine and improve the accuracy of the day-ahead solar\npower forecasts. The performance of the combined model is evaluated over the\nentire year and compared with other combining techniques.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 04:27:00 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Abuella", "Mohamed", ""], ["Chowdhury", "Badrul", ""]]}, {"id": "1705.00614", "submitter": "Alexander Khoperskov V.", "authors": "Tatyana Dyakonova, Alexander Khoperskov, Sergey Khrapov", "title": "Numerical Model of Shallow Water: the Use of NVIDIA CUDA Graphics\n  Processors", "comments": "14 pages, 9 figures", "journal-ref": "Communications in Computer and Information Science, 2017, v.687,\n  pp. 132-145", "doi": "10.1007/978-3-319-55669-7_11", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper we discuss the main features of the software package for\nnumerical simulations of the surface water dynamics. We consider an\napproximation of the shallow water equations together with the parallel\ntechnologies for NVIDIA CUDA graphics processors. The numerical hydrodynamic\ncode is based on the combined Lagrangian-Euler method~(CSPH-TVD). We focused on\nthe features of the parallel implementation of Tesla line of graphics\nprocessors: C2070, K20, K40, K80. By using hierarchical grid systems at\ndifferent spatial scales we increase the efficiency of the computing resources\nusage and speed up our simulations of a various flooding problems.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 09:10:03 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Dyakonova", "Tatyana", ""], ["Khoperskov", "Alexander", ""], ["Khrapov", "Sergey", ""]]}, {"id": "1705.00819", "submitter": "Fedor Gubarev V.", "authors": "D.T.Shvarts, F.V.Gubarev", "title": "Towards an Automated Optimization of Laminated Composite Structures:\n  Hierarchical Zoning Approach with Exact Blending Rules", "comments": "34 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an automated methodology to optimize laminated composite\nstructures. Our approach, inspired by bi-level optimization scheme, avoids its\nprime deficiencies and is based on developed computationally inexpensive\nstacking sequences reconstruction algorithm, which identically satisfies\nconventional set of blending rules. We combine it with proposed tight\napproximation to feasible domain of composite integral parameters and\nhierarchical zoning procedure to get highly efficient optimization methodology,\nwhich we test in two example applications. In both cases it is shown that\nblending rules compliant optimal solution remains within 10% gap from one with\nno rules applied.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 06:47:35 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Shvarts", "D. T.", ""], ["Gubarev", "F. V.", ""]]}, {"id": "1705.00891", "submitter": "Syed Ali Asad Rizvi", "authors": "Syed Ali Asad Rizvi, Stephen J. Roberts, Michael A. Osborne and Favour\n  Nyikosa", "title": "A Novel Approach to Forecasting Financial Volatility with Gaussian\n  Process Envelopes", "comments": "16 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use Gaussian Process (GP) regression to propose a novel\napproach for predicting volatility of financial returns by forecasting the\nenvelopes of the time series. We provide a direct comparison of their\nperformance to traditional approaches such as GARCH. We compare the forecasting\npower of three approaches: GP regression on the absolute and squared returns;\nregression on the envelope of the returns and the absolute returns; and\nregression on the envelope of the negative and positive returns separately. We\nuse a maximum a posteriori estimate with a Gaussian prior to determine our\nhyperparameters. We also test the effect of hyperparameter updating at each\nforecasting step. We use our approaches to forecast out-of-sample volatility of\nfour currency pairs over a 2 year period, at half-hourly intervals. From three\nkernels, we select the kernel giving the best performance for our data. We use\ntwo published accuracy measures and four statistical loss functions to evaluate\nthe forecasting ability of GARCH vs GPs. In mean squared error the GP's perform\n20% better than a random walk model, and 50% better than GARCH for the same\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:30:13 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Rizvi", "Syed Ali Asad", ""], ["Roberts", "Stephen J.", ""], ["Osborne", "Michael A.", ""], ["Nyikosa", "Favour", ""]]}, {"id": "1705.01142", "submitter": "Swetava Ganguli", "authors": "Swetava Ganguli, Jared Dunnmon", "title": "Machine Learning for Better Models for Predicting Bond Prices", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bond prices are a reflection of extremely complex market interactions and\npolicies, making prediction of future prices difficult. This task becomes even\nmore challenging due to the dearth of relevant information, and accuracy is not\nthe only consideration--in trading situations, time is of the essence. Thus,\nmachine learning in the context of bond price predictions should be both fast\nand accurate. In this course project, we use a dataset describing the previous\n10 trades of a large number of bonds among other relevant descriptive metrics\nto predict future bond prices. Each of 762,678 bonds in the dataset is\ndescribed by a total of 61 attributes, including a ground truth trade price. We\nevaluate the performance of various supervised learning algorithms for\nregression followed by ensemble methods, with feature and model selection\nconsiderations being treated in detail. We further evaluate all methods on both\naccuracy and speed. Finally, we propose a novel hybrid time-series aided\nmachine learning method that could be applied to such datasets in future work.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 15:12:49 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Ganguli", "Swetava", ""], ["Dunnmon", "Jared", ""]]}, {"id": "1705.01667", "submitter": "Masahito Ohue", "authors": "Masahito Ohue, Takuro Yamazaki, Tomohiro Ban, and Yutaka Akiyama", "title": "Link Mining for Kernel-based Compound-Protein Interaction Predictions\n  Using a Chemogenomics Approach", "comments": null, "journal-ref": "In the Thirteenth International Conference on Intelligent\n  Computing (ICIC2017), Lecture Notes in Computer Science, 10362: 549-558, 2017", "doi": "10.1007/978-3-319-63312-1_48", "report-no": null, "categories": "q-bio.QM cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual screening (VS) is widely used during computational drug discovery to\nreduce costs. Chemogenomics-based virtual screening (CGBVS) can be used to\npredict new compound-protein interactions (CPIs) from known CPI network data\nusing several methods, including machine learning and data mining. Although\nCGBVS facilitates highly efficient and accurate CPI prediction, it has poor\nperformance for prediction of new compounds for which CPIs are unknown. The\npairwise kernel method (PKM) is a state-of-the-art CGBVS method and shows high\naccuracy for prediction of new compounds. In this study, on the basis of link\nmining, we improved the PKM by combining link indicator kernel (LIK) and\nchemical similarity and evaluated the accuracy of these methods. The proposed\nmethod obtained an average area under the precision-recall curve (AUPR) value\nof 0.562, which was higher than that achieved by the conventional Gaussian\ninteraction profile (GIP) method (0.425), and the calculation time was only\nincreased by a few percent.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 01:29:19 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 03:46:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ohue", "Masahito", ""], ["Yamazaki", "Takuro", ""], ["Ban", "Tomohiro", ""], ["Akiyama", "Yutaka", ""]]}, {"id": "1705.01671", "submitter": "Rui Yao", "authors": "Rui Yao, Kai Sun", "title": "Towards Simulation and Risk Assessment of Weather-Related Cascading\n  Outages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weather and environmental factors are verified to have played significant\nroles in historical major cascading outages and blackouts. Therefore, in the\nsimulation and risk assessment of cascading outages in power systems, it is\nnecessary to consider the weather and environmental effects. This paper\nproposes a method for the risk assessment of weather-related cascading outages.\nBased on the analysis of historical outage records and temperature-dependent\nphysical outage mechanisms of transmission lines, an outage rate model\nconsidering weather condition and conductor temperature is proposed, and the\nanalytical form of outage probability of lines are derived. With the\nweather-dependent outage model, a two-stage risk assessment method based on\nMarkovian tree (MT) search is proposed, which consists of offline full\nassessment, and online efficient update of risk assessment results and\ncontinued MT search using updated NWP data. The test cases on NPCC 140-bus test\nsystem model in winter and summer scenarios verify the advantages of the\nproposed risk assessment method in both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 01:47:19 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Yao", "Rui", ""], ["Sun", "Kai", ""]]}, {"id": "1705.01941", "submitter": "Luigi Troiano", "authors": "Luigi Troiano and Pravesh Kriplani and Irene Diaz", "title": "Regression Driven F--Transform and Application to Smoothing of Financial\n  Time Series", "comments": "IFSA-SCIS 2017, 5 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.CE cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose to extend the definition of fuzzy transform in order\nto consider an interpolation of models that are richer than the standard fuzzy\ntransform. We focus on polynomial models, linear in particular, although the\napproach can be easily applied to other classes of models. As an example of\napplication, we consider the smoothing of time series in finance. A comparison\nwith moving averages is performed using NIFTY 50 stock market index.\nExperimental results show that a regression driven fuzzy transform (RDFT)\nprovides a smoothing approximation of time series, similar to moving average,\nbut with a smaller delay. This is an important feature for finance and other\napplication, where time plays a key role.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 20:51:05 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Troiano", "Luigi", ""], ["Kriplani", "Pravesh", ""], ["Diaz", "Irene", ""]]}, {"id": "1705.02019", "submitter": "Loukianos Spyrou", "authors": "Loukianos Spyrou, Mario Parra and Javier Escudero", "title": "Complex tensor factorisation with PARAFAC2 for the estimation of brain\n  connectivity from the EEG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The coupling between neuronal populations and its magnitude have\nbeen shown to be informative for various clinical applications. One method to\nestimate brain connectivity is with electroencephalography (EEG) from which the\ncross-spectrum between different sensor locations is derived. We wish to test\nthe efficacy of tensor factorisation in the estimation of brain connectivity.\nMethods: Complex tensor factorisation based on PARAFAC2 is used to decompose\nthe EEG into scalp components described by the spatial, spectral, and complex\ntrial profiles. An EEG model in the complex domain was derived that shows the\nsuitability of PARAFAC2. A connectivity metric was also derived on the complex\ntrial profiles of the extracted components. Results: Results on a benchmark EEG\ndataset confirmed that PARAFAC2 can estimate connectivity better than\ntraditional tensor analysis such as PARAFAC within a range of signal-to-noise\nratios. The analysis of EEG from patients with mild cognitive impairment or\nAlzheimer's disease showed that PARAFAC2 identifies loss of brain connectivity\nbetter than traditional approaches and agreeing with prior pathological\nknowledge. Conclusion: The complex PARAFAC2 algorithm is suitable for EEG\nconnectivity estimation since it allows to extract meaningful coupled sources\nand provides better estimates than complex PARAFAC. Significance: A new\nparadigm that employs complex tensor factorisation has demonstrated to be\nsuccessful in identifying brain connectivity and the location of couples\nsources for both a benchmark and a real-world EEG dataset. This can enable\nfuture applications and has the potential to solve some the issues that\ndeteriorate the performance of traditional connectivity metrics.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:59:24 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Spyrou", "Loukianos", ""], ["Parra", "Mario", ""], ["Escudero", "Javier", ""]]}, {"id": "1705.02274", "submitter": "Piero Triverio", "authors": "Fadime Bekmambetova, Xinyue Zhang and Piero Triverio", "title": "A Dissipation Theory for Three-Dimensional FDTD with Application to\n  Stability Analysis and Subgridding", "comments": null, "journal-ref": null, "doi": "10.1109/TAP.2018.2869617", "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finite-difference time-domain (FDTD) algorithm is a popular numerical\nmethod for solving electromagnetic problems. FDTD simulations can suffer from\ninstability due to the explicit nature of the method. Stability enforcement can\nbe particularly challenging in scenarios where a setup is composed of multiple\ncomponents, such as grids of different resolution, advanced boundary\nconditions, reduced-order models, and lumped elements. We propose a dissipation\ntheory for 3-D FDTD inspired by the principle of energy conservation. We view\nthe FDTD update equations for a 3-D region as a dynamical system, and show\nunder which conditions the system is dissipative. By requiring each component\nof an FDTD-like scheme to be dissipative, the stability of the overall coupled\nscheme follows by construction. The proposed framework enables the creation of\nprovably stable schemes in an easy and modular fashion, since conditions are\nimposed on the individual components, rather than on the overall coupled scheme\nas in existing approaches. With the proposed framework, we derive a new\nsubgridding scheme with guaranteed stability, low reflections, support for\nmaterial traverse and arbitrary (odd) grid refinement ratio.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 15:44:14 GMT"}, {"version": "v2", "created": "Sat, 22 Jul 2017 06:56:52 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Bekmambetova", "Fadime", ""], ["Zhang", "Xinyue", ""], ["Triverio", "Piero", ""]]}, {"id": "1705.03233", "submitter": "Adamantios Ntakaris Mr", "authors": "Adamantios Ntakaris, Martin Magris, Juho Kanniainen, Moncef Gabbouj,\n  Alexandros Iosifidis", "title": "Benchmark Dataset for Mid-Price Forecasting of Limit Order Book Data\n  with Machine Learning Methods", "comments": "Published: Journal of Forecasting", "journal-ref": null, "doi": "10.1002/for.2543", "report-no": null, "categories": "cs.CE q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing the prediction of metrics in high-frequency financial markets is a\nchallenging task. An efficient way is by monitoring the dynamics of a limit\norder book to identify the information edge. This paper describes the first\npublicly available benchmark dataset of high-frequency limit order markets for\nmid-price prediction. We extracted normalized data representations of time\nseries data for five stocks from the NASDAQ Nordic stock market for a time\nperiod of ten consecutive days, leading to a dataset of ~4,000,000 time series\nsamples in total. A day-based anchored cross-validation experimental protocol\nis also provided that can be used as a benchmark for comparing the performance\nof state-of-the-art methodologies. Performance of baseline approaches are also\nprovided to facilitate experimental comparisons. We expect that such a\nlarge-scale dataset can serve as a testbed for devising novel solutions of\nexpert systems for high-frequency limit order book data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 08:56:06 GMT"}, {"version": "v2", "created": "Mon, 22 May 2017 13:26:57 GMT"}, {"version": "v3", "created": "Tue, 30 May 2017 18:24:20 GMT"}, {"version": "v4", "created": "Thu, 23 Aug 2018 20:46:23 GMT"}, {"version": "v5", "created": "Wed, 11 Mar 2020 16:38:56 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Ntakaris", "Adamantios", ""], ["Magris", "Martin", ""], ["Kanniainen", "Juho", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "1705.03870", "submitter": "Yongxing Wang", "authors": "Yongxing Wang, Peter K. Jimack, and Mark A. Walkley", "title": "A One-Field Energy-conserving Monolithic Fictitious Domain Method for\n  Fluid-Structure Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we analyze and numerically assess a new fictitious domain\nmethod for fluid-structure interactions in two and three dimensions. The\ndistinguishing feature of the proposed method is that it only solves for one\nvelocity field for the whole fluid-structure domain; the interactions remain\ndecoupled until solving the final linear algebraic equations. To achieve this\nthe finite element procedures are carried out separately on two different\nmeshes for the fluid and solid respectively, and the assembly of the final\nlinear system brings the fluid and solid parts together via an isoparametric\ninterpolation matrix between the two meshes. In this article, an implicit\nversion of this approach is introduced. The property of energy conservation is\nproved, which is a strong indication of stability. The solvability and error\nestimate for the corresponding stationary problem (one time step of the\ntransient problem) are analyzed. Finally, 2D and 3D numerical examples are\npresented to validate the conservation properties.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 17:45:04 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Wang", "Yongxing", ""], ["Jimack", "Peter K.", ""], ["Walkley", "Mark A.", ""]]}, {"id": "1705.03872", "submitter": "Sebastian Sch\\\"ops", "authors": "Zeger Bontinck and Oliver Lass and Sebastian Sch\\\"ops and Oliver Rain", "title": "Model Order Reduction for Rotating Electrical Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of electric rotating machines is both computationally\nexpensive and memory intensive. To overcome these costs, model order reduction\ntechniques can be applied. The focus of this contribution is especially on\nmachines that contain non-symmetric components. These are usually introduced\nduring the mass production process and are modeled by small perturbations in\nthe geometry (e.g., eccentricity) or the material parameters. While model order\nreduction for symmetric machines is clear and does not need special treatment,\nthe non-symmetric setting adds additional challenges. An adaptive strategy\nbased on proper orthogonal decomposition is developed to overcome these\ndifficulties. Equipped with an a posteriori error estimator the obtained\nsolution is certified. Numerical examples are presented to demonstrate the\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 17:49:19 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Bontinck", "Zeger", ""], ["Lass", "Oliver", ""], ["Sch\u00f6ps", "Sebastian", ""], ["Rain", "Oliver", ""]]}, {"id": "1705.04279", "submitter": "Boyce Griffith", "authors": "Ali Hasan, Ebrahim M. Kolahdouz, Andinet Enquobahrie, Thomas G.\n  Caranasos, John P. Vavalle, Boyce E. Griffith", "title": "Image-based immersed boundary model of the aortic root", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CE cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, approximately 300,000 heart valve repair or replacement procedures\nare performed worldwide, including approximately 70,000 aortic valve\nreplacement surgeries in the United States alone. This paper describes progress\nin constructing anatomically and physiologically realistic immersed boundary\n(IB) models of the dynamics of the aortic root and ascending aorta. This work\nbuilds on earlier IB models of fluid-structure interaction (FSI) in the aortic\nroot, which previously achieved realistic hemodynamics over multiple cardiac\ncycles, but which also were limited to simplified aortic geometries and\nidealized descriptions of the biomechanics of the aortic valve cusps. By\ncontrast, the model described herein uses an anatomical geometry reconstructed\nfrom patient-specific computed tomography angiography (CTA) data, and employs a\ndescription of the elasticity of the aortic valve leaflets based on a\nfiber-reinforced constitutive model fit to experimental tensile test data.\nNumerical tests show that the model is able to resolve the leaflet biomechanics\nin diastole and early systole at practical grid spacings. The model is also\nused to examine differences in the mechanics and fluid dynamics yielded by\nfresh valve leaflets and glutaraldehyde-fixed leaflets similar to those used in\nbioprosthetic heart valves. Although there are large differences in the leaflet\ndeformations during diastole, the differences in the open configurations of the\nvalve models are relatively small, and nearly identical hemodynamics are\nobtained in all cases considered.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 06:34:16 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Hasan", "Ali", ""], ["Kolahdouz", "Ebrahim M.", ""], ["Enquobahrie", "Andinet", ""], ["Caranasos", "Thomas G.", ""], ["Vavalle", "John P.", ""], ["Griffith", "Boyce E.", ""]]}, {"id": "1705.04374", "submitter": "Jonas \\v{S}ukys", "authors": "Jonas \\v{S}ukys, Ursula Rasthofer, Fabian Wermelinger, Panagiotis\n  Hadjidoukas, and Petros Koumoutsakos", "title": "Optimal fidelity multi-level Monte Carlo for quantification of\n  uncertainty in simulations of cloud cavitation collapse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify uncertainties in the location and magnitude of extreme pressure\nspots revealed from large scale multi-phase flow simulations of cloud\ncavitation collapse. We examine clouds containing 500 cavities and quantify\nuncertainties related to their initial spatial arrangement. The resulting\n2000-dimensional space is sampled using a non-intrusive and computationally\nefficient Multi-Level Monte Carlo (MLMC) methodology. We introduce novel\noptimal control variate coefficients to enhance the variance reduction in MLMC.\nThe proposed optimal fidelity MLMC leads to more than two orders of magnitude\nspeedup when compared to standard Monte Carlo methods. We identify large\nuncertainties in the location and magnitude of the peak pressure pulse and\npresent its statistical correlations and joint probability density functions\nwith the geometrical characteristics of the cloud. Characteristic properties of\nspatial cloud structure are identified as potential causes of significant\nuncertainties in exerted collapse pressures.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 20:54:51 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["\u0160ukys", "Jonas", ""], ["Rasthofer", "Ursula", ""], ["Wermelinger", "Fabian", ""], ["Hadjidoukas", "Panagiotis", ""], ["Koumoutsakos", "Petros", ""]]}, {"id": "1705.04678", "submitter": "Nikhil Galagali", "authors": "Nikhil Galagali and Youssef M. Marzouk", "title": "Exploiting network topology for large-scale inference of nonlinear\n  reaction models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of chemical reaction models aids understanding and prediction\nin areas ranging from biology to electrochemistry and combustion. A systematic\napproach to building reaction network models uses observational data not only\nto estimate unknown parameters, but also to learn model structure. Bayesian\ninference provides a natural approach to this data-driven construction of\nmodels. Yet traditional Bayesian model inference methodologies that numerically\nevaluate the evidence for each model are often infeasible for nonlinear\nreaction network inference, as the number of plausible models can be\ncombinatorially large. Alternative approaches based on model-space sampling can\nenable large-scale network inference, but their realization presents many\nchallenges. In this paper, we present new computational methods that make\nlarge-scale nonlinear network inference tractable. First, we exploit the\ntopology of networks describing potential interactions among chemical species\nto design improved \"between-model\" proposals for reversible-jump Markov chain\nMonte Carlo. Second, we introduce a sensitivity-based determination of move\ntypes which, when combined with network-aware proposals, yields significant\nadditional gains in sampling performance. These algorithms are demonstrated on\ninference problems drawn from systems biology, with nonlinear differential\nequation models of species interactions.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:55:44 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 18:35:07 GMT"}, {"version": "v3", "created": "Sun, 14 Oct 2018 16:11:46 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2018 01:26:55 GMT"}, {"version": "v5", "created": "Sat, 19 Jan 2019 03:43:48 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Galagali", "Nikhil", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1705.05783", "submitter": "Matei Tene", "authors": "Matei Tene, Yixuan Wang, Hadi Hajibeygi", "title": "Adaptive Algebraic Multiscale Solver for Compressible Flow in\n  Heterogeneous Porous Media", "comments": null, "journal-ref": "J. Comput. Phys. 300 (2015) 679-694", "doi": "10.1016/j.jcp.2015.08.009", "report-no": null, "categories": "math.NA cs.CE physics.comp-ph physics.flu-dyn", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the development of an Adaptive Algebraic Multiscale\nSolver for Compressible flow (C-AMS) in heterogeneous porous media. Similar to\nthe recently developed AMS for incompressible (linear) flows [Wang et al., JCP,\n2014], C-AMS operates by defining primal and dual-coarse blocks on top of the\nfine-scale grid. These coarse grids facilitate the construction of a\nconservative (finite volume) coarse-scale system and the computation of local\nbasis functions, respectively. However, unlike the incompressible (elliptic)\ncase, the choice of equations to solve for basis functions in compressible\nproblems is not trivial. Therefore, several basis function formulations\n(incompressible and compressible, with and without accumulation) are considered\nin order to construct an efficient multiscale prolongation operator. As for the\nrestriction operator, C-AMS allows for both multiscale finite volume (MSFV) and\nfinite element (MSFE) methods. Finally, in order to resolve high-frequency\nerrors, fine-scale (pre- and post-) smoother stages are employed. In order to\nreduce computational expense, the C-AMS operators (prolongation, restriction,\nand smoothers) are updated adaptively. In addition to this, the linear system\nin the Newton-Raphson loop is infrequently updated. Systematic numerical\nexperiments are performed to determine the effect of the various options,\noutlined above, on the C-AMS convergence behaviour. An efficient C-AMS strategy\nfor heterogeneous 3D compressible problems is developed based on overall CPU\ntimes. Finally, C-AMS is compared against an industrial-grade Algebraic\nMultiGrid (AMG) solver. Results of this comparison illustrate that the C-AMS is\nquite efficient as a nonlinear solver, even when iterated to machine accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 12:51:29 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Tene", "Matei", ""], ["Wang", "Yixuan", ""], ["Hajibeygi", "Hadi", ""]]}, {"id": "1705.05784", "submitter": "Matei Tene", "authors": "Matei Tene, Mohammed Saad Al Kobaisi, Hadi Hajibeygi", "title": "Algebraic multiscale method for flow in heterogeneous porous media with\n  embedded discrete fractures (F-AMS)", "comments": null, "journal-ref": "J. Comput. Phys. 321 (2016) 819-845", "doi": "10.1016/j.jcp.2016.06.012", "report-no": null, "categories": "math.NA cs.CE physics.comp-ph physics.flu-dyn", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces an Algebraic MultiScale method for simulation of flow\nin heterogeneous porous media with embedded discrete Fractures (F-AMS). First,\nmultiscale coarse grids are independently constructed for both porous matrix\nand fracture networks. Then, a map between coarse- and fine-scale is obtained\nby algebraically computing basis functions with local support. In order to\nextend the localization assumption to the fractured media, four types of basis\nfunctions are investigated: (1) Decoupled-AMS, in which the two media are\ncompletely decoupled, (2) Frac-AMS and (3) Rock-AMS, which take into account\nonly one-way transmissibilities, and (4) Coupled-AMS, in which the matrix and\nfracture interpolators are fully coupled. In order to ensure scalability, the\nF-AMS framework permits full flexibility in terms of the resolution of the\nfracture coarse grids. Numerical results are presented for two- and\nthree-dimensional heterogeneous test cases. During these experiments, the\nperformance of F-AMS, paired with ILU(0) as second-stage smoother in a\nconvergent iterative procedure, is studied by monitoring CPU times and\nconvergence rates. Finally, in order to investigate the scalability of the\nmethod, an extensive benchmark study is conducted, where a commercial algebraic\nmultigrid solver is used as reference. The results show that, given an\nappropriate coarsening strategy, F-AMS is insensitive to severe fracture and\nmatrix conductivity contrasts, as well as the length of the fracture networks.\nIts unique feature is that a fine-scale mass conservative flux field can be\nreconstructed after any iteration, providing efficient approximate solutions in\ntime-dependent simulations.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 12:50:54 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Tene", "Matei", ""], ["Kobaisi", "Mohammed Saad Al", ""], ["Hajibeygi", "Hadi", ""]]}, {"id": "1705.06149", "submitter": "Daniel Ruprecht", "authors": "Roberto Croce and Daniel Ruprecht and Rolf Krause", "title": "Parallel-in-Space-and-Time Simulation of the Three-Dimensional, Unsteady\n  Navier-Stokes Equations for Incompressible Flow", "comments": null, "journal-ref": "Modeling, Simulation and Optimization of Complex Processes - HPSC\n  2012, Springer International Publishing, pages 13-23, 2014", "doi": "10.1007/978-3-319-09063-4_2", "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we combine the Parareal parallel-in-time method together with\nspatial parallelization and investigate this space-time parallel scheme by\nmeans of solving the three-dimensional incompressible Navier-Stokes equations.\nParallelization of time stepping provides a new direction of parallelization\nand allows to employ additional cores to further speed up simulations after\nspatial parallelization has saturated. We report on numerical experiments\nperformed on a Cray XE6, simulating a driven cavity flow with and without\nobstacles. Distributed memory parallelization is used in both space and time,\nfeaturing up to 2,048 cores in total. It is confirmed that the\nspace-time-parallel method can provide speedup beyond the saturation of the\nspatial parallelization.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 13:34:22 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Croce", "Roberto", ""], ["Ruprecht", "Daniel", ""], ["Krause", "Rolf", ""]]}, {"id": "1705.06559", "submitter": "Zhaoming Yin", "authors": "Zhaoming Yin, Jijun Tang, Stephen W. Schaeffer, David A. Bader", "title": "Exemplar or Matching: Modeling DCJ Problems with Unequal Content Genome\n  Data", "comments": "17 pages", "journal-ref": "Journal of Combinatorial Optimization, 2016", "doi": "10.1007/s10878-015-9940-4", "report-no": null, "categories": "cs.DS cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The edit distance under the DCJ model can be computed in linear time for\ngenomes with equal content or with Indels. But it becomes NP-Hard in the\npresence of duplications, a problem largely unsolved especially when Indels are\nconsidered. In this paper, we compare two mainstream methods to deal with\nduplications and associate them with Indels: one by deletion, namely\nDCJ-Indel-Exemplar distance; versus the other by gene matching, namely\nDCJ-Indel-Matching distance. We design branch-and-bound algorithms with set of\noptimization methods to compute exact distances for both. Furthermore, median\nproblems are discussed in alignment with both of these distance methods, which\nare to find a median genome that minimizes distances between itself and three\ngiven genomes. Lin-Kernighan (LK) heuristic is leveraged and powered up by\nsub-graph decomposition and search space reduction technologies to handle\nmedian computation. A wide range of experiments are conducted on synthetic data\nsets and real data sets to show pros and cons of these two distance metrics per\nse, as well as putting them in the median computation scenario.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 12:53:56 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Yin", "Zhaoming", ""], ["Tang", "Jijun", ""], ["Schaeffer", "Stephen W.", ""], ["Bader", "David A.", ""]]}, {"id": "1705.07024", "submitter": "Irina Georgescu", "authors": "Irina Georgescu, Ana Maria Lucia Casademunt", "title": "Optimal prevention with possibilistic and mixed background risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the effect of posibilistic or mixed background risk on the\nlevel of optimal prevention is studied. In the framework of five purely\npossibilistic or mixed models, necessary and sufficient conditions are found\nsuch that the level of optimal saving decreases or increases as a result of the\nactions of various types of background risk. This way our results complete\nthose obtained by Courbage and Rey for some prevention models with\nprobabilistic background risk.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 14:41:56 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Georgescu", "Irina", ""], ["Casademunt", "Ana Maria Lucia", ""]]}, {"id": "1705.07413", "submitter": "Matthias M\\\"oller", "authors": "Matthias M\\\"oller, Cornelis Vuik", "title": "On the impact of quantum computing technology on future developments in\n  high-performance scientific computing", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": "10.1007/s10676-017-9438-0", "report-no": null, "categories": "cs.CE cs.CY math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum computing technologies have become a hot topic in academia and\nindustry receiving much attention and financial support from all sides.\nBuilding a quantum computer that can be used practically is in itself an\noutstanding challenge that has become the 'new race to the moon'. Next to\nresearchers and vendors of future computing technologies, national authorities\nare showing strong interest in maturing this technology due to its known\npotential to break many of today's encryption techniques, which would have\nsignificant impact on our society. It is however quite likely that quantum\ncomputing has beneficial impact on many computational disciplines.\n  In this article we describe our vision of future developments in scientific\ncomputing that would be enabled by the advent of software-programmable quantum\ncomputers. We thereby assume that quantum computers will form part of a hybrid\naccelerated computing platform like GPUs and co-processor cards do today. In\nparticular, we address the potential of quantum algorithms to bring major\nbreakthroughs in applied mathematics and its applications. Finally, we give\nseveral examples that demonstrate the possible impact of quantum-accelerated\nscientific computing on society.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 08:28:18 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 18:58:39 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["M\u00f6ller", "Matthias", ""], ["Vuik", "Cornelis", ""]]}, {"id": "1705.07980", "submitter": "Hiroki Sayama", "authors": "Minjun Kim, Hiroki Sayama", "title": "Predicting stock market movements using network science: An information\n  theoretic approach", "comments": "13 pages, 7 figures, 3 tables", "journal-ref": "Applied Network Sciencevolume 2, Article number: 35 (2017)", "doi": "10.1007/s41109-017-0055-y", "report-no": null, "categories": "cs.SI cs.CE cs.IT math.IT physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stock market is considered as one of the highly complex systems, which\nconsists of many components whose prices move up and down without having a\nclear pattern. The complex nature of a stock market challenges us on making a\nreliable prediction of its future movements. In this paper, we aim at building\na new method to forecast the future movements of Standard & Poor's 500 Index\n(S&P 500) by constructing time-series complex networks of S&P 500 underlying\ncompanies by connecting them with links whose weights are given by the mutual\ninformation of 60-minute price movements of the pairs of the companies with the\nconsecutive 5,340 minutes price records. We showed that the changes in the\nstrength distributions of the networks provide an important information on the\nnetwork's future movements. We built several metrics using the strength\ndistributions and network measurements such as centrality, and we combined the\nbest two predictors by performing a linear combination. We found that the\ncombined predictor and the changes in S&P 500 show a quadratic relationship,\nand it allows us to predict the amplitude of the one step future change in S&P\n500. The result showed significant fluctuations in S&P 500 Index when the\ncombined predictor was high. In terms of making the actual index predictions,\nwe built ARIMA models. We found that adding the network measurements into the\nARIMA models improves the model accuracy. These findings are useful for\nfinancial market policy makers as an indicator based on which they can\ninterfere with the markets before the markets make a drastic change, and for\nquantitative investors to improve their forecasting models.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 20:14:31 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Kim", "Minjun", ""], ["Sayama", "Hiroki", ""]]}, {"id": "1705.08019", "submitter": "Sebastian Sch\\\"ops", "authors": "Melina Merkel and Innocent Niyonzima and Sebastian Sch\\\"ops", "title": "ParaExp using Leapfrog as Integrator for High-Frequency Electromagnetic\n  Simulations", "comments": "Corrected typos. arXiv admin note: text overlap with arXiv:1607.00368", "journal-ref": null, "doi": "10.1002/2017RS006357", "report-no": null, "categories": "math.NA cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, ParaExp was proposed for the time integration of linear hyperbolic\nproblems. It splits the time interval of interest into sub-intervals and\ncomputes the solution on each sub-interval in parallel. The overall solution is\ndecomposed into a particular solution defined on each sub-interval with zero\ninitial conditions and a homogeneous solution propagated by the matrix\nexponential applied to the initial conditions. The efficiency of the method\ndepends on fast approximations of this matrix exponential based on recent\nresults from numerical linear algebra. This paper deals with the application of\nParaExp in combination with Leapfrog to electromagnetic wave problems in\ntime-domain. Numerical tests are carried out for a simple toy problem and a\nrealistic spiral inductor model discretized by the Finite Integration\nTechnique.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 21:57:27 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 10:05:33 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Merkel", "Melina", ""], ["Niyonzima", "Innocent", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "1705.08173", "submitter": "Ulrich R\\\"omer", "authors": "Armin Galetzka, Zeger Bontinck, Ulrich R\\\"omer, Sebastian Sch\\\"ops", "title": "Multilevel Monte Carlo Simulation of the Eddy Current Problem With\n  Random Parameters", "comments": "Applied Computational Electromagnetics Society Symposium - Italy\n  (ACES), 2017 International, http://ieeexplore.ieee.org/document/7916062/", "journal-ref": null, "doi": "10.23919/ROPACES.2017.7916062", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multilevel Monte Carlo method is applied to an academic example in the\nfield of electromagnetism. The method exhibits a reduced variance by assigning\nthe samples to multiple models with a varying spatial resolution. For the given\nexample it is found that the main costs of the method are spent on the coarsest\nlevel.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 10:46:25 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Galetzka", "Armin", ""], ["Bontinck", "Zeger", ""], ["R\u00f6mer", "Ulrich", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "1705.08229", "submitter": "Arthur Barnes", "authors": "Arthur Barnes, Harsha Nagarajan, Emre Yamangil, Russell Bent, Scott\n  Backhaus", "title": "Tools for improving resilience of electric distribution systems with\n  networked microgrids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the electrical grid, the distribution system is themost vulnerable to\nsevere weather events. Well-placed and coordinatedupgrades, such as the\ncombination of microgrids, systemhardening and additional line redundancy, can\ngreatly reduce thenumber of electrical outages during extreme events. Indeed,\nithas been suggested that resilience is one of the primary benefitsof networked\nmicrogrids. We formulate a resilient distributiongrid design problem as a\ntwo-stage stochastic program andmake use of decomposition-based heuristic\nalgorithms to scaleto problems of practical size. We demonstrate the\nfeasibilityof a resilient distribution design tool on a model of an\nactualdistribution network. We vary the study parameters, i.e., thecapital cost\nof microgrid generation relative to system hardeningand target system\nresilience metrics, and find regions in thisparametric space corresponding to\ndifferent distribution systemarchitectures, such as individual microgrids,\nhardened networks,and a transition region that suggests the benefits of\nmicrogridsnetworked via hardened circuit segments.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:14:22 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Barnes", "Arthur", ""], ["Nagarajan", "Harsha", ""], ["Yamangil", "Emre", ""], ["Bent", "Russell", ""], ["Backhaus", "Scott", ""]]}, {"id": "1705.08738", "submitter": "Il-Young Son", "authors": "Birsen Yazici and Il-Young Son and H. Cagri Yanik", "title": "Doppler Synthetic Aperture Radar Interferometry: A Novel SAR\n  Interferometry for Height Mapping using Ultra-Narrowband Waveforms", "comments": "Submitted to Inverse Problems", "journal-ref": null, "doi": "10.1088/1361-6420/aab24c", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new and novel radar interferometry based on Doppler\nsynthetic aperture radar (Doppler-SAR) paradigm. Conventional SAR\ninterferometry relies on wideband transmitted waveforms to obtain high range\nresolution. Topography of a surface is directly related to the range difference\nbetween two antennas configured at different positions. Doppler-SAR is a novel\nimaging modality that uses ultra-narrowband continuous waves (UNCW). It takes\nadvantage of high resolution Doppler information provided by UNCWs to form high\nresolution SAR images.\n  We introduced the theory of Doppler-SAR interferometry. We derived\ninterferometric phase model and develop the equations of height mapping. Unlike\nconventional SAR interferometry, we show that the topography of a scene is\nrelated to the difference in Doppler between two antennas configured at\ndifferent velocities. While the conventional SAR interferometry uses range,\nDoppler and Doppler due to interferometric phase in height mapping, Doppler-SAR\ninterferometry uses Doppler, Doppler-rate and Doppler-rate due to\ninterferometric phase in height mapping. We demonstrate our theory in numerical\nsimulations.\n  Doppler-SAR interferometry offers the advantages of long-range, robust,\nenvironmentally friendly operations; low-power, low-cost, lightweight systems\nsuitable for low-payload platforms, such as micro-satellites; and passive\napplications using sources of opportunity transmitting UNCW.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 13:09:55 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Yazici", "Birsen", ""], ["Son", "Il-Young", ""], ["Yanik", "H. Cagri", ""]]}, {"id": "1705.08849", "submitter": "Amir Geranmayeh Dr.-Ing.", "authors": "Amir Geranmayeh", "title": "Parallel Matrix-Free Implementation of Frequency-Domain Finite\n  Difference Methods for Cluster Computing", "comments": "7 pages, 10 figures including: Matrix-free 3D finite-difference\n  frequency-domain (FDFD) methods, Simultaneous reduction in memory usage and\n  computational costs of FDFD, Broadband impedance calculation of electrically\n  large interconnects, Ease of solver modification for mutual field coupling\n  simulation between many ports, Domain decomposition for passing the mesh\n  information to parallel machines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full-wave 3D electromagnetic simulations of complex planar devices,\nmultilayer interconnects, and chip packages are presented for wide-band\nfrequency-domain analysis using the finite difference integration technique\ndeveloped in the PETSc software package. Initial reordering of the index\nassignment to the unknowns makes the resulting system matrix diagonally\ndominant. The rearrangement also facilitates the decomposition of large domain\ninto slices for passing the mesh information to different machines. Matrix-free\nmethods are then exploited to minimize the number of element-wise\nmultiplications and memory requirements in the construction of the system of\nlinear equations. Besides, the recipes provide extreme ease of modifications in\nthe kernel of the code. The applicability of different Krylov subspace solvers\nis investigated. The accuracy is checked through comparisons with CST MICROWAVE\nSTUDIO transient solver results. The parallel execution of the compiled code on\nspecific number of processors in multi-core distributed-memory architectures\ndemonstrate high scalability of the computational algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:45:20 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Geranmayeh", "Amir", ""]]}, {"id": "1705.08883", "submitter": "Kalyana Babu Nakshatrala", "authors": "S. H. S. Joodat, K. B. Nakshatrala and R. Ballarini", "title": "Modeling flow in porous media with double porosity/permeability: A\n  stabilized mixed formulation, error analysis, and numerical solutions", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2018.04.004", "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flow of incompressible fluids through porous media plays a crucial role\nin many technological applications such as enhanced oil recovery and geological\ncarbon-dioxide sequestration. The flow within numerous natural and synthetic\nporous materials that contain multiple scales of pores cannot be adequately\ndescribed by the classical Darcy equations. It is for this reason that\nmathematical models for fluid flow in media with multiple scales of pores have\nbeen proposed in the literature. However, these models are analytically\nintractable for realistic problems. In this paper, a stabilized mixed\nfour-field finite element formulation is presented to study the flow of an\nincompressible fluid in porous media exhibiting double porosity/permeability.\nThe stabilization terms and the stabilization parameters are derived in a\nmathematically and thermodynamically consistent manner, and the computationally\nconvenient equal-order interpolation of all the field variables is shown to be\nstable. A systematic error analysis is performed on the resulting stabilized\nweak formulation. Representative problems, patch tests and numerical\nconvergence analyses are performed to illustrate the performance and\nconvergence behavior of the proposed mixed formulation in the discrete setting.\nThe accuracy of numerical solutions is assessed using the mathematical\nproperties satisfied by the solutions of this double porosity/permeability\nmodel. Moreover, it is shown that the proposed framework can perform well under\ntransient conditions and that it can capture well-known instabilities such as\nviscous fingering.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:55:12 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 18:31:23 GMT"}, {"version": "v3", "created": "Sun, 5 Nov 2017 18:30:59 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Joodat", "S. H. S.", ""], ["Nakshatrala", "K. B.", ""], ["Ballarini", "R.", ""]]}, {"id": "1705.10312", "submitter": "Dajiang Zhu", "authors": "Dajiang Zhu, Brandalyn C. Riedel, Neda Jahanshad, Nynke A. Groenewold,\n  Dan J. Stein, Ian H. Gotlib, Matthew D. Sacchet, Danai Dima, James H. Cole,\n  Cynthia H.Y. Fu, Henrik Walter, Ilya M. Veer, Thomas Frodl, Lianne Schmaal,\n  Dick J. Veltman, Paul M. Thompson", "title": "Classification of Major Depressive Disorder via Multi-Site Weighted\n  LASSO Model", "comments": "Accepted by MICCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale collaborative analysis of brain imaging data, in psychiatry and\nneu-rology, offers a new source of statistical power to discover features that\nboost ac-curacy in disease classification, differential diagnosis, and outcome\nprediction. However, due to data privacy regulations or limited accessibility\nto large datasets across the world, it is challenging to efficiently integrate\ndistributed information. Here we propose a novel classification framework\nthrough multi-site weighted LASSO: each site performs an iterative weighted\nLASSO for feature selection separately. Within each iteration, the\nclassification result and the selected features are collected to update the\nweighting parameters for each feature. This new weight is used to guide the\nLASSO process at the next iteration. Only the fea-tures that help to improve\nthe classification accuracy are preserved. In tests on da-ta from five sites\n(299 patients with major depressive disorder (MDD) and 258 normal controls),\nour method boosted classification accuracy for MDD by 4.9% on average. This\nresult shows the potential of the proposed new strategy as an ef-fective and\npractical collaborative platform for machine learning on large scale\ndistributed imaging and biobank data.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 21:19:22 GMT"}, {"version": "v2", "created": "Sat, 3 Jun 2017 18:54:04 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Zhu", "Dajiang", ""], ["Riedel", "Brandalyn C.", ""], ["Jahanshad", "Neda", ""], ["Groenewold", "Nynke A.", ""], ["Stein", "Dan J.", ""], ["Gotlib", "Ian H.", ""], ["Sacchet", "Matthew D.", ""], ["Dima", "Danai", ""], ["Cole", "James H.", ""], ["Fu", "Cynthia H. Y.", ""], ["Walter", "Henrik", ""], ["Veer", "Ilya M.", ""], ["Frodl", "Thomas", ""], ["Schmaal", "Lianne", ""], ["Veltman", "Dick J.", ""], ["Thompson", "Paul M.", ""]]}, {"id": "1705.10437", "submitter": "Nikolaos Sahinidis", "authors": "Nikolaos Ploskas, Christopher Laughman, Arvind U. Raghunathan,\n  Nikolaos V. Sahinidis", "title": "Optimization of circuitry arrangements for heat exchangers using\n  derivative-free optimization", "comments": null, "journal-ref": null, "doi": "10.1016/j.cherd.2017.05.015", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of the refrigerant circuitry can improve a heat exchanger's\nperformance. Design engineers currently choose the refrigerant circuitry\naccording to their experience and heat exchanger simulations. However, the\ndesign of an optimized refrigerant circuitry is difficult. The number of\nrefrigerant circuitry candidates is enormous. Therefore, exhaustive search\nalgorithms cannot be used and intelligent techniques must be developed to\nexplore the solution space efficiently. In this paper, we formulate refrigerant\ncircuitry design as a binary constrained optimization problem. We use\nCoilDesigner, a simulation and design tool of air to refrigerant heat\nexchangers, in order to simulate the performance of different refrigerant\ncircuitry designs. We treat CoilDesigner as a black-box system since the exact\nrelationship of the objective function with the decision variables is not\nexplicit. Derivative-free optimization (DFO) algorithms are suitable for\nsolving this black-box model since they do not require explicit functional\nrepresentations of the objective function and the constraints. The aim of this\npaper is twofold. First, we compare four mixed-integer constrained DFO solvers\nand one box-bounded DFO solver and evaluate their ability to solve a difficult\nindustrially relevant problem. Second, we demonstrate that the proposed\nformulation is suitable for optimizing the circuitry configuration of heat\nexchangers. We apply the DFO solvers to 17 heat exchanger design problems.\nResults show that TOMLAB/glcDirect and TOMLAB/glcSolve can find optimal or\nnear-optimal refrigerant circuitry designs after a relatively small number of\ncircuit simulations.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 02:43:08 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Ploskas", "Nikolaos", ""], ["Laughman", "Christopher", ""], ["Raghunathan", "Arvind U.", ""], ["Sahinidis", "Nikolaos V.", ""]]}, {"id": "1705.10478", "submitter": "Christian Schmidt", "authors": "Christian Schmidt and Ursula van Rienen", "title": "Adaptive Estimation of the Neural Activation Extent in Computational\n  Volume Conductor Models of Deep Brain Stimulation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The aim of this study is to propose an adaptive scheme embedded\ninto an open-source environment for the estimation of the neural activation\nextent during deep brain stimulation and to investigate the feasibility of\napproximating the neural activation extent by thresholds of the field solution.\nMethods: Open-source solutions for solving the field equation in volume\nconductor models of deep brain stimulation and computing the neural activation\nare embedded into a Python package to estimate the neural activation dependent\non the dielectric tissue properties and axon parameters by employing a\nspatially adaptive scheme. Feasibility of the approximation of the neural\nactivation extent by field thresholds is investigated to further reduce the\ncomputational expense. Results: The varying extents of neural activation for\ndifferent patient-specific dielectric properties were estimated with the\nadaptive scheme. The results revealed the strong influence of the dielectric\nproperties of the encapsulation layer in the acute and chronic phase after\nsurgery. The computational time required to determine the neural activation\nextent in each studied model case was substantially reduced. Conclusion: The\nneural activation extent is altered by patient-specific parameters. Threshold\nvalues of the electric potential and electric field norm facilitate a\ncomputationally efficient method to estimate the neural activation extent.\nSignificance: The presented adaptive scheme is able to robustly determine\nneural activation extents and field threshold estimates for varying dielectric\ntissue properties and axon diameters while reducing substantially the\ncomputational expense.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 07:12:21 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 14:50:12 GMT"}, {"version": "v3", "created": "Mon, 7 Aug 2017 18:24:45 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Schmidt", "Christian", ""], ["van Rienen", "Ursula", ""]]}, {"id": "1705.10496", "submitter": "Christian Schmidt", "authors": "Christian Schmidt and Eleanor Dunn and Madeleine Lowery and Ursula van\n  Rienen", "title": "Uncertainty Quantification of Oscillation Suppression during DBS in a\n  Coupled Finite Element and Network Model", "comments": "10 pages", "journal-ref": null, "doi": "10.1109/TNSRE.2016.2608925", "report-no": null, "categories": "q-bio.NC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models of the cortico-basal ganglia network and volume conductor models of\nthe brain can provide insight into the mechanisms of action of deep brain\nstimulation (DBS). In this study, the coupling of a network model, under\nparkinsonian conditions, to the extracellular field distribution obtained from\na three dimensional finite element model of a rodent's brain during DBS is\npresented. This coupled model is used to investigate the influence of\nuncertainty in the electrical properties of brain tissue and encapsulation\ntissue, formed around the electrode after implantation, on the suppression of\noscillatory neural activity during DBS. The resulting uncertainty in this\neffect of DBS on the network activity is quantified using a computationally\nefficient and non-intrusive stochastic approach based on the generalized\nPolynomial Chaos. The results suggest that variations in the electrical\nproperties of brain tissue may have a substantial influence on the level of\nsuppression of oscillatory activity during DBS. Applying a global sensitivity\nanalysis on the suppression of the simulated oscillatory activity showed that\nthe influence of uncertainty in the electrical properties of the encapsulation\ntissue had only a minor influence, in agreement with previous experimental and\ncomputational studies investigating the mechanisms of current-controlled DBS in\nthe literature.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 07:59:49 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Schmidt", "Christian", ""], ["Dunn", "Eleanor", ""], ["Lowery", "Madeleine", ""], ["van Rienen", "Ursula", ""]]}, {"id": "1705.11063", "submitter": "Simon Hill", "authors": "Simon Hill (1 and 3), Daniel Deising (2), Thomas Acher (1), Harald\n  Klein (3), Dieter Bothe (2), Holger Marschall (2) ((1) Linde Engineering AG,\n  Pullach, Germany, (2) Technische Universit\\\"at Darmstadt, Darmstadt, Germany,\n  (3) Technische Universit\\\"at M\\\"unchen, Germany)", "title": "Boundedness-Preserving Implicit Correction of Mesh-Induced Errors for\n  VoF Based Heat and Mass Transfer", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.09.027", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial discretisation of geometrically complex computational domains often\nentails unstructured meshes of general topology for Computational Fluid\nDynamics (CFD). Mesh skewness is then typically encountered causing severe\ndeterioration of the formal order of accuracy of the discretisation, or\nboundedness of the solution, or both. Particularly methods inherently relying\non the accurate and bounded transport of sharp fields suffer from all types of\nmesh-induced skewness errors, namely both non-orthogonality and\nnon-conjunctionality errors. This work is devoted to a boundedness-preserving\nstrategy to correct for skewness errors arising from discretisation of\nadvection and diffusion terms within the context of interfacial heat and mass\ntransfer based on the Volume-of-Fluid methodology. The implementation has been\naccomplished using a second-order finite volume method with support for\nunstructured meshes of general topology. We examine and advance suitable\ncorrections for the finite volume discretisation of a consistent single-field\nmodel, where both accurate and bounded transport due to diffusion and advection\nis crucial. In order to ensure consistency of both the volume fraction and the\nspecies concentration transport, i.e. to avoid artificial heat or species\ntransfer, corrections are studied for both cases. The cross interfacial jump\nand adjacent sharp gradients of species concentration render the correction for\nskewness-induced diffusion and advection errors additionally demanding and has\nnot so far been addressed in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 12:36:38 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 13:42:48 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Hill", "Simon", "", "1 and 3"], ["Deising", "Daniel", ""], ["Acher", "Thomas", ""], ["Klein", "Harald", ""], ["Bothe", "Dieter", ""], ["Marschall", "Holger", ""]]}]