[{"id": "1501.00324", "submitter": "Jonathan Wong", "authors": "Jonathan Wong, Ellen Kuhl, Eric Darve", "title": "A New Sparse Matrix Vector Multiplication GPU Algorithm Designed for\n  Finite Element Problems", "comments": "35 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, graphics processors (GPUs) have been increasingly leveraged in a\nvariety of scientific computing applications. However, architectural\ndifferences between CPUs and GPUs necessitate the development of algorithms\nthat take advantage of GPU hardware. As sparse matrix vector multiplication\n(SPMV) operations are commonly used in finite element analysis, a new SPMV\nalgorithm and several variations are developed for unstructured finite element\nmeshes on GPUs. The effective bandwidth of current GPU algorithms and the newly\nproposed algorithms are measured and analyzed for 15 sparse matrices of varying\nsizes and varying sparsity structures. The effects of optimization and\ndifferences between the new GPU algorithm and its variants are then\nsubsequently studied. Lastly, both new and current SPMV GPU algorithms are\nutilized in the GPU CG Solver in GPU finite element simulations of the heart.\nThese results are then compared against parallel PETSc finite element\nimplementation results. The effective bandwidth tests indicate that the new\nalgorithms compare very favorably with current algorithms for a wide variety of\nsparse matrices and can yield very notable benefits. GPU finite element\nsimulation results demonstrate the benefit of using GPUs for finite element\nanalysis, and also show that the proposed algorithms can yield speedup factors\nup to 12-fold for real finite element applications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 21:57:19 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Wong", "Jonathan", ""], ["Kuhl", "Ellen", ""], ["Darve", "Eric", ""]]}, {"id": "1501.00349", "submitter": "A.Aziz Altowayan", "authors": "A. Aziz Altowayan", "title": "Static Analysis for Biological Systems (BioAmbients)", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I present a summary on some works that utilized static\nanalysis techniques for understanding biological systems. Control flow\nanalysis, context dependent analysis, and other techniques were employed to\ninvestigate the properties of BioAmbients. In this summary report, I tried to\nintroduce the ideas and explain the techniques used in the subject papers. This\nsummary will highlight the biological concepts of BioAmbients.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 06:31:25 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Altowayan", "A. Aziz", ""]]}, {"id": "1501.00419", "submitter": "Christopher Rook J", "authors": "Christopher J. Rook", "title": "Minimizing the Probability of Ruin in Retirement", "comments": "Proofs appendix with full C++ implementation is included", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.CE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retirees who exhaust their savings while still alive are said to experience\nfinancial ruin. These savings are typically grown during the accumulation phase\nthen spent during the retirement decumulation phase. Extensive research into\ninvest-and-harvest decumulation strategies has been conducted, but\nrecommendations differ markedly. This has likely been a source of concern and\nconfusion for the retiree. Our goal is to find what has heretofore been\nelusive, namely an optimal decumulation strategy. Optimality implies that no\nalternate strategy exists or can be constructed that delivers a lower\nprobability of ruin, given a fixed inflation-adjusted withdrawal rate.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 14:55:13 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Rook", "Christopher J.", ""]]}, {"id": "1501.00440", "submitter": "Tatjana Petrov", "authors": "Andreea Beica, Calin Guet, Tatjana Petrov", "title": "Efficient reduction of Kappa models by static inspection of the rule-set", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LO cs.PL q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing genetic circuits, the typical primitives used in major\nexisting modelling formalisms are gene interaction graphs, where edges between\ngenes denote either an activation or inhibition relation. However, when\ndesigning experiments, it is important to be precise about the low-level\nmechanistic details as to how each such relation is implemented. The rule-based\nmodelling language Kappa allows to unambiguously specify mechanistic details\nsuch as DNA binding sites, dimerisation of transcription factors, or\nco-operative interactions. However, such a detailed description comes with\ncomplexity and computationally costly execution. We propose a general method\nfor automatically transforming a rule-based program, by eliminating\nintermediate species and adjusting the rate constants accordingly. Our method\nconsists of searching for those interaction patterns known to be amenable to\nequilibrium approximations (e.g. Michaelis-Menten scheme). The reduced model is\nefficiently obtained by static inspection over the rule-set, and it represents\na particular theoretical limit of the original model. The Bhattacharyya\ndistance is proposed as a metric to estimate the reduction error for a given\nobservable. The tool is tested on a detailed rule-based model of a\n$\\lambda$-phage switch, which lists $96$ rules and $16$ agents. The reduced\nmodel has $11$ rules and $5$ agents, and provides a dramatic reduction in\nsimulation time of several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 17:30:59 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Beica", "Andreea", ""], ["Guet", "Calin", ""], ["Petrov", "Tatjana", ""]]}, {"id": "1501.00607", "submitter": "Kwetishe Danjuma", "authors": "Kwetishe Danjuma and Adenike O. Osofisan", "title": "Evaluation of Predictive Data Mining Algorithms in Erythemato-Squamous\n  Disease Diagnosis", "comments": "10 pages, 3 figures 2 tables", "journal-ref": "IJCSI International Journal of Computer Science Issues, 11(6),\n  85-94 (2014)", "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A lot of time is spent searching for the most performing data mining\nalgorithms applied in clinical diagnosis. The study set out to identify the\nmost performing predictive data mining algorithms applied in the diagnosis of\nErythemato-squamous diseases. The study used Naive Bayes, Multilayer Perceptron\nand J48 decision tree induction to build predictive data mining models on 366\ninstances of Erythemato-squamous diseases datasets. Also, 10-fold\ncross-validation and sets of performance metrics were used to evaluate the\nbaseline predictive performance of the classifiers. The comparative analysis\nshows that the Naive Bayes performed best with accuracy of 97.4%, Multilayer\nPerceptron came out second with accuracy of 96.6%, and J48 came out the worst\nwith accuracy of 93.5%. The evaluation of these classifiers on clinical\ndatasets, gave an insight into the predictive ability of different data mining\nalgorithms applicable in clinical diagnosis especially in the diagnosis of\nErythemato-squamous diseases.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 21:34:35 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Danjuma", "Kwetishe", ""], ["Osofisan", "Adenike O.", ""]]}, {"id": "1501.01392", "submitter": "Pallavi Bdry r", "authors": "Pallavi Badry and Neelima Satyam", "title": "DSSI for pile supported asymmetrical buildings : a review", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the reference of the several documents in the field of soil structure\ninteraction a document of present and past literature has been made with the\nincluding a main focus on interaction of pile supported frames. This study\nfocuses on the complexity and excessive simplification of the model for\nfoundation system and structures, and should be carried forward for its\nsignificance. The review is carried out including analytical, experimental and\nnumerical approaches considered in the past study. The perusal of literature\nreveals that very few studies investigated on asymmetrical buildings supported\non pile foundations. In this paper, an attempt is made to understand research\ncarried out in pile soil structure interaction and research gap along with the\nscope of research has been identified to carry out the present research work.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 08:39:52 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Badry", "Pallavi", ""], ["Satyam", "Neelima", ""]]}, {"id": "1501.01675", "submitter": "Henk Mulder", "authors": "Henk Mulder", "title": "Derivative coordinates for analytic tree fractals and fractal\n  engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.CE math.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We introduce an alternative coordinate system based on derivative polar and\nspherical coordinate functions and construct a root-to-canopy analytic\nformulation for tree fractals. We develop smooth tree fractals and demonstrate\nthe equivalence of their canopies with iterative straight lined tree fractals.\nWe then consider implementation and application of the analytic formulation\nfrom a computational perspective. Finally we formulate the basis for\nconcatenation and composition of fractal trees as a basis for fractal\nengineering of which we provide some examples.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 22:24:32 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Mulder", "Henk", ""]]}, {"id": "1501.01745", "submitter": "Nicholas Guttenberg", "authors": "Nicholas Guttenberg and Matthieu Laneuville and Melissa Ilardo and\n  Nathanael Aubert-Kato", "title": "Transferable measurements of Heredity in models of the Origins of Life", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0140663", "report-no": null, "categories": "q-bio.PE cs.CE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a metric which can be used to compute the amount of heritable\nvariation enabled by a given dynamical system. A distribution of selection\npressures is used such that each pressure selects a particular fixed point via\ncompetitive exclusion in order to determine the corresponding distribution of\npotential fixed points in the population dynamics. This metric accurately\ndetects the number of species present in artificially prepared test systems,\nand furthermore can correctly determine the number of heritable sets in\nclustered transition matrix models in which there are no clearly defined\ngenomes. Finally, we apply our metric to the GARD model and show that it\naccurately reproduces prior measurements of the model's heritability.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 06:53:18 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Laneuville", "Matthieu", ""], ["Ilardo", "Melissa", ""], ["Aubert-Kato", "Nathanael", ""]]}, {"id": "1501.01779", "submitter": "Qixia Yuan", "authors": "Andrzej Mizera, Jun Pang, Qixia Yuan", "title": "Reviving the Two-state Markov Chain Approach (Technical Report)", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Boolean networks (PBNs) is a well-established computational\nframework for modelling biological systems. The steady-state dynamics of PBNs\nis of crucial importance in the study of such systems. However, for large PBNs,\nwhich often arise in systems biology, obtaining the steady-state distribution\nposes a significant challenge. In fact, statistical methods for steady-state\napproximation are the only viable means when dealing with large networks. In\nthis paper, we revive the two-state Markov chain approach presented in the\nliterature. We first identify a problem of generating biased results, due to\nthe size of the initial sample with which the approach needs to start and we\npropose a few heuristics to avoid such a pitfall. Second, we conduct an\nextensive experimental comparison of the two-state Markov chain approach and\nanother approach based on the Skart method and we show that statistically the\ntwo-state Markov chain has a better performance. Finally, we apply this\napproach to a large PBN model of apoptosis in hepatocytes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 09:48:58 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 14:30:17 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2016 13:31:36 GMT"}, {"version": "v4", "created": "Tue, 25 Oct 2016 12:16:38 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Mizera", "Andrzej", ""], ["Pang", "Jun", ""], ["Yuan", "Qixia", ""]]}, {"id": "1501.02287", "submitter": "Boyce Griffith", "authors": "Vittoria Flamini and Abe DeAnda and Boyce E. Griffith", "title": "Immersed boundary-finite element model of fluid-structure interaction in\n  the aortic root", "comments": null, "journal-ref": null, "doi": "10.1007/s00162-015-0374-5", "report-no": null, "categories": "cs.CE math.NA q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been recognized that aortic root elasticity helps to ensure\nefficient aortic valve closure, but our understanding of the functional\nimportance of the elasticity and geometry of the aortic root continues to\nevolve as increasingly detailed in vivo imaging data become available. Herein,\nwe describe fluid-structure interaction models of the aortic root, including\nthe aortic valve leaflets, the sinuses of Valsalva, the aortic annulus, and the\nsinotubular junction, that employ a version of Peskin's immersed boundary (IB)\nmethod with a finite element (FE) description of the structural elasticity. We\ndevelop both an idealized model of the root with three-fold symmetry of the\naortic sinuses and valve leaflets, and a more realistic model that accounts for\nthe differences in the sizes of the left, right, and noncoronary sinuses and\ncorresponding valve cusps. As in earlier work, we use fiber-based models of the\nvalve leaflets, but this study extends earlier IB models of the aortic root by\nemploying incompressible hyperelastic models of the mechanics of the sinuses\nand ascending aorta using a constitutive law fit to experimental data from\nhuman aortic root tissue. In vivo pressure loading is accounted for by a\nbackwards displacement method that determines the unloaded configurations of\nthe root models. Our models yield realistic cardiac output at physiological\npressures, with low transvalvular pressure differences during forward flow,\nminimal regurgitation during valve closure, and realistic pressure loads when\nthe valve is closed during diastole. Further, results from high-resolution\ncomputations demonstrate that IB models of the aortic valve are able to produce\nessentially grid-converged dynamics at practical grid spacings for the\nhigh-Reynolds number flows of the aortic root.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 23:00:57 GMT"}], "update_date": "2016-01-27", "authors_parsed": [["Flamini", "Vittoria", ""], ["DeAnda", "Abe", ""], ["Griffith", "Boyce E.", ""]]}, {"id": "1501.03015", "submitter": "Albrecht Zimmermann", "authors": "Albrecht Zimmermann, Bj\\\"orn Bringmann, Luc De Raedt", "title": "Exploring the efficacy of molecular fragments of different complexity in\n  computational SAR modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important first step in computational SAR modeling is to transform the\ncompounds into a representation that can be processed by predictive modeling\ntechniques. This is typically a feature vector where each feature indicates the\npresence or absence of a molecular fragment. While the traditional approach to\nSAR modeling employed size restricted fingerprints derived from path fragments,\nmuch research in recent years focussed on mining more complex graph based\nfragments. Today, there seems to be a growing consensus in the data mining\ncommunity that these more expressive fragments should be more useful. We\nquestion this consensus and show experimentally that fragments of low\ncomplexity, i.e. sequences, perform better than equally large sets of more\ncomplex ones, an effect we explain by pairwise correlation among fragments and\nthe ability of a fragment set to encode compounds from different classes\ndistinctly. The size restriction on these sets is based on ordering the\nfragments by class-correlation scores. In addition, we also evaluate the\neffects of using a significance value instead of a length restriction for path\nfragments and find a significant reduction in the number of features with\nlittle loss in performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 14:24:58 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Zimmermann", "Albrecht", ""], ["Bringmann", "Bj\u00f6rn", ""], ["De Raedt", "Luc", ""]]}, {"id": "1501.03044", "submitter": "Wei  Lu", "authors": "Wei Lu", "title": "Effects of Data Resolution and Human Behavior on Large Scale Evacuation\n  Simulations", "comments": "PhD dissertation. UT Knoxville. 130 pages, 37 figures, 8 tables.\n  University of Tennessee, 2013. http://trace.tennessee.edu/utk_graddiss/2595", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CE", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Traffic Analysis Zones (TAZ) based macroscopic simulation studies are mostly\napplied in evacuation planning and operation areas. The large size in TAZ and\naggregated information of macroscopic simulation underestimate the real\nevacuation performance. To take advantage of the high resolution demographic\ndata LandScan USA (the zone size is much smaller than TAZ) and agent-based\nmicroscopic traffic simulation models, many new problems appeared and novel\nsolutions are needed. A series of studies are conducted using LandScan USA\nPopulation Cells (LPC) data for evacuation assignments with different network\nconfigurations, travel demand models, and travelers compliance behavior.\n  First, a new Multiple Source Nearest Destination Shortest Path (MSNDSP)\nproblem is defined for generating Origin Destination matrix in evacuation\nassignments when using LandScan dataset. Second, a new agent-based traffic\nassignment framework using LandScan and TRANSIMS modules is proposed for\nevacuation planning and operation study. Impact analysis on traffic analysis\narea resolutions (TAZ vs LPC), evacuation start times (daytime vs nighttime),\nand departure time choice models (normal S shape model vs location based model)\nare studied. Third, based on the proposed framework, multi-scale network\nconfigurations (two levels of road networks and two scales of zone sizes) and\nthree routing schemes (shortest network distance, highway biased, and shortest\nstraight-line distance routes) are implemented for the evacuation performance\ncomparison studies. Fourth, to study the impact of human behavior under\nevacuation operations, travelers compliance behavior with compliance levels\nfrom total complied to total non-complied are analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 19:49:52 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Lu", "Wei", ""]]}, {"id": "1501.03396", "submitter": "Davide Fransos", "authors": "Luca Bruno and Davide Fransos", "title": "Sand transverse dune aerodynamics: 3D Coherent Flow Structures from a\n  computational study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.CE physics.ao-ph physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The engineering interest about dune fields is dictated by the their\ninteraction with a number of human infrastructures in arid environments. Sand\ndunes dynamics is dictated by wind and its ability to induce sand erosion,\ntransport and deposition. A deep understanding of dune aerodynamics serves then\nto ground effective strategies for the protection of human infrastructures from\nsand, the so-called sand mitigation. Because of their simple geometry and their\nfrequent occurrence in desert area, transverse sand dunes are usually adopted\nin literature as a benchmark to investigate dune aerodynamics by means of both\ncomputational or experimental approaches, usually in nominally 2D setups. The\npresent study aims at evaluating 3D flow features in the wake of a idealised\ntransverse dune, if any, under different nominally 2D setup conditions by means\nof computational simulations and to compare the obtained results with\nexperimental measurements available in literature.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 13:31:40 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 12:03:15 GMT"}], "update_date": "2015-02-05", "authors_parsed": [["Bruno", "Luca", ""], ["Fransos", "Davide", ""]]}, {"id": "1501.03461", "submitter": "Ariful Azad", "authors": "Ariful Azad", "title": "An Algorithmic Pipeline for Analyzing Multi-parametric Flow Cytometry\n  Data", "comments": "PhD dissertation, May 2014, Purdue University", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow cytometry (FC) is a single-cell profiling platform for measuring the\nphenotypes of individual cells from millions of cells in biological samples. FC\nemploys high-throughput technologies and generates high-dimensional data, and\nhence algorithms for analyzing the data represent a bottleneck. This\ndissertation addresses several computational challenges arising in modern\ncytometry while mining information from high-dimensional and high-content\nbiological data. A collection of combinatorial and statistical algorithms for\nlocating, matching, prototyping, and classifying cellular populations from\nmulti-parametric FC data is developed.\n  The algorithmic pipeline, flowMatch, developed in this dissertation consists\nof five well-defined algorithmic modules to (1) transform data to stabilize\nwithin-population variance, (2) identify cell populations by robust clustering\nalgorithms, (3) register cell populations across samples, (4) encapsulate a\nclass of samples with templates, and (5) classify samples based on their\nsimilarity with the templates. Components of flowMatch can work independently\nor collaborate with each other to perform the complete data analysis. flowMatch\nis made available as an open-source R package in Bioconductor.\n  We have employed flowMatch for classifying leukemia samples, evaluating the\nphosphorylation effects on T cells, classifying healthy immune profiles, and\nclassifying the vaccination status of HIV patients. In these analyses, the\npipeline is able to reach biologically meaningful conclusions quickly and\nefficiently with the automated algorithms. The algorithms included in flowMatch\ncan also be applied to problems outside of flow cytometry such as in microarray\ndata analysis and image recognition. Therefore, this dissertation contributes\nto the solution of fundamental problems in computational cytometry and related\ndomains.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 20:02:13 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Azad", "Ariful", ""]]}, {"id": "1501.03994", "submitter": "Ha Bui", "authors": "Yilin Gui, Ha H. Bui, Jayantha Kodikara", "title": "Numerical modelling of sandstone uniaxial compression test using a\n  mix-mode cohesive fracture model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mix-mode cohesive fracture model considering tension, compression and shear\nmaterial behaviour is presented, which has wide applications to geotechnical\nproblems. The model considers both elastic and inelastic displacements.\nInelastic displacement comprises fracture and plastic displacements. The norm\nof inelastic displacement is used to control the fracture behaviour. Meantime,\na failure function describing the fracture strength is proposed. Using the\ninternal programming FISH, the cohesive fracture model is programmed into a\nhybrid distinct element algorithm as encoded in Universal Distinct Element Code\n(UDEC). The model is verified through uniaxial tension and direct shear tests.\nThe developed model is then applied to model the behaviour of a uniaxial\ncompression test on Gosford sandstone. The modelling results indicate that the\nproposed cohesive fracture model is capable of simulating combined failure\nbehaviour applicable to rock.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 14:45:29 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Gui", "Yilin", ""], ["Bui", "Ha H.", ""], ["Kodikara", "Jayantha", ""]]}, {"id": "1501.04000", "submitter": "Ha Bui", "authors": "H.H. Bui, J.A. Kodikara, R. Pathegama, A. Bouazza, A. Haque", "title": "Large deformation and post-failure simulations of segmental retaining\n  walls using mesh-free method (SPH)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical methods are extremely useful in gaining insights into the behaviour\nof reinforced soil retaining walls. However, traditional numerical approaches\nsuch as limit equilibrium or finite element methods are unable to simulate\nlarge deformation and post-failure behaviour of soils and retaining wall blocks\nin the reinforced soil retaining walls system. To overcome this limitation, a\nnovel numerical approach is developed aiming to predict accurately the large\ndeformation and post-failure behaviour of soil and segmental wall blocks.\nHerein, soil is modelled using an elasto-plastic constitutive model, while\nsegmental wall blocks are assumed rigid with full degrees of freedom. A soft\ncontact model is proposed to simulate the interaction between soil-block and\nblock-block. A two dimensional experiment of reinforced soil retaining walls\ncollapse was conducted to verify the numerical results. It is shown that the\nproposed method can simulate satisfactory post-failure behaviour of segmental\nwall blocks in reinforced soil retaining wall systems. The comparison showed\nthat the proposed method can provide satisfactory agreement with experiments.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 14:52:42 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Bui", "H. H.", ""], ["Kodikara", "J. A.", ""], ["Pathegama", "R.", ""], ["Bouazza", "A.", ""], ["Haque", "A.", ""]]}, {"id": "1501.04006", "submitter": "Ha Bui", "authors": "P. Rajeev, Ha H. Bui, N. Sivakugan", "title": "Seismic Earth Pressure Development in Sheet Pile Retaining Walls: A\n  Numerical Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of retaining walls requires the complete knowledge of the earth\npressure distribution behind the wall. Due to the complex soil-structure\neffect, the estimation of earth pressure is not an easy task; even in the\nstatic case. The problem becomes even more complex for the dynamic (i.e.,\nseismic) analysis and design of retaining walls. Several earth pressure models\nhave been developed over the years to integrate the dynamic earth pressure with\nthe static earth pressure and to improve the design of retaining wall in\nseismic regions. Among all the models, MononobeOkabe (M-O) method is commonly\nused to estimate the magnitude of seismic earth pressures in retaining walls\nand is adopted in design practices around the world (e.g., EuroCode and\nAustralian Standards). However, the M-O method has several drawbacks and does\nnot provide reliable estimate of the earth pressure in many instances. This\nstudy investigates the accuracy of the M-O method to predict the dynamic earth\npressure in sheet pile wall. A 2D plane strain finite element model of the\nwall-soil system was developed in DIANA. The backfill soil was modelled with\nMohr-Coulomb failure criterion while the wall was assumed behave elastically.\nThe numerically predicted dynamic earth pressure was compared with the M-O\nmodel prediction. Further, the point of application of total dynamic force was\ndetermined and compared with the static case. Finally, the applicability of M-O\nmethods to compute the seismic earth pressure was discussed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 15:03:20 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Rajeev", "P.", ""], ["Bui", "Ha H.", ""], ["Sivakugan", "N.", ""]]}, {"id": "1501.04659", "submitter": "Lorenzo Livi", "authors": "Francesca Possemato, Maurizio Paschero, Lorenzo Livi, Antonello Rizzi,\n  Alireza Sadeghian", "title": "On the impact of topological properties of smart grids in power losses\n  optimization problems", "comments": "35 pages, 38 references", "journal-ref": null, "doi": "10.1016/j.ijepes.2015.12.022", "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power losses reduction is one of the main targets for any electrical energy\ndistribution company. In this paper, we face the problem of joint optimization\nof both topology and network parameters in a real smart grid. We consider a\nportion of the Italian electric distribution network managed by the ACEA\nDistribuzione S.p.A. located in Rome. We perform both the power factor\ncorrection (PFC) for tuning the generators and the distributed feeder\nreconfiguration (DFR) to set the state of the breakers. This joint optimization\nproblem is faced considering a suitable objective function and by adopting\ngenetic algorithms as global optimization strategy. We analyze admissible\nnetwork configurations, showing that some of these violate constraints on\ncurrent and voltage at branches and nodes. Such violations depend only on pure\ntopological properties of the configurations. We perform tests by feeding the\nsimulation environment with real data concerning hourly samples of dissipated\nand generated active and reactive power values of the ACEA smart grid. Results\nshow that removing the configurations violating the electrical constraints from\nthe solution space leads to interesting improvements in terms of power loss\nreduction. To conclude, we provide also an electrical interpretation of the\nphenomenon using graph-based pattern analysis techniques.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 22:19:16 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 20:29:58 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Possemato", "Francesca", ""], ["Paschero", "Maurizio", ""], ["Livi", "Lorenzo", ""], ["Rizzi", "Antonello", ""], ["Sadeghian", "Alireza", ""]]}, {"id": "1501.04682", "submitter": "Peter Sarlin", "authors": "Markus Holopainen, Peter Sarlin", "title": "Toward robust early-warning models: A horse race, ensembles and model\n  uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE q-fin.CP q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents first steps toward robust models for crisis prediction.\nWe conduct a horse race of conventional statistical methods and more recent\nmachine learning methods as early-warning models. As individual models are in\nthe literature most often built in isolation of other methods, the exercise is\nof high relevance for assessing the relative performance of a wide variety of\nmethods. Further, we test various ensemble approaches to aggregating the\ninformation products of the built models, providing a more robust basis for\nmeasuring country-level vulnerabilities. Finally, we provide approaches to\nestimating model uncertainty in early-warning exercises, particularly model\nperformance uncertainty and model output uncertainty. The approaches put\nforward in this paper are shown with Europe as a playground. Generally, our\nresults show that the conventional statistical approaches are outperformed by\nmore advanced machine learning methods, such as k-nearest neighbors and neural\nnetworks, and particularly by model aggregation approaches through ensemble\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 00:18:18 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 15:55:18 GMT"}, {"version": "v3", "created": "Fri, 1 Apr 2016 14:22:36 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Holopainen", "Markus", ""], ["Sarlin", "Peter", ""]]}, {"id": "1501.04709", "submitter": "Boleslaw Szymanski", "authors": "Chris Gaiteri, Mingming Chen, Boleslaw Szymanski, Konstantin Kuzmin,\n  Jierui Xie, Changkyu Lee, Timothy Blanche, Elias Chaibub Neto, Su-Chun Huang,\n  Thomas Grabowski, Tara Madhyastha, Vitalina Komashko", "title": "Identifying robust communities and multi-community nodes by combining\n  top-down and bottom-up approaches to clustering", "comments": null, "journal-ref": "Scientific Reports 5, Article number: 16361 (2015)", "doi": "10.1038/srep16361", "report-no": null, "categories": "cs.CE cs.SI physics.bio-ph q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological functions are carried out by groups of interacting molecules,\ncells or tissues, known as communities. Membership in these communities may\noverlap when biological components are involved in multiple functions. However,\ntraditional clustering methods detect non-overlapping communities. These\ndetected communities may also be unstable and difficult to replicate, because\ntraditional methods are sensitive to noise and parameter settings. These\naspects of traditional clustering methods limit our ability to detect\nbiological communities, and therefore our ability to understand biological\nfunctions.\n  To address these limitations and detect robust overlapping biological\ncommunities, we propose an unorthodox clustering method called SpeakEasy which\nidentifies communities using top-down and bottom-up approaches simultaneously.\nSpecifically, nodes join communities based on their local connections, as well\nas global information about the network structure. This method can quantify the\nstability of each community, automatically identify the number of communities,\nand quickly cluster networks with hundreds of thousands of nodes.\n  SpeakEasy shows top performance on synthetic clustering benchmarks and\naccurately identifies meaningful biological communities in a range of datasets,\nincluding: gene microarrays, protein interactions, sorted cell populations,\nelectrophysiology and fMRI brain imaging.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 04:15:17 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 00:30:22 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Gaiteri", "Chris", ""], ["Chen", "Mingming", ""], ["Szymanski", "Boleslaw", ""], ["Kuzmin", "Konstantin", ""], ["Xie", "Jierui", ""], ["Lee", "Changkyu", ""], ["Blanche", "Timothy", ""], ["Neto", "Elias Chaibub", ""], ["Huang", "Su-Chun", ""], ["Grabowski", "Thomas", ""], ["Madhyastha", "Tara", ""], ["Komashko", "Vitalina", ""]]}, {"id": "1501.04741", "submitter": "{\\L}ukasz {\\L}aniewski-Wo{\\l}{\\l}k", "authors": "{\\L}ukasz {\\L}aniewski-Wo{\\l}{\\l}k, Jacek Rokicki", "title": "Adjoint Lattice Boltzmann for Topology Optimization on multi-GPU\n  architecture", "comments": "18 pages, 11 figures, 3 tables, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a topology optimization technique applicable to a\nbroad range of flow design problems. We propose also a discrete adjoint\nformulation effective for a wide class of Lattice Boltzmann Methods (LBM). This\nadjoint formulation is used to calculate sensitivity of the LBM solution to\nseveral type of parameters, both global and local. The numerical scheme for\nsolving the adjoint problem has many properties of the original system,\nincluding locality and explicit time-stepping. Thus it is possible to integrate\nit with the standard LBM solver, allowing for straightforward and efficient\nparallelization (overcoming limitations typical for discrete adjoint solvers).\nThis approach is successfully used for the channel flow to design a\nfree-topology mixer and a heat exchanger. Both resulting geometries being very\ncomplex maximize their objective functions, while keeping viscous losses at\nacceptable level.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 09:06:34 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["\u0141aniewski-Wo\u0142\u0142k", "\u0141ukasz", ""], ["Rokicki", "Jacek", ""]]}, {"id": "1501.04784", "submitter": "Francisco Javier Ramirez Gil", "authors": "Francisco Javier Ram\\'irez-Gil, Marcos de Sales Guerra Tsuzuki and\n  Wilfredo Montealegre-Rubio", "title": "Global finite element matrix construction based on a CPU-GPU\n  implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CE cs.DC cs.PF math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finite element method (FEM) has several computational steps to\nnumerically solve a particular problem, to which many efforts have been\ndirected to accelerate the solution stage of the linear system of equations.\nHowever, the finite element matrix construction, which is also time-consuming\nfor unstructured meshes, has been less investigated. The generation of the\nglobal finite element matrix is performed in two steps, computing the local\nmatrices by numerical integration and assembling them into a global system,\nwhich has traditionally been done in serial computing. This work presents a\nfast technique to construct the global finite element matrix that arises by\nsolving the Poisson's equation in a three-dimensional domain. The proposed\nmethodology consists in computing the numerical integration, due to its\nintrinsic parallel opportunities, in the graphics processing unit (GPU) and\ncomputing the matrix assembly, due to its intrinsic serial operations, in the\ncentral processing unit (CPU). In the numerical integration, only the lower\ntriangular part of each local stiffness matrix is computed thanks to its\nsymmetry, which saves GPU memory and computing time. As a result of symmetry,\nthe global sparse matrix also contains non-zero elements only in its lower\ntriangular part, which reduces the assembly operations and memory usage. This\nmethodology allows generating the global sparse matrix from any unstructured\nfinite element mesh size on GPUs with little memory capacity, only limited by\nthe CPU memory.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 12:41:28 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Ram\u00edrez-Gil", "Francisco Javier", ""], ["Tsuzuki", "Marcos de Sales Guerra", ""], ["Montealegre-Rubio", "Wilfredo", ""]]}, {"id": "1501.04986", "submitter": "Istv\\'an Mikl\\'os", "authors": "Joseph L. Herman and Adrienn Szab\\'o and Instv\\'an Mikl\\'os and Jotun\n  Hein", "title": "Approximate statistical alignment by iterative sampling of substitution\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline a procedure for jointly sampling substitution matrices and\nmultiple sequence alignments, according to an approximate posterior\ndistribution, using an MCMC-based algorithm. This procedure provides an\nefficient and simple method by which to generate alternative alignments\naccording to their expected accuracy, and allows appropriate parameters for\nsubstitution matrices to be selected in an automated fashion. In the cases\nconsidered here, the sampled alignments with the highest likelihood have an\naccuracy consistently higher than alignments generated using the standard\nBLOSUM62 matrix.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 09:19:51 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Herman", "Joseph L.", ""], ["Szab\u00f3", "Adrienn", ""], ["Mikl\u00f3s", "Instv\u00e1n", ""], ["Hein", "Jotun", ""]]}, {"id": "1501.05290", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves", "title": "Managing large-scale scientific hypotheses as uncertain and\n  probabilistic data", "comments": "145 pages, 61 figures, 1 table. PhD thesis, National Laboratory for\n  Scientific Computing (LNCC), Brazil, February 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of the paradigm shift that makes science ever more data-driven, in\nthis thesis we propose a synthesis method for encoding and managing large-scale\ndeterministic scientific hypotheses as uncertain and probabilistic data.\n  In the form of mathematical equations, hypotheses symmetrically relate\naspects of the studied phenomena. For computing predictions, however,\ndeterministic hypotheses can be abstracted as functions. We build upon Simon's\nnotion of structural equations in order to efficiently extract the (so-called)\ncausal ordering between variables, implicit in a hypothesis structure (set of\nmathematical equations).\n  We show how to process the hypothesis predictive structure effectively\nthrough original algorithms for encoding it into a set of functional\ndependencies (fd's) and then performing causal reasoning in terms of acyclic\npseudo-transitive reasoning over fd's. Such reasoning reveals important causal\ndependencies implicit in the hypothesis predictive data and guide our synthesis\nof a probabilistic database. Like in the field of graphical models in AI, such\na probabilistic database should be normalized so that the uncertainty arisen\nfrom competing hypotheses is decomposed into factors and propagated properly\nonto predictive data by recovering its joint probability distribution through a\nlossless join. That is motivated as a design-theoretic principle for\ndata-driven hypothesis management and predictive analytics.\n  The method is applicable to both quantitative and qualitative deterministic\nhypotheses and demonstrated in realistic use cases from computational science.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 20:46:23 GMT"}, {"version": "v2", "created": "Thu, 12 Feb 2015 20:52:29 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""]]}, {"id": "1501.05480", "submitter": "Roberto M\\'inguez", "authors": "Roberto Minguez and Raquel Garcia-Bertrand", "title": "Robust Transmission Network Expansion Planning in Energy Systems:\n  Improving Computational Performance", "comments": "32 pages and 2 figures in European Journal of Operational Research\n  2014", "journal-ref": "European Journal of Operational Research 248 (1), 21-32, 2016", "doi": "10.1016/j.ejor.2015.06.068", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent advances in solving the problem of transmission network expansion\nplanning, the use of robust optimization techniques has been put forward, as an\nalternative to stochastic mathematical programming methods, to make the problem\ntractable in realistic systems. Different sources of uncertainty have been\nconsidered, mainly related to the capacity and availability of generation\nfacilities and demand, and making use of adaptive robust optimization models.\nThe mathematical formulations for these models give rise to three-level\nmixed-integer optimization problems, which are solved using different\nstrategies. Although it is true that these robust methods are more efficient\nthan their stochastic counterparts, it is also correct that solution times for\nmixed-integer linear programming problems increase exponentially with respect\nto the size of the problem. Because of this, practitioners and system operators\nneed to use computationally efficient methods when solving this type of\nproblem. In this paper the issue of improving computational performance by\ntaking different features from existing algorithms is addressed. In particular,\nwe replace the lower-level problem with a dual one, and solve the resulting\nbi-level problem using a primal cutting plane algorithm within a decomposition\nscheme. By using this alternative and simple approach, the computing time for\nsolving transmission expansion planning problems has been reduced drastically.\nNumerical results in an illustrative example, the IEEE-24 and IEEE 118-bus test\nsystems demonstrate that the algorithm is superior in terms of computational\nperformance with respect to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 12:49:51 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2015 08:13:24 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Minguez", "Roberto", ""], ["Garcia-Bertrand", "Raquel", ""]]}, {"id": "1501.05739", "submitter": "Claudio Bosco", "authors": "Claudio Bosco and Graham Sander", "title": "Estimating the effects of water-induced shallow landslides on soil\n  erosion", "comments": "14 pages, 4 figures, 1 table, published in IEEE Earthzine 2014 Vol. 7\n  Issue 2, 910137+ 2nd quarter theme. Geospatial Semantic Array Programming.\n  Available: http://www.earthzine.org/?p=910137", "journal-ref": "IEEE Earthzine, vol. 7, no. 2, pp. 910137+, 2014", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rainfall induced landslides and soil erosion are part of a complex system of\nmultiple interacting processes, and both are capable of significantly affecting\nsediment budgets. These sediment mass movements also have the potential to\nsignificantly impact on a broad network of ecosystems health, functionality and\nthe services they provide. To support the integrated assessment of these\nprocesses it is necessary to develop reliable modelling architectures. This\npaper proposes a semi-quantitative integrated methodology for a robust\nassessment of soil erosion rates in data poor regions affected by landslide\nactivity. It combines heuristic, empirical and probabilistic approaches. This\nproposed methodology is based on the geospatial semantic array programming\nparadigm and has been implemented on a catchment scale methodology using\nGeographic Information Systems (GIS) spatial analysis tools and GNU Octave. The\nintegrated data-transformation model relies on a modular architecture, where\nthe information flow among modules is constrained by semantic checks. In order\nto improve computational reproducibility, the geospatial data transformations\nimplemented in ESRI ArcGis are made available in the free software GRASS GIS.\nThe proposed modelling architecture is flexible enough for future\ntransdisciplinary scenario analysis to be more easily designed. In particular,\nthe architecture might contribute as a novel component to simplify future\nintegrated analyses of the potential impact of wildfires or vegetation types\nand distributions, on sediment transport from water induced landslides and\nerosion.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 08:44:13 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Bosco", "Claudio", ""], ["Sander", "Graham", ""]]}, {"id": "1501.05810", "submitter": "Tobias Preclik", "authors": "Tobias Preclik and Ulrich R\\\"ude", "title": "Ultrascale Simulations of Non-smooth Granular Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents new algorithms for massively parallel granular dynamics\nsimulations on distributed memory architectures using a domain partitioning\napproach. Collisions are modelled with hard contacts in order to hide their\nmicro-dynamics and thus to extend the time and length scales that can be\nsimulated. The multi-contact problem is solved using a non-linear block\nGauss-Seidel method that is conforming to the subdomain structure. The parallel\nalgorithms employ a sophisticated protocol between processors that delegate\nalgorithmic tasks such as contact treatment and position integration uniquely\nand robustly to the processors. Communication overhead is minimized through\naggressive message aggregation, leading to excellent strong and weak scaling.\nThe robustness and scalability is assessed on three clusters including two\npeta-scale supercomputers with up to 458752 processor cores. The simulations\ncan reach unprecedented resolution of up to ten billion non-spherical particles\nand contacts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 14:30:21 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Preclik", "Tobias", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1501.05973", "submitter": "Ashish Kapoor", "authors": "Ashish Kapoor, E. Paxon Frady, Stefanie Jegelka, William B. Kristan\n  and Eric Horvitz", "title": "Inferring and Learning from Neuronal Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study methods for inferring and learning from\ncorrespondences among neurons. The approach enables alignment of data from\ndistinct multiunit studies of nervous systems. We show that the methods for\ninferring correspondences combine data effectively from cross-animal studies to\nmake joint inferences about behavioral decision making that are not possible\nwith the data from a single animal. We focus on data collection, machine\nlearning, and prediction in the representative and long-studied invertebrate\nnervous system of the European medicinal leech. Acknowledging the computational\nintractability of the general problem of identifying correspondences among\nneurons, we introduce efficient computational procedures for matching neurons\nacross animals. The methods include techniques that adjust for missing cells or\nadditional cells in the different data sets that may reflect biological or\nexperimental variation. The methods highlight the value harnessing inference\nand learning in new kinds of computational microscopes for multiunit\nneurobiological studies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 22:29:13 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 08:23:24 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Kapoor", "Ashish", ""], ["Frady", "E. Paxon", ""], ["Jegelka", "Stefanie", ""], ["Kristan", "William B.", ""], ["Horvitz", "Eric", ""]]}, {"id": "1501.05992", "submitter": "Stephen Ord", "authors": "S. M. Ord, B. Crosse, D. Emrich, D. Pallot, R. B. Wayth, M. A. Clark,\n  S. E. Tremblay, W. Arcus, D. Barnes, M. Bell, G. Bernardi, N. D. R. Bhat, J.\n  D. Bowman, F. Briggs, J. D. Bunton, R. J. Cappallo, B. E. Corey, A. A.\n  Deshpande, L. deSouza, A. Ewell-Wice, L. Feng, R. Goeke, L. J. Greenhill, B.\n  J. Hazelton, D. Herne, J. N. Hewitt, L. Hindson, H. Hurley-Walker, D. Jacobs,\n  M. Johnston-Hollitt, D. L. Kaplan, J. C. Kasper, B. B. Kincaid, R. Koenig, E.\n  Kratzenberg, N. Kudryavtseva, E. Lenc, C. J. Lonsdale, M. J. Lynch, B.\n  McKinley, S. R. McWhirter, D. A. Mitchell, M. F. Morales, E. Morgan, D.\n  Oberoi, A. Offringa, J. Pathikulangara, B. Pindor, T. Prabu, P. Procopio, R.\n  A. Remillard, J. Riding, A. E. E. Rogers, A. Roshi, J. E. Salah, R. J. Sault,\n  N. Udaya Shankar, K. S. Srivani, J. Stevens, R. Subrahmanyan, S. J. Tingay,\n  M. Waterson, R. L. Webster, A. R. Whitney, A. Williams, C. L. Williams, J. S.\n  B. Wyithe", "title": "The Murchison Widefield Array Correlator", "comments": "17 pages, 9 figures. Accepted for publication in PASA. Some figures\n  altered to meet astro-ph submission requirements", "journal-ref": null, "doi": "10.1017/pasa.2015.5", "report-no": null, "categories": "astro-ph.IM cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Murchison Widefield Array (MWA) is a Square Kilometre Array (SKA)\nPrecursor. The telescope is located at the Murchison Radio--astronomy\nObservatory (MRO) in Western Australia (WA). The MWA consists of 4096 dipoles\narranged into 128 dual polarisation aperture arrays forming a connected element\ninterferometer that cross-correlates signals from all 256 inputs. A hybrid\napproach to the correlation task is employed, with some processing stages being\nperformed by bespoke hardware, based on Field Programmable Gate Arrays (FPGAs),\nand others by Graphics Processing Units (GPUs) housed in general purpose rack\nmounted servers. The correlation capability required is approximately 8 TFLOPS\n(Tera FLoating point Operations Per Second). The MWA has commenced operations\nand the correlator is generating 8.3 TB/day of correlation products, that are\nsubsequently transferred 700 km from the MRO to Perth (WA) in real-time for\nstorage and offline processing. In this paper we outline the correlator design,\nsignal path, and processing elements and present the data format for the\ninternal and external interfaces.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jan 2015 02:42:39 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ord", "S. M.", ""], ["Crosse", "B.", ""], ["Emrich", "D.", ""], ["Pallot", "D.", ""], ["Wayth", "R. B.", ""], ["Clark", "M. A.", ""], ["Tremblay", "S. E.", ""], ["Arcus", "W.", ""], ["Barnes", "D.", ""], ["Bell", "M.", ""], ["Bernardi", "G.", ""], ["Bhat", "N. D. R.", ""], ["Bowman", "J. D.", ""], ["Briggs", "F.", ""], ["Bunton", "J. D.", ""], ["Cappallo", "R. J.", ""], ["Corey", "B. E.", ""], ["Deshpande", "A. A.", ""], ["deSouza", "L.", ""], ["Ewell-Wice", "A.", ""], ["Feng", "L.", ""], ["Goeke", "R.", ""], ["Greenhill", "L. J.", ""], ["Hazelton", "B. J.", ""], ["Herne", "D.", ""], ["Hewitt", "J. N.", ""], ["Hindson", "L.", ""], ["Hurley-Walker", "H.", ""], ["Jacobs", "D.", ""], ["Johnston-Hollitt", "M.", ""], ["Kaplan", "D. L.", ""], ["Kasper", "J. C.", ""], ["Kincaid", "B. B.", ""], ["Koenig", "R.", ""], ["Kratzenberg", "E.", ""], ["Kudryavtseva", "N.", ""], ["Lenc", "E.", ""], ["Lonsdale", "C. J.", ""], ["Lynch", "M. J.", ""], ["McKinley", "B.", ""], ["McWhirter", "S. R.", ""], ["Mitchell", "D. A.", ""], ["Morales", "M. F.", ""], ["Morgan", "E.", ""], ["Oberoi", "D.", ""], ["Offringa", "A.", ""], ["Pathikulangara", "J.", ""], ["Pindor", "B.", ""], ["Prabu", "T.", ""], ["Procopio", "P.", ""], ["Remillard", "R. A.", ""], ["Riding", "J.", ""], ["Rogers", "A. E. E.", ""], ["Roshi", "A.", ""], ["Salah", "J. E.", ""], ["Sault", "R. J.", ""], ["Shankar", "N. Udaya", ""], ["Srivani", "K. S.", ""], ["Stevens", "J.", ""], ["Subrahmanyan", "R.", ""], ["Tingay", "S. J.", ""], ["Waterson", "M.", ""], ["Webster", "R. L.", ""], ["Whitney", "A. R.", ""], ["Williams", "A.", ""], ["Williams", "C. L.", ""], ["Wyithe", "J. S. B.", ""]]}, {"id": "1501.06102", "submitter": "Terrence Adams", "authors": "Terrence Adams", "title": "Development of a Big Data Framework for Connectomic Research", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines research and development of a new Hadoop-based\narchitecture for distributed processing and analysis of electron microscopy of\nbrains. We show development of a new C++ library for implementation of 3D image\nanalysis techniques, and deployment in a distributed map/reduce framework. We\ndemonstrate our new framework on a subset of the Kasthuri11 dataset from the\nOpen Connectome Project.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 01:42:09 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Adams", "Terrence", ""]]}, {"id": "1501.06326", "submitter": "Blesson Varghese", "authors": "Blesson Varghese", "title": "The GPU vs Phi Debate: Risk Analytics Using Many-Core Computing", "comments": "A modified version of this article is accepted to the Computers and\n  Electrical Engineering Journal under the title - \"The Hardware Accelerator\n  Debate: A Financial Risk Case Study Using Many-Core Computing\"; Blesson\n  Varghese, \"The Hardware Accelerator Debate: A Financial Risk Case Study Using\n  Many-Core Computing,\" Computers and Electrical Engineering, 2015", "journal-ref": null, "doi": "10.1016/j.compeleceng.2015.01.012", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk of reinsurance portfolios covering globally occurring natural\ncatastrophes, such as earthquakes and hurricanes, is quantified by employing\nsimulations. These simulations are computationally intensive and require large\namounts of data to be processed. The use of many-core hardware accelerators,\nsuch as the Intel Xeon Phi and the NVIDIA Graphics Processing Unit (GPU), are\ndesirable for achieving high-performance risk analytics. In this paper, we set\nout to investigate how accelerators can be employed in risk analytics, focusing\non developing parallel algorithms for Aggregate Risk Analysis, a simulation\nwhich computes the Probable Maximum Loss of a portfolio taking both primary and\nsecondary uncertainties into account. The key result is that both hardware\naccelerators are useful in different contexts; without taking data transfer\ntimes into account the Phi had lowest execution times when used independently\nand the GPU along with a host in a hybrid platform yielded best performance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 10:52:18 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Varghese", "Blesson", ""]]}, {"id": "1501.06396", "submitter": "Ben Teng", "authors": "Ben Teng, Can Yang, Jiming Liu, Zhipeng Cai and Xiang Wan", "title": "Exploring the genetic patterns of complex diseases via the integrative\n  genome-wide approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Genome-wide association studies (GWASs), which assay more than a\nmillion single nucleotide polymorphisms (SNPs) in thousands of individuals,\nhave been widely used to identify genetic risk variants for complex diseases.\nHowever, most of the variants that have been identified contribute relatively\nsmall increments of risk and only explain a small portion of the genetic\nvariation in complex diseases. This is the so-called missing heritability\nproblem. Evidence has indicated that many complex diseases are genetically\nrelated, meaning these diseases share common genetic risk variants. Therefore,\nexploring the genetic correlations across multiple related studies could be a\npromising strategy for removing spurious associations and identifying\nunderlying genetic risk variants, and thereby uncovering the mystery of missing\nheritability in complex diseases. Results: We present a general and robust\nmethod to identify genetic patterns from multiple large-scale genomic datasets.\nWe treat the summary statistics as a matrix and demonstrate that genetic\npatterns will form a low-rank matrix plus a sparse component. Hence, we\nformulate the problem as a matrix recovering problem, where we aim to discover\nrisk variants shared by multiple diseases/traits and those for each individual\ndisease/trait. We propose a convex formulation for matrix recovery and an\nefficient algorithm to solve the problem. We demonstrate the advantages of our\nmethod using both synthesized datasets and real datasets. The experimental\nresults show that our method can successfully reconstruct both the shared and\nthe individual genetic patterns from summary statistics and achieve better\nperformance compared with alternative methods under a wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 13:59:23 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Teng", "Ben", ""], ["Yang", "Can", ""], ["Liu", "Jiming", ""], ["Cai", "Zhipeng", ""], ["Wan", "Xiang", ""]]}, {"id": "1501.06613", "submitter": "Roberto M\\'inguez", "authors": "Cristina Rold\\'an, Roberto M\\'inguez, Raquel Garc\\'ia-Bertrand and\n  Jos\\'e Manuel Arroyo", "title": "Robust Transmission Network Expansion Planning under Correlated\n  Uncertainty", "comments": "12 pages, 2 figures. https://ieeexplore.ieee.org/document/8585130", "journal-ref": "IEEE Transactions on Power Systems (Early Access), 1-1, 2018", "doi": "10.1109/TPWRS.2018.2889032", "report-no": null, "categories": "cs.CE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the transmission network expansion planning problem\nunder uncertain demand and generation capacity. A two-stage adaptive robust\noptimization framework is adopted whereby the worst-case operating cost is\naccounted for under a given user-defined uncertainty set. This work differs\nfrom previously reported robust solutions in two respects. First, the typically\ndisregarded correlation of uncertainty sources is explicitly considered through\nan ellipsoidal uncertainty set relying on their variance-covariance matrix. In\naddition, we describe the analogy between the corresponding second-stage\nproblem and a certain class of mathematical programs arising in structural\nreliability. This analogy gives rise to a relevant probabilistic interpretation\nof the second stage, thereby revealing an undisclosed feature of the worst-case\nsetting characterizing robust optimization with ellipsoidal uncertainty sets.\nMore importantly, a novel nested decomposition approach based on results from\nstructural reliability is devised to solve the proposed robust counterpart,\nwhich is cast as an instance of mixed-integer trilevel programming. Numerical\nresults from several case studies demonstrate that the effect of correlated\nuncertainty can be captured by the proposed robust approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 22:37:17 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 20:41:13 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Rold\u00e1n", "Cristina", ""], ["M\u00ednguez", "Roberto", ""], ["Garc\u00eda-Bertrand", "Raquel", ""], ["Arroyo", "Jos\u00e9 Manuel", ""]]}, {"id": "1501.06873", "submitter": "Roberto M\\'inguez", "authors": "R. M\\'inguez, E. Castillo, R. Pruneda and C. Solares", "title": "Truss Analysis Discussion and Interpretation Using Linear Systems of\n  Equalities and Inequalities", "comments": "39 pages and 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows the complementary roles of mathematical and engineering\npoints of view when dealing with truss analysis problems involving systems of\nlinear equations and inequalities. After the compatibility condition and the\nmathematical structure of the general solution of a system of linear equations\nis discussed, the truss analysis problem is used to illustrate its mathematical\nand engineering multiple aspects, including an analysis of the compatibility\nconditions and a physical interpretation of the general solution, and the\ngenerators of the resulting affine space. Next, the compatibility and the\nmathematical structure of the general solution of linear systems of\ninequalities are analyzed and the truss analysis problem revisited adding some\ninequality constraints, and discussing how they affect the resulting general\nsolution and many other aspects of it. Finally, some conclusions are drawn.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 19:18:00 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["M\u00ednguez", "R.", ""], ["Castillo", "E.", ""], ["Pruneda", "R.", ""], ["Solares", "C.", ""]]}, {"id": "1501.07293", "submitter": "Ru Zhu", "authors": "Ru Zhu", "title": "Accelerate micromagnetic simulations with GPU programming in MATLAB", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A finite-difference Micromagnetic simulation code written in MATLAB is\npresented with Graphics Processing Unit (GPU) acceleration. The high\nperformance of Graphics Processing Unit (GPU) is demonstrated compared to a\ntypical Central Processing Unit (CPU) based code. The speed-up of GPU to CPU is\nshown to be greater than 30 for problems with larger sizes on a mid-end GPU in\nsingle precision. The code is less than 200 lines and suitable for new\nalgorithm developing.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 18:21:58 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Zhu", "Ru", ""]]}, {"id": "1501.07400", "submitter": "Huber Markus", "authors": "Markus Huber, Bj\\\"orn Gmeiner, Ulrich R\\\"ude, Barbara Wohlmuth", "title": "Resilience for Exascale Enabled Multigrid Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing number of components and further miniaturization the mean\ntime between faults in supercomputers will decrease. System level fault\ntolerance techniques are expensive and cost energy, since they are often based\non redundancy. Also classical check-point-restart techniques reach their limits\nwhen the time for storing the system state to backup memory becomes excessive.\nTherefore, algorithm-based fault tolerance mechanisms can become an attractive\nalternative. This article investigates the solution process for elliptic\npartial differential equations that are discretized by finite elements. Faults\nthat occur in the parallel geometric multigrid solver are studied in various\nmodel scenarios. In a standard domain partitioning approach, the impact of a\nfailure of a core or a node will affect one or several subdomains. Different\nstrategies are developed to compensate the effect of such a failure\nalgorithmically. The recovery is achieved by solving a local subproblem with\nDirichlet boundary conditions using local multigrid cycling algorithms.\nAdditionally, we propose a superman strategy where extra compute power is\nemployed to minimize the time of the recovery process.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 10:21:24 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Huber", "Markus", ""], ["Gmeiner", "Bj\u00f6rn", ""], ["R\u00fcde", "Ulrich", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1501.07671", "submitter": "Vena Pearl Bongolan Dr.", "authors": "Vena Pearl Bo\\~ngolan, Karessa Alexandra O. Baritua, Marie Junne\n  Santos", "title": "Prioritizing the Components of Vulnerability in a Genetic Algorithms\n  Minimization of Flood Risk", "comments": "eight pages in pdf, with figures included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two prioritization schemes for the components of flooding\nvulnerability: urbanized area ration, literacy rate, mortality rate, poverty,\nradio/tv penetration, non-structural measures and structural measure. We\nprioritize the components, giving each a weight. We then express the\nvulnerability function as a weighted sum of its components. This weighted sum\nserves as the fitness function in a genetic algorithm, which comes up with the\noptimal design for a flood-resistant city.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 06:06:37 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Bo\u00f1golan", "Vena Pearl", ""], ["Baritua", "Karessa Alexandra O.", ""], ["Santos", "Marie Junne", ""]]}]