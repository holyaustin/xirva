[{"id": "1409.0367", "submitter": "Tom Crick", "authors": "Tom Crick, Benjamin A. Hall, Samin Ishtiaq and Kenji Takeda", "title": "\"Share and Enjoy\": Publishing Useful and Usable Scientific Models", "comments": "Accepted for the 1st International Workshop on Recomputability (part\n  of UCC 2014); 5 pages, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reproduction and replication of reported scientific results is a hot\ntopic within the academic community. The retraction of numerous studies from a\nwide range of disciplines, from climate science to bioscience, has drawn the\nfocus of many commentators, but there exists a wider socio-cultural problem\nthat pervades the scientific community. Sharing code, data and models often\nrequires extra effort; this is currently seen as a significant overhead that\nmay not be worth the time investment.\n  Automated systems, which allow easy reproduction of results, offer the\npotential to incentivise a culture change and drive the adoption of new\ntechniques to improve the efficiency of scientific exploration. In this paper,\nwe discuss the value of improved access and sharing of the two key types of\nresults arising from work done in the computational sciences: models and\nalgorithms. We propose the development of an integrated cloud-based system\nunderpinning computational science, linking together software and data\nrepositories, toolchains, workflows and outputs, providing a seamless automated\ninfrastructure for the verification and validation of scientific models and in\nparticular, performance benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 11:16:21 GMT"}, {"version": "v2", "created": "Tue, 14 Oct 2014 13:03:37 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Crick", "Tom", ""], ["Hall", "Benjamin A.", ""], ["Ishtiaq", "Samin", ""], ["Takeda", "Kenji", ""]]}, {"id": "1409.0405", "submitter": "Freddie Witherden", "authors": "F. D. Witherden and B. C. Vermeire and P. E. Vincent", "title": "Heterogeneous Computing on Mixed Unstructured Grids with PyFR", "comments": "21 pages, 9 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.CE physics.comp-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  PyFR is an open-source high-order accurate computational fluid dynamics\nsolver for mixed unstructured grids that can target a range of hardware\nplatforms from a single codebase. In this paper we demonstrate the ability of\nPyFR to perform high-order accurate unsteady simulations of flow on mixed\nunstructured grids using heterogeneous multi-node hardware. Specifically, after\nbenchmarking single-node performance for various platforms, PyFR v0.2.2 is used\nto undertake simulations of unsteady flow over a circular cylinder at Reynolds\nnumber 3 900 using a mixed unstructured grid of prismatic and tetrahedral\nelements on a desktop workstation containing an Intel Xeon E5-2697 v2 CPU, an\nNVIDIA Tesla K40c GPU, and an AMD FirePro W9100 GPU. Both the performance and\naccuracy of PyFR are assessed. PyFR v0.2.2 is freely available under a 3-Clause\nNew Style BSD license (see www.pyfr.org).\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 13:06:17 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Witherden", "F. D.", ""], ["Vermeire", "B. C.", ""], ["Vincent", "P. E.", ""]]}, {"id": "1409.0658", "submitter": "Uwe Aickelin", "authors": "Yihui liu and Uwe Aickelin", "title": "Detect Adverse Drug Reactions for Drug Aspirin", "comments": "IEEE fifth International Conference on Advanced Computational\n  Intelligence (ICACI), pp. 234-237, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse drug reaction (ADR) is widely concerned for public health issue. In\nthis study we propose an original approach to detect the ADRs using feature\nmatrix and feature selection. The experiments are carried out on the drug\nAspirin. Major side effects for the drug are detected and better performance is\nachieved compared to other computerized methods. The detected ADRs are based on\nthe computerized method, further investigation is needed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 10:43:25 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["liu", "Yihui", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1409.0718", "submitter": "Uwe Aickelin", "authors": "Ian Dent, Tony Craig, Uwe Aickelin and Tom Rodden", "title": "An Approach for Assessing Clustering of Households by Electricity Usage", "comments": "UKCI 2012, the 12th Annual Workshop on Computational Intelligence,\n  Heriot-Watt University, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How a household varies their regular usage of electricity is useful\ninformation for organisations to allow accurate targeting of behaviour\nmodification initiatives with the aim of improving the overall efficiency of\nthe electricity network. The variability of regular activities in a household\nis one possible indication of that household's willingness to accept incentives\nto change their behaviour.\n  An approach is presented for identifying a way of representing the\nvariability of a household's behaviour and developing an efficient way of\nclustering the households, using these measures of variability, into a few,\nusable groupings.\n  To evaluate the effectiveness of the variability measures, a number of\ncluster validity indexes are explored with regard to how the indexes vary with\nthe number of clusters, the number of attributes, and the quality of the\nattributes. The Cluster Dispersion Indicator (CDI) and the Davies-Boulden\nIndicator (DBI) are selected for future work developing various indicators of\nhousehold behaviour variability.\n  The approach is tested using data from 180 UK households monitored for over a\nyear at a sampling interval of 5 minutes. Data is taken from the evening peak\nelectricity usage period of 4pm to 8pm.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 14:12:59 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Dent", "Ian", ""], ["Craig", "Tony", ""], ["Aickelin", "Uwe", ""], ["Rodden", "Tom", ""]]}, {"id": "1409.0748", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack\n  Gibson, Richard Hubbard", "title": "Comparison of algorithms that detect drug side effects using electronic\n  healthcare databases", "comments": "Soft Computing, 17(12) pp. 2381-2397, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electronic healthcare databases are starting to become more readily\navailable and are thought to have excellent potential for generating adverse\ndrug reaction signals. The Health Improvement Network (THIN) database is an\nelectronic healthcare database containing medical information on over 11\nmillion patients that has excellent potential for detecting ADRs. In this paper\nwe apply four existing electronic healthcare database signal detecting\nalgorithms (MUTARA, HUNT, Temporal Pattern Discovery and modified ROR) on the\nTHIN database for a selection of drugs from six chosen drug families. This is\nthe first comparison of ADR signalling algorithms that includes MUTARA and HUNT\nand enabled us to set a benchmark for the adverse drug reaction signalling\nability of the THIN database. The drugs were selectively chosen to enable a\ncomparison with previous work and for variety. It was found that no algorithm\nwas generally superior and the algorithms' natural thresholds act at variable\nstringencies. Furthermore, none of the algorithms perform well at detecting\nrare ADRs.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:16:26 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack", ""], ["Hubbard", "Richard", ""]]}, {"id": "1409.0758", "submitter": "Uwe Aickelin", "authors": "Grazziela P Figueredo, Peer-Olaf Siebers, Markus R Owen, Jenna Reps,\n  Uwe Aickelin", "title": "Comparing Stochastic Differential Equations and Agent-Based Modelling\n  and Simulation for Early-stage Cancer", "comments": "PLoS ONE, 9 (4), pp. e95150, 2014", "journal-ref": null, "doi": "10.1371/journal.pone.0095150", "report-no": null, "categories": "cs.MA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is great potential to be explored regarding the use of agent-based\nmodelling and simulation as an alternative paradigm to investigate early-stage\ncancer interactions with the immune system. It does not suffer from some\nlimitations of ordinary differential equation models, such as the lack of\nstochasticity, representation of individual behaviours rather than aggregates\nand individual memory. In this paper we investigate the potential contribution\nof agent-based modelling and simulation when contrasted with stochastic\nversions of ODE models using early-stage cancer examples. We seek answers to\nthe following questions: (1) Does this new stochastic formulation produce\nsimilar results to the agent-based version? (2) Can these methods be used\ninterchangeably? (3) Do agent-based models outcomes reveal any benefit when\ncompared to the Gillespie results? To answer these research questions we\ninvestigate three well-established mathematical models describing interactions\nbetween tumour cells and immune elements. These case studies were\nre-conceptualised under an agent-based perspective and also converted to the\nGillespie algorithm formulation. Our interest in this work, therefore, is to\nestablish a methodological discussion regarding the usability of different\nsimulation approaches, rather than provide further biological insights into the\ninvestigated case studies. Our results show that it is possible to obtain\nequivalent models that implement the same mechanisms; however, the incapacity\nof the Gillespie algorithm to retain individual memory of past events affects\nthe similarity of some results. Furthermore, the emergent behaviour of ABMS\nproduces extra patters of behaviour in the system, which was not obtained by\nthe Gillespie algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:36:00 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Figueredo", "Grazziela P", ""], ["Siebers", "Peer-Olaf", ""], ["Owen", "Markus R", ""], ["Reps", "Jenna", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1409.0768", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria, Jack\n  E. Gibson, Richard B. Hubbard", "title": "A Novel Semi-Supervised Algorithm for Rare Prescription Side Effect\n  Discovery", "comments": null, "journal-ref": "IEEE Journal of Biomedical and Health Informatics, 18 (2), pp.\n  537-547, 2014", "doi": "10.2139/ssrn.2823251", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drugs are frequently prescribed to patients with the aim of improving each\npatient's medical state, but an unfortunate consequence of most prescription\ndrugs is the occurrence of undesirable side effects. Side effects that occur in\nmore than one in a thousand patients are likely to be signalled efficiently by\ncurrent drug surveillance methods, however, these same methods may take decades\nbefore generating signals for rarer side effects, risking medical morbidity or\nmortality in patients prescribed the drug while the rare side effect is\nundiscovered. In this paper we propose a novel computational meta-analysis\nframework for signalling rare side effects that integrates existing methods,\nknowledge from the web, metric learning and semi-supervised clustering. The\nnovel framework was able to signal many known rare and serious side effects for\nthe selection of drugs investigated, such as tendon rupture when prescribed\nCiprofloxacin or Levofloxacin, renal failure with Naproxen and depression\nassociated with Rimonabant. Furthermore, for the majority of the drug\ninvestigated it generated signals for rare side effects at a more stringent\nsignalling threshold than existing methods and shows the potential to become a\nfundamental part of post marketing surveillance to detect rare side effects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:00:23 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1409.0772", "submitter": "Uwe Aickelin", "authors": "Jenna M. Reps, Jonathan M. Garibaldi, Uwe Aickelin, Daniele Soria,\n  Jack E. Gibson, Richard B. Hubbard", "title": "Signalling Paediatric Side Effects using an Ensemble of Simple Study\n  Designs", "comments": "Drug Safety, 37 (3), pp. 163-170, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Children are frequently prescribed medication off-label, meaning\nthere has not been sufficient testing of the medication to determine its safety\nor effectiveness. The main reason this safety knowledge is lacking is due to\nethical restrictions that prevent children from being included in the majority\nof clinical trials. Objective: The objective of this paper is to investigate\nwhether an ensemble of simple study designs can be implemented to signal\nacutely occurring side effects effectively within the paediatric population by\nusing historical longitudinal data. The majority of pharmacovigilance\ntechniques are unsupervised, but this research presents a supervised framework.\nMethods: Multiple measures of association are calculated for each drug and\nmedical event pair and these are used as features that are fed into a\nclassiffier to determine the likelihood of the drug and medical event pair\ncorresponding to an adverse drug reaction. The classiffier is trained using\nknown adverse drug reactions or known non-adverse drug reaction relationships.\nResults: The novel ensemble framework obtained a false positive rate of 0:149,\na sensitivity of 0:547 and a specificity of 0:851 when implemented on a\nreference set of drug and medical event pairs. The novel framework consistently\noutperformed each individual simple study design. Conclusion: This research\nshows that it is possible to exploit the mechanism of causality and presents a\nframework for signalling adverse drug reactions effectively.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:17:25 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Reps", "Jenna M.", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1409.0775", "submitter": "Uwe Aickelin", "authors": "Yihui Liu and Uwe Aickelin", "title": "Feature selection in detection of adverse drug reactions from the Health\n  Improvement Network (THIN) database", "comments": "International Journal of Information Technology and Computer Science\n  (IJITCS), in print, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse drug reaction (ADR) is widely concerned for public health issue. ADRs\nare one of most common causes to withdraw some drugs from market. Prescription\nevent monitoring (PEM) is an important approach to detect the adverse drug\nreactions. The main problem to deal with this method is how to automatically\nextract the medical events or side effects from high-throughput medical events,\nwhich are collected from day to day clinical practice. In this study we propose\na novel concept of feature matrix to detect the ADRs. Feature matrix, which is\nextracted from big medical data from The Health Improvement Network (THIN)\ndatabase, is created to characterize the medical events for the patients who\ntake drugs. Feature matrix builds the foundation for the irregular and big\nmedical data. Then feature selection methods are performed on feature matrix to\ndetect the significant features. Finally the ADRs can be located based on the\nsignificant features. The experiments are carried out on three drugs:\nAtorvastatin, Alendronate, and Metoclopramide. Major side effects for each drug\nare detected and better performance is achieved compared to other computerized\nmethods. The detected ADRs are based on computerized methods, further\ninvestigation is needed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:25:58 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Liu", "Yihui", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1409.0788", "submitter": "Uwe Aickelin", "authors": "Chris Roadknight, Uwe Aickelin, John Scholefield, Lindy Durrant", "title": "Ensemble Learning of Colorectal Cancer Survival Rates", "comments": "IEEE International Conference on Computational Intelligence and\n  Virtual Environments for Measurement Systems and Applications (CIVEMSA) 2013,\n  pp. 82 - 86, 2013", "journal-ref": null, "doi": "10.1109/CIVEMSA.2013.6617400", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe a dataset relating to cellular and physical\nconditions of patients who are operated upon to remove colorectal tumours. This\ndata provides a unique insight into immunological status at the point of tumour\nremoval, tumour classification and post-operative survival. We build on\nexisting research on clustering and machine learning facets of this data to\ndemonstrate a role for an ensemble approach to highlighting patients with\nclearer prognosis parameters. Results for survival prediction using 3 different\napproaches are shown for a subset of the data which is most difficult to model.\nThe performance of each model individually is compared with subsets of the data\nwhere some agreement is reached for multiple models. Significant improvements\nin model accuracy on an unseen test set can be achieved for patients where\nagreement between models is achieved.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 16:52:16 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Roadknight", "Chris", ""], ["Aickelin", "Uwe", ""], ["Scholefield", "John", ""], ["Durrant", "Lindy", ""]]}, {"id": "1409.0814", "submitter": "Swakkhar Shatabda", "authors": "Rezaul Karim, Mohd. Momin Al Aziz, Swakkhar Shatabda, M. Sohel Rahman,\n  Md. Abul Kashem Mia, Farhana Zaman and Salman Rakin", "title": "CoMOGrad and PHOG: From Computer Vision to Fast and Accurate Protein\n  Tertiary Structure Retrieval", "comments": "draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the advancements in technology number of entries in the structural\ndatabase of proteins are increasing day by day. Methods for retrieving protein\ntertiary structures from this large database is the key to comparative analysis\nof structures which plays an important role to understand proteins and their\nfunction. In this paper, we present fast and accurate methods for the retrieval\nof proteins from a large database with tertiary structures similar to a query\nprotein. Our proposed methods borrow ideas from the field of computer vision.\nThe speed and accuracy of our methods comes from the two newly introduced\nfeatures, the co-occurrence matrix of the oriented gradient and pyramid\nhistogram of oriented gradient and from the use of Euclidean distance as the\ndistance measure. Experimental results clearly indicate the superiority of our\napproach in both running time and accuracy. Our method is readily available for\nuse from this website: http://research.buet.ac.bd:8080/Comograd/.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 18:26:50 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Karim", "Rezaul", ""], ["Aziz", "Mohd. Momin Al", ""], ["Shatabda", "Swakkhar", ""], ["Rahman", "M. Sohel", ""], ["Mia", "Md. Abul Kashem", ""], ["Zaman", "Farhana", ""], ["Rakin", "Salman", ""]]}, {"id": "1409.1043", "submitter": "Uwe Aickelin", "authors": "Ian Dent, Tony Craig, Uwe Aickelin and Tom Rodden", "title": "Variability of Behaviour in Electricity Load Profile Clustering; Who\n  Does Things at the Same Time Each Day?", "comments": "Advances in Data Mining, pp. 70-84, Springer, Heidelberg, 2014, ISBN\n  978-3-319-08975-1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UK electricity market changes provide opportunities to alter households'\nelectricity usage patterns for the benefit of the overall electricity network.\nWork on clustering similar households has concentrated on daily load profiles\nand the variability in regular household behaviours has not been considered.\nThose households with most variability in regular activities may be the most\nreceptive to incentives to change timing.\n  Whether using the variability of regular behaviour allows the creation of\nmore consistent groupings of households is investigated and compared with daily\nload profile clustering. 204 UK households are analysed to find repeating\npatterns (motifs). Variability in the time of the motif is used as the basis\nfor clustering households. Different clustering algorithms are assessed by the\nconsistency of the results.\n  Findings show that variability of behaviour, using motifs, provides more\nconsistent groupings of households across different clustering algorithms and\nallows for more efficient targeting of behaviour change interventions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 11:42:33 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Dent", "Ian", ""], ["Craig", "Tony", ""], ["Aickelin", "Uwe", ""], ["Rodden", "Tom", ""]]}, {"id": "1409.1053", "submitter": "Uwe Aickelin", "authors": "Jenna M. Reps, Uwe Aickelin and Jonathan M. Garibaldi", "title": "Tuning a Multiple Classifier System for Side Effect Discovery using\n  Genetic Algorithms", "comments": "Proceedings of the 2014 World Congress on Computational Intelligence\n  (WCCI 2014), pp. 910-917, IEEE, Beijing, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work, a novel supervised framework implementing a binary\nclassifier was presented that obtained excellent results for side effect\ndiscovery. Interestingly, unique side effects were identified when different\nbinary classifiers were used within the framework, prompting the investigation\nof applying a multiple classifier system. In this paper we investigate tuning a\nside effect multiple classifying system using genetic algorithms. The results\nof this research show that the novel framework implementing a multiple\nclassifying system trained using genetic algorithms can obtain a higher partial\narea under the receiver operating characteristic curve than implementing a\nsingle classifier. Furthermore, the framework is able to detect side effects\nefficiently and obtains a low false positive rate.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:11:54 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Reps", "Jenna M.", ""], ["Aickelin", "Uwe", ""], ["Garibaldi", "Jonathan M.", ""]]}, {"id": "1409.1055", "submitter": "Uwe Aickelin", "authors": "Diman Hassan, Uwe Aickelin and Christian Wagner", "title": "Comparison of Distance Metrics for Hierarchical Data in Medical\n  Databases", "comments": "Proceedings of the 2014 World Congress on Computational Intelligence\n  (WCCI 2014), pp. 3636-3643, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance metrics are broadly used in different research areas and\napplications, such as bio-informatics, data mining and many other fields.\nHowever, there are some metrics, like pq-gram and Edit Distance used\nspecifically for data with a hierarchical structure. Other metrics used for\nnon-hierarchical data are the geometric and Hamming metrics. We have applied\nthese metrics to The Health Improvement Network (THIN) database which has some\nhierarchical data. The THIN data has to be converted into a tree-like structure\nfor the first group of metrics. For the second group of metrics, the data are\nconverted into a frequency table or matrix, then for all metrics, all distances\nare found and normalised. Based on this particular data set, our research\nquestion: which of these metrics is useful for THIN data? This paper compares\nthe metrics, particularly the pq-gram metric on finding the similarities of\npatients' data. It also investigates the similar patients who have the same\nclose distances as well as the metrics suitability for clustering the whole\npatient population. Our results show that the two groups of metrics perform\ndifferently as they represent different structures of the data. Nevertheless,\nall the metrics could represent some similar data of patients as well as\ndiscriminate sufficiently well in clustering the patient population using\n$k$-means clustering algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:19:19 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Hassan", "Diman", ""], ["Aickelin", "Uwe", ""], ["Wagner", "Christian", ""]]}, {"id": "1409.1057", "submitter": "Uwe Aickelin", "authors": "Alexandros Ladas, Jonathan M. Garibaldi, Rodrigo Scarpel and Uwe\n  Aickelin", "title": "Augmented Neural Networks for Modelling Consumer Indebtness", "comments": "Proceedings of the 2014 World Congress on Computational Intelligence\n  (WCCI 2014), pp. 3086-3093, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer Debt has risen to be an important problem of modern societies,\ngenerating a lot of research in order to understand the nature of consumer\nindebtness, which so far its modelling has been carried out by statistical\nmodels. In this work we show that Computational Intelligence can offer a more\nholistic approach that is more suitable for the complex relationships an\nindebtness dataset has and Linear Regression cannot uncover. In particular, as\nour results show, Neural Networks achieve the best performance in modelling\nconsumer indebtness, especially when they manage to incorporate the significant\nand experimentally verified results of the Data Mining process in the model,\nexploiting the flexibility Neural Networks offer in designing their topology.\nThis novel method forms an elaborate framework to model Consumer indebtness\nthat can be extended to any other real world application.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:23:50 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Ladas", "Alexandros", ""], ["Garibaldi", "Jonathan M.", ""], ["Scarpel", "Rodrigo", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1409.1059", "submitter": "Uwe Aickelin", "authors": "Yihui Liu and Uwe Aickelin", "title": "Detecting adverse drug reactions for the drug Simvastatin", "comments": "Fourth International Conference on Multimedia Information Networking\n  and Security (MINES), pp. 246-249, 2012. arXiv admin note: substantial text\n  overlap with arXiv:1409.0658", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adverse drug reactions (ADR) are widely concerning for public health issue.\nIn this study we propose an original approach to detect ADRs using a feature\nmatrix and feature selection. The experiments are carried out on the drug\nSimvastatin. Major side effects for the drug are detected and better\nperformance is achieved compared to other computerized methods. Because\ncurrently the detected ADRs are based solely on computerized methods, further\nexpert investigation is needed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 12:30:05 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Liu", "Yihui", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1409.1456", "submitter": "Siamak Ravanbakhsh", "authors": "Siamak Ravanbakhsh, Philip Liu, Trent Bjorndahl, Rupasri Mandal, Jason\n  R. Grant, Michael Wilson, Roman Eisner, Igor Sinelnikov, Xiaoyu Hu, Claudio\n  Luchinat, Russell Greiner and David S. Wishart", "title": "Accurate, fully-automated NMR spectral profiling for metabolomics", "comments": null, "journal-ref": "PLoS ONE 10(5): e0124219, 2015", "doi": "10.1371/journal.pone.0124219", "report-no": null, "categories": "cs.AI cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many diseases cause significant changes to the concentrations of small\nmolecules (aka metabolites) that appear in a person's biofluids, which means\nsuch diseases can often be readily detected from a person's \"metabolic\nprofile\". This information can be extracted from a biofluid's NMR spectrum.\nToday, this is often done manually by trained human experts, which means this\nprocess is relatively slow, expensive and error-prone. This paper presents a\ntool, Bayesil, that can quickly, accurately and autonomously produce a complex\nbiofluid's (e.g., serum or CSF) metabolic profile from a 1D1H NMR spectrum.\nThis requires first performing several spectral processing steps then matching\nthe resulting spectrum against a reference compound library, which contains the\n\"signatures\" of each relevant metabolite. Many of these steps are novel\nalgorithms and our matching step views spectral matching as an inference\nproblem within a probabilistic graphical model that rapidly approximates the\nmost probable metabolic profile. Our extensive studies on a diverse set of\ncomplex mixtures, show that Bayesil can autonomously find the concentration of\nall NMR-detectable metabolites accurately (~90% correct identification and ~10%\nquantification error), in <5minutes on a single CPU. These results demonstrate\nthat Bayesil is the first fully-automatic publicly-accessible system that\nprovides quantitative NMR spectral profiling effectively -- with an accuracy\nthat meets or exceeds the performance of trained experts. We anticipate this\ntool will usher in high-throughput metabolomics and enable a wealth of new\napplications of NMR in clinical settings. Available at http://www.bayesil.ca.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 14:50:56 GMT"}, {"version": "v2", "created": "Fri, 5 Sep 2014 16:23:42 GMT"}, {"version": "v3", "created": "Mon, 8 Sep 2014 01:25:52 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Ravanbakhsh", "Siamak", ""], ["Liu", "Philip", ""], ["Bjorndahl", "Trent", ""], ["Mandal", "Rupasri", ""], ["Grant", "Jason R.", ""], ["Wilson", "Michael", ""], ["Eisner", "Roman", ""], ["Sinelnikov", "Igor", ""], ["Hu", "Xiaoyu", ""], ["Luchinat", "Claudio", ""], ["Greiner", "Russell", ""], ["Wishart", "David S.", ""]]}, {"id": "1409.2051", "submitter": "Mike Steel Prof.", "authors": "Sebastien Roch and Mike Steel", "title": "Likelihood-based tree reconstruction on a concatenation of alignments\n  can be positively misleading", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reconstruction of a species tree from genomic data faces a double hurdle.\nFirst, the (gene) tree describing the evolution of each gene may differ from\nthe species tree, for instance, due to incomplete lineage sorting. Second, the\naligned genetic sequences at the leaves of each gene tree provide merely an\nimperfect estimate of the topology of the gene tree. In this note, we\ndemonstrate formally that a basic statistical problem arises if one tries to\navoid accounting for these two processes and analyses the genetic data directly\nvia a concatenation approach. More precisely, we show that, under the\nmulti-species coalescent with a standard site substitution model, maximum\nlikelihood estimation on sequence data that has been concatenated across genes\nand performed under the incorrect assumption that all sites have evolved\nindependently and identically on a fixed tree is a statistically inconsistent\nestimator of the species tree. Our results provide a formal justification of\nsimulation results described of Kubatko and Degnan (2007) and others, and\ncomplements recent theoretical results by DeGorgio and Degnan (2010) and\nChifman and Kubtako (2014).\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 19:51:05 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Roch", "Sebastien", ""], ["Steel", "Mike", ""]]}, {"id": "1409.4276", "submitter": "Paul Vitanyi", "authors": "Rudi L. Cilibrasi (CWI, Amsterdam) and Paul M.B. Vitanyi (CWI and\n  University of Amsterdam)", "title": "A Fast Quartet Tree Heuristic for Hierarchical Clustering", "comments": "LaTeX, 40 pages, 11 figures; this paper has substantial overlap with\n  arXiv:cs/0606048 in cs.DS", "journal-ref": "Pattern Recognition, 44 (2011) 662-677", "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Quartet Tree Cost problem is to construct an optimal weight tree\nfrom the $3{n \\choose 4}$ weighted quartet topologies on $n$ objects, where\noptimality means that the summed weight of the embedded quartet topologies is\noptimal (so it can be the case that the optimal tree embeds all quartets as\nnonoptimal topologies). We present a Monte Carlo heuristic, based on randomized\nhill climbing, for approximating the optimal weight tree, given the quartet\ntopology weights. The method repeatedly transforms a dendrogram, with all\nobjects involved as leaves, achieving a monotonic approximation to the exact\nsingle globally optimal tree. The problem and the solution heuristic has been\nextensively used for general hierarchical clustering of nontree-like\n(non-phylogeny) data in various domains and across domains with heterogeneous\ndata. We also present a greatly improved heuristic, reducing the running time\nby a factor of order a thousand to ten thousand. All this is implemented and\navailable, as part of the CompLearn package. We compare performance and running\ntime of the original and improved versions with those of UPGMA, BioNJ, and NJ,\nas implemented in the SplitsTree package on genomic data for which the latter\nare optimized.\n  Keywords: Data and knowledge visualization, Pattern\nmatching--Clustering--Algorithms/Similarity measures, Hierarchical clustering,\nGlobal optimization, Quartet tree, Randomized hill-climbing,\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 15:55:25 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Cilibrasi", "Rudi L.", "", "CWI, Amsterdam"], ["Vitanyi", "Paul M. B.", "", "CWI and\n  University of Amsterdam"]]}, {"id": "1409.4812", "submitter": "Nicolas Guarin-Zapata", "authors": "Nicol\\'as Guar\\'in-Zapata and Juan Gomez", "title": "Evaluation of the Spectral Finite Element Method With the Theory of\n  Phononic Crystals", "comments": "20 pages, 8 figures, Preprint of an article published in Journal of\n  Computational Acoustics", "journal-ref": "Journal of Computational Acoustics, Vol. 23 (2015) 1550004 (17\n  pages)", "doi": "10.1142/S0218396X15500046", "report-no": null, "categories": "cs.CE math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluated the performance of the classical and spectral finite element\nmethod in the simulation of elastodynamic problems. We used as a quality\nmeasure their ability to capture the actual dispersive behavior of the\nmaterial. Four different materials are studied: a homogeneous non-dispersive\nmaterial, a bilayer material, and composite materials consisting of an aluminum\nmatrix and brass inclusions or voids. To obtain the dispersion properties,\nspatial periodicity is assumed so the analysis is conducted using Floquet-Bloch\nprinciples. The effects in the dispersion properties of the lumping process for\nthe mass matrices resulting from the classical finite element method are also\ninvestigated, since that is a common practice when the problem is solved with\nexplicit time marching schemes. At high frequencies the predictions with the\nspectral technique exactly match the analytical dispersion curves, while the\nclassical method does not. This occurs even at the same computational demands.\nAt low frequencies however, the results from both the classical (consistent or\nmass-lumped) and spectral finite element coincide with the analytically\ndetermined curves. Surprisingly, at low frequencies even the results obtained\nwith the artificial diagonal mass matrix from the classical technique exactly\nmatch the analytic dispersion curves.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 21:39:53 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 21:22:39 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Guar\u00edn-Zapata", "Nicol\u00e1s", ""], ["Gomez", "Juan", ""]]}, {"id": "1409.4822", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Xiu Yang, Giovanni Marucci, Paolo Maffezzoni, Ibrahim\n  (Abe) M. Elfadel, George Em Karniadakis, Luca Daniel", "title": "Stochastic Testing Simulator for Integrated Circuits and MEMS:\n  Hierarchical and Sparse Techniques", "comments": "Accepted to IEEE Custom Integrated Circuits Conference in June 2014.\n  arXiv admin note: text overlap with arXiv:1407.3023", "journal-ref": "Z. Zhang, X. Yang, G. Marucci, P. Maffezzoni, I. M. Elfadel, G. E.\n  Karniadakis and L. Daniel, \"Stochastic Testing Simulator for Integrated\n  Circuits and MEMS: Hierarchical and Sparse Techniques\", in Proc. CICC, Sept.\n  2014", "doi": "10.1109/CICC.2014.6946009", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process variations are a major concern in today's chip design since they can\nsignificantly degrade chip performance. To predict such degradation, existing\ncircuit and MEMS simulators rely on Monte Carlo algorithms, which are typically\ntoo slow. Therefore, novel fast stochastic simulators are highly desired. This\npaper first reviews our recently developed stochastic testing simulator that\ncan achieve speedup factors of hundreds to thousands over Monte Carlo. Then, we\ndevelop a fast hierarchical stochastic spectral simulator to simulate a complex\ncircuit or system consisting of several blocks. We further present a fast\nsimulation approach based on anchored ANOVA (analysis of variance) for some\ndesign problems with many process variations. This approach can reduce the\nsimulation cost and can identify which variation sources have strong impacts on\nthe circuit's performance. The simulation results of some circuit and MEMS\nexamples are reported to show the effectiveness of our simulator\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 22:15:47 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Zheng", "", "Abe"], ["Yang", "Xiu", "", "Abe"], ["Marucci", "Giovanni", "", "Abe"], ["Maffezzoni", "Paolo", "", "Abe"], ["Ibrahim", "", "", "Abe"], ["Elfadel", "M.", ""], ["Karniadakis", "George Em", ""], ["Daniel", "Luca", ""]]}, {"id": "1409.4824", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Ibrahim (Abe) M. Elfadel, Luca Daniel", "title": "Uncertainty Quantification for Integrated Circuits: Stochastic Spectral\n  Methods", "comments": "published in Proc. ICCCAD 2013", "journal-ref": "Int. Conf. Computer-Aided Design, pp. 803-810, San Jose, CA, Nov.\n  2013", "doi": null, "report-no": null, "categories": "cs.CE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to significant manufacturing process variations, the performance of\nintegrated circuits (ICs) has become increasingly uncertain. Such uncertainties\nmust be carefully quantified with efficient stochastic circuit simulators. This\npaper discusses the recent advances of stochastic spectral circuit simulators\nbased on generalized polynomial chaos (gPC). Such techniques can handle both\nGaussian and non-Gaussian random parameters, showing remarkable speedup over\nMonte Carlo for circuits with a small or medium number of parameters. We focus\non the recently developed stochastic testing and the application of\nconventional stochastic Galerkin and stochastic collocation schemes to\nnonlinear circuit problems. The uncertainty quantification algorithms for\nstatic, transient and periodic steady-state simulations are presented along\nwith some practical simulation results. Some open problems in this field are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 22:31:11 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Zhang", "Zheng", "", "Abe"], ["Ibrahim", "", "", "Abe"], ["Elfadel", "M.", ""], ["Daniel", "Luca", ""]]}, {"id": "1409.4826", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Tarek A. El-Moselhy, Paolo Maffezzoni, Ibrahim (Abe) M.\n  Elfadel, Luca Daniel", "title": "Efficient Uncertainty Quantification for the Periodic Steady State of\n  Forced and Autonomous Circuits", "comments": "Published by IEEE Trans Circuits and Systems II: Express Briefs in\n  2013", "journal-ref": "IEEE Trans. Circuits and Systems II: Express Briefs, vol. 60,\n  no.10, pp. 687-691, (2013)", "doi": "10.1109/TCSII.2013.2278110", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This brief paper proposes an uncertainty quantification method for the\nperiodic steady-state (PSS) analysis with both Gaussian and non-Gaussian\nvariations. Our stochastic testing formulation for the PSS problem provides\nsuperior efficiency over both Monte Carlo methods and existing spectral\nmethods. The numerical implementation of a stochastic shooting Newton solver is\npresented for both forced and autonomous circuits. Simulation results on some\nanalog/RF circuits are reported to show the effectiveness of our proposed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 22:46:49 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Zheng", "", "Abe"], ["El-Moselhy", "Tarek A.", "", "Abe"], ["Maffezzoni", "Paolo", "", "Abe"], ["Ibrahim", "", "", "Abe"], ["Elfadel", "M.", ""], ["Daniel", "Luca", ""]]}, {"id": "1409.4829", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Tarek A. El-Moselhy, Ibrahim (Abe) M. Elfadel, Luca\n  Daniel", "title": "Calculation of Generalized Polynomial-Chaos Basis Functions and Gauss\n  Quadrature Rules in Hierarchical Uncertainty Quantification", "comments": "Published by IEEE Trans CAD in May 2014", "journal-ref": "IEEE Trans. Computer-Aided Design of Integrated Circuits and\n  Systems, vol. 33, no. 5, pp. 728-740, May 2014", "doi": "10.1109/TCAD.2013.2295818", "report-no": null, "categories": "cs.CE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic spectral methods are efficient techniques for uncertainty\nquantification. Recently they have shown excellent performance in the\nstatistical analysis of integrated circuits. In stochastic spectral methods,\none needs to determine a set of orthonormal polynomials and a proper numerical\nquadrature rule. The former are used as the basis functions in a generalized\npolynomial chaos expansion. The latter is used to compute the integrals\ninvolved in stochastic spectral methods. Obtaining such information requires\nknowing the density function of the random input {\\it a-priori}. However,\nindividual system components are often described by surrogate models rather\nthan density functions. In order to apply stochastic spectral methods in\nhierarchical uncertainty quantification, we first propose to construct\nphysically consistent closed-form density functions by two monotone\ninterpolation schemes. Then, by exploiting the special forms of the obtained\ndensity functions, we determine the generalized polynomial-chaos basis\nfunctions and the Gauss quadrature rules that are required by a stochastic\nspectral simulator. The effectiveness of our proposed algorithm is verified by\nboth synthetic and practical circuit examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 23:15:01 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Zheng", "", "Abe"], ["El-Moselhy", "Tarek A.", "", "Abe"], ["Ibrahim", "", "", "Abe"], ["Elfadel", "M.", ""], ["Daniel", "Luca", ""]]}, {"id": "1409.4831", "submitter": "Zheng Zhang", "authors": "Zheng Zhang, Tarek A. El-Moselhy, Ibrahim (Abe) M. Elfadel, Luca\n  Daniel", "title": "Stochastic Testing Method for Transistor-Level Uncertainty\n  Quantification Based on Generalized Polynomial Chaos", "comments": "published by IEEE Trans CAD in Oct 2013", "journal-ref": "IEEE Trans. Computer-Aided Design of Integrated Circuits and\n  Systems, vol. 32, no. 10, pp. 1533-1545, Oct. 2013", "doi": "10.1109/TCAD.2013.2263039", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainties have become a major concern in integrated circuit design. In\norder to avoid the huge number of repeated simulations in conventional Monte\nCarlo flows, this paper presents an intrusive spectral simulator for\nstatistical circuit analysis. Our simulator employs the recently developed\ngeneralized polynomial chaos expansion to perform uncertainty quantification of\nnonlinear transistor circuits with both Gaussian and non-Gaussian random\nparameters. We modify the nonintrusive stochastic collocation (SC) method and\ndevelop an intrusive variant called stochastic testing (ST) method to\naccelerate the numerical simulation. Compared with the stochastic Galerkin (SG)\nmethod, the resulting coupled deterministic equations from our proposed ST\nmethod can be solved in a decoupled manner at each time point. At the same\ntime, ST uses fewer samples and allows more flexible time step size controls\nthan directly using a nonintrusive SC solver. These two properties make ST more\nefficient than SG and than existing SC methods, and more suitable for\ntime-domain circuit simulation. Simulation results of several digital, analog\nand RF circuits are reported. Since our algorithm is based on generic\nmathematical models, the proposed ST algorithm can be applied to many other\nengineering problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 23:32:44 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Zhang", "Zheng", "", "Abe"], ["El-Moselhy", "Tarek A.", "", "Abe"], ["Ibrahim", "", "", "Abe"], ["Elfadel", "M.", ""], ["Daniel", "Luca", ""]]}, {"id": "1409.5671", "submitter": "Ebru Aydin Gol", "authors": "Ebru Aydin Gol and Ezio Bartocci and Calin Belta", "title": "A Formal Methods Approach to Pattern Synthesis in Reaction Diffusion\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.LG cs.LO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a technique to detect and generate patterns in a network of\nlocally interacting dynamical systems. Central to our approach is a novel\nspatial superposition logic, whose semantics is defined over the quad-tree of a\npartitioned image. We show that formulas in this logic can be efficiently\nlearned from positive and negative examples of several types of patterns. We\nalso demonstrate that pattern detection, which is implemented as a model\nchecking algorithm, performs very well for test data sets different from the\nlearning sets. We define a quantitative semantics for the logic and integrate\nthe model checking algorithm with particle swarm optimization in a\ncomputational framework for synthesis of parameters leading to desired patterns\nin reaction-diffusion systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 05:21:06 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Gol", "Ebru Aydin", ""], ["Bartocci", "Ezio", ""], ["Belta", "Calin", ""]]}, {"id": "1409.5774", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Jonathan M. Garibaldi, Uwe Aickelin and Daniele Soria,\n  Jack E. Gibson and Richard B. Hubbard", "title": "Attributes for Causal Inference in Longitudinal Observational Databases", "comments": "The 26th IEEE International Symposium on Computer-Based Medical\n  Systems, Porto, pp. 548 - 549, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pharmaceutical industry is plagued by the problem of side effects that\ncan occur anytime a prescribed medication is ingested. There has been a recent\ninterest in using the vast quantities of medical data available in longitudinal\nobservational databases to identify causal relationships between drugs and\nmedical events. Unfortunately the majority of existing post marketing\nsurveillance algorithms measure how dependant or associated an event is on the\npresence of a drug rather than measuring causality. In this paper we\ninvestigate potential attributes that can be used in causal inference to\nidentify side effects based on the Bradford-Hill causality criteria. Potential\nattributes are developed by considering five of the causality criteria and\nfeature selection is applied to identify the most suitable of these attributes\nfor detecting side effects. We found that attributes based on the specificity\ncriterion may improve side effect signalling algorithms but the experiment and\ndosage criteria attributes investigated in this paper did not offer sufficient\nadditional information.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 09:07:53 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Reps", "Jenna", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Soria", "Daniele", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1409.6369", "submitter": "Bassam Alkindy Mr.", "authors": "Bassam AlKindy, Jean-Fran\\c{c}ois Couchot, Christophe Guyeux, Arnaud\n  Mouly, Michel Salomon, Jacques M. Bahi", "title": "Finding the Core-Genes of Chloroplasts", "comments": null, "journal-ref": "Journal of Bioscience, Biochemistry, and Bioinformatics,\n  4(5):357--364, 2014", "doi": null, "report-no": null, "categories": "cs.CE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the recent evolution of sequencing techniques, the number of available\ngenomes is rising steadily, leading to the possibility to make large scale\ngenomic comparison between sets of close species. An interesting question to\nanswer is: what is the common functionality genes of a collection of species,\nor conversely, to determine what is specific to a given species when compared\nto other ones belonging in the same genus, family, etc. Investigating such\nproblem means to find both core and pan genomes of a collection of species,\n\\textit{i.e.}, genes in common to all the species vs. the set of all genes in\nall species under consideration. However, obtaining trustworthy core and pan\ngenomes is not an easy task, leading to a large amount of computation, and\nrequiring a rigorous methodology. Surprisingly, as far as we know, this\nmethodology in finding core and pan genomes has not really been deeply\ninvestigated. This research work tries to fill this gap by focusing only on\nchloroplastic genomes, whose reasonable sizes allow a deep study. To achieve\nthis goal, a collection of 99 chloroplasts are considered in this article. Two\nmethodologies have been investigated, respectively based on sequence\nsimilarities and genes names taken from annotation tools. The obtained results\nwill finally be evaluated in terms of biological relevance.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 23:26:30 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["AlKindy", "Bassam", ""], ["Couchot", "Jean-Fran\u00e7ois", ""], ["Guyeux", "Christophe", ""], ["Mouly", "Arnaud", ""], ["Salomon", "Michel", ""], ["Bahi", "Jacques M.", ""]]}, {"id": "1409.7403", "submitter": "Simon DeDeo", "authors": "David H. Wolpert, Joshua A. Grochow, Eric Libby, Simon DeDeo", "title": "Optimal high-level descriptions of dynamical systems", "comments": "33 pages. Updated discussion and references", "journal-ref": null, "doi": null, "report-no": "SFI Working Paper #15-06-017", "categories": "cs.IT cond-mat.stat-mech cs.AI cs.CE math.IT q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyze high-dimensional systems, many fields in science and engineering\nrely on high-level descriptions, sometimes called \"macrostates,\"\n\"coarse-grainings,\" or \"effective theories\". Examples of such descriptions\ninclude the thermodynamic properties of a large collection of point particles\nundergoing reversible dynamics, the variables in a macroeconomic model\ndescribing the individuals that participate in an economy, and the summary\nstate of a cell composed of a large set of biochemical networks.\n  Often these high-level descriptions are constructed without considering the\nultimate reason for needing them in the first place. Here, we formalize and\nquantify one such purpose: the need to predict observables of interest\nconcerning the high-dimensional system with as high accuracy as possible, while\nminimizing the computational cost of doing so. The resulting State Space\nCompression (SSC) framework provides a guide for how to solve for the {optimal}\nhigh-level description of a given dynamical system, rather than constructing it\nbased on human intuition alone.\n  In this preliminary report, we introduce SSC, and illustrate it with several\ninformation-theoretic quantifications of \"accuracy\", all with different\nimplications for the optimal compression. We also discuss some other possible\napplications of SSC beyond the goal of accurate prediction. These include SSC\nas a measure of the complexity of a dynamical system, and as a way to quantify\ninformation flow between the scales of a system.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 20:01:47 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 19:31:07 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Wolpert", "David H.", ""], ["Grochow", "Joshua A.", ""], ["Libby", "Eric", ""], ["DeDeo", "Simon", ""]]}, {"id": "1409.7418", "submitter": "Matthew Knepley", "authors": "Jaydeep P. Bardhan and Matthew G. Knepley", "title": "Modeling Charge-Sign Asymmetric Solvation Free Energies With Nonlinear\n  Boundary Conditions", "comments": "7 pages, 2 figures, accepted to Journal of Chemical Physics", "journal-ref": null, "doi": "10.1063/1.4897324", "report-no": null, "categories": "physics.chem-ph cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that charge-sign-dependent asymmetric hydration can be modeled\naccurately using linear Poisson theory but replacing the standard\nelectric-displacement boundary condition with a simple nonlinear boundary\ncondition. Using a single multiplicative scaling factor to determine atomic\nradii from molecular dynamics Lennard-Jones parameters, the new model\naccurately reproduces MD free-energy calculations of hydration asymmetries for\n(i) monatomic ions, (ii) titratable amino acids in both their protonated and\nunprotonated states, and (iii) the Mobley \"bracelet\" and \"rod\" test problems\n[J. Phys. Chem. B, v. 112:2408, 2008]. Remarkably, the model also justifies the\nuse of linear response expressions for charging free energies. Our\nboundary-element method implementation demonstrates the ease with which other\ncontinuum-electrostatic solvers can be extended to include asymmetry.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 20:53:50 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Bardhan", "Jaydeep P.", ""], ["Knepley", "Matthew G.", ""]]}, {"id": "1409.7966", "submitter": "Daniele de Rigo", "authors": "Dario Rodriguez-Aseretto, Christian Schaerer and Daniele de Rigo", "title": "Architecture of Environmental Risk Modelling: for a faster and more\n  robust response to natural disasters", "comments": "12 pages, 1 figure, 1 text box, presented at the 3rd Conference of\n  Computational Interdisciplinary Sciences (CCIS 2014), Asuncion, Paraguay", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demands on the disaster response capacity of the European Union are likely to\nincrease, as the impacts of disasters continue to grow both in size and\nfrequency. This has resulted in intensive research on issues concerning\nspatially-explicit information and modelling and their multiple sources of\nuncertainty. Geospatial support is one of the forms of assistance frequently\nrequired by emergency response centres along with hazard forecast and event\nmanagement assessment. Robust modelling of natural hazards requires dynamic\nsimulations under an array of multiple inputs from different sources.\nUncertainty is associated with meteorological forecast and calibration of the\nmodel parameters. Software uncertainty also derives from the data\ntransformation models (D-TM) needed for predicting hazard behaviour and its\nconsequences. On the other hand, social contributions have recently been\nrecognized as valuable in raw-data collection and mapping efforts traditionally\ndominated by professional organizations. Here an architecture overview is\nproposed for adaptive and robust modelling of natural hazards, following the\nSemantic Array Programming paradigm to also include the distributed array of\nsocial contributors called Citizen Sensor in a semantically-enhanced strategy\nfor D-TM modelling. The modelling architecture proposes a multicriteria\napproach for assessing the array of potential impacts with qualitative rapid\nassessment methods based on a Partial Open Loop Feedback Control (POLFC) schema\nand complementing more traditional and accurate a-posteriori assessment. We\ndiscuss the computational aspect of environmental risk modelling using\narray-based parallel paradigms on High Performance Computing (HPC) platforms,\nin order for the implications of urgency to be introduced into the systems\n(Urgent-HPC).\n", "versions": [{"version": "v1", "created": "Sun, 28 Sep 2014 22:48:22 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Rodriguez-Aseretto", "Dario", ""], ["Schaerer", "Christian", ""], ["de Rigo", "Daniele", ""]]}, {"id": "1409.8593", "submitter": "Roberto M\\'inguez", "authors": "Roberto M\\'inguez and V\\'ictor Casero-Alonso", "title": "Robust solutions of uncertain mixed-integer linear programs using\n  decomposition techniques", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CE math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust optimization is a framework for modeling optimization problems\ninvolving data uncertainty and during the last decades has been an area of\nactive research. If we focus on linear programming (LP) problems with i)\nuncertain data, ii) binary decisions and iii) hard constraints within an\nellipsoidal uncertainty set, this paper provides a different interpretation of\ntheir robust counterpart (RC) inspired from decomposition techniques. This new\ninterpretation allows the proposal of an ad-hoc decomposition technique to\nsolve the RC problem with the following advantages: i) it improves\ntractability, specially for large-scale problems, and ii) it provides the exact\nprobability of constraint violation in case the probability distribution of\nuncertain parameters are completely defined by using first and second-order\nprobability moments. An attractive aspect of our method is that it decomposes\nthe second-order cone programming problem, associated with the robust\ncounterpart, into a linear master problem and different quadratically\nconstrained problems (QCP) of considerable lower size. The optimal solution is\nachieved through the solution of these master and subproblems within an\niterative scheme based on cutting plane approximations of the second-order cone\nconstraints. In addition, proof of convergence of the iterative method is\ngiven.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 15:24:24 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2016 22:04:43 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2016 12:22:34 GMT"}, {"version": "v4", "created": "Tue, 14 Feb 2017 07:55:08 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["M\u00ednguez", "Roberto", ""], ["Casero-Alonso", "V\u00edctor", ""]]}]