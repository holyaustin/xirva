[{"id": "1803.00046", "submitter": "Janine Mergel", "authors": "Janine C. Mergel, Riad Sahli, Julien Scheibert, Roger A. Sauer", "title": "Continuum contact models for coupled adhesion and friction", "comments": null, "journal-ref": "The Journal of Adhesion, 95(12):1101-1133, 2019", "doi": "10.1080/00218464.2018.1479258", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two new continuum contact models for coupled adhesion and\nfriction, and discuss them in the context of existing models proposed in the\nliterature. Our new models are able to describe sliding friction even under\ntensile normal forces, which seems reasonable for certain adhesion mechanisms.\nIn contrast, existing continuum models for combined adhesion and friction\ntypically include sliding friction only if local contact stresses are\ncompressive. Although such models work well for structures with sufficiently\nstrong local compression, they fail to capture sliding friction for soft and\ncompliant systems (like adhesive pads), for which the resistance to bending is\nlow. This can be overcome with our new models. For further motivation, we\nadditionally present experimental results for the onset of sliding of a smooth\nglass plate on a smooth elastomer cap under low normal loads. As shown, the\nfindings from these experiments agree well with the results from our models. In\nthis paper we focus on the motivation and derivation of our continuum contact\nmodels, and provide a corresponding literature survey. Their implementation in\na nonlinear finite element framework as well as the algorithmic treatment of\nadhesion and friction will be discussed in future work.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 19:29:08 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 19:00:56 GMT"}, {"version": "v3", "created": "Sun, 20 Oct 2019 13:45:47 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Mergel", "Janine C.", ""], ["Sahli", "Riad", ""], ["Scheibert", "Julien", ""], ["Sauer", "Roger A.", ""]]}, {"id": "1803.00053", "submitter": "Saptarshi Das", "authors": "Valentina Bono, Saptarshi Das, Wasifa Jamal, Koushik Maharatna", "title": "Hybrid Wavelet and EMD/ICA Approach for Artifact Suppression in\n  Pervasive EEG", "comments": "45 pages, 47 figures", "journal-ref": "Journal of Neuroscience Methods (Elsevier), Volume 267, 15 July\n  2016, Pages 89-107", "doi": "10.1016/j.jneumeth.2016.04.006", "report-no": null, "categories": "physics.med-ph cs.CE physics.bio-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalogram (EEG) signals are often corrupted with unintended\nartifacts which need to be removed for extracting meaningful clinical\ninformation from them. Typically a priori knowledge of the nature of the\nartifacts is needed for such purpose. Artifact contamination of EEG is even\nmore prominent for pervasive EEG systems where the subjects are free to move\nand thereby introducing a wide variety of motion-related artifacts. This makes\nhard to get a priori knowledge about their characteristics rendering\nconventional artifact removal techniques often ineffective. In this paper, we\nexplore the performance of two hybrid artifact removal algorithms: Wavelet\npacket transform followed by Independent Component Analysis (WPTICA) and\nWavelet Packet Transform followed by Empirical Mode Decomposition (WPTEMD) in\npervasive EEG recording scenario, assuming existence of no a priori knowledge\nabout the artifacts and compare their performance with two existing artifact\nremoval algorithms. Artifact cleaning performance has been measured using Root\nMean Square Error (RMSE) and Artifact to Signal Ratio (ASR) - an index similar\nto traditional Signal to Noise Ratio (SNR), and also by observing normalized\npower distribution topography over the scalp. Comparison has been made first\nusing semi-simulated signals and then with real experimentally acquired EEG\ndata with commercially available 19-channel pervasive EEG system Enobio\ncorrupted by eight types of artifact. Our explorations show that WPTEMD\nconsistently gives best artifact cleaning performance not only in\nsemi-simulated scenario but also in the case of real EEG data containing\nartifacts.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 13:33:35 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Bono", "Valentina", ""], ["Das", "Saptarshi", ""], ["Jamal", "Wasifa", ""], ["Maharatna", "Koushik", ""]]}, {"id": "1803.00160", "submitter": "Hamid Foroughi", "authors": "Hamid Foroughi, Hamidreza Askarieh Yazdi, Mojtaba Azhari", "title": "Buckling of thin composite plates reinforced with randomly oriented,\n  straight single-walled carbon nanotubes using B3-Spline finite strip method", "comments": null, "journal-ref": "1st National Conference on New Materials and Systems in Civil\n  Engineering, Graduate University of Advanced Technology, Kerman, Iran, 2013", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the mechanical buckling analysis of thin composite\nplates under straight single-walled carbon nanotubes reinforcement with uniform\ndistribution and random orientations. In order to develop the fundamental\nequations, the B3-Spline finite strip method along with the classical plate\ntheory is employed and the total potential energy is minimized which leads to\nan eigenvalue problem. For deriving the effective modulus of thin composite\nplates reinforced with carbon nanotubes, the Mori-Tanaka method is used in\nwhich each straight carbon nanotube is modeled as a fiber with transversely\nisotropic elastic properties. The results of our numerical experiments\nincluding the critical buckling loads for rectangular thin composite plates\nreinforced by carbon nanotubes with various boundary conditions and different\nvolume fractions of nanotubes are provided and the positive effect of using\ncarbon nanotubes reinforcement in mechanical buckling of thin plates is\nillustrated.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 01:51:49 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Foroughi", "Hamid", ""], ["Yazdi", "Hamidreza Askarieh", ""], ["Azhari", "Mojtaba", ""]]}, {"id": "1803.00919", "submitter": "Bang Liu", "authors": "Bang Liu, Borislav Mavrin, Di Niu, Linglong Kong", "title": "House Price Modeling over Heterogeneous Regions with Hierarchical\n  Spatial Functional Analysis", "comments": "Accepted by ICDM 2016; 6 pages", "journal-ref": null, "doi": "10.1109/ICDM.2016.0134", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online real-estate information systems such as Zillow and Trulia have gained\nincreasing popularity in recent years. One important feature offered by these\nsystems is the online home price estimate through automated data-intensive\ncomputation based on housing information and comparative market value analysis.\nState-of-the-art approaches model house prices as a combination of a latent\nland desirability surface and a regression from house features. However, by\nusing uniformly damping kernels, they are unable to handle irregularly shaped\nregions or capture land value discontinuities within the same region due to the\nexistence of implicit sub-communities, which are common in real-world\nscenarios. In this paper, we explore the novel application of recent advances\nin spatial functional analysis to house price modeling and propose the\nHierarchical Spatial Functional Model (HSFM), which decomposes house values\ninto land desirability at both the global scale and hidden local scales as well\nas the feature regression component. We propose statistical learning algorithms\nbased on finite-element spatial functional analysis and spatial constrained\nclustering to train our model. Extensive evaluations based on housing data in a\nmajor Canadian city show that our proposed approach can reduce the mean\nrelative house price estimation error down to 6.60%.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 03:41:55 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Liu", "Bang", ""], ["Mavrin", "Borislav", ""], ["Niu", "Di", ""], ["Kong", "Linglong", ""]]}, {"id": "1803.01211", "submitter": "Amritanshu Pandey", "authors": "Amritanshu Pandey, Marko Jereminov, Martin R. Wagner, David M.\n  Bromberg, Gabriela Hug and Larry Pileggi", "title": "Robust Power Flow and Three-Phase Power Flow Analyses", "comments": "Accepted manuscript for IEEE Transactions on Power Systems (under\n  review). arXiv admin note: text overlap with arXiv:1711.01471", "journal-ref": "IEEE Transactions on Power Systems, 2018", "doi": "10.1109/TPWRS.2018.2863042", "report-no": null, "categories": "eess.SP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust simulation is essential for reliable operation and planning of\ntransmission and distribution power grids. At present, disparate methods exist\nfor steady-state analysis of the transmission (power flow) and distribution\npower grid (three-phase power flow). Due to the non-linear nature of the\nproblem, it is difficult for alternating current (AC) power flow and\nthree-phase power flow analyses to ensure convergence to the correct physical\nsolution, particularly from arbitrary initial conditions, or when evaluating a\nchange (e.g. contingency) in the grid. In this paper, we describe our\nequivalent circuit formulation approach with current and voltage variables that\nmodels both the positive sequence network of the transmission grid and\nthree-phase network of the distribution grid without loss of generality. The\nproposed circuit models and formalism enable the extension and application of\ncircuit simulation techniques to solve for the steady-state solution with\nexcellent robustness of convergence. Examples for positive sequence\ntransmission and three-phase distribution systems, including actual 75k+ nodes\nEastern Interconnection transmission test cases and 8k+ nodes taxonomy\ndistribution test cases, are solved from arbitrary initial guesses to\ndemonstrate the efficacy of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 18:02:25 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 23:46:53 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Pandey", "Amritanshu", ""], ["Jereminov", "Marko", ""], ["Wagner", "Martin R.", ""], ["Bromberg", "David M.", ""], ["Hug", "Gabriela", ""], ["Pileggi", "Larry", ""]]}, {"id": "1803.01451", "submitter": "Yugandhar Sarkale", "authors": "Saeed Nozhati, Yugandhar Sarkale, Bruce Ellingwood, Edwin K. P. Chong,\n  Hussam Mahmoud", "title": "Near-optimal planning using approximate dynamic programming to enhance\n  post-hazard community resilience management", "comments": "The first two authors contributed equally and are listed\n  alphabetically", "journal-ref": "Reliab. Eng. & Syst. Saf. 181 (2019) 116-126", "doi": "10.1016/j.ress.2018.09.011", "report-no": null, "categories": "math.OC cs.CE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of a comprehensive decision-making approach at the community level\nis an important problem that warrants immediate attention. Network-level\ndecision-making algorithms need to solve large-scale optimization problems that\npose computational challenges. The complexity of the optimization problems\nincreases when various sources of uncertainty are considered. This research\nintroduces a sequential discrete optimization approach, as a decision-making\nframework at the community level for recovery management. The proposed\nmathematical approach leverages approximate dynamic programming along with\nheuristics for the determination of recovery actions. Our methodology overcomes\nthe curse of dimensionality and manages multi-state, large-scale infrastructure\nsystems following disasters. We also provide computational results showing that\nour methodology not only incorporates recovery policies of responsible public\nand private entities within the community but also substantially enhances the\nperformance of their underlying strategies with limited resources. The\nmethodology can be implemented efficiently to identify near-optimal recovery\ndecisions following a severe earthquake based on multiple objectives for an\nelectrical power network of a testbed community coarsely modeled after Gilroy,\nCalifornia, United States. The proposed optimization method supports\nrisk-informed community decision makers within chaotic post-hazard\ncircumstances.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 01:22:22 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 02:49:12 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Nozhati", "Saeed", ""], ["Sarkale", "Yugandhar", ""], ["Ellingwood", "Bruce", ""], ["Chong", "Edwin K. P.", ""], ["Mahmoud", "Hussam", ""]]}, {"id": "1803.02800", "submitter": "Sebastian Roch", "authors": "Sebastien Roch and Michael Nute and Tandy Warnow", "title": "Long-branch attraction in species tree estimation: inconsistency of\n  partitioned likelihood and topology-based summary methods", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in sequencing technologies, there are now massive amounts of\ngenomic data from across all life, leading to the possibility that a robust\nTree of Life can be constructed. However, \"gene tree heterogeneity\", which is\nwhen different genomic regions can evolve differently, is a common phenomenon\nin multi-locus datasets, and reduces the accuracy of standard methods for\nspecies tree estimation that do not take this heterogeneity into account. New\nmethods have been developed for species tree estimation that specifically\naddress gene tree heterogeneity, and that have been proven to converge to the\ntrue species tree when the number of loci and number of sites per locus both\nincrease (i.e., the methods are said to be \"statistically consistent\"). Yet,\nlittle is known about the biologically realistic condition where the number of\nsites per locus is bounded. We show that when the sequence length of each locus\nis bounded (by any arbitrarily chosen value), the most common approaches to\nspecies tree estimation that take heterogeneity into account (i.e., traditional\nfully partitioned concatenated maximum likelihood and newer approaches, called\nsummary methods, that estimate the species tree by combining gene trees) are\nnot statistically consistent, even when the heterogeneity is extremely\nconstrained. The main challenge is the presence of conditions such as long\nbranch attraction that create biased tree estimation when the number of sites\nis restricted. Hence, our study uncovers a fundamental challenge to species\ntree estimation using both traditional and new methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 18:19:21 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Roch", "Sebastien", ""], ["Nute", "Michael", ""], ["Warnow", "Tandy", ""]]}, {"id": "1803.02977", "submitter": "Richard Barnes", "authors": "Richard Barnes", "title": "Accelerating a fluvial incision and landscape evolution model with\n  parallelism", "comments": "13 pages, 12 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.geomorph.2019.01.002", "report-no": null, "categories": "cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving inverse problems and achieving statistical rigour in landscape\nevolution models requires running many model realizations. Parallel computation\nis necessary to achieve this in a reasonable time. However, no previous\nalgorithm is well-suited to leveraging modern parallelism. Here, I describe an\nalgorithm that can utilize the parallel potential of GPUs, many-core\nprocessors, and SIMD instructions, in addition to working well in serial. The\nnew algorithm runs 43x faster (70s vs. 3,000s on a 10,000x10,000 input) than\nthe previous state of the art and exhibits sublinear scaling with input size. I\nalso identify methods for using multidirectional flow routing and quickly\neliminating landscape depressions and local minima. Tips for parallelization\nand a step-by-step guide to achieving it are given to help others achieve good\nperformance with their own code. Complete, well-commented, easily adaptable\nsource code for all versions of the algorithm is available as a supplement and\non Github.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 05:58:54 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Barnes", "Richard", ""]]}, {"id": "1803.03861", "submitter": "Nils Bertschinger", "authors": "Nils Bertschinger and Iurii Mozzhorin and Sitabhra Sinha", "title": "Reality-check for Econophysics: Likelihood-based fitting of\n  physics-inspired market models to empirical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical description and modeling of volatility plays a prominent role\nin econometrics, risk management and finance. GARCH and stochastic volatility\nmodels have been extensively studied and are routinely fitted to market data,\nalbeit providing a phenomenological description only.\n  In contrast, the field of econophysics starts from the premise that modern\neconomies consist of a vast number of individual actors with heterogeneous\nexpectations and incentives. In turn explaining observed market statistics as\nemerging from the collective dynamics of many actors following heterogeneous,\nyet simple, rather mechanistic rules. While such models generate volatility\ndynamics qualitatively matching several stylized facts and thus illustrate the\npossible role of different mechanisms, such as chartist trading, herding\nbehavior etc., rigorous and quantitative statistical fits are still mostly\nlacking.\n  Here, we show how Stan, a modern probabilistic programming language for\nBayesian modeling, can be used to fit several models from econophysics. In\ncontrast to the method of moment matching, which is currently popular, our fits\nare purely likelihood based with many advantages, including systematic model\ncomparison and principled generation of model predictions conditional on the\nobserved price history. In particular, we investigate models by Vikram & Sinha\nand Franke & Westerhoff, and provide a quantitative comparison with standard\neconometric models.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 21:08:57 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Bertschinger", "Nils", ""], ["Mozzhorin", "Iurii", ""], ["Sinha", "Sitabhra", ""]]}, {"id": "1803.04144", "submitter": "Yugandhar Sarkale", "authors": "Yugandhar Sarkale, Saeed Nozhati, Edwin K.P. Chong, Bruce Ellingwood,\n  Hussam Mahmoud", "title": "Solving Markov decision processes for network-level post-hazard recovery\n  via simulation optimization and rollout", "comments": "Submitted to Simulation Optimization for Cyber Physical Energy\n  Systems (Special Session) in 14th IEEE International Conference on Automation\n  Science and Engineering", "journal-ref": "2018 IEEE 14th Int. Conf. Autom. Sci. and Eng. (CASE) 906 - 912", "doi": "10.1109/COASE.2018.8560473", "report-no": null, "categories": "math.OC cs.CE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of optimal recovery decisions for community resilience assurance\npost-hazard is a combinatorial decision-making problem under uncertainty. It\ninvolves solving a large-scale optimization problem, which is significantly\naggravated by the introduction of uncertainty. In this paper, we draw upon\nestablished tools from multiple research communities to provide an effective\nsolution to this challenging problem. We provide a stochastic model of damage\nto the water network (WN) within a testbed community following a severe\nearthquake and compute near-optimal recovery actions for restoration of the\nwater network. We formulate this stochastic decision-making problem as a Markov\nDecision Process (MDP), and solve it using a popular class of heuristic\nalgorithms known as rollout. A simulation-based representation of MDPs is\nutilized in conjunction with rollout and the Optimal Computing Budget\nAllocation (OCBA) algorithm to address the resulting stochastic simulation\noptimization problem. Our method employs non-myopic planning with efficient use\nof simulation budget. We show, through simulation results, that rollout fused\nwith OCBA performs competitively with respect to rollout with total equal\nallocation (TEA) at a meagre simulation budget of 5-10% of rollout with TEA,\nwhich is a crucial step towards addressing large-scale community recovery\nproblems following natural disasters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 07:54:30 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 02:39:44 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Sarkale", "Yugandhar", ""], ["Nozhati", "Saeed", ""], ["Chong", "Edwin K. P.", ""], ["Ellingwood", "Bruce", ""], ["Mahmoud", "Hussam", ""]]}, {"id": "1803.04346", "submitter": "Praveen Chandrashekar", "authors": "Deepak Varma, Praveen Chandrashekar", "title": "A second-order, discretely well-balanced finite volume scheme for Euler\n  equations with gravity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a well-balanced, second order, Godunov-type finite volume scheme\nfor compressible Euler equations with gravity. By construction, the scheme\nadmits a discrete stationary solution which is a second order accurate\napproximation to the exact stationary solution. Such a scheme is useful for\nproblems involving complex equations of state and/or hydrostatic solutions\nwhich are not known in closed form expression. No \\'a priori knowledge of the\nhydrostatic solution is required to achieve the well-balanced property. The\nperformance of the scheme is demonstrated on several test cases in terms of\npreservation of hydrostatic solution and computation of small perturbations\naround a hydrostatic solution.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:13:19 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 04:02:17 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Varma", "Deepak", ""], ["Chandrashekar", "Praveen", ""]]}, {"id": "1803.04774", "submitter": "Rion Brattig Correia", "authors": "Rion Brattig Correia and Alexander J. Gates and Xuan Wang and Luis M.\n  Rocha", "title": "CANA: A python package for quantifying control and canalization in\n  Boolean Networks", "comments": "Submitted to the Systems Biology section of Frontiers in Physiology", "journal-ref": "Frontiers in Physiology, 9:1046, 2018", "doi": "10.3389/fphys.2018.01046", "report-no": null, "categories": "cs.OH cs.CE cs.DM cs.SY q-bio.MN q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logical models offer a simple but powerful means to understand the complex\ndynamics of biochemical regulation, without the need to estimate kinetic\nparameters. However, even simple automata components can lead to collective\ndynamics that are computationally intractable when aggregated into networks. In\nprevious work we demonstrated that automata network models of biochemical\nregulation are highly canalizing, whereby many variable states and their\ngroupings are redundant (Marques-Pita and Rocha, 2013). The precise charting\nand measurement of such canalization simplifies these models, making even very\nlarge networks amenable to analysis. Moreover, canalization plays an important\nrole in the control, robustness, modularity and criticality of Boolean network\ndynamics, especially those used to model biochemical regulation (Gates and\nRocha, 2016; Gates et al., 2016; Manicka, 2017). Here we describe a new\npublicly-available Python package that provides the necessary tools to extract,\nmeasure, and visualize canalizing redundancy present in Boolean network models.\nIt extracts the pathways most effective in controlling dynamics in these\nmodels, including their effective graph and dynamics canalizing map, as well as\nother tools to uncover minimum sets of control variables.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 20:07:52 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 18:15:06 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Correia", "Rion Brattig", ""], ["Gates", "Alexander J.", ""], ["Wang", "Xuan", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1803.04813", "submitter": "Saptarshi Das", "authors": "Daya Shankar Pandey, Saptarshi Das, Indranil Pan, James J. Leahy,\n  Witold Kwapinski", "title": "Artificial neural network based modelling approach for municipal solid\n  waste gasification in a fluidized bed reactor", "comments": "34 pages, 11 figures", "journal-ref": "Waste Management (Elsevier), Volume 58, December 2016, Pages\n  202-213", "doi": "10.1016/j.wasman.2016.08.023", "report-no": null, "categories": "cs.LG cs.CE cs.NE physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, multi-layer feed forward neural networks are used to predict\nthe lower heating value of gas (LHV), lower heating value of gasification\nproducts including tars and entrained char (LHVp) and syngas yield during\ngasification of municipal solid waste (MSW) during gasification in a fluidized\nbed reactor. These artificial neural networks (ANNs) with different\narchitectures are trained using the Levenberg-Marquardt (LM) back-propagation\nalgorithm and a cross validation is also performed to ensure that the results\ngeneralise to other unseen datasets. A rigorous study is carried out on\noptimally choosing the number of hidden layers, number of neurons in the hidden\nlayer and activation function in a network using multiple Monte Carlo runs.\nNine input and three output parameters are used to train and test various\nneural network architectures in both multiple output and single output\nprediction paradigms using the available experimental datasets. The model\nselection procedure is carried out to ascertain the best network architecture\nin terms of predictive accuracy. The simulation results show that the ANN based\nmethodology is a viable alternative which can be used to predict the\nperformance of a fluidized bed gasifier.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 21:50:22 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Pandey", "Daya Shankar", ""], ["Das", "Saptarshi", ""], ["Pan", "Indranil", ""], ["Leahy", "James J.", ""], ["Kwapinski", "Witold", ""]]}, {"id": "1803.05482", "submitter": "Evgeny Burnaev", "authors": "Vladimir Ignatiev, Alexey Trekin, Viktor Lobachev, Georgy Potapov,\n  Evgeny Burnaev", "title": "Targeted change detection in remote sensing images", "comments": "10 pages, 1 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in the remote sensing systems and image processing made\nit possible to propose a new method of the object classification and detection\nof the specific changes in the series of satellite Earth images (so called\ntargeted change detection). In this paper we propose a formal problem statement\nthat allows to use effectively the deep learning approach to analyze\ntime-dependent series of remote sensing images. We also introduce a new\nframework for the development of deep learning models for targeted change\ndetection and demonstrate some cases of business applications it can be used\nfor.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 19:09:24 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Ignatiev", "Vladimir", ""], ["Trekin", "Alexey", ""], ["Lobachev", "Viktor", ""], ["Potapov", "Georgy", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1803.06635", "submitter": "Andre Massing", "authors": "Ceren G\\\"urkan and Andr\\'e Massing", "title": "A stabilized cut discontinuous Galerkin framework: I. Elliptic boundary\n  value and interface problems", "comments": "35 pages, 12 figures, 2 tables", "journal-ref": null, "doi": "10.1016/j.cma.2018.12.041", "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a stabilized cut discontinuous Galerkin framework for the\nnumerical solution of el- liptic boundary value and interface problems on\ncomplicated domains. The domain of interest is embedded in a structured,\nunfitted background mesh in R d , so that the boundary or interface can cut\nthrough it in an arbitrary fashion. The method is based on an unfitted variant\nof the classical symmetric interior penalty method using piecewise\ndiscontinuous polynomials defined on the back- ground mesh. Instead of the cell\nagglomeration technique commonly used in previously introduced unfitted\ndiscontinuous Galerkin methods, we employ and extend ghost penalty techniques\nfrom recently developed continuous cut finite element methods, which allows for\na minimal extension of existing fitted discontinuous Galerkin software to\nhandle unfitted geometries. Identifying four abstract assumptions on the ghost\npenalty, we derive geometrically robust a priori error and con- dition number\nestimates for the Poisson boundary value problem which hold irrespective of the\nparticular cut configuration. Possible realizations of suitable ghost penalties\nare discussed. We also demonstrate how the framework can be elegantly applied\nto discretize high contrast interface problems. The theoretical results are\nillustrated by a number of numerical experiments for various approximation\norders and for two and three-dimensional test problems.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 10:25:30 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 09:03:57 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["G\u00fcrkan", "Ceren", ""], ["Massing", "Andr\u00e9", ""]]}, {"id": "1803.07016", "submitter": "Louis Viot", "authors": "Louis Viot, Laurent Saas and Florian De Vuyst", "title": "Solving coupled problems of lumped parameter models in a platform for\n  severe accidents in nuclear reactors", "comments": null, "journal-ref": null, "doi": "10.1615/IntJMultCompEng.2018025643", "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on solving coupled problems of lumped parameter models.\nSuch problems are of interest for the simulation of severe accidents in nuclear\nreactors~: these coarse-grained models allow for fast calculations for\nstatistical analysis used for risk assessment and solutions of large problems\nwhen considering the whole severe accident scenario. However, this modeling\napproach has several numerical flaws. Besides, in this industrial context,\ncomputational efficiency is of great importance leading to various numerical\nconstraints. The objective of this research is to analyze the applicability of\nexplicit coupling strategies to solve such coupled problems and to design\nimplicit coupling schemes allowing stable and accurate computations. The\nproposed schemes are theoretically analyzed and tested within CEA's procor\nplatform on a problem of heat conduction solved with coupled lumped parameter\nmodels and coupled 1D models. Numerical results are discussed and allow us to\nemphasize the benefits of using the designed coupling schemes instead of the\nusual explicit coupling schemes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 16:20:08 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Viot", "Louis", ""], ["Saas", "Laurent", ""], ["De Vuyst", "Florian", ""]]}, {"id": "1803.09122", "submitter": "Sebastian Sch\\\"ops", "authors": "Armin Galetzka, Zeger Bontinck, Ulrich R\\\"omer, Sebastian Sch\\\"ops", "title": "A multilevel Monte Carlo method for high-dimensional uncertainty\n  quantification of low-frequency electromagnetic devices", "comments": null, "journal-ref": "IEEE Transactions on Magnetics, Volume: 55, Issue: 8, Aug. 2019,\n  Number: 7401712", "doi": "10.1109/TMAG.2019.2911053", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses uncertainty quantification of electromagnetic devices\ndetermined by the eddy current problem. The multilevel Monte Carlo (MLMC)\nmethod is used for the treatment of uncertain parameters while the devices are\ndiscretized in space by the finite element method. Both methods yield numerical\napproximations such that the total errors is split into stochastic and spatial\ncontributions. We propose a particular implementation where the spatial error\nis controlled based on a Richardson-extrapolation-based error indicator. The\nstochastic error in turn is efficiently reduced in the MLMC approach by\ndistributing the samples on multiple grids. The method is applied to a toy\nproblem with closed-form solution and a permanent magnet synchronous machine\nwith uncertainties. The uncertainties under consideration are related to the\nmaterial properties in the stator and the magnets in the rotor. The examples\nshow that the error indicator works reliably, the meshes used for the different\nlevels do not have to be nested and, most importantly, MLMC reduces the\ncomputational cost by at least one order of magnitude compared to standard\nMonte Carlo.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 15:03:51 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 11:03:16 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Galetzka", "Armin", ""], ["Bontinck", "Zeger", ""], ["R\u00f6mer", "Ulrich", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "1803.09136", "submitter": "Gabriel Spadon", "authors": "Gabriel Spadon, Bruno B. Machado, Danilo M. Eler, Jose Fernando\n  Rodrigues-Jr", "title": "A distance-based tool-set to track inconsistent urban structures through\n  complex-networks", "comments": "Paper to be published on the International Conference on\n  Computational Science (ICCS), 2018", "journal-ref": null, "doi": "10.1007/978-3-319-93698-7_22", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks can be used for modeling street meshes and urban\nagglomerates. With such a model, many aspects of a city can be investigated to\npromote a better quality of life to its citizens. Along these lines, this paper\nproposes a set of distance-based pattern-discovery algorithmic instruments to\nimprove urban structures modeled as complex networks, detecting nodes that lack\naccess from/to points of interest in a given city. Furthermore, we introduce a\ngreedy algorithm that is able to recommend improvements to the structure of a\ncity by suggesting where points of interest are to be placed. We contribute to\na thorough process to deal with complex networks, including mathematical\nmodeling and algorithmic innovation. The set of our contributions introduces a\nsystematic manner to treat a recurrent problem of broad interest in cities.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 17:43:46 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Spadon", "Gabriel", ""], ["Machado", "Bruno B.", ""], ["Eler", "Danilo M.", ""], ["Rodrigues-Jr", "Jose Fernando", ""]]}, {"id": "1803.09632", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "Big Data Challenges in Genome Informatics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.OT cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have witnessed a dramatic data explosion in genomics,\nthanks to the improvement in sequencing technologies and the drastically\ndecreasing costs. We are entering the era of millions of available genomes.\nNotably, each genome can be composed of billions of nucleotides stored as plain\ntext files in GigaBytes (GBs). It is undeniable that those genome data impose\nunprecedented data challenges for us. In this article, we briefly discuss the\nbig data challenges associated with genomics in recent years.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 18:23:42 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Wong", "Ka-Chun", ""]]}, {"id": "1803.09948", "submitter": "Mustafa Abduljabbar", "authors": "Mustafa Abduljabbar, Mohammed Al Farhan, Noha Al-Harthi, Rui Chen, Rio\n  Yokota, Hakan Bagci, and David Keyes", "title": "Extreme Scale FMM-Accelerated Boundary Integral Equation Solver for Wave\n  Scattering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic and architecture-oriented optimizations are essential for\nachieving performance worthy of anticipated energy-austere exascale systems. In\nthis paper, we present an extreme scale FMM-accelerated boundary integral\nequation solver for wave scattering, which uses FMM as a matrix-vector\nmultiplication inside the GMRES iterative method. Our FMM Helmholtz kernels\ntreat nontrivial singular and near-field integration points. We implement\nhighly optimized kernels for both shared and distributed memory, targeting\nemerging Intel extreme performance HPC architectures. We extract the potential\nthread- and data-level parallelism of the key Helmholtz kernels of FMM. Our\napplication code is well optimized to exploit the AVX-512 SIMD units of Intel\nSkylake and Knights Landing architectures. We provide different performance\nmodels for tuning the task-based tree traversal implementation of FMM, and\ndevelop optimal architecture-specific and algorithm aware partitioning, load\nbalancing, and communication reducing mechanisms to scale up to 6,144 compute\nnodes of a Cray XC40 with 196,608 hardware cores. With shared memory\noptimizations, we achieve roughly 77% of peak single precision floating point\nperformance of a 56-core Skylake processor, and on average 60% of peak single\nprecision floating point performance of a 72-core KNL. These numbers represent\nnearly 5.4x and 10x speedup on Skylake and KNL, respectively, compared to the\nbaseline scalar code. With distributed memory optimizations, on the other hand,\nwe report near-optimal efficiency in the weak scalability study with respect to\nboth the logarithmic communication complexity as well as the theoretical\nscaling complexity of FMM. In addition, we exhibit up to 85% efficiency in\nstrong scaling. We compute in excess of 2 billion DoF on the full-scale of the\nCray XC40 supercomputer.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 08:12:13 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Abduljabbar", "Mustafa", ""], ["Farhan", "Mohammed Al", ""], ["Al-Harthi", "Noha", ""], ["Chen", "Rui", ""], ["Yokota", "Rio", ""], ["Bagci", "Hakan", ""], ["Keyes", "David", ""]]}, {"id": "1803.10551", "submitter": "Bilen Emek Abali", "authors": "B. E. Abali and A. F. Queiruga", "title": "Theory and computation of electromagnetic fields and thermomechanical\n  structure interaction for systems undergoing large deformations", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2019.05.045", "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an accurate description of electromagneto-thermomechanical systems,\nelectromagnetic fields need to be described in a Eulerian frame, whereby the\nthermomechanics is solved in a Lagrangean frame. It is possible to map the\nEulerian frame to the current placement of the matter and the Lagrangean frame\nto a reference placement. We present a rigorous and thermodynamically\nconsistent derivation of governing equations for fully coupled\nelectromagneto-thermomechanical systems properly handling finite deformations.\nA clear separation of the different frames is necessary. There are various\nattempts to formulate electromagnetism in the Lagrangean frame, or even to\ncompute all fields in the current placement. Both formulations are challenging\nand heavily discussed in the literature. In this work, we propose another\nsolution scheme that exploits the capabilities of advanced computational tools.\nInstead of amending the formulation, we can solve thermomechanics in the\nLagrangean frame and electromagnetism in the Eulerian frame and manage the\ninteraction between the fields. The approach is similar to its analog in fluid\nstructure interaction, but more challenging because the field equations in\nelectromagnetism must also be solved within the solid body while following\ntheir own different set of transformation rules. We additionally present a\nmesh-morphing algorithm necessary to accommodate finite deformations to solve\nthe electromagnetic fields outside of the material body. We illustrate the use\nof the new formulation by developing an open-source implementation using the\nFEniCS package and applying this implementation to several engineering problems\nin electromagnetic structure interaction undergoing large deformations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 12:09:16 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 09:19:36 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Abali", "B. E.", ""], ["Queiruga", "A. F.", ""]]}, {"id": "1803.10848", "submitter": "Maximilian Sch\\\"afer", "authors": "Maximilian Sch\\\"afer, Wayan Wicke, Rudolf Rabenstein and Robert\n  Schober", "title": "Analytical Models for Particle Diffusion and Flow in a Horizontal\n  Cylinder with a Vertical Force", "comments": "7 pages, 4 figures, 1 table, Accepted for publication at IEEE\n  International Conference on Communications (ICC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers particle propagation in a cylindrical molecular\ncommunication channel, e.g. a simplified model of a blood vessel. Emitted\nparticles are influenced by diffusion, flow, and a vertical force induced e.g.\nby gravity or magnetism. The dynamics of the diffusion process are modeled by\nmulti-dimensional transfer functions in a spatio-temporal frequency domain.\nRealistic boundary conditions are incorporated by the design of a feedback\nloop. The result is a discrete-time semi-analytical model for the particle\nconcentration in the channel. The model is validated by comparison to\nparticle-based simulations. These numerical experiments reveal that the\nparticle concentration of the proposed semi-analytical model and the\nparticle-based model are in excellent agreement. The analytical form of the\nproposed solution provides several benefits over purely numerical models, e.g.\nhigh flexibility, existence of low run-time algorithms, extendability to\nseveral kinds of boundary conditions, and analytical connection to parameters\nfrom communication theory.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 20:53:09 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 14:16:20 GMT"}, {"version": "v3", "created": "Sat, 23 Feb 2019 11:28:09 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Sch\u00e4fer", "Maximilian", ""], ["Wicke", "Wayan", ""], ["Rabenstein", "Rudolf", ""], ["Schober", "Robert", ""]]}]