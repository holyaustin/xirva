[{"id": "1209.0127", "submitter": "Alexandra Faynburd Mrs", "authors": "Ran El-Yaniv, Alexandra Faynburd", "title": "Autoregressive short-term prediction of turning points using support\n  vector regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with autoregressive prediction of turning points in\nfinancial price sequences. Such turning points are critical local extrema\npoints along a series, which mark the start of new swings. Predicting the\nfuture time of such turning points or even their early or late identification\nslightly before or after the fact has useful applications in economics and\nfinance. Building on recently proposed neural network model for turning point\nprediction, we propose and study a new autoregressive model for predicting\nturning points of small swings. Our method relies on a known turning point\nindicator, a Fourier enriched representation of price histories, and support\nvector regression. We empirically examine the performance of the proposed\nmethod over a long history of the Dow Jones Industrial average. Our study shows\nthat the proposed method is superior to the previous neural network model, in\nterms of trading performance of a simple trading application and also exhibits\na quantifiable advantage over the buy-and-hold benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2012 19:53:23 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2012 19:28:24 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["El-Yaniv", "Ran", ""], ["Faynburd", "Alexandra", ""]]}, {"id": "1209.0616", "submitter": "Zyed Bouzarkouna", "authors": "Zyed Bouzarkouna, Didier Yu Ding (IFPEN), Anne Auger (INRIA Saclay -\n  Ile de France)", "title": "Well Placement Optimization under Uncertainty with CMA-ES Using the\n  Neighborhood", "comments": null, "journal-ref": "13th European Conference on the Mathematics of Oil Recovery ECMOR\n  2012 (2012)", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the well placement problem, as well as in other field development\noptimization problems, geological uncertainty is a key source of risk affecting\nthe viability of field development projects. Well placement problems under\ngeological uncertainty are formulated as optimization problems in which the\nobjective function is evaluated using a reservoir simulator on a number of\npossible geological realizations. In this paper, we present a new approach to\nhandle geological uncertainty for the well placement problem with a reduced\nnumber of reservoir simulations. The proposed approach uses already simulated\nwell configurations in the neighborhood of each well configuration for the\nobjective function evaluation. We use thus only one single reservoir simulation\nperformed on a randomly chosen realization together with the neighborhood to\nestimate the objective function instead of using multiple simulations on\nmultiple realizations. This approach is combined with the stochastic optimizer\nCMA-ES. The proposed approach is shown on the benchmark reservoir case PUNQ-S3\nto be able to capture the geological uncertainty using a smaller number of\nreservoir simulations. This approach is compared to the reference approach\nusing all the possible realizations for each well configuration, and shown to\nbe able to reduce significantly the number of reservoir simulations (around\n80%).\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2012 11:52:14 GMT"}], "update_date": "2012-09-05", "authors_parsed": [["Bouzarkouna", "Zyed", "", "IFPEN"], ["Ding", "Didier Yu", "", "IFPEN"], ["Auger", "Anne", "", "INRIA Saclay -\n  Ile de France"]]}, {"id": "1209.1711", "submitter": "Matthew Knepley", "authors": "Matthew G. Knepley", "title": "Programming Languages for Scientific Computing", "comments": "21 pages", "journal-ref": "Encyclopedia of Applied and Computational Mathematics, Springer,\n  2012", "doi": "10.1007/978-3-540-70529-1", "report-no": null, "categories": "cs.PL cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific computation is a discipline that combines numerical analysis,\nphysical understanding, algorithm development, and structured programming.\nSeveral yottacycles per year on the world's largest computers are spent\nsimulating problems as diverse as weather prediction, the properties of\nmaterial composites, the behavior of biomolecules in solution, and the quantum\nnature of chemical compounds. This article is intended to review specfic\nlanguages features and their use in computational science. We will review the\nstrengths and weaknesses of different programming styles, with examples taken\nfrom widely used scientific codes.\n", "versions": [{"version": "v1", "created": "Sat, 8 Sep 2012 12:31:50 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 15:22:10 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Knepley", "Matthew G.", ""]]}, {"id": "1209.2641", "submitter": "Haibin Wang", "authors": "Haibin Wang", "title": "C-PASS-PC: A Cloud-driven Prototype of Multi-Center Proactive\n  Surveillance System for Prostate Cancer", "comments": null, "journal-ref": "IJCSIT 2012", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently there are many clinical trials using paper case report forms as the\nprimary data collection tool. Cloud Computing platforms provide big potential\nfor increasing efficiency through a web-based data collection interface,\nespecially for large-scale multi-center trials. Traditionally, clinical and\nbiological data for multi-center trials are stored in one dedicated,\ncentralized database system running at a data coordinating center (DCC). This\npaper presents C-PASS-PC, a cloud-driven prototype of multi-center proactive\nsurveillance system for prostate cancer. The prototype is developed in PHP,\nJQuery and CSS with an Oracle backend in a local Web server and database server\nand deployed on Google App Engine (GAE) and Google Cloud SQL-MySQL. The\ndeploying process is fast and easy to follow. The C-PASS-PC prototype can be\naccessed through an SSL-enabled web browser. Our approach proves the concept\nthat cloud computing platforms such as GAE is a suitable and flexible solution\nin the near future for multi-center clinical trials.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 15:29:12 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Wang", "Haibin", ""]]}, {"id": "1209.2660", "submitter": "Antonio Andrea Gentile", "authors": "Antonio A. Gentile", "title": "Review of strategies for a comprehensive simulation in sputtering\n  devices", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of sputtering facilities, at the moment, is mainly pursued\nthrough experimental tests, or simply by expertise in the field, and relies\nmuch less on numerical simulation of the process environment. This leads to\ngreat efforts and empirically, roughly optimized solutions: in fact, the\nsimulation of these devices, at the state of art, is quite good in predicting\nthe behavior of single steps of the overall deposition process, but it seems\nstill ahead a full integration among the tools simulating the various phenomena\ninvolved in a sputter. We summarize here the techniques and codes already\navailable for problems of interest in sputtering facilities, and we try to\noutline the possible features of a comprehensive simulation framework. This\nframework should be able to integrate the single paradigms, dealing with\naspects going from the plasma environment up to the distribution and properties\nof the deposited film, not only on the surface of the substrate, but also on\nthe walls of the process chamber.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2012 16:47:23 GMT"}], "update_date": "2012-09-13", "authors_parsed": [["Gentile", "Antonio A.", ""]]}, {"id": "1209.2946", "submitter": "Frederic Rodriguez", "authors": "Fr\\'ed\\'eric Rodriguez (SPCMIB)", "title": "Technical Report: CSVM Ecosystem", "comments": "31 pages including 2p of Annex", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DS q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CSVM format is derived from CSV format and allows the storage of tabular\nlike data with a limited but extensible amount of metadata. This approach could\nhelp computer scientists because all information needed to uses subsequently\nthe data is included in the CSVM file and is particularly well suited for\nhandling RAW data in a lot of scientific fields and to be used as a canonical\nformat. The use of CSVM has shown that it greatly facilitates: the data\nmanagement independently of using databases; the data exchange; the integration\nof RAW data in dataflows or calculation pipes; the search for best practices in\nRAW data management. The efficiency of this format is closely related to its\nplasticity: a generic frame is given for all kind of data and the CSVM parsers\ndon't make any interpretation of data types. This task is done by the\napplication layer, so it is possible to use same format and same parser codes\nfor a lot of purposes. In this document some implementation of CSVM format for\nten years and in different laboratories are presented. Some programming\nexamples are also shown: a Python toolkit for using the format, manipulating\nand querying is available. A first specification of this format (CSVM-1) is now\ndefined, as well as some derivatives such as CSVM dictionaries used for data\ninterchange. CSVM is an Open Format and could be used as a support for Open\nData and long term conservation of RAW or unpublished data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2012 15:44:41 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Rodriguez", "Fr\u00e9d\u00e9ric", "", "SPCMIB"]]}, {"id": "1209.3916", "submitter": "Tom Kelsey", "authors": "Thomas W. Kelsey, Lars Kotthoff, Christoffer A. Jefferson, Stephen A.\n  Linton, Ian Miguel, Peter Nightingale, Ian P. Gent", "title": "Qualitative Modelling via Constraint Programming: Past, Present and\n  Future", "comments": "15 pages plus references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI math.DS q-bio.CB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qualitative modelling is a technique integrating the fields of theoretical\ncomputer science, artificial intelligence and the physical and biological\nsciences. The aim is to be able to model the behaviour of systems without\nestimating parameter values and fixing the exact quantitative dynamics.\nTraditional applications are the study of the dynamics of physical and\nbiological systems at a higher level of abstraction than that obtained by\nestimation of numerical parameter values for a fixed quantitative model.\nQualitative modelling has been studied and implemented to varying degrees of\nsophistication in Petri nets, process calculi and constraint programming. In\nthis paper we reflect on the strengths and weaknesses of existing frameworks,\nwe demonstrate how recent advances in constraint programming can be leveraged\nto produce high quality qualitative models, and we describe the advances in\ntheory and technology that would be needed to make constraint programming the\nbest option for scientific investigation in the broadest sense.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2012 11:57:44 GMT"}], "update_date": "2012-09-19", "authors_parsed": [["Kelsey", "Thomas W.", ""], ["Kotthoff", "Lars", ""], ["Jefferson", "Christoffer A.", ""], ["Linton", "Stephen A.", ""], ["Miguel", "Ian", ""], ["Nightingale", "Peter", ""], ["Gent", "Ian P.", ""]]}, {"id": "1209.4236", "submitter": "Sarod Yatawatta", "authors": "Sarod Yatawatta", "title": "Estimation of Radio Interferometer Beam Shapes Using Riemannian\n  Optimization", "comments": "Accepted on 18-09-2012. Draft version. The final publication is\n  available at springerlink.com ; Experimental Astronomy, 2012", "journal-ref": "Experimental Astronomy, Volume 35, Issue 3, pp.469-487, 2013", "doi": "10.1007/s10686-012-9318-x", "report-no": null, "categories": "astro-ph.IM cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge of receiver beam shapes is essential for accurate radio\ninterferometric imaging. Traditionally, this information is obtained by\nholographic techniques or by numerical simulation. However, such methods are\nnot feasible for an observation with time varying beams, such as the beams\nproduced by a phased array radio interferometer. We propose the use of the\nobserved data itself for the estimation of the beam shapes. We use the\ndirectional gains obtained along multiple sources across the sky for the\nconstruction of a time varying beam model. The construction of this model is an\nill posed non linear optimization problem. Therefore, we propose to use\nRiemannian optimization, where we consider the constraints imposed as a\nmanifold. We compare the performance of the proposed approach with traditional\nunconstrained optimization and give results to show the superiority of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2012 13:25:48 GMT"}], "update_date": "2013-07-19", "authors_parsed": [["Yatawatta", "Sarod", ""]]}, {"id": "1209.4506", "submitter": "Duy Truong", "authors": "Truong Vinh Truong Duy and Taisuke Ozaki", "title": "A three-dimensional domain decomposition method for large-scale DFT\n  electronic structure calculations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With tens of petaflops supercomputers already in operation and exaflops\nmachines expected to appear within the next 10 years, efficient parallel\ncomputational methods are required to take advantage of such extreme-scale\nmachines. In this paper, we present a three-dimensional domain decomposition\nscheme for enabling large-scale electronic calculations based on density\nfunctional theory (DFT) on massively parallel computers. It is composed of two\nmethods: (i) atom decomposition method and (ii) grid decomposition method. In\nthe former, we develop a modified recursive bisection method based on inertia\ntensor moment to reorder the atoms along a principal axis so that atoms that\nare close in real space are also close on the axis to ensure data locality. The\natoms are then divided into sub-domains depending on their projections onto the\nprincipal axis in a balanced way among the processes. In the latter, we define\nfour data structures for the partitioning of grids that are carefully\nconstructed to make data locality consistent with that of the clustered atoms\nfor minimizing data communications between the processes. We also propose a\ndecomposition method for solving the Poisson equation using three-dimensional\nFFT in Hartree potential calculation, which is shown to be better than a\npreviously proposed parallelization method based on a two-dimensional\ndecomposition in terms of communication efficiency. For evaluation, we perform\nbenchmark calculations with our open-source DFT code, OpenMX, paying particular\nattention to the O(N) Krylov subspace method. The results show that our scheme\nexhibits good strong and weak scaling properties, with the parallel efficiency\nat 131,072 cores being 67.7% compared to the baseline of 16,384 cores with\n131,072 diamond atoms on the K computer.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 12:22:04 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Duy", "Truong Vinh Truong", ""], ["Ozaki", "Taisuke", ""]]}, {"id": "1209.4608", "submitter": "Mahesh Khadka", "authors": "Mahesh S. Khadka, K. M. George, N. Park, J. B. Kim", "title": "Performance Analysis of Hybrid Forecasting Model In Stock Market\n  Forecasting", "comments": null, "journal-ref": "International Journal of Managing Information Technology (IJMIT),\n  Vol. 4, No. 3, August 2012", "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents performance analysis of hybrid model comprise of\nconcordance and Genetic Programming (GP) to forecast financial market with some\nexisting models. This scheme can be used for in depth analysis of stock market.\nDifferent measures of concordances such as Kendalls Tau, Ginis Mean Difference,\nSpearmans Rho, and weak interpretation of concordance are used to search for\nthe pattern in past that look similar to present. Genetic Programming is then\nused to match the past trend to present trend as close as possible. Then\nGenetic Program estimates what will happen next based on what had happened\nnext. The concept is validated using financial time series data (S&P 500 and\nNASDAQ indices) as sample data sets. The forecasted result is then compared\nwith standard ARIMA model and other model to analyse its performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 18:50:41 GMT"}, {"version": "v2", "created": "Wed, 15 May 2013 16:29:21 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Khadka", "Mahesh S.", ""], ["George", "K. M.", ""], ["Park", "N.", ""], ["Kim", "J. B.", ""]]}, {"id": "1209.4854", "submitter": "Marta Szilvasi-Nagy Dr", "authors": "M\\'arta Szilv\\'asi-Nagy, Gyula M\\'aty\\'asi, Szilvia B\\'ela", "title": "Geometric simulation of locally optimal tool paths in three-axis milling", "comments": "Submitted to Journal for Geometry and Graphics (2012)", "journal-ref": "Journal for Geometry and Graphics Vol. 17.(2) pp. 189-201. (2013)", "doi": null, "report-no": null, "categories": "cs.CG cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most important aim in tool path generation methods is to increase the\nmachining efficiency by minimizing the total length of tool paths while the\nerror is kept under a prescribed tolerance. This can be achieved by determining\nthe moving direction of the cutting tool such that the machined stripe is the\nwidest. From a technical point of view it is recommended that the angle between\nthe tool axis and the surface normal does not change too much along the tool\npath in order to ensure even abrasion of the tool. In this paper a mathematical\nmethod for tool path generation in 3-axis milling is presented, which considers\nthese requirements by combining the features of isophotic curves and principal\ncurvatures. It calculates the proposed moving direction of the tool at each\npoint of the surface. The proposed direction depends on the measurement of the\ntool and on the curvature values of the surface. For triangulated surfaces a\nnew local offset computation method is presented, which is suitable also for\ndetecting tool collision with the target surface and self intersection in the\noffset mesh.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2012 13:07:48 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2013 12:54:47 GMT"}], "update_date": "2014-02-06", "authors_parsed": [["Szilv\u00e1si-Nagy", "M\u00e1rta", ""], ["M\u00e1ty\u00e1si", "Gyula", ""], ["B\u00e9la", "Szilvia", ""]]}, {"id": "1209.4992", "submitter": "Praveen Chandrashekar", "authors": "Praveen Chandrashekar", "title": "Discontinuous Galerkin method for Navier-Stokes equations using kinetic\n  flux vector splitting", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2012.09.017", "report-no": null, "categories": "cs.NA cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kinetic schemes for compressible flow of gases are constructed by exploiting\nthe connection between Boltzmann equation and the Navier-Stokes equations. This\nconnection allows us to construct a flux splitting for the Navier-Stokes\nequations based on the direction of molecular motion from which a numerical\nflux can be obtained. The naive use of such a numerical flux function in a\ndiscontinuous Galerkin (DG) discretization leads to an unstable scheme in the\nviscous dominated case. Stable schemes are constructed by adding additional\nterms either in a symmetric or non-symmetric manner which are motivated by the\nDG schemes for elliptic equations. The novelty of the present scheme is the use\nof kinetic fluxes to construct the stabilization terms. In the symmetric case,\ninterior penalty terms have to be added for stability and the resulting schemes\ngive optimal convergence rates in numerical experiments. The non-symmetric\nschemes lead to a cell energy/entropy inequality but exhibit sub-optimal\nconvergence rates. These properties are studied by applying the schemes to a\nscalar convection-diffusion equation and the 1-D compressible Navier-Stokes\nequations. In the case of Navier-Stokes equations, entropy variables are used\nto construct stable schemes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 12:55:49 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Chandrashekar", "Praveen", ""]]}, {"id": "1209.4994", "submitter": "Praveen Chandrashekar", "authors": "Praveen Chandrashekar", "title": "Kinetic energy preserving and entropy stable finite volume schemes for\n  compressible Euler and Navier-Stokes equations", "comments": null, "journal-ref": null, "doi": "10.4208/cicp.170712.010313a", "report-no": null, "categories": "cs.NA cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Centered numerical fluxes can be constructed for compressible Euler equations\nwhich preserve kinetic energy in the semi-discrete finite volume scheme. The\nessential feature is that the momentum flux should be of the form $f^m_\\jph =\n\\tp_\\jph + \\avg{u}_\\jph f^\\rho_\\jph$ where $\\avg{u}_\\jph = (u_j + u_{j+1})/2$\nand $\\tp_\\jph, f^\\rho_\\jph$ are {\\em any} consistent approximations to the\npressure and the mass flux. This scheme thus leaves most terms in the numerical\nflux unspecified and various authors have used simple averaging. Here we\nenforce approximate or exact entropy consistency which leads to a unique choice\nof all the terms in the numerical fluxes. As a consequence novel entropy\nconservative flux that also preserves kinetic energy for the semi-discrete\nfinite volume scheme has been proposed. These fluxes are centered and some\ndissipation has to be added if shocks are present or if the mesh is coarse. We\nconstruct scalar artificial dissipation terms which are kinetic energy stable\nand satisfy approximate/exact entropy condition. Secondly, we use entropy-\nvariable based matrix dissipation flux which leads to kinetic energy and\nentropy stable schemes. These schemes are shown to be free of entropy violating\nsolutions unlike the original Roe scheme. For hypersonic flows a blended scheme\nis proposed which gives carbuncle free solutions for blunt body flows.\nNumerical results for Euler and Navier-Stokes equations are presented to\ndemonstrate the performance of the different schemes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2012 13:09:42 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Chandrashekar", "Praveen", ""]]}, {"id": "1209.5145", "submitter": "Viral Shah", "authors": "Jeff Bezanson, Stefan Karpinski, Viral B. Shah, Alan Edelman", "title": "Julia: A Fast Dynamic Language for Technical Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic languages have become popular for scientific computing. They are\ngenerally considered highly productive, but lacking in performance. This paper\npresents Julia, a new dynamic language for technical computing, designed for\nperformance from the beginning by adapting and extending modern programming\nlanguage techniques. A design based on generic functions and a rich type system\nsimultaneously enables an expressive programming model and successful type\ninference, leading to good performance for a wide range of programs. This makes\nit possible for much of the Julia library to be written in Julia itself, while\nalso incorporating best-of-breed C and Fortran libraries.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 03:55:45 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Bezanson", "Jeff", ""], ["Karpinski", "Stefan", ""], ["Shah", "Viral B.", ""], ["Edelman", "Alan", ""]]}, {"id": "1209.5231", "submitter": "Eric Mjolsness", "authors": "Eric Mjolsness", "title": "Time-Ordered Product Expansions for Computational Stochastic Systems\n  Biology", "comments": "Submitted to Q-Bio 2012 conference, Santa Fe, New Mexico", "journal-ref": null, "doi": "10.1088/1478-3975/10/3/035009", "report-no": null, "categories": "q-bio.QM cs.CE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time-ordered product framework of quantum field theory can also be used\nto understand salient phenomena in stochastic biochemical networks. It is used\nhere to derive Gillespie's Stochastic Simulation Algorithm (SSA) for chemical\nreaction networks; consequently, the SSA can be interpreted in terms of Feynman\ndiagrams. It is also used here to derive other, more general simulation and\nparameter-learning algorithms including simulation algorithms for networks of\nstochastic reaction-like processes operating on parameterized objects, and also\nhybrid stochastic reaction/differential equation models in which systems of\nordinary differential equations evolve the parameters of objects that can also\nundergo stochastic reactions. Thus, the time-ordered product expansion (TOPE)\ncan be used systematically to derive simulation and parameter-fitting\nalgorithms for stochastic systems.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2012 11:32:24 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Mjolsness", "Eric", ""]]}, {"id": "1209.5905", "submitter": "Subhankar Roy", "authors": "Subhankar Roy, Sunirmal Khatua, Sudipta Roy and Samir K. Bandyopadhyay", "title": "An Efficient Biological Sequence Compression Technique Using LUT And\n  Repeat In The Sequence", "comments": "9 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data compression plays an important role to deal with high volumes of DNA\nsequences in the field of Bioinformatics. Again data compression techniques\ndirectly affect the alignment of DNA sequences. So the time needed to\ndecompress a compressed sequence has to be given equal priorities as with\ncompression ratio. This article contains first introduction then a brief review\nof different biological sequence compression after that my proposed work then\nour two improved Biological sequence compression algorithms after that result\nfollowed by conclusion and discussion, future scope and finally references.\nThese algorithms gain a very good compression factor with higher saving\npercentage and less time for compression and decompression than the previous\nBiological Sequence compression algorithms. Keywords: Hash map table, Tandem\nrepeats, compression factor, compression time, saving percentage, compression,\ndecompression process.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2012 11:47:33 GMT"}, {"version": "v2", "created": "Sun, 11 Nov 2012 14:40:27 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Roy", "Subhankar", ""], ["Khatua", "Sunirmal", ""], ["Roy", "Sudipta", ""], ["Bandyopadhyay", "Samir K.", ""]]}, {"id": "1209.6129", "submitter": "Deepak Garg Dr", "authors": "Deepak Garg, S C Saxena, L M Bhardwaj", "title": "A New Middle Path Approach For Alignements In Blast", "comments": null, "journal-ref": "Journal of Biological Systems, Vol. 14, No. 4 , pp. 567-581 ISSN\n  0218-3390 2006", "doi": null, "report-no": null, "categories": "cs.DS cs.CE q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper deals with a new middle path approach developed for reducing\nalignment calculations in BLAST algorithm. This is a new step which is\nintroduced in BLAST algorithm in between the ungapped and gapped alignments.\nThis step of middle path approach between the ungapped and gapped alignments\nreduces the number of sequences going for gapped alignment. This results in the\nimprovement in speed for alignment up to 30 percent.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2012 05:47:32 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Garg", "Deepak", ""], ["Saxena", "S C", ""], ["Bhardwaj", "L M", ""]]}, {"id": "1209.6425", "submitter": "Houtao Deng", "authors": "Houtao Deng and George Runger", "title": "Gene selection with guided regularized random forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regularized random forest (RRF) was recently proposed for feature\nselection by building only one ensemble. In RRF the features are evaluated on a\npart of the training data at each tree node. We derive an upper bound for the\nnumber of distinct Gini information gain values in a node, and show that many\nfeatures can share the same information gain at a node with a small number of\ninstances and a large number of features. Therefore, in a node with a small\nnumber of instances, RRF is likely to select a feature not strongly relevant.\nHere an enhanced RRF, referred to as the guided RRF (GRRF), is proposed. In\nGRRF, the importance scores from an ordinary random forest (RF) are used to\nguide the feature selection process in RRF. Experiments on 10 gene data sets\nshow that the accuracy performance of GRRF is, in general, more robust than RRF\nwhen their parameters change. GRRF is computationally efficient, can select\ncompact feature subsets, and has competitive accuracy performance, compared to\nRRF, varSelRF and LASSO logistic regression (with evaluations from an RF\nclassifier). Also, RF applied to the features selected by RRF with the minimal\nregularization outperforms RF applied to all the features for most of the data\nsets considered here. Therefore, if accuracy is considered more important than\nthe size of the feature subset, RRF with the minimal regularization may be\nconsidered. We use the accuracy performance of RF, a strong classifier, to\nevaluate feature selection methods, and illustrate that weak classifiers are\nless capable of capturing the information contained in a feature subset. Both\nRRF and GRRF were implemented in the \"RRF\" R package available at CRAN, the\nofficial R package archive.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 04:59:33 GMT"}, {"version": "v2", "created": "Sat, 25 May 2013 03:50:59 GMT"}, {"version": "v3", "created": "Thu, 20 Jun 2013 05:41:39 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Deng", "Houtao", ""], ["Runger", "George", ""]]}, {"id": "1209.6489", "submitter": "Deepak Garg Dr", "authors": "Sandeep Kumar and Deepak Garg", "title": "Online Financial Algorithms Competitive Analysis", "comments": null, "journal-ref": "International Journal of Computer Applications 40(7):8-14, 2012", "doi": "10.5120/4974-7228", "report-no": null, "categories": "cs.CE cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of algorithms with complete knowledge of its inputs is sometimes not\nup to our expectations. Many times we are surrounded with such scenarios where\ninputs are generated without any prior knowledge. Online Algorithms have found\ntheir applicability in broad areas of computer engineering. Among these, an\nonline financial algorithm is one of the most important areas where lots of\nefforts have been used to produce an efficient algorithm. In this paper various\nOnline Algorithms have been reviewed for their efficiency and various\nalternative measures have been explored for analysis purposes.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 11:46:28 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Kumar", "Sandeep", ""], ["Garg", "Deepak", ""]]}, {"id": "1209.6630", "submitter": "Anthony Scemama", "authors": "Anthony Scemama (LCPQ), Michel Caffarel (LCPQ), Emmanuel Oseret (ECR),\n  William Jalby (ECR, PRISM)", "title": "Quantum Monte Carlo for large chemical systems: Implementing efficient\n  strategies for petascale platforms and beyond", "comments": null, "journal-ref": null, "doi": "10.1002/jcc.23216", "report-no": null, "categories": "cs.PF cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various strategies to implement efficiently QMC simulations for large\nchemical systems are presented. These include: i.) the introduction of an\nefficient algorithm to calculate the computationally expensive Slater matrices.\nThis novel scheme is based on the use of the highly localized character of\natomic Gaussian basis functions (not the molecular orbitals as usually done),\nii.) the possibility of keeping the memory footprint minimal, iii.) the\nimportant enhancement of single-core performance when efficient optimization\ntools are employed, and iv.) the definition of a universal, dynamic,\nfault-tolerant, and load-balanced computational framework adapted to all kinds\nof computational platforms (massively parallel machines, clusters, or\ndistributed grids). These strategies have been implemented in the QMC=Chem code\ndeveloped at Toulouse and illustrated with numerical applications on small\npeptides of increasing sizes (158, 434, 1056 and 1731 electrons). Using 10k-80k\ncomputing cores of the Curie machine (GENCI-TGCC-CEA, France) QMC=Chem has been\nshown to be capable of running at the petascale level, thus demonstrating that\nfor this machine a large part of the peak performance can be achieved.\nImplementation of large-scale QMC simulations for future exascale platforms\nwith a comparable level of efficiency is expected to be feasible.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 19:58:58 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 12:36:47 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Scemama", "Anthony", "", "LCPQ"], ["Caffarel", "Michel", "", "LCPQ"], ["Oseret", "Emmanuel", "", "ECR"], ["Jalby", "William", "", "ECR, PRISM"]]}]