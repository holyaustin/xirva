[{"id": "1910.01850", "submitter": "Shunchuan Yang", "authors": "Yangfan Zhang, Pengfei Wang, Wenping Li, Shunchuan Yang", "title": "An Accurate Edge-based FEM for Electromagnetic Analysis with Its\n  Applications to Multiscale Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an accurate edge-based smoothed finite element method\n(ES-FEM) for electromagnetic analysis for both two dimensional cylindrical and\nthree dimensional cartesian systems, which shows much better performance in\nterms of accuracy and numerical stability for mesh distortion compared with the\ntraditional FEM. Unlike the traditional FEM, the computational domain in ES-FEM\nis divided into nonoverlapping smoothing domains associated with each edge of\nelements, triangles in two dimensional domain and tetrahedrons in three\ndimensional domain. Then, the gradient smoothing technique (GST) is used to\nsmooth the gradient components in the stiff matrix of the FEM. Several\nnumerical experiments are carried out to validate its accuracy and numerical\nstability. Numerical results show that the ES-FEM can obtain much more accurate\nresults and is almost independent of mesh distortion.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2019 09:44:13 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 14:51:00 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Zhang", "Yangfan", ""], ["Wang", "Pengfei", ""], ["Li", "Wenping", ""], ["Yang", "Shunchuan", ""]]}, {"id": "1910.02858", "submitter": "Nico Krais", "authors": "Nico Krais, Andrea Beck, Thomas Bolemann, Hannes Frank, David Flad,\n  Gregor Gassner, Florian Hindenlang, Malte Hoffmann, Thomas Kuhn, Matthias\n  Sonntag, Claus-Dieter Munz", "title": "FLEXI: A high order discontinuous Galerkin framework for\n  hyperbolic-parabolic conservation laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High order (HO) schemes are attractive candidates for the numerical solution\nof multiscale problems occurring in fluid dynamics and related disciplines.\nAmong the HO discretization variants, discontinuous Galerkin schemes offer a\ncollection of advantageous features which have lead to a strong increase in\ninterest in them and related formulations in the last decade. The methods have\nmatured sufficiently to be of practical use for a range of problems, for\nexample in direct numerical and large eddy simulation of turbulence. However,\nin order to take full advantage of the potential benefits of these methods, all\nsteps in the simulation chain must be designed and executed with HO in mind.\nEspecially in this area, many commercially available closed-source solutions\nfall short. In this work, we therefor present the FLEXI framework, a HO\nconsistent, open-source simulation tool chain for solving the compressible\nNavier-Stokes equations in a high performance computing setting. We describe\nthe numerical algorithms and implementation details and give an overview of the\nfeatures and capabilities of all parts of the framework. Beyond these technical\ndetails, we also discuss the important, but often overlooked issues of code\nstability, reproducibility and user-friendliness. The benefits gained by\ndeveloping an open-source framework are discussed, with a particular focus on\nusability for the open-source community. We close with sample applications that\ndemonstrate the wide range of use cases and the expandability of FLEXI and an\noverview of current and future developments.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 15:37:39 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Krais", "Nico", ""], ["Beck", "Andrea", ""], ["Bolemann", "Thomas", ""], ["Frank", "Hannes", ""], ["Flad", "David", ""], ["Gassner", "Gregor", ""], ["Hindenlang", "Florian", ""], ["Hoffmann", "Malte", ""], ["Kuhn", "Thomas", ""], ["Sonntag", "Matthias", ""], ["Munz", "Claus-Dieter", ""]]}, {"id": "1910.03936", "submitter": "Mohammed Alser", "authors": "Mohammed H K Alser", "title": "Accelerating the Understanding of Life's Code Through Better Algorithms\n  and Hardware Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculating the similarities between a pair of genomic sequences is one of\nthe most fundamental computational steps in genomic analysis. This step --\ncalled sequence alignment -- is the computational bottleneck because: (1) it is\nimplemented using quadratic-time dynamic programming algorithms and (2) the\nmajority of candidate locations in the reference genome do not align with a\ngiven read due to high dissimilarity. Calculating the alignment of such\nincorrect candidate locations consumes an overwhelming majority of a modern\nread mapper's execution time.\n  In this thesis, we introduce four new algorithms (GateKeeper, Shouji, MAGNET,\nand SneakySnake) that function as a pre-alignment step and aim to filter out\nmost incorrect candidate locations. The first key idea of our pre-alignment\nfilters is to provide high filtering accuracy by correctly detecting all\nsimilar segments shared between two sequences. The second key idea is to\nexploit the massively parallel architecture of modern FPGAs for accelerating\nour filtering algorithms. We also develop an efficient CPU implementation of\nthe SneakySnake algorithm for commodity desktops and servers. We evaluate the\nbenefits and downsides of our pre-alignment filtering approach in detail using\n12 real datasets. In our evaluation, we demonstrate that our hardware\npre-alignment filters show two to three orders of magnitude speedup over their\nequivalent CPU implementations. We also demonstrate that integrating our\nhardware pre-alignment filters with the state-of-the-art read aligners reduces\nthe aligner's execution time by up to 21.5x. Finally, we show that efficient\nCPU implementation of pre-alignment filtering still provides significant\nbenefits. We show that SneakySnake on average reduces the execution time of the\nbest performing CPU-based read aligners Edlib and Parasail, by up to 43x and\n57.9x, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 8 Oct 2019 15:52:53 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Alser", "Mohammed H K", ""]]}, {"id": "1910.04561", "submitter": "Marco Paggi", "authors": "Marco Paggi and Andrea Amicarelli and Pietro Lenarda", "title": "SPH modelling of hydrodynamic lubrication: laminar fluid flow-structure\n  interaction with no-slip conditions for slider bearings", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FOSS CFD-SPH code SPHERA v.9.0.0 (RSE SpA) is empowered to deal with\nfluid-solid body interactions under no-slip conditions and laminar regimes for\nthe simulation of hydrodynamic lubrication. The code is herein validated in\nrelation to a uniform slider bearing (i.e., for a constant lubricant film\ndepth) and a linear slider bearing (i.e., for a film depth with a linear\nprofile variation along the main flow direction). Validations refer to\ncomparisons with analytical solutions, herein generalized to consider any\nDirichlet boundary condition. Further, this study allows a first code\nvalidation of the fluid-fixed frontier interactions under no-slip conditions.\nWith respect to the most state-of-the-art models (2D codes based on Reynolds'\nequation for fluid films), the following distinctive features are highlighted:\n(i) 3D formulation on all the terms of the Navier-Stokes equations for\nincompressible fluids with uniform viscosity; (ii) validations on both local\nand global quantities (pressure and velocity profiles; load-bearing capacity);\n(iii) possibility to simulate any 3D topology. This study also shows the\nadvantages of using a CFD-SPH code in simulating the inertia and 3D effects\nclose to the slider edges, and it opens new research directions overcoming the\nlimitations of the codes for hydrodynamic lubrication based on the Reynolds'\nequation for fluid films. This study finally allows SPHERA to deal with\nhydrodynamic lubrication and empowers the code for other relevant application\nfields involving fluid-structure interactions (e.g., transport of solid bodies\nby floods and earth landslides; rock landslides). SPHERA is developed and\ndistributed on a GitHub public repository.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2019 07:46:00 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Paggi", "Marco", ""], ["Amicarelli", "Andrea", ""], ["Lenarda", "Pietro", ""]]}, {"id": "1910.04759", "submitter": "Mohammadreza Mashayekhi", "authors": "Homayoon E. Estekanchi, Mohammadreza Mashayekhi, Hassan Vafai, Goodarz\n  Ahmadi, S. Ali Mirfarhadi, Mojtaba Harati", "title": "A state-of-knowledge review on the Endurance Time Method", "comments": "It has been published in the journal of Structures", "journal-ref": "Structures (2020)", "doi": "10.1016/j.istruc.2020.07.062", "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Endurance time method is a time history dynamic analysis in which structures\nare subjected to predesigned intensifying excitations. This method provides a\ntool for response prediction that correlates structural responses to the\nintensity of earthquakes with a considerably less computational demand as\ncompared to conventional time history analysis. The endurance time method is\nbeing used in different areas of earthquake engineering such as\nperformance-based assessment and design, life-cycle cost-based design,\nvalue-based design, seismic safety, seismic assessment, and multicomponent\nseismic analysis. Successful implementation of the endurance time method relies\nheavily on the quality of endurance time excitations. In this paper, a review\nof the endurance time method from conceptual development to its practical\napplications is provided. Different types of endurance time excitations are\ndescribed. Features related to the existing endurance time excitations are also\npresented. Particular attention is given to different applications of the\nendurance time method in the field of earthquake engineering.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2019 21:01:13 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 17:09:27 GMT"}, {"version": "v3", "created": "Sun, 23 Aug 2020 06:29:53 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Estekanchi", "Homayoon E.", ""], ["Mashayekhi", "Mohammadreza", ""], ["Vafai", "Hassan", ""], ["Ahmadi", "Goodarz", ""], ["Mirfarhadi", "S. Ali", ""], ["Harati", "Mojtaba", ""]]}, {"id": "1910.05078", "submitter": "Deli Chen", "authors": "Deli Chen, Yanyan Zou, Keiko Harimoto, Ruihan Bao, Xuancheng Ren, Xu\n  Sun", "title": "Incorporating Fine-grained Events in Stock Movement Prediction", "comments": "Accepted by 2th ECONLP workshop in EMNLP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering event structure information has proven helpful in text-based\nstock movement prediction. However, existing works mainly adopt the\ncoarse-grained events, which loses the specific semantic information of diverse\nevent types. In this work, we propose to incorporate the fine-grained events in\nstock movement prediction. Firstly, we propose a professional finance event\ndictionary built by domain experts and use it to extract fine-grained events\nautomatically from finance news. Then we design a neural model to combine\nfinance news with fine-grained event structure and stock trade data to predict\nthe stock movement. Besides, in order to improve the generalizability of the\nproposed method, we design an advanced model that uses the extracted\nfine-grained events as the distant supervised label to train a multi-task\nframework of event extraction and stock prediction. The experimental results\nshow that our method outperforms all the baselines and has good\ngeneralizability.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:02:45 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Chen", "Deli", ""], ["Zou", "Yanyan", ""], ["Harimoto", "Keiko", ""], ["Bao", "Ruihan", ""], ["Ren", "Xuancheng", ""], ["Sun", "Xu", ""]]}, {"id": "1910.05091", "submitter": "Mirko Trisolini", "authors": "Mirko Trisolini, Hugh G. Lewis, Camilla Colombo", "title": "Spacecraft design optimisation for demise and survivability", "comments": "Paper accepted for publication in Aerospace Science and Technology", "journal-ref": null, "doi": "10.1016/j.ast.2018.04.006", "report-no": null, "categories": "cs.NE astro-ph.EP cs.CE math.OC physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the mitigation measures introduced to cope with the space debris issue\nthere is the de-orbiting of decommissioned satellites. Guidelines for\nre-entering objects call for a ground casualty risk no higher than 0.0001. To\ncomply with this requirement, satellites can be designed through a\ndesign-for-demise philosophy. Still, a spacecraft designed to demise has to\nsurvive the debris-populated space environment for many years. The demisability\nand the survivability of a satellite can both be influenced by a set of common\ndesign choices such as the material selection, the geometry definition, and the\nposition of the components. Within this context, two models have been developed\nto analyse the demise and the survivability of satellites. Given the competing\nnature of the demisability and the survivability, a multi-objective\noptimisation framework was developed, with the aim to identify trade-off\nsolutions for the preliminary design of satellites. As the problem is nonlinear\nand involves the combination of continuous and discrete variables, classical\nderivative based approaches are unsuited and a genetic algorithm was selected\ninstead. The genetic algorithm uses the developed demisability and\nsurvivability criteria as the fitness functions of the multi-objective\nalgorithm. The paper presents a test case, which considers the preliminary\noptimisation of tanks in terms of material, geometry, location, and number of\ntanks for a representative Earth observation mission. The configuration of the\nexternal structure of the spacecraft is fixed. Tanks were selected because they\nare sensitive to both design requirements: they represent critical components\nin the demise process and impact damage can cause the loss of the mission\nbecause of leaking and ruptures. The results present the possible trade off\nsolutions, constituting the Pareto front obtained from the multi-objective\noptimisation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 11:39:22 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Trisolini", "Mirko", ""], ["Lewis", "Hugh G.", ""], ["Colombo", "Camilla", ""]]}, {"id": "1910.05117", "submitter": "Steven Atkinson", "authors": "Steven Atkinson and Waad Subber and Liping Wang and Genghis Khan and\n  Philippe Hawi and Roger Ghanem", "title": "Data-driven discovery of free-form governing differential equations", "comments": "Approved for public release; distribution is unlimited", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of discovering governing differential equations from data\nwithout the need to specify a priori the terms to appear in the equation. The\ninput to our method is a dataset (or ensemble of datasets) corresponding to a\nparticular solution (or ensemble of particular solutions) of a differential\nequation. The output is a human-readable differential equation with parameters\ncalibrated to the individual particular solutions provided. The key to our\nmethod is to learn differentiable models of the data that subsequently serve as\ninputs to a genetic programming algorithm in which graphs specify computation\nover arbitrary compositions of functions, parameters, and (potentially\ndifferential) operators on functions. Differential operators are composed and\nevaluated using recursive application of automatic differentiation, allowing\nour algorithm to explore arbitrary compositions of operators without the need\nfor human intervention. We also demonstrate an active learning process to\nidentify and remedy deficiencies in the proposed governing equations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 02:12:19 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 17:17:48 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Atkinson", "Steven", ""], ["Subber", "Waad", ""], ["Wang", "Liping", ""], ["Khan", "Genghis", ""], ["Hawi", "Philippe", ""], ["Ghanem", "Roger", ""]]}, {"id": "1910.05372", "submitter": "Fenghua Guo", "authors": "Fenghua Guo, Alexander Leemans, Max A. Viergever, Flavio Dell'Acqua,\n  Alberto De Luca", "title": "Generalized Richardson-Lucy (GRL) for analyzing multi-shell diffusion\n  MRI data", "comments": null, "journal-ref": "Neuroimage Volume 218, September 2020, 116948", "doi": "10.1016/j.neuroimage.2020.116948", "report-no": null, "categories": "physics.med-ph cs.CE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spherical deconvolution is a widely used approach to quantify fiber\norientation distribution from diffusion MRI data. The damped Richardson-Lucy\n(dRL) is developed to perform robust spherical deconvolution on single shell\ndiffusion MRI data. While the dRL algorithm could in theory be directly applied\nto multi-shell data, it is not optimised to model the signal from multiple\ntissue types. In this work, we introduce a new framework based on dRL - dubbed\nGeneralized Richardson Lucy (GRL) - that uses multi-shell data in combination\nwith user-chosen tissue models to disentangle partial volume effects and\nincrease the accuracy in FOD estimation. The optimal weighting of multi-shell\ndata in the fit and the robustness to noise and partial volume effects of GRL\nwas studied with synthetic data. Subsequently, we investigated the performances\nof GRL in comparison to dRL on a high-resolution diffusion MRI dataset from the\nHuman Connectome Project and on an MRI dataset acquired at 3T on a clinical\nscanner. The feasibility of including intra-voxel incoherent motion (IVIM)\neffects in the modelling was studied on a third dataset. Results of simulations\nshow that GRL can robustly disentangle different tissue types at SNR above 20\nand improves the angular accuracy of the FOD estimation. On real data, GRL\nprovides signal fraction maps that are physiologically plausible and consistent\nbetween datasets. When considering IVIM effects, high blood pseudo-diffusion\nfraction is observed in the medial temporal lobe and in the sagittal sinus. In\ncomparison to dRL, GRL provides sharper FODs and less spurious peaks in\npresence of partial volume effects and results in a better tract termination at\nthe grey/white matter interface or at the outer cortical surface. In\nconclusion, GRL offers a new modular and flexible framework to perform\nspherical deconvolution of multi-shell data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Oct 2019 18:49:31 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 09:16:36 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 16:12:14 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Guo", "Fenghua", ""], ["Leemans", "Alexander", ""], ["Viergever", "Max A.", ""], ["Dell'Acqua", "Flavio", ""], ["De Luca", "Alberto", ""]]}, {"id": "1910.05568", "submitter": "Qiao-Le He", "authors": "Qiao-Le He, Eric von Lieres, Zhaoxi Sun, Liming Zhao", "title": "Model-based process design of a ternary protein separation using\n  multi-step gradient ion-exchange SMB chromatography", "comments": "26 pages, 14 figures, 7 tables", "journal-ref": null, "doi": "10.1016/j.compchemeng.2020.106581", "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Model-based process design of ion-exchange simulated moving bed (IEX-SMB)\nchromatography for center-cut separation of proteins is studied. Use of\nnonlinear binding models that describe more accurate adsorption behaviours of\nmacro-molecules could make it impossible to utilize triangle theory to obtain\noperating parameters. Moreover, triangle theory provides no rules to design\nsalt profiles in IEX-SMB. In the modelling study here, proteins (i.e.,\nribonuclease, cytochrome and lysozyme) on the chromatographic columns packed\nwith strong cation-exchanger SP Sepharose FF is used as an example system. The\ngeneral rate model with steric mass-action kinetics was used; two closed-loop\nIEX-SMB network schemes were investigated (i.e., cascade and eight-zone\nschemes). Performance of the IEX-SMB schemes was examined with respect to\nmulti-objective indicators (i.e., purity and yield) and productivity, and\ncompared to a single column batch system with the same amount of resin\nutilized. A multi-objective sampling algorithm, Markov Chain Monte Carlo\n(MCMC), was used to generate samples for constructing the Pareto optimal\nfronts. MCMC serves on the sampling purpose, which is interested in sampling\nthe Pareto optimal points as well as those near Pareto optimal. Pareto fronts\nof the three schemes provide the full information of trade-off between the\nconflicting indicators of purity and yield. The results indicate the cascade\nIEX-SMB scheme and the integrated eight-zone IEX-SMB scheme have the similar\nperformance that both outperforms the single column batch system.\n", "versions": [{"version": "v1", "created": "Sat, 12 Oct 2019 14:33:25 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 07:35:57 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["He", "Qiao-Le", ""], ["von Lieres", "Eric", ""], ["Sun", "Zhaoxi", ""], ["Zhao", "Liming", ""]]}, {"id": "1910.05709", "submitter": "Lei Zhang", "authors": "Qiang Du, Ruotai Li, Lei Zhang", "title": "Variational Phase Field Formulations of Polarization and Phase\n  Transition in Ferroelectric Thin Films", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electric field plays an important role in ferroelectric phase transition.\nThere have been numerous phase field formulations attempting to account for\nelectrostatic interactions subject to different boundary conditions. In this\npaper, we develop new variational forms of the phase field electrostatic energy\nand the relaxation dynamics of the polarization vector that involves a hybrid\nrepresentation in both real and Fourier variables. The new formulations avoid\nambiguities appeared in earlier studies and lead to much more effective ways to\nperform variational studies and numerical simulations. Computations of\npolarization switching in a single domain by applying the new formulations are\nprovided as illustrative examples.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 07:34:57 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Du", "Qiang", ""], ["Li", "Ruotai", ""], ["Zhang", "Lei", ""]]}, {"id": "1910.05977", "submitter": "Liang Chen", "authors": "Liang Chen and Hakan Bagci", "title": "Steady-state Simulation of Semiconductor Devices using Discontinuous\n  Galerkin Methods", "comments": null, "journal-ref": "IEEE Access, vol. 8, pp. 16203-16215 (2020)", "doi": "10.1109/ACCESS.2020.2967125", "report-no": null, "categories": "physics.comp-ph cs.CE cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design of modern nanostructured semiconductor devices often calls for\nsimulation tools capable of modeling arbitrarily-shaped multiscale geometries.\nIn this work, to this end, a discontinuous Galerkin (DG) method-based framework\nis developed to simulate steady-state response of semiconductor devices. The\nproposed framework solves a system of Poisson equation (in electric potential)\nand drift-diffusion equations (in charge densities), which are nonlinearly\ncoupled via the drift current and the charge distribution. This system is\ndecoupled and linearized using the Gummel method and the resulting equations\nare discretized using a local DG scheme. The proposed framework is used to\nsimulate geometrically intricate semiconductor devices with realistic models of\nmobility and recombination rate. Its accuracy is demonstrated by comparing the\nresults to those obtained by the finite volume and finite element methods\nimplemented in a commercial software package.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 08:21:32 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Chen", "Liang", ""], ["Bagci", "Hakan", ""]]}, {"id": "1910.06084", "submitter": "Denys Dutykh", "authors": "Simone Rusconi and Denys Dutykh and Arghir Zarnescu and Dmitri\n  Sokolovski and Elena Akhmatskaya", "title": "An optimal scaling to computationally tractable dimensionless models:\n  Study of latex particles morphology formation", "comments": "29 pages, 7 figures, 5 tables, 34 references, 3 appendices. Other\n  author's papers can be downloaded at http://www.denys-dutykh.com/", "journal-ref": "Computer Physics Communications (2020), Vol. 247, p. 106944", "doi": "10.1016/j.cpc.2019.106944", "report-no": "https://www.sciencedirect.com/science/article/pii/S0010465519302954", "categories": "cs.CE physics.app-ph physics.chem-ph physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In modelling of chemical, physical or biological systems it may occur that\nthe coefficients, multiplying various terms in the equation of interest, differ\ngreatly in magnitude, if a particular system of units is used. Such is, for\ninstance, the case of the Population Balance Equations (PBE) proposed to model\nthe Latex Particles Morphology formation. The obvious way out of this\ndifficulty is the use of dimensionless scaled quantities, although often the\nscaling procedure is not unique. In this paper, we introduce a conceptually new\ngeneral approach, called Optimal Scaling (OS). The method is tested on the\nknown examples from classical and quantum mechanics, and applied to the Latex\nParticles Morphology model, where it allows us to reduce the variation of the\nrelevant coefficients from 49 to just 4 orders of magnitudes. The PBE are then\nsolved by a novel Generalised Method Of Characteristics, and the OS is shown to\nhelp reduce numerical error, and avoid unphysical behaviour of the solution.\nAlthough inspired by a particular application, the proposed scaling algorithm\nis expected find application in a wide range of chemical, physical and\nbiological problems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 19:45:00 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 15:37:13 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Rusconi", "Simone", ""], ["Dutykh", "Denys", ""], ["Zarnescu", "Arghir", ""], ["Sokolovski", "Dmitri", ""], ["Akhmatskaya", "Elena", ""]]}, {"id": "1910.06468", "submitter": "Vasil Kolev", "authors": "Vasil Kolev, Todor Cooklev, Fritz Keinert", "title": "Matrix Spectral Factorization for SA4 Multiwavelet", "comments": "This is a preprint of a paper whose final and definite form is\n  published in https://link.springer.com/article/10.1007/s11045-017-0520-x", "journal-ref": "Multidimensional Systems and Signal Processing,vol. 29, Issue 4,\n  pp. 1613 - 1641, 2018", "doi": "10.1007/s11045-017-0520-x", "report-no": null, "categories": "math.NA cs.CE cs.NA eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we investigate Bauer's method for the matrix spectral\nfactorization of an r-channel matrix product filter which is a halfband\nautocorrelation matrix. We regularize the resulting matrix spectral factors by\nan averaging approach and by multiplication by a unitary matrix. This leads to\nthe approximate and exact orthogonal SA4 multiscaling functions. We also find\nthe corresponding orthogonal multiwavelet functions, based on the QR\ndecomposition.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 00:42:35 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kolev", "Vasil", ""], ["Cooklev", "Todor", ""], ["Keinert", "Fritz", ""]]}, {"id": "1910.06691", "submitter": "Lisa Hug", "authors": "Lisa Hug, Stefan Kollmannsberger, Zohar Yosibash, Ernst Rank", "title": "A 3D benchmark problem for crack propagation in brittle fracture", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.112905", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a full 3D benchmark problem for brittle fracture based on\nexperiments as well as a validation in the context of phase-field models. The\nexample consists of a series of four-point bending tests on graphite specimens\nwith sharp V-notches at different inclination angles. This simple setup leads\nto a mixed mode (I + II + III) loading which results in complex yet stably\nreproducible crack surfaces. The proposed problem is well suited for\nbenchmarking numerical methods for brittle fracture and allows for a\nquantitative comparison of failure loads and propagation paths as well as\ninitiation angles and the fracture surface. For evaluation of the crack\nsurfaces image-based 3D models of the fractured specimen are provided along\nwith experimental and numerical results. In addition, measured failure loads\nand computed load-displacement curves are given. To demonstrate the\napplicability of the benchmark problem, we show that for a phase-field model\nbased on the Finite Cell Method and multi-level hp-refinement the complex crack\nsurface as well as the failure loads can be well reproduced.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 12:54:12 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Hug", "Lisa", ""], ["Kollmannsberger", "Stefan", ""], ["Yosibash", "Zohar", ""], ["Rank", "Ernst", ""]]}, {"id": "1910.07059", "submitter": "Varun Shankar", "authors": "Varun Shankar, Grady B. Wright, and Akil Narayan", "title": "A Robust Hyperviscosity Formulation for Stable RBF-FD Discretizations of\n  Advection-Diffusion-Reaction Equations on Manifolds", "comments": "26 pages, 11 figures, accepted to SIAM Journal on Scientific\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new hyperviscosity formulation for stabilizing radial basis\nfunction-finite difference (RBF-FD) discretizations of\nadvection-diffusion-reaction equations on manifolds $\\mathbb{M} \\subset\n\\mathbb{R}^3$ of co-dimension one. Our technique involves automatic addition of\nartificial hyperviscosity to damp out spurious modes in the differentiation\nmatrices corresponding to surface gradients, in the process overcoming a\ntechnical limitation of a recently-developed Euclidean formulation. Like the\nEuclidean formulation, the manifold formulation relies on von Neumann stability\nanalysis performed on auxiliary differential operators that mimic the spurious\nsolution growth induced by RBF-FD differentiation matrices. We demonstrate\nhigh-order convergence rates on problems involving surface advection and\nsurface advection-diffusion. Finally, we demonstrate the applicability of our\nformulation to advection-diffusion-reaction equations on manifolds described\npurely as point clouds. Our surface discretizations use the recently-developed\nRBF-LOI method, and with the addition of hyperviscosity, are now empirically\nhigh-order accurate, stable, and free of stagnation errors.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2019 21:25:20 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 17:42:06 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Shankar", "Varun", ""], ["Wright", "Grady B.", ""], ["Narayan", "Akil", ""]]}, {"id": "1910.07133", "submitter": "Vasil Kolev", "authors": "Vasil Kolev, Todor Cooklev, Fritz Keinert", "title": "Design of a Simple Orthogonal Multiwavelet Filter by Matrix Spectral\n  Factorization", "comments": "This is a preprint of a paper whose final and definite form is\n  published in Circuits, Systems, and Signal Processing,, Springer,\n  https://link.springer.com/article/10.1007/s00034-019-01240-9, ISSN 0278-081X\n  (print), ISSN 1531-5878 (Online)", "journal-ref": null, "doi": "10.1007/s00034-019-01240-9", "report-no": null, "categories": "cs.CV cs.CE cs.NA cs.SY eess.SY math.NA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider the design of an orthogonal symmetric/antisymmetric multiwavelet\nfrom its matrix product filter by matrix spectral factorization (MSF). As a\ntest problem, we construct a simple matrix product filter with desirable\nproperties, and factor it using Bauer's method, which in this case can be done\nin closed form. The corresponding orthogonal multiwavelet function is derived\nusing algebraic techniques which allow symmetry to be considered. This leads to\nthe known orthogonal multiwavelet SA1, which can also be derived directly. We\nalso give a lifting scheme for SA1, investigate the influence of the number of\nsignificant digits in the calculations, and show some numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 02:21:52 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kolev", "Vasil", ""], ["Cooklev", "Todor", ""], ["Keinert", "Fritz", ""]]}, {"id": "1910.07241", "submitter": "Francesco Statti", "authors": "Damir Filipovi\\'c, Kathrin Glau, Yuji Nakatsukasa, Francesco Statti", "title": "Weighted Monte Carlo with least squares and randomized extended Kaczmarz\n  for option pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology for computing single and multi-asset European option\nprices, and more generally expectations of scalar functions of (multivariate)\nrandom variables. This new approach combines the ability of Monte Carlo\nsimulation to handle high-dimensional problems with the efficiency of function\napproximation. Specifically, we first generalize the recently developed method\nfor multivariate integration in [arXiv:1806.05492] to integration with respect\nto probability measures. The method is based on the principle \"approximate and\nintegrate\" in three steps i) sample the integrand at points in the integration\ndomain, ii) approximate the integrand by solving a least-squares problem, iii)\nintegrate the approximate function. In high-dimensional applications we face\nmemory limitations due to large storage requirements in step ii). Combining\nweighted sampling and the randomized extended Kaczmarz algorithm we obtain a\nnew efficient approach to solve large-scale least-squares problems. Our\nconvergence and cost analysis along with numerical experiments show the\neffectiveness of the method in both low and high dimensions, and under the\nassumption of a limited number of available simulations.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 09:36:29 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Filipovi\u0107", "Damir", ""], ["Glau", "Kathrin", ""], ["Nakatsukasa", "Yuji", ""], ["Statti", "Francesco", ""]]}, {"id": "1910.07268", "submitter": "Sebastian Schmitt", "authors": "Jakub Kmec and Sebastian Schmitt", "title": "Exploring the fitness landscape of a realistic turbofan rotor blade\n  optimization", "comments": null, "journal-ref": "In: Rodrigues H. et al. (eds) EngOpt 2018 Proceedings of the 6th\n  International Conference on Engineering Optimization. EngOpt 2018", "doi": "10.1007/978-3-319-97773-7_46", "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerodynamic shape optimization has established itself as a valuable tool in\nthe engineering design process to achieve highly efficient results. A central\naspect for such approaches is the mapping from the design parameters which\nencode the geometry of the shape to be improved to the quality criteria which\ndescribe its performance. The choices to be made in the setup of the\noptimization process strongly influence this mapping and thus are expected to\nhave a profound influence on the achievable result. In this work we explore the\ninfluence of such choices on the effects on the shape optimization of a\nturbofan rotor blade as it can be realized within an aircraft engine design\nprocess. The blade quality is assessed by realistic three dimensional\ncomputational fluid dynamics (CFD) simulations.\n  We investigate the outcomes of several optimization runs which differ in\nvarious configuration options, such as optimization algorithm, initialization,\nnumber of degrees of freedom for the parametrization. For all such variations,\nwe generally find that the achievable improvement of the blade quality is\ncomparable for most settings and thus rather insensitive to the details of the\nsetup.\n  On the other hand, even supposedly minor changes in the settings, such as\nusing a different random seed for the initialization of the optimizer\nalgorithm, lead to very different shapes. Optimized shapes which show\ncomparable performance usually differ quite strongly in their geometries over\nthe complete blade.\n  Our analyses indicate that the fitness landscape for such a realistic\nturbofan rotor blade optimization is highly multi-modal with many local optima,\nwhere very different shapes show similar performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 10:38:58 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Kmec", "Jakub", ""], ["Schmitt", "Sebastian", ""]]}, {"id": "1910.07347", "submitter": "Arne Ilseng", "authors": "Arne Ilseng, Victorien Prot, Bj{\\o}rn T. Stokke, Bj{\\o}rn H. Skallerud", "title": "Buckling initiation in layered hydrogels during transient swelling", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmps.2019.04.008", "report-no": null, "categories": "cond-mat.soft cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjected to compressive stresses, soft polymers with stiffness gradients can\ndisplay various buckling patterns. These compressive stresses can have\ndifferent origins, like mechanical forces, temperature changes, or, for\nhydrogel materials, osmotic swelling. Here, we focus on the influence of the\ntransient nature of osmotic swelling on the initiation of buckling in confined\nlayered hydrogel structures. A constitutive model for transient hydrogel\nswelling is outlined and implemented as a user-subroutine for the commercial\nfinite element software Abaqus. The finite element procedure is benchmarked\nagainst linear perturbation analysis results for equilibrium swelling showing\nexcellent correspondence. Based on the finite element results we conclude that\nthe initiation of buckling in a two-layered hydrogel structure is highly\naffected by transient swelling effects, with instability emerging at lower\nswelling ratios and later in time with a lower diffusion coefficient. In\naddition, for hard-on-soft systems the wavelength of the buckling pattern is\nfound to decrease as the diffusivity of the material is reduced for gels with a\nrelatively low stiffness gradient between the substrate and the upper film.\nThis study highlights the difference between equilibrium and transient swelling\nwhen it comes to the onset of instability in hydrogels, which is believed to be\nof importance as a fundamental aspect of swelling as well as providing input to\nguiding principles in the design of specific hydrogel systems.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 13:50:42 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Ilseng", "Arne", ""], ["Prot", "Victorien", ""], ["Stokke", "Bj\u00f8rn T.", ""], ["Skallerud", "Bj\u00f8rn H.", ""]]}, {"id": "1910.07577", "submitter": "Wei Xing", "authors": "Wei Xing, Robert M. Kirby, Shandian Zhe", "title": "Deep Coregionalization for the Emulation of Spatial-Temporal Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven surrogate models are widely used for applications such as design\noptimization and uncertainty quantification, where repeated evaluations of an\nexpensive simulator are required. For most partial differential equation (PDE)\nsimulators, the outputs of interest are often spatial or spatial-temporal\nfields, leading to very high-dimensional outputs. Despite the success of\nexisting data-driven surrogates for high-dimensional outputs, most methods\nrequire a significant number of samples to cover the response surface in order\nto achieve a reasonable degree of accuracy. This demand makes the idea of\nsurrogate models less attractive considering the high computational cost to\ngenerate the data. To address this issue, we exploit the multi-fidelity nature\nof a PDE simulator and introduce deep coregionalization, a Bayesian\nnon-parametric autoregressive framework for efficient emulation of\nspatial-temporal fields. To effectively extract the output correlations in the\ncontext of multi-fidelity data, we develop a novel dimension reduction\ntechnique, residual principal component analysis. Our model can simultaneously\ncapture the rich output correlations and the fidelity correlations and make\nhigh-fidelity predictions with only a few expensive, high-fidelity simulation\nsamples. We show the advantages of our model in three canonical PDE models and\na fluid dynamics problem. The results show that the proposed method cannot only\napproximate a simulator with significantly less cost (at bout 10%-25%) but also\nfurther improve model accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2019 19:10:28 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Xing", "Wei", ""], ["Kirby", "Robert M.", ""], ["Zhe", "Shandian", ""]]}, {"id": "1910.08191", "submitter": "Rebecca E. Morrison", "authors": "Rebecca E Morrison", "title": "Embedded discrepancy operators in reduced models of interacting species", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of interacting systems, we are only interested in the\ndynamic behavior of a subset of all possible active species. For example, this\nis true in combustion models (many transient chemical species are not of\ninterest in a given reaction) and in epidemiological models (only certain\ncritical populations are truly consequential). Thus it is common to use greatly\nreduced models, in which only the interactions among the species of interest\nare retained. However, reduction introduces a model error, or discrepancy,\nwhich typically is not well characterized. In this work, we explore the use of\nan embedded and statistically calibrated discrepancy operator to represent\nmodel error. The operator is embedded within the differential equations of the\nmodel, which allows the action of the operator to be interpretable. Moreover,\nit is constrained by available physical information, and calibrated over many\nscenarios. These qualities of the discrepancy model---interpretability,\nphysical-consistency, and robustness to different scenarios---are intended to\nsupport reliable predictions under extrapolative conditions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2019 22:46:58 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Morrison", "Rebecca E", ""]]}, {"id": "1910.09025", "submitter": "Arzhang Angoshtari", "authors": "Arzhang Angoshtari and Ali Gerami Matin", "title": "A Conformal Three-Field Formulation for Nonlinear Elasticity: From\n  Differential Complexes to Mixed Finite Element Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of mixed finite element methods for 2D and 3D\ncompressible nonlinear elasticity. The independent unknowns of these conformal\nmethods are displacement, displacement gradient, and the first Piola-Kirchhoff\nstress tensor. The so-called edge finite elements of the curl operator is\nemployed to discretize the trial space of displacement gradients. Motivated by\nthe differential complex of nonlinear elasticity, this choice guarantees that\ndiscrete displacement gradients satisfy the Hadamard jump condition for the\nstrain compatibility. We study the stability of the proposed mixed finite\nelement methods by deriving some inf-sup conditions. By considering 32 choices\nof simplicial conformal finite elements of degrees 1 and 2, we show that 10\nchoices are not stable as they do not satisfy the inf-sup conditions. We\nnumerically study the stable choices and conclude that they can achieve optimal\nconvergence rates. By solving several 2D and 3D numerical examples, we show\nthat the proposed methods are capable of providing accurate approximations of\nstrain and stress.\n", "versions": [{"version": "v1", "created": "Sun, 20 Oct 2019 17:12:09 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Angoshtari", "Arzhang", ""], ["Matin", "Ali Gerami", ""]]}, {"id": "1910.09153", "submitter": "Lu Bai", "authors": "Lu Bai, Lixin Cui, Lixiang Xu, Yue Wang, Zhihong Zhang, Edwin R.\n  Hancock", "title": "Entropic Dynamic Time Warping Kernels for Co-evolving Financial Time\n  Series Analysis", "comments": "Previously, the original version of this manuscript appeared as\n  arXiv:1902.09947v2, that was submitted as a replacement by a mistake. Now,\n  that article has been replaced to correct the error, and this manuscript is\n  distinct from that article", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems, 2020", "doi": "10.1109/TNNLS.2020.3006738", "report-no": null, "categories": "q-fin.ST cs.AI cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we develop a novel framework to measure the similarity between\ndynamic financial networks, i.e., time-varying financial networks.\nParticularly, we explore whether the proposed similarity measure can be\nemployed to understand the structural evolution of the financial networks with\ntime. For a set of time-varying financial networks with each vertex\nrepresenting the individual time series of a different stock and each edge\nbetween a pair of time series representing the absolute value of their Pearson\ncorrelation, our start point is to compute the commute time matrix associated\nwith the weighted adjacency matrix of the network structures, where each\nelement of the matrix can be seen as the enhanced correlation value between\npairwise stocks. For each network, we show how the commute time matrix allows\nus to identify a reliable set of dominant correlated time series as well as an\nassociated dominant probability distribution of the stock belonging to this\nset. Furthermore, we represent each original network as a discrete dominant\nShannon entropy time series computed from the dominant probability\ndistribution. With the dominant entropy time series for each pair of financial\nnetworks to hand, we develop a similarity measure based on the classical\ndynamic time warping framework, for analyzing the financial time-varying\nnetworks. We show that the proposed similarity measure is positive definite and\nthus corresponds to a kernel measure on graphs. The proposed kernel bridges the\ngap between graph kernels and the classical dynamic time warping framework for\nmultiple financial time series analysis. Experiments on time-varying networks\nextracted through New York Stock Exchange (NYSE) database demonstrate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 05:08:09 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Bai", "Lu", ""], ["Cui", "Lixin", ""], ["Xu", "Lixiang", ""], ["Wang", "Yue", ""], ["Zhang", "Zhihong", ""], ["Hancock", "Edwin R.", ""]]}, {"id": "1910.09315", "submitter": "Mohammadali Hedayat", "authors": "Mohammadali Hedayat, Iman Borazjani", "title": "A parallel dynamic overset grid framework for immersed boundary methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A parallel dynamic overset framework has been developed for the curvilinear\nimmersed boundary (overset-CURVIB) method to enable tackling a wide range of\nchallenging flow problems. The dynamic overset grids are used to locally\nincrease the grid resolution near complex immersed bodies, which are handled\nusing a sharp interface immersed boundary method, undergoing large movements as\nwell as arbitrary relative motions. The new framework extends the previous\noverset-CURVIB method with fixed overset grids and a sequential grid assembly\nto moving overset grids with an efficient parallel grid assembly. In addition,\na new method for the interpolation of variables at the grid boundaries is\ndeveloped which can drastically decrease the execution time and increase the\nparallel efficiency of our framework compared to the previous strategy. The\nmoving/rotating overset grids are solved in a non-inertial frame of reference\nto avoid recalculating the curvilinear metrics of transformation while the\nbackground/stationary grids are solved in the inertial frame. The new framework\nis verified and validated against experimental data, and analytical/benchmark\nsolutions. In addition, the results of the overset grid are compared with\nresults over a similar single grid. The method is shown to be 2nd order\naccurate, decrease the computational cost relative to a single grid, and good\noverall parallel speedup. The grid assembly takes less than 7% of the total CPU\ntime even at the highest number of CPUs tested in this work. The capabilities\nof our method are demonstrated by simulating the flow past a school of\nself-propelled aquatic swimmers arranged initially in a diamond pattern.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 17:14:53 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Hedayat", "Mohammadali", ""], ["Borazjani", "Iman", ""]]}, {"id": "1910.09365", "submitter": "Yi Wu", "authors": "Y. Wu, Eric Li, Z. C. He, X. Y. Lin, H. X. Jiang", "title": "Robust concurrent topology optimization of structure and its composite\n  material considering uncertainty with imprecise probability", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.112927", "report-no": null, "categories": "cs.CE math.OC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studied a robust concurrent topology optimization (RCTO) approach\nto design the structure and its composite materials simultaneously. For the\nfirst time, the material uncertainty with imprecise probability is integrated\ninto the multi-scale concurrent topology optimization (CTO) framework. To\ndescribe the imprecise probabilistic uncertainty efficiently, the type I hybrid\ninterval random model is adopted. An improved hybrid perturbation analysis\n(IHPA) method is formulated to estimate the expectation and stand variance of\nthe objective function in the worst case. Combined with the bi-directional\nevolutionary structural optimization (BESO) framework, the robust designs of\nthe structure and its composite material are carried out. Several 2D and 3D\nnumerical examples are presented to illustrate the effectiveness of the\nproposed method. The results show that the proposed method has high efficiency\nand low precision loss. In addition, the proposed RCTO approach remains\nefficient in both of linear static and dynamic structures, which shows its\nextensive adaptability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 13:35:43 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wu", "Y.", ""], ["Li", "Eric", ""], ["He", "Z. C.", ""], ["Lin", "X. Y.", ""], ["Jiang", "H. X.", ""]]}, {"id": "1910.10614", "submitter": "El Mostafa Kalmoun", "authors": "Mohamed M S Nasser and El Mostafa Kalmoun", "title": "Application of integral equations to simulate local fields in carbon\n  nanotube reinforced composites", "comments": null, "journal-ref": "In: 2D and Quasi-2D Composite and Nanocomposite Materials\n  Properties and Photonic Applications Micro and Nano Technologies 2020,\n  233-248", "doi": "10.1016/B978-0-12-818819-4.00016-7", "report-no": null, "categories": "cs.CE math.CV physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the steady heat conduction problem within a thermal isotropic and\nhomogeneous square ring composite reinforced by non-overlapping and randomly\ndistributed carbon nanotubes (CNTs). We treat the CNTs as rigid line inclusions\nand assume their temperature distribution to be fixed to an undetermined\nconstant value along each line. We suppose also that the temperature\ndistribution is known on the outer boundary and that there is no heat flux\nthrough the inner square. The equations for the temperature distribution are\ngoverned by the two-dimensional Laplace equation with mixed Dirichlet- Neumann\nboundary conditions. This boundary value problem is solved using a boundary\nintegral equation method. We demonstrate the performance of our approach\nthrough four numerical examples with small and large numbers of CNTs and\ndifferent side length of the inner square.\n", "versions": [{"version": "v1", "created": "Sun, 13 Oct 2019 09:57:37 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 19:42:32 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Nasser", "Mohamed M S", ""], ["Kalmoun", "El Mostafa", ""]]}, {"id": "1910.10623", "submitter": "Fabrice Zaoui", "authors": "Fabrice Zaoui (EDF R&D LNHE), C\\'edric Goeury (EDF R&D LNHE), Yoann\n  Audouin (EDF R&D LNHE)", "title": "A Metamodel of the Telemac Errors", "comments": null, "journal-ref": "XXVIth TELEMAC-MASCARET USER CONFERENCE, Oct 2019, Toulouse,\n  France", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Telemac study is a computationally intensive application for the real cases\nand in the context of quantifying or optimizing uncertainties, the running\ntimes can be too long. This paper is an example of an approximation of the\nTelemac results by a more abstract but significantly faster model. It shows how\na metamodel can be easily built with low computational costs, and how it can\nhelp to understand and improve some global results of Telemac. I. INTRODUCTION\nMany sources of uncertainty lie in the real-world problems. Telemac as any\nmodel (i.e. approximation of reality) is error prone since uncertainties appear\nin the initial or boundary conditions, the system parameters, the modelling\nsimplification or the numerical calculations themselves. Therefore, it is\ndifficult to say with confidence if the design of a Telemac model has met all\nthe requirements to be optimal. Calibration consists of tuning the model\nparameters so that the results are in better agreement with a set of\nobservations. This phase is crucial before any further study can be conducted\nby avoiding a meaningless analysis or prediction based on false or too\ninaccurate results. This paper presents a statistical calibration of a Telemac\n2D model (Gironde Estuary in France) with the learning of Telemac errors by a\nmetamodel (i.e. a model of the simulation errors) to make the best use of\nlimited observations data over a short time period. The metamodel here is a\nsimplified version of Telemac behaving the same for all the locations where\nobservation points are available. If the metamodel is correct, it will be able\nto compute as Telemac would do but with a highly reduced computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2019 15:19:36 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Zaoui", "Fabrice", "", "EDF R&D LNHE"], ["Goeury", "C\u00e9dric", "", "EDF R&D LNHE"], ["Audouin", "Yoann", "", "EDF R&D LNHE"]]}, {"id": "1910.10770", "submitter": "Julian Norato", "authors": "Fabian Wein and Peter Dunning and Juli\\'an A. Norato", "title": "A Review on Feature-Mapping Methods for Structural Optimization", "comments": "41 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this review we identify a new category of structural optimization methods\nthat has emerged over the last 20 years, which we propose to call\nfeature-mapping methods. The two defining aspects of these methods are that the\ndesign is parameterized by a high-level geometric description and that features\nare mapped onto a fixed grid for analysis. The main motivation for using these\nmethods is to gain better control over the geometry to, for example, facilitate\nimposing direct constraints on geometric features, whilst avoiding issues with\nre-meshing. The review starts by providing some key definitions and then\nexamines the ingredients that these methods use to map geometric features onto\na fixed-grid. One of these ingredients corresponds to the mechanism for mapping\nthe geometry of a single feature onto a fixed analysis grid, from which an\nersatz material or an immersed boundary approach is used for the analysis. For\nthe former case, which we refer to as the pseudo-density approach, a test\nproblem is formulated to investigate aspects of the material interpolation,\nboundary smoothing and numerical integration. We also review other ingredients\nof feature-mapping techniques, including approaches for combining features\n(which are required to perform topology optimization) and methods for imposing\na minimum separation distance among features. A literature review of\nfeature-mapping methods is provided for shape optimization, combined\nfeature/free-form optimization, and topology optimization. Finally, we discuss\npotential future research directions for feature-mapping methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 19:09:22 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Wein", "Fabian", ""], ["Dunning", "Peter", ""], ["Norato", "Juli\u00e1n A.", ""]]}, {"id": "1910.13002", "submitter": "Jeroen Groen", "authors": "Jeroen Groen, Florian Stutz, Niels Aage, J. Andreas B{\\ae}rentzen and\n  Ole Sigmund", "title": "De-homogenization of optimal multi-scale 3D topologies", "comments": "Preprint submitted to Computer Methods in Applied Mechanics and\n  Engineering", "journal-ref": null, "doi": "10.1016/j.cma.2020.112979", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a highly efficient method to obtain high-resolution,\nnear-optimal 3D topologies optimized for minimum compliance on a standard PC.\nUsing an implicit geometry description we derive a single-scale interpretation\nof optimal multi-scale designs on a very fine mesh (de-homogenization). By\nperforming homogenization-based topology optimization, optimal multi-scale\ndesigns are obtained on a relatively coarse mesh resulting in a low\ncomputational cost. As microstructure parameterization we use orthogonal rank-3\nmicrostructures, which are known to be optimal for a single loading case.\nFurthermore, a method to get explicit control of the minimum feature size and\ncomplexity of the final shapes will be discussed. Numerical examples show\nexcellent performance of these fine-scale designs resulting in objective values\nsimilar to the homogenization-based designs. Comparisons with well-established\ndensity-based topology optimization methods show a reduction in computational\ncost of 3 orders of magnitude, paving the way for giga-scale designs on a\nstandard PC.\n", "versions": [{"version": "v1", "created": "Mon, 28 Oct 2019 23:00:19 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Groen", "Jeroen", ""], ["Stutz", "Florian", ""], ["Aage", "Niels", ""], ["B\u00e6rentzen", "J. Andreas", ""], ["Sigmund", "Ole", ""]]}, {"id": "1910.13200", "submitter": "Stefano Moriconi", "authors": "Stefano Moriconi, Rafael Rehwald, Maria A. Zuluaga, H. Rolf J\\\"ager,\n  Parashkev Nachev, S\\'ebastien Ourselin, M. Jorge Cardoso", "title": "Towards Quantifying Neurovascular Resilience", "comments": null, "journal-ref": "Machine Learning and Medical Engineering for Cardiovascular Health\n  and Intravascular Imaging and Computer Assisted Stenting. MLMECH 2019,\n  CVII-STENT 2019. Lecture Notes in Computer Science, vol 11794. Springer, Cham", "doi": "10.1007/978-3-030-33327-0_18", "report-no": null, "categories": "q-bio.QM cs.CE physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst grading neurovascular abnormalities is critical for prompt surgical\nrepair, no statistical markers are currently available for predicting the risk\nof adverse events, such as stroke, and the overall resilience of a network to\nvascular complications. The lack of compact, fast, and scalable simulations\nwith network perturbations impedes the analysis of the vascular resilience to\nlife-threatening conditions, surgical interventions and long-term follow-up. We\nintroduce a graph-based approach for efficient simulations, which statistically\nestimates biomarkers from a series of perturbations on the patient-specific\nvascular network. Analog-equivalent circuits are derived from clinical\nangiographies. Vascular graphs embed mechanical attributes modelling the\nimpedance of a tubular structure with stenosis, tortuosity and complete\nocclusions. We evaluate pressure and flow distributions, simulating healthy\ntopologies and abnormal variants with perturbations in key pathological\nscenarios. These describe the intrinsic network resilience to pathology, and\ndelineate the underlying cerebrovascular autoregulation mechanisms. Lastly, a\nputative graph sampling strategy is devised on the same formulation, to support\nthe topological inference of uncertain neurovascular graphs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Oct 2019 11:16:25 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Moriconi", "Stefano", ""], ["Rehwald", "Rafael", ""], ["Zuluaga", "Maria A.", ""], ["J\u00e4ger", "H. Rolf", ""], ["Nachev", "Parashkev", ""], ["Ourselin", "S\u00e9bastien", ""], ["Cardoso", "M. Jorge", ""]]}, {"id": "1910.13246", "submitter": "Bo Zhao", "authors": "Bo Zhao, Luke Bryant, Michael Wilde, Rebecca Cordell, Dahlia Salman,\n  Dorota Ruszkiewicz, Wadah Ibrahim, Amisha Singapuri, Tim Coats, Erol\n  Gaillard, Caroline Beardsmore, Toru Suzuki, Leong Ng, Neil Greening, Paul\n  Thomas, Paul S. Monks, Christopher Brightling, Salman Siddiqui and Robert C.\n  Free", "title": "LabPipe: an extensible informatics platform to streamline management of\n  metabolomics data and metadata", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Summary: Data management in clinical metabolomics studies is often\ninadequate. To improve this situation we created LabPipe to provide a guided,\ncustomisable approach to study-specific sample collection. It is driven through\na local client which manages the process and pushes local data to a remote\nserver through an access controlled web API. The platform is able to support\ndata management for different sampling approaches across multiple sites /\nstudies and is now an essential study management component for supporting\nclinical metabolomics locally at the EPSRC/MRC funded East Midlands Breathomics\nPathology Node. Availability and Implementation: LabPipe is freely available to\ndownload under a non-commercial open-source license (NPOSL 3.0) along with\ndocumentation and installation instructions at http://labpipe.org. Contact:\nrob.free@le.ac.uk\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 14:56:45 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Zhao", "Bo", ""], ["Bryant", "Luke", ""], ["Wilde", "Michael", ""], ["Cordell", "Rebecca", ""], ["Salman", "Dahlia", ""], ["Ruszkiewicz", "Dorota", ""], ["Ibrahim", "Wadah", ""], ["Singapuri", "Amisha", ""], ["Coats", "Tim", ""], ["Gaillard", "Erol", ""], ["Beardsmore", "Caroline", ""], ["Suzuki", "Toru", ""], ["Ng", "Leong", ""], ["Greening", "Neil", ""], ["Thomas", "Paul", ""], ["Monks", "Paul S.", ""], ["Brightling", "Christopher", ""], ["Siddiqui", "Salman", ""], ["Free", "Robert C.", ""]]}, {"id": "1910.13313", "submitter": "Hesaneh Kazemi", "authors": "Hesaneh Kazemi, Ashkan Vaziri and Julian A. Norato", "title": "Multi-material Topology Optimization of Lattice Structures using\n  Geometry Projection", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2020.112895", "report-no": null, "categories": "cs.CE math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work presents a computational method for the design of architected truss\nlattice materials where each strut can be made of one of a set of available\nmaterials. We design the lattices to extremize effective properties. As\ncustomary in topology optimization, we design a periodic unit cell of the\nlattice and obtain the effective properties via numerical homogenization. Each\nbar is represented as a cylindrical offset surface of a medial axis\nparameterized by the positions of the endpoints of the medial axis. These\nparameters are smoothly mapped onto a continuous density field for the primal\nand sensitivity analysis via the geometry projection method. A size variable\nper material is ascribed to each bar and penalized as in density-based topology\noptimization to facilitate the entire removal of bars from the design. During\nthe optimization, we allow bars to be made of a mixture of the available\nmaterials. However, to ensure each bar is either exclusively made of one\nmaterial or removed altogether from the optimal design, we impose optimization\nconstraints that ensure each size variable is 0 or 1, and that at most one\nmaterial size variable is 1. The proposed material interpolation scheme readily\naccommodates any number of materials. To obtain lattices with desired material\nsymmetries, we design only a reference region of the unit cell and reflect its\ngeometry projection with respect to the appropriate planes of symmetry. Also,\nto ensure bars remain whole upon reflection inside the unit cell or with\nrespect to the periodic boundaries, we impose a no-cut constraint on the bars.\nWe demonstrate the efficacy of our method via numerical examples of bulk and\nshear moduli maximization and Poisson's ratio minimization for two- and\nthree-material lattices with cubic symmetry.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2019 20:40:13 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Kazemi", "Hesaneh", ""], ["Vaziri", "Ashkan", ""], ["Norato", "Julian A.", ""]]}, {"id": "1910.13484", "submitter": "Alessandro Pluchino", "authors": "A. Greco, F. Cannizzaro, A. Pluchino", "title": "A Novel Procedure for the Assessment of the Seismic Performance of Frame\n  Structures by Means of Limit Analysis", "comments": "27 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limit analysis is a computationally efficient tool to assess the resistance\nand the failure mode of structures but does not provide any information on the\ndisplacement capacity, which is one of the concepts which most affects the\nseismic safety. Therefore, since many researchers did not consider limit\nanalysis as a possible tool for the seismic assessment of structures, its\nwidespread employment has been prevented. In this paper this common belief is\nquestioned and the authors show that limit analysis can be useful in the\nevaluation of the seismic performance of frame structures. In particular, to\novercome the limitation on the possibility to evaluate the displacements of a\nstructure based on a limit analysis approach, an approximated capacity curve is\nreconstructed. The latter is based on a limit analysis strategy, which takes\ninto account the second order effects, and evaluates the displacement capacity\nconsidering a post-peak softening branch and a threshold on the allowed plastic\nrotations. Then, based on this simplified capacity curve, an equivalent single\ndegree of freedom system is defined in order to assess the seismic performance\nof frame structures. The proposed simplified strategy is implemented in a\ndedicated software and the obtained results are validated with well-established\napproaches based on nonlinear static analyses, showing the reliability and the\ncomputational efficiency of this methodology\n", "versions": [{"version": "v1", "created": "Thu, 24 Oct 2019 07:17:53 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Greco", "A.", ""], ["Cannizzaro", "F.", ""], ["Pluchino", "A.", ""]]}]