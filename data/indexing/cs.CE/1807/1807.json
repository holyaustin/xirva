[{"id": "1807.00092", "submitter": "Ralf-Peter Mundani", "authors": "Ralf-Peter Mundani (1), J\\'er\\^ome Frisch (2), Vasco Varduhn (3), and\n  Ernst Rank (1) ((1) Technische Universit\\\"at M\\\"unchen, Munich, Germany, (2)\n  RWTH Aachen University, Aachen, Germany, (3) University of Minnesota,\n  Minneapolis, MN, USA)", "title": "A sliding window technique for interactive high-performance computing\n  scenarios", "comments": "21 pages, 12 figures", "journal-ref": "Advances in Engineering Software 84 (2015) 21-30", "doi": "10.1016/j.advengsoft.2015.02.003", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive high-performance computing is doubtlessly beneficial for many\ncomputational science and engineering applications whenever simulation results\nshould be visually processed in real time, i.e. during the computation process.\nNevertheless, interactive HPC entails a lot of new challenges that have to be\nsolved - one of them addressing the fast and efficient data transfer between a\nsimulation back end and visualisation front end, as several gigabytes of data\nper second are nothing unusual for a simulation running on some (hundred)\nthousand cores. Here, a new approach based on a sliding window technique is\nintroduced that copes with any bandwidth limitations and allows users to study\nboth large and small scale effects of the simulation results in an interactive\nfashion.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 00:17:52 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Mundani", "Ralf-Peter", ""], ["Frisch", "J\u00e9r\u00f4me", ""], ["Varduhn", "Vasco", ""], ["Rank", "Ernst", ""]]}, {"id": "1807.00123", "submitter": "Marinka Zitnik", "authors": "Marinka Zitnik, Francis Nguyen, Bo Wang, Jure Leskovec, Anna\n  Goldenberg, Michael M. Hoffman", "title": "Machine Learning for Integrating Data in Biology and Medicine:\n  Principles, Practice, and Opportunities", "comments": null, "journal-ref": "Information Fusion 50 (2019) 71-91", "doi": "10.1016/j.inffus.2018.09.012", "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New technologies have enabled the investigation of biology and human health\nat an unprecedented scale and in multiple dimensions. These dimensions include\na myriad of properties describing genome, epigenome, transcriptome, microbiome,\nphenotype, and lifestyle. No single data type, however, can capture the\ncomplexity of all the factors relevant to understanding a phenomenon such as a\ndisease. Integrative methods that combine data from multiple technologies have\nthus emerged as critical statistical and computational approaches. The key\nchallenge in developing such approaches is the identification of effective\nmodels to provide a comprehensive and relevant systems view. An ideal method\ncan answer a biological or medical question, identifying important features and\npredicting outcomes, by harnessing heterogeneous data across several dimensions\nof biological variation. In this Review, we describe the principles of data\nintegration and discuss current methods and available implementations. We\nprovide examples of successful data integration in biology and medicine.\nFinally, we discuss current challenges in biomedical integrative methods and\nour perspective on the future development of the field.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 04:31:59 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 18:35:23 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Zitnik", "Marinka", ""], ["Nguyen", "Francis", ""], ["Wang", "Bo", ""], ["Leskovec", "Jure", ""], ["Goldenberg", "Anna", ""], ["Hoffman", "Michael M.", ""]]}, {"id": "1807.00149", "submitter": "Ralf-Peter Mundani", "authors": "Nevena Perovi\\'c (1), J\\'er\\^ome Frisch (1), Ralf-Peter Mundani (1),\n  Ernst Rank (1) ((1) Technische Universit\\\"at M\\\"unchen, Munich, Germany)", "title": "Interactive data exploration for high-performance fluid flow\n  computations through porous media", "comments": "8 pages,8 figures", "journal-ref": "Proceedings of the 16th International Symposium on Symbolic and\n  Numeric Algorithms for Scientific Computing (2014) 463-470", "doi": "10.1109/SYNASC.2014.68", "report-no": null, "categories": "cs.CE physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Huge data advent in high-performance computing (HPC) applications such as\nfluid flow simulations usually hinders the interactive processing and\nexploration of simulation results. Such an interactive data exploration not\nonly allows scientiest to 'play' with their data but also to visualise huge\n(distributed) data sets in both an efficient and easy way. Therefore, we\npropose an HPC data exploration service based on a sliding window concept, that\nenables researches to access remote data (available on a supercomputer or\ncluster) during simulation runtime without exceeding any bandwidth limitations\nbetween the HPC back-end and the user front-end.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 09:47:54 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Perovi\u0107", "Nevena", "", "Technische Universit\u00e4t M\u00fcnchen, Munich, Germany"], ["Frisch", "J\u00e9r\u00f4me", "", "Technische Universit\u00e4t M\u00fcnchen, Munich, Germany"], ["Mundani", "Ralf-Peter", "", "Technische Universit\u00e4t M\u00fcnchen, Munich, Germany"], ["Rank", "Ernst", "", "Technische Universit\u00e4t M\u00fcnchen, Munich, Germany"]]}, {"id": "1807.00499", "submitter": "Jana Lipkova", "authors": "Jana Lipkova, Panagiotis Angelikopoulos, Stephen Wu, Esther Alberts,\n  Benedikt Wiestler, Christian Diehl, Christine Preibisch, Thomas Pyka,\n  Stephanie Combs, Panagiotis Hadjidoukas, Koen Van Leemput, Petros\n  Koumoutsakos, John S. Lowengrub and Bjoern Menze", "title": "Personalized Radiotherapy Design for Glioblastoma: Integrating\n  Mathematical Tumor Models, Multimodal Scans and Bayesian Inference", "comments": "Copyright (c) 2019 IEEE. Personal use of this material is permitted.\n  However, permission to use this material for any other purposes must be\n  obtained from the IEEE by sending a request to pubs-permissions@ieee.org.\n  Accepted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2902044", "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Glioblastoma is a highly invasive brain tumor, whose cells infiltrate\nsurrounding normal brain tissue beyond the lesion outlines visible in the\ncurrent medical scans. These infiltrative cells are treated mainly by\nradiotherapy. Existing radiotherapy plans for brain tumors derive from\npopulation studies and scarcely account for patient-specific conditions. Here\nwe provide a Bayesian machine learning framework for the rational design of\nimproved, personalized radiotherapy plans using mathematical modeling and\npatient multimodal medical scans. Our method, for the first time, integrates\ncomplementary information from high resolution MRI scans and highly specific\nFET-PET metabolic maps to infer tumor cell density in glioblastoma patients.\nThe Bayesian framework quantifies imaging and modeling uncertainties and\npredicts patient-specific tumor cell density with confidence intervals. The\nproposed methodology relies only on data acquired at a single time point and\nthus is applicable to standard clinical settings. An initial clinical\npopulation study shows that the radiotherapy plans generated from the inferred\ntumor cell infiltration maps spare more healthy tissue thereby reducing\nradiation toxicity while yielding comparable accuracy with standard\nradiotherapy protocols. Moreover, the inferred regions of high tumor cell\ndensities coincide with the tumor radioresistant areas, providing guidance for\npersonalized dose-escalation. The proposed integration of multimodal scans and\nmathematical modeling provides a robust, non-invasive tool to assist\npersonalized radiotherapy design.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 07:33:37 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 10:17:00 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 08:16:49 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Lipkova", "Jana", ""], ["Angelikopoulos", "Panagiotis", ""], ["Wu", "Stephen", ""], ["Alberts", "Esther", ""], ["Wiestler", "Benedikt", ""], ["Diehl", "Christian", ""], ["Preibisch", "Christine", ""], ["Pyka", "Thomas", ""], ["Combs", "Stephanie", ""], ["Hadjidoukas", "Panagiotis", ""], ["Van Leemput", "Koen", ""], ["Koumoutsakos", "Petros", ""], ["Lowengrub", "John S.", ""], ["Menze", "Bjoern", ""]]}, {"id": "1807.00567", "submitter": "Ralf-Peter Mundani", "authors": "Atanas Atanasov (1), Hans-Joachim Bungartz (1), J\\'er\\^ome Frisch (1),\n  Miriam Mehl (1), Ralf-Peter Mundani (1), Ernst Rank (1), Christoph van Treeck\n  (2) ((1) Technische Universit\\\"at M\\\"unchen, Munich, Germany, (2)\n  Fraunhofer-Institut f\\\"ur Bauphysik, Holzkirchen, Germany)", "title": "Computational steering of complex flow simulations", "comments": "12 pages, 8 figures", "journal-ref": "High Performance Computing in Science and Engineering (2010) 63-74", "doi": "10.1007/978-3-642-13872-0_6", "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Steering, the combination of a simulation back-end with a\nvisualisation front-end, offers great possibilities to exploit and optimise\nscenarios in engineering applications. Due to its interactivity, it requires\nfast grid generation, simulation, and visualisation and, therefore, mostly has\nto rely on coarse and inaccurate simulations typically performed on rather\nsmall interactive computing facilities and not on much more powerful\nhigh-performance computing architectures operated in batch-mode. This paper\npresents a steering environment that intends to bring these two worlds - the\ninteractive and the classical HPC world - together in an integrated way. The\nenvironment consists of efficient fluid dynamics simulation codes and a\nsteering and visualisation framework providing a user interface, communication\nmethods for distributed steering, and parallel visualisation tools. The gap\nbetween steering and HPC is bridged by a hierarchical approach that performs\nfast interactive simulations for many scenario variants increasing the accuracy\nvia hierarchical refinements in dependence of the time the user wants to wait.\nFinally, the user can trigger large simulations for selected setups on an HPC\narchitecture exploiting the pre-computations already done on the interactive\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 09:43:45 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Atanasov", "Atanas", ""], ["Bungartz", "Hans-Joachim", ""], ["Frisch", "J\u00e9r\u00f4me", ""], ["Mehl", "Miriam", ""], ["Mundani", "Ralf-Peter", ""], ["Rank", "Ernst", ""], ["van Treeck", "Christoph", ""]]}, {"id": "1807.00672", "submitter": "Fabrice Zaoui", "authors": "Fabrice Zaoui (EDF R\\&D STEP)", "title": "A GPU-enabled finite volume solver for large shallow water simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS physics.comp-ph physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the implementation of a HLLC finite volume solver using\nGPU technology for the solution of shallow water problems in two dimensions. It\ncompares both CPU and GPU approaches for implementing all the solver's steps.\nThe technology of graphics and central processors is highlighted with a\nparticular emphasis on the CUDA architecture of NVIDIA. The simple and\nwell-documented Application Programming Interface (CUDA API) facilitates the\nuse of the display card workstation as an additional computer unit to the\ncentral processor. Four professional solutions of the NVIDIA Quadro line are\ntested. Comparison tests between CPU and GPU are carried out on unstructured\ngrids of small sizes (up to 10,000 elements), medium and large sizes (up to\n10,000,000 elements). For all test cases, the accuracy of results is of the\nsame order of magnitude for both approaches. Furthermore, the obtained speed\ngains with the GPU strongly depend on the model of the graphics card, the size\nof the problem and the simulation time.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 08:11:29 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zaoui", "Fabrice", "", "EDF R\\&D STEP"]]}, {"id": "1807.01117", "submitter": "Matthias R\\\"othlin", "authors": "Matthias R\\\"othlin, Hagen Klippel, Konrad Wegener", "title": "Meshless Methods for Large Deformation Elastodynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Meshless methods are a promising candidate to reliably simulate materials\nundergoing large deformations. Unlike mesh based methods like the FEM, meshless\nmethods are not limited in the amount of deformation they can reproduce since\nthere are no mesh regularity constraints to consider. However, other numerical\nissues like zero energy modes, the tensile instability and disorder of the\ndiscretization points due to the deformation may impose limits on the\ndeformations possible. It is thus worthwhile to benchmark a wide array of these\nmethods since a proper review to this end has been missing from the literature\nso far. In the interest of reproducibility, the complete source code of all\nmethods considered is made public.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 12:29:15 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 12:51:56 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 13:04:58 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["R\u00f6thlin", "Matthias", ""], ["Klippel", "Hagen", ""], ["Wegener", "Konrad", ""]]}, {"id": "1807.01480", "submitter": "Mats G Larson", "authors": "Erik Burman, Peter Hansbo, Mats G. Larson, Andre Massing, Sara Zahedi", "title": "A Stabilized Cut Streamline Diffusion Finite Element Method for\n  Convection-Diffusion Problems on Surfaces", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a stabilized cut finite element method for the stationary\nconvection diffusion problem on a surface embedded in ${\\mathbb{R}}^d$. The cut\nfinite element method is based on using an embedding of the surface into a\nthree dimensional mesh consisting of tetrahedra and then using the restriction\nof the standard piecewise linear continuous elements to a piecewise linear\napproximation of the surface. The stabilization consists of a standard\nstreamline diffusion stabilization term on the discrete surface and a so called\nnormal gradient stabilization term on the full tetrahedral elements in the\nactive mesh. We prove optimal order a priori error estimates in the standard\nnorm associated with the streamline diffusion method and bounds for the\ncondition number of the resulting stiffness matrix. The condition number is of\noptimal order $O(h^{-1})$ for a specific choice of method parameters. Numerical\nexample supporting our theoretical results are also included.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 08:34:32 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Burman", "Erik", ""], ["Hansbo", "Peter", ""], ["Larson", "Mats G.", ""], ["Massing", "Andre", ""], ["Zahedi", "Sara", ""]]}, {"id": "1807.01538", "submitter": "Andreas Selmar Hauptmann", "authors": "Andreas Hauptmann, Masaru Ikehata, Hiromichi Itou, and Samuli Siltanen", "title": "Revealing cracks inside conductive bodies by electric surface\n  measurements", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aaf273", "report-no": null, "categories": "math.AP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is introduced for using electrical surface measurements to\ndetect and monitor cracks inside a two-dimensional conductive body. The\ntechnique is based on transforming the probing functions of the classical\nenclosure method by the Kelvin transform. The transform makes it possible to\nuse virtual discs for probing the interior of the body using electric\nmeasurements performed on a flat surface. Theoretical results are presented to\nenable probing of the full domain to create a profile indicating cracks in the\ndomain. Feasibility of the method is demonstrated with a simulated model of\nattaching metal sheets together by resistance spot welding.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 12:23:06 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Hauptmann", "Andreas", ""], ["Ikehata", "Masaru", ""], ["Itou", "Hiromichi", ""], ["Siltanen", "Samuli", ""]]}, {"id": "1807.01769", "submitter": "Pierre Augier", "authors": "Ashwin Vishnu Mohanan, Cyrille Bonamy, Miguel Calpe Linares, Pierre\n  Augier", "title": "FluidSim: modular, object-oriented Python package for high-performance\n  CFD simulations", "comments": null, "journal-ref": null, "doi": "10.5334/jors.239", "report-no": null, "categories": "cs.CE physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Python package fluidsim is introduced in this article as an extensible\nframework for Computational Fluid Mechanics (CFD) solvers. It is developed as a\npart of FluidDyn project (Augier et al., 2018), an effort to promote\nopen-source and open-science collaboration within fluid mechanics community and\nintended for both educational as well as research purposes. Solvers in fluidsim\nare scalable, High-Performance Computing (HPC) codes which are powered under\nthe hood by the rich, scientific Python ecosystem and the Application\nProgramming Interfaces (API) provided by fluiddyn and fluidfft packages\n(Mohanan et al., 2018). The present article describes the design aspects of\nfluidsim, viz. use of Python as the main language; focus on the ease of use,\nreuse and maintenance of the code without compromising performance. The\nimplementation details including optimization methods, modular organization of\nfeatures and object-oriented approach of using classes to implement solvers are\nalso briefly explained. Currently, fluidsim includes solvers for a variety of\nphysical problems using different numerical methods (including\nfinite-difference methods). However, this metapaper shall dwell only on the\nimplementation and performance of its pseudo-spectral solvers, in particular\nthe two- and three-dimensional Navier-Stokes solvers. We investigate the\nperformance and scalability of fluidsim in a state of the art HPC cluster.\nThree similar pseudo-spectral CFD codes based on Python (Dedalus, SpectralDNS)\nand Fortran (NS3D) are presented and qualitatively and quantitatively compared\nto fluidsim. The source code is hosted at Bitbucket as a Mercurial repository\nbitbucket.org/fluiddyn/fluidsim and the documentation generated using Sphinx\ncan be read online at fluidsim.readthedocs.io.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 10:17:29 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Mohanan", "Ashwin Vishnu", ""], ["Bonamy", "Cyrille", ""], ["Linares", "Miguel Calpe", ""], ["Augier", "Pierre", ""]]}, {"id": "1807.01778", "submitter": "Chunfeng Cui", "authors": "Chunfeng Cui and Zheng Zhang", "title": "Uncertainty Quantification of Electronic and Photonic ICs with\n  Non-Gaussian Correlated Process Variations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.CE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the invention of generalized polynomial chaos in 2002, uncertainty\nquantification has impacted many engineering fields, including variation-aware\ndesign automation of integrated circuits and integrated photonics. Due to the\nfast convergence rate, the generalized polynomial chaos expansion has achieved\norders-of-magnitude speedup than Monte Carlo in many applications. However,\nalmost all existing generalized polynomial chaos methods have a strong\nassumption: the uncertain parameters are mutually independent or Gaussian\ncorrelated. This assumption rarely holds in many realistic applications, and it\nhas been a long-standing challenge for both theorists and practitioners.\n  This paper propose a rigorous and efficient solution to address the challenge\nof non-Gaussian correlation. We first extend generalized polynomial chaos, and\npropose a class of smooth basis functions to efficiently handle non-Gaussian\ncorrelations. Then, we consider high-dimensional parameters, and develop a\nscalable tensor method to compute the proposed basis functions. Finally, we\ndevelop a sparse solver with adaptive sample selections to solve\nhigh-dimensional uncertainty quantification problems. We validate our theory\nand algorithm by electronic and photonic ICs with 19 to 57 non-Gaussian\ncorrelated variation parameters. The results show that our approach outperforms\nMonte Carlo by $2500\\times$ to $3000\\times$ in terms of efficiency. Moreover,\nour method can accurately predict the output density functions with multiple\npeaks caused by non-Gaussian correlations, which is hard to handle by existing\nmethods.\n  Based on the results in this paper, many novel uncertainty quantification\nalgorithms can be developed and can be further applied to a broad range of\nengineering domains.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 23:38:08 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Cui", "Chunfeng", ""], ["Zhang", "Zheng", ""]]}, {"id": "1807.02021", "submitter": "Nicholas Crisp", "authors": "Nicholas H. Crisp, Sabrina Livadiotti and Peter C.E. Roberts", "title": "A Semi-Analytical Method for Calculating Revisit Time for Satellite\n  Constellations with Discontinuous Coverage", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unique approach to the problem of calculating revisit\ntime metrics for different satellite orbits, sensor geometries, and\nconstellation configurations with application to early lifecycle design and\noptimisation processes for Earth observation missions. The developed\nsemi-analytical approach uses an elliptical projected footprint geometry to\nprovide an accuracy similar to that of industry standard numerical orbit\nsimulation software but with an efficiency of published analytical methods.\nUsing the developed method, extensive plots of maximum revisit time are\npresented for varying altitude, inclination, target latitudes, sensor\ncapabilities, and constellation configuration, providing valuable reference for\nEarth observation system design.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 14:27:42 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Crisp", "Nicholas H.", ""], ["Livadiotti", "Sabrina", ""], ["Roberts", "Peter C. E.", ""]]}, {"id": "1807.02685", "submitter": "Koki Ho", "authors": "Pauline C. M. Jakob, Seiichi Shimizu, Shoji Yoshikawa, Koki Ho", "title": "Optimal Satellite Constellation Spare Strategy Using Multi-Echelon\n  Inventory Control", "comments": "32 pages, 7 figures, Published", "journal-ref": "Journal of Spacecraft and Rockets, Vol. 56, No. 5, pp. 1449-1461,\n  2019", "doi": "10.2514/1.A34387", "report-no": null, "categories": "math.OC cs.CE cs.SY physics.space-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent growing trend to develop large-scale satellite constellations\n(i.e., mega-constellation) with low-cost small satellites has brought the need\nfor an efficient and scalable maintenance strategy decision plan. Traditional\nspare strategies for satellite constellations cannot handle these\nmega-constellations due to their limited scalability in number of satellites\nand/or frequency of failures. In this paper, we propose a novel spare strategy\nusing an inventory management approach. We consider a set of parking orbits at\na lower altitude than the constellation for spare storage, and model satellite\nconstellation spare strategy problem using a multi-echelon (s,Q)-type inventory\npolicy, viewing Earth's ground as a supplier, parking orbits as warehouses, and\nin-plane spare stocks as retailers. This inventory model is unique in that the\nparking orbits (warehouses) drift away from the orbital planes over time due to\norbital mechanics' effects, and the in-plane spare stocks (retailers) would\nreceive the resupply from the closest (i.e., minimum waiting time) available\nwarehouse at the time of delivery. The parking orbits (warehouses) are also\nresupplied from the ground (supplier) with stochastic lead time caused by the\norder processing and launch opportunities, leveraging the cost saving effects\nby launching many satellites in one rocket (i.e., batch launch discount). The\nproposed analytical model is validated against simulations using Latin\nHypercube Sampling. Furthermore, based on the proposed model, an optimization\nformulation is introduced to identify the optimal spare strategy, comprising\nthe parking orbits characteristics and all locations policies, to minimize the\nmaintenance cost of the system given performance requirements. The proposed\nmodel and optimization method are applied to a real-world case study of\nsatellite mega-constellation to demonstrate their value.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 16:18:16 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2018 18:02:52 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 03:36:39 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 21:39:05 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Jakob", "Pauline C. M.", ""], ["Shimizu", "Seiichi", ""], ["Yoshikawa", "Shoji", ""], ["Ho", "Koki", ""]]}, {"id": "1807.03628", "submitter": "Felix Wolf", "authors": "J\\\"urgen D\\\"olz and Stefan Kurz and Sebastian Sch\\\"ops and Felix Wolf", "title": "A Numerical Comparison of an Isogeometric and a Classical Higher-Order\n  Approach to the Electric Field Integral Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we advocate a novel spline-based isogeometric approach for\nboundary elements and its efficient implementation. We compare solutions\nobtained by both an isogeometric approach, and a classical parametric\nhigher-order approach via Raviart-Thomas elements to the solution of the\nelectric field integral equation; i.e., the solution to an electromagnetic\nscattering problem, promising high convergence orders w.r.t. pointwise error.\nWe discuss both, the obtained accuracy per DOF, as well as the effort required\nto solve the corresponding system iteratively, on three numerical examples of\nvarying complexity.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 13:35:19 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["D\u00f6lz", "J\u00fcrgen", ""], ["Kurz", "Stefan", ""], ["Sch\u00f6ps", "Sebastian", ""], ["Wolf", "Felix", ""]]}, {"id": "1807.04521", "submitter": "Paul Carpenter", "authors": "Theo Ungerer and Paul Carpenter", "title": "Eurolab-4-HPC Long-Term Vision on High-Performance Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radical changes in computing are foreseen for the next decade. The US IEEE\nsociety wants to \"reboot computing\" and the HiPEAC Vision 2017 sees the time to\n\"re-invent computing\", both by challenging its basic assumptions. This document\npresents the \"EuroLab-4-HPC Long-Term Vision on High-Performance Computing\" of\nAugust 2017, a road mapping effort within the EC CSA1 Eurolab-4-HPC that\ntargets potential changes in hardware, software, and applications in\nHigh-Performance Computing (HPC).\n  The objective of the Eurolab-4-HPC vision is to provide a long-term roadmap\nfrom 2023 to 2030 for High-Performance Computing (HPC). Because of the\nlong-term perspective and its speculative nature, the authors started with an\nassessment of future computing technologies that could influence HPC hardware\nand software. The proposal on research topics is derived from the report and\ndiscussions within the road mapping expert group. We prefer the term \"vision\"\nover \"roadmap\", firstly because timings are hard to predict given the long-term\nperspective, and secondly because EuroLab-4-HPC will have no direct control\nover the realization of its vision.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:50:44 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Ungerer", "Theo", ""], ["Carpenter", "Paul", ""]]}, {"id": "1807.04655", "submitter": "Dejan Brkic", "authors": "Pavel Praks and Dejan Brkic", "title": "One-Log Call Iterative Solution of the Colebrook Equation for Flow\n  Friction Based on Pade Polynomials", "comments": "12 pages, 2 figures, 2 tables", "journal-ref": "Praks, P.; Brkic, D. One-Log Call Iterative Solution of the\n  Colebrook Equation for Flow Friction Based on Pade Polynomials. Energies\n  2018, 11, 1825", "doi": "10.3390/en11071825", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 80 year-old empirical Colebrook function, widely used as an informal\nstandard for hydraulic resistance, relates implicitly the unknown flow friction\nfactor, with the known Reynolds number and the known relative roughness of a\npipe inner surface. It is based on logarithmic law in the form that captures\nthe unknown flow friction factor in a way that it cannot be extracted\nanalytically. As an alternative to the explicit approximations or to the\niterative procedures that require at least a few evaluations of computationally\nexpensive logarithmic function or non-integer powers, this paper offers an\naccurate and computationally cheap iterative algorithm based on Pade\npolynomials with only one log-call in total for the whole procedure (expensive\nlog-calls are substituted with Pade polynomials in each iteration with the\nexception of the first). The proposed modification is computationally less\ndemanding compared with the standard approaches of engineering practice, but\ndoes not influence the accuracy or the number of iterations required to reach\nthe final balanced solution.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 14:48:16 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Praks", "Pavel", ""], ["Brkic", "Dejan", ""]]}, {"id": "1807.05298", "submitter": "Hui Liu Mr", "authors": "He Zhong, Hui Liu, Tao Cui, Lihua Shen, Bo Yang, Ruijian He, Zhangxin\n  Chen", "title": "Numerical Simulations of Polymer Flooding Process in Porous Media on\n  Distributed-memory Parallel Computers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polymer flooding is a mature enhanced oil recovery technique that has been\nsuccessfully applied in many field projects. By injecting polymer into a\nreservoir, the viscosity of water is increased, and the efficiency of water\nflooding is improved. As a result, more oil can be recovered. This paper\npresents numerical simulations of a polymer flooding process using parallel\ncomputers, where the numerical modeling of polymer retention, inaccessible pore\nvolumes, a permeability reduction and polymer absorption are considered.\nDarcy's law is employed to model the behavoir of a fluid in porous media, and\nthe upstream finite difference (volume) method is applied to discretize the\nmass conservation equations. Numerical methods, including discretization\nschemes, linear solver methods, nonlinearization methods and parallel\ntechniques are introduced. Numerical experiments show that, on one hand,\ncomputed results match those from the commercial simulator,\nSchlumberger-Eclipse, which is widely applied by the petroleum industry, and,\non the other hand, our simulator has excellent scalability, which is\ndemonstrated by field applications with up to 27 million grid blocks using up\nto 2048 CPU cores.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 22:26:44 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 17:17:05 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 04:11:51 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Zhong", "He", ""], ["Liu", "Hui", ""], ["Cui", "Tao", ""], ["Shen", "Lihua", ""], ["Yang", "Bo", ""], ["He", "Ruijian", ""], ["Chen", "Zhangxin", ""]]}, {"id": "1807.05623", "submitter": "Timur Bazhirov", "authors": "Protik Das, Mohammad Mohammadi, Timur Bazhirov", "title": "Accessible computational materials design with high fidelity and high\n  throughput", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.mtrl-sci cond-mat.other cs.CE cs.DC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite multiple successful applications of high-throughput computational\nmaterials design from first principles, there is a number of factors that\ninhibit its future adoption. Of particular importance are limited ability to\nprovide high fidelity in a reliable manner and limited accessibility to\nnon-expert users. We present example applications of a novel approach, where\nhigh-fidelity first-principles simulation techniques, Density Functional Theory\nwith Hybrid Screened Exchange (HSE) and GW approximation, are standardized and\nmade available online in an accessible and repeatable setting. We apply this\napproach to extract electronic band gaps and band structures for a diverse set\nof 71 materials ranging from pure elements to III-V and II-VI compounds,\nternary oxides and alloys. We find that for HSE and G0W0, the average relative\nerror fits within 20%, whereas for conventional Generalized Gradient\nApproximation the error is 55%. For HSE we find the average calculation time on\nan up-to-date server centrally available from a public cloud provider to fit\nwithin 48 hours. This work provides a cost-effective, accessible and repeatable\npractical recipe for performing high-fidelity first-principles calculations of\nelectronic materials in a high-throughput manner.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 22:04:59 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Das", "Protik", ""], ["Mohammadi", "Mohammad", ""], ["Bazhirov", "Timur", ""]]}, {"id": "1807.05634", "submitter": "Andre Massing", "authors": "Ceren G\\\"urkan, Simon Sticko, Andr\\'e Massing", "title": "Stabilized CutDG methods for advection-reaction problems", "comments": "Final version accepted by SISC", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop novel stabilized cut discontinuous Galerkin (CutDG) methods for\nadvection-reaction problems. The domain of interest is embedded into a\nstructured, unfitted background mesh in $\\mathbb{R}^d$ where the domain\nboundary can cut through the mesh in an arbitrary fashion. To cope with\nrobustness problems caused by small cut elements, we introduce ghost penalties\nin the vicinity of the embedded boundary to stabilize certain (semi)-norms\nassociated with the advection and reaction operator. A few abstract assumptions\non the ghost penalties are identified enabling us to derive geometrically\nrobust and optimal a priori error and condition number estimates for the\nstationary advection-reaction problem which hold irrespective of the particular\ncut configuration. Possible realizations of suitable ghost penalties are\ndiscussed. The theoretical results are corroborated by a number of\ncomputational studies for various approximation orders and for two and\nthree-dimensional test problems.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 23:47:02 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 23:03:54 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["G\u00fcrkan", "Ceren", ""], ["Sticko", "Simon", ""], ["Massing", "Andr\u00e9", ""]]}, {"id": "1807.05837", "submitter": "arXiv Admin", "authors": "Vladimir Soloviev, Andrey Belinskiy", "title": "Methods of nonlinear dynamics and the construction of cryptocurrency\n  crisis phenomena precursors", "comments": "arXiv admin note: submission has been withdrawn by arXiv\n  administrators due to inappropriate text reuse from external sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article demonstrates the possibility of constructing indicators of\ncritical and crisis phenomena in the volatile market of cryptocurrency. For\nthis purpose, the methods of the theory of complex systems such as recurrent\nanalysis of dynamic systems and the calculation of permutation entropy are\nused. It is shown that it is possible to construct dynamic measures of\ncomplexity, both recurrent and entropy, which behave in a proper way during\nactual pre-crisis periods. This fact is used to build predictors of crisis\nphenomena on the example of the main five crises recorded in the time series of\nthe key cryptocurrency bitcoin, the effectiveness of the proposed\nindicators-precursors of crises has been identified.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 01:06:21 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 19:35:03 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Soloviev", "Vladimir", ""], ["Belinskiy", "Andrey", ""]]}, {"id": "1807.06279", "submitter": "David Orden", "authors": "Omar Aloui, David Orden, Landolf Rhode-Barbarigos", "title": "Generation of planar tensegrity structures through cellular\n  multiplication", "comments": "29 pages, 19 figures, to appear at Applied Mathematical Modeling", "journal-ref": "Applied Mathematical Modelling 64 (2018), 71-92", "doi": "10.1016/j.apm.2018.07.024", "report-no": null, "categories": "cs.CE cs.CG cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensegrity structures are frameworks in a stable self-equilibrated prestress\nstate that have been applied in various fields in science and engineering.\nResearch into tensegrity structures has resulted in reliable techniques for\ntheir form finding and analysis. However, most techniques address topology and\nform separately. This paper presents a bio-inspired approach for the combined\ntopology identification and form finding of planar tensegrity structures.\nTensegrity structures are generated using tensegrity cells (elementary stable\nself-stressed units that have been proven to compose any tensegrity structure)\naccording to two multiplication mechanisms: cellular adhesion and fusion.\nChanges in the dimension of the self-stress space of the structure are found to\ndepend on the number of adhesion and fusion steps conducted as well as on the\ninteraction among the cells composing the system. A methodology for defining a\nbasis of the self-stress space is also provided. Through the definition of the\nequilibrium shape, the number of nodes and members as well as the number of\nself-stress states, the cellular multiplication method can integrate design\nconsiderations, providing great flexibility and control over the tensegrity\nstructure designed and opening the door to the development of a whole new realm\nof planar tensegrity systems with controllable characteristics.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 08:44:37 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Aloui", "Omar", ""], ["Orden", "David", ""], ["Rhode-Barbarigos", "Landolf", ""]]}, {"id": "1807.06454", "submitter": "Kalyana Babu Nakshatrala", "authors": "W. Witarto, K. B. Nakshatrala, and Y. L. Mo", "title": "Global sensitivity analysis of frequency band gaps in one-dimensional\n  phononic crystals", "comments": "K. C. Chang, Y. Tang and R. Kassawara have contributed to the\n  experimental component of the research project, which complements the\n  research present in this paper. This paper deals only with the modeling\n  component", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phononic crystals have been widely employed in many engineering fields, which\nis due to their unique feature of frequency band gaps. For example, their\ncapability to filter out the incoming elastic waves, which include seismic\nwaves, will have a significant impact on the seismic safety of nuclear\ninfrastructure. In order to accurately design the desired frequency band gaps,\none must pay attention on how the input parameters and the interaction of the\nparameters can affect the frequency band gaps. Global sensitivity analysis can\ndecompose the dispersion relationship of the phononic crystals and screen the\nvariance attributed to each of the parameters and the interaction between them.\nPrior to the application in one-dimensional (1D) phononic crystals, this paper\nwill first review the theory of global sensitivity analysis using variance\ndecomposition (Sobol sensitivity analysis). Afterwards, the sensitivity\nanalysis is applied to study a simple mathematical model with three input\nvariables for better understanding of the concept. Then, the sensitivity\nanalysis is utilized to study the characteristic of the first frequency band\ngap in 1D phononic crystals with respect to the input parameters. This study\nreveals the quantified influence of the parameters and their correlation in\ndetermining the first frequency band gap. In addition, simple straight-forward\ndesign equations based on reduced Sobol functions are proposed to easily\nestimate the first frequency band gap. Finally, the error associated with the\nproposed design equations is also addressed.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 17:01:40 GMT"}, {"version": "v2", "created": "Sat, 29 Dec 2018 18:51:07 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Witarto", "W.", ""], ["Nakshatrala", "K. B.", ""], ["Mo", "Y. L.", ""]]}, {"id": "1807.07328", "submitter": "Abdolrahman Khoshrou", "authors": "Abdolrahman Khoshrou and Eric J. Pauwels", "title": "Quantifying Volatility Reduction in German Day-ahead Spot Market in the\n  Period 2006 through 2016", "comments": null, "journal-ref": null, "doi": "10.1109/PESGM.2018.8586020", "report-no": null, "categories": "q-fin.ST cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Europe, Germany is taking the lead in the switch from the conventional to\nrenewable energy. This poses new challenges as wind and solar energy are\nfundamentally intermittent, weather-dependent and less predictable. It is\ntherefore of considerable interest to investigate the evolution of price\nvolatility in this post-transition era. There are a number of reasons, however,\nthat makes the practical studies difficult. For instance, EPEX prices can be\nzero or negative. Consequently, the standard approach in financial time series\nanalysis to switch to logarithmic measures is inapplicable. Furthermore, in\ncontrast to the stock market prices which are only available for trading days,\nEPEX prices cover the whole year, including weekends and holidays. Accordingly,\nthere is a lot of underlying variability in the data which has nothing to do\nwith volatility, but simply reflects diurnal activity patterns. An important\ndistinction of the present work is the application of matrix decomposition\ntechniques, namely the singular value decomposition (SVD), for defining an\nalternative notion of volatility. This approach is systematically more robust\ntoward outliers and also the diurnal patterns. Our observations show that the\nday-ahead market is becoming less volatile in recent years.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 10:17:07 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Khoshrou", "Abdolrahman", ""], ["Pauwels", "Eric J.", ""]]}, {"id": "1807.07485", "submitter": "Niklas Georg", "authors": "Niklas Georg, Dimitrios Loukrezis, Ulrich R\\\"omer, Sebastian Sch\\\"ops", "title": "Enhanced adaptive surrogate models with applications in uncertainty\n  quantification for nanoplasmonics", "comments": null, "journal-ref": "International Journal for Uncertainty Quantification,\n  10(2):165-193, 2020", "doi": "10.1615/Int.J.UncertaintyQuantification.2020031727", "report-no": null, "categories": "cs.CE cs.NA math.NA physics.comp-ph physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient surrogate modeling technique for uncertainty\nquantification. The method is based on a well-known dimension-adaptive\ncollocation scheme. We improve the scheme by enhancing sparse polynomial\nsurrogates with conformal maps and adjoint error correction. The methodology is\napplied to Maxwell's source problem with random input data. This setting\ncomprises many applications of current interest from computational\nnanoplasmonics, such as grating couplers or optical waveguides. Using a\nnon-trivial benchmark model we show the benefits and drawbacks of using\nenhanced surrogate models through various numerical studies. The proposed\nstrategy allows us to conduct a thorough uncertainty analysis, taking into\naccount a moderately large number of random parameters.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 15:18:54 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 17:10:33 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 08:55:13 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Georg", "Niklas", ""], ["Loukrezis", "Dimitrios", ""], ["R\u00f6mer", "Ulrich", ""], ["Sch\u00f6ps", "Sebastian", ""]]}, {"id": "1807.07693", "submitter": "Hessam Sarjoughian Ph.D.", "authors": "Hessam S. Sarjoughian, William A. Boyd, Miguel F. Acevedo", "title": "Challenges of Achieving Efficient Simulations Through Model Abstraction", "comments": "Internal Report, School of Computing, Informatics, and Decision\n  Systems Engineering, Arizona State University, Tempe, Arizona, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coupled natural systems are generally modeled at multiple abstraction levels.\nBoth structural scale and behavioral complexity of these models are\ndeterminants in the kinds of questions that can be posed and answered. As scale\nand complexity of models increase, simulation efficiency must increase to\nresolve tradeoffs between model resolution and simulation time. From this\nvantage point, we will show some problems and solutions by using as example a\nvegetation-landscape model where individual plants belonging to different\nspecies are represented as collectives that undergo growth and decline cycles\nspanning hundreds of years. Collective plant entities are assigned to cells of\na static, two-dimensional grid. This coarse-grain model, guided by homomorphic\nmodeling ideas, is derived from a fine-grain model representing plants as\nindividual objects. These models are developed using Python and GRASS tools. A\nset of experiments is devised to reveal some barriers in modeling and\nsimulating this class of systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 02:09:52 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Sarjoughian", "Hessam S.", ""], ["Boyd", "William A.", ""], ["Acevedo", "Miguel F.", ""]]}, {"id": "1807.08622", "submitter": "Mohammed Ishaquddin", "authors": "Md.Ishaquddin, S.Gopalakrishnan", "title": "Differential quadrature element for second strain gradient beam theory", "comments": "arXiv admin note: text overlap with arXiv:1802.08115", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, first we present the variational formulation for a second\nstrain gradient Euler-Bernoulli beam theory for the first time. The governing\nequation and associated classical and non-classical boundary conditions are\nobtained. Later, we propose a novel and efficient differential quadrature\nelement based on Lagrange interpolation to solve the eight order partial\ndifferential equation associated with the second strain gradient\nEuler-Bernoulli beam theory. The second strain gradient theory has\ndisplacement, slope, curvature and triple displacement derivative as degrees of\nfreedom. A generalize scheme is proposed herein to implement these\nmulti-degrees of freedom in a simplified and efficient way. The proposed\nelement is based on the strong form of governing equation and has displacement\nas the only degree of freedom in the domain, whereas, at the boundaries it has\ndisplacement, slope, curvature and triple derivative of displacement. A novel\nDQ framework is presented to incorporate the classical and non-classical\nboundary conditions by modifying the conventional weighting coefficients. The\naccuracy and efficiency of the proposed element is demonstrated through\nnumerical examples on static, free vibration and stability analysis of second\nstrain gradient elastic beams for different boundary conditions and intrinsic\nlength scale values.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 17:58:40 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ishaquddin", "Md.", ""], ["Gopalakrishnan", "S.", ""]]}, {"id": "1807.08625", "submitter": "Mohammed Ishaquddin", "authors": "Md.Ishaquddin, S.Gopalakrishnan", "title": "Novel weak form quadrature elements for second strain gradient\n  Euler-Bernoulli beam theory", "comments": "arXiv admin note: text overlap with arXiv:1802.05541", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two novel version of weak form quadrature elements are proposed based on\nLagrange and Hermite interpolations, respectively, for a sec- ond strain\ngradient Euler-Bernoulli beam theory. The second strain gradient theory is\ngoverned by eighth order partial differential equa- tion with displacement,\nslope, curvature and triple derivative of dis- placement as degrees of freedom.\nA simple and efficient differential quadrature frame work is proposed herein to\nimplement these classi- cal and non-classical degrees of freedom. A novel\nprocedure to com- pute the modified weighting coefficient matrices for the beam\nelement is presented. The proposed elements have displacement as the only\ndegree of freedom in the element domain and displacement, slope, cur- vature\nand triple derivative of displacement at the boundaries. The\nGauss-Lobatto-Legender quadrature points are assumed as element nodes and also\nused for numerical integration of the element matrices. Numerical examples are\npresented to demonstrate the efficiency and accuracy of the proposed beam\nelement.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 18:02:50 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Ishaquddin", "Md.", ""], ["Gopalakrishnan", "S.", ""]]}, {"id": "1807.09196", "submitter": "Ajinkya Kadu", "authors": "Ajinkya Kadu and Tristan van Leeuwen", "title": "A Convex Formulation for Binary Tomography", "comments": null, "journal-ref": "IEEE Transactions on Computational Imaging, vol. 6, pp. 1-11, 2020", "doi": "10.1109/TCI.2019.2898333", "report-no": null, "categories": "eess.IV cs.CE eess.SP math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Binary tomography is concerned with the recovery of binary images from a few\nof their projections (i.e., sums of the pixel values along various directions).\nTo reconstruct an image from noisy projection data, one can pose it as a\nconstrained least-squares problem. As the constraints are non-convex, many\napproaches for solving it rely on either relaxing the constraints or\nheuristics. In this paper we propose a novel convex formulation, based on the\nLagrange dual of the constrained least-squares problem. The resulting problem\nis a generalized LASSO problem which can be solved efficiently. It is a\nrelaxation in the sense that it can only be guaranteed to give a feasible\nsolution; not necessarily the optimal one. In exhaustive experiments on small\nimages (2x2, 3x3, 4x4) we find, however, that if the problem has a unique\nsolution, our dual approach finds it. In case of multiple solutions, our\napproach finds the commonalities between the solutions. Further experiments on\nrealistic numerical phantoms and an experiment on X-ray dataset show that our\nmethod compares favourably to Total Variation and DART.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 15:50:22 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 10:51:11 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2020 22:16:03 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kadu", "Ajinkya", ""], ["van Leeuwen", "Tristan", ""]]}, {"id": "1807.09688", "submitter": "Timofey Mukha", "authors": "Timofey Mukha", "title": "Turbulucid: A Python Package for Post-Processing of Fluid Flow\n  Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Python package for post-processing of plane two-dimensional data from\ncomputational fluid dynamics simulations is presented. The package, called\nturbulucid, provides means for scripted, reproducible analysis of large\nsimulation campaigns and includes routines for both data extraction and\nvisualization. For the former, the Visualization Toolkit (VTK) is used,\nallowing for post-processing of simulations performed on unstructured meshes.\nFor visualization, several matplotlib-based functions for creating highly\ncustomizable, publication-quality plots are provided. To demonstrate\nturbulucid's functionality it is here applied to post-processing a simulation\nof a flow over a backward-facing step. The implementation and architecture of\nthe package are also discussed, as well as its reuse potential.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 16:05:52 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Mukha", "Timofey", ""]]}, {"id": "1807.09761", "submitter": "Sujith Mangalathu", "authors": "Sujith Mangalathu and Jong-Su Jeon", "title": "Stripe-Based Fragility Analysis of Concrete Bridge Classes Using Machine\n  Learning Techniques", "comments": "10 Figure, 4 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for the generation of bridge-specific fragility utilizing the\ncapabilities of machine learning and stripe-based approach is presented in this\npaper. The proposed methodology using random forests helps to generate or\nupdate fragility curves for a new set of input parameters with less\ncomputational effort and expensive re-simulation. The methodology does not\nplace any assumptions on the demand model of various components and helps to\nidentify the relative importance of each uncertain variable in their seismic\ndemand model. The methodology is demonstrated through the case studies of\nmulti-span concrete bridges in California. Geometric, material and structural\nuncertainties are accounted for in the generation of bridge models and\nfragility curves. It is also noted that the traditional lognormality assumption\non the demand model leads to unrealistic fragility estimates. Fragility results\nobtained the proposed methodology curves can be deployed in risk assessment\nplatform such as HAZUS for regional loss estimation.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 01:29:13 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Mangalathu", "Sujith", ""], ["Jeon", "Jong-Su", ""]]}, {"id": "1807.09829", "submitter": "Zeliang Liu", "authors": "Zeliang Liu, C.T. Wu, M. Koishi", "title": "A deep material network for multiscale topology learning and accelerated\n  nonlinear modeling of heterogeneous materials", "comments": "33 pages, 19 figures", "journal-ref": null, "doi": "10.1016/j.cma.2018.09.020", "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new data-driven multiscale material modeling method, which\nwe refer to as deep material network, is developed based on mechanistic\nhomogenization theory of representative volume element (RVE) and advanced\nmachine learning techniques. We propose to use a collection of connected\nmechanistic building blocks with analytical homogenization solutions which\navoids the loss of essential physics in generic neural networks, and this\nconcept is demonstrated for 2-dimensional RVE problems and network depth up to\n7. Based on linear elastic RVE data from offline direct numerical simulations,\nthe material network can be effectively trained using stochastic gradient\ndescent with backpropagation algorithm, enhanced by model compression methods.\nImportantly, the trained network is valid for any local material laws without\nthe need for additional calibration or micromechanics assumption. Its\nextrapolations to unknown material and loading spaces for a wide range of\nproblems are validated through numerical experiments, including linear\nelasticity with high contrast of phase properties, nonlinear history-dependent\nplasticity and finite-strain hyperelasticity under large deformations.\n  By discovering a proper topological representation of RVE with fewer degrees\nof freedom, this intelligent material model is believed to open new\npossibilities of high-fidelity efficient concurrent simulations for a\nlarge-scale heterogeneous structure. It also provides a mechanistic\nunderstanding of structure-property relations across material length scales and\nenables the development of parameterized microstructural database for material\ndesign and manufacturing.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 19:14:37 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 18:50:15 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 13:41:24 GMT"}, {"version": "v4", "created": "Wed, 24 Oct 2018 15:57:02 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Liu", "Zeliang", ""], ["Wu", "C. T.", ""], ["Koishi", "M.", ""]]}, {"id": "1807.09913", "submitter": "Dejan Brkic", "authors": "Pavel Praks and Dejan Brkic", "title": "Advanced iterative procedures for solving the implicit Colebrook\n  equation for fluid flow friction", "comments": null, "journal-ref": "Advances in Civil Engineering vol. 2018, article 5451034", "doi": "10.1155/2018/5451034", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Colebrook equation from 1939 is still accepted as an informal\nstandard to calculate friction factor during the turbulent flow through pipes\nfrom smooth with almost negligible relative roughness to the very rough inner\nsurface. The Colebrook equation contains flow friction factor in implicit\nlogarithmic form where it is, aside of itself, a function of the Reynolds\nnumber Re and the relative roughness of inner pipe surface. To evaluate the\nerror introduced by many available explicit approximations to the Colebrook\nequation, it is necessary to determinate value of the friction factor from the\nColebrook equation as accurate as possible. The most accurate way to achieve\nthat is using some kind of iterative methods. Usually classical approach also\nknown as simple fixed point method requires up to 8 iterations to achieve the\nhigh level of accuracy, but does not require derivatives of the Colebrook\nfunction as here presented accelerated Householder approach (3rd order, 2nd\norder: Halley and Schroder method and 1st order: Newton-Raphson) which needs\nonly 3 to 7 iteration and three-point iterative methods which needs only 1 to 4\niteration to achieve the same high level of accuracy. Strategies how to find\nderivatives of the Colebrook function in symbolic form, how to avoid use of the\nderivatives (Secant method) and how to choose optimal starting point for the\niterative procedure are shown. Householder approach to the Colebrook equations\nexpressed through the Lambert W-function is also analyzed. One approximation to\nthe Colebrook equation based on the analysis from the paper with the error of\nno more than 0.0617% is shown.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 01:41:33 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Praks", "Pavel", ""], ["Brkic", "Dejan", ""]]}, {"id": "1807.11379", "submitter": "Benedikt Schott", "authors": "Benedikt Schott, Christoph Ager, Wolfgang A. Wall", "title": "Monolithic cut finite element based approaches for fluid-structure\n  interaction", "comments": "41 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cut finite element method (CutFEM) based approaches towards challenging\nfluid-structure interaction (FSI) are proposed. The different considered\nmethods combine the advantages of competing novel Eulerian (fixed-grid) and\nestablished Arbitrary-Lagrangian-Eulerian (ALE) (moving mesh) finite element\nformulations for the fluid. The objective is to highlight the benefit of using\ncut finite element techniques for moving domain problems and to demonstrate\ntheir high potential with regards to simplified mesh generation, treatment of\nlarge structural motions in surrounding flows, capturing boundary layers, their\nability to deal with topological changes in the fluid phase and their general\nstraightforward extensibility to other coupled multiphysics problems. In\naddition to a pure fixed-grid FSI method, also advanced fluid domain\ndecomposition techniques are considered rendering in highly flexible\ndiscretization methods for the FSI problem. All stabilized formulations include\nNitsche-based weak coupling of the phases supported by the ghost penalty\ntechnique for the flow field. For the resulting systems, monolithic solution\nstrategies are presented. Various 2D and 3D FSI-cases of different complexity\nvalidate the methods and demonstrate their capabilities and limitations in\ndifferent situations.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 14:46:58 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Schott", "Benedikt", ""], ["Ager", "Christoph", ""], ["Wall", "Wolfgang A.", ""]]}, {"id": "1807.11537", "submitter": "Maruti Mudunuru", "authors": "M. K. Mudunuru, N. Panda, S. Karra, G. Srinivasan, V. T. Chau, E.\n  Rougier, A. Hunter, and H. S. Viswanathan", "title": "Estimating Failure in Brittle Materials using Graph Theory", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In brittle fracture applications, failure paths, regions where the failure\noccurs and damage statistics, are some of the key quantities of interest (QoI).\nHigh-fidelity models for brittle failure that accurately predict these QoI\nexist but are highly computationally intensive, making them infeasible to\nincorporate in upscaling and uncertainty quantification frameworks. The goal of\nthis paper is to provide a fast heuristic to reasonably estimate quantities\nsuch as failure path and damage in the process of brittle failure. Towards this\ngoal, we first present a method to predict failure paths under tensile loading\nconditions and low-strain rates. The method uses a $k$-nearest neighbors\nalgorithm built on fracture process zone theory, and identifies the set of all\npossible pre-existing cracks that are likely to join early to form a large\ncrack. The method then identifies zone of failure and failure paths using\nweighted graphs algorithms. We compare these failure paths to those computed\nwith a high-fidelity model called the Hybrid Optimization Software Simulation\nSuite (HOSS). A probabilistic evolution model for average damage in a system is\nalso developed that is trained using 150 HOSS simulations and tested on 40\nsimulations. A non-parametric approach based on confidence intervals is used to\ndetermine the damage evolution over time along the dominant failure path. For\nupscaling, damage is the key QoI needed as an input by the continuum models.\nThis needs to be informed accurately by the surrogate models for calculating\neffective modulii at continuum-scale. We show that for the proposed average\ndamage evolution model, the prediction accuracy on the test data is more than\n90\\%. In terms of the computational time, the proposed models are $\\approx\n\\mathcal{O}(10^6)$ times faster compared to high-fidelity HOSS.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 19:21:57 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Mudunuru", "M. K.", ""], ["Panda", "N.", ""], ["Karra", "S.", ""], ["Srinivasan", "G.", ""], ["Chau", "V. T.", ""], ["Rougier", "E.", ""], ["Hunter", "A.", ""], ["Viswanathan", "H. S.", ""]]}, {"id": "1807.11834", "submitter": "Gabriele Pozzetti", "authors": "Gabriele Pozzetti, Hrvoje Jasak, Xavier Besseron, Alban Rousset,\n  Bernhard Peters", "title": "A parallel dual-grid multiscale approach to CFD-DEM couplings", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2018.11.030", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a new parallel dual-grid multiscale approach for CFD-DEM\ncouplings is investigated. Dual- grid multiscale CFD-DEM couplings have been\nrecently developed and successfully adopted in different applications still, an\nefficient parallelization for such a numerical method represents an open issue.\nDespite its ability to provide grid convergent solutions and more accurate\nresults than standard CFD-DEM couplings, this young numerical method requires\ngood parallel performances in order to be applied to large-scale problems and,\ntherefore, extend its range of application. The parallelization strategy here\nproposed aims to take advantage of the enhanced complexity of a dual-grid\ncoupling to gain more flexibility in the domain partitioning while keeping a\nlow inter-process communication cost. In particular, it allows avoiding inter-\nprocess communication between CFD and DEM software and still allows adopting\ncomplex partitioning strategies thanks to an optimized grid-based\ncommunication. It is shown how the parallelized multiscale coupling holds all\nits natural advantages over a mono-scale coupling and can also have better\nparallel performance. Three benchmark cases are presented to assess the\naccuracy and performance of the strategy. It is shown how the proposed method\nallows maintaining good parallel performance when operated over 1000 processes.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 14:20:40 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Pozzetti", "Gabriele", ""], ["Jasak", "Hrvoje", ""], ["Besseron", "Xavier", ""], ["Rousset", "Alban", ""], ["Peters", "Bernhard", ""]]}]