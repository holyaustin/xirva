[{"id": "1703.00446", "submitter": "Andjela Draganic", "authors": "Zoja Vulaj, Andjela Draganic, Milos Brajovic, Irena Orovic", "title": "A tool for ECG signal analysis using standard and optimized Hermite\n  transform", "comments": "accepted for presentation at the MECO 2017 conference (6th\n  Mediterranean Conference on Embedded Computing MECO 2017, Bar, Montenegro)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of a system that would ease the diagnosis of heart diseases\nwould also fasten the work of the cardiologic department in hospitals and\nfacilitate the monitoring of patients with portable devices. This paper\npresents a tool for ECG signal analysis which is designed in Matlab. The\nHermite transform domain is exploited for the analysis. The proposed transform\ndomain is very convenient for ECG signal analysis and classification. Parts of\nthe ECG signals, i.e. QRS complexes, show shape similarity with the Hermite\nbasis functions, which is one of the reasons for choosing this domain. Also,\nthe information about the signal can be represented using a small set of\ncoefficients in this domain, which makes data transmission and analysis faster.\nThe signal concentration in the Hermite domain and consequently, the number of\nsamples required for signal representation, can additionally be reduced by\nperforming the parametization of the Hermite transform. For the comparison\npurpose, the Fourier transform domain is also implemented within the software,\nin order to compare the signal concentration in two transform domains.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 17:00:42 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 16:56:04 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Vulaj", "Zoja", ""], ["Draganic", "Andjela", ""], ["Brajovic", "Milos", ""], ["Orovic", "Irena", ""]]}, {"id": "1703.00981", "submitter": "Daniel Moyer", "authors": "Daniel Moyer, Boris A Gutman, Neda Jahanshad, Paul M. Thompson", "title": "A Restaurant Process Mixture Model for Connectivity Based Parcellation\n  of the Cortex", "comments": "In the Proceedings of Information Processing in Medical Imaging 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.CE cs.CV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary objectives of human brain mapping is the division of the\ncortical surface into functionally distinct regions, i.e. parcellation. While\nit is generally agreed that at macro-scale different regions of the cortex have\ndifferent functions, the exact number and configuration of these regions is not\nknown. Methods for the discovery of these regions are thus important,\nparticularly as the volume of available information grows. Towards this end, we\npresent a parcellation method based on a Bayesian non-parametric mixture model\nof cortical connectivity.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 23:03:56 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Moyer", "Daniel", ""], ["Gutman", "Boris A", ""], ["Jahanshad", "Neda", ""], ["Thompson", "Paul M.", ""]]}, {"id": "1703.01022", "submitter": "Jacek Cyranka", "authors": "Jacek Cyranka, Thomas Wanner", "title": "Computer-assisted proof of heteroclinic connections in the\n  one-dimensional Ohta-Kawasaki model", "comments": null, "journal-ref": null, "doi": "10.1137/17M111938X", "report-no": null, "categories": "math.AP cs.CE math.DS math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computer-assisted proof of heteroclinic connections in the\none-dimensional Ohta-Kawasaki model of diblock copolymers. The model is a\nfourth-order parabolic partial differential equation subject to homogeneous\nNeumann boundary conditions, which contains as a special case the celebrated\nCahn-Hilliard equation. While the attractor structure of the latter model is\ncompletely understood for one-dimensional domains, the diblock copolymer\nextension exhibits considerably richer long-term dynamical behavior, which\nincludes a high level of multistability. In this paper, we establish the\nexistence of certain heteroclinic connections between the homogeneous\nequilibrium state, which represents a perfect copolymer mixture, and all local\nand global energy minimizers. In this way, we show that not every solution\noriginating near the homogeneous state will converge to the global energy\nminimizer, but rather is trapped by a stable state with higher energy. This\nphenomenon can not be observed in the one-dimensional Cahn-Hillard equation,\nwhere generic solutions are attracted by a global minimizer.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 03:13:45 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 16:14:33 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 18:28:28 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Cyranka", "Jacek", ""], ["Wanner", "Thomas", ""]]}, {"id": "1703.01202", "submitter": "Chao Yang", "authors": "Ying Wei and Chao Yang and Jizu Huang", "title": "Parallel energy-stable phase field crystal simulations based on domain\n  decomposition methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a parallel numerical algorithm for solving the\nphase field crystal equation. In the algorithm, a semi-implicit finite\ndifference scheme is derived based on the discrete variational derivative\nmethod. Theoretical analysis is provided to show that the scheme is\nunconditionally energy stable and can achieve second-order accuracy in both\nspace and time. An adaptive time step strategy is adopted such that the time\nstep size can be flexibly controlled based on the dynamical evolution of the\nproblem. At each time step, a nonlinear algebraic system is constructed from\nthe discretization of the phase field crystal equation and solved by a domain\ndecomposition based, parallel Newton--Krylov--Schwarz method with improved\nboundary conditions for subdomain problems. Numerical experiments with several\ntwo and three dimensional test cases show that the proposed algorithm is\nsecond-order accurate in both space and time, energy stable with large time\nsteps, and highly scalable to over ten thousands processor cores on the Sunway\nTaihuLight supercomputer.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 15:27:32 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Wei", "Ying", ""], ["Yang", "Chao", ""], ["Huang", "Jizu", ""]]}, {"id": "1703.01963", "submitter": "Wen Jiang", "authors": "Zichang He and Wen Jiang", "title": "A new belief Markov chain model and its application in inventory\n  prediction", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov chain model is widely applied in many fields, especially the field of\nprediction. The classical Discrete-time Markov chain(DTMC) is a widely used\nmethod for prediction. However, the classical DTMC model has some limitation\nwhen the system is complex with uncertain information or state space is not\ndiscrete. To address it, a new belief Markov chain model is proposed by\ncombining Dempster-Shafer evidence theory with Markov chain. In our model, the\nuncertain data is allowed to be handle in the form of interval number and the\nbasic probability assignment(BPA) is generated based on the distance between\ninterval numbers. The new belief Markov chain model overcomes the shortcomings\nof classical Markov chain and has an efficient ability in dealing with\nuncertain information. Moreover, an example of inventory prediction and the\ncomparison between our model and classical DTMC model can show the\neffectiveness and rationality of our proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 16:43:13 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["He", "Zichang", ""], ["Jiang", "Wen", ""]]}, {"id": "1703.02311", "submitter": "Olivier Pironneau", "authors": "S\\'ebastien Geeraert, Charles-Albert Lehalle, Barak Pearlmutter,\n  Olivier Pironneau (LJLL), Adil Reghai", "title": "Mini-symposium on automatic differentiation and its applications in the\n  financial industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic differentiation is involved for long in applied mathematics as an\nalternative to finite difference to improve the accuracy of numerical\ncomputation of derivatives. Each time a numerical minimization is involved,\nautomatic differentiation can be used. In between formal derivation and\nstandard numerical schemes, this approach is based on software solutions\napplying mechanically the chain rule to obtain an exact value for the desired\nderivative. It has a cost in memory and cpu consumption. For participants of\nfinancial markets (banks, insurances, financial intermediaries, etc), computing\nderivatives is needed to obtain the sensitivity of its exposure to well-defined\npotential market moves. It is a way to understand variations of their balance\nsheets in specific cases. Since the 2008 crisis, regulation demand to compute\nthis kind of exposure to many different case, to be sure market participants\nare aware and ready to face a wide spectrum of configurations. This paper shows\nhow automatic differentiation provides a partial answer to this recent\nexplosion of computation to perform. One part of the answer is a\nstraightforward application of Adjoint Algorithmic Differentiation (AAD), but\nit is not enough. Since financial sensitivities involves specific functions and\nmix differentiation with Monte-Carlo simulations, dedicated tools and\nassociated theoretical results are needed. We give here short introductions to\ntypical cases arising when one use AAD on financial markets.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 10:18:41 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 14:35:41 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Geeraert", "S\u00e9bastien", "", "LJLL"], ["Lehalle", "Charles-Albert", "", "LJLL"], ["Pearlmutter", "Barak", "", "LJLL"], ["Pironneau", "Olivier", "", "LJLL"], ["Reghai", "Adil", ""]]}, {"id": "1703.03001", "submitter": "Shobhit Jain", "authors": "Shobhit Jain, Paolo Tiso and George Haller", "title": "Exact Nonlinear Model Reduction for a von Karman beam: Slow-Fast\n  Decomposition and Spectral Submanifolds", "comments": null, "journal-ref": null, "doi": "10.1016/j.jsv.2018.01.049", "report-no": null, "categories": "math.DS cs.CE nlin.CD physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply two recently formulated mathematical techniques, Slow-Fast\nDecomposition (SFD) and Spectral Submanifold (SSM) reduction, to a von Karman\nbeam with geometric nonlinearities and viscoelastic damping. SFD identifies a\nglobal slow manifold in the full system which attracts solutions at rates\nfaster than typical rates within the manifold. An SSM, the smoothest nonlinear\ncontinuation of a linear modal subspace, is then used to further reduce the\nbeam equations within the slow manifold. This two-stage, mathematically exact\nprocedure results in a drastic reduction of the finite-element beam model to a\none-degree-of freedom nonlinear oscillator. We also introduce the technique of\nspectral quotient analysis, which gives the number of modes relevant for\nreduction as output rather than input to the reduction process.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 19:25:57 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Jain", "Shobhit", ""], ["Tiso", "Paolo", ""], ["Haller", "George", ""]]}, {"id": "1703.03030", "submitter": "Gustav Markkula", "authors": "Gustav Markkula, Erwin Boer, Richard Romano, Natasha Merat", "title": "Sustained sensorimotor control as intermittent decisions about\n  prediction errors: Computational framework and application to ground vehicle\n  steering", "comments": null, "journal-ref": null, "doi": "10.1007/s00422-017-0743-9", "report-no": null, "categories": "q-bio.NC cs.CE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conceptual and computational framework is proposed for modelling of human\nsensorimotor control, and is exemplified for the sensorimotor task of steering\na car. The framework emphasises control intermittency, and extends on existing\nmodels by suggesting that the nervous system implements intermittent control\nusing a combination of (1) motor primitives, (2) prediction of sensory outcomes\nof motor actions, and (3) evidence accumulation of prediction errors. It is\nshown that approximate but useful sensory predictions in the intermittent\ncontrol context can be constructed without detailed forward models, as a\nsuperposition of simple prediction primitives, resembling neurobiologically\nobserved corollary discharges. The proposed mathematical framework allows\nstraightforward extension to intermittent behaviour from existing\none-dimensional continuous models in the linear control and ecological\npsychology traditions. Empirical observations from a driving simulator provide\nsupport for some of the framework assumptions: It is shown that human steering\ncontrol, in routine lane-keeping and in a demanding near-limit task, is better\ndescribed as a sequence of discrete stepwise steering adjustments, than as\ncontinuous control. Furthermore, the amplitudes of individual steering\nadjustments are well predicted by a compound visual cue signalling steering\nerror, and even better so if also adjusting for predictions of how the same cue\nis affected by previous control. Finally, evidence accumulation is shown to\nexplain observed covariability between inter-adjustment durations and\nadjustment amplitudes, seemingly better so than the type of threshold\nmechanisms that are typically assumed in existing models of intermittent\ncontrol.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 20:56:04 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Markkula", "Gustav", ""], ["Boer", "Erwin", ""], ["Romano", "Richard", ""], ["Merat", "Natasha", ""]]}, {"id": "1703.03076", "submitter": "Daniele Ramazzotti", "authors": "Gelin Gao and Bud Mishra and Daniele Ramazzotti", "title": "Causal Data Science for Financial Stress Testing", "comments": null, "journal-ref": null, "doi": "10.1016/j.jocs.2018.04.003", "report-no": null, "categories": "cs.LG cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most recent financial upheavals have cast doubt on the adequacy of some\nof the conventional quantitative risk management strategies, such as VaR (Value\nat Risk), in many common situations. Consequently, there has been an increasing\nneed for verisimilar financial stress testings, namely simulating and analyzing\nfinancial portfolios in extreme, albeit rare scenarios. Unlike conventional\nrisk management which exploits statistical correlations among financial\ninstruments, here we focus our analysis on the notion of probabilistic\ncausation, which is embodied by Suppes-Bayes Causal Networks (SBCNs); SBCNs are\nprobabilistic graphical models that have many attractive features in terms of\nmore accurate causal analysis for generating financial stress scenarios. In\nthis paper, we present a novel approach for conducting stress testing of\nfinancial portfolios based on SBCNs in combination with classical machine\nlearning classification tools. The resulting method is shown to be capable of\ncorrectly discovering the causal relationships among financial factors that\naffect the portfolios and thus, simulating stress testing scenarios with a\nhigher accuracy and lower computational complexity than conventional Monte\nCarlo Simulations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 23:54:09 GMT"}, {"version": "v2", "created": "Sat, 14 Apr 2018 14:08:44 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Gao", "Gelin", ""], ["Mishra", "Bud", ""], ["Ramazzotti", "Daniele", ""]]}, {"id": "1703.03597", "submitter": "Ammar Daskin", "authors": "Ammar Daskin and Sabre Kais", "title": "Direct Application of the Phase Estimation Algorithm to Find the\n  Eigenvalues of the Hamiltonians", "comments": "10 pages, 3 figures", "journal-ref": "Chemical Physics, Volume 514, Pages 87-94, 2018", "doi": "10.1016/j.chemphys.2018.01.002", "report-no": null, "categories": "quant-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eigenvalue of a Hamiltonian, $\\mathcal{H}$, can be estimated through the\nphase estimation algorithm given the matrix exponential of the Hamiltonian,\n$exp(-i\\mathcal{H})$. The difficulty of this exponentiation impedes the\napplications of the phase estimation algorithm particularly when $\\mathcal{H}$\nis composed of non-commuting terms. In this paper, we present a method to use\nthe Hamiltonian matrix directly in the phase estimation algorithm by using an\nancilla based framework: In this framework, we also show how to find the power\nof the Hamiltonian matrix-which is necessary in the phase estimation\nalgorithm-through the successive applications. This may eliminate the necessity\nof matrix exponential for the phase estimation algorithm and therefore provide\nan efficient way to estimate the eigenvalues of particular Hamiltonians. The\nclassical and quantum algorithmic complexities of the framework are analyzed\nfor the Hamiltonians which can be written as a sum of simple unitary matrices\nand shown that a Hamiltonian of order $2^n$ written as a sum of $L$ number of\nsimple terms can be used in the phase estimation algorithm with $(n+1+logL)$\nnumber of qubits and $O(2^anL)$ number of quantum operations, where $a$ is the\nnumber of iterations in the phase estimation. In addition, we use the\nHamiltonian of the hydrogen molecule as an example system and present the\nsimulation results for finding its ground state energy.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 09:37:48 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Daskin", "Ammar", ""], ["Kais", "Sabre", ""]]}, {"id": "1703.03930", "submitter": "Fan Ye", "authors": "Fan Ye, Hu Wang", "title": "A simple Python code for computing effective properties of 2D and 3D\n  representative volume element under periodic boundary conditions", "comments": "A simple Python code is programmed to obtain effective properties of\n  Representative Volume Element (RVE) under Periodic Boundary Conditions\n  (PBCs).With simple modifications, the basic Python code may be extended to\n  the computation of the effective properties of the more complex\n  microstructure.For the 3D case, the user may experiment with various\n  algorithms and tackle a wide range of problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiscale optimization is an attractive research field recently. For the\nmost of optimization tools, design parameters should be updated during a close\nloop. Therefore, a simple Python code is programmed to obtain effective\nproperties of Representative Volume Element (RVE) under Periodic Boundary\nConditions (PBCs). It can compute the mechanical properties of a composite with\na periodic structure, in two or three dimensions. The computation method is\nbased on the Asymptotic Homogenization Theory (AHT). With simple modifications,\nthe basic Python code may be extended to the computation of the effective\nproperties of more complex microstructure. Moreover, the code provides a\nconvenient platform upon the optimization for the material and geometric\ncomposite design. The user may experiment with various algorithms and tackle a\nwide range of problems. To verify the effectiveness and reliability of the\ncode, a three-dimensional case is employed to illuminate the code. Finally\nnumerical results obtained by the code agree well with the available\ntheoretical and experimental results\n", "versions": [{"version": "v1", "created": "Sat, 11 Mar 2017 08:29:45 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Ye", "Fan", ""], ["Wang", "Hu", ""]]}, {"id": "1703.04355", "submitter": "Zhenxing Cheng", "authors": "Zhenxing Cheng, Hu Wang", "title": "A Meshless-based Local Reanalysis Method for Structural Analysis", "comments": "18 pages,31 figures, 13 tables,Reanalysis, Meshless, Kriging\n  interpolation, Combined approximations, Indirect factorization updating", "journal-ref": "Cheng, Z. X., & Wang, H. (2017). A meshless-based local reanalysis\n  method for structural analysis. Computers & Structures, 192, 126-143", "doi": "10.1016/j.compstruc.2017.07.011", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a meshless-based local reanalysis (MLR) method. The\npurpose of this study is to extend reanalysis methods to the Kriging\ninterpolation meshless method due to its high efficiency. In this study, two\nreanalysis methods: combined approximations CA) and indirect factorization\nupdating (IFU) methods are utilized. Considering the computational cost of\nmeshless methods, the reanalysis method improves the efficiency of the full\nmeshless method significantly. Compared with finite element method (FEM)-based\nreanalysis methods, the main superiority of meshless-based reanalysis method is\nto break the limitation of mesh connection. The meshless-based reanalysis is\nmuch easier to obtain the stiffness matrix even for solving the mesh distortion\nproblems. However, compared with the FEM-based reanalysis method, the critical\nchallenge is to use much more nodes in the influence domain due to high order\ninterpolation. Therefore, a local reanalysis method which only needs to\ncalculate the local stiffness matrix in the influence domain is suggested to\nimprove the efficiency further. Several typical numerical examples are tested\nand the performance of the suggested method is verified.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 12:25:04 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 02:14:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Cheng", "Zhenxing", ""], ["Wang", "Hu", ""]]}, {"id": "1703.04446", "submitter": "Lars Ruthotto", "authors": "Andreas Mang and Lars Ruthotto", "title": "A Lagrangian Gauss-Newton-Krylov Solver for Mass- and\n  Intensity-Preserving Diffeomorphic Image Registration", "comments": "code available at:\n  https://github.com/C4IR/FAIR.m/tree/master/add-ons/LagLDDMM", "journal-ref": "SIAM J. Sci. Comput., 39(5), B860-B885, 2017", "doi": "10.1137/17M1114132", "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient solver for diffeomorphic image registration problems\nin the framework of Large Deformations Diffeomorphic Metric Mappings (LDDMM).\nWe use an optimal control formulation, in which the velocity field of a\nhyperbolic PDE needs to be found such that the distance between the final state\nof the system (the transformed/transported template image) and the observation\n(the reference image) is minimized. Our solver supports both stationary and\nnon-stationary (i.e., transient or time-dependent) velocity fields. As\ntransformation models, we consider both the transport equation (assuming\nintensities are preserved during the deformation) and the continuity equation\n(assuming mass-preservation).\n  We consider the reduced form of the optimal control problem and solve the\nresulting unconstrained optimization problem using a discretize-then-optimize\napproach. A key contribution is the elimination of the PDE constraint using a\nLagrangian hyperbolic PDE solver. Lagrangian methods rely on the concept of\ncharacteristic curves that we approximate here using a fourth-order Runge-Kutta\nmethod. We also present an efficient algorithm for computing the derivatives of\nfinal state of the system with respect to the velocity field. This allows us to\nuse fast Gauss-Newton based methods. We present quickly converging iterative\nlinear solvers using spectral preconditioners that render the overall\noptimization efficient and scalable. Our method is embedded into the image\nregistration framework FAIR and, thus, supports the most commonly used\nsimilarity measures and regularization functionals. We demonstrate the\npotential of our new approach using several synthetic and real world test\nproblems with up to 14.7 million degrees of freedom.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 15:29:32 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 11:04:29 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Mang", "Andreas", ""], ["Ruthotto", "Lars", ""]]}, {"id": "1703.05158", "submitter": "Patrick Mutabaruka", "authors": "Patrick Mutabaruka and Ken Kamrin", "title": "A simulation technique for slurries interacting with moving parts and\n  deformable solids with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A numerical method for particle-laden fluids interacting with a deformable\nsolid domain and mobile rigid parts is proposed and implemented in a full\nengineering system. The fluid domain is modeled with a lattice Boltzmann\nrepresentation, the particles and rigid parts are modeled with a discrete\nelement representation, and the deformable solid domain is modeled using a\nLagrangian mesh. The main issue of this work, since separately each of these\nmethods is a mature tool, is to develop coupling and model-reduction approaches\nin order to efficiently simulate coupled problems of this nature, as occur in\nvarious geological and engineering applications. The lattice Boltzmann method\nincorporates a large-eddy simulation technique using the Smagorinsky turbulence\nmodel. The discrete element method incorporates spherical and polyhedral\nparticles for stiff contact interactions. A neo-Hookean hyperelastic model is\nused for the deformable solid. We provide a detailed description of how to\ncouple the three solvers within a unified algorithm. The technique we propose\nfor rubber modeling/coupling exploits a simplification that prevents having to\nsolve a finite-element problem each time step. We also develop a technique to\nreduce the domain size of the full system by replacing certain zones with\nquasi-analytic solutions, which act as effective boundary conditions for the\nlattice Boltzmann method. The major ingredients of the routine are are\nseparately validated. To demonstrate the coupled method in full, we simulate\nslurry flows in two kinds of piston-valve geometries. The dynamics of the valve\nand slurry are studied and reported over a large range of input parameters.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 13:54:30 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Mutabaruka", "Patrick", ""], ["Kamrin", "Ken", ""]]}, {"id": "1703.05522", "submitter": "Thilo Moshagen", "authors": "Dirk Scharff, Thilo Moshagen, Jaroslav Vond\\v{r}ejc", "title": "Treating Smoothness and Balance during Data Exchange in Explicit\n  Simulator Coupling or Cosimulation", "comments": "30 pages. This paper has been submitted as research paper and is\n  currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cosimulation methods allow combination of simulation tools of physical\nsystems running in parallel to act as a single simulation environment for a big\nsystem. As data is passed across subsystem boundaries instead of solving the\nsystem as one single equation system, it is not ensured that systemwide\nbalances are fulfilled. If the exchanged data is a flow of a conserved\nquantity, approximation errors can accumulate and make simulation results\ninaccurate. The problem of approximation errors is typically addressed with\nextrapolation of exchanged data. Nevertheless balance errors occur as\nextrapolation is approximation. This problem can be handled with balance\ncorrection methods which compensate these errors by adding corrections for the\nbalances to the signal in next coupling time step. This work aims at combining\nextrapolation of exchanged data and balance correction in a way that the\nexchanged signal not only remains smooth, meaning the existence of continuous\nderivatives, but even in a way reducing the derivatives, in order to avoid\nunphysical dynamics caused by the coupling. To this end, suitable switch and\nhat functions are constructed and applied to the problem.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 09:16:32 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Scharff", "Dirk", ""], ["Moshagen", "Thilo", ""], ["Vond\u0159ejc", "Jaroslav", ""]]}, {"id": "1703.05812", "submitter": "EPTCS", "authors": "Holger Hermanns, Peter H\\\"ofner", "title": "Proceedings 2nd Workshop on Models for Formal Analysis of Real Systems", "comments": null, "journal-ref": "EPTCS 244, 2017", "doi": "10.4204/EPTCS.244", "report-no": null, "categories": "cs.LO cs.CE cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the proceedings of MARS 2017, the second workshop on\nModels for Formal Analysis of Real Systems, held on April 29, 2017 in Uppala,\nSweden, as an affiliated workshop of ETAPS 2017, the European Joint Conferences\non Theory and Practice of Software.\n  The workshop emphasises modelling over verification. It aims at discussing\nthe lessons learned from making formal methods for the verification and\nanalysis of realistic systems. Examples are:\n  (1) Which formalism is chosen, and why?\n  (2) Which abstractions have to be made and why?\n  (3) How are important characteristics of the system modelled?\n  (4) Were there any complications while modelling the system?\n  (5) Which measures were taken to guarantee the accuracy of the model?\n  We invited papers that present full models of real systems, which may lay the\nbasis for future comparison and analysis. An aim of the workshop is to present\ndifferent modelling approaches and discuss pros and cons for each of them.\nAlternative formal descriptions of the systems presented at this workshop are\nencouraged, which should foster the development of improved specification\nformalisms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 07:16:39 GMT"}], "update_date": "2017-03-20", "authors_parsed": [["Hermanns", "Holger", ""], ["H\u00f6fner", "Peter", ""]]}, {"id": "1703.05985", "submitter": "Arijit Hazra", "authors": "Arijit Hazra, Gert Lube, Hans-Georg Raumer", "title": "Numerical Simulation of Bloch Equations for Dynamic Magnetic Resonance\n  Imaging", "comments": "Section 1 is improved with a description of the state-of-art. Some\n  simulation results are added in section 5. Mathematical theorems and\n  estimates are re-arranged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic Resonance Imaging (MRI) is a widely applied non-invasive imaging\nmodality based on non-ionizing radiation which gives excellent images and soft\ntissue contrast of living tissues. We consider the modified Bloch problem as a\nmodel of MRI for flowing spins in an incompressible flow field. After\nestablishing the well-posedness of the corresponding evolution problem, we\nanalyze its spatial semidiscretization using discontinuous Galerkin methods.\nThe high frequency time evolution requires a proper explicit and adaptive\ntemporal discretization. The applicability of the approach is shown for basic\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 12:14:53 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 01:06:26 GMT"}, {"version": "v3", "created": "Sun, 10 Sep 2017 15:59:49 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Hazra", "Arijit", ""], ["Lube", "Gert", ""], ["Raumer", "Hans-Georg", ""]]}, {"id": "1703.06118", "submitter": "Eduardo Ogasawara", "authors": "Alice Sternberg, Jorge Soares, Diego Carvalho and Eduardo Ogasawara", "title": "A Review on Flight Delay Prediction", "comments": null, "journal-ref": null, "doi": "10.1080/01441647.2020.1861123", "report-no": null, "categories": "cs.CY cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flight delays hurt airlines, airports, and passengers. Their prediction is\ncrucial during the decision-making process for all players of commercial\naviation. Moreover, the development of accurate prediction models for flight\ndelays became cumbersome due to the complexity of air transportation system,\nthe number of methods for prediction, and the deluge of flight data. In this\ncontext, this paper presents a thorough literature review of approaches used to\nbuild flight delay prediction models from the Data Science perspective. We\npropose a taxonomy and summarize the initiatives used to address the flight\ndelay prediction problem, according to scope, data, and computational methods,\ngiving particular attention to an increased usage of machine learning methods.\nBesides, we also present a timeline of significant works that depicts\nrelationships between flight delay prediction problems and research trends to\naddress them.\n  The published version of this paper is made available at\n\\url{https://doi.org/10.1080/01441647.2020.1861123}.\n  Please cite as:\n  L. Carvalho, A. Sternberg, L. Maia Gon\\c{c}alves, A. Beatriz Cruz, J.A.\nSoares, D. Brand\\~ao, D. Carvalho, e E. Ogasawara, 2020, On the relevance of\ndata science for flight delay research: a systematic review, Transport Reviews\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 12:01:52 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 14:35:30 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 15:19:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Sternberg", "Alice", ""], ["Soares", "Jorge", ""], ["Carvalho", "Diego", ""], ["Ogasawara", "Eduardo", ""]]}, {"id": "1703.06578", "submitter": "EPTCS", "authors": "Muhammad Usama Sardar (SEECS, NUST, Islamabad, Pakistan), Osman Hasan\n  (SEECS, NUST, Islamabad, Pakistan)", "title": "Towards Probabilistic Formal Modeling of Robotic Cell Injection Systems", "comments": "In Proceedings MARS 2017, arXiv:1703.05812", "journal-ref": "EPTCS 244, 2017, pp. 271-282", "doi": "10.4204/EPTCS.244.11", "report-no": null, "categories": "cs.LO cs.CE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell injection is a technique in the domain of biological cell\nmicro-manipulation for the delivery of small volumes of samples into the\nsuspended or adherent cells. It has been widely applied in various areas, such\nas gene injection, in-vitro fertilization (IVF), intracytoplasmic sperm\ninjection (ISCI) and drug development. However, the existing manual and\nsemi-automated cell injection systems require lengthy training and suffer from\nhigh probability of contamination and low success rate. In the recently\nintroduced fully automated cell injection systems, the injection force plays a\nvital role in the success of the process since even a tiny excessive force can\ndestroy the membrane or tissue of the biological cell. Traditionally, the force\ncontrol algorithms are analyzed using simulation, which is inherently\nnon-exhaustive and incomplete in terms of detecting system failures. Moreover,\nthe uncertainties in the system are generally ignored in the analysis. To\novercome these limitations, we present a formal analysis methodology based on\nprobabilistic model checking to analyze a robotic cell injection system\nutilizing the impedance force control algorithm. The proposed methodology,\ndeveloped using the PRISM model checker, allowed to find a discrepancy in the\nalgorithm, which was not found by any of the previous analysis using the\ntraditional methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 02:49:51 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Sardar", "Muhammad Usama", "", "SEECS, NUST, Islamabad, Pakistan"], ["Hasan", "Osman", "", "SEECS, NUST, Islamabad, Pakistan"]]}, {"id": "1703.06708", "submitter": "Hassan Hijazi", "authors": "David Rey and Hassan Hijazi", "title": "Complex Number Formulation and Convex Relaxations for Aircraft Conflict\n  Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel complex number formulation along with tight convex\nrelaxations for the aircraft conflict resolution problem. Our approach combines\nboth speed and heading control and provides global optimality guarantees\ndespite non-convexities in the feasible region. As a side result, we present a\nnew characterization of the conflict separation condition in the form of\ndisjunctive linear constraints. Our formulation features one binary variable\nper pair of aircraft, is free of trigonometric functions, and captures the\nnon-convexity in a set of quadratic concave constraints. Using our approach, we\nare able to close a number of open instances and reduce computational time by\nup to two orders of magnitude on standard instances.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 12:30:31 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 03:38:05 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Rey", "David", ""], ["Hijazi", "Hassan", ""]]}, {"id": "1703.07218", "submitter": "Hossein Sangrody", "authors": "Ahvand Jalali, S K. Mohammadi, H. Sangrody, A. Rahim-Zadegan", "title": "DG-Embedded Radial Distribution System Planning Using Binary-Selective\n  PSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing rate of power consumption, many new distribution systems\nneed to be constructed to accommodate connecting the new consumers to the power\ngrid. On the other hand, the increasing penetration of renewable distributed\ngeneration (DG) resources into the distribution systems and the necessity of\noptimally place them in the network can dramatically change the problem of\ndistribution system planning and design. In this paper, the problem of optimal\ndistribution system planning including conductor sizing, DG placement,\nalongside with placement and sizing of shunt capacitors is studied. A new\nBinary-Selective Particle Swarm Optimization (PSO) approach which is capable of\nhandling all types of continuous, binary and selective variables,\nsimultaneously, is proposed to solve the optimization problem of distribution\nsystem planning. The objective of the problem is to minimize the system costs.\nLoad growth rate, cost of energy, cost of power, and inflation rate are all\ntaken into account. The efficacy of the proposed method is tested on a 26-bus\ndistribution system.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 01:47:28 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Jalali", "Ahvand", ""], ["Mohammadi", "S K.", ""], ["Sangrody", "H.", ""], ["Rahim-Zadegan", "A.", ""]]}, {"id": "1703.07231", "submitter": "Hantao Cui", "authors": "Hantao Cui, Fangxing Li, Haoyu Yuan", "title": "Control and Limit Enforcements for VSC Multi-Terminal HVDC in Newton\n  Power Flow", "comments": "IEEE PES General Meeting 2017", "journal-ref": null, "doi": "10.1109/PESGM.2017.8274242", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method to automatically enforce controls and\nlimits for Voltage Source Converter (VSC) based multi-terminal HVDC in the\nNewton power flow iteration process. A general VSC MT-HVDC model with primary\nPQ or PV control and secondary voltage control is formulated. Both the\ndependent and independent variables are included in the propose formulation so\nthat the algebraic variables of the VSC MT-HVDC are adjusted simultaneously.\nThe proposed method also maintains the number of equations and the dimension of\nthe Jacobian matrix unchanged so that, when a limit is reached and a control is\nreleased, the Jacobian needs no re-factorization. Simulations on the IEEE\n14-bus and Polish 9241-bus systems are performed to demonstrate the\neffectiveness of the method.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 03:18:13 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Cui", "Hantao", ""], ["Li", "Fangxing", ""], ["Yuan", "Haoyu", ""]]}, {"id": "1703.07770", "submitter": "Danial Faghihi", "authors": "Danial Faghihi, Subhasis Sarkar, Mehdi Naderi, Lloyd Hackel, Nagaraja\n  Iyyer", "title": "A Probabilistic Design Method for Fatigue Life of Metallic Component", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present study, a general probabilistic design framework is developed\nfor cyclic fatigue life prediction of metallic hardware using methods that\naddress uncertainty in experimental data and computational model. The\nmethodology involves (i) fatigue test data conducted on coupons of Ti6Al4V\nmaterial (ii) continuum damage mechanics based material constitutive models to\nsimulate cyclic fatigue behavior of material (iii) variance-based global\nsensitivity analysis (iv) Bayesian framework for model calibration and\nuncertainty quantification and (v) computational life prediction and\nprobabilistic design decision making under uncertainty. The outcomes of\ncomputational analyses using the experimental data prove the feasibility of the\nprobabilistic design methods for model calibration in presence of incomplete\nand noisy data. Moreover, using probabilistic design methods result in\nassessment of reliability of fatigue life predicted by computational models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 17:52:53 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 02:24:19 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 13:10:24 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Faghihi", "Danial", ""], ["Sarkar", "Subhasis", ""], ["Naderi", "Mehdi", ""], ["Hackel", "Lloyd", ""], ["Iyyer", "Nagaraja", ""]]}, {"id": "1703.08835", "submitter": "Sam Ma", "authors": "Zhanshan Ma and Aaron M. Ellison", "title": "A new dominance concept and its application to diversity-stability\n  analysis", "comments": "For all correspondence to Zhanshan Sam Ma (ma@vandals.uidaho.edu)", "journal-ref": "Ecological Monographs 69, e01358 (2019)", "doi": "10.1002/ecm.1358", "report-no": null, "categories": "cs.CE q-bio.PE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new dominance concept consisting of three new dominance\nmetrics based on Lloyd's (1967) mean crowding index. The new metrics link\ncommunities and species, whereas existing ones are applicable only to\ncommunities. Our community-level metric is a function of Simpson's diversity\nindex. For species, our metric quantifies the difference between community\ndominance and the dominance of a virtual community whose mean population size\n(per species) equals the population size of the focal species. The new metrics\nhave at least two immediate applications: (i) acting as proxies for diversity\nin diversity-stability modeling (ii) replacing population abundance in\nreconstructing species dominance networks. The first application is\ndemonstrated here using data from a longitudinal study of the human vaginal\nmicrobiome, and provides new insights relevant for microbial community\nstability and disease etiology.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 16:14:34 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ma", "Zhanshan", ""], ["Ellison", "Aaron M.", ""]]}, {"id": "1703.09265", "submitter": "Boyce Griffith", "authors": "Boyce E. Griffith", "title": "Immersed boundary model of aortic heart valve dynamics with\n  physiological driving and loading conditions", "comments": null, "journal-ref": "Int J Numer Meth Biomed Eng, 28(3):317-345, 2012", "doi": "10.1002/cnm.1445", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The immersed boundary (IB) method is a mathematical and numerical framework\nfor problems of fluid-structure interaction, treating the particular case in\nwhich an elastic structure is immersed in a viscous incompressible fluid. The\nIB approach to such problems is to describe the elasticity of the immersed\nstructure in Lagrangian form, and to describe the momentum, viscosity, and\nincompressibility of the coupled fluid-structure system in Eulerian form.\nInteraction between Lagrangian and Eulerian variables is mediated by integral\nequations with Dirac delta function kernels. The IB method provides a unified\nformulation for fluid-structure interaction models involving both thin elastic\nboundaries and also thick viscoelastic bodies. In this work, we describe the\napplication of an adaptive, staggered-grid version of the IB method to the\nthree-dimensional simulation of the fluid dynamics of the aortic heart valve.\nOur model describes the thin leaflets of the aortic valve as immersed elastic\nboundaries, and describes the wall of the aortic root as a thick, semi-rigid\nelastic structure. A physiological left-ventricular pressure waveform is used\nto drive flow through the model valve, and dynamic pressure loading conditions\nare provided by a reduced (zero-dimensional) circulation model that has been\nfit to clinical data. We use this model and method to simulate aortic valve\ndynamics over multiple cardiac cycles. The model is shown to approach rapidly a\nperiodic steady state in which physiological cardiac output is obtained at\nphysiological pressures. These realistic flow rates are not specified in the\nmodel, however. Instead, they emerge from the fluid-structure interaction\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 18:53:15 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Griffith", "Boyce E.", ""]]}, {"id": "1703.09651", "submitter": "Divya Shyam Singh", "authors": "Divya Shyam Singha, G.B.L. Chowdarya, D Roy Mahapatraa", "title": "Structural Damage Identification Using Artificial Neural Network and\n  Synthetic data", "comments": "6 pages,6 figures, ISSS conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents real-time vibration based identification technique using\nmeasured frequency response functions(FRFs) under random vibration loading.\nArtificial Neural Networks (ANNs) are trained to map damage fingerprints to\ndamage characteristic parameters. Principal component statistical analysis(PCA)\ntechnique was used to tackle the problem of high dimensionality and high noise\nof data, which is common for industrial structures. The present study considers\nCrack, Rivet hole expansion and redundant uniform mass as damages on the\nstructure. Frequency response function data after being reduced in size using\nPCA is fed to individual neural networks to localize and predict the severity\nof damage on the structure. The system of ANNs trained with both numerical and\nexperimental model data to make the system reliable and robust. The methodology\nis applied to a numerical model of stiffened panel structure, where damages are\nconfined close to the stiffener. The results showed that, in all the cases\nconsidered, it is possible to localize and predict severity of the damage\noccurrence with very good accuracy and reliability.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 08:54:09 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Singha", "Divya Shyam", ""], ["Chowdarya", "G. B. L.", ""], ["Mahapatraa", "D Roy", ""]]}, {"id": "1703.09851", "submitter": "Mohamed Abuella", "authors": "Mohamed Abuella and Badrul Chowdhury", "title": "Solar Power Forecasting Using Support Vector Regression", "comments": "This works has been presented in the American Society for Engineering\n  Management, International Annual Conference, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Generation and load balance is required in the economic scheduling of\ngenerating units in the smart grid. Variable energy generations, particularly\nfrom wind and solar energy resources, are witnessing a rapid boost, and, it is\nanticipated that with a certain level of their penetration, they can become\nnoteworthy sources of uncertainty. As in the case of load demand, energy\nforecasting can also be used to mitigate some of the challenges that arise from\nthe uncertainty in the resource. While wind energy forecasting research is\nconsidered mature, solar energy forecasting is witnessing a steadily growing\nattention from the research community. This paper presents a support vector\nregression model to produce solar power forecasts on a rolling basis for 24\nhours ahead over an entire year, to mimic the practical business of energy\nforecasting. Twelve weather variables are considered from a high-quality\nbenchmark dataset and new variables are extracted. The added value of the heat\nindex and wind speed as additional variables to the model is studied across\ndifferent seasons. The support vector regression model performance is compared\nwith artificial neural networks and multiple linear regression models for\nenergy forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 00:58:01 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Abuella", "Mohamed", ""], ["Chowdhury", "Badrul", ""]]}, {"id": "1703.09876", "submitter": "Hongge Chen", "authors": "Hongge Chen, Duane Boning and Zheng Zhang", "title": "Efficient Spatial Variation Characterization via Matrix Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a novel method to estimate and characterize spatial\nvariations on dies or wafers. This new technique exploits recent developments\nin matrix completion, enabling estimation of spatial variation across wafers or\ndies with a small number of randomly picked sampling points while still\nachieving fairly high accuracy. This new approach can be easily generalized,\nincluding for estimation of mixed spatial and structure or device type\ninformation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 03:48:14 GMT"}], "update_date": "2017-03-30", "authors_parsed": [["Chen", "Hongge", ""], ["Boning", "Duane", ""], ["Zhang", "Zheng", ""]]}, {"id": "1703.10119", "submitter": "Denys Dutykh", "authors": "Suelen Gasparin (LAMA, PUCPR), Julien Berger (LOCIE, PUCPR), Denys\n  Dutykh (LAMA), Nathan Mendes (PUCPR)", "title": "An improved explicit scheme for whole-building hygrothermal simulation", "comments": "38 pages, 19 figures, 3 tables, 28 references. Other author's papers\n  can be downloaded at http://www.denys-dutykh.com/", "journal-ref": "Building Simulation (2018), Vol. 11, pp. 465-481", "doi": "10.1007/s12273-017-0419-3", "report-no": null, "categories": "cs.CE physics.class-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit schemes require important sub-iterations when dealing with highly\nnonlinear problems such as the combined heat and moisture transfer through\nporous building elements. The computational cost rises significantly when the\nwhole-building is simulated, especially when there is important coupling among\nthe building elements themselves with neighbouring zones and with HVAC (Heating\nVentilation and Air Conditioning) systems. On the other hand, the classical\nEuler explicit scheme is generally not used because its stability condition\nimposes very fine time discretisation. Hence, this paper explores the use of an\nimproved explicit approach - the Dufort-Frankel scheme - to overcome the\ndisadvantage of the classical explicit one and to bring benefits that cannot be\nobtained by implicit methods. The Dufort-Frankel approach is first compared to\nthe classical Euler implicit and explicit schemes to compute the solution of\nnonlinear heat and moisture transfer through porous materials. Then, the\nanalysis of the Dufort-Frankel unconditionally stable explicit scheme is\nextended to the coupled heat and moisture balances on the scale of a one- and a\ntwo-zone building models. The Dufort-Frankel scheme has the benefits of being\nunconditionally stable, second-order accurate in time O(dt^2) and to compute\nexplicitly the solution at each time step, avoiding costly sub-iterations. This\napproach may reduce the computational cost by twenty, as well as it may enable\nperfect synchronism for whole-building simulation and co-simulation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 07:41:50 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 07:30:47 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Gasparin", "Suelen", "", "LAMA, PUCPR"], ["Berger", "Julien", "", "LOCIE, PUCPR"], ["Dutykh", "Denys", "", "LAMA"], ["Mendes", "Nathan", "", "PUCPR"]]}]