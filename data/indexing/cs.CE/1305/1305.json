[{"id": "1305.0062", "submitter": "Zeinab Taghavi", "authors": "Zeinab Taghavi, Narjes S. Movahedi, Sorin Draghici, Hamidreza Chitsaz", "title": "Distilled Single Cell Genome Sequencing and De Novo Assembly for Sparse\n  Microbial Communities", "comments": null, "journal-ref": null, "doi": "10.1093/bioinformatics/btt420", "report-no": null, "categories": "q-bio.GN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of every single genome present in a microbial sample is an\nimportant and challenging task with crucial applications. It is challenging\nbecause there are typically millions of cells in a microbial sample, the vast\nmajority of which elude cultivation. The most accurate method to date is\nexhaustive single cell sequencing using multiple displacement amplification,\nwhich is simply intractable for a large number of cells. However, there is hope\nfor breaking this barrier as the number of different cell types with distinct\ngenome sequences is usually much smaller than the number of cells.\n  Here, we present a novel divide and conquer method to sequence and de novo\nassemble all distinct genomes present in a microbial sample with a sequencing\ncost and computational complexity proportional to the number of genome types,\nrather than the number of cells. The method is implemented in a tool called\nSqueezambler. We evaluated Squeezambler on simulated data. The proposed divide\nand conquer method successfully reduces the cost of sequencing in comparison\nwith the naive exhaustive approach.\n  Availability: Squeezambler and datasets are available under\nhttp://compbio.cs.wayne.edu/software/squeezambler/.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 00:49:29 GMT"}, {"version": "v2", "created": "Wed, 22 May 2013 21:39:04 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Taghavi", "Zeinab", ""], ["Movahedi", "Narjes S.", ""], ["Draghici", "Sorin", ""], ["Chitsaz", "Hamidreza", ""]]}, {"id": "1305.0458", "submitter": "Giuliano Andrea Pagani", "authors": "Giuliano Andrea Pagani, Marco Aiello", "title": "From the Grid to the Smart Grid, Topologically", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CE cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Smart Grid is not just about the digitalization of the Power Grid. In its\nmore visionary acceptation, it is a model of energy management in which the\nusers are engaged in producing energy as well as consuming it, while having\ninformation systems fully aware of the energy demand-response of the network\nand of dynamically varying prices. A natural question is then: to make the\nSmart Grid a reality will the Distribution Grid have to be updated? We assume a\npositive answer to the question and we consider the lower layers of Medium and\nLow Voltage to be the most affected by the change. In our previous work, we\nhave analyzed samples of the Dutch Distribution Grid in our previous work and\nwe have considered possible evolutions of these using synthetic topologies\nmodeled after studies of complex systems in other technological domains in\nanother previous work. In this paper, we take an extra important further step\nby defining a methodology for evolving any existing physical Power Grid to a\ngood Smart Grid model thus laying the foundations for a decision support system\nfor utilities and governmental organizations. In doing so, we consider several\npossible evolution strategies and apply then to the Dutch Distribution Grid. We\nshow how more connectivity is beneficial in realizing more efficient and\nreliable networks. Our proposal is topological in nature, and enhanced with\neconomic considerations of the costs of such evolutions in terms of cabling\nexpenses and economic benefits of evolving the Grid.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 14:51:33 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2013 11:47:15 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Pagani", "Giuliano Andrea", ""], ["Aiello", "Marco", ""]]}, {"id": "1305.0596", "submitter": "Taha Hasan", "authors": "Taha Hassan, Fahad Javed and Naveed Arshad", "title": "An Empirical Investigation of V-I Trajectory based Load Signatures for\n  Non-Intrusive Load Monitoring", "comments": "11 pages, 11 figures. Under review for IEEE Transactions on Smart\n  Grid", "journal-ref": null, "doi": "10.1109/TSG.2013.2271282", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choice of load signature or feature space is one of the most fundamental\ndesign choices for non-intrusive load monitoring or energy disaggregation\nproblem. Electrical power quantities, harmonic load characteristics, canonical\ntransient and steady-state waveforms are some of the typical choices of load\nsignature or load signature basis for current research addressing appliance\nclassification and prediction. This paper expands and evaluates appliance load\nsignatures based on V-I trajectory - the mutual locus of instantaneous voltage\nand current waveforms - for precision and robustness of prediction in\nclassification algorithms used to disaggregate residential overall energy use\nand predict constituent appliance profiles. We also demonstrate the use of\nvariants of differential evolution as a novel strategy for selection of optimal\nload models in context of energy disaggregation. A publicly available benchmark\ndataset REDD is employed for evaluation purposes. Our experimental evaluations\nindicate that these load signatures, in conjunction with a number of popular\nclassification algorithms, offer better or generally comparable overall\nprecision of prediction, robustness and reliability against dynamic, noisy and\nhighly similar load signatures with reference to electrical power quantities\nand harmonic content. Herein, wave-shape features are found to be an effective\nnew basis of classification and prediction for semi-automated energy\ndisaggregation and monitoring.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 23:32:00 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Hassan", "Taha", ""], ["Javed", "Fahad", ""], ["Arshad", "Naveed", ""]]}, {"id": "1305.1112", "submitter": "Tommaso Urli", "authors": "Tommaso Urli", "title": "json2run: a tool for experiment design & analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  json2run is a tool to automate the running, storage and analysis of\nexperiments. The main advantage of json2run is that it allows to describe a set\nof experiments concisely as a JSON-formatted parameter tree. It also supports\nparallel execution of experiments, automatic parameter tuning through the\nF-Race framework and storage and analysis of experiments with MongoDB and R.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 08:31:48 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Urli", "Tommaso", ""]]}, {"id": "1305.1375", "submitter": "Rob Gysel", "authors": "Rob Gysel", "title": "Unique Perfect Phylogeny Characterizations via Uniquely Representable\n  Chordal Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CE math.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perfect phylogeny problem is a classic problem in computational biology,\nwhere we seek an unrooted phylogeny that is compatible with a set of\nqualitative characters. Such a tree exists precisely when an intersection graph\nassociated with the character set, called the partition intersection graph, can\nbe triangulated using a restricted set of fill edges. Semple and Steel used the\npartition intersection graph to characterize when a character set has a unique\nperfect phylogeny. Bordewich, Huber, and Semple showed how to use the partition\nintersection graph to find a maximum compatible set of characters. In this\npaper, we build on these results, characterizing when a unique perfect\nphylogeny exists for a subset of partial characters. Our characterization is\nstated in terms of minimal triangulations of the partition intersection graph\nthat are uniquely representable, also known as ur-chordal graphs. Our\ncharacterization is motivated by the structure of ur-chordal graphs, and the\nfact that the block structure of minimal triangulations is mirrored in the\ngraph that has been triangulated.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 01:20:15 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 08:15:31 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Gysel", "Rob", ""]]}, {"id": "1305.1861", "submitter": "Rajat Roy", "authors": "Rajat Shuvro Roy, Debashish Bhattacharya and Alexander Schliep", "title": "Turtle: Identifying frequent k-mers with cache-efficient algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting the frequencies of k-mers in read libraries is often a first step in\nthe analysis of high-throughput sequencing experiments. Infrequent k-mers are\nassumed to be a result of sequencing errors. The frequent k-mers constitute a\nreduced but error-free representation of the experiment, which can inform read\nerror correction or serve as the input to de novo assembly methods. Ideally,\nthe memory requirement for counting should be linear in the number of frequent\nk-mers and not in the, typically much larger, total number of k-mers in the\nread library.\n  We present a novel method that balances time, space and accuracy requirements\nto efficiently extract frequent k-mers even for high coverage libraries and\nlarge genomes such as human. Our method is designed to minimize cache-misses in\na cache-efficient manner by using a Pattern-blocked Bloom filter to remove\ninfrequent k-mers from consideration in combination with a novel\nsort-and-compact scheme, instead of a Hash, for the actual counting. While this\nincreases theoretical complexity, the savings in cache misses reduce the\nempirical running times. A variant can resort to a counting Bloom filter for\neven larger savings in memory at the expense of false negatives in addition to\nthe false positives common to all Bloom filter based approaches. A comparison\nto the state-of-the-art shows reduced memory requirements and running times.\nNote that we also provide the first competitive method to count k-mers up to\nsize 64.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 15:49:39 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Roy", "Rajat Shuvro", ""], ["Bhattacharya", "Debashish", ""], ["Schliep", "Alexander", ""]]}, {"id": "1305.2322", "submitter": "Harry Boyer", "authors": "Harimalala Razanamanampisoa, Zely Arivelo Randriamanantany, Hery Tiana\n  Rakotondramiarana, Fran\\c{c}ois Garde (PIMENT), Harry Boyer (PIMENT)", "title": "Simulation of a typical house in the region of Antananarivo, Madagascar.\n  Determination of passive solutions using local materials", "comments": null, "journal-ref": "3rd International Madagascar Conference in High-Energy Physics\n  (HEP-MAD 07), Antanarivo : Madagascar (2007)", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with new proposals for the design of passive solutions\nadapted to the climate of the highlands of Madagascar. While the strongest\npopulation density is located in the central highlands, the problem of thermal\ncomfort in buildings occurs mainly during winter time. Currently, people use\nraw wood to warm the poorly designed houses. This leads to a large scale\ndeforestation of the areas and causes erosion and environmental problems. The\nmethodology used consisted of the identification of a typical building and of a\ntypical meteorological year. Simulations were carried out using a thermal and\nairflow software (CODYRUN) to improve each building component (roof, walls,\nwindows, and soil) in such a way as to estimate the influence of some technical\nsolutions on each component in terms of thermal comfort. The proposed solutions\nalso took into account the use of local materials and the standard of living of\nthe country.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 12:16:41 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Razanamanampisoa", "Harimalala", "", "PIMENT"], ["Randriamanantany", "Zely Arivelo", "", "PIMENT"], ["Rakotondramiarana", "Hery Tiana", "", "PIMENT"], ["Garde", "Fran\u00e7ois", "", "PIMENT"], ["Boyer", "Harry", "", "PIMENT"]]}, {"id": "1305.2550", "submitter": "Guiomar Niso", "authors": "Guiomar Niso, Ricardo Bru\\~na, Ernesto Pereda, Ricardo Guti\\'errez,\n  Ricardo Bajo, Fernando Maest\\'u and Francisco del-Pozo", "title": "HERMES: towards an integrated toolbox to characterize functional and\n  effective brain connectivity", "comments": "58 pages, 10 figures, 3 tables, Neuroinformatics 2013", "journal-ref": null, "doi": "10.1007/s12021-013-9186-1", "report-no": null, "categories": "q-bio.NC cs.CE cs.MS physics.bio-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the interdependence between time series has become an\nimportant field of research in the last years, mainly as a result of advances\nin the characterization of dynamical systems from the signals they produce, the\nintroduction of concepts such as generalized and phase synchronization and the\napplication of information theory to time series analysis. In neurophysiology,\ndifferent analytical tools stemming from these concepts have added to the\n'traditional' set of linear methods, which includes the cross-correlation and\nthe coherency function in the time and frequency domain, respectively, or more\nelaborated tools such as Granger Causality. This increase in the number of\napproaches to tackle the existence of functional (FC) or effective connectivity\n(EC) between two (or among many) neural networks, along with the mathematical\ncomplexity of the corresponding time series analysis tools, makes it desirable\nto arrange them into a unified-easy-to-use software package. The goal is to\nallow neuroscientists, neurophysiologists and researchers from related fields\nto easily access and make use of these analysis methods from a single\nintegrated toolbox. Here we present HERMES (http://hermes.ctb.upm.es), a\ntoolbox for the Matlab environment (The Mathworks, Inc), which is designed for\nthe analysis functional and effective brain connectivity from\nneurophysiological data such as multivariate EEG and/or MEG records. It\nincludes also visualization tools and statistical methods to address the\nproblem of multiple comparisons. We believe that this toolbox will be very\nhelpful to all the researchers working in the emerging field of brain\nconnectivity analysis.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 01:04:55 GMT"}, {"version": "v2", "created": "Mon, 27 May 2013 03:53:02 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Niso", "Guiomar", ""], ["Bru\u00f1a", "Ricardo", ""], ["Pereda", "Ernesto", ""], ["Guti\u00e9rrez", "Ricardo", ""], ["Bajo", "Ricardo", ""], ["Maest\u00fa", "Fernando", ""], ["del-Pozo", "Francisco", ""]]}, {"id": "1305.2876", "submitter": "Ricardo Fabbri", "authors": "Ricardo Fabbri, Ivan N. Bastos, Francisco D. Moura Neto, Francisco J.\n  P. Lopes, Wesley N. Goncalves, Odemir M. Bruno", "title": "Multi-q Pattern Classification of Polarization Curves", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.physa.2013.09.048", "report-no": null, "categories": "cs.CE cs.CV", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Several experimental measurements are expressed in the form of\none-dimensional profiles, for which there is a scarcity of methodologies able\nto classify the pertinence of a given result to a specific group. The\npolarization curves that evaluate the corrosion kinetics of electrodes in\ncorrosive media are an application where the behavior is chiefly analyzed from\nprofiles. Polarization curves are indeed a classic method to determine the\nglobal kinetics of metallic electrodes, but the strong nonlinearity from\ndifferent metals and alloys can overlap and the discrimination becomes a\nchallenging problem. Moreover, even finding a typical curve from replicated\ntests requires subjective judgement. In this paper we used the so-called\nmulti-q approach based on the Tsallis statistics in a classification engine to\nseparate multiple polarization curve profiles of two stainless steels. We\ncollected 48 experimental polarization curves in aqueous chloride medium of two\nstainless steel types, with different resistance against localized corrosion.\nMulti-q pattern analysis was then carried out on a wide potential range, from\ncathodic up to anodic regions. An excellent classification rate was obtained,\nat a success rate of 90%, 80%, and 83% for low (cathodic), high (anodic), and\nboth potential ranges, respectively, using only 2% of the original profile\ndata. These results show the potential of the proposed approach towards\nefficient, robust, systematic and automatic classification of highly non-linear\nprofile curves.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 04:31:49 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Fabbri", "Ricardo", ""], ["Bastos", "Ivan N.", ""], ["Neto", "Francisco D. Moura", ""], ["Lopes", "Francisco J. P.", ""], ["Goncalves", "Wesley N.", ""], ["Bruno", "Odemir M.", ""]]}, {"id": "1305.3149", "submitter": "Xiao-Bo Jin", "authors": "Xiao-Bo Jin, Qiang Lu, Feng Wang, Quan-gong Huo", "title": "Qualitative detection of oil adulteration with machine learning\n  approaches", "comments": "18 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The study focused on the machine learning analysis approaches to identify the\nadulteration of 9 kinds of edible oil qualitatively and answered the following\nthree questions: Is the oil sample adulterant? How does it constitute? What is\nthe main ingredient of the adulteration oil? After extracting the\nhigh-performance liquid chromatography (HPLC) data on triglyceride from 370 oil\nsamples, we applied the adaptive boosting with multi-class Hamming loss\n(AdaBoost.MH) to distinguish the oil adulteration in contrast with the support\nvector machine (SVM). Further, we regarded the adulterant oil and the pure oil\nsamples as ones with multiple labels and with only one label, respectively.\nThen multi-label AdaBoost.MH and multi-label learning vector quantization\n(ML-LVQ) model were built to determine the ingredients and their relative ratio\nin the adulteration oil. The experimental results on six measures show that\nML-LVQ achieves better performance than multi-label AdaBoost.MH.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 13:23:19 GMT"}], "update_date": "2013-05-15", "authors_parsed": [["Jin", "Xiao-Bo", ""], ["Lu", "Qiang", ""], ["Wang", "Feng", ""], ["Huo", "Quan-gong", ""]]}, {"id": "1305.3758", "submitter": "Jennifer Warrender", "authors": "Jennifer D. Warrender and Phillip Lord", "title": "The Karyotype Ontology: a computational representation for human\n  cytogenetic patterns", "comments": "4 pages, 1 figure, to be submitted to Bio-Ontologies 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The karyotype ontology describes the human chromosome complement as\ndetermined cytogenetically, and is designed as an initial step toward the goal\nof replacing the current system which is based on semantically meaningful\nstrings. This ontology uses a novel, semi-programmatic methodology based around\nthe tawny library to construct many classes rapidly. Here, we describe our use\ncase, methodology and the event-based approach that we use to represent\nkaryotypes.\n  The ontology is available at http://www.purl.org/ontolink/karyotype/. The\nclojure code is available at http://code.google.com/p/karyotype-clj/.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2013 11:07:42 GMT"}], "update_date": "2013-05-17", "authors_parsed": [["Warrender", "Jennifer D.", ""], ["Lord", "Phillip", ""]]}, {"id": "1305.4048", "submitter": "Martin Horsch", "authors": "Martin Horsch and Stefan Becker and Juan Manuel Castillo and Stephan\n  Deublein and Agnes Fr\\\"oscher and Steffen Reiser and Stephan Werth and Jadran\n  Vrabec and Hans Hasse", "title": "Molecular modelling and simulation of electrolyte solutions,\n  biomolecules, and wetting of component surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.soft cond-mat.mes-hall cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively-parallel molecular dynamics simulation is applied to systems\ncontaining electrolytes, vapour-liquid interfaces, and biomolecules in contact\nwith water-oil interfaces. Novel molecular models of alkali halide salts are\npresented and employed for the simulation of electrolytes in aqueous solution.\nThe enzymatically catalysed hydroxylation of oleic acid is investigated by\nmolecular dynamics simulation taking the internal degrees of freedom of the\nmacromolecules into account. Thereby, Ewald summation methods are used to\ncompute the long range electrostatic interactions. In systems with a phase\nboundary, the dispersive interaction, which is modelled by the Lennard-Jones\npotential here, has a more significant long range contribution than in\nhomogeneous systems. This effect is accounted for by implementing the Janecek\ncutoff correction scheme. On this basis, the HPC infrastructure at the\nSteinbuch Centre for Computing was accessed and efficiently used, yielding new\ninsights on the molecular systems under consideration.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 11:43:34 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Horsch", "Martin", ""], ["Becker", "Stefan", ""], ["Castillo", "Juan Manuel", ""], ["Deublein", "Stephan", ""], ["Fr\u00f6scher", "Agnes", ""], ["Reiser", "Steffen", ""], ["Werth", "Stephan", ""], ["Vrabec", "Jadran", ""], ["Hasse", "Hans", ""]]}, {"id": "1305.5024", "submitter": "Shilpa Gulati", "authors": "Shilpa Gulati, Chetan Jhurani, Benjamin Kuipers", "title": "A Nonlinear Constrained Optimization Framework for Comfortable and\n  Customizable Motion Planning of Nonholonomic Mobile Robots - Part I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this series of papers, we present a motion planning framework for planning\ncomfortable and customizable motion of nonholonomic mobile robots such as\nintelligent wheelchairs and autonomous cars. In this first one we present the\nmathematical foundation of our framework.\n  The motion of a mobile robot that transports a human should be comfortable\nand customizable. We identify several properties that a trajectory must have\nfor comfort. We model motion discomfort as a weighted cost functional and\ndefine comfortable motion planning as a nonlinear constrained optimization\nproblem of computing trajectories that minimize this discomfort given the\nappropriate boundary conditions and constraints. The optimization problem is\ninfinite-dimensional and we discretize it using conforming finite elements. We\nalso outline a method by which different users may customize the motion to\nachieve personal comfort.\n  There exists significant past work in kinodynamic motion planning, to the\nbest of our knowledge, our work is the first comprehensive formulation of\nkinodynamic motion planning for a nonholonomic mobile robot as a nonlinear\noptimization problem that includes all of the following - a careful analysis of\nboundary conditions, continuity requirements on trajectory, dynamic\nconstraints, obstacle avoidance constraints, and a robust numerical\nimplementation.\n  In this paper, we present the mathematical foundation of the motion planning\nframework and formulate the full nonlinear constrained optimization problem. We\ndescribe, in brief, the discretization method using finite elements and the\nprocess of computing initial guesses for the optimization problem. Details of\nthe above two are presented in Part II of the series.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 05:37:25 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Gulati", "Shilpa", ""], ["Jhurani", "Chetan", ""], ["Kuipers", "Benjamin", ""]]}, {"id": "1305.5025", "submitter": "Shilpa Gulati", "authors": "Shilpa Gulati, Chetan Jhurani, Benjamin Kuipers", "title": "A Nonlinear Constrained Optimization Framework for Comfortable and\n  Customizable Motion Planning of Nonholonomic Mobile Robots - Part II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this series of papers, we present a motion planning framework for planning\ncomfortable and customizable motion of nonholonomic mobile robots such as\nintelligent wheelchairs and autonomous cars. In Part I, we presented the\nmathematical foundation of our framework, where we model motion discomfort as a\nweighted cost functional and define comfortable motion planning as a nonlinear\nconstrained optimization problem of computing trajectories that minimize this\ndiscomfort given the appropriate boundary conditions and constraints.\n  In this paper, we discretize the infinite-dimensional optimization problem\nusing conforming finite elements. We describe shape functions to handle\ndifferent kinds of boundary conditions and the choice of unknowns to obtain a\nsparse Hessian matrix. We also describe in detail how any trajectory\ncomputation problem can have infinitely many locally optimal solutions and our\nmethod of handling them. Additionally, since we have a nonlinear and\nconstrained problem, computation of high quality initial guesses is crucial for\nefficient solution. We show how to compute them.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 05:37:35 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Gulati", "Shilpa", ""], ["Jhurani", "Chetan", ""], ["Kuipers", "Benjamin", ""]]}, {"id": "1305.5524", "submitter": "Changchuan Yin Dr.", "authors": "Changchuan Yin, Dongchul Yoo, Stephen S.-T. Yau", "title": "Denoising the 3-Base Periodicity Walks of DNA Sequences in Gene Finding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonlinear Tracking-Differentiator is one-input-two-output system that can\ngenerate smooth approximation of measured signals and get the derivatives of\nthe signals. The nonlinear tracking-Differentiator is explored to denoise and\ngenerate the derivatives of the walks of the 3-periodicity of DNA sequences. An\nimproved algorithm for gene finding is presented using the nonlinear\nTracking-Differentiator. The gene finding algorithm employs the 3-base\nperiodicity of coding region. The 3-base periodicity DNA walks are denoised and\ntracked using the nonlinear Tracking-Differentiator. Case studies demonstrate\nthat the nonlinear Tracking-Differentiator is an effective method to improve\nthe accuracy of the gene finding algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 19:24:25 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Yin", "Changchuan", ""], ["Yoo", "Dongchul", ""], ["Yau", "Stephen S. -T.", ""]]}, {"id": "1305.5750", "submitter": "Khalid Raza", "authors": "Khalid Raza, Rajni Jaiswal", "title": "Reconstruction and Analysis of Cancer-specific Gene Regulatory Networks\n  from Gene Expression Profiles", "comments": "10 pages, 1 figure, 2 tables", "journal-ref": "International Journal on Bioinformatics & Biosciences (IJBB),\n  3(2):25-34, June 2013", "doi": "10.5121/ijbb.2013.3103", "report-no": null, "categories": "cs.SY cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of Systems Biology research is to reconstruct biological\nnetworks for its topological analysis so that reconstructed networks can be\nused for the identification of various kinds of disease. The availability of\nhigh-throughput data generated by microarray experiments fueled researchers to\nuse whole-genome gene expression profiles to understand cancer and to\nreconstruct key cancer-specific gene regulatory network. Now, the researchers\nare taking a keen interest in the development of algorithm for the\nreconstruction of gene regulatory network from whole genome expression\nprofiles. In this study, a cancer-specific gene regulatory network (prostate\ncancer) has been constructed using a simple and novel statistics based\napproach. First, significant genes differentially expressing them self in the\ndisease condition has been identified using a two-stage filtering approach\nt-test and fold-change measure. Next, regulatory relationships between the\nidentified genes has been computed using Pearson correlation coefficient. The\nobtained results has been validated with the available databases and\nliterature. We obtained a cancer-specific regulatory network of 29 genes with a\ntotal of 55 regulatory relations in which some of the genes has been identified\nas hub genes that can act as drug target for the cancer diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 19:21:45 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2013 18:21:52 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Raza", "Khalid", ""], ["Jaiswal", "Rajni", ""]]}, {"id": "1305.5796", "submitter": "Alexandru Cioaca Mr", "authors": "Alexandru Cioaca, Adrian Sandu, and Eric de Sturler", "title": "Efficient methods for computing observation impact in 4D-Var data\n  assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": "CSL-TR-2-2013", "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a practical computational approach to quantify the effect\nof individual observations in estimating the state of a system. Such an\nanalysis can be used for pruning redundant measurements, and for designing\nfuture sensor networks. The mathematical approach is based on computing the\nsensitivity of the reanalysis (unconstrained optimization solution) with\nrespect to the data. The computational cost is dominated by the solution of a\nlinear system, whose matrix is the Hessian of the cost function, and is only\navailable in operator form. The right hand side is the gradient of a scalar\ncost function that quantifies the forecast error of the numerical model. The\nuse of adjoint models to obtain the necessary first and second order\nderivatives is discussed. We study various strategies to accelerate the\ncomputation, including matrix-free iterative solvers, preconditioners, and an\nin-house multigrid solver. Experiments are conducted on both a small-size\nshallow-water equations model, and on a large-scale numerical weather\nprediction model, in order to illustrate the capabilities of the new\nmethodology.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 17:00:00 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 20:32:52 GMT"}], "update_date": "2013-07-22", "authors_parsed": [["Cioaca", "Alexandru", ""], ["Sandu", "Adrian", ""], ["de Sturler", "Eric", ""]]}, {"id": "1305.6046", "submitter": "Sidahmed Mokeddem", "authors": "Sidahmed Mokeddem, Baghdad Atmani and Mostefa Mokaddem", "title": "Supervised Feature Selection for Diagnosis of Coronary Artery Disease\n  Based on Genetic Algorithm", "comments": "First International Conference on Computational Science and\n  Engineering (CSE-2013), May 18 ~ 19, 2013, Dubai, UAE. Volume Editors:\n  Sundarapandian Vaidyanathan, Dhinaharan Nagamalai", "journal-ref": null, "doi": "10.5121/csit.2013.3305", "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature Selection (FS) has become the focus of much research on decision\nsupport systems areas for which data sets with tremendous number of variables\nare analyzed. In this paper we present a new method for the diagnosis of\nCoronary Artery Diseases (CAD) founded on Genetic Algorithm (GA) wrapped Bayes\nNaive (BN) based FS. Basically, CAD dataset contains two classes defined with\n13 features. In GA BN algorithm, GA generates in each iteration a subset of\nattributes that will be evaluated using the BN in the second step of the\nselection procedure. The final set of attribute contains the most relevant\nfeature model that increases the accuracy. The algorithm in this case produces\n85.50% classification accuracy in the diagnosis of CAD. Thus, the asset of the\nAlgorithm is then compared with the use of Support Vector Machine (SVM),\nMultiLayer Perceptron (MLP) and C4.5 decision tree Algorithm. The result of\nclassification accuracy for those algorithms are respectively 83.5%, 83.16% and\n80.85%. Consequently, the GA wrapped BN Algorithm is correspondingly compared\nwith other FS algorithms. The Obtained results have shown very promising\noutcomes for the diagnosis of CAD.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2013 18:16:52 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Mokeddem", "Sidahmed", ""], ["Atmani", "Baghdad", ""], ["Mokaddem", "Mostefa", ""]]}, {"id": "1305.6569", "submitter": "David Aristoff", "authors": "David Aristoff and Tony Leli\\`evre", "title": "Mathematical Analysis of Temperature Accelerated Dynamics", "comments": "28 pages, 2 figures", "journal-ref": "Multiscale Model. Simul., 12(1), 290--317 (2014)", "doi": "10.1137/130923063", "report-no": null, "categories": "math-ph cs.CE math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a mathematical framework for temperature accelerated dynamics (TAD),\nan algorithm proposed by M.R. S{\\o}rensen and A.F. Voter to efficiently\ngenerate metastable stochastic dynamics. Using the notion of quasistationary\ndistributions, we propose some modifications to TAD. Then considering the\nmodified algorithm in an idealized setting, we show how TAD can be made\nmathematically rigorous.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 17:51:31 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2014 21:17:00 GMT"}], "update_date": "2014-08-18", "authors_parsed": [["Aristoff", "David", ""], ["Leli\u00e8vre", "Tony", ""]]}, {"id": "1305.6861", "submitter": "J\\\"urgen K\\\"ursch", "authors": "J\\\"urgen K\\\"ursch", "title": "Design and Realization of a Scalable Simulator of Magnetic Resonance\n  Tomography", "comments": "PhD thesis, RWTH Aachen, Germany, 2003, in German, available at\n  http://darwin.bth.rwth-aachen.de/opus3/volltexte/2003/709/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In research activities regarding Magnetic Resonance Imaging in medicine,\nsimulation tools with a universal approach are rare. Usually, simulators are\ndeveloped and used which tend to be restricted to a particular, small range of\napplications. This led to the design and implementation of a new simulator\nPARSPIN, the subject of this thesis. In medical applications, the Bloch\nequation is a well-suited mathematical model of the underlying physics with a\nwide scope. In this thesis, it is shown how analytical solutions of the Bloch\nequation can be found, which promise substantial execution time advantages over\nnumerical solution methods. From these analytical solutions of the Bloch\nequation, a new formalism for the description and the analysis of complex\nimaging experiments is derived, the K-t formalism. It is shown that modern\nimaging methods can be better explained by the K-t formalism than by observing\nand analysing the magnetization of each spin of a spin ensemble. Various\napproaches for a numerical simulation of Magnetic Resonance imaging are\ndiscussed. It is shown that a simulation tool based on the K-t formalism\npromises a substantial gain in execution time. Proper spatial discretization\naccording to the sampling theorem, a topic rarely discussed in literature, is\nuniversally derived from the K-t formalism in this thesis. A spin-based\nsimulator is an application with high demands to computing facilities even on\nmodern hardware. In this thesis, two approaches for a parallelized software\narchitecture are designed, analysed and evaluated with regard to a reduction of\nexecution time. A number of possible applications in research and education are\ndemonstrated. For a choice of imaging experiments, results produced both\nexperimentally and by simulation are compared.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2013 16:37:13 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["K\u00fcrsch", "J\u00fcrgen", ""]]}, {"id": "1305.7071", "submitter": "Markus Gusenbauer", "authors": "Markus Gusenbauer, Harald \\\"Ozelt, Johann Fischbacher, Franz Reichel,\n  Lukas Exl, Simon Bance, Nadezhda Kataeva, Claudia Binder, Hubert Br\\\"uckl,\n  Thomas Schrefl", "title": "Simulation of magnetic active polymers for versatile microfluidic\n  devices", "comments": null, "journal-ref": "EPJ Web of Conferences. Vol. 40. EDP Sciences, 2013", "doi": "10.1051/epjconf/20134002001", "report-no": null, "categories": "physics.bio-ph cs.CE physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use a compound of magnetic nanoparticles (20-100 nm) embedded\nin a flexible polymer (Polydimethylsiloxane PDMS) to filter circulating tumor\ncells (CTCs). The analysis of CTCs is an emerging tool for cancer biology\nresearch and clinical cancer management including the detection, diagnosis and\nmonitoring of cancer. The combination of experiments and simulations lead to a\nversatile microfluidic lab-on-chip device. Simulations are essential to\nunderstand the influence of the embedded nanoparticles in the elastic PDMS when\napplying a magnetic gradient field. It combines finite element calculations of\nthe polymer, magnetic simulations of the embedded nanoparticles and the fluid\ndynamic calculations of blood plasma and blood cells. With the use of magnetic\nactive polymers a wide range of tunable microfluidic structures can be created.\nThe method can help to increase the yield of needed isolated CTCs.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 11:55:16 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Gusenbauer", "Markus", ""], ["\u00d6zelt", "Harald", ""], ["Fischbacher", "Johann", ""], ["Reichel", "Franz", ""], ["Exl", "Lukas", ""], ["Bance", "Simon", ""], ["Kataeva", "Nadezhda", ""], ["Binder", "Claudia", ""], ["Br\u00fcckl", "Hubert", ""], ["Schrefl", "Thomas", ""]]}, {"id": "1305.7072", "submitter": "Markus Gusenbauer", "authors": "Markus Gusenbauer, Ha Nguyen, Franz Reichel, Lukas Exl, Simon Bance,\n  Johann Fischbacher, Harald \\\"Ozelt, Alexander Kovacs, Martin Brandl, Thomas\n  Schrefl", "title": "Guided self-assembly of magnetic beads for biomedical applications", "comments": null, "journal-ref": null, "doi": "10.1016/j.physb.2013.08.050", "report-no": null, "categories": "physics.bio-ph cs.CE physics.comp-ph physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Micromagnetic beads are widely used in biomedical applications for cell\nseparation, drug delivery, and hypothermia cancer treatment. Here we propose to\nuse self-organized magnetic bead structures which accumulate on fixed magnetic\nseeding points to isolate circulating tumor cells. The analysis of circulating\ntumor cells is an emerging tool for cancer biology research and clinical cancer\nmanagement including the detection, diagnosis and monitoring of cancer.\nMicrofluidic chips for isolating circulating tumor cells use either affinity,\nsize or density capturing methods. We combine multiphysics simulation\ntechniques to understand the microscopic behavior of magnetic beads interacting\nwith Nickel accumulation points used in lab-on-chip technologies. Our proposed\nchip technology offers the possibility to combine affinity and size capturing\nwith special antibody-coated bead arrangements using a magnetic gradient field\ncreated by Neodymium Iron Boron permanent magnets. The multiscale simulation\nenvironment combines magnetic field computation, fluid dynamics and discrete\nparticle dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 12:10:12 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Gusenbauer", "Markus", ""], ["Nguyen", "Ha", ""], ["Reichel", "Franz", ""], ["Exl", "Lukas", ""], ["Bance", "Simon", ""], ["Fischbacher", "Johann", ""], ["\u00d6zelt", "Harald", ""], ["Kovacs", "Alexander", ""], ["Brandl", "Martin", ""], ["Schrefl", "Thomas", ""]]}, {"id": "1305.7422", "submitter": "Uwe Aickelin", "authors": "Galina Sherman, Peer-Olaf Siebers, David Menachof, Uwe Aickelin", "title": "Evaluating Different Cost-Benefit Analysis Methods for Port Security\n  Operations", "comments": "Decision Making in Service Industries: A Practical Approach, 279-302,\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Service industries, such as ports, are attentive to their standards, a smooth\nservice flow and economic viability. Cost benefit analysis has proven itself as\na useful tool to support this type of decision making; it has been used by\nbusinesses and governmental agencies for many years. In this book chapter we\ndemonstrate different modelling methods that are used for estimating input\nfactors required for conducting cost benefit analysis based on a single case\nstudy. These methods are: scenario analysis, decision trees, Monte-Carlo\nsimulation modelling and discrete event simulation modelling. Our aims are, on\nthe one hand, to guide the analyst through the modelling processes and, on the\nother hand, to demonstrate what additional decision support information can be\nobtained from applying each of these modelling methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 14:36:59 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Sherman", "Galina", ""], ["Siebers", "Peer-Olaf", ""], ["Menachof", "David", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1305.7424", "submitter": "Uwe Aickelin", "authors": "Adrian Adewunmi, Uwe Aickelin", "title": "Investigating the effectiveness of Variance Reduction Techniques in\n  Manufacturing, Call Center and Cross-docking Discrete Event Simulation Models", "comments": "Use Cases of Discrete Event Simulation Appliance and Research.\n  Bangsow, Steffen (Ed.). Springer Berlin Heidelberg, 1-24, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance reduction techniques have been shown by others in the past to be a\nuseful tool to reduce variance in Simulation studies. However, their\napplication and success in the past has been mainly domain specific, with\nrelatively little guidelines as to their general applicability, in particular\nfor novices in this area. To facilitate their use, this study aims to\ninvestigate the robustness of individual techniques across a set of scenarios\nfrom different domains. Experimental results show that Control Variates is the\nonly technique which achieves a reduction in variance across all domains.\nFurthermore, applied individually, Antithetic Variates and Control Variates\nperform particularly well in the Cross-docking scenarios, which was previously\nunknown.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 14:39:34 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Adewunmi", "Adrian", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1305.7437", "submitter": "Uwe Aickelin", "authors": "Tao Zhang, Peer-Olaf Siebers, Uwe Aickelin", "title": "Modelling Electricity Consumption in Office Buildings: An Agent Based\n  Approach", "comments": "Energy and Buildings 43(10), 2882-2892, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an agent-based model which integrates four\nimportant elements, i.e. organisational energy management policies/regulations,\nenergy management technologies, electric appliances and equipment, and human\nbehaviour, to simulate the electricity consumption in office buildings. Based\non a case study, we use this model to test the effectiveness of different\nelectricity management strategies, and solve practical office electricity\nconsumption problems. This paper theoretically contributes to an integration of\nthe four elements involved in the complex organisational issue of office\nelectricity consumption, and practically contributes to an application of an\nagent-based approach for office building electricity consumption study.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 15:01:01 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Zhang", "Tao", ""], ["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1305.7458", "submitter": "Uwe Aickelin", "authors": "Chris Roadknight, Uwe Aickelin, Galina Sherman", "title": "Validation of a Microsimulation of the Port of Dover", "comments": "Journal of Computational Science 3 (1-2), 56-66, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling and simulating the traffic of heavily used but secure environments\nsuch as seaports and airports is of increasing importance. Errors made when\nsimulating these environments can have long standing economic, social and\nenvironmental implications. This paper discusses issues and problems that may\narise when designing a simulation strategy. Data for the Port is presented,\nmethods for lightweight vehicle assessment that can be used to calibrate and\nvalidate simulations are also discussed along with a diagnosis of\novercalibration issues. We show that decisions about where the intelligence\nlies in a system has important repercussions for the reliability of system\nstatistics. Finally, conclusions are drawn about how microsimulations can be\nmoved forward as a robust planning tool for the 21st century.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 15:40:49 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Roadknight", "Chris", ""], ["Aickelin", "Uwe", ""], ["Sherman", "Galina", ""]]}, {"id": "1305.7465", "submitter": "Uwe Aickelin", "authors": "Yihui Liu, Uwe Aickelin, Jan Feyereisl, Lindy G. Durrant", "title": "Wavelet feature extraction and genetic algorithm for biomarker detection\n  in colorectal cancer data", "comments": null, "journal-ref": "Knowledge-Based Systems 37, 502-514, 2013", "doi": null, "report-no": null, "categories": "cs.NE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomarkers which predict patient's survival can play an important role in\nmedical diagnosis and treatment. How to select the significant biomarkers from\nhundreds of protein markers is a key step in survival analysis. In this paper a\nnovel method is proposed to detect the prognostic biomarkers of survival in\ncolorectal cancer patients using wavelet analysis, genetic algorithm, and Bayes\nclassifier. One dimensional discrete wavelet transform (DWT) is normally used\nto reduce the dimensionality of biomedical data. In this study one dimensional\ncontinuous wavelet transform (CWT) was proposed to extract the features of\ncolorectal cancer data. One dimensional CWT has no ability to reduce\ndimensionality of data, but captures the missing features of DWT, and is\ncomplementary part of DWT. Genetic algorithm was performed on extracted wavelet\ncoefficients to select the optimized features, using Bayes classifier to build\nits fitness function. The corresponding protein markers were located based on\nthe position of optimized features. Kaplan-Meier curve and Cox regression model\nwere used to evaluate the performance of selected biomarkers. Experiments were\nconducted on colorectal cancer dataset and several significant biomarkers were\ndetected. A new protein biomarker CD46 was found to significantly associate\nwith survival time.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 15:53:08 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Liu", "Yihui", ""], ["Aickelin", "Uwe", ""], ["Feyereisl", "Jan", ""], ["Durrant", "Lindy G.", ""]]}, {"id": "1305.7471", "submitter": "Uwe Aickelin", "authors": "Grazziela P. Figueredo, Peer-Olaf Siebers, Uwe Aickelin", "title": "Investigating Mathematical Models of Immuno-Interactions with\n  Early-Stage Cancer under an Agent-Based Modelling Perspective", "comments": null, "journal-ref": "BMC Bioinformatics 14(Suppl 6), S6, 2013", "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many advances in research regarding immuno-interactions with cancer were\ndeveloped with the help of ordinary differential equation (ODE) models. These\nmodels, however, are not effectively capable of representing problems involving\nindividual localisation, memory and emerging properties, which are common\ncharacteristics of cells and molecules of the immune system. Agent-based\nmodelling and simulation is an alternative paradigm to ODE models that\novercomes these limitations. In this paper we investigate the potential\ncontribution of agent-based modelling and simulation when compared to ODE\nmodelling and simulation. We seek answers to the following questions: Is it\npossible to obtain an equivalent agent-based model from the ODE formulation? Do\nthe outcomes differ? Are there any benefits of using one method compared to the\nother? To answer these questions, we have considered three case studies using\nestablished mathematical models of immune interactions with early-stage cancer.\nThese case studies were re-conceptualised under an agent-based perspective and\nthe simulation results were then compared with those from the ODE models. Our\nresults show that it is possible to obtain equivalent agent-based models (i.e.\nimplementing the same mechanisms); the simulation output of both types of\nmodels however might differ depending on the attributes of the system to be\nmodelled. In some cases, additional insight from using agent-based modelling\nwas obtained. Overall, we can confirm that agent-based modelling is a useful\naddition to the tool set of immunologists, as it has extra features that allow\nfor simulations with characteristics that are closer to the biological\nphenomena.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 16:09:00 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Figueredo", "Grazziela P.", ""], ["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""]]}]