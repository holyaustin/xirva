[{"id": "1504.00062", "submitter": "Tom Crick", "authors": "Neil P. Chue Hong, Tom Crick, Ian P. Gent, Lars Kotthoff and Kenji\n  Takeda", "title": "Top Tips to Make Your Research Irreproducible", "comments": "2 pages, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is an unfortunate convention of science that research should pretend to be\nreproducible; our top tips will help you mitigate this fussy conventionality,\nenabling you to enthusiastically showcase your irreproducible work.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 23:02:11 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 22:38:46 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Hong", "Neil P. Chue", ""], ["Crick", "Tom", ""], ["Gent", "Ian P.", ""], ["Kotthoff", "Lars", ""], ["Takeda", "Kenji", ""]]}, {"id": "1504.00304", "submitter": "Christopher Whidden", "authors": "Chris Whidden and Frederick A. Matsen IV", "title": "Ricci-Ollivier Curvature of the Rooted Phylogenetic\n  Subtree-Prune-Regraft Graph", "comments": "17 2-column pages, 6 figures, 2 tables. To appear in the Proceedings\n  of the Thirteenth Workshop on Analytic Algorithmics and Combinatorics\n  (ANALCO)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.CE q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical phylogenetic inference methods use tree rearrangement operations\nto perform either hill-climbing local search or Markov chain Monte Carlo across\ntree topologies. The canonical class of such moves are the\nsubtree-prune-regraft (SPR) moves that remove a subtree and reattach it\nsomewhere else via the cut edge of the subtree. Phylogenetic trees and such\nmoves naturally form the vertices and edges of a graph, such that tree search\nalgorithms perform a (potentially stochastic) traversal of this SPR graph.\nDespite the centrality of such graphs in phylogenetic inference, rather little\nis known about their large-scale properties. In this paper we learn about the\nrooted-tree version of the graph, known as the rSPR graph, by calculating the\nRicci-Ollivier curvature for pairs of vertices in the rSPR graph with respect\nto two simple random walks on the rSPR graph. By proving theorems and direct\ncalculation with novel algorithms, we find a remarkable diversity of different\ncurvatures on the rSPR graph for pairs of vertices separated by the same\ndistance. We confirm using simulation that degree and curvature have the\nexpected impact on mean access time distributions, demonstrating relevance of\nthese curvature results to stochastic tree search. This indicates significant\nstructure of the rSPR graph beyond that which was previously understood in\nterms of pairwise distances and vertex degrees; a greater understanding of\ncurvature could ultimately lead to improved strategies for tree search.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 17:24:29 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 19:09:09 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2015 23:52:48 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Whidden", "Chris", ""], ["Matsen", "Frederick A.", "IV"]]}, {"id": "1504.01142", "submitter": "Nam-phuong Nguyen", "authors": "Nam-phuong Nguyen, Siavash Mirarab, Keerthana Kumar, Tandy Warnow", "title": "Ultra-large alignments using Phylogeny-aware Profiles", "comments": "Online supplemental materials and data are available at\n  http://www.cs.utexas.edu/users/phylo/software/upp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological questions, including the estimation of deep evolutionary\nhistories and the detection of remote homology between protein sequences, rely\nupon multiple sequence alignments (MSAs) and phylogenetic trees of large\ndatasets. However, accurate large-scale multiple sequence alignment is very\ndifficult, especially when the dataset contains fragmentary sequences. We\npresent UPP, an MSA method that uses a new machine learning technique - the\nEnsemble of Hidden Markov Models - that we propose here. UPP produces highly\naccurate alignments for both nucleotide and amino acid sequences, even on\nultra-large datasets or datasets containing fragmentary sequences. UPP is\navailable at https://github.com/smirarab/sepp.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 17:15:38 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Nguyen", "Nam-phuong", ""], ["Mirarab", "Siavash", ""], ["Kumar", "Keerthana", ""], ["Warnow", "Tandy", ""]]}, {"id": "1504.01304", "submitter": "Yuzhen Ye", "authors": "Yuzhen Ye and Haixu Tang", "title": "Utilizing de Bruijn graph of metagenome assembly for metatranscriptome\n  analysis", "comments": "8 pages, 4 figures, accepted in RECOMB-Seq 2015, under consideration\n  in Bioinformatics (a special issue for RECOMB-Seq/CBB)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metagenomics research has accelerated the studies of microbial organisms,\nproviding insights into the composition and potential functionality of various\nmicrobial communities. Metatranscriptomics (studies of the transcripts from a\nmixture of microbial species) and other meta-omics approaches hold even greater\npromise for providing additional insights into functional and regulatory\ncharacteristics of the microbial communities. Current metatranscriptomics\nprojects are often carried out without matched metagenomic datasets (of the\nsame microbial communities). For the projects that produce both\nmetatranscriptomic and metagenomic datasets, their analyses are often not\nintegrated. Metagenome assemblies are far from perfect, partially explaining\nwhy metagenome assemblies are not used for the analysis of metatranscriptomic\ndatasets. Here we report a reads mapping algorithm for mapping of short reads\nonto a de Bruijn graph of assemblies. A hash table of junction k-mers (k-mers\nspanning branching structures in the de Bruijn graph) is used to facilitate\nfast mapping of reads to the graph. We developed an application of this mapping\nalgorithm: a reference based approach to metatranscriptome assembly using\ngraphs of metagenome assembly as the reference. Our results show that this new\napproach (called TAG) helps to assemble substantially more transcripts that\notherwise would have been missed or truncated because of the fragmented nature\nof the reference metagenome. TAG was implemented in C++ and has been tested\nextensively on the linux platform. It is available for download as open source\nat http://omics.informatics.indiana.edu/TAG.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 15:17:29 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Ye", "Yuzhen", ""], ["Tang", "Haixu", ""]]}, {"id": "1504.01310", "submitter": "Tom Crick", "authors": "Tom Crick, Benjamin A. Hall and Samin Ishtiaq", "title": "Reproducibility as a Technical Specification", "comments": "Submitted to the 18th IEEE International Conference on Computational\n  Science and Engineering (CSE 2015); 6 pages, LaTeX. arXiv admin note:\n  substantial text overlap with arXiv:1502.02448", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility of computationally-derived scientific discoveries should be a\ncertainty. As the product of several person-years' worth of effort, results --\nwhether disseminated through academic journals, conferences or exploited\nthrough commercial ventures -- should at some level be expected to be\nrepeatable by other researchers. While this stance may appear to be obvious and\ntrivial, a variety of factors often stand in the way of making it commonplace.\nWhilst there has been detailed cross-disciplinary discussions of the various\nsocial, cultural and ideological drivers and (potential) solutions, one factor\nwhich has had less focus is the concept of reproducibility as a technical\nchallenge. Specifically, that the definition of an unambiguous and measurable\nstandard of reproducibility would offer a significant benefit to the wider\ncomputational science community.\n  In this paper, we propose a high-level technical specification for a service\nfor reproducibility, presenting cyberinfrastructure and associated workflow for\na service which would enable such a specification to be verified and validated.\nIn addition to addressing a pressing need for the scientific community, we\nfurther speculate on the potential contribution to the wider software\ndevelopment community of services which automate de novo compilation and\ntesting of code from source. We illustrate our proposed specification and\nworkflow by using the BioModelAnalyzer tool as a running example.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 15:59:49 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 00:20:57 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2015 01:57:43 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Crick", "Tom", ""], ["Hall", "Benjamin A.", ""], ["Ishtiaq", "Samin", ""]]}, {"id": "1504.01329", "submitter": "Ray Grout", "authors": "R.W. Grout and H. Kolla and M.L. Minion and J.B. Bell", "title": "Achieving algorithmic resilience for temporal integration through\n  spectral deferred corrections", "comments": null, "journal-ref": "Commun. Appl. Math. Comput. Sci. 12 (2017) 25-50", "doi": "10.2140/camcos.2017.12.25", "report-no": null, "categories": "cs.CE cs.MS", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Spectral deferred corrections (SDC) is an iterative approach for constructing\nhigher- order accurate numerical approximations of ordinary differential\nequations. SDC starts with an initial approximation of the solution defined at\na set of Gaussian or spectral collocation nodes over a time interval and uses\nan iterative application of lower-order time discretizations applied to a\ncorrection equation to improve the solution at these nodes. Each deferred\ncorrection sweep increases the formal order of accuracy of the method up to the\nlimit inherent in the accuracy defined by the collocation points. In this\npaper, we demonstrate that SDC is well suited to recovering from soft\n(transient) hardware faults in the data. A strategy where extra correction\niterations are used to recover from soft errors and provide algorithmic\nresilience is proposed. Specifically, in this approach the iteration is\ncontinued until the residual (a measure of the error in the approximation) is\nsmall relative to the residual on the first correction iteration and changes\nslowly between successive iterations. We demonstrate the effectiveness of this\nstrategy for both canonical test problems and a comprehen- sive situation\ninvolving a mature scientific application code that solves the reacting\nNavier-Stokes equations for combustion research.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 17:26:32 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Grout", "R. W.", ""], ["Kolla", "H.", ""], ["Minion", "M. L.", ""], ["Bell", "J. B.", ""]]}, {"id": "1504.01380", "submitter": "Maitham Alhubail", "authors": "Maitham Makki Alhubail and Qiqi Wang", "title": "The swept rule for breaking the latency barrier in time advancing PDEs", "comments": "30 pages", "journal-ref": "Journal of Computational Physics (2016), pp. 110-121", "doi": "10.1016/j.jcp.2015.11.026", "report-no": null, "categories": "cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates the swept rule of space-time domain decomposition,\nan idea to break the latency barrier via communicating less often when\nexplicitly solving time-dependent PDEs. The swept rule decomposes space and\ntime among computing nodes in ways that exploit the domains of influence and\nthe domain of dependency, making it possible to communicate once per many\ntimesteps without redundant computation. The article presents simple\ntheoretical analysis to the performance of the swept rule which then was shown\nto be accurate by conducting numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 16:00:32 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2015 16:26:05 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Alhubail", "Maitham Makki", ""], ["Wang", "Qiqi", ""]]}, {"id": "1504.01786", "submitter": "Mihai Cucuringu", "authors": "Mihai Cucuringu and Radek Erban", "title": "ADM-CLE approach for detecting slow variables in continuous time Markov\n  chains and dynamic data", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA physics.chem-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for detecting intrinsic slow variables in high-dimensional\nstochastic chemical reaction networks is developed and analyzed. It combines\nanisotropic diffusion maps (ADM) with approximations based on the chemical\nLangevin equation (CLE). The resulting approach, called ADM-CLE, has the\npotential of being more efficient than the ADM method for a large class of\nchemical reaction systems, because it replaces the computationally most\nexpensive step of ADM (running local short bursts of simulations) by using an\napproximation based on the CLE. The ADM-CLE approach can be used to estimate\nthe stationary distribution of the detected slow variable, without any a-priori\nknowledge of it. If the conditional distribution of the fast variables can be\nobtained analytically, then the resulting ADM-CLE approach does not make any\nuse of Monte Carlo simulations to estimate the distributions of both slow and\nfast variables.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 00:11:17 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Cucuringu", "Mihai", ""], ["Erban", "Radek", ""]]}, {"id": "1504.02290", "submitter": "Jakub Narojczyk Ph.D.", "authors": "Jakub Narojczyk, Krzysztof W. Wojciechowski", "title": "Computer simulation of Poisson's ratio of soft polydisperse discs at\n  zero temperature", "comments": "8th International Conference on Intermolecular and Magnetic\n  Interactions in Matter, Naleczow, POLAND, SEP 08-10, 2005", "journal-ref": "Materials Science-Poland Volume: 24 Issue: 4 pp.921-927 Published:\n  2006", "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.soft cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple algorithm is proposed for studies of structural and elastic\nproperties in the presence of structural disorder at zero temperature. The\nalgorithm is used to determine the properties of the polydisperse soft disc\nsystem. It is shown that the Poisson's ratio of the system essentially depends\non the size polydispersity parameter - larger polydispersity implies larger\nPoisson's ratio. In the presence of any size polidispersity the Poisson's ratio\nincreases also when the interactions between the particles tend to the hard\npotential.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 13:06:00 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Narojczyk", "Jakub", ""], ["Wojciechowski", "Krzysztof W.", ""]]}, {"id": "1504.02367", "submitter": "Changchuan Yin Dr.", "authors": "Changchuan Yin, Jiasong Wang", "title": "Periodic power spectrum with applications in detection of latent\n  periodicities in DNA sequences", "comments": null, "journal-ref": null, "doi": "10.1007/s00285-016-0982-8", "report-no": null, "categories": "cs.DM cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent periodic elements in genomes play important roles in genomic\nfunctions. Many complex periodic elements in genomes are difficult to be\ndetected by commonly used digital signal processing (DSP). We present a novel\nmethod to compute the periodic power spectrum of a DNA sequence based on the\nnucleotide distributions on periodic positions of the sequence. The method\ndirectly calculates full periodic spectrum of a DNA sequence rather than\nfrequency spectrum by Fourier transform. The magnitude of the periodic power\nspectrum reflects the strength of the periodicity signals, thus, the algorithm\ncan capture all the latent periodicities in DNA sequences. We apply this method\non detection of latent periodicities in different genome elements, including\nexons and microsatellite DNA sequences. The results show that the method\nminimizes the impact of spectral leakage, captures a much broader latent\nperiodicities in genomes, and outperforms the conventional Fourier transform.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 16:32:11 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 17:07:51 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Yin", "Changchuan", ""], ["Wang", "Jiasong", ""]]}, {"id": "1504.03021", "submitter": "Jakub Narojczyk Ph.D.", "authors": "J. W. Narojczyk and K. W. Wojciechowski", "title": "Elastic properties of the degenerate f.c.c. crystal of polydisperse soft\n  dimers at zero temperature", "comments": "Conference: International Workshop on Functional and Nanostructured\n  Materials (FNMA)/International Conference on Intermolecular and Magnetic\n  Interactions in Matter (IMIM) Location: L'Aquila, ITALY Date: SEP 27-30,2009", "journal-ref": "Journal of Non Crystalline Solids. 356 pp. 2026-2032 (2010)", "doi": "10.1016/j.jnoncrysol.2010.05.080", "report-no": null, "categories": "physics.comp-ph cond-mat.soft cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elastic properties of soft, three-dimensional dimers, interacting through\nsite-site n-inverse-power potential, are determined by computer simulations at\nzero temperature. The degenerate crystal of dimers exhibiting (Gaussian) size\ndistribution of atomic diameters - i.e. size polydispersity - is studied at the\nmolecular number density $1/\\sqrt{2}$; the distance between centers of atoms\nforming dimers is considered as a length unit. It is shown that, at the fixed\nnumber density of the dimers, increasing polydispersity causes, typically, an\nincrease of pressure, elastic constants and Poisson's ratio; the latter is\npositive in most direction. A direction is found, however, in which the size\npolydispersity causes substantial decrease of Poisson's ratio, down to negative\nvalues for large $n$. Thus, the system is partially auxetic for large\npolydispersity and large n.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 21:04:47 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Narojczyk", "J. W.", ""], ["Wojciechowski", "K. W.", ""]]}, {"id": "1504.03080", "submitter": "Andrew McPherson", "authors": "Andrew McPherson, Andrew Roth, Gavin Ha, Sohrab P. Shah, Cedric\n  Chauve, S. Cenk Sahinalp", "title": "Joint Inference of Genome Structure and Content in Heterogeneous Tumour\n  Samples", "comments": "Presented at RECOMB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a genomically unstable cancer, a single tumour biopsy will often contain\na mixture of competing tumour clones. These tumour clones frequently differ\nwith respect to their genomic content (copy number of each gene) and structure\n(order of genes on each chromosome). Modern bulk genome sequencing mixes the\nsignals of tumour clones and contaminating normal cells, complicating inference\nof genomic content and structure. We propose a method to unmix tumour and\ncontaminating normal signals and jointly predict genomic structure and content\nof each tumour clone. We use genome graphs to represent tumour clones, and\nmodel the likelihood of the observed reads given clones and mixing proportions.\nOur use of haplotype blocks allows us to accurately measure allele specific\nread counts, and infer allele specific copy number for each clone. The proposed\nmethod is a heuristic local search based on applying incremental, locally\noptimal modifications of the genome graphs. Using simulated data, we show that\nour method predicts copy counts and gene adjacencies with reasonable accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 07:17:36 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2015 22:51:37 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["McPherson", "Andrew", ""], ["Roth", "Andrew", ""], ["Ha", "Gavin", ""], ["Shah", "Sohrab P.", ""], ["Chauve", "Cedric", ""], ["Sahinalp", "S. Cenk", ""]]}, {"id": "1504.03490", "submitter": "Evangelos Karvelas", "authors": "N.K. Lampropoulos, E.G. Karvelas, I.E. Sarris", "title": "Computational Modeling of an MRI Guided Drug Delivery System Based on\n  Magnetic Nanoparticle Aggregations for the Navigation of Paramagnetic\n  Nanocapsules", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.soft cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computational method for magnetically guided drug delivery is presented and\nthe results are compared for the aggregation process of magnetic particles\nwithin a fluid environment. The model is developed for the simulation of the\naggregation patterns of magnetic nanoparticles under the influence of MRI\nmagnetic coils. A novel approach for the calculation of the drag coefficient of\naggregates is presented. The comparison against experimental and numerical\nresults from the literature is showed that the proposed method predicts well\nthe aggregations in respect to their size and pattern dependance, on the\nconcentration and the strength of the magnetic field, as well as their velocity\nwhen particles are driven through the fluid by magnetic gradients.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 10:44:18 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Lampropoulos", "N. K.", ""], ["Karvelas", "E. G.", ""], ["Sarris", "I. E.", ""]]}, {"id": "1504.03834", "submitter": "Pornchai Phukpattaranont Dr", "authors": "Pornchai Phukpattaranont", "title": "Comparisons of wavelet functions in QRS signal to noise ratio\n  enhancement and detection accuracy", "comments": "16 pages, 8 figures, Article submitted to Journal of the Korean\n  Physical Society for considering of publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the capability of wavelet functions used for noise removal in\npreprocessing step of a QRS detection algorithm in the electrocardiogram (ECG)\nsignal. The QRS signal to noise ratio enhancement and the detection accuracy of\neach wavelet function are evaluated using three measures: (1) the ratio of the\nmaximum beat amplitude to the minimum beat amplitude (RMM), (2) the mean of\nabsolute of time error (MATE), and (3) the figure of merit (FOM). Three wavelet\nfunctions from previous well-known publications are explored, i.e., Bior1.3,\nDb10, and Mexican hat wavelet functions. Results evaluated with the ECG signal\nfrom MIT-BIH arrhythmia database show that the Mexican hat wavelet function is\nbetter than the others. While the scale 8 of Mexican hat wavelet function can\nprovide the best enhancement in QRS signal to noise ratio, the scale 4 of\nMexican hat wavelet function can provide the best detection accuracy. These\nresults may be combined and may enable the use of a single fixed threshold for\nall ECG records leading to the reduction in computational complexity of the QRS\ndetection algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2015 09:22:31 GMT"}, {"version": "v2", "created": "Thu, 16 Apr 2015 04:35:37 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Phukpattaranont", "Pornchai", ""]]}, {"id": "1504.06110", "submitter": "Bassam AlKindy Mr.", "authors": "Bassam AlKindy, Huda Al-Nayyef, Christophe Guyeux, Jean-Fran\\c{c}ois\n  Couchot, Michel Salomon, Jacques M. Bahi", "title": "Improved Core Genes Prediction for Constructing well-supported\n  Phylogenetic Trees in large sets of Plant Species", "comments": "12 pages, 7 figures, IWBBIO 2015 (3rd International Work-Conference\n  on Bioinformatics and Biomedical Engineering)", "journal-ref": "Springer LNBI 9043, 2015, 379--390", "doi": "10.1007/978-3-319-16483-0_38", "report-no": null, "categories": "q-bio.GN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way to infer well-supported phylogenetic trees that precisely reflect the\nevolutionary process is a challenging task that completely depends on the way\nthe related core genes have been found. In previous computational biology\nstudies, many similarity based algorithms, mainly dependent on calculating\nsequence alignment matrices, have been proposed to find them. In these kinds of\napproaches, a significantly high similarity score between two coding sequences\nextracted from a given annotation tool means that one has the same genes. In a\nprevious work article, we presented a quality test approach (QTA) that improves\nthe core genes quality by combining two annotation tools (namely NCBI, a\npartially human-curated database, and DOGMA, an efficient annotation algorithm\nfor chloroplasts). This method takes the advantages from both sequence\nsimilarity and gene features to guarantee that the core genome contains correct\nand well-clustered coding sequences (\\emph{i.e.}, genes). We then show in this\narticle how useful are such well-defined core genes for biomolecular\nphylogenetic reconstructions, by investigating various subsets of core genes at\nvarious family or genus levels, leading to subtrees with strong bootstraps that\nare finally merged in a well-supported supertree.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 09:45:07 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["AlKindy", "Bassam", ""], ["Al-Nayyef", "Huda", ""], ["Guyeux", "Christophe", ""], ["Couchot", "Jean-Fran\u00e7ois", ""], ["Salomon", "Michel", ""], ["Bahi", "Jacques M.", ""]]}, {"id": "1504.06664", "submitter": "Q He", "authors": "Q. He", "title": "Fast and Rigorous DC Solution in Finite Element Method for Integrated\n  Circuit Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale circuit simulation, such as power delivery network analysis, has\nbecome increasingly challenge in the VLSI design verification flow. Power\ndelivery network can be simulated by both SPICE-type circuit-based model and\neletromagnetics-based model when full-wave accuracy is desired. In the early\ntime of the time domain finite element simulation for integrated circuit, the\nmodes having the highest eigenvalues supported by the numerical system will be\nexcited. Because of the band limited source, after the early time, the modes\nhaving a resonance frequency well beyond the input frequency band will die\ndown, and all physically important high-order modes and DC mode will show up\nand become dominant. Among these modes, the DC mode is the last one to show up.\nAlthough the convergence criterion is not applied on the DC mode, the existence\nof DC mode in the field solution will deteriorate the convergence rate of the\nfirst several high order modes. Therefore, this paper first analyzed the\nmathematic characteristics of the DC mode and proposed a rigorous and fast\nsolution to extract the DC mode from the numerical system in order to speed up\nthe convergence rate. Experimental results demonstrated the robustness and\nsuperior performance of this method.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2015 23:23:33 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["He", "Q.", ""]]}, {"id": "1504.07094", "submitter": "Yu-Hang Tang", "authors": "Zhen Li, Yu-Hang Tang, Xuejin Li and George Em Karniadakis", "title": "Mesoscale modeling of phase transition dynamics of thermoresponsive\n  polymers", "comments": "Manuscript submitted to Chemical Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph cond-mat.mtrl-sci cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a non-isothermal mesoscopic model for investigation of the phase\ntransition dynamics of thermoresponsive polymers. Since this model conserves\nenergy in the simulations, it is able to correctly capture not only the\ntransient behavior of polymer precipitation from solvent, but also the energy\nvariation associated with the phase transition process. Simulations provide\ndynamic details of the thermally induced phase transition and confirm two\ndifferent mechanisms dominating the phase transition dynamics. A shift of\nendothermic peak with concentration is observed and the underlying mechanism is\nexplored.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 14:00:40 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Li", "Zhen", ""], ["Tang", "Yu-Hang", ""], ["Li", "Xuejin", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1504.07499", "submitter": "Sophie Lemaitre", "authors": "Sophie Lemaitre and Vladimir Salnikov and Daniel Choi and Philippe\n  Karamian", "title": "Computation of thermal properties via 3D homogenization of multiphase\n  materials using FFT-based accelerated scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the thermal effective behaviour for 3D multiphase\ncomposite material consisting of three isotropic phases which are the matrix,\nthe inclusions and the coating media. For this purpose we use an accelerated\nFFT-based scheme initially proposed in Eyre and Milton (1999) to evaluate the\nthermal conductivity tensor. Matrix and spherical inclusions media are polymers\nwith similar properties whereas the coating medium is metallic hence better\nconducting. Thus, the contrast between the coating and the others media is very\nlarge. For our study, we use RVEs (Representative volume elements) generated by\nRSA (Random Sequential Adsorption) method developed in our previous works,\nthen, we compute effective thermal properties using an FFT-based homogenization\ntechnique validated by comparison with the direct finite elements method. We\nstudy the thermal behaviour of the 3D-multiphase composite material and we show\nwhat features should be taken into account to make the computational approach\nefficient.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 14:32:52 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Lemaitre", "Sophie", ""], ["Salnikov", "Vladimir", ""], ["Choi", "Daniel", ""], ["Karamian", "Philippe", ""]]}, {"id": "1504.07665", "submitter": "Jakub Narojczyk Ph.D.", "authors": "Jakub W. Narojczyk, P. M. Piglowski, K. W. Wojciechowski and K. V.\n  Tretiakov", "title": "Elastic properties of mono- and polydisperse two-dimensional crystals of\n  hard--core repulsive Yukawa particles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.soft cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo simulations of mono-- and polydisperse two--dimensional crystals\nare reported. The particles in the studied system, interacting through\nhard--core repulsive Yukawa potential, form a solid phase of hexagonal lattice.\nThe elastic properties of crystalline Yukawa systems are determined in the\n$NpT$ ensemble with variable shape of the periodic box. Effects of the Debye\nscreening length ($\\kappa^{-1}$), contact value of the potential ($\\epsilon$),\nand the size polydispersity of particles on elastic properties of the system\nare studied. The simulations show that the polydispersity of particles strongly\ninfluences the elastic properties of the studied system, especially on the\nshear modulus. It is also found that the elastic moduli increase with density\nand their growth rate depends on the screening length. Shorter screening length\nleads to faster increase of elastic moduli with density and decrease of the\nPoisson's ratio. In contrast to its three-dimensional version, the studied\nsystem is non-auxetic, i.e. shows positive Poisson's ratio.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2015 21:41:44 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Narojczyk", "Jakub W.", ""], ["Piglowski", "P. M.", ""], ["Wojciechowski", "K. W.", ""], ["Tretiakov", "K. V.", ""]]}, {"id": "1504.07795", "submitter": "Derek Groen", "authors": "Mohamed A. Itani, Ulf D. Schiller, Sebastian Schmieschek, James\n  Hetherington, Miguel O. Bernabeu, Hoskote Chandrashekar, Fergus Robertson,\n  Peter V. Coveney and Derek Groen", "title": "An automated multiscale ensemble simulation approach for vascular blood\n  flow", "comments": "Journal of Computational Science (in press), 10 pages, 6 figures, 2\n  tables", "journal-ref": null, "doi": "10.1016/j.jocs.2015.04.008", "report-no": null, "categories": "cs.DC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cerebrovascular diseases such as brain aneurysms are a primary cause of adult\ndisability. The flow dynamics in brain arteries, both during periods of rest\nand increased activity, are known to be a major factor in the risk of aneurysm\nformation and rupture. The precise relation is however still an open field of\ninvestigation. We present an automated ensemble simulation method for modelling\ncerebrovascular blood flow under a range of flow regimes. By automatically\nconstructing and performing an ensemble of multiscale simulations, where we\nunidirectionally couple a 1D solver with a 3D lattice-Boltzmann code, we are\nable to model the blood flow in a patient artery over a range of flow regimes.\nWe apply the method to a model of a middle cerebral artery, and find that this\napproach helps us to fine-tune our modelling techniques, and opens up new ways\nto investigate cerebrovascular flow properties.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 10:22:56 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Itani", "Mohamed A.", ""], ["Schiller", "Ulf D.", ""], ["Schmieschek", "Sebastian", ""], ["Hetherington", "James", ""], ["Bernabeu", "Miguel O.", ""], ["Chandrashekar", "Hoskote", ""], ["Robertson", "Fergus", ""], ["Coveney", "Peter V.", ""], ["Groen", "Derek", ""]]}, {"id": "1504.07865", "submitter": "Snehanshu Saha", "authors": "Snehanshu Saha, Surbhi Agrawal, Manikandan. R, Kakoli Bora, Swati\n  Routh, Anand Narasimhamurthy", "title": "ASTROMLSKIT: A New Statistical Machine Learning Toolkit: A Platform for\n  Data Analytics in Astronomy", "comments": "Habitability Catalog (HabCat), Supernova classification, data\n  analysis, Astroinformatics, Machine learning, ASTROMLS toolkit, Na\\\"ive\n  Bayes, SVD, PCA, Random Forest, SVM, Decision Tree, LDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE astro-ph.IM cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astroinformatics is a new impact area in the world of astronomy, occasionally\ncalled the final frontier, where several astrophysicists, statisticians and\ncomputer scientists work together to tackle various data intensive astronomical\nproblems. Exponential growth in the data volume and increased complexity of the\ndata augments difficult questions to the existing challenges. Classical\nproblems in Astronomy are compounded by accumulation of astronomical volume of\ncomplex data, rendering the task of classification and interpretation\nincredibly laborious. The presence of noise in the data makes analysis and\ninterpretation even more arduous. Machine learning algorithms and data analytic\ntechniques provide the right platform for the challenges posed by these\nproblems. A diverse range of open problem like star-galaxy separation,\ndetection and classification of exoplanets, classification of supernovae is\ndiscussed. The focus of the paper is the applicability and efficacy of various\nmachine learning algorithms like K Nearest Neighbor (KNN), random forest (RF),\ndecision tree (DT), Support Vector Machine (SVM), Na\\\"ive Bayes and Linear\nDiscriminant Analysis (LDA) in analysis and inference of the decision theoretic\nproblems in Astronomy. The machine learning algorithms, integrated into\nASTROMLSKIT, a toolkit developed in the course of the work, have been used to\nanalyze HabCat data and supernovae data. Accuracy has been found to be\nappreciably good.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 14:06:18 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Saha", "Snehanshu", ""], ["Agrawal", "Surbhi", ""], ["R", "Manikandan.", ""], ["Bora", "Kakoli", ""], ["Routh", "Swati", ""], ["Narasimhamurthy", "Anand", ""]]}, {"id": "1504.07890", "submitter": "Diego Fabregat-Traver", "authors": "Alvaro Frank, Diego Fabregat-Traver and Paolo Bientinesi", "title": "Large-scale linear regression: Development of high-performance routines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistics, series of ordinary least squares problems (OLS) are used to\nstudy the linear correlation among sets of variables of interest; in many\nstudies, the number of such variables is at least in the millions, and the\ncorresponding datasets occupy terabytes of disk space. As the availability of\nlarge-scale datasets increases regularly, so does the challenge in dealing with\nthem. Indeed, traditional solvers---which rely on the use of black-box\"\nroutines optimized for one single OLS---are highly inefficient and fail to\nprovide a viable solution for big-data analyses. As a case study, in this paper\nwe consider a linear regression consisting of two-dimensional grids of related\nOLS problems that arise in the context of genome-wide association analyses, and\ngive a careful walkthrough for the development of {\\sc ols-grid}, a\nhigh-performance routine for shared-memory architectures; analogous steps are\nrelevant for tailoring OLS solvers to other applications. In particular, we\nfirst illustrate the design of efficient algorithms that exploit the structure\nof the OLS problems and eliminate redundant computations; then, we show how to\neffectively deal with datasets that do not fit in main memory; finally, we\ndiscuss how to cast the computation in terms of efficient kernels and how to\nachieve scalability. Importantly, each design decision along the way is\njustified by simple performance models. {\\sc ols-grid} enables the solution of\n$10^{11}$ correlated OLS problems operating on terabytes of data in a matter of\nhours.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 15:24:33 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Frank", "Alvaro", ""], ["Fabregat-Traver", "Diego", ""], ["Bientinesi", "Paolo", ""]]}, {"id": "1504.08027", "submitter": "Prashanti Manda", "authors": "Prashanti Manda, Fiona McCarthy, Bindu Nanduri, Hui Wang, Susan M.\n  Bridges", "title": "Information-theoretic Interestingness Measures for Cross-Ontology Data\n  Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community annotation of biological entities with concepts from multiple\nbio-ontologies has created large and growing repositories of ontology-based\nannotation data with embedded implicit relationships among orthogonal\nontologies. Development of efficient data mining methods and metrics to mine\nand assess the quality of the mined relationships has not kept pace with the\ngrowth of annotation data. In this study, we present a data mining method that\nuses ontology-guided generalization to discover relationships across ontologies\nalong with a new interestingness metric based on information theory. We apply\nour data mining algorithm and interestingness measures to datasets from the\nGene Expression Database at the Mouse Genome Informatics as a preliminary proof\nof concept to mine relationships between developmental stages in the mouse\nanatomy ontology and Gene Ontology concepts (biological process, molecular\nfunction and cellular component). In addition, we present a comparison of our\ninterestingness metric to four existing metrics. Ontology-based annotation\ndatasets provide a valuable resource for discovery of relationships across\nontologies. The use of efficient data mining methods and appropriate\ninterestingness metrics enables the identification of high quality\nrelationships.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 21:15:46 GMT"}, {"version": "v2", "created": "Mon, 16 May 2016 08:58:17 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Manda", "Prashanti", ""], ["McCarthy", "Fiona", ""], ["Nanduri", "Bindu", ""], ["Wang", "Hui", ""], ["Bridges", "Susan M.", ""]]}]