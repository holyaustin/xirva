[{"id": "1605.00003", "submitter": "Snehanshu Saha", "authors": "Luckyson Khaidem, Snehanshu Saha and Sudeepa Roy Dey", "title": "Predicting the direction of stock market prices using random forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting trends in stock market prices has been an area of interest for\nresearchers for many years due to its complex and dynamic nature. Intrinsic\nvolatility in stock market across the globe makes the task of prediction\nchallenging. Forecasting and diffusion modeling, although effective can't be\nthe panacea to the diverse range of problems encountered in prediction,\nshort-term or otherwise. Market risk, strongly correlated with forecasting\nerrors, needs to be minimized to ensure minimal risk in investment. The authors\npropose to minimize forecasting error by treating the forecasting problem as a\nclassification problem, a popular suite of algorithms in Machine learning. In\nthis paper, we propose a novel way to minimize the risk of investment in stock\nmarket by predicting the returns of a stock using a class of powerful machine\nlearning algorithms known as ensemble learning. Some of the technical\nindicators such as Relative Strength Index (RSI), stochastic oscillator etc are\nused as inputs to train our model. The learning model used is an ensemble of\nmultiple decision trees. The algorithm is shown to outperform existing algo-\nrithms found in the literature. Out of Bag (OOB) error estimates have been\nfound to be encouraging. Key Words: Random Forest Classifier, stock price\nforecasting, Exponential smoothing, feature extraction, OOB error and\nconvergence.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2016 17:53:08 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Khaidem", "Luckyson", ""], ["Saha", "Snehanshu", ""], ["Dey", "Sudeepa Roy", ""]]}, {"id": "1605.00303", "submitter": "Tommaso Mansi", "authors": "Dominik Neumann, Tommaso Mansi, Lucian Itu, Bogdan Georgescu, Elham\n  Kayvanpour, Farbod Sedaghat-Hamedani, Ali Amr, Jan Haas, Hugo Katus, Benjamin\n  Meder, Stefan Steidl, Joachim Hornegger, Dorin Comaniciu", "title": "A Self-Taught Artificial Agent for Multi-Physics Computational Model\n  Personalization", "comments": "Submitted to Medical Image Analysis, Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization is the process of fitting a model to patient data, a critical\nstep towards application of multi-physics computational models in clinical\npractice. Designing robust personalization algorithms is often a tedious,\ntime-consuming, model- and data-specific process. We propose to use artificial\nintelligence concepts to learn this task, inspired by how human experts\nmanually perform it. The problem is reformulated in terms of reinforcement\nlearning. In an off-line phase, Vito, our self-taught artificial agent, learns\na representative decision process model through exploration of the\ncomputational model: it learns how the model behaves under change of\nparameters. The agent then automatically learns an optimal strategy for on-line\npersonalization. The algorithm is model-independent; applying it to a new model\nrequires only adjusting few hyper-parameters of the agent and defining the\nobservations to match. The full knowledge of the model itself is not required.\nVito was tested in a synthetic scenario, showing that it could learn how to\noptimize cost functions generically. Then Vito was applied to the inverse\nproblem of cardiac electrophysiology and the personalization of a whole-body\ncirculation model. The obtained results suggested that Vito could achieve\nequivalent, if not better goodness of fit than standard methods, while being\nmore robust (up to 11% higher success rates) and with faster (up to seven\ntimes) convergence rate. Our artificial intelligence approach could thus make\npersonalization algorithms generalizable and self-adaptable to any patient and\nany model.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 20:19:25 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Neumann", "Dominik", ""], ["Mansi", "Tommaso", ""], ["Itu", "Lucian", ""], ["Georgescu", "Bogdan", ""], ["Kayvanpour", "Elham", ""], ["Sedaghat-Hamedani", "Farbod", ""], ["Amr", "Ali", ""], ["Haas", "Jan", ""], ["Katus", "Hugo", ""], ["Meder", "Benjamin", ""], ["Steidl", "Stefan", ""], ["Hornegger", "Joachim", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "1605.00590", "submitter": "Alexandros Syrakos", "authors": "Alexandros Syrakos, Georgios C. Georgiou, Andreas N. Alexandrou", "title": "Performance of the finite volume method in solving regularised Bingham\n  flows: inertia effects in the lid-driven cavity flow", "comments": null, "journal-ref": "Journal of Non-Newtonian Fluid Mechanics 208-209 (2014) 88-107", "doi": "10.1016/j.jnnfm.2014.03.004", "report-no": null, "categories": "physics.comp-ph cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend our recent work on the creeping flow of a Bingham fluid in a\nlid-driven cavity, to the study of inertial effects, using a finite volume\nmethod and the Papanastasiou regularisation of the Bingham constitutive model\n[J. Rheology 31 (1987) 385-404]. The finite volume method used belongs to a\nvery popular class of methods for solving Newtonian flow problems, which use\nthe SIMPLE algorithm to solve the discretised set of equations, and have\nmatured over the years. By regularising the Bingham constitutive equation it is\neasy to extend such a solver to Bingham flows since all that this requires is\nto modify the viscosity function. This is a tempting approach, since it\nrequires minimum programming effort and makes available all the existing\nfeatures of the mature finite volume solver. On the other hand, regularisation\nintroduces a parameter which controls the error in addition to the grid\nspacing, and makes it difficult to locate the yield surfaces. Furthermore, the\nequations become stiffer and more difficult to solve, while the discontinuity\nat the yield surfaces causes large truncation errors. The present work attempts\nto investigate the strengths and weaknesses of such a method by applying it to\nthe lid-driven cavity problem for a range of Bingham and Reynolds numbers (up\nto 100 and 5000 respectively). By employing techniques such as multigrid, local\ngrid refinement, and an extrapolation procedure to reduce the effect of the\nregularisation parameter on the calculation of the yield surfaces (Liu et al.\nJ. Non-Newtonian Fluid Mech. 102 (2002) 179-191), satisfactory results are\nobtained, although the weaknesses of the method become more noticeable as the\nBingham number is increased.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2016 18:09:41 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Syrakos", "Alexandros", ""], ["Georgiou", "Georgios C.", ""], ["Alexandrou", "Andreas N.", ""]]}, {"id": "1605.00854", "submitter": "Qixia Yuan", "authors": "Andrzej Mizera and Jun Pang and Qixia Yuan", "title": "Fast Simulation of Probabilistic Boolean Networks (Technical Report)", "comments": "15 pages, 3 figures, for CMSB 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Boolean networks (PBNs) is an important mathematical framework\nwidely used for modelling and analysing biological systems. PBNs are suited for\nmodelling large biological systems, which more and more often arise in systems\nbiology. However, the large system size poses a~significant challenge to the\nanalysis of PBNs, in particular, to the crucial analysis of their steady-state\nbehaviour. Numerical methods for performing steady-state analyses suffer from\nthe state-space explosion problem, which makes the utilisation of statistical\nmethods the only viable approach. However, such methods require long\nsimulations of PBNs, rendering the simulation speed a crucial efficiency\nfactor. For large PBNs and high estimation precision requirements, a slow\nsimulation speed becomes an obstacle. In this paper, we propose a\nstructure-based method for fast simulation of PBNs. This method first performs\na network reduction operation and then divides nodes into groups for parallel\nsimulation. Experimental results show that our method can lead to an\napproximately 10 times speedup for computing steady-state probabilities of a\nreal-life biological network.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 16:29:39 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Mizera", "Andrzej", ""], ["Pang", "Jun", ""], ["Yuan", "Qixia", ""]]}, {"id": "1605.01020", "submitter": "Pablo Fernandez", "authors": "Pablo Fernandez, Ngoc-Cuong Nguyen, Xevi Roca, Jaime Peraire", "title": "Implicit large-eddy simulation of compressible flows using the Interior\n  Embedded Discontinuous Galerkin method", "comments": "54th AIAA Aerospace Sciences Meeting, AIAA SciTech, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.CE math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a high-order implicit large-eddy simulation (ILES) approach for\nsimulating transitional turbulent flows. The approach consists of an Interior\nEmbedded Discontinuous Galerkin (IEDG) method for the discretization of the\ncompressible Navier-Stokes equations and a parallel preconditioned Newton-GMRES\nsolver for the resulting nonlinear system of equations. The IEDG method arises\nfrom the marriage of the Embedded Discontinuous Galerkin (EDG) method and the\nHybridizable Discontinuous Galerkin (HDG) method. As such, the IEDG method\ninherits the advantages of both the EDG method and the HDG method to make\nitself well-suited for turbulence simulations. We propose a minimal residual\nNewton algorithm for solving the nonlinear system arising from the IEDG\ndiscretization of the Navier-Stokes equations. The preconditioned GMRES\nalgorithm is based on a restricted additive Schwarz (RAS) preconditioner in\nconjunction with a block incomplete LU factorization at the subdomain level.\nThe proposed approach is applied to the ILES of transitional turbulent flows\nover a NACA 65-(18)10 compressor cascade at Reynolds number 250,000 in both\ndesign and off-design conditions. The high-order ILES results show good\nagreement with a subgrid-scale LES model discretized with a second-order finite\nvolume code while using significantly less degrees of freedom. This work shows\nthat high-order accuracy is key for predicting transitional turbulent flows\nwithout a SGS model.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 18:49:53 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Fernandez", "Pablo", ""], ["Nguyen", "Ngoc-Cuong", ""], ["Roca", "Xevi", ""], ["Peraire", "Jaime", ""]]}, {"id": "1605.01220", "submitter": "Maciej Drwal", "authors": "Maciej Drwal", "title": "Optimal Design of Robust Combinatorial Mechanisms for Substitutable\n  Goods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider multidimensional mechanism design problem for\nselling discrete substitutable items to a group of buyers. Previous work on\nthis problem mostly focus on stochastic description of valuations used by the\nseller. However, in certain applications, no prior information regarding\nbuyers' preferences is known. To address this issue, we consider uncertain\nvaluations and formulate the problem in a robust optimization framework: the\nobjective is to minimize the maximum regret. For a special case of\nrevenue-maximizing pricing problem we present a solution method based on\nmixed-integer linear programming formulation.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 10:56:59 GMT"}, {"version": "v2", "created": "Wed, 18 May 2016 15:03:14 GMT"}, {"version": "v3", "created": "Wed, 14 Dec 2016 17:41:05 GMT"}, {"version": "v4", "created": "Wed, 4 Jan 2017 16:25:28 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Drwal", "Maciej", ""]]}, {"id": "1605.01402", "submitter": "Robert Carlsen", "authors": "Robert W. Carlsen, Paul P.H. Wilson", "title": "Challenging Fuel Cycle Modeling Assumptions: Facility and Time Step\n  Discretization Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the diversity of fuel cycle simulator modeling assumptions, direct\ncomparison and benchmarking can be difficult. In 2012 the Organisation for\nEconomic Co-operation and Development completed a benchmark study that is\nperhaps the most complete published comparison performed. Despite this, various\nresults from the simulators were often significantly different because of\ninconsistencies in modeling decisions involving reprocessing strategies,\nrefueling behavior, reactor end-of-life handling, etc. This work identifies and\nquantifies the effects of selected modeling choices that may sometimes be taken\nfor granted in the fuel cycle simulation domain. Four scenarios are compared\nusing combinations of fleet-based or individually modeled reactors with monthly\nor quarterly (3-month) time steps. The scenarios approximate a transition from\nthe current U.S. once-through light water reactor fleet to a full sodium fast\nreactor fuel cycle. The Cyclus fuel cycle simulator's plug-in capability along\nwith its market-like dynamic material routing allow it to be used as a level\nplaying field for comparing the scenarios. When under supply-constraint\npressure, the four cases exhibit noticeably different behavior. Fleet-based\nmodeling is more efficient in supply-constrained environments at the expense of\nlosing insight on issues such as realistically suboptimal fuel distribution and\nchallenges in reactor refueling cycle staggering. Finer-grained time steps\nenable more efficient material use in supply-constrained environments resulting\nin lower standing inventories of separated Pu. Large simulations with\nfleet-based reactors run much more quickly than their individual reactor\ncounterparts. Gaining a better understanding of how these and other modeling\nchoices affect fuel cycle dynamics will enable making more deliberate decisions\nwith respect to trade-offs such as computational investment vs. realism.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 19:58:48 GMT"}], "update_date": "2016-05-05", "authors_parsed": [["Carlsen", "Robert W.", ""], ["Wilson", "Paul P. H.", ""]]}, {"id": "1605.01583", "submitter": "Daljit Singh Dhillon", "authors": "Daljit Singh J. Dhillon, Michel C. Milinkovitch, Matthias Zwicker", "title": "Bifurcation Analysis of Reaction Diffusion Systems on Arbitrary Surfaces", "comments": "This paper was submitted at the Journal of Mathematical Biology,\n  Springer on 07th July 2015, in its current form (barring image references on\n  the last page and cosmetic changes owning to rebuild for arXiv). The complete\n  body of work presented here was included and defended as a part of my PhD\n  thesis in Nov 2015 at the University of Bern", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present computational techniques to investigate the\nsolutions of two-component, nonlinear reaction-diffusion (RD) systems on\narbitrary surfaces. We build on standard techniques for linear and nonlinear\nanalysis of RD systems, and extend them to operate on large-scale meshes for\narbitrary surfaces. In particular, we use spectral techniques for a linear\nstability analysis to characterize and directly compose patterns emerging from\nhomogeneities. We develop an implementation using surface finite element\nmethods and a numerical eigenanalysis of the Laplace-Beltrami operator on\nsurface meshes. In addition, we describe a technique to explore solutions of\nthe nonlinear RD equations using numerical continuation. Here, we present a\nmultiresolution approach that allows us to trace solution branches of the\nnonlinear equations efficiently even for large-scale meshes. Finally, we\ndemonstrate the working of our framework for two RD systems with applications\nin biological pattern formation: a Brusselator model that has been used to\nmodel pattern development on growing plant tips, and a chemotactic model for\nthe formation of skin pigmentation patterns. While these models have been used\npreviously on simple geometries, our framework allows us to study the impact of\narbitrary geometries on emerging patterns.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 13:29:36 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Dhillon", "Daljit Singh J.", ""], ["Milinkovitch", "Michel C.", ""], ["Zwicker", "Matthias", ""]]}, {"id": "1605.02029", "submitter": "Tommaso Mansi", "authors": "Dorin Comaniciu, Klaus Engel, Bogdan Georgescu, Tommaso Mansi", "title": "Shaping the Future through Innovations: From Medical Imaging to\n  Precision Medicine", "comments": "Submitted to Medical Image Analysis, Elsevier, 20th Anniversary\n  Special Issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical images constitute a source of information essential for disease\ndiagnosis, treatment and follow-up. In addition, due to its patient-specific\nnature, imaging information represents a critical component required for\nadvancing precision medicine into clinical practice. This manuscript describes\nrecently developed technologies for better handling of image information:\nphotorealistic visualization of medical images with Cinematic Rendering,\nartificial agents for in-depth image understanding, support for minimally\ninvasive procedures, and patient-specific computational models with enhanced\npredictive power. Throughout the manuscript we will analyze the capabilities of\nsuch technologies and extrapolate on their potential impact to advance the\nquality of medical care, while reducing its cost.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 20:02:16 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 21:02:19 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Comaniciu", "Dorin", ""], ["Engel", "Klaus", ""], ["Georgescu", "Bogdan", ""], ["Mansi", "Tommaso", ""]]}, {"id": "1605.04897", "submitter": "Tianshi Wang", "authors": "Tianshi Wang, Jaijeet Roychowdhury", "title": "Well-Posed Models of Memristive Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing compact models for memristive devices (including RRAM and CBRAM) all\nsuffer from issues related to mathematical ill-posedness and/or improper\nimplementation. This limits their value for simulation and design and in some\ncases, results in qualitatively unphysical predictions. We identify the causes\nof ill-posedness in these models. We then show how memristive devices in\ngeneral can be modelled using only continuous/smooth primitives in such a way\nthat they always respect physical bounds for filament length and also feature\nwell-defined and correct DC behaviour. We show how to express these models\nproperly in languages like Verilog-A and ModSpec (MATLAB). We apply these\nmethods to correct previously published RRAM and memristor models and make them\nwell posed. The result is a collection of memristor models that may be dubbed\n\"simulation-ready\", i.e., that feature the right physical characteristics and\nare suitable for robust and consistent simulation in DC, AC, transient, etc.,\nanalyses. We provide implementations of these models in both ModSpec/MATLAB and\nVerilog-A.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 04:17:43 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Wang", "Tianshi", ""], ["Roychowdhury", "Jaijeet", ""]]}, {"id": "1605.04984", "submitter": "Nikita Gordienko", "authors": "Nikita Gordienko", "title": "Multi-Parametric Statistical Method for Estimation of Accumulated\n  Fatigue by Sensors in Ordinary Gadgets", "comments": "7 pages, 6 figures; Proc. International Conference \"Science in XXI\n  century: Current Problems in Physics\" (May 17-19, 2016) Kyiv, Ukraine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new method is proposed to monitor the level of currently accumulated\nfatigue and estimate it by the several statistical methods. The experimental\nsoftware application was developed and used to get data from sensors\n(accelerometer, GPS, gyroscope, magnetometer, and camera), conducted\nexperiments, collected data, calculated parameters of their distributions\n(mean, standard deviation, skewness, kurtosis), and analyzed them by\nstatistical methods (moment analysis, cluster analysis, bootstrapping,\nperiodogram and spectrogram analyses). The hypothesis 1 (physical activity can\nbe estimated and classified by moment and cluster analysis) and hypothesis 2\n(fatigue can be estimated by moment analysis, bootstrapping analysis,\nperiodogram, and spectrogram) were proposed and proved. Several \"fatigue\nmetrics\" were proposed: location, size, shape of clouds of points on\nbootstrapping plot. The most promising fatigue metrics is the distance from the\n\"rest\" state point to the \"fatigue\" state point (sum of 3 squared non-normal\ndistribution of non-correlated acceleration values) on the skewness-kurtosis\nplot. These hypotheses were verified on several persons of various age, gender,\nfitness level and improved standard statistical methods in similar researches.\nThe method can be used in practice for ordinary people in everyday situations\n(to estimate their fatigue, give tips about it and advice on context-related\ninformation).\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 23:25:40 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Gordienko", "Nikita", ""]]}, {"id": "1605.05142", "submitter": "Santosh Tirunagari", "authors": "Santosh Tirunagari, Simon Bull and Norman Poh", "title": "Automatic Classification of Irregularly Sampled Time Series with Unequal\n  Lengths: A Case Study on Estimated Glomerular Filtration Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": "CS-CKD-2016-01", "categories": "cs.LG cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A patient's estimated glomerular filtration rate (eGFR) can provide important\ninformation about disease progression and kidney function. Traditionally, an\neGFR time series is interpreted by a human expert labelling it as stable or\nunstable. While this approach works for individual patients, the time consuming\nnature of it precludes the quick evaluation of risk in large numbers of\npatients. However, automating this process poses significant challenges as eGFR\nmeasurements are usually recorded at irregular intervals and the series of\nmeasurements differs in length between patients. Here we present a two-tier\nsystem to automatically classify an eGFR trend. First, we model the time series\nusing Gaussian process regression (GPR) to fill in `gaps' by resampling a fixed\nsize vector of fifty time-dependent observations. Second, we classify the\nresampled eGFR time series using a K-NN/SVM classifier, and evaluate its\nperformance via 5-fold cross validation. Using this approach we achieved an\nF-score of 0.90, compared to 0.96 for 5 human experts when scored amongst\nthemselves.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 12:46:46 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Tirunagari", "Santosh", ""], ["Bull", "Simon", ""], ["Poh", "Norman", ""]]}, {"id": "1605.05231", "submitter": "Junqi Tang", "authors": "Junqi Tang", "title": "The Non-uniform Fast Fourier Transform in Computed Tomography", "comments": "50 pages. A Masters thesis achieved in the Institute of Digital\n  Communications, the University of Edinburgh. Computer Science/Computation\n  Complexity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project is aimed at designing the fast forward projection algorithm and\nalso the backprojection algorithm for cone beam CT imaging systems with\ncircular X-ray source trajectory. The principle of the designs is based on\nutilizing the potential computational efficiency which the Fourier Slice\nTheorem and the Non-uniform Fast Fourier Transform (NUFFT) will bring forth. In\nthis Masters report, the detailed design of the NUFFT based forward projector\nincluding a novel 3D (derivative of) Radon space resampling method will be\ngiven. Meanwhile the complexity of the NUFFT based forward projector is\nanalysed and compared with the non-Fourier based CT projector, and the\nadvantage of the NUFFT based forward projection in terms of the computational\nefficiency is demonstrated in this report. Base on the design of the forward\nalgorithm, the NUFFT based 3D direct reconstruction algorithm will be derived.\nThe experiments will be taken to test the performance of the forward algorithm\nand the backprojection algorithm to show the practicability and accuracy of\nthese designs by comparing them jointly with the well-acknowledged cone beam CT\noperators: the CT linear interpolation forward projector and the FDK algorithm.\nThis Master report will demonstrate a novel and efficient way of implementing\nthe cone beam CT operator, a detailed summary of the project, and the future\nresearch prospects of the NUFFT based cone beam CT algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 16:39:54 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Tang", "Junqi", ""]]}, {"id": "1605.06015", "submitter": "Irina  Biktasheva", "authors": "Mario Antonioletti, Vadim N. Biktashev, Adrian Jackson, Sanjay R.\n  Kharche, Tomas Stary, Irina V. Biktasheva", "title": "BeatBox - HPC Simulation Environment for Biophysically and Anatomically\n  Realistic Cardiac Electrophysiology", "comments": "37 pages, 10 figures, last version submitted to PLOS ONE", "journal-ref": "PLoS ONE 12(5): e0172292, 2017", "doi": "10.1371/journal.pone.0172292", "report-no": null, "categories": "cs.CE nlin.AO nlin.PS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BeatBox simulation environment combines flexible script language user\ninterface with the robust computational tools, in order to setup cardiac\nelectrophysiology in-silico experiments without re-coding at low-level, so that\ncell excitation, tissue/anatomy models, stimulation protocols may be included\ninto a BeatBox script, and simulation run either sequentially or in parallel\n(MPI) without re-compilation. BeatBox is a free software written in C language\nto be run on a Unix-based platform. It provides the whole spectrum of multi\nscale tissue modelling from 0-dimensional individual cell simulation,\n1-dimensional fibre, 2-dimensional sheet and 3-dimensional slab of tissue, up\nto anatomically realistic whole heart simulations, with run time measurements\nincluding cardiac re-entry tip/filament tracing, ECG, local/global samples of\nany variables, etc. BeatBox solvers, cell, and tissue/anatomy models\nrepositories are extended via robust and flexible interfaces, thus providing an\nopen framework for new developments in the field. In this paper we give an\noverview of the BeatBox current state, together with a description of the main\ncomputational methods and MPI parallelisation approaches.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 15:46:25 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 20:53:12 GMT"}, {"version": "v3", "created": "Sun, 12 Feb 2017 21:38:56 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Antonioletti", "Mario", ""], ["Biktashev", "Vadim N.", ""], ["Jackson", "Adrian", ""], ["Kharche", "Sanjay R.", ""], ["Stary", "Tomas", ""], ["Biktasheva", "Irina V.", ""]]}, {"id": "1605.06288", "submitter": "Fehmi Cirak", "authors": "Kosala Bandara and Fehmi Cirak", "title": "Isogeometric shape optimisation of shell structures using\n  multiresolution subdivision surfaces", "comments": null, "journal-ref": null, "doi": "10.1016/j.cad.2017.09.006", "report-no": null, "categories": "math.NA cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the isogeometric shape optimisation of thin shell structures\nusing subdivision surfaces. Both triangular Loop and quadrilateral\nCatmull-Clark subdivision schemes are considered for geometry modelling and\nfinite element analysis. A gradient-based shape optimisation technique is\nimplemented to minimise compliance, i.e. to maximise stiffness. Different\ncontrol meshes describing the same surface are used for geometry\nrepresentation, optimisation and finite element analysis. The finite element\nanalysis is performed with subdivision basis functions corresponding to a\nsufficiently refined control mesh. During iterative shape optimisation the\ngeometry is updated starting from the coarsest control mesh and proceeding to\nincreasingly finer control meshes. This multiresolution approach provides a\nmeans for regularising the optimisation problem and prevents the appearance of\nsub-optimal jagged geometries with fine-scale oscillations. The finest control\nmesh for optimisation is chosen in accordance with the desired smallest feature\nsize in the optimised geometry. The proposed approach is applied to three\noptimisation examples, namely a catenary, a roof over a rectangular domain and\na freeform architectural shell roof. The influence of the geometry description\nand the used subdivision scheme on the obtained optimised curved geometries is\ninvestigated in detail.\n", "versions": [{"version": "v1", "created": "Fri, 20 May 2016 11:07:28 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 12:30:56 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Bandara", "Kosala", ""], ["Cirak", "Fehmi", ""]]}, {"id": "1605.06735", "submitter": "HyungSeon Oh", "authors": "HyungSeon Oh", "title": "Tensors in Power System Computation I: Distributed Computation for\n  Optimal Power Flow, DC OPF", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decomposition plays a key role in identifying common features across a\ncollection of matrices in many areas of science. A fundamental need in big data\nresearch is to process data tabulated as large-scale matrices using\neigenvectors. A higher order generalized singular value decomposition technique\nsuccessfully captures the common features of the same organ from multiple\nanimals in genomic signal processing. A recent semidefinite programming\napproach to solve an AC optimal power flow was accompanied by the problem\nformulation in the Cartesian coordinate system. The collection of nodal\nKirchhoff laws introduces a 3D tensor with a common feature of individual\nmatrices to maintain local power balance. In this paper, the mathematical\nprocess is established and the common feature is identified. The common feature\nis a key element to a fully decentralized and therefore scalable algorithm to\nsolve AC optimal power flow.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2016 04:43:10 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Oh", "HyungSeon", ""]]}, {"id": "1605.07091", "submitter": "Florian De Vuyst J", "authors": "Florian De Vuyst and Marie B\\'echereau and Thibault Gasc and Renaud\n  Motte and Mathieu Peybernes and Raphael Poncet", "title": "Stable and accurate interface capturing advection schemes", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, stable and \"low-diffusive\" multidimensional interface\ncapturing (IC) schemes using slope limiters are discussed. It is known that\ndirection-by-direction slope-limited MUSCL schemes create geometrical artifacts\nand thus return a poor accuracy. We here focus on this particular issue and\nshow that the reconstruction of gradient directions are an important factor of\naccuracy. The use of a multidimensional limiting process (MLP) added with an\nadequate time integration scheme leads to an artifact-free and instability-free\ninterface capturing (IC) approach. Numerical experiments like the reference\nKothe-Rider forward-backward advection case show the accuracy of the approach.\nWe also show that the approach can be extended to the more complex compressible\nmultimaterial hydrodynamics case, with potentially an arbitrary number of\nfluids. We also believe that this approach is appropriate for\nmulticore/manycore architecture because of its SIMD feature, which may be\nanother asset compared to interface reconstruction approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2016 16:52:03 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["De Vuyst", "Florian", ""], ["B\u00e9chereau", "Marie", ""], ["Gasc", "Thibault", ""], ["Motte", "Renaud", ""], ["Peybernes", "Mathieu", ""], ["Poncet", "Raphael", ""]]}, {"id": "1605.09162", "submitter": "Vladim\\'ir Luke\\v{s}", "authors": "Eduard Rohan, Vladim\\'ir Luke\\v{s} and Alena Jon\\'a\\v{s}ov\\'a", "title": "Modeling of the contrast-enhanced perfusion test in liver based on the\n  multi-compartment flow in porous media", "comments": "28 pages, 18 figures", "journal-ref": "Journal of Mathematical Biology, 77(2): 421-454 (2018)", "doi": "10.1007/s00285-018-1209-y", "report-no": null, "categories": "cs.CE physics.med-ph q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with modeling the liver perfusion intended to improve\nquantitative analysis of the tissue scans provided by the contrast-enhanced\ncomputed tomography (CT). For this purpose, we developed a model of dynamic\ntransport of the contrast fluid through the hierarchies of the perfusion trees.\nConceptually, computed time-space distributions of the so-called tissue density\ncan be compared with the measured data obtained from CT; such a modeling\nfeedback can be used for model parameter identification. The blood flow is\ncharacterized at several scales for which different models are used. Flows in\nupper hierarchies represented by larger branching vessels are described using\nsimple 1D models based on the Bernoulli equation extended by correction terms\nto respect the local pressure losses. To describe flows in smaller vessels and\nin the tissue parenchyma, we propose a 3D continuum model of porous medium\ndefined in terms of hierarchically matched compartments characterized by\nhydraulic permeabilities. The 1D models corresponding to the portal and hepatic\nveins are coupled with the 3D model through point sources, or sinks. The\ncontrast fluid saturation is governed by transport equations adapted for the 1D\nand 3D flow models. The complex perfusion model has been implemented using the\nfinite element and finite volume methods. We report numerical examples computed\nfor anatomically relevant geometries of the liver organ and of the principal\nvascular trees. The simulated tissue density corresponding to the CT\nexamination output reflects a pathology modeled as a localized permeability\ndeficiency.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 10:12:21 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Rohan", "Eduard", ""], ["Luke\u0161", "Vladim\u00edr", ""], ["Jon\u00e1\u0161ov\u00e1", "Alena", ""]]}, {"id": "1605.09181", "submitter": "Krzysztof Domino", "authors": "Krzysztof Domino", "title": "The use of the multi-cumulant tensor analysis for the algorithmic\n  optimisation of investment portfolios", "comments": "19 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.physa.2016.10.042", "report-no": null, "categories": "q-fin.PM cs.CE cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulant analysis plays an important role in non Gaussian distributed\ndata analysis. The shares' prices returns are good example of such data. The\npurpose of this research is to develop the cumulant based algorithm and use it\nto determine eigenvectors that represent investment portfolios with low\nvariability. Such algorithm is based on the Alternating Least Square method and\ninvolves the simultaneous minimisation 2'nd -- 6'th cumulants of the\nmultidimensional random variable (percentage shares' returns of many\ncompanies). Then the algorithm was tested during the recent crash on the Warsaw\nStock Exchange. To determine incoming crash and provide enter and exit signal\nfor the investment strategy the Hurst exponent was calculated using the local\nDFA. It was shown that introduced algorithm is on average better that benchmark\nand other portfolio determination methods, but only within examination window\ndetermined by low values of the Hurst exponent. Remark that the algorithm of is\nbased on cumulant tensors up to the 6'th order calculated for a\nmultidimensional random variable, what is the novel idea. It can be expected\nthat the algorithm would be useful in the financial data analysis on the world\nwide scale as well as in the analysis of other types of non Gaussian\ndistributed data.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 11:34:31 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 07:13:24 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Domino", "Krzysztof", ""]]}]