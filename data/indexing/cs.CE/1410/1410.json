[{"id": "1410.0547", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "Design Mining Interacting Wind Turbines", "comments": null, "journal-ref": "Evolutionary Computation (2016), 24(1):89-111", "doi": "10.1162/EVCO_a_00144", "report-no": null, "categories": "cs.NE cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An initial study of surrogate-assisted evolutionary algorithms used to design\nvertical-axis wind turbines wherein candidate prototypes are evaluated under\nfan generated wind conditions after being physically instantiated by a 3D\nprinter has recently been presented. Unlike other approaches, such as\ncomputational fluid dynamics simulations, no mathematical formulations were\nused and no model assumptions were made. This paper extends that work by\nexploring alternative surrogate modelling and evolutionary techniques. The\naccuracy of various modelling algorithms used to estimate the fitness of\nevaluated individuals from the initial experiments is compared. The effect of\ntemporally windowing surrogate model training samples is explored. A\nsurrogate-assisted approach based on an enhanced local search is introduced;\nand alternative coevolution collaboration schemes are examined.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 13:32:59 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 14:20:23 GMT"}], "update_date": "2016-03-14", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "1410.1233", "submitter": "Pavel Sakov", "authors": "Pavel Sakov", "title": "EnKF-C user guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EnKF-C provides a compact generic framework for off-line data assimilation\ninto large-scale layered geophysical models with the ensemble Kalman filter\n(EnKF). It is coded in C for GNU/Linux platform and can work either in EnKF,\nensemble optimal interpolation (EnOI), or hybrid (EnKF/EnOI) modes.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 01:29:42 GMT"}, {"version": "v10", "created": "Thu, 25 Mar 2021 11:31:44 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 03:33:40 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2015 05:12:12 GMT"}, {"version": "v4", "created": "Thu, 30 Jul 2015 03:04:59 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2016 22:50:35 GMT"}, {"version": "v6", "created": "Sun, 29 Oct 2017 21:53:38 GMT"}, {"version": "v7", "created": "Wed, 9 May 2018 02:21:16 GMT"}, {"version": "v8", "created": "Thu, 21 Feb 2019 04:43:27 GMT"}, {"version": "v9", "created": "Thu, 5 Nov 2020 11:58:42 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Sakov", "Pavel", ""]]}, {"id": "1410.2476", "submitter": "George Barnett Mr", "authors": "George L. Barnett and Simon W. Funke and Matthew D. Piggott", "title": "Hybrid global-local optimisation algorithms for the layout design of\n  tidal turbine arrays", "comments": "36 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tidal stream power generation represents a promising source of renewable\nenergy. In order to extract an economically useful amount of power, tens to\nhundreds of tidal turbines need to be placed within an array. The layout of\nthese turbines can have a significant impact on the power extracted and hence\non the viability of the site. Funke et al. formulated the question of the best\nturbine layout as an optimisation problem constrained by the shallow water\nequations and solved it using a local, gradient-based optimisation algorithm.\nGiven the local nature of this approach, the question arises of how optimal the\nlayouts actually are. This becomes particularly important for scenarios with\ncomplex bathymetry and layout constraints, both of which typically introduce\nlocally optimal layouts. Optimisation algorithms which find the global optima\ngenerally require orders of magnitude more iterations than local optimisation\nalgorithms and are thus infeasible in combination with an expensive flow model.\nThis paper presents an analytical wake model to act as an efficient proxy to\nthe shallow water model. Based upon this, a hybrid global-local two-stage\noptimisation approach is presented in which turbine layouts are first optimised\nwith the analytical wake model via a global optimisation algorithm, and further\noptimised with the shallow water model via a local gradient-based optimisation\nalgorithm. This procedure is applied to a number of idealised cases and a more\nrealistic case with complex bathymetry in the Pentland Firth, Scotland. It is\nshown that in cases where bathymetry is considered, the two-stage optimisation\nprocedure is able to improve the power extracted from the array by as much as\n25% compared to local optimisation for idealised scenarios and by as much as\n12% for the more realistic Pentland Firth scenario whilst in many cases\nreducing the overall computation time by approximately 35%.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 14:13:00 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Barnett", "George L.", ""], ["Funke", "Simon W.", ""], ["Piggott", "Matthew D.", ""]]}, {"id": "1410.2707", "submitter": "Giovanni Caudullo", "authors": "Giovanni Caudullo", "title": "Applying Geospatial Semantic Array Programming for a Reproducible Set of\n  Bioclimatic Indices in Europe", "comments": "10 pages, 4 figures, 1 table, published in IEEE Earthzine 2014 Vol. 7\n  Issue 2, 877975+ 2nd quarter theme. Geospatial Semantic Array Programming.\n  Available: http://www.earthzine.org/?p=877975", "journal-ref": "IEEE Earthzine, vol. 7, no. 2, pp. 877 975+, 2014", "doi": "10.1101/009589", "report-no": null, "categories": "cs.CE physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bioclimate-driven regression analysis is a widely used approach for modelling\necological niches and zonation. Although the bioclimatic complexity of the\nEuropean continent is high, a particular combination of 12 climatic and\ntopographic covariates was recently found able to reliably reproduce the\necological zoning of the Food and Agriculture Organization of the United\nNations (FAO) for forest resources assessment at pan-European scale, generating\nthe first fuzzy similarity map of FAO ecozones in Europe. The reproducible\nprocedure followed to derive this collection of bioclimatic indices is now\npresented. It required an integration of data-transformation modules (D-TM)\nusing geospatial tools such as Geographic Information System (GIS) software,\nand array-based mathematical implementation such as semantic array programming\n(SemAP). Base variables, intermediate and final covariates are described and\nsemantically defined by providing the workflow of D-TMs and the mathematical\nformulation following the SemAP notation. Source layers to derive base\nvariables were extracted by exclusively relying on global-scale public open\ngeodata in order for the same set of bioclimatic covariates to be reproducible\nin any region worldwide. In particular, two freely available datasets were\nexploited for temperature and precipitation (WorldClim) and elevation (Global\nMulti-resolution Terrain Elevation Data). The working extent covers the\nEuropean continent to the Urals with a resolution of 30 arc-second. The\nproposed set of bioclimatic covariates will be made available as open data in\nthe European Forest Data Centre (EFDAC). The forthcoming complete set of D-TM\ncodelets will enable the 12 covariates to be easily reproduced and expanded\nthrough free software.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 08:20:10 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Caudullo", "Giovanni", ""]]}, {"id": "1410.3015", "submitter": "Michal Banaszak", "authors": "Piotr Knychala and Michal Banaszak", "title": "Heuristic Monte Carlo Method Applied to Cooperative Motion Algorithm for\n  Binary Lattice Fluid", "comments": "Accpeted in Journal of Computational Methods in Sciences and\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cooperative Motion Algorithm is an efficient lattice method to simulate\ndense polymer systems and is often used with two different criteria to generate\na Markov chain in the configuration space. While the first method is the\nwell-established Metropolis algorithm, the other one is an heuristic algorithm\nwhich needs justification. As an introductory step towards justification for\nthe 3D lattice polymers, we study a simple system which is the binary equimolar\nuid on a 2D triangular lattice. Since all lattice sites are occupied only\nselected type of motions are considered, such the vacancy movements, swapping\nneighboring lattice sites (Kawasaki dynamics) and cooperative loops. We compare\nboth methods, calculating the energy as well as heat capacity as a function of\ntemperature. The critical temperature, which was determined using the Binder\ncumulant, was the same for all methods with the simulation accuracy and in\nagreement with the exact critical temperature for the Ising model on the 2D\ntriangular lattice. In order to achieve reliable results at low temperatures we\nemploy the parallel tempering algorithm which enables simultaneous simulations\nof replicas of the system in a wide range of temperatures.\n", "versions": [{"version": "v1", "created": "Sat, 11 Oct 2014 17:42:43 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Knychala", "Piotr", ""], ["Banaszak", "Michal", ""]]}, {"id": "1410.3016", "submitter": "Michal Banaszak", "authors": "Michal Dziecielski and Krzysztof Lewandowski and Michal Banaszak", "title": "Phase Diagram of Diblock Copolymer Melt in Dimension d=5", "comments": null, "journal-ref": "Computational Methods in Science and Technology 17(1-2, 17-23\n  (2011)", "doi": null, "report-no": null, "categories": "cond-mat.soft cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the self-consistent field theory (SCFT) in spherical unit cells of\nvarious dimensionalities, D, a phase diagram of a diblock, A-b-B, is calculated\nin 5 dimensional space, d = 5. This is an extension of a previous work for d =\n4. The phase diagram is parameterized by the chain composition, f, and\nincompatibility between A and B , quantified by the product \\c{hi} N. We\npredict 5 stable nanophases: layers, cylinders, 3 D spherical cells, 4D\nspherical cells, and 5D spherical cells. In the strong segregation limit, that\nis for large \\c{hi}N, the order-order transition compositions are determined by\nthe strong segregation theory (SST) in its simplest form. While the predictions\nof the SST theory are close to the corresponding SCFT extrapolations for d=4,\nthe extrapolations for d=5 significantly differ from them. We find that the S5\nnanophase is stable in a narrow strip between the ordered S4 nanophase and the\ndisordered phase. The calculated order-disorder transition lines depend weakly\non d, as expected.\n", "versions": [{"version": "v1", "created": "Sat, 11 Oct 2014 18:13:58 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Dziecielski", "Michal", ""], ["Lewandowski", "Krzysztof", ""], ["Banaszak", "Michal", ""]]}, {"id": "1410.3632", "submitter": "David \\v{S}afr\\'anek", "authors": "L. Brim, J. Niznan, D. Safranek", "title": "Compact Representation of Photosynthesis Dynamics by Rule-based Models\n  (Full Version)", "comments": "SASB 2014 full paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Traditional mathematical models of photosynthesis are based on mass action\nkinetics of light reactions. This approach requires the modeller to enumerate\nall the possible state combinations of the modelled chemical species. This\nleads to combinatorial explosion in the number of reactions although the\nstructure of the model could be expressed more compactly. We explore the use of\nrule-based modelling, in particular, a simplified variant of Kappa, to\ncompactly capture and automatically reduce existing mathematical models of\nphotosynthesis. Finally, the reduction procedure is implemented in BioNetGen\nlanguage and demonstrated on several ODE models of photosynthesis processes.\nThis is an extended version of the paper published in proceedings of 5th\nInternational Workshop on Static Analysis and Systems Biology (SASB) 2014.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 10:01:44 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Brim", "L.", ""], ["Niznan", "J.", ""], ["Safranek", "D.", ""]]}, {"id": "1410.3778", "submitter": "Michal Banaszak", "authors": "Krzysztof Lewandowski and Piotr Knychala and Michal Banaszak", "title": "Parallel-Tempering Monte-Carlo Simulation with Feedback-Optimized\n  Algorithm Applied to a Coil-to-Globule Transition of a Lattice Homopolymer", "comments": null, "journal-ref": "Computational Methods in Science and Technology, 16(1), 29-35,\n  2010", "doi": null, "report-no": null, "categories": "cond-mat.stat-mech cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study of the parallel tempering (replica exchange) Monte Carlo\nmethod, with special focus on the feedback-optimized parallel tempering\nalgorithm, used for generating an optimal set of simulation temperatures. This\nmethod is applied to a lattice simulation of a homopolymer chain undergoing a\ncoil-to-globule transition upon cooling. We select the optimal number of\nreplicas for different chain lengths, N = 25, 50 and 75, using replica's\nround-trip time in temperature space, in order to determine energy, specific\nheat, and squared end-to-end distance of the hopolymer chain for the selected\ntemperatures. We also evaluate relative merits of this optimization method.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 17:51:59 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Lewandowski", "Krzysztof", ""], ["Knychala", "Piotr", ""], ["Banaszak", "Michal", ""]]}, {"id": "1410.4049", "submitter": "Jakub Krajniak", "authors": "Jakub Krajniak and Michal Banaszak", "title": "Monte Carlo Study of Patchy Nanostructures Self-Assembled from a Single\n  Multiblock Chain", "comments": null, "journal-ref": "Computational Methods in Science and Technology, 19(3) 137-143,\n  2013", "doi": "10.12921/cmst.2013.19.03.137-143", "report-no": null, "categories": "cond-mat.soft cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a lattice Monte Carlo simulation for a multiblock copolymer chain\nof length N=240 and microarchitecture $(10-10)_{12}$.The simulation was\nperformed using the Monte Carlo method with the Metropolis algorithm. We\nmeasured average energy, heat capacity, the mean squared radius of gyration,\nand the histogram of cluster count distribution. Those quantities were\ninvestigated as a function of temperature and incompatibility between segments,\nquantified by parameter {\\omega}. We determined the temperature of the\ncoil-globule transition and constructed the phase diagram exhibiting a variety\nof patchy nanostructures. The presented results yield a qualitative agreement\nwith those of the off-lattice Monte Carlo method reported earlier, with a\nsignificant exception for small incompatibilities,{\\omega}, and low\ntemperatures, where 3-cluster patchy nanostructures are observed in contrast to\nthe 2-cluster structures observed for the off-lattice $(10-10)_{12}$ chain. We\nattribute this difference to a considerable stiffness of lattice chains in\ncomparison to that of the off-lattice chains.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 13:07:35 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 06:39:58 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Krajniak", "Jakub", ""], ["Banaszak", "Michal", ""]]}, {"id": "1410.4399", "submitter": "Ynte Vanderhoydonc", "authors": "Ynte Vanderhoydonc and Wim Vanroose", "title": "Constrained Runs algorithm as a lifting operator for the Boltzmann\n  equation", "comments": "submitted to SIAM MMS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifting operators play an important role in starting a kinetic Boltzmann\nmodel from given macroscopic information. The macroscopic variables need to be\nmapped to the distribution functions, mesoscopic variables of the Boltzmann\nmodel. A well-known numerical method for the initialization of Boltzmann models\nis the Constrained Runs algorithm. This algorithm is used in literature for the\ninitialization of lattice Boltzmann models, special discretizations of the\nBoltzmann equation. It is based on the attraction of the dynamics toward the\nslow manifold and uses lattice Boltzmann steps to converge to the desired\ndynamics on the slow manifold. We focus on applying the Constrained Runs\nalgorithm to map density, average flow velocity, and temperature, the\nmacroscopic variables, to distribution functions. Furthermore, we do not\nconsider only lattice Boltzmann models. We want to perform the algorithm for\ndifferent discretizations of the Boltzmann equation and consider a standard\nfinite volume discretization.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 12:46:27 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Vanderhoydonc", "Ynte", ""], ["Vanroose", "Wim", ""]]}, {"id": "1410.4428", "submitter": "Ynte Vanderhoydonc", "authors": "Ynte Vanderhoydonc and Wim Vanroose", "title": "Initialization of lattice Boltzmann models with the help of the\n  numerical Chapman-Enskog expansion", "comments": "arXiv admin note: text overlap with arXiv:1108.4919", "journal-ref": "Procedia Computer Science, vol 18, pp. 1036-1045 (2013)", "doi": "10.1016/j.procs.2013.05.269", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the applicability of the numerical Chapman-Enskog expansion as a\nlifting operator for lattice Boltzmann models to map density and momentum to\ndistribution functions. In earlier work [Vanderhoydonc et al. Multiscale Model.\nSimul. 10(3): 766-791, 2012] such an expansion was constructed in the context\nof lifting only the zeroth order velocity moment, namely the density. A lifting\noperator is necessary to convert information from the macroscopic to the\nmesoscopic scale. This operator is used for the initialization of lattice\nBoltzmann models. Given only density and momentum, the goal is to initialize\nthe distribution functions of lattice Boltzmann models. For this\ninitialization, the numerical Chapman-Enskog expansion is used in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 13:58:04 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Vanderhoydonc", "Ynte", ""], ["Vanroose", "Wim", ""]]}, {"id": "1410.4577", "submitter": "Michal Banaszak", "authors": "Krzysztof Lewandowski and Michal Banaszak", "title": "Intra-Globular Structures in Multiblock Copolymer Chains from a Monte\n  Carlo Simulation", "comments": null, "journal-ref": "Physical Review E, 84, 011806, 2011", "doi": "10.1103/PhysRevE.84.011806", "report-no": null, "categories": "cond-mat.soft cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiblock copolymer chains in implicit nonselective solvents are studied by\nMonte Carlo method which employs a parallel tempering algorithm. Chains\nconsisting of 120 $A$ and 120 $B$ monomers, arranged in three distinct\nmicroarchitectures: $(10-10)_{12}$, $(6-6)_{20}$, and $(3-3)_{40}$, collapse to\nglobular states upon cooling, as expected. By varying both the reduced\ntemperature $T^*$ and compatibility between monomers $\\omega$, numerous\nintra-globular structures are obtained: diclusters (handshake, spiral, torus\nwith a core, etc.), triclusters, and $n$-clusters with $n>3$ (lamellar and\nother), which are reminiscent of the block copolymer nanophases for spherically\nconfined geometries. Phase diagrams for various chains in the $(T^*,\n\\omega)$-space are mapped. The structure factor $S(k)$, for a selected\nmicroarchitecture and $\\omega$, is calculated. Since $S(k)$ can be measured in\nscattering experiments, it can be used to relate simulation results to an\nexperiment. Self-assembly in those systems is interpreted in term of\ncompetition between minimization of the interfacial area separating different\ntypes of monomers and minimization of contacts between chain and solvent.\nFinally, the relevance of this model to the protein folding is addressed.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 20:11:35 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Lewandowski", "Krzysztof", ""], ["Banaszak", "Michal", ""]]}, {"id": "1410.4598", "submitter": "Stefan Hoehme", "authors": "Adrian Friebel, Johannes Neitsch, Tim Johann, Seddik Hammad, Jan G.\n  Hengstler, Dirk Drasdo, Stefan Hoehme", "title": "TiQuant: Software for tissue analysis, quantification and surface\n  reconstruction", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.GR q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: TiQuant is a modular software tool for efficient quantification\nof biological tissues based on volume data obtained by biomedical image\nmodalities. It includes a number of versatile image and volume processing\nchains tailored to the analysis of different tissue types which have been\nexperimentally verified. TiQuant implements a novel method for the\nreconstruction of three-dimensional surfaces of biological systems, data that\noften cannot be obtained experimentally but which is of utmost importance for\ntissue modelling in systems biology. Availability: TiQuant is freely available\nfor non-commercial use at msysbio.com/tiquant. Windows, OSX and Linux are\nsupported.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 22:29:10 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Friebel", "Adrian", ""], ["Neitsch", "Johannes", ""], ["Johann", "Tim", ""], ["Hammad", "Seddik", ""], ["Hengstler", "Jan G.", ""], ["Drasdo", "Dirk", ""], ["Hoehme", "Stefan", ""]]}, {"id": "1410.5103", "submitter": "Joseph Crawford", "authors": "Joseph Crawford and Tijana Milenkovi\\'c", "title": "GREAT: GRaphlet Edge-based network AlignmenT", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network alignment aims to find regions of topological or functional\nsimilarities between networks. In computational biology, it can be used to\ntransfer biological knowledge from a well-studied species to a poorly-studied\nspecies between aligned network regions. Typically, existing network aligners\nfirst compute similarities between nodes in different networks (via a node cost\nfunction) and then aim to find a high-scoring alignment (node mapping between\nthe networks) with respect to \"node conservation\", typically the total node\ncost function over all aligned nodes. Only after an alignment is constructed,\nthe existing methods evaluate its quality with respect to an alternative\nmeasure, such as \"edge conservation\". Thus, we recently aimed to directly\noptimize edge conservation while constructing an alignment, which improved\nalignment quality. Here, we approach a novel idea of maximizing both node and\nedge conservation, and we also approach this idea from a novel perspective, by\naligning optimally edges between networks first in order to improve node cost\nfunction needed to then align well nodes between the networks. In the process,\nunlike the existing measures of edge conservation that treat each conserved\nedge the same, we favor conserved edges that are topologically similar over\nconserved edges that are topologically dissimilar. We show that our novel\nmethod, which we call GRaphlet Edge AlignmenT (GREAT), improves upon\nstate-of-the-art methods that aim to optimize node conservation only or edge\nconservation only.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 19:03:50 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Crawford", "Joseph", ""], ["Milenkovi\u0107", "Tijana", ""]]}, {"id": "1410.5242", "submitter": "Moritz Kreutzer", "authors": "Moritz Kreutzer, Georg Hager, Gerhard Wellein, Andreas Pieper, Andreas\n  Alvermann, Holger Fehske", "title": "Performance Engineering of the Kernel Polynomial Method on Large-Scale\n  CPU-GPU Systems", "comments": "10 pages, 12 figures", "journal-ref": "Proceedings of the 2015 IEEE International Parallel and\n  Distributed Processing Symposium (IPDPS) 417-426", "doi": "10.1109/IPDPS.2015.76", "report-no": null, "categories": "cs.CE cond-mat.mes-hall cs.DC cs.PF physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kernel Polynomial Method (KPM) is a well-established scheme in quantum\nphysics and quantum chemistry to determine the eigenvalue density and spectral\nproperties of large sparse matrices. In this work we demonstrate the high\noptimization potential and feasibility of peta-scale heterogeneous CPU-GPU\nimplementations of the KPM. At the node level we show that it is possible to\ndecouple the sparse matrix problem posed by KPM from main memory bandwidth both\non CPU and GPU. To alleviate the effects of scattered data access we combine\nloosely coupled outer iterations with tightly coupled block sparse matrix\nmultiple vector operations, which enables pure data streaming. All\noptimizations are guided by a performance analysis and modelling process that\nindicates how the computational bottlenecks change with each optimization step.\nFinally we use the optimized node-level KPM with a hybrid-parallel framework to\nperform large scale heterogeneous electronic structure calculations for novel\ntopological materials on a petascale-class Cray XC30 system.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 12:16:22 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 10:52:10 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Kreutzer", "Moritz", ""], ["Hager", "Georg", ""], ["Wellein", "Gerhard", ""], ["Pieper", "Andreas", ""], ["Alvermann", "Andreas", ""], ["Fehske", "Holger", ""]]}, {"id": "1410.6121", "submitter": "Branislav Nikolic", "authors": "J. K. Freericks, B. K. Nikolic, and O. Frieder", "title": "The Nonequilibrium Many-Body Problem as a paradigm for extreme data\n  science", "comments": "33 pages, 7 figures, invited review for Int. J. Mod. Phys. B;\n  published version with additional references", "journal-ref": "Int J. Mod. Phys. B 28, 1430021 (2014)", "doi": "10.1142/S0217979214300217", "report-no": null, "categories": "cond-mat.str-el cond-mat.stat-mech cs.CC cs.CE math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating big data pervades much of physics. But some problems, which we\ncall extreme data problems, are too large to be treated within big data\nscience. The nonequilibrium quantum many-body problem on a lattice is just such\na problem, where the Hilbert space grows exponentially with system size and\nrapidly becomes too large to fit on any computer (and can be effectively\nthought of as an infinite-sized data set). Nevertheless, much progress has been\nmade with computational methods on this problem, which serve as a paradigm for\nhow one can approach and attack extreme data problems. In addition, viewing\nthese physics problems from a computer-science perspective leads to new\napproaches that can be tried to solve them more accurately and for longer\ntimes. We review a number of these different ideas here.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 17:55:53 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 18:21:56 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Freericks", "J. K.", ""], ["Nikolic", "B. K.", ""], ["Frieder", "O.", ""]]}, {"id": "1410.6153", "submitter": "Benjamin Ivorra Prof.", "authors": "Benjamin Ivorra, Di\\`ene Ngom, \\'Angel Manuel Ramos", "title": "Be-CoDiS: A mathematical model to predict the risk of human diseases\n  spread between countries. Validation and application to the 2014-15 Ebola\n  Virus Disease epidemic", "comments": "34 pages; Version 5; Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE math.DS physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ebola virus disease is a lethal human and primate disease that currently\nrequires a particular attention from the international health authorities due\nto important outbreaks in some Western African countries and isolated cases in\nthe United Kingdom, the USA and Spain. Regarding the emergency of this\nsituation, there is a need of development of decision tools, such as\nmathematical models, to assist the authorities to focus their efforts in\nimportant factors to eradicate Ebola. In this work, we propose a novel\ndeterministic spatial-temporal model, called Be-CoDiS (Between-Countries\nDisease Spread), to study the evolution of human diseases within and between\ncountries. The main interesting characteristics of Be-CoDiS are the\nconsideration of the movement of people between countries, the control measure\neffects and the use of time dependent coefficients adapted to each country.\nFirst, we focus on the mathematical formulation of each component of the model\nand explain how its parameters and inputs are obtained. Then, in order to\nvalidate our approach, we consider two numerical experiments regarding the\n2014-15 Ebola epidemic. The first one studies the ability of the model in\npredicting the EVD evolution between countries starting from the index cases in\nGuinea in December 2013. The second one consists of forecasting the evolution\nof the epidemic by using some recent data. The results obtained with Be-CoDiS\nare compared to real data and other models outputs found in the literature.\nFinally, a brief parameter sensitivity analysis is done. A free Matlab version\nof Be-CoDiS is available at: http://www.mat.ucm.es/momat/software.htm\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 19:52:56 GMT"}, {"version": "v2", "created": "Sat, 8 Nov 2014 22:06:05 GMT"}, {"version": "v3", "created": "Thu, 20 Nov 2014 12:58:15 GMT"}, {"version": "v4", "created": "Mon, 15 Dec 2014 16:33:56 GMT"}, {"version": "v5", "created": "Tue, 12 May 2015 14:09:24 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Ivorra", "Benjamin", ""], ["Ngom", "Di\u00e8ne", ""], ["Ramos", "\u00c1ngel Manuel", ""]]}, {"id": "1410.6335", "submitter": "Pavel Hron", "authors": "Pavel Hron, Daniel Jost, Peter Bastian, Claudia Gallert, Josef Winter,\n  Olaf Ippisch", "title": "Application of reactive transport modelling to growth and transport of\n  microorganisms in the capillary fringe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multicomponent multiphase reactive transport simulator has been developed\nto facilitate the investigation of a large variety of phenomena in porous media\nincluding component transport, diffusion, microbiological growth and decay,\ncell attachment and detachment and phase exchange. The coupled problem is\nsolved using operator splitting. This approach allows a flexible adaptation of\nthe solution strategy to the concrete problem.\n  Moreover, the individual submodels were optimised to be able to describe\nbehaviour of Escherichia coli (HB101 K12 pGLO) in the capillary fringe in the\npresence or absence of dissolved organic carbon and oxygen under steady-state\nand flow conditions. Steady-state and flow through experiments in a Hele-Shaw\ncell, filled with quartz sand, were conducted to study eutrophic bacterial\ngrowth and transport in both saturated and unsaturated porous media. As E. coli\ncells can form the green fluorescent protein (GFP), the cell densities,\ncalculated by evaluation of measured fluorescence intensities (in situ\ndetection) were compared with the cell densities computed by numerical\nsimulation. The comparison showed the laboratory experiments can be well\ndescribed by our mathematical model.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 12:10:11 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Hron", "Pavel", ""], ["Jost", "Daniel", ""], ["Bastian", "Peter", ""], ["Gallert", "Claudia", ""], ["Winter", "Josef", ""], ["Ippisch", "Olaf", ""]]}, {"id": "1410.6455", "submitter": "Yong Kong", "authors": "Yong Kong", "title": "Btrim: A fast, lightweight adapter and quality trimming program for\n  next-generation sequencing technologies", "comments": "8 pages, 1 figure", "journal-ref": "Genomics, 98, 152-153 (2001)", "doi": "10.1016/j.ygeno.2011.05.009", "report-no": null, "categories": "q-bio.GN cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Btrim is a fast and lightweight software to trim adapters and low quality\nregions in reads from ultra high-throughput next-generation sequencing\nmachines. It also can reliably identify barcodes and assign the reads to the\noriginal samples. Based on a modified Myers's bit-vector dynamic programming\nalgorithm, Btrim can handle indels in adapters and barcodes. It removes low\nquality regions and trims off adapters at both or either end of the reads. A\ntypical trimming of 30M reads with two sets of adapter pairs can be done in\nabout a minute with a small memory footprint. Btrim is a versatile stand-alone\ntool that can be used as the first step in virtually all next-generation\nsequence analysis pipelines. The program is available at\n\\url{http://graphics.med.yale.edu/trim/}.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 18:56:10 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Kong", "Yong", ""]]}, {"id": "1410.6609", "submitter": "Dominik Bartuschat", "authors": "Dominik Bartuschat, Ulrich R\\\"ude", "title": "Parallel Multiphysics Simulations of Charged Particles in Microfluidic\n  Flows", "comments": "Submitted to Journal of Computational Science (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes parallel multiphysics simulations of charged particles\nin microfluidic flows with the waLBerla framework. To this end, three physical\neffects are coupled: rigid body dynamics, fluid flow modelled by a lattice\nBoltzmann algorithm, and electric potentials represented by a finite volume\ndiscretisation. For solving the finite volume discretisation for the\nelectrostatic forces, a cell-centered multigrid algorithm is developed that\nconforms to the lattice Boltzmann meshes and the parallel communication\nstructure of waLBerla. The new functionality is validated with suitable\nbenchmark scenarios. Additionally, the parallel scaling and the numerical\nefficiency of the algorithms are analysed on an advanced supercomputer.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 08:10:13 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Bartuschat", "Dominik", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1410.6951", "submitter": "H. O. Ghaffari", "authors": "H.O. Ghaffari, P. Benson, K.Xia, R.P.Young", "title": "Observation of the Kibble-Zurek Mechanism in Microscopic Acoustic\n  Cracking Noises", "comments": null, "journal-ref": null, "doi": "10.1038/srep21210", "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci cond-mat.other physics.geo-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The fast evolution of microstructure is key to understanding crackling\nphenomena. It has been proposed that formation of a nonlinear zone around a\nmoving crack tip controls the crack tip velocity. Progress in understanding the\nphysics of this critical zone has been limited due to the lack of hard data\ndescribing the detailed complex physical processes that occur within. For the\nfirst time, we show that the signature of the non-linear elastic zone around a\nmicroscopic dynamic crack maps directly to generic phases of acoustic noises,\nsupporting the formation of a strongly weak zone near the moving crack tips. We\nadditionally show that the rate of traversing to non-linear zone controls the\nrate of weakening, i.e. speed of global rupture propagation. We measure the\npower-law dependence of nonlinear zone size on the traversing rate, and show\nthat our observations are in agreement with the Kibble-Zurek mechanism (KZM) .\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 19:09:53 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2015 03:36:32 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Ghaffari", "H. O.", ""], ["Benson", "P.", ""], ["Xia", "K.", ""], ["Young", "R. P.", ""]]}, {"id": "1410.7704", "submitter": "Tatjana Petrov", "authors": "Mirco Giacobbe, Calin C. Guet, Ashutosh Gupta, Thomas A. Henzinger,\n  Tiago Paixao, and Tatjana Petrov", "title": "Model Checking Gene Regulatory Networks", "comments": "19 pages, 20 references, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LO q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The behaviour of gene regulatory networks (GRNs) is typically analysed using\nsimulation-based statistical testing-like methods. In this paper, we\ndemonstrate that we can replace this approach by a formal verification-like\nmethod that gives higher assurance and scalability. We focus on Wagner weighted\nGRN model with varying weights, which is used in evolutionary biology. In the\nmodel, weight parameters represent the gene interaction strength that may\nchange due to genetic mutations. For a property of interest, we synthesise the\nconstraints over the parameter space that represent the set of GRNs satisfying\nthe property. We experimentally show that our parameter synthesis procedure\ncomputes the mutational robustness of GRNs -an important problem of interest in\nevolutionary biology- more efficiently than the classical simulation method. We\nspecify the property in linear temporal logics. We employ symbolic bounded\nmodel checking and SMT solving to compute the space of GRNs that satisfy the\nproperty, which amounts to synthesizing a set of linear constraints on the\nweights.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 17:19:13 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 17:25:12 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Giacobbe", "Mirco", ""], ["Guet", "Calin C.", ""], ["Gupta", "Ashutosh", ""], ["Henzinger", "Thomas A.", ""], ["Paixao", "Tiago", ""], ["Petrov", "Tatjana", ""]]}, {"id": "1410.7851", "submitter": "Andrew Connor", "authors": "Andy M. Connor, Keith A. Seffen, Geoffrey T. Parks and P. John\n  Clarkson", "title": "Efficient optimisation of structures using tabu search", "comments": null, "journal-ref": "Connor, A.M., Seffen, K.A., Clarkson, P.J. & Parks, G.T. (1999)\n  \"Efficient optimisation of structures using tabu search\" Proceedings of the\n  1st ASMO/ISSMO Conference on Engineering Design Optimization, 127-134", "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to the optimisation of structures using\na Tabu search (TS) method. TS is a metaheuristic which is used to guide local\nsearch methods towards a globally optimal solution by using flexible memory\ncycles of differing time spans. Results are presented for the well established\nten bar truss problem and compared to results published in the literature. In\nthe first example a truss is optimised to minimise mass and the results\ncompared to results obtained using an alternative TS implementation. In the\nsecond example, the problem has multiple objectives that are compounded into a\nsingle objective function value using game theory. In general the results\ndemonstrate that the TS method is capable of solving structural optimisation\nproblems at least as efficiently as other numerical optimisation approaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 01:11:14 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["Connor", "Andy M.", ""], ["Seffen", "Keith A.", ""], ["Parks", "Geoffrey T.", ""], ["Clarkson", "P. John", ""]]}, {"id": "1410.8489", "submitter": "Scott Norris", "authors": "Scott A. Norris", "title": "PyCraters: A Python framework for crater function analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.mtrl-sci cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Python framework designed to automate the most common tasks\nassociated with the extraction and upscaling of the statistics of single-impact\ncrater functions to inform coefficients of continuum equations describing\nsurface morphology evolution. Designed with ease-of-use in mind, the framework\nallows users to extract meaningful statistical estimates with very short Python\nprograms. Wrappers to interface with specific simulation packages, routines for\nstatistical extraction of output, and fitting and differentiation libraries are\nall hidden behind simple, high-level user-facing functions. In addition, the\nframework is extensible, allowing advanced users to specify the collection of\nspecialized statistics or the creation of customized plots. The framework is\nhosted on the BitBucket service under an open-source license, with the aim of\nhelping non-specialists easily extract preliminary estimates of relevant crater\nfunction results associated with a particular experimental system.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 21:13:02 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Norris", "Scott A.", ""]]}, {"id": "1410.8581", "submitter": "Dilek K\\\"u\\c{c}\\\"uk", "authors": "Dilek K\\\"u\\c{c}\\\"uk and Yusuf Arslan", "title": "Semi-Automatic Construction of a Domain Ontology for Wind Energy Using\n  Wikipedia Articles", "comments": null, "journal-ref": "Renewable Energy, Volume 62, pp. 484-489, February 2014", "doi": "10.1016/j.renene.2013.08.002", "report-no": null, "categories": "cs.CL cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain ontologies are important information sources for knowledge-based\nsystems. Yet, building domain ontologies from scratch is known to be a very\nlabor-intensive process. In this study, we present our semi-automatic approach\nto building an ontology for the domain of wind energy which is an important\ntype of renewable energy with a growing share in electricity generation all\nover the world. Related Wikipedia articles are first processed in an automated\nmanner to determine the basic concepts of the domain together with their\nproperties and next the concepts, properties, and relationships are organized\nto arrive at the ultimate ontology. We also provide pointers to other\nengineering ontologies which could be utilized together with the proposed wind\nenergy ontology in addition to its prospective application areas. The current\nstudy is significant as, to the best of our knowledge, it proposes the first\nconsiderably wide-coverage ontology for the wind energy domain and the ontology\nis built through a semi-automatic process which makes use of the related Web\nresources, thereby reducing the overall cost of the ontology building process.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 22:38:11 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["K\u00fc\u00e7\u00fck", "Dilek", ""], ["Arslan", "Yusuf", ""]]}, {"id": "1410.8616", "submitter": "Abhijit Chandra", "authors": "Abhijit Chandra and Oliva Kar", "title": "Data Driven Prognosis: A multi-physics approach verified via balloon\n  burst experiment", "comments": null, "journal-ref": null, "doi": "10.1098/rspa.2014.0525", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-physics formulation for Data Driven Prognosis (DDP) is developed.\nUnlike traditional predictive strategies that require controlled off-line\nmeasurements or training for determination of constitutive parameters to derive\nthe transitional statistics, the proposed DDP algorithm relies solely on in\nsitu measurements. It utilizes a deterministic mechanics framework, but the\nstochastic nature of the solution arises naturally from the underlying\nassumptions regarding the order of the conservation potential as well as the\nnumber of dimensions involved. The proposed DDP scheme is capable of predicting\nonset of instabilities. Since the need for off-line testing (or training) is\nobviated, it can be easily implemented for systems where such a priori testing\nis difficult or even impossible to conduct. The prognosis capability is\ndemonstrated here via a balloon burst experiment where the instability is\npredicted utilizing only on-line visual observations. The DDP scheme never\nfailed to predict the incipient failure, and no false positives were issued.\nThe DDP algorithm is applicable to others types of datasets. Time horizons of\nDDP predictions can be adjusted by using memory over different time windows.\nThus, a big dataset can be parsed in time to make a range of predictions over\nvarying time horizons.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 02:05:09 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Chandra", "Abhijit", ""], ["Kar", "Oliva", ""]]}, {"id": "1410.8674", "submitter": "Jan Zeman", "authors": "Alena Zemanov\\'a, Jan Zeman, Michal \\v{S}ejnoha", "title": "Finite element model based on refined plate theories for laminated glass\n  units", "comments": "22 pages, 10 figures, 3 tables", "journal-ref": "Latin American Journal of Solids and Structures 15(6):1158--1180,\n  2015", "doi": "10.1590/1679-78251676", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laminated glass units exhibit complex response as a result of different\nmechanical behavior and properties of glass and polymer foil. We aim to develop\na finite element model for elastic laminated glass plates based on the refined\nplate theory by Mau. For a geometrically nonlinear description of the behavior\nof units, each layer behaves according to the Reissner-Mindlin kinematics,\ncomplemented with membrane effects and the von K\\'{a}rm\\'{a}n assumptions.\nNodal Lagrange multipliers enforce the compatibility of independent layers in\nthis approach. We have derived the discretized model by the energy-minimization\narguments, assuming that the unknown fields are approximated by bi-linear\nfunctions at the element level, and solved the resulting system by the Newton\nmethod with consistent linearization. We have demonstrated through verification\nand validation examples that the proposed formulation is reliable and\naccurately reproduces the behavior of laminated glass units. This study\nrepresents a first step to the development of a comprehensive, mechanics-based\nmodel for laminated glass systems that is suitable for implementation in common\nengineering finite element solvers.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 09:00:37 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Zemanov\u00e1", "Alena", ""], ["Zeman", "Jan", ""], ["\u0160ejnoha", "Michal", ""]]}]