[{"id": "1302.0317", "submitter": "Valeria Krzhizhanovskaya", "authors": "V.V. Krzhizhanovskaya, N.B. Melnikova, A.M. Chirkin, S.V. Ivanov, A.V.\n  Boukhanovsky, P.M.A. Sloot", "title": "Distributed simulation of city inundation by coupled surface and\n  subsurface porous flow for urban flood decision support system", "comments": "Pre-print submitted to the 2013 International Conference on\n  Computational Science", "journal-ref": "Procedia Computer Science, Volume 18, 2013, Pages 1046-1056", "doi": "10.1016/j.procs.2013.05.270", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a decision support system for flood early warning and disaster\nmanagement. It includes the models for data-driven meteorological predictions,\nfor simulation of atmospheric pressure, wind, long sea waves and seiches; a\nmodule for optimization of flood barrier gates operation; models for stability\nassessment of levees and embankments, for simulation of city inundation\ndynamics and citizens evacuation scenarios. The novelty of this paper is a\ncoupled distributed simulation of surface and subsurface flows that can predict\ninundation of low-lying inland zones far from the submerged waterfront areas,\nas observed in St. Petersburg city during the floods. All the models are\nwrapped as software services in the CLAVIRE platform for urgent computing,\nwhich provides workflow management and resource orchestration.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2013 23:42:35 GMT"}], "update_date": "2014-01-31", "authors_parsed": [["Krzhizhanovskaya", "V. V.", ""], ["Melnikova", "N. B.", ""], ["Chirkin", "A. M.", ""], ["Ivanov", "S. V.", ""], ["Boukhanovsky", "A. V.", ""], ["Sloot", "P. M. A.", ""]]}, {"id": "1302.0710", "submitter": "Ana L. Teixeira", "authors": "Ana L. Teixeira, Rui C. Santos, Joao P. Leal, Jose A. Martinho Simoes,\n  and Andre O. Falcao", "title": "ThermInfo: Collecting, Retrieving, and Estimating Reliable\n  Thermochemical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard enthalpies of formation are used for assessing the efficiency and\nsafety of chemical processes in the chemical industry. However, the number of\ncompounds for which the enthalpies of formation are available is many orders of\nmagnitude smaller than the number of known compounds. Thermochemical data\nprediction methods are therefore clearly needed. Several commercial and free\nchemical databases are currently available, the NIST WebBook being the most\nused free source. To overcome this problem a cheminformatics system was\ndesigned and built with two main objectives in mind: collecting and retrieving\ncritically evaluated thermochemical values, and estimating new data. In its\npresent version, by using cheminformatics techniques, ThermInfo allows the\nretrieval of the value of a thermochemical property, such as a gas-phase\nstandard enthalpy of formation, by inputting, for example, the molecular\nstructure or the name of a compound. The same inputs can also be used to\nestimate data (presently restricted to non-polycyclic hydrocarbons) by using\nthe Extended Laidler Bond Additivity (ELBA) method. The information system is\npublicly available at http://www.therminfo.com or\nhttp://therminfo.lasige.di.fc.ul.pt. ThermInfo's strength lies in the data\nquality, availability (free access), search capabilities, and, in particular,\nprediction ability, based on a user-friendly interface that accepts inputs in\nseveral formats.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2013 14:56:51 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Teixeira", "Ana L.", ""], ["Santos", "Rui C.", ""], ["Leal", "Joao P.", ""], ["Simoes", "Jose A. Martinho", ""], ["Falcao", "Andre O.", ""]]}, {"id": "1302.1400", "submitter": "Abdesslem Layeb", "authors": "Abdesslem Layeb, Amira Boudra, Wissem Korichi, Salim Chikhi", "title": "A new greedy randomized adaptive search procedure for multiobjective RNA\n  structural alignment", "comments": null, "journal-ref": "International Journal in Foundations of Computer Science &\n  Technology (IJFCST), Vol. 3, No.1,pp. 9-24, January 2013", "doi": "10.5121/ijfcst.2013.3102", "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RNA secondary structures prediction is one of the main issues in\nbioinformatics. It seeks to elucidate structural conserved regions within a set\nof RNA sequences. Unfortunately, finding an accurate conserved structure is a\nvery hard task to do. Within the present study, the prediction problem is\nconsidered as a multiobjective optimization process in which the structural\nconservation and the sensitivity of the multiple alignment are optimized. The\nproposed method called GRASPMORSA is based on an aggregate function and GRASP\nprocedure. The initial solutions are obtained by using a random progressive\nlocal/ global algorithm, and then they are refined by an iterative realignment.\nExperiments within a large scale of data have shown the efficacy and\neffectiveness of the proposed method and its capacity to reach good quality\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2013 15:13:03 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Layeb", "Abdesslem", ""], ["Boudra", "Amira", ""], ["Korichi", "Wissem", ""], ["Chikhi", "Salim", ""]]}, {"id": "1302.1733", "submitter": "Llu\\'is Belanche-Mu\\~noz", "authors": "Fernando Gonz\\'alez, Llu\\'is A. Belanche", "title": "Feature Selection for Microarray Gene Expression Data using Simulated\n  Annealing guided by the Multivariate Joint Entropy", "comments": "12 pages, 6 Tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a new way to calculate the multivariate joint entropy is\npresented. This measure is the basis for a fast information-theoretic based\nevaluation of gene relevance in a Microarray Gene Expression data context. Its\nlow complexity is based on the reuse of previous computations to calculate\ncurrent feature relevance. The mu-TAFS algorithm --named as such to\ndifferentiate it from previous TAFS algorithms-- implements a simulated\nannealing technique specially designed for feature subset selection. The\nalgorithm is applied to the maximization of gene subset relevance in several\npublic-domain microarray data sets. The experimental results show a notoriously\nhigh classification performance and low size subsets formed by biologically\nmeaningful genes.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2013 12:49:57 GMT"}], "update_date": "2013-02-08", "authors_parsed": [["Gonz\u00e1lez", "Fernando", ""], ["Belanche", "Llu\u00eds A.", ""]]}, {"id": "1302.3123", "submitter": "Nizar Banu P K", "authors": "P. K. Nizar Banu, H. Hannah Inbarani", "title": "An Analysis of Gene Expression Data using Penalized Fuzzy C-Means\n  Approach", "comments": "14; IJCCI, Vol. 1, Issue 2,(January-July)2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid advances of microarray technologies, large amounts of\nhigh-dimensional gene expression data are being generated, which poses\nsignificant computational challenges. A first step towards addressing this\nchallenge is the use of clustering techniques, which is essential in the data\nmining process to reveal natural structures and identify interesting patterns\nin the underlying data. A robust gene expression clustering approach to\nminimize undesirable clustering is proposed. In this paper, Penalized Fuzzy\nC-Means (PFCM) Clustering algorithm is described and compared with the most\nrepresentative off-line clustering techniques: K-Means Clustering, Rough\nK-Means Clustering and Fuzzy C-Means clustering. These techniques are\nimplemented and tested for a Brain Tumor gene expression Dataset. Analysis of\nthe performance of the proposed approach is presented through qualitative\nvalidation experiments. From experimental results, it can be observed that\nPenalized Fuzzy C-Means algorithm shows a much higher usability than the other\nprojected clustering algorithms used in our comparison study. Significant and\npromising clustering results are presented using Brain Tumor Gene expression\ndataset. Thus patterns seen in genome-wide expression experiments can be\ninterpreted as indications of the status of cellular processes. In these\nclustering results, we find that Penalized Fuzzy C-Means algorithm provides\nuseful information as an aid to diagnosis in oncology.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 17:16:39 GMT"}], "update_date": "2013-02-14", "authors_parsed": [["Banu", "P. K. Nizar", ""], ["Inbarani", "H. Hannah", ""]]}, {"id": "1302.3663", "submitter": "David Bortz", "authors": "Jason F. Hammond, Elizabeth J. Stewart, John G. Younger, Michael J.\n  Solomon, David M. Bortz", "title": "Spatially Heterogeneous Biofilm Simulations using an Immersed Boundary\n  Method with Lagrangian Nodes Defined by Bacterial Locations", "comments": "43 pages, 18 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider how surface-adherent bacterial biofilm communities\nrespond in flowing systems. We simulate the fluid-structure interaction and\nseparation process using the immersed boundary method. In these simulations we\nmodel and simulate different density and viscosity values of the biofilm than\nthat of the surrounding fluid. The simulation also includes breakable springs\nconnecting the bacteria in the biofilm. This allows the inclusion of erosion\nand detachment into the simulation. We use the incompressible Navier-Stokes\n(N-S) equations to describe the motion of the flowing fluid. We discretize the\nfluid equations using finite differences and use a geometric multigrid method\nto solve the resulting equations at each time step. The use of multigrid is\nnecessary because of the dramatically different densities and viscosities\nbetween the biofilm and the surrounding fluid. We investigate and simulate the\nmodel in both two and three dimensions.\n  Our method differs from previous attempts of using IBM for modeling\nbiofilm/flow interactions in the following ways: the density and viscosity of\nthe biofilm can differ from the surrounding fluid, and the Lagrangian node\nlocations correspond to experimentally measured bacterial cell locations from\n3D images taken of Staphylococcus epidermidis in a biofilm.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2013 02:30:11 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Hammond", "Jason F.", ""], ["Stewart", "Elizabeth J.", ""], ["Younger", "John G.", ""], ["Solomon", "Michael J.", ""], ["Bortz", "David M.", ""]]}, {"id": "1302.4000", "submitter": "Micha{\\l} Jamr\\'oz", "authors": "Jamr\\'oz Micha{\\l}, Koli\\'nski Andrzej", "title": "ClusCo: clustering and comparison of protein models", "comments": null, "journal-ref": null, "doi": "10.1186/1471-2105-14-62", "report-no": null, "categories": "q-bio.BM cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The development, optimization and validation of protein modeling\nmethods require efficient tools for structural comparison. Frequently, a large\nnumber of models need to be compared with the target native structure. The main\nreason for the development of Clusco software was to create a high-throughput\ntool for all-versus-all comparison, because calculating similarity matrix is\nthe one of the bottlenecks in the protein modeling pipeline. Results: Clusco is\nfast and easy-to-use software for high-throughput comparison of protein models\nwith different similarity measures (cRMSD, dRMSD, GDT_TS, TM-Score, MaxSub,\nContact Map Overlap) and clustering of the comparison results with standard\nmethods: K-means Clustering or Hierarchical Agglomerative Clustering.\nConclusions: The application was highly optimized and written in C/C++,\nincluding the code for parallel execution on CPU and GPU version of cRMSD,\nwhich resulted in a significant speedup over similar clustering and scoring\ncomputation programs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2013 21:11:35 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 14:55:55 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Micha\u0142", "Jamr\u00f3z", ""], ["Andrzej", "Koli\u0144ski", ""]]}, {"id": "1302.4136", "submitter": "David  Gao", "authors": "Kun Cai, David Y. Gao, and Qing H. Qin", "title": "Post-buckling Solutions of Hyper-elastic Beam by Canonical Dual Finite\n  Element Method", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post buckling problem of a large deformed beam is analyzed using canonical\ndual finite element method (CD-FEM). The feature of this method is to choose\ncorrectly the canonical dual stress so that the original non-convex potential\nenergy functional is reformulated in a mixed complementary energy form with\nboth displacement and stress fields, and a pure complementary energy is\nexplicitly formulated in finite dimensional space. Based on the canonical\nduality theory and the associated triality theorem, a primal-dual algorithm is\nproposed, which can be used to find all possible solutions of this nonconvex\npost-buckling problem. Numerical results show that the global maximum of the\npure-complementary energy leads to a stable buckled configuration of the beam.\nWhile the local extrema of the pure-complementary energy present unstable\ndeformation states, especially. We discovered that the unstable buckled state\nis very sensitive to the number of total elements and the external loads.\nTheoretical results are verified through numerical examples and some\ninteresting phenomena in post-bifurcation of this large deformed beam are\nobserved.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2013 23:53:52 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Cai", "Kun", ""], ["Gao", "David Y.", ""], ["Qin", "Qing H.", ""]]}, {"id": "1302.4332", "submitter": "Paolo Bientinesi", "authors": "Lucas Beyer (1), Paolo Bientinesi (1), ((1) AICES, RWTH Aachen)", "title": "Streaming Data from HDD to GPUs for Sustained Peak Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": "AICES-2013/02-1", "categories": "cs.DC cs.CE cs.MS q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the genome-wide association studies (GWAS), one has to\nsolve long sequences of generalized least-squares problems; such a task has two\nlimiting factors: execution time --often in the range of days or weeks-- and\ndata management --data sets in the order of Terabytes. We present an algorithm\nthat obviates both issues. By pipelining the computation, and thanks to a\nsophisticated transfer strategy, we stream data from hard disk to main memory\nto GPUs and achieve sustained peak performance; with respect to a\nhighly-optimized CPU implementation, our algorithm shows a speedup of 2.6x.\nMoreover, the approach lends itself to multiple GPUs and attains almost perfect\nscalability. When using 4 GPUs, we observe speedups of 9x over the\naforementioned implementation, and 488x over a widespread biology library.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 16:03:08 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Beyer", "Lucas", "", "AICES, RWTH Aachen"], ["Bientinesi", "Paolo", "", "AICES, RWTH Aachen"]]}, {"id": "1302.4391", "submitter": "Mohammadreza Ghodsi", "authors": "Mohammadreza Ghodsi", "title": "Constructing a genome assembly that has the maximum likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate genome assembly problem as an optimization problem in which the\nobjective function is the likelihood of the assembly given the reads.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2013 19:14:38 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2013 15:43:10 GMT"}, {"version": "v3", "created": "Thu, 7 Apr 2016 06:14:03 GMT"}], "update_date": "2016-04-08", "authors_parsed": [["Ghodsi", "Mohammadreza", ""]]}, {"id": "1302.4557", "submitter": "Kirana Kumara P", "authors": "Kirana Kumara P", "title": "Extracting Three Dimensional Surface Model of Human Kidney from the\n  Visible Human Data Set using Free Software", "comments": "14 pages, 8 figures, accepted version", "journal-ref": "Leonardo Electronic Journal of Practices and Technologies (LEJPT),\n  Issue 20 (January-June), 2012 (11), p. 115-126 [ISSN: 1583-1078]", "doi": null, "report-no": null, "categories": "physics.med-ph cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three dimensional digital model of a representative human kidney is needed\nfor a surgical simulator that is capable of simulating a laparoscopic surgery\ninvolving kidney. Buying a three dimensional computer model of a representative\nhuman kidney, or reconstructing a human kidney from an image sequence using\ncommercial software, both involve (sometimes significant amount of) money. In\nthis paper, author has shown that one can obtain a three dimensional surface\nmodel of human kidney by making use of images from the Visible Human Data Set\nand a few free software packages (ImageJ, ITK-SNAP, and MeshLab in particular).\nImages from the Visible Human Data Set, and the software packages used here,\nboth do not cost anything. Hence, the practice of extracting the geometry of a\nrepresentative human kidney for free, as illustrated in the present work, could\nbe a free alternative to the use of expensive commercial software or to the\npurchase of a digital model.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2013 09:44:51 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["P", "Kirana Kumara", ""]]}, {"id": "1302.5150", "submitter": "Shigeki Matsutani", "authors": "Shigeki Matsutani and Yoshiyuki Shimosako", "title": "Measuring Agglomeration of Agglomerated Particles Pictures", "comments": "agglomeration, digital image processing procedure, Euler number", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math-ph math.AT math.MP math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce a novel geometrical index $\\delta_{agg}$, which\nis associated with the Euler number and is obtained by an image processing\nprocedure for a given digital picture of aggregated particles such that\n$\\delta_{agg}$ exhibits the degree of the agglomerations of the particles. In\nthe previous work (Matsutani, Shimosako, Wang, Appl.Math.Modeling {\\bf{37}}\n(2013), 4007-4022), we proposed an algorithm to construct a picture of\nagglomerated particles as a Monte-Carlo simulation whose agglomeration degree\nis controlled by $\\gamma_{agg} \\in (0,1)$. By applying the image processing\nprocedure to the pictures of the agglomeration particles constructed following\nthe algorithm, we show that $\\delta_{agg}$ statistically reproduces the\nagglomeration parameter $\\gamma_{agg}$.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2013 23:57:10 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2013 04:03:27 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2013 08:00:36 GMT"}, {"version": "v4", "created": "Sun, 30 Jun 2013 23:25:53 GMT"}], "update_date": "2013-07-05", "authors_parsed": [["Matsutani", "Shigeki", ""], ["Shimosako", "Yoshiyuki", ""]]}, {"id": "1302.5941", "submitter": "Harry Boyer", "authors": "Milorad Bojic, Alexandre Patou Parvedy, Harry Boyer (PIMENT)", "title": "Optimization of thermal comfort in building through envelope design", "comments": null, "journal-ref": "International Conference on Efficiency, Cost, Optimization,\n  Simulation and Environmental Impact of Energy Systems, ECOS 2012, Perigia :\n  Italy (2012)", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the current environmental situation, energy saving has become the\nleading drive in modern research. Although the residential houses in tropical\nclimate do not use air conditioning to maintain thermal comfort in order to\navoid use of electricity. As the thermal comfort is maintained by adequate\nenvelope composition and natural ventilation, this paper shows that it is\npossible to determine the thickness of envelope layers for which the best\nthermal comfort is obtained. The building is modeled in EnergyPlus software and\nHookeJeves optimization methodology. The investigated house is a typical\nresidential house one-storey high with five thermal zones located at Reunion\nIsland, France. Three optimizations are performed such as the optimization of\nthe thickness of the concrete block layer, of the wood layer, and that of the\nthermal insulation layer. The results show optimal thickness of thermal\nenvelope layers that yield the maximum TC according to Fanger predicted mean\nvote.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2013 19:23:56 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Bojic", "Milorad", "", "PIMENT"], ["Parvedy", "Alexandre Patou", "", "PIMENT"], ["Boyer", "Harry", "", "PIMENT"]]}, {"id": "1302.5942", "submitter": "Harry Boyer", "authors": "Milorad Boji\\'c, Dragan Cvetkovic, Jasmina Skerli\\'c, Danijela\n  Nikoli\\'c, Harry Boyer (PIMENT)", "title": "Performances of Low Temperature Radiant Heating Systems", "comments": "Second International Conference on Building Energy and Environment,\n  COBEE 2012, Colorado : United States (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low temperature heating panel systems offer distinctive advantages in terms\nof thermal comfort and energy consumption, allowing work with low exergy\nsources. The purpose of this paper is to compare floor, wall, ceiling, and\nfloor-ceiling panel heating systems in terms of energy, exergy and CO2\nemissions. Simulation results for each of the analyzed panel system are given\nby its energy (the consumption of gas for heating, electricity for pumps and\nprimary energy) and exergy consumption, the price of heating, and its carbon\ndioxide emission. Then, the values of the air temperatures of rooms are\ninvestigated and that of the surrounding walls and floors. It is found that the\nfloor-ceiling heating system has the lowest energy, exergy, CO2 emissions,\noperating costs, and uses boiler of the lowest power. The worst system by all\nthese parameters is the classical ceiling heating\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2013 19:24:26 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Boji\u0107", "Milorad", "", "PIMENT"], ["Cvetkovic", "Dragan", "", "PIMENT"], ["Skerli\u0107", "Jasmina", "", "PIMENT"], ["Nikoli\u0107", "Danijela", "", "PIMENT"], ["Boyer", "Harry", "", "PIMENT"]]}, {"id": "1302.6030", "submitter": "Srikrishnan Divakaran", "authors": "Srikrishnan Divakaran, Arpit Mithal, and Namit Jain", "title": "A Fast Template Based Heuristic For Global Multiple Sequence Alignment", "comments": "20pages, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.QM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Advances in bio-technology have made available massive amounts of functional,\nstructural and genomic data for many biological sequences. This increased\navailability of heterogeneous biological data has resulted in biological\napplications where a multiple sequence alignment (msa) is required for aligning\nsimilar features, where a feature is described in structural, functional or\nevolutionary terms. In these applications, for a given set of sequences,\ndepending on the feature of interest the optimal msa is likely to be different,\nand sequence similarity can only be used as a rough initial estimate on the\naccuracy of an msa. This has motivated the growth in template based heuristics\nthat supplement the sequence information with evolutionary, structural and\nfunctional data and exploit feature similarity instead of sequence similarity\nto construct multiple sequence alignments that are biologically more accurate.\nHowever, current frameworks for designing template based heuristics do not\nallow the user to explicitly specify information that can help to classify\nfeatures into types and associate weights signifying the relative importance of\na feature with respect to other features. In this paper, we first provide a\nmechanism where as a part of the template information the user can explicitly\nspecify for each feature, its type, and weight. The type is to classify the\nfeatures into different categories based on their characteristics and the\nweight signifies the relative importance of a feature with respect to other\nfeatures in that sequence. Second, we exploit the above information to define\nscoring models for pair-wise sequence alignment that assume segment\nconservation as opposed to single character (residue) conservation. Finally, we\npresent a fast progressive alignment based heuristic framework that helps in\nconstructing a global msa efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2013 09:55:16 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Divakaran", "Srikrishnan", ""], ["Mithal", "Arpit", ""], ["Jain", "Namit", ""]]}]