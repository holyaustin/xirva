[{"id": "1402.0247", "submitter": "Rumaisah Munir", "authors": "Ms. Rumaisah Munir", "title": "Secure Debit Card Device Model", "comments": "Royal Institute of Technology, KTH", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The project envisages the implementation of an e-payment system utilizing\nFIPS-201 Smart Card. The system combines hardware and software modules. The\nhardware module takes data insertions (e.g. currency notes), processes the data\nand then creates connection with the smart card using serial/USB ports to\nperform further mathematical manipulations. The hardware interacts with servers\nat the back for authentication and identification of users and for data storage\npertaining to a particular user. The software module manages database, handles\nidentities, provide authentication and secure communication between the various\nsystem components. It will also provide a component to the end users. This\ncomponent can be in the form of software for computer or executable binaries\nfor PoS devices. The idea is to receive data in the embedded system from data\nreader and smart card. After manipulations, the updated data is imprinted on\nsmart card memory and also updated in the back end servers maintaining\ndatabase. The information to be sent to a server is sent through a PoS device\nwhich has multiple transfer mediums involving wired and un-wired mediums. The\nuser device also acts as an updater; therefore, whenever the smart card is\ninserted by user, it is automatically updated by synchronizing with back-end\ndatabase. The project required expertise in embedded systems, networks, java\nand C++ (Optional).\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2014 21:08:07 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Munir", "Ms. Rumaisah", ""]]}, {"id": "1402.0362", "submitter": "S\\'ebastien Mathieu", "authors": "S\\'ebastien Mathieu, Quentin Louveaux, Damien Ernst and Bertrand\n  Corn\\'elusse", "title": "A quantitative analysis of the effect of flexible loads on reserve\n  markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a day-ahead reserve market model that handles bids\nfrom flexible loads. This pool market model takes into account the fact that a\nload modulation in one direction must usually be compensated later by a\nmodulation of the same magnitude in the opposite direction. Our analysis takes\ninto account the gaming possibilities of producers and retailers, controlling\nload flexibility, in the day-ahead energy and reserve markets, and in imbalance\nsettlement. This analysis is carried out by an agent-based approach where, for\nevery round, each actor uses linear programs to maximize its profit according\nto forecasts of the prices. The procurement of a reserve is assumed to be\ndetermined, for each period, as a fixed percentage of the total consumption\ncleared in the energy market for the same period. The results show that the\nprovision of reserves by flexible loads has a negligible impact on the energy\nmarket prices but markedly decreases the cost of reserve procurement. However,\nas the rate of flexible loads increases, the system operator has to rely more\nand more on non-contracted reserves, which may cancel out the benefits made in\nthe procurement of reserves.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 12:34:25 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 09:51:02 GMT"}], "update_date": "2014-05-28", "authors_parsed": [["Mathieu", "S\u00e9bastien", ""], ["Louveaux", "Quentin", ""], ["Ernst", "Damien", ""], ["Corn\u00e9lusse", "Bertrand", ""]]}, {"id": "1402.0429", "submitter": "Tabrez Ali", "authors": "S. Tabrez Ali", "title": "Defmod - Parallel multiphysics finite element code for modeling crustal\n  deformation during the earthquake/rifting cycle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present Defmod, an open source, fully unstructured, two\nor three dimensional, parallel finite element code for modeling crustal\ndeformation over time scales ranging from milliseconds to thousands of years.\nUnlike existing public domain numerical codes, Defmod can simulate deformation\ndue to all major processes that make up the earthquake/rifting cycle, in\nnon-homogeneous media. Specifically, it can be used to model deformation due to\ndynamic and quasistatic processes such as co-seismic slip or dike intrusion(s),\nporoelastic rebound due to fluid flow and post-seismic or post-rifting\nviscoelastic relaxation. It can also be used to model deformation due to\nprocesses such as post-glacial rebound, hydrological (un)loading, injection\nand/or withdrawal of fluids from subsurface reservoirs etc. Defmod is written\nin Fortran 95 and uses PETSc's parallel sparse data structures and implicit\nsolvers. Problems can be solved using (stabilized) linear triangular,\nquadrilateral, tetrahedral or hexahedral elements on shared or distributed\nmemory machines with hundreds or even thousands of processor cores. In the\ncurrent version of the code, prescribed loading is supported. Results are\nwritten in ASCII VTK format for easy visualization. The source code is released\nunder the terms of GNU General Public License (v3.0) and is freely available\nfrom https://bitbucket.org/stali/defmod/.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 16:55:11 GMT"}, {"version": "v2", "created": "Mon, 14 Jul 2014 18:57:13 GMT"}, {"version": "v3", "created": "Wed, 30 Dec 2015 19:19:20 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Ali", "S. Tabrez", ""]]}, {"id": "1402.0936", "submitter": "Ahmadreza Baghaie", "authors": "Ahmadreza Baghaie, Zeyun Yu", "title": "An Optimization Method For Slice Interpolation Of Medical Images", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slice interpolation is a fast growing field in medical image processing.\nIntensity-based interpolation and object-based interpolation are two major\ngroups of methods in the literature. In this paper, we describe an\nobject-oriented, optimization method based on a modified version of\ncurvature-based image registration, in which a displacement field is computed\nfor the missing slice between two known slices and used to interpolate the\nintensities of the missing slice. The proposed approach is evaluated\nquantitatively by using the Mean Squared Difference (MSD) as a metric. The\nproduced results also show visual improvement in preserving sharp edges in\nimages.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2014 05:31:59 GMT"}, {"version": "v2", "created": "Thu, 27 Mar 2014 07:12:28 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Baghaie", "Ahmadreza", ""], ["Yu", "Zeyun", ""]]}, {"id": "1402.1467", "submitter": "Evgeny Nikulchev", "authors": "Evgeny Nikulchev", "title": "Reconstruction Models for Attractors in the Technical and Economic\n  Processes", "comments": "5 pages", "journal-ref": "International Journal of Computer Trends and Technology. 2013. V.6\n  N.3. P.171-175", "doi": "10.14445/22312803/IJCTT-V6N3P128", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article discusses building models based on the reconstructed attractors\nof the time series. Discusses the use of the properties of dynamical chaos,\nnamely to identify the strange attractors structure models. Here is used the\ngroup properties of differential equations, which consist in the symmetry of\nparticular solutions. Examples of modeling engineering systems are given.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 19:54:39 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Nikulchev", "Evgeny", ""]]}, {"id": "1402.1485", "submitter": "Jan Sykora", "authors": "Jan S\\'ykora and Anna Ku\\v{c}erov\\'a", "title": "Uncertainty Propagation in Elasto-Plastic Material", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Macroscopically heterogeneous materials, characterised mostly by comparable\nheterogeneity lengthscale and structural sizes, can no longer be modelled by\ndeterministic approach instead. It is convenient to introduce stochastic\napproach with uncertain material parameters quantified as random fields and/or\nrandom variables. The present contribution is devoted to propagation of these\nuncertainties in mechanical modelling of inelastic behaviour. In such case the\nMonte Carlo method is the traditional approach for solving the proposed\nproblem. Nevertheless, convergence rate is relatively slow, thus new methods\n(e.g. stochastic Galerkin method, stochastic collocation approach, etc.) have\nbeen recently developed to offer fast convergence for sufficiently smooth\nsolution in the probability space. Our goal is to accelerate the uncertainty\npropagation using a polynomial chaos expansion based on stochastic collocation\nmethod. The whole concept is demonstrated on a simple numerical example of\nuniaxial test at a material point where interesting phenomena can be clearly\nunderstood.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 20:55:49 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["S\u00fdkora", "Jan", ""], ["Ku\u010derov\u00e1", "Anna", ""]]}, {"id": "1402.1523", "submitter": "Val\\'erio Ramos Batista", "authors": "Antonio Elias Fabris, Marcelo Zanchetta do Nascimento, Val\\'erio Ramos\n  Batista", "title": "Programming plantation lines on driverless tractors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Agricultural Engineering include image processing,\nrobotics and geographic information systems (GIS). Some tasks are still\naccomplished manually, like drawing plantation lines that optimize\nproductivity. Herewith we present an algorithm to find the optimal plantation\nlines in linear time. The algorithm is based upon classical results of Geometry\nwhich enabled a source code with only 573 lines. We have implemented it in\nMatlab for sugar cane, and it can be easily adapted to other crops like coffee,\nmaize and soy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 23:14:58 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Fabris", "Antonio Elias", ""], ["Nascimento", "Marcelo Zanchetta do", ""], ["Batista", "Val\u00e9rio Ramos", ""]]}, {"id": "1402.1635", "submitter": "Wasantha Samarathunga", "authors": "Wasantha Samarathunga, Masatoshi Seki, Hidenobu Saito, Ken Ichiryu,\n  Yasuhiro Ohyama", "title": "Product Evaluation In Elliptical Helical Pipe Bending", "comments": null, "journal-ref": "International Journal of Computer Trends and Technology, volume 4\n  Issue 10 Oct 2013", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes a computation approach to address the evaluation of\nend product machining accuracy in elliptical surfaced helical pipe bending\nusing 6dof parallel manipulator as a pipe bender. The target end product is\nwearable metal muscle supporters used in build-to-order welfare product\nmanufacturing. This paper proposes a product testing model that mainly corrects\nthe surface direction estimation errors of existing least squares ellipse\nfittings, followed by arc length and central angle evaluations. This\npost-machining modelling requires combination of reverse rotations and\ntranslations to a specific location before accuracy evaluation takes place,\ni.e. the reverse comparing to pre-machining product modelling. This specific\nlocation not only allows us to compute surface direction but also the amount of\nexcessive surface twisting as a rotation angle about a specified axis, i.e.\nquantification of surface torsion. At first we experimented three ellipse\nfitting methods such as, two least-squares fitting methods with Bookstein\nconstraint and Trace constraint, and one non- linear least squares method using\nGauss-Newton algorithm. From fitting results, we found that using Trace\nconstraint is more reliable and designed a correction filter for surface\ntorsion observation. Finally we apply 2D total least squares line fitting\nmethod with a rectification filter for surface direction detection.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 13:38:20 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Samarathunga", "Wasantha", ""], ["Seki", "Masatoshi", ""], ["Saito", "Hidenobu", ""], ["Ichiryu", "Ken", ""], ["Ohyama", "Yasuhiro", ""]]}, {"id": "1402.1637", "submitter": "Wasantha Samarathunga", "authors": "Wasantha Samarathunga, Masatoshi Seki, Hidenobu Saito, Ken Ichiryu,\n  Yasuhiro Ohyama", "title": "Vertical Clustering of 3D Elliptical Helical Data", "comments": null, "journal-ref": "International Journal of Computer Trends and Technology, volume 6\n  number 2,Dec 2013", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research proposes an effective vertical clustering strategy of 3D data\nin an elliptical helical shape based on 2D geometry. The clustering object is\nan elliptical cross-sectioned metal pipe which is been bended in to an\nelliptical helical shape which is used in wearable muscle support designing for\nwelfare industry. The aim of this proposed method is to maximize the vertical\nclustering (vertical partitioning) ability of surface data in order to run the\nproduct evaluation process addressed in research [2]. The experiment results\nprove that the proposed method outperforms the existing threshold no of\nclusters that preserves the vertical shape than applying the conventional 3D\ndata. This research also proposes a new product testing strategy that provides\nthe flexibility in computer aided testing by not restricting the sequence\ndepending measurements which apply weight on measuring process. The clustering\nalgorithms used for the experiments in this research are self-organizing map\n(SOM) and K-medoids.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 13:42:54 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Samarathunga", "Wasantha", ""], ["Seki", "Masatoshi", ""], ["Saito", "Hidenobu", ""], ["Ichiryu", "Ken", ""], ["Ohyama", "Yasuhiro", ""]]}, {"id": "1402.1652", "submitter": "Tobias Kretz", "authors": "Vidal Roca, Vicente Torres, Tobias Kretz, Karsten Lehmann, Ingmar\n  Hofs\\\"a{\\ss}", "title": "How to Apply Assignment Methods that were Developed for Vehicular\n  Traffic to Pedestrian Microsimulations", "comments": "contribution to PANAM 2014 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.MA physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying assignment methods to compute user-equilibrium route choice is very\ncommon in traffic planning. It is common sense that vehicular traffic arranges\nin a user-equilibrium based on generalized costs in which travel time is a\nmajor factor. Surprisingly travel time has not received much attention for the\nroute choice of pedestrians. In microscopic simulations of pedestrians the\nvastly dominating paradigm for the computation of the preferred walking\ndirection is set into the direction of the (spatially) shortest path. For\nsituations where pedestrians have travel time as primary determinant for their\nwalking behavior it would be desirable to also have an assignment method in\npedestrian simulations. To apply existing (road traffic) assignment methods\nwith simulations of pedestrians one has to reduce the nondenumerably many\npossible pedestrian trajectories to a small subset of routes which represent\nthe main, relevant, and significantly distinguished routing alternatives. All\nexcept one of these routes will mark detours, i.e. not the shortest connection\nbetween origin and destination. The proposed assignment method is intended to\nwork with common operational models of pedestrian dynamics. These - as\nmentioned before - usually send pedestrians into the direction of the spatially\nshortest path. Thus, all detouring routes have to be equipped with intermediate\ndestinations, such that pedestrians can do a detour as a piecewise connection\nof segments on which they walk into the direction of the shortest path. One has\nthen to take care that the transgression from one segment to the following one\nno artifacts are introduced into the pedestrian trajectory.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 14:38:46 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Roca", "Vidal", ""], ["Torres", "Vicente", ""], ["Kretz", "Tobias", ""], ["Lehmann", "Karsten", ""], ["Hofs\u00e4\u00df", "Ingmar", ""]]}, {"id": "1402.1718", "submitter": "Nicolas Courtois", "authors": "Nicolas T. Courtois, Lear Bahack", "title": "On Subversive Miner Strategies and Block Withholding Attack in Bitcoin\n  Digital Currency", "comments": "not published elsewhere", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin is a \"crypto currency\", a decentralized electronic payment scheme\nbased on cryptography. Bitcoin economy grows at an incredibly fast rate and is\nnow worth some 10 billions of dollars. Bitcoin mining is an activity which\nconsists of creating (minting) the new coins which are later put into\ncirculation. Miners spend electricity on solving cryptographic puzzles and they\nare also gatekeepers which validate bitcoin transactions of other people.\nMiners are expected to be honest and have some incentives to behave well.\nHowever. In this paper we look at the miner strategies with particular\nattention paid to subversive and dishonest strategies or those which could put\nbitcoin and its reputation in danger. We study in details several recent\nattacks in which dishonest miners obtain a higher reward than their relative\ncontribution to the network. In particular we revisit the concept of block\nwithholding attacks and propose a new concrete and practical block withholding\nattack which we show to maximize the advantage gained by rogue miners.\n  RECENT EVENTS: it seems that the attack was recently executed, see Section\nXI-A.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2014 13:54:02 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2014 14:20:19 GMT"}, {"version": "v3", "created": "Sun, 16 Mar 2014 12:11:10 GMT"}, {"version": "v4", "created": "Sun, 6 Jul 2014 23:14:56 GMT"}, {"version": "v5", "created": "Tue, 2 Dec 2014 16:45:26 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Courtois", "Nicolas T.", ""], ["Bahack", "Lear", ""]]}, {"id": "1402.1736", "submitter": "Eric Vanden-Eijnden", "authors": "Maria Cameron and Eric Vanden-Eijnden", "title": "Flows in Complex Networks: Theory, Algorithms, and Application to\n  Lennard-Jones Cluster Rearrangement", "comments": "32 pages, 13 figures", "journal-ref": null, "doi": "10.1007/s10955-014-0997-8", "report-no": null, "categories": "cond-mat.stat-mech cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A set of analytical and computational tools based on transition path theory\n(TPT) is proposed to analyze flows in complex networks. Specifically, TPT is\nused to study the statistical properties of the reactive trajectories by which\ntransitions occur between specific groups of nodes on the network. Sampling\ntools are built upon the outputs of TPT that allow to generate these reactive\ntrajectories directly, or even transition paths that travel from one group of\nnodes to the other without making any detour and carry the same probability\ncurrent as the reactive trajectories. These objects permit to characterize the\nmechanism of the transitions, for example by quantifying the width of the tubes\nby which these transitions occur, the location and distribution of their\ndynamical bottlenecks, etc. These tools are applied to a network modeling the\ndynamics of the Lennard-Jones cluster with 38 atoms (LJ38) and used to\nunderstand the mechanism by which this cluster rearranges itself between its\ntwo most likely states at various temperatures.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2014 10:00:25 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Cameron", "Maria", ""], ["Vanden-Eijnden", "Eric", ""]]}, {"id": "1402.1794", "submitter": "Jesse Meyer", "authors": "Jesse G. Meyer", "title": "In silico Proteome Cleavage Reveals Iterative Digestion Strategy for\n  High Sequence Coverage", "comments": "10 pages of text/references followed by figure/table legends, six\n  figures, and one table", "journal-ref": "ISRN Computational Biology 2014", "doi": "10.1155/2014/960902", "report-no": null, "categories": "q-bio.GN cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the post-genome era, biologists have sought to measure the complete\ncomplement of proteins, termed proteomics. Currently, the most effective method\nto measure the proteome is with shotgun, or bottom-up, proteomics, in which the\nproteome is digested into peptides that are identified followed by protein\ninference. Despite continuous improvements to all steps of the shotgun\nproteomics workflow, observed proteome coverage is often low; some proteins are\nidentified by a single peptide sequence. Complete proteome sequence coverage\nwould allow comprehensive characterization of RNA splicing variants and all\npost translational modifications, which would drastically improve the accuracy\nof biological models. There are many reasons for the sequence coverage deficit,\nbut ultimately peptide length determines sequence observability. Peptides that\nare too short are lost because they match many protein sequences and their true\norigin is ambiguous. The maximum observable peptide length is determined by\nseveral analytical challenges. This paper explores computationally how peptide\nlengths produced from several common proteome digestion methods limit\nobservable proteome coverage. Iterative proteome cleavage strategies are also\nexplored. These simulations reveal that maximized proteome coverage can be\nachieved by use of an iterative digestion protocol involving multiple proteases\nand chemical cleavages that theoretically allow 91.1% proteome coverage.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 23:13:48 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Meyer", "Jesse G.", ""]]}, {"id": "1402.1881", "submitter": "Bo Chen", "authors": "Shuyu Zhou, Xiandong Zhang, Bo Chen, Steef van de Velde", "title": "Tactical Fixed Job Scheduling with Spread-Time Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the tactical fixed job scheduling problem with spread-time\nconstraints. In such a problem, there are a fixed number of classes of machines\nand a fixed number of groups of jobs. Jobs of the same group can only be\nprocessed by machines of a given set of classes. All jobs have their fixed\nstart and end times. Each machine is associated with a cost according to its\nmachine class. Machines have spread-time constraints, with which each machine\nis only available for $L$ consecutive time units from the start time of the\nearliest job assigned to it. The objective is to minimize the total cost of the\nmachines used to process all the jobs. For this strongly NP-hard problem, we\ndevelop a branch-and-price algorithm, which solves instances with up to $300$\njobs, as compared with CPLEX, which cannot solve instances of $100$ jobs. We\nfurther investigate the influence of machine flexibility by computational\nexperiments. Our results show that limited machine flexibility is sufficient in\nmost situations.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2014 19:30:16 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Zhou", "Shuyu", ""], ["Zhang", "Xiandong", ""], ["Chen", "Bo", ""], ["van de Velde", "Steef", ""]]}, {"id": "1402.2440", "submitter": "Regina Ammer", "authors": "Regina Ammer, Matthias Markl, Vera J\\\"uchter, Carolin K\\\"orner, Ulrich\n  R\\\"ude", "title": "Validation Experiments for LBM Simulations of Electron Beam Melting", "comments": "submitted to \"International Journal of Modern Physics C\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper validates 3D simulation results of electron beam melting (EBM)\nprocesses comparing experimental and numerical data. The physical setup is\npresented which is discretized by a three dimensional (3D) thermal lattice\nBoltzmann method (LBM). An experimental process window is used for the\nvalidation depending on the line energy injected into the metal powder bed and\nthe scan velocity of the electron beam. In the process window the EBM products\nare classified into the categories, porous, good and swelling, depending on the\nquality of the surface. The same parameter sets are used to generate a\nnumerical process window. A comparison of numerical and experimental process\nwindows shows a good agreement. This validates the EBM model and justifies\nsimulations for future improvements of EBM processes. In particular numerical\nsimulations can be used to explain future process window scenarios and find the\nbest parameter set for a good surface quality and dense products.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 10:59:00 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Ammer", "Regina", ""], ["Markl", "Matthias", ""], ["J\u00fcchter", "Vera", ""], ["K\u00f6rner", "Carolin", ""], ["R\u00fcde", "Ulrich", ""]]}, {"id": "1402.2453", "submitter": "Michele Piana", "authors": "Cristian Toraci, Gabriele Zaccaria, Stefano Ceriani, David Wilson,\n  Marco Fato, Michele Piana", "title": "Sliding window and compressive sensing for low-field dynamic magnetic\n  resonance imaging", "comments": "Submitted to IEEE Transactions on Medical Imaging", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an acquisition/processing procedure for image reconstruction in\ndynamic Magnetic Resonance Imaging (MRI). The approach requires sliding window\nto record a set of trajectories in the k-space, standard regularization to\nreconstruct an estimate of the object and compressed sensing to recover image\nresiduals. We validated this approach in the case of specific simulated\nexperiments and, in the case of real measurements, we showed that the procedure\nis reliable even in the case of data acquired by means of a low-field scanner.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 11:24:11 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Toraci", "Cristian", ""], ["Zaccaria", "Gabriele", ""], ["Ceriani", "Stefano", ""], ["Wilson", "David", ""], ["Fato", "Marco", ""], ["Piana", "Michele", ""]]}, {"id": "1402.2551", "submitter": "Aishwarya B U", "authors": "Aishwarya B U, Mohammed Saaqib A, Rajashree H R, Vigasini B", "title": "Modeling European Options", "comments": "arXiv admin note: substantial text overlap with arXiv:1311.0438 by\n  other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Option contracts can be valued by using the Black-Scholes equation, a partial\ndifferential equation with initial conditions. An exact solution for European\nstyle options is known. The computation time and the error need to be minimized\nsimultaneously. In this paper, the authors have solved the Black-Scholes\nequation by employing a reasonably accurate implicit method. Options with known\nanalytic solutions have been evaluated. Furthermore, an overall second order\naccurate space and time discretization has been accomplished in this paper.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 16:31:46 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["U", "Aishwarya B", ""], ["A", "Mohammed Saaqib", ""], ["R", "Rajashree H", ""], ["B", "Vigasini", ""]]}, {"id": "1402.2642", "submitter": "Marco Compagnoni", "authors": "Marco Compagnoni, Roberto Notari, Fabio Antonacci, Augusto Sarti", "title": "A comprehensive analysis of the geometry of TDOA maps in localisation\n  problems", "comments": "51 pages (3 appendices of 12 pages), 12 figures", "journal-ref": "Inverse Problems, Vol. 30, Number 3, Pages 035004, 2014", "doi": "10.1088/0266-5611/30/3/035004", "report-no": null, "categories": "math-ph cs.CE cs.SD gr-qc math.AC math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we consider the well-established problem of TDOA-based\nsource localization and propose a comprehensive analysis of its solutions for\narbitrary sensor measurements and placements. More specifically, we define the\nTDOA map from the physical space of source locations to the space of range\nmeasurements (TDOAs), in the specific case of three receivers in 2D space. We\nthen study the identifiability of the model, giving a complete analytical\ncharacterization of the image of this map and its invertibility. This analysis\nhas been conducted in a completely mathematical fashion, using many different\ntools which make it valid for every sensor configuration. These results are the\nfirst step towards the solution of more general problems involving, for\nexample, a larger number of sensors, uncertainty in their placement, or lack of\nsynchronization.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2014 18:08:20 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Compagnoni", "Marco", ""], ["Notari", "Roberto", ""], ["Antonacci", "Fabio", ""], ["Sarti", "Augusto", ""]]}, {"id": "1402.2845", "submitter": "Alex Gorodetsky", "authors": "Alex A. Gorodetsky, Youssef M. Marzouk", "title": "Efficient Localization of Discontinuities in Complex Computational\n  Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate models for computational simulations are input-output\napproximations that allow computationally intensive analyses, such as\nuncertainty propagation and inference, to be performed efficiently. When a\nsimulation output does not depend smoothly on its inputs, the error and\nconvergence rate of many approximation methods deteriorate substantially. This\npaper details a method for efficiently localizing discontinuities in the input\nparameter domain, so that the model output can be approximated as a piecewise\nsmooth function. The approach comprises an initialization phase, which uses\npolynomial annihilation to assign function values to different regions and thus\nseed an automated labeling procedure, followed by a refinement phase that\nadaptively updates a kernel support vector machine representation of the\nseparating surface via active learning. The overall approach avoids structured\ngrids and exploits any available simplicity in the geometry of the separating\nsurface, thus reducing the number of model evaluations required to localize the\ndiscontinuity. The method is illustrated on examples of up to eleven\ndimensions, including algebraic models and ODE/PDE systems, and demonstrates\nimproved scaling and efficiency over other discontinuity localization\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 15:16:11 GMT"}, {"version": "v2", "created": "Mon, 4 Aug 2014 18:06:35 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Gorodetsky", "Alex A.", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1402.3173", "submitter": "Jan Sykora", "authors": "Jan S\\'ykora and Michal \\v{S}ejnoha and Ji\\v{r}\\'i \\v{S}ejnoha", "title": "Homogenization of coupled heat and moisture transport in masonry\n  structures including interfaces", "comments": null, "journal-ref": null, "doi": "10.1016/j.amc.2011.02.050", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Homogenization of a simultaneous heat and moisture flow in a masonry wall is\npresented in this paper. The principle objective is to examine an impact of the\nassumed imperfect hydraulic contact on the resulting homogenized properties.\nSuch a contact is characterized by a certain mismatching resistance allowing us\nto represent a discontinuous evolution of temperature and moisture fields\nacross the interface, which is in general attributed to discontinuous capillary\npressures caused by different pore size distributions of the adjacent porous\nmaterials. In achieving this, two particular laboratory experiments were\nperformed to provide distributions of temperature and relative humidity in a\nsample of the masonry wall, which in turn served to extract the corresponding\njumps and subsequently to obtain the required interface transition parameters\nby matching numerical predictions and experimental results. The results suggest\na low importance of accounting for imperfect hydraulic contact for the\nderivation of macroscopic homogenized properties. On the other hand, they\nstrongly support the need for a fully coupled multi-scale analysis due to\nsignificant dependence of the homogenized properties on actual moisture\ngradients and corresponding values of both macroscopic temperature and relative\nhumidity.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 15:10:26 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["S\u00fdkora", "Jan", ""], ["\u0160ejnoha", "Michal", ""], ["\u0160ejnoha", "Ji\u0159\u00ed", ""]]}, {"id": "1402.3174", "submitter": "Jan Sykora", "authors": "J. S\\'ykora", "title": "Modeling of Degradation Processes in Historical Mortars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of presented paper is modeling of degradation processes in historical\nmortars exposed to moisture impact during freezing. Internal damage caused by\nice crystallization in pores is one of the most important factors limiting the\nservice life of historical structures. Coupling the transport processes with\nthe mechanical part will allow us to address the impact of moisture on the\ndurability, strength and stiffness of mortars. This should be accomplished with\nthe help of a complex thermo-hygro-mechanical model representing one of the\nprime objectives of this work. The proposed formulation is based on the\nextension of the classical poroelasticity models with the damage mechanics. An\nexample of two-dimensional moisture transport in the environment with\ntemperature below freezing point is presented to support the theoretical\nderivations.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2014 15:10:39 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["S\u00fdkora", "J.", ""]]}, {"id": "1402.3847", "submitter": "Daniele de Rigo", "authors": "Claudio Bosco, Daniele de Rigo, Olivier Dewitte and Luca Montanarella", "title": "Towards the reproducibility in soil erosion modeling: a new Pan-European\n  soil erosion map", "comments": "9 pages, from a poster presented at the Wageningen Conference on\n  Applied Soil Science \"Soil Science in a Changing World\", 18 - 22 September\n  2011, Wageningen, The Netherlands", "journal-ref": null, "doi": "10.6084/m9.figshare.936872", "report-no": null, "categories": "cs.SY cs.CE physics.geo-ph", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Soil erosion by water is a widespread phenomenon throughout Europe and has\nthe potentiality, with his on-site and off-site effects, to affect water\nquality, food security and floods. Despite the implementation of numerous and\ndifferent models for estimating soil erosion by water in Europe, there is still\na lack of harmonization of assessment methodologies.\n  Often, different approaches result in soil erosion rates significantly\ndifferent. Even when the same model is applied to the same region the results\nmay differ. This can be due to the way the model is implemented (i.e. with the\nselection of different algorithms when available) and/or to the use of datasets\nhaving different resolution or accuracy. Scientific computation is emerging as\none of the central topic of the scientific method, for overcoming these\nproblems there is thus the necessity to develop reproducible computational\nmethod where codes and data are available.\n  The present study illustrates this approach. Using only public available\ndatasets, we applied the Revised Universal Soil loss Equation (RUSLE) to locate\nthe most sensitive areas to soil erosion by water in Europe.\n  A significant effort was made for selecting the better simplified equations\nto be used when a strict application of the RUSLE model is not possible. In\nparticular for the computation of the Rainfall Erosivity factor (R) the\nreproducible research paradigm was applied. The calculation of the R factor was\nimplemented using public datasets and the GNU R language. An easily\nreproducible validation procedure based on measured precipitation time series\nwas applied using MATLAB language. Designing the computational modelling\narchitecture with the aim to ease as much as possible the future reuse of the\nmodel in analysing climate change scenarios is also a challenging goal of the\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 22:10:42 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Bosco", "Claudio", ""], ["de Rigo", "Daniele", ""], ["Dewitte", "Olivier", ""], ["Montanarella", "Luca", ""]]}, {"id": "1402.4101", "submitter": "Val\\'erio Ramos Batista", "authors": "Marcelo Zanchetta do Nascimento and Val\\'erio Ramos Batista", "title": "First steps to Virtual Mammography: Simulating external compressions of\n  the breast with the Surface Evolver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a computational modelling that reproduces the\nbreast compression processes used to obtain the mammogram. The main result is a\nprogramme in which one can track the first steps of virtual mammography. On the\none hand, our modelling enables addition of structures that represent different\ntissues, muscles and glands in the breast. On the other hand, we shall validate\nand implement it by means of laboratory tests with phantoms. To the best of our\nknowledge, these two characteristics do confer originality to our research.\nThis is because their interrelation seems not to be properly established\nelsewhere yet. We conclude that our model reproduces the same shapes and\nmeasurements really taken from the volunteer's breasts.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 19:56:22 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Nascimento", "Marcelo Zanchetta do", ""], ["Batista", "Val\u00e9rio Ramos", ""]]}, {"id": "1402.4876", "submitter": "Ruibang Luo", "authors": "Sze-Hang Chan, Jeanno Cheung, Edward Wu, Heng Wang, Chi-Man Liu,\n  Xiaoqian Zhu, Shaoliang Peng, Ruibang Luo, Tak-Wah Lam", "title": "MICA: A fast short-read aligner that takes full advantage of Intel Many\n  Integrated Core Architecture (MIC)", "comments": "2 pages, 1 figure, 2 tables, submitted to Bioinformatics as an\n  application note", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Short-read aligners have recently gained a lot of speed by\nexploiting the massive parallelism of GPU. An uprising alternative to GPU is\nIntel MIC; supercomputers like Tianhe-2, currently top of TOP500, is built with\n48,000 MIC boards to offer ~55 PFLOPS. The CPU-like architecture of MIC allows\nCPU-based software to be parallelized easily; however, the performance is often\ninferior to GPU counterparts as an MIC board contains only ~60 cores (while a\nGPU board typically has over a thousand cores). Results: To better utilize\nMIC-enabled computers for NGS data analysis, we developed a new short-read\naligner MICA that is optimized in view of MICs limitation and the extra\nparallelism inside each MIC core. Experiments on aligning 150bp paired-end\nreads show that MICA using one MIC board is 4.9 times faster than the BWA-MEM\n(using 6-core of a top-end CPU), and slightly faster than SOAP3-dp (using a\nGPU). Furthermore, MICAs simplicity allows very efficient scale-up when\nmultiple MIC boards are used in a node (3 cards give a 14.1-fold speedup over\nBWA-MEM). Summary: MICA can be readily used by MIC-enabled supercomputers for\nproduction purpose. We have tested MICA on Tianhe-2 with 90 WGS samples (17.47\nTera-bases), which can be aligned in an hour less than 400 nodes. MICA has\nimpressive performance even though the current MIC is at its initial stage of\ndevelopment (the next generation of MIC has been announced to release in late\n2014).\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 03:19:50 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Chan", "Sze-Hang", ""], ["Cheung", "Jeanno", ""], ["Wu", "Edward", ""], ["Wang", "Heng", ""], ["Liu", "Chi-Man", ""], ["Zhu", "Xiaoqian", ""], ["Peng", "Shaoliang", ""], ["Luo", "Ruibang", ""], ["Lam", "Tak-Wah", ""]]}, {"id": "1402.5208", "submitter": "Bhaskar DasGupta", "authors": "Bhaskar DasGupta and Lakshmi Kaligounder", "title": "Densely Entangled Financial Systems", "comments": "to appear in Network Models in Economics and Finance, V. Kalyagin, P.\n  M. Pardalos and T. M. Rassias (editors), Springer Optimization and Its\n  Applications series, Springer, 2014", "journal-ref": "in Network Models in Economics and Finance, V. Kalyagin, P. M.\n  Pardalos and Th. M. Rassias (eds.), Springer Optimization and Its\n  Applications series, 100, 85-105, Springer, 2014", "doi": "10.1007/978-3-319-09683-4__5", "report-no": null, "categories": "q-fin.RM cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In [1] Zawadoski introduces a banking network model in which the asset and\ncounter-party risks are treated separately and the banks hedge their assets\nrisks by appropriate OTC contracts. In his model, each bank has only two\ncounter-party neighbors, a bank fails due to the counter-party risk only if at\nleast one of its two neighbors default, and such a counter-party risk is a low\nprobability event. Informally, the author shows that the banks will hedge their\nasset risks by appropriate OTC contracts, and, though it may be socially\noptimal to insure against counter-party risk, in equilibrium banks will {\\em\nnot} choose to insure this low probability event.\n  In this paper, we consider the above model for more general network\ntopologies, namely when each node has exactly 2r counter-party neighbors for\nsome integer r>0. We extend the analysis of [1] to show that as the number of\ncounter-party neighbors increase the probability of counter-party risk also\nincreases, and in particular the socially optimal solution becomes privately\nsustainable when each bank hedges its risk to at least n/2 banks, where n is\nthe number of banks in the network, i.e., when 2r is at least n/2, banks not\nonly hedge their asset risk but also hedge its counter-party risk.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 05:54:10 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["DasGupta", "Bhaskar", ""], ["Kaligounder", "Lakshmi", ""]]}, {"id": "1402.5323", "submitter": "Francis Bell III", "authors": "Francis Bell, Chunyu Zhao, Ahmet Sacan", "title": "PDBCirclePlot: A Novel Visualization Method for Protein Structures", "comments": "Application note, 5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive molecular graphics applications facilitate analysis of three\ndimensional protein structures. Naturally, non-interactive 2-D snapshots of the\nprotein structures do not convey the same level of geometric detail. Several\n2-D visualization methods have been in use to summarize structural information,\nincluding contact maps and 2-D cartoon views. We present a new approach for 2-D\nvisualization of protein structures where amino acid residues are displayed on\na circle and spatially close residues are depicted by links. Furthermore,\nresidue-specific properties, such as conservation, accessibility, temperature\nfactor, can be displayed as plots on the same circular view.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 20:35:48 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Bell", "Francis", ""], ["Zhao", "Chunyu", ""], ["Sacan", "Ahmet", ""]]}, {"id": "1402.5466", "submitter": "Chanabasayya Vastrad M", "authors": "Doreswamy, Chanabasyya M. Vastrad", "title": "Predictive Comparative QSAR analysis of Sulfathiazole Analogues as\n  Mycobacterium Tuberculosis H37RV Inhabitors", "comments": "arXiv admin note: substantial text overlap with arXiv:1312.2841", "journal-ref": "published 2012", "doi": null, "report-no": null, "categories": "cs.CE q-bio.QM", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Antitubercular activity of Sulfathiazole Derivitives series were subjected to\nQuantitative Structure Activity Relationship (QSAR) Analysis with an attempt to\nderive and understand a correlation between the Biologically Activity as\ndependent variable and various descriptors as independent variables. QSAR\nmodels generated using 28 compounds. Several statistical regression expressions\nwere obtained using Partial Least Squares (PLS) Regression, Multiple Linear\nRegression (MLR) and Principal Component Regression (PCR) methods. The among\nthese methods, Partial Least Square Regression (PLS) method has shown very\npromising result as compare to other two methods. A QSAR model was generated by\na training set of 18 molecules with correlation coefficient r (r square) of\n0.9191, significant cross validated correlation coefficient (q square) of\n0.8300, F test of 53.5783, r square for external test set pred_r square\n-3.6132, coefficient of correlation of predicted data set pred_r_se square\n1.4859 and degree of freedom 14 by Partial Least Squares Regression Method.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 02:52:52 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Doreswamy", "", ""], ["Vastrad", "Chanabasyya M.", ""]]}, {"id": "1402.5684", "submitter": "Orhan Firat", "authors": "Orhan Firat and Mete Ozay and Ilke Oztekin and Fatos T. Yarman Vural", "title": "Discriminative Functional Connectivity Measures for Brain Decoding", "comments": "This paper has been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical learning model for classifying cognitive processes\nbased on distributed patterns of neural activation in the brain, acquired via\nfunctional magnetic resonance imaging (fMRI). In the proposed learning method,\nlocal meshes are formed around each voxel. The distance between voxels in the\nmesh is determined by using a functional neighbourhood concept. In order to\ndefine the functional neighbourhood, the similarities between the time series\nrecorded for voxels are measured and functional connectivity matrices are\nconstructed. Then, the local mesh for each voxel is formed by including the\nfunctionally closest neighbouring voxels in the mesh. The relationship between\nthe voxels within a mesh is estimated by using a linear regression model. These\nrelationship vectors, called Functional Connectivity aware Local Relational\nFeatures (FC-LRF) are then used to train a statistical learning machine. The\nproposed method was tested on a recognition memory experiment, including data\npertaining to encoding and retrieval of words belonging to ten different\nsemantic categories. Two popular classifiers, namely k-nearest neighbour (k-nn)\nand Support Vector Machine (SVM), are trained in order to predict the semantic\ncategory of the item being retrieved, based on activation patterns during\nencoding. The classification performance of the Functional Mesh Learning model,\nwhich range in 62%-71% is superior to the classical multi-voxel pattern\nanalysis (MVPA) methods, which range in 40%-48%, for ten semantic categories.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2014 22:01:11 GMT"}, {"version": "v2", "created": "Thu, 6 Mar 2014 19:02:07 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Firat", "Orhan", ""], ["Ozay", "Mete", ""], ["Oztekin", "Ilke", ""], ["Vural", "Fatos T. Yarman", ""]]}, {"id": "1402.5708", "submitter": "Lavdim Kurtaj", "authors": "Lavdim Kurtaj, Ilir Limani, Vjosa Shatri and Avni Skeja", "title": "The Cerebellum: New Computational Model that Reveals its Primary\n  Function to Calculate Multibody Dynamics Conform to Lagrange-Euler\n  Formulation", "comments": "18 pages, 4 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 10,\n  Issue 5, No 2, September 2013", "doi": null, "report-no": null, "categories": "cs.NE cs.CE cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cerebellum is part of the brain that occupies only 10% of the brain volume,\nbut it contains about 80% of total number of brain neurons. New cerebellar\nfunction model is developed that sets cerebellar circuits in context of\nmultibody dynamics model computations, as important step in controlling balance\nand movement coordination, functions performed by two oldest parts of the\ncerebellum. Model gives new functional interpretation for granule cells-Golgi\ncell circuit, including distinct function for upper and lower Golgi cell\ndendritc trees, and resolves issue of sharing Granule cells between Purkinje\ncells. Sets new function for basket cells, and for stellate cells according to\nposition in molecular layer. New model enables easily and direct integration of\nsensory information from vestibular system and cutaneous mechanoreceptors, for\nbalance, movement and interaction with environments. Model gives explanation of\nPurkinje cells convergence on deep-cerebellar nuclei.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 02:40:05 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Kurtaj", "Lavdim", ""], ["Limani", "Ilir", ""], ["Shatri", "Vjosa", ""], ["Skeja", "Avni", ""]]}, {"id": "1402.5757", "submitter": "Richard McClatchey", "authors": "Kamran Munir, Saad Liaquat Kiani, Khawar Hasham, Richard McClatchey,\n  Andrew Branson, Jetendr Shamdasani and the N4U Consortium", "title": "An Integrated e-science Analysis Base for Computation Neuroscience\n  Experiments and Analysis", "comments": "8 pages & 4 figures", "journal-ref": "Procedia - Social and Behavioral Sciences. Vol 73 pp 85-92 (2013)\n  Elsevier Publishers", "doi": "10.1016/j.sbspro.2013.02.026.", "report-no": null, "categories": "cs.SE cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in data management and imaging technologies have\nsignificantly affected diagnostic and extrapolative research in the\nunderstanding of neurodegenerative diseases. However, the impact of these new\ntechnologies is largely dependent on the speed and reliability with which the\nmedical data can be visualised, analysed and interpreted. The EUs neuGRID for\nUsers (N4U) is a follow-on project to neuGRID, which aims to provide an\nintegrated environment to carry out computational neuroscience experiments.\nThis paper reports on the design and development of the N4U Analysis Base and\nrelated Information Services, which addresses existing research and practical\nchallenges by offering an integrated medical data analysis environment with the\nnecessary building blocks for neuroscientists to optimally exploit neuroscience\nworkflows, large image datasets and algorithms in order to conduct analyses.\nThe N4U Analysis Base enables such analyses by indexing and interlinking the\nneuroimaging and clinical study datasets stored on the N4U Grid infrastructure,\nalgorithms and scientific workflow definitions along with their associated\nprovenance information.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2014 09:14:44 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Munir", "Kamran", ""], ["Kiani", "Saad Liaquat", ""], ["Hasham", "Khawar", ""], ["McClatchey", "Richard", ""], ["Branson", "Andrew", ""], ["Shamdasani", "Jetendr", ""], ["Consortium", "the N4U", ""]]}, {"id": "1402.6366", "submitter": "Mustafa Abdul Salam", "authors": "Osman Hegazy, Omar S. Soliman and Mustafa Abdul Salam", "title": "LSSVM-ABC Algorithm for Stock Price prediction", "comments": "12 pages. International Journal of Computer Trends and Technology\n  (IJCTT)2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, Artificial Bee Colony (ABC) algorithm which inspired from the\nbehavior of honey bees swarm is presented. ABC is a stochastic population-based\nevolutionary algorithm for problem solving. ABC algorithm, which is considered\none of the most recently swarm intelligent techniques, is proposed to optimize\nleast square support vector machine (LSSVM) to predict the daily stock prices.\nThe proposed model is based on the study of stocks historical data, technical\nindicators and optimizing LSSVM with ABC algorithm. ABC selects best free\nparameters combination for LSSVM to avoid over-fitting and local minima\nproblems and improve prediction accuracy. LSSVM optimized by Particle swarm\noptimization (PSO) algorithm, LSSVM, and ANN techniques are used for comparison\nwith proposed model. Proposed model tested with twenty datasets representing\ndifferent sectors in S&P 500 stock market. Results presented in this paper show\nthat the proposed model has fast convergence speed, and it also achieves better\naccuracy than compared techniques in most cases.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2014 23:02:08 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Hegazy", "Osman", ""], ["Soliman", "Omar S.", ""], ["Salam", "Mustafa Abdul", ""]]}, {"id": "1402.6636", "submitter": "Iain Rice Mr", "authors": "Iain Rice, Roger Benton, Les Hart and David Lowe", "title": "Analysis of Multibeam SONAR Data using Dissimilarity Representations", "comments": "Presented at IMA Mathematics in Defence 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of low-dimensional visualisation of very\nhigh dimensional information sources for the purpose of situation awareness in\nthe maritime environment. In response to the requirement for human decision\nsupport aids to reduce information overload (and specifically, data amenable to\ninter-point relative similarity measures) appropriate to the below-water\nmaritime domain, we are investigating a preliminary prototype topographic\nvisualisation model. The focus of the current paper is on the mathematical\nproblem of exploiting a relative dissimilarity representation of signals in a\nvisual informatics mapping model, driven by real-world sonar systems. An\nindependent source model is used to analyse the sonar beams from which a simple\nprobabilistic input model to represent uncertainty is mapped to a latent\nvisualisation space where data uncertainty can be accommodated. The use of\neuclidean and non-euclidean measures are used and the motivation for future use\nof non-euclidean measures is made. Concepts are illustrated using a simulated\n64 beam weak SNR dataset with realistic sonar targets.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2014 10:21:34 GMT"}], "update_date": "2014-02-27", "authors_parsed": [["Rice", "Iain", ""], ["Benton", "Roger", ""], ["Hart", "Les", ""], ["Lowe", "David", ""]]}, {"id": "1402.6775", "submitter": "Chandrima Sarkar", "authors": "Chandrima Sarkar, Raamesh Deshpande, Chad Myers", "title": "Analysis of Barcode sequence features to find anomalies due to\n  amplification Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we aim at investigating whether barcode sequence features can\npredict the read count ambiguities caused during PCR based next generation\nsequencing techniques. The methodologies we used are mutual information based\nmotif discovery and Lasso regression technique using features generated from\nthe barcode sequence. The results indicate that there is a certain degree of\ncorrelation between motifs discovered in the sequences and the read counts. Our\nmain contribution in this paper is a thorough investigation of the barcode\nfeatures that gave us useful information regarding the significance of the\nsequence features and the sequence containing the discovered motifs in\nprediction of read counts.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 02:59:52 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Sarkar", "Chandrima", ""], ["Deshpande", "Raamesh", ""], ["Myers", "Chad", ""]]}, {"id": "1402.7015", "submitter": "Fabian Pedregosa", "authors": "Fabian Pedregosa (INRIA Saclay - Ile de France, INRIA Paris -\n  Rocquencourt), Michael Eickenberg (INRIA Saclay - Ile de France, LNAO),\n  Philippe Ciuciu (INRIA Saclay - Ile de France, NEUROSPIN), Bertrand Thirion\n  (INRIA Saclay - Ile de France, NEUROSPIN), Alexandre Gramfort (LTCI)", "title": "Data-driven HRF estimation for encoding and decoding models", "comments": "appears in NeuroImage (2015)", "journal-ref": null, "doi": "10.1016/j.neuroimage.2014.09.060", "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the common usage of a canonical, data-independent, hemodynamic\nresponse function (HRF), it is known that the shape of the HRF varies across\nbrain regions and subjects. This suggests that a data-driven estimation of this\nfunction could lead to more statistical power when modeling BOLD fMRI data.\nHowever, unconstrained estimation of the HRF can yield highly unstable results\nwhen the number of free parameters is large. We develop a method for the joint\nestimation of activation and HRF using a rank constraint causing the estimated\nHRF to be equal across events/conditions, yet permitting it to be different\nacross voxels. Model estimation leads to an optimization problem that we\npropose to solve with an efficient quasi-Newton method exploiting fast gradient\ncomputations. This model, called GLM with Rank-1 constraint (R1-GLM), can be\nextended to the setting of GLM with separate designs which has been shown to\nimprove decoding accuracy in brain activity decoding experiments. We compare 10\ndifferent HRF modeling methods in terms of encoding and decoding score in two\ndifferent datasets. Our results show that the R1-GLM model significantly\noutperforms competing methods in both encoding and decoding settings,\npositioning it as an attractive method both from the points of view of accuracy\nand computational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 18:50:58 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2014 06:11:17 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 11:14:00 GMT"}, {"version": "v4", "created": "Mon, 6 Oct 2014 16:39:55 GMT"}, {"version": "v5", "created": "Fri, 31 Oct 2014 13:47:01 GMT"}, {"version": "v6", "created": "Fri, 7 Nov 2014 11:27:19 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Pedregosa", "Fabian", "", "INRIA Saclay - Ile de France, INRIA Paris -\n  Rocquencourt"], ["Eickenberg", "Michael", "", "INRIA Saclay - Ile de France, LNAO"], ["Ciuciu", "Philippe", "", "INRIA Saclay - Ile de France, NEUROSPIN"], ["Thirion", "Bertrand", "", "INRIA Saclay - Ile de France, NEUROSPIN"], ["Gramfort", "Alexandre", "", "LTCI"]]}, {"id": "1402.7216", "submitter": "Jana Paz\\'urikov\\'a", "authors": "Jana Paz\\'urikov\\'a", "title": "Large-Scale Molecular Dynamics Simulations for Highly Parallel\n  Infrastructures", "comments": "thesis proposal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.CE physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Computational chemistry allows researchers to experiment in sillico: by\nrunning a computer simulations of a biological or chemical processes of\ninterest. Molecular dynamics with molecular mechanics model of interactions\nsimulates N-body problem of atoms$-$it computes movements of atoms according to\nNewtonian physics and empirical descriptions of atomic electrostatic\ninteractions. These simulations require high performance computing resources,\nas evaluations within each step are computationally demanding and billions of\nsteps are needed to reach interesting timescales. Current methods decompose the\nspatial domain of the problem and calculate on parallel/distributed\ninfrastructures. Even the methods with the highest strong scaling hit the limit\nat half a million cores: they are not able to cut the time to result if\nprovided with more processors. At the dawn of exascale computing with massively\nparallel computational resources, we want to increase the level of parallelism\nby incorporating parallel-in-time computation to molecular dynamics\nsimulations. Calculation of results in several successive time points\nsimultaneously without a priori knowledge has been examined with no major\nsuccess. We will study and implement a novel combinations of methods that\naccording to our theoretical analyses should achieve promising speed-up\ncompared to sequential-in-time calculation.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 11:59:08 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Paz\u00farikov\u00e1", "Jana", ""]]}, {"id": "1402.7324", "submitter": "Evgeny Nikulchev", "authors": "Evgeny Nikulchev", "title": "Geometrical approach to modeling of nonlinear systems from experimental\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": "ISBN 978--5--8122--0926--1", "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This monograph presents a geometric modeling method nonlinear dynamical\nsystems from experimental data . basis method is a qualitative approach to the\nanalysis of linear models and construction of the symmetry groups of attractors\nof dynamical systems with controls . A theoretical study including the central\ntheorem manifold defining conditions of existence of the class in question\nmodels in the local area , taking into account the group properties ,\nestimation algorithms invariant characteristics , methods of constructing\nmodels and identifiable description of the results obtained using the method\nfor simulation -driven engineering processes . included two application is the\ndevelopment of the proposed approach : identification of groups symmetries on\nthe phase portraits of dynamical systems and the method of constructing neural\nnetwork predictive models\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 17:22:50 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Nikulchev", "Evgeny", ""]]}, {"id": "1402.7351", "submitter": "Mustafa Abdul Salam", "authors": "Osman Hegazy, Omar S. Soliman and Mustafa Abdul Salam", "title": "A Machine Learning Model for Stock Market Prediction", "comments": "7 Pages. arXiv admin note: substantial text overlap with\n  arXiv:1402.6366", "journal-ref": "International Journal of Computer Science and Telecommunications\n  [Volume 4, Issue 12, December 2013]", "doi": null, "report-no": null, "categories": "cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock market prediction is the act of trying to determine the future value of\na company stock or other financial instrument traded on a financial exchange.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 19:12:50 GMT"}], "update_date": "2014-03-03", "authors_parsed": [["Hegazy", "Osman", ""], ["Soliman", "Omar S.", ""], ["Salam", "Mustafa Abdul", ""]]}]