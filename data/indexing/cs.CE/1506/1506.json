[{"id": "1506.00080", "submitter": "Francesco Gadaleta", "authors": "Francesco Gadaleta and Kyrylo Bessonov", "title": "Integration of Gene Expression Data and Methylation Reveals Genetic\n  Networks for Glioblastoma", "comments": "This paper has been withdrawn by the author due to submission to\n  commercial journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.GN", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Motivation: The consistent amount of different types of omics data requires\nnovel methods of analysis and data integration. In this work we describe\nRegression2Net, a computational approach to analyse gene expression and\nmethylation profiles via regression analysis and network-based techniques.\n  Results: We identified 284 and 447 unique candidate genes potentially\nassociated to the Glioblastoma pathology from two networks inferred from mixed\ngenetic datasets. In-depth biological analysis of these networks reveals genes\nthat are related to energy metabolism, cell cycle control (AATF), immune system\nresponse and several types of cancer. Importantly, we observed significant\nover- representation of cancer related pathways including glioma especially in\nthe methylation network. This confirms the strong link between methylation and\nglioblastomas. Potential glioma suppressor genes ACCN3 and ACCN4 linked to\nNBPF1 neuroblastoma breakpoint family have been identified in our expression\nnetwork. Numerous ABC transporter genes (ABCA1, ABCB1) present in the\nexpression network suggest drug resistance of glioblastoma tumors.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 07:02:48 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 12:08:25 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Gadaleta", "Francesco", ""], ["Bessonov", "Kyrylo", ""]]}, {"id": "1506.00366", "submitter": "Khalid Raza", "authors": "Khalid Raza", "title": "Formal Concept Analysis for Knowledge Discovery from Biological Data", "comments": "14 pages, 2 figures", "journal-ref": "International Journal of Data Mining and Bioinformatics,\n  Inderscience, 18(4): 281-300 (2017)", "doi": "10.1504/IJDMB.2017.10009312", "report-no": null, "categories": "cs.AI cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to rapid advancement in high-throughput techniques, such as microarrays\nand next generation sequencing technologies, biological data are increasing\nexponentially. The current challenge in computational biology and\nbioinformatics research is how to analyze these huge raw biological data to\nextract biologically meaningful knowledge. This review paper presents the\napplications of formal concept analysis for the analysis and knowledge\ndiscovery from biological data, including gene expression discretization, gene\nco-expression mining, gene expression clustering, finding genes in gene\nregulatory networks, enzyme/protein classifications, binding site\nclassifications, and so on. It also presents a list of FCA-based software tools\napplied in biological domain and covers the challenges faced so far.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 07:18:09 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Raza", "Khalid", ""]]}, {"id": "1506.00571", "submitter": "Aristides T. Hatjimihail MD PhD", "authors": "Aristides T. Hatjimihail", "title": "Calculation of the confidence bounds for the fraction nonconforming of\n  normal populations of measurements in clinical laboratory medicine", "comments": "22 pages, 5 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": "hcsltr04", "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The fraction nonconforming is a key quality measure used in statistical\nquality control design in clinical laboratory medicine. The confidence bounds\nof normal populations of measurements for the fraction nonconforming each of\nthe lower and upper quality specification limits when both the random and the\nsystematic error are unknown can be calculated using the noncentral\nt-distribution, as it is described in detail and illustrated with examples.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 17:04:23 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 16:19:43 GMT"}, {"version": "v3", "created": "Thu, 4 Jun 2015 11:47:24 GMT"}, {"version": "v4", "created": "Sun, 23 Dec 2018 19:06:39 GMT"}, {"version": "v5", "created": "Thu, 27 Dec 2018 20:23:28 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Hatjimihail", "Aristides T.", ""]]}, {"id": "1506.00716", "submitter": "Szil\\'ard P\\'all", "authors": "P\\'all Szil\\'ard, Mark James Abraham, Carsten Kutzner, Berk Hess, Erik\n  Lindahl", "title": "Tackling Exascale Software Challenges in Molecular Dynamics Simulations\n  with GROMACS", "comments": "EASC 2014 conference proceeding", "journal-ref": "Proc. EASC 2014, 8759 pp. 3-27, Springer LNCS", "doi": "10.1007/978-3-319-15976-8_1", "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  GROMACS is a widely used package for biomolecular simulation, and over the\nlast two decades it has evolved from small-scale efficiency to advanced\nheterogeneous acceleration and multi-level parallelism targeting some of the\nlargest supercomputers in the world. Here, we describe some of the ways we have\nbeen able to realize this through the use of parallelization on all levels,\ncombined with a constant focus on absolute performance. Release 4.6 of GROMACS\nuses SIMD acceleration on a wide range of architectures, GPU offloading\nacceleration, and both OpenMP and MPI parallelism within and between nodes,\nrespectively. The recent work on acceleration made it necessary to revisit the\nfundamental algorithms of molecular simulation, including the concept of\nneighborsearching, and we discuss the present and future challenges we see for\nexascale simulation - in particular a very fine-grained task parallelism. We\nalso discuss the software management, code peer review and continuous\nintegration testing required for a project of this complexity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 00:37:50 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Szil\u00e1rd", "P\u00e1ll", ""], ["Abraham", "Mark James", ""], ["Kutzner", "Carsten", ""], ["Hess", "Berk", ""], ["Lindahl", "Erik", ""]]}, {"id": "1506.01190", "submitter": "Leila Musabekova Lmm", "authors": "A.M. Brener, L.M. Musabekova", "title": "Modeling of through-reactors with allowance of Large-Scale Effect on\n  Heat and Mass Efficiency of Chemical Apparatuses", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals also with a problem of gas absorption accompanied by an\ninstantaneous, irreversible reaction in the liquid layer. The well-known\nmethods for calculating such processes are based usually on the certain\namendments to solutions, which are obtained disregarding the chemical reaction.\nUnlike the known work (1. D. Baetens, R. Van Keer, L.H. Hosten. Gas-liquid\nreaction: absorption accompanied by an instantaneous, irreversible reaction//\nMoving Boundaries IV, Southampton, Boston, 1997) the approach we used takes\ninto account the influence of reaction resulting product on the arising and\nvelocity of a moving reaction plane. The known results in the theory of\nchemical apparatuses scaling are devoted to apparatuses with non-regular\npackings mainly. However how the phases distribution over the regular packings\nof chemical columns effects the heat and mass efficiency is studied lesser.\nThis paper deals with the methods of simulation the scaling effects applying to\nchemical towers with regular packings of various types. The models for\ndescribing the influence of initial liquid and gas distribution in chemical\ncolumns with regular packing on the mass transfer efficiency have been\nsubmitted. The sufficiently simple methods for evaluating the influence of\nlarge-scale factor on the efficiency of mass transfer have been obtained. These\nmethods are suitable for use in engineering calculation techniques.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 10:12:55 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Brener", "A. M.", ""], ["Musabekova", "L. M.", ""]]}, {"id": "1506.02085", "submitter": "Min Xu", "authors": "Min Xu, Rudy Setiono", "title": "Gene selection for cancer classification using a hybrid of univariate\n  and multivariate feature selection methods", "comments": null, "journal-ref": "Applied Genomics and Proteomics. 2003:2(2)79-91", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various approaches to gene selection for cancer classification based on\nmicroarray data can be found in the literature and they may be grouped into two\ncategories: univariate methods and multivariate methods. Univariate methods\nlook at each gene in the data in isolation from others. They measure the\ncontribution of a particular gene to the classification without considering the\npresence of the other genes. In contrast, multivariate methods measure the\nrelative contribution of a gene to the classification by taking the other genes\nin the data into consideration. Multivariate methods select fewer genes in\ngeneral. However, the selection process of multivariate methods may be\nsensitive to the presence of irrelevant genes, noises in the expression and\noutliers in the training data. At the same time, the computational cost of\nmultivariate methods is high. To overcome the disadvantages of the two types of\napproaches, we propose a hybrid method to obtain gene sets that are small and\nhighly discriminative.\n  We devise our hybrid method from the univariate Maximum Likelihood method\n(LIK) and the multivariate Recursive Feature Elimination method (RFE). We\nanalyze the properties of these methods and systematically test the\neffectiveness of our proposed method on two cancer microarray datasets. Our\nexperiments on a leukemia dataset and a small, round blue cell tumors dataset\ndemonstrate the effectiveness of our hybrid method. It is able to discover sets\nconsisting of fewer genes than those reported in the literature and at the same\ntime achieve the same or better prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 23:29:06 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Xu", "Min", ""], ["Setiono", "Rudy", ""]]}, {"id": "1506.02087", "submitter": "Min Xu", "authors": "Min Xu", "title": "Global Gene Expression Analysis Using Machine Learning Methods", "comments": "Author's master thesis (National University of Singapore, May 2003).\n  Adviser: Rudy Setiono", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CE cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray is a technology to quantitatively monitor the expression of large\nnumber of genes in parallel. It has become one of the main tools for global\ngene expression analysis in molecular biology research in recent years. The\nlarge amount of expression data generated by this technology makes the study of\ncertain complex biological problems possible and machine learning methods are\nplaying a crucial role in the analysis process. At present, many machine\nlearning methods have been or have the potential to be applied to major areas\nof gene expression analysis. These areas include clustering, classification,\ndynamic modeling and reverse engineering.\n  In this thesis, we focus our work on using machine learning methods to solve\nthe classification problems arising from microarray data. We first identify the\nmajor types of the classification problems; then apply several machine learning\nmethods to solve the problems and perform systematic tests on real and\nartificial datasets. We propose improvement to existing methods. Specifically,\nwe develop a multivariate and a hybrid feature selection method to obtain high\nclassification performance for high dimension classification problems. Using\nthe hybrid feature selection method, we are able to identify small sets of\nfeatures that give predictive accuracy that is as good as that from other\nmethods which require many more features.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 23:37:20 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Xu", "Min", ""]]}, {"id": "1506.02424", "submitter": "Yue Wang", "authors": "Yue Wang and Zhongkai Zhao", "title": "Algorithms for finding transposons in gene sequences", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the process of evolution, some genes will change their relative\npositions in gene sequence. These \"jumping genes\" are called transposons.\nThrough some intuitive rules, we give a criterion to determine transposons\namong gene sequences of different individuals of the same species. Then we turn\nthis problem into graph theory and give algorithms for different situations\nwith acceptable time complexities. One of these algorithms has been reported\nbriefly as the \"iteration algorithm\" in Kang et al.'s paper (in this paper,\ntransposon is called \"core-gene-defined genome organizational framework\",\ncGOF). This paper provides the omitted details and discussions on general\ncases.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 09:58:19 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Wang", "Yue", ""], ["Zhao", "Zhongkai", ""]]}, {"id": "1506.02808", "submitter": "Kirana Kumara P", "authors": "Kirana Kumara P", "title": "Simulations using meshfree methods", "comments": "preprint (draft) + 3 figures, 1 table, 2 appendices, 2 images, 1\n  MATLAB code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, attempt is made to solve a few problems using the Polynomial\nPoint Collocation Method (PPCM), the Radial Point Collocation Method (RPCM),\nSmoothed Particle Hydrodynamics (SPH), and the Finite Point Method (FPM). A few\nobservations on the accuracy of these methods are recorded. All the simulations\nin this paper are three dimensional linear elastostatic simulations, without\naccounting for body forces.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 07:41:26 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["P", "Kirana Kumara", ""]]}, {"id": "1506.03610", "submitter": "Florin Nichita F", "authors": "Florin F. Nichita", "title": "Yang-Baxter Equations, Computational Methods and Applications", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational methods are an important tool for solving the Yang-Baxter\nequations(in small dimensions), for classifying (unifying) structures, and for\nsolving related problems. This paper is an account of some of the latest\ndevelopments on the Yang-Baxter equation, its set-theoretical version, and its\napplications. We construct new set-theoretical solutions for the Yang-Baxter\nequation. Unification theories and other results are proposed or proved.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 10:11:19 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 19:36:56 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Nichita", "Florin F.", ""]]}, {"id": "1506.03611", "submitter": "Stephan Kramer", "authors": "Stephan C Kramer and Matthew D Piggott", "title": "A correction to the enhanced bottom drag parameterisation of tidal\n  turbines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hydrodynamic modelling is an important tool for the development of tidal\nstream energy projects. Many hydrodynamic models incorporate the effect of\ntidal turbines through an enhanced bottom drag. In this paper we show that\nalthough for coarse grid resolutions (kilometre scale) the resulting force\nexerted on the flow agrees well with the theoretical value, the force starts\ndecreasing with decreasing grid sizes when these become smaller than the length\nscale of the wake recovery. This is because the assumption that the upstream\nvelocity can be approximated by the local model velocity, is no longer valid.\nUsing linear momentum actuator disc theory however, we derive a relationship\nbetween these two velocities and formulate a correction to the enhanced bottom\ndrag formulation that consistently applies a force that remains closed to the\ntheoretical value, for all grid sizes down to the turbine scale. In addition, a\nbetter understanding of the relation between the model, upstream, and actual\nturbine velocity, as predicted by actuator disc theory, leads to an improved\nestimate of the usefully extractable energy. We show how the corrections can be\napplied (demonstrated here for the models MIKE 21 and Fluidity) by a simple\nmodification of the drag coefficient.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 10:18:17 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Kramer", "Stephan C", ""], ["Piggott", "Matthew D", ""]]}, {"id": "1506.04447", "submitter": "Qinran Hu", "authors": "Qinran Hu, Fangxing Li", "title": "An Optimal Framework for Residential Load Aggregator", "comments": "to be submitted to IEEE Trans. on Smart Grid", "journal-ref": null, "doi": "10.1109/TSG.2016.2631083", "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the development of intelligent demand-side management with automatic\ncontrol, distributed populations of large residential loads, such as air\nconditioners (ACs) and electrical water heaters (EWHs), have the opportunities\nto provide effective demand-side ancillary services for load serving entities\n(LSEs) to reduce the emissions and network operating costs. Most present\napproaches are restricted to 1) the scenarios involving with efficiently\nscheduling the large number of appliances in real time, 2) the issues about\nevaluating the contributions of individual residents towards participating\ndemand response (DR) program, and fairly distributing the rewards, and 3) the\nconcerns on performing cost-effective demand reduction request (DRR) for LSEs\nwith minimal rewards costs while not affecting their living comfortableness.\nTherefore, this paper presents an optimal framework for residential load\naggregators (RLAs) which helps solve the problems mentioned above. Under this\nframework, RLAs are able to realize the DRR for LSEs to generate optimal\ncontrol strategies over residential appliances quickly and efficiently. To\nresidents, the framework is designed with probabilistic model of\ncomfortableness, which minimizes the impact of DR program to their daily life.\nTo LSEs, the framework helps minimize the total reward costs of performing\nDRRs. Moreover, the framework fairly and strategically distributes the\nfinancial rewards to residents, which may stimulate the potential capability of\nloads optimized and controlled by RLAs in demand side management. The proposed\nframework has been validated on several numerical case studies.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 22:30:14 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Hu", "Qinran", ""], ["Li", "Fangxing", ""]]}, {"id": "1506.05070", "submitter": "Soumi Chaki", "authors": "Soumi Chaki", "title": "Reservoir Characterization: A Machine Learning Approach", "comments": "Supervisors: Prof. Aurobinda Routray and Prof. William K. Mohanty", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reservoir Characterization (RC) can be defined as the act of building a\nreservoir model that incorporates all the characteristics of the reservoir that\nare pertinent to its ability to store hydrocarbons and also to produce them.It\nis a difficult problem due to non-linear and heterogeneous subsurface\nproperties and associated with a number of complex tasks such as data fusion,\ndata mining, formulation of the knowledge base, and handling of the\nuncertainty.This present work describes the development of algorithms to obtain\nthe functional relationships between predictor seismic attributes and target\nlithological properties. Seismic attributes are available over a study area\nwith lower vertical resolution. Conversely, well logs and lithological\nproperties are available only at specific well locations in a study area with\nhigh vertical resolution.Sand fraction, which represents per unit sand volume\nwithin the rock, has a balanced distribution between zero to unity.The thesis\naddresses the issues of handling the information content mismatch between\npredictor and target variables and proposes regularization of target property\nprior to building a prediction model.In this thesis, two Artificial Neural\nNetwork (ANN) based frameworks are proposed to model sand fraction from\nmultiple seismic attributes without and with well tops information\nrespectively. The performances of the frameworks are quantified in terms of\nCorrelation Coefficient, Root Mean Square Error, Absolute Error Mean, etc.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 16:20:23 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 16:09:33 GMT"}], "update_date": "2015-07-23", "authors_parsed": [["Chaki", "Soumi", ""]]}, {"id": "1506.05101", "submitter": "Dhruba Bhattacharyya", "authors": "Hirak Kashyap, Hasin Afzal Ahmed, Nazrul Hoque, Swarup Roy and Dhruba\n  Kumar Bhattacharyya", "title": "Big Data Analytics in Bioinformatics: A Machine Learning Perspective", "comments": "20 pages survey paper on Big data analytics in Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bioinformatics research is characterized by voluminous and incremental\ndatasets and complex data analytics methods. The machine learning methods used\nin bioinformatics are iterative and parallel. These methods can be scaled to\nhandle big data using the distributed and parallel computing technologies.\n  Usually big data tools perform computation in batch-mode and are not\noptimized for iterative processing and high data dependency among operations.\nIn the recent years, parallel, incremental, and multi-view machine learning\nalgorithms have been proposed. Similarly, graph-based architectures and\nin-memory big data tools have been developed to minimize I/O cost and optimize\niterative processing.\n  However, there lack standard big data architectures and tools for many\nimportant bioinformatics problems, such as fast construction of co-expression\nand regulatory networks and salient module identification, detection of\ncomplexes over growing protein-protein interaction data, fast analysis of\nmassive DNA, RNA, and protein sequence data, and fast querying on incremental\nand heterogeneous disease networks. This paper addresses the issues and\nchallenges posed by several big data problems in bioinformatics, and gives an\noverview of the state of the art and the future research opportunities.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 11:32:00 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Kashyap", "Hirak", ""], ["Ahmed", "Hasin Afzal", ""], ["Hoque", "Nazrul", ""], ["Roy", "Swarup", ""], ["Bhattacharyya", "Dhruba Kumar", ""]]}, {"id": "1506.05185", "submitter": "Paolo Ribeca", "authors": "{\\L}ukasz Roguski, Paolo Ribeca", "title": "CARGO: Effective format-free compressed storage of genomic information", "comments": "13 (Main) + 31 (Supplementary) + 88 (Manual) pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.CE cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent super-exponential growth in the amount of sequencing data\ngenerated worldwide has put techniques for compressed storage into the focus.\nMost available solutions, however, are strictly tied to specific bioinformatics\nformats, sometimes inheriting from them suboptimal design choices; this hinders\nflexible and effective data sharing. Here we present CARGO (Compressed\nARchiving for GenOmics), a high-level framework to automatically generate\nsoftware systems optimized for the compressed storage of arbitrary types of\nlarge genomic data collections. Straightforward applications of our approach to\nFASTQ and SAM archives require a few lines of code, produce solutions that\nmatch and sometimes outperform specialized format-tailored compressors, and\nscale well to multi-TB datasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 02:11:42 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Roguski", "\u0141ukasz", ""], ["Ribeca", "Paolo", ""]]}, {"id": "1506.05887", "submitter": "Jean-Paul Comet", "authors": "Gilles Bernot and Jean-Paul Comet and Zohra Khalis and Adrien Richard\n  and Olivier Roux", "title": "A Genetically Modified Hoare Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem when modeling gene networks lies in the identification\nof parameters, even if we consider a purely discrete framework as the one of\nRen\\'e Thomas. Here we are interested in the exhaustive search of all parameter\nvalues that are consistent with observed behaviors of the gene network. We\npresent in this article a new approach based on Hoare Logic and on a weakest\nprecondition calculus to generate constraints on possible parameter values.\nObserved behaviors play the role of \"programs\" for the classical Hoare logic,\nand computed weakest preconditions represent the sets of all compatible\nparameterizations expressed as constraints on parameters. Finally we give a\nproof of correctness of our Hoare logic for gene networks as well as a proof of\ncompleteness based on the computation of the weakest precondition.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 06:37:46 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Bernot", "Gilles", ""], ["Comet", "Jean-Paul", ""], ["Khalis", "Zohra", ""], ["Richard", "Adrien", ""], ["Roux", "Olivier", ""]]}, {"id": "1506.05905", "submitter": "Anmer Daskin", "authors": "Anmer Daskin", "title": "Quantum IsoRank: Efficient Alignment of Multiple PPI Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.MN quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparative analyses of protein-protein interaction networks play important\nroles in the understanding of biological processes. However, the growing\nenormity of available data on the networks becomes a computational challenge\nfor the conventional alignment algorithms. Quantum algorithms generally provide\ngreater efficiency over their classical counterparts in solving various\nproblems.\n  One of such algorithms is the quantum phase estimation algorithm which\ngenerates the principal eigenvector of a stochastic matrix with probability\none.\n  Using the quantum phase estimation algorithm, we introduce a quantum\ncomputing approach for the alignment of protein-protein interaction networks by\nfollowing the classical algorithm IsoRank which uses the principal eigenvector\nof the stochastic matrix representing the Kronecker product of the normalized\nadjacency matrices of networks for the pairwise alignment. We also present a\ngreedy quantum measurement scheme to efficiently procure the alignment from the\noutput state of the phase estimation algorithm where the eigenvector is encoded\nas the amplitudes of this state. The complexity of the quantum approach\noutperforms the classical running time.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 08:25:29 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 12:07:18 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Daskin", "Anmer", ""]]}, {"id": "1506.05996", "submitter": "Rajesh Gandham", "authors": "J.-F. Remacle, R. Gandham, T. Warburton", "title": "GPU accelerated spectral finite elements on all-hex meshes", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2016.08.005", "report-no": null, "categories": "cs.CE cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a spectral element finite element scheme that efficiently\nsolves elliptic problems on unstructured hexahedral meshes. The discrete\nequations are solved using a matrix-free preconditioned conjugate gradient\nalgorithm. An additive Schwartz two-scale preconditioner is employed that\nallows h-independence convergence. An extensible multi-threading programming\nAPI is used as a common kernel language that allows runtime selection of\ndifferent computing devices (GPU and CPU) and different threading interfaces\n(CUDA, OpenCL and OpenMP). Performance tests demonstrate that problems with\nover 50 million degrees of freedom can be solved in a few seconds on an\noff-the-shelf GPU.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 13:27:05 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Remacle", "J. -F.", ""], ["Gandham", "R.", ""], ["Warburton", "T.", ""]]}, {"id": "1506.06102", "submitter": "Paul Bauman T.", "authors": "Paul T. Bauman, Roy H. Stogner", "title": "GRINS: A Multiphysics Framework Based on the libMesh Finite Element\n  Library", "comments": "Submitted to SISC CSE Special Issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progression of scientific computing resources has enabled the numerical\napproximation of mathematical models describing complex physical phenomena. A\nsignificant portion of researcher time is typically dedicated to the\ndevelopment of software to compute the numerical solutions. This work describes\na flexible C++ software framework, built on the libMesh finite element library,\ndesigned to alleviate developer burden and provide easy access to modern\ncomputational algorithms, including quantity-of-interest-driven parallel\nadaptive mesh refinement on unstructured grids and adjoint-based sensitivities.\nOther software environments are highlighted and the current work motivated; in\nparticular, the present work is an attempt to balance software infrastructure\nand user flexibility. The applicable class of problems and design of the\nsoftware components is discussed in detail. Several examples demonstrate the\neffectiveness of the design, including applications that incorporate\nuncertainty. Current and planned developments are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 18:04:01 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Bauman", "Paul T.", ""], ["Stogner", "Roy H.", ""]]}, {"id": "1506.06366", "submitter": "He-Wen Chen", "authors": "He-Wen Chen and Zih-Ci Wang and Shu-Yu Kuo and Yao-Hsin Chou", "title": "A Novel Method for Stock Forecasting based on Fuzzy Time Series Combined\n  with the Longest Common/Repeated Sub-sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock price forecasting is an important issue for investors since extreme\naccuracy in forecasting can bring about high profits. Fuzzy Time Series (FTS)\nand Longest Common/Repeated Sub-sequence (LCS/LRS) are two important issues for\nforecasting prices. However, to the best of our knowledge, there are no\nsignificant studies using LCS/LRS to predict stock prices. It is impossible\nthat prices stay exactly the same as historic prices. Therefore, this paper\nproposes a state-of-the-art method which combines FTS and LCS/LRS to predict\nstock prices. This method is based on the principle that history will repeat\nitself. It uses different interval lengths in FTS to fuzzify the prices, and\nLCS/LRS to look for the same pattern in the historical prices to predict future\nstock prices. In the experiment, we examine various intervals of fuzzy time\nsets in order to achieve high prediction accuracy. The proposed method\noutperforms traditional methods in terms of prediction accuracy and,\nfurthermore, it is easy to implement.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 14:03:42 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Chen", "He-Wen", ""], ["Wang", "Zih-Ci", ""], ["Kuo", "Shu-Yu", ""], ["Chou", "Yao-Hsin", ""]]}, {"id": "1506.06972", "submitter": "Gergo Barta", "authors": "Gergo Barta, Gyula Borbely, Gabor Nagy, Sandor Kazi, Tamas Henk", "title": "GEFCOM 2014 - Probabilistic Electricity Price Forecasting", "comments": "10 pages, 5 figures, KES-IDT 2015 conference. The final publication\n  is available at Springer via http://dx.doi.org/10.1007/978-3-319-19857-6_7", "journal-ref": null, "doi": "10.1007/978-3-319-19857-6_7", "report-no": null, "categories": "stat.ML cs.CE cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy price forecasting is a relevant yet hard task in the field of\nmulti-step time series forecasting. In this paper we compare a well-known and\nestablished method, ARMA with exogenous variables with a relatively new\ntechnique Gradient Boosting Regression. The method was tested on data from\nGlobal Energy Forecasting Competition 2014 with a year long rolling window\nforecast. The results from the experiment reveal that a multi-model approach is\nsignificantly better performing in terms of error metrics. Gradient Boosting\ncan deal with seasonality and auto-correlation out-of-the box and achieve lower\nrate of normalized mean absolute error on real-world data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 12:27:50 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Barta", "Gergo", ""], ["Borbely", "Gyula", ""], ["Nagy", "Gabor", ""], ["Kazi", "Sandor", ""], ["Henk", "Tamas", ""]]}, {"id": "1506.07214", "submitter": "Pascal Van Hentenryck", "authors": "Conrado Borraz-Sanchez, Russell Bent, Scott Backhaus, Hassan Hijazi,\n  Pascal Van Hentenryck", "title": "Convex Relaxations for Gas Expansion Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expansion of natural gas networks is a critical process involving substantial\ncapital expenditures with complex decision-support requirements. Given the\nnon-convex nature of gas transmission constraints, global optimality and\ninfeasibility guarantees can only be offered by global optimisation approaches.\nUnfortunately, state-of-the-art global optimisation solvers are unable to scale\nup to real-world size instances. In this study, we present a convex\nmixed-integer second-order cone relaxation for the gas expansion planning\nproblem under steady-state conditions. The underlying model offers tight lower\nbounds with high computational efficiency. In addition, the optimal solution of\nthe relaxation can often be used to derive high-quality solutions to the\noriginal problem, leading to provably tight optimality gaps and, in some cases,\nglobal optimal soluutions. The convex relaxation is based on a few key ideas,\nincluding the introduction of flux direction variables, exact McCormick\nrelaxations, on/off constraints, and integer cuts. Numerical experiments are\nconducted on the traditional Belgian gas network, as well as other real larger\nnetworks. The results demonstrate both the accuracy and computational speed of\nthe relaxation and its ability to produce high-quality solutions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 00:04:44 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Borraz-Sanchez", "Conrado", ""], ["Bent", "Russell", ""], ["Backhaus", "Scott", ""], ["Hijazi", "Hassan", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1506.07220", "submitter": "Hui Jiang", "authors": "Yangtuo Peng and Hui Jiang", "title": "Leverage Financial News to Predict Stock Price Movements Using Word\n  Embeddings and Deep Neural Networks", "comments": "5 pages, 2 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial news contains useful information on public companies and the\nmarket. In this paper we apply the popular word embedding methods and deep\nneural networks to leverage financial news to predict stock price movements in\nthe market. Experimental results have shown that our proposed methods are\nsimple but very effective, which can significantly improve the stock prediction\naccuracy on a standard financial database over the baseline system using only\nthe historical price information.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 01:43:11 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Peng", "Yangtuo", ""], ["Jiang", "Hui", ""]]}, {"id": "1506.07411", "submitter": "Jaderick Pabico", "authors": "Merly F. Tataro, Marian G. Arada, Jaderick P. Pabico", "title": "Selecting the Best Traffic Scheme for the Bicutan Roundabout: A\n  Microsimulation Approach with Multiple Driver-Agents", "comments": "9 pages, 5 figures", "journal-ref": "Asia Pacific Journal of Multidisciplinary Research 3(3):127-135,\n  2015", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present the result of our microsimulation study on the effects of six\ntraffic schemes $T=\\{t_0, t_1, \\dots, t_5\\}$ on the mean total delay time\n($\\Delta$) and mean speed ($\\Sigma$) of vehicles at the non-signalized Bicutan\nRoundabout (BR) in Upper Bicutan, Taguig City, Metro Manila, with $t_0$ as the\ncurrent traffic scheme being enforced and $t_{i>0}$ as the proposed ones. We\npresent first that our simulation approach, a hybridized multi-agent system\n(MAS) with the car-following and lane-changing models (CLM), can mimic the\ncurrent observed traffic scenario $C$ at a statistical significance of\n$\\alpha=0.05$. That is, the respective absolute differences of the $\\Delta$ and\n$\\Sigma$ between $C$ and $t_0$ are not statistically different from zero. Next,\nusing our MAS-CLM, we simulated all proposed $t_{i>0}$ and compared their\nrespective $\\Delta$ and $\\Sigma$. We found out using DMRT that the best traffic\nscheme is $t_3$ (i.e., when we converted the bi-directional 4-lane PNR-PNCC\nroad into a bi-directional 1-lane PNR-to-PNCC and 3-lane PNCC-to-PNR routes\nduring rush hours). Then, we experimented on converting BR into a signalized\njunction and re-implemented all $t_3$ with controlled stops of $S=\\{15s,\n45s\\}$. We found out that $t_3$ with a 15-s stop has the best performance.\nFinally, we simulated the effect of increased in vehicular volume $V$ due to\ntraffic improvement and we found out that $t_3$ with 15-s stop still\noutperforms the others for all increased in $V=\\{10\\%, 50\\%, 100\\%\\}$.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 15:07:12 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Tataro", "Merly F.", ""], ["Arada", "Marian G.", ""], ["Pabico", "Jaderick P.", ""]]}, {"id": "1506.07424", "submitter": "Jaderick Pabico", "authors": "Marian G. Arada, Merly F. Tataro, Jaderick P. Pabico", "title": "Simulating the Effects of Various Road Infrastructure Improvements to\n  Vehicular Traffic in a Busy Three-road Fork", "comments": "9 pages, 5 figures, Proceedings of the 11th National Conference on\n  Information Technology Education (NCITE 2013), pp. 195-203", "journal-ref": "Philipine Information Technology Journal 8(1):49-57, June 2015", "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Using microsimulations of vehicular dynamics, we studied the effects of\nseveral proposed infrastructure developments to the mean travel delay\ntime~$\\Delta$ and mean speed~$\\Sigma$ of vehicles passing a busy three-road\nfork, particularly in the non-signalized roundabout junction of Lower Bicutan,\nTaguig City, Metro Manila. We designed and implemented multi-agent-based\nmicrosimulation models to mimic the autonomous driving behavior of\nheterogeneous individuals and measured the effect of various proposed\ninfrastructure developments on~$\\Delta$ and~$\\Sigma$. Our aim is to find out\nthe best infrastructure development from among three choices being considered\nby the local government for the purpose of solving the traffic problems in the\narea. We created simulation models of the current vehicular traffic situation\nin the area using the mean travel times~$\\tau$ of statistically sampled\nvehicles to show that our model can simulate the real-world at a significance\nlevel of $\\alpha=0.05$. Based on these models, we then simulated the effect of\nthe proposed infrastructure developments on~$\\Delta$ and~$\\Sigma$ and used\nthese metrics as our basis of comparison. We found out that the proposed\nwidening of one fork from two lanes to three lanes has the most improved\nmetrics at the same $\\alpha=0.05$ compared to the metrics we observed in the\ncurrent situation. Under this infrastructure development, the~$\\Delta$\nincreases linearly ($R^2=0.98$) at the rate of 1.03~$s$, while the~$\\Sigma$\ndecreases linearly ($R^2>0.99$) at the rate of 0.14~$km/h$ per percent increase\nin the total vehicle volume~$\\mathcal{V}$.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 15:36:11 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Arada", "Marian G.", ""], ["Tataro", "Merly F.", ""], ["Pabico", "Jaderick P.", ""]]}, {"id": "1506.08231", "submitter": "Brian Hanley", "authors": "Brian P. Hanley", "title": "A zero-sum monetary system, interest rates, and implications", "comments": "12 pages, 4 figures, 2 tables, 3 equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To the knowledge of the author, this is the first time it has been shown that\ninterest rates that are extremely high by modern standards (100% and higher)\nare necessary within a zero-sum monetary system, and not just driven by greed.\nExtreme interest rates that appeared in various places and times reinforce the\nidea that hard money may have contributed to high rates of interest. Here a\nmodel is presented that examines the interest rate required to succeed as an\ninvestor in a zero-sum fixed quantity hard-money system. Even when the playing\nfield is significantly tilted toward the investor, interest rates need to be\nmuch higher than expected. In a completely fair zero-sum system, an investor\ncannot break even without charging 100% interest. Even with a 5% advantage, an\ninvestor won't break even at 15% interest. From this it is concluded that what\nwe consider usurious rates today are, within a hard-money system, driven by\nnecessity.\n  Cryptocurrency is a novel form of hard-currency. The inability to virtualize\nthe money creates a system close to zero-sum because of the limited supply\ndesign. Therefore, within the bounds of a cryptocurrency system that limits\nmoney creation, interest rates must rise to levels that the modern world\nconsiders usury. It is impossible, therefore, that a cryptocurrency that is not\nexpandable could take over a modern economy and replace modern fiat currency.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 22:23:22 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 21:53:34 GMT"}, {"version": "v3", "created": "Sat, 29 Sep 2018 02:27:52 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Hanley", "Brian P.", ""]]}, {"id": "1506.08235", "submitter": "Hongyi Xin", "authors": "Hongyi Xin, Richard Zhu, Sunny Nahar, John Emmons, Gennady Pekhimenko,\n  Carl Kingsford, Can Alkan and Onur Mutlu", "title": "Optimal Seed Solver: Optimizing Seed Selection in Read Mapping", "comments": "10 pages of main text. 6 pages of supplementary materials. Under\n  review by Oxford Bioinformatics", "journal-ref": "Bioinformatics, Jun 1;32(11):1632-42, 2016", "doi": "10.1093/bioinformatics/btv670", "report-no": null, "categories": "cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Optimizing seed selection is an important problem in read\nmapping. The number of non-overlapping seeds a mapper selects determines the\nsensitivity of the mapper while the total frequency of all selected seeds\ndetermines the speed of the mapper. Modern seed-and-extend mappers usually\nselect seeds with either an equal and fixed-length scheme or with an inflexible\nplacement scheme, both of which limit the potential of the mapper to select\nless frequent seeds to speed up the mapping process. Therefore, it is crucial\nto develop a new algorithm that can adjust both the individual seed length and\nthe seed placement, as well as derive less frequent seeds.\n  Results: We present the Optimal Seed Solver (OSS), a dynamic programming\nalgorithm that discovers the least frequently-occurring set of x seeds in an\nL-bp read in $O(x \\times L)$ operations on average and in $O(x \\times L^{2})$\noperations in the worst case. We compared OSS against four state-of-the-art\nseed selection schemes and observed that OSS provides a 3-fold reduction of\naverage seed frequency over the best previous seed selection optimizations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 23:32:29 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Xin", "Hongyi", ""], ["Zhu", "Richard", ""], ["Nahar", "Sunny", ""], ["Emmons", "John", ""], ["Pekhimenko", "Gennady", ""], ["Kingsford", "Carl", ""], ["Alkan", "Can", ""], ["Mutlu", "Onur", ""]]}, {"id": "1506.08258", "submitter": "Ali Pinar", "authors": "Janine C. Bennett, Ankit Bhagatwala, Jacqueline H. Chen, C. Seshadhri,\n  Ali Pinar, Maher Salloum", "title": "Trigger detection for adaptive scientific workflows using percentile\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing complexity of scientific simulations and HPC architectures are\ndriving the need for adaptive workflows, where the composition and execution of\ncomputational and data manipulation steps dynamically depend on the\nevolutionary state of the simulation itself. Consider for example, the\nfrequency of data storage. Critical phases of the simulation should be captured\nwith high frequency and with high fidelity for post-analysis, however we cannot\nafford to retain the same frequency for the full simulation due to the high\ncost of data movement. We can instead look for triggers, indicators that the\nsimulation will be entering a critical phase and adapt the workflow\naccordingly.\n  We present a method for detecting triggers and demonstrate its use in direct\nnumerical simulations of turbulent combustion using S3D. We show that chemical\nexplosive mode analysis (CEMA) can be used to devise a noise-tolerant indicator\nfor rapid increase in heat release. However, exhaustive computation of CEMA\nvalues dominates the total simulation, thus is prohibitively expensive. To\novercome this bottleneck, we propose a quantile-sampling approach. Our\nalgorithm comes with provable error/confidence bounds, as a function of the\nnumber of samples. Most importantly, the number of samples is independent of\nthe problem size, thus our proposed algorithm offers perfect scalability. Our\nexperiments on homogeneous charge compression ignition (HCCI) and reactivity\ncontrolled compression ignition (RCCI) simulations show that the proposed\nmethod can detect rapid increases in heat release, and its computational\noverhead is negligible. Our results will be used for dynamic workflow decisions\nabout data storage and mesh resolution in future combustion simulations.\nProposed framework is generalizable and we detail how it could be applied to a\nbroad class of scientific simulation workflows.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 04:34:26 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Bennett", "Janine C.", ""], ["Bhagatwala", "Ankit", ""], ["Chen", "Jacqueline H.", ""], ["Seshadhri", "C.", ""], ["Pinar", "Ali", ""], ["Salloum", "Maher", ""]]}, {"id": "1506.08435", "submitter": "Kalyana Babu Nakshatrala", "authors": "J. Chang, S. Karra and K. B. Nakshatrala", "title": "Large-scale Optimization-based Non-negative Computational Framework for\n  Diffusion Equations: Parallel Implementation and Performance Studies", "comments": null, "journal-ref": null, "doi": "10.1007/s10915-016-0250-5", "report-no": null, "categories": "cs.NA cs.CE cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that the standard Galerkin formulation, which is often the\nformulation of choice under the finite element method for solving self-adjoint\ndiffusion equations, does not meet maximum principles and the non-negative\nconstraint for anisotropic diffusion equations. Recently, optimization-based\nmethodologies that satisfy maximum principles and the non-negative constraint\nfor steady-state and transient diffusion-type equations have been proposed. To\ndate, these methodologies have been tested only on small-scale academic\nproblems. The purpose of this paper is to systematically study the performance\nof the non-negative methodology in the context of high performance computing\n(HPC). PETSc and TAO libraries are, respectively, used for the parallel\nenvironment and optimization solvers. For large-scale problems, it is important\nfor computational scientists to understand the computational performance of\ncurrent algorithms available in these scientific libraries. The numerical\nexperiments are conducted on the state-of-the-art HPC systems, and a\nsingle-core performance model is used to better characterize the efficiency of\nthe solvers. Our studies indicate that the proposed non-negative computational\nframework for diffusion-type equations exhibits excellent strong scaling for\nreal-world large-scale problems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 18:57:25 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2015 21:45:18 GMT"}, {"version": "v3", "created": "Sat, 9 Apr 2016 18:44:40 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Chang", "J.", ""], ["Karra", "S.", ""], ["Nakshatrala", "K. B.", ""]]}, {"id": "1506.08527", "submitter": "Christoph Koutschan", "authors": "Christoph Koutschan, Helene Ranetbauer, Georg Regensburger,\n  Marie-Therese Wolfram", "title": "Symbolic Derivation of Mean-Field PDEs from Lattice-Based Models", "comments": null, "journal-ref": "Proceedings of the 17th International Symposium on Symbolic and\n  Numeric Algorithms for Scientific Computing (SYNASC), pp. 27-33, 2015. ISBN\n  978-1-5090-0461-4", "doi": "10.1109/SYNASC.2015.14", "report-no": null, "categories": "cs.SC cs.CE math.AP nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation processes, which play a prominent role in the life and social\nsciences, are typically described by discrete models on lattices. For studying\ntheir dynamics a continuous formulation of the problem via partial differential\nequations (PDE) is employed. In this paper we propose a symbolic computation\napproach to derive mean-field PDEs from a lattice-based model. We start with\nthe microscopic equations, which state the probability to find a particle at a\ngiven lattice site. Then the PDEs are formally derived by Taylor expansions of\nthe probability densities and by passing to an appropriate limit as the time\nsteps and the distances between lattice sites tend to zero. We present an\nimplementation in a computer algebra system that performs this transition for a\ngeneral class of models. In order to rewrite the mean-field PDEs in a\nconservative formulation, we adapt and implement symbolic integration methods\nthat can handle unspecified functions in several variables. To illustrate our\napproach, we consider an application in crowd motion analysis where the\ndynamics of bidirectional flows are studied. However, the presented approach\ncan be applied to various transportation processes of multiple species with\nvariable size in any dimension, for example, to confirm several proposed\nmean-field models for cell motility.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 07:50:21 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2015 17:22:18 GMT"}, {"version": "v3", "created": "Mon, 21 Mar 2016 12:54:00 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Koutschan", "Christoph", ""], ["Ranetbauer", "Helene", ""], ["Regensburger", "Georg", ""], ["Wolfram", "Marie-Therese", ""]]}, {"id": "1506.08691", "submitter": "Attila Wohlbrandt", "authors": "Attila Wohlbrandt and Nan Hu and Sebastien Guerin and Roland Ewert", "title": "Analytical reconstruction of isotropic turbulence spectra based on the\n  Gaussian transform", "comments": "Preprint, submitted to Computers & Fluids", "journal-ref": null, "doi": "10.1016/j.compfluid.2016.03.023", "report-no": null, "categories": "cs.CE physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Random Particle Mesh (RPM) method used to simulate turbulence-induced\nbroadband noise in several aeroacoustic applications is extended to realise\nisotropic turbulence spectra. With this method turbulent fluctuations are\nsynthesised by filtering white noise with a Gaussian filter kernel that in turn\ngives a Gaussian spectrum. The Gaussian function is smooth and its derivatives\nand integrals are again Gaussian functions. The Gaussian filter is efficient\nand finds wide-spread applications in stochastic signal processing. However in\nmany applications Gaussian spectra do not correspond to real turbulence\nspectra. Thus in turbo-machines the von K\\'arm\\'an, Liepmann, and modified von\nK\\'arm\\'an spectra are more realistic model spectra. In this note we\nanalytically derive weighting functions to realise arbitrary isotropic\nsolenoidal spectra using a superposition of weighted Gaussian spectra of\ndifferent length scales. The analytic weighting functions for the von\nK\\'arm\\'an, the Liepmann, and the modified von K\\'arm\\'an spectra are derived\nsubsequently. Finally a method is proposed to discretise the problem using a\nlimited number of Gaussian spectra. The effectivity of this approach is\ndemonstrated by realising a von K\\'arm\\'an velocity spectrum using the RPM\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 15:43:22 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 15:09:15 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Wohlbrandt", "Attila", ""], ["Hu", "Nan", ""], ["Guerin", "Sebastien", ""], ["Ewert", "Roland", ""]]}, {"id": "1506.08781", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "On Design Mining: Coevolution and Surrogate Models", "comments": null, "journal-ref": "Artificial Life (2017), 23(2):186-205", "doi": "10.1162/ARTL_a_00225", "report-no": null, "categories": "cs.NE cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design mining is the use of computational intelligence techniques to\niteratively search and model the attribute space of physical objects evaluated\ndirectly through rapid prototyping to meet given objectives. It enables the\nexploitation of novel materials and processes without formal models or complex\nsimulation. In this paper, we focus upon the coevolutionary nature of the\ndesign process when it is decomposed into concurrent sub-design threads due to\nthe overall complexity of the task. Using an abstract, tuneable model of\ncoevolution we consider strategies to sample sub-thread designs for whole\nsystem testing and how best to construct and use surrogate models within the\ncoevolutionary scenario. Drawing on our findings, the paper then describes the\neffective design of an array of six heterogeneous vertical-axis wind turbines.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 18:57:34 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2015 14:05:42 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 17:29:20 GMT"}, {"version": "v4", "created": "Mon, 15 Feb 2016 14:43:39 GMT"}, {"version": "v5", "created": "Wed, 26 Oct 2016 22:09:08 GMT"}, {"version": "v6", "created": "Wed, 23 Nov 2016 20:24:17 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "1506.09000", "submitter": "Ren\\'e Heideklang", "authors": "Ren\\'e Heideklang, Parisa Shokouhi", "title": "Decision-level multi-method fusion of spatially scattered data from\n  nondestructive inspection of ferromagnetic parts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with the fusion of flaw detections from multi-sensor\nnondestructive materials testing. Because each testing method makes use of\ndifferent physical effects for defect localization, a multi-method approach is\npromising to effectively distinguish the many false alarms from actual material\ndefects. To this end, we propose a new fusion technique for scattered two- or\nthree-dimensional location data. Using a density-based approach, the proposed\nmethod is able to explicitly address the localization uncertainties such as\nregistration errors. We provide guidelines on how to set all key parameters and\ndemonstrate the technique's robustness. Finally, we apply our fusion approach\nto experimental data and demonstrate its ability to find small defects by\nsubstantially reducing false alarms under conditions where no single-sensor\nmethod is adequate.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 09:13:13 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Heideklang", "Ren\u00e9", ""], ["Shokouhi", "Parisa", ""]]}, {"id": "1506.09153", "submitter": "Gunnar R\\\"atsch", "authors": "Christian Widmer, Marius Kloft, Vipin T Sreedharan, Gunnar R\\\"atsch", "title": "Framework for Multi-task Multiple Kernel Learning and Applications in\n  Genome Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general regularization-based framework for Multi-task learning\n(MTL), in which the similarity between tasks can be learned or refined using\n$\\ell_p$-norm Multiple Kernel learning (MKL). Based on this very general\nformulation (including a general loss function), we derive the corresponding\ndual formulation using Fenchel duality applied to Hermitian matrices. We show\nthat numerous established MTL methods can be derived as special cases from\nboth, the primal and dual of our formulation. Furthermore, we derive a modern\ndual-coordinate descend optimization strategy for the hinge-loss variant of our\nformulation and provide convergence bounds for our algorithm. As a special\ncase, we implement in C++ a fast LibLinear-style solver for $\\ell_p$-norm MKL.\nIn the experimental section, we analyze various aspects of our algorithm such\nas predictive performance and ability to reconstruct task relationships on\nbiologically inspired synthetic data, where we have full control over the\nunderlying ground truth. We also experiment on a new dataset from the domain of\ncomputational biology that we collected for the purpose of this paper. It\nconcerns the prediction of transcription start sites (TSS) over nine organisms,\nwhich is a crucial task in gene finding. Our solvers including all discussed\nspecial cases are made available as open-source software as part of the SHOGUN\nmachine learning toolbox (available at \\url{http://shogun.ml}).\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 16:52:27 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Widmer", "Christian", ""], ["Kloft", "Marius", ""], ["Sreedharan", "Vipin T", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1506.09163", "submitter": "Gautier Marti", "authors": "Gautier Marti and Frank Nielsen and Philippe Very and Philippe Donnat", "title": "Comment partitionner automatiquement des marches al\\'eatoires ? Avec\n  application \\`a la finance quantitative", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a novel non-parametric approach useful for\nclustering Markov processes. We introduce a pre-processing step consisting in\nmapping multivariate independent and identically distributed samples from\nrandom variables to a generic non-parametric representation which factorizes\ndependency and marginal distribution apart without losing any. An associated\nmetric is defined where the balance between random variables dependency and\ndistribution information is controlled by a single parameter. This mixing\nparameter can be learned or played with by a practitioner, such use is\nillustrated on the case of clustering financial time series. Experiments,\nimplementation and results obtained on public financial time series are online\non a web portal \\url{http://www.datagrapple.com}.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 17:17:10 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Marti", "Gautier", ""], ["Nielsen", "Frank", ""], ["Very", "Philippe", ""], ["Donnat", "Philippe", ""]]}]