[{"id": "1506.00176", "submitter": "Lianwen Jin", "authors": "Liquan Qiu, Lianwen Jin, Ruifen Dai, Yuxiang Zhang, Lei Li", "title": "An Open Source Testing Tool for Evaluating Handwriting Input Methods", "comments": "5 pages, 3 figures, 11 tables. Accepted to appear at ICDAR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an open source tool for testing the recognition accuracy\nof Chinese handwriting input methods. The tool consists of two modules, namely\nthe PC and Android mobile client. The PC client reads handwritten samples in\nthe computer, and transfers them individually to the Android client in\naccordance with the socket communication protocol. After the Android client\nreceives the data, it simulates the handwriting on screen of client device, and\ntriggers the corresponding handwriting recognition method. The recognition\naccuracy is recorded by the Android client. We present the design principles\nand describe the implementation of the test platform. We construct several test\ndatasets for evaluating different handwriting recognition systems, and conduct\nan objective and comprehensive test using six Chinese handwriting input methods\nwith five datasets. The test results for the recognition accuracy are then\ncompared and analyzed.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 22:35:55 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Qiu", "Liquan", ""], ["Jin", "Lianwen", ""], ["Dai", "Ruifen", ""], ["Zhang", "Yuxiang", ""], ["Li", "Lei", ""]]}, {"id": "1506.00527", "submitter": "Gianluigi Ciocca", "authors": "Simone Bianco, Gianluigi Ciocca", "title": "User Preferences Modeling and Learning for Pleasing Photo Collage\n  Generation", "comments": "To be published in ACM Transactions on Multimedia Computing,\n  Communications, and Applications (TOMM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider how to automatically create pleasing photo collages\ncreated by placing a set of images on a limited canvas area. The task is\nformulated as an optimization problem. Differently from existing\nstate-of-the-art approaches, we here exploit subjective experiments to model\nand learn pleasantness from user preferences. To this end, we design an\nexperimental framework for the identification of the criteria that need to be\ntaken into account to generate a pleasing photo collage. Five different\nthematic photo datasets are used to create collages using state-of-the-art\ncriteria. A first subjective experiment where several subjects evaluated the\ncollages, emphasizes that different criteria are involved in the subjective\ndefinition of pleasantness. We then identify new global and local criteria and\ndesign algorithms to quantify them. The relative importance of these criteria\nare automatically learned by exploiting the user preferences, and new collages\nare generated. To validate our framework, we performed several psycho-visual\nexperiments involving different users. The results shows that the proposed\nframework allows to learn a novel computational model which effectively encodes\nan inter-user definition of pleasantness. The learned definition of\npleasantness generalizes well to new photo datasets of different themes and\nsizes not used in the learning. Moreover, compared with two state of the art\napproaches, the collages created using our framework are preferred by the\nmajority of the users.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 15:20:29 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Bianco", "Simone", ""], ["Ciocca", "Gianluigi", ""]]}, {"id": "1506.00930", "submitter": "Tiago Guerreiro", "authors": "Diogo Marques, Lu\\'is Carri\\c{c}o, Tiago Guerreiro", "title": "Assessing Inconspicuous Smartphone Authentication for Blind People", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  As people store more personal data in their smartphones, the consequences of\nhaving it stolen or lost become an increasing concern. A typical\ncounter-measure to avoid this risk is to set up a secret code that has to be\nentered to unlock the device after a period of inactivity. However, for blind\nusers, PINs and passwords are inadequate, since entry 1) consumes a non-trivial\namount of time, e.g. using screen readers, 2) is susceptible to observation,\nwhere nearby people can see or hear the secret code, and 3) might collide with\nsocial norms, e.g. disrupting personal interactions. Tap-based authentication\nmethods have been presented and allow unlocking to be performed in a short time\nand support naturally occurring inconspicuous behavior (e.g. concealing the\ndevice inside a jacket) by being usable with a single hand. This paper presents\na study with blind users (N = 16) where an authentication method based on tap\nphrases is evaluated. Results showed the method to be usable and to support the\ndesired inconspicuity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 15:40:32 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Marques", "Diogo", ""], ["Carri\u00e7o", "Lu\u00eds", ""], ["Guerreiro", "Tiago", ""]]}, {"id": "1506.01062", "submitter": "Panos Ipeirotis", "authors": "Panagiotis G. Ipeirotis and Evgeniy Gabrilovich", "title": "Quizz: Targeted crowdsourcing with a billion (potential) users", "comments": "WWW '14 Proceedings of the 23rd international conference on World\n  Wide Web. 11 pages", "journal-ref": null, "doi": "10.1145/2566486.2567988", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Quizz, a gamified crowdsourcing system that simultaneously\nassesses the knowledge of users and acquires new knowledge from them. Quizz\noperates by asking users to complete short quizzes on specific topics; as a\nuser answers the quiz questions, Quizz estimates the user's competence. To\nacquire new knowledge, Quizz also incorporates questions for which we do not\nhave a known answer; the answers given by competent users provide useful\nsignals for selecting the correct answers for these questions. Quizz actively\ntries to identify knowledgeable users on the Internet by running advertising\ncampaigns, effectively leveraging the targeting capabilities of existing,\npublicly available, ad placement services. Quizz quantifies the contributions\nof the users using information theory and sends feedback to the\nadvertisingsystem about each user. The feedback allows the ad targeting\nmechanism to further optimize ad placement.\n  Our experiments, which involve over ten thousand users, confirm that we can\ncrowdsource knowledge curation for niche and specialized topics, as the\nadvertising network can automatically identify users with the desired expertise\nand interest in the given topic. We present controlled experiments that examine\nthe effect of various incentive mechanisms, highlighting the need for having\nshort-term rewards as goals, which incentivize the users to contribute.\nFinally, our cost-quality analysis indicates that the cost of our approach is\nbelow that of hiring workers through paid-crowdsourcing platforms, while\noffering the additional advantage of giving access to billions of potential\nusers all over the planet, and being able to reach users with specialized\nexpertise that is not typically available through existing labor marketplaces.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 21:12:22 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Ipeirotis", "Panagiotis G.", ""], ["Gabrilovich", "Evgeniy", ""]]}, {"id": "1506.01943", "submitter": "Sakyoud Zakaria", "authors": "Sakyoud Zakaria, Ga\\\"etan Rey, Eladnani Mohamed, St\\'ephane Lavirotte,\n  El Fazziki Abdelaziz, Jean-Yves Tigli", "title": "Smart Geographic object: Toward a new understanding of GIS Technology in\n  Ubiquitous Computing", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Volume 12,\n  Issue 2, March 2015", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental aspects of ubiquitous computing is the instrumentation\nof the real world by smart devices. This instrumentation constitutes an\nopportunity to rethink the interactions between human beings and their\nenvironment on the one hand, and between the components of this environment on\nthe other. In this paper we discuss what this understanding of ubiquitous\ncomputing can bring to geographic science and particularly to GIS technology.\nOur main idea is the instrumentation of the geographic environment through the\ninstrumentation of geographic objects composing it. And then investigate how\nthis instrumentation can meet the current limitations of GIS technology, and\noffers a new stage of rapprochement between the earth and its abstraction. As\nresult, the current research work proposes a new concept we named Smart\nGeographic Object SGO. The latter is a convergence point between the smart\nobjects and geographic objects, two concepts appertaining respectively to t\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 15:45:50 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Zakaria", "Sakyoud", ""], ["Rey", "Ga\u00ebtan", ""], ["Mohamed", "Eladnani", ""], ["Lavirotte", "St\u00e9phane", ""], ["Abdelaziz", "El Fazziki", ""], ["Tigli", "Jean-Yves", ""]]}, {"id": "1506.01976", "submitter": "Jeffrey Peters", "authors": "Jeffrey R. Peters, Amit Surana, Luca Bertuccelli", "title": "Eye-Tracking Metrics for Task-Based Supervisory Control", "comments": "6 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-based, rather than vehicle-based, control architectures have been shown\nto provide superior performance in certain human supervisory control missions.\nThese results motivate the need for the development of robust, reliable\nusability metrics to aid in creating interfaces for use in this domain. To this\nend, we conduct a pilot usability study of a particular task-based supervisory\ncontrol interface called the Research Environment for Supervisory Control of\nHeterogenous Unmanned Vehicles (RESCHU). In particular, we explore the use of\neye-tracking metrics as an objective means of evaluating the RESCHU interface\nand providing guidance in improving usability. Our main goals for this study\nare to 1) better understand how eye-tracking can augment standard usability\nmetrics, 2) formulate initial models of operator behavior, and 3) identify\ninteresting areas of future research.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 17:08:28 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Peters", "Jeffrey R.", ""], ["Surana", "Amit", ""], ["Bertuccelli", "Luca", ""]]}, {"id": "1506.02200", "submitter": "Lesandro Ponciano", "authors": "Lesandro Ponciano and Francisco Brasileiro and Nazareno Andrade and\n  L\\'ivia Sampaio", "title": "Considering Human Aspects on Strategies for Designing and Managing\n  Distributed Human Computation", "comments": "3 figures, 1 table", "journal-ref": "Journal of Internet Services and Applications 2014, 5:10", "doi": "10.1186/s13174-014-0010-4", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A human computation system can be viewed as a distributed system in which the\nprocessors are humans, called workers. Such systems harness the cognitive power\nof a group of workers connected to the Internet to execute relatively simple\ntasks, whose solutions, once grouped, solve a problem that systems equipped\nwith only machines could not solve satisfactorily. Examples of such systems are\nAmazon Mechanical Turk and the Zooniverse platform. A human computation\napplication comprises a group of tasks, each of them can be performed by one\nworker. Tasks might have dependencies among each other. In this study, we\npropose a theoretical framework to analyze such type of application from a\ndistributed systems point of view. Our framework is established on three\ndimensions that represent different perspectives in which human computation\napplications can be approached: quality-of-service requirements, design and\nmanagement strategies, and human aspects. By using this framework, we review\nhuman computation in the perspective of programmers seeking to improve the\ndesign of human computation applications and managers seeking to increase the\neffectiveness of human computation infrastructures in running such\napplications. In doing so, besides integrating and organizing what has been\ndone in this direction, we also put into perspective the fact that the human\naspects of the workers in such systems introduce new challenges in terms of,\nfor example, task assignment, dependency management, and fault prevention and\ntolerance. We discuss how they are related to distributed systems and other\nareas of knowledge.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 22:38:10 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Ponciano", "Lesandro", ""], ["Brasileiro", "Francisco", ""], ["Andrade", "Nazareno", ""], ["Sampaio", "L\u00edvia", ""]]}, {"id": "1506.02245", "submitter": "Min Chen", "authors": "Min Chen and Amos Golan", "title": "What May Visualization Processes Optimize?", "comments": "10 pages", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics,\n  22(12):2619-2632, 2016", "doi": "10.1109/TVCG.2015.2513410", "report-no": null, "categories": "cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an abstract model of visualization and inference\nprocesses and describe an information-theoretic measure for optimizing such\nprocesses. In order to obtain such an abstraction, we first examined six\nclasses of workflows in data analysis and visualization, and identified four\nlevels of typical visualization components, namely disseminative,\nobservational, analytical and model-developmental visualization. We noticed a\ncommon phenomenon at different levels of visualization, that is, the\ntransformation of data spaces (referred to as alphabets) usually corresponds to\nthe reduction of maximal entropy along a workflow. Based on this observation,\nwe establish an information-theoretic measure of cost-benefit ratio that may be\nused as a cost function for optimizing a data visualization process. To\ndemonstrate the validity of this measure, we examined a number of successful\nvisualization processes in the literature, and showed that the\ninformation-theoretic measure can mathematically explain the advantages of such\nprocesses over possible alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 09:57:58 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Chen", "Min", ""], ["Golan", "Amos", ""]]}, {"id": "1506.02263", "submitter": "Dmitry Namiot", "authors": "Dmitry Namiot, Manfred Sneps-Sneppe", "title": "On Network Proximity in Web Applications", "comments": "submitted to DCCN 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.NI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss one approach for development and deployment of web\nsites (web pages) devoted to the description of objects (events) with a\nprecisely delineated geographic scope. This article describes the usage of\ncontext-aware programming models for web development. In our paper, we propose\nmechanisms to create mobile web applications which content links to some\npredefined geographic area. The accuracy of such a binding allows us to\ndistinguish individual areas within the same indoor space. Target areas for\nsuch development are applications for Smart Cities and retail.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 13:43:58 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Namiot", "Dmitry", ""], ["Sneps-Sneppe", "Manfred", ""]]}, {"id": "1506.04105", "submitter": "Marta Piekarska", "authors": "Marta Piekarska, Yun Zhou, Dominik Strohmeier and Alexander Raake", "title": "Because we care: Privacy Dashboard on Firefox OS", "comments": "In Proceedings of the 9th Workshop on Web 2.0 Security and Privacy\n  (W2SP) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Privacy Dashboard -- a tool designed to inform\nand empower the people using mobile devices, by introducing features such as\nRemote Privacy Protection, Backup, Adjustable Location Accuracy, Permission\nControl and Secondary-User Mode. We have implemented our solution on FirefoxOS\nand conducted user studies to verify the usefulness and usability of our tool.\nThe paper starts with a discussion of different aspects of mobile privacy, how\nusers perceive it and how much they are willing to give up for better\nusability. Then we describe the tool in detail, presenting what incentives\ndrove us to certain design decisions. During our studies we tried to understand\nhow users interact with the system and what are their priorities. We have\nverified our hypothesis, and the impact of the educational aspects on the\ndecisions about the privacy settings. We show that by taking a user-centric\ndevelopment of privacy extensions we can reduce the gap between protection and\nusability.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 18:43:34 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Piekarska", "Marta", ""], ["Zhou", "Yun", ""], ["Strohmeier", "Dominik", ""], ["Raake", "Alexander", ""]]}, {"id": "1506.04226", "submitter": "Tomasz Rutkowski", "authors": "Tomasz M. Rutkowski", "title": "Student Teaching and Research Laboratory Focusing on Brain-computer\n  Interface Paradigms - A Creative Environment for Computer Science Students -", "comments": "4 pages, 4 figures, accepted for EMBC 2015, IEEE copyright", "journal-ref": null, "doi": "10.1109/EMBC.2015.7319188", "report-no": null, "categories": "q-bio.NC cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper presents an applied concept of a brain-computer interface (BCI)\nstudent research laboratory (BCI-LAB) at the Life Science Center of TARA,\nUniversity of Tsukuba, Japan. Several successful case studies of the student\nprojects are reviewed together with the BCI Research Award 2014 winner case.\nThe BCI-LAB design and project-based teaching philosophy is also explained.\nFuture teaching and research directions summarize the review.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 05:15:33 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Rutkowski", "Tomasz M.", ""]]}, {"id": "1506.04333", "submitter": "Nikos Bikakis", "authors": "Nikos Bikakis, John Liagouris, Maria Krommyda, George Papastefanatos,\n  Timos Sellis", "title": "Towards Scalable Visual Exploration of Very Large RDF Graphs", "comments": "12th Extended Semantic Web Conference (ESWC 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we outline our work on developing a disk-based infrastructure\nfor efficient visualization and graph exploration operations over very large\ngraphs. The proposed platform, called graphVizdb, is based on a novel technique\nfor indexing and storing the graph. Particularly, the graph layout is indexed\nwith a spatial data structure, i.e., an R-tree, and stored in a database. In\nruntime, user operations are translated into efficient spatial operations\n(i.e., window queries) in the backend.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 23:23:21 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 21:32:30 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Bikakis", "Nikos", ""], ["Liagouris", "John", ""], ["Krommyda", "Maria", ""], ["Papastefanatos", "George", ""], ["Sellis", "Timos", ""]]}, {"id": "1506.04374", "submitter": "Tomasz Rutkowski", "authors": "Chisaki Nakaizumi, Shoji Makino, and Tomasz M. Rutkowski", "title": "Head-related Impulse Response Cues for Spatial Auditory Brain-computer\n  Interface", "comments": "4 pages, 4 figures, accepted for EMBC 2015, IEEE copyright", "journal-ref": null, "doi": "10.1109/EMBC.2015.7318550", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This study provides a comprehensive test of a head-related impulse response\n(HRIR) cues for a spatial auditory brain-computer interface (saBCI) speller\nparadigm. We present a comparison with the conventional virtual sound\nheadphone-based spatial auditory modality. We propose and optimize the three\ntypes of sound spatialization settings using a variable elevation in order to\nevaluate the HRIR efficacy for the saBCI. Three experienced and seven naive BCI\nusers participated in the three experimental setups based on ten presented\nJapanese syllables. The obtained EEG auditory evoked potentials (AEP) resulted\nwith encouragingly good and stable P300 responses in online BCI experiments.\nOur case study indicated that users could perceive elevation in the saBCI\nexperiments generated using the HRIR measured from a general head model. The\nsaBCI accuracy and information transfer rate (ITR) scores have been improved\ncomparing to the classical horizontal plane-based virtual spatial sound\nreproduction modality, as far as the healthy users in the current pilot study\nare concerned.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 10:36:29 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nakaizumi", "Chisaki", ""], ["Makino", "Shoji", ""], ["Rutkowski", "Tomasz M.", ""]]}, {"id": "1506.04458", "submitter": "Tomasz Rutkowski", "authors": "Kensuke Shimizu, Shoji Makino, and Tomasz M. Rutkowski", "title": "Inter-stimulus Interval Study for the Tactile Point-pressure\n  Brain-computer Interface", "comments": "4 pages, 5 figures, accepted for EMBC 2015, IEEE copyright", "journal-ref": null, "doi": "10.1109/EMBC.2015.7318756", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The paper presents a study of an inter-stimulus interval (ISI) influence on a\ntactile point-pressure stimulus-based brain-computer interface's (tpBCI)\nclassification accuracy. A novel tactile pressure generating tpBCI stimulator\nis also discussed, which is based on a three-by-three pins' matrix prototype.\nThe six pin-linear patterns are presented to the user's palm during the online\ntpBCI experiments in an oddball style paradigm allowing for \"the aha-responses\"\nelucidation, within the event related potential (ERP). A subsequent\nclassification accuracies' comparison is discussed based on two ISI settings in\nan online tpBCI application. A research hypothesis of classification\naccuracies' non-significant differences with various ISIs is confirmed based on\nthe two settings of 120 ms and 300 ms, as well as with various numbers of ERP\nresponse averaging scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 02:30:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shimizu", "Kensuke", ""], ["Makino", "Shoji", ""], ["Rutkowski", "Tomasz M.", ""]]}, {"id": "1506.04461", "submitter": "Tomasz Rutkowski", "authors": "Daiki Aminaka, Shoji Makino, and Tomasz M. Rutkowski", "title": "Chromatic and High-frequency cVEP-based BCI Paradigm", "comments": "4 pages, 4 figures, accepted for EMBC 2015, IEEE copyright", "journal-ref": null, "doi": "10.1109/EMBC.2015.7318755", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present results of an approach to a code-modulated visual evoked potential\n(cVEP) based brain-computer interface (BCI) paradigm using four high-frequency\nflashing stimuli. To generate higher frequency stimulation compared to the\nstate-of-the-art cVEP-based BCIs, we propose to use the light-emitting diodes\n(LEDs) driven from a small micro-controller board hardware generator designed\nby our team. The high-frequency and green-blue chromatic flashing stimuli are\nused in the study in order to minimize a danger of a photosensitive epilepsy\n(PSE). We compare the the green-blue chromatic cVEP-based BCI accuracies with\nthe conventional white-black flicker based interface.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 02:43:39 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2015 14:21:23 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Aminaka", "Daiki", ""], ["Makino", "Shoji", ""], ["Rutkowski", "Tomasz M.", ""]]}, {"id": "1506.05573", "submitter": "Sanlaville Kevin", "authors": "Kevin Sanlaville, G\\'erard Assayag, Fr\\'ed\\'eric Bevilacqua, Catherine\n  Pelachaud (LTCI)", "title": "Emergence of synchrony in an Adaptive Interaction Model", "comments": "Intelligent Virtual Agents 2015 Doctoral Consortium, Aug 2015, Delft,\n  Netherlands. IVA Doctoral Consortium, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Human-Computer Interaction context, we aim to elaborate an adaptive and\ngeneric interaction model in two different use cases: Embodied Conversational\nAgents and Creative Musical Agents for musical improvisation. To reach this\ngoal, we'll try to use the concepts of adaptation and synchronization to\nenhance the interactive abilities of our agents and guide the development of\nour interaction model, and will try to make synchrony emerge from non-verbal\ndimensions of interaction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 07:47:49 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Sanlaville", "Kevin", "", "LTCI"], ["Assayag", "G\u00e9rard", "", "LTCI"], ["Bevilacqua", "Fr\u00e9d\u00e9ric", "", "LTCI"], ["Pelachaud", "Catherine", "", "LTCI"]]}, {"id": "1506.06668", "submitter": "Yoichi Ochiai Prof.", "authors": "Yoichi Ochiai, Kota Kumagai, Takayuki Hoshi, Jun Rekimoto, Satoshi\n  Hasegawa, and Yoshio Hayasaki", "title": "Fairy Lights in Femtoseconds: Aerial and Volumetric Graphics Rendered by\n  Focused Femtosecond Laser Combined with Computational Holographic Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method of rendering aerial and volumetric graphics using\nfemtosecond lasers. A high-intensity laser excites a physical matter to emit\nlight at an arbitrary 3D position. Popular applications can then be explored\nespecially since plasma induced by a femtosecond laser is safer than that\ngenerated by a nanosecond laser. There are two methods of rendering graphics\nwith a femtosecond laser in air: Producing holograms using spatial light\nmodulation technology, and scanning of a laser beam by a galvano mirror. The\nholograms and workspace of the system proposed here occupy a volume of up to 1\ncm^3; however, this size is scalable depending on the optical devices and their\nsetup. This paper provides details of the principles, system setup, and\nexperimental evaluation, and discussions on scalability, design space, and\napplications of this system. We tested two laser sources: an adjustable (30-100\nfs) laser which projects up to 1,000 pulses per second at energy up to 7 mJ per\npulse, and a 269-fs laser which projects up to 200,000 pulses per second at an\nenergy up to 50 uJ per pulse. We confirmed that the spatiotemporal resolution\nof volumetric displays, implemented with these laser sources, is 4,000 and\n200,000 dots per second. Although we focus on laser-induced plasma in air, the\ndiscussion presented here is also applicable to other rendering principles such\nas fluorescence and microbubble in solid/liquid materials.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 16:20:34 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ochiai", "Yoichi", ""], ["Kumagai", "Kota", ""], ["Hoshi", "Takayuki", ""], ["Rekimoto", "Jun", ""], ["Hasegawa", "Satoshi", ""], ["Hayasaki", "Yoshio", ""]]}, {"id": "1506.06832", "submitter": "Alex James Dr", "authors": "Assel Davletcharova, Sherin Sugathan, Bibia Abraham, Alex Pappachen\n  James", "title": "Detection and Analysis of Emotion From Speech Signals", "comments": "2nd International Symposium on Computer Vision and the Internet,\n  2015; to appear in Procedia Computer Science Journal, Elsevier, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Recognizing emotion from speech has become one the active research themes in\nspeech processing and in applications based on human-computer interaction. This\npaper conducts an experimental study on recognizing emotions from human speech.\nThe emotions considered for the experiments include neutral, anger, joy and\nsadness. The distinuishability of emotional features in speech were studied\nfirst followed by emotion classification performed on a custom dataset. The\nclassification was performed for different classifiers. One of the main feature\nattribute considered in the prepared dataset was the peak-to-peak distance\nobtained from the graphical representation of the speech signals. After\nperforming the classification tests on a dataset formed from 30 different\nsubjects, it was found that for getting better accuracy, one should consider\nthe data collected from one person rather than considering the data from a\ngroup of people.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 00:28:08 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Davletcharova", "Assel", ""], ["Sugathan", "Sherin", ""], ["Abraham", "Bibia", ""], ["James", "Alex Pappachen", ""]]}, {"id": "1506.07272", "submitter": "Sergio Mascetti", "authors": "Sergio Mascetti, Lorenzo Picinali, Andrea Gerino, Dragan Ahmetovic,\n  Cristian Bernareggi", "title": "Sonification of guidance data during road crossing for people with\n  visual impairments or blindness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last years several solutions were proposed to support people with\nvisual impairments or blindness during road crossing. These solutions focus on\ncomputer vision techniques for recognizing pedestrian crosswalks and computing\ntheir relative position from the user. Instead, this contribution addresses a\ndifferent problem; the design of an auditory interface that can effectively\nguide the user during road crossing. Two original auditory guiding modes based\non data sonification are presented and compared with a guiding mode based on\nspeech messages.\n  Experimental evaluation shows that there is no guiding mode that is best\nsuited for all test subjects. The average time to align and cross is not\nsignificantly different among the three guiding modes, and test subjects\ndistribute their preferences for the best guiding mode almost uniformly among\nthe three solutions. From the experiments it also emerges that higher effort is\nnecessary for decoding the sonified instructions if compared to the speech\ninstructions, and that test subjects require frequent `hints' (in the form of\nspeech messages). Despite this, more than 2/3 of test subjects prefer one of\nthe two guiding modes based on sonification. There are two main reasons for\nthis: firstly, with speech messages it is harder to hear the sound of the\nenvironment, and secondly sonified messages convey information about the\n\"quantity\" of the expected movement.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 07:56:05 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Mascetti", "Sergio", ""], ["Picinali", "Lorenzo", ""], ["Gerino", "Andrea", ""], ["Ahmetovic", "Dragan", ""], ["Bernareggi", "Cristian", ""]]}, {"id": "1506.08754", "submitter": "Andrew Moran", "authors": "Andrew Moran, Vijay Gadepally, Matthew Hubbell, Jeremy Kepner", "title": "Improving Big Data Visual Analytics with Interactive Virtual Reality", "comments": "6 pages, 8 figures, 2015 IEEE High Performance Extreme Computing\n  Conference (HPEC '15); corrected typos", "journal-ref": null, "doi": "10.1109/HPEC.2015.7322473", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades, the growth and volume of digital data collection has made it\nchallenging to digest large volumes of information and extract underlying\nstructure. Coined 'Big Data', massive amounts of information has quite often\nbeen gathered inconsistently (e.g from many sources, of various forms, at\ndifferent rates, etc.). These factors impede the practices of not only\nprocessing data, but also analyzing and displaying it in an efficient manner to\nthe user. Many efforts have been completed in the data mining and visual\nanalytics community to create effective ways to further improve analysis and\nachieve the knowledge desired for better understanding. Our approach for\nimproved big data visual analytics is two-fold, focusing on both visualization\nand interaction. Given geo-tagged information, we are exploring the benefits of\nvisualizing datasets in the original geospatial domain by utilizing a virtual\nreality platform. After running proven analytics on the data, we intend to\nrepresent the information in a more realistic 3D setting, where analysts can\nachieve an enhanced situational awareness and rely on familiar perceptions to\ndraw in-depth conclusions on the dataset. In addition, developing a\nhuman-computer interface that responds to natural user actions and inputs\ncreates a more intuitive environment. Tasks can be performed to manipulate the\ndataset and allow users to dive deeper upon request, adhering to desired\ndemands and intentions. Due to the volume and popularity of social media, we\ndeveloped a 3D tool visualizing Twitter on MIT's campus for analysis. Utilizing\nemerging technologies of today to create a fully immersive tool that promotes\nvisualization and interaction can help ease the process of understanding and\nrepresenting big data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 17:50:20 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2015 18:19:42 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Moran", "Andrew", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1506.09169", "submitter": "Ali Avanaki", "authors": "Ali R. N. Avanaki, Kathryn S. Espig, Tom R. L. Kimpe, Andrew D. A.\n  Maidment", "title": "On anthropomorphic decision making in a model observer", "comments": null, "journal-ref": null, "doi": "10.1117/12.2082129", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By analyzing human readers' performance in detecting small round lesions in\nsimulated digital breast tomosynthesis background in a location known exactly\nscenario, we have developed a model observer that is a better predictor of\nhuman performance with different levels of background complexity (i.e.,\nanatomical and quantum noise). Our analysis indicates that human observers\nperform a lesion detection task by combining a number of sub-decisions, each an\nindicator of the presence of a lesion in the image stack. This is in contrast\nto a channelized Hotelling observer, where the detection task is conducted\nholistically by thresholding a single decision variable, made from an optimally\nweighted linear combination of channels. However, it seems that the sub-par\nperformance of human readers compared to the CHO cannot be fully explained by\ntheir reliance on sub-decisions, or perhaps we do not consider a sufficient\nnumber of sub-decisions. To bridge the gap between the performances of human\nreaders and the model observer based upon sub-decisions, we use an additive\nnoise model, the power of which is modulated with the level of background\ncomplexity. The proposed model observer better predicts the fast drop in human\ndetection performance with background complexity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 17:36:33 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Avanaki", "Ali R. N.", ""], ["Espig", "Kathryn S.", ""], ["Kimpe", "Tom R. L.", ""], ["Maidment", "Andrew D. A.", ""]]}, {"id": "1506.09191", "submitter": "Jaeul Choo", "authors": "Jina Huh, Bum Chul Kwon, Jaegul Choo, Sung-Hee Kim, Ji Soo Yi", "title": "Coddlers, Scientists, Adventurers, and Opportunists: Personas to Inform\n  Online Health Community Development", "comments": "10 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As online health communities (OHCs) grow, users find it challenging to\nproperly search, read, and contribute to the community because of its\noverwhelming content. Our goal is to understand OHC users' needs and\nrequirements for better delivering large-scale OHC content. We interviewed 14\nOHC users with interests in diabetes to investigate their attitudes and needs\ntowards using OHCs and 2 OHC administrators to assess our findings. Four\npersonas -Coddlers, Scientists, Adventurers, and Opportunists- emerged, which\ninform users' interaction behavior and attitudes with OHCs. An individual can\npossess the characteristics of multiple personas, which can also change over\ntime. Our personas uniquely describe users' OHC participation intertwined with\nillness contexts compared to existing social types in general online\ncommunities. We discuss broader implications back to the literature and how our\nfindings apply to other illness contexts in OHCs. We end with requirements for\npersonalized delivery of large-scale OHC content.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 18:22:41 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Huh", "Jina", ""], ["Kwon", "Bum Chul", ""], ["Choo", "Jaegul", ""], ["Kim", "Sung-Hee", ""], ["Yi", "Ji Soo", ""]]}]