[{"id": "1306.1034", "submitter": "Mehul Bhatt", "authors": "Mehul Bhatt and Jakob Suchan and Christian Freksa", "title": "ROTUNDE - A Smart Meeting Cinematography Initiative: Tools, Datasets,\n  and Benchmarks for Cognitive Interpretation and Control", "comments": "Appears in AAAI-2013 Workshop on: Space, Time, and Ambient\n  Intelligence (STAMI 2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construe smart meeting cinematography with a focus on professional\nsituations such as meetings and seminars, possibly conducted in a distributed\nmanner across socio-spatially separated groups. The basic objective in smart\nmeeting cinematography is to interpret professional interactions involving\npeople, and automatically produce dynamic recordings of discussions, debates,\npresentations etc in the presence of multiple communication modalities. Typical\nmodalities include gestures (e.g., raising one's hand for a question,\napplause), voice and interruption, electronic apparatus (e.g., pressing a\nbutton), movement (e.g., standing-up, moving around) etc. ROTUNDE, an instance\nof smart meeting cinematography concept, aims to: (a) develop\nfunctionality-driven benchmarks with respect to the interpretation and control\ncapabilities of human-cinematographers, real-time video editors, surveillance\npersonnel, and typical human performance in everyday situations; (b) Develop\ngeneral tools for the commonsense cognitive interpretation of dynamic scenes\nfrom the viewpoint of visuo-spatial cognition centred perceptual\nnarrativisation. Particular emphasis is placed on declarative representations\nand interfacing mechanisms that seamlessly integrate within large-scale\ncognitive (interaction) systems and companion technologies consisting of\ndiverse AI sub-components. For instance, the envisaged tools would provide\ngeneral capabilities for high-level commonsense reasoning about space, events,\nactions, change, and interaction.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 09:40:24 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Bhatt", "Mehul", ""], ["Suchan", "Jakob", ""], ["Freksa", "Christian", ""]]}, {"id": "1306.1630", "submitter": "Arunasalam Sambhanthan", "authors": "Arunasalam Sambhanthan, Alice Good", "title": "Enhancing Tourism Destination Accessibility in Developing Countries\n  through Virtual Worlds", "comments": "Journal article corresponding to arXiv:1302.5199", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of destination accessibility is a vital concern in the\nsustainable tourism development in the emerging regions due to the increasing\nnumbers of tourism business growth in the recent times. Tourism is one of the\npotential foreign exchange earning sectors, which place sustainability as one\nof the main success metrics for benchmarking the overall development of the\nindustry. On the other hand, there are several destinations, which are\ninaccessible to tourists due to several reasons. Underutilization of potential\ndestinations in both pre purchase and consumption stages is a strategic\ndisadvantage for emerging countries on leading their tourism industry towards\nsustainability. This research embarks on a content analysis of second life\nbased tourism groups and places. Requirement of a virtual world model to\nincrease the destination accessibility of tourism products has been outlined.\nThe model has to be designed with visual and auditory experience to tourists.\nThe model is expected to enhance the accessibility of destinations for users of\ndifferent categories.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 06:39:58 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Sambhanthan", "Arunasalam", ""], ["Good", "Alice", ""]]}, {"id": "1306.1746", "submitter": "Zahid Halim", "authors": "Alamgir Naushad", "title": "Condition Driven Adaptive Music Generation for Computer Games", "comments": null, "journal-ref": null, "doi": "10.5120/10652-5416", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video game industry has grown to a multi-billion dollar, worldwide\nindustry. The background music tends adaptively in reference to the specific\ngame content during the game length of the play. Adaptive music should be\nfurther explored by looking at the particular condition in the game; such\ncondition is driven by generating a specific music in the background which best\nfits in with the active game content throughout the length of the gameplay.\nThis research paper outlines the use of condition driven adaptive music\ngeneration for audio and video to dynamically incorporate adaptively.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 16:07:23 GMT"}], "update_date": "2014-02-04", "authors_parsed": [["Naushad", "Alamgir", ""]]}, {"id": "1306.1752", "submitter": "Federico Cabitza", "authors": "Federico Cabitza and Carla Simone", "title": "Appropriation as neglected practice in communities: presenting a\n  framework to enable EUD design for CoPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Communities present considerable challenges for the design and application of\nsupportive information technology (IT), especially when these develop in\nloosely-integrated, informal and scarcely organized contexts, like it is often\nthe case of Communities of Practice (CoP). An approach that actively supports\nuser communities in the process of IT appropriation can help alleviate the\nimpossibility of the members of these communities to rely on professional\nsupport, and enable even complex forms of tailoring and End-User Development\n(EUD). Although this approach has been already explored by an increasing number\nof researchers, however there is still a lack of a general framework that could\nplay a role in the comparison of existing proposals and in the development of\nnew EUD solutions for CoPs. The paper proposes a conceptual framework and a\nrelated architecture, called Logic of Bricolage, that aims to be a step further\nin this direction to enable better EUD-oriented support for digitized\ncommunities. The framework is described and the architecture instantiated in\nthree concrete EUD environments that specifically regard collaborative\nactivities in order to show the generality and applicability of the framework.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 15:48:39 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Cabitza", "Federico", ""], ["Simone", "Carla", ""]]}, {"id": "1306.1772", "submitter": "Daniel Graziotin", "authors": "Daniel Graziotin, Xiaofeng Wang, Pekka Abrahamsson (Free University of\n  Bozen-Bolzano)", "title": "Are Happy Developers more Productive? The Correlation of Affective\n  States of Software Developers and their self-assessed Productivity", "comments": "16 pages, 3 Figure. The final publication is available at\n  link.springer.com. Link:\n  http://link.springer.com/chapter/10.1007%2F978-3-642-39259-7_7. DOI:\n  10.1007/978-3-642-39259-7_7", "journal-ref": "Proceedings of the 14th International Conference on\n  Product-Focused Software Process Improvement (PROFES 2013), LNCS 7983,\n  Springer-Verlag, pp. 50-64, 2013", "doi": "10.1007/978-3-642-39259-7_7", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For decades now, it has been claimed that a way to improve software\ndevelopers' productivity is to focus on people. Indeed, while human factors\nhave been recognized in Software Engineering research, few empirical\ninvestigations have attempted to verify the claim. Development tasks are\nundertaken through cognitive processing abilities. Affective states - emotions,\nmoods, and feelings - have an impact on work-related behaviors, cognitive\nprocessing activities, and the productivity of individuals. In this paper, we\nreport an empirical study on the impact of affective states on software\ndevelopers' performance while programming. Two affective states dimensions are\npositively correlated with self-assessed productivity. We demonstrate the value\nof applying psychometrics in Software Engineering studies and echo a call to\nvalorize the human, individualized aspects of software developers. We introduce\nand validate a measurement instrument and a linear mixed-effects model to study\nthe correlation of affective states and the productivity of software\ndevelopers.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 16:51:39 GMT"}], "update_date": "2013-06-10", "authors_parsed": [["Graziotin", "Daniel", "", "Free University of\n  Bozen-Bolzano"], ["Wang", "Xiaofeng", "", "Free University of\n  Bozen-Bolzano"], ["Abrahamsson", "Pekka", "", "Free University of\n  Bozen-Bolzano"]]}, {"id": "1306.2356", "submitter": "Mirco Musolesi", "authors": "Veljko Pejovic and Mirco Musolesi", "title": "Anticipatory Mobile Computing: A Survey of the State of the Art and\n  Research Challenges", "comments": "29 pages, 5 figures", "journal-ref": "ACM Computing Surveys (CSUR). Volume 47 Issue 3, April 2015. ACM\n  Press", "doi": "10.1145/2693843", "report-no": "School of Computer Science University of Birmingham Technical Report\n  CSR-13-02", "categories": "cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's mobile phones are far from mere communication devices they were ten\nyears ago. Equipped with sophisticated sensors and advanced computing hardware,\nphones can be used to infer users' location, activity, social setting and more.\nAs devices become increasingly intelligent, their capabilities evolve beyond\ninferring context to predicting it, and then reasoning and acting upon the\npredicted context. This article provides an overview of the current state of\nthe art in mobile sensing and context prediction paving the way for\nfull-fledged anticipatory mobile computing. We present a survey of phenomena\nthat mobile phones can infer and predict, and offer a description of machine\nlearning techniques used for such predictions. We then discuss proactive\ndecision making and decision delivery via the user-device feedback loop.\nFinally, we discuss the challenges and opportunities of anticipatory mobile\ncomputing.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 20:57:10 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2013 22:43:53 GMT"}, {"version": "v3", "created": "Tue, 8 Jul 2014 18:31:13 GMT"}, {"version": "v4", "created": "Sat, 18 Oct 2014 22:54:07 GMT"}, {"version": "v5", "created": "Fri, 2 Jan 2015 23:51:31 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Pejovic", "Veljko", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1306.2404", "submitter": "Weidong Huang", "authors": "Weidong Huang", "title": "An Aggregation-Based Overall Quality Measurement for Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Aesthetics are often used to evaluate the quality of graph drawings. However,\nthe existing aesthetic criteria are useful in judging the extents to which a\ndrawing conforms to particular drawing rules. They have limitations in\nevaluating overall quality. Currently the overall quality of graph drawings is\nmainly evaluated based on personal judgments and user studies. Personal\njudgments are not reliable, while user studies can be costly to run. Therefore,\nthere is a need for a direct measure of overall quality. This measure can be\nused by visualization designers to quickly compare the quality of drawings at\nhand at the design stage and make decisions accordingly. In an attempt to meet\nthis need, we propose a measure that measures overall quality based on\naggregation of individual aesthetic criteria. We present a user study that\nvalidates this measure and demonstrates its capacity in predicting the\nperformance of human graph comprehension. The implications of the proposed\nmeasure for future research are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 02:01:17 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Huang", "Weidong", ""]]}, {"id": "1306.3054", "submitter": "Uwe Aickelin", "authors": "Haichang Gao, Zhongjie Ren, Xiuling Chang, Xiyang Liu, Uwe Aickelin", "title": "The effect of baroque music on the PassPoints graphical password", "comments": "ACM International Conference on Image and Video Retrieval, 129-134,\n  2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical passwords have been demonstrated to be the possible alternatives to\ntraditional alphanumeric passwords. However, they still tend to follow\npredictable patterns that are easier to attack. The crux of the problem is\nusers' memory limitations. Users are the weakest link in password\nauthentication mechanism. It shows that baroque music has positive effects on\nhuman memorizing and learning. We introduce baroque music to the PassPoints\ngraphical password scheme and conduct a laboratory study in this paper. Results\nshown that there is no statistic difference between the music group and the\ncontrol group without music in short-term recall experiments, both had high\nrecall success rates. But in long-term recall, the music group performed\nsignificantly better. We also found that the music group tended to set\nsignificantly more complicated passwords, which are usually more resistant to\ndictionary and other guess attacks. But compared with the control group, the\nmusic group took more time to log in both in short-term and long-term tests.\nBesides, it appears that background music does not work in terms of hotspots.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 08:20:38 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Gao", "Haichang", ""], ["Ren", "Zhongjie", ""], ["Chang", "Xiuling", ""], ["Liu", "Xiyang", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1306.3055", "submitter": "Uwe Aickelin", "authors": "Haichang Gao, Xiuling Chang, Zhongjie Ren, Uwe Aickelin, Liming Wang", "title": "Can background baroque music help to improve the memorability of\n  graphical passwords?", "comments": "Proceedings of the International Conference on Image Analysis and\n  Recognition, ICIAR2010, Povoa de Varzim, Portugal, 378-387, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical passwords have been proposed as an alternative to alphanumeric\npasswords with their advantages in usability and security. However, they still\ntend to follow predictable patterns that are easier for attackers to exploit,\nprobably due to users' memory limitations. Various literatures show that\nbaroque music has positive effects on human learning and memorizing. To\nalleviate users' memory burden, we investigate the novel idea of introducing\nbaroque music to graphical password schemes (specifically DAS, PassPoints and\nStory) and conduct a laboratory study to see whether it is helpful. In a ten\nminutes short-term recall, we found that participants in all conditions had\nhigh recall success rates that were not statistically different from each\nother. After one week, the music group coped PassPoints passwords significantly\nbetter than the group without music. But there was no statistical difference\nbetween two groups in recalling DAS passwords or Story passwords. Further more,\nwe found that the music group tended to set significantly more complicated\nPassPoints passwords but less complicated DAS passwords.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 08:33:06 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Gao", "Haichang", ""], ["Chang", "Xiuling", ""], ["Ren", "Zhongjie", ""], ["Aickelin", "Uwe", ""], ["Wang", "Liming", ""]]}, {"id": "1306.3133", "submitter": "Piotr Sapiezynski", "authors": "Jakob Eg Larsen, Piotr Sapiezynski, Arkadiusz Stopczynski, Morten\n  Moerup, Rasmus Theodorsen", "title": "Crowds, Bluetooth, and Rock-n-Roll. Understanding Music Festival\n  Participant Behavior", "comments": "Presented at Sunbelt 2013 in Hamburg on May, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a study of sensing and analyzing an offline social\nnetwork of participants at a large-scale music festival (8 days, 130,000+\nparticipants). We place 33 fixed-location Bluetooth scanners in strategic spots\naround the festival area to discover Bluetooth-enabled mobile phones carried by\nthe participants, and thus collect spatio-temporal traces of their mobility and\ninteractions. We subsequently analyze the data on two levels. On the micro\nlevel, we run a community detection algorithm to reveal a variety of groups the\nfestival participants form. On the macro level, we employ an Infinite\nRelational Model (IRM) in order to recover the structure of the social network\nrelated to participants' music preferences. The obtained structure in the form\nof clusters of concerts and participants is then interpreted using\nmeta-information about music genres, band origins, stages, and dates of\nperformances. We show that most of the concerts clusters can be described by\none or more of the meta-features, effectively revealing preferences of\nparticipants (e.g. a cluster of US bands) and discuss the significance of the\nfindings and the potential and limitations of the used method. Finally, we\ndiscuss the possibility of employing the described method and techniques for\ncreating user-oriented applications and extending the sensing capabilities\nduring large-scale events by introducing user involvement.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 15:08:22 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 08:39:32 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Larsen", "Jakob Eg", ""], ["Sapiezynski", "Piotr", ""], ["Stopczynski", "Arkadiusz", ""], ["Moerup", "Morten", ""], ["Theodorsen", "Rasmus", ""]]}, {"id": "1306.3474", "submitter": "Yijun Wang", "authors": "Yijun Wang", "title": "Classifying Single-Trial EEG during Motor Imagery with a Small Training\n  Set", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before the operation of a motor imagery based brain-computer interface (BCI)\nadopting machine learning techniques, a cumbersome training procedure is\nunavoidable. The development of a practical BCI posed the challenge of\nclassifying single-trial EEG with a small training set. In this letter, we\naddressed this problem by employing a series of signal processing and machine\nlearning approaches to alleviate overfitting and obtained test accuracy similar\nto training accuracy on the datasets from BCI Competition III and our own\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 18:24:19 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Wang", "Yijun", ""]]}, {"id": "1306.3767", "submitter": "Simon Harper", "authors": "Simon Harper, Tianyi Chen, and Yeliz Yesilada", "title": "Controlled Experimentation in Naturalistic Mobile Settings", "comments": "12 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing controlled user experiments on small devices in naturalistic\nmobile settings has always proved to be a difficult undertaking for many Human\nFactors researchers. Difficulties exist, not least, because mimicking natural\nsmall device usage suffers from a lack of unobtrusive data to guide\nexperimental design, and then validate that the experiment is proceeding\nnaturally.Here we use observational data to derive a set of protocols and a\nsimple checklist of validations which can be built into the design of any\ncontrolled experiment focused on the user interface of a small device. These,\nhave been used within a series of experimental designs to measure the utility\nand application of experimental software. The key-point is the validation\nchecks -- based on the observed behaviour of 400 mobile users -- to ratify that\na controlled experiment is being perceived as natural by the user. While the\ndesign of the experimental route which the user follows is a major factor in\nthe experimental setup, without check validations based on unobtrusive observed\ndata there can be no certainty that an experiment designed to be natural is\nactually progressing as the design implies.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 08:36:11 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 06:57:13 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Harper", "Simon", ""], ["Chen", "Tianyi", ""], ["Yesilada", "Yeliz", ""]]}, {"id": "1306.3860", "submitter": "Samuel R\\\"onnqvist", "authors": "Peter Sarlin and Samuel R\\\"onnqvist", "title": "Cluster coloring of the Self-Organizing Map: An information\n  visualization perspective", "comments": "Forthcoming in Proceedings of 17th International Conference\n  Information Visualisation (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This paper takes an information visualization perspective to visual\nrepresentations in the general SOM paradigm. This involves viewing SOM-based\nvisualizations through the eyes of Bertin's and Tufte's theories on data\ngraphics. The regular grid shape of the Self-Organizing Map (SOM), while being\na virtue for linking visualizations to it, restricts representation of cluster\nstructures. From the viewpoint of information visualization, this paper\nprovides a general, yet simple, solution to projection-based coloring of the\nSOM that reveals structures. First, the proposed color space is easy to\nconstruct and customize to the purpose of use, while aiming at being\nperceptually correct and informative through two separable dimensions. Second,\nthe coloring method is not dependent on any specific method of projection, but\nis rather modular to fit any objective function suitable for the task at hand.\nThe cluster coloring is illustrated on two datasets: the iris data, and welfare\nand poverty indicators.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 13:57:00 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Sarlin", "Peter", ""], ["R\u00f6nnqvist", "Samuel", ""]]}, {"id": "1306.4094", "submitter": "Mohammadi Akheela Khanum Mrs", "authors": "Mohammadi Akheela Khanum, Munesh C. Trivedi", "title": "Exploring Verbalization and Collaboration during Usability Evaluation\n  with Children in Context", "comments": "7 pages. arXiv admin note: text overlap with arXiv:1204.2138", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the effect of context on usability evaluation.\nThe focus is on how children behave and perform when they are tested in\ndifferent settings. Two most commonly applied usability evaluation methods: the\nthink-aloud and constructive interactions are applied to the children in\ndifferent physical contexts. We present an experimental design involving 54\nchildren participating in two different configurations of constructive\ninteraction and a traditional think-aloud. The behavior and performance of the\nchildren in two different physical contexts is measured by evaluating the\nresults of application of think-aloud and constructive interaction. Finally, we\noutline lessons on the impact of context on involving children in usability\ntesting.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 08:03:11 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Khanum", "Mohammadi Akheela", ""], ["Trivedi", "Munesh C.", ""]]}, {"id": "1306.4139", "submitter": "Suket Arora", "authors": "Preeti Verma, Suket Arora, Kamaljit Batra", "title": "Punjabi Language Interface to Database: a brief review", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike most user-computer interfaces, a natural language interface allows\nusers to communicate fluently with a computer system with very little\npreparation. Databases are often hard to use in cooperating with the users\nbecause of their rigid interface. A good NLIDB allows a user to enter commands\nand ask questions in native language and then after interpreting respond to the\nuser in native language. For a large number of applications requiring\ninteraction between humans and the computer systems, it would be convenient to\nprovide the end-user friendly interface. Punjabi language interface to database\nwould proof fruitful to native people of Punjab, as it provides ease to them to\nuse various e-governance applications like Punjab Sewa, Suwidha, Online Public\nUtility Forms, Online Grievance Cell, Land Records Management System,legacy\nmatters, e-District, agriculture, etc. Punjabi is the mother tongue of more\nthan 110 million people all around the world. According to available\ninformation, Punjabi ranks 10th from top out of a total of 6,900 languages\nrecognized internationally by the United Nations. This paper covers a brief\noverview of the Natural language interface to database, its different\ncomponents, its advantages, disadvantages, approaches and techniques used. The\npaper ends with the work done on Punjabi language interface to database and\nfuture enhancements that can be done.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 11:03:36 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Verma", "Preeti", ""], ["Arora", "Suket", ""], ["Batra", "Kamaljit", ""]]}, {"id": "1306.4724", "submitter": "Ognjen Arandjelovi\\'c PhD", "authors": "Ognjen Arandjelovic", "title": "Computer simulation based parameter selection for resistance exercise", "comments": "In Modelling and Simulation, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to most scientific disciplines, sports science research has been\ncharacterized by comparatively little effort investment in the development of\nrelevant phenomenological models. Scarcer yet is the application of said models\nin practice. We present a framework which allows resistance training\npractitioners to employ a recently proposed neuromuscular model in actual\ntraining program design. The first novelty concerns the monitoring aspect of\ncoaching. A method for extracting training performance characteristics from\nloosely constrained video sequences, effortlessly and with minimal human input,\nusing computer vision is described. The extracted data is subsequently used to\nfit the underlying neuromuscular model. This is achieved by solving an inverse\ndynamics problem corresponding to a particular exercise. Lastly, a computer\nsimulation of hypothetical training bouts, using athlete-specific capability\nparameters, is used to predict the effected adaptation and changes in\nperformance. The software described here allows the practitioner to manipulate\nhypothetical training parameters and immediately see their effect on predicted\nadaptation for a specific athlete. Thus, this work presents a holistic view of\nthe monitoring-assessment-adjustment loop.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 00:47:21 GMT"}], "update_date": "2013-06-21", "authors_parsed": [["Arandjelovic", "Ognjen", ""]]}, {"id": "1306.5279", "submitter": "Jesse Hoey", "authors": "Jesse Hoey, Tobias Schroeder, Areej Alhothali", "title": "Affect Control Processes: Intelligent Affective Interaction using a\n  Partially Observable Markov Decision Process", "comments": null, "journal-ref": null, "doi": "10.1016/j.artint.2015.09.004", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel method for building affectively intelligent\nhuman-interactive agents. The method is based on a key sociological insight\nthat has been developed and extensively verified over the last twenty years,\nbut has yet to make an impact in artificial intelligence. The insight is that\nresource bounded humans will, by default, act to maintain affective\nconsistency. Humans have culturally shared fundamental affective sentiments\nabout identities, behaviours, and objects, and they act so that the transient\naffective sentiments created during interactions confirm the fundamental\nsentiments. Humans seek and create situations that confirm or are consistent\nwith, and avoid and supress situations that disconfirm or are inconsistent\nwith, their culturally shared affective sentiments. This \"affect control\nprinciple\" has been shown to be a powerful predictor of human behaviour. In\nthis paper, we present a probabilistic and decision-theoretic generalisation of\nthis principle, and we demonstrate how it can be leveraged to build affectively\nintelligent artificial agents. The new model, called BayesAct, can maintain\nmultiple hypotheses about sentiments simultaneously as a probability\ndistribution, and can make use of an explicit utility function to make\nvalue-directed action choices. This allows the model to generate affectively\nintelligent interactions with people by learning about their identity,\npredicting their behaviours using the affect control principle, and taking\nactions that are simultaneously goal-directed and affect-sensitive. We\ndemonstrate this generalisation with a set of simulations. We then show how our\nmodel can be used as an emotional \"plug-in\" for artificially intelligent\nsystems that interact with humans in two different settings: an exam practice\nassistant (tutor) and an assistive device for persons with a cognitive\ndisability.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 01:02:03 GMT"}, {"version": "v2", "created": "Thu, 3 Apr 2014 13:49:43 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Hoey", "Jesse", ""], ["Schroeder", "Tobias", ""], ["Alhothali", "Areej", ""]]}, {"id": "1306.5308", "submitter": "Mehul Bhatt", "authors": "Mehul Bhatt, Jakob Suchan, Carl Schultz", "title": "Cognitive Interpretation of Everyday Activities: Toward Perceptual\n  Narrative Based Visuo-Spatial Scene Interpretation", "comments": "To appear at: Computational Models of Narrative (CMN) 2013., a\n  satellite event of CogSci 2013: The 35th meeting of the Cognitive Science\n  Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We position a narrative-centred computational model for high-level knowledge\nrepresentation and reasoning in the context of a range of assistive\ntechnologies concerned with \"visuo-spatial perception and cognition\" tasks. Our\nproposed narrative model encompasses aspects such as \\emph{space, events,\nactions, change, and interaction} from the viewpoint of commonsense reasoning\nand learning in large-scale cognitive systems. The broad focus of this paper is\non the domain of \"human-activity interpretation\" in smart environments, ambient\nintelligence etc. In the backdrop of a \"smart meeting cinematography\" domain,\nwe position the proposed narrative model, preliminary work on perceptual\nnarrativisation, and the immediate outlook on constructing general-purpose\nopen-source tools for perceptual narrativisation.\n  ACM Classification: I.2 Artificial Intelligence: I.2.0 General -- Cognitive\nSimulation, I.2.4 Knowledge Representation Formalisms and Methods, I.2.10\nVision and Scene Understanding: Architecture and control structures, Motion,\nPerceptual reasoning, Shape, Video analysis\n  General keywords: cognitive systems; human-computer interaction; spatial\ncognition and computation; commonsense reasoning; spatial and temporal\nreasoning; assistive technologies\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 10:37:34 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Bhatt", "Mehul", ""], ["Suchan", "Jakob", ""], ["Schultz", "Carl", ""]]}, {"id": "1306.5884", "submitter": "Sandeep Venkatesh", "authors": "Sandeep Venkatesh, Meera V Patil, Nanditha Swamy", "title": "Design of an Agent for Answering Back in Smart Phones", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  erro", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the paper is to design an agent which provides efficient\nresponse to the caller when a call goes unanswered in smartphones. The agent\nprovides responses through text messages, email etc stating the most likely\nreason as to why the callee is unable to answer a call. Responses are composed\ntaking into consideration the importance of the present call and the situation\nthe callee is in at the moment like driving, sleeping, at work etc. The agent\nmakes decisons in the compostion of response messages based on the patterns it\nhas come across in the learning environment. Initially the user helps the agent\nto compose response messages. The agent associates this message to the percept\nit recieves with respect to the environment the callee is in. The user may\nthereafter either choose to make to response system automatic or choose to\nrecieve suggestions from the agent for responses messages and confirm what is\nto be sent to the caller.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 08:56:58 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2014 11:02:00 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Venkatesh", "Sandeep", ""], ["Patil", "Meera V", ""], ["Swamy", "Nanditha", ""]]}, {"id": "1306.6294", "submitter": "Ashesh Jain", "authors": "Ashesh Jain and Brian Wojcik, Thorsten Joachims and Ashutosh Saxena", "title": "Learning Trajectory Preferences for Manipulators via Iterative\n  Improvement", "comments": "9 pages. To appear in NIPS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning good trajectories for manipulation tasks.\nThis is challenging because the criterion defining a good trajectory varies\nwith users, tasks and environments. In this paper, we propose a co-active\nonline learning framework for teaching robots the preferences of its users for\nobject manipulation tasks. The key novelty of our approach lies in the type of\nfeedback expected from the user: the human user does not need to demonstrate\noptimal trajectories as training data, but merely needs to iteratively provide\ntrajectories that slightly improve over the trajectory currently proposed by\nthe system. We argue that this co-active preference feedback can be more easily\nelicited from the user than demonstrations of optimal trajectories, which are\noften challenging and non-intuitive to provide on high degrees of freedom\nmanipulators. Nevertheless, theoretical regret bounds of our algorithm match\nthe asymptotic rates of optimal trajectory algorithms. We demonstrate the\ngeneralizability of our algorithm on a variety of grocery checkout tasks, for\nwhom, the preferences were not only influenced by the object being manipulated\nbut also by the surrounding environment.\\footnote{For more details and a\ndemonstration video, visit: \\url{http://pr.cs.cornell.edu/coactive}}\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 17:07:58 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2013 17:55:31 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Jain", "Ashesh", ""], ["Wojcik", "Brian", ""], ["Joachims", "Thorsten", ""], ["Saxena", "Ashutosh", ""]]}]