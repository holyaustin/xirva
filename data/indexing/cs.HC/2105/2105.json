[{"id": "2105.00076", "submitter": "Lucy Lu Wang", "authors": "Lucy Lu Wang, Isabel Cachola, Jonathan Bragg, Evie Yu-Yen Cheng,\n  Chelsea Haupt, Matt Latzke, Bailey Kuehl, Madeleine van Zuylen, Linda Wagner,\n  Daniel S. Weld", "title": "Improving the Accessibility of Scientific Documents: Current State, User\n  Needs, and a System Solution to Enhance Scientific PDF Accessibility for\n  Blind and Low Vision Users", "comments": "44 pages, 11 figures, 10 tables, 4 appendices; accessible PDF is\n  available at https://llwang.net/publications/2021_wang_scia11y.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The majority of scientific papers are distributed in PDF, which pose\nchallenges for accessibility, especially for blind and low vision (BLV)\nreaders. We characterize the scope of this problem by assessing the\naccessibility of 11,397 PDFs published 2010--2019 sampled across various fields\nof study, finding that only 2.4% of these PDFs satisfy all of our defined\naccessibility criteria. We introduce the SciA11y system to offset some of the\nissues around inaccessibility. SciA11y incorporates several machine learning\nmodels to extract the content of scientific PDFs and render this content as\naccessible HTML, with added novel navigational features to support screen\nreader users. An intrinsic evaluation of extraction quality indicates that the\nmajority of HTML renders (87%) produced by our system have no or only some\nreadability issues. We perform a qualitative user study to understand the needs\nof BLV researchers when reading papers, and to assess whether the SciA11y\nsystem could address these needs. We summarize our user study findings into a\nset of five design recommendations for accessible scientific reader systems.\nUser response to SciA11y was positive, with all users saying they would be\nlikely to use the system in the future, and some stating that the system, if\navailable, would become their primary workflow. We successfully produce HTML\nrenders for over 12M papers, of which an open access subset of 1.5M are\navailable for browsing at https://scia11y.org/\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 20:31:38 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Lucy Lu", ""], ["Cachola", "Isabel", ""], ["Bragg", "Jonathan", ""], ["Cheng", "Evie Yu-Yen", ""], ["Haupt", "Chelsea", ""], ["Latzke", "Matt", ""], ["Kuehl", "Bailey", ""], ["van Zuylen", "Madeleine", ""], ["Wagner", "Linda", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2105.00121", "submitter": "Doris Jung-Lin Lee", "authors": "Doris Jung-Lin Lee, Dixin Tang, Kunal Agarwal, Thyne Boonmark, Caitlyn\n  Chen, Jake Kang, Ujjaini Mukhopadhyay, Jerry Song, Micah Yong, Marti A.\n  Hearst, Aditya G. Parameswaran", "title": "Lux: Always-on Visualization Recommendations for Exploratory Data\n  Science", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Exploratory data science largely happens in computational notebooks with\ndataframe API, such as pandas, that support flexible means to transform, clean,\nand analyze data. Yet, visually exploring data in dataframes remains tedious,\nrequiring substantial programming effort for visualization and mental effort to\ndetermine what analysis to perform next. We propose Lux, an always-on framework\nfor accelerating visual insight discovery in data science workflows. When users\nprint a dataframe in their notebooks, Lux recommends visualizations to provide\na quick overview of the patterns and trends and suggests promising analysis\ndirections. Lux features a high-level language for generating visualizations\non-demand to encourage rapid visual experimentation with data. We demonstrate\nthat through the use of a careful design and three system optimizations, Lux\nadds no more than two seconds of overhead on top of pandas for over 98% of\ndatasets in the UCI repository. We evaluate Lux in terms of usability via a\ncontrolled first-use study and interviews with early adopters, finding that Lux\nhelps fulfill the needs of data scientists for visualization support within\ntheir dataframe workflows. Lux has already been embraced by data science\npractitioners, with over 1.9k stars on Github within its first 15 months.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 23:28:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lee", "Doris Jung-Lin", ""], ["Tang", "Dixin", ""], ["Agarwal", "Kunal", ""], ["Boonmark", "Thyne", ""], ["Chen", "Caitlyn", ""], ["Kang", "Jake", ""], ["Mukhopadhyay", "Ujjaini", ""], ["Song", "Jerry", ""], ["Yong", "Micah", ""], ["Hearst", "Marti A.", ""], ["Parameswaran", "Aditya G.", ""]]}, {"id": "2105.00199", "submitter": "Shahab Saquib Sohail PhD", "authors": "Shahab Saquib Sohail, Jamshed Siddiqui, Rashid Ali, S. Hamid Hasan,\n  M.Afshar Alam", "title": "Can we aggregate human intelligence? an approach for human centric\n  aggregation using ordered weighted averaging operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The primary objective of this paper is to present an approach for recommender\nsystems that can assimilate ranking to the voters or rankers so that\nrecommendation can be made by giving priority to experts suggestion over usual\nrecommendation. To accomplish this, we have incorporated the concept of\nhuman-centric aggregation via Ordered Weighted Aggregation (OWA). Here, we are\nadvocating ranked recommendation where rankers are assigned weights according\nto their place in the ranking. Further, the recommendation process which is\npresented here for the recommendation of books to university students exploits\nlinguistic data summaries and Ordered Weighted Aggregation (OWA) technique. In\nthe suggested approach, the weights are assigned in a way that it associates\nhigher weights to best ranked university. The approach has been evaluated over\neight different parameters. The superiority of the proposed approach is evident\nfrom the evaluation results. We claim that proposed scheme saves storage spaces\nrequired in traditional recommender systems as well as it does not need users\nprior preferences and hence produce a solution for cold start problem. This\nenvisaged that the proposed scheme can be very useful in decision making\nproblems, especially for recommender systems. In addition, it emphasizes on how\nhuman-centric aggregation can be useful in recommendation researches, and also\nit gives a new direction about how various human specific tasks can be\nnumerically aggregated.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 09:05:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Sohail", "Shahab Saquib", ""], ["Siddiqui", "Jamshed", ""], ["Ali", "Rashid", ""], ["Hasan", "S. Hamid", ""], ["Alam", "M. Afshar", ""]]}, {"id": "2105.00201", "submitter": "Safinah Ali", "authors": "Safinah Ali, Nisha Devasia, Cynthia Breazeal", "title": "Designing Games for Enabling Co-creation with Social Agents", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Digital tools have long been used for supporting children's creativity.\nDigital games that allow children to create artifacts and express themselves in\na playful environment serve as efficient Creativity Support Tools (or CSTs).\nCreativity is also scaffolded by social interactions with others in their\nenvironment. In our work, we explore the use of game-based interactions with a\nsocial agent to scaffold children's creative expression as game players. We\ndesigned three collaborative games and play-tested with 146 5-10 year old\nchildren played with the social robot Jibo, which affords three different kinds\nof creativity: verbal creativity, figural creativity and divergent thinking\nduring creative problem solving. In this paper, we reflect on game mechanic\npractices that we incorporated to design for stimulating creativity in\nchildren. These strategies may be valuable to game designers and HCI\nresearchers designing games and social agents for supporting children's\ncreativity.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 09:37:32 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ali", "Safinah", ""], ["Devasia", "Nisha", ""], ["Breazeal", "Cynthia", ""]]}, {"id": "2105.00215", "submitter": "Pinar Uluer", "authors": "Pinar Uluer, Hatice Kose, Agnieszka Landowska, Tatjana Zorcec, Ben\n  Robins and Duygun Erol Barkana", "title": "Child-Robot Interaction Studies During COVID-19 Pandemic", "comments": "Presented in Child-Robot Interaction: Present and Future\n  Relationships Workshop at International Conference on Social Robotics (ICSR)\n  on 16 November 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coronavirus disease (COVID-19) pandemic affected our lives deeply, just\nlike everyone else, the children also suffered from the restrictions due to\nCOVID-19 affecting their education and social interactions with others, being\nrestricted from play areas and schools for a long time. Although social robots\nprovide a promising solution to support children in their education, healthcare\nand social interaction with others, the precautions due to COVID-19 also\nintroduced new constraints in the social robotics research. In this paper, we\nwill discuss the benefits and challenges encountered in child-robot interaction\ndue to COVID-19 based on two user studies. The first study involves children\nwith hearing disabilities, and Pepper humanoid robot to support their\naudiometry tests. The second study includes the child-sized humanoid robot\nKaspar and interaction games with children with autism spectrum disorder (ASD).\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:05:47 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Uluer", "Pinar", ""], ["Kose", "Hatice", ""], ["Landowska", "Agnieszka", ""], ["Zorcec", "Tatjana", ""], ["Robins", "Ben", ""], ["Barkana", "Duygun Erol", ""]]}, {"id": "2105.00431", "submitter": "Abdifatah Farah Ali", "authors": "Abdifatah Farah Ali, Rusli Haji Abdulah, Mohamed M. Mohamed", "title": "Toward Developing Intelligent Mobile Obe System In Higher Learning\n  Institution", "comments": null, "journal-ref": "INTERNATIONAL JOURNAL OF SCIENTIFIC & TECHNOLOGY RESEARCH, JANUARY\n  2020", "doi": null, "report-no": "VOLUME 9, ISSUE 01, JANUARY 2020", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid growth in Mobile application users has made the researchers and\npractitioners to think of intelligent tools that can help the users and\napplications in delivering quality of services. Therefore, intelligent agent is\nexpected to become the tool for development of mobile outcome based education\n(OBE) particularly in higher learning Institutions (HLI). In this context,\nthere is a lacking of OBE intelligent agent in assisting the academicians to\nuse in OBE management for mobile application system environment. This paper\npresents the conceptual design and development of a mobile intelligent agent\nbased on mobile OBE called as i-MOBE. Since that, i-MOBE that we developed is\nconsidered very important for academicians and students to facilitate them in\nusing for academic purpose in HLI particularly in helping them to monitor the\nperformance in teaching and learning (T&L). The system architecture will be\ncovering the conceptual design and its interaction as well as the system\nconfiguration in helping the academicians to use the system in their T&L toward\neffective and efficiency also can be applied in monitoring based on scenarios\nsuch as test, assignment and projects and so on.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 09:41:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ali", "Abdifatah Farah", ""], ["Abdulah", "Rusli Haji", ""], ["Mohamed", "Mohamed M.", ""]]}, {"id": "2105.00502", "submitter": "Florian Mihola", "authors": "Florian Mihola", "title": "What Way Is It Meant To Be Played?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The most commonly used interface between a video game and the human user is a\nhandheld \"game controller\", \"game pad\", or in some occasions an \"arcade stick.\"\nDirectional pads, analog sticks and buttons - both digital and analog - are\nlinked to in-game actions. One or multiple simultaneous inputs may be necessary\nto communicate the intentions of the user. Activating controls may be more or\nless convenient depending on their position and size. In order to enable the\nuser to perform all inputs which are necessary during gameplay, it is thus\nimperative to find a mapping between in-game actions and buttons, analog\nsticks, and so on. We present simple formats for such mappings as well as for\nthe constraints on possible inputs which are either determined by a physical\ngame controller or required to be met for a game software, along with methods\nto transform said constraints via a button-action mapping and to check one\nconstraint set against another, i.e., to check whether a button-action mapping\nallows a controller to be used in conjunction with a game software, while\npreserving all desired properties.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 16:17:45 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mihola", "Florian", ""]]}, {"id": "2105.00506", "submitter": "Stephan Schl\\\"ogl PhD", "authors": "Anna Esposito, Terry Amorese, Marialucia Cuciniello, Antonietta M.\n  Esposito, Alda Troncone, Maria Ines Torres, Stephan Schl\\\"ogl, Gennaro\n  Cordasco", "title": "Seniors' acceptance of virtual humanoid agents", "comments": "14 pages", "journal-ref": null, "doi": "10.1007/978-3-030-05921-7_35", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on a study conducted as part of the EU EMPATHIC project,\nwhose goal is to develop an empathic virtual coach capable of enhancing\nseniors' well-being, focusing on user requirements and expectations with\nrespect to participants' age and technology experiences (i.e. participants'\nfamiliarity with technological devices such as smartphones, laptops, and\ntablets). The data shows that seniors' favorite technological device is the\nsmartphone, and this device was also the one that scored the highest in terms\nof easiness to use. We found statistically significant differences on the\npreferences expressed by seniors toward the gender of the agents. Seniors\n(independently from their gender) prefer to interact with female humanoid\nagents on both the pragmatic and hedonic dimensions of an interactive system\nand are more in favor to commit themselves in a long-lasting interaction with\nthem. In addition, we found statistically significant effects of the seniors'\ntechnology savviness on the hedonic qualities of the proposed interactive\nsystems. Seniors with technological experience felt less motivated and judged\nthe proposed agents less captivating, exciting, and appealing.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 16:37:00 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Esposito", "Anna", ""], ["Amorese", "Terry", ""], ["Cuciniello", "Marialucia", ""], ["Esposito", "Antonietta M.", ""], ["Troncone", "Alda", ""], ["Torres", "Maria Ines", ""], ["Schl\u00f6gl", "Stephan", ""], ["Cordasco", "Gennaro", ""]]}, {"id": "2105.00543", "submitter": "Sarnab Bhattacharya", "authors": "Sarnab Bhattacharya, Keum San Chun, Edison Thomaz", "title": "MagSurface: Wireless 2D Finger Tracking Leveraging Magnetic Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the ubiquity of touchscreens, touch input modality has become a popular\nway of interaction. However, current touchscreen technology is limiting in its\ndesign as it restricts touch interactions to specially instrumented touch\nsurfaces. Surface contaminants like water can also hinder proper interactions.\nIn this paper, we propose the use of magnetic field sensing to enable finger\ntracking on a surface with minimal instrumentation. Our system, MagSurface,\nturns everyday surfaces into a touch medium, thus allowing more flexibility in\nthe types of touch surfaces. The evaluation of our system consists of\nquantifying the accuracy of the system in locating an object on 2D flat\nsurfaces. We test our system on three different surface materials to validate\nits usage scenarios. A qualitative user experience study was also conducted to\nget feedback on the ease of use and comfort of the system. Localization error\nas low as a few millimeters was achieved\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 20:02:49 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bhattacharya", "Sarnab", ""], ["Chun", "Keum San", ""], ["Thomaz", "Edison", ""]]}, {"id": "2105.00580", "submitter": "Siddharth Karamcheti", "authors": "Siddharth Karamcheti, Albert J. Zhai, Dylan P. Losey, Dorsa Sadigh", "title": "Learning Visually Guided Latent Actions for Assistive Teleoperation", "comments": "Accepted at Learning for Dynamics and Control (L4DC) 2021. 12 pages,\n  4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is challenging for humans -- particularly those living with physical\ndisabilities -- to control high-dimensional, dexterous robots. Prior work\nexplores learning embedding functions that map a human's low-dimensional inputs\n(e.g., via a joystick) to complex, high-dimensional robot actions for assistive\nteleoperation; however, a central problem is that there are many more\nhigh-dimensional actions than available low-dimensional inputs. To extract the\ncorrect action and maximally assist their human controller, robots must reason\nover their context: for example, pressing a joystick down when interacting with\na coffee cup indicates a different action than when interacting with knife. In\nthis work, we develop assistive robots that condition their latent embeddings\non visual inputs. We explore a spectrum of visual encoders and show that\nincorporating object detectors pretrained on small amounts of cheap,\neasy-to-collect structured data enables i) accurately and robustly recognizing\nthe current context and ii) generalizing control embeddings to new objects and\ntasks. In user studies with a high-dimensional physical robot arm, participants\nleverage this approach to perform new tasks with unseen objects. Our results\nindicate that structured visual representations improve few-shot performance\nand are subjectively preferred by users.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 23:58:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Karamcheti", "Siddharth", ""], ["Zhai", "Albert J.", ""], ["Losey", "Dylan P.", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2105.00584", "submitter": "Rahul Arora", "authors": "Rahul Arora, Jiannan Li, Gongyi Shi, Karan Singh", "title": "Thinking Outside the Lab: VR Size & Depth Perception in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Size and distance perception in Virtual Reality (VR) have been widely\nstudied, albeit in a controlled laboratory setting with a small number of\nparticipants. We describe a fully remote perceptual study with a gamified\nprotocol to encourage participant engagement, which allowed us to quickly\ncollect high-quality data from a large, diverse participant pool (N=60). Our\nstudy aims to understand medium-field size and egocentric distance perception\nin real-world usage of consumer VR devices. We utilized two perceptual matching\ntasks -- distance bisection and size matching -- at the same target distances\nof 1--9 metres. While the bisection protocol indicated a near-universal trend\nof nonlinear distance compression, the size matching estimates were more\nequivocal. Varying eye-height from the floor plane showed no significant effect\non the judgements. We also discuss the pros and cons of a fully remote\nperceptual study in VR, the impact of hardware variation, and measures needed\nto ensure high-quality data.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 00:20:44 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Arora", "Rahul", ""], ["Li", "Jiannan", ""], ["Shi", "Gongyi", ""], ["Singh", "Karan", ""]]}, {"id": "2105.00691", "submitter": "Dominik Dellermann", "authors": "Dominik Dellermann, Philipp Ebel, Matthias Soellner, Jan Marco\n  Leimeister", "title": "Hybrid Intelligence", "comments": null, "journal-ref": null, "doi": "10.1007/s12599-019-00595-2", "report-no": null, "categories": "cs.AI cs.HC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research has a long history of discussing what is superior in predicting\ncertain outcomes: statistical methods or the human brain. This debate has\nrepeatedly been sparked off by the remarkable technological advances in the\nfield of artificial intelligence (AI), such as solving tasks like object and\nspeech recognition, achieving significant improvements in accuracy through\ndeep-learning algorithms (Goodfellow et al. 2016), or combining various methods\nof computational intelligence, such as fuzzy logic, genetic algorithms, and\ncase-based reasoning (Medsker 2012). One of the implicit promises that underlie\nthese advancements is that machines will 1 day be capable of performing complex\ntasks or may even supersede humans in performing these tasks. This triggers new\nheated debates of when machines will ultimately replace humans (McAfee and\nBrynjolfsson 2017). While previous research has proved that AI performs well in\nsome clearly defined tasks such as playing chess, playing Go or identifying\nobjects on images, it is doubted that the development of an artificial general\nintelligence (AGI) which is able to solve multiple tasks at the same time can\nbe achieved in the near future (e.g., Russell and Norvig 2016). Moreover, the\nuse of AI to solve complex business problems in organizational contexts occurs\nscarcely, and applications for AI that solve complex problems remain mainly in\nlaboratory settings instead of being implemented in practice. Since the road to\nAGI is still a long one, we argue that the most likely paradigm for the\ndivision of labor between humans and machines in the next decades is Hybrid\nIntelligence. This concept aims at using the complementary strengths of human\nintelligence and AI, so that they can perform better than each of the two could\nseparately (e.g., Kamar 2016).\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 08:56:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Dellermann", "Dominik", ""], ["Ebel", "Philipp", ""], ["Soellner", "Matthias", ""], ["Leimeister", "Jan Marco", ""]]}, {"id": "2105.00916", "submitter": "Yujiang Wang", "authors": "Yuhu Chang, Yingying Zhao, Mingzhi Dong, Yujiang Wang, Yutian Lu, Qin\n  Lv, Robert P. Dick, Tun Lu, Ning Gu, Li Shang", "title": "MemX: An Attention-Aware Smart Eyewear System for Personalized Moment\n  Auto-capture", "comments": "Proceedings of the ACM on Interactive, Mobile, Wearable and\n  Ubiquitous Technologies (IMWUT)", "journal-ref": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., Volume 5\n  Issue 2, Article 56. June 2021", "doi": "10.1145/3463509", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents MemX: a biologically-inspired attention-aware eyewear\nsystem developed with the goal of pursuing the long-awaited vision of a\npersonalized visual Memex. MemX captures human visual attention on the fly,\nanalyzes the salient visual content, and records moments of personal interest\nin the form of compact video snippets. Accurate attentive scene detection and\nanalysis on resource-constrained platforms is challenging because these tasks\nare computation and energy intensive. We propose a new temporal visual\nattention network that unifies human visual attention tracking and salient\nvisual content analysis. Attention tracking focuses computation-intensive video\nanalysis on salient regions, while video analysis makes human attention\ndetection and tracking more accurate. Using the YouTube-VIS dataset and 30\nparticipants, we experimentally show that MemX significantly improves the\nattention tracking accuracy over the eye-tracking-alone method, while\nmaintaining high system energy efficiency. We have also conducted 11 in-field\npilot studies across a range of daily usage scenarios, which demonstrate the\nfeasibility and potential benefits of MemX.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:54:16 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 14:07:30 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Chang", "Yuhu", ""], ["Zhao", "Yingying", ""], ["Dong", "Mingzhi", ""], ["Wang", "Yujiang", ""], ["Lu", "Yutian", ""], ["Lv", "Qin", ""], ["Dick", "Robert P.", ""], ["Lu", "Tun", ""], ["Gu", "Ning", ""], ["Shang", "Li", ""]]}, {"id": "2105.01057", "submitter": "Soshi Shimada", "authors": "Soshi Shimada and Vladislav Golyanik and Weipeng Xu and Patrick\n  P\\'erez and Christian Theobalt", "title": "Neural Monocular 3D Human Motion Capture with Physical Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new trainable system for physically plausible markerless 3D\nhuman motion capture, which achieves state-of-the-art results in a broad range\nof challenging scenarios. Unlike most neural methods for human motion capture,\nour approach, which we dub physionical, is aware of physical and environmental\nconstraints. It combines in a fully differentiable way several key innovations,\ni.e., 1. a proportional-derivative controller, with gains predicted by a neural\nnetwork, that reduces delays even in the presence of fast motions, 2. an\nexplicit rigid body dynamics model and 3. a novel optimisation layer that\nprevents physically implausible foot-floor penetration as a hard constraint.\nThe inputs to our system are 2D joint keypoints, which are canonicalised in a\nnovel way so as to reduce the dependency on intrinsic camera parameters -- both\nat train and test time. This enables more accurate global translation\nestimation without generalisability loss. Our model can be finetuned only with\n2D annotations when the 3D annotations are not available. It produces smooth\nand physically principled 3D motions in an interactive frame rate in a wide\nvariety of challenging scenes, including newly recorded ones. Its advantages\nare especially noticeable on in-the-wild sequences that significantly differ\nfrom common 3D pose estimation benchmarks such as Human 3.6M and MPI-INF-3DHP.\nQualitative results are available at\nhttp://gvv.mpi-inf.mpg.de/projects/PhysAware/\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:57:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Shimada", "Soshi", ""], ["Golyanik", "Vladislav", ""], ["Xu", "Weipeng", ""], ["P\u00e9rez", "Patrick", ""], ["Theobalt", "Christian", ""]]}, {"id": "2105.01357", "submitter": "Ziran Wang", "authors": "Ziran Wang and Kyungtae Han and Prashant Tiwari", "title": "Digital Twin-Assisted Cooperative Driving at Non-Signalized\n  Intersections", "comments": "This paper was accepted by IEEE Transactions on Intelligent Vehicles", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.HC cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital Twin, as an emerging technology related to Cyber-Physical Systems\n(CPS) and Internet of Things (IoT), has attracted increasing attentions during\nthe past decade. Conceptually, a Digital Twin is a digital replica of a\nphysical entity in the real world, and this technology is leveraged in this\nstudy to design a cooperative driving system at non-signalized intersections,\nallowing connected vehicles to cooperate with each other to cross intersections\nwithout any full stops. Within the proposed Digital Twin framework, we\ndeveloped an enhanced first-in-first-out (FIFO) slot reservation algorithm to\nschedule the sequence of crossing vehicles, a consensus motion control\nalgorithm to calculate vehicles' referenced longitudinal motion, and a\nmodel-based motion estimation algorithm to tackle communication delay and\npacket loss. Additionally, an augmented reality (AR) human-machine-interface\n(HMI) is designed to provide the guidance to drivers to cooperate with other\nconnected vehicles. Agent-based modeling and simulation of the proposed system\nis conducted in Unity game engine based on a real-world map in San Francisco,\nand the human-in-the-loop (HITL) simulation results prove the benefits of the\nproposed algorithms with 20% reduction in travel time and 23.7% reduction in\nenergy consumption, respectively, when compared with traditional signalized\nintersections.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 08:26:39 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Wang", "Ziran", ""], ["Han", "Kyungtae", ""], ["Tiwari", "Prashant", ""]]}, {"id": "2105.01488", "submitter": "Garreth Tigwell", "authors": "Garreth W. Tigwell and Kristen Shinohara and Laleh Nourian", "title": "Accessibility Across Borders", "comments": "Accepted as part of the CHI '21 Workshop: Decolonizing HCI Across\n  Borders", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since prior work has identified that cultural differences influence user\ndesign preferences and interaction methods, as well as emphasizing the need to\nreflect on the appropriateness of popular HCI principles, we believe that it is\nequally important to apply this inquiry to digital accessibility and how\naccessibility fits within the design process around the world. Our long-term\nplan is to build upon work in this area by investigating how digital designers\nin different parts of the world consider accessibility and whether current\naccessibility resources (often developed in the west) meet or conflict with\ntheir approach to design.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 02:27:00 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 03:06:52 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Tigwell", "Garreth W.", ""], ["Shinohara", "Kristen", ""], ["Nourian", "Laleh", ""]]}, {"id": "2105.01734", "submitter": "Jason Wu", "authors": "Jason Wu, Gabriel Reyes, Sam C. White, Xiaoyi Zhang, Jeffrey P. Bigham", "title": "When Can Accessibility Help?: An Exploration of Accessibility Feature\n  Recommendation on Mobile Devices", "comments": "Accepted to Web4All 2021 (W4A '21)", "journal-ref": null, "doi": "10.1145/3430263.3452434", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous accessibility features have been developed and included in consumer\noperating systems to provide people with a variety of disabilities additional\nways to access computing devices. Unfortunately, many users, especially older\nadults who are more likely to experience ability changes, are not aware of\nthese features or do not know which combination to use. In this paper, we first\nquantify this problem via a survey with 100 participants, demonstrating that\nvery few people are aware of built-in accessibility features on their phones.\nThese observations led us to investigate accessibility recommendation as a way\nto increase awareness and adoption. We developed four prototype recommenders\nthat span different accessibility categories, which we used to collect insights\nfrom 20 older adults. Our work demonstrates the need to increase awareness of\nexisting accessibility features on mobile devices, and shows that automated\nrecommendation could help people find beneficial accessibility features.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:11:29 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Wu", "Jason", ""], ["Reyes", "Gabriel", ""], ["White", "Sam C.", ""], ["Zhang", "Xiaoyi", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "2105.01745", "submitter": "Nicholas Dacre PhD", "authors": "Katerina Antonopoulou, Nicholas Dacre", "title": "Exploring Diffusion Characteristics that Influence Serious Games\n  Adoption Decisions", "comments": "Citation: Antonopoulou, K., & Dacre, N. (2015). Exploring Diffusion\n  Characteristics that Influence Serious Games Adoption Decisions. Innovation\n  in Information Infrastructures, Coventry, UK.\n  https://dx.doi.org/10.2139/ssrn.3829185", "journal-ref": "Innovation in Information Infrastructures 2015", "doi": "10.2139/ssrn.3829185", "report-no": null, "categories": "econ.GN cs.HC q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we discuss the diffusion of serious games and present reasons\nfor why Rogers traditional approach is limited in this context. We present an\nalternative overview through the characteristics of relative advantage,\ncompatibility, complexity, trialability, and observability, that reflect on the\nadoption decision and contributes on the commercialization of serious games.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:51:17 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Antonopoulou", "Katerina", ""], ["Dacre", "Nicholas", ""]]}, {"id": "2105.01753", "submitter": "Matej Kr\\'alik", "authors": "Matej Kr\\'alik, Marek \\v{S}uppa", "title": "WaveGlove: Transformer-based hand gesture recognition using multiple\n  inertial sensors", "comments": "Accepted to EUSIPCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hand Gesture Recognition (HGR) based on inertial data has grown considerably\nin recent years, with the state-of-the-art approaches utilizing a single\nhandheld sensor and a vocabulary comprised of simple gestures.\n  In this work we explore the benefits of using multiple inertial sensors.\nUsing WaveGlove, a custom hardware prototype in the form of a glove with five\ninertial sensors, we acquire two datasets consisting of over $11000$ samples.\n  To make them comparable with prior work, they are normalized along with $9$\nother publicly available datasets, and subsequently used to evaluate a range of\nMachine Learning approaches for gesture recognition, including a newly proposed\nTransformer-based architecture. Our results show that even complex gestures\ninvolving different fingers can be recognized with high accuracy.\n  An ablation study performed on the acquired datasets demonstrates the\nimportance of multiple sensors, with an increase in performance when using up\nto three sensors and no significant improvements beyond that.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:50:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Kr\u00e1lik", "Matej", ""], ["\u0160uppa", "Marek", ""]]}, {"id": "2105.01757", "submitter": "Gopika Ajaykumar", "authors": "Gopika Ajaykumar, Maureen Steele, Chien-Ming Huang (Johns Hopkins\n  University)", "title": "A Survey on End-User Robot Programming", "comments": "35 pages, 1 figure", "journal-ref": null, "doi": "10.1145/3466819", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As robots interact with a broader range of end-users, end-user robot\nprogramming has helped democratize robot programming by empowering end-users\nwho may not have experience in robot programming to customize robots to meet\ntheir individual contextual needs. This article surveys work on end-user robot\nprogramming, with a focus on end-user program specification. It describes the\nprimary domains, programming phases, and design choices represented by the\nend-user robot programming literature. The survey concludes by highlighting\nopen directions for further investigation to enhance and widen the reach of\nend-user robot programming systems.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:55:01 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 03:06:54 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ajaykumar", "Gopika", "", "Johns Hopkins\n  University"], ["Steele", "Maureen", "", "Johns Hopkins\n  University"], ["Huang", "Chien-Ming", "", "Johns Hopkins\n  University"]]}, {"id": "2105.01772", "submitter": "Stephen Gilbert", "authors": "Charles Peasley, Rachel Dianiska, Emily Oldham, Nicholas Wilson,\n  Stephen Gilbert, Peggy Wu, Brett Israelsen, James Oliver", "title": "Evaluating Metrics for Standardized Benchmarking of Remote Presence\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To reduce the need for business-related air travel and its associated energy\nconsumption and carbon footprint, the U.S. Department of Energy's ARPA-E is\nsupporting a research project called SCOTTIE - Systematic Communication\nObjectives and Telecommunications Technology Investigations and Evaluations.\nSCOTTIE tests virtual and augmented reality platforms in a functional\ncomparison with face-to-face (FtF) interactions to derive travel replacement\nthresholds for common industrial training scenarios. The primary goal of Study\n1 is to match the communication effectiveness and learning outcomes obtained\nfrom a FtF control using virtual reality (VR) training scenarios in which a\nlocal expert with physical equipment trains a remote apprentice without\nphysical equipment immediately present. This application scenario is\ncommonplace in industrial settings where access to expensive equipment and\nmaterials is limited and a number of apprentices must travel to a central\nlocation in order to undergo training. Supplying an empirically validated\nvirtual training alternative constitutes a readily adoptable use-case for\nbusinesses looking to reduce time and monetary expenditures associated with\ntravel. The technology used for three different virtual presence technologies\nwas strategically selected for feasibility, relatively low cost, business\nrelevance, and potential for impact through transition. The authors suggest\nthat the results of this study might generalize to the challenge of virtual\nconferences.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 21:36:53 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Peasley", "Charles", ""], ["Dianiska", "Rachel", ""], ["Oldham", "Emily", ""], ["Wilson", "Nicholas", ""], ["Gilbert", "Stephen", ""], ["Wu", "Peggy", ""], ["Israelsen", "Brett", ""], ["Oliver", "James", ""]]}, {"id": "2105.01878", "submitter": "Stephan Schl\\\"ogl PhD", "authors": "M. I. Torres, J. M. Olaso, C. Montenegro, R. Santana, A. V\\'azquez, R.\n  Justo, J. A. Lozano, S. Schl\\\"ogl, G. Chollet, N. Dugan, M. Irvine, N.\n  Glackin, C. Pickard, A. Esposito, G. Cordasco, A. Troncone, D.\n  Petrovska-Delacretaz, A. Mtibaa, M. A. Hmani, M. S. Korsnes, L. J.\n  Martinussen, S. Escalera, C. Palmero Cantari\\~no, O. Deroo, O. Gordeeva, J.\n  Tenorio-Laranga, E. Gonzalez-Fraile, B. Fernandez-Ruanova, A. Gonzalez-Pinto", "title": "The EMPATHIC Project: Mid-term Achievements", "comments": "12 pages", "journal-ref": null, "doi": "10.1145/3316782.3322764", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of active aging is to promote changes in the elderly community so as\nto maintain an active, independent and socially-engaged lifestyle.\nTechnological advancements currently provide the necessary tools to foster and\nmonitor such processes. This paper reports on mid-term achievements of the\nEuropean H2020 EMPATHIC project, which aims to research, innovate, explore and\nvalidate new interaction paradigms and platforms for future generations of\npersonalized virtual coaches to assist the elderly and their carers to reach\nthe active aging goal, in the vicinity of their home. The project focuses on\nevidence-based, user-validated research and integration of intelligent\ntechnology, and context sensing methods through automatic voice, eye and facial\nanalysis, integrated with visual and spoken dialogue system capabilities. In\nthis paper, we describe the current status of the system, with a special\nemphasis on its components and their integration, the creation of a Wizard of\nOz platform, and findings gained from user interaction studies conducted\nthroughout the first 18 months of the project.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:56:52 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Torres", "M. I.", ""], ["Olaso", "J. M.", ""], ["Montenegro", "C.", ""], ["Santana", "R.", ""], ["V\u00e1zquez", "A.", ""], ["Justo", "R.", ""], ["Lozano", "J. A.", ""], ["Schl\u00f6gl", "S.", ""], ["Chollet", "G.", ""], ["Dugan", "N.", ""], ["Irvine", "M.", ""], ["Glackin", "N.", ""], ["Pickard", "C.", ""], ["Esposito", "A.", ""], ["Cordasco", "G.", ""], ["Troncone", "A.", ""], ["Petrovska-Delacretaz", "D.", ""], ["Mtibaa", "A.", ""], ["Hmani", "M. A.", ""], ["Korsnes", "M. S.", ""], ["Martinussen", "L. J.", ""], ["Escalera", "S.", ""], ["Cantari\u00f1o", "C. Palmero", ""], ["Deroo", "O.", ""], ["Gordeeva", "O.", ""], ["Tenorio-Laranga", "J.", ""], ["Gonzalez-Fraile", "E.", ""], ["Fernandez-Ruanova", "B.", ""], ["Gonzalez-Pinto", "A.", ""]]}, {"id": "2105.01891", "submitter": "Pol Van Rijn", "authors": "Pol van Rijn, Silvan Mertes, Dominik Schiller, Peter M. C. Harrison,\n  Pauline Larrouy-Maestri, Elisabeth Andr\\'e, Nori Jacoby", "title": "Exploring emotional prototypes in a high dimensional TTS latent space", "comments": "Submitted to INTERSPEECH'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent TTS systems are able to generate prosodically varied and realistic\nspeech. However, it is unclear how this prosodic variation contributes to the\nperception of speakers' emotional states. Here we use the recent psychological\nparadigm 'Gibbs Sampling with People' to search the prosodic latent space in a\ntrained GST Tacotron model to explore prototypes of emotional prosody.\nParticipants are recruited online and collectively manipulate the latent space\nof the generative speech model in a sequentially adaptive way so that the\nstimulus presented to one group of participants is determined by the response\nof the previous groups. We demonstrate that (1) particular regions of the\nmodel's latent space are reliably associated with particular emotions, (2) the\nresulting emotional prototypes are well-recognized by a separate group of human\nraters, and (3) these emotional prototypes can be effectively transferred to\nnew sentences. Collectively, these experiments demonstrate a novel approach to\nthe understanding of emotional speech by providing a tool to explore the\nrelation between the latent space of generative models and human semantics.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 06:49:21 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["van Rijn", "Pol", ""], ["Mertes", "Silvan", ""], ["Schiller", "Dominik", ""], ["Harrison", "Peter M. C.", ""], ["Larrouy-Maestri", "Pauline", ""], ["Andr\u00e9", "Elisabeth", ""], ["Jacoby", "Nori", ""]]}, {"id": "2105.01961", "submitter": "Youjia Zhou", "authors": "Youjia Zhou, Nathaniel Saul, Ilkin Safarli, Bala Krishnamoorthy, Bei\n  Wang", "title": "Stitch Fix for Mapper and Information Gains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mapper construction is a powerful tool from topological data analysis\nthat is designed for the analysis and visualization of multivariate data. In\nthis paper, we investigate a method for stitching a pair of univariate mappers\ntogether into a bivariate mapper and study topological notions of information\ngains during such a process. We further provide implementations that visualize\nsuch information gains.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 10:15:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Zhou", "Youjia", ""], ["Saul", "Nathaniel", ""], ["Safarli", "Ilkin", ""], ["Krishnamoorthy", "Bala", ""], ["Wang", "Bei", ""]]}, {"id": "2105.01968", "submitter": "\\c{C}etin T\\\"uker Mr.", "authors": "Cetin Tuker and Togan Tong", "title": "Comparing Field Trips, VR Experiences and Video Representations on\n  Spatial Layout Learning in Complex Buildings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study aimed to compare and investigate the efficacy of the real-world\nexperiences, immersive virtual reality (IVR) experiences, and video walkthrough\nrepresentations on layout-learning in a complex building. A quasi-experimental,\nintervention, and delayed post-test research design was used among three\ngroups: real-world, IVR, and video walkthrough representation. A total of 41\nfirst-year design students from architecture, and game design departments were\nattended the study. Design students were selected as they already know how to\ncommunicate graphically the layout of a building on paper. IVR and video\nwalkthrough groups experienced the representations of the building by\nthemselves, but real-world group experienced the building within a group as\nimitating a design education field trip. After 10 days, a total of 26\nparticipants out of 41 tested for spatial recall performance. Recall\nperformances were measured as an indicator of layout learning by analysing the\nplan sketches drawn by the participants. Results showed IVR group recalled\nsignificantly more spatial memories compared with the real-world and video\nwalkthrough representation groups. Real-world group performed the worst among\nthree groups. This result was interpreted to three conclusions. First, the\nlayout learning tends to be overly sensitive to distractions and lower levels\nof distraction may lead to better levels of layout learning. Second, for the\ndomain of layout knowledge learning, a remarkably simple IVR model is enough.\nThird, although real-world experiences give direct and richer sensory\ninformation about the spaces, layout knowledge acquisition from the surrounding\nenvironment requires psychologically active wayfinding decisions.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 10:35:19 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tuker", "Cetin", ""], ["Tong", "Togan", ""]]}, {"id": "2105.01992", "submitter": "Yu Li", "authors": "Yu Li, Josh Arnold, Feifan Yan, Weiyan Shi and Zhou Yu", "title": "LEGOEval: An Open-Source Toolkit for Dialogue System Evaluation via\n  Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LEGOEval, an open-source toolkit that enables researchers to\neasily evaluate dialogue systems in a few lines of code using the online\ncrowdsource platform, Amazon Mechanical Turk. Compared to existing toolkits,\nLEGOEval features a flexible task design by providing a Python API that maps to\ncommonly used React.js interface components. Researchers can personalize their\nevaluation procedures easily with our built-in pages as if playing with LEGO\nblocks. Thus, LEGOEval provides a fast, consistent method for reproducing human\nevaluation results. Besides the flexible task design, LEGOEval also offers an\neasy API to review collected data.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 11:38:14 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Li", "Yu", ""], ["Arnold", "Josh", ""], ["Yan", "Feifan", ""], ["Shi", "Weiyan", ""], ["Yu", "Zhou", ""]]}, {"id": "2105.02098", "submitter": "Giulia Perugia Dr.", "authors": "Giulia Perugia, Maike Paetzel-Pr\\\"ussman, Isabelle Hupont, Giovanna\n  Varni, Mohamed Chetouani, Christopher Edward Peters, and Ginevra Castellano", "title": "Does the Goal Matter? Emotion Recognition Tasks Can Change the Social\n  Value of Facial Mimicry towards Artificial Agents", "comments": "27 pages, 8 figures, 7 tables (Submitted to Frontiers in Robotics and\n  AI, Human-Robot Interaction)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a study aimed at understanding whether the\nembodiment and humanlikeness of an artificial agent can affect people's\nspontaneous and instructed mimicry of its facial expressions. The study\nfollowed a mixed experimental design and revolved around an emotion recognition\ntask. Participants were randomly assigned to one level of humanlikeness\n(between-subject variable: humanlike, characterlike, or morph facial texture of\nthe artificial agents) and observed the facial expressions displayed by a human\n(control) and three artificial agents differing in embodiment (within-subject\nvariable: video-recorded robot, physical robot, and virtual agent). To study\nboth spontaneous and instructed facial mimicry, we divided the experimental\nsessions into two phases. In the first phase, we asked participants to observe\nand recognize the emotions displayed by the agents. In the second phase, we\nasked them to look at the agents' facial expressions, replicate their dynamics\nas closely as possible, and then identify the observed emotions. In both cases,\nwe assessed participants' facial expressions with an automated Action Unit (AU)\nintensity detector. Contrary to our hypotheses, our results disclose that the\nagent that was perceived as the least uncanny, and most anthropomorphic,\nlikable, and co-present, was the one spontaneously mimicked the least.\nMoreover, they show that instructed facial mimicry negatively predicts\nspontaneous facial mimicry. Further exploratory analyses revealed that\nspontaneous facial mimicry appeared when participants were less certain of the\nemotion they recognized. Hence, we postulate that an emotion recognition goal\ncan flip the social value of facial mimicry as it transforms a likable\nartificial agent into a distractor.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:58:04 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Perugia", "Giulia", ""], ["Paetzel-Pr\u00fcssman", "Maike", ""], ["Hupont", "Isabelle", ""], ["Varni", "Giovanna", ""], ["Chetouani", "Mohamed", ""], ["Peters", "Christopher Edward", ""], ["Castellano", "Ginevra", ""]]}, {"id": "2105.02139", "submitter": "Daniele Giunchi", "authors": "Daniele Giunchi, Alejandro Sztrajman, Stuart James, Anthony Steed", "title": "Mixing Modalities of 3D Sketching and Speech for Interactive Model\n  Retrieval in Virtual Reality", "comments": "Published at IMX 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketch and speech are intuitive interaction methods that convey complementary\ninformation and have been independently used for 3D model retrieval in virtual\nenvironments. While sketch has been shown to be an effective retrieval method,\nnot all collections are easily navigable using this modality alone. We design a\nnew challenging database for sketch comprised of 3D chairs where each of the\ncomponents (arms, legs, seat, back) are independently colored. To overcome\nthis, we implement a multimodal interface for querying 3D model databases\nwithin a virtual environment. We base the sketch on the state-of-the-art for 3D\nSketch Retrieval, and use a Wizard-of-Oz style experiment to process the voice\ninput. In this way, we avoid the complexities of natural language processing\nwhich frequently requires fine-tuning to be robust. We conduct two user studies\nand show that hybrid search strategies emerge from the combination of\ninteractions, fostering the advantages provided by both modalities.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:44:20 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Giunchi", "Daniele", ""], ["Sztrajman", "Alejandro", ""], ["James", "Stuart", ""], ["Steed", "Anthony", ""]]}, {"id": "2105.02182", "submitter": "Caroline Pitt", "authors": "Caroline Pitt (1), Adam Bell (2), Brandyn S. Boyd (1), Nikki Demmel\n  (1), and Katie Davis (1) ((1) The Information School, University of\n  Washington, (2) College of Education, University of Washington)", "title": "Connected Learning, Collapsed Contexts", "comments": "22 pages, 2 figures, Proceedings of the ACM Conference on Human\n  Factors in Computing Systems (CHI '21)", "journal-ref": null, "doi": "10.1145/3411764.3445635", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Researchers and designers have incorporated social media affordances into\nlearning technologies to engage young people and support personally relevant\nlearning, but youth may reject these attempts because they do not meet user\nexpectations. Through in-depth case studies, we explore the sociotechnical\necosystems of six teens (ages 15-18) working at a science center that had\nrecently introduced a digital badge system to track and recognize their\nlearning. By analyzing interviews, observations, ecological momentary\nassessments, and system data, we examined tensions in how badges as connected\nlearning technologies operate in teens' sociotechnical ecosystems. We found\nthat, due to issues of unwanted context collapse and incongruent identity\nrepresentations, youth only used certain affordances of the system and did so\nsporadically. Additionally, we noted that some features seemed to prioritize\nvalues of adult stakeholders over youth. Using badges as a lens, we reveal\ncritical tensions and offer design recommendations for networked learning\ntechnologies.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:50:52 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Pitt", "Caroline", ""], ["Bell", "Adam", ""], ["Boyd", "Brandyn S.", ""], ["Demmel", "Nikki", ""], ["Davis", "Katie", ""]]}, {"id": "2105.02189", "submitter": "Caroline Pitt", "authors": "Caroline Pitt (1), Ari Hock (2), Leila Zelnick (3), and Katie Davis\n  (1) ((1) The Information School, University of Washington, (2) College of\n  Education, University of Washington, (3) Department of Medicine, University\n  of Washington)", "title": "The Kids Are / Not / Sort of All Right", "comments": "22 pages, 2 figures, 2 tables, Proceedings of the ACM Conference on\n  Human Factors in Computing Systems (CHI '21)", "journal-ref": null, "doi": "10.1145/3411764.3445541", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We investigated changes in and factors affecting American adolescents'\nsubjective wellbeing during the early months (April - August 2020) of the\ncoronavirus pandemic in the United States. Twenty-one teens (14 - 19 years)\nparticipated in interviews at the start and end of the study and completed\necological momentary assessments three times per week between the interviews.\nThere was an aggregate trend toward increased wellbeing, with considerable\nvariation within and across participants. Teens reported greater reliance on\nnetworked technologies as their unstructured time increased during lockdown.\nUsing multilevel growth modeling, we found that how much total time teens spent\nwith technology had less bearing on daily fluctuations in wellbeing than the\nsatisfaction and meaning they derived from their technology use. Ultimately,\nteens felt online communication could not replace face-to-face interactions. We\nconducted two follow-up participatory design sessions with nine teens to\nexplore these insights in greater depth and reflect on general implications for\ndesign to support teens' meaningful technology experiences and wellbeing during\ndisruptive life events.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:59:46 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Pitt", "Caroline", ""], ["Hock", "Ari", ""], ["Zelnick", "Leila", ""], ["Davis", "Katie", ""]]}, {"id": "2105.02357", "submitter": "Samanta Knapi\\v{c}", "authors": "Samanta Knapi\\v{c}, Avleen Malhi, Rohit Saluja, Kary Fr\\\"amling", "title": "Explainable Artificial Intelligence for Human Decision-Support System in\n  Medical Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the present paper we present the potential of Explainable Artificial\nIntelligence methods for decision-support in medical image analysis scenarios.\nWith three types of explainable methods applied to the same medical image data\nset our aim was to improve the comprehensibility of the decisions provided by\nthe Convolutional Neural Network (CNN). The visual explanations were provided\non in-vivo gastral images obtained from a Video capsule endoscopy (VCE), with\nthe goal of increasing the health professionals' trust in the black box\npredictions. We implemented two post-hoc interpretable machine learning methods\nLIME and SHAP and the alternative explanation approach CIU, centered on the\nContextual Value and Utility (CIU). The produced explanations were evaluated\nusing human evaluation. We conducted three user studies based on the\nexplanations provided by LIME, SHAP and CIU. Users from different non-medical\nbackgrounds carried out a series of tests in the web-based survey setting and\nstated their experience and understanding of the given explanations. Three user\ngroups (n=20, 20, 20) with three distinct forms of explanations were\nquantitatively analyzed. We have found that, as hypothesized, the CIU\nexplainable method performed better than both LIME and SHAP methods in terms of\nincreasing support for human decision-making as well as being more transparent\nand thus understandable to users. Additionally, CIU outperformed LIME and SHAP\nby generating explanations more rapidly. Our findings suggest that there are\nnotable differences in human decision-making between various explanation\nsupport settings. In line with that, we present three potential explainable\nmethods that can with future improvements in implementation be generalized on\ndifferent medical data sets and can provide great decision-support for medical\nexperts.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:29:28 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Knapi\u010d", "Samanta", ""], ["Malhi", "Avleen", ""], ["Saluja", "Rohit", ""], ["Fr\u00e4mling", "Kary", ""]]}, {"id": "2105.02460", "submitter": "Manh Duong Phung", "authors": "Manh Duong Phung, Cong Hoang Quach and Quang Vinh Tran", "title": "Development of a Fast and Robust Gaze Tracking System for Game\n  Applications", "comments": "arXiv admin note: substantial text overlap with arXiv:1611.09427", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, a novel eye tracking system using a visual camera is developed\nto extract human's gaze, and it can be used in modern game machines to bring\nnew and innovative interactive experience to players. Central to the components\nof the system, is a robust iris-center and eye-corner detection algorithm\nbasing on it the gaze is continuously and adaptively extracted. Evaluation\ntests were applied to nine people to evaluate the accuracy of the system and\nthe results were 2.50 degrees (view angle) in horizontal direction and 3.07\ndegrees in vertical direction.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 06:41:30 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Phung", "Manh Duong", ""], ["Quach", "Cong Hoang", ""], ["Tran", "Quang Vinh", ""]]}, {"id": "2105.02738", "submitter": "Marija Slavkovik", "authors": "Marija Slavkovik, Clemens Stachl, Caroline Pitman, Jonathan Askonas", "title": "Digital Voodoo Dolls", "comments": "Accepted for publication at Artificial Intelligence, Ethics and\n  Society (AIES-2021)", "journal-ref": null, "doi": "10.1145/3461702.3462626", "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An institution, be it a body of government, commercial enterprise, or a\nservice, cannot interact directly with a person. Instead, a model is created to\nrepresent us. We argue the existence of a new high-fidelity type of person\nmodel which we call a digital voodoo doll. We conceptualize it and compare its\nfeatures with existing models of persons. Digital voodoo dolls are\ndistinguished by existing completely beyond the influence and control of the\nperson they represent. We discuss the ethical issues that such a lack of\naccountability creates and argue how these concerns can be mitigated.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:56:54 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 16:29:06 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Slavkovik", "Marija", ""], ["Stachl", "Clemens", ""], ["Pitman", "Caroline", ""], ["Askonas", "Jonathan", ""]]}, {"id": "2105.02825", "submitter": "Leon Sixt", "authors": "Martin Schuessler, Philipp Wei{\\ss}, Leon Sixt", "title": "Two4Two: Evaluating Interpretable Machine Learning - A Synthetic Dataset\n  For Controlled Experiments", "comments": "6 pages, 3 figures, presented at the ICLR 2021 RAI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of approaches exist to generate explanations for image\nclassification. However, few of these approaches are subjected to human-subject\nevaluations, partly because it is challenging to design controlled experiments\nwith natural image datasets, as they leave essential factors out of the\nresearcher's control. With our approach, researchers can describe their desired\ndataset with only a few parameters. Based on these, our library generates\nsynthetic image data of two 3D abstract animals. The resulting data is suitable\nfor algorithmic as well as human-subject evaluations. Our user study results\ndemonstrate that our method can create biases predictive enough for a\nclassifier and subtle enough to be noticeable only to every second participant\ninspecting the data visually. Our approach significantly lowers the barrier for\nconducting human subject evaluations, thereby facilitating more rigorous\ninvestigations into interpretable machine learning. For our library and\ndatasets see, https://github.com/mschuessler/two4two/\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:14:39 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Schuessler", "Martin", ""], ["Wei\u00df", "Philipp", ""], ["Sixt", "Leon", ""]]}, {"id": "2105.02923", "submitter": "Tanner Bohn", "authors": "Tanner Bohn and Charles X. Ling", "title": "Hone as You Read: A Practical Type of Interactive Summarization", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present HARE, a new task where reader feedback is used to optimize\ndocument summaries for personal interest during the normal flow of reading.\nThis task is related to interactive summarization, where personalized summaries\nare produced following a long feedback stage where users may read the same\nsentences many times. However, this process severely interrupts the flow of\nreading, making it impractical for leisurely reading. We propose to gather\nminimally-invasive feedback during the reading process to adapt to user\ninterests and augment the document in real-time. Building off of recent\nadvances in unsupervised summarization evaluation, we propose a suitable metric\nfor this task and use it to evaluate a variety of approaches. Our approaches\nrange from simple heuristics to preference-learning and their analysis provides\ninsight into this important task. Human evaluation additionally supports the\npracticality of HARE. The code to reproduce this work is available at\nhttps://github.com/tannerbohn/HoneAsYouRead.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 19:36:40 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bohn", "Tanner", ""], ["Ling", "Charles X.", ""]]}, {"id": "2105.02933", "submitter": "Maria Bada Dr", "authors": "Maria Bada and Basie von Solms", "title": "A Cybersecurity Guide for Using Fitness Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The popularity of wearable devices is growing exponentially, with consumers\nusing these for a variety of services. Fitness devices are currently offering\nnew services such as shopping or buying train tickets using contactless\npayment. In addition, fitness devices are collecting a number of personal\ninformation such as body temperature, pulse rate, food habits and body weight,\nsteps-distance travelled, calories burned and sleep stage. Although these\ndevices can offer convenience to consumers, more and more reports are warning\nof the cybersecurity risks of such devices, and the possibilities for such\ndevices to be hacked and used as springboards to other systems. Due to their\nwireless transmissions, these devices can potentially be vulnerable to a\nmalicious attack allowing the data collected to be exposed. The vulnerabilities\nof these devices stem from lack of authentication, disadvantages of Bluetooth\nconnections, location tracking as well as third party vulnerabilities.\nGuidelines do exist for securing such devices, but most of such guidance is\ndirected towards device manufacturers or IoT providers, while consumers are\noften unaware of potential risks. The aim of this paper is to provide\ncybersecurity guidelines for users in order to take measures to avoid risks\nwhen using fitness devices.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 20:10:34 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bada", "Maria", ""], ["von Solms", "Basie", ""]]}, {"id": "2105.02960", "submitter": "Abu Sufian", "authors": "Abu Sufian, Changsheng You and Mianxiong Dong", "title": "A Deep Transfer Learning-based Edge Computing Method for Home Health\n  Monitoring", "comments": "6 pages, 4 figures. Pre-print copy", "journal-ref": "55th Annual Conference on Information Sciences and Systems (CISS),\n  2021", "doi": "10.1109/CISS50987.2021.9400321", "report-no": null, "categories": "cs.CV cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The health-care gets huge stress in a pandemic or epidemic situation. Some\ndiseases such as COVID-19 that causes a pandemic is highly spreadable from an\ninfected person to others. Therefore, providing health services at home for\nnon-critical infected patients with isolation shall assist to mitigate this\nkind of stress. In addition, this practice is also very useful for monitoring\nthe health-related activities of elders who live at home. The home health\nmonitoring, a continuous monitoring of a patient or elder at home using visual\nsensors is one such non-intrusive sub-area of health services at home. In this\narticle, we propose a transfer learning-based edge computing method for home\nhealth monitoring. Specifically, a pre-trained convolutional neural\nnetwork-based model can leverage edge devices with a small amount of\nground-labeled data and fine-tuning method to train the model. Therefore,\non-site computing of visual data captured by RGB, depth, or thermal sensor\ncould be possible in an affordable way. As a result, raw data captured by these\ntypes of sensors is not required to be sent outside from home. Therefore,\nprivacy, security, and bandwidth scarcity shall not be issues. Moreover,\nreal-time computing for the above-mentioned purposes shall be possible in an\neconomical way.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:01:41 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Sufian", "Abu", ""], ["You", "Changsheng", ""], ["Dong", "Mianxiong", ""]]}, {"id": "2105.02970", "submitter": "Huansheng Ning Prof", "authors": "Yuanyuan Zheng, Rongyang Li, Sha Li, Yudong Zhang, Shunkun Yang,\n  Huansheng Ning", "title": "A Review on Serious Games for ADHD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention deficit and hyperactivity disorder (ADHD) have two main\ncharacteristics: inattention and impulsivity.It has many obstacles to the\nnormal development of children and is very common among children.As an\nintervention, Serious games for ADHD(SGADs) have shown great potential and are\nvery effective for these ADHD patients.Although many serious games have been\ndeveloped for ADHD patients, but a review paper that summarizes and generalizes\nthe topic of video games has not yet appeared.In this article, we first\nclassified serious games about ADHD according to different platforms developed\nby video games, and then we conducted a systematic review of video games that\ncan help children with ADHD diagnose and treat. Finally, we discussed and made\nsuggestions based on the current development of SGADs.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:10:28 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zheng", "Yuanyuan", ""], ["Li", "Rongyang", ""], ["Li", "Sha", ""], ["Zhang", "Yudong", ""], ["Yang", "Shunkun", ""], ["Ning", "Huansheng", ""]]}, {"id": "2105.02980", "submitter": "Kenneth Holstein", "authors": "Hong Shen, Alicia DeVos, Motahhare Eslami, Kenneth Holstein", "title": "Everyday algorithm auditing: Understanding the power of everyday users\n  in surfacing harmful algorithmic behaviors", "comments": "The co-first and co-senior authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": "27414781 27414781 27414781", "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A growing body of literature has proposed formal approaches to audit\nalgorithmic systems for biased and harmful behaviors. While formal auditing\napproaches have been greatly impactful, they often suffer major blindspots,\nwith critical issues surfacing only in the context of everyday use once systems\nare deployed. Recent years have seen many cases in which everyday users of\nalgorithmic systems detect and raise awareness about harmful behaviors that\nthey encounter in the course of their everyday interactions with these systems.\nHowever, to date little academic attention has been granted to these bottom-up,\nuser-driven auditing processes. In this paper, we propose and explore the\nconcept of everyday algorithm auditing, a process in which users detect,\nunderstand, and interrogate problematic machine behaviors via their day-to-day\ninteractions with algorithmic systems. We argue that everyday users are\npowerful in surfacing problematic machine behaviors that may elude detection\nvia more centrally-organized forms of auditing, regardless of users' knowledge\nabout the underlying algorithms. We analyze several real-world cases of\neveryday algorithm auditing, drawing lessons from these cases for the design of\nfuture platforms and tools that facilitate such auditing behaviors. Finally, we\ndiscuss work that lies ahead, toward bridging the gaps between formal auditing\napproaches and the organic auditing behaviors that emerge in everyday use of\nalgorithmic systems.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 21:50:47 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Shen", "Hong", ""], ["DeVos", "Alicia", ""], ["Eslami", "Motahhare", ""], ["Holstein", "Kenneth", ""]]}, {"id": "2105.03123", "submitter": "Kostas Karpouzis", "authors": "Kostas Karpouzis and George Tsatiris", "title": "AI in (and for) Games", "comments": "Submitted to G. Tsihrintzis, M. Virvou, L. Tsoukalas,. A. Esposito,\n  L. Jain (Eds.), \"Artificial Intelligence and Assistive Technologies\",\n  Springer, 2021. arXiv admin note: text overlap with arXiv:2101.11333", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This chapter outlines the relation between artificial intelligence (AI) /\nmachine learning (ML) algorithms and digital games. This relation is two-fold:\non one hand, AI/ML researchers can generate large, in-the-wild datasets of\nhuman affective activity, player behaviour (i.e. actions within the game\nworld), commercial behaviour, interaction with graphical user interface\nelements or messaging with other players, while games can utilise intelligent\nalgorithms to automate testing of game levels, generate content, develop\nintelligent and responsive non-player characters (NPCs) or predict and respond\nplayer behaviour across a wide variety of player cultures. In this work, we\ndiscuss some of the most common and widely accepted uses of AI/ML in games and\nhow intelligent systems can benefit from those, elaborating on estimating\nplayer experience based on expressivity and performance, and on generating\nproper and interesting content for a language learning game.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:57:07 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Karpouzis", "Kostas", ""], ["Tsatiris", "George", ""]]}, {"id": "2105.03181", "submitter": "Ilyena Hirskyj-Douglas Dr", "authors": "Ilyena Hirskyj-Douglas and Andr\\'es Lucero", "title": "On the Internet, Nobody Knows You're a Dog... Unless You're Another Dog", "comments": "10 pages", "journal-ref": "In Proceedings of the 2019 CHI Conference on Human Factors in\n  Computing Systems (CHI 19)", "doi": "10.1145/3290605.3300347", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How humans use computers has evolved from human-machine interfaces to\nhuman-human computer mediated communication. Whilst the field of\nanimal-computer interaction has roots in HCI, technology developed in this area\ncurrently only supports animal-computer communication. This design fiction\npaper presents animal-animal connected interfaces, using dogs as an instance.\nThrough a co-design workshop, we created six proposals. The designs focused on\nwhat a dog internet could look like and how interactions might be presented.\nAnalysis of the narratives and conceived designs indicated that participants'\nconcerns focused around asymmetries within the interaction. This resulted in\nthe use of objects seen as familiar to dogs. This was conjoined with interest\nin how to initiate and end interactions, which was often achieved through\nnotification systems. This paper builds upon HCI methods for unconventional\nusers, and applies a design fiction approach to uncover key questions towards\nthe creation of animal-to-animal interfaces.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:44:31 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hirskyj-Douglas", "Ilyena", ""], ["Lucero", "Andr\u00e9s", ""]]}, {"id": "2105.03184", "submitter": "Ilyena Hirskyj-Douglas Dr", "authors": "Ilyena Hirskyj-Douglas, Mikko Kyto and David McGookin", "title": "Head-mounted Displays, Smartphones, or Smartwatches? -- Augmenting\n  Conversations with Digital Representation of Self", "comments": null, "journal-ref": "Proc. ACM Hum.-Comput. Interact. 2019. 3, CSCW, Article 179", "doi": "10.1145/3359281", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Technologies that augment face-to-face interactions with a digital sense of\nself have been used to support conversations. That work has employed one\nhomogenous technology, either 'off-the-shelf' or with a bespoke prototype,\nacross all participants. Beyond speculative instances, it is unclear what\ntechnology individuals themselves would choose, if any, to augment their social\ninteractions; what influence it may exert; or how use of heterogeneous devices\nmay affect the value of this augmentation. This is important, as the devices\nthat we use directly affect our behaviour, influencing affordances and how we\nengage in social interactions. Through a study of 28 participants, we compared\nhead-mounted display, smartphones, and smartwatches to support digital\naugmentation of self during face-to-face interactions within a group. We\nidentified a preference among participants for head-mounted displays to support\nprivacy, while smartwatches and smartphones better supported conversational\nevents (such as grounding and repair), along with group use through\nscreen-sharing. Accordingly, we present software and hardware design\nrecommendations and user interface guidelines for integrating a digital form of\nself into face-to-face conversations.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 11:49:02 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Hirskyj-Douglas", "Ilyena", ""], ["Kyto", "Mikko", ""], ["McGookin", "David", ""]]}, {"id": "2105.03354", "submitter": "Dominik Dellermann", "authors": "Dominik Dellermann, Adrian Calma, Nikolaus Lipusch, Thorsten Weber,\n  Sascha Weigel, and Philipp Ebel", "title": "The future of human-AI collaboration: a taxonomy of design knowledge for\n  hybrid intelligence systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent technological advances, especially in the field of machine learning,\nprovide astonishing progress on the road towards artificial general\nintelligence. However, tasks in current real-world business applications cannot\nyet be solved by machines alone. We, therefore, identify the need for\ndeveloping socio-technological ensembles of humans and machines. Such systems\npossess the ability to accomplish complex goals by combining human and\nartificial intelligence to collectively achieve superior results and\ncontinuously improve by learning from each other. Thus, the need for structured\ndesign knowledge for those systems arises. Following a taxonomy development\nmethod, this article provides three main contributions: First, we present a\nstructured overview of interdisciplinary research on the role of humans in the\nmachine learning pipeline. Second, we envision hybrid intelligence systems and\nconceptualize the relevant dimensions for system design for the first time.\nFinally, we offer useful guidance for system developers during the\nimplementation of such applications.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:10:44 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Dellermann", "Dominik", ""], ["Calma", "Adrian", ""], ["Lipusch", "Nikolaus", ""], ["Weber", "Thorsten", ""], ["Weigel", "Sascha", ""], ["Ebel", "Philipp", ""]]}, {"id": "2105.03356", "submitter": "Dominik Dellermann", "authors": "Dominik Dellermann, Nikolaus Lipusch, Philipp Ebel, and Jan Marco\n  Leimeister", "title": "Design principles for a hybrid intelligence decision support system for\n  business model validation", "comments": null, "journal-ref": null, "doi": "10.1007/s12525-018-0309-2", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the most critical tasks for startups is to validate their business\nmodel. Therefore, entrepreneurs try to collect information such as feedback\nfrom other actors to assess the validity of their assumptions and make\ndecisions. However, previous work on decisional guidance for business model\nvalidation provides no solution for the highly uncertain and complex context of\nearlystage startups. The purpose of this paper is, thus, to develop design\nprinciples for a Hybrid Intelligence decision support system (HI-DSS) that\ncombines the complementary capabilities of human and machine intelligence. We\nfollow a design science research approach to design a prototype artifact and a\nset of design principles. Our study provides prescriptive knowledge for HI-DSS\nand contributes to previous work on decision support for business models, the\napplications of complementary strengths of humans and machines for making\ndecisions, and support systems for extremely uncertain decision-making\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:13:36 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Dellermann", "Dominik", ""], ["Lipusch", "Nikolaus", ""], ["Ebel", "Philipp", ""], ["Leimeister", "Jan Marco", ""]]}, {"id": "2105.03360", "submitter": "Dominik Dellermann", "authors": "Dominik Dellermann, Nikolaus Lipusch, Philipp Ebel, Karl Michael Popp,\n  and Jan Marco Leimeister", "title": "Finding the unicorn: Predicting early stage startup success through a\n  hybrid intelligence method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence is an emerging topic and will soon be able to perform\ndecisions better than humans. In more complex and creative contexts such as\ninnovation, however, the question remains whether machines are superior to\nhumans. Machines fail in two kinds of situations: processing and interpreting\nsoft information (information that cannot be quantified) and making predictions\nin unknowable risk situations of extreme uncertainty. In such situations, the\nmachine does not have representative information for a certain outcome.\nThereby, humans are still the gold standard for assessing soft signals and make\nuse of intuition. To predict the success of startups, we, thus, combine the\ncomplementary capabilities of humans and machines in a Hybrid Intelligence\nmethod. To reach our aim, we follow a design science research approach to\ndevelop a Hybrid Intelligence method that combines the strength of both machine\nand collective intelligence to demonstrate its utility for predictions under\nextreme uncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:16:36 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Dellermann", "Dominik", ""], ["Lipusch", "Nikolaus", ""], ["Ebel", "Philipp", ""], ["Popp", "Karl Michael", ""], ["Leimeister", "Jan Marco", ""]]}, {"id": "2105.03365", "submitter": "Dominik Dellermann", "authors": "Dominik Dellermann", "title": "Accelerating Entrepreneurial Decision-Making Through Hybrid Intelligence", "comments": null, "journal-ref": null, "doi": "10.17170/kobra-202004301196", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accelerating Entrepreneurial Decision-Making Through Hybrid Intelligence\nDESIGN PARADIGMS AND PRINCIPLES FOR DECISIONAL GUIDANCE IN ENTREPRENEURSHIP\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 16:24:47 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Dellermann", "Dominik", ""]]}, {"id": "2105.03456", "submitter": "Garrett Allen", "authors": "Garrett Allen, Katherine Landau Wright, Jerry Alan Fails, Casey\n  Kennington, Maria Soledad Pera", "title": "CASTing a Net: Supporting Teachers with Search Technology", "comments": "KidRec '21: 5th International and Interdisciplinary Perspectives on\n  Children & Recommender and Information Retrieval Systems (KidRec) Search and\n  Recommendation Technology through the Lens of a Teacher- Co-located with ACM\n  IDC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Past and current research has typically focused on ensuring that search\ntechnology for the classroom serves children. In this paper, we argue for the\nneed to broaden the research focus to include teachers and how search\ntechnology can aid them. In particular, we share how furnishing a\nbehind-the-scenes portal for teachers can empower them by providing a window\ninto the spelling, writing, and concept connection skills of their students.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 18:13:49 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Allen", "Garrett", ""], ["Wright", "Katherine Landau", ""], ["Fails", "Jerry Alan", ""], ["Kennington", "Casey", ""], ["Pera", "Maria Soledad", ""]]}, {"id": "2105.03708", "submitter": "Maria Soledad Pera", "authors": "Emiliana Murgia, Monica Landoni, Theo Huibers, Maria Soledad Pera", "title": "All Together Now: Teachers as Research Partners in the Design of Search\n  Technology for the Classroom", "comments": "In KidRec '21: 5th International and Interdisciplinary Perspectives\n  on Children & Recommender and Information Retrieval Systems (KidRec) Search\n  and Recommendation Technology through the Lens of a Teacher- Co-located with\n  ACM IDC 2021; June 26, 2021; Online Event", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the classroom environment, search tools are the means for students to\naccess Web resources. The perspectives of students, researchers, and industry\npractitioners lead the ongoing research debate in this area. In this article,\nwe argue in favor of incorporating a new voice into this debate: teachers. We\nshowcase the value of involving teachers in all aspects related to the design\nof search tools for the classroom; from the beginning till the end. Driven by\nour research experience designing, developing, and evaluating new tools to\nsupport children's information discovery in the classroom, we share insights on\nthe role of the experts-in-the-loop, i.e., teachers who provide the connection\nbetween search tools and students. And yes, in our case, always involving a\nteacher as a research partner.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 14:25:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Murgia", "Emiliana", ""], ["Landoni", "Monica", ""], ["Huibers", "Theo", ""], ["Pera", "Maria Soledad", ""]]}, {"id": "2105.03839", "submitter": "Aditi Mishra", "authors": "Aditi Mishra, Shashank Ginjpalli, Chris Bryan", "title": "News Kaleidoscope: Visual Investigation of Coverage Diversity in News\n  Event Reporting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a visual analytics system, NewsKaleidoscope, to investigate the\nhow news reporting of events varies. NewsKaleidoscope combines several backend\ntext language processing techniques with a coordinated visualization interface\ntailored for visualization non-expert users. To robustly evaluate\nNewsKaleidoscope, we conduct a trio of user studies. (1) A usability study with\nnews novices assesses the overall system and the specific insights promoted for\njournalism-agnostic users. (2) A follow-up study with news experts assesses the\ninsights promoted for journalism-savvy users. (3) Based on identified system\nlimitations in these two studies, we amend NewsKaleidoscope design and conduct\na third study to validate these improvements. Results indicate that, for both\nnews novice and experts, NewsKaleidoscope supports an effective, task-driven\nworkflow for analyzing the diversity of news coverage about events, though\njournalism expertise has a significant influence on the user insights and\ntakeaways. Our insights while developing and evaluating NewsKaleidoscope can\naid future interface designs that combine visualization with natural language\nprocessing to analyze coverage diversity in news event reporting.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 05:23:45 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Mishra", "Aditi", ""], ["Ginjpalli", "Shashank", ""], ["Bryan", "Chris", ""]]}, {"id": "2105.04015", "submitter": "M.C. Schraefel", "authors": "m.c. schraefel and Michael Jones", "title": "Discomfort: a New Material for Interaction Design", "comments": "8 pages of text + 2 pages refs, 36 references. Accepted paper, 4th\n  Body as Starting Point Workshop, part of ACM CHI2021 conference\n  (https://dl.acm.org/doi/10.1145/3411763.3441335)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes discomfort as a new material for HCI researchers and\ndesigners to consider in any application that helps a person develop a new\nskill, practice or state. Discomfort is a fundamental precursor of adaptation\nand adaptation leads to new skill, practice or state. The way in which\ndiscomfort is perceived, and when it is experienced, is also often part of a\nrationale for rejecting or adopting a practice. Engaging effectively with\ndiscomfort may lead to increased personal development. We propose incorporating\ndiscomfort-as-material into our designs explicitly as a mechanism to make\ndesired adaptations available to more of us, more effectively and more of the\ntime. To explore this possibility, we offer an overview of the physiology and\nneurology of discomfort in adaptation and propose 3 issues related to\nincorporating discomfort into design: preparation for discomfort, need for\nrecovery, and value of the practice. We look forward in the Workshop to\nexploring and developing ideas for specific Discomfortable Designs to insource\ndiscomfort as part of positive, resilient adaptation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 20:12:51 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["schraefel", "m. c.", ""], ["Jones", "Michael", ""]]}, {"id": "2105.04022", "submitter": "Daniel Klug", "authors": "Daniel Klug, Elke Schlote", "title": "Designing a Web Application for Simple and Collaborative Video\n  Annotation That Meets Teaching Routines and Educational Requirements", "comments": "24 pages, 9 figures", "journal-ref": null, "doi": "10.18420/ecscw2021_ep27", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video annotation and analysis is an important activity for teaching with and\nabout audiovisual media artifacts because it helps students to learn how to\nidentify textual and formal connections in media products. But school teachers\nlack adequate tools for video annotation and analysis in media education that\nare easy-to-use, integrate into established teaching organization, and support\nquick collaborative work. To address these challenges, we followed a\ndesign-based research approach and conducted qualitative interviews with\nteachers to develop TRAVIS GO, a web application for simple and collaborative\nvideo annotation. TRAVIS GO allows for quick and easy use within established\nteaching settings. The web application provides basic analytical features in an\nadaptable work space. Key didactic features include tagging and commenting on\nposts, sharing and exporting projects, and working in live collaboration.\nTeachers can create assignments according to grade level, learning subject, and\nclass size. Our work contributes further insights for the CSCW community about\nhow to implement user demands into developing educational tools.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 21:02:19 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Klug", "Daniel", ""], ["Schlote", "Elke", ""]]}, {"id": "2105.04293", "submitter": "Giovanni Mauro", "authors": "Paolo Cintia, Giovanni Mauro, Luca Pappalardo, Paolo Ferragina", "title": "An interactive dashboard for searching and comparing soccer performance\n  scores", "comments": "4 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of soccer players is one of most discussed aspects by many\nactors in the soccer industry: from supporters to journalists, from coaches to\ntalent scouts. Unfortunately, the dashboards available online provide no\neffective way to compare the evolution of the performance of players or to find\nplayers behaving similarly on the field. This paper describes the design of a\nweb dashboard that interacts via APIs with a performance evaluation algorithm\nand provides graphical tools that allow the user to perform many tasks, such as\nto search or compare players by age, role or trend of growth in their\nperformance, find similar players based on their pitching behavior, change the\nalgorithm's parameters to obtain customized performance scores. We also\ndescribe an example of how a talent scout can interact with the dashboard to\nfind young, promising talents.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:50:26 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 13:39:02 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Cintia", "Paolo", ""], ["Mauro", "Giovanni", ""], ["Pappalardo", "Luca", ""], ["Ferragina", "Paolo", ""]]}, {"id": "2105.04294", "submitter": "Tonatiuh Hern\\'andez-Del-Toro M.Sc.", "authors": "Tonatiuh Hern\\'andez-Del-Toro, Carlos A. Reyes-Garc\\'ia, Luis\n  Villase\\~nor-Pineda", "title": "Toward asynchronous EEG-based BCI: Detecting imagined words segments in\n  continuous EEG signals", "comments": "10 pages, 14 figures", "journal-ref": "Biomedical Signal Processing and Control. Volume 65 (2021), 102351", "doi": "10.1016/j.bspc.2020.102351", "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An asynchronous Brain--Computer Interface (BCI) based on imagined speech is a\ntool that allows to control an external device or to emit a message at the\nmoment the user desires to by decoding EEG signals of imagined speech. In order\nto correctly implement these types of BCI, we must be able to detect from a\ncontinuous signal, when the subject starts to imagine words. In this work, five\nmethods of feature extraction based on wavelet decomposition, empirical mode\ndecomposition, frequency energies, fractal dimension and chaos theory features\nare presented to solve the task of detecting imagined words segments from\ncontinuous EEG signals as a preliminary study for a latter implementation of an\nasynchronous BCI based on imagined speech. These methods are tested in three\ndatasets using four different classifiers and the higher F1 scores obtained are\n0.73, 0.79, and 0.68 for each dataset, respectively. This results are promising\nto build a system that automatizes the segmentation of imagined words segments\nfor latter classification.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 00:13:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hern\u00e1ndez-Del-Toro", "Tonatiuh", ""], ["Reyes-Garc\u00eda", "Carlos A.", ""], ["Villase\u00f1or-Pineda", "Luis", ""]]}, {"id": "2105.04295", "submitter": "Salvatore Vilella", "authors": "Alfonso Semeraro, Salvatore Vilella and Giancarlo Ruffo", "title": "PyPlutchik: visualising and comparing emotion-annotated corpora", "comments": "18 pages, 13 figures. Submitted to IEEE for possible publication;\n  copyright may change", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing availability of textual corpora and data fetched from social\nnetworks is fuelling a huge production of works based on the model proposed by\npsychologist Robert Plutchik, often referred simply as the ``Plutchik Wheel''.\nRelated researches range from annotation tasks description to emotions\ndetection tools. Visualisation of such emotions is traditionally carried out\nusing the most popular layouts, as bar plots or tables, which are however\nsub-optimal. The classic representation of the Plutchik's wheel follows the\nprinciples of proximity and opposition between pairs of emotions: spatial\nproximity in this model is also a semantic proximity, as adjacent emotions\nelicit a complex emotion (a primary dyad) when triggered together; spatial\nopposition is a semantic opposition as well, as positive emotions are opposite\nto negative emotions. The most common layouts fail to preserve both features,\nnot to mention the need of visually allowing comparisons between different\ncorpora in a blink of an eye, that is hard with basic design solutions. We\nintroduce PyPlutchik, a Python library specifically designed for the\nvisualisation of Plutchik's emotions in texts or in corpora. PyPlutchik draws\nthe Plutchik's flower with each emotion petal sized after how much that emotion\nis detected or annotated in the corpus, also representing three degrees of\nintensity for each of them. Notably, PyPlutchik allows users to display also\nprimary, secondary, tertiary and opposite dyads in a compact, intuitive way. We\nsubstantiate our claim that PyPlutchik outperforms other classic visualisations\nwhen displaying Plutchik emotions and we showcase a few examples that display\nour library's most compelling features.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:34:44 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Semeraro", "Alfonso", ""], ["Vilella", "Salvatore", ""], ["Ruffo", "Giancarlo", ""]]}, {"id": "2105.04505", "submitter": "Maximilian Idahl", "authors": "Maximilian Idahl, Lijun Lyu, Ujwal Gadiraju, Avishek Anand", "title": "Towards Benchmarking the Utility of Explanations for Model Debugging", "comments": "Short paper, to appear at TrustNLP @ NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-hoc explanation methods are an important class of approaches that help\nunderstand the rationale underlying a trained model's decision. But how useful\nare they for an end-user towards accomplishing a given task? In this vision\npaper, we argue the need for a benchmark to facilitate evaluations of the\nutility of post-hoc explanation methods. As a first step to this end, we\nenumerate desirable properties that such a benchmark should possess for the\ntask of debugging text classifiers. Additionally, we highlight that such a\nbenchmark facilitates not only assessing the effectiveness of explanations but\nalso their efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:57:33 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Idahl", "Maximilian", ""], ["Lyu", "Lijun", ""], ["Gadiraju", "Ujwal", ""], ["Anand", "Avishek", ""]]}, {"id": "2105.04904", "submitter": "Evropi Stefanidi", "authors": "Evropi Stefanidi, Maria Korozi, Asterios Leonidis, Dimitrios\n  Arampatzis, Margherita Antona, George Papagiannakis", "title": "When Children Program Intelligent Environments: Lessons Learned from a\n  Serious AR Game", "comments": "In Interaction Design and Children 2021", "journal-ref": null, "doi": "10.1145/3459990.3462463", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the body of research focusing on Intelligent Environments (IEs)\nprogramming by adults is steadily growing, informed insights about children as\nprogrammers of such environments are limited. Previous work already established\nthat young children can learn programming basics. Yet, there is still a need to\ninvestigate whether this capability can be transferred in the context of IEs,\nsince encouraging children to participate in the management of their\nintelligent surroundings can enhance responsibility, independence, and the\nspirit of cooperation. We performed a user study (N=15) with children aged\n7-12, using a block-based, gamified AR spatial coding prototype allowing to\nmanipulate smart artifacts in an Intelligent Living room. Our results validated\nthat children understand and can indeed program IEs. Based on our findings, we\ncontribute preliminary implications regarding the use of specific technologies\nand paradigms (e.g. AR, trigger-action programming) to inspire future systems\nthat enable children to create enriching experiences in IEs.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:46:58 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Stefanidi", "Evropi", ""], ["Korozi", "Maria", ""], ["Leonidis", "Asterios", ""], ["Arampatzis", "Dimitrios", ""], ["Antona", "Margherita", ""], ["Papagiannakis", "George", ""]]}, {"id": "2105.04961", "submitter": "Aleksandra Urman", "authors": "Aleksandra Urman and Mykola Makhortykh", "title": "You Are How (and Where) You Search? Comparative Analysis of Web Search\n  Behaviour Using Web Tracking Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We conduct a comparative analysis of desktop web search behaviour of users\nfrom Germany (n=558) and Switzerland (n=563) based on a combination of web\ntracking and survey data. We find that web search accounts for 13% of all\ndesktop browsing, with the share being higher in Switzerland than in Germany.\nWe find that in over 50% of cases users clicked on the first search result,\nwith over 97% of all clicks being made on the first page of search outputs.\nMost users rely on Google when conducting searches, and users preferences for\nother engines are related to their demographics. We also test relationships\nbetween user demographics and daily number of searches, average share of search\nactivities among tracked events by user as well as the tendency to click on\nhigher- or lower-ranked results. We find differences in such relationships\nbetween the two countries that highlights the importance of comparative\nresearch in this domain. Further, we observe differences in the temporal\npatterns of web search use between women and men, marking the necessity of\ndisaggregating data by gender in observational studies regarding online\ninformation behaviour.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 11:58:29 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Urman", "Aleksandra", ""], ["Makhortykh", "Mykola", ""]]}, {"id": "2105.05182", "submitter": "Tianyi Ma", "authors": "Yaohua Bu, Tianyi Ma, Weijun Li, Hang Zhou, Jia Jia, Shengqi Chen,\n  Kaiyuan Xu, Dachuan Shi, Haozhe Wu, Zhihan Yang, Kun Li, Zhiyong Wu, Yuanchun\n  Shi, Xiaobo Lu, Ziwei Liu", "title": "PTeacher: a Computer-Aided Personalized Pronunciation Training System\n  with Exaggerated Audio-Visual Corrective Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Second language (L2) English learners often find it difficult to improve\ntheir pronunciations due to the lack of expressive and personalized corrective\nfeedback. In this paper, we present Pronunciation Teacher (PTeacher), a\nComputer-Aided Pronunciation Training (CAPT) system that provides personalized\nexaggerated audio-visual corrective feedback for mispronunciations. Though the\neffectiveness of exaggerated feedback has been demonstrated, it is still\nunclear how to define the appropriate degrees of exaggeration when interacting\nwith individual learners. To fill in this gap, we interview 100 L2 English\nlearners and 22 professional native teachers to understand their needs and\nexperiences. Three critical metrics are proposed for both learners and teachers\nto identify the best exaggeration levels in both audio and visual modalities.\nAdditionally, we incorporate the personalized dynamic feedback mechanism given\nthe English proficiency of learners. Based on the obtained insights, a\ncomprehensive interactive pronunciation training course is designed to help L2\nlearners rectify mispronunciations in a more perceptible, understandable, and\ndiscriminative manner. Extensive user studies demonstrate that our system\nsignificantly promotes the learners' learning efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:35:09 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 03:16:09 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Bu", "Yaohua", ""], ["Ma", "Tianyi", ""], ["Li", "Weijun", ""], ["Zhou", "Hang", ""], ["Jia", "Jia", ""], ["Chen", "Shengqi", ""], ["Xu", "Kaiyuan", ""], ["Shi", "Dachuan", ""], ["Wu", "Haozhe", ""], ["Yang", "Zhihan", ""], ["Li", "Kun", ""], ["Wu", "Zhiyong", ""], ["Shi", "Yuanchun", ""], ["Lu", "Xiaobo", ""], ["Liu", "Ziwei", ""]]}, {"id": "2105.05306", "submitter": "Mla{\\dj}an Jovanovi\\'c Dr", "authors": "Mladjan Jovanovic, Aleksandar Jevremovic, Milica Pejovic-Milovancevic", "title": "Intelligent interactive technologies for mental health and well-being", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mental healthcare has seen numerous benefits from interactive technologies\nand artificial intelligence. Various interventions have successfully used\nintelligent technologies to automate the assessment and evaluation of\npsychological treatments and mental well-being and functioning. These\ntechnologies include different types of robots, video games, and conversational\nagents. The paper critically analyzes existing solutions with the outlooks for\ntheir future. In particular, we: i)give an overview of the technology for\nmental health, ii) critically analyze the technology against the proposed\ncriteria, and iii) provide the design outlooks for these technologies.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 19:04:21 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Jovanovic", "Mladjan", ""], ["Jevremovic", "Aleksandar", ""], ["Pejovic-Milovancevic", "Milica", ""]]}, {"id": "2105.05322", "submitter": "Kevin Hogan", "authors": "Kevin Hogan, Annabelle Baer and James Purtilo", "title": "Diplomat: A conversational agent framework for goal-oriented group\n  discussion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work in human-computer interaction has explored the use of\nconversational agents as facilitators for group goal-oriented discussions.\nInspired by this work and by the apparent lack of tooling available to support\nit, we created Diplomat, a Python-based framework for building conversational\nagent facilitators. Diplomat is designed to support simple specification of\nagent functionality as well as customizable integration with online chat\nservices. We document a preliminary user study we conducted to help inform the\ndesign of Diplomat. We also describe the architecture, capabilities, and\nlimitations of our tool, which we have shared on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 19:36:18 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Hogan", "Kevin", ""], ["Baer", "Annabelle", ""], ["Purtilo", "James", ""]]}, {"id": "2105.05343", "submitter": "Panagiotis Kourtesis", "authors": "Panagiotis Kourtesis, Ferran Argelaguet, Sebastian Vizcay, Maud\n  Marchal, Claudio Pacchierotti", "title": "Electrotactile feedback for hand interactions:A systematic review,\n  meta-analysis,and future directions", "comments": "15 pages, 1 table, 9 figures, under review in Transactions on\n  Haptics. This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible.Upon acceptance of the article by IEEE, the preprint\n  article will be replaced with the accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Haptic feedback is critical in a broad range of\nhuman-machine/computer-interaction applications. However, the high cost and low\nportability/wearability of haptic devices remains an unresolved issue, severely\nlimiting the adoption of this otherwise promising technology. Electrotactile\ninterfaces have the advantage of being more portable and wearable due to its\nreduced actuators' size, as well as benefiting from lower power consumption and\nmanufacturing cost. The usages of electrotactile feedback have been explored in\nhuman-computer interaction and human-machine-interaction for facilitating\nhand-based interactions in applications such as prosthetics, virtual reality,\nrobotic teleoperation, surface haptics, portable devices, and rehabilitation.\nThis paper presents a systematic review and meta-analysis of electrotactile\nfeedback systems for hand-based interactions in the last decade. We categorize\nthe different electrotactile systems according to their type of stimulation and\nimplementation/application. We also present and discuss a quantitative\ncongregation of the findings, so as to offer a high-level overview into the\nstate-of-art and suggest future directions. Electrotactile feedback was\nsuccessful in rendering and/or augmenting most tactile sensations, eliciting\nperceptual processes, and improving performance in many scenarios, especially\nin those where the wearability/portability of the system is important. However,\nknowledge gaps, technical drawbacks, and methodological limitations were\ndetected, which should be addressed in future studies.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 21:03:20 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["Argelaguet", "Ferran", ""], ["Vizcay", "Sebastian", ""], ["Marchal", "Maud", ""], ["Pacchierotti", "Claudio", ""]]}, {"id": "2105.05375", "submitter": "Wei Xu", "authors": "Wei Xu", "title": "Maximizing the Value of Enterprise Human-Computer Interaction Standards:\n  Strategies and Applications", "comments": "16 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human factors/ergonomics (HFE) standards are not only a useful reference for\nexperienced HFE practitioners but can also provide guidance for organizations\nthat are inexperienced in HFE design practice. HFE standards can give\ncredibility to the value of introducing user centered methods. As computing\ntechnologies advance, knowledge of HFE has spread to the computing related\nwork, and the field of human-computer interaction (HCI) has grown rapidly.\nAccordingly, HCI standards have evolved for guiding practice. There is great\ndeal of literature concerned with the development and practice of international\nstandards (e.g., International Organization for Standardization/ISO) and\nnational (e.g., The American National Standards Institute/ANSI) HFE or HCI\nstandards, but little on the practice of HCI standards at lower levels, such as\nat the level of enterprise. This chapter will focus on the practice of HCI\nstandards there. It intends to assess the challenges of enterprise HCI\nstandards from strategy, development, and governance perspectives.\nSpecifically, we discuss the practices at Intel Corporation, a high-tech\nenterprise environment.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 00:31:35 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Xu", "Wei", ""]]}, {"id": "2105.05392", "submitter": "Philippe Laban", "authors": "Philippe Laban, John Canny, Marti A. Hearst", "title": "What's The Latest? A Question-driven News Chatbot", "comments": "ACL2020 Demo Track, 8 pages, 5 figures", "journal-ref": "ACL Demos (2020) 380-387", "doi": "10.18653/v1/2020.acl-demos.43", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes an automatic news chatbot that draws content from a\ndiverse set of news articles and creates conversations with a user about the\nnews. Key components of the system include the automatic organization of news\narticles into topical chatrooms, integration of automatically generated\nquestions into the conversation, and a novel method for choosing which\nquestions to present which avoids repetitive suggestions. We describe the\nalgorithmic framework and present the results of a usability study that shows\nthat news readers using the system successfully engage in multi-turn\nconversations about specific news stories.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 01:41:20 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Laban", "Philippe", ""], ["Canny", "John", ""], ["Hearst", "Marti A.", ""]]}, {"id": "2105.05424", "submitter": "Wei Xu", "authors": "Wei Xu, Marvin J. Dainoff, Liezhong Ge, Zaifeng Gao", "title": "Transitioning to human interaction with AI systems: New challenges and\n  opportunities for HCI professionals to enable human-centered AI", "comments": "72 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While AI has benefited humans, it may also harm humans if not appropriately\ndeveloped. The focus of HCI work is transiting from conventional human\ninteraction with non-AI computing systems to interaction with AI systems. We\nconducted a high-level literature review and a holistic analysis of current\nwork in developing AI systems from an HCI perspective. Our review and analysis\nhighlight the new changes introduced by AI technology and the new challenges\nthat HCI professionals face when applying the human-centered AI (HCAI) approach\nin the development of AI systems. We also identified seven main issues in human\ninteraction with AI systems, which HCI professionals did not encounter when\ndeveloping non-AI computing systems. To further enable the implementation of\nthe HCAI approach, we identified new HCI opportunities tied to specific\nHCAI-driven design goals to guide HCI professionals in addressing these new\nissues. Finally, our assessment of current HCI methods shows the limitations of\nthese methods in support of developing AI systems. We propose alternative\nmethods that can help overcome these limitations and effectively help HCI\nprofessionals apply the HCAI approach to the development of AI systems. We also\noffer strategic recommendations for HCI professionals to effectively influence\nthe development of AI systems with the HCAI approach, eventually developing\nHCAI systems.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 04:30:45 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 00:28:44 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Xu", "Wei", ""], ["Dainoff", "Marvin J.", ""], ["Ge", "Liezhong", ""], ["Gao", "Zaifeng", ""]]}, {"id": "2105.05492", "submitter": "Steve Benford", "authors": "Steve Benford, Paul Mansfield, Jocelyn Spence", "title": "Producing Liveness: The Trials of Moving Folk Clubs Online During the\n  Global Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global pandemic has driven musicians online. We report an ethnographic\naccount of how two traditional folk clubs with little previous interest in\ndigital platforms transitioned to online experiences. They followed very\ndifferent approaches: one adapted their existing singaround format to video\nconferencing while the other evolved a weekly community-produced, pre-recorded\nshow that could be watched together. However, despite their successes,\nparticipants ultimately remained unable to sing in chorus due to network\nconstraints. We draw on theories of liveness from performance studies to\nexplain our findings, arguing that HCI might orientate itself to online\nliveness as being co-produced through rich participatory structures that\ndissolve traditional distinctions between live and recorded and performer and\naudience. We discuss how participants appropriated existing platforms to\nachieve this, but these in turn shaped their practices in unforeseen ways. We\ndraw out implications for the design and deployment of future live performance\nplatforms.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:01:53 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Benford", "Steve", ""], ["Mansfield", "Paul", ""], ["Spence", "Jocelyn", ""]]}, {"id": "2105.05571", "submitter": "Chen Shani", "authors": "Chen Shani, Alexander Libov, Sofia Tolmach, Liane Lewin-Eytan, Yoelle\n  Maarek, Dafna Shahaf", "title": "\"Alexa, what do you do for fun?\" Characterizing playful requests with\n  virtual assistants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual assistants such as Amazon's Alexa, Apple's Siri, Google Home, and\nMicrosoft's Cortana, are becoming ubiquitous in our daily lives and\nsuccessfully help users in various daily tasks, such as making phone calls or\nplaying music. Yet, they still struggle with playful utterances, which are not\nmeant to be interpreted literally. Examples include jokes or absurd requests or\nquestions such as, \"Are you afraid of the dark?\", \"Who let the dogs out?\", or\n\"Order a zillion gummy bears\". Today, virtual assistants often return\nirrelevant answers to such utterances, except for hard-coded ones addressed by\ncanned replies.\n  To address the challenge of automatically detecting playful utterances, we\nfirst characterize the different types of playful human-virtual assistant\ninteraction. We introduce a taxonomy of playful requests rooted in theories of\nhumor and refined by analyzing real-world traffic from Alexa. We then focus on\none node, personification, where users refer to the virtual assistant as a\nperson (\"What do you do for fun?\"). Our conjecture is that understanding such\nutterances will improve user experience with virtual assistants. We conducted a\nWizard-of-Oz user study and showed that endowing virtual assistant s with the\nability to identify humorous opportunities indeed has the potential to increase\nuser satisfaction. We hope this work will contribute to the understanding of\nthe landscape of the problem and inspire novel ideas and techniques towards the\nvision of giving virtual assistants a sense of humor.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:48:00 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Shani", "Chen", ""], ["Libov", "Alexander", ""], ["Tolmach", "Sofia", ""], ["Lewin-Eytan", "Liane", ""], ["Maarek", "Yoelle", ""], ["Shahaf", "Dafna", ""]]}, {"id": "2105.05729", "submitter": "Mla{\\dj}an Jovanovi\\'c Dr", "authors": "Mladjan Jovanovic, Antonella De Angeli, Andrew McNeill, Lynne Coventry", "title": "User requirements for inclusive technology for older adults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active aging technologies are increasingly designed to support an active\nlifestyle. However, the way in which they are designed can raise different\nbarriers to acceptance of and use by older adults. Their designers can adopt a\nnegative stereotype of aging. Thorough understanding of user requirements is\ncentral to this problem. This paper investigates user requirements for\ntechnologies that encourage an active lifestyle and provide older people with\nthe means to self-manage their physical, mental, and emotional health. This\nrequires consideration of the person and the sociotechnical context of use. We\ndescribe our work in collecting and analyzing older adults' requirements for a\ntechnology which enables an active lifestyle. The main contribution of the\npaper is a model of user requirements for inclusive technology for older\npeople.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:20:35 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Jovanovic", "Mladjan", ""], ["De Angeli", "Antonella", ""], ["McNeill", "Andrew", ""], ["Coventry", "Lynne", ""]]}, {"id": "2105.05849", "submitter": "Sean Kross", "authors": "Sean Kross, Philip J. Guo", "title": "Orienting, Framing, Bridging, Magic, and Counseling: How Data Scientists\n  Navigate the Outer Loop of Client Collaborations in Industry and Academia", "comments": "28 pages, To Appear in the Proceedings of the ACM (PACM)\n  Human-Computer Interaction, CSCW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scientists often collaborate with clients to analyze data to meet a\nclient's needs. What does the end-to-end workflow of a data scientist's\ncollaboration with clients look like throughout the lifetime of a project? To\ninvestigate this question, we interviewed ten data scientists (5 female, 4\nmale, 1 non-binary) in diverse roles across industry and academia. We\ndiscovered that they work with clients in a six-stage outer-loop workflow,\nwhich involves 1) laying groundwork by building trust before a project begins,\n2) orienting to the constraints of the client's environment, 3) collaboratively\nframing the problem, 4) bridging the gap between data science and domain\nexpertise, 5) the inner loop of technical data analysis work, 6) counseling to\nhelp clients emotionally cope with analysis results. This novel outer-loop\nworkflow contributes to CSCW by expanding the notion of what collaboration\nmeans in data science beyond the widely-known inner-loop technical workflow\nstages of acquiring, cleaning, analyzing, modeling, and visualizing data. We\nconclude by discussing the implications of our findings for data science\neducation, parallels to design work, and unmet needs for tool development.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 06:00:28 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 16:06:46 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Kross", "Sean", ""], ["Guo", "Philip J.", ""]]}, {"id": "2105.06269", "submitter": "Janet Rafner", "authors": "Janet Rafner, Arthur Hjorth, Carrie Weidner, Shaeema Zaman Ahmed,\n  Christian Poulsen, Clemens Klokmose, Jacob Sherson", "title": "SciNote: Collaborative Problem Solving and Argumentation Tool", "comments": "International Society of the Learning Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As educators push for students to learn science by doing science, there is a\nneed for computational scaffolding to assist students' evaluation of scientific\nevidence and argument building. In this paper, we present a pilot study of\nSciNote, a CSCL tool allowing educators to integrate third-party software into\na flexible and collaborative workspace. We explore how SciNote enables teams to\nbuild data-driven arguments during inquiry-based learning activities.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:33:11 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Rafner", "Janet", ""], ["Hjorth", "Arthur", ""], ["Weidner", "Carrie", ""], ["Ahmed", "Shaeema Zaman", ""], ["Poulsen", "Christian", ""], ["Klokmose", "Clemens", ""], ["Sherson", "Jacob", ""]]}, {"id": "2105.06320", "submitter": "Aleksandar Jevremovic PhD", "authors": "Aleksandar Jevremovic, Panayiotis Zaphiris, Sasa Adamovic, Mati\n  Mottus, Andri Ioannou", "title": "Applying information theory and entropy to eliminate errors in\n  mouse-tracking results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mouse-tracking of computer system users represents a less expensive, but also\na far more applicable alternative to eye-tracking. The main disadvantage of\nmouse-tracking are errors manifested as discrepancies between the actual\neye-gaze position and the mouse cursor position. This paper presents a method\nfor automated correction of errors arising in mouse-tracking. Our approach\ndraws upon information theory and employs Shannon entropy. The method is based\non calculating the entropy of a visual representation of a Web page, i.e., we\nquantify information potential values of various positions. Information\nobtained, thereby, is paired with cumulative time intervals, spent by the mouse\ncursor in each position. In this way, we identify cursor positions that do not\nmatch eye-gaze positions. To verify the effectiveness of our method, we\ncompared the eye gaze and mouse cursor heat maps in the following ways: We\ncalculated the coefficient of correlation between the two; we computed\nEuclidean distance between their centers of gravity; and we performed visual\ncomparison.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:17:44 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Jevremovic", "Aleksandar", ""], ["Zaphiris", "Panayiotis", ""], ["Adamovic", "Sasa", ""], ["Mottus", "Mati", ""], ["Ioannou", "Andri", ""]]}, {"id": "2105.06354", "submitter": "Sian Gooding", "authors": "Sian Gooding, Yevgeni Berzak, Tony Mak, Matt Sharifi", "title": "Predicting Text Readability from Scrolling Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Judging the readability of text has many important applications, for instance\nwhen performing text simplification or when sourcing reading material for\nlanguage learners. In this paper, we present a 518 participant study which\ninvestigates how scrolling behaviour relates to the readability of a text. We\nmake our dataset publicly available and show that (1) there are statistically\nsignificant differences in the way readers interact with text depending on the\ntext level, (2) such measures can be used to predict the readability of text,\nand (3) the background of a reader impacts their reading interactions and the\nfactors contributing to text difficulty.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 15:27:00 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Gooding", "Sian", ""], ["Berzak", "Yevgeni", ""], ["Mak", "Tony", ""], ["Sharifi", "Matt", ""]]}, {"id": "2105.06598", "submitter": "Vineet Garg", "authors": "Vineet Garg, Wonil Chang, Siddharth Sigtia, Saurabh Adya, Pramod\n  Simha, Pranay Dighe, Chandra Dhir", "title": "Streaming Transformer for Hardware Efficient Voice Trigger Detection and\n  False Trigger Mitigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified and hardware efficient architecture for two stage voice\ntrigger detection (VTD) and false trigger mitigation (FTM) tasks. Two stage VTD\nsystems of voice assistants can get falsely activated to audio segments\nacoustically similar to the trigger phrase of interest. FTM systems cancel such\nactivations by using post trigger audio context. Traditional FTM systems rely\non automatic speech recognition lattices which are computationally expensive to\nobtain on device. We propose a streaming transformer (TF) encoder architecture,\nwhich progressively processes incoming audio chunks and maintains audio context\nto perform both VTD and FTM tasks using only acoustic features. The proposed\njoint model yields an average 18% relative reduction in false reject rate (FRR)\nfor the VTD task at a given false alarm rate. Moreover, our model suppresses\n95% of the false triggers with an additional one second of post-trigger audio.\nFinally, on-device measurements show 32% reduction in runtime memory and 56%\nreduction in inference time compared to non-streaming version of the model.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 00:41:42 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Garg", "Vineet", ""], ["Chang", "Wonil", ""], ["Sigtia", "Siddharth", ""], ["Adya", "Saurabh", ""], ["Simha", "Pramod", ""], ["Dighe", "Pranay", ""], ["Dhir", "Chandra", ""]]}, {"id": "2105.06637", "submitter": "Nan Gao", "authors": "Nan Gao, Max Marschall, Jane Burry, Simon Watkins, Flora D. Salim", "title": "Understanding occupants' behaviour, engagement, emotion, and comfort\n  indoors with heterogeneous sensors and wearables", "comments": "This paper introduces In-Gauge and En-Gage datasets. The link for the\n  datasets:\n  https://rmit.figshare.com/articles/dataset/In-Gauge_and_En-Gage_Datasets/14578908", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We conducted a field study at a K-12 private school in the suburbs of\nMelbourne, Australia. The data capture contained two elements: First, a 5-month\nlongitudinal field study In-Gauge using two outdoor weather stations, as well\nas indoor weather stations in 17 classrooms and temperature sensors on the\nvents of occupant-controlled room air-conditioners; these were collated into\nindividual datasets for each classroom at a 5-minute logging frequency,\nincluding additional data on occupant presence. The dataset was used to derive\npredictive models of how occupants operate room air-conditioning units. Second,\nwe tracked 23 students and 6 teachers in a 4-week cross-sectional study\nEn-Gage, using wearable sensors to log physiological data, as well as daily\nsurveys to query the occupants' thermal comfort, learning engagement, emotions\nand seating behaviours. This is the first publicly available dataset studying\nthe daily behaviours and engagement of high school students using heterogeneous\nmethods. The combined data could be used to analyse the relationships between\nindoor climates and mental states of school students.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 04:17:24 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gao", "Nan", ""], ["Marschall", "Max", ""], ["Burry", "Jane", ""], ["Watkins", "Simon", ""], ["Salim", "Flora D.", ""]]}, {"id": "2105.06677", "submitter": "Sebastian Palacio", "authors": "Sebastian Palacio, Adriano Lucieri, Mohsin Munir, J\\\"orn Hees, Sheraz\n  Ahmed, Andreas Dengel", "title": "XAI Handbook: Towards a Unified Framework for Explainable AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The field of explainable AI (XAI) has quickly become a thriving and prolific\ncommunity. However, a silent, recurrent and acknowledged issue in this area is\nthe lack of consensus regarding its terminology. In particular, each new\ncontribution seems to rely on its own (and often intuitive) version of terms\nlike \"explanation\" and \"interpretation\". Such disarray encumbers the\nconsolidation of advances in the field towards the fulfillment of scientific\nand regulatory demands e.g., when comparing methods or establishing their\ncompliance with respect to biases and fairness constraints. We propose a\ntheoretical framework that not only provides concrete definitions for these\nterms, but it also outlines all steps necessary to produce explanations and\ninterpretations. The framework also allows for existing contributions to be\nre-contextualized such that their scope can be measured, thus making them\ncomparable to other methods. We show that this framework is compliant with\ndesiderata on explanations, on interpretability and on evaluation metrics. We\npresent a use-case showing how the framework can be used to compare LIME, SHAP\nand MDNet, establishing their advantages and shortcomings. Finally, we discuss\nrelevant trends in XAI as well as recommendations for future work, all from the\nstandpoint of our framework.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 07:28:21 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Palacio", "Sebastian", ""], ["Lucieri", "Adriano", ""], ["Munir", "Mohsin", ""], ["Hees", "J\u00f6rn", ""], ["Ahmed", "Sheraz", ""], ["Dengel", "Andreas", ""]]}, {"id": "2105.06730", "submitter": "Alarith Uhde", "authors": "Alarith Uhde and Marc Hassenzahl", "title": "Simulating Social Acceptability With Agent-based Modeling", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social acceptability is an important consideration for HCI designers who\ndevelop technologies for social contexts. However, the current theoretical\nfoundations of social acceptability research do not account for the complex\ninteractions among the actors in social situations and the specific role of\ntechnology. In order to improve the understanding of how context shapes and is\nshaped by situated technology interactions, we suggest to reframe the social\nspace as a dynamic bundle of social practices and explore it with simulation\nstudies using agent-based modeling. We outline possible research directions\nthat focus on specific interactions among practices as well as regularities in\nemerging patterns.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 09:31:43 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Uhde", "Alarith", ""], ["Hassenzahl", "Marc", ""]]}, {"id": "2105.07062", "submitter": "Nicol\\`o Felicioni", "authors": "Nicol\\`o Felicioni, Maurizio Ferrari Dacrema, Paolo Cremonesi", "title": "Measuring the User Satisfaction in a Recommendation Interface with\n  Multiple Carousels", "comments": null, "journal-ref": "ACM International Conference on Interactive Media Experiences (IMX\n  '21), June 21--23, 2021, Virtual Event, NY, USA", "doi": "10.1145/3452918.3465493", "report-no": null, "categories": "cs.IR cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for video-on-demand and music streaming services to adopt a user\ninterface composed of several recommendation lists, i.e. widgets or swipeable\ncarousels, each generated according to a specific criterion or algorithm (e.g.\nmost recent, top popular, recommended for you, editors' choice, etc.).\nSelecting the appropriate combination of carousel has significant impact on\nuser satisfaction. A crucial aspect of this user interface is that to measure\nthe relevance a new carousel for the user it is not sufficient to account\nsolely for its individual quality. Instead, it should be considered that other\ncarousels will already be present in the interface. This is not considered by\ntraditional evaluation protocols for recommenders systems, in which each\ncarousel is evaluated in isolation, regardless of (i) which other carousels are\ndisplayed to the user and (ii) the relative position of the carousel with\nrespect to other carousels. Hence, we propose a two-dimensional evaluation\nprotocol for a carousel setting that will measure the quality of a\nrecommendation carousel based on how much it improves upon the quality of an\nalready available set of carousels. Our evaluation protocol takes into account\nalso the position bias, i.e. users do not explore the carousels sequentially,\nbut rather concentrate on the top-left corner of the screen.\n  We report experiments on the movie domain and notice that under a carousel\nsetting the definition of which criteria has to be preferred to generate a list\nof recommended items changes with respect to what is commonly understood.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 20:33:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Felicioni", "Nicol\u00f2", ""], ["Dacrema", "Maurizio Ferrari", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "2105.07333", "submitter": "Douglas Zytko", "authors": "Douglas Zytko, Nicholas Furlo, Bailey Carlin, Matthew Archer", "title": "Computer-Mediated Consent to Sex: The Context of Tinder", "comments": "Pre-print. To appear in Proceedings of the ACM on Human-Computer\n  Interaction, Vol. 5, CSCW1", "journal-ref": "In Proceedings of the ACM on Human-Computer Interaction, Vol. 5,\n  CSCW1, Article 189, 2021. ACM, New York, NY, USA", "doi": "10.1145/3449288", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports an interview study about how consent to sexual activity is\ncomputer-mediated. The study's context of online dating is chosen due to the\nprevalence of sexual violence, or nonconsensual sexual activity, that is\nassociated with dating app-use. Participants (n=19) represent a range of gender\nidentities and sexual orientations, and predominantly used the dating app\nTinder. Findings reveal two computer-mediated consent processes: consent\nsignaling and affirmative consent. With consent signaling, users employed\nTinder's interface to infer and imply agreement to sex without any explicit\nconfirmation before making sexual advances in-person. With affirmative consent,\nusers employed the interface to establish patterns of overt discourse around\nsex and consent across online and offline modalities. The paper elucidates\nshortcomings of both computer-mediated consent processes that leave users\nsusceptible to sexual violence and envisions dating apps as potential sexual\nviolence prevention solutions if deliberately designed to mediate consent\nexchange.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 02:55:31 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zytko", "Douglas", ""], ["Furlo", "Nicholas", ""], ["Carlin", "Bailey", ""], ["Archer", "Matthew", ""]]}, {"id": "2105.07445", "submitter": "Hyesun Chung", "authors": "Hyesun Chung and Woojin Park", "title": "Enhancing the Usability of Self-service Kiosks for Older Adults: Effects\n  of Using Privacy Partitions and Chairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study aimed to evaluate the effects of possible physical design features\nof self-service kiosks (SSK), side and back partitions and chairs, on workload\nand task performance of older users during a typical SSK task. The study\ncomparatively evaluated eight physical SSK design alternatives, and younger and\nolder participants performed a menu ordering task using each physical design\nalternative. Older participants showed a large variation in task performance\nacross the design alternatives indicating stronger impacts of the physical\ndesign features. In particular, sitting significantly reduced task completion\ntime and workload in multiple dimensions, including time pressure and\nfrustration. In addition, the use of either side or back partitions reduced\nmean ratings of mental demand and effort. The study suggests placing chairs and\neither side or back partitions to enhance older adults' user experience. The\nuse of the proposed physical design recommendations would greatly help them use\nSSK more effectively.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 14:43:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chung", "Hyesun", ""], ["Park", "Woojin", ""]]}, {"id": "2105.07796", "submitter": "David Glowacki", "authors": "David R. Glowacki, Rhoslyn Roebuck Williams, Olivia M. Maynard, James\n  E. Pike, Rachel Freire, Mark D. Wonnacott, Mike Chatziapostolou", "title": "Dissolving yourself in connection to others: shared experiences of ego\n  attenuation and connectedness during group VR experiences can be comparable\n  to psychedelics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With a growing body of research highlighting the therapeutic potential of\nexperiential phenomenology which diminishes egoic identity and increases one's\nsense of connectedness, there is significant interest in how to elicit such\n'self-transcendent experiences' (STEs) in laboratory contexts. Psychedelic\ndrugs (YDs) have proven particularly effective in this respect, producing\nsubjective phenomenology which reliably elicits intense STEs. With virtual\nreality (VR) emerging as a powerful tool for constructing new perceptual\nenvironments, we describe a VR framework called 'Isness-distributed' (Isness-D)\nwhich harnesses the unique affordances of distributed multi-person VR to blur\nconventional self-other boundaries. Within Isness-D, groups of participants\nco-habit a shared virtual space, collectively experiencing their bodies as\nluminous energetic essences with diffuse spatial boundaries. It enables moments\nof 'energetic coalescence', a new class of embodied phenomenological\nintersubjective experience where bodies can fluidly merge, enabling\nparticipants to have an experience of including multiple others within their\nself-representation. To evaluate Isness-D, we adopted a citizen science\napproach, coordinating an international network of Isness-D 'nodes'. We\nanalyzed the results (N = 58) using 4 different self-report scales previously\napplied to analyze subjective YD phenomenology (the inclusion of community in\nself scale, ego-dissolution inventory, communitas scale, and the MEQ30 mystical\nexperience questionnaire). Despite the complexities associated with a\ndistributed experiment like this, the Isness-D scores on all 4 scales were\nstatistically indistinguishable from recently published YD studies,\ndemonstrating that distributed VR can be used to design intersubjective STEs\nwhere people dissolve their sense of self in the connection to others.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:07:32 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Glowacki", "David R.", ""], ["Williams", "Rhoslyn Roebuck", ""], ["Maynard", "Olivia M.", ""], ["Pike", "James E.", ""], ["Freire", "Rachel", ""], ["Wonnacott", "Mark D.", ""], ["Chatziapostolou", "Mike", ""]]}, {"id": "2105.07804", "submitter": "Juliana Ferreira J", "authors": "Juliana Jansen Ferreira and Mateus Monteiro", "title": "Designer-User Communication for XAI: An epistemological approach to\n  discuss XAI design", "comments": "ACM CHI Workshop on Operationalizing Human-Centered Perspectives in\n  Explainable AI at CHI 2021. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial Intelligence is becoming part of any technology we use nowadays.\nIf the AI informs people's decisions, the explanation about AI's outcomes,\nresults, and behavior becomes a necessary capability. However, the discussion\nof XAI features with various stakeholders is not a trivial task. Most of the\navailable frameworks and methods for XAI focus on data scientists and ML\ndevelopers as users. Our research is about XAI for end-users of AI systems. We\nargue that we need to discuss XAI early in the AI-system design process and\nwith all stakeholders. In this work, we aimed at investigating how to\noperationalize the discussion about XAI scenarios and opportunities among\ndesigners and developers of AI and its end-users. We took the Signifying\nMessage as our conceptual tool to structure and discuss XAI scenarios. We\nexperiment with its use for the discussion of a healthcare AI-System.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:18:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Ferreira", "Juliana Jansen", ""], ["Monteiro", "Mateus", ""]]}, {"id": "2105.07854", "submitter": "Filipo Sharevski", "authors": "Filipo Sharevski, Anna Slowinski, Peter Jachim, Emma Pieroni", "title": "\"Hey Alexa, What do You Know About the COVID-19 Vaccine?\" --\n  (Mis)perceptions of Mass Immunization Among Voice Assistant Users", "comments": "arXiv admin note: text overlap with arXiv:2104.04077", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we analyzed the perceived accuracy of COVID-19 vaccine\ninformation spoken back by Amazon Alexa. Unlike social media, Amazon Alexa\ndoesn't apply soft moderation to unverified content, allowing for use of\nthird-party malicious skills to arbitrarily phrase COVID-19 vaccine\ninformation. The results from a 210-participant study suggest that a\nthird-party malicious skill could successful reduce the perceived accuracy\namong the users of information as to who gets the vaccine first, vaccine\ntesting, and the side effects of the vaccine. We also found that the\nvaccine-hesitant participants are drawn to pessimistically rephrased Alexa\nresponses focused on the downsides of the mass immunization. We discuss\nsolutions for soft moderation against misperception-inducing or altogether\nCOVID-19 misinformation malicious third-party skills.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:26:48 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Sharevski", "Filipo", ""], ["Slowinski", "Anna", ""], ["Jachim", "Peter", ""], ["Pieroni", "Emma", ""]]}, {"id": "2105.07917", "submitter": "Giulia Cisotto", "authors": "Alberto Zancanaro, Giulia Cisotto, Jo\\~ao Ruivo Paulo, Gabriel Pires,\n  and Urbano J. Nunes", "title": "CNN-based Approaches For Cross-Subject Classification in Motor Imagery:\n  From The State-of-The-Art to DynamicNet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC cs.NE eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor imagery (MI)-based brain-computer interface (BCI) systems are being\nincreasingly employed to provide alternative means of communication and control\nfor people suffering from neuro-motor impairments, with a special effort to\nbring these systems out of the controlled lab environments. Hence, accurately\nclassifying MI from brain signals, e.g., from electroencephalography (EEG), is\nessential to obtain reliable BCI systems. However, MI classification is still a\nchallenging task, because the signals are characterized by poor SNR, high\nintra-subject and cross-subject variability. Deep learning approaches have\nstarted to emerge as valid alternatives to standard machine learning\ntechniques, e.g., filter bank common spatial pattern (FBCSP), to extract\nsubject-independent features and to increase the cross-subject classification\nperformance of MI BCI systems. In this paper, we first present a review of the\nmost recent studies using deep learning for MI classification, with particular\nattention to their cross-subject performance. Second, we propose DynamicNet, a\nPython-based tool for quick and flexible implementations of deep learning\nmodels based on convolutional neural networks. We show-case the potentiality of\nDynamicNet by implementing EEGNet, a well-established architecture for\neffective EEG classification. Finally, we compare its performance with FBCSP in\na 4-class MI classification over public datasets. To explore its cross-subject\nclassification ability, we applied three different cross-validation schemes.\nFrom our results, we demonstrate that DynamicNet-implemented EEGNet outperforms\nFBCSP by about 25%, with a statistically significant difference when\ncross-subject validation schemes are applied.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 14:57:13 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zancanaro", "Alberto", ""], ["Cisotto", "Giulia", ""], ["Paulo", "Jo\u00e3o Ruivo", ""], ["Pires", "Gabriel", ""], ["Nunes", "Urbano J.", ""]]}, {"id": "2105.08388", "submitter": "Thomas Baier", "authors": "Selene B\\'aez Santamar\\'ia, Thomas Baier, Taewoon Kim, Lea Krause,\n  Jaap Kruijt, Piek Vossen", "title": "EMISSOR: A platform for capturing multimodal interactions as Episodic\n  Memories and Interpretations with Situated Scenario-based Ontological\n  References", "comments": "Accepted to MMSR I workshop at IWCS (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present EMISSOR: a platform to capture multimodal interactions as\nrecordings of episodic experiences with explicit referential interpretations\nthat also yield an episodic Knowledge Graph (eKG). The platform stores streams\nof multiple modalities as parallel signals. Each signal is segmented and\nannotated independently with interpretation. Annotations are eventually mapped\nto explicit identities and relations in the eKG. As we ground signal segments\nfrom different modalities to the same instance representations, we also ground\ndifferent modalities across each other. Unique to our eKG is that it accepts\ndifferent interpretations across modalities, sources and experiences and\nsupports reasoning over conflicting information and uncertainties that may\nresult from multimodal experiences. EMISSOR can record and annotate experiments\nin virtual and real-world, combine data, evaluate system behavior and their\nperformance for preset goals but also model the accumulation of knowledge and\ninterpretations in the Knowledge Graph as a result of these episodic\nexperiences.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:27:14 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Santamar\u00eda", "Selene B\u00e1ez", ""], ["Baier", "Thomas", ""], ["Kim", "Taewoon", ""], ["Krause", "Lea", ""], ["Kruijt", "Jaap", ""], ["Vossen", "Piek", ""]]}, {"id": "2105.08390", "submitter": "Soumyabrata Dev", "authors": "Xingyu Pan, Xuanhui Xu, Soumyabrata Dev, and Abraham G Campbell", "title": "3D Displays: Their Evolution, Inherent Challenges & Future Perspectives", "comments": "Published in Future Technologies Conference (FTC), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of 3D displays has risen drastically over the past few decades\nbut these displays are still merely a novelty compared to their true potential.\nThe development has mostly focused on Head Mounted Displays (HMD) development\nfor Virtual Reality and in general ignored non-HMD 3D displays. This is due to\nthe inherent difficulty in the creation of these displays and their\nimpracticability in general use due to cost, performance, and lack of\nmeaningful use cases. In fairness to the hardware manufacturers who have made\nstriking innovations in this field, there has been a dereliction of duty of\nsoftware developers and researchers in terms of developing software to best\nutilize these displays.\n  This paper will seek to identify what areas of future software development\ncould mitigate this dereliction. To achieve this goal, the paper will first\nexamine the current state of the art and perform a comparative analysis on\ndifferent types of 3D displays, from this analysis a clear researcher gap\nexists in terms of software development for Light field displays which are the\ncurrent state of the art of non-HMD-based 3D displays.\n  The paper will then outline six distinct areas where the context-awareness\nconcept will allow for non-HMD-based 3D displays in particular light field\ndisplays that can not only compete but surpass their HMD-based brethren for\nmany specific use cases.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:34:25 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Pan", "Xingyu", ""], ["Xu", "Xuanhui", ""], ["Dev", "Soumyabrata", ""], ["Campbell", "Abraham G", ""]]}, {"id": "2105.08466", "submitter": "Liao Wu", "authors": "Liao Wu, Fangwen Yu, Jiaole Wang, Thanh Nho Do", "title": "Camera Frame Misalignment in a Teleoperated Eye-in-Hand Robot: Effects\n  and a Simple Correction Method", "comments": "Submitted to IEEE Transactions on Human-Machine Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misalignment between the camera frame and the operator frame is commonly seen\nin a teleoperated system and usually degrades the operation performance. The\neffects of such misalignment have not been fully investigated for eye-in-hand\nsystems - systems that have the camera (eye) mounted to the end-effector (hand)\nto gain compactness in confined spaces such as in endoscopic surgery. This\npaper provides a systematic study on the effects of the camera frame\nmisalignment in a teleoperated eye-in-hand robot and proposes a simple\ncorrection method in the view display. A simulation is designed to compare the\neffects of the misalignment under different conditions. Users are asked to move\na rigid body from its initial position to the specified target position via\nteleoperation, with different levels of misalignment simulated. It is found\nthat misalignment between the input motion and the output view is much more\ndifficult to compensate by the operators when it is in the orthogonal direction\n(~40s) compared with the opposite direction (~20s). An experiment on a real\nconcentric tube robot with an eye-in-hand configuration is also conducted.\nUsers are asked to telemanipulate the robot to complete a pick-and-place task.\nResults show that with the correction enabled, there is a significant\nimprovement in the operation performance in terms of completion time (mean\n40.6%, median 38.6%), trajectory length (mean 34.3%, median 28.1%), difficulty\n(50.5%), unsteadiness (49.4%), and mental stress (60.9%).\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:18:35 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Wu", "Liao", ""], ["Yu", "Fangwen", ""], ["Wang", "Jiaole", ""], ["Do", "Thanh Nho", ""]]}, {"id": "2105.08631", "submitter": "Huili Chen", "authors": "Huili Chen, Cynthia Breazeal", "title": "Toward Designing Social Human-Robot Interactions for Deep Space\n  Exploration", "comments": "spaceCHI workshop at CHI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In planning for future human space exploration, it is important to consider\nhow to design for uplifting interpersonal communications and social dynamics\namong crew members. What if embodied social robots could help to improve the\noverall team interaction experience in space? On Earth, social robots have been\nshown effective in providing companionship, relieving stress and anxiety,\nfostering connection among people, enhancing team performance, and mediating\nconflicts in human groups. In this paper, we introduce a set of novel research\nquestions exploring social human-robot interactions in long-duration space\nexploration missions.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:01:25 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Chen", "Huili", ""], ["Breazeal", "Cynthia", ""]]}, {"id": "2105.08667", "submitter": "Uthaipon Tantipongpipat", "authors": "Kyra Yee, Uthaipon Tantipongpipat, Shubhanshu Mishra", "title": "Image Cropping on Twitter: Fairness Metrics, their Limitations, and the\n  Importance of Representation, Design, and Agency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twitter uses machine learning to crop images, where crops are centered around\nthe part predicted to be the most salient. In fall 2020, Twitter users raised\nconcerns that the automated image cropping system on Twitter favored\nlight-skinned over dark-skinned individuals, as well as concerns that the\nsystem favored cropping woman's bodies instead of their heads. In order to\naddress these concerns, we conduct an extensive analysis using formalized group\nfairness metrics. We find systematic disparities in cropping and identify\ncontributing factors, including the fact that the cropping based on the single\nmost salient point can amplify the disparities. However, we demonstrate that\nformalized fairness metrics and quantitative analysis on their own are\ninsufficient for capturing the risk of representational harm in automatic\ncropping. We suggest the removal of saliency-based cropping in favor of a\nsolution that better preserves user agency. For developing a new solution that\nsufficiently address concerns related to representational harm, our critique\nmotivates a combination of quantitative and qualitative methods that include\nhuman-centered design.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 16:50:50 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yee", "Kyra", ""], ["Tantipongpipat", "Uthaipon", ""], ["Mishra", "Shubhanshu", ""]]}, {"id": "2105.08827", "submitter": "Shruti Phadke", "authors": "Shruti Phadke, Tanushree Mitra", "title": "Educators, Solicitors, Flamers, Motivators, Sympathizers: Characterizing\n  Roles in Online Extremist Movements", "comments": "Accepted at Computer Supported Cooperative Work (CSCW 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Social media provides the means by which extremist social movements, such as\nwhite supremacy and anti LGBTQ, thrive online. Yet, we know little about the\nroles played by the participants of such movements. In this paper, we\ninvestigate these participants to characterize their roles, their role\ndynamics, and their influence in spreading online extremism. Our participants,\nonline extremist accounts, are 4,876 public Facebook pages or groups that have\nshared information from the websites of 289 Southern Poverty Law Center\ndesignated extremist groups. By clustering the quantitative features followed\nby qualitative expert validation, we identify five roles surrounding extremist\nactivism: educators, solicitors, flamers, motivators, sympathizers. For\nexample, solicitors use links from extremist websites to attract donations and\nparticipation in extremist issues, whereas flamers share inflammatory extremist\ncontent inciting anger. We further investigate role dynamics such as, how\nstable these roles are over time and how likely will extremist accounts\ntransition from one role into another. We find that roles core to the movement,\neducators and solicitors, are more stable, while flamers and motivators can\ntransition to sympathizers with high probability. We further find that\neducators and solicitors exert the most influence in triggering extremist link\nposts, whereas flamers are influential in triggering the spread of information\nfrom fake news sources. Our results help in situating various roles on the\ntrajectory of deeper engagement into the extremist movements and understanding\nthe potential effect of various counter extremism interventions. Our findings\nhave implications for understanding how online extremist movements flourish\nthrough participatory activism and how they gain a spectrum of allies for\nmobilizing extremism online.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 20:55:11 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Phadke", "Shruti", ""], ["Mitra", "Tanushree", ""]]}, {"id": "2105.08847", "submitter": "Michael Madaio", "authors": "Michael Madaio, Su Lin Blodgett, Elijah Mayfield, Ezekiel\n  Dixon-Rom\\'an", "title": "Confronting Structural Inequities in AI for Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Educational technologies, and the systems of schooling in which they are\ndeployed, enact particular ideologies about what is important to know and how\nlearners should learn. As artificial intelligence technologies -- in education\nand beyond -- have led to inequitable outcomes for marginalized communities,\nvarious approaches have been developed to evaluate and mitigate AI systems'\ndisparate impact. However, we argue in this paper that the dominant paradigm of\nevaluating fairness on the basis of performance disparities in AI models is\ninadequate for confronting the structural inequities that educational AI\nsystems (re)produce. We draw on a lens of structural injustice informed by\ncritical theory and Black feminist scholarship to critically interrogate\nseveral widely-studied and widely-adopted categories of educational AI systems\nand demonstrate how educational AI technologies are bound up in and reproduce\nhistorical legacies of structural injustice and inequity, regardless of the\nparity of their models' performance. We close with alternative visions for a\nmore equitable future for educational AI research.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 22:13:35 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Madaio", "Michael", ""], ["Blodgett", "Su Lin", ""], ["Mayfield", "Elijah", ""], ["Dixon-Rom\u00e1n", "Ezekiel", ""]]}, {"id": "2105.08870", "submitter": "Daniel Capurro", "authors": "Daniel Capurro and Eduardo Velloso", "title": "Dark Patterns, Electronic Medical Records, and the Opioid Epidemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dark patterns have emerged as a set of methods to exploit cognitive biases to\ntrick users to make decisions that are more aligned with a third party than to\ntheir own. These patterns can have consequences that might range from\ninconvenience to global disasters. We present a case of a drug company and an\nelectronic medical record vendor who colluded to modify the medical record's\ninterface to induce clinicians to increase the prescription of extended-release\nopioids, a class of drugs that has a high potential for addiction and has\ncaused almost half a million additional deaths in the past two decades. Through\nthis case, we present the use and effects of dark patterns in healthcare,\ndiscuss the current challenges, and offer some recommendations on how to\naddress this pressing issue.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:07:35 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Capurro", "Daniel", ""], ["Velloso", "Eduardo", ""]]}, {"id": "2105.08907", "submitter": "Chrisogonas Odhiambo Mr.", "authors": "Chrisogonas Odhiambo (1 and 3), Pamela Wright (2 and 3), Cindy Corbett\n  (2 and 3), Homayoun Valafar (1 and 3) ((1) Computer Science and Engineering\n  Department, (2) College of Nursing, (3) University of South Carolina)", "title": "MedSensor: Medication Adherence Monitoring Using Neural Networks on\n  Smartwatch Accelerometer Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Poor medication adherence presents serious economic and health problems\nincluding compromised treatment effectiveness, medical complications, and loss\nof billions of dollars in wasted medicine or procedures. Though various\ninterventions have been proposed to address this problem, there is an urgent\nneed to leverage light, smart, and minimally obtrusive technology such as\nsmartwatches to develop user tools to improve medication use and adherence. In\nthis study, we conducted several experiments on medication-taking activities,\ndeveloped a smartwatch android application to collect the accelerometer hand\ngesture data from the smartwatch, and conveyed the data collected to a central\ncloud database. We developed neural networks, then trained the networks on the\nsensor data to recognize medication and non-medication gestures. With the\nproposed machine learning algorithm approach, this study was able to achieve\naverage accuracy scores of 97% on the protocol-guided gesture data, and 95% on\nnatural gesture data.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 03:42:30 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Odhiambo", "Chrisogonas", "", "1 and 3"], ["Wright", "Pamela", "", "2 and 3"], ["Corbett", "Cindy", "", "2 and 3"], ["Valafar", "Homayoun", "", "1 and 3"]]}, {"id": "2105.08915", "submitter": "Waqas Ahmed", "authors": "Waqas Ahmed, Habiba Akter, Sheikh M. Hizam, Ilham Sentosa and Syeliya\n  Md. Zaini", "title": "Assessing the Learning Behavioral Intention of Commuters in Mobility\n  Practices", "comments": "09 pages, 02 figures, 05 table, Proceedings of the First Workshop on\n  Technology Enhanced Learning Environments for Blended Education\n  (teleXbe2021), January 2122, 2021, Foggia, Italy", "journal-ref": "2021 CEUR Workshop Proceedings, Vol-2817", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning behavior mechanism is widely anticipated in managed settings through\nthe formal syllabus. However, heading for learning stimulus whilst daily\nmobility practices through urban transit is the novel feature in learning\nsciences. Theory of planned behavior (TPB), technology acceptance model (TAM),\nand service quality of transit are conceptualized to assess the learning\nbehavioral intention (LBI) of commuters in Greater Kuala Lumpur. An online\nsurvey was conducted to understand the LBI of 117 travelers who use the\ntechnology to engage in the informal learning process during daily commuting.\nThe results explored that all the model variables i.e., perceived ease of use,\nperceived usefulness, service quality, and subjective norms are significant\npredictors of LBI. The perceived usefulness of learning during traveling and\ntransit service quality has a vibrant impact on LBI. The research will support\nthe informal learning mechanism from commuters point of view. The study is a\nnovel contribution to transport and learning literature that will open the new\nprospect of research in urban mobility and its connotation with personal\nlearning and development.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 04:27:11 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Ahmed", "Waqas", ""], ["Akter", "Habiba", ""], ["Hizam", "Sheikh M.", ""], ["Sentosa", "Ilham", ""], ["Zaini", "Syeliya Md.", ""]]}, {"id": "2105.08927", "submitter": "Waqas Ahmed", "authors": "S. M. Hizam, H. Akter, I. Sentosa, W. Ahmed", "title": "Digital competency of educators in the virtual learning environment: a\n  structural equation modeling analysis", "comments": "10 Pages, 02 Figures, 04 Tables, 2nd ICCETIM (International\n  Conference on Creative Economics, Tourism & Information Management) 2020,\n  Yogyakarta, Indonesia", "journal-ref": "IOP Conf. Series: Earth and Environmental Science 704 (2021)\n  012023", "doi": "10.1088/1755-1315/704/1/012023", "report-no": null, "categories": "cs.HC cs.CY stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study integrates the educators digital competency (DC), as an individual\ncharacteristic construct of the task-technology fit (TTF) theory, to examine a\nbetter fit between Moodle using and teaching task, and to investigate its\neffect on both Moodles utilization and their task performance. For assessing\nour proposed hypotheses, an online survey was conducted with 238 teaching staff\nfrom different departments of universities in Malaysia. Using Structural\nEquation Modelling (SEM), our analysis revealed that all the proposed\ncomponents (i.e., technology literacy, knowledge deepening, presentation\nskills, and professional skills) of digital competency significantly influenced\nthe TTF. The Task-Technology Fit was also found as an influential construct,\nwhich positively and significantly affected both Moodles utilization and\nteachers task performance. Besides, Moodles utilization was confirmed to be a\nsubstantial determinant of the performance impact. In the end, this study\nincluded limitations and future directions based on how the study's\ncontribution can support academics and practitioners for assessing and\nunderstanding what particular components of digital competency impact TTF,\nwhich in turn may influence the systems utilization and performance impact.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 05:05:09 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Hizam", "S. M.", ""], ["Akter", "H.", ""], ["Sentosa", "I.", ""], ["Ahmed", "W.", ""]]}, {"id": "2105.08929", "submitter": "David Eccles Mr", "authors": "David A. Eccles, Tilman Dingler", "title": "Three prophylactic interventions to counter fake news on social media", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fake news on Social Media undermines democratic institutions and processes.\nEspecially since 2016, researchers from many disciplines have focussed on ways\nto address the phenomenon. Much of the research focus to date has been on\nidentification and understanding the nature of the phenomenon in and between\nsocial networks and of a rather reactive nature. We propose interventions that\nfocus on individual user empowerment, and social media structural change that\nis prophylactic (pre exposure), rather than therapeutic (post exposure) with\nthe goal of reducing the population exposed to fake news. We investigate\ninterventions that result in greater user elaboration (cognitive effort) before\nexposure to fake news. We propose three interventions i) psychological\ninoculation, ii) fostering digital and media literacy and iii) imposition of\nuser transaction costs. Each intervention promises to illicit greater cognitive\neffort in message evaluation and reduce the likelihood of creating, sharing,\nliking and consuming 'fake news'.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 05:24:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Eccles", "David A.", ""], ["Dingler", "Tilman", ""]]}, {"id": "2105.09153", "submitter": "Cheryl Tollola", "authors": "C. Tollola", "title": "Procedural animations in interactive art experiences -- A state of the\n  art review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state of the art review broadly oversees the use of novel research\nutilized in the creation of virtual environments applied in interactive art\nexperiences, with a specific focus on the application of procedural animation\nin spatially augmented reality (SAR) exhibitions. These art exhibitions\nfrequently combine sensory displays that appeal, replace, and augment the\nvisual, auditory and touch or haptic senses. We analyze and break down\nart-technology related innovations in the last three years, and thoroughly\nidentify the most recent and vibrant applications of interactive art\nexperiences in the review of numerous installation applications, studies, and\nevents. Display mediums such as virtual reality, augmented reality, mixed\nreality, and robotics are overviewed in the context of art experiences such as\nvisual art museums, park or historic site tours, live concerts, and theatre. We\nexplore research and extrapolate how recent innovations can lead to different\napplications that will be seen in the future.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 05:14:56 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Tollola", "C.", ""]]}, {"id": "2105.09207", "submitter": "Hiromu Yakura", "authors": "Hiromu Yakura, Yuki Koyama, Masataka Goto", "title": "Tool- and Domain-Agnostic Parameterization of Style Transfer Effects\n  Leveraging Pretrained Perceptual Metrics", "comments": "To appear in Proceedings of the 30th International Joint Conference\n  on Artificial Intelligence (IJCAI 2021); Project page available at\n  https://yumetaro.info/projects/parametric-transcription/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning techniques for style transfer would not be optimal for\ndesign support since their \"one-shot\" transfer does not fit exploratory design\nprocesses. To overcome this gap, we propose parametric transcription, which\ntranscribes an end-to-end style transfer effect into parameter values of\nspecific transformations available in an existing content editing tool. With\nthis approach, users can imitate the style of a reference sample in the tool\nthat they are familiar with and thus can easily continue further exploration by\nmanipulating the parameters. To enable this, we introduce a framework that\nutilizes an existing pretrained model for style transfer to calculate a\nperceptual style distance to the reference sample and uses black-box\noptimization to find the parameters that minimize this distance. Our\nexperiments with various third-party tools, such as Instagram and Blender, show\nthat our framework can effectively leverage deep learning techniques for\ncomputational design support.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 15:39:10 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Yakura", "Hiromu", ""], ["Koyama", "Yuki", ""], ["Goto", "Masataka", ""]]}, {"id": "2105.09222", "submitter": "Siddharth Mehrotra", "authors": "Siddharth Mehrotra, Catholijn M. Jonker, Myrthe L. Tielman", "title": "More Similar Values, More Trust? -- the Effect of Value Similarity on\n  Trust in Human-Agent Interaction", "comments": "4th AAAI/ACM Conference on AI, Ethics, and Society", "journal-ref": "S Mehrotra, C. M. Jonker, and M. L. Tielman. More Similar Values,\n  More Trust? - the Effect of Value Similarity on Trust in Human-Agent\n  Interaction in Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES 21)", "doi": "10.1145/3461702.3462576", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As AI systems are increasingly involved in decision making, it also becomes\nimportant that they elicit appropriate levels of trust from their users. To\nachieve this, it is first important to understand which factors influence trust\nin AI. We identify that a research gap exists regarding the role of personal\nvalues in trust in AI. Therefore, this paper studies how human and agent Value\nSimilarity (VS) influences a human's trust in that agent. To explore this, 89\nparticipants teamed up with five different agents, which were designed with\nvarying levels of value similarity to that of the participants. In a\nwithin-subjects, scenario-based experiment, agents gave suggestions on what to\ndo when entering the building to save a hostage. We analyzed the agent's scores\non subjective value similarity, trust and qualitative data from open-ended\nquestions. Our results show that agents rated as having more similar values\nalso scored higher on trust, indicating a positive effect between the two. With\nthis result, we add to the existing understanding of human-agent trust by\nproviding insight into the role of value-similarity.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:06:46 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Mehrotra", "Siddharth", ""], ["Jonker", "Catholijn M.", ""], ["Tielman", "Myrthe L.", ""]]}, {"id": "2105.09275", "submitter": "Adrien Bibal", "authors": "Cristina Morariu, Adrien Bibal, Rene Cutura, Beno\\^it Fr\\'enay and\n  Michael Sedlmair", "title": "DumbleDR: Predicting User Preferences of Dimensionality Reduction\n  Projection Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A plethora of dimensionality reduction techniques have emerged over the past\ndecades, leaving researchers and analysts with a wide variety of choices for\nreducing their data, all the more so given some techniques come with additional\nparametrization (e.g. t-SNE, UMAP, etc.). Recent studies are showing that\npeople often use dimensionality reduction as a black-box regardless of the\nspecific properties the method itself preserves. Hence, evaluating and\ncomparing 2D projections is usually qualitatively decided, by setting\nprojections side-by-side and letting human judgment decide which projection is\nthe best. In this work, we propose a quantitative way of evaluating\nprojections, that nonetheless places human perception at the center. We run a\ncomparative study, where we ask people to select 'good' and 'misleading' views\nbetween scatterplots of low-level projections of image datasets, simulating the\nway people usually select projections. We use the study data as labels for a\nset of quality metrics whose purpose is to discover and quantify what exactly\npeople are looking for when deciding between projections. With this proxy for\nhuman judgments, we use it to rank projections on new datasets, explain why\nthey are relevant, and quantify the degree of subjectivity in projections\nselected.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:39:20 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Morariu", "Cristina", ""], ["Bibal", "Adrien", ""], ["Cutura", "Rene", ""], ["Fr\u00e9nay", "Beno\u00eet", ""], ["Sedlmair", "Michael", ""]]}, {"id": "2105.09296", "submitter": "Amifa Raj", "authors": "Amifa Raj, Ashlee Milton, Michael D. Ekstrand", "title": "Pink for Princesses, Blue for Superheroes: The Need to Examine Gender\n  Stereotypes in Kid's Products in Search and Recommendations", "comments": "KidRec '21: 5th International and Interdisciplinary Perspectives on\n  Children \\& Recommender and Information Retrieval Systems (KidRec) Search and\n  Recommendation Technology through the Lens of a Teacher- Co-located with ACM\n  IDC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this position paper, we argue for the need to investigate if and how\ngender stereotypes manifest in search and recommender systems.As a starting\npoint, we particularly focus on how these systems may propagate and reinforce\ngender stereotypes through their results in learning environments, a context\nwhere teachers and children in their formative stage regularly interact with\nthese systems. We provide motivating examples supporting our concerns and\noutline an agenda to support future research addressing the phenomena.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 21:37:23 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Raj", "Amifa", ""], ["Milton", "Ashlee", ""], ["Ekstrand", "Michael D.", ""]]}, {"id": "2105.09457", "submitter": "Danula Hettiachchi", "authors": "Danula Hettiachchi, Mike Schaekermann, Tristan McKinney and Matthew\n  Lease", "title": "The Challenge of Variable Effort Crowdsourcing and How Visible Gold Can\n  Help", "comments": "25 pages, To appear in the Proceedings of the ACM on Human-Computer\n  Interaction, CSCW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of variable effort human annotation tasks in which the\nnumber of labels required per item can greatly vary (e.g., finding all faces in\nan image, named entities in a text, bird calls in an audio recording, etc.). In\nsuch tasks, some items require far more effort than others to annotate.\nFurthermore, the per-item annotation effort is not known until after each item\nis annotated since determining the number of labels required is an implicit\npart of the annotation task itself. On an image bounding-box task with\ncrowdsourced annotators, we show that annotator accuracy and recall\nconsistently drop as effort increases. We hypothesize reasons for this drop and\ninvestigate a set of approaches to counteract it. Firstly, we benchmark on this\ntask a set of general best-practice methods for quality crowdsourcing. Notably,\nonly one of these methods actually improves quality: the use of visible gold\nquestions that provide periodic feedback to workers on their accuracy as they\nwork. Given these promising results, we then investigate and evaluate variants\nof the visible gold approach, yielding further improvement. Final results show\na 7% improvement in bounding-box accuracy over the baseline. We discuss the\ngenerality of the visible gold approach and promising directions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 02:21:27 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hettiachchi", "Danula", ""], ["Schaekermann", "Mike", ""], ["McKinney", "Tristan", ""], ["Lease", "Matthew", ""]]}, {"id": "2105.09490", "submitter": "Harry Nguyen", "authors": "Thuy-Trinh Nguyen, Kellie Sim, Anthony To Yiu Kuen, Ronald R.\n  O'donnell, Suan Tee Lim, Wenru Wang and Hoang D. Nguyen", "title": "Designing AI-based Conversational Agent for Diabetes Care in a\n  Multilingual Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents (CAs) represent an emerging research field in health\ninformation systems, where there are great potentials in empowering patients\nwith timely information and natural language interfaces. Nevertheless, there\nhave been limited attempts in establishing prescriptive knowledge on designing\nCAs in the healthcare domain in general, and diabetes care specifically. In\nthis paper, we conducted a Design Science Research project and proposed three\ndesign principles for designing health-related CAs that embark on artificial\nintelligence (AI) to address the limitations of existing solutions. Further, we\ninstantiated the proposed design and developed AMANDA - an AI-based\nmultilingual CA in diabetes care with state-of-the-art technologies for\nnatural-sounding localised accent. We employed mean opinion scores and system\nusability scale to evaluate AMANDA's speech quality and usability,\nrespectively. This paper provides practitioners with a blueprint for designing\nCAs in diabetes care with concrete design guidelines that can be extended into\nother healthcare domains.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:20:24 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Nguyen", "Thuy-Trinh", ""], ["Sim", "Kellie", ""], ["Kuen", "Anthony To Yiu", ""], ["O'donnell", "Ronald R.", ""], ["Lim", "Suan Tee", ""], ["Wang", "Wenru", ""], ["Nguyen", "Hoang D.", ""]]}, {"id": "2105.09496", "submitter": "Umar Yahya PhD", "authors": "Cherinor Umaru Bah, Afzaal Hussain Seyal, and Umar Yahya", "title": "Combining PIN and Biometric Identifications as Enhancement to User\n  Authentication in Internet Banking", "comments": "7th Brunei International Conference on Engineering and Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Internet banking (IB) continues to face security concerns arising from\nillegal access to users accounts. Use of personal identification numbers (PIN)\nas a single authentication method for IB users is prone to insecurities such as\nphishing, hacking and shoulder surfing. Fingerprint matching (FPM) as an\nalternative to PIN equally has a downside as fingerprints reside on individual\nmobile devices. A survey we conducted from 170 IB respondents of 5 different\nbanks in Brunei established that majority (65%) of them preferred use of\nbiometric authentication methods. In this work, we propose a two-level\nintegrated authentication mechanism (2L-IAM). At the first level, the user logs\nin to their IB portal using either PIN or FPM. At the second level, user is\nauthenticated by means of face recognition (FR) should they initiate a\ntransaction classified as sensitive. The merits of the introduced 2L-IAM are\n3-fold: - (1) FR guarantees the identity of the rightful user irrespective of\nthe login device; (2) By classifying banking products sensitivity, the\nsensitive transactions are more effectively secured; (3) It is accommodative of\ndifferent users authentication preferences. Adoption of this framework could\nthus improve both users and banks experiences in terms of enhanced security and\nservice delivery respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:43:18 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Bah", "Cherinor Umaru", ""], ["Seyal", "Afzaal Hussain", ""], ["Yahya", "Umar", ""]]}, {"id": "2105.09787", "submitter": "Devleena Das", "authors": "Devleena Das, Yasutaka Nishimura, Rajan P. Vivek, Naoto Takeda, Sean\n  T. Fish, Thomas Ploetz, Sonia Chernova", "title": "Explainable Activity Recognition for Smart Home Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart home environments are designed to provide services that help improve\nthe quality of life for the occupant via a variety of sensors and actuators\ninstalled throughout the space. Many automated actions taken by a smart home\nare governed by the output of an underlying activity recognition system.\nHowever, activity recognition systems may not be perfectly accurate and\ntherefore inconsistencies in smart home operations can lead a user to wonder\n\"why did the smart home do that?\" In this work, we build on insights from\nExplainable Artificial Intelligence (XAI) techniques to contribute\ncomputational methods for explainable activity recognition. Specifically, we\ngenerate explanations for smart home activity recognition systems that explain\nwhat about an activity led to the given classification. To do so, we introduce\nfour computational techniques for generating natural language explanations of\nsmart home data and compare their effectiveness at generating meaningful\nexplanations. Through a study with everyday users, we evaluate user preferences\ntowards the four explanation types. Our results show that the leading approach,\nSHAP, has a 92% success rate in generating accurate explanations. Moreover, 84%\nof sampled scenarios users preferred natural language explanations over a\nsimple activity label, underscoring the need for explainable activity\nrecognition systems. Finally, we show that explanations generated by some XAI\nmethods can lead users to lose confidence in the accuracy of the underlying\nactivity recognition model, while others lead users to gain confidence. Taking\nall studied factors into consideration, we make a recommendation regarding\nwhich existing XAI method leads to the best performance in the domain of smart\nhome automation, and discuss a range of topics for future work in this area.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:35:51 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Das", "Devleena", ""], ["Nishimura", "Yasutaka", ""], ["Vivek", "Rajan P.", ""], ["Takeda", "Naoto", ""], ["Fish", "Sean T.", ""], ["Ploetz", "Thomas", ""], ["Chernova", "Sonia", ""]]}, {"id": "2105.09807", "submitter": "Juan M Gandarias", "authors": "Juan M. Gandarias, Pietro Balatti, Edoardo Lamon, Marta Lorenzini,\n  Arash Ajoudani (Human-Robot Interfaces and Physical Interaction, Istituto\n  Italiano di Tecnologia, Genoa, Italy)", "title": "A User-Centered Interface for Enhanced Conjoined Human-Robot Actions in\n  Industrial Tasks", "comments": "6 pages, 5 figures, submitted to IEEE International Conference on\n  Robot & Human Interactive Communication, for associated video, see\n  https://youtu.be/WgnJihYHjYo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a user-centered physical interface for collaborative\nmobile manipulators in industrial manufacturing and logistics applications. The\nproposed work builds on our earlier MOCA-MAN interface, through which a mobile\nmanipulator could be physically coupled to the operators to assist them in\nperforming daily activities. The new interface instead presents the following\nadditions: i) A simplistic, industrial-like design that allows the worker to\ncouple/decouple easily and to operate mobile manipulators locally; ii) Enhanced\nloco-manipulation capabilities that do not compromise the worker mobility.\nBesides, an experimental evaluation with six human subjects is carried out to\nanalyze the enhanced locomotion and flexibility of the proposed interface in\nterms of mobility constraint, usability, and physical load reduction.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:02:25 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gandarias", "Juan M.", "", "Human-Robot Interfaces and Physical Interaction, Istituto\n  Italiano di Tecnologia, Genoa, Italy"], ["Balatti", "Pietro", "", "Human-Robot Interfaces and Physical Interaction, Istituto\n  Italiano di Tecnologia, Genoa, Italy"], ["Lamon", "Edoardo", "", "Human-Robot Interfaces and Physical Interaction, Istituto\n  Italiano di Tecnologia, Genoa, Italy"], ["Lorenzini", "Marta", "", "Human-Robot Interfaces and Physical Interaction, Istituto\n  Italiano di Tecnologia, Genoa, Italy"], ["Ajoudani", "Arash", "", "Human-Robot Interfaces and Physical Interaction, Istituto\n  Italiano di Tecnologia, Genoa, Italy"]]}, {"id": "2105.09809", "submitter": "Soheil Gholami", "authors": "Soheil Gholami (1 and 2), Marta Lorenzini (1), Elena De Momi (2), and\n  Arash Ajoudani (1) ((1) Human-Robot Interfaces and physical Interaction,\n  Istituto Italiano di Tecnologia, Genoa, Italy, (2) NearLab, Deptartment of\n  Electronics, Information and Bioengineering, Politecnico di Milano, Milan,\n  Italy)", "title": "Quantitative Physical Ergonomics Assessment of Teleoperation Interfaces", "comments": "10 pages, 9 figures, submitted to IEEE Transactions on Human-Machine\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human factors and ergonomics are the essential constituents of teleoperation\ninterfaces, which can significantly affect the human operator's performance.\nThus, a quantitative evaluation of these elements and the ability to establish\nreliable comparison bases for different teleoperation interfaces are the keys\nto select the most suitable one for a particular application. However, most of\nthe works on teleoperation have so far focused on the stability analysis and\nthe transparency improvement of these systems, and do not cover the important\nusability aspects. In this work, we propose a foundation to build a general\nframework for the analysis of human factors and ergonomics in employing diverse\nteleoperation interfaces. The proposed framework will go beyond the traditional\nsubjective analyses of usability by complementing it with online measurements\nof the human body configurations. As a result, multiple quantitative metrics\nsuch as joints' usage, range of motion comfort, center of mass divergence, and\nposture comfort are introduced. To demonstrate the potential of the proposed\nframework, two different teleoperation interfaces are considered, and\nreal-world experiments with eleven participants performing a simulated\nindustrial remote pick-and-place task are conducted. The quantitative results\nof this analysis are provided, and compared with subjective questionnaires,\nillustrating the effectiveness of the proposed framework.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:03:23 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gholami", "Soheil", "", "1 and 2"], ["Lorenzini", "Marta", ""], ["De Momi", "Elena", ""], ["Ajoudani", "Arash", ""]]}, {"id": "2105.10045", "submitter": "Zahra Rezaei Khavas", "authors": "Zahra Rezaei Khavas", "title": "A Review on Trust in Human-Robot Interaction", "comments": "10 pages, 1 table. arXiv admin note: substantial text overlap with\n  arXiv:2011.04796", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to agile developments in the field of robotics and human-robot\ninteraction, prospective robotic agents are intended to play the role of\nteammates and partner with humans to perform operations, rather than tools that\nare replacing humans helping humans in a specific task. this notion of\npartnering with robots raises new challenges for human-robot interaction (HRI),\nwhich gives rise to a new field of research in HRI, namely human-robot trust.\nWhere humans and robots are working as partners, the performance of the work\ncan be diminished if humans do not trust robots appropriately. Considering the\nimpact of human-robot trust observed in different HRI fields, many researchers\nhave investigated the field of human-robot trust and examined various concerns\nrelated to human-robot trust. In this work, we review the past works on\nhuman-robot trust based on the research topics and discuss selected trends in\nthis field. Based on these reviews, we finally propose some ideas and areas of\npotential future research at the end of this paper.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 21:50:03 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Khavas", "Zahra Rezaei", ""]]}, {"id": "2105.10047", "submitter": "Ross Greer", "authors": "Ross Greer and Shlomo Dubnov", "title": "Restoring Eye Contact to the Virtual Classroom with Machine Learning", "comments": null, "journal-ref": "Proceedings of CSME 2021, ISBN: 978-989-758-502-9", "doi": "10.5220/0010539806980708", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Nonverbal communication, in particular eye contact, is a critical element of\nthe music classroom, shown to keep students on task, coordinate musical flow,\nand communicate improvisational ideas. Unfortunately, this nonverbal aspect to\nperformance and pedagogy is lost in the virtual classroom. In this paper, we\npropose a machine learning system which uses single instance, single camera\nimage frames as input to estimate the gaze target of a user seated in front of\ntheir computer, augmenting the user's video feed with a display of the\nestimated gaze target and thereby restoring nonverbal communication of directed\ngaze. The proposed estimation system consists of modular machine learning\nblocks, leading to a target-oriented (rather than coordinate-oriented) gaze\nprediction. We instantiate one such example of the complete system to run a\npilot study in a virtual music classroom over Zoom software. Inference time and\naccuracy meet benchmarks for videoconferencing applications, and quantitative\nand qualitative results of pilot experiments include improved success of cue\ninterpretation and student-reported formation of collaborative, communicative\nrelationships between conductor and musician.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 21:51:44 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Greer", "Ross", ""], ["Dubnov", "Shlomo", ""]]}, {"id": "2105.10127", "submitter": "Toby Jia-Jun Li", "authors": "Toby Jia-Jun Li and Brad A. Myers", "title": "A Need-finding Study for Understanding Text Entry in Smartphone App\n  Usage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text entry makes up about one-fourth of the smartphone interaction events,\nand is known to be challenging and difficult. However, there has been little\nstudy about the characteristics of text entry in the context of smartphone app\nusage. In this paper, we present a mixed-method in-situ study conducted in 2016\nwith 17 active smartphone users to better understand text entry in smartphone\napp usage. Our results show 80% of text was entered into communication apps,\nwith different apps exhibiting distinct usage patterns. We found that\nstructured data such as URLs and email addresses are rarely typed but instead\nare auto-completed or replaced with search, copy-and-paste is rarely used, and\nsessions of smartphone usage with text entry involve more apps and last longer.\nWe conclude with a discussion about the implications on the development of\nsystems to better support mobile interaction.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 04:43:44 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 03:52:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Li", "Toby Jia-Jun", ""], ["Myers", "Brad A.", ""]]}, {"id": "2105.10153", "submitter": "Chen-Chieh Liao", "authors": "Chen-Chieh Liao, Dong-Hyun Hwang and Hideki Koike", "title": "How Can I Swing Like Pro?: Golf Swing Analysis Tool for Self Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present an analysis tool to help golf beginners compare\ntheir swing motion with experts' swing motion. The proposed application\nsynchronizes videos with different swing phase timings using the latent\nfeatures extracted by a neural network-based encoder and detects key frames\nwhere discrepant motions occur. We visualize synchronized image frames and 3D\nposes that help users recognize the difference and the key factors that can be\nimportant for their swing skill improvement.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 06:35:16 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Liao", "Chen-Chieh", ""], ["Hwang", "Dong-Hyun", ""], ["Koike", "Hideki", ""]]}, {"id": "2105.10186", "submitter": "Matthias Laschke", "authors": "Judith D\\\"orrenb\\\"acher, Matthias Laschke, Diana L\\\"offler, Ronda\n  Ringfort, Sabrina Gro{\\ss}kopp, Marc Hassenzahl", "title": "Experiencing Utopia. A Positive Approach to Design Fiction", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Design Fiction is known for its provocative and often dystopian speculations\nabout the future. In this paper, we present an alternative approach that\nfocuses primarily on the positive. We propose to imagine, enact, and evaluate\nutopia with participants. By doing so, we react to four main critiques\nconcerning Design Fiction: (1) its negativity, (2) its contextlessness, (3) its\nelitist authorship, and (4) its missing evaluation methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:53:40 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["D\u00f6rrenb\u00e4cher", "Judith", ""], ["Laschke", "Matthias", ""], ["L\u00f6ffler", "Diana", ""], ["Ringfort", "Ronda", ""], ["Gro\u00dfkopp", "Sabrina", ""], ["Hassenzahl", "Marc", ""]]}, {"id": "2105.10223", "submitter": "Andre Rodrigues", "authors": "Andr\\'e Rodrigues, Andr\\'e Santos, Kyle Montague, Hugo Nicolau and\n  Tiago Guerreiro", "title": "WildKey: A Privacy-Aware Keyboard Toolkit for Data Collection\n  In-The-Wild", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Touch data, and in particular text-entry data, has been mostly collected in\nthe laboratory, under controlled conditions. While touch and text-entry data\nhave consistently shown its potential for monitoring and detecting a variety of\nconditions and impairments, its deployment in-the-wild remains a challenge. In\nthis paper, we present WildKey, an Android keyboard toolkit that allows for the\nusable deployment of in-the-wild user studies. WildKey is able to analyze\ntext-entry behaviors through implicit and explicit text-entry data collection\nwhile ensuring user privacy. We detail each of the WildKey's components and\nfeatures, all of the metrics collected, and discuss the steps taken to ensure\nuser privacy and promote compliance.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:22:09 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Rodrigues", "Andr\u00e9", ""], ["Santos", "Andr\u00e9", ""], ["Montague", "Kyle", ""], ["Nicolau", "Hugo", ""], ["Guerreiro", "Tiago", ""]]}, {"id": "2105.10366", "submitter": "Maria Korozi", "authors": "Asterios Leonidis, Maria Korozi, Vassilis Kouroumalis, Emmanouil\n  Adamakis, Dimitris Milathianakis and Constantine Stephanidis", "title": "Going Beyond Second Screens: Applications for the Multi-display\n  Intelligent Living Room", "comments": null, "journal-ref": null, "doi": "10.1145/3452918.3465486", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to investigate how the amenities offered by Intelligent\nEnvironments can be used to shape new types of useful, exciting and fulfilling\nexperiences while watching sports or movies. Towards this direction, two\nambient media players were developed aspiring to offer live access to secondary\ninformation via the available displays of an Intelligent Living Room, and to\nappropriately exploit the technological equipment so as to support natural\ninteraction. Expert-based evaluation experiments revealed some factors that can\ninfluence the overall experience significantly, without hindering the viewers'\nimmersion to the main media.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 14:07:57 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Leonidis", "Asterios", ""], ["Korozi", "Maria", ""], ["Kouroumalis", "Vassilis", ""], ["Adamakis", "Emmanouil", ""], ["Milathianakis", "Dimitris", ""], ["Stephanidis", "Constantine", ""]]}, {"id": "2105.10498", "submitter": "Josef Spjut", "authors": "Josef Spjut and Ben Boudaoud and Joohwan Kim", "title": "A Case Study of First Person Aiming at Low Latency for Esports", "comments": "4 pages, 7 figures, presented at EHPHCI 2021", "journal-ref": null, "doi": "10.31219/osf.io/nu9p3", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lower computer system input-to-output latency substantially reduces many task\ncompletion times. In fact, literature shows that reduction in targeting task\ncompletion time from decreased latency often exceeds the decrease in latency\nalone. However, for aiming in first person shooter (FPS) games, some prior work\nhas demonstrated diminishing returns below 40 ms of local input-to-output\ncomputer system latency. In this paper, we review this prior art and provide an\nadditional case study with data demonstrating the importance of local system\nlatency improvement, even at latency values below 20 ms. Though other factors\nmay determine victory in a particular esports challenge, ensuring balanced\nlocal computer latency among competitors is essential to fair competition.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 23:27:01 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Spjut", "Josef", ""], ["Boudaoud", "Ben", ""], ["Kim", "Joohwan", ""]]}, {"id": "2105.10614", "submitter": "Ruijiang Gao", "authors": "Ruijiang Gao, Maytal Saar-Tsechansky, Maria De-Arteaga, Ligong Han,\n  Min Kyung Lee, Matthew Lease", "title": "Human-AI Collaboration with Bandit Feedback", "comments": "Accepted at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-machine complementarity is important when neither the algorithm nor the\nhuman yield dominant performance across all instances in a given domain. Most\nresearch on algorithmic decision-making solely centers on the algorithm's\nperformance, while recent work that explores human-machine collaboration has\nframed the decision-making problems as classification tasks. In this paper, we\nfirst propose and then develop a solution for a novel human-machine\ncollaboration problem in a bandit feedback setting. Our solution aims to\nexploit the human-machine complementarity to maximize decision rewards. We then\nextend our approach to settings with multiple human decision makers. We\ndemonstrate the effectiveness of our proposed methods using both synthetic and\nreal human responses, and find that our methods outperform both the algorithm\nand the human when they each make decisions on their own. We also show how\npersonalized routing in the presence of multiple human decision-makers can\nfurther improve the human-machine team performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 00:59:29 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gao", "Ruijiang", ""], ["Saar-Tsechansky", "Maytal", ""], ["De-Arteaga", "Maria", ""], ["Han", "Ligong", ""], ["Lee", "Min Kyung", ""], ["Lease", "Matthew", ""]]}, {"id": "2105.10720", "submitter": "Raghav Mittal", "authors": "Raghav Mittal, Sai Anirudh Karre, Y. Raghu Reddy", "title": "Designing Limitless Path in Virtual Reality Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walking in a Virtual Environment is a bounded task. It is challenging for a\nsubject to navigate a large virtual environment designed in a limited physical\nspace. External hardware support may be required to achieve such an act in a\nconcise physical area without compromising navigation and virtual scene\nrendering quality. This paper proposes an algorithmic approach to let a subject\nnavigate a limitless virtual environment within a limited physical space with\nno additional external hardware support apart from the regular\nHead-Mounted-Device (HMD) itself. As part of our work, we developed a Virtual\nArt Gallery as a use-case to validate our algorithm. We conducted a simple\nuser-study to gather feedback from the participants to evaluate the ease of\nlocomotion of the application. The results showed that our algorithm could\ngenerate limitless paths of our use-case under predefined conditions and can be\nextended to other use-cases.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 13:09:02 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Mittal", "Raghav", ""], ["Karre", "Sai Anirudh", ""], ["Reddy", "Y. Raghu", ""]]}, {"id": "2105.10731", "submitter": "Pablo E. Torres", "authors": "Pablo E. Torres, Philip I. N. Ulrich, Veronica Cucuiat, Mutlu\n  Cukurova, Maria Fercovic De la Presa, Rose Luckin, Amanda Carr, Thomas Dylan,\n  Abigail Durrant, John Vines, and Shaun Lawson", "title": "A systematic review of physical-digital play technology and\n  developmentally relevant child behaviour", "comments": "11 Tables, 1 Figure, 4 Appendices; Keywords: Systematic review,\n  digital play, child development, child behaviour, child-computer interactions\n  *Corresponding author info: Faculty of Education, University of Cambridge.\n  Email: pelt2@cam.ac.uk; torresp.uk@gmail.com", "journal-ref": null, "doi": "10.1016/j.ijcci.2021.100323", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New interactive physical-digital play technologies are shaping the way\nchildren plan. These technologies refer to digital play technologies that\nengage children in analogue forms of behaviour, either alone or with others.\nCurrent interactive physical-digital play technologies include robots, digital\nagents, mixed or augmented reality devices, and smart-eye based gaming. Little\nis known, however, about the ways in which these technologies could promote or\ndamage child development. This systematic review was aimed at understanding if\nand how these physical-digital play technologies promoted developmentally\nrelevant behaviour in typically developing 0 to 12 year-olds. Psychology,\nEducation, and Computer Science databases were searched producing 635 paper. A\ntotal of 31 papers met the inclusion criteria, of which 17 were of high enough\nquality to be included for synthesis. Results indicate that these new\ninteractive play technologies could have a positive effect on children's\ndevelopmentally relevant behaviour. The review indicated specific ways in which\ndifferent behaviour were promoted. Providing information about own performance\npromoted self-monitoring. Slowing interactivity, play interdependency, and\njoint object accessibility promoted collaboration. Offering delimited choices\npromoted decision making. Problem solving and physical activity were promoted\nby requiring children to engage in them to keep playing. Four principles\nunderpinned the ways in which physical digital play technologies afforded child\nbehaviour. These included social expectations framing play situations, the\ndirectiveness of action regulations (inviting, guiding or forcing behaviours),\nthe technical features of play technologies (digital play mechanics and\nphysical characteristics), and the alignment between play goals, play\ntechnology and the play behaviours promoted.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 13:41:40 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 16:29:05 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Torres", "Pablo E.", ""], ["Ulrich", "Philip I. N.", ""], ["Cucuiat", "Veronica", ""], ["Cukurova", "Mutlu", ""], ["De la Presa", "Maria Fercovic", ""], ["Luckin", "Rose", ""], ["Carr", "Amanda", ""], ["Dylan", "Thomas", ""], ["Durrant", "Abigail", ""], ["Vines", "John", ""], ["Lawson", "Shaun", ""]]}, {"id": "2105.10735", "submitter": "Mina Khan", "authors": "Mina Khan and Pattie Maes", "title": "PAL: Intelligence Augmentation using Egocentric Visual Context Detection", "comments": null, "journal-ref": "CVPR EPIC Workshop 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Egocentric visual context detection can support intelligence augmentation\napplications. We created a wearable system, called PAL, for wearable,\npersonalized, and privacy-preserving egocentric visual context detection. PAL\nhas a wearable device with a camera, heart-rate sensor, on-device deep\nlearning, and audio input/output. PAL also has a mobile/web application for\npersonalized context labeling. We used on-device deep learning models for\ngeneric object and face detection, low-shot custom face and context recognition\n(e.g., activities like brushing teeth), and custom context clustering (e.g.,\nindoor locations). The models had over 80\\% accuracy in in-the-wild contexts\n(~1000 images) and we tested PAL for intelligence augmentation applications\nlike behavior change. We have made PAL is open-source to further support\nintelligence augmentation using personalized and privacy-preserving egocentric\nvisual contexts.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 14:01:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Khan", "Mina", ""], ["Maes", "Pattie", ""]]}, {"id": "2105.10754", "submitter": "Caglar Yildirim", "authors": "Michael Carroll, Ethan Osborne, Caglar Yildirim", "title": "Effects of VR Gaming and Game Genre on Player Experience", "comments": "2019 IEEE Games, Entertainment, Media Conference (GEM)", "journal-ref": null, "doi": "10.1109/GEM.2019.8811554", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the increasing availability of modern virtual reality (VR) headsets, the\nuse and applications of VR technology for gaming purposes have become more\npervasive than ever. Despite the growing popularity of VR gaming, user studies\ninto how it might affect the player experience (PX) during the gameplay are\nscarce. Accordingly, the current study investigated the effects of VR gaming\nand game genre on PX. We compared PX metrics for two game genres, strategy\n(more interactive) and racing (less interactive), across two gaming platforms,\nVR and traditional desktop gaming. Participants were randomly assigned to one\nof the gaming platforms, played both a strategy and racing game on their\ncorresponding platform, and provided PX ratings. Results revealed that,\nregardless of the game genre, participants in the VR gaming condition\nexperienced a greater level of sense of presence than did those in the desktop\ngaming condition. That said, results showed that the two gaming platforms did\nnot significantly differ from one another in PX ratings. As for the effect of\ngame genre, participants provided greater PX ratings for the strategy game than\nfor the racing game, regardless of whether the game was played on a VR headset\nor desktop computer. Collectively, these results indicate that although VR\ngaming affords a greater sense of presence in the game environment, this\nincrease in presence does not seem to translate into a more satisfactory PX\nwhen playing either a strategy or racing game.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 16:09:28 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Carroll", "Michael", ""], ["Osborne", "Ethan", ""], ["Yildirim", "Caglar", ""]]}, {"id": "2105.10756", "submitter": "Caglar Yildirim", "authors": "Caglar Yildirim, Tara OGrady", "title": "The Efficacy of a Virtual Reality-Based Mindfulness Intervention", "comments": "2020 IEEE International Conference on Artificial Intelligence and\n  Virtual Reality (AIVR)", "journal-ref": null, "doi": "10.1109/AIVR50618.2020.00035", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mindfulness can be defined as increased awareness of and sustained\nattentiveness to the present moment. Recently, there has been a growing\ninterest in the applications of mindfulness for empirical research in wellbeing\nand the use of virtual reality (VR) environments and 3D interfaces as a conduit\nfor mindfulness training. Accordingly, the current experiment investigated\nwhether a brief VR-based mindfulness intervention could induce a greater level\nof state mindfulness, when compared to an audio-based intervention and control\ngroup. Results indicated two mindfulness interventions, VR-based and\naudio-based, induced a greater state of mindfulness, compared to the control\ngroup. Participants in the VR-based mindfulness intervention group reported a\ngreater state of mindfulness than those in the guided audio group, indicating\nthe immersive mindfulness intervention was more robust. Collectively, these\nresults provide empirical support for the efficaciousness of a brief VR-based\nmindfulness intervention in inducing a robust state of mindfulness in\nlaboratory settings.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 16:12:06 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Yildirim", "Caglar", ""], ["OGrady", "Tara", ""]]}, {"id": "2105.10783", "submitter": "Srinjita Bhaduri", "authors": "Srinjita Bhaduri, Peter Gyory, Tamara Sumner", "title": "3DARVisualizer: Debugging 3D Models using Augmented Reality", "comments": "Presented virtually as a Demo at the FabLearn Flagship conference,\n  NYC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Often neglected in traditional education, spatial thinking has played a\ncritical role in science, technology, engineering, and mathematics (STEM)\neducation. Spatial thinking skills can be enhanced by training, life\nexperience, and practice. One approach to train these skills is through 3D\nmodeling (also known as Computer-Aided Design or CAD). Although 3D modeling\ntools have shown promising results in training and enhancing spatial thinking\nskills in undergraduate engineering students when it comes to novices,\nespecially middle and high-school students, they are not sufficient to provide\nrich 3D experience since the 3D models created in CAD are isolated the actual\n3D physical world. Resulting in novice students finding it difficult to create\nerror-free 3D models that would 3D print successfully. This leads to student\nfrustration where students are not motivated to create 3D models themselves;\ninstead, they prefer to download them from online repositories. To address this\nproblem, researchers are focusing on integrating 3D models and displays into\nthe physical world with the help of technologies like Augmented Reality (AR).\nIn this demo, we present an AR application, 3DARVisualizer, that helps us\nexplore the role of AR as a 3D model debugger, including enhancing 3D modeling\nabilities and spatial thinking skills of middle- and high-school students.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 18:15:39 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 02:51:30 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Bhaduri", "Srinjita", ""], ["Gyory", "Peter", ""], ["Sumner", "Tamara", ""]]}, {"id": "2105.10833", "submitter": "Dietmar Offenhuber", "authors": "Dietmar Offenhuber and Sam Auinger", "title": "Distancing interfaces -- improvisational media architectures for\n  place-based discourse under lockdown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper presents the case study of BERLIN_LOKAL_ZEIT, a collaborative\narchive and exhibition project aiming to document and reflect individual\nexperiences of the COVID-19 lockdowns in Berlin, Germany. What began as an\nobservational effort in spring 2020became a year-long archive, exhibition, and\nbroadcasting platform that generated various hyper-local interfaces. The paper\narticulates an improvisational approach to media architecture in the form of\nself-reflection by the project initiators. Necessitated by the limitations\nimposed by the lockdowns on cultural production and public discourse, the paper\npresents an alternative conceptual approach to media architecture that is not\nbased on a fully-specified technological infrastructure for discourse and\ninteraction but instead on improvisational practices that manifest themselves\nin different technical interfaces. This improvisational approach is not merely\na mode of production, but raises questions of about the discourse of media\narchitecture and its underlying assumptions of methodological rigor. In\ntraditional HCI, systems and artifacts are often presented as stable and fully\nspecified, while their users are considered interchangeable. In contrast, we\nconsider urban media interfaces as improvisation-driven infrastructures (or\nimprostructures) in which the actors and their relationships are stable, while\nthe technologies they create are provisional and in flux. As a consequence, the\ndefining property of media architecture is no longer the structural integration\nof media into architecture but the embodiment of mediated communication in\nobjects and performances at concrete places. In an environment where many\nphysical spaces are longer accessible and human contact among strangers\ndiminished, the project explores the restorative function and capacities of\nmedia architecture for shared reflection and discourse in physical public\nspace.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 00:31:41 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Offenhuber", "Dietmar", ""], ["Auinger", "Sam", ""]]}, {"id": "2105.10880", "submitter": "Yang Li", "authors": "Yang Li, Hermawan Mulyono, Ying Chen, Zhiyin Lu, Desmond Chan", "title": "RtFPS: An Interactive Map that Visualizes and Predicts Wildfires in the\n  US", "comments": "Source code: https://github.com/yangland/rtfps", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Climate change has largely impacted our daily lives. As one of its\nconsequences, we are experiencing more wildfires. In the year 2020, wildfires\nburned a record number of 8,888,297 acres in the US. To awaken people's\nattention to climate change, and to visualize the current risk of wildfires, We\ndeveloped RtFPS, \"Real-Time Fire Prediction System\". It provides a real-time\nprediction visualization of wildfire risk at specific locations base on a\nMachine Learning model. It also provides interactive map features that show the\nhistorical wildfire events with environmental info.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 08:07:01 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 01:58:27 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Li", "Yang", ""], ["Mulyono", "Hermawan", ""], ["Chen", "Ying", ""], ["Lu", "Zhiyin", ""], ["Chan", "Desmond", ""]]}, {"id": "2105.11000", "submitter": "Beau Schelble", "authors": "Nathan J. McNeese, Beau G. Schelble, Lorenzo Barberis Canonico,\n  Mustafa Demir", "title": "Who/What is My Teammate? Team Composition Considerations in Human-AI\n  Teaming", "comments": "12 Pages, 6 Figures, IEEE Transactions on Human-Machine Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There are many unknowns regarding the characteristics and dynamics of\nhuman-AI teams, including a lack of understanding of how certain human-human\nteaming concepts may or may not apply to human-AI teams and how this\ncomposition affects team performance. This paper outlines an experimental\nresearch study that investigates essential aspects of human-AI teaming such as\nteam performance, team situation awareness, and perceived team cognition in\nvarious mixed composition teams (human-only, human-human-AI, human-AI-AI, and\nAI-only) through a simulated emergency response management scenario. Results\nindicate dichotomous outcomes regarding perceived team cognition and\nperformance metrics, as perceived team cognition was not predictive of\nperformance. Performance metrics like team situational awareness and team score\nshowed that teams composed of all human participants performed at a lower level\nthan mixed human-AI teams, with the AI-only teams attaining the highest\nperformance. Perceived team cognition was highest in human-only teams, with\nmixed composition teams reporting perceived team cognition 58% below the\nall-human teams. These results inform future mixed teams of the potential\nperformance gains in utilizing mixed teams' over human-only teams in certain\napplications, while also highlighting mixed teams' adverse effects on perceived\nteam cognition.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 19:06:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["McNeese", "Nathan J.", ""], ["Schelble", "Beau G.", ""], ["Canonico", "Lorenzo Barberis", ""], ["Demir", "Mustafa", ""]]}, {"id": "2105.11037", "submitter": "Rajath Chikkatur Srinivasa", "authors": "Rajath Chikkatur Srinivasa, Supriya Arun, Lauren James and Ying Zhu", "title": "Visualization -- a vital decision driving tool for enterprises", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report documents the results found through surveys and interviews on how\nvisualizations help the employees in their workspace. The objectives of this\nstudy were to get in-depth knowledge on what prepares an employee to have the\nright skill set in constructing an informative visualization as well as the\ntools and techniques that they use on their daily basis for analysis and\nvisualization purposes. Using the results gathered, we sorted the information\nin many different ways for analysis and came to conclusions ranging from\ncorporation-based strategies to individualized employee and position\npreferences.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 22:57:17 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Srinivasa", "Rajath Chikkatur", ""], ["Arun", "Supriya", ""], ["James", "Lauren", ""], ["Zhu", "Ying", ""]]}, {"id": "2105.11056", "submitter": "Bruno Gabriel Cavalcante Lima Mr.", "authors": "Bruno Lima, Lucas Amaral, Givanildo Nascimento-Jr, Victor Mafra, Bruno\n  Georgevich Ferreira, Tiago Vieira, Thales Vieira", "title": "User-oriented Natural Human-Robot Control with Thin-Plate Splines and\n  LRCN", "comments": "15 pages, 9 figures, demo video available in\n  https://youtu.be/Rk3iS_KnaWc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a real-time vision-based teleoperation approach for robotic arms\nthat employs a single depth-based camera, exempting the user from the need for\nany wearable devices. By employing a natural user interface, this novel\napproach leverages the conventional fine-tuning control, turning it into a\ndirect body pose capture process. The proposed approach is comprised of two\nmain parts. The first is a nonlinear customizable pose mapping based on\nThin-Plate Splines (TPS), to directly transfer human body motion to robotic arm\nmotion in a nonlinear fashion, thus allowing matching dissimilar bodies with\ndifferent workspace shapes and kinematic constraints. The second is a Deep\nNeural Network hand-state classifier based on Long-term Recurrent Convolutional\nNetworks (LRCN) that exploits the temporal coherence of the acquired depth\ndata. We validate, evaluate and compare our approach through both classical\ncross-validation experiments of the proposed hand state classifier; and user\nstudies over a set of practical experiments involving variants of\npick-and-place and manufacturing tasks. Results revealed that LRCN networks\noutperform single image Convolutional Neural Networks; and that users' learning\ncurves were steep, thus allowing the successful completion of the proposed\ntasks. When compared to a previous approach, the TPS approach revealed no\nincrease in task complexity and similar times of completion, while providing\nmore precise operation in regions closer to workspace boundaries.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 01:41:12 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Lima", "Bruno", ""], ["Amaral", "Lucas", ""], ["Nascimento-Jr", "Givanildo", ""], ["Mafra", "Victor", ""], ["Ferreira", "Bruno Georgevich", ""], ["Vieira", "Tiago", ""], ["Vieira", "Thales", ""]]}, {"id": "2105.11216", "submitter": "Alvaro Garcia", "authors": "Santiago Gonz\\'alez and Alvaro Garc\\'ia and Ana N\\'u\\~nez", "title": "CONECT4: Desarrollo de componentes basados en Realidad Mixta, Realidad\n  Virtual Y Conocimiento Experto para generaci\\'on de entornos de aprendizaje\n  Hombre-M\\'aquina", "comments": "15 pages, 16 figures, in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents the results of project CONECT4, which addresses the\nresearch and development of new non-intrusive communication methods for the\ngeneration of a human-machine learning ecosystem oriented to predictive\nmaintenance in the automotive industry. Through the use of innovative\ntechnologies such as Augmented Reality, Virtual Reality, Digital Twin and\nexpert knowledge, CONECT4 implements methodologies that allow improving the\nefficiency of training techniques and knowledge management in industrial\ncompanies. The research has been supported by the development of content and\nsystems with a low level of technological maturity that address solutions for\nthe industrial sector applied in training and assistance to the operator. The\nresults have been analyzed in companies in the automotive sector, however, they\nare exportable to any other type of industrial sector. -- --\n  En esta publicaci\\'on se presentan los resultados del proyecto CONECT4, que\naborda la investigaci\\'on y desarrollo de nuevos m\\'etodos de comunicaci\\'on no\nintrusivos para la generaci\\'on de un ecosistema de aprendizaje\nhombre-m\\'aquina orientado al mantenimiento predictivo en la industria de\nautomoci\\'on. A trav\\'es del uso de tecnolog\\'ias innovadoras como la Realidad\nAumentada, la Realidad Virtual, el Gemelo Digital y conocimiento experto,\nCONECT4 implementa metodolog\\'ias que permiten mejorar la eficiencia de las\nt\\'ecnicas de formaci\\'on y gesti\\'on de conocimiento en las empresas\nindustriales. La investigaci\\'on se ha apoyado en el desarrollo de contenidos y\nsistemas con un nivel de madurez tecnol\\'ogico bajo que abordan soluciones para\nel sector industrial aplicadas en la formaci\\'on y asistencia al operario. Los\nresultados han sido analizados en empresas del sector de automoci\\'on, no\nobstante, son exportables a cualquier otro tipo de sector industrial.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:51:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gonz\u00e1lez", "Santiago", ""], ["Garc\u00eda", "Alvaro", ""], ["N\u00fa\u00f1ez", "Ana", ""]]}, {"id": "2105.11516", "submitter": "Hyekang Joo", "authors": "Hyekang Joo, Calvin Bao, Ishan Sen, Furong Huang, and Leilani Battle", "title": "Guided Hyperparameter Tuning Through Visualization and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For deep learning practitioners, hyperparameter tuning for optimizing model\nperformance can be a computationally expensive task. Though visualization can\nhelp practitioners relate hyperparameter settings to overall model performance,\nsignificant manual inspection is still required to guide the hyperparameter\nsettings in the next batch of experiments. In response, we present a\nstreamlined visualization system enabling deep learning practitioners to more\nefficiently explore, tune, and optimize hyperparameters in a batch of\nexperiments. A key idea is to directly suggest more optimal hyperparameter\nvalues using a predictive mechanism. We then integrate this mechanism with\ncurrent visualization practices for deep learning. Moreover, an analysis on the\nvariance in a selected performance metric in the context of the model\nhyperparameters shows the impact that certain hyperparameters have on the\nperformance metric. We evaluate the tool with a user study on deep learning\nmodel builders, finding that our participants have little issue adopting the\ntool and working with it as part of their workflow.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 19:55:24 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Joo", "Hyekang", ""], ["Bao", "Calvin", ""], ["Sen", "Ishan", ""], ["Huang", "Furong", ""], ["Battle", "Leilani", ""]]}, {"id": "2105.11582", "submitter": "Zackory Erickson", "authors": "Zackory Erickson, Henry M. Clever, Vamsee Gangaram, Eliot Xing, Greg\n  Turk, C. Karen Liu, and Charles C. Kemp", "title": "Characterizing Multidimensional Capacitive Servoing for Physical\n  Human-Robot Interaction", "comments": "17 pages, 22 figures, 4 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Towards the goal of robots performing robust and intelligent physical\ninteractions with people, it is crucial that robots are able to accurately\nsense the human body, follow trajectories around the body, and track human\nmotion. This study introduces a capacitive servoing control scheme that allows\na robot to sense and navigate around human limbs during close physical\ninteractions. Capacitive servoing leverages temporal measurements from a\nmulti-electrode capacitive sensor array mounted on a robot's end effector to\nestimate the relative position and orientation (pose) of a nearby human limb.\nCapacitive servoing then uses these human pose estimates from a data-driven\npose estimator within a feedback control loop in order to maneuver the robot's\nend effector around the surface of a human limb. We provide a design overview\nof capacitive sensors for human-robot interaction and then investigate the\nperformance and generalization of capacitive servoing through an experiment\nwith 12 human participants. The results indicate that multidimensional\ncapacitive servoing enables a robot's end effector to move proximally or\ndistally along human limbs while adapting to human pose. Using a\ncross-validation experiment, results further show that capacitive servoing\ngeneralizes well across people with different body size.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 00:04:46 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Erickson", "Zackory", ""], ["Clever", "Henry M.", ""], ["Gangaram", "Vamsee", ""], ["Xing", "Eliot", ""], ["Turk", "Greg", ""], ["Liu", "C. Karen", ""], ["Kemp", "Charles C.", ""]]}, {"id": "2105.11620", "submitter": "Yanjun Wang", "authors": "Yanjun Wang, Zixuan Li, Xiaokang Qiu, Sanjay G. Rao", "title": "Comparative Synthesis: Learning Optimal Programs with Indeterminate\n  Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitative program synthesis aims to generate a program that satisfies not\nonly boolean specifications but also quantitative objectives. Nonetheless,\nobtaining precise quantitative objectives per se can be a challenging task. In\nthis paper, we propose comparative synthesis, a bootstrapping quantitative\nsynthesis framework in which an indeterminate objective and a satisfying\nprogram are synthesized in tandem. The key idea is to make comparative queries\nto learn the user's preference over candidate programs, with which objectives\ncan be conjectured. These objectives, which are indeterminate as they can be\nrefined along the course of user interaction, guide the search of satisfying\nprograms.\n  Within the comparative synthesis framework, we developed two novel\ncomparative synthesis procedures (CLPs) with the aim of minimizing the number\nof queries to the user. We prove that both CLPs converge and use them in two\ncase studies: generating bandwidth allocations for network design and solving\nSyGuS benchmarks with syntactic objectives. Experiments show that our framework\ncan successfully synthesize satisfying/optimal solutions by making queries\nonly, without a priori knowledge about the quantitative objective.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 02:36:13 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wang", "Yanjun", ""], ["Li", "Zixuan", ""], ["Qiu", "Xiaokang", ""], ["Rao", "Sanjay G.", ""]]}, {"id": "2105.11751", "submitter": "Lachlan Urquhart Ph.D", "authors": "Jiahong Chen and Lachlan Urquhart", "title": "'They're all about pushing the products and shiny things rather than\n  fundamental security' Mapping Socio-technical Challenges in Securing the\n  Smart Home", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insecure connected devices can cause serious threats not just to smart home\nowners, but also the underlying infrastructural network as well. There has been\nincreasing academic and regulatory interest in addressing cybersecurity risks\nfrom both the standpoint of Internet of Things (IoT) vendors and that of\nend-users. In addition to the current data protection and network security\nlegal frameworks, for example, the UK government has initiated the 'Secure by\nDesign' campaign. While there has been work on how organisations and\nindividuals manage their own cybersecurity risks, it remains unclear to what\nextent IoT vendors are supporting end-users to perform day-to-day management of\nsuch risks in a usable way, and what is stopping the vendors from improving\nsuch support. We interviewed 13 experts in the field of IoT and identified\nthree main categories of barriers to making IoT products usably secure:\ntechnical, legal and organisational. In this paper we further discuss the\npolicymaking implications of these findings and make some recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 08:38:36 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Chen", "Jiahong", ""], ["Urquhart", "Lachlan", ""]]}, {"id": "2105.11767", "submitter": "Nungduk Yun", "authors": "Nungduk Yun and Seiji Yamada", "title": "Empirical Investigation of Factors that Influence Human Presence and\n  Agency in Telepresence Robot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, a community starts to find the need for human presence in an\nalternative way, there has been tremendous research and development in\nadvancing telepresence robots. People tend to feel closer and more comfortable\nwith telepresence robots as many senses a human presence in robots. In general,\nmany people feel the sense of agency from the face of a robot, but some\ntelepresence robots without arm and body motions tend to give a sense of human\npresence. It is important to identify and configure how the telepresence robots\naffect a sense of presence and agency to people by including human face and\nslight face and arm motions. Therefore, we carried out extensive research via\nweb-based experiment to determine the prototype that can result in soothing\nhuman interaction with the robot. The experiments featured videos of a\ntelepresence robot n = 128, 2 x 2 between-participant study robot face factor:\nvideo-conference, robot-like face; arm motion factor: moving vs. static) to\ninvestigate the factors significantly affecting human presence and agency with\nthe robot. We used two telepresence robots: an affordable robot platform and a\nmodified version for human interaction enhancements. The findings suggest that\nparticipants feel agency that is closer to human-likeness when the robot's face\nwas replaced with a human's face and without a motion. The robot's motion\ninvokes a feeling of human presence whether the face is human or robot-like.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:03:56 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yun", "Nungduk", ""], ["Yamada", "Seiji", ""]]}, {"id": "2105.11794", "submitter": "Diana C. Hernandez-Bocanegra", "authors": "Diana C. Hernandez-Bocanegra and Juergen Ziegler", "title": "Effects of interactivity and presentation on review-based explanations\n  for recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  User reviews have become an important source for recommending and explaining\nproducts or services. Particularly, providing explanations based on user\nreviews may improve users' perception of a recommender system (RS). However,\nlittle is known about how review-based explanations can be effectively and\nefficiently presented to users of RS. We investigate the potential of\ninteractive explanations in review-based RS in the domain of hotels, and\npropose an explanation scheme inspired by dialog models and formal argument\nstructures. Additionally, we also address the combined effect of interactivity\nand different presentation styles (i.e. using only text, a bar chart or a\ntable), as well as the influence that different user characteristics might have\non users' perception of the system and its explanations. To such effect, we\nimplemented a review-based RS using a matrix factorization explanatory method,\nand conducted a user study. Our results show that providing more interactive\nexplanations in review-based RS has a significant positive influence on the\nperception of explanation quality, effectiveness and trust in the system by\nusers, and that user characteristics such as rational decision-making style and\nsocial awareness also have a significant influence on this perception.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 09:54:42 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hernandez-Bocanegra", "Diana C.", ""], ["Ziegler", "Juergen", ""]]}, {"id": "2105.11909", "submitter": "Panagiotis Kourtesis", "authors": "Panagiotis Kourtesis and Sarah E. MacPherson", "title": "Immersive virtual reality methods in cognitive neuroscience and\n  neuropsychology: Meeting the criteria of the National Academy of\n  Neuropsychology and American Academy of Clinical Neuropsychology", "comments": "30 Pages, 2 Tables, 4 Figures, under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clinical tools involving immersive virtual reality (VR) may bring several\nadvantages to cognitive neuroscience and neuropsychology. However, there are\nsome technical and methodological pitfalls. The American Academy of Clinical\nNeuropsychology (AACN) and the National Academy of Neuropsychology (NAN) raised\n8 key issues pertaining to Computerized Neuropsychological Assessment Devices.\nThese issues pertain to: (1) the safety and effectivity; (2) the identity of\nthe end-user; (3) the technical hardware and software features; (4) privacy and\ndata security; (5) the psychometric properties; (6) examinee issues; (7) the\nuse of reporting services; and (8) the reliability of the responses and\nresults. The VR Everyday Assessment Lab (VR-EAL) is the first immersive VR\nneuropsychological battery with enhanced ecological validity for the assessment\nof everyday cognitive functions by offering a pleasant testing experience\nwithout inducing cybersickness. The VR-EAL meets the criteria of the NAN and\nAACN, addresses the methodological pitfalls, and brings advantages for\nneuropsychological testing. However, there are still shortcomings of the\nVR-EAL, which should be addressed. Future iterations should strive to improve\nthe embodiment illusion in VR-EAL and the creation of an open access VR\nsoftware library should be attempted. The discussed studies demonstrate the\nutility of VR methods in cognitive neuroscience and neuropsychology.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:15:57 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["MacPherson", "Sarah E.", ""]]}, {"id": "2105.11941", "submitter": "Jingwen Fu", "authors": "Jingwen Fu, Xiaoyi Zhang, Yuwang Wang, Wenjun Zeng, Sam Yang and\n  Grayson Hilliard", "title": "Understanding Mobile GUI: from Pixel-Words to Screen-Sentences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of mobile phones makes mobile GUI understanding an important\ntask. Most previous works in this domain require human-created metadata of\nscreens (e.g. View Hierarchy) during inference, which unfortunately is often\nnot available or reliable enough for GUI understanding. Inspired by the\nimpressive success of Transformers in NLP tasks, targeting for purely\nvision-based GUI understanding, we extend the concepts of Words/Sentence to\nPixel-Words/Screen-Sentence, and propose a mobile GUI understanding\narchitecture: Pixel-Words to Screen-Sentence (PW2SS). In analogy to the\nindividual Words, we define the Pixel-Words as atomic visual components (text\nand graphic components), which are visually consistent and semantically clear\nacross screenshots of a large variety of design styles. The Pixel-Words\nextracted from a screenshot are aggregated into Screen-Sentence with a Screen\nTransformer proposed to model their relations. Since the Pixel-Words are\ndefined as atomic visual components, the ambiguity between their visual\nappearance and semantics is dramatically reduced. We are able to make use of\nmetadata available in training data to auto-generate high-quality annotations\nfor Pixel-Words. A dataset, RICO-PW, of screenshots with Pixel-Words\nannotations is built based on the public RICO dataset, which will be released\nto help to address the lack of high-quality training data in this area. We\ntrain a detector to extract Pixel-Words from screenshots on this dataset and\nachieve metadata-free GUI understanding during inference. We conduct\nexperiments and show that Pixel-Words can be well extracted on RICO-PW and well\ngeneralized to a new dataset, P2S-UI, collected by ourselves. The effectiveness\nof PW2SS is further verified in the GUI understanding tasks including relation\nprediction, clickability prediction, screen retrieval, and app type\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:45:54 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Fu", "Jingwen", ""], ["Zhang", "Xiaoyi", ""], ["Wang", "Yuwang", ""], ["Zeng", "Wenjun", ""], ["Yang", "Sam", ""], ["Hilliard", "Grayson", ""]]}, {"id": "2105.11955", "submitter": "Mark Christopher Ballandies", "authors": "Mark C. Ballandies and Marcus M. Dapp and Benjamin A. Degenhart and\n  Dirk Helbing", "title": "Finance 4.0: Design principles for a value-sensitive cryptoecnomic\n  system to address sustainability", "comments": null, "journal-ref": "ECIS 2021 Research Papers. 62 (2021)", "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptoeconomic systems derive their power but can not be controlled by the\nunderlying software systems and the rules they enshrine. This adds a level of\ncomplexity to the software design process. At the same time, such systems, when\ndesigned with human values in mind, offer new approaches to tackle\nsustainability challenges, that are plagued by commons dilemmas and negative\nexternal effects caused by a one-dimensional monetary system. This paper\nproposes a design science research methodology with value-sensitive design\nmethods to derive design principles for a value-sensitive socio-ecological\ncryptoeconomic system that incentivizes actions toward sustainability via\nmulti-dimensional token incentives. These design principles are implemented in\na software that is validated in user studies that demonstrate its relevance,\nusability and impact. Our findings provide new insights on designing\ncryptoeconomic systems. Moreover, the identified design principles for a\nvalue-sensitive socio-ecological financial system indicate opportunities for\nnew research directions and business innovations.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:09:50 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ballandies", "Mark C.", ""], ["Dapp", "Marcus M.", ""], ["Degenhart", "Benjamin A.", ""], ["Helbing", "Dirk", ""]]}, {"id": "2105.12032", "submitter": "Shogo Kawanaka", "authors": "Shogo Kawanaka, Juliana Miehle, Yuki Matsuda, Hirohiko Suwa, Keiichi\n  Yasumoto, Wolfgang Minker", "title": "Task allocation interface design and personalization in gamified\n  participatory sensing for tourism", "comments": "21 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The collection of spatiotemporal tourism information is important in smart\ntourism and user-generated contents are perceived as reliable information.\nParticipatory sensing is a useful method for collecting such data, and the\nactive contribution of users is an important aspect for continuous and\nefficient data collection. This study has focused on the impact of task\nallocation interface design and individual personality on data collection\nefficiency and their contribution in gamified participatory sensing for\ntourism. We have designed two types of interfaces: a map-based with active\nselection and a chat-based with passive selection. Moreover, different levels\nof elaborateness and indirectness have been introduced into the chat-based\ninterface. We have employed the Gamification User Types Hexad framework to\nidentify the differences in the contributions and interface preferences of\ndifferent user types. The results of our tourism experiment with 108\nparticipants show that the map-based interface collects more data, while the\nchat-based interface collects data for spots with higher information demand. We\nalso found that the contribution to sensing behavior and interface preference\ndiffered depending on the individual user type.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:11:05 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Kawanaka", "Shogo", ""], ["Miehle", "Juliana", ""], ["Matsuda", "Yuki", ""], ["Suwa", "Hirohiko", ""], ["Yasumoto", "Keiichi", ""], ["Minker", "Wolfgang", ""]]}, {"id": "2105.12370", "submitter": "Thijs Waardenburg", "authors": "Thijs Waardenburg, Niels van Huizen, Jelle van Dijk, Maurice Magn\\'ee,\n  Wouter Staal, Jan-Pieter Teunisse and Mascha van der Voort", "title": "Design Your Life: User-Initiated Design of Technology to Support\n  Independent Living of Young Autistic Adults", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-78224-5_26", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the development of and first experiences with 'Design\nYour Life': a novel method aimed at user-initiated design of technologies\nsupporting young autistic adults in independent living. A conceptual,\nphenomenological background resulting in four core principles is described.\nTaking a practice-oriented Research-through-Design approach, three co-design\ncase studies were conducted, in which promising methods from the co-design\nliterature with the lived experiences and practical contexts of autistic young\nadults and their caregivers is contrasted. This explorative inquiry provided\nsome first insights into several design directions of the Design Your\nLife-process. In a series of new case studies that shall follow, the Design\nYour Life-method will be iteratively developed, refined and ultimately\nvalidated in practice.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 07:26:19 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Waardenburg", "Thijs", ""], ["van Huizen", "Niels", ""], ["van Dijk", "Jelle", ""], ["Magn\u00e9e", "Maurice", ""], ["Staal", "Wouter", ""], ["Teunisse", "Jan-Pieter", ""], ["van der Voort", "Mascha", ""]]}, {"id": "2105.12453", "submitter": "Ahmed Arif", "authors": "Ahmed Sabbir Arif, Wolfgang Stuerzlinger, Euclides Jose de Mendonca\n  Filho, Alec Gordynski", "title": "How Do Users Interact with an Error-Prone In-Air Gesture Recognizer?", "comments": "In CHI 2014 Workshop on Gesture-based Interaction Design:\n  Communication and Cognition (April 26, 2014). Toronto, Ontario, Canada, 69-72", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present results of two pilot studies that investigated human error\nbehaviours with an error prone in-air gesture recognizer. During the studies,\nusers performed a small set of simple in-air gestures. In the first study,\nthese gestures were abstract. The second study associated concrete tasks with\neach gesture. Interestingly, the error patterns observed in the two studies\nwere substantially different.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 10:30:50 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Arif", "Ahmed Sabbir", ""], ["Stuerzlinger", "Wolfgang", ""], ["Filho", "Euclides Jose de Mendonca", ""], ["Gordynski", "Alec", ""]]}, {"id": "2105.12477", "submitter": "Stephan Wiefling", "authors": "Johannes Kunke, Stephan Wiefling, Markus Ullmann, Luigi Lo Iacono", "title": "Evaluation of Account Recovery Strategies with FIDO2-based Passwordless\n  Authentication", "comments": "12 pages, 1 figure, 1 table", "journal-ref": "Open Identity Summit 2021 (OID '21), June 1-2, 2021. Pages 59-70", "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Threats to passwords are still very relevant due to attacks like phishing or\ncredential stuffing. One way to solve this problem is to remove passwords\ncompletely. User studies on passwordless FIDO2 authentication using security\ntokens demonstrated the potential to replace passwords. However, widespread\nacceptance of FIDO2 depends, among other things, on how user accounts can be\nrecovered when the security token becomes permanently unavailable. For this\nreason, we provide a heuristic evaluation of 12 account recovery mechanisms\nregarding their properties for FIDO2 passwordless authentication. Our results\nshow that the currently used methods have many drawbacks. Some even rely on\npasswords, taking passwordless authentication ad absurdum. Still, our\nevaluation identifies promising account recovery solutions and provides\nrecommendations for further studies.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:21:37 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Kunke", "Johannes", ""], ["Wiefling", "Stephan", ""], ["Ullmann", "Markus", ""], ["Iacono", "Luigi Lo", ""]]}, {"id": "2105.12538", "submitter": "Valentina Andries", "authors": "Valentina Andries and Sabina Savadova", "title": "Understanding the Role of Digital Technology in the Transitions of\n  Refugee Families with Young Children into A New Culture: A Case Study of\n  Scotland", "comments": "5 pages in total (including references). The paper has been accepted\n  at the Interaction Design and Children (IDC) 2021 conference for the work in\n  progress track; the conference will be held online between 24th and 30th June\n  2021", "journal-ref": null, "doi": "10.1145/3459990.3465185", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The worldwide refugee crisis is a major current challenge, affecting the\nhealth and education of millions of families with children due to displacement.\nDespite the various challenges and risks of migration practices, numerous\nrefugee families have access to interactive technologies during these\nprocesses. The aim of this ongoing study is to explore the role of technologies\nin the transitions of refugee families in Scotland. Based on Tudge's\necocultural theory, a qualitative case-study approach has been adopted.\nSemi-structured interviews have been conducted with volunteers who work with\nrefugee families in a big city in Scotland, and proxy observations of young\nchildren were facilitated remotely by their refugee parents. A preliminary\noverview of the participants' insights of the use and role of technology for\ntransitioning into a new culture is provided here.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 13:27:54 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Andries", "Valentina", ""], ["Savadova", "Sabina", ""]]}, {"id": "2105.12610", "submitter": "Guojun Chen", "authors": "Guojun Chen, Noah Weiner, Lin Zhong", "title": "POD: A Smartphone That Flies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present POD, a smartphone that flies, as a new way to achieve hands-free,\neyes-up mobile computing. Unlike existing drone-carried user interfaces, POD\nfeatures a smartphone-sized display and the computing and sensing power of a\nmodern smartphone. We share our experience in building a prototype of POD,\ndiscuss the technical challenges facing it, and describe early results toward\naddressing them.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 15:07:12 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chen", "Guojun", ""], ["Weiner", "Noah", ""], ["Zhong", "Lin", ""]]}, {"id": "2105.12651", "submitter": "Daniel Diethei", "authors": "Daniel Diethei, Ashley Colley, Lisa Dannenberg, Muhammad Fawad Jawaid\n  Malik, Johannes Sch\\\"oning", "title": "The Usability and Trustworthiness of Medical Eye Images", "comments": null, "journal-ref": "2021 IEEE International Conference on Healthcare Informatics\n  (ICHI)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of blindness is preventable, and is located in developing\ncountries. While mHealth applications for retinal imaging in combination with\naffordable smartphone lens adaptors are a step towards better eye care access,\nthe expert knowledge and additional hardware needed are often unavailable in\ndeveloping countries. Eye screening apps without lens adaptors exist, but we do\nnot know much about the experience of guiding users to take medical eye images.\nAdditionally, when an AI based diagnosis is provided, trust plays an important\nrole in ensuring in the adoption. This work addresses factors that impact the\nusability and trustworthiness dimensions of mHealth applications. We present\nthe design, development and evaluation of EyeGuide, a mobile app that assists\nusers in taking medical eye images using only their smartphone camera. In a\nstudy (n=28) we observed that users of an interactive tutorial captured images\nfaster compared to audible tone based guidance. In a second study (n=40) we\nfound out that providing disease-specific background information was the most\neffective factor to increase trustworthiness in the AI based diagnosis.\nApplication areas of EyeGuide are AI based disease detection and telemedicine\nexaminations.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 16:10:06 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Diethei", "Daniel", ""], ["Colley", "Ashley", ""], ["Dannenberg", "Lisa", ""], ["Malik", "Muhammad Fawad Jawaid", ""], ["Sch\u00f6ning", "Johannes", ""]]}, {"id": "2105.12724", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Yuhang Hu, Lianfeng Li, Sara Cummings, Hod Lipson", "title": "Smile Like You Mean It: Driving Animatronic Robotic Face with Learned\n  Models", "comments": "ICRA 2021. Website:http://www.cs.columbia.edu/~bchen/aiface/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ability to generate intelligent and generalizable facial expressions is\nessential for building human-like social robots. At present, progress in this\nfield is hindered by the fact that each facial expression needs to be\nprogrammed by humans. In order to adapt robot behavior in real time to\ndifferent situations that arise when interacting with human subjects, robots\nneed to be able to train themselves without requiring human labels, as well as\nmake fast action decisions and generalize the acquired knowledge to diverse and\nnew contexts. We addressed this challenge by designing a physical animatronic\nrobotic face with soft skin and by developing a vision-based self-supervised\nlearning framework for facial mimicry. Our algorithm does not require any\nknowledge of the robot's kinematic model, camera calibration or predefined\nexpression set. By decomposing the learning process into a generative model and\nan inverse model, our framework can be trained using a single motor babbling\ndataset. Comprehensive evaluations show that our method enables accurate and\ndiverse face mimicry across diverse human subjects. The project website is at\nhttp://www.cs.columbia.edu/~bchen/aiface/\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:57:19 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chen", "Boyuan", ""], ["Hu", "Yuhang", ""], ["Li", "Lianfeng", ""], ["Cummings", "Sara", ""], ["Lipson", "Hod", ""]]}, {"id": "2105.12762", "submitter": "Jonathan K Kummerfeld", "authors": "Jonathan K. Kummerfeld", "title": "Quantifying and Avoiding Unfair Qualification Labour in Crowdsourcing", "comments": "To appear at ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive work has argued in favour of paying crowd workers a wage that is at\nleast equivalent to the U.S. federal minimum wage. Meanwhile, research on\ncollecting high quality annotations suggests using a qualification that\nrequires workers to have previously completed a certain number of tasks. If\nmost requesters who pay fairly require workers to have completed a large number\nof tasks already then workers need to complete a substantial amount of poorly\npaid work before they can earn a fair wage. Through analysis of worker\ndiscussions and guidance for researchers, we estimate that workers spend\napproximately 2.25 months of full time effort on poorly paid tasks in order to\nget the qualifications needed for better paid tasks. We discuss alternatives to\nthis qualification and conduct a study of the correlation between\nqualifications and work quality on two NLP tasks. We find that it is possible\nto reduce the burden on workers while still collecting high quality data.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:02:39 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kummerfeld", "Jonathan K.", ""]]}, {"id": "2105.12791", "submitter": "Roberto Calandra", "authors": "Mike Lambeta, Huazhe Xu, Jingwei Xu, Po-Wei Chou, Shaoxiong Wang,\n  Trevor Darrell, Roberto Calandra", "title": "PyTouch: A Machine Learning Library for Touch Processing", "comments": "7 pages. Accepted at ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased availability of rich tactile sensors, there is an equally\nproportional need for open-source and integrated software capable of\nefficiently and effectively processing raw touch measurements into high-level\nsignals that can be used for control and decision-making. In this paper, we\npresent PyTouch -- the first machine learning library dedicated to the\nprocessing of touch sensing signals. PyTouch, is designed to be modular,\neasy-to-use and provides state-of-the-art touch processing capabilities as a\nservice with the goal of unifying the tactile sensing community by providing a\nlibrary for building scalable, proven, and performance-validated modules over\nwhich applications and research can be built upon. We evaluate PyTouch on\nreal-world data from several tactile sensors on touch processing tasks such as\ntouch detection, slip and object pose estimations. PyTouch is open-sourced at\nhttps://github.com/facebookresearch/pytouch .\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:55:18 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Lambeta", "Mike", ""], ["Xu", "Huazhe", ""], ["Xu", "Jingwei", ""], ["Chou", "Po-Wei", ""], ["Wang", "Shaoxiong", ""], ["Darrell", "Trevor", ""], ["Calandra", "Roberto", ""]]}, {"id": "2105.12865", "submitter": "Adam Williams", "authors": "Adam S. Williams and Francisco R. Ortega", "title": "A Concise Guide to Elicitation Methodology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the open questions in the field of interaction design is \"what inputs\nor interaction techniques should be used with augmented reality devices?\" The\ntransition from a touchpad and a keyboard to a multi-touch device was\nrelatively small. The transition from a multi-touch device to an HMD with no\ncontrollers or clear surface to interact with is more complicated. This book is\na guide for how to figure out what interaction techniques and modalities people\nprefer when interacting with those devices. The name of the technique covered\nhere is Elicitation. Elicitation is a form of participatory design, meaning\ndesign with direct end-user involvement. By running an elicitation study\nresearchers can observe unconstrained human interactions with emerging\ntechnologies to help guide input design.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 22:14:45 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Williams", "Adam S.", ""], ["Ortega", "Francisco R.", ""]]}, {"id": "2105.12928", "submitter": "Raja Kushalnagar", "authors": "Raja S. Kushalnagar", "title": "Legibility of Videos with ASL signers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The viewing size of a signer correlates with legibility, i.e., the ease with\nwhich a viewer can recognize individual signs. The WCAG 2.0 guidelines (G54)\nmention in the notes that there should be a mechanism to adjust the size to\nensure the signer is discernible but does not state minimum discernibility\nguidelines. The fluent range (the range over which sign viewers can follow the\nsigners at maximum speed) extends from about 7{\\deg} to 20{\\deg}, which is far\ngreater than 2{\\deg} for print. Assuming a standard viewing distance of 16\ninches from a 5-inch smartphone display, the corresponding sizes are from 2 to\n5 inches, i.e., from 1/3rd to full-screen. This is consistent with vision\nscience findings about human visual processing properties, and how they play a\ndominant role in constraining the distribution of signer sizes.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 03:38:22 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Kushalnagar", "Raja S.", ""]]}, {"id": "2105.12938", "submitter": "Christian Arzate Cruz", "authors": "Christian Arzate Cruz and Takeo Igarashi", "title": "Interactive Explanations: Diagnosis and Repair of Reinforcement Learning\n  Based Agent Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning techniques successfully generate convincing agent\nbehaviors, but it is still difficult to tailor the behavior to align with a\nuser's specific preferences. What is missing is a communication method for the\nsystem to explain the behavior and for the user to repair it. In this paper, we\npresent a novel interaction method that uses interactive explanations using\ntemplates of natural language as a communication method. The main advantage of\nthis interaction method is that it enables a two-way communication channel\nbetween users and the agent; the bot can explain its thinking procedure to the\nusers, and the users can communicate their behavior preferences to the bot\nusing the same interactive explanations. In this manner, the thinking procedure\nof the bot is transparent, and users can provide corrections to the bot that\ninclude a suggested action to take, a goal to achieve, and the reasons behind\nthese decisions. We tested our proposed method in a clone of the video game\nnamed \\textit{Super Mario Bros.}, and the results demonstrate that our\ninteractive explanation approach is effective at diagnosing and repairing bot\nbehaviors.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 04:17:48 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Cruz", "Christian Arzate", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2105.12944", "submitter": "Christian Arzate Cruz", "authors": "Christian Arzate Cruz and Takeo Igarashi", "title": "MarioMix: Creating Aligned Playstyles for Bots with Interactive\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a generic framework that enables game developers\nwithout knowledge of machine learning to create bot behaviors with playstyles\nthat align with their preferences. Our framework is based on interactive\nreinforcement learning (RL), and we used it to create a behavior authoring tool\ncalled MarioMix. This tool enables non-experts to create bots with varied\nplaystyles for the game titled Super Mario Bros. The main interaction procedure\nof MarioMix consists of presenting short clips of gameplay displaying\nprecomputed bots with different playstyles to end-users. Then, end-users can\nselect the bot with the playstyle that behaves as intended. We evaluated\nMarioMix by incorporating input from game designers working in the industry.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 05:30:23 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Cruz", "Christian Arzate", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2105.12949", "submitter": "Christian Arzate Cruz", "authors": "Christian Arzate Cruz and Takeo Igarashi", "title": "A Survey on Interactive Reinforcement Learning: Design Principles and\n  Open Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive reinforcement learning (RL) has been successfully used in various\napplications in different fields, which has also motivated HCI researchers to\ncontribute in this area. In this paper, we survey interactive RL to empower\nhuman-computer interaction (HCI) researchers with the technical background in\nRL needed to design new interaction techniques and propose new applications. We\nelucidate the roles played by HCI researchers in interactive RL, identifying\nideas and promising research directions. Furthermore, we propose generic design\nprinciples that will provide researchers with a guide to effectively implement\ninteractive RL applications.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 05:46:43 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Cruz", "Christian Arzate", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2105.13061", "submitter": "Junxiao Shen Mr", "authors": "Junxiao Shen and John Dudley and Per Ola Kristensson", "title": "The Imaginative Generative Adversarial Network: Automatic Data\n  Augmentation for Dynamic Skeleton-Based Hand Gesture and Human Action\n  Recognition", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning approaches deliver state-of-the-art performance in recognition\nof spatiotemporal human motion data. However, one of the main challenges in\nthese recognition tasks is limited available training data. Insufficient\ntraining data results in over-fitting and data augmentation is one approach to\naddress this challenge. Existing data augmentation strategies, such as\ntransformations including scaling, shifting and interpolating, require\nhyperparameter optimization that can easily cost hundreds of GPU hours. In this\npaper, we present a novel automatic data augmentation model, the Imaginative\nGenerative Adversarial Network (GAN) that approximates the distribution of the\ninput data and samples new data from this distribution. It is automatic in that\nit requires no data inspection and little hyperparameter tuning and therefore\nit is a low-cost and low-effort approach to generate synthetic data. The\nproposed data augmentation strategy is fast to train and the synthetic data\nleads to higher recognition accuracy than using data augmented with a classical\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:07:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Shen", "Junxiao", ""], ["Dudley", "John", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "2105.13240", "submitter": "Haoyu Li", "authors": "Haoyu Li and Han-Wei Shen", "title": "Time Varying Particle Data Feature Extraction and Tracking with Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Analyzing particle data plays an important role in many scientific\napplications such as fluid simulation, cosmology simulation and molecular\ndynamics. While there exist methods that can perform feature extraction and\ntracking for volumetric data, performing those tasks for particle data is more\nchallenging because of the lack of explicit connectivity information. Although\none may convert the particle data to volume first, this approach is at risk of\nincurring error and increasing the size of the data. In this paper, we take a\ndeep learning approach to create feature representations for scientific\nparticle data to assist feature extraction and tracking. We employ a deep\nlearning model, which produces latent vectors to represent the relation between\nspatial locations and physical attributes in a local neighborhood. With the\nlatent vectors, features can be extracted by clustering these vectors. To\nachieve fast feature tracking, the mean-shift tracking algorithm is applied in\nthe feature space, which only requires inference of the latent vector for\nselected regions of interest. We validate our approach using two datasets and\ncompare our method with other existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:38:14 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Li", "Haoyu", ""], ["Shen", "Han-Wei", ""]]}, {"id": "2105.13295", "submitter": "Michael Proulx", "authors": "Michael J. Proulx, Theodoros Eracleous, Ben Spencer, Anna Passfield,\n  Alexandra de Sousa, and Ali Mohammadi", "title": "Electromagnetic actuation for a vibrotactile display: Assessing stimuli\n  complexity and usability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory substitution has influenced the design of many tactile visual\nsubstitution systems with the aim of offering visual aids for the blind. This\npaper focuses on whether a novel electromagnetic vibrotactile display, a four\nby four vibrotactile matrix of taxels, can serve as an aid for dynamic\ncommunication for visually impaired people. A mixed methods approach was used\nto firstly assess whether pattern complexity affected undergraduate\nparticipants' perceptive success, and secondly, if participants total score\npositively correlated with their perceived success ratings. A thematic analysis\nwas also conducted on participants' experiences with the vibrotactile display\nand what methods of interaction they used. The results indicated that complex\npatterns were less accurately perceived than simple and linear patterns\nrespectively, and no significant correlation was found between participants'\nscore and perceived success ratings. Additionally, most participants interacted\nwith the vibrotactile display in similar ways using one finger to feel one\ntaxel at a time; arguably, the most effective strategy from previous research.\nThis technology could have applications to navigational and communication aids\nfor the visually impaired and road users.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:45:17 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Proulx", "Michael J.", ""], ["Eracleous", "Theodoros", ""], ["Spencer", "Ben", ""], ["Passfield", "Anna", ""], ["de Sousa", "Alexandra", ""], ["Mohammadi", "Ali", ""]]}, {"id": "2105.13428", "submitter": "Arnaud Blouin", "authors": "Arnaud Blouin, Jean-Marc J\\'ez\\'equel", "title": "Interacto: A Modern User Interaction Processing Model", "comments": null, "journal-ref": "IEEE Transactions on Software Engineering, 2021", "doi": "10.1109/TSE.2021.3083321", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since most software systems provide their users with interactive features,\nbuilding user interfaces (UI) is one of the core software engineering tasks. It\nconsists in designing, implementing and testing ever more sophisticated and\nversatile ways for users to interact with software systems, and safely\nconnecting these interactions with commands querying or modifying their state.\nHowever, most UI frameworks still rely on a low level model, the bare bone UI\nevent processing model. This model was suitable for the rather simple UIs of\nthe early 80s (menus, buttons, keyboards, mouse clicks), but now exhibits major\nsoftware engineering flaws for modern, highly interactive UIs. These flaws\ninclude lack of separation of concerns, weak modularity and thus low\nreusability of code for advanced interactions, as well as low testability. To\nmitigate these flaws, we propose Interacto as a high level user interaction\nprocessing model. By reifying the concept of user interaction, Interacto makes\nit easy to design, implement and test modular and reusable advanced user\ninteractions, and to connect them to commands with built-in undo/redo support.\nTo demonstrate its applicability and generality, we briefly present two open\nsource implementations of Interacto for Java/JavaFX and TypeScript/Angular. We\nevaluate Interacto interest (1) on a real world case study, where it has been\nused since 2013, and with (2) a controlled experiment with 44 master students,\ncomparing it with traditionnal UI frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:59:49 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Blouin", "Arnaud", ""], ["J\u00e9z\u00e9quel", "Jean-Marc", ""]]}, {"id": "2105.13533", "submitter": "Zeeshan Ahmad", "authors": "Zeeshan Ahmad, Naimul Khan", "title": "Inertial Sensor Data To Image Encoding For Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) are successful deep learning models in\nthe field of computer vision. To get the maximum advantage of CNN model for\nHuman Action Recognition (HAR) using inertial sensor data, in this paper, we\nuse 4 types of spatial domain methods for transforming inertial sensor data to\nactivity images, which are then utilized in a novel fusion framework. These\nfour types of activity images are Signal Images (SI), Gramian Angular Field\n(GAF) Images, Markov Transition Field (MTF) Images and Recurrence Plot (RP)\nImages. Furthermore, for creating a multimodal fusion framework and to exploit\nactivity image, we made each type of activity images multimodal by convolving\nwith two spatial domain filters : Prewitt filter and High-boost filter.\nResnet-18, a CNN model, is used to learn deep features from multi-modalities.\nLearned features are extracted from the last pooling layer of each ReNet and\nthen fused by canonical correlation based fusion (CCF) for improving the\naccuracy of human action recognition. These highly informative features are\nserved as input to a multiclass Support Vector Machine (SVM). Experimental\nresults on three publicly available inertial datasets show the superiority of\nthe proposed method over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 01:22:52 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Khan", "Naimul", ""]]}, {"id": "2105.13634", "submitter": "Eman Alashwali", "authors": "Eman Alashwali and Fatimah Alashwali", "title": "Saudi Parents' Security and Privacy Concerns about their Children's\n  Smart Device Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we investigate Saudi parents' security and privacy concerns\nregarding their children's smart device applications (apps). To this end, we\nconducted a survey and analysed 119 responses. Our results show that Saudi\nparents expressed a high level of concern regarding their children's security\nand privacy when using smart device apps. However, they expressed higher\nconcerns about apps' content than privacy issues such as apps' requests to\naccess sensitive data. Furthermore, parents' concerns are not in line with most\nof the children's installed apps, which contain apps inappropriate for their\nage, require parental guidance, and request access to sensitive data such as\nlocation. We also compare Saudi parents' practices and concerns with those\nreported by Western (mainly from the UK) and Chinese parents in previous\nreports. We find interesting patterns and establish new relationships.\nFurthermore, Saudi and Western parents show higher levels of privacy concerns\nthan Chinese parents. The low level of privacy concerns expressed by Chinese\nparents even after being informed about possible privacy implications could be\nrelated cultural or political reasons. Finally, we tested 14 security and\nprivacy practices and concerns against high vs. low socioeconomic classes\n(parents' education, technical background, and income) to find whether there\nare significant differences between high and low classes. Out of 42 tests (14\nproperties x 3 classes) we find significant differences between high and low\nclasses in 7 tests only. While this is a positive trend overall, it is\nimportant to work on bridging these gaps. The results of this paper provide key\nfindings to identify areas of improvements and recommendations, especially for\nSaudis, which can be used by parents, developers, researchers, regulators, and\npolicy makers.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:20:50 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Alashwali", "Eman", ""], ["Alashwali", "Fatimah", ""]]}, {"id": "2105.13984", "submitter": "Pierce Burke", "authors": "Pierce Burke and Richard Klein", "title": "Confident in the Crowd: Bayesian Inference to Improve Data Labelling in\n  Crowdsourcing", "comments": "6 pages, 4 figures", "journal-ref": "2020 International SAUPEC/RobMech/PRASA Conference, 2020, pp. 1-6", "doi": "10.1109/SAUPEC/RobMech/PRASA48453.2020.9041099", "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increased interest in machine learning and big data problems, the\nneed for large amounts of labelled data has also grown. However, it is often\ninfeasible to get experts to label all of this data, which leads many\npractitioners to crowdsourcing solutions. In this paper, we present new\ntechniques to improve the quality of the labels while attempting to reduce the\ncost. The naive approach to assigning labels is to adopt a majority vote\nmethod, however, in the context of data labelling, this is not always ideal as\ndata labellers are not equally reliable. One might, instead, give higher\npriority to certain labellers through some kind of weighted vote based on past\nperformance. This paper investigates the use of more sophisticated methods,\nsuch as Bayesian inference, to measure the performance of the labellers as well\nas the confidence of each label. The methods we propose follow an iterative\nimprovement algorithm which attempts to use the least amount of workers\nnecessary to achieve the desired confidence in the inferred label. This paper\nexplores simulated binary classification problems with simulated workers and\nquestions to test the proposed methods. Our methods outperform the standard\nvoting methods in both cost and accuracy while maintaining higher reliability\nwhen there is disagreement within the crowd.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 17:09:45 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Burke", "Pierce", ""], ["Klein", "Richard", ""]]}, {"id": "2105.14066", "submitter": "Florian M. Farke", "authors": "Florian M. Farke (1), David G. Balash (2), Maximilian Golla (3),\n  Markus D\\\"urmuth (1), Adam J. Aviv (2) ((1) Ruhr University Bochum, (2) The\n  George Washington University, (3) Max Planck Institute for Security and\n  Privacy)", "title": "Are Privacy Dashboards Good for End Users? Evaluating User Perceptions\n  and Reactions to Google's My Activity (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy dashboards and transparency tools help users review and manage the\ndata collected about them online. Since 2016, Google has offered such a tool,\nMy Activity, which allows users to review and delete their activity data from\nGoogle services. We conducted an online survey with $n = 153$ participants to\nunderstand if Google's My Activity, as an example of a privacy transparency\ntool, increases or decreases end-users' concerns and benefits regarding data\ncollection. While most participants were aware of Google's data collection, the\nvolume and detail was surprising, but after exposure to My Activity,\nparticipants were significantly more likely to be both less concerned about\ndata collection and to view data collection more beneficially. Only $25\\,\\%$\nindicated that they would change any settings in the My Activity service or\nchange any behaviors. This suggests that privacy transparency tools are quite\nbeneficial for online services as they garner trust with their users and\nimprove their perceptions without necessarily changing users' behaviors. At the\nsame time, though, it remains unclear if such transparency tools actually\nimprove end user privacy by sufficiently assisting or motivating users to\nchange or review data collection settings.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:08:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Farke", "Florian M.", ""], ["Balash", "David G.", ""], ["Golla", "Maximilian", ""], ["D\u00fcrmuth", "Markus", ""], ["Aviv", "Adam J.", ""]]}, {"id": "2105.14083", "submitter": "Glenn Dawson", "authors": "Glenn Dawson, Robi Polikar", "title": "Rethinking Noisy Label Models: Labeler-Dependent Noise with Adversarial\n  Awareness", "comments": "9 pages, 3 figures, 3 algorithms. Currently under blind review at\n  NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most studies on learning from noisy labels rely on unrealistic models of\ni.i.d. label noise, such as class-conditional transition matrices. More recent\nwork on instance-dependent noise models are more realistic, but assume a single\ngenerative process for label noise across the entire dataset. We propose a more\nprincipled model of label noise that generalizes instance-dependent noise to\nmultiple labelers, based on the observation that modern datasets are typically\nannotated using distributed crowdsourcing methods. Under our labeler-dependent\nmodel, label noise manifests itself under two modalities: natural error of\ngood-faith labelers, and adversarial labels provided by malicious actors. We\npresent two adversarial attack vectors that more accurately reflect the label\nnoise that may be encountered in real-world settings, and demonstrate that\nunder our multimodal noisy labels model, state-of-the-art approaches for\nlearning from noisy labels are defeated by adversarial label attacks. Finally,\nwe propose a multi-stage, labeler-aware, model-agnostic framework that reliably\nfilters noisy labels by leveraging knowledge about which data partitions were\nlabeled by which labeler, and show that our proposed framework remains robust\neven in the presence of extreme adversarial label noise.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:58:18 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 01:40:37 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Dawson", "Glenn", ""], ["Polikar", "Robi", ""]]}, {"id": "2105.14116", "submitter": "Daniel Steinberg", "authors": "Daniel Steinberg, Paul Munro", "title": "Visualizing Representations of Adversarially Perturbed Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that deep learning models are vulnerable to adversarial\nattacks. We seek to further understand the consequence of such attacks on the\nintermediate activations of neural networks. We present an evaluation metric,\nPOP-N, which scores the effectiveness of projecting data to N dimensions under\nthe context of visualizing representations of adversarially perturbed inputs.\nWe conduct experiments on CIFAR-10 to compare the POP-2 score of several\ndimensionality reduction algorithms across various adversarial attacks.\nFinally, we utilize the 2D data corresponding to high POP-2 scores to generate\nexample visualizations.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:34:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Steinberg", "Daniel", ""], ["Munro", "Paul", ""]]}, {"id": "2105.14134", "submitter": "Sudarshan Lamkhede", "authors": "Sudarshan Lamkhede and Christoph Kofler", "title": "Recommendations and Results Organization in Netflix Search", "comments": "Extended abstract submitted to RecSys 2021 Industry Track. 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Personalized recommendations on the Netflix Homepage are based on a user's\nviewing habits and the behavior of similar users. These recommendations,\norganized for efficient browsing, enable users to discover the next great video\nto watch and enjoy without additional input or an explicit expression of their\nintents or goals. The Netflix Search experience, on the other hand, allows\nusers to take active control of discovering new videos by explicitly expressing\ntheir entertainment needs via search queries.\n  In this talk, we discuss the importance of producing search results that go\nbeyond traditional keyword-matches to effectively satisfy users' search needs\nin the Netflix entertainment setting. Motivated by users' various search\nintents, we highlight the necessity to improve Search by applying approaches\nthat have historically powered the Homepage. Specifically, we discuss our\napproach to leverage recommendations in the context of Search and to\neffectively organize search results to provide a product experience that\nmeaningfully adds value for our users.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 23:01:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lamkhede", "Sudarshan", ""], ["Kofler", "Christoph", ""]]}, {"id": "2105.14171", "submitter": "Weishen Pan", "authors": "Weishen Pan, Changshui Zhang", "title": "The Definitions of Interpretability and Learning of Interpretable Models", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning algorithms getting adopted in an ever-increasing number\nof applications, interpretation has emerged as a crucial desideratum. In this\npaper, we propose a mathematical definition for the human-interpretable model.\nIn particular, we define interpretability between two information process\nsystems. If a prediction model is interpretable by a human recognition system\nbased on the above interpretability definition, the prediction model is defined\nas a completely human-interpretable model. We further design a practical\nframework to train a completely human-interpretable model by user interactions.\nExperiments on image datasets show the advantages of our proposed model in two\naspects: 1) The completely human-interpretable model can provide an entire\ndecision-making process that is human-understandable; 2) The completely\nhuman-interpretable model is more robust against adversarial attacks.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 01:44:12 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Pan", "Weishen", ""], ["Zhang", "Changshui", ""]]}, {"id": "2105.14362", "submitter": "Alberto Garcia-Robledo Ph.D.", "authors": "Alberto Garcia-Robledo and Mahboobeh Zangiabady", "title": "Dash Sylvereye: A WebGL-powered Library for Dashboard-driven\n  Visualization of Large Street Networks", "comments": "Submitted to IEEE Access on May 8, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-of-the-art open network visualization tools like Gephi, KeyLines, and\nCytoscape are not suitable for studying street networks with thousands of roads\nsince they do not support simultaneously polylines for edges, navigable maps,\nGPU-accelerated rendering, interactivity, and the means for visualizing\nmultivariate data. The present paper presents Dash Sylvereye: a new Python\nlibrary to produce interactive visualizations of primal street networks on top\nof tiled web maps to fill this gap. Dash Sylvereye can render large street\ngraphs in commodity computers by exploiting WebGL for GPU acceleration. Dash\nSylvereye also provides convenient functions to easily import OpenStreetMap\nstreet topologies obtained with the OSMnx library. Thanks to its integration\nwith the Dash framework, Dash Sylvereye can be used to develop web dashboards\naround temporal and multivariate street data by coordinating the various\nelements of a Dash Sylvereye visualization with other plotting and UI\ncomponents provided by Dash. We conduct experiments to assess the performance\nof Dash Sylvereye on a commodity computer in terms of animation CPU time and\nframes per second. To further illustrate the features of Dash Sylvereye, we\nalso describe a web dashboard application that exploits Dash Sylvereye for the\nanalysis of a SUMO vehicle traffic simulation.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 19:39:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Garcia-Robledo", "Alberto", ""], ["Zangiabady", "Mahboobeh", ""]]}, {"id": "2105.14457", "submitter": "David Chuan-En Lin", "authors": "David Chuan-En Lin, Nikolas Martelaro", "title": "Learning Personal Style from Few Examples", "comments": null, "journal-ref": null, "doi": "10.1145/3461778.3462115", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key task in design work is grasping the client's implicit tastes. Designers\noften do this based on a set of examples from the client. However, recognizing\na common pattern among many intertwining variables such as color, texture, and\nlayout and synthesizing them into a composite preference can be challenging. In\nthis paper, we leverage the pattern recognition capability of computational\nmodels to aid in this task. We offer a set of principles for computationally\nlearning personal style. The principles are manifested in PseudoClient, a deep\nlearning framework that learns a computational model for personal graphic\ndesign style from only a handful of examples. In several experiments, we found\nthat PseudoClient achieves a 79.40% accuracy with only five positive and\nnegative examples, outperforming several alternative methods. Finally, we\ndiscuss how PseudoClient can be utilized as a building block to support the\ndevelopment of future design applications.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 08:04:42 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 12:30:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lin", "David Chuan-En", ""], ["Martelaro", "Nikolas", ""]]}, {"id": "2105.14465", "submitter": "Pradipta Biswas", "authors": "Gowdham Prabhakar and Pradipta Biswas", "title": "A Brief Survey on Interactive Automotive UI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automotive User Interface (AutoUI) is relatively a new discipline in the\ncontext of both Transportation Engineering and Human Machine Interaction (HMI).\nIt covers various HMI aspects both inside and outside vehicle ranging from\noperating the vehicle itself, undertaking various secondary tasks, driver\nbehaviour analysis, cognitive load estimation and so on. This review paper\ndiscusses various interactive HMI inside a vehicle used for undertaking\nsecondary tasks. We divided recent HMIs through four sections on virtual touch\ninterfaces, wearable devices, speech recognition and non-visual interfaces and\neye gaze controlled systems. Finally, we summarized advantages and\ndisadvantages of various technologies.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 08:37:35 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Prabhakar", "Gowdham", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2105.14470", "submitter": "Seyed Mojtaba Hosseini Bamakan", "authors": "Seyed Mojtaba Hosseini Bamakan, Aref Toghroljerdib, Peyman Tirandazib", "title": "Internet of Everything Driven Neuromarketing: Key Technologies and\n  Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Preserving customers' expectations and understanding factors affecting their\npurchasing decisions would significantly affect designing effective marketing\nand advertising strategies. However, constantly and swiftly changing the\ncustomers' interests and consumption behaviors, make it inevitable to utilize\nthe sophisticated tools and approaches based on advanced technologies. Among\nthem, neuromarketing by measuring the customers' physiological and neural\nsignals, studying the consumers' cognitive, and affective responses to\nmarketing stimulus, provides deep insight into the customers' motivations,\npreferences, and decisions. Recently, the Internet of Everything (IoE) has\nbrought many new opportunities to the industry and has attracted the attention\nof many researchers in recent years. The main objective of this paper is to\naddress how the Internet of Everything (IoE) would empower neuromarketing\ntechniques. In particular, applications of IoE gadgets and devices in eleven\ngroups of neuromarketing techniques are discussed to present numerous solutions\nthat would help meet this goal. Moreover, we present an in-depth understanding\nof current research issues as well as emerging trends.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:09:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bamakan", "Seyed Mojtaba Hosseini", ""], ["Toghroljerdib", "Aref", ""], ["Tirandazib", "Peyman", ""]]}, {"id": "2105.14493", "submitter": "Tamer Olmez", "authors": "Nuri Korkan, Tamer Olmez, Zumray Dokur", "title": "Generating Ten BCI Commands Using Four Simple Motor Imageries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The brain computer interface (BCI) systems are utilized for transferring\ninformation among humans and computers by analyzing electroencephalogram (EEG)\nrecordings.The process of mentally previewing a motor movement without\ngenerating the corporal output can be described as motor imagery (MI).In this\nemerging research field, the number of commands is also limited in relation to\nthe number of MI tasks; in the current literature, mostly two or four commands\n(classes) are studied. As a solution to this problem, it is recommended to use\nmental tasks as well as MI tasks. Unfortunately, the use of this approach\nreduces the classification performance of MI EEG signals. The fMRI analyses\nshow that the resources in the brain associated with the motor imagery can be\nactivated independently. It is assumed that the brain activity induced by the\nMI of the combination of body parts corresponds to the superposition of the\nactivities generated during each body parts's simple MI. In this study, in\norder to create more than four BCI commands, we suggest to generate combined MI\nEEG signals artificially by using left hand, right hand, tongue, and feet motor\nimageries in pairs. A maximum of ten different BCI commands can be generated by\nusing four motor imageries in pairs.This study aims to achieve high\nclassification performances for BCI commands produced from four motor imageries\nby implementing a small-sized deep neural network (DNN).The presented method is\nevaluated on the four-class datasets of BCI Competitions III and IV, and an\naverage classification performance of 81.8% is achieved for ten classes. The\nabove assumption is also validated on a different dataset which consists of\nsimple and combined MI EEG signals acquired in real time. Trained with the\nartificially generated combined MI EEG signals, DivFE resulted in an average of\n76.5% success rate for the combined MI EEG signals acquired in real-time.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 10:34:41 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Korkan", "Nuri", ""], ["Olmez", "Tamer", ""], ["Dokur", "Zumray", ""]]}, {"id": "2105.14619", "submitter": "Noel Warford", "authors": "Noel Warford (1), Collins W. Munyendo (2), Ashna Mediratta (1), Adam\n  J. Aviv (2), and Michelle L. Mazurek (1) ((1) University of Maryland, (2) The\n  George Washington University)", "title": "Strategies and Perceived Risks of Sending Sensitive Documents", "comments": "25 pages, to appear in USENIX Security Symposium 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  People are frequently required to send documents, forms, or other materials\ncontaining sensitive data (e.g., personal information, medical records,\nfinancial data) to remote parties, sometimes without a formal procedure to do\nso securely. The specific transmission mechanisms end up relying on the\nknowledge and preferences of the parties involved. Through two online surveys\n($n=60$ and $n=250$), we explore the various methods used to transmit sensitive\ndocuments, as well as the perceived risk and satisfaction with those methods.\nWe find that users are more likely to recognize risk to data-at-rest after\nreceipt (but not at the sender, namely, themselves). When not using an online\nportal provided by the recipient, participants primarily envision transmitting\nsensitive documents in person or via email, and have little experience using\nsecure, privacy-preserving alternatives. Despite recognizing general risks,\nparticipants express high privacy satisfaction and convenience with actually\nexperienced situations. These results suggest opportunities to design new\nsolutions to promote securely sending sensitive materials, perhaps as new\nutilities within standard email workflows.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 20:31:55 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Warford", "Noel", ""], ["Munyendo", "Collins W.", ""], ["Mediratta", "Ashna", ""], ["Aviv", "Adam J.", ""], ["Mazurek", "Michelle L.", ""]]}, {"id": "2105.14680", "submitter": "Franklin Mingzhe Li", "authors": "Wei Sun, Franklin Mingzhe Li, Congshu Huang, Zhenyu Lei, Benjamin\n  Steeper, Songyun Tao, Feng Tian, Cheng Zhang", "title": "ThumbTrak: Recognizing Micro-finger Poses Using a Ring with Proximity\n  Sensing", "comments": "MobileHCI '21: The ACM International Conference on Mobile\n  Human-Computer Interaction, September 27 - October 1, 2021, Toulouse, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ThumbTrak is a novel wearable input device that recognizes 12 micro-finger\nposes in real-time. Poses are characterized by the thumb touching each of the\n12 phalanges on the hand. It uses a thumb-ring, built with a flexible printed\ncircuit board, which hosts nine proximity sensors. Each sensor measures the\ndistance from the thumb to various parts of the palm or other fingers.\nThumbTrak uses a support-vector-machine (SVM) model to classify finger poses\nbased on distance measurements in real-time. A user study with ten participants\nshowed that ThumbTrak could recognize 12 micro finger poses with an average\naccuracy of 93.6%. We also discuss potential opportunities and challenges in\napplying ThumbTrak in real-world applications.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 02:47:56 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sun", "Wei", ""], ["Li", "Franklin Mingzhe", ""], ["Huang", "Congshu", ""], ["Lei", "Zhenyu", ""], ["Steeper", "Benjamin", ""], ["Tao", "Songyun", ""], ["Tian", "Feng", ""], ["Zhang", "Cheng", ""]]}, {"id": "2105.14779", "submitter": "Shammur Absar Chowdhury", "authors": "Shammur Absar Chowdhury, Amir Hussein, Ahmed Abdelali, Ahmed Ali", "title": "Towards One Model to Rule All: Multilingual Strategy for Dialectal\n  Code-Switching Arabic ASR", "comments": "Accepted in INTERSPEECH 2021, Multilingual ASR, Multi-dialectal ASR,\n  Code-Switching ASR, Arabic ASR, Conformer, Transformer, E2E ASR, Speech\n  Recognition, ASR, Arabic, English, French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the advent of globalization, there is an increasing demand for\nmultilingual automatic speech recognition (ASR), handling language and\ndialectal variation of spoken content. Recent studies show its efficacy over\nmonolingual systems. In this study, we design a large multilingual end-to-end\nASR using self-attention based conformer architecture. We trained the system\nusing Arabic (Ar), English (En) and French (Fr) languages. We evaluate the\nsystem performance handling: (i) monolingual (Ar, En and Fr); (ii)\nmulti-dialectal (Modern Standard Arabic, along with dialectal variation such as\nEgyptian and Moroccan); (iii) code-switching -- cross-lingual (Ar-En/Fr) and\ndialectal (MSA-Egyptian dialect) test cases, and compare with current\nstate-of-the-art systems. Furthermore, we investigate the influence of\ndifferent embedding/character representations including character vs\nword-piece; shared vs distinct input symbol per language. Our findings\ndemonstrate the strength of such a model by outperforming state-of-the-art\nmonolingual dialectal Arabic and code-switching Arabic ASR.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:20:38 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 09:16:53 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chowdhury", "Shammur Absar", ""], ["Hussein", "Amir", ""], ["Abdelali", "Ahmed", ""], ["Ali", "Ahmed", ""]]}, {"id": "2105.14787", "submitter": "Seo-Hyun Lee", "authors": "Seo-Hyun Lee, Young-Eun Lee, Seong-Whan Lee", "title": "Voice of Your Brain: Cognitive Representations of Imagined Speech,Overt\n  Speech, and Speech Perception Based on EEG", "comments": "5 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every people has their own voice, likewise, brain signals dis-play distinct\nneural representations for each individual. Al-though recent studies have\nrevealed the robustness of speech-related paradigms for efficient\nbrain-computer interface, the dis-tinction on their cognitive representations\nwith practical usabil-ity still remains to be discovered. Herein, we\ninvestigate the dis-tinct brain patterns from electroencephalography (EEG)\nduringimagined speech, overt speech, and speech perception in termsof subject\nvariations with its practical use of speaker identifica-tion from single\nchannel EEG. We performed classification ofnine subjects using deep neural\nnetwork that captures temporal-spectral-spatial features from EEG of imagined\nspeech, overtspeech, and speech perception. Furthermore, we demonstratedthe\nunderlying neural features of individual subjects while per-forming imagined\nspeech by comparing the functional connec-tivity and the EEG envelope features.\nOur results demonstratethe possibility of subject identification from single\nchannel EEGof imagined speech and overt speech. Also, the comparison ofthe\nthree speech-related paradigms will provide valuable infor-mation for the\npractical use of speech-related brain signals inthe further studies.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:25:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lee", "Seo-Hyun", ""], ["Lee", "Young-Eun", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2105.14815", "submitter": "Thiemo Wambsganss", "authors": "Thiemo Wambsganss, Christina Niklaus, Matthias S\\\"ollner, Siegfried\n  Handschuh and Jan Marco Leimeister", "title": "Supporting Cognitive and Emotional Empathic Writing of Students", "comments": "to be published in The Joint Conference of the 59th Annual Meeting of\n  the Association for Computational Linguistics and the 11th International\n  Joint Conference on Natural Language Processing (ACL-IJCNLP 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present an annotation approach to capturing emotional and cognitive\nempathy in student-written peer reviews on business models in German. We\npropose an annotation scheme that allows us to model emotional and cognitive\nempathy scores based on three types of review components. Also, we conducted an\nannotation study with three annotators based on 92 student essays to evaluate\nour annotation scheme. The obtained inter-rater agreement of {\\alpha}=0.79 for\nthe components and the multi-{\\pi}=0.41 for the empathy scores indicate that\nthe proposed annotation scheme successfully guides annotators to a substantial\nto moderate agreement. Moreover, we trained predictive models to detect the\nannotated empathy structures and embedded them in an adaptive writing support\nsystem for students to receive individual empathy feedback independent of an\ninstructor, time, and location. We evaluated our tool in a peer learning\nexercise with 58 students and found promising results for perceived empathy\nskill learning, perceived feedback accuracy, and intention to use. Finally, we\npresent our freely available corpus of 500 empathy-annotated, student-written\npeer reviews on business models and our annotation guidelines to encourage\nfuture research on the design and development of empathy support systems.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 09:18:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wambsganss", "Thiemo", ""], ["Niklaus", "Christina", ""], ["S\u00f6llner", "Matthias", ""], ["Handschuh", "Siegfried", ""], ["Leimeister", "Jan Marco", ""]]}, {"id": "2105.14901", "submitter": "Marie-Laure Zollinger", "authors": "Marie-Laure Zollinger and Verena Distler and Peter B. Roenne and Peter\n  Y. A. Ryan and Carine Lallemand and Vincent Koenig", "title": "User Experience Design for E-Voting: How mental models align with\n  security mechanisms", "comments": "E-Vote-ID 2019 TalTech Proceedings", "journal-ref": "Fourth International Joint Conference on Electronic Voting\n  E-Vote-ID 2019, p187--202", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a mobile application for vote-casting and\nvote-verification based on the Selene e-voting protocol and explains how it was\ndeveloped and implemented using the User Experience Design process. The\nresulting interface was tested with 38 participants, and user experience data\nwas collected via questionnaires and semi-structured interviews on user\nexperience and perceived security. Results concerning the impact of displaying\nsecurity mechanisms on UX were presented in a complementary paper. Here we\nexpand on this analysis by studying the mental models revealed during the\ninterviews and compare them with theoretical security notions. Finally, we\npropose a list of improvements for designs of future voting protocols.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:56:09 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 08:15:27 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Zollinger", "Marie-Laure", ""], ["Distler", "Verena", ""], ["Roenne", "Peter B.", ""], ["Ryan", "Peter Y. A.", ""], ["Lallemand", "Carine", ""], ["Koenig", "Vincent", ""]]}, {"id": "2105.14915", "submitter": "Hamed Rahimi", "authors": "Hamed Rahimi, Iago Felipe Trentin, Fano Ramparany, Olivier Boissier", "title": "SMASH: a Semantic-enabled Multi-agent Approach for Self-adaptation of\n  Human-centered IoT", "comments": "Submitted to PAAMS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, IoT devices have an enlarging scope of activities spanning from\nsensing, computing to acting and even more, learning, reasoning and planning.\nAs the number of IoT applications increases, these objects are becoming more\nand more ubiquitous. Therefore, they need to adapt their functionality in\nresponse to the uncertainties of their environment to achieve their goals. In\nHuman-centered IoT, objects and devices have direct interactions with human\nbeings and have access to online contextual information. Self-adaptation of\nsuch applications is a crucial subject that needs to be addressed in a way that\nrespects human goals and human values. Hence, IoT applications must be equipped\nwith self-adaptation techniques to manage their run-time uncertainties locally\nor in cooperation with each other. This paper presents SMASH: a multi-agent\napproach for self-adaptation of IoT applications in human-centered\nenvironments. In this paper, we have considered the Smart Home as the case\nstudy of smart environments. SMASH agents are provided with a 4-layer\narchitecture based on the BDI agent model that integrates human values with\ngoal-reasoning, planning, and acting. It also takes advantage of a\nsemantic-enabled platform called Home'In to address interoperability issues\namong non-identical agents and devices with heterogeneous protocols and data\nformats. This approach is compared with the literature and is validated by\ndeveloping a scenario as the proof of concept. The timely responses of SMASH\nagents show the feasibility of the proposed approach in human-centered\nenvironments.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 12:33:27 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Rahimi", "Hamed", ""], ["Trentin", "Iago Felipe", ""], ["Ramparany", "Fano", ""], ["Boissier", "Olivier", ""]]}, {"id": "2105.14943", "submitter": "Christof Paar", "authors": "Steffen Becker and Carina Wiesen and Nils Albartus and Nikol Rummel\n  and Christof Paar", "title": "An Exploratory Study of Hardware Reverse Engineering Technical and\n  Cognitive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the internals of Integrated Circuits (ICs), referred to as\nHardware Reverse Engineering (HRE), is of interest to both legitimate and\nmalicious parties. HRE is a complex process in which semi-automated steps are\ninterwoven with human sense-making processes. Currently, little is known about\nthe technical and cognitive processes which determine the success of HRE.\n  This paper performs an initial investigation on how reverse engineers solve\nproblems, how manual and automated analysis methods interact, and which\ncognitive factors play a role. We present the results of an exploratory\nbehavioral study with eight participants that was conducted after they had\ncompleted a 14-week training. We explored the validity of our findings by\ncomparing them with the behavior (strategies applied and solution time) of an\nHRE expert. The participants were observed while solving a realistic HRE task.\nWe tested cognitive abilities of our participants and collected large sets of\nbehavioral data from log files. By comparing the least and most efficient\nreverse engineers, we were able to observe successful strategies. Moreover, our\nanalyses suggest a phase model for reverse engineering, consisting of three\nphases. Our descriptive results further indicate that the cognitive factor\nWorking Memory (WM) might play a role in efficiently solving HRE problems. Our\nexploratory study builds the foundation for future research in this topic and\noutlines ideas for designing cognitively difficult countermeasures (\"cognitive\nobfuscation\") against HRE.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:21:52 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Becker", "Steffen", ""], ["Wiesen", "Carina", ""], ["Albartus", "Nils", ""], ["Rummel", "Nikol", ""], ["Paar", "Christof", ""]]}, {"id": "2105.14944", "submitter": "Anh Nguyen", "authors": "Giang Nguyen, Daeyoung Kim, Anh Nguyen", "title": "The effectiveness of feature attribution methods and its correlation\n  with automatic evaluation scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explaining the decisions of an Artificial Intelligence (AI) model is\nincreasingly critical in many real-world, high-stake applications. Hundreds of\npapers have either proposed new feature attribution methods, discussed or\nharnessed these tools in their work. However, despite humans being the target\nend-users, most attribution methods were only evaluated on proxy\nautomatic-evaluation metrics. In this paper, we conduct the first, large-scale\nuser study on 320 lay and 11 expert users to shed light on the effectiveness of\nstate-of-the-art attribution methods in assisting humans in ImageNet\nclassification, Stanford Dogs fine-grained classification, and these two tasks\nbut when the input image contains adversarial perturbations. We found that, in\noverall, feature attribution is surprisingly not more effective than showing\nhumans nearest training-set examples. On a hard task of fine-grained dog\ncategorization, presenting attribution maps to humans does not help, but\ninstead hurts the performance of human-AI teams compared to AI alone.\nImportantly, we found automatic attribution-map evaluation measures to\ncorrelate poorly with the actual human-AI team performance. Our findings\nencourage the community to rigorously test their methods on the downstream\nhuman-in-the-loop applications and to rethink the existing evaluation metrics.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:23:50 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 22:00:51 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Nguyen", "Giang", ""], ["Kim", "Daeyoung", ""], ["Nguyen", "Anh", ""]]}, {"id": "2105.14980", "submitter": "Xin Zhang", "authors": "Xin Zhang, Guangwei Xu, Yueheng Sun, Meishan Zhang, Pengjun Xie", "title": "Crowdsourcing Learning as Domain Adaptation: A Case Study on Named\n  Entity Recognition", "comments": "Accepted by ACL-IJCNLP 2021 (long paper), accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is regarded as one prospective solution for effective\nsupervised learning, aiming to build large-scale annotated training data by\ncrowd workers. Previous studies focus on reducing the influences from the\nnoises of the crowdsourced annotations for supervised models. We take a\ndifferent point in this work, regarding all crowdsourced annotations as\ngold-standard with respect to the individual annotators. In this way, we find\nthat crowdsourcing could be highly similar to domain adaptation, and then the\nrecent advances of cross-domain methods can be almost directly applied to\ncrowdsourcing. Here we take named entity recognition (NER) as a study case,\nsuggesting an annotator-aware representation learning model that inspired by\nthe domain adaptation methods which attempt to capture effective domain-aware\nfeatures. We investigate both unsupervised and supervised crowdsourcing\nlearning, assuming that no or only small-scale expert annotations are\navailable. Experimental results on a benchmark crowdsourced NER dataset show\nthat our method is highly effective, leading to a new state-of-the-art\nperformance. In addition, under the supervised setting, we can achieve\nimpressive performance gains with only a very small scale of expert\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:11:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhang", "Xin", ""], ["Xu", "Guangwei", ""], ["Sun", "Yueheng", ""], ["Zhang", "Meishan", ""], ["Xie", "Pengjun", ""]]}, {"id": "2105.15029", "submitter": "Andrea Fronzetti Colladon PhD", "authors": "P. A. Gloor, A. Fronzetti Colladon, F. Grippa, P. Budner, J. Eirich", "title": "Aristotle Said \"Happiness is a State of Activity\" -- Predicting Mood\n  through Body Sensing with Smartwatches", "comments": null, "journal-ref": "Journal of Systems Science and Systems Engineering 27(5), 586-612\n  (2018)", "doi": "10.1007/s11518-018-5383-7", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We measure and predict states of Activation and Happiness using a body\nsensing application connected to smartwatches. Through the sensors of\ncommercially available smartwatches we collect individual mood states and\ncorrelate them with body sensing data such as acceleration, heart rate, light\nlevel data, and location, through the GPS sensor built into the smartphone\nconnected to the smartwatch. We polled users on the smartwatch for seven weeks\nfour times per day asking for their mood state. We found that both Happiness\nand Activation are negatively correlated with heart beats and with the levels\nof light. People tend to be happier when they are moving more intensely and are\nfeeling less activated during weekends. We also found that people with a lower\nConscientiousness and Neuroticism and higher Agreeableness tend to be happy\nmore frequently. In addition, more Activation can be predicted by lower\nOpenness to experience and higher Agreeableness and Conscientiousness. Lastly,\nwe find that tracking people's geographical coordinates might play an important\nrole in predicting Happiness and Activation. The methodology we propose is a\nfirst step towards building an automated mood tracking system, to be used for\nbetter teamwork and in combination with social network analysis studies.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 14:14:04 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gloor", "P. A.", ""], ["Colladon", "A. Fronzetti", ""], ["Grippa", "F.", ""], ["Budner", "P.", ""], ["Eirich", "J.", ""]]}, {"id": "2105.15030", "submitter": "Chandresh Maurya", "authors": "Chandresh Kumar Maurya, Seemandhar Jain, Vishal Thakre", "title": "Digital Contact Tracing for Covid 19", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID19 pandemic created a worldwide emergency as it is estimated that\nsuch a large number of infections are due to human-to-human transmission of the\nCOVID19. As a necessity, there is a need to track users who came in contact\nwith users having travel history, asymptomatic and not yet symptomatic, but\nthey can be in the future. To solve this problem, the present work proposes a\nsolution for contact tracing based on assisted GPS and cloud computing\ntechnologies. An application is developed to collect each user's assisted GPS\ncoordinates once all the users install this application. This application\nperiodically sends assisted GPS data to the cloud. To determine which devices\nare within the permissible limit of 5m, we perform clustering over assisted GPS\ncoordinates and track the clusters for about t mins to allow the measure of\nspread. We assume that it takes around 3 or 5 mins to get the virus from an\ninfected object. For clustering, the proposed M way like tree data structure\nstores the assisted GPS coordinates in degree, minute, and second format. Thus,\nevery user is mapped to a leaf node of the tree. We split the \"seconds\" part of\nthe assisted GPS location into m equal parts, which amount to d meter in\nlatitude(longitude). Hence, two users who are within d meter range will map to\nthe same leaf node. Thus, by mapping assisted GPS locations every t mins, we\ncan find out how many users came in contact with a particular user for at least\nt mins. Our work's salient feature is that it runs in linear time O(n) for n\nusers in the static case, i.e., when users are not moving. We also propose a\nvariant of our solution to handle the dynamic case, that is, when users are\nmoving. Besides, the proposed solution offers potential hotspot detection and\nsafe-route recommendation as an additional feature, and proof of concept is\npresented through experiments on simulated data of 10M users.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 07:03:50 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Maurya", "Chandresh Kumar", ""], ["Jain", "Seemandhar", ""], ["Thakre", "Vishal", ""]]}, {"id": "2105.15110", "submitter": "Martin Gerlach", "authors": "Martin Gerlach and Marshall Miller and Rita Ho and Kosta Harlan and\n  Djellel Difallah", "title": "A Multilingual Entity Linking System for Wikipedia with a\n  Machine-in-the-Loop Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperlinks constitute the backbone of the Web; they enable user navigation,\ninformation discovery, content ranking, and many other crucial services on the\nInternet. In particular, hyperlinks found within Wikipedia allow the readers to\nnavigate from one page to another to expand their knowledge on a given subject\nof interest or to discover a new one. However, despite Wikipedia editors'\nefforts to add and maintain its content, the distribution of links remains\nsparse in many language editions. This paper introduces a machine-in-the-loop\nentity linking system that can comply with community guidelines for adding a\nlink and aims at increasing link coverage in new pages and wiki-projects with\nlow-resources. To tackle these challenges, we build a context and language\nagnostic entity linking model that combines data collected from millions of\nanchors found across wiki-projects, as well as billions of users' reading\nsessions. We develop an interactive recommendation interface that proposes\ncandidate links to editors who can confirm, reject, or adapt the recommendation\nwith the overall aim of providing a more accessible editing experience for\nnewcomers through structured tasks. Our system's design choices were made in\ncollaboration with members of several language communities. When the system is\nimplemented as part of Wikipedia, its usage by volunteer editors will help us\nbuild a continuous evaluation dataset with active feedback. Our experimental\nresults show that our link recommender can achieve a precision above 80% while\nensuring a recall of at least 50% across 6 languages covering different sizes,\ncontinents, and families.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 16:29:42 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Gerlach", "Martin", ""], ["Miller", "Marshall", ""], ["Ho", "Rita", ""], ["Harlan", "Kosta", ""], ["Difallah", "Djellel", ""]]}]