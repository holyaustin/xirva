[{"id": "2004.00077", "submitter": "Michael Hersche", "authors": "Xiaying Wang, Michael Hersche, Batuhan T\\\"omekce, Burak Kaya, Michele\n  Magno, Luca Benini", "title": "An Accurate EEGNet-based Motor-Imagery Brain-Computer Interface for\n  Low-Power Edge Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an accurate and robust embedded motor-imagery\nbrain-computer interface (MI-BCI). The proposed novel model, based on EEGNet,\nmatches the requirements of memory footprint and computational resources of\nlow-power microcontroller units (MCUs), such as the ARM Cortex-M family.\nFurthermore, the paper presents a set of methods, including temporal\ndownsampling, channel selection, and narrowing of the classification window, to\nfurther scale down the model to relax memory requirements with negligible\naccuracy degradation. Experimental results on the Physionet EEG Motor\nMovement/Imagery Dataset show that standard EEGNet achieves 82.43%, 75.07%, and\n65.07% classification accuracy on 2-, 3-, and 4-class MI tasks in global\nvalidation, outperforming the state-of-the-art (SoA) convolutional neural\nnetwork (CNN) by 2.05%, 5.25%, and 5.48%. Our novel method further scales down\nthe standard EEGNet at a negligible accuracy loss of 0.31% with 7.6x memory\nfootprint reduction and a small accuracy loss of 2.51% with 15x reduction. The\nscaled models are deployed on a commercial Cortex-M4F MCU taking 101ms and\nconsuming 4.28mJ per inference for operating the smallest model, and on a\nCortex-M7 with 44ms and 18.1mJ per inference for the medium-sized model,\nenabling a fully autonomous, wearable, and accurate low-power BCI.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 19:52:05 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 06:39:45 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wang", "Xiaying", ""], ["Hersche", "Michael", ""], ["T\u00f6mekce", "Batuhan", ""], ["Kaya", "Burak", ""], ["Magno", "Michele", ""], ["Benini", "Luca", ""]]}, {"id": "2004.00101", "submitter": "Hye Won Chung", "authors": "Doyeon Kim and Hye Won Chung", "title": "Crowdsourced Labeling for Worker-Task Specialization Model", "comments": "To appear at IEEE International Symposium on Information Theory\n  (ISIT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider crowdsourced labeling under a $d$-type worker-task specialization\nmodel, where each worker and task is associated with one particular type among\na finite set of types and a worker provides a more reliable answer to tasks of\nthe matched type than to tasks of unmatched types. We design an inference\nalgorithm that recovers binary task labels (up to any given recovery accuracy)\nby using worker clustering, worker skill estimation and weighted majority\nvoting. The designed inference algorithm does not require any information about\nworker/task types, and achieves any targeted recovery accuracy with the best\nknown performance (minimum number of queries per task).\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2020 13:27:03 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 06:55:56 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Kim", "Doyeon", ""], ["Chung", "Hye Won", ""]]}, {"id": "2004.00167", "submitter": "Ruikun Luo", "authors": "Ruikun Luo, Yifan Weng, Yifan Wang, Paramsothy Jayakumar, Mark J.\n  Brudnak, Victor Paul, Vishnu R. Desaraju, Jeffrey L. Stein, Tulga Ersal, X.\n  Jessie Yang", "title": "A Workload Adaptive Haptic Shared Control Scheme for Semi-Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Haptic shared control is used to manage the control authority allocation\nbetween a human and an autonomous agent in semi-autonomous driving. Existing\nhaptic shared control schemes, however, do not take full consideration of the\nhuman agent. To fill this research gap, this study presents a haptic shared\ncontrol scheme that adapts to a human operator's workload, eyes on road and\ninput torque in real-time. We conducted human-in-the-loop experiments with 24\nparticipants. In the experiment, a human operator and an autonomy module for\nnavigation shared the control of a simulated notional High Mobility\nMultipurpose Wheeled Vehicle (HMMWV) at a fixed speed. At the same time, the\nhuman operator performed a target detection task for surveillance. The autonomy\ncould be either adaptive or non-adaptive to the above-mentioned human factors.\nResults indicate that the adaptive haptic control scheme resulted in\nsignificantly lower workload, higher trust in autonomy, better driving task\nperformance and smaller control effort.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 23:53:54 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Luo", "Ruikun", ""], ["Weng", "Yifan", ""], ["Wang", "Yifan", ""], ["Jayakumar", "Paramsothy", ""], ["Brudnak", "Mark J.", ""], ["Paul", "Victor", ""], ["Desaraju", "Vishnu R.", ""], ["Stein", "Jeffrey L.", ""], ["Ersal", "Tulga", ""], ["Yang", "X. Jessie", ""]]}, {"id": "2004.00267", "submitter": "Noemi Mauro", "authors": "Liliana Ardissono, Matteo Delsanto, Maurizio Lucenteforte, Noemi\n  Mauro, Adriano Savoca and Daniele Scanu", "title": "Map-Based Visualization of 2D/3D Spatial Data via Stylization and Tuning\n  of Information Emphasis", "comments": null, "journal-ref": "Proceedings of the 2018 International Conference on Advanced\n  Visual Interfaces (AVI 2018)", "doi": "10.1145/3206505.3206516", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Geographical Information search, map visualization can challenge the user\nbecause results can consist of a large set of heterogeneous items, increasing\nvisual complexity. We propose a novel visualization model to address this\nissue. Our model represents results as markers, or as geometric objects, on\n2D/3D layers, using stylized and highly colored shapes to enhance their\nvisibility. Moreover, the model supports interactive information filtering in\nthe map by enabling the user to focus on different data categories, using\ntransparency sliders to tune the opacity, and thus the emphasis, of the\ncorresponding data items. A test with users provided positive results\nconcerning the efficacy of the model.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 07:48:10 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Ardissono", "Liliana", ""], ["Delsanto", "Matteo", ""], ["Lucenteforte", "Maurizio", ""], ["Mauro", "Noemi", ""], ["Savoca", "Adriano", ""], ["Scanu", "Daniele", ""]]}, {"id": "2004.00505", "submitter": "Eoin Brophy", "authors": "Eoin Brophy, Willie Muehlhausen, Alan F. Smeaton, Tomas E. Ward", "title": "Optimised Convolutional Neural Networks for Heart Rate Estimation and\n  Human Activity Recognition in Wrist Worn Sensing Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wrist-worn smart devices are providing increased insights into human health,\nbehaviour and performance through sophisticated analytics. However, battery\nlife, device cost and sensor performance in the face of movement-related\nartefact present challenges which must be further addressed to see effective\napplications and wider adoption through commoditisation of the technology. We\naddress these challenges by demonstrating, through using a simple optical\nmeasurement, photoplethysmography (PPG) used conventionally for heart rate\ndetection in wrist-worn sensors, that we can provide improved heart rate and\nhuman activity recognition (HAR) simultaneously at low sample rates, without an\ninertial measurement unit. This simplifies hardware design and reduces costs\nand power budgets. We apply two deep learning pipelines, one for human activity\nrecognition and one for heart rate estimation. HAR is achieved through the\napplication of a visual classification approach, capable of robust performance\nat low sample rates. Here, transfer learning is leveraged to retrain a\nconvolutional neural network (CNN) to distinguish characteristics of the PPG\nduring different human activities. For heart rate estimation we use a CNN\nadopted for regression which maps noisy optical signals to heart rate\nestimates. In both cases, comparisons are made with leading conventional\napproaches. Our results demonstrate a low sampling frequency can achieve good\nperformance without significant degradation of accuracy. 5 Hz and 10 Hz were\nshown to have 80.2% and 83.0% classification accuracy for HAR respectively.\nThese same sampling frequencies also yielded a robust heart rate estimation\nwhich was comparative with that achieved at the more energy-intensive rate of\n256 Hz.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 11:44:58 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Brophy", "Eoin", ""], ["Muehlhausen", "Willie", ""], ["Smeaton", "Alan F.", ""], ["Ward", "Tomas E.", ""]]}, {"id": "2004.00588", "submitter": "Kayo Yin", "authors": "Kayo Yin and Jesse Read", "title": "Better Sign Language Translation with STMC-Transformer", "comments": "Proceedings of the 28th International Conference on Computational\n  Linguistics (COLING'2020)", "journal-ref": "28th International Conference on Computational Linguistics 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sign Language Translation (SLT) first uses a Sign Language Recognition (SLR)\nsystem to extract sign language glosses from videos. Then, a translation system\ngenerates spoken language translations from the sign language glosses. This\npaper focuses on the translation system and introduces the STMC-Transformer\nwhich improves on the current state-of-the-art by over 5 and 7 BLEU\nrespectively on gloss-to-text and video-to-text translation of the\nPHOENIX-Weather 2014T dataset. On the ASLG-PC12 corpus, we report an increase\nof over 16 BLEU.\n  We also demonstrate the problem in current methods that rely on gloss\nsupervision. The video-to-text translation of our STMC-Transformer outperforms\ntranslation of GT glosses. This contradicts previous claims that GT gloss\ntranslation acts as an upper bound for SLT performance and reveals that glosses\nare an inefficient representation of sign language. For future SLT research, we\ntherefore suggest an end-to-end training of the recognition and translation\nmodels, or using a different sign language annotation scheme.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 17:20:04 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 00:59:54 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Yin", "Kayo", ""], ["Read", "Jesse", ""]]}, {"id": "2004.00646", "submitter": "Dietmar Jannach", "authors": "Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li Chen", "title": "A Survey on Conversational Recommender Systems", "comments": "35 pages, 5 figures", "journal-ref": "ACM Computing Surveys, Volume 54, Issue 5, 2021", "doi": "10.1145/3453154", "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are software applications that help users to find items\nof interest in situations of information overload. Current research often\nassumes a one-shot interaction paradigm, where the users' preferences are\nestimated based on past observed behavior and where the presentation of a\nranked list of suggestions is the main, one-directional form of user\ninteraction. Conversational recommender systems (CRS) take a different approach\nand support a richer set of interactions. These interactions can, for example,\nhelp to improve the preference elicitation process or allow the user to ask\nquestions about the recommendations and to give feedback. The interest in CRS\nhas significantly increased in the past few years. This development is mainly\ndue to the significant progress in the area of natural language processing, the\nemergence of new voice-controlled home assistants, and the increased use of\nchatbot technology. With this paper, we provide a detailed survey of existing\napproaches to conversational recommendation. We categorize these approaches in\nvarious dimensions, e.g., in terms of the supported user intents or the\nknowledge they use in the background. Moreover, we discuss technological\napproaches, review how CRS are evaluated, and finally identify a number of gaps\nthat deserve more research in the future.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 18:00:47 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 06:16:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Jannach", "Dietmar", ""], ["Manzoor", "Ahtsham", ""], ["Cai", "Wanling", ""], ["Chen", "Li", ""]]}, {"id": "2004.00689", "submitter": "David Robb", "authors": "David A. Robb, Muneeb I. Ahmad, Carlo Tiseo, Simona Aracri, Alistair\n  C. McConnell, Vincent Page, Christian Dondrup, Francisco J. Chiyah Garcia,\n  Hai-Nguyen Nguyen, \\`Eric Pairet, Paola Ard\\'on Ram\\'irez, Tushar Semwal,\n  Hazel M. Taylor, Lindsay J. Wilson, David Lane, Helen Hastie, Katrin Lohan", "title": "Robots in the Danger Zone: Exploring Public Perception through\n  Engagement", "comments": "Accepted in HRI 2020, Keywords: Human robot interaction, robotics,\n  artificial intelligence, public engagement, public perceptions of robots,\n  robotics and society", "journal-ref": "In Human-Robot Interaction HRI 2020, ACM, NY, USA, 10 pages", "doi": "10.1145/3319502.3374789", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public perceptions of Robotics and Artificial Intelligence (RAI) are\nimportant in the acceptance, uptake, government regulation and research funding\nof this technology. Recent research has shown that the public's understanding\nof RAI can be negative or inaccurate. We believe effective public engagement\ncan help ensure that public opinion is better informed. In this paper, we\ndescribe our first iteration of a high throughput in-person public engagement\nactivity. We describe the use of a light touch quiz-format survey instrument to\nintegrate in-the-wild research participation into the engagement, allowing us\nto probe both the effectiveness of our engagement strategy, and public\nperceptions of the future roles of robots and humans working in dangerous\nsettings, such as in the off-shore energy sector. We critique our methods and\nshare interesting results into generational differences within the public's\nview of the future of Robotics and AI in hazardous environments. These findings\ninclude that older peoples' views about the future of robots in hazardous\nenvironments were not swayed by exposure to our exhibit, while the views of\nyounger people were affected by our exhibit, leading us to consider carefully\nin future how to more effectively engage with and inform older people.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 20:10:53 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Robb", "David A.", ""], ["Ahmad", "Muneeb I.", ""], ["Tiseo", "Carlo", ""], ["Aracri", "Simona", ""], ["McConnell", "Alistair C.", ""], ["Page", "Vincent", ""], ["Dondrup", "Christian", ""], ["Garcia", "Francisco J. Chiyah", ""], ["Nguyen", "Hai-Nguyen", ""], ["Pairet", "\u00c8ric", ""], ["Ram\u00edrez", "Paola Ard\u00f3n", ""], ["Semwal", "Tushar", ""], ["Taylor", "Hazel M.", ""], ["Wilson", "Lindsay J.", ""], ["Lane", "David", ""], ["Hastie", "Helen", ""], ["Lohan", "Katrin", ""]]}, {"id": "2004.00701", "submitter": "Maryam Arab", "authors": "Maryam Arab, Thomas D LaToza, Amy J Ko", "title": "An Exploratory Study of Writing and Revising Explicit Programming\n  Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge sharing plays a crucial role throughout all software application\ndevelopment activities. When programmers learn and share through media like\nStack overflow, GitHub, Meetups, videos, discussion forums, wikis, and blogs,\nevery developer benefits. However, there is one kind of knowledge that\ndevelopers share far less often: strategic knowledge for how to approach\nprogramming problems (e.g., how to debug server-side Python errors, how to\nresolve a merge conflict, how to evaluate the stability of an API one is\nconsidering for adoption). In this paper, we investigate the feasibility of\ndevelopers articulating and sharing their strategic knowledge, and the use of\nthese strategies to support other developers in their problem-solving. We\nspecifically investigate challenges that developers face in articulating\nstrategies in a form in which other developers can use to increase their\nproductivity. To observe this, we simulated a knowledge-sharing platform,\nasking experts to articulate one of their own strategies and then asked the\nsecond set of developers to try to use the strategies and provide feedback on\nthe strategies to authors. During the study, we asked both strategy authors and\nusers to reflect on the challenges they faced. In analyzing the strategies\nauthors created, the use of the strategies, the feedback that users provided to\nauthors, and the difficulties that authors faced addressing this feedback, we\nfound that developers can share strategic knowledge, but authoring strategies\nrequire substantial feedback from diverse audiences to be helpful to\nprogrammers with varying prior knowledge. Our results also raise challenging\nquestions about how future work should support searching and browsing for\nstrategies that support varying prior knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 20:55:33 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 16:02:55 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Arab", "Maryam", ""], ["LaToza", "Thomas D", ""], ["Ko", "Amy J", ""]]}, {"id": "2004.00762", "submitter": "Michael Iuzzolino", "authors": "Michael L. Iuzzolino, Tetsumichi Umada, Nisar R. Ahmed, and Danielle\n  A. Szafir", "title": "In Automation We Trust: Investigating the Role of Uncertainty in Active\n  Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate how different active learning (AL) query policies coupled with\nclassification uncertainty visualizations affect analyst trust in automated\nclassification systems. A current standard policy for AL is to query the oracle\n(e.g., the analyst) to refine labels for datapoints where the classifier has\nthe highest uncertainty. This is an optimal policy for the automation system as\nit yields maximal information gain. However, model-centric policies neglect the\neffects of this uncertainty on the human component of the system and the\nconsequent manner in which the human will interact with the system\npost-training. In this paper, we present an empirical study evaluating how AL\nquery policies and visualizations lending transparency to classification\ninfluence trust in automated classification of image data. We found that query\npolicy significantly influences an analyst's trust in an image classification\nsystem, and we use these results to propose a set of oracle query policies and\nvisualizations for use during AL training phases that can influence analyst\ntrust in classification.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 00:52:49 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Iuzzolino", "Michael L.", ""], ["Umada", "Tetsumichi", ""], ["Ahmed", "Nisar R.", ""], ["Szafir", "Danielle A.", ""]]}, {"id": "2004.00935", "submitter": "Avi Segal", "authors": "Laura Schelenz, Avi Segal, and Kobi Gal", "title": "Applying Transparency in Artificial Intelligence based Personalization\n  Systems", "comments": "8 pages; For ECAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence based systems increasingly use personalization to\nprovide users with relevant content, products, and solutions. Personalization\nis intended to support users and address their respective needs and\npreferences. However, users are becoming increasingly vulnerable to online\nmanipulation due to algorithmic advancements and lack of transparency. Such\nmanipulation decreases users' levels of trust, autonomy, and satisfaction\nconcerning the systems with which they interact. Increasing transparency is an\nimportant goal for personalization based systems. Unfortunately, system\ndesigners lack guidance in assessing and implementing transparency in their\ndeveloped systems.\n  In this work we combine insights from technology ethics and computer science\nto generate a list of transparency best practices for machine generated\npersonalization. Based on these best practices, we develop a checklist to be\nused by designers wishing to evaluate and increase the transparency of their\nalgorithmic systems. Adopting a designer perspective, we apply the checklist to\nprominent online services and discuss its advantages and shortcomings. We\nencourage researchers to adopt the checklist in various environments and to\nwork towards a consensus-based tool for measuring transparency in the\npersonalization community.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 11:07:38 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 13:49:54 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Schelenz", "Laura", ""], ["Segal", "Avi", ""], ["Gal", "Kobi", ""]]}, {"id": "2004.01451", "submitter": "Gerard Llorach", "authors": "Gerard Llorach, Maartje M.E. Hendrikse, Giso Grimm and Volker Hohmann", "title": "Comparison of a Head-Mounted Display and a Curved Screen in a\n  Multi-Talker Audiovisual Listening Task", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual audiovisual technology has matured but the methodology has yet to be\nestablished for speech- and audio-related perception research. This study\nexamined the effects of different audiovisual conditions on head yaw and gaze\ndirection when listening to multi-talker conversations. Two immersive displays\nwere tested and compared: a curved screen (CS) and a head-mounted display\n(HMD). Using three visual conditions (audio-only, virtual characters and video\nrecordings), three groups of participants were tested: seventeen young\nnormal-hearing, eleven older normal-hearing and ten older hearing-impaired\nlisteners with hearing aids. The results showed that when there were no visual\ncues, participants tended to look ahead; when visual information was available,\nthey looked at the target speaker. Significant differences between displays and\nvisual conditions were found, suggesting that using different audiovisual\nsetups may lead to slightly different head yaw and gaze directions. No\nsignificant differences were found between groups. Open interviews showed that\nthe CS was preferred over the HMD and that video recordings were the preferred\nvisual condition.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 09:52:50 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 07:47:33 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 14:17:37 GMT"}, {"version": "v4", "created": "Mon, 26 Oct 2020 09:27:10 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Llorach", "Gerard", ""], ["Hendrikse", "Maartje M. E.", ""], ["Grimm", "Giso", ""], ["Hohmann", "Volker", ""]]}, {"id": "2004.01532", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Jan-Niklas Voigt-Antons, Eero Lehtonen, Andres Pinilla Palacios,\n  Danish Ali, Tanja Koji\\'c, Sebastian M\\\"oller", "title": "Comparing emotional states induced by 360$^{\\circ}$ videos via\n  head-mounted display and computer screen", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years 360$^{\\circ}$ videos have been becoming more popular. For\ntraditional media presentations, e.g., on a computer screen, a wide range of\nassessment methods are available. Different constructs, such as perceived\nquality or the induced emotional state of viewers, can be reliably assessed by\nsubjective scales. Many of the subjective methods have only been validated\nusing stimuli presented on a computer screen. This paper is using 360$^{\\circ}$\nvideos to induce varying emotional states. Videos were presented 1) via a\nhead-mounted display (HMD) and 2) via a traditional computer screen.\nFurthermore, participants were asked to rate their emotional state 1) in\nretrospect on the self-assessment manikin scale and 2) continuously on a\n2-dimensional arousal-valence plane. In a repeated measures design, all\nparticipants (N = 18) used both presentation systems and both rating systems.\nResults indicate that there is a statistically significant difference in\ninduced presence due to the presentation system. Furthermore, there was no\nstatistically significant difference in ratings gathered with the two\npresentation systems. Finally, it was found that for arousal measures, a\nstatistically significant difference could be found for the different rating\nmethods, potentially indicating an underestimation of arousal ratings gathered\nin retrospect for screen presentation. In the future, rating methods such as a\n2-dimensional arousal-valence plane could offer the advantage of enabling a\nreliable measurement of emotional states while being more embedded in the\nexperience itself, enabling a more precise capturing of the emotional states.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 12:51:44 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Voigt-Antons", "Jan-Niklas", ""], ["Lehtonen", "Eero", ""], ["Palacios", "Andres Pinilla", ""], ["Ali", "Danish", ""], ["Koji\u0107", "Tanja", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2004.01545", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Tanja Koji\\'c, Danish Ali, Robert Greinacher, Sebastian M\\\"oller,\n  Jan-Niklas Voigt-Antons", "title": "User Experience of Reading in Virtual Reality -- Finding Values for Text\n  Distance, Size and Contrast", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) has an increasing impact on the market in many fields,\nfrom education and medicine to engineering and entertainment, by creating\ndifferent applications that replicate or in the case of augmentation enhance\nreal-life scenarios. Intending to present realistic environments, VR\napplications are including text that we are surrounded by every day. However,\ntext can only add value to the virtual environment if it is designed and\ncreated in such a way that users can comfortably read it. With the aim to\nexplore what values for text parameters users find comfortable while reading in\nvirtual reality, a study was conducted allowing participants to manipulate text\nparameters such as font size, distance, and contrast. Therefore two different\nstandalone virtual reality devices were used, Oculus Go and Quest, together\nwith three different text samples: Short (2 words), medium (21 words), and long\n(51 words). Participants had the task of setting text parameters to the best\nand worst possible value. Additionally, participants were asked to rate their\nexperience of reading in virtual reality. Results report mean values for\nangular size (the combination of distance and font size) and color contrast\ndepending on the different device used as well as the varying text length, for\nboth tasks. Significant differences were found for values of angular size,\ndepending on the length of the displayed text. However, different device types\nhad no significant influence on text parameters but on the experiences reported\nusing the self-assessment manikin (SAM) scale.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:14:42 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Koji\u0107", "Tanja", ""], ["Ali", "Danish", ""], ["Greinacher", "Robert", ""], ["M\u00f6ller", "Sebastian", ""], ["Voigt-Antons", "Jan-Niklas", ""]]}, {"id": "2004.01555", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Robert Greinacher, Tanja Koji\\'c, Luis Meier, Rudresha Gulaganjihalli\n  Parameshappa, Sebastian M\\\"oller, Jan-Niklas Voigt-Antons", "title": "Impact of Tactile and Visual Feedback on Breathing Rhythm and User\n  Experience in VR Exergaming", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining interconnected wearables provides fascinating opportunities like\naugmenting exergaming with virtual coaches, feedback on the execution of sports\nactivities, or how to improve on them. Breathing rhythm is a particularly\ninteresting physiological dimension since it is easy and unobtrusive to measure\nand gained data provide valuable insights regarding the correct execution of\nmovements, especially when analyzed together with additional movement data in\nreal-time. In this work, we focus on indoor rowing since it is a popular sport\nthat's often done alone without extensive instructions. We compare a visual\nbreathing indication with haptic guidance in order for athletes to maintain a\ncorrect, efficient, and healthy breathing-movement-synchronicity (BMS) while\nworking out. Also, user experience and acceptance of the different modalities\nwere measured. The results show a positive and statistically significant impact\nof purely verbal instructions and purely tactile feedback on BMS and no\nsignificant impact of visual feedback. Interestingly, the subjective ratings\nindicate a strong preference for the visual modality and even an aversion for\nthe haptic feedback, although objectively the performance benefited most from\nusing the latter.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:25:32 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Greinacher", "Robert", ""], ["Koji\u0107", "Tanja", ""], ["Meier", "Luis", ""], ["Parameshappa", "Rudresha Gulaganjihalli", ""], ["M\u00f6ller", "Sebastian", ""], ["Voigt-Antons", "Jan-Niklas", ""]]}, {"id": "2004.01697", "submitter": "Alberto Alvarez", "authors": "Alberto Alvarez, Jose Font, Julian Togelius", "title": "Designer Modeling through Design Style Clustering", "comments": "10 pages, Submitted to IEEE Transactions on Games (TOG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose modeling designer style in mixed-initiative game content creation\ntools as archetypical design traces. These design traces are formulated as\ntransitions between design styles; these design styles are in turn found\nthrough clustering all intermediate designs along the way to making a complete\ndesign. This method is implemented in the Evolutionary Dungeon Designer, a\nresearch platform for mixed-initiative systems to create roguelike games. We\npresent results both in the form of design styles for rooms, which can be\nanalyzed to better understand the kind of rooms designed by users, and in the\nform of archetypical sequences between these rooms. We further discuss how the\nresults here can be used to create style-sensitive suggestions. Such\nsuggestions would allow the system to be one step ahead of the designer,\noffering suggestions for the next cluster, assuming that the designer will\nfollow one of the archetypical design traces.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 17:57:41 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 13:41:35 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Alvarez", "Alberto", ""], ["Font", "Jose", ""], ["Togelius", "Julian", ""]]}, {"id": "2004.01790", "submitter": "Yan Chen", "authors": "Yan Chen, Andr\\'es Monroy-Hern\\'andez, Ian Wehrman, Steve Oney, Walter\n  S. Lasecki and Rajan Vaish", "title": "Sifter: A Hybrid Workflow for Theme-based Video Curation at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-generated content platforms curate their vast repositories into thematic\ncompilations that facilitate the discovery of high-quality material. Platforms\nthat seek tight editorial control employ people to do this curation, but this\nprocess involves time-consuming routine tasks, such as sifting through\nthousands of videos. We introduce Sifter, a system that improves the curation\nprocess by combining automated techniques with a human-powered pipeline that\nbrowses, selects, and reaches an agreement on what videos to include in a\ncompilation. We evaluated Sifter by creating 12 compilations from over 34,000\nuser-generated videos. Sifter was more than three times faster than dedicated\ncurators, and its output was of comparable quality. We reflect on the\nchallenges and opportunities introduced by Sifter to inform the design of\ncontent curation systems that need subjective human judgments of videos at\nscale.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 21:56:54 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 01:01:27 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Chen", "Yan", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""], ["Wehrman", "Ian", ""], ["Oney", "Steve", ""], ["Lasecki", "Walter S.", ""], ["Vaish", "Rajan", ""]]}, {"id": "2004.01949", "submitter": "Vinoth Pandian Sermuga Pandian", "authors": "Vinoth Pandian Sermuga Pandian, Sarah Suleri", "title": "BlackBox Toolkit: Intelligent Assistance to UI Design", "comments": "Workshop position paper for CHI'20, Workshop on Artificial\n  Intelligence for HCI: A Modern Approach; 4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  User Interface (UI) design is an creative process that involves considerable\nreiteration and rework. Designers go through multiple iterations of different\nprototyping fidelities to create a UI design. In this research, we propose to\nmodify the UI design process by assisting it with artificial intelligence (AI).\nWe propose to enable AI to perform repetitive tasks for the designer while\nallowing the designer to take command of the creative process. This approach\nmakes the machine act as a black box that intelligently assists the designers\nin creating UI design. We believe this approach would greatly benefit designers\nin co-creating design solutions with AI.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 14:50:26 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 13:30:41 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Pandian", "Vinoth Pandian Sermuga", ""], ["Suleri", "Sarah", ""]]}, {"id": "2004.01973", "submitter": "Xun Wu", "authors": "Xun Wu, Wei-Long Zheng, and Bao-Liang Lu", "title": "Investigating EEG-Based Functional Connectivity Patterns for Multimodal\n  Emotion Recognition", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with the rich studies on the motor brain-computer interface (BCI),\nthe recently emerging affective BCI presents distinct challenges since the\nbrain functional connectivity networks involving emotion are not well\ninvestigated. Previous studies on emotion recognition based on\nelectroencephalography (EEG) signals mainly rely on single-channel-based\nfeature extraction methods. In this paper, we propose a novel emotion-relevant\ncritical subnetwork selection algorithm and investigate three EEG functional\nconnectivity network features: strength, clustering coefficient, and\neigenvector centrality. The discrimination ability of the EEG connectivity\nfeatures in emotion recognition is evaluated on three public emotion EEG\ndatasets: SEED, SEED-V, and DEAP. The strength feature achieves the best\nclassification performance and outperforms the state-of-the-art differential\nentropy feature based on single-channel analysis. The experimental results\nreveal that distinct functional connectivity patterns are exhibited for the\nfive emotions of disgust, fear, sadness, happiness, and neutrality.\nFurthermore, we construct a multimodal emotion recognition model by combining\nthe functional connectivity features from EEG and the features from eye\nmovements or physiological signals using deep canonical correlation analysis.\nThe classification accuracies of multimodal emotion recognition are 95.08/6.42%\non the SEED dataset, 84.51/5.11% on the SEED-V dataset, and 85.34/2.90% and\n86.61/3.76% for arousal and valence on the DEAP dataset, respectively. The\nresults demonstrate the complementary representation properties of the EEG\nconnectivity features with eye movement data. In addition, we find that the\nbrain networks constructed with 18 channels achieve comparable performance with\nthat of the 62-channel network in multimodal emotion recognition and enable\neasier setups for BCI systems in real scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 16:51:56 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Wu", "Xun", ""], ["Zheng", "Wei-Long", ""], ["Lu", "Bao-Liang", ""]]}, {"id": "2004.02028", "submitter": "Bhavya Ghai", "authors": "Bhavya Ghai, Q. Vera Liao, Yunfeng Zhang, Klaus Mueller", "title": "Measuring Social Biases of Crowd Workers using Counterfactual Queries", "comments": "Accepted at the Workshop on Fair and Responsible AI at ACM CHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social biases based on gender, race, etc. have been shown to pollute machine\nlearning (ML) pipeline predominantly via biased training datasets.\nCrowdsourcing, a popular cost-effective measure to gather labeled training\ndatasets, is not immune to the inherent social biases of crowd workers. To\nensure such social biases aren't passed onto the curated datasets, it's\nimportant to know how biased each crowd worker is. In this work, we propose a\nnew method based on counterfactual fairness to quantify the degree of inherent\nsocial bias in each crowd worker. This extra information can be leveraged\ntogether with individual worker responses to curate a less biased dataset.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 21:41:55 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Ghai", "Bhavya", ""], ["Liao", "Q. Vera", ""], ["Zhang", "Yunfeng", ""], ["Mueller", "Klaus", ""]]}, {"id": "2004.02289", "submitter": "Jonathan Martinez", "authors": "Jonathan Martinez (1), Kobi Gal (1 and 2), Ece Kamar (3), Levi H. S.\n  Lelis (4) ((1) Ben-Gurion University, (2) University of Edinburgh, (3)\n  Microsoft Research, (4) University of Alberta)", "title": "Personalization in Human-AI Teams: Improving the Compatibility-Accuracy\n  Tradeoff", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI systems that model and interact with users can update their models over\ntime to reflect new information and changes in the environment. Although these\nupdates may improve the overall performance of the AI system, they may actually\nhurt the performance with respect to individual users. Prior work has studied\nthe trade-off between improving the system's accuracy following an update and\nthe compatibility of the updated system with prior user experience. The more\nthe model is forced to be compatible with a prior version, the higher loss in\naccuracy it will incur. In this paper, we show that by personalizing the loss\nfunction to specific users, in some cases it is possible to improve the\ncompatibility-accuracy trade-off with respect to these users (increase the\ncompatibility of the model while sacrificing less accuracy). We present\nexperimental results indicating that this approach provides moderate\nimprovements on average (around 20%) but large improvements for certain users\n(up to 300%).\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 19:35:18 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 13:13:22 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Martinez", "Jonathan", "", "1 and 2"], ["Gal", "Kobi", "", "1 and 2"], ["Kamar", "Ece", ""], ["Lelis", "Levi H. S.", ""]]}, {"id": "2004.02363", "submitter": "Kushal Chawla", "authors": "Kushal Chawla, Gale Lucas, Jonathan May, Jonathan Gratch", "title": "Exploring Early Prediction of Buyer-Seller Negotiation Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents that negotiate with humans find broad applications in pedagogy and\nconversational AI. Most efforts in human-agent negotiations rely on restrictive\nmenu-driven interfaces for communication. To advance the research in\nlanguage-based negotiation systems, we explore a novel task of early prediction\nof buyer-seller negotiation outcomes, by varying the fraction of utterances\nthat the model can access. We explore the feasibility of early prediction by\nusing traditional feature-based methods, as well as by incorporating the\nnon-linguistic task context into a pretrained language model using sentence\ntemplates. We further quantify the extent to which linguistic features help in\nmaking better predictions apart from the task-specific price information.\nFinally, probing the pretrained model helps us to identify specific features,\nsuch as trust and agreement, that contribute to the prediction performance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 00:49:20 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 03:17:36 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Chawla", "Kushal", ""], ["Lucas", "Gale", ""], ["May", "Jonathan", ""], ["Gratch", "Jonathan", ""]]}, {"id": "2004.02481", "submitter": "Michael Braun", "authors": "Michael Braun, Jingyi Li, Florian Weber, Bastian Pfleging, Andreas\n  Butz, Florian Alt", "title": "What If Your Car Would Care? Exploring Use Cases For Affective\n  Automotive User Interfaces", "comments": null, "journal-ref": null, "doi": "10.1145/3379503.3403530", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present use cases for affective user interfaces (UIs) in\ncars and how they are perceived by potential users in China and Germany.\nEmotion-aware interaction is enabled by the improvement of ubiquitous sensing\nmethods and provides potential benefits for both traffic safety and personal\nwell-being. To promote the adoption of affective interaction at an\ninternational scale, we developed 20 mobile in-car use cases through an\ninter-cultural design approach and evaluated them with 65 drivers in Germany\nand China. Our data shows perceived benefits in specific areas of pragmatic\nquality as well as cultural differences, especially for socially interactive\nuse cases. We also discuss general implications for future affective automotive\nUI. Our results provide a perspective on cultural peculiarities and a concrete\nstarting point for practitioners and researchers working on emotion-aware\ninterfaces.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 08:33:16 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Braun", "Michael", ""], ["Li", "Jingyi", ""], ["Weber", "Florian", ""], ["Pfleging", "Bastian", ""], ["Butz", "Andreas", ""], ["Alt", "Florian", ""]]}, {"id": "2004.02793", "submitter": "Mike Thelwall Prof", "authors": "Mike Thelwall, Saheeda Thelwall", "title": "A thematic analysis of highly retweeted early COVID -19 tweets:\n  Consensus, information, dissent, and lockdown life", "comments": null, "journal-ref": null, "doi": "10.1108/AJIM-05-2020-0134", "report-no": null, "categories": "cs.DL cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Public attitudes towards COVID-19 and social distancing are critical\nin reducing its spread. It is therefore important to understand public\nreactions and information dissemination in all major forms, including on social\nmedia. This article investigates important issues reflected on Twitter in the\nearly stages of the public reaction to COVID-19. Design/methodology/approach: A\nthematic analysis of the most retweeted English-language tweets mentioning\nCOVID-19 during March 10-29, 2020. Findings: The main themes identified for the\n87 qualifying tweets accounting for 14 million retweets were: lockdown life;\nattitude towards social restrictions; politics; safety messages; people with\nCOVID-19; support for key workers; work; and COVID-19 facts/news. Research\nlimitations/implications: Twitter played many positive roles, mainly through\nunofficial tweets. Users shared social distancing information, helped build\nsupport for social distancing, criticised government responses, expressed\nsupport for key workers, and helped each other cope with social isolation. A\nfew popular tweets not supporting social distancing show that government\nmessages sometimes failed. Practical implications: Public health campaigns in\nfuture may consider encouraging grass roots social web activity to support\ncampaign goals. At a methodological level, analysing retweet counts emphasised\npolitics and ignored practical implementation issues. Originality/value: This\nis the first qualitative analysis of general COVID-19-related retweeting.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:34:23 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 11:13:13 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 15:33:34 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Thelwall", "Mike", ""], ["Thelwall", "Saheeda", ""]]}, {"id": "2004.02989", "submitter": "Anahita Sanandaji", "authors": "Anahita Sanandaji, Cindy Grimm, Ruth West, Max Parola, Meghan\n  Kajihara, Kathryn Hays, Luke Hillard, Brandon Lane, and Molly Beyer", "title": "Analyzing 3D Volume Segmentation by Low-level Perceptual Cues,\n  High-level Cognitive Tasks, and Decision-making Processes", "comments": "20 pages, 20 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D volume segmentation is a fundamental task in many scientific and medical\napplications. Producing accurate segmentations efficiently is challenging, in\npart due to low imaging data quality (e.g., noise and low image resolution) and\nambiguity in the data that can only be resolved with higher-level knowledge of\nthe structure. Automatic algorithms do exist, but there are many use cases\nwhere they fail. The gold standard is still manual segmentation or review.\nUnfortunately, even for an expert, manual segmentation is laborious, time\nconsuming, and prone to errors. Existing 3D segmentation tools are often\ndesigned based on the underlying algorithm, and do not take into account human\nmental models, their lower-level perception abilities, and higher-level\ncognitive tasks. Our goal is to analyze manual segmentation using the critical\ndecision method (CDM) in order to gain a better understanding of the low-level\n(perceptual and marking) actions and higher-level decision-making processes\nthat segmenters use. A key challenge we faced is that decision-making consists\nof an accumulated set of low-level visual-spatial decisions that are\ninter-related and difficult to articulate verbally. To address this, we\ndeveloped a novel hybrid protocol which integrates CDM with eye-tracking,\nobservation, and targeted questions. In this paper, we develop and validate\ndata coding schemes for this hybrid data set that discern segmenters' low-level\nactions, higher-level cognitive tasks, overall task structures, and\ndecision-making processes. We successfully detect the visual processing changes\nbased on tasks sequences and micro decisions reflected in the eye-gaze data and\nidentified different segmentation decision strategies utilized by the\nsegmenters.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:36:48 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Sanandaji", "Anahita", ""], ["Grimm", "Cindy", ""], ["West", "Ruth", ""], ["Parola", "Max", ""], ["Kajihara", "Meghan", ""], ["Hays", "Kathryn", ""], ["Hillard", "Luke", ""], ["Lane", "Brandon", ""], ["Beyer", "Molly", ""]]}, {"id": "2004.03047", "submitter": "Yordan Raykov", "authors": "Yordan P. Raykov, Luc J.W. Evers, Reham Badawy, Bastiaan Bloem, Tom M.\n  Heskes, Marjan Meinders, Kasper Claes, Max A. Little", "title": "Probabilistic modelling of gait for robust passive monitoring in daily\n  life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passive monitoring in daily life may provide invaluable insights about a\nperson's health throughout the day. Wearable sensor devices are likely to play\na key role in enabling such monitoring in a non-obtrusive fashion. However,\nsensor data collected in daily life reflects multiple health and behavior\nrelated factors together. This creates the need for structured principled\nanalysis to produce reliable and interpretable predictions that can be used to\nsupport clinical diagnosis and treatment. In this work we develop a principled\nmodelling approach for free-living gait (walking) analysis. Gait is a promising\ntarget for non-obtrusive monitoring because it is common and indicative of\nvarious movement disorders such as Parkinson's disease (PD), yet its analysis\nhas largely been limited to experimentally controlled lab settings. To locate\nand characterize stationary gait segments in free living using accelerometers,\nwe present an unsupervised statistical framework designed to segment signals\ninto differing gait and non-gait patterns. Our flexible probabilistic framework\ncombines empirical assumptions about gait into a principled graphical model\nwith all of its merits. We demonstrate the approach on a new video-referenced\ndataset including unscripted daily living activities of 25 PD patients and 25\ncontrols, in and around their own houses. We evaluate our ability to detect\ngait and predict medication induced fluctuations in PD patients based on\nmodelled gait. Our evaluation includes a comparison between sensors attached at\nmultiple body locations including wrist, ankle, trouser pocket and lower back.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 00:05:30 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Raykov", "Yordan P.", ""], ["Evers", "Luc J. W.", ""], ["Badawy", "Reham", ""], ["Bloem", "Bastiaan", ""], ["Heskes", "Tom M.", ""], ["Meinders", "Marjan", ""], ["Claes", "Kasper", ""], ["Little", "Max A.", ""]]}, {"id": "2004.03209", "submitter": "Simon Perrault", "authors": "Atima Tharatipyakul, Kenny Choo, Simon T. Perrault", "title": "Pose Estimation for Facilitating Movement Learning from Online Videos", "comments": "4+1 pages", "journal-ref": null, "doi": "10.1145/3399715.3399835", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exists a multitude of online video tutorials to teach physical\nmovements such as exercises. Yet, users lack support to verify the accuracy of\ntheir movements when following such videos and have to rely on their own\nperception. To address this, we developed a web-based application that performs\nhuman pose estimation using both video inputs from the online video and web\ncamera, then provides different types of visual feedback to a user. Our study\nsuggests that the user's skeleton overlaid on the user's camera feed improved\nuser performance, whereas the user's skeleton on its own or trainer's skeleton\nwith the trainer video offered limited benefits. We believe that our\napplication demonstrates the potential to enhance learning physical movements\nfrom online videos and provides a basis for other guidance systems to design\nsuitable visualizations.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 08:58:14 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Tharatipyakul", "Atima", ""], ["Choo", "Kenny", ""], ["Perrault", "Simon T.", ""]]}, {"id": "2004.03282", "submitter": "Simon Rudkin", "authors": "Pawel Dlotko and Simon Rudkin", "title": "Visualising the Evolution of English Covid-19 Cases with Topological\n  Data Analysis Ball Mapper", "comments": "Updated to include April 17 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.HC econ.EM q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding disease spread through data visualisation has concentrated on\ntrends and maps. Whilst these are helpful, they neglect important\nmulti-dimensional interactions between characteristics of communities. Using\nthe Topological Data Analysis Ball Mapper algorithm we construct an abstract\nrepresentation of NUTS3 level economic data, overlaying onto it the confirmed\ncases of Covid-19 in England. In so doing we may understand how the disease\nspreads on different socio-economical dimensions. It is observed that some\nareas of the characteristic space have quickly raced to the highest levels of\ninfection, while others close by in the characteristic space, do not show large\ninfection growth. Likewise, we see patterns emerging in very different areas\nthat command more monitoring. A strong contribution for Topological Data\nAnalysis, and the Ball Mapper algorithm especially, in comprehending dynamic\nepidemic data is signposted.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 11:37:24 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 16:50:27 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Dlotko", "Pawel", ""], ["Rudkin", "Simon", ""]]}, {"id": "2004.03323", "submitter": "Niklas K\\\"uhl", "authors": "Svenja Laing, Niklas K\\\"uhl", "title": "Comfort-as-a-Service: Designing a User-Oriented Thermal Comfort Artifact\n  for Office Buildings", "comments": null, "journal-ref": "Thirty Ninth International Conference on Information Systems 2018", "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most people spend up to 90 % of their time indoors. However, literature in\nthe field of facility management and related disciplines mostly focus on energy\nand cost saving aspects of buildings. Especially in the area of commercial\nbuildings, only few articles take a user-centric perspective and none of them\nconsiders the subjectivity of thermal comfort. This work addresses this\nresearch gap and aims to optimize individual environmental comfort in open\noffice environments, taking advantage of changes in modern office\ninfrastructure and considering actual user feedback without interfering with\nexisting systems. Based on a Design Science Research approach, we first perform\na user experience testing in an exemplary corporate office building.\nFurthermore, we build a mechanism to gather user feedback on environmental\ncomfort. Based on this, we build a machine learning model including different\nIoT data sources (e.g. building data and weather data) with an average\ncoefficient of determination of 41.5%. Using these insights, we are able to\nsuggest current individual comfort zones within the building and help employees\nto make better informed decisions on where to sit or what to wear, to feel\ncomfortable and work productively. Therefore, we contribute to the body of\nknowledge by proposing a user-centric design within a cross-disciplinary\ncontext on the basis of analytical processes.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 19:06:38 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Laing", "Svenja", ""], ["K\u00fchl", "Niklas", ""]]}, {"id": "2004.03360", "submitter": "Tan Le", "authors": "Abrar Zahin, Le Thanh Tan, and Rose Qingyang Hu", "title": "A Machine Learning Based Framework for the Smart Healthcare Monitoring", "comments": null, "journal-ref": "2020 Intermountain Engineering, Technology and Computing (IETC)", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel framework for the smart healthcare system,\nwhere we employ the compressed sensing (CS) and the combination of the\nstate-of-the-art machine learning based denoiser as well as the alternating\ndirection of method of multipliers (ADMM) structure. This integration\nsignificantly simplifies the software implementation for the lowcomplexity\nencoder, thanks to the modular structure of ADMM. Furthermore, we focus on\ndetecting fall down actions from image streams. Thus, teh primary purpose of\nthus study is to reconstruct the image as visibly clear as possible and hence\nit helps the detection step at the trained classifier. For this efficient smart\nhealth monitoring framework, we employ the trained binary convolutional neural\nnetwork (CNN) classifier for the fall-action classifier, because this scheme is\na part of surveillance scenario. In this scenario, we deal with the fallimages,\nthus, we compress, transmit and reconstruct the fallimages. Experimental\nresults demonstrate the impacts of network parameters and the significant\nperformance gain of the proposal compared to traditional methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 17:41:28 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zahin", "Abrar", ""], ["Tan", "Le Thanh", ""], ["Hu", "Rose Qingyang", ""]]}, {"id": "2004.03408", "submitter": "Misgina Tsighe Hagos", "authors": "Misgina Tsighe Hagos, Shri Kant, Surayya Ado Bala", "title": "Automated Smartphone based System for Diagnosis of Diabetic Retinopathy", "comments": "12 pages, 4 figures, 4 tables, 1 appendix. Copyright \\copyright 2019,\n  IEEE. Published in: 2019 International Conference on Computing,\n  Communication, and Intelligent Systems (ICCCIS)", "journal-ref": "International Conference on Computing, Communication, and\n  Intelligent Systems (ICCCIS), Greater Noida, India, 2019, pp. 256-261", "doi": "10.1109/ICCCIS48478.2019.8974492", "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of diabetic retinopathy for treatment of the disease has been\nfailing to reach diabetic people living in rural areas. Shortage of trained\nophthalmologists, limited availability of healthcare centers, and expensiveness\nof diagnostic equipment are among the reasons. Although many deep\nlearning-based automatic diagnosis of diabetic retinopathy techniques have been\nimplemented in the literature, these methods still fail to provide a\npoint-of-care diagnosis. This raises the need for an independent diagnostic of\ndiabetic retinopathy that can be used by a non-expert. Recently the usage of\nsmartphones has been increasing across the world. Automated diagnoses of\ndiabetic retinopathy can be deployed on smartphones in order to provide an\ninstant diagnosis to diabetic people residing in remote areas. In this paper,\ninception based convolutional neural network and binary decision tree-based\nensemble of classifiers have been proposed and implemented to detect and\nclassify diabetic retinopathy. The proposed method was further imported into a\nsmartphone application for mobile-based classification, which provides an\noffline and automatic system for diagnosis of diabetic retinopathy.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 14:01:36 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Hagos", "Misgina Tsighe", ""], ["Kant", "Shri", ""], ["Bala", "Surayya Ado", ""]]}, {"id": "2004.03472", "submitter": "Pierre-Yves Oudeyer", "authors": "Mehdi Alaimi, Edith Law, Kevin Daniel Pantasdo, Pierre-Yves Oudeyer,\n  Helene Sauzeon", "title": "Pedagogical Agents for Fostering Question-Asking Skills in Children", "comments": "Accepted at CHI 2020", "journal-ref": null, "doi": "10.1145/3313831.3376776", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question asking is an important tool for constructing academic knowledge, and\na self-reinforcing driver of curiosity. However, research has found that\nquestion asking is infrequent in the classroom and children's questions are\noften superficial, lacking deep reasoning. In this work, we developed a\npedagogical agent that encourages children to ask divergent-thinking questions,\na more complex form of questions that is associated with curiosity. We\nconducted a study with 95 fifth grade students, who interacted with an agent\nthat encourages either convergent-thinking or divergent-thinking questions.\nResults showed that both interventions increased the number of\ndivergent-thinking questions and the fluency of question asking, while they did\nnot significantly alter children's perception of curiosity despite their high\nintrinsic motivation scores. In addition, children's curiosity trait has a\nmediating effect on question asking under the divergent-thinking agent,\nsuggesting that question-asking interventions must be personalized to each\nstudent based on their tendency to be curious.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:18:58 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Alaimi", "Mehdi", ""], ["Law", "Edith", ""], ["Pantasdo", "Kevin Daniel", ""], ["Oudeyer", "Pierre-Yves", ""], ["Sauzeon", "Helene", ""]]}, {"id": "2004.03500", "submitter": "Francisco Maria Calisto", "authors": "Francisco Maria Calisto, Nuno Jardim Nunes, Jacinto Carlos Nascimento", "title": "BreastScreening: On the Use of Multi-Modality in Medical Imaging\n  Diagnosis", "comments": "AVI 2020 Short Papers, 5 pages, 2 figures, for associated files, see\n  https://github.com/MIMBCD-UI/avi-2020-short-paper", "journal-ref": null, "doi": "10.1145/3399715.3399744", "report-no": null, "categories": "cs.HC cs.LG cs.SE eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes the field research, design and comparative deployment of\na multimodal medical imaging user interface for breast screening. The main\ncontributions described here are threefold: 1) The design of an advanced visual\ninterface for multimodal diagnosis of breast cancer (BreastScreening); 2)\nInsights from the field comparison of single vs multimodality screening of\nbreast cancer diagnosis with 31 clinicians and 566 images, and 3) The\nvisualization of the two main types of breast lesions in the following image\nmodalities: (i) MammoGraphy (MG) in both Craniocaudal (CC) and Mediolateral\noblique (MLO) views; (ii) UltraSound (US); and (iii) Magnetic Resonance Imaging\n(MRI). We summarize our work with recommendations from the radiologists for\nguiding the future design of medical imaging interfaces.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:53:26 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 14:38:48 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Calisto", "Francisco Maria", ""], ["Nunes", "Nuno Jardim", ""], ["Nascimento", "Jacinto Carlos", ""]]}, {"id": "2004.03577", "submitter": "Anastasios Angelopoulos", "authors": "Anastasios N. Angelopoulos, Julien N.P. Martel, Amit P.S. Kohli, Jorg\n  Conradt, Gordon Wetzstein", "title": "Event Based, Near Eye Gaze Tracking Beyond 10,000Hz", "comments": "IEEEVR oral/TVCG paper Dataset at\n  https://github.com/aangelopoulos/event_based_gaze_tracking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cameras in modern gaze-tracking systems suffer from fundamental bandwidth\nand power limitations, constraining data acquisition speed to 300 Hz\nrealistically. This obstructs the use of mobile eye trackers to perform, e.g.,\nlow latency predictive rendering, or to study quick and subtle eye motions like\nmicrosaccades using head-mounted devices in the wild. Here, we propose a hybrid\nframe-event-based near-eye gaze tracking system offering update rates beyond\n10,000 Hz with an accuracy that matches that of high-end desktop-mounted\ncommercial trackers when evaluated in the same conditions. Our system builds on\nemerging event cameras that simultaneously acquire regularly sampled frames and\nadaptively sampled events. We develop an online 2D pupil fitting method that\nupdates a parametric model every one or few events. Moreover, we propose a\npolynomial regressor for estimating the point of gaze from the parametric pupil\nmodel in real time. Using the first event-based gaze dataset, available at\nhttps://github.com/aangelopoulos/event_based_gaze_tracking , we demonstrate\nthat our system achieves accuracies of 0.45 degrees--1.75 degrees for fields of\nview from 45 degrees to 98 degrees. With this technology, we hope to enable a\nnew generation of ultra-low-latency gaze-contingent rendering and display\ntechniques for virtual and augmented reality.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 17:57:18 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 00:39:24 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Angelopoulos", "Anastasios N.", ""], ["Martel", "Julien N. P.", ""], ["Kohli", "Amit P. S.", ""], ["Conradt", "Jorg", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "2004.03737", "submitter": "Zhecan Wang", "authors": "Zhecan Wang, Jian Zhao, Cheng Lu, Han Huang, Fan Yang, Lianji Li,\n  Yandong Guo", "title": "Learning to Detect Head Movement in Unconstrained Remote Gaze Estimation\n  in the Wild", "comments": "2020 Winter Conference on Applications of Computer Vision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unconstrained remote gaze estimation remains challenging mostly due to its\nvulnerability to the large variability in head-pose. Prior solutions struggle\nto maintain reliable accuracy in unconstrained remote gaze tracking. Among\nthem, appearance-based solutions demonstrate tremendous potential in improving\ngaze accuracy. However, existing works still suffer from head movement and are\nnot robust enough to handle real-world scenarios. Especially most of them study\ngaze estimation under controlled scenarios where the collected datasets often\ncover limited ranges of both head-pose and gaze which introduces further bias.\nIn this paper, we propose novel end-to-end appearance-based gaze estimation\nmethods that could more robustly incorporate different levels of head-pose\nrepresentations into gaze estimation. Our method could generalize to real-world\nscenarios with low image quality, different lightings and scenarios where\ndirect head-pose information is not available. To better demonstrate the\nadvantage of our methods, we further propose a new benchmark dataset with the\nmost rich distribution of head-gaze combination reflecting real-world\nscenarios. Extensive evaluations on several public datasets and our own dataset\ndemonstrate that our method consistently outperforms the state-of-the-art by a\nsignificant margin.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 22:38:49 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Wang", "Zhecan", ""], ["Zhao", "Jian", ""], ["Lu", "Cheng", ""], ["Huang", "Han", ""], ["Yang", "Fan", ""], ["Li", "Lianji", ""], ["Guo", "Yandong", ""]]}, {"id": "2004.03922", "submitter": "Suchismita Das", "authors": "Suchismita Das and Nikhil R. Pal", "title": "Nonlinear Dimensionality Reduction for Data Visualization: An\n  Unsupervised Fuzzy Rule-based Approach", "comments": null, "journal-ref": "IEEE Transactions on Fuzzy Systems ( Early Access ) 2021", "doi": "10.1109/TFUZZ.2021.3076583", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we propose an unsupervised fuzzy rule-based dimensionality reduction\nmethod primarily for data visualization. It considers the following important\nissues relevant to dimensionality reduction-based data visualization: (i)\npreservation of neighborhood relationships, (ii) handling data on a non-linear\nmanifold, (iii) the capability of predicting projections for new test data\npoints, (iv) interpretability of the system, and (v) the ability to reject test\npoints if required. For this, we use a first-order Takagi-Sugeno type model. We\ngenerate rule antecedents using clusters in the input data. In this context, we\nalso propose a new variant of the Geodesic c-means clustering algorithm. We\nestimate the rule parameters by minimizing an error function that preserves the\ninter-point geodesic distances (distances over the manifold) as Euclidean\ndistances on the projected space. We apply the proposed method on three\nsynthetic and three real-world data sets and visually compare the results with\nfour other standard data visualization methods. The obtained results show that\nthe proposed method behaves desirably and performs better than or comparable to\nthe methods compared with. The proposed method is found to be robust to the\ninitial conditions. The predictability of the proposed method for test points\nis validated by experiments. We also assess the ability of our method to reject\noutput points when it should. Then, we extend this concept to provide a general\nframework for learning an unsupervised fuzzy model for data projection with\ndifferent objective functions. To the best of our knowledge, this is the first\nattempt to manifold learning using unsupervised fuzzy modeling.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 10:33:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Das", "Suchismita", ""], ["Pal", "Nikhil R.", ""]]}, {"id": "2004.03985", "submitter": "Xavier Favory", "authors": "Xavier Favory, Frederic Font and Xavier Serra", "title": "Search Result Clustering in Collaborative Sound Collections", "comments": "8 pages, 4 figures, Proceedings of the 2020 International Conference\n  on Multimedia Retrieval (ICMR 20), June 8-11, 2020, Dublin, Ireland. ACM,\n  NewYork, NY, USA, 8 pages", "journal-ref": null, "doi": "10.1145/3372278.3390691", "report-no": null, "categories": "cs.IR cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large size of nowadays' online multimedia databases makes retrieving\ntheir content a difficult and time-consuming task. Users of online sound\ncollections typically submit search queries that express a broad intent, often\nmaking the system return large and unmanageable result sets. Search Result\nClustering is a technique that organises search-result content into coherent\ngroups, which allows users to identify useful subsets in their results.\nObtaining coherent and distinctive clusters that can be explored with a\nsuitable interface is crucial for making this technique a useful complement of\ntraditional search engines. In our work, we propose a graph-based approach\nusing audio features for clustering diverse sound collections obtained when\nquerying large online databases. We propose an approach to assess the\nperformance of different features at scale, by taking advantage of the metadata\nassociated with each sound. This analysis is complemented with an evaluation\nusing ground-truth labels from manually annotated datasets. We show that using\na confidence measure for discarding inconsistent clusters improves the quality\nof the partitions. After identifying the most appropriate features for\nclustering, we conduct an experiment with users performing a sound design task,\nin order to evaluate our approach and its user interface. A qualitative\nanalysis is carried out including usability questionnaires and semi-structured\ninterviews. This provides us with valuable new insights regarding the features\nthat promote efficient interaction with the clusters.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 13:08:17 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Favory", "Xavier", ""], ["Font", "Frederic", ""], ["Serra", "Xavier", ""]]}, {"id": "2004.04107", "submitter": "Theerawit Wilaiprasitporn", "authors": "Rattanaphon Chaisaen, Phairot Autthasan, Nopparada Mingchinda,\n  Pitshaporn Leelaarporn, Narin Kunaseth, Suppakorn Tammajarung, Poramate\n  Manoonpong, Subhas Chandra Mukhopadhyay and Theerawit Wilaiprasitporn", "title": "Decoding EEG Rhythms During Action Observation, Motor Imagery, and\n  Execution for Standing and Sitting", "comments": "in press", "journal-ref": "IEEE Sensors Journal 2020", "doi": "10.1109/JSEN.2020.3005968", "report-no": null, "categories": "cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Event-related desynchronization and synchronization (ERD/S) and\nmovement-related cortical potential (MRCP) play an important role in\nbrain-computer interfaces (BCI) for lower limb rehabilitation, particularly in\nstanding and sitting. However, little is known about the differences in the\ncortical activation between standing and sitting, especially how the brain's\nintention modulates the pre-movement sensorimotor rhythm as they do for\nswitching movements. In this study, we aim to investigate the decoding of\ncontinuous EEG rhythms during action observation (AO), motor imagery (MI), and\nmotor execution (ME) for the actions of standing and sitting. We developed a\nbehavioral task in which participants were instructed to perform both AO and\nMI/ME in regard to the transitioning actions of sit-to-stand and stand-to-sit.\nOur results demonstrated that the ERD was prominent during AO, whereas ERS was\ntypical during MI at the alpha band across the sensorimotor area. A combination\nof the filter bank common spatial pattern (FBCSP) and support vector machine\n(SVM) for classification was used for both offline and classifier testing\nanalyses. The offline analysis indicated the classification of AO and MI\nproviding the highest mean accuracy at 82.73$\\pm$2.54\\% in the stand-to-sit\ntransition. By applying the classifier testing analysis, we demonstrated the\nhigher performance of decoding neural intentions from the MI paradigm in\ncomparison to the ME paradigm. These observations led us to the promising\naspect of using our developed tasks based on the integration of both AO and MI\nto build future exoskeleton-based rehabilitation systems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 09:29:22 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 06:52:48 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 07:08:22 GMT"}, {"version": "v4", "created": "Sat, 27 Jun 2020 15:29:26 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Chaisaen", "Rattanaphon", ""], ["Autthasan", "Phairot", ""], ["Mingchinda", "Nopparada", ""], ["Leelaarporn", "Pitshaporn", ""], ["Kunaseth", "Narin", ""], ["Tammajarung", "Suppakorn", ""], ["Manoonpong", "Poramate", ""], ["Mukhopadhyay", "Subhas Chandra", ""], ["Wilaiprasitporn", "Theerawit", ""]]}, {"id": "2004.04208", "submitter": "Hamed Jahromi", "authors": "Hamed Z. Jahromi, Declan T. Delaney, Andrew Hines", "title": "How Crisp is the Crease? A Subjective Study on Web Browsing Perception\n  of Above-The-Fold", "comments": null, "journal-ref": null, "doi": "10.1109/NetSoft48620.2020.9165497", "report-no": null, "categories": "cs.HC cs.NI cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Quality of Experience (QoE) for various types of websites has gained\nsignificant attention in recent years. In order to design and evaluate\nwebsites, a metric that can estimate a user's experienced quality robustly for\ndiverse content is necessary. SpeedIndex (SI) has been widely adopted to\nestimate perceived web page loading progress. It measures the speed of\nrendering pixels for the webpage that is visible in the browser window. This is\ntermed Above-The-Fold (ATF). The influence of animated content on the\nperception of ATF has been less comprehensively explored. In this paper, we\npresent an experimental design and methodology to measure ATF perception for\nwebsites with and without animated elements for various page content\ncategories. We found that pages with animated elements caused people to have\nmore varied perceptions of ATF under different network conditions. Animated\ncontent also impacts the page load estimation accuracy of SI for websites. We\ndiscuss how the difference in the perception of ATF will impact the QoE\nmanagement of web applications. We explain the necessity of revisiting the\nvisual assessment of ATF to include the animated contents and improve the\nrobustness of metrics like SI.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 19:09:33 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Jahromi", "Hamed Z.", ""], ["Delaney", "Declan T.", ""], ["Hines", "Andrew", ""]]}, {"id": "2004.04374", "submitter": "Oliver Bendel", "authors": "Oliver Bendel", "title": "Co-Robots as Care Robots", "comments": "Accepted paper of the AAAI 2020 Spring Symposium \"Applied AI in\n  Healthcare: Safety, Community, and the Environment\" (Stanford University).\n  Because of the COVID-19 outbreak, the physical meeting has been postponed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperation and collaboration robots, co-robots or cobots for short, are an\nintegral part of factories. For example, they work closely with the fitters in\nthe automotive sector, and everyone does what they do best. However, the novel\nrobots are not only relevant in production and logistics, but also in the\nservice sector, especially where proximity between them and the users is\ndesired or unavoidable. For decades, individual solutions of a very different\nkind have been developed in care. Now experts are increasingly relying on\nco-robots and teaching them the special tasks that are involved in care or\ntherapy. This article presents the advantages, but also the disadvantages of\nco-robots in care and support, and provides information with regard to\nhuman-robot interaction and communication. The article is based on a model that\nhas already been tested in various nursing and retirement homes, namely Lio\nfrom F&P Robotics, and uses results from accompanying studies. The authors can\nshow that co-robots are ideal for care and support in many ways. Of course, it\nis also important to consider a few points in order to guarantee functionality\nand acceptance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 05:58:26 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Bendel", "Oliver", ""]]}, {"id": "2004.04428", "submitter": "Oliver Bendel", "authors": "Oliver Bendel", "title": "Care Robots with Sexual Assistance Functions", "comments": "Accepted paper of the AAAI 2020 Spring Symposium \"Applied AI in\n  Healthcare: Safety, Community, and the Environment\" (Stanford University).\n  Because of the COVID-19 outbreak, the physical meeting has been postponed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residents in retirement and nursing homes have sexual needs just like other\npeople. However, the semi-public situation makes it difficult for them to\nsatisfy these existential concerns. In addition, they may not be able to meet a\nsuitable partner or find it difficult to have a relationship for mental or\nphysical reasons. People who live or are cared for at home can also be affected\nby this problem. Perhaps they can host someone more easily and discreetly than\nthe residents of a health facility, but some elderly and disabled people may be\nrestricted in some ways. This article examines the opportunities and risks that\narise with regard to care robots with sexual assistance functions. First of\nall, it deals with sexual well-being. Then it presents robotic systems ranging\nfrom sex robots to care robots. Finally, the focus is on care robots, with the\nauthor exploring technical and design issues. A brief ethical discussion\ncompletes the article. The result is that care robots with sexual assistance\nfunctions could be an enrichment of the everyday life of people in need of\ncare, but that we also have to consider some technical, design and moral\naspects.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 09:02:27 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Bendel", "Oliver", ""]]}, {"id": "2004.04797", "submitter": "Alain Giordanengo", "authors": "Alain Giordanengo", "title": "Impact of PGHD reliability on the usefulness of a clinical decision\n  support system", "comments": "arXiv admin note: this version has been removed by arXiv\n  administrators because the submitter did not have the right to agree to the\n  license applied", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using personal generated health data (PGHD) during medical consultations can\nbe beneficial for both patients and clinicians. However, multiple acceptance\nbarriers such as lack of PGHD reliability prevents a routine usage of this\ndata. A clinical decision support system, called FullFlow, has been developed\nto address these acceptance barriers. The objective of this study was to\ndetermine if FullFlow was useful during consultations and to verify the\nhypothesis that the higher PGHD reliability, the more effective the system is.\nThe assessment relied on a medical pilot during which clinicians and patients\nwith diabetes used the FullFlow during medical consultations. The data\ncollection relied on a post-consultation questionnaire in addition to system\nlogs. This study showed that the PGHD reliability was low for an overwhelming\nmajority of consultations. The information displayed was useful in half of the\nconsultations according to the clinicians who answered the questionnaire.\nDespite this, the overwhelming majority of clinicians who answered the\nquestionnaire found that the designed FullFlow system permitted to gain\ninsights of the situation of the patients. The study showed the higher the PGHD\nreliability is, the more useful the system is for clinicians. PGHD usage in\nclinical settings can permit clinicians to gain valuable information regarding\nthe situations of their patients. A clinical decision system can present useful\ninformation to clinicians. While the PGHD reliability is correlated to the\nusefulness of such system, it is not the only factor impacting it: context of\nthe clinicians and patients such as novelty of usage and personal goals also\nplays a role in determining on how such system is useful for clinicians.\nHowever, due to a limited number of participants, a new medical pilot must be\nperformed in order to confirm the results of this study.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 09:46:42 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Giordanengo", "Alain", ""]]}, {"id": "2004.04827", "submitter": "Devansh Saxena", "authors": "Devansh Saxena, Patrick Skeba, Shion Guha, and Eric P. S. Baumer", "title": "Methods for Generating Typologies of Non/use", "comments": null, "journal-ref": null, "doi": "10.1145/3392832", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior studies of technology non-use demonstrate the need for approaches that\ngo beyond a simple binary distinction between users and non-users. This paper\nproposes a set of two different methods by which researchers can identify types\nof non/use$^{1}$ relevant to the particular sociotechnical settings they are\nstudying. These methods are demonstrated by applying them to survey data about\nFacebook non/use. The results demonstrate that the different methods proposed\nhere identify fairly comparable types of non/use. They also illustrate how the\ntwo methods make different trade offs between the granularity of the resulting\ntypology and the total sample size. The paper also demonstrates how the\ndifferent typologies resulting from these methods can be used in predictive\nmodeling, allowing for the two methods to corroborate or disconfirm results\nfrom one another. The discussion considers implications and applications of\nthese methods, both for research on technology non/use and for studying social\ncomputing more broadly.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 21:43:26 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Saxena", "Devansh", ""], ["Skeba", "Patrick", ""], ["Guha", "Shion", ""], ["Baumer", "Eric P. S.", ""]]}, {"id": "2004.04860", "submitter": "Mohtashim Baqar", "authors": "Mohtashim Baqar, Azfar Ghani, Azeem Aftab, Shahzad Karim Khawar", "title": "Brain Interface Based Wheel Chair Control System for Handicap -- An\n  advance and viable approach", "comments": "Journal Article", "journal-ref": "Asian Journal of Engineering, Sciences & Technology . Mar2016,\n  Vol. 6 Issue 1, p14-16. 3p", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents advancement towards making an efficient and viable wheel\nchair control system based on brain computer interface via electro-oculogram\n(EOG) signals. The system utilizes the movement of eye as the element of\npurpose for controlling the movement of the wheel chair. Skin-surface\nelectrodes are placed over skin for the purpose of acquiring the\nelectro-oculogram signal and with the help of differential amplifier the\nbio-potential is measured between the reference and the point of interest,\nafterwards these obtained low voltage pulses are amplified, then passed through\na sallen-key filter for noise removal and smoothening. These pulses are then\ncollected on to the micro-controller; based on these pulses motor is switched\nto move in either right or left direction. A prototype system was developed and\ntested. The system showed promising results. The test conducted showed 99.5%\nefficiency of movement in correct direction.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 00:01:12 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Baqar", "Mohtashim", ""], ["Ghani", "Azfar", ""], ["Aftab", "Azeem", ""], ["Khawar", "Shahzad Karim", ""]]}, {"id": "2004.05235", "submitter": "Hemant Surale", "authors": "Jeremy Hartmann, Hemant Bhaskar Surale, Aakar Gupta, Daniel Vogel", "title": "Using Conformity to Probe Interaction Challenges in XR Collaboration", "comments": "4 pages, 3 Figures, CHI 2018 April 21 to 26, 2018, Montreal, QC,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concept of a conformity spectrum is introduced to describe the degree to\nwhich virtualization adheres to real world physical characteristics surrounding\nthe user. This is then used to examine interaction challenges when\ncollaborating across different levels of virtuality and conformity.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 21:37:18 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Hartmann", "Jeremy", ""], ["Surale", "Hemant Bhaskar", ""], ["Gupta", "Aakar", ""], ["Vogel", "Daniel", ""]]}, {"id": "2004.05349", "submitter": "Benjamin L'Huillier", "authors": "Sung-A Jang, Benjamin L'Huillier", "title": "Rediscovering Korea's Ancient Skies: An Immersive, Interactive 3D Map of\n  Traditional Korean Constellations in the Milky Way", "comments": "4 pages, in proceedings of International Symposium on Electronic Art\n  2019 (ISEA)", "journal-ref": "proceedings of International Symposium on Electronic Art 2019\n  (ISEA), p 616-617", "doi": null, "report-no": null, "categories": "cs.HC physics.ed-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we visualized Korea's traditional constellations within an\ninteractive 3D star map we created of the Milky Way. Unlike virtual\nplanetariums based on celestial star coordinates from Earth's viewpoint, our\nvisualization enables people to experience and interact with Korean\nconstellation forms and its constituent stars in 3D space, and appreciate their\nhistorical, cultural significance from a contemporary perspective. Our\ninteractive constellation map is based on the most detailed and accurate\ninformation on the stars in our Galaxy to date, and combines our expanding\nscientific understanding of the stars with contextual information reflecting\nKorea's unique astronomical culture and heritage.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 09:31:59 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Jang", "Sung-A", ""], ["L'Huillier", "Benjamin", ""]]}, {"id": "2004.05363", "submitter": "Ralf L\\\"ammel", "authors": "John Ahlgren, Maria Eugenia Berezin, Kinga Bojarczuk, Elena Dulskyte,\n  Inna Dvortsova, Johann George, Natalija Gucevska, Mark Harman, Ralf L\\\"ammel,\n  Erik Meijer, Silvia Sapora, Justin Spahr-Summers", "title": "WES: Agent-based User Interaction Simulation on Real Infrastructure", "comments": "Author order is alphabetical. Correspondence to Mark Harman\n  (markharman@fb.com). This paper appears in GI 2020: 8th International\n  Workshop on Genetic Improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Web-Enabled Simulation (WES) research agenda, and describe\nFACEBOOK's WW system. We describe the application of WW to reliability,\nintegrity and privacy at FACEBOOK , where it is used to simulate social media\ninteractions on an infrastructure consisting of hundreds of millions of lines\nof code. The WES agenda draws on research from many areas of study, including\nSearch Based Software Engineering, Machine Learning, Programming Languages,\nMulti Agent Systems, Graph Theory, Game AI, and AI Assisted Game Play. We\nconclude with a set of open problems and research challenges to motivate wider\ninvestigation.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 10:50:34 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Ahlgren", "John", ""], ["Berezin", "Maria Eugenia", ""], ["Bojarczuk", "Kinga", ""], ["Dulskyte", "Elena", ""], ["Dvortsova", "Inna", ""], ["George", "Johann", ""], ["Gucevska", "Natalija", ""], ["Harman", "Mark", ""], ["L\u00e4mmel", "Ralf", ""], ["Meijer", "Erik", ""], ["Sapora", "Silvia", ""], ["Spahr-Summers", "Justin", ""]]}, {"id": "2004.05502", "submitter": "Babak Naderi", "authors": "Babak Naderi, Sebastian M\\\"oller", "title": "Application of Just-Noticeable Difference in Quality as Environment\n  Suitability Test for Crowdsourcing Speech Quality Assessment Task", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": "10.1109/QoMEX48832.2020.9123093", "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing micro-task platforms facilitate subjective media quality\nassessment by providing access to a highly scale-able, geographically\ndistributed and demographically diverse pool of crowd workers. Those workers\nparticipate in the experiment remotely from their own working environment,\nusing their own hardware. In the case of speech quality assessment, preliminary\nwork showed that environmental noise at the listener's side and the listening\ndevice (loudspeaker or headphone) significantly affect perceived quality, and\nconsequently the reliability and validity of subjective ratings. As a\nconsequence, ITU-T Rec. P.808 specifies requirements for the listening\nenvironment of crowd workers when assessing speech quality. In this paper, we\npropose a new Just Noticeable Difference of Quality (JNDQ) test as a remote\nscreening method for assessing the suitability of the work environment for\nparticipating in speech quality assessment tasks. In a laboratory experiment,\nparticipants performed this JNDQ test with different listening devices in\ndifferent listening environments, including a silent room according to ITU-T\nRec. P.800 and a simulated background noise scenario. Results show a\nsignificant impact of the environment and the listening device on the JNDQ\nthreshold. Thus, the combination of listening device and background noise needs\nto be screened in a crowdsourcing speech quality test. We propose a minimum\nthreshold of our JNDQ test as an easily applicable screening method for this\npurpose.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 22:37:59 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Naderi", "Babak", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2004.05838", "submitter": "Christian Marzahl", "authors": "Christian Marzahl, Christof A. Bertram, Marc Aubreville, Anne Petrick,\n  Kristina Weiler, Agnes C. Gl\\\"asel, Marco Fragoso, Sophie Merz, Florian\n  Bartenschlager, Judith Hoppe, Alina Langenhagen, Anne Jasensky, J\\\"orn Voigt,\n  Robert Klopfleisch, Andreas Maier", "title": "Are fast labeling methods reliable? A case study of computer-aided\n  expert annotations on microscopy slides", "comments": "10 pages, send to MICCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-learning-based pipelines have shown the potential to revolutionalize\nmicroscopy image diagnostics by providing visual augmentations to a trained\npathology expert. However, to match human performance, the methods rely on the\navailability of vast amounts of high-quality labeled data, which poses a\nsignificant challenge. To circumvent this, augmented labeling methods, also\nknown as expert-algorithm-collaboration, have recently become popular. However,\npotential biases introduced by this operation mode and their effects for\ntraining neuronal networks are not entirely understood. This work aims to shed\nlight on some of the effects by providing a case study for three pathologically\nrelevant diagnostic settings. Ten trained pathology experts performed a\nlabeling tasks first without and later with computer-generated augmentation. To\ninvestigate different biasing effects, we intentionally introduced errors to\nthe augmentation. Furthermore, we developed a novel loss function which\nincorporates the experts' annotation consensus in the training of a deep\nlearning classifier. In total, the pathology experts annotated 26,015 cells on\n1,200 images in this novel annotation study. Backed by this extensive data set,\nwe found that the consensus of multiple experts and the deep learning\nclassifier accuracy, was significantly increased in the computer-aided setting,\nversus the unaided annotation. However, a significant percentage of the\ndeliberately introduced false labels was not identified by the experts.\nAdditionally, we showed that our loss function profited from multiple experts\nand outperformed conventional loss functions. At the same time, systematic\nerrors did not lead to a deterioration of the trained classifier accuracy.\nFurthermore, a classifier trained with annotations from a single expert with\ncomputer-aided support can outperform the combined annotations from up to nine\nexperts.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 09:36:43 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Marzahl", "Christian", ""], ["Bertram", "Christof A.", ""], ["Aubreville", "Marc", ""], ["Petrick", "Anne", ""], ["Weiler", "Kristina", ""], ["Gl\u00e4sel", "Agnes C.", ""], ["Fragoso", "Marco", ""], ["Merz", "Sophie", ""], ["Bartenschlager", "Florian", ""], ["Hoppe", "Judith", ""], ["Langenhagen", "Alina", ""], ["Jasensky", "Anne", ""], ["Voigt", "J\u00f6rn", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "2004.05878", "submitter": "Anastasia Kovalkov", "authors": "Anastasia Kovalkov, Avi Segal and Kobi Gal", "title": "In the Eye of the Beholder? Detecting Creativity in Visual Programming\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual programming environments are increasingly part of the curriculum in\nschools. Their potential for promoting creative thinking of students is an\nimportant factor in their adoption. However, there does not exist a standard\napproach for detecting creativity in students' programming behavior, and\nanalyzing programs manually requires human expertise and is time consuming.\nThis work provides a computational tool for measuring creativity in visual\nprogramming that combines theory from the literature with data mining\napproaches. It adapts the classical dimensions of creative processes to our\nsetting, as well as considering new aspects such as visual elements of the\nprojects. We apply this approach to the Scratch programming environment,\nmeasuring the creativity score of hundreds of projects. We show that current\nmetrics of computational thinking in Scratch fail to capture important aspects\nof creativity, such as the visual artifacts of projects. Interviews conducted\nwith Scratch teachers validate our approach.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 15:22:10 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Kovalkov", "Anastasia", ""], ["Segal", "Avi", ""], ["Gal", "Kobi", ""]]}, {"id": "2004.05880", "submitter": "Saiki Sarkar Sarkar", "authors": "Shanta Khatun, Fahim Hossain Saiki and Milon Biswas", "title": "SecureIT using Firebase, Google map and Node.Js", "comments": "17 pages", "journal-ref": null, "doi": "10.31234/osf.io/uhvew", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is about describing the features of a software that was developed\nfor its user safety which we called SecureIT is a android based soft ware using\nAndroid SDK along with Firebase and Google map SDK along with Node.Js. The aim\nof developing this project was to make sure and taking its users safety to a\nnext level. Actually now a days some crime incidents like rapes, fire accidents\nand snatchings are very common and we believe many of those can be prevented if\nvictim got support at the right time. According to the well known daily news\npaper of Bangladesh The Daily Star there were about 1413 women was rapped where\n76 women were dead in 2019. On the same pa-per it also said that in 2018 and\n2017 the number of rapes were 732 and 818 where we can easily get that the\nnumber increases to almost double in 2019. Where we get a point that if those\ngirls get support or get people known about their location at the right time\nthey might get rid of the situation and the number of rapes could be reduced a\nlot because we all know that now a days using mo-bile smartphone is too easy\nfor people. Although leading Chinese mobile phone company Xiaomi introduced a\nnew feature Emergency SOS service in their mobile phones at the end of 2018\nwith a MIUI 10 update but this feature is only limited to the Xiaomi phones and\nit was well advertised as well. This paper will briefly describe all the\nfeatures of our software and its usages.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 21:33:45 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Khatun", "Shanta", ""], ["Saiki", "Fahim Hossain", ""], ["Biswas", "Milon", ""]]}, {"id": "2004.05886", "submitter": "Sao Mai Nguyen", "authors": "Sao Mai Nguyen, Nathalie Collot-Lavenne (CHU - BREST), Christophe Lohr\n  (INFO), S\\'ebastien Guillon (IMT Atlantique), Patricio Tula (IMT Atlantique),\n  Alvaro Paez (IMT Atlantique), Mouad Bouaida (IMT Atlantique), Arthus Anin\n  (IMT Atlantique), Saad El Qacemi (IMT Atlantique)", "title": "An implementation of an imitation game with ASD children to learn\n  nursery rhymes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have suggested that being imitated by an adult is an\neffective intervention with children with autism and developmental delay. The\npurpose of this study is to investigate if an imitation game with a robot can\narise interest from children and constitute an effective tool to be used in\nclinical activities. In this paper, we describe the design of our nursery rhyme\nimitation game, its implementation based on RGB image pose recognition and the\npreliminary tests we performed.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 06:50:35 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Nguyen", "Sao Mai", "", "CHU - BREST"], ["Collot-Lavenne", "Nathalie", "", "CHU - BREST"], ["Lohr", "Christophe", "", "INFO"], ["Guillon", "S\u00e9bastien", "", "IMT Atlantique"], ["Tula", "Patricio", "", "IMT Atlantique"], ["Paez", "Alvaro", "", "IMT Atlantique"], ["Bouaida", "Mouad", "", "IMT Atlantique"], ["Anin", "Arthus", "", "IMT Atlantique"], ["Qacemi", "Saad El", "", "IMT Atlantique"]]}, {"id": "2004.06286", "submitter": "Dongrui Wu", "authors": "Dongrui Wu and Yifan Xu and Bao-Liang Lu", "title": "Transfer Learning for EEG-Based Brain-Computer Interfaces: A Review of\n  Progress Made Since 2016", "comments": null, "journal-ref": "IEEE Transactions on Cognitive and Developmental Systems, 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A brain-computer interface (BCI) enables a user to communicate with a\ncomputer directly using brain signals. The most common non-invasive BCI\nmodality, electroencephalogram (EEG), is sensitive to noise/artifact and\nsuffers between-subject/within-subject non-stationarity. Therefore, it is\ndifficult to build a generic pattern recognition model in an EEG-based BCI\nsystem that is optimal for different subjects, during different sessions, for\ndifferent devices and tasks. Usually, a calibration session is needed to\ncollect some training data for a new subject, which is time-consuming and user\nunfriendly. Transfer learning (TL), which utilizes data or knowledge from\nsimilar or relevant subjects/sessions/devices/tasks to facilitate learning for\na new subject/session/device/task, is frequently used to reduce the amount of\ncalibration effort. This paper reviews journal publications on TL approaches in\nEEG-based BCIs in the last few years, i.e., since 2016. Six paradigms and\napplications -- motor imagery, event-related potentials, steady-state visual\nevoked potentials, affective BCIs, regression problems, and adversarial attacks\n-- are considered. For each paradigm/application, we group the TL approaches\ninto cross-subject/session, cross-device, and cross-task settings and review\nthem separately. Observations and conclusions are made at the end of the paper,\nwhich may point to future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 16:44:55 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 22:13:09 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 22:19:40 GMT"}, {"version": "v4", "created": "Fri, 3 Jul 2020 23:34:11 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wu", "Dongrui", ""], ["Xu", "Yifan", ""], ["Lu", "Bao-Liang", ""]]}, {"id": "2004.06435", "submitter": "Abishek Puri", "authors": "Abishek Puri, Bon Kyung Ku, Yong Wang, Huamin Qu", "title": "RankBooster: Visual Analysis of Ranking Predictions", "comments": "4 Pages, Accepted as a short paper for EuroVis 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking is a natural and ubiquitous way to facilitate decision-making in\nvarious applications. However, different rankings are often used for the same\nset of entities, with each ranking method placing emphasis on different\nfactors. These factors can also be multi-dimensional in nature, compounding the\nproblem. This complexity can make it challenging for an entity which is being\nranked to understand what they can do to improve their rankings, and to analyze\nthe effect of changes in various factors to their overall rank. In this paper,\nwe present RankBooster, a novel visual analytics system to help users\nconveniently investigate ranking predictions. We take university rankings as an\nexample and focus on helping universities to better explore their rankings,\nwhere they can compare themselves to their rivals in key areas as well as\noverall. Novel visualizations are proposed to enable efficient analysis of\nrankings, including a Scenario Analysis View to show a high-level summary of\ndifferent ranking scenarios, a Relationship View to visualize the influence of\neach attribute on different indicators and a Rival View to compare the ranking\nof a university and those of its rivals. A case study demonstrates the\nusefulness and effectiveness of RankBooster in facilitating the visual analysis\nof ranking predictions and helping users better understand their current\nsituation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 11:51:35 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Puri", "Abishek", ""], ["Ku", "Bon Kyung", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2004.06510", "submitter": "Brian Subirana", "authors": "Brian Subirana, Ferran Hueto, Prithvi Rajasekaran, Jordi Laguarta,\n  Susana Puig, Josep Malvehy, Oriol Mitja, Antoni Trilla, Carlos Iv\\'an Moreno,\n  Jos\\'e Francisco Mu\\~noz Valle, Ana Esther Mercado Gonz\\'alez, Barbara\n  Vizmanos, Sanjay Sarma", "title": "Hi Sigma, do I have the Coronavirus?: Call for a New Artificial\n  Intelligence Approach to Support Health Care Professionals Dealing With The\n  COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": "MIT Auto-ID Laboratory, Report 2020-4-10-1", "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just like your phone can detect what song is playing in crowded spaces, we\nshow that Artificial Intelligence transfer learning algorithms trained on cough\nphone recordings results in diagnostic tests for COVID-19. To gain adoption by\nthe health care community, we plan to validate our results in a clinical trial\nand three other venues in Mexico, Spain and the USA . However, if we had data\nfrom other on-going clinical trials and volunteers, we may do much more. For\nexample, for confirmed stay-at-home COVID-19 patients, a longitudinal audio\ntest could be developed to determine contact-with-hospital recommendations, and\nfor the most critical COVID-19 patients a success ratio forecast test,\nincluding patient clinical data, to prioritize ICU allocation. As a challenge\nto the engineering community and in the context of our clinical trial, the\nauthors suggest distributing cough recordings daily, hoping other trials and\ncrowdsourcing users will contribute more data. Previous approaches to complex\nAI tasks have either used a static dataset or were private efforts led by large\ncorporations. All existing COVID-19 trials published also follow this paradigm.\nInstead, we suggest a novel open collective approach to large-scale real-time\nhealth care AI. We will be posting updates at https://opensigma.mit.edu. Our\npersonal view is that our approach is the right one for large scale pandemics,\nand therefore is here to stay - will you join?\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 21:03:49 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Subirana", "Brian", ""], ["Hueto", "Ferran", ""], ["Rajasekaran", "Prithvi", ""], ["Laguarta", "Jordi", ""], ["Puig", "Susana", ""], ["Malvehy", "Josep", ""], ["Mitja", "Oriol", ""], ["Trilla", "Antoni", ""], ["Moreno", "Carlos Iv\u00e1n", ""], ["Valle", "Jos\u00e9 Francisco Mu\u00f1oz", ""], ["Gonz\u00e1lez", "Ana Esther Mercado", ""], ["Vizmanos", "Barbara", ""], ["Sarma", "Sanjay", ""]]}, {"id": "2004.06594", "submitter": "Sarah Janboecke", "authors": "Sarah Janboecke, Alina Gawlitta, Judith Doerrenbaecher, Marc\n  Hassenzahl", "title": "Finding the Inner Clock: A Chronobiology-based Calendar", "comments": "7 pages to be published in the Extended Abstracts of the 2020 CHI\n  ConferenceConference on Human Factors in Computing Systems", "journal-ref": null, "doi": "10.1145/3334480.3382830", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time and its lack of play a central role in our everyday lives. Despite\nincreasing productivity, many people experience time stress, exhaustion and a\nlonging for time affluence, and at the same time, a fear of not being busy\nenough. All this leads to a neglect of natural time, especially the patterns\nand rhythms created by physiological processes, subsumed under the heading of\nchronobiology. The present paper presents and evaluates a calendar application,\nwhich uses chronobiological knowledge to support people s planning activities.\nParticipants found our calendar to be interesting and engaging. It especially\nmade them think more about their bodies and appropriate times for particular\nactivities. All in all, it supported participants in negotiating. external\ndemands and personal health and wellbeing. This shows that technology does not\nnecessarily has to be neutral or even further current (mal-)practices. Our\ncalendar cares about changing perspectives and thus about enhancing users\nwellbeing.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:25:24 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Janboecke", "Sarah", ""], ["Gawlitta", "Alina", ""], ["Doerrenbaecher", "Judith", ""], ["Hassenzahl", "Marc", ""]]}, {"id": "2004.06848", "submitter": "Kyle Olszewski", "authors": "Kyle Olszewski, Duygu Ceylan, Jun Xing, Jose Echevarria, Zhili Chen,\n  Weikai Chen, Hao Li", "title": "Intuitive, Interactive Beard and Hair Synthesis with Generative Models", "comments": "To be presented in the 2020 Conference on Computer Vision and Pattern\n  Recognition (CVPR 2020, Oral Presentation). Supplementary video can be seen\n  at: https://www.youtube.com/watch?v=v4qOtBATrvM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive approach to synthesizing realistic variations in\nfacial hair in images, ranging from subtle edits to existing hair to the\naddition of complex and challenging hair in images of clean-shaven subjects. To\ncircumvent the tedious and computationally expensive tasks of modeling,\nrendering and compositing the 3D geometry of the target hairstyle using the\ntraditional graphics pipeline, we employ a neural network pipeline that\nsynthesizes realistic and detailed images of facial hair directly in the target\nimage in under one second. The synthesis is controlled by simple and sparse\nguide strokes from the user defining the general structural and color\nproperties of the target hairstyle. We qualitatively and quantitatively\nevaluate our chosen method compared to several alternative approaches. We show\ncompelling interactive editing results with a prototype user interface that\nallows novice users to progressively refine the generated image to match their\ndesired hairstyle, and demonstrate that our approach also allows for flexible\nand high-fidelity scalp hair synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 01:20:10 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Olszewski", "Kyle", ""], ["Ceylan", "Duygu", ""], ["Xing", "Jun", ""], ["Echevarria", "Jose", ""], ["Chen", "Zhili", ""], ["Chen", "Weikai", ""], ["Li", "Hao", ""]]}, {"id": "2004.06883", "submitter": "Jon McCormack", "authors": "Nina Rajcic and Jon McCormack", "title": "Mirror Ritual: Human-Machine Co-Construction of Emotion", "comments": "Paper presented at ACM TEI Conference 2020 Arts Track, Sydney\n  Australia", "journal-ref": "TEI '20: Proceedings of the Fourteenth International Conference on\n  Tangible, Embedded, and Embodied Interaction, February 2020, Pages 697-702", "doi": "10.1145/3374920.3375293", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mirror Ritual is an interactive installation that challenges the existing\nparadigms in our understanding of human emotion and machine perception. In\ncontrast to prescriptive interfaces, the work's real-time affective interface\nengages the audience in the iterative conceptualisation of their emotional\nstate through the use of affectively-charged machine generated poetry. The\naudience are encouraged to make sense of the mirror's poetry by framing it with\nrespect to their recent life experiences, effectively `putting into words'\ntheir felt emotion. This process of affect labelling and contextualisation\nworks to not only regulate emotion, but helps to construct the rich personal\nnarratives that constitute human identity.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 05:09:38 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Rajcic", "Nina", ""], ["McCormack", "Jon", ""]]}, {"id": "2004.06894", "submitter": "Haizi Yu", "authors": "Haizi Yu, Heinrich Taube, James A. Evans, Lav R. Varshney", "title": "Human Evaluation of Interpretability: The Case of AI-Generated Music\n  Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability of machine learning models has gained more and more\nattention among researchers in the artificial intelligence (AI) and\nhuman-computer interaction (HCI) communities. Most existing work focuses on\ndecision making, whereas we consider knowledge discovery. In particular, we\nfocus on evaluating AI-discovered knowledge/rules in the arts and humanities.\nFrom a specific scenario, we present an experimental procedure to collect and\nassess human-generated verbal interpretations of AI-generated music\ntheory/rules rendered as sophisticated symbolic/numeric objects. Our goal is to\nreveal both the possibilities and the challenges in such a process of decoding\nexpressive messages from AI sources. We treat this as a first step towards 1)\nbetter design of AI representations that are human interpretable and 2) a\ngeneral methodology to evaluate interpretability of AI-discovered knowledge\nrepresentations.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 06:03:34 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Yu", "Haizi", ""], ["Taube", "Heinrich", ""], ["Evans", "James A.", ""], ["Varshney", "Lav R.", ""]]}, {"id": "2004.07031", "submitter": "Guotai Wang", "authors": "Qi Duan, Guotai Wang, Rui Wang, Chao Fu, Xinjun Li, Maoliang Gong,\n  Xinglong Liu, Qing Xia, Xiaodi Huang, Zhiqiang Hu, Ning Huang, Shaoting Zhang", "title": "SenseCare: A Research Platform for Medical Image Informatics and\n  Interactive 3D Visualization", "comments": "11 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical research on smart healthcare has an increasing demand for\nintelligent and clinic-oriented medical image computing algorithms and\nplatforms that support various applications. To this end, we have developed\nSenseCare research platform for smart healthcare, which is designed to boost\ntranslational research on intelligent diagnosis and treatment planning in\nvarious clinical scenarios. To facilitate clinical research with Artificial\nIntelligence (AI), SenseCare provides a range of AI toolkits for different\ntasks, including image segmentation, registration, lesion and landmark\ndetection from various image modalities ranging from radiology to pathology. In\naddition, SenseCare is clinic-oriented and supports a wide range of clinical\napplications such as diagnosis and surgical planning for lung cancer, pelvic\ntumor, coronary artery disease, etc. SenseCare provides several appealing\nfunctions and features such as advanced 3D visualization, concurrent and\nefficient web-based access, fast data synchronization and high data security,\nmulti-center deployment, support for collaborative research, etc. In this\npaper, we will present an overview of SenseCare as an efficient platform\nproviding comprehensive toolkits and high extensibility for intelligent image\nanalysis and clinical research in different application scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 03:17:04 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Duan", "Qi", ""], ["Wang", "Guotai", ""], ["Wang", "Rui", ""], ["Fu", "Chao", ""], ["Li", "Xinjun", ""], ["Gong", "Maoliang", ""], ["Liu", "Xinglong", ""], ["Xia", "Qing", ""], ["Huang", "Xiaodi", ""], ["Hu", "Zhiqiang", ""], ["Huang", "Ning", ""], ["Zhang", "Shaoting", ""]]}, {"id": "2004.07032", "submitter": "Sarah Janboecke", "authors": "Sarah Janboecke, Diana Loeffler, Marc Hassenzahl", "title": "Using Experimental Vignettes to Study Early-Stage Automation Adoption", "comments": "5 pages to be published in Extended Abstracts of the 2020 CHI\n  Conference on Human Factors in Computing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When discussing the future of work and in detail the concerns of workers\nwithin and beyond established workplace settings, technology-wise we act on\nrather new ground. Especially preserving a meaningful work environment gains\nnew importance when introducing disruptive technologies. We sometimes do not\neven have the technology which effects we are willing to discuss. To measure\nimplications for employees and thus create meaningful design variants we need\nto test systems and their effects before developing them. Confronted with the\nsame problem we used the experimental vignette method to study the effects of\nAI use in work contexts. During the workshop, we will report our experiences.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:15:15 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Janboecke", "Sarah", ""], ["Loeffler", "Diana", ""], ["Hassenzahl", "Marc", ""]]}, {"id": "2004.07044", "submitter": "Muhammad Nazrul Islam", "authors": "Muhammad Nazrul Islam, Md. Mahboob Karim, Toki Tahmid Inan, A. K. M.\n  Najmul Islam", "title": "Investigating usability of mobile health applications in Bangladesh", "comments": "13 pages, 4 figures", "journal-ref": "BMC Medical Informatics and Decision Making, (2020) 20:19", "doi": "10.1186/s12911-020-1033-3", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Lack of usability can be a major barrier for the rapid adoption\nof mobile services. Therefore, the purpose of this paper is to investigate the\nusability of Mobile Health applications in Bangladesh.\n  Method: We followed a 3-stage approach in our research. First, we conducted a\nkeyword-based application search in the popular app stores. We followed the\naffinity diagram approach and clustered the found applications into nine\ngroups. Second, we randomly selected four apps from each group (36 apps in\ntotal) and conducted a heuristic evaluation. Finally, we selected the highest\ndownloaded app from each group and conducted user studies with 30 participants.\n  Results: We found 61% usability problems are catastrophe or major in nature\nfrom heuristic inspection. The most (21%) violated heuristic is aesthetic and\nminimalist design. The user studies revealed low System Usability Scale (SUS)\nscores for those apps that had a high number of usability problems based on the\nheuristic evaluation. Thus, the results of heuristic evaluation and user\nstudies complement each other.\n  Conclusion: Overall, the findings suggest that the usability of the mobile\nhealth apps in Bangladesh is not satisfactory in general and could be a\npotential barrier for wider adoption of mobile health services.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:21:27 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Islam", "Muhammad Nazrul", ""], ["Karim", "Md. Mahboob", ""], ["Inan", "Toki Tahmid", ""], ["Islam", "A. K. M. Najmul", ""]]}, {"id": "2004.07132", "submitter": "Maximillian Langenkamp", "authors": "Max Langenkamp, Allan Costa, Chris Cheung", "title": "Hiring Fairly in the Age of Algorithms", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Widespread developments in automation have reduced the need for human input.\nHowever, despite the increased power of machine learning, in many contexts\nthese programs make decisions that are problematic. Biases within data and\nopaque models have amplified human prejudices, giving rise to such tools as\nAmazon's (now defunct) experimental hiring algorithm, which was found to\nconsistently downgrade resumes when the word \"women's\" was added before an\nactivity. This article critically surveys the existing legal and technological\nlandscape surrounding algorithmic hiring. We argue that the negative impact of\nhiring algorithms can be mitigated by greater transparency from the employers\nto the public, which would enable civil advocate groups to hold employers\naccountable, as well as allow the U.S. Department of Justice to litigate. Our\nmain contribution is a framework for automated hiring transparency, algorithmic\ntransparency reports, which employers using automated hiring software would be\nrequired to publish by law. We also explain how existing regulations in\nemployment and trade secret law can be extended by the Equal Employment\nOpportunity Commission and Congress to accommodate these reports.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 14:58:52 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Langenkamp", "Max", ""], ["Costa", "Allan", ""], ["Cheung", "Chris", ""]]}, {"id": "2004.07198", "submitter": "Pawe{\\l} W. Wo\\'zniak", "authors": "Jasmin Niess, Pawe{\\l} W. Wo\\'zniak", "title": "Embracing Companion Technologies", "comments": "Proceedings of the 11th Nordic Conference on Human-Computer\n  Interaction: Shaping Experiences, Shaping Society (NordiCHI '20), October\n  25--29, 2020, Tallinn, Estonia", "journal-ref": null, "doi": "10.1145/3419249.3420134", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an increasing number of interactive devices offer human-like assistance,\nthere is a growing need to understand the human experience of interactive\nagents. When interactive artefacts with human-like features become intertwined\nin our everyday experience, we need to make sure that they assume the right\nroles and contribute to our wellbeing. In this theoretical exploration, we\npropose a reframing of our understanding of interactions with everyday\ntechnologies by proposing the metaphor of digital companions. We employ the\ntheory in the philosophy of empathy to propose a framework for understanding\nhow users develop relationships with digital agents. The experiential framework\nfor companion technologies provides connections between the users'\npsychological needs and companion-like features of interactive systems. Our\nwork provides a theoretical basis for rethinking the user experience of\neveryday artefacts with a humanistic mindset and poses future challenges for\nHCI.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 16:56:21 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 10:14:15 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 16:27:24 GMT"}], "update_date": "2020-09-11", "authors_parsed": [["Niess", "Jasmin", ""], ["Wo\u017aniak", "Pawe\u0142 W.", ""]]}, {"id": "2004.07359", "submitter": "Aakash Gautam", "authors": "Aakash Gautam", "title": "Usable, Acceptable, Appropriable: Towards Practicable Privacy", "comments": "6 pages, position paper submitted to the CHI 2020 workshop on\n  Networked Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A majority of the work on digital privacy and security has focused on users\nfrom developed countries who account for only around 20\\% of the global\npopulation. Moreover, the privacy needs for population that is already\nmarginalized and vulnerable differ from users who have privilege to access a\ngreater social support system. We reflect on our experiences of introducing\ncomputers and the Internet to a group of sex-trafficking survivors in Nepal and\nhighlight a few socio-political factors that have influenced the design space\naround digital privacy. These factors include the population's limited digital\nand text literacy skills and the fear of stigma against trafficked persons\nwidely prevalent in Nepali society. We underscore the need to widen our\nperspective by focusing on practicable privacy, that is, privacy practices that\nare (1) usable, (2) acceptable, and (3) appropriable.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 21:39:33 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Gautam", "Aakash", ""]]}, {"id": "2004.07594", "submitter": "Sebastian Wallk\\\"otter", "authors": "Sebastian Wallkotter, Rebecca Stower, Arvid Kappas, Ginevra Castellano", "title": "A Robot by Any Other Frame: Framing and Behaviour Influence Mind\n  Perception in Virtual but not Real-World Environments", "comments": null, "journal-ref": null, "doi": "10.1145/3319502.3374800", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mind perception in robots has been an understudied construct in human-robot\ninteraction (HRI) compared to similar concepts such as anthropomorphism and the\nintentional stance. In a series of three experiments, we identify two factors\nthat could potentially influence mind perception and moral concern in robots:\nhow the robot is introduced (framing), and how the robot acts (social\nbehaviour). In the first two online experiments, we show that both framing and\nbehaviour independently influence participants' mind perception. However, when\nwe combined both variables in the following real-world experiment, these\neffects failed to replicate. We hence identify a third factor post-hoc: the\nonline versus real-world nature of the interactions. After analysing potential\nconfounds, we tentatively suggest that mind perception is harder to influence\nin real-world experiments, as manipulations are harder to isolate compared to\nvirtual experiments, which only provide a slice of the interaction.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 11:08:45 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Wallkotter", "Sebastian", ""], ["Stower", "Rebecca", ""], ["Kappas", "Arvid", ""], ["Castellano", "Ginevra", ""]]}, {"id": "2004.07662", "submitter": "Guoming Tang", "authors": "Guoming Tang, Kui Wu, Yangjing Wu, Hanlong Liao, Deke Guo, Yi Wang", "title": "Quantifying Low-Battery Anxiety of Mobile Users and Its Impacts on Video\n  Watching Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People nowadays are increasingly dependent on mobile phones for daily\ncommunication, study, and business. Along with this it incurs the low-battery\nanxiety (LBA). Although having been unveiled for a while, LBA has not been\nthoroughly investigated yet. Without a better understanding of LBA, it would be\ndifficult to precisely validate energy saving and management techniques in\nterms of alleviating LBA and enhancing Quality of Experience (QoE) of mobile\nusers. To fill the gap, we conduct an investigation over 2000+ mobile users,\nlook into their feelings and reactions towards LBA, and quantify their anxiety\ndegree during the draining of battery power. As a case study, we also\ninvestigate the impact of LBA on user's behavior at video watching, and with\nthe massive collected answers we are able to quantify user's abandoning\nlikelihood of attractive videos versus the battery status of mobile phone. The\nempirical findings and quantitative models obtained in this work not only\ndisclose the characteristics of LBA among modern mobile users, but also provide\nvaluable references for the design, evaluation, and improvement of QoE-aware\nmobile applications and services.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:54:34 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Tang", "Guoming", ""], ["Wu", "Kui", ""], ["Wu", "Yangjing", ""], ["Liao", "Hanlong", ""], ["Guo", "Deke", ""], ["Wang", "Yi", ""]]}, {"id": "2004.07668", "submitter": "Wim Martens", "authors": "Angela Bonifati, Giovanna Guerrini, Carsten Lutz, Wim Martens, Lara\n  Mazilu, Norman Paton, Marcos Antonio Vaz Salles, Marc H. Scholl, Yongluan\n  Zhou", "title": "Holding a Conference Online and Live due to COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint EDBT/ICDT conference (International Conference on Extending\nDatabase Technology/International Conference on Database Theory) is a well\nestablished conference series on data management, with annual meetings in the\nsecond half of March that attract 250 to 300 delegates. Three weeks before\nEDBT/ICDT 2020 was planned to take place in Copenhagen, the rapidly developing\nCovid-19 pandemic led to the decision to cancel the face-to-face event. In the\ninterest of the research community, it was decided to move the conference\nonline while trying to preserve as much of the real-life experience as\npossible. As far as we know, we are one of the first conferences that moved to\na fully synchronous online experience due to the COVID-19 outbreak. With fully\nsynchronous, we mean that participants jointly listened to presentations, had\nlive Q&A, and attended other live events associated with the conference. In\nthis report, we share our decisions, experiences, and lessons learned.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 14:05:24 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 05:52:37 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2020 07:03:53 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Bonifati", "Angela", ""], ["Guerrini", "Giovanna", ""], ["Lutz", "Carsten", ""], ["Martens", "Wim", ""], ["Mazilu", "Lara", ""], ["Paton", "Norman", ""], ["Salles", "Marcos Antonio Vaz", ""], ["Scholl", "Marc H.", ""], ["Zhou", "Yongluan", ""]]}, {"id": "2004.07716", "submitter": "Nitish Nag", "authors": "Vaibhav Pandey, Nitish Nag, Ramesh Jain", "title": "Continuous Health Interface Event Retrieval", "comments": "ACM International Conference on Multimedia Retrieval 2020 (ICMR\n  2020), held in Dublin, Ireland from June 8-11, 2020", "journal-ref": "ICMR 2020: Proceedings of the 2020 International Conference on\n  Multimedia Retrieval, June 2020, Pages 486-494", "doi": "10.1145/3372278.3390705", "report-no": null, "categories": "cs.HC cs.CY q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing the state of our health at every moment in time is critical for\nadvances in health science. Using data obtained outside an episodic clinical\nsetting is the first step towards building a continuous health estimation\nsystem. In this paper, we explore a system that allows users to combine events\nand data streams from different sources to retrieve complex biological events,\nsuch as cardiovascular volume overload. These complex events, which have been\nexplored in biomedical literature and which we call interface events, have a\ndirect causal impact on relevant biological systems. They are the interface\nthrough which the lifestyle events influence our health. We retrieve the\ninterface events from existing events and data streams by encoding domain\nknowledge using an event operator language.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 15:49:13 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Pandey", "Vaibhav", ""], ["Nag", "Nitish", ""], ["Jain", "Ramesh", ""]]}, {"id": "2004.07777", "submitter": "Bhanuka Mahanama", "authors": "Bhanuka Mahanama, Yasith Jayawardana and Sampath Jayarathna", "title": "Gaze-Net: Appearance-Based Gaze Estimation using Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on appearance based gaze estimation indicate the ability of\nNeural Networks to decode gaze information from facial images encompassing pose\ninformation. In this paper, we propose Gaze-Net: A capsule network capable of\ndecoding, representing, and estimating gaze information from ocular region\nimages. We evaluate our proposed system using two publicly available datasets,\nMPIIGaze (200,000+ images in the wild) and Columbia Gaze (5000+ images of users\nwith 21 gaze directions observed at 5 camera angles/positions). Our model\nachieves a Mean Absolute Error (MAE) of 2.84$^\\circ$ for Combined angle error\nestimate within dataset for MPI-IGaze dataset. Further, model achieves a MAE of\n10.04$^\\circ$ for across dataset gaze estimation error for Columbia gaze\ndataset. Through transfer learning, the error is reduced to 5.9$^\\circ$. The\nresults show this approach is promising with implications towards using\ncommodity webcams to develop low-cost multi-user gaze tracking systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:12:06 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Mahanama", "Bhanuka", ""], ["Jayawardana", "Yasith", ""], ["Jayarathna", "Sampath", ""]]}, {"id": "2004.07800", "submitter": "Akash Mehra", "authors": "Akash Mehra, Jerome R. Bellegarda, Ojas Bapat, Partha Lal, Xin Wang", "title": "Leveraging GANs to Improve Continuous Path Keyboard Input Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous path keyboard input has higher inherent ambiguity than standard\ntapping, because the path trace may exhibit not only local\novershoots/undershoots (as in tapping) but also, depending on the user,\nsubstantial mid-path excursions. Deploying a robust solution thus requires a\nlarge amount of high-quality training data, which is difficult to\ncollect/annotate. In this work, we address this challenge by using GANs to\naugment our training corpus with user-realistic synthetic data. Experiments\nshow that, even though GAN-generated data does not capture all the\ncharacteristics of real user data, it still provides a substantial boost in\naccuracy at a 5:1 GAN-to-real ratio. GANs therefore inject more robustness in\nthe model through greatly increased word coverage and path diversity.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 22:42:29 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 19:55:45 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Mehra", "Akash", ""], ["Bellegarda", "Jerome R.", ""], ["Bapat", "Ojas", ""], ["Lal", "Partha", ""], ["Wang", "Xin", ""]]}, {"id": "2004.07964", "submitter": "Florian Heimerl", "authors": "Michael Gleicher, Aditya Barve, Xinyi Yu, Florian Heimerl", "title": "Boxer: Interactive Comparison of Classifier Results", "comments": "accepted to Computer Graphic Forum (CGF) to be presented at\n  Eurographics Conference on Visualization (EuroVis) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning practitioners often compare the results of different\nclassifiers to help select, diagnose and tune models. We present Boxer, a\nsystem to enable such comparison. Our system facilitates interactive\nexploration of the experimental results obtained by applying multiple\nclassifiers to a common set of model inputs. The approach focuses on allowing\nthe user to identify interesting subsets of training and testing instances and\ncomparing performance of the classifiers on these subsets. The system couples\nstandard visual designs with set algebra interactions and comparative elements.\nThis allows the user to compose and coordinate views to specify subsets and\nassess classifier performance on them. The flexibility of these compositions\nallow the user to address a wide range of scenarios in developing and assessing\nclassifiers. We demonstrate Boxer in use cases including model selection,\ntuning, fairness assessment, and data quality diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 21:05:34 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Gleicher", "Michael", ""], ["Barve", "Aditya", ""], ["Yu", "Xinyi", ""], ["Heimerl", "Florian", ""]]}, {"id": "2004.07993", "submitter": "Maria Glenski", "authors": "Dustin Arendt, Zhuanyi Huang, Prasha Shrestha, Ellyn Ayton, Maria\n  Glenski, Svitlana Volkova", "title": "CrossCheck: Rapid, Reproducible, and Interpretable Model Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluation beyond aggregate performance metrics, e.g. F1-score, is crucial to\nboth establish an appropriate level of trust in machine learning models and\nidentify future model improvements. In this paper we demonstrate CrossCheck, an\ninteractive visualization tool for rapid crossmodel comparison and reproducible\nerror analysis. We describe the tool and discuss design and implementation\ndetails. We then present three use cases (named entity recognition, reading\ncomprehension, and clickbait detection) that show the benefits of using the\ntool for model evaluation. CrossCheck allows data scientists to make informed\ndecisions to choose between multiple models, identify when the models are\ncorrect and for which examples, investigate whether the models are making the\nsame mistakes as humans, evaluate models' generalizability and highlight\nmodels' limitations, strengths and weaknesses. Furthermore, CrossCheck is\nimplemented as a Jupyter widget, which allows rapid and convenient integration\ninto data scientists' model development workflows.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 23:29:43 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Arendt", "Dustin", ""], ["Huang", "Zhuanyi", ""], ["Shrestha", "Prasha", ""], ["Ayton", "Ellyn", ""], ["Glenski", "Maria", ""], ["Volkova", "Svitlana", ""]]}, {"id": "2004.08010", "submitter": "Tica Lin", "authors": "Tica Lin, Yalong Yang, Johanna Beyer, Hanspeter Pfister", "title": "SportsXR -- Immersive Analytics in Sports", "comments": "7 pages, 5 figures", "journal-ref": "4th Workshop on Immersive Analytics: Envisioning Future\n  Productivity for Immersive Analytics at ACM CHI 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present our initial investigation of key challenges and potentials of\nimmersive analytics (IA) in sports, which we call SportsXR. Sports are usually\nhighly dynamic and collaborative by nature, which makes real-time decision\nmaking ubiquitous. However, there is limited support for athletes and coaches\nto make informed and clear-sighted decisions in real-time. SportsXR aims to\nsupport situational awareness for better and more agile decision making in\nsports. In this paper, we identify key challenges in SportsXR, including data\ncollection, in-game decision making, situated sport-specific visualization\ndesign, and collaborating with domain experts. We then present potential user\nscenarios in training, coaching, and fan experiences. This position paper aims\nto inform and inspire future SportsXR research.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 00:50:59 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Lin", "Tica", ""], ["Yang", "Yalong", ""], ["Beyer", "Johanna", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2004.08029", "submitter": "Jaybie de Guzman", "authors": "Jaybie A. de Guzman, Kanchana Thilakarathna, Aruna Seneviratne", "title": "Conservative Plane Releasing for Spatial Privacy Protection in Mixed\n  Reality", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR) or mixed reality (MR) platforms require spatial\nunderstanding to detect objects or surfaces, often including their structural\n(i.e. spatial geometry) and photometric (e.g. color, and texture) attributes,\nto allow applications to place virtual or synthetic objects seemingly\n\"anchored\" on to real world objects; in some cases, even allowing interactions\nbetween the physical and virtual objects. These functionalities require AR/MR\nplatforms to capture the 3D spatial information with high resolution and\nfrequency; however, these pose unprecedented risks to user privacy. Aside from\nobjects being detected, spatial information also reveals the location of the\nuser with high specificity, e.g. in which part of the house the user is. In\nthis work, we propose to leverage spatial generalizations coupled with\nconservative releasing to provide spatial privacy while maintaining data\nutility. We designed an adversary that builds up on existing place and shape\nrecognition methods over 3D data as attackers to which the proposed spatial\nprivacy approach can be evaluated against. Then, we simulate user movement\nwithin spaces which reveals more of their space as they move around utilizing\n3D point clouds collected from Microsoft HoloLens. Results show that revealing\nno more than 11 generalized planes--accumulated from successively revealed\nspaces with large enough radius, i.e. $r\\leq1.0m$--can make an adversary fail\nin identifying the spatial location of the user for at least half of the time.\nFurthermore, if the accumulated spaces are of smaller radius, i.e. each\nsuccessively revealed space is $r\\leq 0.5m$, we can release up to 29\ngeneralized planes while enjoying both better data utility and privacy.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 01:57:58 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["de Guzman", "Jaybie A.", ""], ["Thilakarathna", "Kanchana", ""], ["Seneviratne", "Aruna", ""]]}, {"id": "2004.08030", "submitter": "Predrag Lazic", "authors": "Predrag Lazic", "title": "Smartphone camera based pointer", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large screen displays are omnipresent today as a part of infrastructure for\npresentations and entertainment. Also powerful smartphones with integrated\ncamera(s) are ubiquitous. However, there are not many ways in which smartphones\nand screens can interact besides casting the video from a smartphone. In this\npaper, we present a novel idea that turns a smartphone into a direct virtual\npointer on the screen using the phone's camera. The idea and its implementation\nare simple, robust, efficient and fun to use. Besides the mathematical concepts\nof the idea we accompany the paper with a small javascript project\n(www.mobiletvgames.com) which demonstrates the possibility of the new\ninteraction technique presented as a massive multiplayer game in the HTML5\nframework.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 01:59:23 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Lazic", "Predrag", ""]]}, {"id": "2004.08198", "submitter": "Maarten Wijntjes", "authors": "Maarten W.A. Wijntjes and Mitchell van Zuijlen", "title": "Sketch-and-test: picture-centered research with p5.js assisted\n  crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relating human judgements to pictures is central to a wide variety of\nscientific disciplines. Pictures are used to evoke and study faculties of the\nhuman mind, while human input is used to label, understand and model pictorial\nrepresentations. Human input is often collected through online crowdsourcing\nexperiments. This paper discusses the usage of crowdsourcing in two major\nbranches of picture-centered research, human and computer vision, and\nidentifies novel directions such as art history and design. We demonstrate that\na wide variety of experiments can be conducted by using p5.js, a library\noriginally intended to facilitate visual creation. We report five complementary\nexperimental paradigms to illustrated the accessibility and versatility of\np5.js: Change blindness, BubbleView, 3D shape perception, Composition, and\nPerspective reconstruction. Results reveal that literature findings can be\nreproduced and novel insights can easily be achieved with the p5.js library.\nThe creative freedom of p5.js combined with low threshold access to\ncrowdsourcing seems like a powerful combination for all picture-centred\nresearch areas: perception, design, art history, communication, and beyond.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 12:16:28 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 21:58:56 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 20:21:34 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wijntjes", "Maarten W. A.", ""], ["van Zuijlen", "Mitchell", ""]]}, {"id": "2004.08289", "submitter": "Ozan Ozdenizci", "authors": "Mo Han, Ozan Ozdenizci, Ye Wang, Toshiaki Koike-Akino, Deniz Erdogmus", "title": "Disentangled Adversarial Transfer Learning for Physiological Biosignals", "comments": "42nd Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (EMBC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in wearable sensors demonstrate promising results for\nmonitoring physiological status in effective and comfortable ways. One major\nchallenge of physiological status assessment is the problem of transfer\nlearning caused by the domain inconsistency of biosignals across users or\ndifferent recording sessions from the same user. We propose an adversarial\ninference approach for transfer learning to extract disentangled\nnuisance-robust representations from physiological biosignal data in stress\nstatus level assessment. We exploit the trade-off between task-related features\nand person-discriminative information by using both an adversary network and a\nnuisance network to jointly manipulate and disentangle the learned latent\nrepresentations by the encoder, which are then input to a discriminative\nclassifier. Results on cross-subjects transfer evaluations demonstrate the\nbenefits of the proposed adversarial framework, and thus show its capabilities\nto adapt to a broader range of subjects. Finally we highlight that our proposed\nadversarial transfer learning approach is also applicable to other deep feature\nlearning frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 01:56:56 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Han", "Mo", ""], ["Ozdenizci", "Ozan", ""], ["Wang", "Ye", ""], ["Koike-Akino", "Toshiaki", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "2004.08353", "submitter": "Toby Jia-Jun Li", "authors": "Toby Jia-Jun Li, Jingya Chen, Brandon Canfield, Brad A. Myers", "title": "Privacy-Preserving Script Sharing in GUI-based\n  Programming-by-Demonstration Systems", "comments": "In the Proceedings of the ACM on Human-Computer Interaction (PACM)\n  Vol.4 No. CSCW1. (CSCW 2020)", "journal-ref": "Proc. ACM Hum.-Comput. Interact. 4, CSCW1, Article 60 (May 2020),\n  23 pages", "doi": "10.1145/3392869", "report-no": null, "categories": "cs.HC cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important concern in end user development (EUD) is accidentally embedding\npersonal information in program artifacts when sharing them. This issue is\nparticularly important in GUI-based programming-by-demonstration (PBD) systems\ndue to the lack of direct developer control of script contents. Prior studies\nreported that these privacy concerns were the main barrier to script sharing in\nEUD. We present a new approach that can identify and obfuscate the potential\npersonal information in GUI-based PBD scripts based on the uniqueness of\ninformation entries with respect to the corresponding app GUI context. Compared\nwith the prior approaches, ours supports broader types of personal information\nbeyond explicitly pre-specified ones, requires minimal user effort, addresses\nthe threat of re-identification attacks, and can work with third-party apps\nfrom any task domain. Our approach also recovers obfuscated fields locally on\nthe script consumer's side to preserve the shared scripts' transparency,\nreadability, robustness, and generalizability. Our evaluation shows that our\napproach (1) accurately identifies the potential personal information in\nscripts across different apps in diverse task domains; (2) allows end-user\ndevelopers to feel comfortable sharing their own scripts; and (3) enables\nscript consumers to understand the operation of shared scripts despite the\nobfuscated fields.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 17:20:10 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Toby Jia-Jun", ""], ["Chen", "Jingya", ""], ["Canfield", "Brandon", ""], ["Myers", "Brad A.", ""]]}, {"id": "2004.08382", "submitter": "Omar Sosa Tzec", "authors": "Omar Sosa-Tzec, Erik Stolterman Bergqvist, Marty A. Siegel", "title": "From Horseback Riding to Changing the World: UX Competence as a Journey", "comments": "5 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the notion of competence in UX based on the\nperspective of practitioners. As a result of this exploration, we observed four\ndomains through which we conceptualize a plan of sources of competence that\ndescribes the ways a UX practitioner develop competence. Based on this plane,\nwe present the idea of competence as a journey. A journey whose furthest stage\nimplies an urge towards transforming society and UX practice.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 20:23:55 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Sosa-Tzec", "Omar", ""], ["Bergqvist", "Erik Stolterman", ""], ["Siegel", "Marty A.", ""]]}, {"id": "2004.08602", "submitter": "Gerard Wilkinson", "authors": "Gerard Wilkinson, Dan Jackson, Andrew Garbett, Reuben Kirkham, Kyle\n  Montague", "title": "CryptoCam: Privacy Conscious Open Circuit Television", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The prevalence of Closed Circuit Television (CCTV) in today's society has\ngiven rise to an inherent asymmetry of control between the watchers and the\nwatched. A sense of unease relating to the unobservable observer (operator)\noften leads to a lack of trust in the camera and its purpose, despite security\ncameras generally being present as a protective device. In this paper, we\ndetail our concept of Open Circuit Television and prototype CryptoCam, a novel\nsystem for secure sharing of video footage to individuals and potential\nsubjects nearby. Utilizing point-of-capture encryption and wireless transfer of\ntime-based access keys for footage, we have developed a system to encourage a\nmore open approach to information sharing and consumption. Detailing concerns\nhighlighted in existing literature we formalize our over-arching concept into a\nframework called Open Circuit Television (OCTV). Through CryptoCam we hope to\naddress this asymmetry of control by providing subjects with data equity,\ndiscoverability and oversight.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 12:06:38 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Wilkinson", "Gerard", ""], ["Jackson", "Dan", ""], ["Garbett", "Andrew", ""], ["Kirkham", "Reuben", ""], ["Montague", "Kyle", ""]]}, {"id": "2004.08821", "submitter": "Florenc Demrozi Dr.", "authors": "Florenc Demrozi, Graziano Pravadelli, Azra Bihorac, and Parisa Rashidi", "title": "Human Activity Recognition using Inertial, Physiological and\n  Environmental Sensors: a Comprehensive Survey", "comments": "Accepted for Publication in IEEE Access DOI:\n  10.1109/ACCESS.2020.3037715", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3037715", "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, Human Activity Recognition (HAR) has become a vibrant\nresearch area, especially due to the spread of electronic devices such as\nsmartphones, smartwatches and video cameras present in our daily lives. In\naddition, the advance of deep learning and other machine learning algorithms\nhas allowed researchers to use HAR in various domains including sports, health\nand well-being applications. For example, HAR is considered as one of the most\npromising assistive technology tools to support elderly's daily life by\nmonitoring their cognitive and physical function through daily activities. This\nsurvey focuses on critical role of machine learning in developing HAR\napplications based on inertial sensors in conjunction with physiological and\nenvironmental sensors.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 11:32:35 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 09:23:38 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Demrozi", "Florenc", ""], ["Pravadelli", "Graziano", ""], ["Bihorac", "Azra", ""], ["Rashidi", "Parisa", ""]]}, {"id": "2004.09180", "submitter": "Evangelos Pournaras", "authors": "Thomas Asikis, Johannes Klinglmayr, Dirk Helbing, Evangelos Pournaras", "title": "How Value-Sensitive Design Can Empower Sustainable Consumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DC cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a so-called overpopulated world, sustainable consumption is of existential\nimportance.However, the expanding spectrum of product choices and their\nproduction complexity challenge consumers to make informed and value-sensitive\ndecisions. Recent approaches based on (personalized) psychological manipulation\nare often intransparent, potentially privacy-invasive and inconsistent with\n(informational) self-determination. In contrast, responsible consumption based\non informed choices currently requires reasoning to an extent that tends to\noverwhelm human cognitive capacity. As a result, a collective shift towards\nsustainable consumption remains a grand challenge. Here we demonstrate a novel\npersonal shopping assistant implemented as a smart phone app that supports a\nvalue-sensitive design and leverages sustainability awareness, using experts'\nknowledge and \"wisdom of the crowd\" for transparent product information and\nexplainable product ratings. Real-world field experiments in two supermarkets\nconfirm higher sustainability awareness and a bottom-up behavioral shift\ntowards more sustainable consumption. These results encourage novel business\nmodels for retailers and producers, ethically aligned with consumer preferences\nand with higher sustainability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:11:20 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2020 07:16:28 GMT"}, {"version": "v3", "created": "Thu, 14 May 2020 07:43:13 GMT"}, {"version": "v4", "created": "Fri, 4 Dec 2020 14:56:03 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Asikis", "Thomas", ""], ["Klinglmayr", "Johannes", ""], ["Helbing", "Dirk", ""], ["Pournaras", "Evangelos", ""]]}, {"id": "2004.09204", "submitter": "Jonas Oppenlaender", "authors": "Jonas Oppenlaender and Simo Hosio", "title": "Supporting Creative Work with Crowd Feedback Systems", "comments": "5 pages, 5 figures, Workshop on Designing Crowd-powered Creativity\n  Support Systems (DC2S2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd feedback systems have the potential to support creative workers with\nfeedback from the crowd. In this position paper for the Workshop on Designing\nCrowd-powered Creativity Support Systems (DC2S2) at CHI '19, we present three\ncreativity support tools in which we explore how creative workers can be\nassisted with crowdsourced formative and summative feedback. For each of the\nthree crowd feedback systems, we provide one idea for future research.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 11:12:30 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Oppenlaender", "Jonas", ""], ["Hosio", "Simo", ""]]}, {"id": "2004.09376", "submitter": "Liming Zhang", "authors": "Liming Zhang", "title": "Conditional-UNet: A Condition-aware Deep Model for Coherent Human\n  Activity Recognition From Wearables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing human activities from multi-channel time series data collected\nfrom wearable sensors is ever more practical. However, in real-world\nconditions, coherent activities and body movements could happen at the same\ntime, like moving head during walking or sitting. A new problem, so-called\n\"Coherent Human Activity Recognition (Co-HAR)\", is more complicated than normal\nmulti-class classification tasks since signals of different movements are mixed\nand interfered with each other. On the other side, we consider such Co-HAR as a\ndense labelling problem that classify each sample on a time step with a label\nto provide high-fidelity and duration-varied support to applications. In this\npaper, a novel condition-aware deep architecture \"Conditional-UNet\" is\ndeveloped to allow dense labeling for Co-HAR problem. We also contribute a\nfirst-of-its-kind Co-HAR dataset for head movement recognition under walk or\nsit condition for future research. Experiments on head gesture recognition show\nthat our model achieve overall 2%-3% performance gain of F1 score over existing\nstate-of-the-art deep methods, and more importantly, systematic and\ncomprehensive improvements on real head gesture classes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:45:44 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Zhang", "Liming", ""]]}, {"id": "2004.09581", "submitter": "Jason Cody", "authors": "Jason R. Cody, Karina A. Roundtree, Julie A. Adams", "title": "Human-Collective Collaborative Site Selection", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic collectives are large groups (at least 50) of locally sensing and\ncommunicating robots that encompass characteristics of swarms and colonies,\nwhose emergent behaviors accomplish complex tasks. Future human-collective\nteams will extend the ability of operators to monitor, respond, and make\ndecisions in disaster response, search and rescue, and environmental monitoring\nproblems. This manuscript evaluates two collective best-of-n decision models\nfor enabling collectives to identify and choose the highest valued target from\na finite set of n targets. Two challenges impede the future use of\nhuman-collective shared decisions: 1) environmental bias reduces collective\ndecision accuracy when poorer targets are easier to evaluate than higher\nquality targets, and 2) little is understood about shared human-collective\ndecision making interaction strategies. The two evaluated collective best-of-n\nmodels include an existing insect colony decision model and an extended\nbias-reducing model that attempts to reduce environmental bias in order to\nimprove accuracy. Collectives using these two strategies are compared\nindependently and as members of human-collective teams. Independently, the\nextended model is slower than the original model, but the extended algorithm is\n57% more accurate in decisions where the optimal option is more difficult to\nevaluate. Human-collective teams using the bias-reducing model require less\noperator influence and achieve 25% higher accuracy with difficult decisions,\nthan the human-collective teams using the original model. Further, a novel\nhuman-collective interaction strategy enables operators to adjust collective\nautonomy while making multiple simultaneous decisions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 19:16:30 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Cody", "Jason R.", ""], ["Roundtree", "Karina A.", ""], ["Adams", "Julie A.", ""]]}, {"id": "2004.09596", "submitter": "Chlo\\'e Clavel", "authors": "Atef Ben Youssef, Giovanna Varni, Slim Essid, Chlo\\'e Clavel", "title": "On-the-fly Detection of User Engagement Decrease in Spontaneous\n  Human-Robot Interaction, International Journal of Social Robotics, 2019", "comments": null, "journal-ref": "International Journal of Social Robotics December 2019", "doi": "10.1007/s12369-019-00591-2", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the detection of a decrease of engagement by users\nspontaneously interacting with a socially assistive robot in a public space. We\nfirst describe the UE-HRI dataset that collects spontaneous Human-Robot\nInteractions following the guidelines provided by the Affective Computing\nresearch community to collect data \"in-the-wild\". We then analyze the users'\nbehaviors, focusing on proxemics, gaze, head motion, facial expressions and\nspeech during interactions with the robot. Finally, we investigate the use of\ndeep learning techniques (Recurrent and Deep Neural Networks) to detect user\nengagement decrease in realtime. The results of this work highlight, in\nparticular, the relevance of taking into account the temporal dynamics of a\nuser's behavior. Allowing 1 to 2 seconds as buffer delay improves the\nperformance of taking a decision on user engagement.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 19:41:55 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Youssef", "Atef Ben", ""], ["Varni", "Giovanna", ""], ["Essid", "Slim", ""], ["Clavel", "Chlo\u00e9", ""]]}, {"id": "2004.09685", "submitter": "Jon McCormack", "authors": "Nina Rajcic and Jon McCormack", "title": "Mirror Ritual: An Affective Interface for Emotional Self-Reflection", "comments": "Paper presented at ACM CHI2020: Proceedings of the 2020 CHI\n  Conference on Human Factors in Computing Systems, ACM, New York, April 2020", "journal-ref": null, "doi": "10.1145/3313831.3376625", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new form of real-time affective interface that\nengages the user in a process of conceptualisation of their emotional state.\nInspired by Barrett's Theory of Constructed Emotion, `Mirror Ritual' aims to\nexpand upon the user's accessible emotion concepts, and to ultimately provoke\nemotional reflection and regulation. The interface uses classified emotions --\nobtained through facial expression recognition -- as a basis for dynamically\ngenerating poetry. The perceived emotion is used to seed a poetry generation\nsystem based on OpenAI's GPT-2 model, fine-tuned on a specially curated corpus.\nWe evaluate the device's ability to foster a personalised, meaningful\nexperience for individual users over a sustained period. A qualitative analysis\nrevealed that participants were able to affectively engage with the mirror,\nwith each participant developing a unique interpretation of its poetry in the\ncontext of their own emotional landscape.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 00:19:59 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Rajcic", "Nina", ""], ["McCormack", "Jon", ""]]}, {"id": "2004.09759", "submitter": "Akhila Sri Manasa Venigalla", "authors": "Akhila Sri Manasa Venigalla, Dheeraj Vagavolu and Sridhar Chimalakonda", "title": "SurviveCovid-19 -- An Educational Game to Facilitate Habituation of\n  Social Distancing and Other Health Measures for Covid-19 Pandemic", "comments": "17 pages, 9 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Covid-19 has been causing severe loss to the human race. Considering the mode\nof spread and severity, it is essential to make it a habit to follow various\nsafety precautions such as using sanitizers and masks and maintaining social\ndistancing to prevent the spread of Covid-19. Individuals are widely educated\nabout the safety measures against the disease through various modes such as\nannouncements through online or physical awareness campaigns, advertisements in\nthe media and so on. The younger generations today spend considerably more time\non mobile phones and games. However, there are very few applications or games\naimed to help in practicing safety measures against a pandemic, which is much\nlesser in the case of Covid-19. Hence, we propose a 2D survival-based game,\nSurviveCovid-19, aimed to educate people about safety precautions to be taken\nfor Covid-19 outside their homes by incorporating social distancing and usage\nof masks and sanitizers in the game. SurviveCovid-19 has been designed as an\nAndroid-based mobile game, along with a desktop (browser) version, and has been\nevaluated through a remote quantitative user survey, with 30 volunteers using\nthe questionnaire based on the MEEGA+ model. The survey results are promising,\nwith all the survey questions having a mean value greater than 3.5. The game's\nquality factor was 69.3, indicating that the game could be classified as\nexcellent quality, according to the MEEGA+ model.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 05:24:17 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 17:47:52 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Venigalla", "Akhila Sri Manasa", ""], ["Vagavolu", "Dheeraj", ""], ["Chimalakonda", "Sridhar", ""]]}, {"id": "2004.09889", "submitter": "Xiang Zhang", "authors": "Yu Gu, Xiang Zhang, Zhi Liu, Fuji Ren", "title": "WiFE: WiFi and Vision based Intelligent Facial-Gesture Emotion\n  Recognition", "comments": "error in experiment results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion is an essential part of Artificial Intelligence (AI) and human mental\nhealth. Current emotion recognition research mainly focuses on single modality\n(e.g., facial expression), while human emotion expressions are multi-modal in\nnature. In this paper, we propose a hybrid emotion recognition system\nleveraging two emotion-rich and tightly-coupled modalities, i.e., facial\nexpression and body gesture. However, unbiased and fine-grained facial\nexpression and gesture recognition remain a major problem. To this end, unlike\nour rivals relying on contact or even invasive sensors, we explore the\ncommodity WiFi signal for device-free and contactless gesture recognition,\nwhile adopting a vision-based facial expression. However, there exist two\ndesign challenges, i.e., how to improve the sensitivity of WiFi signals and how\nto process the large-volume, heterogeneous, and non-synchronous data\ncontributed by the two-modalities. For the former, we propose a signal\nsensitivity enhancement method based on the Rician K factor theory; for the\nlatter, we combine CNN and RNN to mine the high-level features of bi-modal\ndata, and perform a score-level fusion for fine-grained recognition. To\nevaluate the proposed method, we build a first-of-its-kind Vision-CSI Emotion\nDatabase (VCED) and conduct extensive experiments. Empirical results show the\nsuperiority of the bi-modality by achieving 83.24\\% recognition accuracy for\nseven emotions, as compared with 66.48% and 66.67% recognition accuracy by\ngesture-only based solution and facial-only based solution, respectively. The\nVCED database download link is https://github.com/purpleleaves007/WIFE-Dataset.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 10:35:44 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 13:16:02 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 01:27:57 GMT"}, {"version": "v4", "created": "Tue, 1 Sep 2020 07:51:39 GMT"}, {"version": "v5", "created": "Sun, 20 Dec 2020 10:54:47 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gu", "Yu", ""], ["Zhang", "Xiang", ""], ["Liu", "Zhi", ""], ["Ren", "Fuji", ""]]}, {"id": "2004.09980", "submitter": "David Graus", "authors": "Feng Lu, Anca Dumitrache, David Graus", "title": "Beyond Optimizing for Clicks: Incorporating Editorial Values in News\n  Recommendation", "comments": "To appear in UMAP 2020", "journal-ref": null, "doi": "10.1145/3340631.3394864", "report-no": null, "categories": "cs.IR cs.CL cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the uptake of algorithmic personalization in the news domain, news\norganizations increasingly trust automated systems with previously considered\neditorial responsibilities, e.g., prioritizing news to readers. In this paper\nwe study an automated news recommender system in the context of a news\norganization's editorial values. We conduct and present two online studies with\na news recommender system, which span one and a half months and involve over\n1,200 users. In our first study we explore how our news recommender steers\nreading behavior in the context of editorial values such as serendipity,\ndynamism, diversity, and coverage. Next, we present an intervention study where\nwe extend our news recommender to steer our readers to more dynamic reading\nbehavior. We find that (i) our recommender system yields more diverse reading\nbehavior and yields a higher coverage of articles compared to non-personalized\neditorial rankings, and (ii) we can successfully incorporate dynamism in our\nrecommender system as a re-ranking method, effectively steering our readers to\nmore dynamic articles without hurting our recommender system's accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:24:49 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lu", "Feng", ""], ["Dumitrache", "Anca", ""], ["Graus", "David", ""]]}, {"id": "2004.10002", "submitter": "Yunlong Wang", "authors": "Yunlong Wang, Laura M. Koenig, Harald Reiterer", "title": "A Smartphone App to Support Sedentary Behavior Change by Visualizing\n  Personal Mobility Patterns and Action Planning (SedVis): Development and\n  Pilot Study", "comments": null, "journal-ref": "JMIR Form Res 2021;5(1):e15369", "doi": "10.2196/15369", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the high prevalence of sedentary behavior in daily life, simple yet\npractical solutions for behavior change are needed to avoid detrimental health\neffects. The mobile app SedVis was developed based on the health action process\napproach. The app provides personal mobility pattern visualization (for both\nphysical activity and sedentary behavior) and action planning for sedentary\nbehavior change. The primary aim of the study is to investigate the effect of\nmobility pattern visualization on users' action planning for changing their\nsedentary behavior. The secondary aim is to evaluate user engagement with the\nvisualization and user experience of the app. In a 3-week user study,\nparticipants were allocated to either an active control group (n=8) or an\nintervention group (n=8). In the 1-week baseline period, none of the\nparticipants had access to the functions in the app. In the following 2-week\nintervention period, only the intervention group was given access to the\nvisualizations, whereas both groups were asked to make action plans every day\nand reduce their sedentary behavior. The results suggested that the\nvisualizations in SedVis had no effect on the participants' action planning\naccording to both the NHST and Bayesian statistics. The intervention involving\nvisualizations and action planning in SedVis had a positive effect on reducing\nparticipants' sedentary hours, with weak evidence according to Bayesian\nstatistics, whereas no change in sedentary time was more likely in the active\ncontrol condition. Furthermore, Bayesian analysis weakly suggested that the\nmore frequently the users checked the app, the more likely they were to reduce\ntheir sedentary behavior.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 13:44:04 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 02:56:44 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Wang", "Yunlong", ""], ["Koenig", "Laura M.", ""], ["Reiterer", "Harald", ""]]}, {"id": "2004.10026", "submitter": "Kizito Nkurikiyeyezu", "authors": "Shun Ishii, Kizito Nkurikiyeyezu, Anna Yokokubo and Guillaume Lopez", "title": "ExerSense: Real-Tme Physical Exercise Segmentation, Classification, and\n  Counting Algorithm Using an IMU Sensor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though it is well known that physical exercises have numerous emotional\nand physical health benefits, maintaining a regular exercise routine is quite\nchallenging. Fortunately, there exist technologies that promote physical\nactivity. Nonetheless, almost all of these technologies only target a narrow\nset of physical activities (e.g., either running or walking but not both) and\nare only applicable either in indoor or in outdoor environments, but do not\nwork well in both environments. This paper introduces a real-time segmentation\nand classification algorithm that recognizes physical exercises and that works\nwell in both indoor and outdoor environments. The proposed algorithm achieves a\n95\\% classification accuracy for five indoor and outdoor exercises, including\nsegmentation error. This accuracy is similar or better than previous works that\nhandled only indoor workouts and those use a vision-based approach. Moreover,\nwhile comparable machine learning-based approaches need a lot of training data,\nthe proposed correlation-based method needs one sample of motion data of each\ntarget exercises.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 14:04:19 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Ishii", "Shun", ""], ["Nkurikiyeyezu", "Kizito", ""], ["Yokokubo", "Anna", ""], ["Lopez", "Guillaume", ""]]}, {"id": "2004.10197", "submitter": "Jason Kelly", "authors": "Jason D Kelly, Nicholas Heller, Ashley Petersen, Thomas S Lendvay,\n  Timothy M Kowalewski", "title": "The Effect of Video Playback Speed on Perception of Technical Skill in\n  Robotic Surgery", "comments": "arXiv admin note: text overlap with arXiv:1911.00737", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Previous research has shown that obtaining non-expert crowd\nevaluations of surgical performances concords with the gold standard of expert\nsurgeon review, and that faster playback speed increases ratings for videos of\nhigher-skilled surgeons in laparoscopic simulation. The aim of this research is\nto extend this investigation to real surgeries that use non-expert crowd\nevaluations. We address two questions (1) whether crowds award more favorable\nratings to videos shown at increased playback speeds, and (2) if crowd\nevaluations of the first minute of a surgical procedure differ from crowd\nevaluations of the entire performance. Methods: A set of 56 videos of\npracticing surgeons were used to evaluate the technical skill of the surgeons\nat each video playback speed used for the first minute of the previously rated\nperformance, using the GEARS assessment criteria. Results: Crowds on average\ndid rate videos higher as playback speed was increased. This effect was\nobserved for both proficient and expert surgeons. Each increase in the playback\nspeed by 0.8x was associated with, on average, a 0.16-point increase in the\nGEARS score for expert surgeons and a 0.27-point increase in GEARS score for\nproficient surgeons, with both groups being perceived as obtaining relatively\nequal skill at the fastest playback speed. It was also found that 22 out of the\n56 surgeons were perceived to be significantly different in skill when just\nviewing the first minute of performance. Conclusion: The observed increase in\nskill ratings with video playback speed replicates findings for laparoscopy in\n[2], and extends to real robotic surgeries. Furthermore, the large differences\nin skill labels when comparing the first minute of surgery to the entire 15\nminute video warrants further investigation into how much perceived skill\nratings vary in time (sub-task level) vs. summative metrics (task level).\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:52:08 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Kelly", "Jason D", ""], ["Heller", "Nicholas", ""], ["Petersen", "Ashley", ""], ["Lendvay", "Thomas S", ""], ["Kowalewski", "Timothy M", ""]]}, {"id": "2004.10313", "submitter": "Baihan Lin", "authors": "Baihan Lin", "title": "Keep It Real: a Window to Real Reality in Virtual Reality", "comments": "IJCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposed a new interaction paradigm in the virtual reality (VR)\nenvironments, which consists of a virtual mirror or window projected onto a\nvirtual surface, representing the correct perspective geometry of a mirror or\nwindow reflecting the real world. This technique can be applied to various\nvideos, live streaming apps, augmented and virtual reality settings to provide\nan interactive and immersive user experience. To support such a\nperspective-accurate representation, we implemented computer vision algorithms\nfor feature detection and correspondence matching. To constrain the solutions,\nwe incorporated an automatically tuning scaling factor upon the homography\ntransform matrix such that each image frame follows a smooth transition with\nthe user in sight. The system is a real-time rendering framework where users\ncan engage their real-life presence with the virtual space.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 21:33:14 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 07:23:51 GMT"}, {"version": "v3", "created": "Thu, 12 Nov 2020 07:02:43 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lin", "Baihan", ""]]}, {"id": "2004.10428", "submitter": "Arjun Srinivasan", "authors": "Arjun Srinivasan, Bongshin Lee, John Stasko", "title": "Interweaving Multimodal Interaction with Flexible Unit Visualizations\n  for Data Exploration", "comments": "15 pages, 9 figures, 3 tables", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics. March\n  2020", "doi": "10.1109/TVCG.2020.2978050", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal interfaces that combine direct manipulation and natural language\nhave shown great promise for data visualization. Such multimodal interfaces\nallow people to stay in the flow of their visual exploration by leveraging the\nstrengths of one modality to complement the weaknesses of others. In this work,\nwe introduce an approach that interweaves multimodal interaction combining\ndirect manipulation and natural language with flexible unit visualizations. We\nemploy the proposed approach in a proof-of-concept system, DataBreeze. Coupling\npen, touch, and speech-based multimodal interaction with flexible unit\nvisualizations, DataBreeze allows people to create and interact with both\nsystematically bound (e.g., scatterplots, unit column charts) and manually\ncustomized views, enabling a novel visual data exploration experience. We\ndescribe our design process along with DataBreeze's interface and interactions,\ndelineating specific aspects of the design that empower the synergistic use of\nmultiple modalities. We also present a preliminary user study with DataBreeze,\nhighlighting the data exploration patterns that participants employed. Finally,\nreflecting on our design process and preliminary user study, we discuss future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 07:53:01 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Srinivasan", "Arjun", ""], ["Lee", "Bongshin", ""], ["Stasko", "John", ""]]}, {"id": "2004.10933", "submitter": "Kuniaki Ozawa", "authors": "Kuniaki Ozawa, Masayoshi Naito, Naoki Tanaka and Shiryu Wada", "title": "A Word Communication System with Caregiver Assist for Amyotrophic\n  Lateral Sclerosis Patients in Completely and Almost Completely Locked-in\n  State", "comments": "8 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People with heavy physical impairment such as amyotrophic lateral sclerosis\n(ALS) in a completely locked-in state (CLIS) suffer from inability to express\ntheir thoughts to others. To solve this problem, many brain-computer interface\n(BCI) systems have been developed, but they have not proven sufficient for\nCLIS. In this paper, we propose a word communication system: a BCI with\ncaregiver assist, in which caregivers play an active role in helping patients\nexpress a word. We report here that four ALS patients in almost CLIS and one in\nCLIS succeeded in expressing their own words (in Japanese) in response to\nwh-questions that could not be answered \"yes/no.\" Each subject selected vowels\n(maximum three) contained in the word that he or she wanted to express in a\nsequential way, by using a \"yes/no\" communication aid based on near-infrared\nlight. Then, a caregiver entered the selected vowels into a dictionary with\nvowel entries, which returned candidate words having those vowels. When there\nwere no appropriate words, the caregiver changed one vowel and searched again\nor started over from the beginning. When an appropriate word was selected, it\nwas confirmed by the subject via \"yes/no\" answers. Three subjects expressed\n\"yes\" for the selected word at least six times out of eight (reliability of\n91.0% by a statistical measure), one subject (in CLIS) did so five times out of\neight (74.6%), and one subject three times out of four (81.3%). We have thus\ntaken the first step toward a practical word communication system for such\npatients.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 02:06:17 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ozawa", "Kuniaki", ""], ["Naito", "Masayoshi", ""], ["Tanaka", "Naoki", ""], ["Wada", "Shiryu", ""]]}, {"id": "2004.11113", "submitter": "Cl\\'ement Gautrais", "authors": "Cl\\'ement Gautrais, Yann Dauxais, Stefano Teso, Samuel Kolb, Gust\n  Verbruggen, Luc De Raedt", "title": "Human-Machine Collaboration for Democratizing Data Science", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Everybody wants to analyse their data, but only few posses the data science\nexpertise to to this. Motivated by this observation we introduce a novel\nframework and system \\textsc{VisualSynth} for human-machine collaboration in\ndata science.\n  It wants to democratize data science by allowing users to interact with\nstandard spreadsheet software in order to perform and automate various data\nanalysis tasks ranging from data wrangling, data selection, clustering,\nconstraint learning, predictive modeling and auto-completion.\n\\textsc{VisualSynth} relies on the user providing colored sketches, i.e.,\ncoloring parts of the spreadsheet, to partially specify data science tasks,\nwhich are then determined and executed using artificial intelligence\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 12:50:52 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Gautrais", "Cl\u00e9ment", ""], ["Dauxais", "Yann", ""], ["Teso", "Stefano", ""], ["Kolb", "Samuel", ""], ["Verbruggen", "Gust", ""], ["De Raedt", "Luc", ""]]}, {"id": "2004.11375", "submitter": "Wolfgang Gatterbauer", "authors": "Aristotelis Leventidis, Jiahui Zhang, Cody Dunne, Wolfgang\n  Gatterbauer, H.V. Jagadish, Mirek Riedewald", "title": "QueryVis: Logic-based diagrams help users understand complicated SQL\n  queries faster", "comments": "Full version of paper appearing in SIGMOD 2020", "journal-ref": null, "doi": "10.1145/3318464.3389767", "report-no": null, "categories": "cs.DB cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the meaning of existing SQL queries is critical for code\nmaintenance and reuse. Yet SQL can be hard to read, even for expert users or\nthe original creator of a query. We conjecture that it is possible to capture\nthe logical intent of queries in \\emph{automatically-generated visual diagrams}\nthat can help users understand the meaning of queries faster and more\naccurately than SQL text alone. We present initial steps in that direction with\nvisual diagrams that are based on the first-order logic foundation of SQL and\ncan capture the meaning of deeply nested queries. Our diagrams build upon a\nrich history of diagrammatic reasoning systems in logic and were designed using\na large body of human-computer interaction best practices: they are\n\\emph{minimal} in that no visual element is superfluous; they are\n\\emph{unambiguous} in that no two queries with different semantics map to the\nsame visualization; and they \\emph{extend} previously existing visual\nrepresentations of relational schemata and conjunctive queries in a natural\nway. An experimental evaluation involving 42 users on Amazon Mechanical Turk\nshows that with only a 2--3 minute static tutorial, participants could\ninterpret queries meaningfully faster with our diagrams than when reading SQL\nalone. Moreover, we have evidence that our visual diagrams result in\nparticipants making fewer errors than with SQL. We believe that more regular\nexposure to diagrammatic representations of SQL can give rise to a\n\\emph{pattern-based} and thus more intuitive use and re-use of SQL. All details\non the experimental study, the evaluation stimuli, raw data, and analyses, and\nsource code are available at https://osf.io/mycr2\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:55:32 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Leventidis", "Aristotelis", ""], ["Zhang", "Jiahui", ""], ["Dunne", "Cody", ""], ["Gatterbauer", "Wolfgang", ""], ["Jagadish", "H. V.", ""], ["Riedewald", "Mirek", ""]]}, {"id": "2004.11440", "submitter": "Sungsoo (Ray) Hong", "authors": "Sungsoo Ray Hong, Jessica Hullman, Enrico Bertini", "title": "Human Factors in Model Interpretability: Industry Practices, Challenges,\n  and Needs", "comments": "ACM CSCW 2020", "journal-ref": null, "doi": "10.1145/3392878", "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the use of machine learning (ML) models in product development and\ndata-driven decision-making processes became pervasive in many domains,\npeople's focus on building a well-performing model has increasingly shifted to\nunderstanding how their model works. While scholarly interest in model\ninterpretability has grown rapidly in research communities like HCI, ML, and\nbeyond, little is known about how practitioners perceive and aim to provide\ninterpretability in the context of their existing workflows. This lack of\nunderstanding of interpretability as practiced may prevent interpretability\nresearch from addressing important needs, or lead to unrealistic solutions. To\nbridge this gap, we conducted 22 semi-structured interviews with industry\npractitioners to understand how they conceive of and design for\ninterpretability while they plan, build, and use their models. Based on a\nqualitative analysis of our results, we differentiate interpretability roles,\nprocesses, goals and strategies as they exist within organizations making heavy\nuse of ML models. The characterization of interpretability work that emerges\nfrom our analysis suggests that model interpretability frequently involves\ncooperation and mental model comparison between people in different roles,\noften aimed at building trust not only between people and models but also\nbetween people within the organization. We present implications for design that\ndiscuss gaps between the interpretability challenges that practitioners face in\ntheir practice and approaches proposed in the literature, highlighting possible\nresearch directions that can better address real-world needs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 19:54:39 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 12:10:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hong", "Sungsoo Ray", ""], ["Hullman", "Jessica", ""], ["Bertini", "Enrico", ""]]}, {"id": "2004.11479", "submitter": "David Prokop", "authors": "david Prokop, Joseph Babigumira, Ashleigh Lewis", "title": "Pill Identification using a Mobile Phone App for Assessing Medication\n  Adherence and Post-Market Drug Surveillance", "comments": "12 pages, 1 photo, 6 tables, 3 charts, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: Medication non-adherence is an important factor in clinical\npractice and research methodology. There have been many methods of measuring\nadherence yet no recognized standard for adherence. Here we conduct a software\nstudy of the usefulness and efficacy of a mobile phone app to measure\nmedication adherence using photographs taken by a phone app of medications and\nself-reported health measures.\n  Results: The participants were asked by the app 'would help to keep track of\nyour medication', their response indicated 92.9% felt the app 'would you use\nthis app every day' to improve their medication adherence. The subjects were\nalso asked by the app if they 'would photograph their pills on a daily basis'.\nSubject responses indicated 63% would use the app on a daily basis. By using\nthe data collected, we determined that subjects who used the app on daily basis\nwere more likely to adhere to the prescribed regimen.\n  Conclusions: Pill photographs are a useful measure of adherence, allowing\nmore accurate time measures and more frequent adherence assessment. Given the\nubiquity of mobile telephone use, and the relative ease of this adherence\nmeasurement method, we believe it is a useful and cost-effective approach.\nHowever we feel the 'manual' nature of using the phone for taking a photograph\nof a pill has individual variability and an 'automatic' method is needed to\nreduce data inconsistency.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 22:24:01 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Prokop", "david", ""], ["Babigumira", "Joseph", ""], ["Lewis", "Ashleigh", ""]]}, {"id": "2004.11894", "submitter": "Rezarta Islamaj", "authors": "Rezarta Islamaj, Dongseop Kwon, Sun Kim, Zhiyong Lu", "title": "TeamTat: a collaborative text annotation tool", "comments": "11 pages, 4 figures, NAR Web Server Issue 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manually annotated data is key to developing text-mining and\ninformation-extraction algorithms. However, human annotation requires\nconsiderable time, effort and expertise. Given the rapid growth of biomedical\nliterature, it is paramount to build tools that facilitate speed and maintain\nexpert quality. While existing text annotation tools may provide user-friendly\ninterfaces to domain experts, limited support is available for image display,\nproject management, and multi-user team annotation. In response, we developed\nTeamTat (teamtat.org), a web-based annotation tool (local setup available),\nequipped to manage team annotation projects engagingly and efficiently. TeamTat\nis a novel tool for managing multi-user, multi-label document annotation,\nreflecting the entire production life cycle. Project managers can specify\nannotation schema for entities and relations and select annotator(s) and\ndistribute documents anonymously to prevent bias. Document input format can be\nplain text, PDF or BioC, (uploaded locally or automatically retrieved from\nPubMed or PMC), and output format is BioC with inline annotations. TeamTat\ndisplays figures from the full text for the annotators convenience. Multiple\nusers can work on the same document independently in their workspaces, and the\nteam manager can track task completion. TeamTat provides corpus-quality\nassessment via inter-annotator agreement statistics, and a user-friendly\ninterface convenient for annotation review and inter-annotator disagreement\nresolution to improve corpus quality.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:58:23 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Islamaj", "Rezarta", ""], ["Kwon", "Dongseop", ""], ["Kim", "Sun", ""], ["Lu", "Zhiyong", ""]]}, {"id": "2004.11978", "submitter": "Andrea Bellotti", "authors": "Andrea Bellotti, Sergey Antopolskiy, Anna Marchenkova, Alessia\n  Colucciello, Pietro Avanzini, Giovanni Vecchiato, Jonas Ambeck-Madsen, Luca\n  Ascari", "title": "Brain-based control of car infotainment", "comments": null, "journal-ref": null, "doi": "10.1109/SMC.2019.8914448", "report-no": null, "categories": "cs.HC cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the possibility to run advanced AI on embedded systems allows\nnatural interaction between humans and machines, especially in the automotive\nfield. We present a custom portable EEG-based Brain-Computer Interface (BCI)\nthat exploits Event-Related Potentials (ERPs) induced with an oddball\nexperimental paradigm to control the infotainment menu of a car. A preliminary\nevaluation of the system was performed on 10 participants in a standard\nlaboratory setting and while driving on a closed private track. The task\nconsisted of repeated presentations of 6 different menu icons in oddball\nfashion. Subject-specific models were trained with different machine learning\napproaches on cerebral data from either only laboratory or driving experiments\n(in-lab and in-car models) or a combination of the two (hybrid model) to\nclassify EEG responses to target and non-target stimuli. All models were tested\non the subjects' last in-car sessions that were not used for the training.\nAnalysis of ERPs amplitude showed statistically significant (p < 0.05)\ndifferences between the EEG responses associated with target and non-target\nicons, both in the laboratory and while driving. Classification Accuracy (CA)\nwas above chance level for all subjects in all training configurations, with a\ndeep CNN trained on the hybrid set achieving the highest scores (mean CA = 53\n$\\pm$ 12 %, with 16 % chance level for the 6-class discrimination). The ranking\nof the features importance provided by a classical BCI approach suggests an\nERP-based discrimination between target and non-target responses. No\nstatistical differences were observed between the CAs for the in-lab and in-car\ntraining sets, nor between the EEG responses in these conditions, indicating\nthat the data collected in the standard laboratory setting could be readily\nused for a real driving application without a noticeable decrease in\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 20:32:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bellotti", "Andrea", ""], ["Antopolskiy", "Sergey", ""], ["Marchenkova", "Anna", ""], ["Colucciello", "Alessia", ""], ["Avanzini", "Pietro", ""], ["Vecchiato", "Giovanni", ""], ["Ambeck-Madsen", "Jonas", ""], ["Ascari", "Luca", ""]]}, {"id": "2004.12016", "submitter": "Sungjin Nam", "authors": "Sungjin Nam, Zoya Bylinskii, Christopher Tensmeyer, Curtis Wigington,\n  Rajiv Jain, Tong Sun", "title": "Using Behavioral Interactions from a Mobile Device to Classify the\n  Reader's Prior Familiarity and Goal Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A student reads a textbook to learn a new topic; an attorney leafs through\nfamiliar legal documents. Each reader may have a different goal for, and prior\nknowledge of, their reading. A mobile context, which captures interaction\nbehavior, can provide insights about these reading conditions. In this paper,\nwe focus on understanding the different reading conditions of mobile readers,\nas such an understanding can facilitate the design of effective personalized\nfeatures for supporting mobile reading. With this motivation in mind, we\nanalyzed the reading behaviors of 285 Mechanical Turk participants who read\narticles on mobile devices with different familiarity and reading goal\nconditions. The data was collected non-invasively, only including behavioral\ninteractions recorded from a mobile phone in a non-laboratory setting. Our\nfindings suggest that features based on touch locations can be used to\ndistinguish among familiarity conditions, while scroll-based features and\nreading time features can be used to differentiate between reading goal\nconditions. Using the collected data, we built a model that can predict the\nreading goal condition (67.5%) significantly more accurately than a baseline\nmodel. Our model also predicted the familiarity level (56.2%) marginally more\naccurately than the baseline. These findings can contribute to developing an\nevidence-based design of reading support features for mobile reading\napplications. Furthermore, our study methodology can be easily expanded to\ndifferent real-world reading environments, leaving much potential for future\ninvestigations.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 23:21:33 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Nam", "Sungjin", ""], ["Bylinskii", "Zoya", ""], ["Tensmeyer", "Christopher", ""], ["Wigington", "Curtis", ""], ["Jain", "Rajiv", ""], ["Sun", "Tong", ""]]}, {"id": "2004.12081", "submitter": "Zhe Sun", "authors": "Zhe Sun, Zihao Huang, Feng Duan, Yu Liu", "title": "A novel multimodal approach for hybrid brain-computer interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interface (BCI) technologies have been widely used in many\nareas. In particular, non-invasive technologies such as electroencephalography\n(EEG) or near-infrared spectroscopy (NIRS) have been used to detect motor\nimagery, disease, or mental state. It has been already shown in literature that\nthe hybrid of EEG and NIRS has better results than their respective individual\nsignals. The fusion algorithm for EEG and NIRS sources is the key to implement\nthem in real-life applications. In this research, we propose three fusion\nmethods for the hybrid of the EEG and NIRS-based brain-computer interface\nsystem: linear fusion, tensor fusion, and $p$th-order polynomial fusion.\nFirstly, our results prove that the hybrid BCI system is more accurate, as\nexpected. Secondly, the $p$th-order polynomial fusion has the best\nclassification results out of the three methods, and also shows improvements\ncompared with previous studies. For a motion imagery task and a mental\narithmetic task, the best detection accuracy in previous papers were 74.20\\%\nand 88.1\\%, whereas our accuracy achieved was 77.53\\% and 90.19\\% .\nFurthermore, unlike complex artificial neural network methods, our proposed\nmethods are not as computationally demanding.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 08:11:50 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Sun", "Zhe", ""], ["Huang", "Zihao", ""], ["Duan", "Feng", ""], ["Liu", "Yu", ""]]}, {"id": "2004.12195", "submitter": "Georg Rehm", "authors": "Georg Rehm, Peter Bourgonje, Stefanie Hegele, Florian Kintzel,\n  Juli\\'an Moreno Schneider, Malte Ostendorff, Karolina Zaczynska, Armin\n  Berger, Stefan Grill, S\\\"oren R\\\"auchle, Jens Rauenbusch, Lisa Rutenburg,\n  Andr\\'e Schmidt, Mikka Wild, Henry Hoffmann, Julian Fink, Sarah Schulz,\n  Jurica Seva, Joachim Quantz, Joachim B\\\"ottger, Josefine Matthey, Rolf\n  Fricke, Jan Thomsen, Adrian Paschke, Jamal Al Qundus, Thomas Hoppe, Naouel\n  Karam, Frauke Weichhardt, Christian Fillies, Clemens Neudecker, Mike Gerber,\n  Kai Labusch, Vahid Rezanezhad, Robin Schaefer, David Zellh\\\"ofer, Daniel\n  Siewert, Patrick Bunk, Lydia Pintscher, Elena Aleynikova, Franziska Heine", "title": "QURATOR: Innovative Technologies for Content and Data Curation", "comments": "Proceedings of QURATOR 2020: The conference for intelligent content\n  solutions, Berlin, Germany, February 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In all domains and sectors, the demand for intelligent systems to support the\nprocessing and generation of digital content is rapidly increasing. The\navailability of vast amounts of content and the pressure to publish new content\nquickly and in rapid succession requires faster, more efficient and smarter\nprocessing and generation methods. With a consortium of ten partners from\nresearch and industry and a broad range of expertise in AI, Machine Learning\nand Language Technologies, the QURATOR project, funded by the German Federal\nMinistry of Education and Research, develops a sustainable and innovative\ntechnology platform that provides services to support knowledge workers in\nvarious industries to address the challenges they face when curating digital\ncontent. The project's vision and ambition is to establish an ecosystem for\ncontent curation technologies that significantly pushes the current state of\nthe art and transforms its region, the metropolitan area Berlin-Brandenburg,\ninto a global centre of excellence for curation technologies.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 17:21:15 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Rehm", "Georg", ""], ["Bourgonje", "Peter", ""], ["Hegele", "Stefanie", ""], ["Kintzel", "Florian", ""], ["Schneider", "Juli\u00e1n Moreno", ""], ["Ostendorff", "Malte", ""], ["Zaczynska", "Karolina", ""], ["Berger", "Armin", ""], ["Grill", "Stefan", ""], ["R\u00e4uchle", "S\u00f6ren", ""], ["Rauenbusch", "Jens", ""], ["Rutenburg", "Lisa", ""], ["Schmidt", "Andr\u00e9", ""], ["Wild", "Mikka", ""], ["Hoffmann", "Henry", ""], ["Fink", "Julian", ""], ["Schulz", "Sarah", ""], ["Seva", "Jurica", ""], ["Quantz", "Joachim", ""], ["B\u00f6ttger", "Joachim", ""], ["Matthey", "Josefine", ""], ["Fricke", "Rolf", ""], ["Thomsen", "Jan", ""], ["Paschke", "Adrian", ""], ["Qundus", "Jamal Al", ""], ["Hoppe", "Thomas", ""], ["Karam", "Naouel", ""], ["Weichhardt", "Frauke", ""], ["Fillies", "Christian", ""], ["Neudecker", "Clemens", ""], ["Gerber", "Mike", ""], ["Labusch", "Kai", ""], ["Rezanezhad", "Vahid", ""], ["Schaefer", "Robin", ""], ["Zellh\u00f6fer", "David", ""], ["Siewert", "Daniel", ""], ["Bunk", "Patrick", ""], ["Pintscher", "Lydia", ""], ["Aleynikova", "Elena", ""], ["Heine", "Franziska", ""]]}, {"id": "2004.12217", "submitter": "Shubhankar Mohan", "authors": "Shubhankar Mohan, Aditi Chaudhary, Prachie Gupta, Dr. Ritu Tiwari", "title": "Gesture controlled environment using sixth sense technology and its\n  implementation in IoT", "comments": "9 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an idea of building an interface to merge the existing\ntechnologies like Image processing, Internet of Things, Sixth sense, etc. at\none place to reduce the hardware restrictions imposed on a user and improve the\nresponsiveness of the system. The wearable device comprises of a camera, a\nprojector, and its own gesture-controlled environment having smart tools based\non trending techniques like gesture recognition, color marker detection, and\nspeech recognition. The interface is trained using machine learning. It is also\ninterfaced with an IoT based lab to access the lab controls remotely, enhance\nthe security, and to connect devices present in the lab.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 19:39:35 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mohan", "Shubhankar", ""], ["Chaudhary", "Aditi", ""], ["Gupta", "Prachie", ""], ["Tiwari", "Dr. Ritu", ""]]}, {"id": "2004.12240", "submitter": "Kayhan Ghafoor", "authors": "Halgurd S. Maghdid, Kayhan Zrar Ghafoor", "title": "A Smartphone enabled Approach to Manage COVID-19 Lockdown and Economic\n  Crisis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of novel COVID-19 causing an overload in health system and high\nmortality rate. The key priority is to contain the epidemic and prevent the\ninfection rate. In this context, many countries are now in some degree of\nlockdown to ensure extreme social distancing of entire population and hence\nslowing down the epidemic spread. Further, authorities use case quarantine\nstrategy and manual second/third contact-tracing to contain the COVID-19\ndisease. However, manual contact tracing is time consuming and labor-intensive\ntask which tremendously overload public health systems. In this paper, we\ndeveloped a smartphone-based approach to automatically and widely trace the\ncontacts for confirmed COVID-19 cases. Particularly, contact-tracing approach\ncreates a list of individuals in the vicinity and notifying contacts or\nofficials of confirmed COVID-19 cases. This approach is not only providing\nawareness to individuals they are in the proximity to the infected area, but\nalso tracks the incidental contacts that the COVID-19 carrier might not recall.\nThereafter, we developed a dashboard to provide a plan for government officials\non how lockdown/mass quarantine can be safely lifted, and hence tackling the\neconomic crisis. The dashboard used to predict the level of lockdown area based\non collected positions and distance measurements of the registered users in the\nvicinity. The prediction model uses K-means algorithm as an unsupervised\nmachine learning technique for lockdown management.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 21:42:07 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 19:44:23 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Maghdid", "Halgurd S.", ""], ["Ghafoor", "Kayhan Zrar", ""]]}, {"id": "2004.12246", "submitter": "Hangxin Liu", "authors": "Zeyu Zhang, Hangxin Liu, Ziyuan Jiao, Yixin Zhu, Song-Chun Zhu", "title": "Congestion-aware Evacuation Routing using Augmented Reality Devices", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a congestion-aware routing solution for indoor evacuation, which\nproduces real-time individual-customized evacuation routes among multiple\ndestinations while keeping tracks of all evacuees' locations. A population\ndensity map, obtained on-the-fly by aggregating locations of evacuees from\nuser-end Augmented Reality (AR) devices, is used to model the congestion\ndistribution inside a building. To efficiently search the evacuation route\namong all destinations, a variant of A* algorithm is devised to obtain the\noptimal solution in a single pass. In a series of simulated studies, we show\nthat the proposed algorithm is more computationally optimized compared to\nclassic path planning algorithms; it generates a more time-efficient evacuation\nroute for each individual that minimizes the overall congestion. A complete\nsystem using AR devices is implemented for a pilot study in real-world\nenvironments, demonstrating the efficacy of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 22:54:35 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhang", "Zeyu", ""], ["Liu", "Hangxin", ""], ["Jiao", "Ziyuan", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2004.12316", "submitter": "Peixiang Zhong", "authors": "Peixiang Zhong, Chen Zhang, Hao Wang, Yong Liu, Chunyan Miao", "title": "Towards Persona-Based Empathetic Conversational Models", "comments": "Accepted to EMNLP 2020 (A new dataset is proposed:\n  https://github.com/zhongpeixiang/PEC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empathetic conversational models have been shown to improve user satisfaction\nand task outcomes in numerous domains. In Psychology, persona has been shown to\nbe highly correlated to personality, which in turn influences empathy. In\naddition, our empirical analysis also suggests that persona plays an important\nrole in empathetic conversations. To this end, we propose a new task towards\npersona-based empathetic conversations and present the first empirical study on\nthe impact of persona on empathetic responding. Specifically, we first present\na novel large-scale multi-domain dataset for persona-based empathetic\nconversations. We then propose CoBERT, an efficient BERT-based response\nselection model that obtains the state-of-the-art performance on our dataset.\nFinally, we conduct extensive experiments to investigate the impact of persona\non empathetic responding. Notably, our results show that persona improves\nempathetic responding more when CoBERT is trained on empathetic conversations\nthan non-empathetic ones, establishing an empirical link between persona and\nempathy in human conversations.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 08:51:01 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 01:55:05 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 03:40:56 GMT"}, {"version": "v4", "created": "Wed, 16 Sep 2020 06:48:24 GMT"}, {"version": "v5", "created": "Wed, 23 Sep 2020 08:23:51 GMT"}, {"version": "v6", "created": "Mon, 5 Oct 2020 09:21:06 GMT"}, {"version": "v7", "created": "Thu, 19 Nov 2020 11:00:23 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Zhong", "Peixiang", ""], ["Zhang", "Chen", ""], ["Wang", "Hao", ""], ["Liu", "Yong", ""], ["Miao", "Chunyan", ""]]}, {"id": "2004.12388", "submitter": "Po-Ming Law", "authors": "Po-Ming Law, Sana Malik, Fan Du, Moumita Sinha", "title": "The Impact of Presentation Style on Human-In-The-Loop Detection of\n  Algorithmic Bias", "comments": "Published at Graphics Interface 2020 (GI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While decision makers have begun to employ machine learning, machine learning\nmodels may make predictions that bias against certain demographic groups.\nSemi-automated bias detection tools often present reports of\nautomatically-detected biases using a recommendation list or visual cues.\nHowever, there is a lack of guidance concerning which presentation style to use\nin what scenarios. We conducted a small lab study with 16 participants to\ninvestigate how presentation style might affect user behaviors in reviewing\nbias reports. Participants used both a prototype with a recommendation list and\na prototype with visual cues for bias detection. We found that participants\noften wanted to investigate the performance measures that were not\nautomatically detected as biases. Yet, when using the prototype with a\nrecommendation list, they tended to give less consideration to such measures.\nGrounded in the findings, we propose information load and comprehensiveness as\ntwo axes for characterizing bias detection tasks and illustrate how the two\naxes could be adopted to reason about when to use a recommendation list or\nvisual cues.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 14:05:23 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 14:33:08 GMT"}, {"version": "v3", "created": "Sat, 9 May 2020 22:30:29 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Law", "Po-Ming", ""], ["Malik", "Sana", ""], ["Du", "Fan", ""], ["Sinha", "Moumita", ""]]}, {"id": "2004.12389", "submitter": "Keyu Yang", "authors": "Keyu Yang, Yunjun Gao, Lei Liang, Song Bian, Lu Chen, Baihua Zheng", "title": "CrowdTSC: Crowd-based Neural Networks for Text Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment classification is a fundamental task in content analysis. Although\ndeep learning has demonstrated promising performance in text classification\ncompared with shallow models, it is still not able to train a satisfying\nclassifier for text sentiment. Human beings are more sophisticated than machine\nlearning models in terms of understanding and capturing the emotional\npolarities of texts. In this paper, we leverage the power of human intelligence\ninto text sentiment classification. We propose Crowd-based neural networks for\nText Sentiment Classification (CrowdTSC for short). We design and post the\nquestions on a crowdsourcing platform to collect the keywords in texts.\nSampling and clustering are utilized to reduce the cost of crowdsourcing. Also,\nwe present an attention-based neural network and a hybrid neural network, which\nincorporate the collected keywords as human being's guidance into deep neural\nnetworks. Extensive experiments on public datasets confirm that CrowdTSC\noutperforms state-of-the-art models, justifying the effectiveness of\ncrowd-based keyword guidance.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 14:08:15 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Yang", "Keyu", ""], ["Gao", "Yunjun", ""], ["Liang", "Lei", ""], ["Bian", "Song", ""], ["Chen", "Lu", ""], ["Zheng", "Baihua", ""]]}, {"id": "2004.12480", "submitter": "Najma Mathema", "authors": "Najma Mathema, Michael A. Goodrich, and Jacob W. Crandall", "title": "Predicting Plans and Actions in Two-Player Repeated Games", "comments": "Accepted in The AAAI 2020 Workshop on Plan, Activity, and Intent\n  Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) agents will need to interact with both other AI\nagents and humans. Creating models of associates help to predict the modeled\nagents' actions, plans, and intentions. This work introduces algorithms that\npredict actions, plans and intentions in repeated play games, with providing an\nexploration of algorithms. We form a generative Bayesian approach to model S#.\nS# is designed as a robust algorithm that learns to cooperate with its\nassociate in 2 by 2 matrix games. The actions, plans and intentions associated\nwith each S# expert are identified from the literature, grouping the S# experts\naccordingly, and thus predicting actions, plans, and intentions based on their\nstate probabilities. Two prediction methods are explored for Prisoners Dilemma:\nthe Maximum A Posteriori (MAP) and an Aggregation approach. MAP (~89% accuracy)\nperformed the best for action prediction. Both methods predicted plans of S#\nwith ~88% accuracy. Paired T-test shows that MAP performs significantly better\nthan Aggregation for predicting S#'s actions without cheap talk. Intention is\nexplored based on the goals of the S# experts; results show that goals are\npredicted precisely when modeling S#. The obtained results show that the\nproposed Bayesian approach is well suited for modeling agents in two-player\nrepeated games.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 21:03:28 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mathema", "Najma", ""], ["Goodrich", "Michael A.", ""], ["Crandall", "Jacob W.", ""]]}, {"id": "2004.12501", "submitter": "Raz Saremi", "authors": "Razieh Saremi, Mostaan Lotfalian Saremi, Prasad Desai, and Robert\n  Anzalone", "title": "Is This the Right Time to Post My Task? An Empirical Analysis on a Task\n  Similarity Arrival in TopCoder", "comments": "15 pages, 6 figures, 2 tables, HCI International 2020", "journal-ref": "S. Yamamoto and H. Mori (Eds.) HCII 2020, LNCS 12185, pp. 96-110,\n  2020", "doi": "10.1007/978-3-030-50017-7", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existed studies have shown that crowd workers are more interested in taking\nsimilar tasks in terms of context, field, and required technology, rather than\ntasks from the same project. Therefore, it is important for task owners to not\nonly be able to plan 'when the new task should arrive?' but also, to justify\n'what the strategic task arrival plan should be?' in order to receive a valid\nsubmission for the posted task. To address these questions this research\nreports an empirical analysis on the impact of similar task arrival in the\nplatform, on both tasks' success level and workers' performance. Our study\nsupports that 1- A higher number of arrival tasks with similarity level greater\nthan 70% will negatively impact on task competition level, 2- A bigger pool of\nsimilar open and arrival tasks would lead to lower worker attraction and\nelasticity, and 3- Workers who register for tasks with lower similarity level\nare more reliable to make a valid submission and 4- arriving task to the pool\nof 60% similar task will provide the highest chance of receiving a valid\nsubmission.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 23:19:25 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Saremi", "Razieh", ""], ["Saremi", "Mostaan Lotfalian", ""], ["Desai", "Prasad", ""], ["Anzalone", "Robert", ""]]}, {"id": "2004.12504", "submitter": "Raz Saremi", "authors": "Mostaan Lotfalian Saremi, Razieh Saremi, Denisse Martinez-Mejorado", "title": "How Much Should I Pay? An Empirical Analysis on Monetary Prize in\n  TopCoder", "comments": "8 pages, 5 figures, 3 tables, HIC International 2020", "journal-ref": "C. Stephanidis and M. Antona (Eds.): HCII 2020, CCIS 1226, pp.\n  202-208, 2020", "doi": "10.1007/978-3-030-50732-9", "report-no": null, "categories": "cs.SE cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is reported that task monetary prize is one of the most important\nmotivating factors to attract crowd workers. While using expert-based methods\nto price Crowdsourcing tasks is a common practice, the challenge of validating\nthe associated prices across different tasks is a constant issue. To address\nthis issue, three different classifications of multiple linear regression,\nlogistic regression, and K-nearest neighbor were compared to find the most\naccurate predicted price, using a dataset from the TopCoder website. The result\nof comparing chosen algorithms showed that the logistics regression model will\nprovide the highest accuracy of 90% to predict the associated price to tasks\nand KNN ranked the second with an accuracy of 64% for K = 7. Also, applying PCA\nwouldn't lead to any better prediction accuracy as data components are not\ncorrelated.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 23:26:32 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Saremi", "Mostaan Lotfalian", ""], ["Saremi", "Razieh", ""], ["Martinez-Mejorado", "Denisse", ""]]}, {"id": "2004.12642", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Jan-Niklas Voigt-Antons, Tanja Koji\\'c, Danish Ali, Sebastian M\\\"oller", "title": "Influence of Hand Tracking as a way of Interaction in Virtual Reality on\n  User Experience", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising interest in Virtual Reality and the fast development and\nimprovement of available devices, new features of interactions are becoming\navailable. One of them that is becoming very popular is hand tracking, as the\nidea to replace controllers for interactions in virtual worlds. This experiment\naims to compare different interaction types in VR using either controllers or\nhand tracking. Participants had to play two simple VR games with various types\nof tasks in those games - grabbing objects or typing numbers. While playing,\nthey were using interactions with different visualizations of hands and\ncontrollers. The focus of this study was to investigate user experience of\nvarying interactions (controller vs. hand tracking) for those two simple tasks.\nResults show that different interaction types statistically significantly\ninfluence reported emotions with Self-Assessment Manikin (SAM), where for hand\ntracking participants were feeling higher valence, but lower arousal and\ndominance. Additionally, task type of grabbing was reported to be more\nrealistic, and participants experienced a higher presence. Surprisingly,\nparticipants rated the interaction type with controllers where both where hands\nand controllers were visualized as statistically most preferred. Finally, hand\ntracking for both tasks was rated with the System Usability Scale (SUS) scale,\nand hand tracking for the task typing was rated as statistically significantly\nmore usable. These results can drive further research and, in the long term,\ncontribute to help selecting the most matching interaction modality for a task.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:43:40 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Voigt-Antons", "Jan-Niklas", ""], ["Koji\u0107", "Tanja", ""], ["Ali", "Danish", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2004.12923", "submitter": "Aimal Rextin Dr", "authors": "Roquia Mushtaq, Naveed Ahmad, Aimal Rextin, and Muhammad Muddassir\n  Malik", "title": "Improving Usability of User Centric Decision Making of Multi-Attribute\n  Products on E-commerce Websites", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high number of products available makes it difficult for a user to find\nthe most suitable products according to their needs. This problem is especially\nexacerbated when the user is trying to optimize multiple attributes during\nproduct selection, e.g. memory size and camera resolution requirements in case\nof smartphones. Previous studies have shown that such users search extensively\nto find a product that best meets their needs. In this paper, we propose an\ninterface that will help users in selecting a multi-attribute product through a\nseries of visualizations. This interface is especially targeted for users that\ndesire to purchase the best possible product according to some criteria. The\ninterface works by allowing the user to progressively shortlist products and\nultimately select the most appropriate product from a very small consideration\nset. We evaluated our proposed interface by conducting a controlled experiment\nthat empirically measures the efficiency, effectiveness and satisfaction of our\nvisualization based interface and a typical e-commerce interface. The results\nshowed that our proposed interface allowed the user to find a desired product\nquickly and correctly, moreover, the subjective opinion of the users also\nfavored our proposed interface.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:37:03 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Mushtaq", "Roquia", ""], ["Ahmad", "Naveed", ""], ["Rextin", "Aimal", ""], ["Malik", "Muhammad Muddassir", ""]]}, {"id": "2004.12960", "submitter": "Roykrong Sukkerd", "authors": "Roykrong Sukkerd, Reid Simmons, and David Garlan", "title": "Tradeoff-Focused Contrastive Explanation for MDP Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-users' trust in automated agents is important as automated\ndecision-making and planning is increasingly used in many aspects of people's\nlives. In real-world applications of planning, multiple optimization objectives\nare often involved. Thus, planning agents' decisions can involve complex\ntradeoffs among competing objectives. It can be difficult for the end-users to\nunderstand why an agent decides on a particular planning solution on the basis\nof its objective values. As a result, the users may not know whether the agent\nis making the right decisions, and may lack trust in it. In this work, we\ncontribute an approach, based on contrastive explanation, that enables a\nmulti-objective MDP planning agent to explain its decisions in a way that\ncommunicates its tradeoff rationale in terms of the domain-level concepts. We\nconduct a human subjects experiment to evaluate the effectiveness of our\nexplanation approach in a mobile robot navigation domain. The results show that\nour approach significantly improves the users' understanding, and confidence in\ntheir understanding, of the tradeoff rationale of the planning agent.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 17:17:58 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 16:07:02 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Sukkerd", "Roykrong", ""], ["Simmons", "Reid", ""], ["Garlan", "David", ""]]}, {"id": "2004.13007", "submitter": "Mar\\'ia N. Moreno Garc\\'ia", "authors": "Diego S\\'anchez-Moreno, Vivian F. L\\'opez Batista, M. Dolores Mu\\~noz\n  Vicente, Ana B. Gil Gonz\\'alez and Mar\\'ia N. Moreno-Garc\\'ia", "title": "A session-based song recommendation approach involving user\n  characterization along the play power-law distribution", "comments": "Accepted in Complexity (ISSN: 1099-0526)", "journal-ref": null, "doi": "10.1155/2020/7309453", "report-no": null, "categories": "cs.IR cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, streaming music platforms have become very popular mainly\ndue to the huge number of songs these systems make available to users. This\nenormous availability means that recommendation mechanisms that help users to\nselect the music they like need to be incorporated. However, developing\nreliable recommender systems in the music field involves dealing with many\nproblems, some of which are generic and widely studied in the literature, while\nothers are specific to this application domain and are therefore less\nwell-known. This work is focused on two important issues that have not received\nmuch attention: managing gray-sheep users and obtaining implicit ratings. The\nfirst one is usually addressed by resorting to content information that is\noften difficult to obtain. The other drawback is related to the sparsity\nproblem that arises when there are obstacles to gather explicit ratings. In\nthis work, the referred shortcomings are addressed by means of a recommendation\napproach based on the users' streaming sessions. The method is aimed at\nmanaging the well-known power-law probability distribution representing the\nlistening behavior of users. This proposal improves the recommendation\nreliability of collaborative filtering methods while reducing the complexity of\nthe procedures used so far to deal with the gray-sheep problem.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 07:17:03 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["S\u00e1nchez-Moreno", "Diego", ""], ["Batista", "Vivian F. L\u00f3pez", ""], ["Vicente", "M. Dolores Mu\u00f1oz", ""], ["Gonz\u00e1lez", "Ana B. Gil", ""], ["Moreno-Garc\u00eda", "Mar\u00eda N.", ""]]}, {"id": "2004.13102", "submitter": "Gagan Bansal", "authors": "Gagan Bansal, Besmira Nushi, Ece Kamar, Eric Horvitz, Daniel S. Weld", "title": "Is the Most Accurate AI the Best Teammate? Optimizing AI for Teamwork", "comments": "v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI practitioners typically strive to develop the most accurate systems,\nmaking an implicit assumption that the AI system will function autonomously.\nHowever, in practice, AI systems often are used to provide advice to people in\ndomains ranging from criminal justice and finance to healthcare. In such\nAI-advised decision making, humans and machines form a team, where the human is\nresponsible for making final decisions. But is the most accurate AI the best\nteammate? We argue \"No\" -- predictable performance may be worth a slight\nsacrifice in AI accuracy. Instead, we argue that AI systems should be trained\nin a human-centered manner, directly optimized for team performance. We study\nthis proposal for a specific type of human-AI teaming, where the human overseer\nchooses to either accept the AI recommendation or solve the task themselves. To\noptimize the team performance for this setting we maximize the team's expected\nutility, expressed in terms of the quality of the final decision, cost of\nverifying, and individual accuracies of people and machines. Our experiments\nwith linear and non-linear models on real-world, high-stakes datasets show that\nthe most accuracy AI may not lead to highest team performance and show the\nbenefit of modeling teamwork during training through improvements in expected\nteam utility across datasets, considering parameters such as human skill and\nthe cost of mistakes. We discuss the shortcoming of current optimization\napproaches beyond well-studied loss functions such as log-loss, and encourage\nfuture work on AI optimization problems motivated by human-AI collaboration.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 19:06:28 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 02:07:57 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 20:22:20 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Bansal", "Gagan", ""], ["Nushi", "Besmira", ""], ["Kamar", "Ece", ""], ["Horvitz", "Eric", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2004.13219", "submitter": "Elissa M. Redmiles", "authors": "Elissa M. Redmiles", "title": "User Concerns & Tradeoffs in Technology-Facilitated Contact Tracing", "comments": null, "journal-ref": "Digital Government: Research and Practice, 2020. Volume 2, Issue 1", "doi": "10.1145/3428093", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID19 pandemic spread across the world in late 2019 and early 2020. As\nthe pandemic spread, technologists joined forces with public health officials\nto develop apps to support COVID19 response. Yet, for these technological\nsolutions to benefit public health, users must be willing to adopt these\napps.This paper details the potential inputs to a user's decision to adopt a\nCOVID19 contact-tracing app or other technology and empirically validates the\nrelevance of these inputs via both the literature and a\ndemographically-representative survey of 1,000 Americans.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 00:25:03 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2020 01:59:02 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 17:34:25 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2020 19:15:34 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Redmiles", "Elissa M.", ""]]}, {"id": "2004.13226", "submitter": "Zonghe Chua", "authors": "Zonghe Chua, Anthony M. Jarc, Sherry Wren, Ilana Nisky and Allison M.\n  Okamura", "title": "Task Dynamics of Prior Training Influence Visual Force Estimation\n  Ability During Teleoperation", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of haptic feedback in Robot-assisted Minimally Invasive Surgery\n(RMIS) is a potential barrier to safe tissue handling during surgery. Bayesian\nmodeling theory suggests that surgeons with experience in open or laparoscopic\nsurgery can develop priors of tissue stiffness that translate to better force\nestimation abilities during RMIS compared to surgeons with no experience. To\ntest if prior haptic experience leads to improved force estimation ability in\nteleoperation, 33 participants were assigned to one of three training\nconditions: manual manipulation, teleoperation with force feedback, or\nteleoperation without force feedback, and learned to tension a silicone sample\nto a set of force values. They were then asked to perform the tension task, and\na previously unencountered palpation task, to a different set of force values\nunder teleoperation without force feedback. Compared to the teleoperation\ngroups, the manual group had higher force error in the tension task outside the\nrange of forces they had trained on, but showed better speed-accuracy functions\nin the palpation task at low force levels. This suggests that the dynamics of\nthe training modality affect force estimation ability during teleoperation,\nwith the prior haptic experience accessible if formed under the same dynamics\nas the task.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 00:40:36 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 18:18:56 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 21:31:47 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Chua", "Zonghe", ""], ["Jarc", "Anthony M.", ""], ["Wren", "Sherry", ""], ["Nisky", "Ilana", ""], ["Okamura", "Allison M.", ""]]}, {"id": "2004.13274", "submitter": "Prasanta Bhattacharya", "authors": "Prasanta Bhattacharya, Raj Kumar Gupta, and Yinping Yang", "title": "Exploring the contextual factors affecting multimodal emotion\n  recognition in videos", "comments": "Accepted version at IEEE Transactions on Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional expressions form a key part of user behavior on today's digital\nplatforms. While multimodal emotion recognition techniques are gaining research\nattention, there is a lack of deeper understanding on how visual and non-visual\nfeatures can be used to better recognize emotions in certain contexts, but not\nothers. This study analyzes the interplay between the effects of multimodal\nemotion features derived from facial expressions, tone and text in conjunction\nwith two key contextual factors: i) gender of the speaker, and ii) duration of\nthe emotional episode. Using a large public dataset of 2,176 manually annotated\nYouTube videos, we found that while multimodal features consistently\noutperformed bimodal and unimodal features, their performance varied\nsignificantly across different emotions, gender and duration contexts.\nMultimodal features performed particularly better for male speakers in\nrecognizing most emotions. Furthermore, multimodal features performed\nparticularly better for shorter than for longer videos in recognizing neutral\nand happiness, but not sadness and anger. These findings offer new insights\ntowards the development of more context-aware emotion recognition and\nempathetic systems.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 04:02:08 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 02:04:23 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 10:28:04 GMT"}, {"version": "v4", "created": "Thu, 8 Apr 2021 01:39:26 GMT"}, {"version": "v5", "created": "Wed, 30 Jun 2021 16:36:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bhattacharya", "Prasanta", ""], ["Gupta", "Raj Kumar", ""], ["Yang", "Yinping", ""]]}, {"id": "2004.13362", "submitter": "Jan-Niklas Voigt-Antons", "authors": "Martin Haug, Paavo Camps, Tobias Umland, Jan-Niklas Voigt-Antons", "title": "Assessing differences in flow state induced by an adaptive music\n  learning software", "comments": "This paper has been accepted for publication in the 2020 Twelfth\n  International Conference on Quality of Multimedia Experience (QoMEX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology can facilitate self-learning for academic and leisure activities\nsuch as music learning. In general, learning to play an unknown musical song at\nsight on the electric piano or any other instrument can be quite a chore. In a\ntraditional self-learning setting, the musician only gets feedback in terms of\nwhat errors they can hear themselves by comparing what they have played with\nthe score. Research has shown that reaching a flow state creates a more\nenjoyable experience during activities. This work explores whether principles\nfrom flow theory and game design can be applied to make the beginner's musical\nexperience adapted to their need and create higher flow. We created and\nevaluated a tool oriented around these considerations in a study with 21\nparticipants. We found that provided feedback and difficulty scaling can help\nto achieve flow and that the effects get more pronounced the more experience\nwith music participants have. In further research, we want to examine the\ninfluence of our approach to learning sheet music.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 08:33:44 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Haug", "Martin", ""], ["Camps", "Paavo", ""], ["Umland", "Tobias", ""], ["Voigt-Antons", "Jan-Niklas", ""]]}, {"id": "2004.13853", "submitter": "I Gede Mahendra Darmawiguna", "authors": "I Gede Mahendra Darmawiguna, Gede Aditra Pradnyana, I Gede Partha\n  Sindu, I Putu Prayoga Susila Karimawan, Ni Kadek Risa Ariani Dwiasri", "title": "Bali Temple VR: The Virtual Reality based Application for the\n  Digitalization of Balinese Temples", "comments": "12 pages, 4 figurs, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this project is the development of Virtual Reality Application in\norder to document one kind of Balinese cultural heritages which are Temples.\nThe Bali Temple VR application will allow users to do the virtual tour and\nexperience the landscape of the temples and all objects inside the temples. The\napplication gives on-site tour guide using virtual reality that allow users\nexperience the visualization of the Balinese culture heritages in this case are\ntemples. The users can walk through the temples and can see the 3D objects of\ntemples and also there is narration of every object inside the temples with\nbackground musics. Right now, the project has completed two temples for virtual\nreality tour guide application. Those temples are Melanting Temples and Pulaki\nTemples. Based on the test results of its functional requirements, this virtual\nreality application has been able to run well as expected. All features that\nhave been developed have been running well. Based on 20 respondents with\nvarious ages and backgrounds, our finding shows that The Bali Temple VR\nApplication attracts people of all ages to use and experience it. They are\neager to use it and hope that there will be more temples that they can\nexperience to visit in this application.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 05:03:39 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Darmawiguna", "I Gede Mahendra", ""], ["Pradnyana", "Gede Aditra", ""], ["Sindu", "I Gede Partha", ""], ["Karimawan", "I Putu Prayoga Susila", ""], ["Dwiasri", "Ni Kadek Risa Ariani", ""]]}, {"id": "2004.13864", "submitter": "Cagatay Basdogan", "authors": "Cagatay Basdogan, Frederic Giraud, Vincent Levesque, Seungmoon Choi", "title": "A Review of Surface Haptics:Enabling Tactile Effects on Touch Surfaces", "comments": "21 pages, 177 references, review paper", "journal-ref": null, "doi": "10.1109/TOH.2020.2990712", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the current technology underlying surface haptics that converts\npassive touch surfaces to active ones (machine haptics), our perception of\ntactile stimuli displayed through active touch surfaces (human haptics), their\npotential applications (human-machine interaction), and finally the challenges\nahead of us in making them available through commercial systems. This review\nprimarily covers the tactile interactions of human fingers or hands with\nsurface-haptics displays by focusing on the three most popular actuation\nmethods: vibrotactile, electrostatic, and ultrasonic.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 21:47:02 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Basdogan", "Cagatay", ""], ["Giraud", "Frederic", ""], ["Levesque", "Vincent", ""], ["Choi", "Seungmoon", ""]]}, {"id": "2004.13908", "submitter": "Daniel Chin", "authors": "Daniel Chin, Yian Zhang, Tianyu Zhang, Jake Zhao, Gus G. Xia", "title": "Interactive Rainbow Score: A Visual-centered Multimodal Flute Tutoring\n  System", "comments": "NIME 2020 poster presentation. 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning to play an instrument is intrinsically multimodal, and we have seen\na trend of applying visual and haptic feedback in music games and\ncomputer-aided music tutoring systems. However, most current systems are still\ndesigned to master individual pieces of music; it is unclear how well the\nlearned skills can be generalized to new pieces. We aim to explore this\nquestion. In this study, we contribute Interactive Rainbow Score, an\ninteractive visual system to boost the learning of sight-playing, the general\nmusical skill to read music and map the visual representations to performance\nmotions. The key design of Interactive Rainbow Score is to associate pitches\n(and the corresponding motions) with colored notation and further strengthen\nsuch association via real-time interactions. Quantitative results show that the\ninteractive feature on average increases the learning efficiency by 31.1%.\nFurther analysis indicates that it is critical to apply the interaction in the\nearly period of learning.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 01:09:54 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Chin", "Daniel", ""], ["Zhang", "Yian", ""], ["Zhang", "Tianyu", ""], ["Zhao", "Jake", ""], ["Xia", "Gus G.", ""]]}, {"id": "2004.14067", "submitter": "Anjana Wijekoon", "authors": "Nirmalie Wiratunga, Kay Cooper, Anjana Wijekoon, Chamath Palihawadana,\n  Vanessa Mendham, Ehud Reiter, Kyle Martin", "title": "FitChat: Conversational Artificial Intelligence Interventions for\n  Encouraging Physical Activity in Older Adults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Delivery of digital behaviour change interventions which encourage physical\nactivity has been tried in many forms. Most often interventions are delivered\nas text notifications, but these do not promote interaction. Advances in\nconversational AI have improved natural language understanding and generation,\nallowing AI chatbots to provide an engaging experience with the user. For this\nreason, chatbots have recently been seen in healthcare delivering digital\ninterventions through free text or choice selection. In this work, we explore\nthe use of voice-based AI chatbots as a novel mode of intervention delivery,\nspecifically targeting older adults to encourage physical activity. We\nco-created \"FitChat\", an AI chatbot, with older adults and we evaluate the\nfirst prototype using Think Aloud Sessions. Our thematic evaluation suggests\nthat older adults prefer voice-based chat over text notifications or free text\nentry and that voice is a powerful mode for encouraging motivation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 10:39:33 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Wiratunga", "Nirmalie", ""], ["Cooper", "Kay", ""], ["Wijekoon", "Anjana", ""], ["Palihawadana", "Chamath", ""], ["Mendham", "Vanessa", ""], ["Reiter", "Ehud", ""], ["Martin", "Kyle", ""]]}, {"id": "2004.14281", "submitter": "Titas De", "authors": "Nick Haber, Catalin Voss, Jena Daniels, Peter Washington, Azar Fazel,\n  Aaron Kline, Titas De, Terry Winograd, Carl Feinstein, Dennis P. Wall", "title": "A Wearable Social Interaction Aid for Children with Autism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With most recent estimates giving an incidence rate of 1 in 68 children in\nthe United States, the autism spectrum disorder (ASD) is a growing public\nhealth crisis. Many of these children struggle to make eye contact, recognize\nfacial expressions, and engage in social interactions. Today the standard for\ntreatment of the core autism-related deficits focuses on a form of behavior\ntraining known as Applied Behavioral Analysis. To address perceived deficits in\nexpression recognition, ABA approaches routinely involve the use of prompts\nsuch as flash cards for repetitive emotion recognition training via\nmemorization. These techniques must be administered by trained practitioners\nand often at clinical centers that are far outnumbered by and out of reach from\nthe many children and families in need of attention. Waitlists for access are\nup to 18 months long, and this wait may lead to children regressing down a path\nof isolation that worsens their long-term prognosis. There is an urgent need to\ninnovate new methods of care delivery that can appropriately empower caregivers\nof children at risk or with a diagnosis of autism, and that capitalize on\nmobile tools and wearable devices for use outside of clinical settings.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 13:14:32 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Haber", "Nick", ""], ["Voss", "Catalin", ""], ["Daniels", "Jena", ""], ["Washington", "Peter", ""], ["Fazel", "Azar", ""], ["Kline", "Aaron", ""], ["De", "Titas", ""], ["Winograd", "Terry", ""], ["Feinstein", "Carl", ""], ["Wall", "Dennis P.", ""]]}, {"id": "2004.14331", "submitter": "Shaoxiong Sun", "authors": "Shaoxiong Sun, Amos Folarin, Yatharth Ranjan, Zulqarnain Rashid,\n  Pauline Conde, Callum Stewart, Nicholas Cummins, Faith Matcham, Gloria Dalla\n  Costa, Sara Simblett, Letizia Leocani, Per Soelberg S{\\o}rensen, Mathias\n  Buron, Ana Isabel Guerrero, Ana Zabalza, Brenda WJH Penninx, Femke Lamers,\n  Sara Siddi, Josep Maria Haro, Inez Myin-Germeys, Aki Rintala, Til Wykes,\n  Vaibhav A. Narayan, Giancarlo Comi, Matthew Hotopf, Richard JB Dobson (on\n  behalf of the RADAR-CNS consortium)", "title": "Using smartphones and wearable devices to monitor behavioural changes\n  during COVID-19", "comments": null, "journal-ref": null, "doi": "10.2196/19992", "report-no": null, "categories": "q-bio.QM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aimed to explore the utility of the recently developed open-source mobile\nhealth platform RADAR-base as a toolbox to rapidly test the effect and response\nto NPIs aimed at limiting the spread of COVID-19. We analysed data extracted\nfrom smartphone and wearable devices and managed by the RADAR-base from 1062\nparticipants recruited in Italy, Spain, Denmark, the UK, and the Netherlands.\nWe derived nine features on a daily basis including time spent at home, maximum\ndistance travelled from home, maximum number of Bluetooth-enabled nearby\ndevices (as a proxy for physical distancing), step count, average heart rate,\nsleep duration, bedtime, phone unlock duration, and social app use duration. We\nperformed Kruskal-Wallis tests followed by post-hoc Dunns tests to assess\ndifferences in these features among baseline, pre-, and during-lockdown\nperiods. We also studied behavioural differences by age, gender, body mass\nindex (BMI), and educational background. We were able to quantify expected\nchanges in time spent at home, distance travelled, and the number of nearby\nBluetooth-enabled devices between pre- and during-lockdown periods. We saw\nreduced sociality as measured through mobility features, and increased virtual\nsociality through phone usage. People were more active on their phones,\nspending more time using social media apps, particularly around major news\nevents. Furthermore, participants had lower heart rate, went to bed later, and\nslept more. We also found that young people had longer homestay than older\npeople during lockdown and fewer daily steps. Although there was no significant\ndifference between the high and low BMI groups in time spent at home, the low\nBMI group walked more. RADAR-base can be used to rapidly quantify and provide a\nholistic view of behavioural changes in response to public health interventions\nas a result of infectious outbreaks such as COVID-19.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:58:39 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 10:19:12 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 13:52:35 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Sun", "Shaoxiong", "", "on\n  behalf of the RADAR-CNS consortium"], ["Folarin", "Amos", "", "on\n  behalf of the RADAR-CNS consortium"], ["Ranjan", "Yatharth", "", "on\n  behalf of the RADAR-CNS consortium"], ["Rashid", "Zulqarnain", "", "on\n  behalf of the RADAR-CNS consortium"], ["Conde", "Pauline", "", "on\n  behalf of the RADAR-CNS consortium"], ["Stewart", "Callum", "", "on\n  behalf of the RADAR-CNS consortium"], ["Cummins", "Nicholas", "", "on\n  behalf of the RADAR-CNS consortium"], ["Matcham", "Faith", "", "on\n  behalf of the RADAR-CNS consortium"], ["Costa", "Gloria Dalla", "", "on\n  behalf of the RADAR-CNS consortium"], ["Simblett", "Sara", "", "on\n  behalf of the RADAR-CNS consortium"], ["Leocani", "Letizia", "", "on\n  behalf of the RADAR-CNS consortium"], ["S\u00f8rensen", "Per Soelberg", "", "on\n  behalf of the RADAR-CNS consortium"], ["Buron", "Mathias", "", "on\n  behalf of the RADAR-CNS consortium"], ["Guerrero", "Ana Isabel", "", "on\n  behalf of the RADAR-CNS consortium"], ["Zabalza", "Ana", "", "on\n  behalf of the RADAR-CNS consortium"], ["Penninx", "Brenda WJH", "", "on\n  behalf of the RADAR-CNS consortium"], ["Lamers", "Femke", "", "on\n  behalf of the RADAR-CNS consortium"], ["Siddi", "Sara", "", "on\n  behalf of the RADAR-CNS consortium"], ["Haro", "Josep Maria", "", "on\n  behalf of the RADAR-CNS consortium"], ["Myin-Germeys", "Inez", "", "on\n  behalf of the RADAR-CNS consortium"], ["Rintala", "Aki", "", "on\n  behalf of the RADAR-CNS consortium"], ["Wykes", "Til", "", "on\n  behalf of the RADAR-CNS consortium"], ["Narayan", "Vaibhav A.", "", "on\n  behalf of the RADAR-CNS consortium"], ["Comi", "Giancarlo", "", "on\n  behalf of the RADAR-CNS consortium"], ["Hotopf", "Matthew", "", "on\n  behalf of the RADAR-CNS consortium"], ["Dobson", "Richard JB", "", "on\n  behalf of the RADAR-CNS consortium"]]}, {"id": "2004.14505", "submitter": "Arjun Srinivasan", "authors": "Ayshwarya Saktheeswaran, Arjun Srinivasan, John Stasko", "title": "Touch? Speech? or Touch and Speech? Investigating Multimodal Interaction\n  for Visual Network Exploration and Analysis", "comments": "12 pages, 3 figures, 3 tables", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics. January\n  2020", "doi": "10.1109/TVCG.2020.2970512", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction plays a vital role during visual network exploration as users\nneed to engage with both elements in the view (e.g., nodes, links) and\ninterface controls (e.g., sliders, dropdown menus). Particularly as the size\nand complexity of a network grow, interactive displays supporting multimodal\ninput (e.g., touch, speech, pen, gaze) exhibit the potential to facilitate\nfluid interaction during visual network exploration and analysis. While\nmultimodal interaction with network visualization seems like a promising idea,\nmany open questions remain. For instance, do users actually prefer multimodal\ninput over unimodal input, and if so, why? Does it enable them to interact more\nnaturally, or does having multiple modes of input confuse users? To answer such\nquestions, we conducted a qualitative user study in the context of a network\nvisualization tool, comparing speech- and touch-based unimodal interfaces to a\nmultimodal interface combining the two. Our results confirm that participants\nstrongly prefer multimodal input over unimodal input attributing their\npreference to: 1) the freedom of expression, 2) the complementary nature of\nspeech and touch, and 3) integrated interactions afforded by the combination of\nthe two modalities. We also describe the interaction patterns participants\nemployed to perform common network visualization operations and highlight\nthemes for future multimodal network visualization systems to consider.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 22:28:50 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Saktheeswaran", "Ayshwarya", ""], ["Srinivasan", "Arjun", ""], ["Stasko", "John", ""]]}, {"id": "2004.14595", "submitter": "Christian Marzahl", "authors": "Christian Marzahl, Marc Aubreville, Christof A. Bertram, Jennifer\n  Maier, Christian Bergler, Christine Kr\\\"oger, J\\\"orn Voigt, Katharina\n  Breininger, Robert Klopfleisch, and Andreas Maier", "title": "EXACT: A collaboration toolset for algorithm-aided annotation of images\n  with annotation version control", "comments": null, "journal-ref": "Scientific Reports 2021", "doi": "10.1038/s41598-021-83827-4", "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many research areas, scientific progress is accelerated by\nmultidisciplinary access to image data and their interdisciplinary annotation.\nHowever, keeping track of these annotations to ensure a high-quality\nmulti-purpose data set is a challenging and labour intensive task. We developed\nthe open-source online platform EXACT (EXpert Algorithm Collaboration Tool)\nthat enables the collaborative interdisciplinary analysis of images from\ndifferent domains online and offline. EXACT supports multi-gigapixel medical\nwhole slide images as well as image series with thousands of images. The\nsoftware utilises a flexible plugin system that can be adapted to diverse\napplications such as counting mitotic figures with a screening mode, finding\nfalse annotations on a novel validation view, or using the latest deep learning\nimage analysis technologies. This is combined with a version control system\nwhich makes it possible to keep track of changes in the data sets and, for\nexample, to link the results of deep learning experiments to specific data set\nversions. EXACT is freely available and has already been successfully applied\nto a broad range of annotation tasks, including highly diverse applications\nlike deep learning supported cytology scoring, interdisciplinary multi-centre\nwhole slide image tumour annotation, and highly specialised whale sound\nspectroscopy clustering.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 06:07:21 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 09:00:42 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 12:29:32 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Marzahl", "Christian", ""], ["Aubreville", "Marc", ""], ["Bertram", "Christof A.", ""], ["Maier", "Jennifer", ""], ["Bergler", "Christian", ""], ["Kr\u00f6ger", "Christine", ""], ["Voigt", "J\u00f6rn", ""], ["Breininger", "Katharina", ""], ["Klopfleisch", "Robert", ""], ["Maier", "Andreas", ""]]}, {"id": "2004.14665", "submitter": "Atia Cort\\'es Mart\\'inez", "authors": "Teresa Scantamburlo, Atia Cort\\'es, Pierre Dewitte, Daphn\\'e Van Der\n  Eycken, Valentina Billa, Pieter Duysburgh, Willemien Laenens", "title": "Covid-19 and contact tracing apps: A review under the European legal\n  framework", "comments": "This is a working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we would like to review the main technologies that have been\nproposed so far to fight the spread of the virus. Also, we would like to give\nan overview of the policy recommendations that some European organisations have\nput forward in these regards. Finally, we conclude with some considerations we\nwould like to present to public attention and discussion.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 10:07:49 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 16:10:29 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Scantamburlo", "Teresa", ""], ["Cort\u00e9s", "Atia", ""], ["Dewitte", "Pierre", ""], ["Van Der Eycken", "Daphn\u00e9", ""], ["Billa", "Valentina", ""], ["Duysburgh", "Pieter", ""], ["Laenens", "Willemien", ""]]}, {"id": "2004.14745", "submitter": "Ralf Raumanns", "authors": "Ralf Raumanns, Elif K Contar, Gerard Schouten, Veronika Cheplygina", "title": "Multi-task Ensembles with Crowdsourced Features Improve Skin Lesion\n  Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has a recognised need for large amounts of annotated data.\nDue to the high cost of expert annotations, crowdsourcing, where non-experts\nare asked to label or outline images, has been proposed as an alternative.\nAlthough many promising results are reported, the quality of diagnostic\ncrowdsourced labels is still unclear. We propose to address this by instead\nasking the crowd about visual features of the images, which can be provided\nmore intuitively, and by using these features in a multi-task learning\nframework through ensemble strategies. We compare our proposed approach to a\nbaseline model with a set of 2000 skin lesions from the ISIC 2017 challenge\ndataset. The baseline model only predicts a binary label from the skin lesion\nimage, while our multi-task model also predicts one of the following features:\nasymmetry of the lesion, border irregularity and color. We show that multi-task\nmodels with individual crowdsourced features have limited effect on the model,\nbut when combined in an ensembles, leads to improved generalisation. The area\nunder the receiver operating characteristic curve is 0.794 for the baseline\nmodel and 0.811 and 0.808 for multi-task ensembles respectively. Finally, we\ndiscuss the findings, identify some limitations and recommend directions for\nfurther research. The code of the models is available at\nhttps://github.com/raumannsr/hints_crowd.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 13:48:40 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 18:12:41 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Raumanns", "Ralf", ""], ["Contar", "Elif K", ""], ["Schouten", "Gerard", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "2004.15004", "submitter": "Zijie Wang", "authors": "Zijie J. Wang, Robert Turko, Omar Shaikh, Haekyu Park, Nilaksh Das,\n  Fred Hohman, Minsuk Kahng, Duen Horng Chau", "title": "CNN Explainer: Learning Convolutional Neural Networks with Interactive\n  Visualization", "comments": "11 pages, 14 figures, to be presented at IEEE VIS 2020. For a demo\n  video, see https://youtu.be/HnWIHWFbuUQ . For a live demo, visit\n  https://poloclub.github.io/cnn-explainer/", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030418", "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning's great success motivates many practitioners and students to\nlearn about this exciting technology. However, it is often challenging for\nbeginners to take their first step due to the complexity of understanding and\napplying deep learning. We present CNN Explainer, an interactive visualization\ntool designed for non-experts to learn and examine convolutional neural\nnetworks (CNNs), a foundational deep learning model architecture. Our tool\naddresses key challenges that novices face while learning about CNNs, which we\nidentify from interviews with instructors and a survey with past students. CNN\nExplainer tightly integrates a model overview that summarizes a CNN's\nstructure, and on-demand, dynamic visual explanation views that help users\nunderstand the underlying components of CNNs. Through smooth transitions across\nlevels of abstraction, our tool enables users to inspect the interplay between\nlow-level mathematical operations and high-level model structures. A\nqualitative user study shows that CNN Explainer helps users more easily\nunderstand the inner workings of CNNs, and is engaging and enjoyable to use. We\nalso derive design lessons from our study. Developed using modern web\ntechnologies, CNN Explainer runs locally in users' web browsers without the\nneed for installation or specialized hardware, broadening the public's\neducation access to modern deep learning techniques.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:49:44 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 01:37:29 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2020 18:42:23 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Wang", "Zijie J.", ""], ["Turko", "Robert", ""], ["Shaikh", "Omar", ""], ["Park", "Haekyu", ""], ["Das", "Nilaksh", ""], ["Hohman", "Fred", ""], ["Kahng", "Minsuk", ""], ["Chau", "Duen Horng", ""]]}]