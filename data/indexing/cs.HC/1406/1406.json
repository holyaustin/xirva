[{"id": "1406.0370", "submitter": "Stefan Diewald", "authors": "Stefan Diewald, Andreas M\\\"oller, Luis Roalter, Matthias Kranz", "title": "Simulation and Virtual Prototyping of Tangible User Interfaces", "comments": "8 pages, 8 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototyping is an important part in research and development of tangible user\ninterfaces (TUIs). On the way from the idea to a working prototype, new\nhardware prototypes usually have to be crafted repeatedly in numerous\niterations. This brings us to think about virtual prototypes that exhibit the\nsame functionality as a real TUI, but reduce the amount of time and resources\nthat have to be spent.\n  Building upon existing open-source software - the middleware Robot Operating\nSystem (ROS) and the 3D simulator Gazebo - we have created a toolkit that can\nbe used for developing and testing fully functional implementations of a\ntangible user interface as a virtual device. The entire interaction between the\nTUI and other hardware and software components is controlled by the middleware,\nwhile the human interaction with the TUI can be explored using the 3D simulator\nand 3D input/output technologies. We argue that by simulating parts of the\nhardware-software co-design process, the overall development effort can be\nreduced.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 13:53:19 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Diewald", "Stefan", ""], ["M\u00f6ller", "Andreas", ""], ["Roalter", "Luis", ""], ["Kranz", "Matthias", ""]]}, {"id": "1406.0532", "submitter": "Gon\\c{c}alo Silva", "authors": "Gon\\c{c}alo Amaral da Silva", "title": "Multimodal vs. Unimodal Physiological Control in Videogames for Enhanced\n  Realism and Depth", "comments": "MSc Thesis, English-written, 89 pages, University of Porto", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (arXiv abridged abstract) In the last two decades, videogames have evolved in\na nearly explosive way from the pixelated graphics to today's near-realistic 3D\nenvironments. The interaction devices traditionally used in videogames have not\nevolved with the same intensity, but recent HCI studies have explored\nbiofeedback interaction - the explicit manipulation of a person's physiological\ndata as input to a system - as an alternative to them. Traditional biofeedback\nprototypes apply 1 sensor to each game mechanic (unimodality).\n  In this dissertation, we introduce the combination of 2 physiological sensors\nsimultaneously per game mechanic (multimodality) and present a First-Person\nShooter game comprised of 8 game mechanics with three interaction flavours (no\nbiofeedback/vanilla, unimodal and multimodal). An empirical study with 32\nregular players was employed to explore and study differences between the three\ninteraction types and where they can be best employed.\n  Players compared the three games in terms of Fun, Ease of Use, Originality,\nPlayability and Favourite Condition. For the sake of completeness, other\nevaluation methods were used as well: IMI Questionnaire, keywords association\nand open-ended commentaries. The vanilla version was considered easier to use,\nbut both biofeedback versions were considered the most fun. Both versions were\npraised differently: the unimodal version for its simplicity of use, and the\nmultimodal for its realism, activation safety of game mechanics and depth added\nto the game. Our conclusion is that multimodal biofeedback can have a relevant\nimpact in terms of added depth, depending on the way it is used inside the\ngame. On a boundary case, it can be used to increase the feeling of empowerment\non the player when using certain abilities, or to intentionally make in-game\nactions more difficult by demanding more physical effort from the player.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jun 2014 20:51:11 GMT"}], "update_date": "2014-06-04", "authors_parsed": [["da Silva", "Gon\u00e7alo Amaral", ""]]}, {"id": "1406.0912", "submitter": "Patrick Seeling", "authors": "Patrick Seeling", "title": "Towards Quality of Experience Determination for Video in Augmented\n  Binocular Vision Scenarios", "comments": "Accepted to Signal Processing: Image Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the continuous growth in the consumer markets of mobile smartphones and\nincreasingly in augmented reality wearable devices, several avenues of research\ninvestigate the relationships between the quality perceived by mobile users and\nthe delivery mechanisms at play to support a high quality of experience for\nmobile users. In this paper, we present the first study that evaluates the\nrelationships of mobile movie quality and the viewer-perceived quality thereof\nin an augmented reality setting with see-through devices. We find that\nparticipants tend to overestimate the video quality and exhibit a significant\nvariation of accuracy that leans onto the movie content and its dynamics. Our\nfindings, thus, can broadly impact future media adaptation and delivery\nmechanisms for this new display format of mobile multimedia.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jun 2014 00:14:06 GMT"}, {"version": "v2", "created": "Thu, 26 Feb 2015 19:56:14 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2015 14:49:04 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Seeling", "Patrick", ""]]}, {"id": "1406.1907", "submitter": "Alun Preece", "authors": "Alun Preece, Chris Gwilliams, Christos Parizas, Diego Pizzocaro,\n  Jonathan Z. Bakdash, Dave Braines", "title": "Conversational Sensing", "comments": null, "journal-ref": null, "doi": "10.1117/12.2053283", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Recent developments in sensing technologies, mobile devices and context-aware\nuser interfaces have made it possible to represent information fusion and\nsituational awareness as a conversational process among actors - human and\nmachine agents - at or near the tactical edges of a network. Motivated by use\ncases in the domain of security, policing and emergency response, this paper\npresents an approach to information collection, fusion and sense-making based\non the use of natural language (NL) and controlled natural language (CNL) to\nsupport richer forms of human-machine interaction. The approach uses a\nconversational protocol to facilitate a flow of collaborative messages from NL\nto CNL and back again in support of interactions such as: turning eyewitness\nreports from human observers into actionable information (from both trained and\nuntrained sources); fusing information from humans and physical sensors (with\nassociated quality metadata); and assisting human analysts to make the best use\nof available sensing assets in an area of interest (governed by management and\nsecurity policies). CNL is used as a common formal knowledge representation for\nboth machine and human agents to support reasoning, semantic information fusion\nand generation of rationale for inferences, in ways that remain transparent to\nhuman users. Examples are provided of various alternative styles for user\nfeedback, including NL, CNL and graphical feedback. A pilot experiment with\nhuman subjects shows that a prototype conversational agent is able to gather\nusable CNL information from untrained human subjects.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jun 2014 17:13:40 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Preece", "Alun", ""], ["Gwilliams", "Chris", ""], ["Parizas", "Christos", ""], ["Pizzocaro", "Diego", ""], ["Bakdash", "Jonathan Z.", ""], ["Braines", "Dave", ""]]}, {"id": "1406.2134", "submitter": "Manish Raj", "authors": "Manish Raj, P.Chakraborty and G.C.Nandi", "title": "Rescue Robotics in Bore well Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique for rescue task in bore well environment has been proposed. India\nis facing a distressed cruel situation where in the previous years a number of\nchild deaths have been reported falling in the bore well. As the diameter of\nthe bore well is quiet narrow for any adult person and the lights goes dark\ninside it, the rescue task in those situations is a challenging task. Here we\nare proposing a robotic system which will attach a harness to the child using\npneumatic arms for picking up. A teleconferencing system will also be attached\nto the robot for communicating with the child.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jun 2014 10:51:44 GMT"}], "update_date": "2014-06-10", "authors_parsed": [["Raj", "Manish", ""], ["Chakraborty", "P.", ""], ["Nandi", "G. C.", ""]]}, {"id": "1406.2775", "submitter": "Jian Zhao", "authors": "Jian Zhao, Hengzhu Liu, Xucan Chen and Zhengfa Liang", "title": "Realization and design of a pilot assist decision-making system based on\n  speech recognition", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": "10.5121/csit.2014.4526", "report-no": null, "categories": "cs.HC cs.SD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A system based on speech recognition is proposed for pilot assist\ndecision-making. It is based on a HIL aircraft simulation platform and uses the\nmicrocontroller SPCE061A as the central processor to achieve better reliability\nand higher cost-effect performance. Technologies of LPCC (linear predictive\ncepstral coding) and DTW (Dynamic Time Warping) are applied for isolated-word\nspeech recognition to gain a smaller amount of calculation and a better\nreal-time performance. Besides, we adopt the PWM (Pulse Width Modulation)\nregulation technology to effectively regulate each control surface by speech,\nand thus to assist the pilot to make decisions. By trial and error, it is\nproved that we have a satisfactory accuracy rate of speech recognition and\ncontrol effect. More importantly, our paper provides a creative idea for\nintelligent human-computer interaction and applications of speech recognition\nin the field of aviation control. Our system is also very easy to be extended\nand applied.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 04:46:27 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Zhao", "Jian", ""], ["Liu", "Hengzhu", ""], ["Chen", "Xucan", ""], ["Liang", "Zhengfa", ""]]}, {"id": "1406.2895", "submitter": "J\\\"urgen Geiger", "authors": "J\\\"urgen T. Geiger, Maximilian Knei{\\ss}l, Bj\\\"orn Schuller and\n  Gerhard Rigoll", "title": "Acoustic Gait-based Person Identification using Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for identifying humans by their walking sounds. This\nproblem is also known as acoustic gait recognition. The goal of the system is\nto analyse sounds emitted by walking persons (mostly the step sounds) and\nidentify those persons. These sounds are characterised by the gait pattern and\nare influenced by the movements of the arms and legs, but also depend on the\ntype of shoe. We extract cepstral features from the recorded audio signals and\nuse hidden Markov models for dynamic classification. A cyclic model topology is\nemployed to represent individual gait cycles. This topology allows to model and\ndetect individual steps, leading to very promising identification rates. For\nexperimental validation, we use the publicly available TUM GAID database, which\nis a large gait recognition database containing 3050 recordings of 305 subjects\nin three variations. In the best setup, an identification rate of 65.5 % is\nachieved out of 155 subjects. This is a relative improvement of almost 30 %\ncompared to our previous work, which used various audio features and support\nvector machines.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jun 2014 13:14:32 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Geiger", "J\u00fcrgen T.", ""], ["Knei\u00dfl", "Maximilian", ""], ["Schuller", "Bj\u00f6rn", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1406.3002", "submitter": "Tyler Kaczmarek", "authors": "Tyler Kaczmarek, Alfed Kobsa, Robert Sy, Gene Tsudik", "title": "The Effect of Visual Noise on The Completion of Security Critical Tasks", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": "10.14722/usec.2015.23014", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User errors while performing security-critical tasks can lead to undesirable\nor even disastrous consequences. One major factor influencing mistakes and\nfailures is complexity of such tasks, which has been studied extensively in\nprior research. Another important issue which hardly received any attention is\nthe impact of both accidental and intended distractions on users performing\nsecurity-critical tasks. In particular, it is unclear whether, and to what\nextent, unexpected sensory cues (e.g., auditory or visual) can influence user\nbehavior and/or trigger mistakes. Better understanding of the effects of\nintended distractions will help clarify their role in adversarial models. As\npart of the research effort described in this paper, we administered a range of\nnaturally occurring -- yet unexpected -- sounds while study participants\nattempted to perform a security-critical task. We found that, although these\nauditory cues lowered participants' failure rates, they had no discernible\neffect on their task completion times. To this end, we overview some relevant\nliterature that explains these somewhat counter-intuitive findings.\n  Conducting a thorough and meaningful study on user errors requires a large\nnumber of participants, since errors are typically infrequent and should not be\ninstigated more than once per subject. To reduce the effort of running numerous\nsubjects, we developed a novel experimental setup that was fully automated and\nunattended. We discuss our experience with this setup and highlight the pros\nand cons of generalizing its usage.\n", "versions": [{"version": "v1", "created": "Wed, 21 May 2014 19:56:53 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 17:29:26 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Kaczmarek", "Tyler", ""], ["Kobsa", "Alfed", ""], ["Sy", "Robert", ""], ["Tsudik", "Gene", ""]]}, {"id": "1406.3117", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Amy Banic", "title": "Low-cost Augmented Reality prototype for controlling network devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the evolution of mobile devices, and smart-phones in particular, comes\nthe ability to create new experiences that enhance the way we see, interact,\nand manipulate objects, within the world that surrounds us. It is now possible\nto blend data from our senses and our devices in numerous ways that simply were\nnot possible before using Augmented Reality technology. In a near future, when\nall of the office devices as well as your personal electronic gadgets are on a\ncommon wireless network, operating them using a universal remote controller\nwould be possible. This paper presents an off-the-shelf, low-cost prototype\nthat leverages the Augmented Reality technology to deliver a novel and\ninteractive way of operating office network devices around using a mobile\ndevice. We believe this type of system may provide benefits to controlling\nmultiple integrated devices and visualizing interconnectivity or utilizing\nvisual elements to pass information from one device to another, or may be\nespecially beneficial to control devices when interacting with them physically\nmay be difficult or pose danger or harm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 04:46:32 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["Nguyen", "Anh", ""], ["Banic", "Amy", ""]]}, {"id": "1406.3225", "submitter": "Stefan Diewald", "authors": "Andreas M\\\"oller, Stefan Diewald, Luis Roalter, Matthias Kranz", "title": "Supporting Mobile Multimodal Interaction with a Rule-Based Framework", "comments": "8 pages, 4 figures, extended version of the short paper at Mensch und\n  Computer 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodality can make (especially mobile) device interaction more efficient.\nSensors and communication capabilities of modern smartphones and tablets lay\nthe technical basis for its implementation. Still, mobile platforms do not make\nmultimodal interaction support trivial. Building multimodal applications\nrequires various APIs with different paradigms, high-level interpretation of\ncontextual data, and a method for fusing individual inputs and outputs. To\nreduce this effort, we created a framework that simplifies and accelerates the\ncreation of multimodal applications for prototyping and research. It provides\nan abstraction of information representations in different modalities, unifies\naccess to implicit and explicit information, and wires together the logic\nbehind context-sensitive modality switches. In the paper, we present the\nstructure and features of our framework, and validate it by four implemented\ndemonstrations of different complexity.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 13:02:54 GMT"}], "update_date": "2014-06-13", "authors_parsed": [["M\u00f6ller", "Andreas", ""], ["Diewald", "Stefan", ""], ["Roalter", "Luis", ""], ["Kranz", "Matthias", ""]]}, {"id": "1406.3337", "submitter": "Jared Moore", "authors": "Jared Moore, Anthony Clark, Philip McKinley", "title": "Evolutionary Robotics on the Web with WebGL and Javascript", "comments": "Presented at WebAL-1: Workshop on Artificial Life and the Web 2014\n  (arXiv:1406.2507)", "journal-ref": null, "doi": null, "report-no": "WebAL1/2014/02", "categories": "cs.NE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-based applications are highly accessible to users, providing rich,\ninteractive content while eliminating the need to install software locally.\nHowever, evolutionary robotics (ER) has faced challenges in this domain as\nweb-based technologies have not been amenable to 3D physics simulations.\nTraditionally, physics-based simulations require a local installation and a\nhigh degree of user knowledge to configure an environment, but the emergence of\nJavascript-based physics engines enables complex simulations to be executed in\nweb browsers. These developments create opportunities for ER research to reach\nnew audiences by increasing accessibility. In this work, we introduce two\nweb-based tools we have built to facilitate the exchange of ideas with other\nresearchers as well as outreach to K-12 students and the general public. The\nfirst tool is intended to distribute and exchange ER research results, while\nthe second is a completely browser-based implementation of an ER environment.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jun 2014 19:49:16 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 13:54:17 GMT"}, {"version": "v3", "created": "Fri, 11 Jul 2014 01:20:24 GMT"}], "update_date": "2014-07-14", "authors_parsed": [["Moore", "Jared", ""], ["Clark", "Anthony", ""], ["McKinley", "Philip", ""]]}, {"id": "1406.3561", "submitter": "Wei  Di", "authors": "Wei Di, Anurag Bhardwaj, Vignesh Jagadeesh, Robinson Piramuthu,\n  Elizabeth Churchill", "title": "When relevance is not Enough: Promoting Visual Attractiveness for\n  Fashion E-commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion, and especially apparel, is the fastest-growing category in online\nshopping. As consumers requires sensory experience especially for apparel goods\nfor which their appearance matters most, images play a key role not only in\nconveying crucial information that is hard to express in text, but also in\naffecting consumer's attitude and emotion towards the product. However,\nresearch related to e-commerce product image has mostly focused on quality at\nperceptual level, but not the quality of content, and the way of presenting.\nThis study aims to address the effectiveness of types of image in showcasing\nfashion apparel in terms of its attractiveness, i.e. the ability to draw\nconsumer's attention, interest, and in return their engagement. We apply\nadvanced vision technique to quantize attractiveness using three common display\ntypes in fashion filed, i.e. human model, mannequin, and flat. We perform\ntwo-stage study by starting with large scale behavior data from real online\nmarket, then moving to well designed user experiment to further deepen our\nunderstandings on consumer's reasoning logic behind the action. We propose a\nFisher noncentral hypergeometric distribution based user choice model to\nquantitatively evaluate user's preference. Further, we investigate the\npotentials to leverage visual impact for a better search that caters to user's\npreference. A visual attractiveness based re-ranking model that incorporates\nboth presentation efficacy and user preference is proposed. We show\nquantitative improvement by promoting visual attractiveness into search on top\nof relevance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jun 2014 15:04:20 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Di", "Wei", ""], ["Bhardwaj", "Anurag", ""], ["Jagadeesh", "Vignesh", ""], ["Piramuthu", "Robinson", ""], ["Churchill", "Elizabeth", ""]]}, {"id": "1406.4803", "submitter": "Deepshree Vadeyar", "authors": "Deepshree A. Vadeyar, Yogish H.K", "title": "Reorganization of Links to Improve User Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Website can be easily design but to efficient user navigation is not a easy\ntask since user behavior is keep changing and developer view is quite different\nfrom what user wants, so to improve navigation one way is reorganization of\nwebsite structure. For reorganization here proposed strategy is farthest first\ntraversal clustering algorithm perform clustering on two numeric parameters and\nfor finding frequent traversal path of user Apriori algorithm is used. Our aim\nis to perform reorganization with fewer changes in website structure.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jun 2014 17:25:06 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Vadeyar", "Deepshree A.", ""], ["K", "Yogish H.", ""]]}, {"id": "1406.5572", "submitter": "Emery Berger", "authors": "Emma Tosch and Emery D. Berger", "title": "SurveyMan: Programming and Automatically Debugging Surveys", "comments": "Submitted version; accepted to OOPSLA 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveys can be viewed as programs, complete with logic, control flow, and\nbugs. Word choice or the order in which questions are asked can unintentionally\nbias responses. Vague, confusing, or intrusive questions can cause respondents\nto abandon a survey. Surveys can also have runtime errors: inattentive\nrespondents can taint results. This effect is especially problematic when\ndeploying surveys in uncontrolled settings, such as on the web or via\ncrowdsourcing platforms. Because the results of surveys drive business\ndecisions and inform scientific conclusions, it is crucial to make sure they\nare correct.\n  We present SurveyMan, a system for designing, deploying, and automatically\ndebugging surveys. Survey authors write their surveys in a lightweight\ndomain-specific language aimed at end users. SurveyMan statically analyzes the\nsurvey to provide feedback to survey authors before deployment. It then\ncompiles the survey into JavaScript and deploys it either to the web or a\ncrowdsourcing platform. SurveyMan's dynamic analyses automatically find survey\nbugs, and control for the quality of responses. We evaluate SurveyMan's\nalgorithms analytically and empirically, demonstrating its effectiveness with\ncase studies of social science surveys conducted via Amazon's Mechanical Turk.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 02:52:48 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Tosch", "Emma", ""], ["Berger", "Emery D.", ""]]}, {"id": "1406.5581", "submitter": "Anh Nguyen", "authors": "Anh Nguyen, Amy Banic", "title": "3DTouch: A wearable 3D input device with an optical sensor and a 9-DOF\n  inertial measurement unit", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present 3DTouch, a novel 3D wearable input device worn on the fingertip\nfor 3D manipulation tasks. 3DTouch is designed to fill the missing gap of a 3D\ninput device that is self-contained, mobile, and universally working across\nvarious 3D platforms. This paper presents a low-cost solution to designing and\nimplementing such a device. Our approach relies on relative positioning\ntechnique using an optical laser sensor and a 9-DOF inertial measurement unit.\n  3DTouch is self-contained, and designed to universally work on various 3D\nplatforms. The device employs touch input for the benefits of passive haptic\nfeedback, and movement stability. On the other hand, with touch interaction,\n3DTouch is conceptually less fatiguing to use over many hours than 3D spatial\ninput devices. We propose a set of 3D interaction techniques including\nselection, translation, and rotation using 3DTouch. An evaluation also\ndemonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for\nsubtle touch interaction in 3D space. Modular solutions like 3DTouch opens up a\nwhole new design space for interaction techniques to further develop on.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jun 2014 06:32:35 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 07:17:44 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Nguyen", "Anh", ""], ["Banic", "Amy", ""]]}, {"id": "1406.5765", "submitter": "Ming Jin", "authors": "Ming Jin, Han Zou, Kevin Weekly, Ruoxi Jia, Alexandre M. Bayen, and\n  Costas J. Spanos", "title": "Environmental Sensing by Wearable Device for Indoor Activity and\n  Location Estimation", "comments": "submitted to the 40th Annual Conference of the IEEE Industrial\n  Electronics Society (IECON)", "journal-ref": null, "doi": "10.1109/IECON.2014.7049320", "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present results from a set of experiments in this pilot study to\ninvestigate the causal influence of user activity on various environmental\nparameters monitored by occupant carried multi-purpose sensors. Hypotheses with\nrespect to each type of measurements are verified, including temperature,\nhumidity, and light level collected during eight typical activities: sitting in\nlab / cubicle, indoor walking / running, resting after physical activity,\nclimbing stairs, taking elevators, and outdoor walking. Our main contribution\nis the development of features for activity and location recognition based on\nenvironmental measurements, which exploit location- and activity-specific\ncharacteristics and capture the trends resulted from the underlying\nphysiological process. The features are statistically shown to have good\nseparability and are also information-rich. Fusing environmental sensing\ntogether with acceleration is shown to achieve classification accuracy as high\nas 99.13%. For building applications, this study motivates a sensor fusion\nparadigm for learning individualized activity, location, and environmental\npreferences for energy management and user comfort.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jun 2014 21:13:58 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Jin", "Ming", ""], ["Zou", "Han", ""], ["Weekly", "Kevin", ""], ["Jia", "Ruoxi", ""], ["Bayen", "Alexandre M.", ""], ["Spanos", "Costas J.", ""]]}, {"id": "1406.6012", "submitter": "Georg Groh", "authors": "Niklas Kl\\\"ugel and Timo Becker and Georg Groh", "title": "Designing Sound Collaboratively - Perceptually Motivated Audio Synthesis", "comments": "Extended version of submission to conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we will discuss a prototype that allows a group of\nusers to design sound collaboratively in real time using a multi-touch\ntabletop. We make use of a machine learning method to generate a mapping from\nperceptual audio features to synthesis parameters. This mapping is then used\nfor visualization and interaction. Finally, we discuss the results of a\ncomparative evaluation study.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jun 2014 18:29:59 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Kl\u00fcgel", "Niklas", ""], ["Becker", "Timo", ""], ["Groh", "Georg", ""]]}, {"id": "1406.6829", "submitter": "Chlo\\\"e Brown", "authors": "Chlo\\\"e Brown, Christos Efstratiou, Ilias Leontiadis, Daniele Quercia,\n  Cecilia Mascolo, James Scott, Peter Key", "title": "The architecture of innovation: Tracking face-to-face interactions with\n  ubicomp technologies", "comments": "14 pages, 9 figures. To appear in ACM International Joint Conference\n  on Pervasive and Ubiquitous Computing (Ubicomp 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The layouts of the buildings we live in shape our everyday lives. In office\nenvironments, building spaces affect employees' communication, which is crucial\nfor productivity and innovation. However, accurate measurement of how spatial\nlayouts affect interactions is a major challenge and traditional techniques may\nnot give an objective view.We measure the impact of building spaces on social\ninteractions using wearable sensing devices. We study a single organization\nthat moved between two different buildings, affording a unique opportunity to\nexamine how space alone can affect interactions. The analysis is based on two\nlarge scale deployments of wireless sensing technologies: short-range,\nlightweight RFID tags capable of detecting face-to-face interactions. We\nanalyze the traces to study the impact of the building change on social\nbehavior, which represents a first example of using ubiquitous sensing\ntechnology to study how the physical design of two workplaces combines with\norganizational structure to shape contact patterns.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 10:17:32 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Brown", "Chlo\u00eb", ""], ["Efstratiou", "Christos", ""], ["Leontiadis", "Ilias", ""], ["Quercia", "Daniele", ""], ["Mascolo", "Cecilia", ""], ["Scott", "James", ""], ["Key", "Peter", ""]]}, {"id": "1406.6873", "submitter": "Ian Wood", "authors": "Matthew R. Francisco, Ian Wood, Selma \\v{S}abanovi\\'c, and Luis M.\n  Rocha", "title": "Designing a minimalist socially aware robotic agent for the home", "comments": "8 pages, 10 figures, To be published in the ALIFE 14 conference\n  proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a minimalist social robot that relies on long timeseries of low\nresolution data such as mechanical vibration, temperature, lighting, sounds and\ncollisions. Our goal is to develop an experimental system for growing socially\nsituated robotic agents whose behavioral repertoire is subsumed by the social\norder of the space. To get there we are designing robots that use their simple\nsensors and motion feedback routines to recognize different classes of human\nactivity and then associate to each class a range of appropriate behaviors. We\nuse the Katie Family of robots, built on the iRobot Create platform, an Arduino\nUno, and a Raspberry Pi. We describe its sensor abilities and exploratory tests\nthat allow us to develop hypotheses about what objects (sensor data) correspond\nto something known and observable by a human subject. We use machine learning\nmethods to classify three social scenarios from over a hundred experiments,\ndemonstrating that it is possible to detect social situations with high\naccuracy, using the low-resolution sensors from our minimalist robot.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jun 2014 13:08:28 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Francisco", "Matthew R.", ""], ["Wood", "Ian", ""], ["\u0160abanovi\u0107", "Selma", ""], ["Rocha", "Luis M.", ""]]}]