[{"id": "1906.00026", "submitter": "Charith Perera", "authors": "Rhys Beckett, Charith Perera", "title": "IoT Skullfort: Exploring the Impact of Internet Connected Cosplay", "comments": null, "journal-ref": "Proceedings of the 2019 ACM International Joint Conference and\n  2019 International Symposium on Pervasive and Ubiquitous Computing and\n  Wearable Computers", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the potential impact of Internet of Things (IoT)\ntechnology may have on the cosplay community. We developed a costume (an IoT\nSkullfort) and embedded IoT technology to enhance its capabilities and user\ninteractions. Sensing technologies are widely used in many different wearable\ndomains including cosplay scenarios. However, in most of these scenarios,\ntypical interaction pattern is that the costume responds to its environment or\nthe player's behaviour (e.g., colour of lights may get changed when player\nmoves hands). In contrast, our research focuses on exploring scenarios where\nthe audience (third party) get to manipulate the costume behaviour (e.g., the\naudience get to change the colour of the Skullfort using a mobile application).\nWe believe such an audience (third party) influenced cosplay brings new\nopportunities for enhanced entertainment. However, it also creates significant\nchallenges. We report the results gathered through a focus group conducted in\ncollaboration with cosplay community experts.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 09:48:24 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 12:36:04 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Beckett", "Rhys", ""], ["Perera", "Charith", ""]]}, {"id": "1906.00108", "submitter": "Gautham Krishna Gudur", "authors": "Gautham Krishna Gudur, Prahalathan Sundaramoorthy and Venkatesh\n  Umaashankar", "title": "ActiveHARNet: Towards On-Device Deep Bayesian Active Learning for Human\n  Activity Recognition", "comments": "6 pages, 5 figures, ACM MobiSys 2019 (3rd International Workshop on\n  Embedded and Mobile Deep Learning - EMDL '19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various health-care applications such as assisted living, fall detection\netc., require modeling of user behavior through Human Activity Recognition\n(HAR). HAR using mobile- and wearable-based deep learning algorithms have been\non the rise owing to the advancements in pervasive computing. However, there\nare two other challenges that need to be addressed: first, the deep learning\nmodel should support on-device incremental training (model updation) from\nreal-time incoming data points to learn user behavior over time, while also\nbeing resource-friendly; second, a suitable ground truthing technique (like\nActive Learning) should help establish labels on-the-fly while also selecting\nonly the most informative data points to query from an oracle. Hence, in this\npaper, we propose ActiveHARNet, a resource-efficient deep ensembled model which\nsupports on-device Incremental Learning and inference, with capabilities to\nrepresent model uncertainties through approximations in Bayesian Neural\nNetworks using dropout. This is combined with suitable acquisition functions\nfor active learning. Empirical results on two publicly available wrist-worn HAR\nand fall detection datasets indicate that ActiveHARNet achieves considerable\nefficiency boost during inference across different users, with a substantially\nlow number of acquired pool points (at least 60% reduction) during incremental\nlearning on both datasets experimented with various acquisition functions, thus\ndemonstrating deployment and Incremental Learning feasibility.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 22:28:40 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Gudur", "Gautham Krishna", ""], ["Sundaramoorthy", "Prahalathan", ""], ["Umaashankar", "Venkatesh", ""]]}, {"id": "1906.00606", "submitter": "Manish Joshi", "authors": "Manish Joshi, Sangeeta Jadhav", "title": "An Extensive Review of Computational Dance Automation Techniques and\n  Applications", "comments": "15 pages, 6 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dance is an art and when technology meets this kind of art, it's a novel\nattempt in itself. Several researchers have attempted to automate several\naspects of dance, right from dance notation to choreography. Furthermore, we\nhave encountered several applications of dance automation like e-learning,\nheritage preservation, etc. Despite several attempts by researchers for more\nthan two decades in various styles of dance all round the world, we found a\nreview paper that portrays the research status in this area dating to 1990\n\\cite{politis1990computers}. Hence, we decide to come up with a comprehensive\nreview article that showcases several aspects of dance automation.\n  This paper is an attempt to review research work reported in the literature,\ncategorize and group all research work completed so far in the field of\nautomating dance. We have explicitly identified six major categories\ncorresponding to the use of computers in dance automation namely dance\nrepresentation, dance capturing, dance semantics, dance generation, dance\nprocessing approaches and applications of dance automation systems. We\nclassified several research papers under these categories according to their\nresearch approach and functionality. With the help of proposed categories and\nsubcategories one can easily determine the state of research and the new\navenues left for exploration in the field of dance automation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 07:17:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Joshi", "Manish", ""], ["Jadhav", "Sangeeta", ""]]}, {"id": "1906.00916", "submitter": "Jeffrey Uhlmann", "authors": "Jeffrey Uhlmann", "title": "Software Adaptation and Generalization of Physically-Constrained Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a case study of the use of relatively sophisticated\nmathematics and algorithms to redefine and adapt a simple traditional\ngame/puzzle to exploit the computational power of smart devices. The focus here\nis not so much on the end product as it is on the process and considerations\nunderpinning its development. Ancillary results of the venture include\ngeneralizations of the circular-shift operator and examination of its\ncomputational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 16:37:23 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Uhlmann", "Jeffrey", ""]]}, {"id": "1906.01071", "submitter": "Keshav Dasu", "authors": "Keshav Dasu, Kwan-Liu Ma, Joyce Ma, Jennifer Frazier", "title": "Sea of Genes: Combining Animation and Narrative Strategies to Visualize\n  Metagenomic Data for Museums", "comments": "This manuscript has been accepted to VIS 2020 and TVCG 9 pages 2\n  references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the application of narrative strategies to present a complex and\nunfamiliar metagenomics dataset to the public in a science museum. Our dataset\ncontains information about microbial gene expressions that scientists use to\ninfer the behavior of microbes. This exhibit had three goals: to inform (the)\npublic about microbes' behavior, cycles, and patterns; to link their behavior\nto the concept of gene expression; and to highlight scientists' use of gene\nexpression data to understand the role of microbes. To address these three\ngoals, we created a visualization with three narrative layers, each layer\ncorresponding to a goal. This study presented us with an opportunity to assess\nexisting frameworks for narrative visualization in a naturalistic setting. We\npresent three successive rounds of design and evaluation of our attempts to\nengage visitors with complex data through narrative visualization. We highlight\nour design choices and their underlying rationale based on extant theories. We\nconclude that a central animation based on a curated dataset could successfully\nachieve our first goal, i.e., to communicate the aggregate behavior and\ninteractions of microbes. We failed to achieve our second goal and had limited\nsuccess with the third goal. Overall, this study highlights the challenges of\ntelling multi-layered stories and the need for new frameworks for communicating\nlayered stories in public settings.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 20:41:16 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 07:08:23 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Dasu", "Keshav", ""], ["Ma", "Kwan-Liu", ""], ["Ma", "Joyce", ""], ["Frazier", "Jennifer", ""]]}, {"id": "1906.01122", "submitter": "Xu Han", "authors": "Xu Han, Tom Yeh", "title": "Evaluating Voice Skills by Design Guidelines Using an Automatic Voice\n  Crawler", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, adaptive voice applications supported by voice assistants (VA) are\nvery popular (i.e., Alexa skills and Google Home Actions). Under this\ncircumstance, how to design and evaluate these voice interactions well is very\nimportant. In our study, we developed a voice crawler to collect responses from\n100 most popular Alexa skills under 10 different categories and evaluated these\nresponses to find out how they comply with 8 selected design guidelines\npublished by Amazon. Our findings show that basic commands support are the most\nfollowed ones while those related to personalised interaction are relatively\nless. There also exists variation in design guidelines compliance across\ndifferent skill categories. Based on our findings and real skill examples, we\noffer suggestions for new guidelines to complement the existing ones and\npropose agendas for future HCI research to improve voice applications' user\nexperiences.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 23:39:22 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Han", "Xu", ""], ["Yeh", "Tom", ""]]}, {"id": "1906.01148", "submitter": "Besmira Nushi", "authors": "Gagan Bansal, Besmira Nushi, Ece Kamar, Dan Weld, Walter Lasecki, Eric\n  Horvitz", "title": "A Case for Backward Compatibility for Human-AI Teams", "comments": "presented at 2019 ICML Workshop on Human in the Loop Learning (HILL\n  2019), Long Beach, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI systems are being deployed to support human decision making in high-stakes\ndomains. In many cases, the human and AI form a team, in which the human makes\ndecisions after reviewing the AI's inferences. A successful partnership\nrequires that the human develops insights into the performance of the AI\nsystem, including its failures. We study the influence of updates to an AI\nsystem in this setting. While updates can increase the AI's predictive\nperformance, they may also lead to changes that are at odds with the user's\nprior experiences and confidence in the AI's inferences, hurting therefore the\noverall team performance. We introduce the notion of the compatibility of an AI\nupdate with prior user experience and present methods for studying the role of\ncompatibility in human-AI teams. Empirical results on three high-stakes domains\nshow that current machine learning algorithms do not produce compatible\nupdates. We propose a re-training objective to improve the compatibility of an\nupdate by penalizing new errors. The objective offers full leverage of the\nperformance/compatibility tradeoff, enabling more compatible yet accurate\nupdates.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 01:09:14 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Bansal", "Gagan", ""], ["Nushi", "Besmira", ""], ["Kamar", "Ece", ""], ["Weld", "Dan", ""], ["Lasecki", "Walter", ""], ["Horvitz", "Eric", ""]]}, {"id": "1906.01197", "submitter": "Yian Zhang", "authors": "Yian Zhang, Yinmiao Li, Daniel Chin, Gus Xia", "title": "Adaptive Multimodal Music Learning via Interactive-haptic Instrument", "comments": "6 pages, 14 figures, 2 tables. This paper is accepted by NIME\n  2019(New Interface for Musical Expression)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haptic interfaces have untapped the sense of touch to assist multimodal music\nlearning. We have recently seen various improvements of interface design on\ntactile feedback and force guidance aiming to make instrument learning more\neffective. However, most interfaces are still quite static; they cannot yet\nsense the learning progress and adjust the tutoring strategy accordingly. To\nsolve this problem, we contribute an adaptive haptic interface based on the\nlatest design of haptic flute. We first adopted a clutch mechanism to enable\nthe interface to turn on and off the haptic control flexibly in real time. The\ninteractive tutor is then able to follow human performances and apply the\n\"teacher force\" only when the software instructs so. Finally, we incorporated\nthe adaptive interface with a step-by-step dynamic learning strategy.\nExperimental results showed that dynamic learning dramatically outperforms\nstatic learning, which boosts the learning rate by 45.3% and shrinks the\nforgetting chance by 86%.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 05:10:41 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Zhang", "Yian", ""], ["Li", "Yinmiao", ""], ["Chin", "Daniel", ""], ["Xia", "Gus", ""]]}, {"id": "1906.01251", "submitter": "Michael P. J. Camilleri Mr", "authors": "Michael P. J. Camilleri and Christopher K. I. Williams", "title": "The Extended Dawid-Skene Model: Fusing Information from Multiple Data\n  Schemas", "comments": "Updated with Author-Preprint version following Publication in P.\n  Cellier and K. Driessens (Eds.): ECML PKDD 2019 Workshops, CCIS 1167, pp. 121\n  - 136, 2020", "journal-ref": "in ECML PKDD 2019 Workshops, CCIS 1167, pp. 121 - 136, 2020", "doi": "10.1007/978-3-030-43823-4_11", "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While label fusion from multiple noisy annotations is a well understood\nconcept in data wrangling (tackled for example by the Dawid-Skene (DS) model),\nwe consider the extended problem of carrying out learning when the labels\nthemselves are not consistently annotated with the same schema. We show that\neven if annotators use disparate, albeit related, label-sets, we can still draw\ninferences for the underlying full label-set. We propose the Inter-Schema\nAdapteR (ISAR) to translate the fully-specified label-set to the one used by\neach annotator, enabling learning under such heterogeneous schemas, without the\nneed to re-annotate the data. We apply our method to a mouse behavioural\ndataset, achieving significant gains (compared with DS) in out-of-sample\nlog-likelihood (-3.40 to -2.39) and F1-score (0.785 to 0.864).\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 07:50:51 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 09:53:16 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Camilleri", "Michael P. J.", ""], ["Williams", "Christopher K. I.", ""]]}, {"id": "1906.01400", "submitter": "Marco Winckler", "authors": "Thaise Costa, Liliane Machado, Ana Maria Gondim Valen\\c{c}a, Marco\n  Winckler (UFRGS, Polytech'Lab, IRIT), Ronei Moraes", "title": "Pegadas: A Portal for Management and Activities Planning with Games and\n  Environments for Education in Health", "comments": null, "journal-ref": "Computers in Entertainment, Association for Computing Machinery\n  (ACM), 2018", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications for learning and training have been developed and highlighted as\nimportant tools in health education. Despite the several approaches and\ninitiatives, these tools have not been used in an integrated way. The specific\nskills approached by each application, the absence of a consensus about how to\nintegrate them in the curricula, and the necessity of evaluation tools that\nstandardize their utilization are the main difficulties. Considering these\nissues, Portal of Games and Environments Management for Designing Activities in\nHealth (Pegadas) was designed and developed as a web portal that offers the\nservices of organizing and sequencing serious games and virtual environments\nand evaluating the performance of the user in these activities. This article\npresents the structure of Pegadas, including the proposal of an evaluation\nmodel based on learning objectives. The results indicate its potential to\ncollaborate with human resources training from the proposal of the sequencing,\nallowing a linked composition of activities and providing the reinforcement or\ncomplement of tasks and contents in a progressive scale with planned\neducational objective-based evaluation. These results can contribute to expand\nthe discussions about ways to integrate the use of these applications in health\ncurricula.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 13:12:55 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Costa", "Thaise", "", "UFRGS, Polytech'Lab, IRIT"], ["Machado", "Liliane", "", "UFRGS, Polytech'Lab, IRIT"], ["Valen\u00e7a", "Ana Maria Gondim", "", "UFRGS, Polytech'Lab, IRIT"], ["Winckler", "Marco", "", "UFRGS, Polytech'Lab, IRIT"], ["Moraes", "Ronei", ""]]}, {"id": "1906.01410", "submitter": "Marco Winckler", "authors": "Sergio Firmenich (UNLP), Gabriela Bosetti, Gustavo Rossi (UNLP), Marco\n  Winckler (UFRGS, Polytech'Lab, IRIT), Jos\\'e Mar\\'ia Corletto (UNLP)", "title": "Distributed Web browsing: supporting frequent uses and opportunistic\n  requirements", "comments": null, "journal-ref": "Universal Access in the Information Society, Springer Verlag, 2017", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the development of Web applications supporting distributed user\ninterfaces (DUI) is straightforward. However, it is still hard to find Web\nsites supporting this kind of user interaction. Although studies on this field\nhave demonstrated that DUI would improve the user experience, users are not\nmassively empowered to manage these kinds of interactions. In this setting, we\npropose to move the responsibility of distributing both the UI and user\ninteraction, from the application (a Web application) to the client (the Web\nbrowser), giving also rise to inter-application interaction distribution. This\npaper presents a platform for client-side DUI, built on the foundations of Web\naugmentation and End User Development. The idea is to empower end users to\napply an augmentation layer over existing Web applications, considering both\nfrequent use and opportunistic DUI requirements. In this work, we present the\narchitecture and a prototype tool supporting this approach and illustrate the\nincorporation of some DUI features through case studies.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 13:29:02 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Firmenich", "Sergio", "", "UNLP"], ["Bosetti", "Gabriela", "", "UNLP"], ["Rossi", "Gustavo", "", "UNLP"], ["Winckler", "Marco", "", "UFRGS, Polytech'Lab, IRIT"], ["Corletto", "Jos\u00e9 Mar\u00eda", "", "UNLP"]]}, {"id": "1906.01417", "submitter": "Marco Winckler", "authors": "Thiago Rocha, Jean-Luc Hak (UPS), Marco Winckler (UFRGS, Polytech'Lab,\n  IRIT), Olivier Nicolas (EA4215)", "title": "A Comparative Study of Milestones for Featuring GUI Prototyping Tools", "comments": null, "journal-ref": "Journal of Software Engineering and Applications, SCIRP, 2017", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prototyping is one of the core activities of User-Centered Design (UCD)\nprocesses and an integral component of Human-Computer Interaction (HCI)\nresearch. For many years, prototyping was synonym of paper-based mockups and\nonly more recently we can say that dedicated tools for supporting prototyping\nactivities really reach the market. In this paper, we propose to analyze the\nevolution of prototyping tools for supporting the development process of\ninteractive systems. For that, this paper presents a review of the literature.\nWe analyze the tools proposed by academic community as a proof of concepts\nand/or support to research activities. Moreover, we also analyze prototyping\ntools that are available in the market. We report our observation in terms of\nfeatures that appear over time and constitute milestones for understating the\nevolution of concerns related to the development and use of prototyping tools.\nThis survey covers publications published since 1988 in some of the main HCI\nconferences and 118 commercial tools available on the web. The results enable a\nbrief comparison of characteristics present in both academic and commercial\ntools, how they have evolved, and what are the gaps that can provide insights\nfor future research and development.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 13:33:14 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Rocha", "Thiago", "", "UPS"], ["Hak", "Jean-Luc", "", "UPS"], ["Winckler", "Marco", "", "UFRGS, Polytech'Lab,\n  IRIT"], ["Nicolas", "Olivier", "", "EA4215"]]}, {"id": "1906.01418", "submitter": "Marco Winckler", "authors": "Gabriela Bosetti, Sergio Firmenich (UNLP), Silvia Gordillo (UNLP),\n  Gustavo Rossi (UNLP), Marco Winckler (UFRGS, Polytech'Lab, IRIT)", "title": "An End-User Development approach for Mobile Web Augmentation", "comments": null, "journal-ref": "Mobile Information Systems, Hindawi/IOS Press, 2017", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trend towards mobile devices usage has put more than ever the Web as a\nubiquitous platform where users perform all kind of tasks. In some cases, users\naccess the Web with 'native' mobile applications developed for well-known\nsites, such as LinkedIn, Facebook, Twitter, etc. These native applications\nmight offer further (e.g. location-based) functionalities to their users in\ncomparison with their corresponding Web sites, because they were developed with\nmobile features in mind. However, most Web applications have not this native\nmobile counterpart and users access them using browsers in the mobile device.\nUsers might eventually want to add mobile features on these Web sites even\nthough those features were not supported originally. In this paper we present a\nnovel approach to allow end users to augment their preferred Web sites with\nmobile features. This end-user approach is supported by a framework for mobile\nWeb augmentation that we describe in the paper. We also present a set of\nsupporting tools and a validation experiment with end users.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 13:34:52 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Bosetti", "Gabriela", "", "UNLP"], ["Firmenich", "Sergio", "", "UNLP"], ["Gordillo", "Silvia", "", "UNLP"], ["Rossi", "Gustavo", "", "UNLP"], ["Winckler", "Marco", "", "UFRGS, Polytech'Lab, IRIT"]]}, {"id": "1906.01699", "submitter": "Evgeny Burnaev", "authors": "Boris B. Velichkovsky and Nikita Khromov and Alexander Korotin and\n  Evgeny Burnaev and Andrey Somov", "title": "Visual Fixations Duration as an Indicator of Skill Level in eSports", "comments": "10 pages, 3 figures", "journal-ref": "17th IFIP TC.13 International Conference on Human-Computer\n  Interaction, Springer LNCS, 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using highly interactive systems like computer games requires a lot of visual\nactivity and eye movements. Eye movements are best characterized by visual\nfixation - periods of time when the eyes stay relatively still over an object.\nWe analyzed the distributions of fixation duration of professional athletes,\namateur and newbie players. We show that the analysis of fixation durations can\nbe used to deduce the skill level in computer game players. Highly skilled\ngaming performance is characterized by more variability in fixation durations\nand by bimodal fixation duration distributions suggesting the presence of two\nfixation types in high skill gamers. These fixation types were identified as\nambient (automatic spatial processing) and focal (conscious visual processing).\nThe analysis of computer gamers' skill level via the analysis of fixation\ndurations may be used in developing adaptive interfaces and in interface\ndesign.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 19:45:50 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 08:27:29 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Velichkovsky", "Boris B.", ""], ["Khromov", "Nikita", ""], ["Korotin", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Somov", "Andrey", ""]]}, {"id": "1906.01756", "submitter": "Dakuo Wang", "authors": "Dakuo Wang, Haoyu Wang, Mo Yu, Zahra Ashktorab, Ming Tan", "title": "Slack Channels Ecology in Enterprises: How Employees Collaborate Through\n  Group Chat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the long history of studying instant messaging usage in\norganizations, we know very little about how today's people participate in\ngroup chat channels and interact with others. In this short note, we aim to\nupdate the existing knowledge on how group chat is used in the context of\ntoday's organizations. We have the privilege of collecting a total of 4300\npublicly available group chat channels in Slack from an R\\&D department in a\nmultinational IT company. Through qualitative coding of 100 channels, we\nidentified 9 channel categories such as project based channels and event\nchannels. We further defined a feature metric with 21 features to depict the\ngroup communication style for these group chat channels, with which we\nsuccessfully trained a machine learning model that can automatically classify a\ngiven group channel into one of the 9 categories. In addition, we illustrated\nhow these communication metrics could be used for analyzing teams'\ncollaboration activities. We focused on 117 project teams as we have their\nperformance data, and further collected 54 out of the 117 teams' Slack group\ndata and generated the communication style metrics for each of them. With these\ndata, we are able to build a regression model to reveal the relationship\nbetween these group communication styles and one indicator of the project team\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 23:54:08 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 10:41:46 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wang", "Dakuo", ""], ["Wang", "Haoyu", ""], ["Yu", "Mo", ""], ["Ashktorab", "Zahra", ""], ["Tan", "Ming", ""]]}, {"id": "1906.01764", "submitter": "Ting-Yao Hsu", "authors": "Ting-Yao Hsu, Chieh-Yang Huang, Yen-Chia Hsu, Ting-Hao 'Kenneth' Huang", "title": "Visual Story Post-Editing", "comments": "Accepted by ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first dataset for human edits of machine-generated visual\nstories and explore how these collected edits may be used for the visual story\npost-editing task. The dataset, VIST-Edit, includes 14,905 human edited\nversions of 2,981 machine-generated visual stories. The stories were generated\nby two state-of-the-art visual storytelling models, each aligned to 5\nhuman-edited versions. We establish baselines for the task, showing how a\nrelatively small set of human edits can be leveraged to boost the performance\nof large visual storytelling models. We also discuss the weak correlation\nbetween automatic evaluation scores and human ratings, motivating the need for\nnew automatic metrics.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 00:33:47 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Hsu", "Ting-Yao", ""], ["Huang", "Chieh-Yang", ""], ["Hsu", "Yen-Chia", ""], ["Huang", "Ting-Hao 'Kenneth'", ""]]}, {"id": "1906.01801", "submitter": "Yingying Jiang", "authors": "Min Chen, Yingying Jiang, Yong Cao, Albert Y. Zomaya", "title": "CreativeBioMan: Brain and Body Wearable Computing based Creative Gaming\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current artificial intelligence (AI) technology is mainly used in rational\nwork such as computation and logical analysis. How to make the machine as\naesthetic and creative as humans has gradually gained attention. This paper\npresents a creative game system (i.e., CreativeBioMan) for the first time. It\ncombines brain wave data and multimodal emotion data, and then uses an AI\nalgorithm for intelligent decision fusion, which can be used in artistic\ncreation, aiming at separating the artist from repeated labor creation. To\nimitate the process of humans' artistic creation, the creation process of the\nalgorithm is related to artists' previous artworks and their emotion. EEG data\nis used to analyze the style of artists and then match them with a style from a\ndata set of historical works. Then, universal AI algorithms are combined with\nthe unique creativity of each artist that evolve into a personalized creation\nalgorithm. According to the results of cloud emotion recognition, the color of\nthe artworks is corrected so that the artist's emotions are fully reflected in\nthe works, and thus novel works of art are created. This allows the machine to\nintegrate the understanding of past art and emotions with the ability to create\nnew art forms, in the same manner as humans. This paper introduces the system\narchitecture of CreativeBioMan from two aspects: data collection of the brain\nand body wearable devices, as well as the intelligent decision-making fusion of\nmodels. A Testbed platform is built for an experiment and the creativity of the\nworks produced by the system is analyzed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 03:15:57 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Chen", "Min", ""], ["Jiang", "Yingying", ""], ["Cao", "Yong", ""], ["Zomaya", "Albert Y.", ""]]}, {"id": "1906.02338", "submitter": "Thiago H. Silva", "authors": "Diego P. Tsutsumi, Amanda Fenerich, Thiago H. Silva", "title": "Towards Business Partnership Recommendation Using User Opinion on\n  Facebook", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of strategic business partnerships can potentially provide\ncompetitive advantages for businesses; however, due to the dynamics and\nuncertainty present in business environments, this task could be challenging.\nTo help businesses in this task, this study presents a similarity model between\nbusinesses that consider the opinions of users on content shared by businesses\non social media. Thus, this model captures significant virtual relationships\namong businesses that are generated by users in the virtual world. Besides, we\npropose an algorithm for detecting business communities in the considered\nmodel. We also propose an algorithm to identify possible business outliers in\nthe detected communities, which could represent an automatic way to identify\nnon-obvious relations that might deserve particular attention of business\nowners. By exploring approximately 280 million user reactions on Facebook, we\nshow that our results could favor the development of, for example, a new\nstrategic business partnership recommendation service.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 22:25:51 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Tsutsumi", "Diego P.", ""], ["Fenerich", "Amanda", ""], ["Silva", "Thiago H.", ""]]}, {"id": "1906.02399", "submitter": "Alireza Abedin Varamin", "authors": "Alireza Abedin, S. Hamid Rezatofighi, Qinfeng Shi, Damith C.\n  Ranasinghe", "title": "SparseSense: Human Activity Recognition from Highly Sparse Sensor\n  Data-streams Using Set-based Neural Networks", "comments": "Accepted at IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batteryless or so called passive wearables are providing new and innovative\nmethods for human activity recognition (HAR), especially in healthcare\napplications for older people. Passive sensors are low cost, lightweight,\nunobtrusive and desirably disposable; attractive attributes for healthcare\napplications in hospitals and nursing homes. Despite the compelling\npropositions for sensing applications, the data streams from these sensors are\ncharacterised by high sparsity---the time intervals between sensor readings are\nirregular while the number of readings per unit time are often limited. In this\npaper, we rigorously explore the problem of learning activity recognition\nmodels from temporally sparse data. We describe how to learn directly from\nsparse data using a deep learning paradigm in an end-to-end manner. We\ndemonstrate significant classification performance improvements on real-world\npassive sensor datasets from older people over the state-of-the-art deep\nlearning human activity recognition models. Further, we provide insights into\nthe model's behaviour through complementary experiments on a benchmark dataset\nand visualisation of the learned activity feature spaces.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 03:50:33 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Abedin", "Alireza", ""], ["Rezatofighi", "S. Hamid", ""], ["Shi", "Qinfeng", ""], ["Ranasinghe", "Damith C.", ""]]}, {"id": "1906.02485", "submitter": "Jonathan Grizou", "authors": "Jonathan Grizou", "title": "The Open Vault Challenge -- Learning how to build calibration-free\n  interactive systems by cracking the code of a vault", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This demo takes the form of a challenge to the IJCAI community. A physical\nvault, secured by a 4-digit code, will be placed in the demo area. The author\nwill publicly open the vault by entering the code on a touch-based interface,\nand as many times as requested. The challenge to the IJCAI participants will be\nto crack the code, open the vault, and collect its content. The interface is\nbased on previous work on calibration-free interactive systems that enables a\nuser to start instructing a machine without the machine knowing how to\ninterpret the user's actions beforehand. The intent and the behavior of the\nhuman are simultaneously learned by the machine. An online demo and videos are\navailable for readers to participate in the challenge. An additional interface\nusing vocal commands will be revealed on the demo day, demonstrating the\nscalability of our approach to continuous input signals.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 09:02:16 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Grizou", "Jonathan", ""]]}, {"id": "1906.02567", "submitter": "Tanju Sirmen R", "authors": "R.Tanju Sirmen, B. Burak Ustundag", "title": "An Information-Theoretical Approach to the Information Capacity and\n  Cost-Effectiveness Evaluation of Color Palettes", "comments": "9 pages", "journal-ref": "International Journal of Computing and Optimization, Vol.4, 2017,\n  No.1, pp. 43-51", "doi": "10.12988/ijco.2017.759", "report-no": null, "categories": "cs.HC cs.CV cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colors are used as effective tools of representing and transferring\ninformation. Number of colors in a palette is the direct arbiter of the\ninformation conveying capacity. Yet it should be well elaborated, since\nincreasing the entropy by adding colors comes with its cost on decoding.\nDespite the possible effects upon diverse applications, a methodology for\ncost-effectiveness evaluation of palettes seems deficient. In this work, this\nneed is being addressed from an information-theoretical perspective, via the\narticulated metrics and formulae. Besides, the proposed metrics are computed\nfor some developed and known palettes, and observed results are evaluated.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 13:38:01 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Sirmen", "R. Tanju", ""], ["Ustundag", "B. Burak", ""]]}, {"id": "1906.02569", "submitter": "Abubakar Abid", "authors": "Abubakar Abid, Ali Abdalla, Ali Abid, Dawood Khan, Abdulrahman\n  Alfozan, James Zou", "title": "Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild", "comments": "Presented at 2019 ICML Workshop on Human in the Loop Learning (HILL\n  2019), Long Beach, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessibility is a major challenge of machine learning (ML). Typical ML\nmodels are built by specialists and require specialized hardware/software as\nwell as ML experience to validate. This makes it challenging for non-technical\ncollaborators and endpoint users (e.g. physicians) to easily provide feedback\non model development and to gain trust in ML. The accessibility challenge also\nmakes collaboration more difficult and limits the ML researcher's exposure to\nrealistic data and scenarios that occur in the wild. To improve accessibility\nand facilitate collaboration, we developed an open-source Python package,\nGradio, which allows researchers to rapidly generate a visual interface for\ntheir ML models. Gradio makes accessing any ML model as easy as sharing a URL.\nOur development of Gradio is informed by interviews with a number of machine\nlearning researchers who participate in interdisciplinary collaborations. Their\nfeedback identified that Gradio should support a variety of interfaces and\nframeworks, allow for easy sharing of the interface, allow for input\nmanipulation and interactive inference by the domain expert, as well as allow\nembedding the interface in iPython notebooks. We developed these features and\ncarried out a case study to understand Gradio's usefulness and usability in the\nsetting of a machine learning collaboration between a researcher and a\ncardiologist.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 13:18:47 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Abid", "Abubakar", ""], ["Abdalla", "Ali", ""], ["Abid", "Ali", ""], ["Khan", "Dawood", ""], ["Alfozan", "Abdulrahman", ""], ["Zou", "James", ""]]}, {"id": "1906.02641", "submitter": "Matthew Rahtz", "authors": "Matthew Rahtz, James Fang, Anca D. Dragan and Dylan Hadfield-Menell", "title": "An Extensible Interactive Interface for Agent Design", "comments": "Presented at 2019 ICML Workshop on Human in the Loop Learning (HILL\n  2019), Long Beach, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In artificial intelligence, we often specify tasks through a reward function.\nWhile this works well in some settings, many tasks are hard to specify this\nway. In deep reinforcement learning, for example, directly specifying a reward\nas a function of a high-dimensional observation is challenging. Instead, we\npresent an interface for specifying tasks interactively using demonstrations.\nOur approach defines a set of increasingly complex policies. The interface\nallows the user to switch between these policies at fixed intervals to generate\ndemonstrations of novel, more complex, tasks. We train new policies based on\nthese demonstrations and repeat the process. We present a case study of our\napproach in the Lunar Lander domain, and show that this simple approach can\nquickly learn a successful landing policy and outperforms an existing\ncomparison-based deep RL method.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 15:18:40 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 11:06:46 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 11:58:45 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Rahtz", "Matthew", ""], ["Fang", "James", ""], ["Dragan", "Anca D.", ""], ["Hadfield-Menell", "Dylan", ""]]}, {"id": "1906.02836", "submitter": "R.Stuart Geiger", "authors": "R. Stuart Geiger, Yoon Jung Jeong, Emily Manders", "title": "Black-boxing the user: internet protocol over xylophone players (IPoXP)", "comments": null, "journal-ref": "In CHI 2012 Extended Abstracts on Human Factors in Computing\n  Systems (CHI EA 2012, alt.chi). ACM, New York, NY, USA, p. 71-80. DOI:\n  https://doi.org/10.1145/2212776.2212785", "doi": "10.1145/2212776.2212785", "report-no": null, "categories": "cs.HC cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce IP over Xylophone Players (IPoXP), a novel Internet protocol\nbetween two computers using xylophone-based Arduino interfaces. In our\nimplementation, human operators are situated within the lowest layer of the\nnetwork, transmitting data between computers by striking designated keys. We\ndiscuss how IPoXP inverts the traditional mode of human-computer interaction,\nwith a computer using the human as an interface to communicate with another\ncomputer.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 22:15:05 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Geiger", "R. Stuart", ""], ["Jeong", "Yoon Jung", ""], ["Manders", "Emily", ""]]}, {"id": "1906.02894", "submitter": "Zaghloul Saad Zaghloul", "authors": "Zaghloul Saad Zaghloul and Magdy Bayoumi", "title": "Early Prediction of Epilepsy Seizures VLSI BCI System", "comments": "Novel predictive BCI VLSI architecture", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling the surrounding world and predicting future events has always\nseemed like a dream, but that could become a reality using a\nBrain-Computer/Machine Interface (BCI/BMI). Epilepsy is a group of neurological\ndiseases characterized by epileptic seizures. It affects millions of people\nworldwide, with 80 percent of cases occurring in developing countries. This can\nresult in accidents and sudden, unexpected death. Seizures can happen\nundetectably in newborns, comatose, or motor-impaired patients, especially due\nto the fact that many medical personnel is not qualified for EEG signal\nanalysis. Therefore, a portable automated detection and monitoring solution is\nin high demand. Thus, in this study, a system of a wireless wearable adaptive\nfor early prediction of epilepsy seizures is proposed, works via minimally\ninvasive wireless technology paired with an external control device (e.g., a\ndoctors smartphone), with a higher than standard accuracy 71 percent and\nprediction time (14.56 sec). This novel architecture has not only opened new\nopportunities for daily usable BCI implementations, but they can also save a\nlife by helping to prevent a seizure fatal consequences\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 04:45:19 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Zaghloul", "Zaghloul Saad", ""], ["Bayoumi", "Magdy", ""]]}, {"id": "1906.03098", "submitter": "Oggi Rudovic", "authors": "Ognjen Rudovic, Meiru Zhang, Bjorn Schuller and Rosalind W. Picard", "title": "Multi-modal Active Learning From Human Data: A Deep Reinforcement\n  Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human behavior expression and experience are inherently multi-modal, and\ncharacterized by vast individual and contextual heterogeneity. To achieve\nmeaningful human-computer and human-robot interactions, multi-modal models of\nthe users states (e.g., engagement) are therefore needed. Most of the existing\nworks that try to build classifiers for the users states assume that the data\nto train the models are fully labeled. Nevertheless, data labeling is costly\nand tedious, and also prone to subjective interpretations by the human coders.\nThis is even more pronounced when the data are multi-modal (e.g., some users\nare more expressive with their facial expressions, some with their voice).\nThus, building models that can accurately estimate the users states during an\ninteraction is challenging. To tackle this, we propose a novel multi-modal\nactive learning (AL) approach that uses the notion of deep reinforcement\nlearning (RL) to find an optimal policy for active selection of the users data,\nneeded to train the target (modality-specific) models. We investigate different\nstrategies for multi-modal data fusion, and show that the proposed model-level\nfusion coupled with RL outperforms the feature-level and modality-specific\nmodels, and the naive AL strategies such as random sampling, and the standard\nheuristics such as uncertainty sampling. We show the benefits of this approach\non the task of engagement estimation from real-world child-robot interactions\nduring an autism therapy. Importantly, we show that the proposed multi-modal AL\napproach can be used to efficiently personalize the engagement classifiers to\nthe target user using a small amount of actively selected users data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 13:46:15 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Rudovic", "Ognjen", ""], ["Zhang", "Meiru", ""], ["Schuller", "Bjorn", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "1906.03168", "submitter": "Luz Rello", "authors": "Luz Rello, Ricardo Baeza-Yates, Abdullah Ali, Jeffrey P. Bigham,\n  Miquel Serra", "title": "Predicting risk of dyslexia with an online gamified test", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0241687", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyslexia is a specific learning disorder related to school failure. Detection\nis both crucial and challenging, especially in languages with transparent\northographies, such as Spanish. To make detecting dyslexia easier, we designed\nan online gamified test and a predictive machine learning model. In a study\nwith more than 3,600 participants, our model correctly detected over 80% of the\nparticipants with dyslexia. To check the robustness of the method we tested our\nmethod using a new data set with over 1,300 participants with age customized\ntests in a different environment -- a tablet instead of a desktop computer --\nreaching a recall of over 72% for the class with dyslexia for children 9 years\nold or older. Our work shows that dyslexia can be screened using a machine\nlearning approach. An online screening tool based on our methods has already\nbeen used by more than 200,000 people.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 15:37:14 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 11:17:15 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Rello", "Luz", ""], ["Baeza-Yates", "Ricardo", ""], ["Ali", "Abdullah", ""], ["Bigham", "Jeffrey P.", ""], ["Serra", "Miquel", ""]]}, {"id": "1906.03524", "submitter": "Charith Perera", "authors": "Luke Jones, Charith Perera", "title": "PizzaBox: Studying Internet Connected Physical Object Manipulation based\n  Food Ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the designing and testing of PizzaBox, a 3D printed,\ninteractive food ordering system that aims to differ from conventional food\nordering systems and provide an entertaining and unique experience when\nordering a pizza by incorporating underlying technologies that support\nubiquitous computing. The PizzaBox has gone through both low and medium\nfidelity testing while working collaboratively with participants to co-design\nand refine a product that is approachable to all age groups while maintaining a\nsimple process for ordering food from start to finish. Final testing was\nconducted at an independent pizzeria where interviews with participants lead us\nto develop four discussion themes 1) usability and end user engagement, 2)\ntowards connected real-time products and services, 3) healthy eating, 4)\nevolution of food ordering systems. Our interviews show that in general,\nPizzaBox would have a greater appeal to a younger audience by providing a\nfantasy of helping in the creation and baking of the pizza but also has a\nnovelty value that all ages would enjoy. We investigate the effect that the\nPizzaBox has in encouraging new healthy habits or promoting a healthier\nlifestyle as well as how we can improve PizzaBox to better encourage these\nlifestyle changes.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 21:28:24 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Jones", "Luke", ""], ["Perera", "Charith", ""]]}, {"id": "1906.03813", "submitter": "Michael McCourt", "authors": "Michael McCourt, Ian Dewancker", "title": "Sampling Humans for Optimizing Preferences in Coloring Artwork", "comments": "6 pages, 4 figures, presented at 2019 ICML Workshop on Human in the\n  Loop Learning (HILL 2019), Long Beach, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many circumstances of practical importance have performance or success\nmetrics which exist implicitly---in the eye of the beholder, so to speak.\nTuning aspects of such problems requires working without defined metrics and\nonly considering pairwise comparisons or rankings. In this paper, we review an\nexisting Bayesian optimization strategy for determining most-preferred\noutcomes, and identify an adaptation to allow it to handle ties. We then\ndiscuss some of the issues we have encountered when humans use this\noptimization strategy to optimize coloring a piece of abstract artwork. We hope\nthat, by participating in this workshop, we can learn how other researchers\nencounter difficulties unique to working with humans in the loop.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 06:46:41 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["McCourt", "Michael", ""], ["Dewancker", "Ian", ""]]}, {"id": "1906.03831", "submitter": "HaiLong Liu", "authors": "Hailong Liu, Toshihiro Hiraoka and Seiya Tanaka", "title": "Explicit behaviors affected by driver's trust in a driving automation\n  system", "comments": "6 pages, 9 figures, accepted by the 5th International Symposium on\n  Future Active Safety Technology toward Zero Accidents (FAST-zero-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As various driving automation system (DAS) are commonly used in the vehicle,\nthe over-trust in the DAS may put the driver in the risk. In order to prevent\nthe over-trust while driving, the trust state of the driver should be\nrecognized. However, description variables of the trust state are not distinct.\nThis paper assumed that the outward expressions of a driver can represent the\ntrust state of him/her-self. The explicit behaviors when driving with DAS is\nseen as those outward expressions. In the experiment, a driving simulator with\na driver monitoring system was used for simulating a vehicle with the adaptive\ncruise control (ACC) and observing the motion information of the driver.\nResults show that if the driver completely trusted in the ACC, then 1) the\nparticipants were likely to put their feet far away from the pedals; 2) the\noperational intervention of the driver will delay in dangerous situations. In\nthe future, a machine learning model will be tried to predict the trust state\nby using the motion information of the driver.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 08:04:46 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Liu", "Hailong", ""], ["Hiraoka", "Toshihiro", ""], ["Tanaka", "Seiya", ""]]}, {"id": "1906.04002", "submitter": "Long-Fei Chen", "authors": "Longfei Chen, Yuichi Nakamura, Kazuaki Kondo", "title": "Detecting Clues for Skill Levels and Machine Operation Difficulty from\n  Egocentric Vision", "comments": "Accepted for presentation at EPIC@CVPR2019 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With respect to machine operation tasks, the experiences from different skill\nlevel operators, especially novices, can provide worthy understanding about the\nmanner in which they perceive the operational environment and formulate\nknowledge to deal with various operation situations. In this study, we describe\nthe operator's behaviors by utilizing the relations among their head, hand, and\noperation location (hotspot) during the operation. A total of 40 experiences\nassociated with a sewing machine operation task performed by amateur operators\nwas recorded via a head-mounted RGB-D camera. We examined important features of\noperational behaviors in different skill level operators and confirmed their\ncorrelation to the difficulties of the operation steps. The result shows that\nthe pure-gazing behavior is significantly reduced when the operator's skill\nimproved. Moreover, the hand-approaching duration and the frequency of\nattention movement before operation are strongly correlated to the operational\ndifficulty in such machine operating environments.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 14:18:34 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Chen", "Longfei", ""], ["Nakamura", "Yuichi", ""], ["Kondo", "Kazuaki", ""]]}, {"id": "1906.04043", "submitter": "Sebastian Gehrmann", "authors": "Sebastian Gehrmann and Hendrik Strobelt and Alexander M. Rush", "title": "GLTR: Statistical Detection and Visualization of Generated Text", "comments": "ACL 2019 Demo Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid improvement of language models has raised the specter of abuse of\ntext generation systems. This progress motivates the development of simple\nmethods for detecting generated text that can be used by and explained to\nnon-experts. We develop GLTR, a tool to support humans in detecting whether a\ntext was generated by a model. GLTR applies a suite of baseline statistical\nmethods that can detect generation artifacts across common sampling schemes. In\na human-subjects study, we show that the annotation scheme provided by GLTR\nimproves the human detection-rate of fake text from 54% to 72% without any\nprior training. GLTR is open-source and publicly deployed, and has already been\nwidely used to detect generated outputs\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 14:52:41 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Gehrmann", "Sebastian", ""], ["Strobelt", "Hendrik", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1906.04176", "submitter": "Caleb Robinson", "authors": "Caleb Robinson, Anthony Ortiz, Kolya Malkin, Blake Elias, Andi Peng,\n  Dan Morris, Bistra Dilkina, Nebojsa Jojic", "title": "Human-Machine Collaboration for Fast Land Cover Mapping", "comments": "To appear in AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose incorporating human labelers in a model fine-tuning system that\nprovides immediate user feedback. In our framework, human labelers can\ninteractively query model predictions on unlabeled data, choose which data to\nlabel, and see the resulting effect on the model's predictions. This\nbi-directional feedback loop allows humans to learn how the model responds to\nnew data. Our hypothesis is that this rich feedback allows human labelers to\ncreate mental models that enable them to better choose which biases to\nintroduce to the model. We compare human-selected points to points selected\nusing standard active learning methods. We further investigate how the\nfine-tuning methodology impacts the human labelers' performance. We implement\nthis framework for fine-tuning high-resolution land cover segmentation models.\nSpecifically, we fine-tune a deep neural network -- trained to segment\nhigh-resolution aerial imagery into different land cover classes in Maryland,\nUSA -- to a new spatial area in New York, USA. The tight loop turns the\nalgorithm and the human operator into a hybrid system that can produce land\ncover maps of a large area much more efficiently than the traditional\nworkflows. Our framework has applications in geospatial machine learning\nsettings where there is a practically limitless supply of unlabeled data, of\nwhich only a small fraction can feasibly be labeled through human efforts.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:04:57 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 01:31:45 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2019 19:30:03 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Robinson", "Caleb", ""], ["Ortiz", "Anthony", ""], ["Malkin", "Kolya", ""], ["Elias", "Blake", ""], ["Peng", "Andi", ""], ["Morris", "Dan", ""], ["Dilkina", "Bistra", ""], ["Jojic", "Nebojsa", ""]]}, {"id": "1906.04251", "submitter": "Amr Jadi", "authors": "Amr Jadi", "title": "Improving the communication for children with speech disorder using the\n  smart toys", "comments": null, "journal-ref": null, "doi": "10.5121/ijaia.2019.10303", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An attempt is made to develop a smart toy to help the children suffering with\ncommunication disorders. The children suffering with such disorders need\nadditional attention and guidance to understand different types of social\nevents and life activities. Various issues and features of the children with\nspeech disorders are identified in this study and based on the inputs from the\nstudy, a working architecture is proposed with suitable policies. A prediction\nmodule with a checker component is designed in this work to produce alerts in\nat the time of abnormal behaviour of the child with communication disorder. The\nmodel is designed very sensitively to the behaviour of the child for a\nparticular voice tone, based on which the smart toy will change to tones\nautomatically. Such an arrangement proved to be helpful for the children to\nimprove the communication with other due to the inclusion of continuous\ntraining for the smart toy from the prediction module.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 09:30:33 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Jadi", "Amr", ""]]}, {"id": "1906.04435", "submitter": "Guenter Wallner", "authors": "G\\\"unter Wallner", "title": "Enhancing Battle Maps through Flow Graphs", "comments": "Accepted at IEEE Conference on Games 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So-called battle maps are an appropriate way to visually summarize the flow\nof battles as they happen in many team-based combat games. Such maps can be a\nvaluable tool for retrospective analysis of battles for the purpose of training\nor for providing a summary representation for spectators. In this paper an\nextension to the battle map algorithm previously proposed by the author and\nwhich addresses a shortcoming in the depiction of troop movements is described.\nThe extension does not require alteration of the original algorithm and can\neasily be added as an intermediate step before rendering. The extension is\nillustrated using gameplay data from the team-based multiplayer game World of\nTanks.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 08:13:15 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Wallner", "G\u00fcnter", ""]]}, {"id": "1906.04837", "submitter": "Kit Kuksenok", "authors": "Kit Kuksenok", "title": "Toward Best Practices for Explainable B2B Machine Learning", "comments": "4 pages, 1 figure; position paper for INTERACT 2019 workshop on\n  Humans in the Loop: Bridging AI and HCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To design tools and data pipelines for explainable B2B machine learning (ML)\nsystems, we need to recognize not only the immediate audience of such tools and\ndata, but also (1) their organizational context and (2) secondary audiences.\nOur learnings are based on building custom ML-based chatbots for recruitment.\nWe believe that in the B2B context, \"explainable\" ML means not only a system\nthat can \"explain itself\" through tools and data pipelines, but also enables\nits domain-expert users to explain it to other stakeholders.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 21:46:24 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Kuksenok", "Kit", ""]]}, {"id": "1906.05047", "submitter": "Souneil Park", "authors": "Chulhong Min, Euihyeok Lee, Souneil Park, Seungwoo Kang", "title": "Tiger:Wearable Glasses for the 20-20-20 Rule to Alleviate Computer\n  Vision Syndrome", "comments": "MobileHCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Tiger, an eyewear system for helping users follow the 20-20-20\nrule to alleviate the Computer Vision Syndrome symptoms. It monitors user's\nscreen viewing activities and provides real-time feedback to help users follow\nthe rule. For accurate screen viewing detection, we devise a light-weight\nmulti-sensory fusion approach with three sensing modalities, color, IMU, and\nlidar. We also design the real-time feedback to effectively lead users to\nfollow the rule. Our evaluation shows that Tiger accurately detects screen\nviewing events, and is robust to the differences in screen types, contents, and\nambient light. Our user study shows positive perception of Tiger regarding its\nusefulness, acceptance, and real-time feedback.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 10:23:33 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Min", "Chulhong", ""], ["Lee", "Euihyeok", ""], ["Park", "Souneil", ""], ["Kang", "Seungwoo", ""]]}, {"id": "1906.05094", "submitter": "Michael Green", "authors": "Michael Cerny Green, Christoph Salge and Julian Togelius", "title": "Organic Building Generation in Minecraft", "comments": "7 pages, 9 figures, published at PCG workshop at the Foundations of\n  Digital Games Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for generating floor plans for structures in\nMinecraft (Mojang 2009). Given a 3D space, it will auto-generate a building to\nfill that space using a combination of constrained growth and cellular\nautomata. The result is a series of organic-looking buildings complete with\nrooms, windows, and doors connecting them. The method is applied to the\nGenerative Design in Minecraft (GDMC) competition to auto-generate buildings in\nMinecraft, and the results are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:24:45 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Green", "Michael Cerny", ""], ["Salge", "Christoph", ""], ["Togelius", "Julian", ""]]}, {"id": "1906.05276", "submitter": "Evgeny Nikulchev", "authors": "Evgeny Nikulchev, Dmitry Ilin, Pavel Kolyasnikov, Ilya Zakharov,\n  Sergey Malykh", "title": "Programming Technologies for the Development of Web-Based Platform for\n  Digital Psychological Tools", "comments": "12 pages", "journal-ref": null, "doi": "10.14569/IJACSA.2018.090806", "report-no": null, "categories": "cs.HC cs.DC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The choice of the tools and programming technologies for information systems\ncreation is relevant. For every projected system, it is necessary to define a\nnumber of criteria for development environment, used libraries and\ntechnologies. The paper describes the choice of technological solutions using\nthe example of the developed web-based platform of the Russian Academy of\nEducation. This platform is used to provide information support for the\nactivities of psychologists in their research (including population and\nlongitudinal researches). There are following system features: large scale and\nsignificant amount of developing time that needs implementation and ensuring\nthe guaranteed computing reliability of a wide range of digital tools used in\npsychological research; ensuring functioning in different environments when\nconducting mass research in schools that have different characteristics of\ncomputing resources and communication channels; possibility of services\nscaling; security and privacy of data; use of technologies and programming\ntools that would ensure the compatibility and conversion of data with other\ntools of psychological research processing. Some criteria were introduced for\nthe developed system. These criteria take into account the feature of the\nfunctioning and life cycle of the software. A specific example shows the\nselection of appropriate technological solutions.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 13:14:37 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Nikulchev", "Evgeny", ""], ["Ilin", "Dmitry", ""], ["Kolyasnikov", "Pavel", ""], ["Zakharov", "Ilya", ""], ["Malykh", "Sergey", ""]]}, {"id": "1906.05498", "submitter": "Lasitha Piyathilaka Dr", "authors": "Lasitha Piyathilaka, Sarath Kodagoda", "title": "Understanding Human Context in 3D Scenes by Learning Spatial Affordances\n  with Virtual Skeleton Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots are often required to operate in environments where humans are not\npresent, but yet require the human context information for better human-robot\ninteraction. Even when humans are present in the environment, detecting their\npresence in cluttered environments could be challenging. As a solution to this\nproblem, this paper presents the concept of spatial affordance map which learns\nhuman context by looking at geometric features of the environment. Instead of\nobserving real humans to learn human context, it uses virtual human models and\ntheir relationships with the environment to map hidden human affordances in 3D\nscenes by placing virtual skeleton models in 3D scenes with their confidence\nvalues. The spatial affordance map learning problem is formulated as a\nmulti-label classification problem that can be learned using Support Vector\nMachine (SVM) based learners. Experiments carried out in a real 3D scene\ndataset recorded promising results and proved the applicability of\naffordance-map for mapping human context.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 06:21:14 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Piyathilaka", "Lasitha", ""], ["Kodagoda", "Sarath", ""]]}, {"id": "1906.05532", "submitter": "Adrien Coppens", "authors": "Adrien Coppens and Tom Mens and Mohamed-Anis Gallas", "title": "Parametric Modelling Within Immersive Environments: Building a Bridge\n  Between Existing Tools and Virtual Reality Headsets", "comments": "6 pages, 36th eCAADe conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though architectural modelling radically evolved over the course of its\nhistory, the current integration of Augmented Reality (AR) and Virtual\nReality(VR) components in the corresponding design tasks is mostly limited to\nenhancing visualisation. Little to none of these tools attempt to tackle the\nchallenge of modelling within immersive environments, that calls for new input\nmodalities in order to move away from the traditional mouse and keyboard\ncombination. In fact, relying on 2D devices for 3D manipulations does not seem\nto be effective as it does not offer the same degrees of freedom. We therefore\npresent a solution that brings VR modelling capabilities to Grasshopper, a\npopular parametric design tool. Together with its associated proof-of-concept\napplication, our extension offers a glimpse at new perspectives in that field.\nBy taking advantage of them,one can edit geometries with real-time feedback on\nthe generated models, without ever leaving the virtual environment. The\ndistinctive characteristics of VR applications provide a range of benefits\nwithout obstructing design activities. The designer can indeed experience the\narchitectural models at full scale from a realistic point-of-view and truly\nfeels immersed right next to them.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 07:58:48 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Coppens", "Adrien", ""], ["Mens", "Tom", ""], ["Gallas", "Mohamed-Anis", ""]]}, {"id": "1906.05710", "submitter": "Alec Jacobson", "authors": "Alec Jacobson", "title": "RodSteward: A Design-to-Assembly System for Fabrication using 3D-Printed\n  Joints and Precision-Cut Rods", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RodSteward, a design-to-assembly system for creating\nfurniture-scale structures composed of 3D printed joints and precision-cut\nrods. The RodSteward systems consists of: RSDesigner, a fabrication-aware\ndesign interface that visualizes accurate geometries during edits and\nidentifies infeasible designs; physical fabrication of parts via novel fully\nautomatic construction of solid 3D-printable joint geometries and automatically\ngenerated cutting plans for rods; and RSAssembler, a guided-assembly interface\nthat prompts the user to place parts in order while showing a focus+context\nvisualization of the assembly in progress. We demonstrate the effectiveness of\nour tools with a number of example constructions of varying complexity, style\nand parameter choices.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 14:14:32 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Jacobson", "Alec", ""]]}, {"id": "1906.05714", "submitter": "Jesse Vig", "authors": "Jesse Vig", "title": "A Multiscale Visualization of Attention in the Transformer Model", "comments": "To appear in ACL 2019 (System Demonstrations). arXiv admin note:\n  substantial text overlap with arXiv:1904.02679", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformer is a sequence model that forgoes traditional recurrent\narchitectures in favor of a fully attention-based approach. Besides improving\nperformance, an advantage of using attention is that it can also help to\ninterpret a model by showing how the model assigns weight to different input\nelements. However, the multi-layer, multi-head attention mechanism in the\nTransformer model can be difficult to decipher. To make the model more\naccessible, we introduce an open-source tool that visualizes attention at\nmultiple scales, each of which provides a unique perspective on the attention\nmechanism. We demonstrate the tool on BERT and OpenAI GPT-2 and present three\nexample use cases: detecting model bias, locating relevant attention heads, and\nlinking neurons to model behavior.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 15:45:26 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Vig", "Jesse", ""]]}, {"id": "1906.05925", "submitter": "Kevin VanHorn", "authors": "Kevin C. VanHorn, Meyer Zinn, Murat Can Cobanoglu", "title": "Deep Learning Development Environment in Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) offers immersive visualization and intuitive\ninteraction. We leverage VR to enable any biomedical professional to deploy a\ndeep learning (DL) model for image classification. While DL models can be\npowerful tools for data analysis, they are also challenging to understand and\ndevelop. To make deep learning more accessible and intuitive, we have built a\nvirtual reality-based DL development environment. Within our environment, the\nuser can move tangible objects to construct a neural network only using their\nhands. Our software automatically translates these configurations into a\ntrainable model and then reports its resulting accuracy on a test dataset in\nreal-time. Furthermore, we have enriched the virtual objects with\nvisualizations of the model's components such that users can achieve insight\nabout the DL models that they are developing. With this approach, we bridge the\ngap between professionals in different fields of expertise while offering a\nnovel perspective for model analysis and data interaction. We further suggest\nthat techniques of development and visualization in deep learning can benefit\nby integrating virtual reality.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 20:53:33 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["VanHorn", "Kevin C.", ""], ["Zinn", "Meyer", ""], ["Cobanoglu", "Murat Can", ""]]}, {"id": "1906.05996", "submitter": "Felice De Luca", "authors": "Felice De Luca, Iqbal Hossain, Kathryn Gray, Stephen Kobourov, Katy\n  B\\\"orner", "title": "Multi-level tree based approach for interactive graph visualization with\n  semantic zoom", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human subject studies that map-like visualizations are as good or better than\nstandard node-link representations of graphs, in terms of task performance,\nmemorization and recall of the underlying data, and engagement [SSKB14,\nSSKB15]. With this in mind, we propose the Zoomable Multi-Level Tree (ZMLT)\nalgorithm for multi-level tree-based, map-like visualization of large graphs.\nWe propose seven desirable properties that such visualization should maintain\nand an algorithm that accomplishes them. (1) The abstract trees represent the\nunderlying graph appropriately at different level of details; (2) The embedded\ntrees represent the underlying graph appropriately at different levels of\ndetails; (3) At every level of detail we show real vertices and real paths from\nthe underlying graph; (4) If any node or edge appears in a given level, then\nthey also appear in all deeper levels; (5) All nodes at the current level and\nhigher levels are labeled and there are no label overlaps; (6) There are no\nedge crossings on any level; (7) The drawing area is proportional to the total\narea of the labels. This algorithm is implemented and we have a functional\nprototype for the interactive interface in a web browser.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 02:58:17 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 18:05:02 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["De Luca", "Felice", ""], ["Hossain", "Iqbal", ""], ["Gray", "Kathryn", ""], ["Kobourov", "Stephen", ""], ["B\u00f6rner", "Katy", ""]]}, {"id": "1906.06089", "submitter": "Joel Fischer", "authors": "Carolina Fuentes, Martin Porcheron, Joel Fischer, Nervo Verdezoto,\n  Oren Zuckerman, Enrico Constanza, Valeria Herskovic and Leila Takayama", "title": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care", "comments": "Participant contributions to the CHI 2019 Workshop on New Directions\n  for the IoT: Automate, Share, Build, and Care, held at the CHI 2019\n  conference on Human Factors in Computing Systems, Glasgow, Scotland, 5 May\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This volume represents the proceedings of the CHI 2019 Workshop on New\nDirections for the IoT: Automate, Share, Build, and Care.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 09:27:25 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Fuentes", "Carolina", ""], ["Porcheron", "Martin", ""], ["Fischer", "Joel", ""], ["Verdezoto", "Nervo", ""], ["Zuckerman", "Oren", ""], ["Constanza", "Enrico", ""], ["Herskovic", "Valeria", ""], ["Takayama", "Leila", ""]]}, {"id": "1906.06428", "submitter": "Zhengshan Shi", "authors": "Zhengshan Shi, Carlos Cancino-Chac\\'on, Gerhard Widmer", "title": "User Curated Shaping of Expressive Performances", "comments": "4 pages, ICML 2019 Machine Learning for Music Discovery Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Musicians produce individualized, expressive performances by manipulating\nparameters such as dynamics, tempo and articulation. This manipulation of\nexpressive parameters is informed by elements of score information such as\npitch, meter, and tempo and dynamics markings (among others). In this paper we\npresent an interactive interface that gives users the opportunity to explore\nthe relationship between structural elements of a score and expressive\nparameters. This interface draws on the basis function models, a data-driven\nframework for expressive performance. In this framework, expressive parameters\nare modeled as a function of score features, i.e., numerical encodings of\nspecific aspects of a musical score, using neural networks. With the proposed\ninterface, users are able to weight the contribution of individual score\nfeatures and understand how an expressive performance is constructed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 22:59:23 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Shi", "Zhengshan", ""], ["Cancino-Chac\u00f3n", "Carlos", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1906.06545", "submitter": "Ronaldo Araujo", "authors": "Gustavo Caran, Ronaldo Araujo, Crispulo Travieso-Rodriguez", "title": "Interactive health communication and the construction of the identity of\n  the person with low vision in social media", "comments": "This is a post-peer-review, pre-copyedit spanish version of the paper\n  accepted at IX Encuentro Ib\\'erico EDICIC, Barcelone, Spain, July 9th to\n  11th, 2019. 16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With use and appropriation of digital environments by the Green Cane movement\nin Brazil, the informational character in health is a central discursive\nresource, and occurs imbued with (self) conceptions about the characteristic\nways of thinking, feeling and acting of the person with low vision. This work\naims at exploring the discourse of the person with low vision in the social\nmedia about their own social identity. Based on the Interactive Health\nCommunication, videos on Youtube and Facebook and Instagram posts of the\nVirtual Group Stargardt, a virtual community made up of people with low vision\nand suffering from Stargardt's Disease, were investigated. From the selection\nof excerpts and their coding, 42 identity traits were identified and grouped\ninto a synthetic model with 08 categories: (T1) we are low vision and we use\nthe green cane; (T2) we have sensitivity to light and we wear sunglasses; (T3)\nwe have difficulties in the day-to-day, but we use assistive strategies and\ntechnologies; (T4) we are a PcD (Person with Disability) and we have rights\nguaranteed by law; (T5) we seek to understand about our disease and we are the\nspokesperson of it; (T6) we have a visual impairment, but we are not just that;\n(T7) we live with conflicts and contradictions by the way we see things, and;\n(T8) we recognize as a community. Such traits are constructed interactively,\nthrough a construction based on moments of agreement, disagreement and good\nhumor. The discourses are oriented in the consolidation of a proper identity,\ndistinct from the person with blindness, but similar in the legal framework of\nthe visually impaired person.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 12:38:29 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Caran", "Gustavo", ""], ["Araujo", "Ronaldo", ""], ["Travieso-Rodriguez", "Crispulo", ""]]}, {"id": "1906.07183", "submitter": "Gavindya Jayawardena", "authors": "Gavindya Jayawardena, Anne Michalek, Sampath Jayarathna", "title": "Eye Gaze Metrics and Analysis of AOI for Indexing Working Memory towards\n  Predicting ADHD", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ADHD is being recognized as a diagnosis which persists into adulthood\nimpacting economic, occupational, and educational outcomes. There is an\nincreased need to accurately diagnose and recommend interventions for this\npopulation. One consideration is the development and implementation of reliable\nand valid outcome measures which reflect core diagnostic criteria. For example,\nadults with ADHD have reduced working memory capacity when compared to their\npeers (Michalek et al., 2014). A reduction in working memory capacity indicates\nattentional control deficits which align with many symptoms outlined on\nbehavioral checklists used to diagnose ADHD. Using computational methods, such\nas eye tracking technology, to generate a relationship between ADHD and\nmeasures of working memory capacity would be useful to advancing our\nunderstanding and treatment of the diagnosis in adults. This chapter will\noutline a feasibility study in which eye tracking was used to measure eye gaze\nmetrics during a working memory capacity task for adults with and without ADHD\nand machine learning algorithms were applied to generate a feature set unique\nto the ADHD diagnosis. The chapter will summarize the purpose, methods,\nresults, and impact of this study.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 16:24:28 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Jayawardena", "Gavindya", ""], ["Michalek", "Anne", ""], ["Jayarathna", "Sampath", ""]]}, {"id": "1906.07254", "submitter": "Ramya Srinivasan", "authors": "Ramya Srinivasan and Ajay Chander", "title": "Crowdsourcing in the Absence of Ground Truth -- A Case Study", "comments": "presented at 2019 ICML Workshop on Human in the Loop Learning (HILL\n  2019), Long Beach, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing information constitutes an important aspect of\nhuman-in-the-loop learning for researchers across multiple disciplines such as\nAI, HCI, and social science. While using crowdsourced data for subjective tasks\nis not new, eliciting useful insights from such data remains challenging due to\na variety of factors such as difficulty of the task, personal prejudices of the\nhuman evaluators, lack of question clarity, etc. In this paper, we consider one\nsuch subjective evaluation task, namely that of estimating experienced emotions\nof distressed individuals who are conversing with a human listener in an online\ncoaching platform. We explore strategies to aggregate the evaluators choices,\nand show that a simple voting consensus is as effective as an optimum\naggregation method for the task considered. Intrigued by how an objective\nassessment would compare to the subjective evaluation of evaluators, we also\ndesigned a machine learning algorithm to perform the same task. Interestingly,\nwe observed a machine learning algorithm that is not explicitly modeled to\ncharacterize evaluators' subjectivity is as reliable as the human evaluation in\nterms of assessing the most dominant experienced emotions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 20:30:54 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Srinivasan", "Ramya", ""], ["Chander", "Ajay", ""]]}, {"id": "1906.07377", "submitter": "Alexander Rind", "authors": "Manuel Dahnert (1), Alexander Rind (2), Wolfgang Aigner (2), and\n  Johannes Kehrer (1 and 3) ((1) Technical University of Munich, Germany, (2)\n  St. Poelten University of Applied Sciences, Austria, (3) Siemens AG,\n  Corporate Technology, Germany)", "title": "Looking beyond the horizon: Evaluation of four compact visualization\n  techniques for time series in a spatial context", "comments": "12 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizing time series in a dense spatial context such as a geographical map\nis a challenging task, which requires careful balance between the amount of\ndepicted data and perceptual precision. Horizon graphs are a well-known\ntechnique for compactly representing time series data. They provide fine\ndetails while simultaneously giving an overview of the data where extrema are\nemphasized. Horizon graphs compress the vertical resolution of the individual\nline graphs, but they do not affect the horizontal resolution. We present two\nvariations of a new visualization technique called collapsed horizon graphs\nwhich extend the idea of horizon graphs to two dimensions. Our main\ncontribution is a quantitative evaluation that experimentally compares four\nvisualization techniques with high visual information resolution (compact\nboxplots, horizon graphs, collapsed horizon graphs, and braided collapsed\nhorizon graphs). The experiment investigates the performance of these\ntechniques across tasks addressing both individual graphs as well as groups of\nadjacent graphs. Compact boxplots consistently provide good results for all\ntasks, horizon graphs excel, for instance, in maximum tasks but underperform in\ntrend detection. Collapsed horizon graphs shine in certain tasks in which an\nincreased horizontal resolution is beneficial. Moreover, our results indicate\nthat the visual complexity of the techniques highly affects users' confidence\nand perceived task difficulty.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 04:48:44 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Dahnert", "Manuel", "", "1 and 3"], ["Rind", "Alexander", "", "1 and 3"], ["Aigner", "Wolfgang", "", "1 and 3"], ["Kehrer", "Johannes", "", "1 and 3"]]}, {"id": "1906.07617", "submitter": "David Gotz", "authors": "David Gotz, Jonathan Zhang, Wenyuan Wang, Joshua Shrestha, David\n  Borland", "title": "Visual Analysis of High-Dimensional Event Sequence Data via Dynamic\n  Hierarchical Aggregation", "comments": "To Appear in IEEE Transactions on Visualization and Computer Graphics\n  (TVCG), Volume 26 Issue 1, 2020. Also part of proceedings for IEEE VAST 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934661", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal event data are collected across a broad range of domains, and a\nvariety of visual analytics techniques have been developed to empower analysts\nworking with this form of data. These techniques generally display aggregate\nstatistics computed over sets of event sequences that share common patterns.\nSuch techniques are often hindered, however, by the high-dimensionality of many\nreal-world event sequence datasets because the large number of distinct event\ntypes within such data prevents effective aggregation. A common coping strategy\nfor this challenge is to group event types together as a pre-process, prior to\nvisualization, so that each group can be represented within an analysis as a\nsingle event type. However, computing these event groupings as a pre-process\nalso places significant constraints on the analysis. This paper presents a\ndynamic hierarchical aggregation technique that leverages a predefined\nhierarchy of dimensions to computationally quantify the informativeness of\nalternative levels of grouping within the hierarchy at runtime. This allows\nusers to dynamically explore the hierarchy to select the most appropriate level\nof grouping to use at any individual step within an analysis. Key contributions\ninclude an algorithm for interactively determining the most informative set of\nevent groupings from within a large-scale hierarchy of event types, and a\nscatter-plus-focus visualization that supports interactive hierarchical\nexploration. While these contributions are generalizable to other types of\nproblems, we apply them to high-dimensional event sequence analysis using\nlarge-scale event type hierarchies from the medical domain. We describe their\nuse within a medical cohort analysis tool called Cadence, demonstrate an\nexample in which the proposed technique supports better views of event sequence\ndata, and report findings from domain expert interviews.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 14:44:58 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 14:16:52 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Gotz", "David", ""], ["Zhang", "Jonathan", ""], ["Wang", "Wenyuan", ""], ["Shrestha", "Joshua", ""], ["Borland", "David", ""]]}, {"id": "1906.07625", "submitter": "David Gotz", "authors": "David Borland, Wenyuan Wang, Jonathan Zhang, Joshua Shrestha, David\n  Gotz", "title": "Selection Bias Tracking and Detailed Subset Comparison for\n  High-Dimensional Data", "comments": "IEEE Transactions on Visualization and Computer Graphics (TVCG),\n  Volume 26 Issue 1, 2020. Also part of proceedings for IEEE VAST 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934209", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection of large, complex datasets has become common across a wide\nvariety of domains. Visual analytics tools increasingly play a key role in\nexploring and answering complex questions about these large datasets. However,\nmany visualizations are not designed to concurrently visualize the large number\nof dimensions present in complex datasets (e.g. tens of thousands of distinct\ncodes in an electronic health record system). This fact, combined with the\nability of many visual analytics systems to enable rapid, ad-hoc specification\nof groups, or cohorts, of individuals based on a small subset of visualized\ndimensions, leads to the possibility of introducing selection bias--when the\nuser creates a cohort based on a specified set of dimensions, differences\nacross many other unseen dimensions may also be introduced. These unintended\nside effects may result in the cohort no longer being representative of the\nlarger population intended to be studied, which can negatively affect the\nvalidity of subsequent analyses. We present techniques for selection bias\ntracking and visualization that can be incorporated into high-dimensional\nexploratory visual analytics systems, with a focus on medical data with\nexisting data hierarchies. These techniques include: (1) tree-based cohort\nprovenance and visualization, with a user-specified baseline cohort that all\nother cohorts are compared against, and visual encoding of the drift for each\ncohort, which indicates where selection bias may have occurred, and (2) a set\nof visualizations, including a novel icicle-plot based visualization, to\ncompare in detail the per-dimension differences between the baseline and a\nuser-specified focus cohort. These techniques are integrated into a medical\ntemporal event sequence visual analytics tool. We present example use cases and\nreport findings from domain expert user interviews.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 14:58:52 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 18:15:00 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 19:42:14 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Borland", "David", ""], ["Wang", "Wenyuan", ""], ["Zhang", "Jonathan", ""], ["Shrestha", "Joshua", ""], ["Gotz", "David", ""]]}, {"id": "1906.07637", "submitter": "David Gotz", "authors": "Bryce Morrow, Trevor Manz, Arlene E. Chung, Nils Gehlenborg, David\n  Gotz", "title": "Periphery Plots for Contextualizing Heterogeneous Time-Based Charts", "comments": "To Appear in IEEE VIS 2019 Short Papers. Open source software and\n  other materials available on github:\n  https://github.com/PrecisionVISSTA/PeripheryPlots Video figure available on\n  Vimeo: https://vimeo.com/349678146", "journal-ref": null, "doi": "10.1109/VISUAL.2019.8933582", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patterns in temporal data can often be found across different scales, such as\ndays, weeks, and months, making effective visualization of time-based data\nchallenging. Here we propose a new approach for providing focus and context in\ntime-based charts to enable interpretation of patterns across time scales. Our\napproach employs a focus zone with a time and a second axis, that can either\nrepresent quantities or categories, as well as a set of adjacent periphery\nplots that can aggregate data along the time, value, or both dimensions. We\npresent a framework for periphery plots and describe two use cases that\ndemonstrate the utility of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 15:17:52 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 15:19:02 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Morrow", "Bryce", ""], ["Manz", "Trevor", ""], ["Chung", "Arlene E.", ""], ["Gehlenborg", "Nils", ""], ["Gotz", "David", ""]]}, {"id": "1906.07669", "submitter": "Manuel Aiple", "authors": "Manuel Aiple, Wouter Gregoor and Andre Schiele", "title": "A Dynamic Robotic Actuator with Variable Physical Stiffness and Damping", "comments": "21 pages, 15 figures, submitted to Mechanism and Machine Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is part of research aiming at increasing the range of dynamic\ntasks for teleoperated field robotics in order to allow operators to use the\nfull range of human motions without being limited by the dynamics of the\nrobotic manipulator. A new variable impedance actuator (VIA) was designed,\ncapable of reproducing motions through teleoperation from precise positioning\ntasks to highly dynamic tasks. The design requirements based on previous human\nuser studies were a stiffness changing time of 50 ms, a peak output velocity of\n20 rad/s and variable damping allowing to suppress undesired oscillations. This\nis a unique combination of features that was not met by other VIAs. The new\ndesign has three motors in parallel configuration: two responsible for changing\nthe VIA's neutral position and effective stiffness through a sliding pivot\npoint lever mechanism, and the third acting as variable damper. A prototype was\nbuilt and its performance measured with an effective stiffness changing time of\n50 to 120 ms for small to large stiffness steps, nominal output velocity of 16\nrad/s and a variable damper with a damping torque from 0 to 3 Nm. Its effective\nstiffness range is 0.2 to 313 Nm/rad. This concludes that the new actuator is\nparticularly suitable for highly dynamic tasks. At the same time, the new\nactuator is also very versatile, making it especially interesting for\nteleoperation and human-robot collaboration.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 16:11:32 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 16:40:02 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Aiple", "Manuel", ""], ["Gregoor", "Wouter", ""], ["Schiele", "Andre", ""]]}, {"id": "1906.07716", "submitter": "Daniel Karl I. Weidele", "authors": "Daniel Karl I. Weidele", "title": "Conditional Parallel Coordinates", "comments": "5 pages, 8 figures, VIS 2019 Short Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel Coordinates are a popular data visualization technique for\nmultivariate data. Dating back to as early as 1880 PC are nearly as old as John\nSnow's famous cholera outbreak map of 1855, which is frequently regarded as a\nhistoric landmark for modern data visualization. Numerous extensions have been\nproposed to address integrity, scalability and readability. We make a new case\nto employ PC on conditional data, where additional dimensions are only unfolded\nif certain criteria are met in an observation. Compared to standard PC which\noperate on a flat set of dimensions the ontology of our input to Conditional\nParallel Coordinates is of hierarchical nature. We therefore briefly review\nrelated work around hierarchical PC using aggregation or nesting techniques.\nOur contribution is a visualization to seamlessly adapt PC for conditional data\nunder preservation of intuitive interaction patterns to select or highlight\npolylines. We conclude with intuitions on how to operate CPC on two data sets:\nan AutoML hyperparameter search log, and session results from a conversational\nagent.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 17:58:49 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 13:43:13 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Weidele", "Daniel Karl I.", ""]]}, {"id": "1906.07720", "submitter": "Dmitri Presnov", "authors": "Dmitri Presnov, Julia Kurz, Judith Willkomm, Daniel Alt, Johannes\n  Dillmann, Robert Zilke, Veit Braun, Cornelius Schubert, and Andreas Kolb", "title": "On-Body Visualization of Patient Data for Cooperative Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR) systematically represent patient data in\ndigital form. However, text and visualization based EHR systems are poorly\nintegrated in the hospital workflow due to their complex and rather\nnon-intuitive access structure. This is especially disadvantageous in clinical\ncooperative situations that require an efficient, task specific information\ntransfer.\n  In this paper we introduce a novel concept of anatomically integrated\nin-place visualization designed to engage with cooperative tasks on a\nneurosurgical ward. Based on the findings of our field studies and the derived\ndesign goals, we propose an approach that follows a visual tradition in\nmedicine, which is tightly related with anatomy, by using a virtual patient's\nbody as spatial representation of visually encoded abstract medical data. More\nspecifically, we provide a generic set of formal requirements for these kinds\nof in-place visualizations, we apply these requirements in order to achieve a\nspecific visualization of neurological symptoms related to the differential\ndiagnosis of spinal disc herniation, and we present a prototypical\nimplementation of the visualization concept on a mobile device. Moreover, we\ndiscuss various challenges related to visual encoding and visibility of the\nbody model components. Finally, the prototype is evaluated by 10 neurosurgeons,\nwho assess the validity and the further potential of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 15:41:16 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 16:09:08 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Presnov", "Dmitri", ""], ["Kurz", "Julia", ""], ["Willkomm", "Judith", ""], ["Alt", "Daniel", ""], ["Dillmann", "Johannes", ""], ["Zilke", "Robert", ""], ["Braun", "Veit", ""], ["Schubert", "Cornelius", ""], ["Kolb", "Andreas", ""]]}, {"id": "1906.07837", "submitter": "Ran Xu", "authors": "Ran Xu, Manu Mathew Thomas, Alex Leow, Olusola Ajilore, Angus G.\n  Forbes", "title": "TempoCave: Visualizing Dynamic Connectome Datasets to Support Cognitive\n  Behavioral Therapy", "comments": null, "journal-ref": null, "doi": "10.1109/VISUAL.2019.8933544", "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce TempoCave, a novel visualization application for analyzing\ndynamic brain networks, or connectomes. TempoCave provides a range of\nfunctionality to explore metrics related to the activity patterns and modular\naffiliations of different regions in the brain. These patterns are calculated\nby processing raw data retrieved functional magnetic resonance imaging (fMRI)\nscans, which creates a network of weighted edges between each brain region,\nwhere the weight indicates how likely these regions are to activate\nsynchronously. In particular, we support the analysis needs of clinical\npsychologists, who examine these modular affiliations and weighted edges and\ntheir temporal dynamics, utilizing them to understand relationships between\nneurological disorders and brain activity, which could have a significant\nimpact on the way in which patients are diagnosed and treated. We summarize the\ncore functionality of TempoCave, which supports a range of comparative tasks,\nand runs both in a desktop mode and in an immersive mode. Furthermore, we\npresent a real-world use case that analyzes pre- and post-treatment connectome\ndatasets from 27 subjects in a clinical study investigating the use of\ncognitive behavior therapy to treat major depression disorder, indicating that\nTempoCave can provide new insight into the dynamic behavior of the human brain.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 22:49:42 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 22:01:04 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Xu", "Ran", ""], ["Thomas", "Manu Mathew", ""], ["Leow", "Alex", ""], ["Ajilore", "Olusola", ""], ["Forbes", "Angus G.", ""]]}, {"id": "1906.07864", "submitter": "Wei Shao Dr", "authors": "Nan Gao, Wei Shao, Flora D Salim", "title": "Predicting Personality Traits from Physical Activity Intensity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Call and messaging logs from mobile devices have been used to predict human\npersonality traits successfully in recent years. However, the widely available\naccelerometer data is not yet utilized for this purpose. In this research, we\nexplored some important features describing human physical activity intensity,\nused for the very first time to predict human personality traits through raw\naccelerometer data. Using a set of newly introduced metrics, we combined\nphysical activity intensity features with traditional phone activity features\nfor personality prediction. The experiment results show that the predicted\npersonality scores are closer to the ground truth, with observable reduction of\nerrors in predicting the Big-5 personality traits across male and female.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 01:01:31 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Gao", "Nan", ""], ["Shao", "Wei", ""], ["Salim", "Flora D", ""]]}, {"id": "1906.08032", "submitter": "Ganesh Gowrishankar", "authors": "Kuniharu Sakurada, Gowrishankar Ganesh (IDH), Wenwei Yu, Kahori Kita", "title": "Accurate decoding of materials using a finger mounted accelerometer", "comments": null, "journal-ref": "IEEE International Conference on Robotics and Biomimetics (ROBIO\n  2018), Dec 2018, Kualalampur, Malaysia", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory feedback is the fundamental driving force behind motor control and\nlearning. However, the technology for low-cost and efficient sensory feedback\nremains a big challenge during stroke rehabilitation, and for prosthetic\ndesigns. Here we show that a low-cost accelerometer mounted on the finger can\nprovide accurate decoding of many daily life materials during touch. We first\ndesigned a customized touch analysis system that allowed us to present\ndifferent materials for touch by human participants, while controlling for the\ncontact force and touch speed. Then, we collected data from six participants,\nwho touched seven daily life materials-plastic, cork, wool, aluminum, paper,\ndenim, cotton. We use linear sparse logistic regression and show that the\nmaterials can be classified from accelerometer recordings with an accuracy of\n88% across materials and participants within 7 seconds of touch.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 12:04:25 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Sakurada", "Kuniharu", "", "IDH"], ["Ganesh", "Gowrishankar", "", "IDH"], ["Yu", "Wenwei", ""], ["Kita", "Kahori", ""]]}, {"id": "1906.08562", "submitter": "Elad Yom-Tov", "authors": "Gilie Gefen, Omer Ben-Porat, Moshe Tennenholtz, Elad Yom-Tov", "title": "Privacy, Altruism, and Experience: Estimating the Perceived Value of\n  Internet Data for Medical Uses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People increasingly turn to the Internet when they have a medical condition.\nThe data they create during this process is a valuable source for medical\nresearch and for future health services. However, utilizing these data could\ncome at a cost to user privacy. Thus, it is important to balance the perceived\nvalue that users assign to these data with the value of the services derived\nfrom them. Here we describe experiments where methods from Mechanism Design\nwere used to elicit a truthful valuation from users for their Internet data and\nfor services to screen people for medical conditions. In these experiments, 880\npeople from around the world were asked to participate in an auction to provide\ntheir data for uses differing in their contribution to the participant, to\nsociety, and in the disease they addressed. Some users were offered monetary\ncompensation for their participation, while others were asked to pay to\nparticipate. Our findings show that 99\\% of people were willing to contribute\ntheir data in exchange for monetary compensation and an analysis of their data,\nwhile 53\\% were willing to pay to have their data analyzed. The average\nperceived value users assigned to their data was estimated at US\\$49. Their\nvalue to screen them for a specific cancer was US\\$22 while the value of this\nservice offered to the general public was US\\$22. Participants requested higher\ncompensation when notified that their data would be used to analyze a more\nsevere condition. They were willing to pay more to have their data analyzed\nwhen the condition was more severe, when they had higher education or if they\nhad recently experienced a serious medical condition.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 11:20:40 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 20:09:48 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2020 07:18:46 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Gefen", "Gilie", ""], ["Ben-Porat", "Omer", ""], ["Tennenholtz", "Moshe", ""], ["Yom-Tov", "Elad", ""]]}, {"id": "1906.08591", "submitter": "Chong Liu", "authors": "Chong Liu and Yu-Xiang Wang", "title": "Doubly Robust Crowdsourcing", "comments": "presented at 2019 ICML Workshop on Human in the Loop Learning (HILL\n  2019), Long Beach, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale labeled datasets are the indispensable fuel that ignites the AI\nrevolution as we see today. Most such datasets are constructed using\ncrowdsourcing services such as Amazon Mechanical Turk which provides noisy\nlabels from non-experts at a fair price. The sheer size of such datasets\nmandates that it is only feasible to collect a few labels per data point. We\nformulate the problem of test-time label aggregation as a statistical\nestimation problem of inferring the expected voting score in an ideal world\nwhere all workers label all items. By imitating workers with supervised\nlearners and using them in a doubly robust estimation framework, we prove that\nthe variance of estimation can be substantially reduced, even if the learner is\na poor approximation. Synthetic and real-world experiments show that by\ncombining the doubly robust approach with adaptive worker/item selection, we\noften need as low as 0.1 labels per data point to achieve nearly the same\naccuracy as in the ideal world where all workers label all data points.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2019 18:12:40 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Liu", "Chong", ""], ["Wang", "Yu-Xiang", ""]]}, {"id": "1906.08670", "submitter": "Sadra Fardhosseini", "authors": "Mahmoud Habibnezhad, Jay Puckett, Mohammad Sadra Fardhosseini, Lucky\n  Agung Pratama", "title": "A Mixed VR and Physical Framework to Evaluate Impacts of Virtual Legs\n  and Elevated Narrow Working Space on Construction Workers Gait Pattern", "comments": "36th International Symposium on Automation and Robotics in\n  Construction (ISARC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is difficult to conduct training and evaluate workers' postural\nperformance by using the actual job site environment due to safety concerns.\nVirtual reality (VR) provides an alternative to create immersive working\nenvironments without significant safety concerns. Working on elevated surfaces\nis a dangerous scenario, which may lead to gait and postural instability and,\nconsequently, a serious fall. Previous studies showed that VR is a promising\ntool for measuring the impact of height on the postural sway. However, most of\nthese studies used the treadmill as the walking locomotion apparatus in a\nvirtual environment (VE). This paper was focused on natural walking locomotion\nto reduce the inherent postural perturbations of VR devices. To investigate the\nimpact of virtual height on gait characteristics and keep the level of realism\nand feeling of presence at their highest, we enhanced the\nfirst-person-character model with \"virtual legs\". Afterward, we investigated\nits effect on the gait parameters of the participants with and without the\npresence of height. To that end, twelve healthy adults were asked to walk on a\nvirtual loop path once at the ground level and once at the 17th floor of an\nunfinished structure. By quantitatively comparing the participants' gait\npattern results, we observed a decrease in the stride length and increase in\nthe gait duration of the participants exposed to height. At the ground level,\nthe use of the enhanced model reduced participants' average stride length and\nheight. The results of this study help us understand users' behaviors when they\nwere exposed to elevated surfaces and establish a firm ground for gait\nstability analysis for the future height-related VR studies. We expect this\ndeveloped VR platform can generate reliable results of VR application in more\nconstruction safety studies.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 14:47:49 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Habibnezhad", "Mahmoud", ""], ["Puckett", "Jay", ""], ["Fardhosseini", "Mohammad Sadra", ""], ["Pratama", "Lucky Agung", ""]]}, {"id": "1906.08682", "submitter": "Sadra Fardhosseini", "authors": "Mahmoud Habibnezhad, Jay Puckett, Mohammad Sadra Fardhosseini, Houtan\n  Jebelli, Terry Stentz, Lucky Agung Pratama", "title": "Experiencing Extreme Height for The First Time: The Influence of Height,\n  Self-Judgment of Fear and a Moving Structural Beam on the Heart Rate and\n  Postural Sway During the Quiet Stance", "comments": "36th International Symposium on Automation and Robotics in\n  Construction(ISARC 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Falling from elevated surfaces is the main cause of death and injury at\nconstruction sites. Based on the Bureau of Labor Statistics (BLS) reports, an\naverage of nearly three workers per day suffer fatal injuries from falling.\nStudies show that postural instability is the foremost cause of this\ndisproportional falling rate. To study what affects the postural stability of\nconstruction workers, we conducted a series of experiments in the virtual\nreality (VR). Twelve healthy adults, all students at the University of Nebraska\nwere recruited for this study. During each trial, participants heart rates and\npostural sways were measured as the dependent factors. The independent factors\nincluded a moving structural beam (MB) coming directly at the participants, the\npresence of VR, height, the participants self-judgment of fear, and their level\nof acrophobia. The former was designed in an attempt to simulate some part of\nthe steel erection procedure, which is one of the key tasks of ironworkers. The\nresults of this study indicate that height increase the postural sway.\nSelf-judged fear significantly was found to decrease postural sway, more\nspecifically the normalized total excursion of the center of pressure (TE),\nboth in the presence and absence of height. Also, participants heart rates\nsignificantly increase once they are confronted by a moving beam in the virtual\nenvironment (VE), even though they are informed that the beam will not hit\nthem. The findings of this study can be useful for training novice ironworkers\nthat will be subjected to height and steel erection for the first time.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 15:08:45 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Habibnezhad", "Mahmoud", ""], ["Puckett", "Jay", ""], ["Fardhosseini", "Mohammad Sadra", ""], ["Jebelli", "Houtan", ""], ["Stentz", "Terry", ""], ["Pratama", "Lucky Agung", ""]]}, {"id": "1906.08776", "submitter": "Valentina Fedorova", "authors": "Valentina Fedorova, Gleb Gusev, Pavel Serdyukov", "title": "Latent Distribution Assumption for Unbiased and Consistent Consensus\n  Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of aggregation noisy labels. Usually, it is solved by\nproposing a stochastic model for the process of generating noisy labels and\nthen estimating the model parameters using the observed noisy labels. A\ntraditional assumption underlying previously introduced generative models is\nthat each object has one latent true label. In contrast, we introduce a novel\nlatent distribution assumption, implying that a unique true label for an object\nmight not exist, but rather each object might have a specific distribution\ngenerating a latent subjective label each time the object is observed. Our\nexperiments showed that the novel assumption is more suitable for difficult\ntasks, when there is an ambiguity in choosing a \"true\" label for certain\nobjects.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 13:14:03 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Fedorova", "Valentina", ""], ["Gusev", "Gleb", ""], ["Serdyukov", "Pavel", ""]]}, {"id": "1906.08891", "submitter": "Sandipan Choudhuri", "authors": "Sandipan Choudhuri, Kaustav Basu, Kevin Thomas, Arunabha Sen", "title": "Predicting Future Opioid Incidences Today", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the Center of Disease Control (CDC), the Opioid epidemic has\nclaimed more than 72,000 lives in the US in 2017 alone. In spite of various\nefforts at the local, state and federal level, the impact of the epidemic is\nbecoming progressively worse, as evidenced by the fact that the number of\nOpioid related deaths increased by 12.5\\% between 2016 and 2017. Predictive\nanalytics can play an important role in combating the epidemic by providing\ndecision making tools to stakeholders at multiple levels - from health care\nprofessionals to policy makers to first responders. Generating Opioid incidence\nheat maps from past data, aid these stakeholders to visualize the profound\nimpact of the Opioid epidemic. Such post-fact creation of the heat map provides\nonly retrospective information, and as a result, may not be as useful for\npreventive action in the current or future time-frames. In this paper, we\npresent a novel deep neural architecture, which learns subtle spatio-temporal\nvariations in Opioid incidences data and accurately predicts future heat maps.\nWe evaluated the efficacy of our model on two open source datasets- (i) The\nCincinnati Heroin Overdose dataset, and (ii) Connecticut Drug Related Death\nDataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 22:53:18 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Choudhuri", "Sandipan", ""], ["Basu", "Kaustav", ""], ["Thomas", "Kevin", ""], ["Sen", "Arunabha", ""]]}, {"id": "1906.08937", "submitter": "Qiang Hao", "authors": "Qiang Hao, Jack P Wilson, Camille Ottaway, Naitra Iriumi, Kai Arakawa,\n  David H Smith IV", "title": "Investigating the Essential of Meaningful Automated Formative Feedback\n  for Programming Assignments", "comments": null, "journal-ref": "2019 IEEE Symposium on Visual Languages and Human-Centric\n  Computing (VL/HCC)", "doi": "10.1109/VLHCC.2019.8818922", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigated the essential of meaningful automated feedback for\nprogramming assignments. Three different types of feedback were tested,\nincluding (a) What's wrong - what test cases were testing and which failed, (b)\nGap - comparisons between expected and actual outputs, and (c) Hint - hints on\nhow to fix problems if test cases failed. 46 students taking a CS2 participated\nin this study. They were divided into three groups, and the feedback\nconfigurations for each group were different: (1) Group One - What's wrong, (2)\nGroup Two - What's wrong + Gap, (3) Group Three - What's wrong + Gap + Hint.\nThis study found that simply knowing what failed did not help students\nsufficiently, and might stimulate system gaming behavior. Hints were not found\nto be impactful on student performance or their usage of automated feedback.\nBased on the findings, this study provides practical guidance on the design of\nautomated feedback.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 04:02:12 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 04:25:54 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Hao", "Qiang", ""], ["Wilson", "Jack P", ""], ["Ottaway", "Camille", ""], ["Iriumi", "Naitra", ""], ["Arakawa", "Kai", ""], ["Smith", "David H", "IV"]]}, {"id": "1906.08973", "submitter": "Gaurav Verma", "authors": "Aadhavan M. Nambhi, Bhanu Prakash Reddy, Aarsh Prakash Agarwal, Gaurav\n  Verma, Harvineet Singh, Iftikhar Ahamath Burhanuddin", "title": "Stuck? No worries!: Task-aware Command Recommendation and Proactive Help\n  for Analysts", "comments": "27th Conference on User Modeling, Adaptation and Personalization\n  (UMAP'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics software applications have become an integral part of the\ndecision-making process of analysts. Users of such a software face challenges\ndue to insufficient product and domain knowledge, and find themselves in need\nof help. To alleviate this, we propose a task-aware command recommendation\nsystem, to guide the user on what commands could be executed next. We rely on\ntopic modeling techniques to incorporate information about user's task into our\nmodels. We also present a help prediction model to detect if a user is in need\nof help, in which case the system proactively provides the aforementioned\ncommand recommendations. We leverage the log data of a web-based analytics\nsoftware to quantify the superior performance of our neural models, in\ncomparison to competitive baselines.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 06:30:08 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Nambhi", "Aadhavan M.", ""], ["Reddy", "Bhanu Prakash", ""], ["Agarwal", "Aarsh Prakash", ""], ["Verma", "Gaurav", ""], ["Singh", "Harvineet", ""], ["Burhanuddin", "Iftikhar Ahamath", ""]]}, {"id": "1906.09086", "submitter": "Fatima Haouari", "authors": "Fatima Haouari, Emna Baccour, Aiman Erbad, Amr Mohamed, and Mohsen\n  Guizani", "title": "QoE-Aware Resource Allocation for Crowdsourced Live Streaming: A Machine\n  Learning Approach", "comments": "This paper was accepted in the Proceedings of ICC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.LG cs.MM cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the tremendous technological advancement of personal devices and\nthe prevalence of wireless mobile network accesses, the world has witnessed an\nexplosion in crowdsourced live streaming. Ensuring a better viewers quality of\nexperience (QoE) is the key to maximize the audiences number and increase\nstreaming providers' profits. This can be achieved by advocating a\ngeo-distributed cloud infrastructure to allocate the multimedia resources as\nclose as possible to viewers, in order to minimize the access delay and video\nstalls. Moreover, allocating the exact needed resources beforehand avoids\nover-provisioning, which may lead to significant costs by the service\nproviders. In the contrary, under-provisioning might cause significant delays\nto the viewers. In this paper, we introduce a prediction driven resource\nallocation framework, to maximize the QoE of viewers and minimize the resource\nallocation cost. First, by exploiting the viewers locations available in our\nunique dataset, we implement a machine learning model to predict the viewers\nnumber near each geo-distributed cloud site. Second, based on the predicted\nresults that showed to be close to the actual values, we formulate an\noptimization problem to proactively allocate resources at the viewers\nproximity. Additionally, we will present a trade-off between the video access\ndelay and the cost of resource allocation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 10:57:06 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Haouari", "Fatima", ""], ["Baccour", "Emna", ""], ["Erbad", "Aiman", ""], ["Mohamed", "Amr", ""], ["Guizani", "Mohsen", ""]]}, {"id": "1906.09417", "submitter": "Iv\\'an L\\'opez-Espejo", "authors": "Iv\\'an L\\'opez-Espejo and Zheng-Hua Tan and Jesper Jensen", "title": "Keyword Spotting for Hearing Assistive Devices Robust to External\n  Speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword spotting (KWS) is experiencing an upswing due to the pervasiveness of\nsmall electronic devices that allow interaction with them via speech. Often,\nKWS systems are speaker-independent, which means that any person --user or\nnot-- might trigger them. For applications like KWS for hearing assistive\ndevices this is unacceptable, as only the user must be allowed to handle them.\nIn this paper we propose KWS for hearing assistive devices that is robust to\nexternal speakers. A state-of-the-art deep residual network for small-footprint\nKWS is regarded as a basis to build upon. By following a multi-task learning\nscheme, this system is extended to jointly perform KWS and users'\nown-voice/external speaker detection with a negligible increase in the number\nof parameters. For experiments, we generate from the Google Speech Commands\nDataset a speech corpus emulating hearing aids as a capturing device. Our\nresults show that this multi-task deep residual network is able to achieve a\nKWS accuracy relative improvement of around 32% with respect to a system that\ndoes not deal with external speakers.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 09:51:14 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 09:48:25 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["L\u00f3pez-Espejo", "Iv\u00e1n", ""], ["Tan", "Zheng-Hua", ""], ["Jensen", "Jesper", ""]]}, {"id": "1906.09457", "submitter": "Paul Rosen", "authors": "Paul Rosen, Ashley Suh, Christopher Salgado, Mustafa Hajij", "title": "TopoLines: Topological Smoothing for Line Charts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Line charts are commonly used to visualize a series of data values. When the\ndata are noisy, smoothing is applied to make the signal more apparent.\nConventional methods used to smooth line charts, e.g., using subsampling or\nfilters, such as median, Gaussian, or low-pass, each optimize for different\nproperties of the data. The properties generally do not include retaining peaks\n(i.e., local minima and maxima) in the data, which is an important feature for\ncertain visual analytics tasks. We present TopoLines, a method for smoothing\nline charts using techniques from Topological Data Analysis. The design goal of\nTopoLines is to maintain prominent peaks in the data while minimizing any\nresidual error. We evaluate TopoLines for 2 visual analytics tasks by comparing\nto 5 popular line smoothing methods with data from 4 application domains.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 14:59:54 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 20:08:41 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Rosen", "Paul", ""], ["Suh", "Ashley", ""], ["Salgado", "Christopher", ""], ["Hajij", "Mustafa", ""]]}, {"id": "1906.09569", "submitter": "Nim Dvir Mr.", "authors": "Nim Dvir, Ruti Gafni", "title": "Systematic improvement of user engagement with academic titles using\n  computational linguistics", "comments": "Dvir, N., & Gafni, R. (2019). Systematic improvement of user\n  engagement with academic titles using computational linguistics. Proceedings\n  of the Informing Science and Information Technology Education Conference,\n  Jerusalem, Israel, pp. 501-512 Santa Rosa, CA: Informing Science Institute.\n  https://doi.org/10.28945/4338", "journal-ref": null, "doi": "10.28945/4338", "report-no": null, "categories": "cs.CL cs.DL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a novel approach to systematically improve information\ninteractions based solely on its wording. Following an interdisciplinary\nliterature review, we recognized three key attributes of words that drive user\nengagement: (1) Novelty (2) Familiarity (3) Emotionality. Based on these\nattributes, we developed a model to systematically improve a given content\nusing computational linguistics, natural language processing (NLP) and text\nanalysis (word frequency, sentiment analysis and lexical substitution). We\nconducted a pilot study (n=216) in which the model was used to formalize\nevaluation and optimization of academic titles. A between-group design (A/B\ntesting) was used to compare responses to the original and modified (treatment)\ntitles. Data was collected for selection and evaluation (User Engagement\nScale). The pilot results suggest that user engagement with digital information\nis fostered by, and perhaps dependent upon, the wording being used. They also\nprovide empirical support that engaging content can be systematically evaluated\nand produced. The preliminary results show that the modified (treatment) titles\nhad significantly higher scores for information use and user engagement\n(selection and evaluation). We propose that computational linguistics is a\nuseful approach for optimizing information interactions. The empirically based\ninsights can inform the development of digital content strategies, thereby\nimproving the success of information interactions.elop more sophisticated\ninteraction measures.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 09:23:08 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Dvir", "Nim", ""], ["Gafni", "Ruti", ""]]}, {"id": "1906.09594", "submitter": "Jason R.C. Nurse Dr", "authors": "Maria Bada and Jason R.C. Nurse", "title": "Developing cybersecurity education and awareness programmes for Small\n  and medium-sized enterprises (SMEs)", "comments": "20 pages, 1 figure", "journal-ref": "Information & Computer Security Journal, Vol. 27 Issue: 3,\n  pp.393-410, 2019", "doi": "10.1108/ICS-07-2018-0080", "report-no": null, "categories": "cs.CR cs.CY cs.GL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: An essential component of an organisation's cybersecurity strategy\nis building awareness and education of online threats, and how to protect\ncorporate data and services. This research article focuses on this topic and\nproposes a high-level programme for cybersecurity education and awareness to be\nused when targeting Small-to-Medium-sized Enterprises/Businesses (SMEs/SMBs) at\na city-level. We ground this programme in existing research as well as unique\ninsight into an ongoing city-based project with similar aims. Findings: We find\nthat whilst literature can be informative at guiding education and awareness\nprogrammes, it may not always reach real-world programmes. On the other hand,\nexisting programmes, such as the one we explored, have great potential but\nthere can also be room for improvement. Knowledge from each of these areas can,\nand should, be combined to the benefit of the academic and practitioner\ncommunities. Originality/value: The study contributes to current research\nthrough the outline of a high-level programme for cybersecurity education and\nawareness targeting SMEs/SMBs. Through this research, we engage in a reflection\nof literature in this space, and present insights into the advances and\nchallenges faced by an on-going programme. These analyses allow us to craft a\nproposal for a core programme that can assist in improving the security\neducation, awareness and training that targets SMEs/SMBs.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 13:13:04 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Bada", "Maria", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "1906.09689", "submitter": "Johan F. Hoorn", "authors": "Johan F. Hoorn and Denice J. Tuinhof", "title": "A robot's sense-making of fallacies and rhetorical tropes. Creating\n  ontologies of what humans try to say", "comments": "Hoorn, J. F., & Tuinhof, D. J. (2019). A robot's sense-making of\n  fallacies and rhetorical tropes. Creating ontologies of what humans try to\n  say. arXiv:cs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the design of user-friendly robots, human communication should be\nunderstood by the system beyond mere logics and literal meaning. Robot\ncommunication-design has long ignored the importance of communication and\npoliteness rules that are 'forgiving' and 'suspending disbelief' and cannot\nhandle the basically metaphorical way humans design their utterances. Through\nanalysis of the psychological causes of illogical and non-literal statements,\nsignal detection, fundamental attribution errors, and anthropomorphism, we\ndeveloped a fail-safe protocol for fallacies and tropes that makes use of\nFrege's distinction between reference and sense, Beth's tableau analytics,\nGrice's maxim of quality, and epistemic considerations to have the robot\npolitely make sense of a user's sometimes unintelligible demands. Keywords:\nsocial robots, logical fallacies, metaphors, reference, sense, maxim of\nquality, tableau reasoning, epistemics of the virtual\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 02:04:15 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Hoorn", "Johan F.", ""], ["Tuinhof", "Denice J.", ""]]}, {"id": "1906.09698", "submitter": "Yuan Yuan", "authors": "Yuan Yuan, Tracy Liu, Chenhao Tan, Qian Chen, Alex Pentland, Jie Tang", "title": "Gift Contagion in Online Groups: Evidence From WeChat Red Packets", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.HC cs.SI q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gifts are important instruments for forming bonds in interpersonal\nrelationships. Our study analyzes the phenomenon of gift contagion in online\ngroups. Gift contagion encourages social bonds of prompting further gifts; it\nmay also promote group interaction and solidarity. Using data on 36 million\nonline red packet gifts on China's social site WeChat, we leverage a natural\nexperimental design to identify the social contagion of gift giving in online\ngroups. Our natural experiment is enabled by the randomization of the gift\namount allocation algorithm on WeChat, which addresses the common challenge of\ncausal identifications in observational data. Our study provides evidence of\ngift contagion: on average, receiving one additional dollar causes a recipient\nto send 18 cents back to the group within the subsequent 24 hours. Decomposing\nthis effect, we find that it is mainly driven by the extensive margin -- more\nrecipients are triggered to send red packets. Moreover, we find that this\neffect is stronger for \"luckiest draw\" recipients, suggesting the presence of a\ngroup norm regarding the next red packet sender. Finally, we investigate the\nmoderating effects of group- and individual-level social network\ncharacteristics on gift contagion as well as the causal impact of receiving\ngifts on group network structure. Our study has implications for promoting\ngroup dynamics and designing marketing strategies for product adoption.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 03:08:13 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 09:59:25 GMT"}, {"version": "v3", "created": "Sun, 6 Sep 2020 23:34:41 GMT"}, {"version": "v4", "created": "Mon, 5 Apr 2021 09:09:23 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Yuan", "Yuan", ""], ["Liu", "Tracy", ""], ["Tan", "Chenhao", ""], ["Chen", "Qian", ""], ["Pentland", "Alex", ""], ["Tang", "Jie", ""]]}, {"id": "1906.09740", "submitter": "Robert Konrad", "authors": "Robert Konrad, Anastasios Angelopoulos, and Gordon Wetzstein", "title": "Gaze-Contingent Ocular Parallax Rendering for Virtual Reality", "comments": "Video: https://www.youtube.com/watch?v=FvBYYAObJNM&feature=youtu.be\n  Project Page:\n  http://www.computationalimaging.org/publications/gaze-contingent-ocular-parallax-rendering-for-virtual-reality/", "journal-ref": "ACM Trans. Graph. 39, 2, Article 10 (April 2020), 12 pages", "doi": "10.1145/3361330", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive computer graphics systems strive to generate perceptually realistic\nuser experiences. Current-generation virtual reality (VR) displays are\nsuccessful in accurately rendering many perceptually important effects,\nincluding perspective, disparity, motion parallax, and other depth cues. In\nthis article, we introduce ocular parallax rendering, a technology that\naccurately renders small amounts of gaze-contingent parallax capable of\nimproving depth perception and realism in VR. Ocular parallax describes the\nsmall amounts of depth-dependent image shifts on the retina that are created as\nthe eye rotates. The effect occurs because the centers of rotation and\nprojection of the eye are not the same. We study the perceptual implications of\nocular parallax rendering by designing and conducting a series of user\nexperiments. Specifically, we estimate perceptual detection and discrimination\nthresholds for this effect and demonstrate that it is clearly visible in most\nVR applications. Additionally, we show that ocular parallax rendering provides\nan effective ordinal depth cue and it improves the impression of realistic\ndepth in VR.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 06:11:30 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 00:03:48 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Konrad", "Robert", ""], ["Angelopoulos", "Anastasios", ""], ["Wetzstein", "Gordon", ""]]}, {"id": "1906.09802", "submitter": "Julen Urain de Jesus", "authors": "Julen Urain, Jan Peters", "title": "Generalized Multiple Correlation Coefficient as a Similarity\n  Measurements between Trajectories", "comments": "7 pages, 4 figures, IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity distance measure between two trajectories is an essential tool to\nunderstand patterns in motion, for example, in Human-Robot Interaction or\nImitation Learning. The problem has been faced in many fields, from Signal\nProcessing, Probabilistic Theory field, Topology field or Statistics\nfield.Anyway, up to now, none of the trajectory similarity measurements metrics\nare invariant to all possible linear transformation of the trajectories\n(rotation, scaling, reflection, shear mapping or squeeze mapping). Also not all\nof them are robust in front of noisy signals or fast enough for real-time\ntrajectory classification. To overcome this limitation this paper proposes a\nsimilarity distance metric that will remain invariant in front of any possible\nlinear transformation.Based on Pearson Correlation Coefficient and the\nCoefficient of Determination, our similarity metric, the Generalized Multiple\nCorrelation Coefficient (GMCC) is presented like the natural extension of the\nMultiple Correlation Coefficient. The motivation of this paper is two fold.\nFirst, to introduce a new correlation metric that presents the best properties\nto compute similarities between trajectories invariant to linear\ntransformations and compare it with some state of the art similarity\ndistances.Second, to present a natural way of integrating the similarity metric\nin an Imitation Learning scenario for clustering robot trajectories.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 09:25:49 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 14:19:05 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 11:48:32 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Urain", "Julen", ""], ["Peters", "Jan", ""]]}, {"id": "1906.09816", "submitter": "Hossein Rajaby Faghihi", "authors": "Hossein Rajaby Faghihi, Mohammad Amin Fazli, Jafar Habibi", "title": "Hybrid-Learning approach toward situation recognition and handling", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of smart environments largely depends on their smartness of\nunderstanding the environments' ongoing situations. Accordingly, this task is\nan essence to smart environment central processors. Obtaining knowledge from\nthe environment is often through sensors, and the response to a particular\ncircumstance is offered by actuators. This can be improved by getting user\nfeedback, and capturing environmental changes. Machine learning techniques and\nsemantic reasoning tools are widely used in this area to accomplish the goal of\ninterpretation. In this paper, we have proposed a hybrid approach utilizing\nboth machine learning and semantic reasoning tools to derive a better\nunderstanding from sensors. This method uses situation templates jointly with a\ndecision tree to adapt the system knowledge to the environment. To test this\napproach we have used a simulation process which has resulted in a better\nprecision for detecting situations in an ongoing environment involving living\nagents while capturing its dynamic nature.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 09:43:38 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Faghihi", "Hossein Rajaby", ""], ["Fazli", "Mohammad Amin", ""], ["Habibi", "Jafar", ""]]}, {"id": "1906.09840", "submitter": "I-Chao Shen", "authors": "Toby Chong Long Hin, I-Chao Shen, Issei Sato, Takeo Igarashi", "title": "Interactive Optimization of Generative Image Modeling using Sequential\n  Subspace Search and Content-based Guidance", "comments": "13 pages, Toby Chong Long Hin and I-Chao Shen contributed equally to\n  the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative image modeling techniques such as GAN demonstrate highly\nconvincing image generation result. However, user interaction is often\nnecessary to obtain the desired results. Existing attempts add interactivity\nbut require either tailored architectures or extra data. We present a\nhuman-in-the-optimization method that allows users to directly explore and\nsearch the latent vector space of generative image modeling. Our system\nprovides multiple candidates by sampling the latent vector space, and the user\nselects the best blending weights within the subspace using multiple sliders.\nIn addition, the user can express their intention through image editing tools.\nThe system samples latent vectors based on inputs and presents new candidates\nto the user iteratively. An advantage of our formulation is that one can apply\nour method to arbitrary pre-trained model without developing specialized\narchitecture or data. We demonstrate our method with various generative image\nmodeling applications, and show superior performance in a comparative user\nstudy with prior art iGAN.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 10:34:37 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 01:53:04 GMT"}, {"version": "v3", "created": "Sat, 29 Aug 2020 09:05:11 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hin", "Toby Chong Long", ""], ["Shen", "I-Chao", ""], ["Sato", "Issei", ""], ["Igarashi", "Takeo", ""]]}, {"id": "1906.09850", "submitter": "Omar Khan", "authors": "Omar Khan, Imran Ahmed, Joshua Cottingham, Musa Rahhal, Theodoros N\n  Arvanitis, Mark Elliott", "title": "Multisensory cues facilitate coordination of stepping movements with a\n  virtual reality avatar", "comments": "28 pages, 8 figures, submitted to PLOS ONE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of simple sensory cues for retraining gait have been\ndemonstrated, yet the feasibility of humanoid avatars for entrainment have yet\nto be investigated. Here, we describe the development of a novel method of\nvisually cued training, in the form of a virtual partner, and investigate its\nability to provide movement guidance in the form of stepping. Real stepping\nmovements were mapped onto an avatar using motion capture data. The trajectory\nof one of the avatar step cycles was then accelerated or decelerated by 15% to\ncreate a perturbation. Healthy participants were motion captured while\ninstructed to step in time to the avatar's movements, as viewed through a\nvirtual reality headset. Step onset times were used to measure the timing\nerrors (asynchronies) between them. Participants completed either a visual-only\ncondition, or auditory-visual with footstep sounds included. Participants'\nasynchronies exhibited slow drift in the Visual-Only condition, but became\nstable in the Auditory-Visual condition. Moreover, we observed a clear\ncorrective response to the phase perturbation in both auditory-visual\nconditions. We conclude that an avatar's movements can be used to influence a\nperson's own gait, but should include relevant auditory cues congruent with the\nmovement to ensure a suitable accuracy is achieved.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 11:08:42 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Khan", "Omar", ""], ["Ahmed", "Imran", ""], ["Cottingham", "Joshua", ""], ["Rahhal", "Musa", ""], ["Arvanitis", "Theodoros N", ""], ["Elliott", "Mark", ""]]}, {"id": "1906.09934", "submitter": "Ioannis Chatzigiannakis", "authors": "Chrysanthi Tziortzioti, Irene Mavrommati, Georgios Mylonas, Andrea\n  Vitaletti, Ioannis Chatzigiannakis", "title": "Scenarios for Educational and Game Activities using Internet of Things\n  Data", "comments": "14 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1805.09561", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Raising awareness among young people and changing their behavior and habits\nconcerning energy usage and the environment is key to achieving a sustainable\nplanet. The goal to address the global climate problem requires informing the\npopulation on their roles in mitigation actions and adaptation of sustainable\nbehaviors. Addressing climate change and achieve ambitious energy and climate\ntargets requires a change in citizen behavior and consumption practices. IoT\nsensing and related scenario and practices, which address school children via\ndiscovery, gamification, and educational activities, are examined in this\npaper. Use of seawater sensors in STEM education, that has not previously been\naddressed, is included in these educational scenaria.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 18:26:12 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Tziortzioti", "Chrysanthi", ""], ["Mavrommati", "Irene", ""], ["Mylonas", "Georgios", ""], ["Vitaletti", "Andrea", ""], ["Chatzigiannakis", "Ioannis", ""]]}, {"id": "1906.10017", "submitter": "Wenqiang Cui", "authors": "Wenqiang Cui and Girts Strazdins and Hao Wang", "title": "Confluent-Drawing Parallel Coordinates: Web-Based Interactive Visual\n  Analytics of Large Multi-Dimensional Data", "comments": "7 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel coordinates plot is one of the most popular and widely used\nvisualization techniques for multi-dimensional data sets. Its main challenges\nfor large-scale data sets are visual clutter and overplotting which hamper the\nrecognition of patterns and trends in the data. In this paper, we propose a\nconfluent drawing approach of parallel coordinates to support the web-based\ninteractive visual analytics of large multi-dimensional data. The proposed\nmethod maps multi-dimensional data to node-link diagrams through the data\nbinning-based clustering for each dimension. It uses density-based confluent\ndrawing to visualize clusters and edges to reduce visual clutter and\noverplotting. Its rendering time is independent of the number of data items. It\nsupports interactive visualization of large data sets without hardware\nacceleration in a normal web browser. Moreover, we design interactions to\ncontrol the data binning process with this approach to support interactive\nvisual analytics of large multi-dimensional data sets. Based on the proposed\napproach, we implement a web-based visual analytics application. The efficiency\nof the proposed method is examined through experiments on several data sets.\nThe effectiveness of the proposed method is evaluated through a user study, in\nwhich two typical tasks of parallel coordinates plot are performed by\nparticipants to compare the proposed method with another parallel coordinates\nbundling technique. Results show that the proposed method significantly\nenhances the web-based interactive visual analytics of large multi-dimensional\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 12:14:50 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Cui", "Wenqiang", ""], ["Strazdins", "Girts", ""], ["Wang", "Hao", ""]]}, {"id": "1906.10124", "submitter": "Ahmad Beirami", "authors": "Yunqi Zhao and Igor Borovikov and Jason Rupert and Caedmon Somers and\n  Ahmad Beirami", "title": "On Multi-Agent Learning in Team Sports Games", "comments": "Presented at ICML 2019 Workshop on Imitation, Intent, and Interaction\n  (I3). arXiv admin note: substantial text overlap with arXiv:1903.10545", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, reinforcement learning has been successful in solving video\ngames from Atari to Star Craft II. However, the end-to-end model-free\nreinforcement learning (RL) is not sample efficient and requires a significant\namount of computational resources to achieve superhuman level performance.\nModel-free RL is also unlikely to produce human-like agents for playtesting and\ngameplaying AI in the development cycle of complex video games. In this paper,\nwe present a hierarchical approach to training agents with the goal of\nachieving human-like style and high skill level in team sports games. While\nthis is still work in progress, our preliminary results show that the presented\napproach holds promise for solving the posed multi-agent learning problem.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:18:10 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Zhao", "Yunqi", ""], ["Borovikov", "Igor", ""], ["Rupert", "Jason", ""], ["Somers", "Caedmon", ""], ["Beirami", "Ahmad", ""]]}, {"id": "1906.10166", "submitter": "Mohsen Aghabozorgi Nafchi", "authors": "Mohsen Aghabozorgi Nafchi (1), Maryam Aghabozorgi Nafchi (2) ((1)\n  Shiraz University, (2) Shahrekord University)", "title": "Challenges and Opportunities of Big Data in Healthcare Mobile\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The health and various ways to improve healthcare systems are one of the most\nconcerns of human in history. By the growth of mobile technology, different\nmobile applications in the field of the healthcare system are developed. These\nmobile applications instantly gather and analyze the data of their users to\nhelp them in the health area. This volume of data will be a critical problem.\nBig data in healthcare mobile applications have its challenges and\nopportunities for the users and developers. Does this amount of gathered data\nwhich is increasing day by day can help the human to design new tools in\nhealthcare systems and improve health condition? In this chapter, we will\ndiscuss meticulously the challenges and opportunities of big data in the\nhealthcare mobile applications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 18:40:24 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 06:26:38 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Nafchi", "Mohsen Aghabozorgi", ""], ["Nafchi", "Maryam Aghabozorgi", ""]]}, {"id": "1906.10188", "submitter": "Pegah Karimi", "authors": "Pegah Karimi, Mary Lou Maher, Nicholas Davis, Kazjon Grace", "title": "Deep Learning in a Computational Model for Conceptual Shifts in a\n  Co-Creative Design System", "comments": "9 pages, 3 Figures, 1 Table, Accepted in ICCC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a computational model for conceptual shifts, based on a\nnovelty metric applied to a vector representation generated through deep\nlearning. This model is integrated into a co-creative design system, which\nenables a partnership between an AI agent and a human designer interacting\nthrough a sketching canvas. The AI agent responds to the human designer's\nsketch with a new sketch that is a conceptual shift: intentionally varying the\nvisual and conceptual similarity with increasingly more novelty. The paper\npresents the results of a user study showing that increasing novelty in the AI\ncontribution is associated with higher creative outcomes, whereas low novelty\nleads to less creative outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 19:24:09 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Karimi", "Pegah", ""], ["Maher", "Mary Lou", ""], ["Davis", "Nicholas", ""], ["Grace", "Kazjon", ""]]}, {"id": "1906.10229", "submitter": "Ron Bitton", "authors": "Ron Bitton, Kobi Boymgold, Rami Puzis and Asaf Shabtai", "title": "Evaluating the Information Security Awareness of Smartphone Users", "comments": "Under review in NDSS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information security awareness (ISA) is a practice focused on the set of\nskills, which help a user successfully mitigate a social engineering attack.\nPrevious studies have presented various methods for evaluating the ISA of both\nPC and mobile users. These methods rely primarily on subjective data sources\nsuch as interviews, surveys, and questionnaires that are influenced by human\ninterpretation and sincerity. Furthermore, previous methods for evaluating ISA\ndid not address the differences between classes of social engineering attacks.\nIn this paper, we present a novel framework designed for evaluating the ISA of\nsmartphone users to specific social engineering attack classes. In addition to\nquestionnaires, the proposed framework utilizes objective data sources: a\nmobile agent and a network traffic monitor; both of which are used to analyze\nthe actual behavior of users. We empirically evaluated the ISA scores assessed\nfrom the three data sources (namely, the questionnaires, mobile agent, and\nnetwork traffic monitor) by conducting a long-term user study involving 162\nsmartphone users. All participants were exposed to four different security\nchallenges that resemble real-life social engineering attacks. These challenges\nwere used to assess the ability of the proposed framework to derive a relevant\nISA score. The results of our experiment show that: (1) the self-reported\nbehavior of the users differs significantly from their actual behavior; and (2)\nISA scores derived from data collected by the mobile agent or the network\ntraffic monitor are highly correlated with the users' success in mitigating\nsocial engineering attacks.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 21:01:08 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Bitton", "Ron", ""], ["Boymgold", "Kobi", ""], ["Puzis", "Rami", ""], ["Shabtai", "Asaf", ""]]}, {"id": "1906.10244", "submitter": "Ramya Srinivasan", "authors": "Ramya Srinivasan, Ajay Chander, Pouya Pezeshkpour", "title": "Generating User-friendly Explanations for Loan Denials using GANs", "comments": "Presented at the NeurIPS 2018 Workshop on Challenges and\n  Opportunities for AI in Financial Services: the Impact of Fairness,\n  Explainability, Accuracy, and Privacy, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial decisions impact our lives, and thus everyone from the regulator to\nthe consumer is interested in fair, sound, and explainable decisions. There is\nincreasing competitive desire and regulatory incentive to deploy AI mindfully\nwithin financial services. An important mechanism towards that end is to\nexplain AI decisions to various stakeholders. State-of-the-art explainable AI\nsystems mostly serve AI engineers and offer little to no value to business\ndecision makers, customers, and other stakeholders. Towards addressing this\ngap, in this work we consider the scenario of explaining loan denials. We build\nthe first-of-its-kind dataset that is representative of loan-applicant friendly\nexplanations. We design a novel Generative Adversarial Network (GAN) that can\naccommodate smaller datasets, to generate user-friendly textual explanations.\nWe demonstrate how our system can also generate explanations serving different\npurposes: those that help educate the loan applicants, or help them take\nappropriate action towards a future approval.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 21:41:55 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Srinivasan", "Ramya", ""], ["Chander", "Ajay", ""], ["Pezeshkpour", "Pouya", ""]]}, {"id": "1906.10378", "submitter": "Funda Ustek Spilda", "authors": "Funda Ustek-Spilda, Alison Powell, Irina Shklovski and Sebastian\n  Lehuede", "title": "Peril v. Promise: IoT and the Ethical Imaginaries", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/16", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The future scenarios often associated with Internet of Things (IoT) oscillate\nbetween the peril of IoT for the future of humanity and the promises for an\never-connected and efficient future. Such a dichotomous positioning creates\nproblems not only for expanding the field of application of the technology, but\nalso ensuring ethical and responsible design and production. As part of VirtEU\n(Values and Ethics in Innovation for Responsible Technology in Europe) (EU\nHorizon 2020 FP7), we have conducted ethnographic research into the main hubs\nof IoT in Europe, such as London, Amsterdam, Barcelona and Belgrade, with\ndevelopers and designers of IoT to identify the challenges they face in their\nday-to-day work. In this paper, we focus on the IoT and the ethical imaginaries\nexplore the practical challenges IoT developers face when they are designing,\nproducing and marketing IoT technologies. We argue that top-down ethical\nframeworks that overlook the situated capabilities of developers or the\nsolutionist approaches that treat ethical issues as technical problems are\nunlikely to provide an alternative to the dichotomous imaginary for the future.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 08:31:39 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Ustek-Spilda", "Funda", ""], ["Powell", "Alison", ""], ["Shklovski", "Irina", ""], ["Lehuede", "Sebastian", ""]]}, {"id": "1906.10428", "submitter": "Dorien Herremans", "authors": "Kat Agres, Simon Lui, Dorien Herremans", "title": "A novel music-based game with motion capture to support cognitive and\n  motor function in the elderly", "comments": null, "journal-ref": "IEEE Conference on Games 2019, London, UK", "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel game prototype that uses music and motion\ndetection as preventive medicine for the elderly. Given the aging populations\naround the globe, and the limited resources and staff able to care for these\npopulations, eHealth solutions are becoming increasingly important, if not\ncrucial, additions to modern healthcare and preventive medicine. Furthermore,\nbecause compliance rates for performing physical exercises are often quite low\nin the elderly, systems able to motivate and engage this population are a\nnecessity. Our prototype uses music not only to engage listeners, but also to\nleverage the efficacy of music to improve mental and physical wellness. The\ngame is based on a memory task to stimulate cognitive function, and requires\nusers to perform physical gestures to mimic the playing of different musical\ninstruments. To this end, the Microsoft Kinect sensor is used together with a\nnewly developed gesture detection module in order to process users' gestures.\nThe resulting prototype system supports both cognitive functioning and physical\nstrengthening in the elderly.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 10:02:35 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Agres", "Kat", ""], ["Lui", "Simon", ""], ["Herremans", "Dorien", ""]]}, {"id": "1906.10557", "submitter": "Katrin Lohan", "authors": "Ingo Keller and Muneeb Imtiaz Ahmad and Katrin Lohan", "title": "Multi-Modal Measurements of Mental Load", "comments": "CHI Conference-W12, April 2019, Glasgow, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper describes an experiment conducted to understand the\nrelationships between different physiological measures including pupil\nDiameter, Blinking Rate, Heart Rate, and Heart Rate Variability in order to\ndevelop an estimation of users' mental load in real-time (see Sidebar 1). Our\nexperiment involved performing a task to spot a correct or an incorrect word or\nsentence with different difficulties in order to induce mental load. We briefly\npresent the analysis of task performance and response time for the items of the\nexperiment task.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 14:15:37 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Keller", "Ingo", ""], ["Ahmad", "Muneeb Imtiaz", ""], ["Lohan", "Katrin", ""]]}, {"id": "1906.10747", "submitter": "Fani Deligianni Dr", "authors": "Fani Deligianni, Ines Domingos, Guang-Zhong Yang", "title": "Intention Detection of Gait Adaptation in Natural Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gait adaptation is an important part of gait analysis and its neuronal origin\nand dynamics has been studied extensively. In neurorehabilitation, it is\nimportant as it perturbs neuronal dynamics and allows patients to restore some\nof their motor function. Exoskeletons and robotics of the lower limbs are\nincreasingly used to facilitate rehabilitation as well as supporting daily\nfunction. Their efficiency and safety depends on how well can sense the human\nintention to move and adapt the gait accordingly. This paper presents a gait\nadaptation scheme in natural settings. It allows monitoring of subjects in more\nrealistic environment without the requirement of specialized equipment such as\ntreadmill and foot pressure sensors. We extract gait characteristics based on a\nsingle RBG camera whereas wireless EEG signals are monitored simultaneously. We\ndemonstrate that the method can not only successfully detect adaptation steps\nbut also detect efficiently whether the subject adjust their pace to higher or\nlower speed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 20:08:52 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Deligianni", "Fani", ""], ["Domingos", "Ines", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "1906.10893", "submitter": "Yang Zhao", "authors": "Yang Zhao, Jun Zhao, Linshan Jiang, Rui Tan, Dusit Niyato, Zengxiang\n  Li, Lingjuan Lyu, Yingbo Liu", "title": "Privacy-Preserving Blockchain-Based Federated Learning for IoT Devices", "comments": "This paper appears in IEEE Internet of Things Journal (IoT-J)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.LG cs.NI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Home appliance manufacturers strive to obtain feedback from users to improve\ntheir products and services to build a smart home system. To help manufacturers\ndevelop a smart home system, we design a federated learning (FL) system\nleveraging the reputation mechanism to assist home appliance manufacturers to\ntrain a machine learning model based on customers' data. Then, manufacturers\ncan predict customers' requirements and consumption behaviors in the future.\nThe working flow of the system includes two stages: in the first stage,\ncustomers train the initial model provided by the manufacturer using both the\nmobile phone and the mobile edge computing (MEC) server. Customers collect data\nfrom various home appliances using phones, and then they download and train the\ninitial model with their local data. After deriving local models, customers\nsign on their models and send them to the blockchain. In case customers or\nmanufacturers are malicious, we use the blockchain to replace the centralized\naggregator in the traditional FL system. Since records on the blockchain are\nuntampered, malicious customers or manufacturers' activities are traceable. In\nthe second stage, manufacturers select customers or organizations as miners for\ncalculating the averaged model using received models from customers. By the end\nof the crowdsourcing task, one of the miners, who is selected as the temporary\nleader, uploads the model to the blockchain. To protect customers' privacy and\nimprove the test accuracy, we enforce differential privacy on the extracted\nfeatures and propose a new normalization technique. We experimentally\ndemonstrate that our normalization technique outperforms batch normalization\nwhen features are under differential privacy protection. In addition, to\nattract more customers to participate in the crowdsourcing FL task, we design\nan incentive mechanism to award participants.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 07:53:13 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 16:18:12 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2020 12:44:38 GMT"}, {"version": "v4", "created": "Mon, 1 Feb 2021 11:25:50 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhao", "Yang", ""], ["Zhao", "Jun", ""], ["Jiang", "Linshan", ""], ["Tan", "Rui", ""], ["Niyato", "Dusit", ""], ["Li", "Zengxiang", ""], ["Lyu", "Lingjuan", ""], ["Liu", "Yingbo", ""]]}, {"id": "1906.10896", "submitter": "Shunan Guo", "authors": "Shunan Guo, Zhuochen Jin, Qing Chen, David Gotz, Hongyuan Zha, Nan Cao", "title": "Visual Anomaly Detection in Event Sequence Data", "comments": null, "journal-ref": "Proceedings of 2019 IEEE International Conference on Big Data (Big\n  Data), Los Angeles, CA, USA, 2019, pp. 1125-1130", "doi": "10.1109/BigData47090.2019.9005687", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is a common analytical task that aims to identify rare\ncases that differ from the typical cases that make up the majority of a\ndataset. When applied to the analysis of event sequence data, the task of\nanomaly detection can be complex because the sequential and temporal nature of\nsuch data results in diverse definitions and flexible forms of anomalies. This,\nin turn, increases the difficulty in interpreting detected anomalies. In this\npaper, we propose an unsupervised anomaly detection algorithm based on\nVariational AutoEncoders (VAE) to estimate underlying normal progressions for\neach given sequence represented as occurrence probabilities of events along the\nsequence progression. Events in violation of their occurrence probability are\nidentified as abnormal. We also introduce a visualization system, EventThread3,\nto support interactive exploration and interpretations of anomalies within the\ncontext of normal sequence progressions in the dataset through comprehensive\none-to-many sequence comparison. Finally, we quantitatively evaluate the\nperformance of our anomaly detection algorithm and demonstrate the\neffectiveness of our system through a case study.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 07:55:44 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 07:44:27 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Guo", "Shunan", ""], ["Jin", "Zhuochen", ""], ["Chen", "Qing", ""], ["Gotz", "David", ""], ["Zha", "Hongyuan", ""], ["Cao", "Nan", ""]]}, {"id": "1906.11004", "submitter": "Stephane Safin", "authors": "Tom\\'as Dorta, St\\'ephane Safin (SES, SPE), Sana Boudhra\\^a, Emmanuel\n  Beaudry Marchand", "title": "Co-Designing in Social VR. Process awareness and suitable\n  representations to empower user participation", "comments": null, "journal-ref": "CAADRIA 2019 : Computer assisted architectural design research in\n  Asia, Apr 2019, Wellington, New Zealand", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To allow non-designers' involvement in design projects new methods are\nneeded. Co-design gives the same opportunity to all the multidisciplinary\nparticipants to co-create ideas simultaneously. Nevertheless, current co-design\nprocesses involving such users tend to limit their contribution to the proposal\nof basic design ideas only through brainstorming. The co-design approach needs\nto be enhanced by a properly suited representational ecosystem supporting\nactive participation and by conscious use of structured verbal exchanges giving\nawareness of the creative process. In this respect, we developed two social\nvirtual reality co-design systems, and a co-design verbal exchange methodology\nto favour participants' awareness of the co-creative process. By using such\nrepresentations and verbal exchanges, participants could co-create with more\nease by benefiting from being informed of the process and from the collective\nimmersion, empowering their participation. This paper presents the rationale\nbehind this approach of using Social VR in co-design and the feedback of three\nco-design workshops.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 12:02:20 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Dorta", "Tom\u00e1s", "", "SES, SPE"], ["Safin", "St\u00e9phane", "", "SES, SPE"], ["Boudhra\u00e2", "Sana", ""], ["Marchand", "Emmanuel Beaudry", ""]]}, {"id": "1906.11123", "submitter": "Jun Zhao Dr", "authors": "Ge Wang, Jun Zhao, Nigel Shadbolt", "title": "What concerns do Chinese parents have about their children's digital\n  adoption and how to better support them?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital devices are widely used by children, and children nowadays are\nspending more time online than with other media sources, such as watching\ntelevision or playing offline video games. In the UK, 44% of children aged five\nto ten have been provided with their own tablets, with this percentage\nincreasing annually, while in the US, ownership of tablets by children in this\nage group grew fivefold between 2011 and 2013. Our previous research found that\nUK children and parents need better support in dealing with online privacy\nrisks. Interestingly, very few research was done on Chinese children and\nparents. In this report, we present findings from our online survey of 593\nChinese parents with children aged 6-10 in February and March 2019. Our study\nparticularly focused on understanding Chinese parents' awareness and management\nof their children's online privacy risks. The goal of the survey was to examine\nthe current adoption pattern of digital devices by Chinese families with young\nchildren, the concerns Chinese parents have about their children's online\nactivities and the current practices they use for safeguarding their children\nonline. Our findings imply that we need to continue presenting specific\nguidance to parents in order to support their choice of digital content for\ntheir young children. Further, we need to look more deeply into the roles\nschools are taking in children's online activities, how can we support schools\nand teachers when they are making recommendations to parents and children.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 14:27:36 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wang", "Ge", ""], ["Zhao", "Jun", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "1906.11211", "submitter": "Shane Sims", "authors": "Shane D. Sims, Vanessa Putnam, Cristina Conati", "title": "Predicting Confusion from Eye-Tracking Data with Recurrent Neural\n  Networks", "comments": "This work was presented at the 2nd Workshop on Humanizing AI (HAI) at\n  IJCAI'19 in Macau, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encouraged by the success of deep learning in a variety of domains, we\ninvestigate the suitability and effectiveness of Recurrent Neural Networks\n(RNNs) in a domain where deep learning has not yet been used; namely detecting\nconfusion from eye-tracking data. Through experiments with a dataset of user\ninteractions with ValueChart (an interactive visualization tool), we found that\nRNNs learn a feature representation from the raw data that allows for a more\npowerful classifier than previous methods that use engineered features. This is\nevidenced by the stronger performance of the RNN (0.74/0.71\nsensitivity/specificity), as compared to a Random Forest classifier (0.51/0.70\nsensitivity/specificity), when both are trained on an un-augmented dataset.\nHowever, using engineered features allows for simple data augmentation methods\nto be used. These same methods are not as effective at augmentation for the\nfeature representation learned from the raw data, likely due to an inability to\nmatch the temporal dynamics of the data.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 04:47:16 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Sims", "Shane D.", ""], ["Putnam", "Vanessa", ""], ["Conati", "Cristina", ""]]}, {"id": "1906.11431", "submitter": "Ningxia Wang", "authors": "Li Chen, Ningxia Wang, Yonghua Yang, Keping Yang and Quan Yuan", "title": "User Validation of Recommendation Serendipity Metrics", "comments": "Some errors exist in this article, and another article with more\n  complete experiments and analysis is being prepared", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though it has been recognized that recommending serendipitous (i.e.,\nsurprising and relevant) items can be helpful for increasing users'\nsatisfaction and behavioral intention, how to measure serendipity in the\noffline environment is still an open issue. In recent years, a number of\nmetrics have been proposed, but most of them were based on researchers'\nassumptions due to the serendipity's subjective nature. In order to validate\nthese metrics' actual performance, we collected over 10,000 users' real\nfeedback data and compared with the metrics' results. It turns out the user\nprofile based metrics, especially content-based ones, perform better than those\nbased on item popularity, in terms of estimating the unexpectedness facet of\nrecommendations. Moreover, the full metrics, which involve the unexpectedness\ncomponent, relevance, timeliness, and user curiosity, can more accurately\nindicate the recommendation's serendipity degree, relative to those that just\ninvolve some of them. The application of these metrics to several recommender\nalgorithms further consolidates their practical usage, because the comparison\nresults are consistent with those from user evaluation. Thus, this work is\nconstructive for filling the gap between offline measurement and user study on\nrecommendation serendipity.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 04:25:53 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 08:26:38 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Chen", "Li", ""], ["Wang", "Ningxia", ""], ["Yang", "Yonghua", ""], ["Yang", "Keping", ""], ["Yuan", "Quan", ""]]}, {"id": "1906.11571", "submitter": "Alfonso Balandra", "authors": "Alfonso Balandra and Shoichi Hasegawa", "title": "Sensitivity to Haptic-Audio Envelope Asynchrony", "comments": "(The reported results are wrong, we are currently working on this\n  publication, the article contents will be fixed. ) Work in progress paper for\n  World Haptics 2019", "journal-ref": "World Haptics 2019 (Work in Progress)", "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We want to understand the human capabilities to perceive amplitude\nsimilarities between a haptic and an audio signal. So, four psychophysical\nexperiments were performed. Three of them measured the asynchrony JND (Just\nNoticeable Difference) at the signals' attack, release and decay, while the\nforth experiment measured the amplitude decrease on the middle of the signal.\nAll the experiments used a combination of the constant stimulus and staircase\nmethods to present two stimuli, while the participants' (N=12) task was to\nidentify which of the two stimuli was synchronized. The audiotactile stimulus\nwas defined using an stereo audio signal with an ADSR (Attack Decay Sustain\nRelease) envelope. The partial results reveal JNDs for temporal asynchrony of:\n54ms for attack, 265ms for decay and 57ms for release. Also the results reveal\nan amplitude decrease JND of 25\\%. Although for decay the results were to\ndisperse, therefore we suspect that the participants were not able to the\nchanges on the haptic signal.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 11:56:10 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 08:23:15 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Balandra", "Alfonso", ""], ["Hasegawa", "Shoichi", ""]]}, {"id": "1906.11597", "submitter": "Massimo Zancanaro", "authors": "Massimo Zancanaro, Oliviero Stock, Gianluca Schiavo, Alessandro\n  Cappelletti, Sebastian Gehrmann, Daphna Canetti, Ohad Shaked, Shani Fachter,\n  Rachel Yifat, Ravit Mimran, Patrice L. (Tamar) Weiss", "title": "Evaluating an Automated Mediator for Joint Narratives in a Conflict\n  Situation", "comments": null, "journal-ref": "https://www.tandfonline.com/eprint/5HVSAEZ7NW2AGUSHBNA3/full?target=10.1080/0144929X.2019.1637940", "doi": "10.1080/0144929X.2019.1637940", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint narratives are often used in the context of reconciliation\ninterventions for people in social conflict situations, which arise, for\nexample, due to ethnic or religious differences. The interventions aim to\nencourage a change in attitudes of the participants towards each other.\nTypically, a human mediator is fundamental for achieving a successful\nintervention. In this work, we present an automated approach to support remote\ninteractions between pairs of participants as they contribute to a shared story\nin their own language. A key component is an automated cognitive tutor that\nguides the participants through a controlled escalation/de-escalation process\nduring the development of a joint narrative. We performed a controlled study\ncomparing a trained human mediator to the automated mediator. The results\ndemonstrate that an automated mediator, although simple at this stage,\neffectively supports interactions and helps to achieve positive outcomes\ncomparable to those attained by the trained human mediator.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:01:25 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Zancanaro", "Massimo", "", "Tamar"], ["Stock", "Oliviero", "", "Tamar"], ["Schiavo", "Gianluca", "", "Tamar"], ["Cappelletti", "Alessandro", "", "Tamar"], ["Gehrmann", "Sebastian", "", "Tamar"], ["Canetti", "Daphna", "", "Tamar"], ["Shaked", "Ohad", "", "Tamar"], ["Fachter", "Shani", "", "Tamar"], ["Yifat", "Rachel", "", "Tamar"], ["Mimran", "Ravit", "", "Tamar"], ["L.", "Patrice", "", "Tamar"], ["Weiss", "", ""]]}, {"id": "1906.11669", "submitter": "Christoph Gebhardt", "authors": "Christoph Gebhardt, Benjamin Hepp, Tobias Naegeli, Stefan Stevsic,\n  Otmar Hilliges", "title": "Airways: Optimization-Based Planning of Quadrotor Trajectories according\n  to High-Level User Goals", "comments": "12 pages", "journal-ref": "Proceedings of the 2016 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/2858036.2858353", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a computational design tool that al-lows end-users\nto create advanced quadrotor trajectories witha variety of application\nscenarios in mind. Our algorithm al-lows novice users to create quadrotor based\nuse-cases withoutrequiring deep knowledge in either quadrotor control or\ntheunderlying constraints of the target domain. To achieve thisgoal we propose\nan optimization-based method that gener-ates feasible trajectories which can be\nflown in the real world.Furthermore, the method incorporates high-level human\nob-jectives into the planning of flight trajectories. An easy touse 3D design\ntool allows for quick specification and edit-ing of trajectories as well as for\nintuitive exploration of theresulting solution space. We demonstrate the\nutility of our ap-proach in several real-world application scenarios,\nincludingaerial-videography, robotic light-painting and drone racing.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 14:14:51 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Gebhardt", "Christoph", ""], ["Hepp", "Benjamin", ""], ["Naegeli", "Tobias", ""], ["Stevsic", "Stefan", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1906.11686", "submitter": "Christoph Gebhardt", "authors": "Christoph Gebhardt, Stefan Stevsic, Otmar Hilliges", "title": "Optimizing for Aesthetically Pleasing Quadrotor Camera Motion", "comments": null, "journal-ref": "ACM Transactions on Graphics (TOG), Volume 37 Issue 4, August\n  2018, Article No. 90", "doi": "10.1145/3197517.3201390", "report-no": null, "categories": "cs.GR cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first contribute a large scale online study (N=400) to\nbetter understand aesthetic perception of aerial video. The results indicate\nthat it is paramount to optimize smoothness of trajectories across all\nkeyframes. However, for experts timing control remains an essential tool.\nSatisfying this dual goal is technically challenging because it requires giving\nup desirable properties in the optimization formulation. Second, informed by\nthis study we propose a method that optimizes positional and temporal reference\nfit jointly. This allows to generate globally smooth trajectories, while\nretaining user control over reference timings. The formulation is posed as a\nvariable, infinite horizon, contour-following algorithm. Finally, a comparative\nlab study indicates that our optimization scheme outperforms the\nstate-of-the-art in terms of perceived usability and preference of resulting\nvideos. For novices our method produces smoother and better looking results and\nalso experts benefit from generated timings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 14:28:28 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Gebhardt", "Christoph", ""], ["Stevsic", "Stefan", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1906.11706", "submitter": "Azra Bihorac", "authors": "Triton Ong, Matthew Ruppert, Parisa Rashidi, Tezcan Ozrazgat-Baslanti,\n  Azra Bihorac, Marko Suvajdzic", "title": "The DREAMS Project: Improving the Intensive Care Patient Experience with\n  Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Preliminarily evaluate the feasibility and efficacy of using\nmeditative virtual reality (VR) to improve the hospital experience of intensive\ncare unit (ICU) patients.\n  Methods: Effects of VR were examined in a non-randomized, single-center\ncohort. Fifty-nine patients admitted to the surgical or trauma ICU of the\nUniversity of Florida Health Shands Hospital participated. A Google Daydream\nheadset was used to expose ICU patients to commercially available VR\napplications focused on calmness and relaxation (Google Spotlight Stories and\nRelaxVR). Sessions were conducted once daily for up to seven days. Outcome\nmeasures included pain level, anxiety, depression, medication administration,\nsleep quality, heart rate, respiratory rate, blood pressure, delirium status,\nand patient ratings of the VR system. Comparisons were made using paired\nt-tests and mixed models where appropriate.\n  Results: The VR meditative intervention was found to improve patients' ICU\nexperience with reduced levels of anxiety and depression; however, there was no\nevidence suggesting that VR had any significant effects on physiological\nmeasures, pain, or sleep.\n  Conclusion: The use of VR technology in the ICU was shown to be easily\nimplemented and well-received by patients.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 14:55:05 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 16:50:04 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Ong", "Triton", ""], ["Ruppert", "Matthew", ""], ["Rashidi", "Parisa", ""], ["Ozrazgat-Baslanti", "Tezcan", ""], ["Bihorac", "Azra", ""], ["Suvajdzic", "Marko", ""]]}, {"id": "1906.11753", "submitter": "Thomas Langerak", "authors": "Thomas Langerak, Juan Zarate, Velko Vechev, Daniele Panozzo, Otmar\n  Hilliges", "title": "Dynamic Drawing Guidance via Electromagnetic Haptic Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system to deliver dynamic guidance in drawing, sketching and\nhandwriting tasks via an electromagnet moving underneath a high refresh rate\npressure sensitive tablet. The system allows the user to move the pen at their\nown pace and style and does not take away control. The system continously and\niteratively measures the pen motion and adjusts magnet position and power\naccording to the user input in real-time via a receding horizon optimal control\nformulation. The optimization is based on a novel approximate electromagnet\nmodel that is fast enough for use in real-time methods, yet provides very good\nfit to experimental data. Using a closed-loop time-free approach allows for\nerror-correcting behavior, gently pulling the user back to the desired\ntrajectory rather than pushing or pulling the pen to a continuously advancing\nsetpoint. Our experimental results show that the system can control the pen\nposition with a very low dispersion of 2.8mm (+/-0.8mm). An initial user study\nindicates that it significantly increases accuracy of users drawing a variety\nof shapes and that this improvement increases with complexity of the shape.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:55:24 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Langerak", "Thomas", ""], ["Zarate", "Juan", ""], ["Vechev", "Velko", ""], ["Panozzo", "Daniele", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1906.11889", "submitter": "Lena A. J\\\"ager", "authors": "Lena A. J\\\"ager, Silvia Makowski, Paul Prasse, Sascha Liehr,\n  Maximilian Seidler and Tobias Scheffer", "title": "Deep Eyedentification: Biometric Identification using Micro-Movements of\n  the Eye", "comments": null, "journal-ref": "In: U. Brefeld et al. (Eds.): Machine Learning and Knowledge\n  Discovery in Databases, ECML PKDD 2019, LNCS 11907, pp. 299-314, Springer\n  Nature, Switzerland, 2020", "doi": "10.1007/978-3-030-46147-8_18", "report-no": null, "categories": "cs.CV cs.CL cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study involuntary micro-movements of the eye for biometric identification.\nWhile prior studies extract lower-frequency macro-movements from the output of\nvideo-based eye-tracking systems and engineer explicit features of these\nmacro-movements, we develop a deep convolutional architecture that processes\nthe raw eye-tracking signal. Compared to prior work, the network attains a\nlower error rate by one order of magnitude and is faster by two orders of\nmagnitude: it identifies users accurately within seconds.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 10:36:40 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 08:27:02 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 05:14:56 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2020 10:06:31 GMT"}, {"version": "v5", "created": "Tue, 5 May 2020 08:30:44 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["J\u00e4ger", "Lena A.", ""], ["Makowski", "Silvia", ""], ["Prasse", "Paul", ""], ["Liehr", "Sascha", ""], ["Seidler", "Maximilian", ""], ["Scheffer", "Tobias", ""]]}, {"id": "1906.11960", "submitter": "Tempestt Neal", "authors": "Khadija Zanna, Sayde King, Tempestt Neal, Shaun Canavan", "title": "Studying the Impact of Mood on Identifying Smartphone Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the identification of smartphone users when certain\nsamples collected while the subject felt happy, upset or stressed were absent\nor present. We employ data from 19 subjects using the StudentLife dataset, a\ndataset collected by researchers at Dartmouth College that was originally\ncollected to correlate behaviors characterized by smartphone usage patterns\nwith changes in stress and academic performance. Although many previous works\non behavioral biometrics have implied that mood is a source of intra-person\nvariation which may impact biometric performance, our results contradict this\nassumption. Our findings show that performance worsens when removing samples\nthat were generated when subjects may be happy, upset, or stressed. Thus, there\nis no indication that mood negatively impacts performance. However, we do find\nthat changes existing in smartphone usage patterns may correlate with mood,\nincluding changes in locking, audio, location, calling, homescreen, and e-mail\nhabits. Thus, we show that while mood is a source of intra-person variation, it\nmay be an inaccurate assumption that biometric systems (particularly, mobile\nbiometrics) are likely influenced by mood.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 20:55:16 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Zanna", "Khadija", ""], ["King", "Sayde", ""], ["Neal", "Tempestt", ""], ["Canavan", "Shaun", ""]]}, {"id": "1906.12051", "submitter": "Jiri Motejlek", "authors": "Jiri Motejlek, Esat Alpay", "title": "A Taxonomy for Virtual and Augmented Reality in Education", "comments": "European Society for Engineering Education Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a taxonomy for VR/AR in education is presented that can help\ndifferentiate and categorise education experiences and provide indication as to\nwhy some applications of fail whereas others succeed. Examples will be\npresented to illustrate the taxonomy, including its use in developing and\nplanning two current VR projects in our laboratory. The first project is a VR\napplication for the training of Chemical Engineering students (and potentially\nindustrial operators) on the use of a physical pilot plant facility. The second\nproject involves the use of VR cinematography for enacting ethics scenarios\n(and thus ethical awareness and development) pertinent to engineering work\nsituations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 05:56:57 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Motejlek", "Jiri", ""], ["Alpay", "Esat", ""]]}, {"id": "1906.12088", "submitter": "Chao Shi", "authors": "Chao Shi", "title": "Non-user Inclusive Design for Maintaining Harmony of Real-Virtual Human\n  Interaction in Augmented Reality", "comments": "9 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality enables the illusion of contents such as objects and humans\nin the virtual world co-existing with users in the real world. However,\nnon-users who are not aware of the presence of the virtual world and\ndynamically move nearby might either cause a conflict by directly breaking into\nspace where a user is talking to a Virtual Human (VH), or be troubled when try\nto avoid disturbing the user. To maintain harmony and keep both the user's and\nnon-users' comfort, we propose a method that controls the VH to adjust its own\nposition to avoid such potential conflict. The difficulty to address this\nproblem is that the agent must avoid potential conflict in a natural way to\nkeep the user away from feeling unnatural. Our idea is to endow the VH with\nthree capabilities: anticipating non-users walking around, understanding how to\nestablish and maintain proper formation to adapt to the environment, and\nplanning to avoid conflicts by shifting formation in advance. We develop a\nnon-user inclusive spatial formation model that realizes natural arrangement\nshift corresponding to the environment based on theoretical sources from\nliterature. We implemented our proposed model into a VH behavior planning\nsystem to achieve natural conflict avoidance. Evaluation experiments showed\nthat it successfully reduces potential conflicts caused by non-users.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 08:32:22 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Shi", "Chao", ""]]}, {"id": "1906.12175", "submitter": "Minh Tran", "authors": "Minh Tran, Taylan Sen, Kurtis Haut, Mohammad Rafayet Ali and Mohammed\n  Ehsan Hoque", "title": "Are you really looking at me? A Feature-Extraction Framework for\n  Estimating Interpersonal Eye Gaze from Conventional Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite a revolution in the pervasiveness of video cameras in our daily\nlives, one of the most meaningful forms of nonverbal affective communication,\ninterpersonal eye gaze, i.e. eye gaze relative to a conversation partner, is\nnot available from common video. We introduce the Interpersonal-Calibrating\nEye-gaze Encoder (ICE), which automatically extracts interpersonal gaze from\nvideo recordings without specialized hardware and without prior knowledge of\nparticipant locations. Leveraging the intuition that individuals spend a large\nportion of a conversation looking at each other enables the ICE dynamic\nclustering algorithm to extract interpersonal gaze. We validate ICE in both\nvideo chat using an objective metric with an infrared gaze tracker (F1=0.846,\nN=8), as well as in face-to-face communication with expert-rated evaluations of\neye contact (r= 0.37, N=170). We then use ICE to analyze behavior in two\ndifferent, yet important affective communication domains: interrogation-based\ndeception detection, and communication skill assessment in speed dating. We\nfind that honest witnesses break interpersonal gaze contact and look down more\noften than deceptive witnesses when answering questions (p=0.004, d=0.79). In\npredicting expert communication skill ratings in speed dating videos, we\ndemonstrate that interpersonal gaze alone has more predictive power than facial\nexpressions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 19:23:31 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 20:32:21 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tran", "Minh", ""], ["Sen", "Taylan", ""], ["Haut", "Kurtis", ""], ["Ali", "Mohammad Rafayet", ""], ["Hoque", "Mohammed Ehsan", ""]]}, {"id": "1906.12251", "submitter": "Gregoire Cattan", "authors": "Anton Andreev (GIPSA-Services), Gr\\'egoire Cattan (GIPSA-VIBS,\n  IHMTEK), M Congedo (GIPSA-VIBS)", "title": "Engineering study on the use of Head-Mounted display for Brain- Computer\n  Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we explore the availability of head-mounted display (HMD)\ndevices which can be coupled in a seamless way with P300-based brain-computer\ninterfaces (BCI) using electroencephalography (EEG). The P300 is an\nevent-related potential appearing about 300ms after the onset of a stimulation.\nThe recognition of this potential on the ongoing EEG requires the knowledge of\nthe exact onset of the stimuli. In other words, the stimulations presented in\nthe HMD must be perfectly synced with the acquisition of the EEG signal. This\nis done through a process called tagging. The tagging must be performed in a\nreliable and robust way so as to guarantee the recognition of the P300 and thus\nthe performance of the BCI. An HMD device should also be able to render images\nfast enough to allow an accurate perception of the stimulations, and equally to\nnot perturb the acquisition of the EEG signal. In addition, an affordable HMD\ndevice is needed for both research and entertainment purposes. In this study,\nwe selected and tested two HMD configurations.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 14:55:30 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Andreev", "Anton", "", "GIPSA-Services"], ["Cattan", "Gr\u00e9goire", "", "GIPSA-VIBS,\n  IHMTEK"], ["Congedo", "M", "", "GIPSA-VIBS"]]}]