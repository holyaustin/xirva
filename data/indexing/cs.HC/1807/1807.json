[{"id": "1807.00037", "submitter": "Julian Vicens", "authors": "Juli\\'an Vicens, Josep Perell\\'o and Jordi Duch", "title": "Citizen Social Lab: A digital platform for human behaviour\n  experimentation within a citizen science framework", "comments": "17 pages, 11 figures and 4 tables", "journal-ref": "PLOS ONE 13(12): e0207219 (2018)", "doi": "10.1371/journal.pone.0207219", "report-no": null, "categories": "cs.CY cs.GT cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperation is one of the behavioral traits that define human beings, however\nwe are still trying to understand why humans cooperate. Behavioral experiments\nhave been largely conducted to shed light into the mechanisms behind\ncooperation and other behavioral traits. However, most of these experiments\nhave been conducted in laboratories with highly controlled experimental\nprotocols but with varied limitations which limits the reproducibility and the\ngeneralization of the results obtained. In an attempt to overcome these\nlimitations, some experimental approaches have moved human behavior\nexperimentation from laboratories to public spaces, where behaviors occur\nnaturally, and have opened the participation to the general public within the\ncitizen science framework. Given the open nature of these environments, it is\ncritical to establish the appropriate protocols to maintain the same data\nquality that one can obtain in the laboratories. Here, we introduce Citizen\nSocial Lab, a software platform designed to be used in the wild using citizen\nscience practices. The platform allows researchers to collect data in a more\nrealistic context while maintaining the scientific rigour, and it is structured\nin a modular and scalable way so it can also be easily adapted for online or\nbrick-and-mortar experimental laboratories. Following citizen science\nguidelines, the platform is designed to motivate a more general population into\nparticipation, but also to promote engaging and learning of the scientific\nresearch process. We also review the main results of the experiments performed\nusing the platform up to now, and the set of games that each experiment\nincludes. Finally, we evaluate some properties of the platform, such as the\nheterogeneity of the samples of the experiments and their satisfaction level,\nand the parameters that demonstrate the robustness of the platform and the\nquality of the data collected.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 12:21:32 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Vicens", "Juli\u00e1n", ""], ["Perell\u00f3", "Josep", ""], ["Duch", "Jordi", ""]]}, {"id": "1807.00462", "submitter": "Jiankai Sun", "authors": "Jiankai Sun, Abhinav Vishnu, Aniket Chakrabarti, Charles Siegel, and\n  Srinivasan Parthasarathy", "title": "ColdRoute: Effective Routing of Cold Questions in Stack Exchange Sites", "comments": "Accepted to the Journal Track of The European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases\n  (ECML PKDD 2018); Published by Springer:\n  https://link.springer.com/article/10.1007%2Fs10618-018-0577-7", "journal-ref": "@Article{Sun2018, author=\"Sun, Jiankai and Vishnu, A. and\n  Chakrabarti, A. and Siegel, C. and Parthasarathy, S.\", title=\"ColdRoute:\n  effective routing of cold questions in stack exchange sites\", journal=\"ECML\n  PKDD\", year=\"2018\"}", "doi": "10.1007/s10618-018-0577-7", "report-no": null, "categories": "cs.AI cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Routing questions in Community Question Answer services (CQAs) such as Stack\nExchange sites is a well-studied problem. Yet, cold-start -- a phenomena\nobserved when a new question is posted is not well addressed by existing\napproaches. Additionally, cold questions posted by new askers present\nsignificant challenges to state-of-the-art approaches. We propose ColdRoute to\naddress these challenges. ColdRoute is able to handle the task of routing cold\nquestions posted by new or existing askers to matching experts. Specifically,\nwe use Factorization Machines on the one-hot encoding of critical features such\nas question tags and compare our approach to well-studied techniques such as\nCQARank and semantic matching (LDA, BoW, and Doc2Vec). Using data from eight\nstack exchange sites, we are able to improve upon the routing metrics\n(Precision$@1$, Accuracy, MRR) over the state-of-the-art models such as\nsemantic matching by $159.5\\%$,$31.84\\%$, and $40.36\\%$ for cold questions\nposted by existing askers, and $123.1\\%$, $27.03\\%$, and $34.81\\%$ for cold\nquestions posted by new askers respectively.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 05:08:05 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Sun", "Jiankai", ""], ["Vishnu", "Abhinav", ""], ["Chakrabarti", "Aniket", ""], ["Siegel", "Charles", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1807.00482", "submitter": "Toan Nguyen", "authors": "Toan Nguyen and Nasir Memon", "title": "Tap-based User Authentication for Smartwatches", "comments": "11 pages, 8 figures", "journal-ref": "Computer & Security, Volume 78, September 2018, Pages 174-186", "doi": "10.1016/j.cose.2018.07.001", "report-no": null, "categories": "cs.CR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents TapMeIn, an eyes-free, two-factor authentication method\nfor smartwatches. It allows users to tap a memorable melody (tap-password) of\ntheir choice anywhere on the touchscreen to unlock their watch. A user is\nverified based on the tap-password as well as her physiological and behavioral\ncharacteristics when tapping. Results from preliminary experiments with 41\nparticipants show that TapMeIn could achieve an accuracy of 98.7% with a False\nPositive Rate of only 0.98%. In addition, TapMeIn retains its performance in\ndifferent conditions such as sitting and walking. In terms of speed, TapMeIn\nhas an average authentication time of 2 seconds. A user study with the System\nUsability Scale (SUS) tool suggests that TapMeIn has a high usability score.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 06:27:42 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 19:02:05 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Nguyen", "Toan", ""], ["Memon", "Nasir", ""]]}, {"id": "1807.00948", "submitter": "De'Aira Bryant", "authors": "Tobi Ogunyale, De'Aira Bryant and Ayanna Howard", "title": "Does Removing Stereotype Priming Remove Bias? A Pilot Human-Robot\n  Interaction Study", "comments": "5 pages, 9 figures, 1 table, to be presented at the 5th Workshop on\n  Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2018),\n  Stockholm, Sweden, July 15, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots capable of participating in complex social interactions have shown\ngreat potential in a variety of applications. As these robots grow more\npopular, it is essential to continuously evaluate the dynamics of the\nhuman-robot relationship. One factor shown to have potential impacts on this\ncritical relationship is the human projection of stereotypes onto social\nrobots, a practice that is implicitly known to effect both developers and users\nof this technology. As such, in this research, we wished to investigate the\ndifference in participants' perceptions of the robot interaction if we removed\nstereotype priming. This has not yet been a common practice in similar studies.\nGiven the stereotypes of emotions among ethnic groups, especially in the U.S.,\nthis study specifically sought to investigate the impact that robot \"skin\ncolor\" could potentially have on the human perception of a robot's emotional\nexpressive behavior. A between-subject experiment with 198 individuals was\nconducted. The results showed no significant differences in the overall emotion\nclassification or intensity ratings for the different robot skin colors. These\nresults lend credence to our hypothesis that when individuals are not primed\nwith information related to human stereotypes, robots are evaluated based on\nfunctional attributes versus stereotypical attributes. This provides some\nconfidence that robots, if designed correctly, can potentially be used as a\ntool to override stereotype-based biases associated with human behavior.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 01:48:06 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ogunyale", "Tobi", ""], ["Bryant", "De'Aira", ""], ["Howard", "Ayanna", ""]]}, {"id": "1807.01106", "submitter": "Rodrigo Martin", "authors": "Rodrigo Mart\\'in, Michael Weinmann, Matthias B. Hullin", "title": "A Study of Material Sonification in Touchscreen Devices", "comments": "9 pages", "journal-ref": "Proc. ACM ISS 2018, 305-310", "doi": "10.1145/3279778.3281455", "report-no": null, "categories": "cs.HC cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even in the digital age, designers largely rely on physical material samples\nto illustrate their products, as existing visual representations fail to\nsufficiently reproduce the look and feel of real world materials. Here, we\ninvestigate the use of interactive material sonification as an additional\nsensory modality for communicating well-established material qualities like\nsoftness, pleasantness or value. We developed a custom application for\ntouchscreen devices that receives tactile input and translate it into material\nrubbing sound using granular synthesis. We used this system to perform a\npsychophysical study, in which the ability of the user to rate subjective\nmaterial qualities is evaluated, with the actual material samples serving as\nreference stimulus. Our experimental results indicate that the considered audio\ncues do not significantly contribute to the perception of material qualities\nbut are able to increase the level of immersion when interacting with digital\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 12:05:31 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 09:29:27 GMT"}, {"version": "v3", "created": "Wed, 26 Sep 2018 14:48:22 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Mart\u00edn", "Rodrigo", ""], ["Weinmann", "Michael", ""], ["Hullin", "Matthias B.", ""]]}, {"id": "1807.01227", "submitter": "Ariel Rosenfeld", "authors": "Akiva Kleinerman, Ariel Rosenfeld, Sarit Kraus", "title": "Providing Explanations for Recommendations in Reciprocal Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated platforms which support users in finding a mutually beneficial\nmatch, such as online dating and job recruitment sites, are becoming\nincreasingly popular. These platforms often include recommender systems that\nassist users in finding a suitable match. While recommender systems which\nprovide explanations for their recommendations have shown many benefits,\nexplanation methods have yet to be adapted and tested in recommending suitable\nmatches. In this paper, we introduce and extensively evaluate the use of\n\"reciprocal explanations\" -- explanations which provide reasoning as to why\nboth parties are expected to benefit from the match. Through an extensive\nempirical evaluation, in both simulated and real-world dating platforms with\n287 human participants, we find that when the acceptance of a recommendation\ninvolves a significant cost (e.g., monetary or emotional), reciprocal\nexplanations outperform standard explanation methods which consider the\nrecommendation receiver alone. However, contrary to what one may expect, when\nthe cost of accepting a recommendation is negligible, reciprocal explanations\nare shown to be less effective than the traditional explanation methods.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 15:10:01 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Kleinerman", "Akiva", ""], ["Rosenfeld", "Ariel", ""], ["Kraus", "Sarit", ""]]}, {"id": "1807.01515", "submitter": "Felix Beierle", "authors": "Felix Beierle, Vinh Thuy Tran, Mathias Allemand, Patrick Neff,\n  Winfried Schlee, Thomas Probst, R\\\"udiger Pryss, Johannes Zimmermann", "title": "Context Data Categories and Privacy Model for Mobile Data Collection\n  Apps", "comments": "Accepted for publication at the 15th International Conference on\n  Mobile Systems and Pervasive Computing (MobiSPC 2018)", "journal-ref": null, "doi": "10.1016/j.procs.2018.07.139", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context-aware applications stemming from diverse fields like mobile health,\nrecommender systems, and mobile commerce potentially benefit from knowing\naspects of the user's personality. As filling out personality questionnaires is\ntedious, we propose the prediction of the user's personality from smartphone\nsensor and usage data. In order to collect data for researching the\nrelationship between smartphone data and personality, we developed the Android\napp TYDR (Track Your Daily Routine) which tracks smartphone data and utilizes\npsychometric personality questionnaires. With TYDR, we track a larger variety\nof smartphone data than similar existing apps, including metadata on\nnotifications, photos taken, and music played back by the user. For the\ndevelopment of TYDR, we introduce a general context data model consisting of\nfour categories that focus on the user's different types of interactions with\nthe smartphone: physical conditions and activity, device status and usage, core\nfunctions usage, and app usage. On top of this, we develop the privacy model\nPM-MoDaC specifically for apps related to the collection of mobile data,\nconsisting of nine proposed privacy measures. We present the implementation of\nall of those measures in TYDR. Although the utilization of the user's\npersonality based on the usage of his or her smartphone is a challenging\nendeavor, it seems to be a promising approach for various types of\ncontext-aware mobile applications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 10:57:02 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Beierle", "Felix", ""], ["Tran", "Vinh Thuy", ""], ["Allemand", "Mathias", ""], ["Neff", "Patrick", ""], ["Schlee", "Winfried", ""], ["Probst", "Thomas", ""], ["Pryss", "R\u00fcdiger", ""], ["Zimmermann", "Johannes", ""]]}, {"id": "1807.01597", "submitter": "Joos Behncke", "authors": "Joos Behncke, Robin Tibor Schirrmeister, Wolfram Burgard, Tonio Ball", "title": "The role of robot design in decoding error-related information from EEG\n  signals of a human observer", "comments": null, "journal-ref": null, "doi": "10.5220/0006934900610066", "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For utilization of robotic assistive devices in everyday life, means for\ndetection and processing of erroneous robot actions are a focal aspect in the\ndevelopment of collaborative systems, especially when controlled via brain\nsignals. Though, the variety of possible scenarios and the diversity of used\nrobotic systems pose a challenge for error decoding from recordings of brain\nsignals such as via EEG. For example, it is unclear whether humanoid\nappearances of robotic assistants have an influence on the performance. In this\npaper, we designed a study in which two different robots executed the same task\nboth in an erroneous and a correct manner. We find error-related EEG signals of\nhuman observers indicating that the performance of the error decoding was\nindependent of robot design. However, we can show that it was possible to\nidentify which robot performed the instructed task by means of the EEG signals.\nIn this case, deep convolutional neural networks (deep ConvNets) could reach\nsignificantly higher accuracies than both regularized Linear Discriminanat\nAnalysis (rLDA) and filter bank common spatial patterns (FB-CSP) combined with\nrLDA. Our findings indicate that decoding information about robot action\nsuccess from the EEG, particularly when using deep neural networks, may be an\napplicable approach for a broad range of robot designs.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 14:06:31 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 09:53:33 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Behncke", "Joos", ""], ["Schirrmeister", "Robin Tibor", ""], ["Burgard", "Wolfram", ""], ["Ball", "Tonio", ""]]}, {"id": "1807.01866", "submitter": "Harold Soh", "authors": "Harold Soh, Yaqi Xie, Min Chen, David Hsu", "title": "Multi-Task Trust Transfer for Human-Robot Interaction", "comments": "IJRR and RSS conference", "journal-ref": null, "doi": "10.1177/0278364919866905", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trust is essential in shaping human interactions with one another and with\nrobots. This paper discusses how human trust in robot capabilities transfers\nacross multiple tasks. We first present a human-subject study of two distinct\ntask domains: a Fetch robot performing household tasks and a virtual reality\nsimulation of an autonomous vehicle performing driving and parking maneuvers.\nThe findings expand our understanding of trust and inspire new differentiable\nmodels of trust evolution and transfer via latent task representations: (i) a\nrational Bayes model, (ii) a data-driven neural network model, and (iii) a\nhybrid model that combines the two. Experiments show that the proposed models\noutperform prevailing models when predicting trust over unseen tasks and users.\nThese results suggest that (i) task-dependent functional trust models capture\nhuman trust in robot capabilities more accurately, and (ii) trust transfer\nacross tasks can be inferred to a good degree. The latter enables\ntrust-mediated robot decision-making for fluent human-robot interaction in\nmulti-task settings.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 06:58:13 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 03:37:35 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Soh", "Harold", ""], ["Xie", "Yaqi", ""], ["Chen", "Min", ""], ["Hsu", "David", ""]]}, {"id": "1807.02164", "submitter": "Mao Yang", "authors": "Mao Yang, Bo Li, Guanxiong Feng, Zhongjiang Yan", "title": "V-CNN: When Convolutional Neural Network encounters Data Visualization", "comments": "2 pages, 2 figures, submitted to ACM Sigcomm 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning poses a deep technical revolution in almost\nevery field and attracts great attentions from industry and academia.\nEspecially, the convolutional neural network (CNN), one representative model of\ndeep learning, achieves great successes in computer vision and natural language\nprocessing. However, simply or blindly applying CNN to the other fields results\nin lower training effects or makes it quite difficult to adjust the model\nparameters. In this poster, we propose a general methodology named V-CNN by\nintroducing data visualizing for CNN. V-CNN introduces a data visualization\nmodel prior to CNN modeling to make sure the data after processing is fit for\nthe features of images as well as CNN modeling. We apply V-CNN to the network\nintrusion detection problem based on a famous practical dataset: AWID.\nSimulation results confirm V-CNN significantly outperforms other studies and\nthe recall rate of each invasion category is more than 99.8%.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 10:57:57 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Yang", "Mao", ""], ["Li", "Bo", ""], ["Feng", "Guanxiong", ""], ["Yan", "Zhongjiang", ""]]}, {"id": "1807.02268", "submitter": "Guohao Lan", "authors": "Guohao Lan, Weitao Xu, Dong Ma, Sara Khalifa, Mahbub Hassan, and Wen\n  Hu", "title": "EnTrans:Leveraging Kinetic Energy Harvesting Signal for Transportation\n  Mode Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the daily transportation modes of an individual provides useful\ninformation in many application domains, such as urban design, real-time\njourney recommendation, as well as providing location-based services. In\nexisting systems, accelerometer and GPS are the dominantly used signal sources\nfor transportation context monitoring which drain out the limited battery life\nof the wearable devices very quickly. To resolve the high energy consumption\nissue, in this paper, we present EnTrans, which enables transportation mode\ndetection by using only the kinetic energy harvester as an energy-efficient\nsignal source. The proposed idea is based on the intuition that the vibrations\nexperienced by the passenger during traveling with different transportation\nmodes are distinctive. Thus, voltage signal generated by the energy harvesting\ndevices should contain sufficient features to distinguish different\ntransportation modes. We evaluate our system using over 28 hours of data, which\nis collected by eight individuals using a practical energy harvesting\nprototype. The evaluation results demonstrate that EnTrans is able to achieve\nan overall accuracy over 92% in classifying five different modes while saving\nmore than 34% of the system power compared to conventional accelerometer-based\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 06:14:21 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 20:08:55 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Lan", "Guohao", ""], ["Xu", "Weitao", ""], ["Ma", "Dong", ""], ["Khalifa", "Sara", ""], ["Hassan", "Mahbub", ""], ["Hu", "Wen", ""]]}, {"id": "1807.02386", "submitter": "Holger Schn\\\"adelbach Dr", "authors": "Holger Schn\\\"adelbach, Tom Lodge, Tim Coughlan, Alex Taylor", "title": "Photo Screen: Shaping Perceptions of Residential Communities", "comments": "4 pages, plus references", "journal-ref": null, "doi": "10.13140/RG.2.2.10502.88642", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engaging residential communities with each other and with management remains\na challenge. Housing providers deploy a variety of engagement strategies, some\nof which are supported by digital technologies. Their individual success is\nvaried and integrated, multipronged approaches are seen to be more successful.\nAs part of those, it is important to address people's perceptions of community\nand places, as well as any practical issues that they face. We present the\ndesign and evaluation of Photo Screen, a situated, public photo taking and\nviewing screen which was deployed in the context of a new flagship housing\nestate as part of a range of community engagement measures. In a new context,\nwe confirm the high levels of engagement that can be achieved with this simple\nmechanism. We propose that photo 'tagging' might offer a second-stage\nengagement mechanism and enable meaningful dialogue between residents and\nmanagement. Finally, we discuss how this playful activity allowed residents to\npositively shape the perception of their community.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 12:47:20 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 10:22:59 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Schn\u00e4delbach", "Holger", ""], ["Lodge", "Tom", ""], ["Coughlan", "Tim", ""], ["Taylor", "Alex", ""]]}, {"id": "1807.02472", "submitter": "Kleomenis Katevas", "authors": "Kleomenis Katevas, Ioannis Arapakis, Martin Pielot", "title": "Typical Phone Use Habits: Intense Use Does Not Predict Negative\n  Well-Being", "comments": "10 pages, 6 figures, conference paper", "journal-ref": null, "doi": "10.1145/3229434.3229441", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Not all smartphone owners use their device in the same way. In this work, we\nuncover broad, latent patterns of mobile phone use behavior. We conducted a\nstudy where, via a dedicated logging app, we collected daily mobile phone\nactivity data from a sample of 340 participants for a period of four weeks.\nThrough an unsupervised learning approach and a methodologically rigorous\nanalysis, we reveal five generic phone use profiles which describe at least 10%\nof the participants each: limited use, business use, power use, and\npersonality- & externally induced problematic use. We provide evidence that\nintense mobile phone use alone does not predict negative well-being. Instead,\nour approach automatically revealed two groups with tendencies for lower\nwell-being, which are characterized by nightly phone use sessions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 16:18:28 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Katevas", "Kleomenis", ""], ["Arapakis", "Ioannis", ""], ["Pielot", "Martin", ""]]}, {"id": "1807.02987", "submitter": "Fuat Basik", "authors": "Fuat Basik, Bugra Gedik, Hakan Ferhatosmanoglu, Kun-Lung Wu", "title": "Fair Task Allocation in Crowdsourced Delivery", "comments": "To Appear in IEEE Transactions on Services Computing", "journal-ref": null, "doi": "10.1109/TSC.2018.2854866", "report-no": null, "categories": "cs.MA cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faster and more cost-efficient, crowdsourced delivery is needed to meet the\ngrowing customer demands of many industries, including online shopping,\non-demand local delivery, and on-demand transportation. The power of\ncrowdsourced delivery stems from the large number of workers potentially\navailable to provide services and reduce costs. It has been shown in social\npsychology literature that fairness is key to ensuring high worker\nparticipation. However, existing assignment solutions fall short on modeling\nthe dynamic fairness metric. In this work, we introduce a new assignment\nstrategy for crowdsourced delivery tasks. This strategy takes fairness towards\nworkers into consideration, while maximizing the task allocation ratio. Since\nredundant assignments are not possible in delivery tasks, we first introduce a\n2-phase allocation model that increases the reliability of a worker to complete\na given task. To realize the effectiveness of our model in practice, we present\nboth offline and online versions of our proposed algorithm called F-Aware.\nGiven a task-to-worker bipartite graph, F-Aware assigns each task to a worker\nthat minimizes unfairness, while allocating tasks to use worker capacities as\nmuch as possible. We present an evaluation of our algorithms with respect to\nrunning time, task allocation ratio (TAR), as well as unfairness and assignment\nratio. Experiments show that F-Aware runs around 10^7 x faster than the\nTAR-optimal solution and allocates 96.9% of the tasks that can be allocated by\nit. Moreover, it is shown that, F-Aware is able to provide a much fair\ndistribution of tasks to workers than the best competitor algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 08:45:47 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Basik", "Fuat", ""], ["Gedik", "Bugra", ""], ["Ferhatosmanoglu", "Hakan", ""], ["Wu", "Kun-Lung", ""]]}, {"id": "1807.03125", "submitter": "Moneish Kumar", "authors": "Kranthi Kumar, Moneish Kumar, Vineet Gandhi, Ramanathan Subramanian", "title": "Watch to Edit: Video Retargeting using Gaze", "comments": null, "journal-ref": "Computer Graphics Forum, Volume37, Issue2(2018)205-215", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to optimally retarget videos for varied displays\nwith differing aspect ratios by preserving salient scene content discovered via\neye tracking. Our algorithm performs editing with cut, pan and zoom operations\nby optimizing the path of a cropping window within the original video while\nseeking to (i) preserve salient regions, and (ii) adhere to the principles of\ncinematography. Our approach is (a) content agnostic as the same methodology is\nemployed to re-edit a wide-angle video recording or a close-up movie sequence\ncaptured with a static or moving camera, and (b) independent of video length\nand can in principle re-edit an entire movie in one shot. Our algorithm\nconsists of two steps. The first step employs gaze transition cues to detect\ntime stamps where new cuts are to be introduced in the original video via\ndynamic programming. A subsequent step optimizes the cropping window path (to\ncreate pan and zoom effects), while accounting for the original and new cuts.\nThe cropping window path is designed to include maximum gaze information, and\nis composed of piecewise constant, linear and parabolic segments. It is\nobtained via L(1) regularized convex optimization which ensures a smooth\nviewing experience. We test our approach on a wide variety of videos and\ndemonstrate significant improvement over the state-of-the-art, both in terms of\ncomputational complexity and qualitative aspects. A study performed with 16\nusers confirms that our approach results in a superior viewing experience as\ncompared to gaze driven re-editing and letterboxing methods, especially for\nwide-angle static camera recordings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 16:21:03 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Kumar", "Kranthi", ""], ["Kumar", "Moneish", ""], ["Gandhi", "Vineet", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1807.03224", "submitter": "Aditya Vempaty", "authors": "Ravi Kokku, Aditya Vempaty, Tamer Abuelsaad, Prasenjit Dey, Tammy\n  Humphrey, Akimi Gibson, Jennifer Kotler", "title": "Design and Evaluation of a Tutor Platform for Personalized Vocabulary\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our experiences in designing, implementing, and piloting\nan intelligent vocabulary learning tutor. The design builds on several\nintelligent tutoring design concepts, including graph-based knowledge\nrepresentation, learner modeling, and adaptive learning content and assessment\nexposition. Specifically, we design a novel phased learner model approach to\nenable systematic exposure to words during vocabulary instruction. We also\nbuilt an example application over the tutor platform that uses a learning\nactivity involving videos and an assessment activity involving word to\npicture/image association. More importantly, the tutor adapts to the\nsignificant variation in children's knowledge at the beginning of kindergarten,\nand evolves the application at the speed of each individual learner. A pilot\nstudy with 180 kindergarten learners allowed the tutor to collect various kinds\nof activity information suitable for insights and interventions both at an\nindividual- and class-level. The effort also demonstrates that we can do A/B\ntesting for a variety of hypotheses at scale with such a framework.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 15:19:22 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Kokku", "Ravi", ""], ["Vempaty", "Aditya", ""], ["Abuelsaad", "Tamer", ""], ["Dey", "Prasenjit", ""], ["Humphrey", "Tammy", ""], ["Gibson", "Akimi", ""], ["Kotler", "Jennifer", ""]]}, {"id": "1807.03713", "submitter": "Heiko Drewes", "authors": "Heiko Drewes, Mohamed Khamis, Florian Alt", "title": "DialPlate: Enhancing the Detection of Smooth Pursuits Eye Movements\n  Using Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and evaluate a novel approach for detecting smooth pursuit eye\nmovements that increases the number of distinguishable targets and is more\nrobust against false positives. Being natural and calibration-free, Pursuits\nhas been gaining popularity in the past years. At the same time, current\nimplementations show poor performance when more than eight on-screen targets\nare being used, thus limiting its applicability. Our approach (1) leverages the\nslope of a regression line, and (2) introduces a minimum signal duration that\nimproves both the new and the traditional detection method. After introducing\nthe approach as well as the implementation, we compare it to the traditional\ncorrelation-based Pursuits detection method. We tested the approach up to 24\ntargets and show that, if accepting a similar error rate, nearly twice as many\ntargets can be distinguished compared to state of the art. For fewer targets,\naccuracy increases significantly. We believe our approach will enable more\nrobust pursuit-based user interfaces, thus making it valuable for both\nresearchers and practitioners.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 15:31:18 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Drewes", "Heiko", ""], ["Khamis", "Mohamed", ""], ["Alt", "Florian", ""]]}, {"id": "1807.04184", "submitter": "Arnaud Mas", "authors": "Arnaud Mas (EDF R\\&D PERICLES), Idriss Isma\\\"el (EDF R\\&D PERICLES),\n  Nicolas Filliard (EDF R\\&D PERICLES)", "title": "Indy: a virtual reality multi-player game for navigation skills training", "comments": null, "journal-ref": "IEEE Fourth VR International Workshop on Collaborative Virtual\n  Environments (IEEEVR 2018), Mar 2018, Reutlingen, Germany", "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working in complex industrial facilities requires spatial navigation skills\nthat people build up with time and field experience. Training sessions\nconsisting in guided tours help discover places but they are insufficient to\nbecome intimately familiar with their layout. They imply passive learning\npostures, are time-limited and can be experienced only once because of\norganization constraints and potential interferences with ongoing activities in\nthe buildings. To overcome these limitations and improve the acquisition of\nnavigation skills, we developed Indy, a virtual reality system consisting in a\ncollaborative game of treasure hunting. It has several key advantages: it\nfocuses learners' attention on navigation tasks, implies their active\nengagement and provides them with feedbacks on their achievements. Virtual\nreality makes it possible to multiply the number and duration of situations\nthat learners can experience to better consolidate their skills. This paper\ndiscusses the main design principles and a typical usage scenario of Indy.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:13:06 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Mas", "Arnaud", "", "EDF R\\&D PERICLES"], ["Isma\u00ebl", "Idriss", "", "EDF R\\&D PERICLES"], ["Filliard", "Nicolas", "", "EDF R\\&D PERICLES"]]}, {"id": "1807.04191", "submitter": "Bardia Doosti", "authors": "Bardia Doosti, Tao Dong, Biplab Deka, Jeffrey Nichols", "title": "A Computational Method for Evaluating UI Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UI design languages, such as Google's Material Design, make applications both\neasier to develop and easier to learn by providing a set of standard UI\ncomponents. Nonetheless, it is hard to assess the impact of design languages in\nthe wild. Moreover, designers often get stranded by strong-opinionated debates\naround the merit of certain UI components, such as the Floating Action Button\nand the Navigation Drawer. To address these challenges, this short paper\nintroduces a method for measuring the impact of design languages and informing\ndesign debates through analyzing a dataset consisting of view hierarchies,\nscreenshots, and app metadata for more than 9,000 mobile apps. Our data\nanalysis shows that use of Material Design is positively correlated to app\nratings, and to some extent, also the number of installs. Furthermore, we show\nthat use of UI components vary by app category, suggesting a more nuanced view\nneeded in design debates.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:21:59 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Doosti", "Bardia", ""], ["Dong", "Tao", ""], ["Deka", "Biplab", ""], ["Nichols", "Jeffrey", ""]]}, {"id": "1807.04343", "submitter": "Yanxia Zhang", "authors": "Yanxia Zhang and Hayley Hung", "title": "Using Topic Models to Mine Everyday Object Usage Routines Through\n  Connected IoT Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the tremendous progress in sensing and IoT infrastructure, it is\nforeseeable that IoT systems will soon be available for commercial markets,\nsuch as in people's homes. In this paper, we present a deployment study using\nsensors attached to household objects to capture the resourcefulness of three\nindividuals. The concept of resourcefulness highlights the ability of humans to\nrepurpose objects spontaneously for a different use case than was initially\nintended. It is a crucial element for human health and wellbeing, which is of\ngreat interest for various aspects of HCI and design research. Traditionally,\nresourcefulness is captured through ethnographic practice. Ethnography can only\nprovide sparse and often short duration observations of human experience, often\nrelying on participants being aware of and remembering behaviours or thoughts\nthey need to report on. Our hypothesis is that resourcefulness can also be\ncaptured through continuously monitoring objects being used in everyday life.\nWe developed a system that can record object movement continuously and deployed\nthem in homes of three elderly people for over two weeks. We explored the use\nof probabilistic topic models to analyze the collected data and identify common\npatterns.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 20:30:44 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Zhang", "Yanxia", ""], ["Hung", "Hayley", ""]]}, {"id": "1807.04469", "submitter": "Peter M\\\"uller", "authors": "Peter M\\\"uller and Jan-Hendrik Passoth", "title": "Engineering Collaborative Social Science Toolkits. STS Methods and\n  Concepts as Devices for Interdisciplinary Diplomacy", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The smartification of industries is marked by the development of\ncyber-physical systems, interfaces, and intelligent software featuring\nknowledge models, empirical real-time data, and feedback-loops. This brings up\nnew requirements and challenges for HMI design and industrial labor. Social\nsciences can contribute to such engineering projects with their perspectives,\nconcepts and knowledge. Hence, we claim that, in addition to following their\nown intellectual curiosities, the social sciences can and should contribute to\nsuch projects in terms of an 'applied' science, helping to foster\ninterdisciplinary collaboration and providing toolkits and devices for what we\ncall 'interdisciplinary diplomacy'. We illustrate the benefits of such an\napproach, support them with selected examples of our involvement in such an\nengineering project and propose using methods as diplomatic devices and\nconcepts as social theory plug-ins. The article ends with an outlook and\nreflection on the remaining issue of whether and in how far such 'applied' and\ncritical social science can or should be integrated.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 08:34:45 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["M\u00fcller", "Peter", ""], ["Passoth", "Jan-Hendrik", ""]]}, {"id": "1807.04606", "submitter": "Mateusz Dubiel Mr", "authors": "Mateusz Dubiel, Martin Halvey, Leif Azzopardi", "title": "A Survey Investigating Usage of Virtual Personal Assistants", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant improvements in automatic speech recognition and spoken\nlanguage understanding - human interaction with Virtual Personal Assistants\n(VPAs) through speech remains irregular and sporadic. According to recent\nstudies, currently the usage of VPAs is constrained to basic tasks such as\nchecking facts, playing music, and obtaining weather updates.In this paper, we\npresent results of a survey (N = 118) that analyses usage of VPAs by frequent\nand infrequent users. We investigate how usage experience, performance\nexpectations, and privacy concerns differ between these two groups. The results\nindicate that, compared with infrequent users, frequent users of VPAs are more\nsatisfied with their assistants, more eager to use them in a variety of\nsettings, yet equally concerned about their privacy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 13:41:40 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Dubiel", "Mateusz", ""], ["Halvey", "Martin", ""], ["Azzopardi", "Leif", ""]]}, {"id": "1807.04966", "submitter": "Ricardo Antunes", "authors": "Ricardo Antunes and Mani Poshdar", "title": "Envision of an integrated information system for project-driven\n  production in construction", "comments": "Lean construction, SCADA, machine learning, LiDAR, BIM. 10 pages, 1\n  figure, conference", "journal-ref": "Proc. 26th Annual Conference of the International. Group for Lean\n  Construction (IGLC) 2018", "doi": "10.24928/2018/0511", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Construction frequently appears at the bottom of productivity charts with\ndecreasing indexes of productivity over the years. Lack of innovation and\ndelayed adoption, informal processes or insufficient rigor and consistency in\nprocess execution, insufficient knowledge transfer from project to project,\nweak project monitoring, little cross- functional cooperation, little\ncollaboration with suppliers, conservative company culture, and a shortage of\nyoung talent and people development are usual issues. Whereas work has been\ncarried out on information technology and automation in construction their\napplication is isolated without an interconnected information flow. This paper\nsuggests a framework to address production issues on construction by\nimplementing an integrated automatic supervisory control and data acquisition\nfor management and operations. The system is divided into planning, monitoring,\ncontrolling, and executing groups clustering technologies to track both the\nproject product and production. This research stands on the four pillars of\nmanufacturing knowledge and lean production (production processes, production\nmanagement, equipment/tool design, and automated systems and control). The\nframework offers benefits such as increased information flow, detection and\nprevention of overburdening equipment or labor (Muri) and production unevenness\n(Mura), reduction of waste (Muda), evidential and continuous process\nstandardization and improvement, reuse and abstraction of project information\nacross endeavors.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 08:19:29 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Antunes", "Ricardo", ""], ["Poshdar", "Mani", ""]]}, {"id": "1807.05037", "submitter": "Chris Cundy", "authors": "Chris Cundy, Daniel Filan", "title": "Exploring Hierarchy-Aware Inverse Reinforcement Learning", "comments": "Presented at the first Workshop on Goal Specifications for\n  Reinforcement Learning, ICML 2018, Stockholm, Sweden", "journal-ref": "1st Workshop on Goal Specifications for Reinforcement Learning,\n  ICML 2018, Stockholm, Sweden, 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new generative model for human planning under the Bayesian\nInverse Reinforcement Learning (BIRL) framework which takes into account the\nfact that humans often plan using hierarchical strategies. We describe the\nBayesian Inverse Hierarchical RL (BIHRL) algorithm for inferring the values of\nhierarchical planners, and use an illustrative toy model to show that BIHRL\nretains accuracy where standard BIRL fails. Furthermore, BIHRL is able to\naccurately predict the goals of `Wikispeedia' game players, with inclusion of\nhierarchical structure in the model resulting in a large boost in accuracy. We\nshow that BIHRL is able to significantly outperform BIRL even when we only have\na weak prior on the hierarchical structure of the plans available to the agent,\nand discuss the significant challenges that remain for scaling up this\nframework to more realistic settings.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 12:33:07 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Cundy", "Chris", ""], ["Filan", "Daniel", ""]]}, {"id": "1807.05275", "submitter": "Brandon Wagstaff", "authors": "Brandon Wagstaff and Jonathan Kelly", "title": "LSTM-Based Zero-Velocity Detection for Robust Inertial Navigation", "comments": "In Proceedings of the International Conference on Indoor Positioning\n  and Indoor Navigation (IPIN'18), Nantes, France, Sep. 24-27, 2018", "journal-ref": null, "doi": "10.1109/IPIN.2018.8533770", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to improve the accuracy of a zero-velocity-aided inertial\nnavigation system (INS) by replacing the standard zero-velocity detector with a\nlong short-term memory (LSTM) neural network. While existing threshold-based\nzero-velocity detectors are not robust to varying motion types, our learned\nmodel accurately detects stationary periods of the inertial measurement unit\n(IMU) despite changes in the motion of the user. Upon detection, zero-velocity\npseudo-measurements are fused with a dead reckoning motion model in an extended\nKalman filter (EKF). We demonstrate that our LSTM-based zero-velocity detector,\nused within a zero-velocity-aided INS, improves zero-velocity detection during\nhuman localization tasks. Consequently, localization accuracy is also improved.\n  Our system is evaluated on more than 7.5 km of indoor pedestrian locomotion\ndata, acquired from five different subjects. We show that 3D positioning error\nis reduced by over 34% compared to existing fixed-threshold zero-velocity\ndetectors for walking, running, and stair climbing motions. Additionally, we\ndemonstrate how our learned zero-velocity detector operates effectively during\ncrawling and ladder climbing. Our system is calibration-free (no careful\nthreshold-tuning is required) and operates consistently with differing users,\nIMU placements, and shoe types, while being compatible with any generic\nzero-velocity-aided INS.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 20:25:18 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 17:18:50 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Wagstaff", "Brandon", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1807.05389", "submitter": "Manuel Marin-Jimenez", "authors": "Manuel J. Marin-Jimenez and Francisco J. Romero-Ramirez and Rafael\n  Mu\\~noz-Salinas and Rafael Medina-Carnicer", "title": "3D human pose estimation from depth maps using a deep combination of\n  poses", "comments": "Accepted for publication at \"Journal of Visual Communication and\n  Image Representation\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications require the estimation of human body joints for\nhigher-level tasks as, for example, human behaviour understanding. In recent\nyears, depth sensors have become a popular approach to obtain three-dimensional\ninformation. The depth maps generated by these sensors provide information that\ncan be employed to disambiguate the poses observed in two-dimensional images.\nThis work addresses the problem of 3D human pose estimation from depth maps\nemploying a Deep Learning approach. We propose a model, named Deep Depth Pose\n(DDP), which receives a depth map containing a person and a set of predefined\n3D prototype poses and returns the 3D position of the body joints of the\nperson. In particular, DDP is defined as a ConvNet that computes the specific\nweights needed to linearly combine the prototypes for the given input. We have\nthoroughly evaluated DDP on the challenging 'ITOP' and 'UBC3V' datasets, which\nrespectively depict realistic and synthetic samples, defining a new\nstate-of-the-art on them.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 12:31:38 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Marin-Jimenez", "Manuel J.", ""], ["Romero-Ramirez", "Francisco J.", ""], ["Mu\u00f1oz-Salinas", "Rafael", ""], ["Medina-Carnicer", "Rafael", ""]]}, {"id": "1807.05452", "submitter": "Alexandros Kogkas", "authors": "Ming-Yao Wang, Alexandros A. Kogkas, Ara Darzi, and George P. Mylonas", "title": "Free-View, 3D Gaze-Guided, Assistive Robotic System for Activities of\n  Daily Living", "comments": "7 Pages, 9 Figures, IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2018), Madrid, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients suffering from quadriplegia have limited body motion which prevents\nthem from performing daily activities. We have developed an assistive robotic\nsystem with an intuitive free-view gaze interface. The user's point of regard\nis estimated in 3D space while allowing free head movement and is combined with\nobject recognition and trajectory planning. This framework allows the user to\ninteract with objects using fixations. Two operational modes have been\nimplemented to cater for different eventualities. The automatic mode performs a\npre-defined task associated with a gaze-selected object, while the manual mode\nallows gaze control of the robot's end-effector position on the user's frame of\nreference. User studies reported effortless operation in automatic mode. A\nmanual pick and place task achieved a success rate of 100% on the users' first\nattempt.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 21:37:14 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 17:21:47 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Wang", "Ming-Yao", ""], ["Kogkas", "Alexandros A.", ""], ["Darzi", "Ara", ""], ["Mylonas", "George P.", ""]]}, {"id": "1807.05720", "submitter": "Araz Taeihagh", "authors": "Araz Taeihagh, Hazel Si Min Lim", "title": "Governing autonomous vehicles: emerging responses for safety, liability,\n  privacy, cybersecurity, and industry risks", "comments": "Transport Reviews, 2018", "journal-ref": null, "doi": "10.1080/01441647.2018.1494640", "report-no": null, "categories": "cs.CY cs.AI cs.CR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The benefits of autonomous vehicles (AVs) are widely acknowledged, but there\nare concerns about the extent of these benefits and AV risks and unintended\nconsequences. In this article, we first examine AVs and different categories of\nthe technological risks associated with them. We then explore strategies that\ncan be adopted to address these risks, and explore emerging responses by\ngovernments for addressing AV risks. Our analyses reveal that, thus far,\ngovernments have in most instances avoided stringent measures in order to\npromote AV developments and the majority of responses are non-binding and focus\non creating councils or working groups to better explore AV implications. The\nUS has been active in introducing legislations to address issues related to\nprivacy and cybersecurity. The UK and Germany, in particular, have enacted laws\nto address liability issues, other countries mostly acknowledge these issues,\nbut have yet to implement specific strategies. To address privacy and\ncybersecurity risks strategies ranging from introduction or amendment of non-AV\nspecific legislation to creating working groups have been adopted. Much less\nattention has been paid to issues such as environmental and employment risks,\nalthough a few governments have begun programmes to retrain workers who might\nbe negatively affected.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 08:18:57 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Taeihagh", "Araz", ""], ["Lim", "Hazel Si Min", ""]]}, {"id": "1807.05759", "submitter": "Meredydd Williams", "authors": "Meredydd Williams, Kelvin K. K. Yao, Jason R. C. Nurse", "title": "ToARist: An Augmented Reality Tourism App created through User-Centred\n  Design", "comments": "4 pages, 3 figures, Proceedings of the 31st British HCI Conference\n  (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through Augmented Reality (AR), virtual graphics can transform the physical\nworld. This offers benefits to mobile tourism, where points of interest (POIs)\ncan be annotated on a smartphone screen. Although several of these applications\nexist, usability issues can discourage adoption. User-centred design (UCD)\nsolicits frequent feedback, often contributing to usable products. While AR\nmock-ups have been constructed through UCD, we develop a novel and functional\ntourism app. We solicit requirements through a synthesis of domain analysis,\ntourist observation and semi-structured interviews. Through four rounds of\niterative development, users test and refine the app. The final product, dubbed\nToARist, is evaluated by 20 participants, who engage in a tourism task around a\nUK city. Users regard the system as usable, but find technical issues can\ndisrupt AR. We finish by reflecting on our design and critiquing the challenges\nof a strict user-centred methodology.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 09:44:54 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Williams", "Meredydd", ""], ["Yao", "Kelvin K. K.", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "1807.05906", "submitter": "Nalin Chhibber", "authors": "Nalin Chhibber, Rohail Syed, Mengqiu Teng, Joslin Goh, Kevyn\n  Collins-Thompson, Edith Law", "title": "Human Perception of Surprise: A User Study", "comments": "4 pages. Presented at Computational Surprise Workshop, SIGIR 2018\n  (Michigan)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how to engage users is a critical question in many\napplications. Previous research has shown that unexpected or astonishing events\ncan attract user attention, leading to positive outcomes such as engagement and\nlearning. In this work, we investigate the similarity and differences in how\npeople and algorithms rank the surprisingness of facts. Our crowdsourcing\nstudy, involving 106 participants, shows that computational models of surprise\ncan be used to artificially induce surprise in humans.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 15:02:38 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Chhibber", "Nalin", ""], ["Syed", "Rohail", ""], ["Teng", "Mengqiu", ""], ["Goh", "Joslin", ""], ["Collins-Thompson", "Kevyn", ""], ["Law", "Edith", ""]]}, {"id": "1807.06161", "submitter": "Homanga Bharadhwaj", "authors": "Homanga Bharadhwaj, Shruti Joshi", "title": "Explanations for Temporal Recommendations", "comments": "Accepted at the XAI Workshop in IJCAI/ECAI 2018", "journal-ref": "Homanga Bharadhwaj and Shruti Joshi. \"Explanations for Temporal\n  Recommendations\" IJCAI-18 Workshop on Explainable AI (XAI). 2018", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems are an integral part of Artificial Intelligence (AI)\nand have become increasingly important in the growing age of commercialization\nin AI. Deep learning (DL) techniques for recommendation systems (RS) provide\npowerful latent-feature models for effective recommendation but suffer from the\nmajor drawback of being non-interpretable. In this paper we describe a\nframework for explainable temporal recommendations in a DL model. We consider\nan LSTM based Recurrent Neural Network (RNN) architecture for recommendation\nand a neighbourhood-based scheme for generating explanations in the model. We\ndemonstrate the effectiveness of our approach through experiments on the\nNetflix dataset by jointly optimizing for both prediction accuracy and\nexplainability.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 00:38:40 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Bharadhwaj", "Homanga", ""], ["Joshi", "Shruti", ""]]}, {"id": "1807.06228", "submitter": "Yao Ming", "authors": "Yao Ming and Huamin Qu and Enrico Bertini", "title": "RuleMatrix: Visualizing and Understanding Classifiers with Rules", "comments": "Accepted by IEEE Conference of Visual Analytics Science and\n  Technology 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing adoption of machine learning techniques, there is a surge of\nresearch interest towards making machine learning systems more transparent and\ninterpretable. Various visualizations have been developed to help model\ndevelopers understand, diagnose, and refine machine learning models. However, a\nlarge number of potential but neglected users are the domain experts with\nlittle knowledge of machine learning but are expected to work with machine\nlearning systems. In this paper, we present an interactive visualization\ntechnique to help users with little expertise in machine learning to\nunderstand, explore and validate predictive models. By viewing the model as a\nblack box, we extract a standardized rule-based knowledge representation from\nits input-output behavior. We design RuleMatrix, a matrix-based visualization\nof rules to help users navigate and verify the rules and the black-box model.\nWe evaluate the effectiveness of RuleMatrix via two use cases and a usability\nstudy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 05:29:10 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Ming", "Yao", ""], ["Qu", "Huamin", ""], ["Bertini", "Enrico", ""]]}, {"id": "1807.06641", "submitter": "Bahador Saket", "authors": "Bahador Saket, Dominik Moritz, Halden Lin, Victor Dibia, Cagatay\n  Demiralp, Jeffrey Heer", "title": "Beyond Heuristics: Learning Visualization Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we describe a research agenda for deriving design principles\ndirectly from data. We argue that it is time to go beyond manually curated and\napplied visualization design guidelines. We propose learning models of\nvisualization design from data collected using graphical perception studies and\nbuild tools powered by the learned models. To achieve this vision, we need to\n1) develop scalable methods for collecting training data, 2) collect different\nforms of training data, 3) advance interpretability of machine learning models,\nand 4) develop adaptive models that evolve as more data becomes available.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 19:56:12 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 22:54:46 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Saket", "Bahador", ""], ["Moritz", "Dominik", ""], ["Lin", "Halden", ""], ["Dibia", "Victor", ""], ["Demiralp", "Cagatay", ""], ["Heer", "Jeffrey", ""]]}, {"id": "1807.06706", "submitter": "Jason R.C. Nurse Dr", "authors": "Louise M. Axon and Bushra Alahmadi and Jason R. C. Nurse and Michael\n  Goldsmith and Sadie Creese", "title": "Sonification in security operations centres: what do security\n  practitioners think?", "comments": null, "journal-ref": "Workshop on Usable Security (USEC) at the Network and Distributed\n  System Security (NDSS) Symposium 2018", "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Security Operations Centres (SOCs) security practitioners work using a\nrange of tools to detect and mitigate malicious computer-network activity.\nSonification, in which data is represented as sound, is said to have potential\nas an approach to addressing some of the unique challenges faced by SOCs. For\nexample, sonification has been shown to enable peripheral monitoring of\nprocesses, which could aid practitioners multitasking in busy SOCs. The\nperspectives of security practitioners on incorporating sonification into their\nactual working environments have not yet been examined, however. The aim of\nthis paper therefore is to address this gap by exploring attitudes to using\nsonification in SOCs. We report on the results of a study consisting of an\nonline survey (N=20) and interviews (N=21) with security practitioners working\nin a range of different SOCs. Our contribution is a refined appreciation of the\ncontexts in which sonification could aid in SOC working practice, and an\nunderstanding of the areas in which sonification may not be beneficial or may\neven be problematic.We also analyse the critical requirements for the design of\nsonification systems and their integration into the SOC setting. Our findings\nclarify insights into the potential benefits and challenges of introducing\nsonification to support work in this vital security-monitoring environment.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 23:26:14 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Axon", "Louise M.", ""], ["Alahmadi", "Bushra", ""], ["Nurse", "Jason R. C.", ""], ["Goldsmith", "Michael", ""], ["Creese", "Sadie", ""]]}, {"id": "1807.06729", "submitter": "Tahani Albalawi", "authors": "Tahani Albalawi, Kambiz Ghazinour, and Austin Melton", "title": "Security Mental Model: Cognitive map approach", "comments": "In the IEEE Proceedings of the 2017 International Conference on\n  Computational Science and Computational Intelligence, Cyber Warfare, Cyber\n  Defense, & Cyber Security (CSCI-ISCW), December 14-16, 2017, Las Vegas, USA,\n  6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security models have been designed to ensure data is accessed and used in\nproper manner according to the security policies. Unfortunately, human role in\ndesigning security models has been ignored. Human behavior relates to many\nsecurity breaches and plays a significant part in many security situations.In\nthis paper, we study users' security decision making toward security and\nusability through the mental model approach. To elicit and depict users'\nsecurity and usability mental models, crowd sourcing techniques and cognitive\nmap method are applied and we have performed an experiment to evaluate our\nfindings using Amazon MTurk.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 01:10:34 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Albalawi", "Tahani", ""], ["Ghazinour", "Kambiz", ""], ["Melton", "Austin", ""]]}, {"id": "1807.06913", "submitter": "Saad Aldoihi", "authors": "Saad Aldoihi, Omar Hammami (UEI)", "title": "Evaluation of CT Scan Usability for Saudi Arabian Users", "comments": null, "journal-ref": "International Conference on Computer, Information, and\n  Telecommunication Systems, Jul 2018, Colmar, France", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  - Like consumer electronic products, medical devices are becoming more\ncomplicated, with performance doubling every two years. With multiple commands\nand systems to negotiate, cognitive load can make it difficult for users to\nexecute commands effectively. In the case of medical devices, which use\nadvanced technology and require multidisciplinary inputs for design and\ndevelopment, cognitive workload is a significant factor. As these devices are\nvery expensive and operators require specialized training, effective and\neconomical methods are needed to evaluate the user experience. Heuristic\nevaluation is an effective method of identifying major usability problems and\nrelated issues. This study used heuristic evaluation to assess the usability of\na CT scan and associated physical and mental loads for Saudi Arabian users. The\nfindings indicate a gender difference in terms of consistency, flexibility, and\ndocument attributes, with a statistically significant gender difference in\nmental load.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 13:16:49 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Aldoihi", "Saad", "", "UEI"], ["Hammami", "Omar", "", "UEI"]]}, {"id": "1807.06924", "submitter": "Serhiy Semerikov", "authors": "Yevhenii O. Modlo, Serhiy O. Semerikov", "title": "Development of SageMath filter for Moodle", "comments": "11 pages, 3 figures, in Ukrainian", "journal-ref": "New computer technology 12 (2014) 233-243", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research goals: determine the characteristics of the development process,\ninstallation, configuration and usage of the filter SageMath for learning\nsupport system Moodle. Research objectives: to prove the feasibility of using\nMoodle system as a tool to support the process of competency formation in\ntechnical objects simulation of future bachelors in electromechanical\nengineering; to analyze existing support tools of technical objects simulation\nand to identify the ways of it's integration into Moodle; to describe the\nstructure and features of the software implementation of the new SageMath\nfilter for Moodle; to provide the guidance on installing and configuring\ndeveloped filter; to describe the examples of filter usage. Research subject:\ntext filter development process for learning support system Moodle to\nprocessing the commands of computer mathematics system SageMath. Research\nresults. Designed SageMath filter allows to execute the Sage code on the\nexternal SageMathCell public server, to view the execution results at the\nMoodle pages without reloading by using AJAX technology, to stave off XSS\nattacks and ready for use with Moodle. The main conclusions and\nrecommendations: 1. The perspective direction of learning environment\ndevelopment for bachelors in electromechanical engineering is the integration\nof learning support system Moodle and computer mathematics system SageMath. 2.\nAn effective tool for embedded a computer mathematics systems SageMath models\ninto Moodle is a text filter. The software engineering process for this filter\nis presented in the article. 3. Promising area of future research is the use of\na developed filter in the process of bachelor's in electromechanical\nengineering competencies in technical objects simulation by embedding into\nMoodle learning courses the interactive labs programmed in Sage.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 18:02:22 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Modlo", "Yevhenii O.", ""], ["Semerikov", "Serhiy O.", ""]]}, {"id": "1807.07047", "submitter": "Andr\\'e Lago", "authors": "Andr\\'e Sousa Lago, Hugo Sereno Ferreira", "title": "Conversation-Based Complex Event Management in Smart-Spaces", "comments": "20 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart space management can be done in many ways. On one hand, there are\nconversational assistants such as the Google Assistant or Amazon Alexa that\nenable users to comfortably interact with smart spaces with only their voice,\nbut these have limited functionality and are usually limited to simple\ncommands. On the other hand, there are visual interfaces such as IBM's Node-RED\nthat enable complex features and dependencies between different devices.\nHowever, these are limited since they require users to have a technical\nknowledge of how the smart devices work and the system's interface is more\ncomplicated and harder to use since they require a computer. This project\nproposes a new conversational assistant - Jarvis - that combines the ease of\nuse of current assistants with the operational complexity of the visual\nplatforms. The goal of Jarvis is to make it easier to manage smart spaces by\nproviding intuitive commands and useful features. Jarvis integrates with\nalready existing user interfaces such as the Google Assistant, Slack or\nFacebook Messenger, making it very easy to integrate with existing systems.\nJarvis also provides an innovative feature - causality queries - that enable\nusers to ask it why something happened. For example, a user can ask \"why did\nthe light turn on?\" to understand how the system works.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 17:23:02 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Lago", "Andr\u00e9 Sousa", ""], ["Ferreira", "Hugo Sereno", ""]]}, {"id": "1807.07173", "submitter": "Qiang Hao", "authors": "Qiang Hao, April Galyardt, Bradley Barnes, Robert Maribe Branch, Ewan\n  Wright", "title": "Automatic Identification of Ineffective Online Student Questions in\n  Computing Education", "comments": null, "journal-ref": "Proceedings of IEEE Frontiers in Education (FIE' 18), San Jose,\n  CA, 2018", "doi": "10.1109/FIE.2018.8658642", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Research Full Paper explores automatic identification of ineffective\nlearning questions in the context of large-scale computer science classes. The\nimmediate and accurate identification of ineffective learning questions opens\nthe door to possible automated facilitation on a large scale, such as alerting\nlearners to revise questions and providing adaptive question revision\nsuggestions. To achieve this, 983 questions were collected from a question &\nanswer platform implemented by an introductory programming course over three\nsemesters in a large research university in the Southeastern United States.\nQuestions were firstly manually classified into three hierarchical categories:\n1) learning-irrelevant questions, 2) effective learning-relevant questions, 3)\nineffective learningrelevant questions. The inter-rater reliability of the\nmanual classification (Cohen's Kappa) was .88. Four different machine learning\nalgorithms were then used to automatically classify the questions, including\nNaive Bayes Multinomial, Logistic Regression, Support Vector Machines, and\nBoosted Decision Tree. Both flat and single path strategies were explored, and\nthe most effective algorithms under both strategies were identified and\ndiscussed. This study contributes to the automatic determination of learning\nquestion quality in computer science, and provides evidence for the feasibility\nof automated facilitation of online question & answer in large scale computer\nscience classes.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 22:21:19 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 18:10:07 GMT"}, {"version": "v3", "created": "Sun, 10 Mar 2019 19:47:53 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Hao", "Qiang", ""], ["Galyardt", "April", ""], ["Barnes", "Bradley", ""], ["Branch", "Robert Maribe", ""], ["Wright", "Ewan", ""]]}, {"id": "1807.07255", "submitter": "Wei Wu", "authors": "Can Xu, Wei Wu, Yu Wu", "title": "Towards Explainable and Controllable Open Domain Dialogue Generation\n  with Dialogue Acts", "comments": "The paper is also available on OpenReview of ICLR 2018\n  (https://openreview.net/forum?id=Bym0cU1CZ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study open domain dialogue generation with dialogue acts designed to\nexplain how people engage in social chat. To imitate human behavior, we propose\nmanaging the flow of human-machine interactions with the dialogue acts as\npolicies. The policies and response generation are jointly learned from\nhuman-human conversations, and the former is further optimized with a\nreinforcement learning approach. With the dialogue acts, we achieve significant\nimprovement over state-of-the-art methods on response quality for given\ncontexts and dialogue length in both machine-machine simulation and\nhuman-machine conversation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 06:41:05 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 01:45:32 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Xu", "Can", ""], ["Wu", "Wei", ""], ["Wu", "Yu", ""]]}, {"id": "1807.08074", "submitter": "Stephanie Lukin", "authors": "Stephanie M. Lukin, Felix Gervits, Cory J. Hayes, Anton Leuski, Pooja\n  Moolchandani, John G. Rogers III, Carlos Sanchez Amaro, Matthew Marge, Clare\n  R. Voss, David Traum", "title": "ScoutBot: A Dialogue System for Collaborative Navigation", "comments": "Originally published in the Proceedings of the Association for\n  Computational Linguistics (ACL) 2018, System Demonstrations, 93-98", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ScoutBot is a dialogue interface to physical and simulated robots that\nsupports collaborative exploration of environments. The demonstration will\nallow users to issue unconstrained spoken language commands to ScoutBot.\nScoutBot will prompt for clarification if the user's instruction needs\nadditional input. It is trained on human-robot dialogue collected from\nWizard-of-Oz experiments, where robot responses were initiated by a human\nwizard in previous interactions. The demonstration will show a simulated ground\nrobot (Clearpath Jackal) in a simulated environment supported by ROS (Robot\nOperating System).\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 03:12:36 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Lukin", "Stephanie M.", ""], ["Gervits", "Felix", ""], ["Hayes", "Cory J.", ""], ["Leuski", "Anton", ""], ["Moolchandani", "Pooja", ""], ["Rogers", "John G.", "III"], ["Amaro", "Carlos Sanchez", ""], ["Marge", "Matthew", ""], ["Voss", "Clare R.", ""], ["Traum", "David", ""]]}, {"id": "1807.08076", "submitter": "Stephanie Lukin", "authors": "Stephanie M. Lukin, Kimberly A. Pollard, Claire Bonial, Matthew Marge,\n  Cassidy Henry, Ron Arstein, David Traum, Clare R. Voss", "title": "Consequences and Factors of Stylistic Differences in Human-Robot\n  Dialogue", "comments": "Originally published in the Proceedings of the 19th Annual Meeting of\n  the Special Interest Group on Discourse and Dialogue (SIGDIAL), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper identifies stylistic differences in instruction-giving observed in\na corpus of human-robot dialogue. Differences in verbosity and structure (i.e.,\nsingle-intent vs. multi-intent instructions) arose naturally without\nrestrictions or prior guidance on how users should speak with the robot.\nDifferent styles were found to produce different rates of miscommunication, and\ncorrelations were found between style differences and individual user\nvariation, trust, and interaction experience with the robot. Understanding\npotential consequences and factors that influence style can inform design of\ndialogue systems that are robust to natural variation from human users.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 03:21:56 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Lukin", "Stephanie M.", ""], ["Pollard", "Kimberly A.", ""], ["Bonial", "Claire", ""], ["Marge", "Matthew", ""], ["Henry", "Cassidy", ""], ["Arstein", "Ron", ""], ["Traum", "David", ""], ["Voss", "Clare R.", ""]]}, {"id": "1807.08189", "submitter": "Andr\\'es Monroy-Hern\\'andez", "authors": "Ali Alkhatib, Justin Cranshaw, Andr\\'es Monroy-Hern\\'andez", "title": "Laying the Groundwork for a Worker-Centric Peer Economy", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSR-TR-2016-50", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The \"gig economy\" has transformed the ways in which people work, but in many\nways these markets stifle the growth of workers and the autonomy and\nprotections that workers have grown to expect. We explored the viability of a\n\"worker centric peer economy\"--a system wherein workers benefit as well as\nconsumers-- and conducted ethnographic field work across fields ranging from\ndomestic labor to home health care. We discovered seven facets that system\ndesigners ought to consider when designing a labor market for \"gig workers,\"\nconsisting principally of the following: constructive feedback, assigning work\nfairly, managing customer expectations, protecting vulnerable workers,\nreconciling worker identities, assessing worker qualifications, & communicating\nworker quality. We discuss these considerations and provide guidance toward the\ndesign of a mutually beneficial market for gig workers.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 18:22:56 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Alkhatib", "Ali", ""], ["Cranshaw", "Justin", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""]]}, {"id": "1807.08775", "submitter": "Charlie Hewitt", "authors": "Charlie Hewitt and Hatice Gunes", "title": "CNN-based Facial Affect Analysis on Mobile Devices", "comments": "7 pages, 2 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the design, deployment and evaluation of Convolutional\nNeural Network (CNN) architectures for facial affect analysis on mobile\ndevices. Unlike traditional CNN approaches, models deployed to mobile devices\nmust minimise storage requirements while retaining high performance. We\ntherefore propose three variants of established CNN architectures and\ncomparatively evaluate them on a large, in-the-wild benchmark dataset of facial\nimages. Our results show that the proposed architectures retain similar\nperformance to the dataset baseline while minimising storage requirements:\nachieving 58% accuracy for eight-class emotion classification and average RMSE\nof 0.39 for valence/arousal prediction. To demonstrate the feasibility of\ndeploying these models for real-world applications, we implement a music\nrecommendation interface based on predicted user affect. Although the CNN\nmodels were not trained in the context of music recommendation, our case study\nshows that: (i) the trained models achieve similar prediction performance to\nthe benchmark dataset, and (ii) users tend to positively rate the song\nrecommendations provided by the interface. Average runtime of the deployed\nmodels on an iPhone 6S equates to ~45 fps, suggesting that the proposed\narchitectures are also well suited for real-time deployment on video streams.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 18:19:57 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Hewitt", "Charlie", ""], ["Gunes", "Hatice", ""]]}, {"id": "1807.09069", "submitter": "Cun Li", "authors": "Cun Li, Jun Hu, Bart Hengeveld, Caroline Hummels", "title": "Slots-Memento : A System Facilitating Intergenerational Story Sharing\n  and Preservation of Family Mementos", "comments": "Slots-Memento : A System Facilitating Intergenerational Story Sharing\n  and Preservation of Family Mementos", "journal-ref": "The International Journal of Multimedia & Its Applications (IJMA)\n  Vol.10, No.1/2/3, June 2018", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Family mementos document events shaping family life, telling a story within\nand between family members. The elderly collected some mementos for children,\nbut never recorded stories related to those objects. In this paper, in order to\nunderstand the status quo of memento storytelling and sharing of elderly\npeople, contextual inquiry was conducted, which further helped us to identify\ndesign opportunities and requirements. Resulting design was defined after\nbrainstorm and user consultation, which was Slots- Memento, a system consisting\na slot machine-like device used by the elderly and a flash drive used by the\nyoung. The Slots machine-like device utilizes with the metaphor of slots\nmachine, which integrates functions of memento photo displaying, story\nrecording, and preservation. In the flash disk, the young could copy memento\nphotos to it. The system aims to facilitate memento story sharing and\npreservation within family members. Preliminary evaluation and user test were\nconducted in evaluation section, the results showed that Slots-Memento was\nunderstood and accepted by the elderly users. Photos of mementos were easy to\nrecall memories. It enabled the elderly people to be aware of the stories of\nthe family mementos, as well as aroused their desire to share them with family\nmembers. Related research methodology includes contextual inquiry,\nbrainstorming, prototyping, scenario creation, and user test.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 12:39:15 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Li", "Cun", ""], ["Hu", "Jun", ""], ["Hengeveld", "Bart", ""], ["Hummels", "Caroline", ""]]}, {"id": "1807.09074", "submitter": "Donlaporn Srifar", "authors": "Donlaporn Srifar", "title": "360 virtual reality travel media for elderly", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The objectives of this qualitative research were to study the model of\n360-degree virtual reality travel media, to compare appropriateness of moving\n360-degree virtual reality travel media for elderly with both still and moving\ncameras, and to study satisfaction of elderly in 360-degree virtual reality\ntravel media. The informants are 10 elders with age above and equal to 60 years\nold who live in Bangkok regardless of genders. Data were collected through\ndocuments, detailed interview, and non-participant observation of elders to\n360-degree virtual reality travel media with data triangulation. 1. From the\nliterature review 1. The creation must primarily consider the target consumers\non their physics 2. must have fluidity on changing the view of the camera by\ncalibrating with the target consumers 3. The image displayed must not move too\nfast to prevent dizziness and improve the comfort of the target consumers. It\nis also highly recommended to implement a function to customize the movement\nrate for the customer. 2. From the in-depth interview with the target\nconsumers, the results found that 1. They are worried and not used to the\nequipment 2. They have no idea where to look 3. They feel excited 5. They are\ninterested in what is more to see 6. They feel like they did actually travel\nthere 7. They can hear the sound clearly 8. They do not like when the camera is\nmoving and find still camera more comfortable. 3. From the non-participant\nobservation and found that they are always excited, laughed, and smiled when\nwatching the media. They always asked where this is and why they cannot see\nanything when turning around.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 12:51:25 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Srifar", "Donlaporn", ""]]}, {"id": "1807.09490", "submitter": "Philipp Jordan", "authors": "Philipp Jordan", "title": "Investigating the Intersection of Science Fiction, Human-Computer\n  Interaction and Computer Science Research", "comments": "v1: 5 pages, 0 figures, 0 tables, in review at HICSS 52 (doctoral\n  consortium)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines ongoing dissertation research located in the intersection\nof science fiction, human-computer interaction and computer science. Through an\ninterdisciplinary perspective, drawing from fields such as human-computer\ninteraction, film theory and studies of science and technology, qualitative and\nquantitative content analysis techniques are used to contextually analyze\nexpressions of science fiction in peer-reviewed computer science research\nrepositories, such as the ACM or IEEE Xplore Digital Libraries. This paper\nconcisely summarizes and introduces the relationship of science fiction and\ncomputer science research and presents the research questions, aims and\nimplications in addition to prior work and study methodology. In the latter\npart of this work-in-progress report, preliminary results, current limitations,\nfuture work and a post-dissertation trajectory are outlined.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 09:02:02 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Jordan", "Philipp", ""]]}, {"id": "1807.09739", "submitter": "Alireza Karduni", "authors": "Alireza Karduni, Isaac Cho, Ryan Wesslen, Sashank Santhanam, Svitlana\n  Volkova, Dustin Arendt, Samira Shaikh, Wenwen Dou", "title": "Vulnerable to Misinformation? Verifi!", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Verifi2, a visual analytic system to support the investigation of\nmisinformation on social media. On the one hand, social media platforms empower\nindividuals and organizations by democratizing the sharing of information. On\nthe other hand, even well-informed and experienced social media users are\nvulnerable to misinformation. To address the issue, various models and studies\nhave emerged from multiple disciplines to detect and understand the effects of\nmisinformation. However, there is still a lack of intuitive and accessible\ntools that help social media users distinguish misinformation from verified\nnews. In this paper, we present Verifi2, a visual analytic system that uses\nstate-of-the-art computational methods to highlight salient features from text,\nsocial network, and images. By exploring news on a source level through\nmultiple coordinated views in Verifi2, users can interact with the complex\ndimensions that characterize misinformation and contrast how real and\nsuspicious news outlets differ on these dimensions. To evaluate Verifi2, we\nconduct interviews with experts in digital media, journalism, education,\npsychology, and computing who study misinformation. Our interviews show\npromising potential for Verifi2 to serve as an educational tool on\nmisinformation. Furthermore, our interview results highlight the complexity of\nthe problem of combating misinformation and call for more work from the\nvisualization community.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 17:36:25 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 18:37:34 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Karduni", "Alireza", ""], ["Cho", "Isaac", ""], ["Wesslen", "Ryan", ""], ["Santhanam", "Sashank", ""], ["Volkova", "Svitlana", ""], ["Arendt", "Dustin", ""], ["Shaikh", "Samira", ""], ["Dou", "Wenwen", ""]]}, {"id": "1807.09825", "submitter": "Nikhil Churamani", "authors": "Nikhil Churamani and Alexander Sutherland and Pablo Barros", "title": "An Affective Robot Companion for Assisting the Elderly in a Cognitive\n  Game Scenario", "comments": "Proceedings of the Workshop on Intelligent Assistive Computing, IEEE\n  World Congress on Computational Intelligence (WCCI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Being able to recognize emotions in human users is considered a highly\ndesirable trait in Human-Robot Interaction (HRI) scenarios. However, most\ncontemporary approaches rarely attempt to apply recognized emotional features\nin an active manner to modulate robot decision-making and dialogue for the\nbenefit of the user. In this position paper, we propose a method of\nincorporating recognized emotions into a Reinforcement Learning (RL) based\ndialogue management module that adapts its dialogue responses in order to\nattempt to make cognitive training tasks, like the 2048 Puzzle Game, more\nenjoyable for the users.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 02:27:19 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Churamani", "Nikhil", ""], ["Sutherland", "Alexander", ""], ["Barros", "Pablo", ""]]}, {"id": "1807.09837", "submitter": "Derrik Asher", "authors": "Mark Mittrick, John Richardson, Derrik E. Asher, Alex James, Timothy\n  Hanratty", "title": "Using the Value of Information (VoI) Metric to Improve Sensemaking", "comments": "International Command and Control Research and Technology Symposium\n  (ICCRTS - 2017), 9 pages, 3 figures, 3 tables, 1 equation", "journal-ref": "US Army Research Laboratory Aberdeen Proving Ground United States,\n  2018", "doi": null, "report-no": "ARL-TR-8451", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensemaking is the cognitive process of extracting information, creating\nschemata from knowledge, making decisions from those schemata, and inferring\nconclusions. Human analysts are essential to exploring and quantifying this\nprocess, but they are limited by their inability to process the volume,\nvariety, velocity, and veracity of data. Visualization tools are essential for\nhelping this human-computer interaction. For example, analytical tools that use\ngraphical linknode visualization can help sift through vast amounts of\ninformation. However, assisting the analyst in making connections with visual\ntools can be challenging if the information is not presented in an intuitive\nmanner.\n  Experimentally, it has been shown that analysts increase the number of\nhypotheses formed if they use visual analytic capabilities. Exploring multiple\nperspectives could increase the diversity of those hypotheses, potentially\nminimizing cognitive biases. In this paper, we discuss preliminary research\nresults that indicate an improvement in sensemaking over the traditional\nlink-node visualization tools by incorporating an annotation enhancement that\ndifferentiates links connecting nodes. This enhancement assists by providing a\nvisual cue, which represents the perceived value of reported information. We\nconclude that this improved sensemaking occurs because of the removal of the\nlimitations of mentally consolidating, weighing, and highlighting data. This\nstudy aims to investigate whether line thickness can be used as a valid\nrepresentation of VoI.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 20:22:19 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Mittrick", "Mark", ""], ["Richardson", "John", ""], ["Asher", "Derrik E.", ""], ["James", "Alex", ""], ["Hanratty", "Timothy", ""]]}, {"id": "1807.09886", "submitter": "Pegah Karimi", "authors": "Pegah Karimi, Kazjon Grace, Mary Lou Maher, Nicholas Davis", "title": "Evaluating Creativity in Computational Co-Creative Systems", "comments": "9 pages, 2 Figures, 1 Table, Accepted in ICCC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a framework for evaluating creativity in co-creative\nsystems: those that involve computer programs collaborating with human users on\ncreative tasks. We situate co-creative systems within a broader context of\ncomputational creativity and explain the unique qualities of these systems. We\npresent four main questions that can guide evaluation in co-creative systems:\nWho is evaluating the creativity, what is being evaluated, when does evaluation\noccur and how the evaluation is performed. These questions provide a framework\nfor comparing how existing co-creative systems evaluate creativity, and we\napply them to examples of co-creative systems in art, humor, games and\nrobotics. We conclude that existing co-creative systems tend to focus on\nevaluating the user experience. Adopting evaluation methods from autonomous\ncreative systems may lead to co-creative systems that are self-aware and\nintentional.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 22:38:16 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Karimi", "Pegah", ""], ["Grace", "Kazjon", ""], ["Maher", "Mary Lou", ""], ["Davis", "Nicholas", ""]]}, {"id": "1807.10444", "submitter": "Qiyu Kang", "authors": "Qiyu Kang, Wee Peng Tay", "title": "Task Recommendation in Crowdsourcing Based on Learning Preferences and\n  Reliabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workers participating in a crowdsourcing platform can have a wide range of\nabilities and interests. An important problem in crowdsourcing is the task\nrecommendation problem, in which tasks that best match a particular worker's\npreferences and reliabilities are recommended to that worker. A task\nrecommendation scheme that assigns tasks more likely to be accepted by a worker\nwho is more likely to complete it reliably results in better performance for\nthe task requester. Without prior information about a worker, his preferences\nand reliabilities need to be learned over time. In this paper, we propose a\nmulti-armed bandit (MAB) framework to learn a worker's preferences and his\nreliabilities for different categories of tasks. However, unlike the classical\nMAB problem, the reward from the worker's completion of a task is unobservable.\nWe therefore include the use of gold tasks (i.e., tasks whose solutions are\nknown \\emph{a priori} and which do not produce any rewards) in our task\nrecommendation procedure. Our model could be viewed as a new variant of MAB, in\nwhich the random rewards can only be observed at those time steps where gold\ntasks are used, and the accuracy of estimating the expected reward of\nrecommending a task to a worker depends on the number of gold tasks used. We\nshow that the optimal regret is $O(\\sqrt{n})$, where $n$ is the number of tasks\nrecommended to the worker. We develop three task recommendation strategies to\ndetermine the number of gold tasks for different task categories, and show that\nthey are order optimal. Simulations verify the efficiency of our approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 05:46:38 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Kang", "Qiyu", ""], ["Tay", "Wee Peng", ""]]}, {"id": "1807.10561", "submitter": "Mickey Li Mr", "authors": "Mickey Li, Noyan Songur, Pavel Orlov, Stefan Leutenegger, A Aldo\n  Faisal", "title": "Towards an Embodied Semantic Fovea: Semantic 3D scene reconstruction\n  from ego-centric eye-tracker videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating the physical environment is essential for a complete\nunderstanding of human behavior in unconstrained every-day tasks. This is\nespecially important in ego-centric tasks where obtaining 3 dimensional\ninformation is both limiting and challenging with the current 2D video analysis\nmethods proving insufficient. Here we demonstrate a proof-of-concept system\nwhich provides real-time 3D mapping and semantic labeling of the local\nenvironment from an ego-centric RGB-D video-stream with 3D gaze point\nestimation from head mounted eye tracking glasses. We augment existing work in\nSemantic Simultaneous Localization And Mapping (Semantic SLAM) with collected\ngaze vectors. Our system can then find and track objects both inside and\noutside the user field-of-view in 3D from multiple perspectives with reasonable\naccuracy. We validate our concept by producing a semantic map from images of\nthe NYUv2 dataset while simultaneously estimating gaze position and gaze\nclasses from recorded gaze data of the dataset images.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 12:51:36 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Li", "Mickey", ""], ["Songur", "Noyan", ""], ["Orlov", "Pavel", ""], ["Leutenegger", "Stefan", ""], ["Faisal", "A Aldo", ""]]}, {"id": "1807.10575", "submitter": "Yingruo Fan", "authors": "Yingruo Fan, Jacqueline C.K. Lam and Victor O.K. Li", "title": "Multi-Region Ensemble Convolutional Neural Network for Facial Expression\n  Recognition", "comments": "10pages, 5 figures, Accepted by ICANN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial expressions play an important role in conveying the emotional states\nof human beings. Recently, deep learning approaches have been applied to image\nrecognition field due to the discriminative power of Convolutional Neural\nNetwork (CNN). In this paper, we first propose a novel Multi-Region Ensemble\nCNN (MRE-CNN) framework for facial expression recognition, which aims to\nenhance the learning power of CNN models by capturing both the global and the\nlocal features from multiple human face sub-regions. Second, the weighted\nprediction scores from each sub-network are aggregated to produce the final\nprediction of high accuracy. Third, we investigate the effects of different\nsub-regions of the whole face on facial expression recognition. Our proposed\nmethod is evaluated based on two well-known publicly available facial\nexpression databases: AFEW 7.0 and RAF-DB, and has been shown to achieve the\nstate-of-the-art recognition accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 02:50:35 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Fan", "Yingruo", ""], ["Lam", "Jacqueline C. K.", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1807.10659", "submitter": "Serhiy Semerikov", "authors": "Yevhenii O. Modlo, Yuliia V. Yechkalo, Serhiy O. Semerikov, Viktoriia\n  V. Tkachuk", "title": "Using technology of augmented reality in a mobile-based learning\n  environment of the higher educational institution", "comments": "8 pages, 5 figures, in Ukrainian", "journal-ref": "Naukovi zapysky, Seriia: Problemy metodyky fizyko-matematychnoi i\n  tekhnolohichnoi osvity 11 (2017) 93-100", "doi": null, "report-no": null, "categories": "physics.ed-ph cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The definition of the augmented reality concept is based on the analysis of\nscientific publications. It is noted that online experiments with augmented\nreality provide students with the opportunity to observe and describe the\noperation with real systems by changing their parameters, and also partially\nreplace experimental installations with objects of augmented reality. The\nscheme for realizing the augmented reality is considered. The possibilities of\nworking with augmented reality objects in teaching physics is highlighted. It\nis indicated that the use of the augmented reality tools allows to increase the\nrealness of the research; provides emotional and cognitive experience, helps\nattract students to systematic training; provides correct information about the\ninstallation in the process of experimentation; creates new ways of\nrepresenting real objects in the learning process.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 12:36:54 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Modlo", "Yevhenii O.", ""], ["Yechkalo", "Yuliia V.", ""], ["Semerikov", "Serhiy O.", ""], ["Tkachuk", "Viktoriia V.", ""]]}, {"id": "1807.10986", "submitter": "Shadan Golestan", "authors": "Shadan Golestan, Pegah Soleiman, Hadi Moradi", "title": "A Comprehensive Review of Technologies Used for Screening, Assessment,\n  and Rehabilitation of Autism Spectrum Disorder", "comments": "12 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism Spectrum Disorder (ASD) is an umbrella term for a wide range of\ndevelopmental disorders. For the past two decades, researchers proposed the use\nof various technologies in order to tackle specific symptoms of the disorder.\nAlthough there exist many literature reviews about screening, assessment, and\nrehabilitation of ASD, no comprehensive survey of types of technologies in all\ndefined symptoms of ASD has been presented. Therefore, in this paper a\ncomprehensive survey of previous studies has been presented in which the\nstudies are classified into three main categories, and several sub-categories,\nand three main technologies. An analysis of the number of studies in each\ncategory and sub-category is given to help researchers decide on areas which\nneed further investigation. The analysis show that the majority of studies fall\ninto the software-based systems technology category. Finally, a brief review of\nstudies in each category of ASD is presented for each type of technology. As a\nresult, this paper also helps researchers to obtain an overview of the typical\nmethods of using a specific technology in ASD screening, assessment, and\nrehabilitation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 00:31:26 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Golestan", "Shadan", ""], ["Soleiman", "Pegah", ""], ["Moradi", "Hadi", ""]]}, {"id": "1807.11096", "submitter": "Tian Zhou", "authors": "Tian Zhou and Juan P. Wachs", "title": "Spiking Neural Networks for Early Prediction in Human Robot\n  Collaboration", "comments": "Under review for journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Turn-Taking Spiking Neural Network (TTSNet), which\nis a cognitive model to perform early turn-taking prediction about human or\nagent's intentions. The TTSNet framework relies on implicit and explicit\nmultimodal communication cues (physical, neurological and physiological) to be\nable to predict when the turn-taking event will occur in a robust and\nunambiguous fashion. To test the theories proposed, the TTSNet framework was\nimplemented on an assistant robotic nurse, which predicts surgeon's turn-taking\nintentions and delivers surgical instruments accordingly. Experiments were\nconducted to evaluate TTSNet's performance in early turn-taking prediction. It\nwas found to reach a F1 score of 0.683 given 10% of completed action, and a F1\nscore of 0.852 at 50% and 0.894 at 100% of the completed action. This\nperformance outperformed multiple state-of-the-art algorithms, and surpassed\nhuman performance when limited partial observation is given (< 40%). Such early\nturn-taking prediction capability would allow robots to perform collaborative\nactions proactively, in order to facilitate collaboration and increase team\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 18:33:22 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Zhou", "Tian", ""], ["Wachs", "Juan P.", ""]]}, {"id": "1807.11123", "submitter": "Jingbo Zhao", "authors": "Jingbo Zhao, Robert S. Allison, Margarita Vinnikov, Sion Jennings", "title": "The Effects of Visual and Control Latency on Piloting a Quadcopter using\n  a Head-Mounted Display", "comments": "IEEE SMC 2018, Miyazaki, Japan, Oct 7th - Oct 10th, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has proposed teleoperation of robotic and aerial vehicles\nusing head motion tracked by a head-mounted display (HMD). First-person views\nof the vehicles are usually captured by onboard cameras and presented to users\nthrough the display panels of HMDs. This provides users with a direct,\nimmersive and intuitive interface for viewing and control. However, a typically\noverlooked factor in such designs is the latency introduced by the vehicle\ndynamics. As head motion is coupled with visual updates in such applications,\nvisual and control latency always exists between the issue of control commands\nby head movements and the visual feedback received at the completion of the\nattitude adjustment. This causes a discrepancy between the intended motion, the\nvestibular cue and the visual cue and may potentially result in simulator\nsickness. No research has been conducted on how various levels of visual and\ncontrol latency introduced by dynamics in robots or aerial vehicles affect\nusers' performance and the degree of simulator sickness elicited. Thus, it is\nuncertain how much performance is degraded by latency and whether such designs\nare comfortable from the perspective of users. To address these issues, we\nstudied a prototyped scenario of a head motion controlled quadcopter using an\nHMD. We present a virtual reality (VR) paradigm to systematically assess the\neffects of visual and control latency in simulated drone control scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 23:18:54 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 13:01:56 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 01:10:34 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Zhao", "Jingbo", ""], ["Allison", "Robert S.", ""], ["Vinnikov", "Margarita", ""], ["Jennings", "Sion", ""]]}, {"id": "1807.11154", "submitter": "Benjamin Newman", "authors": "Benjamin A. Newman, Reuben M. Aronson, Siddartha S. Srinivasa, Kris\n  Kitani, Henny Admoni", "title": "HARMONIC: A Multimodal Dataset of Assistive Human-Robot Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Human And Robot Multimodal Observations of Natural Interactive\nCollaboration (HARMONIC) data set. This is a large multimodal data set of human\ninteractions with a robotic arm in a shared autonomy setting designed to\nimitate assistive eating. The data set provides human, robot, and environmental\ndata views of twenty-four different people engaged in an assistive eating task\nwith a 6 degree-of-freedom (DOF) robot arm. From each participant, we recorded\nvideo of both eyes, egocentric video from a head-mounted camera, joystick\ncommands, electromyography from the forearm used to operate the joystick, third\nperson stereo video, and the joint positions of the 6 DOF robot arm. Also\nincluded are several features that come as a direct result of these recordings,\nsuch as eye gaze projected onto the egocentric video, body pose, hand pose, and\nfacial keypoints. These data streams were collected specifically because they\nhave been shown to be closely related to human mental states and intention.\nThis data set could be of interest to researchers studying intention\nprediction, human mental state modeling, and shared autonomy. Data streams are\nprovided in a variety of formats such as video and human-readable CSV and YAML\nfiles.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 02:55:18 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 00:26:33 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Newman", "Benjamin A.", ""], ["Aronson", "Reuben M.", ""], ["Srinivasa", "Siddartha S.", ""], ["Kitani", "Kris", ""], ["Admoni", "Henny", ""]]}, {"id": "1807.11322", "submitter": "Anna Spagnolli", "authors": "Anna Spagnolli, Diletta Mora, Matteo Fanchin, Valeria Orso, Luciano\n  Gamberini", "title": "Embracing ambivalence in studying technology acceptance: A qualitative\n  study on automated visual software for live music performance", "comments": "25 pages,2 figures, 2 tables", "journal-ref": null, "doi": "10.1145/3313831.3376463", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the live music entertainment sector does not directly fuel the\ncurrent debate on automation, it might harbor positions that resonate with it.\nIn this paper we study a prototype software application helping DJs and VJs to\naccurately manage and even automate the synchronization of visuals with music\nduring amateur or professional live performance. The goal of the study was to\nunravel VJs' and DJs' ambivalent positions about this software. We\npreliminarily investigated VJs' and DJs' perception of their sector of activity\nwith seven face-to-face interviews and an online survey (N = 102); then, we\nasked DJs and VJs (N = 25) for their opinions about our prototype software\napplication. Four core controversies were identified in their answers, along\nwith a set of arguments mobilized to take side on them. The advantages of\nfocusing on ambivalence and argumentation when studying users' response to new\nmedia are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 12:46:18 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 18:44:06 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Spagnolli", "Anna", ""], ["Mora", "Diletta", ""], ["Fanchin", "Matteo", ""], ["Orso", "Valeria", ""], ["Gamberini", "Luciano", ""]]}, {"id": "1807.11689", "submitter": "Muhao Chen", "authors": "Muhao Chen, Changping Meng, Gang Huang and Carlo Zaniolo", "title": "Neural Article Pair Modeling for Wikipedia Sub-article Matching", "comments": "ECML-PKDD 2018. 16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, editors tend to separate different subtopics of a long Wiki-pedia\narticle into multiple sub-articles. This separation seeks to improve human\nreadability. However, it also has a deleterious effect on many Wikipedia-based\ntasks that rely on the article-as-concept assumption, which requires each\nentity (or concept) to be described solely by one article. This underlying\nassumption significantly simplifies knowledge representation and extraction,\nand it is vital to many existing technologies such as automated knowledge base\nconstruction, cross-lingual knowledge alignment, semantic search and data\nlineage of Wikipedia entities. In this paper we provide an approach to match\nthe scattered sub-articles back to their corresponding main-articles, with the\nintent of facilitating automated Wikipedia curation and processing. The\nproposed model adopts a hierarchical learning structure that combines multiple\nvariants of neural document pair encoders with a comprehensive set of explicit\nfeatures. A large crowdsourced dataset is created to support the evaluation and\nfeature extraction for the task. Based on the large dataset, the proposed model\nachieves promising results of cross-validation and significantly outperforms\nprevious approaches. Large-scale serving on the entire English Wikipedia also\nproves the practicability and scalability of the proposed model by effectively\nextracting a vast collection of newly paired main and sub-articles.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 07:19:36 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 21:30:17 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Chen", "Muhao", ""], ["Meng", "Changping", ""], ["Huang", "Gang", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1807.11752", "submitter": "PAblo Ortega", "authors": "Pablo Ortega and Cedric Colas and Aldo Faisal", "title": "Compact Convolutional Neural Networks for Multi-Class, Personalised,\n  Closed-Loop EEG-BCI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many people suffering from motor disabilities, assistive devices\ncontrolled with only brain activity are the only way to interact with their\nenvironment. Natural tasks often require different kinds of interactions,\ninvolving different controllers the user should be able to select in a\nself-paced way. We developed a Brain-Computer Interface (BCI) allowing users to\nswitch between four control modes in a self-paced way in real-time. Since the\nsystem is devised to be used in domestic environments in a user-friendly way,\nwe selected non-invasive electroencephalographic (EEG) signals and\nconvolutional neural networks (CNNs), known for their ability to find the\noptimal features in classification tasks. We tested our system using the\nCybathlon BCI computer game, which embodies all the challenges inherent to\nreal-time control. Our preliminary results show that an efficient architecture\n(SmallNet), with only one convolutional layer, can classify 4 mental activities\nchosen by the user. The BCI system is run and validated online. It is kept\nup-to-date through the use of newly collected signals along playing, reaching\nan online accuracy of 47.6% where most approaches only report results obtained\noffline. We found that models trained with data collected online better\npredicted the behaviour of the system in real-time. This suggests that similar\n(CNN based) offline classifying methods found in the literature might\nexperience a drop in performance when applied online. Compared to our previous\ndecoder of physiological signals relying on blinks, we increased by a factor 2\nthe amount of states among which the user can transit, bringing the opportunity\nfor finer control of specific subtasks composing natural grasping in a\nself-paced way. Our results are comparable to those shown at the Cybathlon's\nBCI Race but further improvements on accuracy are required.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 10:53:07 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Ortega", "Pablo", ""], ["Colas", "Cedric", ""], ["Faisal", "Aldo", ""]]}, {"id": "1807.11836", "submitter": "Jean Pierre Char", "authors": "Jean Pierre Char", "title": "Inferring the ground truth through crowdsourcing", "comments": "6 pages, 1 figure, Intelligent Systems seminar SS18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universally valid ground truth is almost impossible to obtain or would come\nat a very high cost. For supervised learning without universally valid ground\ntruth, a recommended approach is applying crowdsourcing: Gathering a large data\nset annotated by multiple individuals of varying possibly expertise levels and\ninferring the ground truth data to be used as labels to train the classifier.\nNevertheless, due to the sensitivity of the problem at hand (e.g. mitosis\ndetection in breast cancer histology images), the obtained data needs\nverification and proper assessment before being used for classifier training.\nEven in the context of organic computing systems, an indisputable ground truth\nmight not always exist. Therefore, it should be inferred through the\naggregation and verification of the local knowledge of each autonomous agent.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 14:21:32 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Char", "Jean Pierre", ""]]}]