[{"id": "1901.00234", "submitter": "Ali Moin", "authors": "Ali Moin, Andy Zhou, Simone Benatti, Abbas Rahimi, Luca Benini, Jan M.\n  Rabaey", "title": "Analysis of Contraction Effort Level in EMG-Based Gesture Recognition\n  Using Hyperdimensional Computing", "comments": "Published as a conference paper at the IEEE BioCAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying contraction levels of muscles is a big challenge in\nelectromyography-based gesture recognition. Some use cases require the\nclassifier to be robust against varying force changes, while others demand to\ndistinguish between different effort levels of performing the same gesture. We\nuse brain-inspired hyperdimensional computing paradigm to build classification\nmodels that are both robust to these variations and able to recognize multiple\ncontraction levels. Experimental results on 5 subjects performing 9 gestures\nwith 3 effort levels show up to 39.17% accuracy drop when training and testing\nacross different effort levels, with up to 30.35% recovery after applying our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 01:44:00 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 06:50:59 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 22:04:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Moin", "Ali", ""], ["Zhou", "Andy", ""], ["Benatti", "Simone", ""], ["Rahimi", "Abbas", ""], ["Benini", "Luca", ""], ["Rabaey", "Jan M.", ""]]}, {"id": "1901.00350", "submitter": "Massimiliano Dal Mas", "authors": "Massimiliano Dal Mas", "title": "Responsive Equilibrium for Self-Adaptive Ubiquitous Interaction", "comments": "5 pages, 3 figures, for details see: http://www.maxdalmas.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work attempts to unify two domains: the Game Theory for cooperative\ncontrol systems and the Responsive Web Design, under the umbrella of\ncrowdsourcing for information gain on Ubiquous Sytems related to different\ndevices (as PC, Tablet, Mobile,...) This paper proposes a framework for\nadapting DOM objects components for a disaggregated system, which dynamically\ncomposes web pages for different kind of devices including ubiquitous/pervasive\ncomputing systems. It introduces the notions of responsive webdesign for\nnon-cooperative Nash equilibrium proposing an algorithm (RE-SAUI) for the\ndynamic interface based on the game theory.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 18:35:26 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Mas", "Massimiliano Dal", ""]]}, {"id": "1901.00449", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Simon J. Julier, Nadia Bianchi-Berthouze", "title": "Instant Automated Inference of Perceived Mental Stress through\n  Smartphone PPG and Thermal Imaging", "comments": "Accepted by Journal of Medical Internet Research (JMIR) Mental Health\n  - Special Issue on Computing and Mental Health (2018)", "journal-ref": null, "doi": "10.2196/10140", "report-no": null, "categories": "physics.med-ph cs.CV cs.HC q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background: A smartphone is a promising tool for daily cardiovascular\nmeasurement and mental stress monitoring. A smartphone camera-based\nPhotoPlethysmoGraphy (PPG) and a low-cost thermal camera can be used to create\ncheap, convenient and mobile monitoring systems. However, to ensure reliable\nmonitoring results, a person has to remain still for several minutes while a\nmeasurement is being taken. This is very cumbersome and makes its use in\nreal-life mobile situations quite impractical.\n  Objective: We propose a system which combines PPG and thermography with the\naim of improving cardiovascular signal quality and capturing stress responses\nquickly.\n  Methods: Using a smartphone camera with a low cost thermal camera added on,\nwe built a novel system which continuously and reliably measures two different\ntypes of cardiovascular events: i) blood volume pulse and ii)\nvasoconstriction/dilation-induced temperature changes of the nose tip. 17\nhealthy participants, involved in a series of stress-inducing mental workload\ntasks, measured their physiological responses to stressors over a short window\nof time (20 seconds) immediately after each task. Participants reported their\nlevel of perceived mental stress using a 10-cm Visual Analogue Scale (VAS). We\nused normalized K-means clustering to reduce interpersonal differences in the\nself-reported ratings. For the instant stress inference task, we built novel\nlow-level feature sets representing variability of cardiovascular patterns. We\nthen used the automatic feature learning capability of artificial Neural\nNetworks (NN) to improve the mapping between the extracted set of features and\nthe self-reported ratings. We compared our proposed method with existing\nhand-engineered features-based machine learning methods.\n  Results, Conclusions: ... due to limited space here, we refer to our\nmanuscript.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 00:49:11 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Cho", "Youngjun", ""], ["Julier", "Simon J.", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "1901.00552", "submitter": "Antonio Luca Alfeo", "authors": "A.L. Alfeo, M.G.C.A. Cimino, G. Vaglini", "title": "Measuring Physical Activity of Older Adults via Smartwatch and\n  Stigmergic Receptive Fields", "comments": "mail: luca.alfeo@ing.unipi.it", "journal-ref": "INSTICC The 6th International Conference on Pattern Recognition\n  Applications and Methods (ICPRAM 2017), pp. 724-730, Porto, Portugal, 24-26\n  February 2016", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Physical activity level (PAL) in older adults can enhance healthy aging,\nimprove functional capacity, and prevent diseases. It is known that human\nannotations of PAL can be affected by subjectivity and inaccuracy. Recently\ndeveloped smart devices can allow a non-invasive, analytic, and continuous\ngathering of physiological signals. We present an innovative computational\nsystem fed by signals of heartbeat rate, wrist motion and pedometer sensed by a\nsmartwatch. More specifically, samples of each signal are aggregated by\nfunctional structures called trails. The trailing process is inspired by\nstigmergy, an insects' coordination mechanism, and is managed by computational\nunits called stigmergic receptive fields (SRFs). SRFs, which compute the\nsimilarity between trails, are arranged in a stigmergic perceptron to detect a\ncollection of micro-behaviours of the raw signal, called archetypes. A SRF is\nadaptive to subjects: its structural parameters are tuned by a differential\nevolution algorithm. SRFs are used in a multilayer architecture, providing\nfurther levels of processing to realize macro analyses in the application\ndomain. As a result, the architecture provides a daily PAL, useful to detect\nbehavioural shift indicating initial signs of disease or deviations in\nperformance. As a proof of concept, the approach has been experimented on three\nsubjects.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 23:11:16 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Alfeo", "A. L.", ""], ["Cimino", "M. G. C. A.", ""], ["Vaglini", "G.", ""]]}, {"id": "1901.00715", "submitter": "A. M. Khalili", "authors": "A. M. Khalili, Abdel-Hamid Soliman, Md Asaduzzaman, Alison Griffiths", "title": "Wi-Fi Sensing: Applications and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wi-Fi technology has strong potentials in indoor and outdoor sensing\napplications, it has several important features which makes it an appealing\noption compared to other sensing technologies. This paper presents a survey on\ndifferent applications of Wi-Fi based sensing systems such as elderly people\nmonitoring, activity classification, gesture recognition, people counting,\nthrough the wall sensing, behind the corner sensing, and many other\napplications. The challenges and interesting future directions are also\nhighlighted.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 10:46:10 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 09:00:05 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 20:34:38 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 15:57:42 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Khalili", "A. M.", ""], ["Soliman", "Abdel-Hamid", ""], ["Asaduzzaman", "Md", ""], ["Griffiths", "Alison", ""]]}, {"id": "1901.00885", "submitter": "Chung Hyuk Park", "authors": "Hifza Javed, Rachael Burns, Myounghoon Jeon, Ayanna M. Howard, Chung\n  Hyuk Park", "title": "An Interactive Robotic Framework to Facilitate Sensory Experiences for\n  Children with ASD", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diagnosis of Autism Spectrum Disorder (ASD) in children is commonly\naccompanied by a diagnosis of sensory processing disorders as well.\nAbnormalities are usually reported in multiple sensory processing domains,\nshowing a higher prevalence of unusual responses, particularly to tactile,\nauditory and visual stimuli. This paper discusses a novel robot-based framework\ndesigned to target sensory difficulties faced by children with ASD in a\ncontrolled setting. The setup consists of a number of sensory stations,\ntogether with robotic agents that navigate the stations and interact with the\nstimuli as they are presented. These stimuli are designed to resemble real\nworld scenarios that form a common part of one's everyday experiences. Given\nthe strong interest of children with ASD in technology in general and robots in\nparticular, we attempt to utilize our robotic platform to demonstrate socially\nacceptable responses to the stimuli in an interactive, pedagogical setting that\nencourages the child's social, motor and vocal skills, while providing a\ndiverse sensory experience. A user study was conducted to evaluate the efficacy\nof the proposed framework, with a total of 18 participants (5 with ASD and 13\ntypically developing) between the ages of 4 and 12 years. We describe our\nmethods of data collection, coding of video data and the analysis of the\nresults obtained from the study. We also discuss the limitations of the current\nwork and detail our plans for the future work to improve the validity of the\nobtained results.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 19:22:13 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Javed", "Hifza", ""], ["Burns", "Rachael", ""], ["Jeon", "Myounghoon", ""], ["Howard", "Ayanna M.", ""], ["Park", "Chung Hyuk", ""]]}, {"id": "1901.01001", "submitter": "Will Crichton", "authors": "Anna Zeng and Will Crichton", "title": "Identifying Barriers to Adoption for Rust through Online Discourse", "comments": null, "journal-ref": "Proceedings of the 9th Workshop on Evaluation and Usability of\n  Programming Languages and Tools, 2018", "doi": null, "report-no": null, "categories": "cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rust is a low-level programming language known for its unique approach to\nmemory-safe systems programming and for its steep learning curve. To understand\nwhat makes Rust difficult to adopt, we surveyed the top Reddit and Hacker News\nposts and comments about Rust; from these online discussions, we identified\nthree hypotheses about Rust's barriers to adoption. We found that certain key\nfeatures, idioms, and integration patterns were not easily accessible to new\nusers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2019 07:36:05 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Zeng", "Anna", ""], ["Crichton", "Will", ""]]}, {"id": "1901.01422", "submitter": "Hamed Ghane", "authors": "Golnoosh Garakani, Hamed Ghane and Mohammad Bagher Menhaj", "title": "Control of a 2-DoF robotic arm using a P300-based brain-computer\n  interface", "comments": "20 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO cs.SY eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel control algorithm for a P-300 based brain-computer\ninterface is fully developed to control a 2-DoF robotic arm. Eight subjects\nincluding 5 men and 3 women, perform a 2-dimensional target tracking task in a\nsimulated environment. Their EEG signals from visual cortex are recorded and\nP-300 components are extracted and evaluated to perform a real-time BCI based\ncontroller. The volunteer's intention is recognized and will be decoded as an\nappropriate command to control the cursor. The final goal of the system is to\ncontrol a simulated robotic arm in a 2-dimensional space for writing some\nEnglish letters. The results show that the system allows the robot end-effector\nto move between arbitrary positions in a point-to-point session with the\ndesired accuracy. This model is tested on and compared with Dataset II of the\nBCI Competition. The best result is obtained with a multi-class SVM solution as\nthe classifier, with a recognition rate of 97 percent, without pre-channel\nselection.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2019 14:30:03 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Garakani", "Golnoosh", ""], ["Ghane", "Hamed", ""], ["Menhaj", "Mohammad Bagher", ""]]}, {"id": "1901.01769", "submitter": "Mansoor Ahmed", "authors": "Mansoor Ahmed-Rengers, Ilia Shumailov, Ross Anderson", "title": "Tendrils of Crime: Visualizing the Diffusion of Stolen Bitcoins", "comments": "Accepted at The Fifth International Workshop on Graphical Models for\n  Security, hosted at FLoC 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first six months of 2018 saw cryptocurrency thefts of $761 million, and\nthe technology is also the latest and greatest tool for money laundering. This\nincrease in crime has caused both researchers and law enforcement to look for\nways to trace criminal proceeds. Although tracing algorithms have improved\nrecently, they still yield an enormous amount of data of which very few\ndatapoints are relevant or interesting to investigators, let alone ordinary\nbitcoin owners interested in provenance. In this work we describe efforts to\nvisualize relevant data on a blockchain. To accomplish this we come up with a\ngraphical model to represent the stolen coins and then implement this using a\nvariety of visualization techniques.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 12:18:27 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 01:08:33 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Ahmed-Rengers", "Mansoor", ""], ["Shumailov", "Ilia", ""], ["Anderson", "Ross", ""]]}, {"id": "1901.01920", "submitter": "Evan Peck", "authors": "Evan M. Peck, Sofia E. Ayuso, Omar El-Etr", "title": "Data is Personal: Attitudes and Perceptions of Data Visualization in\n  Rural Pennsylvania", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the guidelines that inform how designers create data visualizations\noriginate in studies that unintentionally exclude populations that are most\nlikely to be among the 'data poor'. In this paper, we explore which factors may\ndrive attention and trust in rural populations with diverse economic and\neducational backgrounds - a segment that is largely underrepresented in the\ndata visualization literature. In 42 semi-structured interviews in rural\nPennsylvania (USA), we find that a complex set of factors intermix to inform\nattitudes and perceptions about data visualization - including educational\nbackground, political affiliation, and personal experience. The data and\nmaterials for this research can be found at https://osf.io/uxwts/\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 16:59:14 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Peck", "Evan M.", ""], ["Ayuso", "Sofia E.", ""], ["El-Etr", "Omar", ""]]}, {"id": "1901.02198", "submitter": "Jeremy Frey", "authors": "Gilad Ostrin (IDC), J\\'er\\'emy Frey (IDC), Jessica Cauchard (IDC)", "title": "Interactive Narrative in Virtual Reality", "comments": null, "journal-ref": "MUM 2018 Proceedings of the 17th International Conference on\n  Mobile and Ubiquitous Multimedia, Nov 2018, Cairo, Egypt. pp.463-467", "doi": "10.1145/3282894.3289740", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive fiction is a literary genre that is rapidly gaining popularity.\nIn this genre, readers are able to explicitly take actions in order to guide\nthe course of the story. With the recent popularity of narrative focused games,\nwe propose to design and develop an interactive narrative tool for content\ncreators. In this extended abstract, we show how we leverage this interactive\nmedium to present a tool for interactive storytelling in virtual reality. Using\na simple markup language, content creators and researchers are now able to\ncreate interactive narratives in a virtual reality environment. We further\ndiscuss the potential future directions for a virtual reality storytelling\nengine.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 08:10:58 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Ostrin", "Gilad", "", "IDC"], ["Frey", "J\u00e9r\u00e9my", "", "IDC"], ["Cauchard", "Jessica", "", "IDC"]]}, {"id": "1901.02602", "submitter": "Asanka G. Perera", "authors": "Asanka G Perera, Yee Wei Law, and Javaan Chahl", "title": "UAV-GESTURE: A Dataset for UAV Control and Gesture Recognition", "comments": "12 pages, 4 figures, UAVision workshop, ECCV, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current UAV-recorded datasets are mostly limited to action recognition and\nobject tracking, whereas the gesture signals datasets were mostly recorded in\nindoor spaces. Currently, there is no outdoor recorded public video dataset for\nUAV commanding signals. Gesture signals can be effectively used with UAVs by\nleveraging the UAVs visual sensors and operational simplicity. To fill this gap\nand enable research in wider application areas, we present a UAV gesture\nsignals dataset recorded in an outdoor setting. We selected 13 gestures\nsuitable for basic UAV navigation and command from general aircraft handling\nand helicopter handling signals. We provide 119 high-definition video clips\nconsisting of 37151 frames. The overall baseline gesture recognition\nperformance computed using Pose-based Convolutional Neural Network (P-CNN) is\n91.9 %. All the frames are annotated with body joints and gesture classes in\norder to extend the dataset's applicability to a wider research area including\ngesture recognition, action recognition, human pose recognition and situation\nawareness.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 04:35:18 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Perera", "Asanka G", ""], ["Law", "Yee Wei", ""], ["Chahl", "Javaan", ""]]}, {"id": "1901.02672", "submitter": "Jason R.C. Nurse Dr", "authors": "Maria Bada and Angela M. Sasse and Jason R. C. Nurse", "title": "Cyber Security Awareness Campaigns: Why do they fail to change\n  behaviour?", "comments": null, "journal-ref": "International Conference on Cyber Security for Sustainable\n  Society, 2015", "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper focuses on Cyber Security Awareness Campaigns, and aims to\nidentify key factors regarding security which may lead them to failing to\nappropriately change people's behaviour. Past and current efforts to improve\ninformation-security practices and promote a sustainable society have not had\nthe desired impact. It is important therefore to critically reflect on the\nchallenges involved in improving information-security behaviours for citizens,\nconsumers and employees. In particular, our work considers these challenges\nfrom a Psychology perspective, as we believe that understanding how people\nperceive risks is critical to creating effective awareness campaigns. Changing\nbehaviour requires more than providing information about risks and reactive\nbehaviours - firstly, people must be able to understand and apply the advice,\nand secondly, they must be motivated and willing to do so - and the latter\nrequires changes to attitudes and intentions. These antecedents of behaviour\nchange are identified in several psychological models of behaviour. We review\nthe suitability of persuasion techniques, including the widely used 'fear\nappeals'. From this range of literature, we extract essential components for an\nawareness campaign as well as factors which can lead to a campaign's success or\nfailure. Finally, we present examples of existing awareness campaigns in\ndifferent cultures (the UK and Africa) and reflect on these.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 10:45:57 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Bada", "Maria", ""], ["Sasse", "Angela M.", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "1901.02839", "submitter": "Jean Kossaifi", "authors": "Jean Kossaifi, Robert Walecki, Yannis Panagakis, Jie Shen, Maximilian\n  Schmitt, Fabien Ringeval, Jing Han, Vedhas Pandit, Antoine Toisoul, Bjorn\n  Schuller, Kam Star, Elnar Hajiyev and Maja Pantic", "title": "SEWA DB: A Rich Database for Audio-Visual Emotion and Sentiment Research\n  in the Wild", "comments": null, "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI), 2019", "doi": "10.1109/TPAMI.2019.2944808", "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural human-computer interaction and audio-visual human behaviour sensing\nsystems, which would achieve robust performance in-the-wild are more needed\nthan ever as digital devices are increasingly becoming an indispensable part of\nour life. Accurately annotated real-world data are the crux in devising such\nsystems. However, existing databases usually consider controlled settings, low\ndemographic variability, and a single task. In this paper, we introduce the\nSEWA database of more than 2000 minutes of audio-visual data of 398 people\ncoming from six cultures, 50% female, and uniformly spanning the age range of\n18 to 65 years old. Subjects were recorded in two different contexts: while\nwatching adverts and while discussing adverts in a video chat. The database\nincludes rich annotations of the recordings in terms of facial landmarks,\nfacial action units (FAU), various vocalisations, mirroring, and continuously\nvalued valence, arousal, liking, agreement, and prototypic examples of\n(dis)liking. This database aims to be an extremely valuable resource for\nresearchers in affective computing and automatic human sensing and is expected\nto push forward the research in human behaviour analysis, including cultural\nstudies. Along with the database, we provide extensive baseline experiments for\nautomatic FAU detection and automatic valence, arousal and (dis)liking\nintensity estimation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 17:28:57 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 22:52:44 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Kossaifi", "Jean", ""], ["Walecki", "Robert", ""], ["Panagakis", "Yannis", ""], ["Shen", "Jie", ""], ["Schmitt", "Maximilian", ""], ["Ringeval", "Fabien", ""], ["Han", "Jing", ""], ["Pandit", "Vedhas", ""], ["Toisoul", "Antoine", ""], ["Schuller", "Bjorn", ""], ["Star", "Kam", ""], ["Hajiyev", "Elnar", ""], ["Pantic", "Maja", ""]]}, {"id": "1901.02884", "submitter": "Philipp V. Rouast", "authors": "Philipp V. Rouast, Marc T. P. Adam, and Raymond Chiong", "title": "Deep Learning for Human Affect Recognition: Insights and New\n  Developments", "comments": "To be published in IEEE Transactions on Affective Computing. 20\n  pages, 7 figures, 6 tables", "journal-ref": null, "doi": "10.1109/TAFFC.2018.2890471", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic human affect recognition is a key step towards more natural\nhuman-computer interaction. Recent trends include recognition in the wild using\na fusion of audiovisual and physiological sensors, a challenging setting for\nconventional machine learning algorithms. Since 2010, novel deep learning\nalgorithms have been applied increasingly in this field. In this paper, we\nreview the literature on human affect recognition between 2010 and 2017, with a\nspecial focus on approaches using deep neural networks. By classifying a total\nof 950 studies according to their usage of shallow or deep architectures, we\nare able to show a trend towards deep learning. Reviewing a subset of 233\nstudies that employ deep neural networks, we comprehensively quantify their\napplications in this field. We find that deep learning is used for learning of\n(i) spatial feature representations, (ii) temporal feature representations, and\n(iii) joint feature representations for multimodal sensor data. Exemplary\nstate-of-the-art architectures illustrate the progress. Our findings show the\nrole deep architectures will play in human affect recognition, and can serve as\na reference point for researchers working on related applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 23:33:47 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Rouast", "Philipp V.", ""], ["Adam", "Marc T. P.", ""], ["Chiong", "Raymond", ""]]}, {"id": "1901.02942", "submitter": "Asma Baghdadi", "authors": "Asma Baghdadi, Yassine Aribi, Rahma Fourati, Najla Halouani, Patrick\n  Siarry and Adel M. Alimi", "title": "DASPS: A Database for Anxious States based on a Psychological\n  Stimulation", "comments": "11 pages, IEEE transactions on SMC:systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anxiety affects human capabilities and behavior as much as it affects\nproductivity and quality of life. It can be considered as the main cause of\ndepression and suicide. Anxious states are easily detectable by humans due to\ntheir acquired cognition, humans interpret the interlocutor's tone of speech,\ngesture, facial expressions and recognize their mental state. There is a need\nfor non-invasive reliable techniques that performs the complex task of anxiety\ndetection. In this paper, we present DASPS database containing recorded\nElectroencephalogram (EEG) signals of 23 participants during anxiety\nelicitation by means of face-to-face psychological stimuli. EEG signals were\ncaptured with Emotiv Epoc headset as it's a wireless wearable low-cost\nequipment. In our study, we investigate the impact of different parameters,\nnotably: trial duration, feature type, feature combination and anxiety levels\nnumber. Our findings showed that anxiety is well elicited in 1 second. For\ninstance, stacked sparse autoencoder with different type of features achieves\n83.50% and 74.60% for 2 and 4 anxiety levels detection, respectively. The\npresented results prove the benefits of the use of a low-cost EEG headset\ninstead of medical non-wireless devices and create a starting point for new\nresearches in the field of anxiety detection.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 21:54:35 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 23:46:48 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Baghdadi", "Asma", ""], ["Aribi", "Yassine", ""], ["Fourati", "Rahma", ""], ["Halouani", "Najla", ""], ["Siarry", "Patrick", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1901.02943", "submitter": "Amirmasoud Ahmadi", "authors": "Amirmasoud Ahmadi, Sepideh Farakhor Seghinsara, Mohammad Reza Daliri,\n  Vahid Shalchyan", "title": "Brain Electrical Stimulation for Animal Navigation", "comments": "in Farsi", "journal-ref": "Iranian Journal of Biomedical Engineering, 11(1), pp. 83-100", "doi": "10.22041/IJBME.2017.72949.1276", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain stimulation and its widespread use is one of the most important\nsubjects in studies of neurophysiology. In brain electrical stimulation\nmethods, following the surgery and electrode implantation, electrodes send\nelectrical impulses to the specific targets in the brain. The use of this\nstimulation method is provided therapeutic benefits for treatment chronic pain,\nessential tremor, Parkinsons disease, major depression, and neurological\nmovement disorder syndrome (dystonia). One area in which advancements have been\nrecently made is in controlling the movement and navigation of animals in a\nspecific pathway. It is important to identify brain targets in order to\nstimulate appropriate brain regions for all the applications listed above. An\nanimal navigation system based on brain electrical stimulation is used to\ndevelop new behavioral models for the aim of creating a platform for\ninteracting with the animal nervous system in the spatial learning task. In the\ncontext of animal navigation the electrical stimulation has been used either as\ncreating virtual sensation for movement guidance or virtual reward for movement\nmotivation. In this paper, different approaches and techniques of brain\nelectrical stimulation for this application has been reviewed.\n  Keywords: Rat Robot, Brain Computer Interface, Electrical Stimulation, Cyborg\nIntelligence, Brain to Brain Interface\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 13:56:40 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Ahmadi", "Amirmasoud", ""], ["Seghinsara", "Sepideh Farakhor", ""], ["Daliri", "Mohammad Reza", ""], ["Shalchyan", "Vahid", ""]]}, {"id": "1901.02949", "submitter": "Yea Seul Kim", "authors": "Yea-Seul Kim, Logan A Walls, Peter Krafft, Jessica Hullman", "title": "A Bayesian Cognition Approach to Improve Data Visualization", "comments": null, "journal-ref": null, "doi": "10.1145/3290605.3300912", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People naturally bring their prior beliefs to bear on how they interpret the\nnew information, yet few formal models exist for accounting for the influence\nof users' prior beliefs in interactions with data presentations like\nvisualizations. We demonstrate a Bayesian cognitive model for understanding how\npeople interpret visualizations in light of prior beliefs and show how this\nmodel provides a guide for improving visualization evaluation. In a first\nstudy, we show how applying a Bayesian cognition model to a simple\nvisualization scenario indicates that people's judgments are consistent with a\nhypothesis that they are doing approximate Bayesian inference. In a second\nstudy, we evaluate how sensitive our observations of Bayesian behavior are to\ndifferent techniques for eliciting people subjective distributions, and to\ndifferent datasets. We find that people don't behave consistently with Bayesian\npredictions for large sample size datasets, and this difference cannot be\nexplained by elicitation technique. In a final study, we show how normative\nBayesian inference can be used as an evaluation framework for visualizations,\nincluding of uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 22:08:50 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Kim", "Yea-Seul", ""], ["Walls", "Logan A", ""], ["Krafft", "Peter", ""], ["Hullman", "Jessica", ""]]}, {"id": "1901.02957", "submitter": "Alex Kale", "authors": "Alex Kale, Matthew Kay, Jessica Hullman", "title": "Decision-Making Under Uncertainty in Research Synthesis: Designing for\n  the Garden of Forking Paths", "comments": "This paper will be published in Proceedings of CHI Conference on\n  HumanFactors in Computing Systems Proceedings (CHI 2019). Expected DOI:\n  https://doi.org/10.1145/3290605.3300432 Updates posted on 01/14/2019 in order\n  to clarify limitations of framing researcher decision-making as expected\n  utility maximization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make evidence-based recommendations to decision-makers, researchers\nconducting systematic reviews and meta-analyses must navigate a garden of\nforking paths: a series of analytical decision-points, each of which has the\npotential to influence findings. To identify challenges and opportunities\nrelated to designing systems to help researchers manage uncertainty around\nwhich of multiple analyses is best, we interviewed 11 professional researchers\nwho conduct research synthesis to inform decision-making within three\norganizations. We conducted a qualitative analysis identifying 480 analytical\ndecisions made by researchers throughout the scientific process. We present\ndescriptions of current practices in applied research synthesis and\ncorresponding design challenges: making it more feasible for researchers to try\nand compare analyses, shifting researchers' attention from rationales for\ndecisions to impacts on results, and supporting communication techniques that\nacknowledge decision-makers' aversions to uncertainty. We identify\nopportunities to design systems which help researchers explore, reason about,\nand communicate uncertainty in decision-making about possible analyses in\nresearch synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 22:34:27 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 19:00:16 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Kale", "Alex", ""], ["Kay", "Matthew", ""], ["Hullman", "Jessica", ""]]}, {"id": "1901.02999", "submitter": "Chang-Shing Lee", "authors": "Chang-Shing Lee, Mei-Hui Wang, Li-Wei Ko, Bo-Yu Tsai, Yi-Lin Tsai,\n  Sheng-Chi Yang, Lu-An Lin, Yi-Hsiu Lee, Hirofumi Ohashi, Naoyuki Kubota, and\n  Nan Shuo", "title": "PFML-based Semantic BCI Agent for Game of Go Learning and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a semantic brain computer interface (BCI) agent with\nparticle swarm optimization (PSO) based on a Fuzzy Markup Language (FML) for Go\nlearning and prediction applications. Additionally, we also establish an Open\nGo Darkforest (OGD) cloud platform with Facebook AI research (FAIR) open source\nDarkforest and ELF OpenGo AI bots. The Japanese robot Palro will simultaneously\npredict the move advantage in the board game Go to the Go players for reference\nor learning. The proposed semantic BCI agent operates efficiently by the\nhuman-based BCI data from their brain waves and machine-based game data from\nthe prediction of the OGD cloud platform for optimizing the parameters between\nhumans and machines. Experimental results show that the proposed human and\nsmart machine co-learning mechanism performs favorably. We hope to provide\nstudents with a better online learning environment, combining different kinds\nof handheld devices, robots, or computer equipment, to achieve a desired and\nintellectual learning goal in the future.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 02:32:26 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Lee", "Chang-Shing", ""], ["Wang", "Mei-Hui", ""], ["Ko", "Li-Wei", ""], ["Tsai", "Bo-Yu", ""], ["Tsai", "Yi-Lin", ""], ["Yang", "Sheng-Chi", ""], ["Lin", "Lu-An", ""], ["Lee", "Yi-Hsiu", ""], ["Ohashi", "Hirofumi", ""], ["Kubota", "Naoyuki", ""], ["Shuo", "Nan", ""]]}, {"id": "1901.03329", "submitter": "Savindu Herath Pathirannahalage Mr.", "authors": "H.P. Savindu, K.A. Iroshan, C.D. Panangala, W.L.D.W.P. Perera, A.C De\n  Silva", "title": "BrailleBand: Blind Support Haptic Wearable Band for Communication using\n  Braille Language", "comments": "6 pages, 4 figures, In proceedings of 2017 IEEE International\n  Conference on Systems, Man, and Cybernetics (SMC), pp. 1381-1386. Banff,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visually impaired people are neglected from many modern communication and\ninteraction procedures. Assistive technologies such as text-to-speech and\nbraille displays are the most commonly used means of connecting such visually\nimpaired people with mobile phones and other smart devices. Both these\nsolutions face usability issues, thus this study focused on developing a user\nfriendly wearable solution called the \"BrailleBand\" with haptic technology\nwhile preserving affordability. The \"BrailleBand\" enables passive reading using\nthe Braille language. Connectivity between the BrailleBand and the smart device\n(phone) is established using Bluetooth protocol. It consists of six nodes in\nthree bands worn on the arm to map the braille alphabet, which are actuated to\ngive the sense of touch corresponding to the characters. Three mobile\napplications were developed for training the visually impaired and to integrate\nexisting smart mobile applications such as navigation and short message service\n(SMS) with the device BrailleBand. The adaptability, usability and efficiency\nof reading was tested on a sample of blind users which reflected progressive\nresults. Even though, the reading accuracy depends on the time duration between\nthe characters (character gap) an average Character Transfer Rate of 0.4375\ncharacters per second can be achieved with a character gap of 1000 ms.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 16:47:59 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Savindu", "H. P.", ""], ["Iroshan", "K. A.", ""], ["Panangala", "C. D.", ""], ["Perera", "W. L. D. W. P.", ""], ["De Silva", "A. C", ""]]}, {"id": "1901.03442", "submitter": "Mathias Baumert", "authors": "Simanto Saha, Khondaker A. Mamun, Khawza Ahmed, Raqibul Mostafa,\n  Ganesh R. Naik, Ahsan Khandoker, Sam Darvishi, Mathias Baumert", "title": "Progress in Brain Computer Interfaces: Challenges and Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain computer interfaces (BCI) provide a direct communication link between\nthe brain and a computer or other external devices. They offer an extended\ndegree of freedom either by strengthening or by substituting human peripheral\nworking capacity and have potential applications in various fields such as\nrehabilitation, affective computing, robotics, gaming and artificial\nintelligence. Significant research efforts on a global scale have delivered\ncommon platforms for technology standardization and help tackle highly complex\nand nonlinear brain dynamics and related feature extraction and classification\nchallenges. Psycho-neurophysiological phenomena and their impact on brain\nsignals impose another challenge for BCI researchers to transform the\ntechnology from laboratory experiments to plug-and-play daily life. This review\nsummarizes progress in BCI field and highlights critical challenges.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 00:57:05 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Saha", "Simanto", ""], ["Mamun", "Khondaker A.", ""], ["Ahmed", "Khawza", ""], ["Mostafa", "Raqibul", ""], ["Naik", "Ganesh R.", ""], ["Khandoker", "Ahsan", ""], ["Darvishi", "Sam", ""], ["Baumert", "Mathias", ""]]}, {"id": "1901.03450", "submitter": "Rong Zheng", "authors": "Chao Cai, Rong Zheng, Menglan Hu", "title": "A survey on acoustic sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of Internet-of-Things (IoT) has brought many new sensing mechanisms.\nAmong these mechanisms, acoustic sensing attracts much attention in recent\nyears. Acoustic sensing exploits acoustic sensors beyond their primary uses,\nnamely recording and playing, to enable interesting applications and new user\nexperience. In this paper, we present the first survey of recent advances in\nacoustic sensing using commodity hardware. We propose a general framework that\ncategorizes main building blocks of acoustic sensing systems. This framework\nconsists of three layers, i.e., the physical layer, processing layer, and\napplication layer. We highlight different sensing approaches in the processing\nlayer and fundamental design considerations in the physical layer. Many\nexisting and potential applications including context-aware applications,\nhuman-computer interface, and aerial acoustic communications are presented in\ndepth. Challenges and future research trends are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 01:58:35 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Cai", "Chao", ""], ["Zheng", "Rong", ""], ["Hu", "Menglan", ""]]}, {"id": "1901.03532", "submitter": "David Glowacki", "authors": "Rachel Freire, Becca Rose Glowacki, Rhoslyn Roebuck Williams, Mark\n  Wonnacott, Alexander Jamieson-Binnie, David R. Glowacki", "title": "OMG-VR: Open-source Mudra Gloves for Manipulating Molecular Simulations\n  in VR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.bio-ph physics.chem-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As VR finds increasing application in scientific research domains like\nnanotechnology and biochemistry, we are beginning to better understand the\ndomains in which it brings the most benefit, as well as the gestures and form\nfactors that are most useful for specific applications. Here we describe\nOpen-source Mudra Gloves for Virtual Reality (OMG-VR): etextile gloves designed\nto facilitate research scientists and students carrying out detailed and\ncomplex manipulation of simulated 3d molecular objects in VR. The OMG-VR is\ndesigned to sense when a user pinches together their thumb and index finger, or\nthumb and middle finger, forming a \"mudra\" position. Tests show that they\nprovide good positional tracking of the point at which a pinch takes place,\nrequire no calibration, and are sufficiently accurate and robust to enable\nscientists to accomplish a range of tasks that involve complex spatial\nmanipulation of molecules. The open source design offers a promising\nalternative to existing controllers and more costly commercial VR data gloves.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 10:02:27 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 16:48:02 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Freire", "Rachel", ""], ["Glowacki", "Becca Rose", ""], ["Williams", "Rhoslyn Roebuck", ""], ["Wonnacott", "Mark", ""], ["Jamieson-Binnie", "Alexander", ""], ["Glowacki", "David R.", ""]]}, {"id": "1901.03536", "submitter": "David Glowacki", "authors": "Lisa May Thomas, Helen M. Deeks, Alex J. Jones, Oussama Metatla, David\n  R. Glowacki", "title": "Somatic Practices for Understanding Real, Imagined, and Virtual\n  Realities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In most VR experiences, the visual sense dominates other modes of sensory\ninput, encouraging non-visual senses to respond as if the visual were real. The\nsimulated visual world thus becomes a sort of felt actuality, where the\n'actual' physical body and environment can 'drop away', opening up\npossibilities for designing entirely new kinds of experience. Most VR\nexperiences place visual sensory input (of the simulated environment) in the\nperceptual foreground, and the physical body in the background. In what\nfollows, we discuss methods for resolving the apparent tension which arises\nfrom VR's prioritization of visual perception. We specifically aim to\nunderstand how somatic techniques encouraging participants to 'attend to their\nattention' enable them to access more subtle aspects of sensory phenomena in a\nVR experience, bound neither by rigid definitions of vision-based virtuality\nnor body-based corporeality. During a series of workshops, we implemented\nexperimental somatic-dance practices to better understand perceptual and\nimaginative subtleties that arise for participants whilst they are embedded in\na multi-person VR framework. Our preliminary observations suggest that somatic\nmethods can be used to design VR experiences which enable (i) a tactile quality\nor felt sense of phenomena in the virtual environment (VE), (ii) lingering\nimpacts on participant imagination even after the VR headset is taken off, and\n(iii) an expansion of imaginative potential.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 10:06:46 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Thomas", "Lisa May", ""], ["Deeks", "Helen M.", ""], ["Jones", "Alex J.", ""], ["Metatla", "Oussama", ""], ["Glowacki", "David R.", ""]]}, {"id": "1901.03729", "submitter": "Mark Riedl", "authors": "Upol Ehsan, Pradyumna Tambwekar, Larry Chan, Brent Harrison, Mark\n  Riedl", "title": "Automated Rationale Generation: A Technique for Explainable AI and its\n  Effects on Human Perceptions", "comments": "Accepted to the 2019 International Conference on Intelligent User\n  Interfaces", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated rationale generation is an approach for real-time explanation\ngeneration whereby a computational model learns to translate an autonomous\nagent's internal state and action data representations into natural language.\nTraining on human explanation data can enable agents to learn to generate\nhuman-like explanations for their behavior. In this paper, using the context of\nan agent that plays Frogger, we describe (a) how to collect a corpus of\nexplanations, (b) how to train a neural rationale generator to produce\ndifferent styles of rationales, and (c) how people perceive these rationales.\nWe conducted two user studies. The first study establishes the plausibility of\neach type of generated rationale and situates their user perceptions along the\ndimensions of confidence, humanlike-ness, adequate justification, and\nunderstandability. The second study further explores user preferences between\nthe generated rationales with regard to confidence in the autonomous agent,\ncommunicating failure and unexpected behavior. Overall, we find alignment\nbetween the intended differences in features of the generated rationales and\nthe perceived differences by users. Moreover, context permitting, participants\npreferred detailed rationales to form a stable mental model of the agent's\nbehavior.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 19:55:48 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Ehsan", "Upol", ""], ["Tambwekar", "Pradyumna", ""], ["Chan", "Larry", ""], ["Harrison", "Brent", ""], ["Riedl", "Mark", ""]]}, {"id": "1901.03793", "submitter": "Eda Okur", "authors": "Eda Okur, Sinem Aslan, Nese Alyuz, Asli Arslan Esme, Ryan S. Baker", "title": "The Importance of Socio-Cultural Differences for Annotating and\n  Detecting the Affective States of Students", "comments": "13th Women in Machine Learning Workshop (WiML 2018), co-located with\n  the 32nd Conference on Neural Information Processing Systems (NeurIPS 2018),\n  Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of real-time affect detection models often depends upon\nobtaining annotated data for supervised learning by employing human experts to\nlabel the student data. One open question in annotating affective data for\naffect detection is whether the labelers (i.e., human experts) need to be\nsocio-culturally similar to the students being labeled, as this impacts the\ncost feasibility of obtaining the labels. In this study, we investigate the\nfollowing research questions: For affective state annotation, how does the\nsocio-cultural background of human expert labelers, compared to the subjects,\nimpact the degree of consensus and distribution of affective states obtained?\nSecondly, how do differences in labeler background impact the performance of\naffect detection models that are trained using these labels?\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 03:50:53 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Okur", "Eda", ""], ["Aslan", "Sinem", ""], ["Alyuz", "Nese", ""], ["Esme", "Asli Arslan", ""], ["Baker", "Ryan S.", ""]]}, {"id": "1901.04024", "submitter": "Pengfei Sun", "authors": "Pengfei Sun, David A. Moses, and Edward Chang", "title": "Modeling neural dynamics during speech production using a state space\n  variational autoencoder", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the neural encoding of behavior remains a challenging task in\nmany research areas due in part to complex and noisy spatiotemporal dynamics of\nevoked brain activity. An important aspect of modeling these neural encodings\ninvolves separation of robust, behaviorally relevant signals from background\nactivity, which often contains signals from irrelevant brain processes and\ndecaying information from previous behavioral events. To achieve this\nseparation, we develop a two-branch State Space Variational AutoEncoder (SSVAE)\nmodel to individually describe the instantaneous evoked foreground signals and\nthe context-dependent background signals. We modeled the spontaneous\nspeech-evoked brain dynamics using smoothed Gaussian mixture models. By\napplying the proposed SSVAE model to track ECoG dynamics in one participant\nover multiple hours, we find that the model can predict speech-related dynamics\nmore accurately than other latent factor inference algorithms. Our results\ndemonstrate that separately modeling the instantaneous speech-evoked and slow\ncontext-dependent brain dynamics can enhance tracking performance, which has\nimportant implications for the development of advanced neural encoding and\ndecoding models in various neuroscience sub-disciplines.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 17:26:11 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Sun", "Pengfei", ""], ["Moses", "David A.", ""], ["Chang", "Edward", ""]]}, {"id": "1901.04110", "submitter": "Colleen Crangle", "authors": "Colleen E. Crangle, Rui Wang, Marcos Perreau-Guimaraes, Michelle U.\n  Nguyen, Duc T. Nguyen, Patrick Suppes", "title": "Machine learning for the recognition of emotion in the speech of couples\n  in psychotherapy using the Stanford Suppes Brain Lab Psychotherapy Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic recognition of emotion in speech can inform our understanding\nof language, emotion, and the brain. It also has practical application to\nhuman-machine interactive systems. This paper examines the recognition of\nemotion in naturally occurring speech, where there are no constraints on what\nis said or the emotions expressed. This task is more difficult than that using\ndata collected in scripted, experimentally controlled settings, and fewer\nresults are published. Our data come from couples in psychotherapy. Video and\naudio recordings were made of three couples (A, B, C) over 18 hour-long therapy\nsessions. This paper describes the method used to code the audio recordings for\nthe four emotions of Anger, Sadness, Joy and Tension, plus Neutral, also\ncovering our approach to managing the unbalanced samples that a naturally\noccurring emotional speech dataset produces. Three groups of acoustic features\nwere used in our analysis: filter-bank, frequency, and voice-quality features.\nThe random forests model classified the features. Recognition rates are\nreported for each individual, the result of the speaker-dependent models that\nwe built. In each case, the best recognition rates were achieved using the\nfilter-bank features alone. For Couple A, these rates were 90% for the female\nand 87% for the male for the recognition of three emotions plus Neutral. For\nCouple B, the rates were 84% for the female and 78% for the male for the\nrecognition of all four emotions plus Neutral. For Couple C, a rate of 88% was\nachieved for the female for the recognition of the four emotions plus Neutral\nand 95% for the male for three emotions plus Neutral. For pairwise recognition,\nthe rates ranged from 76% to 99% across the three couples. Our results show\nthat couple therapy is a rich context for the study of emotion in naturally\noccurring speech.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 02:21:08 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Crangle", "Colleen E.", ""], ["Wang", "Rui", ""], ["Perreau-Guimaraes", "Marcos", ""], ["Nguyen", "Michelle U.", ""], ["Nguyen", "Duc T.", ""], ["Suppes", "Patrick", ""]]}, {"id": "1901.04194", "submitter": "Marcos Baez", "authors": "Marcos Baez, Radoslaw Nielek, Fabio Casati, Adam Wierzbicki", "title": "Technologies for promoting social participation in later life", "comments": null, "journal-ref": null, "doi": "10.1007/978-981-13-3693-5_17", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social participation is known to bring great benefits to the health and\nwell-being of people as they age. From being in contact with others to engaging\nin group activities, keeping socially active can help slow down the effects of\nage-related declines, reduce risks of loneliness and social isolation and even\nmortality in old age. There are unfortunately a variety of barriers that make\nit difficult for older adults to engage in social activities in a regular\nbasis. In this chapter, we give an overview of the challenges to social\nparticipation and discuss how technology can help overcome these barriers and\npromote participation in social activities. We examine two particular research\nthreads and designs, exploring ways in which technology can support co-located\nand virtual participation: i) an application that motivates the virtual\nparticipation in group training programs, and ii) a location-based game that\nsupports co-located intergenerational ICT training classes. We discuss the\neffectiveness and limitations of various design choices in the two use cases\nand outline the lessons learned\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 09:05:31 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Baez", "Marcos", ""], ["Nielek", "Radoslaw", ""], ["Casati", "Fabio", ""], ["Wierzbicki", "Adam", ""]]}, {"id": "1901.04375", "submitter": "Bahareh Sarrafzadeh", "authors": "Bahareh Sarrafzadeh, Ahmed Hassan Awadallah, Christopher H. Lin,\n  Chia-Jung Lee, Milad Shokouhi and Susan T. Dumais", "title": "Characterizing and Predicting Email Deferral Behavior", "comments": null, "journal-ref": "WSDM 2019", "doi": "10.1145/3289600.3291028", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Email triage involves going through unhandled emails and deciding what to do\nwith them. This familiar process can become increasingly challenging as the\nnumber of unhandled email grows. During a triage session, users commonly defer\nhandling emails that they cannot immediately deal with to later. These deferred\nemails, are often related to tasks that are postponed until the user has more\ntime or the right information to deal with them. In this paper, through\nqualitative interviews and a large-scale log analysis, we study when and what\nenterprise email users tend to defer. We found that users are more likely to\ndefer emails when handling them involves replying, reading carefully, or\nclicking on links and attachments. We also learned that the decision to defer\nemails depends on many factors such as user's workload and the importance of\nthe sender. Our qualitative results suggested that deferring is very common,\nand our quantitative log analysis confirms that 12% of triage sessions and 16%\nof daily active users had at least one deferred email on weekdays. We also\ndiscuss several deferral strategies such as marking emails as unread and\nflagging that are reported by our interviewees, and illustrate how such\npatterns can be also observed in user logs. Inspired by the characteristics of\ndeferred emails and contextual factors involved in deciding if an email should\nbe deferred, we train a classifier for predicting whether a recently triaged\nemail is actually deferred. Our experimental results suggests that deferral can\nbe classified with modest effectiveness. Overall, our work provides novel\ninsights about how users handle their emails and how deferral can be modeled.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 16:09:53 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Sarrafzadeh", "Bahareh", ""], ["Awadallah", "Ahmed Hassan", ""], ["Lin", "Christopher H.", ""], ["Lee", "Chia-Jung", ""], ["Shokouhi", "Milad", ""], ["Dumais", "Susan T.", ""]]}, {"id": "1901.04856", "submitter": "Nikolaos Lykousas", "authors": "Nikolaos Lykousas, Costantinos Patsakis, Andreas Kaltenbrunner and\n  Vicen\\c{c} G\\'omez", "title": "Sharing emotions at scale: The Vent dataset", "comments": "9 pages, 12 figures, 2 tables. Accepted at the 13th International\n  AAAI Conference on Web and Social Media (ICWSM 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous and increasing use of social media has enabled the expression\nof human thoughts, opinions, and everyday actions publicly at an unprecedented\nscale. We present the Vent dataset, the largest annotated dataset of text,\nemotions, and social connections to date. It comprises more than 33 millions of\nposts by nearly a million of users together with their social connections. Each\npost has an associated emotion. There are 705 different emotions, organized in\n63 \"emotion categories\", forming a two-level taxonomy of affects. Our initial\nstatistical analysis describes the global patterns of activity in the Vent\nplatform, revealing large heterogenities and certain remarkable regularities\nregarding the use of the different emotions. We focus on the aggregated use of\nemotions, the temporal activity, and the social network of users, and outline\npossible methods to infer emotion networks based on the user activity. We also\nanalyze the text and describe the affective landscape of Vent, finding\nagreements with existing (small scale) annotated corpus in terms of emotion\ncategories and positive/negative valences. Finally, we discuss possible\nresearch questions that can be addressed from this unique dataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 14:39:34 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 23:04:20 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Lykousas", "Nikolaos", ""], ["Patsakis", "Costantinos", ""], ["Kaltenbrunner", "Andreas", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1901.04889", "submitter": "Yuanyuan Zhang", "authors": "Yuanyuan Zhang, Zi-Rui Wang, Jun Du", "title": "Deep Fusion: An Attention Guided Factorized Bilinear Pooling for\n  Audio-video Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic emotion recognition (AER) is a challenging task due to the abstract\nconcept and multiple expressions of emotion. Although there is no consensus on\na definition, human emotional states usually can be apperceived by auditory and\nvisual systems. Inspired by this cognitive process in human beings, it's\nnatural to simultaneously utilize audio and visual information in AER. However,\nmost traditional fusion approaches only build a linear paradigm, such as\nfeature concatenation and multi-system fusion, which hardly captures complex\nassociation between audio and video. In this paper, we introduce factorized\nbilinear pooling (FBP) to deeply integrate the features of audio and video.\nSpecifically, the features are selected through the embedded attention\nmechanism from respective modalities to obtain the emotion-related regions. The\nwhole pipeline can be completed in a neural network. Validated on the AFEW\ndatabase of the audio-video sub-challenge in EmotiW2018, the proposed approach\nachieves an accuracy of 62.48%, outperforming the state-of-the-art result.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 15:51:39 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Zhang", "Yuanyuan", ""], ["Wang", "Zi-Rui", ""], ["Du", "Jun", ""]]}, {"id": "1901.05050", "submitter": "Ryan Ebardo", "authors": "Ryan Ebardo", "title": "The Use of Activity Trackers for Health Empowerment and Commitment: The\n  Philippine Cycling Perspective", "comments": "Proceedings of 2017 the 7th International Workshop on Computer\n  Science and Engineering (WCSE 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity tracking devices have found its way in the world of cycling. With\nits projected market demand and increasing popularity of cycling in the\nPhilippines, cyclists are slowly adopting this technology in their daily\ncycling routines. Activity trackers demonstrate real-time data which allow\ncyclists to adjust physical efforts to achieve their personal goals. Six common\nfeatures of activity trackers were formed as constructs to explore its\ninfluence on health empowerment in the context of cycling using Partial Least\nSquares Structural Equation Model. A total of 393 cyclists in the Philippines\nparticipated in the study. Some features demonstrated strong evidence of\npositive influence in achieving health empowerment and commitment. Implications\nfor future design and development of this technology device are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 09:31:24 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Ebardo", "Ryan", ""]]}, {"id": "1901.05415", "submitter": "Braden Hancock", "authors": "Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazar\\'e, Jason Weston", "title": "Learning from Dialogue after Deployment: Feed Yourself, Chatbot!", "comments": "ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of conversations a dialogue agent sees over its lifetime occur\nafter it has already been trained and deployed, leaving a vast store of\npotential training signal untapped. In this work, we propose the self-feeding\nchatbot, a dialogue agent with the ability to extract new training examples\nfrom the conversations it participates in. As our agent engages in\nconversation, it also estimates user satisfaction in its responses. When the\nconversation appears to be going well, the user's responses become new training\nexamples to imitate. When the agent believes it has made a mistake, it asks for\nfeedback; learning to predict the feedback that will be given improves the\nchatbot's dialogue abilities further. On the PersonaChat chit-chat dataset with\nover 131k training examples, we find that learning from dialogue with a\nself-feeding chatbot significantly improves performance, regardless of the\namount of traditional supervision.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 18:02:44 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 07:03:17 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2019 03:23:04 GMT"}, {"version": "v4", "created": "Thu, 13 Jun 2019 06:01:04 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Hancock", "Braden", ""], ["Bordes", "Antoine", ""], ["Mazar\u00e9", "Pierre-Emmanuel", ""], ["Weston", "Jason", ""]]}, {"id": "1901.05574", "submitter": "Chuan Wang", "authors": "Chuan Wang, Takeshi Onishi, Keiichi Nemoto, Kwan-Liu Ma", "title": "Visual Reasoning of Feature Attribution with Deep Recurrent Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Recurrent Neural Network (RNN) has gained popularity in many sequence\nclassification tasks. Beyond predicting a correct class for each data instance,\ndata scientists also want to understand what differentiating factors in the\ndata have contributed to the classification during the learning process. We\npresent a visual analytics approach to facilitate this task by revealing the\nRNN attention for all data instances, their temporal positions in the\nsequences, and the attribution of variables at each value level. We demonstrate\nwith real-world datasets that our approach can help data scientists to\nunderstand such dynamics in deep RNNs from the training results, hence guiding\ntheir modeling process.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 00:57:33 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Wang", "Chuan", ""], ["Onishi", "Takeshi", ""], ["Nemoto", "Keiichi", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1901.05835", "submitter": "Eda Okur", "authors": "Nese Alyuz, Eda Okur, Utku Genc, Sinem Aslan, Cagri Tanriover, Asli\n  Arslan Esme", "title": "Unobtrusive and Multimodal Approach for Behavioral Engagement Detection\n  of Students", "comments": "12th Women in Machine Learning Workshop (WiML 2017), co-located with\n  the 31st Conference on Neural Information Processing Systems (NeurIPS 2017),\n  Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multimodal approach for detection of students' behavioral\nengagement states (i.e., On-Task vs. Off-Task), based on three unobtrusive\nmodalities: Appearance, Context-Performance, and Mouse. Final behavioral\nengagement states are achieved by fusing modality-specific classifiers at the\ndecision level. Various experiments were conducted on a student dataset\ncollected in an authentic classroom.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 02:32:13 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Alyuz", "Nese", ""], ["Okur", "Eda", ""], ["Genc", "Utku", ""], ["Aslan", "Sinem", ""], ["Tanriover", "Cagri", ""], ["Esme", "Asli Arslan", ""]]}, {"id": "1901.05939", "submitter": "Juan G Victores", "authors": "Jennifer J. Gago, Juan G. Victores, Carlos Balaguer", "title": "Sign Language Representation by TEO Humanoid Robot: End-User Interest,\n  Comprehension and Satisfaction", "comments": "21 pages, 11 figures, MDPI Electronics Journal", "journal-ref": "Electronics, 8(1), 57 (2019)", "doi": "10.3390/electronics8010057", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we illustrate our work on improving the accessibility of\nCyber-Physical Systems (CPS), presenting a study on human-robot interaction\nwhere the end-users are either deaf or hearing-impaired people. Current trends\nin robotic designs include devices with robotic arms and hands capable of\nperforming manipulation and grasping tasks. This paper focuses on how these\ndevices can be used for a different purpose, which is that of enabling robotic\ncommunication via sign language. For the study, several tests and\nquestionnaires are run to check and measure how end-users feel about\ninterpreting sign language represented by a humanoid robotic assistant as\nopposed to subtitles on a screen. Stemming from this dichotomy, dactylology,\nbasic vocabulary representation and end-user satisfaction are the main topics\ncovered by a delivered form, in which additional commentaries are valued and\ntaken into consideration for further decision taking regarding robot-human\ninteraction. The experiments were performed using TEO, a household companion\nhumanoid robot developed at the University Carlos III de Madrid (UC3M), via\nrepresentations in Spanish Sign Language (LSE), and a total of 16 deaf and\nhearing-impaired participants.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 18:20:08 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Gago", "Jennifer J.", ""], ["Victores", "Juan G.", ""], ["Balaguer", "Carlos", ""]]}, {"id": "1901.06237", "submitter": "Sha Lai", "authors": "Mehrnoosh Sameki, Sha Lai, Kate K. Mays, Lei Guo, Prakash Ishwar,\n  Margrit Betke", "title": "BUOCA: Budget-Optimized Crowd Worker Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to concerns about human error in crowdsourcing, it is standard practice\nto collect labels for the same data point from multiple internet workers. We\nhere show that the resulting budget can be used more effectively with a\nflexible worker assignment strategy that asks fewer workers to analyze\neasy-to-label data and more workers to analyze data that requires extra\nscrutiny. Our main contribution is to show how the allocations of the number of\nworkers to a task can be computed optimally based on task features alone,\nwithout using worker profiles. Our target tasks are delineating cells in\nmicroscopy images and analyzing the sentiment toward the 2016 U.S. presidential\ncandidates in tweets. We first propose an algorithm that computes\nbudget-optimized crowd worker allocation (BUOCA). We next train a machine\nlearning system (BUOCA-ML) that predicts an optimal number of crowd workers\nneeded to maximize the accuracy of the labeling. We show that the computed\nallocation can yield large savings in the crowdsourcing budget (up to 49\npercent points) while maintaining labeling accuracy. Finally, we envisage a\nhuman-machine system for performing budget-optimized data analysis at a scale\nbeyond the feasibility of crowdsourcing.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2019 22:36:41 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Sameki", "Mehrnoosh", ""], ["Lai", "Sha", ""], ["Mays", "Kate K.", ""], ["Guo", "Lei", ""], ["Ishwar", "Prakash", ""], ["Betke", "Margrit", ""]]}, {"id": "1901.06417", "submitter": "Matthew Guzdial", "authors": "Matthew Guzdial, Nicholas Liao, Jonathan Chen, Shao-Yu Chen, Shukan\n  Shah, Vishwa Shah, Joshua Reno, Gillian Smith and Mark Riedl", "title": "Friend, Collaborator, Student, Manager: How Design of an AI-Driven Game\n  Level Editor Affects Creators", "comments": "13 pages, 3 figures, CHI Conference on Human Factors in Computing\n  Systems", "journal-ref": null, "doi": "10.1145/3290605.3300854", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning advances have afforded an increase in algorithms capable of\ncreating art, music, stories, games, and more. However, it is not yet\nwell-understood how machine learning algorithms might best collaborate with\npeople to support creative expression. To investigate how practicing designers\nperceive the role of AI in the creative process, we developed a game level\ndesign tool for Super Mario Bros.-style games with a built-in AI level\ndesigner. In this paper we discuss our design of the Morai Maker intelligent\ntool through two mixed-methods studies with a total of over one-hundred\nparticipants. Our findings are as follows: (1) level designers vary in their\ndesired interactions with, and role of, the AI, (2) the AI prompted the level\ndesigners to alter their design practices, and (3) the level designers\nperceived the AI as having potential value in their design practice, varying\nbased on their desired role for the AI.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 21:16:38 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Guzdial", "Matthew", ""], ["Liao", "Nicholas", ""], ["Chen", "Jonathan", ""], ["Chen", "Shao-Yu", ""], ["Shah", "Shukan", ""], ["Shah", "Vishwa", ""], ["Reno", "Joshua", ""], ["Smith", "Gillian", ""], ["Riedl", "Mark", ""]]}, {"id": "1901.06525", "submitter": "Leigh Clark", "authors": "Leigh Clark, Nadia Pantidi, Orla Cooney, Philip Doyle, Diego\n  Garaialde, Justin Edwards, Brendan Spillane, Christine Murad, Cosmin\n  Munteanu, Vincent Wade, Benjamin R. Cowan", "title": "What Makes a Good Conversation? Challenges in Designing Truly\n  Conversational Agents", "comments": "Accepted for publication at the 2019 Conference on Human Factors in\n  Computing Systems (CHI 2019)", "journal-ref": null, "doi": "10.1145/3290605.3300705", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents promise conversational interaction but fail to deliver.\nEfforts often emulate functional rules from human speech, without considering\nkey characteristics that conversation must encapsulate. Given its potential in\nsupporting long-term human-agent relationships, it is paramount that HCI\nfocuses efforts on delivering this promise. We aim to understand what people\nvalue in conversation and how this should manifest in agents. Findings from a\nseries of semi-structured interviews show people make a clear dichotomy between\nsocial and functional roles of conversation, emphasising the long-term dynamics\nof bond and trust along with the importance of context and relationship stage\nin the types of conversations they have. People fundamentally questioned the\nneed for bond and common ground in agent communication, shifting to more\nutilitarian definitions of conversational qualities. Drawing on these findings\nwe discuss key challenges for conversational agent design, most notably the\nneed to redefine the design parameters for conversational agent interaction.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 14:00:00 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Clark", "Leigh", ""], ["Pantidi", "Nadia", ""], ["Cooney", "Orla", ""], ["Doyle", "Philip", ""], ["Garaialde", "Diego", ""], ["Edwards", "Justin", ""], ["Spillane", "Brendan", ""], ["Murad", "Christine", ""], ["Munteanu", "Cosmin", ""], ["Wade", "Vincent", ""], ["Cowan", "Benjamin R.", ""]]}, {"id": "1901.06572", "submitter": "Michael Xuelin Huang", "authors": "Michae Xuelin Huang, Jiajia Li, Grace Ngai, Hong Va Leong, Andreas\n  Bulling", "title": "Moment-to-Moment Detection of Internal Thought from Eye Vergence\n  Behaviour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internal thought refers to the process of directing attention away from a\nprimary visual task to internal cognitive processing. Internal thought is a\npervasive mental activity and closely related to primary task performance. As\nsuch, automatic detection of internal thought has significant potential for\nuser modelling in intelligent interfaces, particularly for e-learning\napplications. Despite the close link between the eyes and the human mind, only\na few studies have investigated vergence behaviour during internal thought and\nnone has studied moment-to-moment detection of internal thought from gaze.\nWhile prior studies relied on long-term data analysis and required a large\nnumber of gaze characteristics, we describe a novel method that is\ncomputationally light-weight and that only requires eye vergence information\nthat is readily available from binocular eye trackers. We further propose a\nnovel paradigm to obtain ground truth internal thought annotations that\nexploits human blur perception. We evaluate our method for three increasingly\nchallenging detection tasks: (1) during a controlled math-solving task, (2)\nduring natural viewing of lecture videos, and (3) during daily activities, such\nas coding, browsing, and reading. Results from these evaluations demonstrate\nthe performance and robustness of vergence-based detection of internal thought\nand, as such, open up new directions for research on interfaces that adapt to\nshifts of mental attention.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jan 2019 18:53:47 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Huang", "Michae Xuelin", ""], ["Li", "Jiajia", ""], ["Ngai", "Grace", ""], ["Leong", "Hong Va", ""], ["Bulling", "Andreas", ""]]}, {"id": "1901.06601", "submitter": "Anran Wang", "authors": "Anran Wang and Shyamnath Gollakota", "title": "MilliSonic: Pushing the Limits of Acoustic Motion Tracking", "comments": null, "journal-ref": null, "doi": "10.1145/3290605.3300248", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen interest in device tracking and localization using\nacoustic signals. State-of-the-art acoustic motion tracking systems however do\nnot achieve millimeter accuracy and require large separation between\nmicrophones and speakers, and as a result, do not meet the requirements for\nmany VR/AR applications. Further, tracking multiple concurrent acoustic\ntransmissions from VR devices today requires sacrificing accuracy or frame\nrate. We present MilliSonic, a novel system that pushes the limits of acoustic\nbased motion tracking. Our core contribution is a novel localization algorithm\nthat can provably achieve sub-millimeter 1D tracking accuracy in the presence\nof multipath, while using only a single beacon with a small 4-microphone\narray.Further, MilliSonic enables concurrent tracking of up to four smartphones\nwithout reducing frame rate or accuracy. Our evaluation shows that MilliSonic\nachieves 0.7mm median 1D accuracy and a 2.6mm median 3D accuracy for\nsmartphones, which is 5x more accurate than state-of-the-art systems.\nMilliSonic enables two previously infeasible interaction applications: a) 3D\ntracking of VR headsets using the smartphone as a beacon and b) fine-grained 3D\ntracking for the Google Cardboard VR system using a small microphone array.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 00:01:51 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Wang", "Anran", ""], ["Gollakota", "Shyamnath", ""]]}, {"id": "1901.06681", "submitter": "Briane Paul Samson", "authors": "Briane Paul Samson, Yasuyuki Sumi", "title": "Exploring Factors that Influence Connected Drivers to (Not) Use or\n  Follow Recommended Optimal Routes", "comments": "This paper will be published in the CHI Conference on Human Factors\n  in Computing Systems Proceedings (CHI 2019)", "journal-ref": "Proceedings of the 2019 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3290605.3300601", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation applications are becoming ubiquitous in our daily navigation\nexperiences. With the intention to circumnavigate congested roads, their route\nguidance always follows the basic assumption that drivers always want the\nfastest route. However, it is unclear how their recommendations are followed\nand what factors affect their adoption. We present the results of a\nsemi-structured qualitative study with 17 drivers, mostly from the Philippines\nand Japan. We recorded their daily commutes and occasional trips, and inquired\ninto their navigation practices, route choices and on-the-fly decision-making.\nWe found that while drivers choose a recommended route in urgent situations,\nmany still preferred to follow familiar routes. Drivers deviated because of a\nrecommendation's use of unfamiliar roads, lack of local context, perceived\ndriving unsuitability, and inconsistencies with realized navigation\nexperiences. Our findings and implications emphasize their personalization\nneeds, and how the right amount of algorithmic sophistication can encourage\nbehavioral adaptation.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2019 14:34:51 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Samson", "Briane Paul", ""], ["Sumi", "Yasuyuki", ""]]}, {"id": "1901.06958", "submitter": "Istv\\'an Ketyk\\'o", "authors": "Istv\\'an Ketyk\\'o, Ferenc Kov\\'acs and Kriszti\\'an Zsolt Varga", "title": "Domain Adaptation for sEMG-based Gesture Recognition with Recurrent\n  Neural Networks", "comments": "Typos corrected", "journal-ref": "2019 International Joint Conference on Neural Networks (IJCNN),\n  Budapest, Hungary, 2019, pp. 1-7", "doi": "10.1109/IJCNN.2019.8852018", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface Electromyography (sEMG/EMG) is to record muscles' electrical activity\nfrom a restricted area of the skin by using electrodes. The sEMG-based gesture\nrecognition is extremely sensitive of inter-session and inter-subject\nvariances. We propose a model and a deep-learning-based domain adaptation\nmethod to approximate the domain shift for recognition accuracy enhancement.\nAnalysis performed on sparse and HighDensity (HD) sEMG public datasets validate\nthat our approach outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 15:19:01 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 15:51:32 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ketyk\u00f3", "Istv\u00e1n", ""], ["Kov\u00e1cs", "Ferenc", ""], ["Varga", "Kriszti\u00e1n Zsolt", ""]]}, {"id": "1901.07151", "submitter": "Bilal Farooq", "authors": "Shadi Djavadian, Bilal Farooq, Rafael Vasquez, Grace Yip", "title": "Virtual Immersive Reality based Analysis of Behavioral Responses in\n  Connected and Autonomous Vehicle Environment", "comments": "Included in the book published from the selected papers presented at\n  the 15th International Conference on Travel Behaviour Research held in Santa\n  Barbara, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we developed a dynamic distributed end-to-end vehicle routing\nsystem (E2ECAV) using a network of intelligent intersections and level 5 CAVs\n(Djavadian & Farooq, 2018). The case study of the downtown Toronto Network\nshowed that E2ECAV has the ability to maximize throughput and reduce travel\ntime up to 40%. However, the efficiency of these new technologies relies on the\nacceptance of users in adapting to them and their willingness to give control\nfully or partially to CAVs. In this study a stated preference laboratory\nexperiment is designed employing Virtual Reality Immersive Environment (VIRE)\ndriving simulator to evaluate the behavioral response of drivers to E2ECAV. The\naim is to investigate under what conditions drivers are more willing to adapt.\nThe results show that factors such as locus of control, congestion level and\nability to multi-task have significant impact.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 02:13:10 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Djavadian", "Shadi", ""], ["Farooq", "Bilal", ""], ["Vasquez", "Rafael", ""], ["Yip", "Grace", ""]]}, {"id": "1901.07457", "submitter": "Tharun Reddy", "authors": "Satyam Kumar, Tharun Kumar Reddy, and Laxmidhar Behera", "title": "Divergence Framework for EEG based Multiclass Motor Imagery Brain\n  Computer Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.HC eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similar to most of the real world data, the ubiquitous presence of\nnon-stationarities in the EEG signals significantly perturb the feature\ndistribution thus deteriorating the performance of Brain Computer Interface. In\nthis letter, a novel method is proposed based on Joint Approximate\nDiagonalization (JAD) to optimize stationarity for multiclass motor imagery\nBrain Computer Interface (BCI) in an information theoretic framework.\nSpecifically, in the proposed method, we estimate the subspace which optimizes\nthe discriminability between the classes and simultaneously preserve\nstationarity within the motor imagery classes. We determine the subspace for\nthe proposed approach through optimization using gradient descent on an\northogonal manifold. The performance of the proposed stationarity enforcing\nalgorithm is compared to that of baseline One-Versus-Rest (OVR)-CSP and JAD on\npublicly available BCI competition IV dataset IIa. Results show that an\nimprovement in average classification accuracies across the subjects over the\nbaseline algorithms and thus essence of alleviating within session\nnon-stationarities.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 09:33:12 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Kumar", "Satyam", ""], ["Reddy", "Tharun Kumar", ""], ["Behera", "Laxmidhar", ""]]}, {"id": "1901.07694", "submitter": "Jonathan Dodge", "authors": "Jonathan Dodge, Q. Vera Liao, Yunfeng Zhang, Rachel K. E. Bellamy and\n  Casey Dugan", "title": "Explaining Models: An Empirical Study of How Explanations Impact\n  Fairness Judgment", "comments": null, "journal-ref": null, "doi": "10.1145/3301275.3302310", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring fairness of machine learning systems is a human-in-the-loop process.\nIt relies on developers, users, and the general public to identify fairness\nproblems and make improvements. To facilitate the process we need effective,\nunbiased, and user-friendly explanations that people can confidently rely on.\nTowards that end, we conducted an empirical study with four types of\nprogrammatically generated explanations to understand how they impact people's\nfairness judgments of ML systems. With an experiment involving more than 160\nMechanical Turk workers, we show that: 1) Certain explanations are considered\ninherently less fair, while others can enhance people's confidence in the\nfairness of the algorithm; 2) Different fairness problems--such as model-wide\nfairness issues versus case-specific fairness discrepancies--may be more\neffectively exposed through different styles of explanation; 3) Individual\ndifferences, including prior positions and judgment criteria of algorithmic\nfairness, impact how people react to different styles of explanation. We\nconclude with a discussion on providing personalized and adaptive explanations\nto support fairness judgments of ML systems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 02:29:28 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Dodge", "Jonathan", ""], ["Liao", "Q. Vera", ""], ["Zhang", "Yunfeng", ""], ["Bellamy", "Rachel K. E.", ""], ["Dugan", "Casey", ""]]}, {"id": "1901.08289", "submitter": "Kashyap Todi", "authors": "Camille Gobert, Kashyap Todi, Gilles Bailly, Antti Oulasvirta", "title": "SAM: A Modular Framework for Self-Adapting Web Menus", "comments": "5 pages; published at ACM IUI 2019 conference", "journal-ref": null, "doi": "10.1145/3301275.3302314", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents SAM, a modular and extensible JavaScript framework for\nself-adapting menus on webpages. SAM allows control of two elementary aspects\nfor adapting web menus: (1) the target policy, which assigns scores to menu\nitems for adaptation, and (2) the adaptation style, which specifies how they\nare adapted on display. By decoupling them, SAM enables the exploration of\ndifferent combinations independently. Several policies from literature are\nreadily implemented, and paired with adaptation styles such as reordering and\nhighlighting. The process - including user data logging - is local, offering\nprivacy benefits and eliminating the need for server-side modifications.\nResearchers can use SAM to experiment adaptation policies and styles, and\nbenchmark techniques in an ecological setting with real webpages. Practitioners\ncan make websites self-adapting, and end-users can dynamically personalise\ntypically static web menus.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 08:57:43 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Gobert", "Camille", ""], ["Todi", "Kashyap", ""], ["Bailly", "Gilles", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "1901.08325", "submitter": "Quentin Roy", "authors": "Quentin Roy (GE Healthcare), Yves Guiard, Gilles Bailly, Eric\n  Lecolinet (LTCI), Olivier Rioul", "title": "Glass+Skin: An Empirical Evaluation of the Added Value of Finger\n  Identification to Basic Single-Touch Interaction on Touch Screens", "comments": null, "journal-ref": "15th Human-Computer Interaction (INTERACT), Sep 2015, Bamberg,\n  Germany. Springer, Lecture Notes in Computer Science, LNCS-9299 (Part IV),\n  pp.55-71, 2015, Human-Computer Interaction -- INTERACT 2015", "doi": "10.1007/978-3-319-22723-8_5", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usability of small devices such as smartphones or interactive watches is\noften hampered by the limited size of command vocabularies. This paper is an\nattempt at better understanding how finger identification may help users invoke\ncommands on touch screens, even without recourse to multi-touch input. We\ndescribe how finger identification can increase the size of input vocabularies\nunder the constraint of limited real estate, and we discuss some visual cues to\ncommunicate this novel modality to novice users. We report a controlled\nexperiment that evaluated, over a large range of input-vocabulary sizes, the\nefficiency of single-touch command selections with vs. without finger\nidentification. We analyzed the data not only in terms of traditional time and\nerror metrics, but also in terms of a throughput measure based on Shannon's\ntheory, which we show offers a synthetic and parsimonious account of users'\nperformance. The results show that the larger the input vocabulary needed by\nthe designer, the more promising the identification of individual fingers.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 10:06:17 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Roy", "Quentin", "", "GE Healthcare"], ["Guiard", "Yves", "", "LTCI"], ["Bailly", "Gilles", "", "LTCI"], ["Lecolinet", "Eric", "", "LTCI"], ["Rioul", "Olivier", ""]]}, {"id": "1901.08335", "submitter": "Karla Stepanova", "authors": "Radoslav Skoviera, Karla Stepanova, Michael Tesar, Gabriela Sejnova,\n  Jiri Sedlar, Michal Vavrecka, Robert Babuska, and Josef Sivic", "title": "Teaching robots to imitate a human with no on-teacher sensors. What are\n  the key challenges?", "comments": null, "journal-ref": "The IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS) 2018, Workshop on: Towards Intelligent Social Robots: From\n  Naive Robots to Robot Sapiens\n  http://intelligent-social-robots-ws.com/materials/", "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of learning object manipulation tasks\nfrom human demonstration using RGB or RGB-D cameras. We highlight the key\nchallenges in capturing sufficiently good data with no tracking devices -\nstarting from sensor selection and accurate 6DoF pose estimation to natural\nlanguage processing. In particular, we focus on two showcases: gluing task with\na glue gun and simple block-stacking with variable blocks. Furthermore, we\ndiscuss how a linguistic description of the task could help to improve the\naccuracy of task description. We also present the whole architecture of our\ntransfer of the imitated task to the simulated and real robot environment.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 10:38:24 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Skoviera", "Radoslav", ""], ["Stepanova", "Karla", ""], ["Tesar", "Michael", ""], ["Sejnova", "Gabriela", ""], ["Sedlar", "Jiri", ""], ["Vavrecka", "Michal", ""], ["Babuska", "Robert", ""], ["Sivic", "Josef", ""]]}, {"id": "1901.09224", "submitter": "Zhiyong Yuan", "authors": "Qianqian Tong, Xiaosa Li, Kai Lin, and Zhiyong Yuan", "title": "Cascade LSTM Based Visual-Inertial Navigation for Magnetic Levitation\n  Haptic Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haptic feedback is essential to acquire immersive experience when interacting\nin virtual or augmented reality. Although the existing promising magnetic\nlevitation (maglev) haptic system has advantages of none mechanical friction,\nits performance is limited by its navigation method, which mainly results from\nthe challenge that it is difficult to obtain high precision, high frame rate\nand good stability with lightweight design at the same. In this study, we\npropose to perform the visual-inertial fusion navigation based on\nsequence-to-sequence learning for the maglev haptic interaction. Cascade LSTM\nbased-increment learning method is first presented to progressively learn the\nincrements of the target variables. Then, two cascade LSTM networks are\nseparately trained for accomplishing the visual-inertial fusion navigation in a\nloosely-coupled mode. Additionally, we set up a maglev haptic platform as the\nsystem testbed. Experimental results show that the proposed cascade LSTM\nbased-increment learning method can achieve high-precision prediction, and our\ncascade LSTM based visual-inertial fusion navigation method can reach 200Hz\nwhile maintaining high-precision (the mean absolute error of the position and\norientation is respectively less than 1mm and 0.02{\\deg})navigation for the\nmaglev haptic interaction application.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2019 14:56:16 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Tong", "Qianqian", ""], ["Li", "Xiaosa", ""], ["Lin", "Kai", ""], ["Yuan", "Zhiyong", ""]]}, {"id": "1901.09804", "submitter": "Yefim Shulman", "authors": "Yefim Shulman, Joachim Meyer", "title": "Is Privacy Controllable?", "comments": "The final publication will be available at Springer via\n  http://dx.doi.org/ [in press]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major views of privacy associates privacy with the control over\ninformation. This gives rise to the question how controllable privacy actually\nis. In this paper, we adapt certain formal methods of control theory and\ninvestigate the implications of a control theoretic analysis of privacy. We\nlook at how control and feedback mechanisms have been studied in the privacy\nliterature. Relying on the control theoretic framework, we develop a simplistic\nconceptual control model of privacy, formulate privacy controllability issues\nand suggest directions for possible research.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:57:31 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Shulman", "Yefim", ""], ["Meyer", "Joachim", ""]]}, {"id": "1901.09851", "submitter": "Brian Brost", "authors": "Brian Brost, Rishabh Mehrotra and Tristan Jehan", "title": "The Music Streaming Sessions Dataset", "comments": "Web conference 2019 version with updated link to dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the core of many important machine learning problems faced by online\nstreaming services is a need to model how users interact with the content they\nare served. Unfortunately, there are no public datasets currently available\nthat enable researchers to explore this topic. In order to spur that research,\nwe release the Music Streaming Sessions Dataset (MSSD), which consists of 160\nmillion listening sessions and associated user actions. Furthermore, we provide\naudio features and metadata for the approximately 3.7 million unique tracks\nreferred to in the logs. This is the largest collection of such track metadata\ncurrently available to the public. This dataset enables research on important\nproblems including how to model user listening and interaction behaviour in\nstreaming, as well as Music Information Retrieval (MIR), and session-based\nsequential recommendations. Additionally, a subset of sessions were collected\nusing a uniformly random recommendation setting, enabling their use for\ncounterfactual evaluation of such sequential recommendations. Finally, we\nprovide an analysis of user behavior and suggest further research problems\nwhich can be addressed using the dataset.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 14:22:08 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 18:31:43 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Brost", "Brian", ""], ["Mehrotra", "Rishabh", ""], ["Jehan", "Tristan", ""]]}, {"id": "1901.10245", "submitter": "Jun Zhao Dr", "authors": "Jun Zhao, Ge Wang, Carys Dally, Petr Slovak, Julian Childs, Max Van\n  Klee and Nigel Shadbolt", "title": "`I make up a silly name': Understanding Children's Perception of Privacy\n  Risks Online", "comments": "13 pages, 1 figure", "journal-ref": "CHI Conference on Human Factors in Computing Systems Proceedings\n  (CHI 2019), May 4--9, 2019, Glasgow, Scotland Uk", "doi": "10.1145/3290605.3300336", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Children under 11 are often regarded as too young to comprehend the\nimplications of online privacy. Perhaps as a result, little research has\nfocused on younger kids' risk recognition and coping. Such knowledge is,\nhowever, critical for designing efficient safeguarding mechanisms for this age\ngroup. Through 12 focus group studies with 29 children aged 6-10 from UK\nschools, we examined how children described privacy risks related to their use\nof tablet computers and what information was used by them to identify threats.\nWe found that children could identify and articulate certain privacy risks\nwell, such as information oversharing or revealing real identities online;\nhowever, they had less awareness with respect to other risks, such as online\ntracking or game promotions. Our findings offer promising directions for\nsupporting children's awareness of cyber risks and the ability to protect\nthemselves online.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 12:18:43 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Zhao", "Jun", ""], ["Wang", "Ge", ""], ["Dally", "Carys", ""], ["Slovak", "Petr", ""], ["Childs", "Julian", ""], ["Van Klee", "Max", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "1901.10268", "submitter": "Zhen Xue", "authors": "Wei Cui, Zhen Xue and Khanh-Phuong Thai", "title": "Performance comparison of an AI-based Adaptive Learning System in China", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive learning systems stand apart from traditional learning systems by\noffering a personalized learning experience to students according to their\ndifferent knowledge states. Adaptive systems collect and analyse students'\nbehavior data, update learner profiles, then accordingly provide timely\nindividualized feedback to each student. Such interactions between the learning\nsystem and students can improve the engagement of students and the efficiency\nof learning. This paper evaluates the effectiveness of an adaptive learning\nsystem, \"Yixue Squirrel AI\" (or Yixue), on English and math learning in middle\nschool. The effectiveness of the Yixue's math and English learning systems is\nrespectively compared against (1) traditional classroom math instruction\nconducted by expert human teachers and (2) BOXFiSH, another adaptive learning\nplatform for English language learning. Results suggest that students achieved\nbetter performance using Yixue adaptive learning system than both traditional\nclassroom instruction by expert teachers and another adaptive learning\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:21:03 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Cui", "Wei", ""], ["Xue", "Zhen", ""], ["Thai", "Khanh-Phuong", ""]]}, {"id": "1901.10312", "submitter": "Aythami Morales", "authors": "Alejandro Acien, Aythami Morales, Ruben Vera-Rodriguez, and Julian\n  Fierrez", "title": "MultiLock: Mobile Active Authentication based on Multiple Biometric and\n  Behavioral Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we evaluate mobile active authentication based on an ensemble\nof biometrics and behavior-based profiling signals. We consider seven different\ndata channels and their combination. Touch dynamics (touch gestures and\nkeystroking), accelerometer, gyroscope, WiFi, GPS location and app usage are\nall collected during human-mobile interaction to authenticate the users. We\nevaluate two approaches: one-time authentication and active authentication. In\none-time authentication, we employ the information of all channels available\nduring one session. For active authentication we take advantage of mobile user\nbehavior across multiple sessions by updating a confidence value of the\nauthentication score. Our experiments are conducted on the semi-uncontrolled\nUMDAA-02 database. This database comprises smartphone sensor signals acquired\nduring natural human-mobile interaction. Our results show that different traits\ncan be complementary and multimodal systems clearly increase the performance\nwith accuracies ranging from 82.2% to 97.1% depending on the authentication\nscenario.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 14:39:37 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Acien", "Alejandro", ""], ["Morales", "Aythami", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""]]}, {"id": "1901.10408", "submitter": "Jo\\~ao Ranhel", "authors": "Jo\\~ao Ranhel, Cacilda Vilela", "title": "Guidelines for creating man-machine multimodal interfaces", "comments": "The pseudo-code in section \"5.2.2 The Algorithm Kernel\" needs\n  revision. We are working on systems description using SDL, instead of showing\n  a pseudo-code algorithm. Describing the system using \"Specification and\n  Description Language\" can make reading easier and precise. Explanations of\n  linguistic theories need to be simplified. Tables and figures will also help\n  on understanding the article", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding details of human multimodal interaction can elucidate many\naspects of the type of information processing machines must perform to interact\nwith humans. This article gives an overview of recent findings from Linguistics\nregarding the organization of conversation in turns, adjacent pairs,\n(dis)preferred responses, (self)repairs, etc. Besides, we describe how multiple\nmodalities of signs interfere with each other modifying meanings. Then, we\npropose an abstract algorithm that describes how a machine can implement a\ndouble-feedback system that can reproduces a human-like face-to-face\ninteraction by processing various signs, such as verbal, prosodic, facial\nexpressions, gestures, etc. Multimodal face-to-face interactions enrich the\nexchange of information between agents, mainly because these agents are active\nall the time by emitting and interpreting signs simultaneously. This article is\nnot about an untested new computational model. Instead, it translates findings\nfrom Linguistics as guidelines for designs of multimodal man-machine\ninterfaces. An algorithm is presented. Brought from Linguistics, it is a\ndescription pointing out how human face-to-face interactions work. The\nlinguistic findings reported here are the first steps towards the integration\nof multimodal communication. Some developers involved on interface designs\ncarry on working on isolated models for interpreting text, grammar, gestures\nand facial expressions, neglecting the interwoven between these signs. In\ncontrast, for linguists working on the state-of-the-art multimodal integration,\nthe interpretation of separated modalities leads to an incomplete\ninterpretation, if not to a miscomprehension of information. The algorithm\nproposed herein intends to guide man-machine interface designers who want to\nintegrate multimodal components on face-to-face interactions as close as\npossible to those performed between humans.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 17:23:32 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 19:01:13 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Ranhel", "Jo\u00e3o", ""], ["Vilela", "Cacilda", ""]]}, {"id": "1901.10449", "submitter": "Yunlong Wang", "authors": "Yunlong Wang, Ahmed Fadhil, Harald Reiterer", "title": "Health Behavior Change in HCI: Trends, Patterns, and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unhealthy lifestyles could cause many chronic diseases, which bring patients\nand their families much burden. Research has shown the potential of digital\ntechnologies for supporting health behavior change to help us prevent these\nchronic diseases. The HCI community has contributed to the research on health\nbehavior change for more than a decade. In this paper, we aim to explore the\nresearch trends and patterns of health behavior change in HCI. Our systematic\nreview showed that physical activity drew much more attention than other\nbehaviors. Most of the participants in the reviewed studies were adults, while\nchildren and the elderly were much less addressed. Also, we found there is a\nlack of standardized approaches to evaluating the user experience of\ninterventions for health behavior change in HCI. Based on the reviewed studies,\nwe provide suggestions and research opportunities on six topics, e.g., game\nintegration, social support, and relevant AI application.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 18:55:06 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Wang", "Yunlong", ""], ["Fadhil", "Ahmed", ""], ["Reiterer", "Harald", ""]]}, {"id": "1901.10461", "submitter": "Jonas Beskow", "authors": "Andreas Wedenborn, Preben Wik, Olov Engwall, and Jonas Beskow", "title": "The effect of a physical robot on vocabulary learning", "comments": "This study was presented at the International Workshop on Spoken\n  Dialogue Systems (IWSDS 2016), Saariselk\\\"a, Finland., January 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the effect of a physical robot taking the role of a\nteacher or exercise partner in a language learning exercise. In order to\ninvestigate this, an application was developed enabling a 2:nd language\nlearning vocabulary exercise in three different conditions. In the first\ncondition the learner would receive tutoring from a disembodied voice, in the\nsecond condition the tutor would be embodied by an animated avatar on a\ncomputer screen, and in the final condition the tutor was a physical robotic\nhead with a 3D animated face mask. A Russian language vocabulary exercise with\n15 subjects was conducted. None of the subjects reported any Russian language\nskills prior to the exercises. Each subject were taught a set of 9 words in\neach of the three conditions during a practice phase, and were then asked to\nrecall the words in a test phase. Results show that the recall of the words\npracticed with the physical robot were significantly higher than that of the\nwords practiced with the avatar on the screen or with the disembodied voice.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 13:28:29 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Wedenborn", "Andreas", ""], ["Wik", "Preben", ""], ["Engwall", "Olov", ""], ["Beskow", "Jonas", ""]]}, {"id": "1901.10720", "submitter": "Simon Perrault", "authors": "Simon T. Perrault and Weiyu Zhang", "title": "Effects of Moderation and Opinion Heterogeneity on Attitude towards the\n  Online Deliberation Experience", "comments": "11 pages, CHI 2019", "journal-ref": null, "doi": "10.1145/3290605.3300247", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online deliberation offers a way for citizens to collectively discuss an\nissue and provide input for policy makers. The overall experience of online\ndeliberation can be affected by multiple factors. We decided to investigate the\neffects of moderation and opinion heterogeneity on the perceived deliberation\nexperience, by running the first online deliberation experiment in Singapore.\nOur study took place in three months with three phases. In phase 1, our 2,006\nparticipants answered a survey, that we used to create groups of different\nopinion heterogeneity. During the second phase, 510 participants discussed\nabout the population issue on the online platform we developed. We gathered\ndata on their online deliberation experience during phase 3. We found out that\nhigher levels of moderation negatively impact the experience of deliberation on\nperceived procedural fairness, validity claim and policy legitimacy; and that\nhigh opinion heterogeneity is important in order to get a fair assessment of\nthe deliberation experience.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 09:27:52 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Perrault", "Simon T.", ""], ["Zhang", "Weiyu", ""]]}, {"id": "1901.10906", "submitter": "Xucong Zhang", "authors": "Xucong Zhang, Yusuke Sugano, Andreas Bulling", "title": "Evaluation of Appearance-Based Methods and Implications for Gaze-Based\n  Applications", "comments": null, "journal-ref": "Proc. ACM SIGCHI Conference on Human Factors in Computing Systems\n  (CHI) 2019", "doi": "10.1145/3290605.3300646", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appearance-based gaze estimation methods that only require an off-the-shelf\ncamera have significantly improved but they are still not yet widely used in\nthe human-computer interaction (HCI) community. This is partly because it\nremains unclear how they perform compared to model-based approaches as well as\ndominant, special-purpose eye tracking equipment. To address this limitation,\nwe evaluate the performance of state-of-the-art appearance-based gaze\nestimation for interaction scenarios with and without personal calibration,\nindoors and outdoors, for different sensing distances, as well as for users\nwith and without glasses. We discuss the obtained findings and their\nimplications for the most important gaze-based applications, namely explicit\neye input, attentive user interfaces, gaze-based user modelling, and passive\neye monitoring. To democratise the use of appearance-based gaze estimation and\ninteraction in HCI, we finally present OpenGaze (www.opengaze.org), the first\nsoftware toolkit for appearance-based gaze estimation and interaction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 15:39:57 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Zhang", "Xucong", ""], ["Sugano", "Yusuke", ""], ["Bulling", "Andreas", ""]]}, {"id": "1901.11358", "submitter": "Ansari Saleh Ahmar", "authors": "Harryanto, Muchriana Muchran, Ansari Saleh Ahmar", "title": "Application of TAM model to the use of information technology", "comments": null, "journal-ref": "International Journal of Engineering & Technology (UAE), 7 (2.9)\n  (2018) 37-40", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The purpose of this research is to see the application of modified TAM model\nby entering the experiential variable as a moderation variable to see one's\nintention in the use of technology especially internet banking. Data obtained\nthrough the distribution of questionnaires to customers. The study population\nis bank customers registered as users of internet banking services. The sample\nselection used a simple random sampling technique. Hypothesis testing using\nPartial Least Square (PLS) method through AMOS program. The results showed that\nthe proposed five hypotheses, two significant and three insignificant.\nPerceived ease of use is significantly related to perceived usefulness.\nPer-ceived usefulness is not significantly related to intention to use.\nPerceived ease of use is significantly related to intention to use moderated by\nexperience and not significantly correlated with intention to use moderated by\nthe Experience.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 22:54:30 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Harryanto", "", ""], ["Muchran", "Muchriana", ""], ["Ahmar", "Ansari Saleh", ""]]}, {"id": "1901.11528", "submitter": "Kory W Mathewson", "authors": "Kory W. Mathewson, Pablo Samuel Castro, Colin Cherry, George Foster,\n  Marc G. Bellemare", "title": "Shaping the Narrative Arc: An Information-Theoretic Approach to\n  Collaborative Dialogue", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of designing an artificial agent capable of\ninteracting with humans in collaborative dialogue to produce creative, engaging\nnarratives. In this task, the goal is to establish universe details, and to\ncollaborate on an interesting story in that universe, through a series of\nnatural dialogue exchanges. Our model can augment any probabilistic\nconversational agent by allowing it to reason about universe information\nestablished and what potential next utterances might reveal. Ideally, with each\nutterance, agents would reveal just enough information to add specificity and\nreduce ambiguity without limiting the conversation. We empirically show that\nour model allows control over the rate at which the agent reveals information\nand that doing so significantly improves accuracy in predicting the next line\nof dialogues from movies. We close with a case-study with four professional\ntheatre performers, who preferred interactions with our model-augmented agent\nover an unaugmented agent.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 18:48:19 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Mathewson", "Kory W.", ""], ["Castro", "Pablo Samuel", ""], ["Cherry", "Colin", ""], ["Foster", "George", ""], ["Bellemare", "Marc G.", ""]]}]