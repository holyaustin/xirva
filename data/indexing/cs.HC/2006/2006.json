[{"id": "2006.00059", "submitter": "Mehul Bhatt", "authors": "Vasiliki Kondyli and Mehul Bhatt and Jakob Suchan", "title": "Towards a Human-Centred Cognitive Model of Visuospatial Complexity in\n  Everyday Driving", "comments": "9th European Starting AI Researchers Symposium (STAIRS), at ECAI\n  2020, the 24th European Conference on Artificial Intelligence (ECAI).,\n  Santiago de Compostela, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a human-centred, cognitive model of visuospatial complexity in\neveryday, naturalistic driving conditions. With a focus on visual perception,\nthe model incorporates quantitative, structural, and dynamic attributes\nidentifiable in the chosen context; the human-centred basis of the model lies\nin its behavioural evaluation with human subjects with respect to\npsychophysical measures pertaining to embodied visuoauditory attention. We\nreport preliminary steps to apply the developed cognitive model of visuospatial\ncomplexity for human-factors guided dataset creation and benchmarking, and for\nits use as a semantic template for the (explainable) computational analysis of\nvisuospatial complexity.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 20:12:39 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 07:01:09 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Kondyli", "Vasiliki", ""], ["Bhatt", "Mehul", ""], ["Suchan", "Jakob", ""]]}, {"id": "2006.00114", "submitter": "Abdelaziz Lakhfif", "authors": "Abdelaziz Lakhfif", "title": "Design and Implementation of a Virtual 3D Educational Environment to\n  improve Deaf Education", "comments": "Proceedings of the 7th International Symposium ISKO-Maghreb Knowledge\n  Organization in the Perspective of Digital Humanities: Research &\n  Applications November 25th & 26th, 2018, pp. 201-205, Bejaia, Algeria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in NLP, knowledge representation and computer graphic technologies\ncan provide us insights into the development of educational tool for Deaf\npeople. Actual education materials and tools for deaf pupils present several\nproblems, since textbooks are designed to support normal students in the\nclassroom and most of them are not suitable for people with hearing\ndisabilities. Virtual Reality (VR) technologies appear to be a good tool and a\npromising framework in the education of pupils with hearing disabilities. In\nthis paper, we present a current research tasks surrounding the design and\nimplementation of a virtual 3D educational environment based on X3D and H-Anim\nstandards. The system generates and animates automatically Sign language\nsentence from a semantic representation that encode the whole meaning of the\nArabic input text. Some aspects and issues in Sign language generation will be\ndiscussed, including the model of Sign representation that facilitate reuse and\nreduces the time of Sign generation, conversion of semantic components to sign\nfeatures representation with regard to Sign language linguistics\ncharacteristics and how to generate realistic smooth gestural sequences using\nX3D content to performs transition between signs for natural-looking of\nanimated avatar. Sign language sentences were evaluated by Algerian native Deaf\npeople. The goal of the project is the development of a machine translation\nsystem from Arabic to Algerian Sign Language that can be used as educational\ntool for Deaf children in algerian primary schools.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 22:56:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Lakhfif", "Abdelaziz", ""]]}, {"id": "2006.00285", "submitter": "Michael Gastner", "authors": "Shi Tingsheng, Ian K. Duncan, Yen-Ning Chang, Michael T. Gastner", "title": "Motivating Good Practices for the Creation of Contiguous Area Cartograms", "comments": "10 pages, 6 figures, to appear in the Proceedings of the 8th\n  International Conference on Cartography and GIS", "journal-ref": "Proc. 8th Int. Conf. Cartogr. GIS (eds: T. Bandrova, M.\n  Kone\\v{c}n\\'y, S. Marinova), vol. 1, pp. 589--598 (Bulgarian Cartographic\n  Association, Sofia, 2020). ISSN: 1314-0604. URL:\n  https://tinyurl.com/icc8-2020-pdf", "doi": null, "report-no": null, "categories": "cs.HC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartograms are maps in which the areas of regions (e.g., countries or\nprovinces) are proportional to a thematic mapping variable (e.g., population or\ngross domestic product). A cartogram is called contiguous if it keeps\ngeographically adjacent regions connected. Over the past few years, several web\ntools have been developed for the creation of contiguous cartograms. However,\nmost of these tools do not advise how to use cartograms correctly. To mitigate\nthese shortcomings, we attempt to establish good practices through our recently\ndeveloped web application go-cart.io: (1) use cartograms to show numeric data\nthat add up to an interpretable total, (2) present a cartogram alongside a\nconventional map that uses the same color scheme, (3) indicate whether the data\nfor a region are missing, (4) include a legend so that readers can infer the\nmagnitude of the mapping variable, (5) if a cartogram is presented\nelectronically, assist readers with interactive graphics.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 14:41:56 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Tingsheng", "Shi", ""], ["Duncan", "Ian K.", ""], ["Chang", "Yen-Ning", ""], ["Gastner", "Michael T.", ""]]}, {"id": "2006.00372", "submitter": "Nava Haghighi", "authors": "Nava Haghighi, Nathalie Vladis, Yuanbo Liu, Arvind Satyanarayan", "title": "The Effectiveness of Haptic Properties Under Cognitive Load: An\n  Exploratory Study", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of wearables, haptic interfaces are increasingly favored to\ncommunicate information in an ambient manner. Despite this expectation,\nexisting guidelines are developed in studies where the participant's focus is\nentirely on the haptic task. In this work, we systematically study the\ncognitive load imposed by properties of a haptic signal. Participants wear a\nhaptic device on their forearm, and are asked to perform a 1-back task. Each\nexperimental condition isolates an individual property of the haptic signal\n(e.g., amplitude, waveform, rhythm) and participants are asked to identify the\ngradient of the data. We evaluate each condition across 16 participants,\nmeasuring participants' response times, error rates, and qualitative and\nquantitative surveys (e.g., NASA TLX). Our results indicate that gender and\nlanguage differences may impact preference for some properties, that\nparticipants prefer properties that can be rapidly identified, and that\namplitude imposes the lowest cognitive load.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 21:48:12 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 13:07:17 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Haghighi", "Nava", ""], ["Vladis", "Nathalie", ""], ["Liu", "Yuanbo", ""], ["Satyanarayan", "Arvind", ""]]}, {"id": "2006.00410", "submitter": "Zhu Wang", "authors": "Zhu Wang, Anat Lubetzky, Charles Hendee, Marta Gospodarek, Ken Perlin", "title": "A Virtual Obstacle Course within Diverse Sensory Environments", "comments": "Immersive Pavilion, Siggraph 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a novel assessment platform with untethered virtual reality,\n3-dimensional sounds, and pressure sensing floor mat to help assess the walking\nbalance and negotiation of obstacles given diverse sensory load and/or\ncognitive load. The platform provides an immersive 3D city-like scene with\nanticipated/unanticipated virtual obstacles. Participants negotiate the\nobstacles with perturbations of: auditory load by spatial audio, cognitive load\nby a memory task, and visual flow by generated by avatars movements at various\namounts and speeds. A VR headset displays the scenes while providing real-time\nposition and orientation of the participant's head. A pressure-sensing walkway\nsenses foot pressure and visualizes it in a heatmap. The system helps to assess\nwalking balance via pressure dynamics per foot, success rate of crossing\nobstacles, available response time as well as head kinematics in response to\nobstacles and multitasking. Based on the assessment, specific balance training\nand fall prevention program can be prescribed.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 02:03:45 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wang", "Zhu", ""], ["Lubetzky", "Anat", ""], ["Hendee", "Charles", ""], ["Gospodarek", "Marta", ""], ["Perlin", "Ken", ""]]}, {"id": "2006.00421", "submitter": "Nil-Jana Akpinar", "authors": "Nil-Jana Akpinar, Aaditya Ramdas, Umut Acar", "title": "Analyzing Student Strategies In Blended Courses Using Clickstream Data", "comments": null, "journal-ref": "International Conference on Educational Data Mining 2020", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Educational software data promises unique insights into students' study\nbehaviors and drivers of success. While much work has been dedicated to\nperformance prediction in massive open online courses, it is unclear if the\nsame methods can be applied to blended courses and a deeper understanding of\nstudent strategies is often missing. We use pattern mining and models borrowed\nfrom Natural Language Processing (NLP) to understand student interactions and\nextract frequent strategies from a blended college course. Fine-grained\nclickstream data is collected through Diderot, a non-commercial educational\nsupport system that spans a wide range of functionalities. We find that\ninteraction patterns differ considerably based on the assessment type students\nare preparing for, and many of the extracted features can be used for reliable\nperformance prediction. Our results suggest that the proposed hybrid NLP\nmethods can provide valuable insights even in the low-data setting of blended\ncourses given enough data granularity.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 03:01:00 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Akpinar", "Nil-Jana", ""], ["Ramdas", "Aaditya", ""], ["Acar", "Umut", ""]]}, {"id": "2006.00592", "submitter": "Sahan Bulathwela", "authors": "Sahan Bulathwela, Mar\\'ia P\\'erez-Ortiz, Aldo Lipani, Emine Yilmaz and\n  John Shawe-Taylor", "title": "Predicting Engagement in Video Lectures", "comments": "In Proceedings of International Conference on Educational Data Mining\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion of Open Educational Resources (OERs) in the recent years\ncreates the demand for scalable, automatic approaches to process and evaluate\nOERs, with the end goal of identifying and recommending the most suitable\neducational materials for learners. We focus on building models to find the\ncharacteristics and features involved in context-agnostic engagement (i.e.\npopulation-based), a seldom researched topic compared to other contextualised\nand personalised approaches that focus more on individual learner engagement.\nLearner engagement, is arguably a more reliable measure than popularity/number\nof views, is more abundant than user ratings and has also been shown to be a\ncrucial component in achieving learning outcomes. In this work, we explore the\nidea of building a predictive model for population-based engagement in\neducation. We introduce a novel, large dataset of video lectures for predicting\ncontext-agnostic engagement and propose both cross-modal and modality-specific\nfeature sets to achieve this task. We further test different strategies for\nquantifying learner engagement signals. We demonstrate the use of our approach\nin the case of data scarcity. Additionally, we perform a sensitivity analysis\nof the best performing model, which shows promising performance and can be\neasily integrated into an educational recommender system for OERs.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 19:28:16 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 15:33:02 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bulathwela", "Sahan", ""], ["P\u00e9rez-Ortiz", "Mar\u00eda", ""], ["Lipani", "Aldo", ""], ["Yilmaz", "Emine", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "2006.00622", "submitter": "Thorir Mar Ingolfsson", "authors": "Thorir Mar Ingolfsson, Michael Hersche, Xiaying Wang, Nobuaki\n  Kobayashi, Lukas Cavigelli, Luca Benini", "title": "EEG-TCNet: An Accurate Temporal Convolutional Network for Embedded\n  Motor-Imagery Brain-Machine Interfaces", "comments": "8 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning (DL) has contributed significantly to the\nimprovement of motor-imagery brain-machine interfaces (MI-BMIs) based on\nelectroencephalography(EEG). While achieving high classification accuracy, DL\nmodels have also grown in size, requiring a vast amount of memory and\ncomputational resources. This poses a major challenge to an embedded BMI\nsolution that guarantees user privacy, reduced latency, and low power\nconsumption by processing the data locally. In this paper, we propose\nEEG-TCNet, a novel temporal convolutional network (TCN) that achieves\noutstanding accuracy while requiring few trainable parameters. Its low memory\nfootprint and low computational complexity for inference make it suitable for\nembedded classification on resource-limited devices at the edge. Experimental\nresults on the BCI Competition IV-2a dataset show that EEG-TCNet achieves\n77.35% classification accuracy in 4-class MI. By finding the optimal network\nhyperparameters per subject, we further improve the accuracy to 83.84%.\nFinally, we demonstrate the versatility of EEG-TCNet on the Mother of All BCI\nBenchmarks (MOABB), a large scale test benchmark containing 12 different EEG\ndatasets with MI experiments. The results indicate that EEG-TCNet successfully\ngeneralizes beyond one single dataset, outperforming the current\nstate-of-the-art (SoA) on MOABB by a meta-effect of 0.25.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 21:45:45 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ingolfsson", "Thorir Mar", ""], ["Hersche", "Michael", ""], ["Wang", "Xiaying", ""], ["Kobayashi", "Nobuaki", ""], ["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "2006.00802", "submitter": "Enrica Loria", "authors": "Enrica Loria and Johanna Pirker and Anders Drachen and Annapaola\n  Marconi", "title": "Do Influencers Influence? -- Analyzing Players' Activity in an Online\n  Multiplayer Game", "comments": "accepted for publication in IEEE Conference on Games (CoG) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social and online media, influencers have traditionally been understood as\nhighly visible individuals. Recent outcomes suggest that people are likely to\nmimic influencers' behavior, which can be exploited, for instance, in marketing\nstrategies. Also in the Games User Research field, the interest in studying\nplayer social networks has emerged due to the heavy reliance on online\ninfluencers in marketing campaigns for games, as well as in keeping players\nengaged. Despite the inherent value of those individuals, it is still difficult\nto identify influencers, as the definition of influencers is a debated topic.\nThus, how can we identify influencers, and are they indeed the individuals\nimpacting others' behavior? In this work, we focus on influence in retention to\nverify whether central players impacted others' permanence in the game. We\nidentified the central players in the social network built from the competitive\nplayer-vs-player (PvP) multiplayer (Crucible) matches in the online shooter\nDestiny. Then, we computed influence scores for each player evaluating the\nincrease in similarity over time between two connected individuals. In this\npaper, we were able to show the first indications that the traditional metrics\nfor influencers do not necessarily apply for games. On the contrary, we found\nthat the group of central players was distinct from the group of influential\nplayers, defined as the individuals with the highest influence scores. Then, we\nprovide an analysis of the two groups.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 09:08:05 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Loria", "Enrica", ""], ["Pirker", "Johanna", ""], ["Drachen", "Anders", ""], ["Marconi", "Annapaola", ""]]}, {"id": "2006.00823", "submitter": "Hongjia Wu", "authors": "Hongjia Wu, Mengdi Liu", "title": "A Survey on Universal Design for Fitness Wearable Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the visions of Internet of Things and 5G communications, recent\nyears have seen a paradigm shift in personal mobile devices, from smartphones\ntowards wearable devices. Wearable devices come in many different forms\ntargeting different application scenarios. Among these, the fitness wearable\ndevices (FWDs) are proven to be one of the forms that intrigue the market and\noccupy an increasing trend in terms of the market share. Nevertheless, although\nthe fitness wearable devices nowadays are functionally self-contained based on\nthe advanced sensor, computation, and communicative technologies, there is\nstill a large gap to truly satisfy the target customer group, i.e., accessible\nto and usable by a larger quantity of users. This fuels the research area on\napplying the universal design principles to fitness wearable devices. In this\nsurvey, we first present the background of FWDs and show the acceptance and\nadaption challenges of the corresponding user groups. We then review the\nuniversal design principle and how it and its relative approaches could be used\nin FWDs. Further, we collect the available FWDs that bear the universal design\nprinciples in their development circles. Last, we open up the discussion based\non the surveyed literature and provide the insight of potential future work.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 10:03:49 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wu", "Hongjia", ""], ["Liu", "Mengdi", ""]]}, {"id": "2006.00825", "submitter": "Javier Hernandez-Ortega", "authors": "Javier Hernandez-Ortega, Roberto Daza, Aythami Morales, Julian\n  Fierrez, Ruben Tolosana", "title": "Heart Rate Estimation from Face Videos for Student Assessment:\n  Experiments on edBB", "comments": "Accepted in \"IEEE Computer Society Signature Conference on Computers,\n  Software and Applications (COMPSAC) 2020\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we estimate the heart rate from face videos for student\nassessment. This information could be very valuable to track their status along\ntime and also to estimate other data such as their attention level or the\npresence of stress that may be caused by cheating attempts. The recent\nedBBplat, a platform for student behavior modelling in remote education, is\nconsidered in this study1. This platform permits to capture several signals\nfrom a set of sensors that capture biometric and behavioral data: RGB and near\ninfrared cameras, microphone, EEG band, mouse, smartwatch, and keyboard, among\nothers. In the experimental framework of this study, we focus on the RGB and\nnear-infrared video sequences for performing heart rate estimation applying\nremote photoplethysmography techniques. The experiments include behavioral and\nphysiological data from 25 different students completing a collection of tasks\nrelated to e-learning. Our proposed face heart rate estimation approach is\ncompared with the heart rate provided by the smartwatch, achieving very\npromising results for its future deployment in e-learning applications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 10:04:36 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hernandez-Ortega", "Javier", ""], ["Daza", "Roberto", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Tolosana", "Ruben", ""]]}, {"id": "2006.00860", "submitter": "Inken Hagestedt", "authors": "Inken Hagestedt (1), Michael Backes (1), Andreas Bulling (2) ((1)\n  CISPA Helmholtz Center for Information Security, (2) University of Stuttgart)", "title": "Adversarial Attacks on Classifiers for Eye-based User Modelling", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3379157.3390511", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ever-growing body of work has demonstrated the rich information content\navailable in eye movements for user modelling, e.g. for predicting users'\nactivities, cognitive processes, or even personality traits. We show that\nstate-of-the-art classifiers for eye-based user modelling are highly vulnerable\nto adversarial examples: small artificial perturbations in gaze input that can\ndramatically change a classifier's predictions. We generate these adversarial\nexamples using the Fast Gradient Sign Method (FGSM) that linearises the\ngradient to find suitable perturbations. On the sample task of eye-based\ndocument type recognition we study the success of different adversarial attack\nscenarios: with and without knowledge about classifier gradients (white-box vs.\nblack-box) as well as with and without targeting the attack to a specific\nclass, In addition, we demonstrate the feasibility of defending against\nadversarial attacks by adding adversarial examples to a classifier's training\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 11:42:04 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Hagestedt", "Inken", ""], ["Backes", "Michael", ""], ["Bulling", "Andreas", ""]]}, {"id": "2006.00871", "submitter": "Raz Saremi", "authors": "Denisse Martinez Mejorado, Razieh Saremi, Ye Yang, and Jose E.\n  Ramirez-Marquez", "title": "Study on Patterns and Effect of Task Diversity in Software Crowdsourcing", "comments": "10 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: The success of software crowdsourcing depends on steady tasks supply\nand active worker pool. Existing analysis reveals an average task failure ratio\nof 15.7% in software crowdsourcing market. Goal: The objective of this study is\nto empirically investigate patterns and effect of task diversity in software\ncrowdsourcing platform in order to improve the success and efficiency of\nsoftware crowdsourcing. Method: We propose a conceptual task diversity model,\nand develop an approach to measuring and analyzing task diversity.More\nspecifically, this includes grouping similar tasks, ranking them based on their\ncompetition level and identifying the dominant attributes that distinguish\namong these levels, and then studying the impact of task diversity on task\nsuccess and worker performance in crowdsourcing platform. The empirical study\nis conducted on more than one year's real-world data from TopCoder, the leading\nsoftware crowdsourcing platform. Results: We identified that monetary prize and\ntask complexity are the dominant attributes that differentiate among different\ncompetition levels. Based on these dominant attributes, we found three task\ndiversity patterns (configurations) from workers behavior perspective:\nresponsive to prize, responsive to prize and complexity and over responsive to\nprize. This study supports that1) responsive to prize configuration provides\nhighest level of task density and workers' reliability in a platform; 2)\nresponsive to prize and complexity configuration leads to attracting high level\nof trustworthy workers; 3) over responsive to prize configuration results in\nhighest task stability and the lowest failure ratio in the platform for not\nhigh similar tasks.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 02:07:33 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 22:22:36 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Mejorado", "Denisse Martinez", ""], ["Saremi", "Razieh", ""], ["Yang", "Ye", ""], ["Ramirez-Marquez", "Jose E.", ""]]}, {"id": "2006.00882", "submitter": "Adrien Bennetot", "authors": "Adrien Bennetot, Vicky Charisi, Natalia D\\'iaz-Rodr\\'iguez", "title": "Should artificial agents ask for help in human-robot collaborative\n  problem-solving?", "comments": "Accepted at Brain-PIL Workshop - ICRA2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring as fast as possible the functioning of our brain to artificial\nintelligence is an ambitious goal that would help advance the state of the art\nin AI and robotics. It is in this perspective that we propose to start from\nhypotheses derived from an empirical study in a human-robot interaction and to\nverify if they are validated in the same way for children as for a basic\nreinforcement learning algorithm. Thus, we check whether receiving help from an\nexpert when solving a simple close-ended task (the Towers of Hano\\\"i) allows to\naccelerate or not the learning of this task, depending on whether the\nintervention is canonical or requested by the player. Our experiences have\nallowed us to conclude that, whether requested or not, a Q-learning algorithm\nbenefits in the same way from expert help as children do.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 09:15:30 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Bennetot", "Adrien", ""], ["Charisi", "Vicky", ""], ["D\u00edaz-Rodr\u00edguez", "Natalia", ""]]}, {"id": "2006.01048", "submitter": "Raz Saremi", "authors": "Jordan Urbaczek, Razieh Saremi, Mostaan Lotfalian Saremi, and Julian\n  Togelius", "title": "Greedy Scheduling: A Neural Network Method to Reduce Task Failure in\n  Software Crowdsourcing", "comments": "7 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Highly dynamic and competitive crowdsourcing software development\n(CSD) marketplaces may experience task failure due to unforeseen reasons, such\nas increased competition over shared supplier resources, or uncertainty\nassociated with a dynamic worker supply. Existing analysis reveals an average\ntask failure ratio of 15.7\\% in software crowdsourcing markets.\n  Goal: The objective of this study is to provide a task scheduling\nrecommendation model for software crowdsourcing platforms in order to improve\nthe success and efficiency of software crowdsourcing.\n  Method: We propose a task scheduling method based on neural networks, and\ndevelop a system that can predict and analyze task failure probability upon\narrival. More specifically, the model uses a range of input variables,\nincluding the number of open tasks in the platform, the average task similarity\nbetween arriving tasks and open tasks, the winner's monetary prize, and task\nduration, to predict the probability of task failure on the planned arrival\ndate and two surplus days. This prediction will offer the recommended day\nassociated with the lowest task failure probability to post the task. The\nproposed model is based on the workflow and data of Topcoder, one of the\nprimary software crowdsourcing platforms.\n  Results: We present a model that suggests the best recommended arrival dates\nfor any task in the project with surplus of two days per task in the project.\nThe model on average provided 4\\% lower failure ratio per project.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 01:42:32 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 22:12:06 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 02:24:44 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Urbaczek", "Jordan", ""], ["Saremi", "Razieh", ""], ["Saremi", "Mostaan Lotfalian", ""], ["Togelius", "Julian", ""]]}, {"id": "2006.01169", "submitter": "Stylianos Paraschiakos", "authors": "Stylianos Paraschiakos, Cl\\'audio Rebelo de S\\'a, Jeremiah Okai, Eline\n  P. Slagboom, Marian Beekman, Arno Knobbe", "title": "RNNs on Monitoring Physical Activity Energy Expenditure in Older People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the quantification of physical activity energy expenditure (PAEE),\nhealth care monitoring has the potential to stimulate vital and healthy ageing,\ninducing behavioural changes in older people and linking these to personal\nhealth gains. To be able to measure PAEE in a monitoring environment, methods\nfrom wearable accelerometers have been developed, however, mainly targeted\ntowards younger people. Since elderly subjects differ in energy requirements\nand range of physical activities, the current models may not be suitable for\nestimating PAEE among the elderly. Because past activities influence present\nPAEE, we propose a modeling approach known for its ability to model sequential\ndata, the Recurrent Neural Network (RNN). To train the RNN for an elderly\npopulation, we used the GOTOV dataset with 34 healthy participants of 60 years\nand older (mean 65 years old), performing 16 different activities. We used\naccelerometers placed on wrist and ankle, and measurements of energy counts by\nmeans of indirect calorimetry. After optimization, we propose an architecture\nconsisting of an RNN with 3 GRU layers and a feedforward network combining both\naccelerometer and participant-level data. In this paper, we describe our\nefforts to go beyond the standard facilities of a GRU-based RNN, with the aim\nof achieving accuracy surpassing the state of the art. These efforts include\nswitching aggregation function from mean to dispersion measures (SD, IQR, ...),\ncombining temporal and static data (person-specific details such as age,\nweight, BMI) and adding symbolic activity data as predicted by a previously\ntrained ML model. The resulting architecture manages to increase its\nperformance by approximatelly 10% while decreasing training input by a factor\nof 10. It can thus be employed to investigate associations of PAEE with\nvitality parameters related to metabolic and cognitive health and mental\nwell-being.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 18:02:53 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Paraschiakos", "Stylianos", ""], ["de S\u00e1", "Cl\u00e1udio Rebelo", ""], ["Okai", "Jeremiah", ""], ["Slagboom", "Eline P.", ""], ["Beekman", "Marian", ""], ["Knobbe", "Arno", ""]]}, {"id": "2006.01310", "submitter": "Antonio Ricardo Alexandre Brasil", "authors": "Antonio Ricardo Alexandre Brasil and Jefferson Oliveira Andrade and\n  Karin Satie Komati", "title": "Eye Movements Biometrics: A Bibliometric Analysis from 2004 to 2019", "comments": "9 pages, 2 figures, journal", "journal-ref": "International Journal of Computer Applications 176(24):1-9, May\n  2020", "doi": "10.5120/ijca2020920243", "report-no": null, "categories": "cs.HC cs.CV cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person identification based on eye movements is getting more and more\nattention, as it is anti-spoofing resistant and can be useful for continuous\nauthentication. Therefore, it is noteworthy for researchers to know who and\nwhat is relevant in the field, including authors, journals, conferences, and\ninstitutions. This paper presents a comprehensive quantitative overview of the\nfield of eye movement biometrics using a bibliometric approach. All data and\nanalyses are based on documents written in English published between 2004 and\n2019. Scopus was used to perform information retrieval. This research focused\non temporal evolution, leading authors, most cited papers, leading journals,\ncompetitions and collaboration networks.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 23:14:10 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Brasil", "Antonio Ricardo Alexandre", ""], ["Andrade", "Jefferson Oliveira", ""], ["Komati", "Karin Satie", ""]]}, {"id": "2006.01353", "submitter": "Bon Adriel Aseniero", "authors": "Bon Adriel Aseniero, Charles Perin, Wesley Willett, Anthony Tang,\n  Sheelagh Carpendale", "title": "Activity River: Visualizing Planned and Logged Personal Activities for\n  Reflection", "comments": "9 pages, 6 figures, AVI '20, September 28-October 2, 2020, Salerno,\n  Italy 2020 Association for Computing Machinery", "journal-ref": null, "doi": "10.1145/3399715.3399921", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Activity River, a personal visualization tool which enables\nindividuals to plan, log, and reflect on their self-defined activities. We are\ninterested in supporting this type of reflective practice as prior work has\nshown that reflection can help people plan and manage their time effectively.\nHence, we designed Activity River based on five design goals (visualize\nhistorical and contextual data, facilitate comparison of goals and\nachievements, engage viewers with delightful visuals, support authorship, and\nenable flexible planning and logging) which we distilled from the Information\nVisualization and Human-Computer Interaction literature. To explore our\napproach's strengths and limitations, we conducted a qualitative study of\nActivity River using a role-playing method. Through this qualitative\nexploration, we illustrate how our participants envisioned using our\nvisualization to perform dynamic and continuous reflection on their activities.\nWe observed that they were able to assess their progress towards their plans\nand adapt to unforeseen circumstances using our tool.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 02:40:40 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Aseniero", "Bon Adriel", ""], ["Perin", "Charles", ""], ["Willett", "Wesley", ""], ["Tang", "Anthony", ""], ["Carpendale", "Sheelagh", ""]]}, {"id": "2006.01460", "submitter": "Satwik Kottur", "authors": "Seungwhan Moon, Satwik Kottur, Paul A. Crook, Ankita De, Shivani\n  Poddar, Theodore Levin, David Whitney, Daniel Difranco, Ahmad Beirami,\n  Eunjoon Cho, Rajen Subba, Alborz Geramifard", "title": "Situated and Interactive Multimodal Conversations", "comments": "20 pages, 5 figures, 11 tables, accepted to COLING 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation virtual assistants are envisioned to handle multimodal inputs\n(e.g., vision, memories of previous interactions, in addition to the user's\nutterances), and perform multimodal actions (e.g., displaying a route in\naddition to generating the system's utterance). We introduce Situated\nInteractive MultiModal Conversations (SIMMC) as a new direction aimed at\ntraining agents that take multimodal actions grounded in a co-evolving\nmultimodal input context in addition to the dialog history. We provide two\nSIMMC datasets totalling ~13K human-human dialogs (~169K utterances) using a\nmultimodal Wizard-of-Oz (WoZ) setup, on two shopping domains: (a) furniture\n(grounded in a shared virtual environment) and, (b) fashion (grounded in an\nevolving set of images). We also provide logs of the items appearing in each\nscene, and contextual NLU and coreference annotations, using a novel and\nunified framework of SIMMC conversational acts for both user and assistant\nutterances. Finally, we present several tasks within SIMMC as objective\nevaluation protocols, such as Structural API Prediction and Response\nGeneration. We benchmark a collection of existing models on these SIMMC tasks\nas strong baselines, and demonstrate rich multimodal conversational\ninteractions. Our data, annotations, code, and models are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 09:02:23 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 20:21:19 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Moon", "Seungwhan", ""], ["Kottur", "Satwik", ""], ["Crook", "Paul A.", ""], ["De", "Ankita", ""], ["Poddar", "Shivani", ""], ["Levin", "Theodore", ""], ["Whitney", "David", ""], ["Difranco", "Daniel", ""], ["Beirami", "Ahmad", ""], ["Cho", "Eunjoon", ""], ["Subba", "Rajen", ""], ["Geramifard", "Alborz", ""]]}, {"id": "2006.01644", "submitter": "Luis Leiva", "authors": "Ioannis Arapakis and Luis A. Leiva", "title": "Learning Efficient Representations of Mouse Movements to Predict User\n  Attention", "comments": "arXiv admin note: text overlap with arXiv:2001.07803", "journal-ref": "Proceedings of the 43rd Intl. ACM SIGIR Conf. on Research and\n  Development in Information Retrieval (SIGIR), 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking mouse cursor movements can be used to predict user attention on\nheterogeneous page layouts like SERPs. So far, previous work has relied heavily\non handcrafted features, which is a time-consuming approach that often requires\ndomain expertise. We investigate different representations of mouse cursor\nmovements, including time series, heatmaps, and trajectory-based images, to\nbuild and contrast both recurrent and convolutional neural networks that can\npredict user attention to direct displays, such as SERP advertisements. Our\nmodels are trained over raw mouse cursor data and achieve competitive\nperformance. We conclude that neural network models should be adopted for\ndownstream tasks involving mouse cursor movements, since they can provide an\ninvaluable implicit feedback signal for re-ranking and evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 09:52:26 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Arapakis", "Ioannis", ""], ["Leiva", "Luis A.", ""]]}, {"id": "2006.01862", "submitter": "Hussein Mozannar", "authors": "Hussein Mozannar, David Sontag", "title": "Consistent Estimators for Learning to Defer to an Expert", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms are often used in conjunction with expert decision makers\nin practical scenarios, however this fact is largely ignored when designing\nthese algorithms. In this paper we explore how to learn predictors that can\neither predict or choose to defer the decision to a downstream expert. Given\nonly samples of the expert's decisions, we give a procedure based on learning a\nclassifier and a rejector and analyze it theoretically. Our approach is based\non a novel reduction to cost sensitive learning where we give a consistent\nsurrogate loss for cost sensitive learning that generalizes the cross entropy\nloss. We show the effectiveness of our approach on a variety of experimental\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 18:21:38 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 01:43:06 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 01:43:28 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mozannar", "Hussein", ""], ["Sontag", "David", ""]]}, {"id": "2006.01908", "submitter": "Ashok Goel", "authors": "Ashok Goel", "title": "AI-Powered Learning: Making Education Accessible, Affordable, and\n  Achievable", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have developed an AI-powered socio-technical system for making online\nlearning in higher education more accessible, affordable and achievable. In\nparticular, we have developed four novel and intertwined AI technologies: (1)\nVERA, a virtual experimentation research assistant for supporting inquiry-based\nlearning of scientific knowledge, (2) Jill Watson Q&A, a virtual teaching\nassistant for answering questions based on educational documents including the\nVERA user reference guide, (3) Jill Watson SA, a virtual social agent that\npromotes online interactions, and (4) Agent Smith, that helps generate a Jill\nWatson Q&A agent for new documents such as class syllabi. The results are\npositive: (i) VERA enhances ecological knowledge and is freely available\nonline; (ii) Jill Watson Q&A has been used by >4,000 students in >12 online\nclasses and saved teachers >500 hours of work; (iii) Jill Q&A and Jill Watson\nSA promote learner engagement, interaction, and community; and (iv). Agent\nSmith helps generate Jill Watson Q&A for a new syllabus within ~25 hours. Put\ntogether, these innovative technologies help make online learning\nsimultaneously more accessible (by making materials available online),\naffordable (by saving teacher time), and achievable (by providing learning\nassistance and fostering student engagement).\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:41:52 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Goel", "Ashok", ""]]}, {"id": "2006.01916", "submitter": "Ingyu Jason Choi", "authors": "Jason Ingyu Choi, Eugene Agichtein", "title": "Quantifying the Effects of Prosody Modulation on User Engagement and\n  Satisfaction in Conversational Systems", "comments": "Published in CHIIR 2020, 4 pages", "journal-ref": null, "doi": "10.1145/3343413.3378009", "report-no": null, "categories": "cs.HC cs.AI cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As voice-based assistants such as Alexa, Siri, and Google Assistant become\nubiquitous, users increasingly expect to maintain natural and informative\nconversations with such systems. However, for an open-domain conversational\nsystem to be coherent and engaging, it must be able to maintain the user's\ninterest for extended periods, without sounding boring or annoying. In this\npaper, we investigate one natural approach to this problem, of modulating\nresponse prosody, i.e., changing the pitch and cadence of the response to\nindicate delight, sadness or other common emotions, as well as using\npre-recorded interjections. Intuitively, this approach should improve the\nnaturalness of the conversation, but attempts to quantify the effects of\nprosodic modulation on user satisfaction and engagement remain challenging. To\naccomplish this, we report results obtained from a large-scale empirical study\nthat measures the effects of prosodic modulation on user behavior and\nengagement across multiple conversation domains, both immediately after each\nturn, and at the overall conversation level. Our results indicate that the\nprosody modulation significantly increases both immediate and overall user\nsatisfaction. However, since the effects vary across different domains, we\nverify that prosody modulations do not substitute for coherent, informative\ncontent of the responses. Together, our results provide useful tools and\ninsights for improving the naturalness of responses in conversational systems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:53:13 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Choi", "Jason Ingyu", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2006.01921", "submitter": "Ingyu Jason Choi", "authors": "Jason Ingyu Choi, Ali Ahmadvand, Eugene Agichtein", "title": "Offline and Online Satisfaction Prediction in Open-Domain Conversational\n  Systems", "comments": "Published in CIKM '19, 10 pages", "journal-ref": null, "doi": "10.1145/3357384.3358047", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting user satisfaction in conversational systems has become critical,\nas spoken conversational assistants operate in increasingly complex domains.\nOnline satisfaction prediction (i.e., predicting satisfaction of the user with\nthe system after each turn) could be used as a new proxy for implicit user\nfeedback, and offers promising opportunities to create more responsive and\neffective conversational agents, which adapt to the user's engagement with the\nagent. To accomplish this goal, we propose a conversational satisfaction\nprediction model specifically designed for open-domain spoken conversational\nagents, called ConvSAT. To operate robustly across domains, ConvSAT aggregates\nmultiple representations of the conversation, namely the conversation history,\nutterance and response content, and system- and user-oriented behavioral\nsignals. We first calibrate ConvSAT performance against state of the art\nmethods on a standard dataset (Dialogue Breakdown Detection Challenge) in an\nonline regime, and then evaluate ConvSAT on a large dataset of conversations\nwith real users, collected as part of the Alexa Prize competition. Our\nexperimental results show that ConvSAT significantly improves satisfaction\nprediction for both offline and online setting on both datasets, compared to\nthe previously reported state-of-the-art approaches. The insights from our\nstudy can enable more intelligent conversational systems, which could adapt in\nreal-time to the inferred user satisfaction and engagement.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 20:04:56 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Choi", "Jason Ingyu", ""], ["Ahmadvand", "Ali", ""], ["Agichtein", "Eugene", ""]]}, {"id": "2006.01962", "submitter": "Shiwali Mohan", "authors": "Shiwali Mohan, Matt Klenk, Matthew Shreve, Kent Evans, Aaron Ang, John\n  Maxwell", "title": "Characterizing an Analogical Concept Memory for Architectures\n  Implementing the Common Model of Cognition", "comments": "To be presented the Eighth Annual Conference on Advances in Cognitive\n  Systems (ACS 2020) (https://advancesincognitivesystems.github.io/acs/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.RO cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Architectures that implement the Common Model of Cognition - Soar, ACT-R, and\nSigma - have a prominent place in research on cognitive modeling as well as on\ndesigning complex intelligent agents. In this paper, we explore how\ncomputational models of analogical processing can be brought into these\narchitectures to enable concept acquisition from examples obtained\ninteractively. We propose a new analogical concept memory for Soar that\naugments its current system of declarative long-term memories. We frame the\nproblem of concept learning as embedded within the larger context of\ninteractive task learning (ITL) and embodied language processing (ELP). We\ndemonstrate that the analogical learning methods implemented in the proposed\nmemory can quickly learn a diverse types of novel concepts that are useful not\nonly in recognition of a concept in the environment but also in action\nselection. Our approach has been instantiated in an implemented cognitive\nsystem \\textsc{Aileen} and evaluated on a simulated robotic domain.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 21:54:03 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 00:25:25 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 18:02:17 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Mohan", "Shiwali", ""], ["Klenk", "Matt", ""], ["Shreve", "Matthew", ""], ["Evans", "Kent", ""], ["Ang", "Aaron", ""], ["Maxwell", "John", ""]]}, {"id": "2006.02136", "submitter": "Noble Mathews", "authors": "Noble Saji Mathews, Sridhar Chimalakonda, Suresh Jain", "title": "AiR -- An Augmented Reality Application for Visualizing Air Pollution", "comments": "18 pages and 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Air quality is a term used to describe the concentration levels of various\npollutants in the air we breathe. The air quality, which is degrading rapidly\nacross the globe, has been a source of great concern. Across the globe,\ngovernments are taking various measures to reduce air pollution. Bringing\nawareness about environmental pollution among the public plays a major role in\ncontrolling air pollution, as the programs proposed by governments require the\nsupport of the public. Though information on air quality is present on multiple\nportals such as the Central Pollution Control Board (CPCB), which provides Air\nQuality Index that could be accessed by the public. However, such portals are\nscarcely visited by the general public. Visualizing air quality in the location\nwhere an individual resides could help in bringing awareness among the public.\nThis visualization could be rendered using Augmented Reality techniques.\nConsidering the widespread usage of Android based mobile devices in India, and\nthe importance of air quality visualization, we present AiR, as an Android\nbased mobile application. AiR considers the air quality measured by CPCB, in a\nlocality that is detected by the user's GPS or in a locality of user's choice,\nand visualizes various air pollutants present in the locality $(PM_1{}_0,\nPM_2{}_.{}_5, NO_2, SO_2, CO, O_3 \\& NH_3)$ and displays them in the user's\nsurroundings. AiR also creates awareness in an interactive manner about the\ndifferent pollutants, sources, and their impacts on health.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 10:03:47 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Mathews", "Noble Saji", ""], ["Chimalakonda", "Sridhar", ""], ["Jain", "Suresh", ""]]}, {"id": "2006.02187", "submitter": "Pier Luca Lanzi", "authors": "Fabrizia Corona, Alex De Vita, Giovanni Filocamo, Michaela Foa, Pier\n  Luca Lanzi, Amalia Lopopolo, Antonella Petaccia", "title": "Lower Limb Rehabilitation in Juvenile Idiopathic Arthritis using Serious\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patients undergoing physical rehabilitation therapy must perform series of\nexercises regularly over a long period of time to improve, or at least not to\nworsen, their condition. Rehabilitation can easily become boring because of the\ntedious repetition of simple exercises, which can also cause mild pain and\ndiscomfort. As a consequence, patients often fail to follow their\nrehabilitation schedule with the required regularity, thus endangering their\nrecovery. In the last decade, video games have become largely popular and the\navailability of advanced input controllers has made them a viable approach to\nmake physical rehabilitation more entertaining while increasing patients\nmotivation. In this paper, we present a framework integrating serious games for\nthe lower-limb rehabilitation of children suffering from Juvenile Idiopathic\nArthritis (JIA). The framework comprises games that implement parts of the\ntherapeutic protocol followed by the young patients and provides modules to\ntune, control, record, and analyze the therapeutic sessions. We present the\nresult of a preliminary validation we performed with patients at the clinic\nunder therapists supervision. The feedback we received has been overall very\npositive both from patients, who enjoyed performing their usual therapy using\nvideo games, and therapists, who liked how the games could keep the children\nengaged and motivated while performing the usual therapeutic routine.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 11:49:42 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Corona", "Fabrizia", ""], ["De Vita", "Alex", ""], ["Filocamo", "Giovanni", ""], ["Foa", "Michaela", ""], ["Lanzi", "Pier Luca", ""], ["Lopopolo", "Amalia", ""], ["Petaccia", "Antonella", ""]]}, {"id": "2006.02737", "submitter": "Wa\\~nter Morales-Alvarez", "authors": "Walter Morales Alvarez, Nikita Smirnov, Elmar Matthes, Cristina\n  Olaverri-Monreal", "title": "Vehicle Automation Field Test: Impact on Driver Behavior and Trust", "comments": "Paper accepted for submission in The 23rd IEEE International\n  Conference on Intelligent Transportation Systems", "journal-ref": null, "doi": "10.1109/ITSC45102.2020.9294751", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing technological advances in autonomous driving, the transport\nindustry and research community seek to determine the impact that autonomous\nvehicles (AV) will have on consumers, as well as identify the different factors\nthat will influence their use. Most of the research performed so far relies on\nlaboratory-controlled conditions using driving simulators, as they offer a safe\nenvironment for testing advanced driving assistance systems (ADAS). In this\nstudy we analyze the behavior of drivers that are placed in control of an\nautomated vehicle in a real life driving environment. The vehicle is equipped\nwith advanced autonomy, making driver control of the vehicle unnecessary in\nmany scenarios, although a driver take over is possible and sometimes required.\nIn doing so, we aim to determine the impact of such a system on the driver and\ntheir driving performance. To this end road users' behavior from naturalistic\ndriving data is analyzed focusing on awareness and diagnosis of the road\nsituation. Results showed that the road features determined the level of visual\nattention and trust in the automation. They also showed that the activities\nperformed during the automation affected the reaction time to take over the\ncontrol of the vehicle.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 09:52:31 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Alvarez", "Walter Morales", ""], ["Smirnov", "Nikita", ""], ["Matthes", "Elmar", ""], ["Olaverri-Monreal", "Cristina", ""]]}, {"id": "2006.03121", "submitter": "Nathan TeBlunthuis", "authors": "Nathan TeBlunthuis, Benjamin Mako Hill, Aaron Halfaker", "title": "Effects of algorithmic flagging on fairness: quasi-experimental evidence\n  from Wikipedia", "comments": "27 pages, 11 figures, ACM CSCW", "journal-ref": "Proc. ACM Hum.-Comput. Interact. 5, CSCW1, Article 56 (April\n  2021), 27 pages", "doi": "10.1145/3449130", "report-no": null, "categories": "cs.CY cs.HC cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Online community moderators often rely on social signals such as whether or\nnot a user has an account or a profile page as clues that users may cause\nproblems. Reliance on these clues can lead to overprofiling bias when\nmoderators focus on these signals but overlook the misbehavior of others. We\npropose that algorithmic flagging systems deployed to improve the efficiency of\nmoderation work can also make moderation actions more fair to these users by\nreducing reliance on social signals and making norm violations by everyone else\nmore visible. We analyze moderator behavior in Wikipedia as mediated by\nRCFilters, a system which displays social signals and algorithmic flags, and\nestimate the causal effect of being flagged on moderator actions. We show that\nalgorithmically flagged edits are reverted more often, especially those by\nestablished editors with positive social signals, and that flagging decreases\nthe likelihood that moderation actions will be undone. Our results suggest that\nalgorithmic flagging systems can lead to increased fairness in some contexts\nbut that the relationship is complex and contingent.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 20:25:44 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 00:14:34 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["TeBlunthuis", "Nathan", ""], ["Hill", "Benjamin Mako", ""], ["Halfaker", "Aaron", ""]]}, {"id": "2006.03196", "submitter": "Runsheng Xu", "authors": "Runsheng Xu, Shibo Zhang, Yue Zhao, Peixi Xiong, Allen Yilun Lin,\n  Brent Hecht, Jiaqi Ma", "title": "Towards Better Driver Safety: Empowering Personal Navigation\n  Technologies with Road Safety Awareness", "comments": "Submitted to IEEE Intelligent Transportation System Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has found that navigation systems usually assume that all\nroads are equally safe, directing drivers to dangerous routes, which led to\ncatastrophic consequences. To address this problem, this paper aims to begin\nthe process of adding road safety awareness to navigation systems. To do so, we\nfirst created a definition for road safety that navigation systems can easily\nunderstand by adapting well-established safety standards from transportation\nstudies. Based on this road safety definition, we then developed a machine\nlearning-based road safety classifier that predicts the safety level for road\nsegments using a diverse feature set constructed only from large-scale publicly\navailable geographic data. Evaluations in four different countries show that\nour road safety classifier achieves satisfactory performance. Finally, we\ndiscuss the factors to consider when extending our road safety classifier to\nother regions and potential new safety designs enabled by our road safety\npredictions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 01:44:02 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 18:14:47 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 07:58:26 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Xu", "Runsheng", ""], ["Zhang", "Shibo", ""], ["Zhao", "Yue", ""], ["Xiong", "Peixi", ""], ["Lin", "Allen Yilun", ""], ["Hecht", "Brent", ""], ["Ma", "Jiaqi", ""]]}, {"id": "2006.03519", "submitter": "Dominic Kao", "authors": "Dominic Kao", "title": "Exploring Help Facilities in Game-Making Software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Help facilities have been crucial in helping users learn about software for\ndecades. But despite widespread prevalence of game engines and game editors\nthat ship with many of today's most popular games, there is a lack of empirical\nevidence on how help facilities impact game-making. For instance, certain types\nof help facilities may help users more than others. To better understand help\nfacilities, we created game-making software that allowed us to systematically\nvary the type of help available. We then ran a study of 1646 participants that\ncompared six help facility conditions: 1) Text Help, 2) Interactive Help, 3)\nIntelligent Agent Help, 4) Video Help, 5) All Help, and 6) No Help. Each\nparticipant created their own first-person shooter game level using our\ngame-making software with a randomly assigned help facility condition. Results\nindicate that Interactive Help has a greater positive impact on time spent,\ncontrols learnability, learning motivation, total editor activity, and game\nlevel quality. Video Help is a close second across these same measures.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 15:54:14 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Kao", "Dominic", ""]]}, {"id": "2006.03556", "submitter": "Adam Aviv", "authors": "Raina Samuel and Philipp Markert and Adam J. Aviv and Iulian Neamtiu", "title": "Knock, Knock. Who's There? On the Security of LG's Knock Codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knock Codes are a knowledge-based unlock authentication scheme used on LG\nsmartphones where a user enters a code by tapping or \"knocking\" a sequence on a\n2x2 grid. While a lesser used authentication method, as compared to PINs or\nAndroid patterns, there is likely a large number of Knock Code users; we\nestimate, 700,000--2,500,000 in the US alone. In this paper, we studied Knock\nCodes security asking participants to select codes on mobile devices in three\nsettings: a control treatment, a blocklist treatment, and a treatment with a\nlarger, 2x3 grid. We find that Knock Codes are significantly weaker than other\ndeployed authentication, e.g., PINs or Android patterns. In a simulated\nattacker setting, 2x3 grids offered no additional security, but blocklisting\nwas more beneficial, making Knock Codes' security similar to Android patterns.\nParticipants expressed positive perceptions of Knock Codes, but usability was\nchallenged. SUS values were \"marginal\" or \"ok\" across treatments. Based on\nthese findings, we recommend deploying blacklists for selecting a Knock Code\nbecause it improves security but has limited impact on usability perceptions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 17:10:01 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 14:20:19 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2020 15:58:29 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Samuel", "Raina", ""], ["Markert", "Philipp", ""], ["Aviv", "Adam J.", ""], ["Neamtiu", "Iulian", ""]]}, {"id": "2006.03784", "submitter": "Wonse Jo", "authors": "Wonse Jo, Shyam Sundar Kannan, Go-Eum Cha, Ahreum Lee, and Byung-Cheol\n  Min", "title": "A ROS-based Framework for Monitoring Human and Robot Conditions in a\n  Human-Multi-robot Team", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for monitoring human and robot conditions in\nhuman multi-robot interactions. The proposed framework consists of four\nmodules: 1) human and robot conditions monitoring interface, 2) synchronization\ntime filter, 3) data feature extraction interface, and 4) condition monitoring\ninterface. The framework is based on Robot Operating System (ROS), and it\nsupports physiological and behavioral sensors and devices and robot systems, as\nwell as custom programs. Furthermore, it allows synchronizing the monitoring\nconditions and sharing them simultaneously. In order to validate the proposed\nframework, we present experiment results and analysis obtained from the user\nstudy where 30 human subjects participated and simulated robot experiments.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 05:05:26 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Jo", "Wonse", ""], ["Kannan", "Shyam Sundar", ""], ["Cha", "Go-Eum", ""], ["Lee", "Ahreum", ""], ["Min", "Byung-Cheol", ""]]}, {"id": "2006.03805", "submitter": "Wonse Jo", "authors": "Ahreum Lee, Wonse Jo, Shyam Sundar Kannan, and Byung-Cheol Min", "title": "Investigating the Effect of Deictic Movements of a Multi-robot", "comments": "13 pages, 9 figures;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-robot systems are made up of a team of multiple robots, which provides\nthe advantage of performing complex tasks with high efficiency, flexibility,\nand robustness. Although research on human-robot interaction is ongoing as\nrobots become more readily available and easier to use, the study of\ninteractions between a human and multiple robots represents a relatively new\nfield of research. In particular, how multi-robots could be used for everyday\nusers has not been extensively explored. Additionally, the impact of the\ncharacteristics of multiple robots on human perception and cognition in human\nmulti-robot interaction should be further explored. In this paper, we\nspecifically focus on the benefits of physical affordances generated by the\nmovements of multi-robots, and investigate the effects of deictic movements of\nmulti-robots on information retrieval by conducting a delayed free recall task.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 07:29:07 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Lee", "Ahreum", ""], ["Jo", "Wonse", ""], ["Kannan", "Shyam Sundar", ""], ["Min", "Byung-Cheol", ""]]}, {"id": "2006.03813", "submitter": "Muhammad Zeeshan Baig Dr", "authors": "Muhammad Zeeshan Baig and Manolya Kavakli", "title": "Multimodal Systems: Taxonomy, Methods, and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naturally, humans use multiple modalities to convey information. The\nmodalities are processed both sequentially and in parallel for communication in\nthe human brain, this changes when humans interact with computers. Empowering\ncomputers with the capability to process input multimodally is a major domain\nof investigation in Human-Computer Interaction (HCI). The advancement in\ntechnology (powerful mobile devices, advanced sensors, new ways of output,\netc.) has opened up new gateways for researchers to design systems that allow\nmultimodal interaction. It is a matter of time when the multimodal inputs will\novertake the traditional ways of interactions. The paper provides an\nintroduction to the domain of multimodal systems, explains a brief history,\ndescribes advantages of multimodal systems over unimodal systems, and discusses\nvarious modalities. The input modeling, fusion, and data collection were\ndiscussed. Finally, the challenges in the multimodal systems research were\nlisted. The analysis of the literature showed that multimodal interface systems\nimprove the task completion rate and reduce the errors compared to unimodal\nsystems. The commonly used inputs for multimodal interaction are speech and\ngestures. In the case of multimodal inputs, late integration of input\nmodalities is preferred by researchers because it allows easy update of\nmodalities and corresponding vocabularies.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 08:39:53 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Baig", "Muhammad Zeeshan", ""], ["Kavakli", "Manolya", ""]]}, {"id": "2006.03820", "submitter": "Davide Buffelli", "authors": "Davide Buffelli, Fabio Vandin", "title": "Attention-Based Deep Learning Framework for Human Activity Recognition\n  with User Adaptation", "comments": "Accepted for publication on the IEEE Sensors Journal", "journal-ref": null, "doi": "10.1109/JSEN.2021.3067690", "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor-based human activity recognition (HAR) requires to predict the action\nof a person based on sensor-generated time series data. HAR has attracted major\ninterest in the past few years, thanks to the large number of applications\nenabled by modern ubiquitous computing devices. While several techniques based\non hand-crafted feature engineering have been proposed, the current\nstate-of-the-art is represented by deep learning architectures that\nautomatically obtain high level representations and that use recurrent neural\nnetworks (RNNs) to extract temporal dependencies in the input. RNNs have\nseveral limitations, in particular in dealing with long-term dependencies. We\npropose a novel deep learning framework, \\algname, based on a purely\nattention-based mechanism, that overcomes the limitations of the\nstate-of-the-art. We show that our proposed attention-based architecture is\nconsiderably more powerful than previous approaches, with an average increment,\nof more than $7\\%$ on the F1 score over the previous best performing model.\nFurthermore, we consider the problem of personalizing HAR deep learning models,\nwhich is of great importance in several applications. We propose a simple and\neffective transfer-learning based strategy to adapt a model to a specific user,\nproviding an average increment of $6\\%$ on the F1 score on the predictions for\nthat user. Our extensive experimental evaluation proves the significantly\nsuperior capabilities of our proposed framework over the current\nstate-of-the-art and the effectiveness of our user adaptation technique.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 09:26:07 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 14:41:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Buffelli", "Davide", ""], ["Vandin", "Fabio", ""]]}, {"id": "2006.03846", "submitter": "Himangshu Sarma", "authors": "Himangshu Sarma, Robert Porzel, Jan Smeddinck, Rainer Malaka", "title": "Towards Generating Virtual Movement from Textual Instructions A Case\n  Study in Quality Assessment", "comments": null, "journal-ref": "AAAI HCOMP 2015, San Diego, USA", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many application areas ranging from serious games for health to learning by\ndemonstration in robotics, could benefit from large body movement datasets\nextracted from textual instructions accompanied by images. The interpretation\nof instructions for the automatic generation of the corresponding motions (e.g.\nexercises) and the validation of these movements are difficult tasks. In this\narticle we describe a first step towards achieving automated extraction. We\nhave recorded five different exercises in random order with the help of seven\namateur performers using a Kinect. During the recording, we found that the same\nexercise was interpreted differently by each human performer even though they\nwere given identical textual instructions. We performed a quality assessment\nstudy based on that data using a crowdsourcing approach and tested the\ninter-rater agreement for different types of visualizations, where the RGBbased\nvisualization showed the best agreement among the annotators.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 11:18:30 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Sarma", "Himangshu", ""], ["Porzel", "Robert", ""], ["Smeddinck", "Jan", ""], ["Malaka", "Rainer", ""]]}, {"id": "2006.03899", "submitter": "Aris Kanellopoulos", "authors": "Maria Grammatopoulou, Aris Kanellopoulos, Kyriakos G.~Vamvoudakis,\n  Nathan Lau", "title": "A Multi-step and Resilient Predictive Q-learning Algorithm for IoT with\n  Human Operators in the Loop: A Case Study in Water Supply Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recommending resilient and predictive actions for\nan IoT network in the presence of faulty components, considering the presence\nof human operators manipulating the information of the environment the agent\nsees for containment purposes. The IoT network is formulated as a directed\ngraph with a known topology whose objective is to maintain a constant and\nresilient flow between a source and a destination node. The optimal route\nthrough this network is evaluated via a predictive and resilient Q-learning\nalgorithm which takes into account historical data about irregular operation,\ndue to faults, as well as the feedback from the human operators that are\nconsidered to have extra information about the status of the network concerning\nlocations likely to be targeted by attacks. To showcase our method, we utilize\nanonymized data from Arlington County, Virginia, to compute predictive and\nresilient scheduling policies for a smart water supply system, while avoiding\n(i) all the locations indicated to be attacked according to human operators\n(ii) as many as possible neighborhoods detected to have leaks or other faults.\nThis method incorporates both the adaptability of the human and the computation\ncapability of the machine to achieve optimal implementation containment and\nrecovery actions in water distribution.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 15:51:52 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Grammatopoulou", "Maria", ""], ["Kanellopoulos", "Aris", ""], ["~Vamvoudakis", "Kyriakos G.", ""], ["Lau", "Nathan", ""]]}, {"id": "2006.03931", "submitter": "Alessandro Ecclesie Agazzi", "authors": "Alessandro Ecclesie Agazzi", "title": "Study of the usability of LinkedIn: a social media platform meant to\n  connect employers and employees", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network platforms have increased and become very popular in the last\ndecade; they allow people to create an online account to then interact with\nothers creating a complicated net of connections. LinkedIn is one of the most\nused social media platform, created and used for professional purposes. Here,\nindeed, the user can either apply for job positions or join professional\ncommunities to deepen his own knowledge and expertise and be always up to date\nin the interested field. The primary objectives of this paper are assessing\nLinkedIn's usability, by using both user and expert evaluation and giving\nrecommendations for the developer to improve this social network. This has been\nachieved through different steps; initially, feedbacks have been collected, via\nquestionnaire, from direct users. Later, the usability issues, which have been\nunderlined by users in the questionnaire, have been explored, by simulating\nuser's problem-solving process, through Walkthrough. Finally, the overall\nusability of LinkedIn application has been measured by using SUS (System\nUsability Scale).\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 18:19:45 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Agazzi", "Alessandro Ecclesie", ""]]}, {"id": "2006.04376", "submitter": "Baihan Lin", "authors": "Baihan Lin, Xinxin Zhang", "title": "Speaker Diarization as a Fully Online Learning Problem in MiniVox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a novel machine learning framework to conduct real-time\nmulti-speaker diarization and recognition without prior registration and\npretraining in a fully online learning setting. Our contributions are two-fold.\nFirst, we proposed a new benchmark to evaluate the rarely studied fully online\nspeaker diarization problem. We built upon existing datasets of real world\nutterances to automatically curate MiniVox, an experimental environment which\ngenerates infinite configurations of continuous multi-speaker speech stream.\nSecond, we considered the practical problem of online learning with\nepisodically revealed rewards and introduced a solution based on\nsemi-supervised and self-supervised learning methods. Additionally, we provided\na workable web-based recognition system which interactively handles the cold\nstart problem of new user's addition by transferring representations of old\narms to new ones with an extendable contextual bandit. We demonstrated that our\nproposed method obtained robust performance in the online MiniVox framework.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 06:40:29 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 17:30:57 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 02:56:34 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lin", "Baihan", ""], ["Zhang", "Xinxin", ""]]}, {"id": "2006.04419", "submitter": "Daniel Hernandez Mr", "authors": "Daniel Hernandez, Charles Takashi Toyin Gbadamosi, James Goodman,\n  James Alfred Walker", "title": "Metagame Autobalancing for Competitive Multiplayer Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated game balancing has often focused on single-agent scenarios. In this\npaper we present a tool for balancing multi-player games during game design.\nOur approach requires a designer to construct an intuitive graphical\nrepresentation of their meta-game target, representing the relative scores that\nhigh-level strategies (or decks, or character types) should experience. This\npermits more sophisticated balance targets to be defined beyond a simple\nrequirement of equal win chances. We then find a parameterization of the game\nthat meets this target using simulation-based optimization to minimize the\ndistance to the target graph. We show the capabilities of this tool on examples\ninheriting from Rock-Paper-Scissors, and on a more complex asymmetric fighting\ngame.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 08:55:30 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Hernandez", "Daniel", ""], ["Gbadamosi", "Charles Takashi Toyin", ""], ["Goodman", "James", ""], ["Walker", "James Alfred", ""]]}, {"id": "2006.04631", "submitter": "Dietmar Offenhuber", "authors": "Dietmar Offenhuber", "title": "What we talk about when we talk about data physicality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data physicalizations \"map data to physical form,\" yet many canonical\nexamples are not based on data sets. To address this contradiction, I argue\nthat the practice of physicalization forces us to rethink traditional notions\nof data. This paper proposes a conceptual framework to examine how\nphysicalizations relate to data. This paper develops a two-dimensional\nconceptual space for comparing different perspectives on data used in\nphysicalization, drawing from design theory and critical data studies\nliterature. One axis distinguishes between epistemological and ontological\nperspectives, focusing on the relationship between data and the mind. The\nsecond axis distinguishes how data relate to the world, differentiating between\nrepresentational and relational perspectives. To clarify the aesthetic and\nconceptual implications of these different perspectives, the paper discusses\nexamples of data physicalization for each quadrant of the continuous space. It\nfurther uses the framework to examine the explicit and implicit assumptions\nabout data in physicalization literature. As a theoretical paper, it encourages\npractitioners to think about how data relate to the manifestations and the\nphenomena they try to capture. It invites exploration of the relationship\nbetween data and the world as a generative source of creative tension.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:30:10 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 21:45:53 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Offenhuber", "Dietmar", ""]]}, {"id": "2006.04838", "submitter": "Stela Seo", "authors": "Stela H. Seo, James E. Young, Pourang Irani", "title": "How are your robot friends doing? A design exploration of graphical\n  techniques supporting awareness of robot team members in teleoperation", "comments": "submitted to International Journal of Social Robotics\n  https://www.springer.com/journal/12369/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While teleoperated robots continue to proliferate in domains including search\nand rescue, field exploration, or the military, human error remains a primary\ncause for accidents or mistakes. One challenge is that teleoperating a remote\nrobot is cognitively taxing as the operator needs to understand the robot's\nstate and monitor all its sensor data. In a multi-robot team, an operator needs\nto additionally monitor other robots' progress, states, notifications, errors,\nand so on to maintain team cohesion. One strategy for supporting the operator\nto comprehend this information is to improve teleoperation interface designs to\ncarefully present data. We present a set of prototypes that simplify complex\nteam robot states and actions, with an aim to help the operator to understand\ninformation from the robots easily and quickly. We conduct a series of pilot\nstudies to explore a range of design parameters used in our prototypes (text,\nicon, facial expression, use of color, animation, and number of team robots),\nand develop a set of guidelines for graphically representing team robot states\nin the remote team teleoperation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:02:28 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Seo", "Stela H.", ""], ["Young", "James E.", ""], ["Irani", "Pourang", ""]]}, {"id": "2006.04864", "submitter": "John Noel Victorino", "authors": "John Noel Victorino, Naoto Fukunaga, and Tomohiro Shibata", "title": "Design and Development of an Automated Coimagination Support System", "comments": "6 pages, 9 figures, submitted to The 8th International Conference on\n  Human-Agent Interaction (HAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coimagination method is a novel approach to support interactive communication\nfor activating three (3) cognitive functions: episodic memory, division of\nattention, and planning. These cognitive functions are known to decline at an\nearly stage of mild cognitive impairment (MCI). In previous studies about the\ncoimagination method, experimenters tested different settings in different care\ninstitutions. Out of these experiments, various measures were introduced,\nanalyzed, and presented. However, ease of changing configuration based on\nparticipants, and a quick assessment of captured data remained challenging.\nAlso, several observers and measurers are needed to conduct the coimagination\nmethod. In this paper, we propose the initial design and development of an\nautomated coimagination support system that can handle such challenges. We aim\nto have an automated coimagination support system that can be used easily\neither by healthy participants or elderly participants via a natural voice\ninterface. In this paper, our focus is to measure how well our proposed\nfeatures work with elderly participants. Preliminary experiments were conducted\nwith healthy participants, and notably, with actual elder participants. Healthy\nparticipants experienced longer speaking round and question-and-answer round\nthan with elderly participants; while, the latter had preparation time before\nthe speaking round. In these preliminary experiments, our initial system showed\nthe capability to handle different configurations. Healthy participants have\noperated the system using voice, while elderly participants managed to use the\nsystem with minimal assistance.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 08:14:10 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 15:01:22 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Victorino", "John Noel", ""], ["Fukunaga", "Naoto", ""], ["Shibata", "Tomohiro", ""]]}, {"id": "2006.04882", "submitter": "Os Keyes", "authors": "Calvin Liang, Jevan Hutson and Os Keyes", "title": "Surveillance, Stigma & Sociotechnical Design for HIV", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online dating and hookup platforms have fundamentally changed people's\nday-to-day practices of sex and love-but exist in tension with older social and\nmedicolegal norms. This is particularly the case for people with HIV, who are\nfrequently stigmatized, surveilled, ostracized and incarcerated because of\ntheir status. Efforts to make intimate platforms \"work\" for HIV frequently\nfocus on user-to-user interactions and disclosure of one's HIV status but elide\nboth the structural forces at work in regulating sex and the involvement of the\nstate in queer lives. In an effort to foreground these forces and this\ninvolvement, we analyze the approaches that intimate platforms have taken in\ndesigning for HIV disclosure through a content analysis of 49 current\nplatforms. We argue that the implicit reinforcement of stereotypes about who\nHIV is or is not a concern for, along with the failure to consider state\npractices when designing for data disclosure, opens up serious risks for\nHIV-positive and otherwise marginalized people. While we have no panacea for\nthe tension between disclosure and risk, we point to bottom-up, communal, and\nqueer approaches to design as a way of potentially making that tension easier\nto safely navigate.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 19:03:15 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Liang", "Calvin", ""], ["Hutson", "Jevan", ""], ["Keyes", "Os", ""]]}, {"id": "2006.04959", "submitter": "Rebekah Overdorf", "authors": "Rebekah Overdorf and Christopher Schwartz", "title": "Thinking Taxonomically about Fake Accounts: Classification, False\n  Dichotomies, and the Need for Nuance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often said that war creates a fog in which it becomes difficult to\ndiscern friend from foe on the battlefield. In the ongoing war on fake\naccounts, conscious development of taxonomies of the phenomenon has yet to\noccur, resulting in much confusion on the digital battlefield about what\nexactly a fake account is. This paper intends to address this problem, not by\nproposing a taxonomy of fake accounts, but by proposing a systematic way to\nthink taxonomically about the phenomenon. Specifically, we examine fake\naccounts through both a combined philosophical and computer science-based\nperspective. Through these lenses, we deconstruct narrow binary thinking about\nfake accounts, both in the form of general false dichotomies and specifically\nin relation to the Facebook's conceptual framework \"Coordinated Inauthentic\nBehavior\" (CIB). We then address the false dichotomies by constructing a more\ncomplex way of thinking taxonomically about fake accounts.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 21:40:00 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Overdorf", "Rebekah", ""], ["Schwartz", "Christopher", ""]]}, {"id": "2006.05172", "submitter": "Mete Sertkan", "authors": "Mete Sertkan, Julia Neidhardt, Hannes Werthner", "title": "Eliciting Touristic Profiles: A User Study on Picture Collections", "comments": "Accepted at UMAP 2020 (full paper)", "journal-ref": null, "doi": "10.1145/3340631.3394868", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliciting the preferences and needs of tourists is challenging, since people\noften have difficulties to explicitly express them, especially in the initial\nphase of travel planning. Recommender systems employed at the early stage of\nplanning can therefore be very beneficial to the general satisfaction of a\nuser. Previous studies have explored pictures as a tool of communication and as\na way to implicitly deduce a traveller's preferences and needs. In this paper,\nwe conduct a user study to verify previous claims and conceptual work on the\nfeasibility of modelling travel interests from a selection of a user's\npictures. We utilize fine-tuned convolutional neural networks to compute a\nvector representation of a picture, where each dimension corresponds to a\ntravel behavioural pattern from the traditional Seven-Factor model. In our\nstudy, we followed strict privacy principles and did not save uploaded pictures\nafter computing their vector representation. We aggregate the representations\nof the pictures of a user into a single user representation, i.e., touristic\nprofile, using different strategies. In our user study with 81 participants, we\nlet users adjust the predicted touristic profile and confirm the usefulness of\nour approach. Our results show that given a collection of pictures the\ntouristic profile of a user can be determined.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 10:39:14 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Sertkan", "Mete", ""], ["Neidhardt", "Julia", ""], ["Werthner", "Hannes", ""]]}, {"id": "2006.05175", "submitter": "Antonios Somarakis", "authors": "Antonios Somarakis, Marieke E. Ijsselsteijn, Sietse J. Luk, Boyd\n  Kenkhuis, Noel F.C.C. de Miranda, Boudewijn P.F. Lelieveldt, and Thomas\n  H\\\"ollt", "title": "Visual cohort comparison for spatial single-cell omics-data", "comments": "11 pages, 10 figures, 2 tables. Revised based on IEEE VIS 2020\n  reviewers comments. ACM 2012 CCS - Human-centered computing, Visualization,\n  Visualization application domains, Visual analytics. Binary of the presented\n  tool is available is our repository: https://doi.org/10.5281/zenodo.3885814", "journal-ref": "Presented in IEEE Vis 2020. Published in IEEE Transactions on\n  Visualization and Computer Graphics (TVCG)", "doi": "10.1109/TVCG.2020.3030336", "report-no": null, "categories": "cs.HC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially-resolved omics-data enable researchers to precisely distinguish\ncell types in tissue and explore their spatial interactions, enabling deep\nunderstanding of tissue functionality. To understand what causes or\ndeteriorates a disease and identify related biomarkers, clinical researchers\nregularly perform large-scale cohort studies, requiring the comparison of such\ndata at cellular level. In such studies, with little a-priori knowledge of what\nto expect in the data, explorative data analysis is a necessity. Here, we\npresent an interactive visual analysis workflow for the comparison of cohorts\nof spatially-resolved omics-data. Our workflow allows the comparative analysis\nof two cohorts based on multiple levels-of-detail, from simple abundance of\ncontained cell types over complex co-localization patterns to individual\ncomparison of complete tissue images. As a result, the workflow enables the\nidentification of cohort-differentiating features, as well as outlier samples\nat any stage of the workflow. During the development of the workflow, we\ncontinuously consulted with domain experts. To show the effectiveness of the\nworkflow we conducted multiple case studies with domain experts from different\napplication areas and with different data modalities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 10:47:46 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 14:03:12 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Somarakis", "Antonios", ""], ["Ijsselsteijn", "Marieke E.", ""], ["Luk", "Sietse J.", ""], ["Kenkhuis", "Boyd", ""], ["de Miranda", "Noel F. C. C.", ""], ["Lelieveldt", "Boudewijn P. F.", ""], ["H\u00f6llt", "Thomas", ""]]}, {"id": "2006.05289", "submitter": "Nikhil Churamani", "authors": "Indu P. Bodala, Nikhil Churamani and Hatice Gunes", "title": "Creating a Robot Coach for Mindfulness and Wellbeing: A Longitudinal\n  Study", "comments": "11 pages. Corrected Typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social robots are starting to become incorporated into daily lives by\nassisting in the promotion of physical and mental wellbeing. This paper\ninvestigates the use of social robots for delivering mindfulness sessions. We\ncreated a teleoperated robotic platform that enables an experienced human coach\nto conduct the sessions in a virtual manner by replicating upper body and head\npose in real time. The coach is also able to view the world from the robot's\nperspective and make a conversation with participants by talking and listening\nthrough the robot. We studied how participants interacted with a teleoperated\nrobot mindfulness coach over a period of 5 weeks and compared with the\ninteractions another group of participants had with a human coach. The\nmindfulness sessions delivered by both types of coaching invoked positive\nresponses from the participants for all the sessions. We found that the\nparticipants rated the interactions with human coach consistently high in all\naspects. However, there is a longitudinal change in the ratings for the\ninteraction with the teleoperated robot for the aspects of motion and\nconversation. We also found that the participants' personality traits --\nconscientiousness and neuroticism influenced the perceptions of the robot\ncoach.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 14:31:32 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 09:59:45 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Bodala", "Indu P.", ""], ["Churamani", "Nikhil", ""], ["Gunes", "Hatice", ""]]}, {"id": "2006.05327", "submitter": "Roberto Daza", "authors": "Roberto Daza, Aythami Morales, Julian Fierrez, Ruben Tolosana", "title": "mEBAL: A Multimodal Database for Eye Blink Detection and Attention Level\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents mEBAL, a multimodal database for eye blink detection and\nattention level estimation. The eye blink frequency is related to the cognitive\nactivity and automatic detectors of eye blinks have been proposed for many\ntasks including attention level estimation, analysis of neuro-degenerative\ndiseases, deception recognition, drive fatigue detection, or face\nanti-spoofing. However, most existing databases and algorithms in this area are\nlimited to experiments involving only a few hundred samples and individual\nsensors like face cameras. The proposed mEBAL improves previous databases in\nterms of acquisition sensors and samples. In particular, three different\nsensors are simultaneously considered: Near Infrared (NIR) and RGB cameras to\ncapture the face gestures and an Electroencephalography (EEG) band to capture\nthe cognitive activity of the user and blinking events. Regarding the size of\nmEBAL, it comprises 6,000 samples and the corresponding attention level from 38\ndifferent students while conducting a number of e-learning tasks of varying\ndifficulty. In addition to presenting mEBAL, we also include preliminary\nexperiments on: i) eye blink detection using Convolutional Neural Networks\n(CNN) with the facial images, and ii) attention level estimation of the\nstudents based on their eye blink frequency.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 15:05:08 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 11:11:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Daza", "Roberto", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Tolosana", "Ruben", ""]]}, {"id": "2006.05388", "submitter": "Javier Hernando", "authors": "Micha{\\l} Krzemi\\'nski, Javier Hernando", "title": "End-to-end User Recognition using Touchscreen Biometrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the touchscreen data as behavioural biometrics. The goal was to\ncreate an end-to-end system that can transparently identify users using raw\ndata from mobile devices. The touchscreen biometrics was researched only few\ntimes in series of works with disparity in used methodology and databases. In\nthe proposed system data from the touchscreen goes directly, without any\nprocessing, to the input of a deep neural network, which is able to decide on\nthe identity of the user. No hand-crafted features are used. The implemented\nclassification algorithm tries to find patterns by its own from raw data. The\nachieved results show that the proposed deep model is sufficient enough for the\ngiven identification task. The performed tests indicate high accuracy of user\nidentification and better EER results compared to state of the art systems. The\nbest result achieved by our system is 0.65% EER.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:38:09 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Krzemi\u0144ski", "Micha\u0142", ""], ["Hernando", "Javier", ""]]}, {"id": "2006.05419", "submitter": "Jay Heo", "authors": "Jay Heo, Junhyeon Park, Hyewon Jeong, Kwang Joon Kim, Juho Lee, Eunho\n  Yang, Sung Ju Hwang", "title": "Cost-effective Interactive Attention Learning with Neural Attention\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel interactive learning framework which we refer to as\nInteractive Attention Learning (IAL), in which the human supervisors\ninteractively manipulate the allocated attentions, to correct the model's\nbehavior by updating the attention-generating network. However, such a model is\nprone to overfitting due to scarcity of human annotations, and requires costly\nretraining. Moreover, it is almost infeasible for the human annotators to\nexamine attentions on tons of instances and features. We tackle these\nchallenges by proposing a sample-efficient attention mechanism and a\ncost-effective reranking algorithm for instances and features. First, we\npropose Neural Attention Process (NAP), which is an attention generator that\ncan update its behavior by incorporating new attention-level supervisions\nwithout any retraining. Secondly, we propose an algorithm which prioritizes the\ninstances and the features by their negative impacts, such that the model can\nyield large improvements with minimal human feedback. We validate IAL on\nvarious time-series datasets from multiple domains (healthcare, real-estate,\nand computer vision) on which it significantly outperforms baselines with\nconventional attention mechanisms, or without cost-effective reranking, with\nsubstantially less retraining and human-model interaction cost.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 17:36:41 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Heo", "Jay", ""], ["Park", "Junhyeon", ""], ["Jeong", "Hyewon", ""], ["Kim", "Kwang Joon", ""], ["Lee", "Juho", ""], ["Yang", "Eunho", ""], ["Hwang", "Sung Ju", ""]]}, {"id": "2006.05550", "submitter": "Aykut Isleyen", "authors": "Aykut Isleyen, Yasemin Vardar, Cagatay Basdogan", "title": "Tactile Roughness Perception of Virtual Gratings by Electrovibration", "comments": "Manuscript received June 25, 2019; revised November 15, 2019;\n  accepted December 11, 2019", "journal-ref": "IEEE Transactions on Haptics, 2019. [Online]. Available: http://\n  ieeexplore.ieee.org/document/8933496", "doi": "10.1109/TOH.2019.2959993", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Realistic display of tactile textures on touch screens is a big step forward\nfor haptic technology to reach a wide range of consumers utilizing electronic\ndevices on a daily basis. Since the texture topography cannot be rendered\nexplicitly by electrovibration on touch screens, it is important to understand\nhow we perceive the virtual textures displayed by friction modulation via\nelectrovibration. We investigated the roughness perception of real gratings\nmade of plexiglass and virtual gratings displayed by electrovibration through a\ntouch screen for comparison. In particular, we conducted two psychophysical\nexperiments with 10 participants to investigate the effect of spatial period\nand the normal force applied by finger on roughness perception of real and\nvirtual gratings in macro size. We also recorded the contact forces acting on\nthe participants' finger during the experiments. The results showed that the\nroughness perception of real and virtual gratings are different. We argue that\nthis difference can be explained by the amount of fingerpad penetration into\nthe gratings. For real gratings, penetration increased tangential forces acting\non the finger, whereas for virtual ones where skin penetration is absent,\ntangential forces decreased with spatial period. Supporting our claim, we also\nfound that increasing normal force increases the perceived roughness of real\ngratings while it causes an opposite effect for the virtual gratings. These\nresults are consistent with the tangential force profiles recorded for both\nreal and virtual gratings. In particular, the rate of change in tangential\nforce ($dF_t/dt$) as a function of spatial period and normal force followed\ntrends similar to those obtained for the roughness estimates of real and\nvirtual gratings, suggesting that it is a better indicator of the perceived\nroughness than the tangential force magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 23:20:36 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Isleyen", "Aykut", ""], ["Vardar", "Yasemin", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2006.05709", "submitter": "Giulia Cisotto", "authors": "Andrea Zanella, Federico Mason, Patrik Pluchino, Giulia Cisotto,\n  Valeria Orso, Luciano Gamberini", "title": "Internet of Things for Elderly and Fragile People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the potential of the Internet of Things (IoT) paradigm\nin the context of assisted living for elderly and fragile people, in the light\nof the peculiar requirements of such users, both from a functional and a\ntechnological perspective. We stress some aspects that are often disregarded by\nthe technical community, such as technology acceptability and usability, and we\ndescribe the framework and the phases of the current co-design approaches that\nimply the active involvement of the final users in the system design process.\nThereby, we identify a series of design practices to merge technical and\nfragile people's requirements. The discussion is backed up by the description\nof DOMHO, a prototypal IoT-based AAL system that embodies most of the concepts\ndescribed in the paper, and that is being deployed and tested in a shelter\nhouse for elders, and in an apartment for the co-housing of individuals with\ndisabilities. Finally, we discuss the potential and limits of the current\napproaches and present some open challenges and future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 07:56:59 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Zanella", "Andrea", ""], ["Mason", "Federico", ""], ["Pluchino", "Patrik", ""], ["Cisotto", "Giulia", ""], ["Orso", "Valeria", ""], ["Gamberini", "Luciano", ""]]}, {"id": "2006.05884", "submitter": "Marina Neseem", "authors": "Marina Neseem, Jon Nelson, Sherief Reda", "title": "AdaSense: Adaptive Low-Power Sensing and Activity Recognition for\n  Wearable Devices", "comments": "6 pages, 7 figures, To appear in DAC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable devices have strict power and memory limitations. As a result, there\nis a need to optimize the power consumption on those devices without\nsacrificing the accuracy. This paper presents AdaSense: a sensing, feature\nextraction and classification co-optimized framework for Human Activity\nRecognition. The proposed techniques reduce the power consumption by\ndynamically switching among different sensor configurations as a function of\nthe user activity. The framework selects configurations that represent the\npareto-frontier of the accuracy and energy trade-off. AdaSense also uses\nlow-overhead processing and classification methodologies. The introduced\napproach achieves 69% reduction in the power consumption of the sensor with\nless than 1.5% decrease in the activity recognition accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 15:17:11 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Neseem", "Marina", ""], ["Nelson", "Jon", ""], ["Reda", "Sherief", ""]]}, {"id": "2006.05907", "submitter": "Shahinur Alam", "authors": "Shahinur Alam, Md Sultan Mahmud, Mohammed Yeasin", "title": "Toward Building Safer Smart Homes for the People with Disabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situational awareness is a critical foundation for the protection of human\nlife/properties and is challenging to maintain for people with disabilities\n(i.e., visual impairments and limited mobility). In this paper, we present a\ndialog enabled end-to-end assistive solution called \"SafeAccess\" to build a\nsafer smart home by providing situational awareness. The key functions of\nSafeAccess are: - 1) monitoring homes and identifying incoming persons; 2)\nhelping users in assessing incoming threats (e.g., burglary, robbery, gun\nviolence); and, 3) allowing users to grant safe access to homes for\nfriends/families. In this work, we focus on building a robust model for\ndetecting and recognizing person, generating image descriptions, and designing\na prototype for the smart door. To interact with the system, we implemented a\ndialog enabled smartphone app, especially for creating a personalized profile\nfrom face images or videos of friends/families. A Raspberry pi connected to the\nhome monitoring cameras captures the video frames and performs change detection\nto identify frames with activities. Then, we detect human presence using Faster\nr-cnn and extract faces using Multi-task Cascaded Convolutional Networks\n(MTCNN). Subsequently, we match the detected faces using FaceNet/support vector\nmachine (SVM) classifiers. The system notifies users with an MMS containing the\nname of incoming persons or as \"unknown\", scene image, facial description, and\ncontextual information. The users can grant access or call emergency services\nusing the SafeAccess app based on the received notification. Our system\nidentifies persons with an F-score 0.97 and recognizes items to generate image\ndescription with an average F-score 0.97.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 15:50:32 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Alam", "Shahinur", ""], ["Mahmud", "Md Sultan", ""], ["Yeasin", "Mohammed", ""]]}, {"id": "2006.05918", "submitter": "Abenezer Girma Mr", "authors": "Abenezer Girma, Seifemichael Amsalu, Abrham Workineh, Mubbashar Khan,\n  Abdollah Homaifar", "title": "Deep Learning with Attention Mechanism for Predicting Driver Intention\n  at Intersection", "comments": "IEEE Intelligent Vehicles Symposium 2020 (IEEE IV 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a driver's intention prediction near a road intersection is\nproposed. Our approach uses a deep bidirectional Long Short-Term Memory (LSTM)\nwith an attention mechanism model based on a hybrid-state system (HSS)\nframework. As intersection is considered to be as one of the major source of\nroad accidents, predicting a driver's intention at an intersection is very\ncrucial. Our method uses a sequence to sequence modeling with an attention\nmechanism to effectively exploit temporal information out of the time-series\nvehicular data including velocity and yaw-rate. The model then predicts ahead\nof time whether the target vehicle/driver will go straight, stop, or take right\nor left turn. The performance of the proposed approach is evaluated on a\nnaturalistic driving dataset and results show that our method achieves high\naccuracy as well as outperforms other methods. The proposed solution is\npromising to be applied in advanced driver assistance systems (ADAS) and as\npart of active safety system of autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 16:12:00 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Girma", "Abenezer", ""], ["Amsalu", "Seifemichael", ""], ["Workineh", "Abrham", ""], ["Khan", "Mubbashar", ""], ["Homaifar", "Abdollah", ""]]}, {"id": "2006.05977", "submitter": "Lara Gauder", "authors": "Lara Gauder, Pablo Riera, Leonardo Pepino, Silvina Brussino, Jazm\\'in\n  Vidal, Luciana Ferrer, Agust\\'in Gravano", "title": "Trust-UBA: A Corpus for the Study of the Manifestation of Trust in\n  Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel protocol for collecting speech data from\nsubjects induced to have different degrees of trust in the skills of a\nconversational agent. The protocol consists of an interactive session where the\nsubject is asked to respond to a series of factual questions with the help of a\nvirtual assistant. In order to induce subjects to either trust or distrust the\nagent's skills, they are first informed that it was previously rated by other\nusers as being either good or bad; subsequently, the agent answers the\nsubjects' questions consistently to its alleged abilities. All interactions are\nspeech-based, with subjects and agents communicating verbally, which allows the\nrecording of speech produced under different trust conditions. We collected a\nspeech corpus in Argentine Spanish using this protocol, which we are currently\nusing to study the feasibility of predicting the degree of trust from speech.\nWe find clear evidence that the protocol effectively succeeded in influencing\nsubjects into the desired mental state of either trusting or distrusting the\nagent's skills, and present preliminary results of a perceptual study of the\ndegree of trust performed by expert listeners. The collected speech dataset\nwill be made publicly available once ready.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 17:46:32 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 19:31:39 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Gauder", "Lara", ""], ["Riera", "Pablo", ""], ["Pepino", "Leonardo", ""], ["Brussino", "Silvina", ""], ["Vidal", "Jazm\u00edn", ""], ["Ferrer", "Luciana", ""], ["Gravano", "Agust\u00edn", ""]]}, {"id": "2006.06071", "submitter": "Ali Samadani", "authors": "Ali Samadani, Rob Gorbet, Dana Kulic", "title": "Affective Movement Generation using Laban Effort and Shape and Hidden\n  Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body movements are an important communication medium through which affective\nstates can be discerned. Movements that convey affect can also give machines\nlife-like attributes and help to create a more engaging human-machine\ninteraction. This paper presents an approach for automatic affective movement\ngeneration that makes use of two movement abstractions: 1) Laban movement\nanalysis (LMA), and 2) hidden Markov modeling. The LMA provides a systematic\ntool for an abstract representation of the kinematic and expressive\ncharacteristics of movements. Given a desired motion path on which a target\nemotion is to be overlaid, the proposed approach searches a labeled dataset in\nthe LMA Effort and Shape space for similar movements to the desired motion path\nthat convey the target emotion. An HMM abstraction of the identified movements\nis obtained and used with the desired motion path to generate a novel movement\nthat is a modulated version of the desired motion path that conveys the target\nemotion. The extent of modulation can be varied, trading-off between kinematic\nand affective constraints in the generated movement. The proposed approach is\ntested using a full-body movement dataset. The efficacy of the proposed\napproach in generating movements with recognizable target emotions is assessed\nusing a validated automatic recognition model and a user study. The target\nemotions were correctly recognized from the generated movements at a rate of\n72% using the recognition model. Furthermore, participants in the user study\nwere able to correctly perceive the target emotions from a sample of generated\nmovements, although some cases of confusion were also observed.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 21:24:26 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Samadani", "Ali", ""], ["Gorbet", "Rob", ""], ["Kulic", "Dana", ""]]}, {"id": "2006.06105", "submitter": "Jon Saad-Falcon", "authors": "Jon Saad-Falcon, Omar Shaikh, Zijie J. Wang, Austin P. Wright, Sasha\n  Richardson, Duen Horng Chau", "title": "PeopleMap: Visualization Tool for Mapping Out Researchers using Natural\n  Language Processing", "comments": "7 pages, 3 figures, submission to the 29th ACM International\n  Conference on Information and Knowledge Management (CIKM '20), October 19-23,\n  2020, Galway, Ireland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering research expertise at institutions can be a difficult task.\nManually curated university directories easily become out of date and they\noften lack the information necessary for understanding a researcher's interests\nand past work, making it harder to explore the diversity of research at an\ninstitution and identify research talents. This results in lost opportunities\nfor both internal and external entities to discover new connections and nurture\nresearch collaboration. To solve this problem, we have developed PeopleMap, the\nfirst interactive, open-source, web-based tool that visually \"maps out\"\nresearchers based on their research interests and publications by leveraging\nembeddings generated by natural language processing (NLP) techniques. PeopleMap\nprovides a new engaging way for institutions to summarize their research\ntalents and for people to discover new connections. The platform is developed\nwith ease-of-use and sustainability in mind. Using only researchers' Google\nScholar profiles as input, PeopleMap can be readily adopted by any institution\nusing its publicly-accessible repository and detailed documentation.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 23:06:25 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Saad-Falcon", "Jon", ""], ["Shaikh", "Omar", ""], ["Wang", "Zijie J.", ""], ["Wright", "Austin P.", ""], ["Richardson", "Sasha", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2006.06165", "submitter": "David Shamma", "authors": "David A. Shamma and Tony Dunnigan and Lyndon Kennedy", "title": "Automatic Photo to Ideophone Manga Matching", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo applications offer tools for annotation via text and stickers.\nIdeophones, mimetic and onomatopoeic words, which are common in graphic novels,\nhave yet to be explored for photo annotation use. We present a method for\nautomatic ideophone recommendation and positioning of the text on photos. These\nannotations are accomplished by obtaining a list of ideophones with English\ndefinitions and applying a suite of visual object detectors to the image. Next,\na semantic embedding maps the visual objects to the possible relevant\nideophones. Our system stands in contrast to traditional computer vision-based\nannotation systems, which stop at recommending object and scene-level\nannotation, by providing annotations that are communicative, fun, and engaging.\nWe test these annotations in Japanese and find they carry a strong preference\nand increase enjoyment and sharing likelihood when compared to unannotated and\nobject-based annotated photos.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 03:01:43 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Shamma", "David A.", ""], ["Dunnigan", "Tony", ""], ["Kennedy", "Lyndon", ""]]}, {"id": "2006.06201", "submitter": "Paul Peyramaure", "authors": "Alexy Carlier (IETR), Paul Peyramaure (IETR), Ketty Favre (UR1),\n  Muriel Pressigout (IETR)", "title": "Fall Detector Adapted to Nursing Home Needs through an Optical-Flow\n  based CNN", "comments": null, "journal-ref": "42nd Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society - EMBC2020, Jul 2020, Montreal, Canada", "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fall detection in specialized homes for the elderly is challenging.\nVision-based fall detection solutions have a significant advantage over\nsensor-based ones as they do not instrument the resident who can suffer from\nmental diseases. This work is part of a project intended to deploy fall\ndetection solutions in nursing homes. The proposed solution, based on Deep\nLearning, is built on a Convolutional Neural Network (CNN) trained to maximize\na sensitivity-based metric. This work presents the requirements from the\nmedical side and how it impacts the tuning of a CNN. Results highlight the\nimportance of the temporal aspect of a fall. Therefore, a custom metric adapted\nto this use case and an implementation of a decision-making process are\nproposed in order to best meet the medical teams requirements. Clinical\nrelevance This work presents a fall detection solution enabled to detect 86.2%\nof falls while producing only 11.6% of false alarms in average on the\nconsidered databases.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 05:23:12 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Carlier", "Alexy", "", "IETR"], ["Peyramaure", "Paul", "", "IETR"], ["Favre", "Ketty", "", "UR1"], ["Pressigout", "Muriel", "", "IETR"]]}, {"id": "2006.06295", "submitter": "Justin Edwards", "authors": "Justin Edwards and Allison Perrone and Philip R. Doyle", "title": "Transparency in Language Generation: Levels of Automation", "comments": "Accepted for publication at CUI 2020", "journal-ref": null, "doi": "10.1145/3405755.3406136", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language models and conversational systems are growing increasingly advanced,\ncreating outputs that may be mistaken for humans. Consumers may thus be misled\nby advertising, media reports, or vagueness regarding the role of automation in\nthe production of language. We propose a taxonomy of language automation, based\non the SAE levels of driving automation, to establish a shared set of terms for\ndescribing automated language. It is our hope that the proposed taxonomy can\nincrease transparency in this rapidly advancing field.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 10:01:59 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Edwards", "Justin", ""], ["Perrone", "Allison", ""], ["Doyle", "Philip R.", ""]]}, {"id": "2006.06321", "submitter": "Osama Mazhar", "authors": "Osama Mazhar, Sofiane Ramdani, and Andrea Cherubini", "title": "A Deep Learning Framework for Recognizing both Static and Dynamic\n  Gestures", "comments": "19 pages - Accepted in MDPI Sensors: Sensors and Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intuitive user interfaces are indispensable to interact with the human\ncentric smart environments. In this paper, we propose a unified framework that\nrecognizes both static and dynamic gestures, using simple RGB vision (without\ndepth sensing). This feature makes it suitable for inexpensive human-robot\ninteraction in social or industrial settings. We employ a pose-driven spatial\nattention strategy, which guides our proposed Static and Dynamic gestures\nNetwork - StaDNet. From the image of the human upper body, we estimate his/her\ndepth, along with the region-of-interest around his/her hands. The\nConvolutional Neural Network in StaDNet is fine-tuned on a\nbackground-substituted hand gestures dataset. It is utilized to detect 10\nstatic gestures for each hand as well as to obtain the hand image-embeddings.\nThese are subsequently fused with the augmented pose vector and then passed to\nthe stacked Long Short-Term Memory blocks. Thus, human-centred frame-wise\ninformation from the augmented pose vector and from the left/right hands\nimage-embeddings are aggregated in time to predict the dynamic gestures of the\nperforming person. In a number of experiments, we show that the proposed\napproach surpasses the state-of-the-art results on the large-scale Chalearn\n2016 dataset. Moreover, we transfer the knowledge learned through the proposed\nmethodology to the Praxis gestures dataset, and the obtained results also\noutscore the state-of-the-art on this dataset.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 10:39:02 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 10:31:16 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Mazhar", "Osama", ""], ["Ramdani", "Sofiane", ""], ["Cherubini", "Andrea", ""]]}, {"id": "2006.06323", "submitter": "Nikhil Sheoran", "authors": "Atanu R Sinha, Deepali Jain, Nikhil Sheoran, Sopan Khosla, Reshmi\n  Sasidharan", "title": "Surveys without Questions: A Reinforcement Learning Approach", "comments": "The Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19)", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence,\n  vol. 33, July 2019, pp. 257-64", "doi": "10.1609/aaai.v33i01.3301257", "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 'old world' instrument, survey, remains a tool of choice for firms to\nobtain ratings of satisfaction and experience that customers realize while\ninteracting online with firms. While avenues for survey have evolved from\nemails and links to pop-ups while browsing, the deficiencies persist. These\ninclude - reliance on ratings of very few respondents to infer about all\ncustomers' online interactions; failing to capture a customer's interactions\nover time since the rating is a one-time snapshot; and inability to tie back\ncustomers' ratings to specific interactions because ratings provided relate to\nall interactions. To overcome these deficiencies we extract proxy ratings from\nclickstream data, typically collected for every customer's online interactions,\nby developing an approach based on Reinforcement Learning (RL). We introduce a\nnew way to interpret values generated by the value function of RL, as proxy\nratings. Our approach does not need any survey data for training. Yet, on\nvalidation against actual survey data, proxy ratings yield reasonable\nperformance results. Additionally, we offer a new way to draw insights from\nvalues of the value function, which allow associating specific interactions to\ntheir proxy ratings. We introduce two new metrics to represent ratings - one,\ncustomer-level and the other, aggregate-level for click actions across\ncustomers. Both are defined around proportion of all pairwise, successive\nactions that show increase in proxy ratings. This intuitive customer-level\nmetric enables gauging the dynamics of ratings over time and is a better\npredictor of purchase than customer ratings from survey. The aggregate-level\nmetric allows pinpointing actions that help or hurt experience. In sum, proxy\nratings computed unobtrusively from clickstream, for every action, for each\ncustomer, and for every session can offer interpretable and more insightful\nalternative to surveys.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 10:41:07 GMT"}], "update_date": "2020-06-14", "authors_parsed": [["Sinha", "Atanu R", ""], ["Jain", "Deepali", ""], ["Sheoran", "Nikhil", ""], ["Khosla", "Sopan", ""], ["Sasidharan", "Reshmi", ""]]}, {"id": "2006.06328", "submitter": "Justin Edwards", "authors": "Yunhan Wu, Daniel Rough, Anna Bleakley, Justin Edwards, Orla Cooney,\n  Philip R. Doyle, Leigh Clark, and Benjamin R. Cowan", "title": "See what I'm saying? Comparing Intelligent Personal Assistant use for\n  Native and Non-Native Language Speakers", "comments": "Accepted to Mobile HCI 2020", "journal-ref": null, "doi": "10.1145/3379503.3403563", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited linguistic coverage for Intelligent Personal Assistants (IPAs) means\nthat many interact in a non-native language. Yet we know little about how IPAs\ncurrently support or hinder these users. Through native (L1) and non-native\n(L2) English speakers interacting with Google Assistant on a smartphone and\nsmart speaker, we aim to understand this more deeply. Interviews revealed that\nL2 speakers prioritised utterance planning around perceived linguistic\nlimitations, as opposed to L1 speakers prioritising succinctness because of\nsystem limitations. L2 speakers see IPAs as insensitive to linguistic needs\nresulting in failed interaction. L2 speakers clearly preferred using\nsmartphones, as visual feedback supported diagnoses of communication breakdowns\nwhilst allowing time to process query results. Conversely, L1 speakers\npreferred smart speakers, with audio feedback being seen as sufficient. We\ndiscuss the need to tailor the IPA experience for L2 users, emphasising visual\nfeedback whilst reducing the burden of language production.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 11:03:49 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Wu", "Yunhan", ""], ["Rough", "Daniel", ""], ["Bleakley", "Anna", ""], ["Edwards", "Justin", ""], ["Cooney", "Orla", ""], ["Doyle", "Philip R.", ""], ["Clark", "Leigh", ""], ["Cowan", "Benjamin R.", ""]]}, {"id": "2006.06331", "submitter": "Justin Edwards", "authors": "Yunhan Wu, Justin Edwards, Orla Cooney, Anna Bleakley, Philip R.Doyle,\n  Leigh Clark, Daniel Rough, and Benjamin R. Cowan", "title": "Mental Workload and Language Production in Non-Native Speaker IPA\n  Interaction", "comments": "Accepted at CUI 2020", "journal-ref": null, "doi": "10.1145/3405755.3406118", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through proliferation on smartphones and smart speakers, intelligent personal\nassistants (IPAs) have made speech a common interaction modality. Yet, due to\nlinguistic coverage and varying levels of functionality, many speakers engage\nwith IPAs using a non-native language. This may impact the mental workload and\npattern of language production displayed by non-native speakers. We present a\nmixed-design experiment, wherein native (L1) and non-native (L2) English\nspeakers completed tasks with IPAs through smartphones and smart speakers. We\nfound significantly higher mental workload for L2 speakers during IPA\ninteractions. Contrary to our hypotheses, we found no significant differences\nbetween L1 and L2 speakers in terms of number of turns, lexical complexity,\ndiversity, or lexical adaptation when encountering errors. These findings are\ndiscussed in relation to language production and processing load increases for\nL2 speakers in IPA interaction.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 11:06:42 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Wu", "Yunhan", ""], ["Edwards", "Justin", ""], ["Cooney", "Orla", ""], ["Bleakley", "Anna", ""], ["Doyle", "Philip R.", ""], ["Clark", "Leigh", ""], ["Rough", "Daniel", ""], ["Cowan", "Benjamin R.", ""]]}, {"id": "2006.07113", "submitter": "Dookun Park", "authors": "Dookun Park, Hao Yuan, Dongmin Kim, Yinglei Zhang, Matsoukas Spyros,\n  Young-Bum Kim, Ruhi Sarikaya, Edward Guo, Yuan Ling, Kevin Quinn, Pham Hung,\n  Benjamin Yao, Sungjin Lee", "title": "Large-scale Hybrid Approach for Predicting User Satisfaction with\n  Conversational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring user satisfaction level is a challenging task, and a critical\ncomponent in developing large-scale conversational agent systems serving the\nneeds of real users. An widely used approach to tackle this is to collect human\nannotation data and use them for evaluation or modeling. Human annotation based\napproaches are easier to control, but hard to scale. A novel alternative\napproach is to collect user's direct feedback via a feedback elicitation system\nembedded to the conversational agent system, and use the collected user\nfeedback to train a machine-learned model for generalization. User feedback is\nthe best proxy for user satisfaction, but is not available for some ineligible\nintents and certain situations. Thus, these two types of approaches are\ncomplementary to each other. In this work, we tackle the user satisfaction\nassessment problem with a hybrid approach that fuses explicit user feedback,\nuser satisfaction predictions inferred by two machine-learned models, one\ntrained on user feedback data and the other human annotation data. The hybrid\napproach is based on a waterfall policy, and the experimental results with\nAmazon Alexa's large-scale datasets show significant improvements in inferring\nuser satisfaction. A detailed hybrid architecture, an in-depth analysis on user\nfeedback data, and an algorithm that generates data sets to properly simulate\nthe live traffic are presented in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 16:29:09 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Park", "Dookun", ""], ["Yuan", "Hao", ""], ["Kim", "Dongmin", ""], ["Zhang", "Yinglei", ""], ["Spyros", "Matsoukas", ""], ["Kim", "Young-Bum", ""], ["Sarikaya", "Ruhi", ""], ["Guo", "Edward", ""], ["Ling", "Yuan", ""], ["Quinn", "Kevin", ""], ["Hung", "Pham", ""], ["Yao", "Benjamin", ""], ["Lee", "Sungjin", ""]]}, {"id": "2006.07140", "submitter": "Arosha Bandara", "authors": "Camilla Elphick, Richard Philpot, Min Zhang, Avelie Stuart, Zoe\n  Walkington, Lara Frumkin, Graham Pike, Kelly Gardner, Mark Lacey, Mark\n  Levine, Blaine Price, Arosha Bandara and Bashar Nuseibeh", "title": "Building trust in digital policing: A scoping review of community\n  policing apps", "comments": "Police Practice and Research (Taylor & Francis) 2020", "journal-ref": null, "doi": "10.1080/15614263.2020.1861449", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perceptions of police trustworthiness are linked to citizens' willingness to\ncooperate with police. Trust can be fostered by introducing accountability\nmechanisms, or by increasing a shared police/citizen identity, both which can\nbe achieved digitally. Digital mechanisms can also be designed to safeguard,\nengage, reassure, inform, and empower diverse communities. We systematically\nscoped 240 existing online citizen-police and relevant third-party\ncommunication apps, to examine whether they sought to meet community needs and\npolicing visions. We found that 82% required registration or login details, 55%\nof those with a reporting mechanism allowed for anonymous reporting, and 10%\nprovided an understandable privacy policy. Police apps were more likely to seek\nto reassure, safeguard and inform users, while third-party apps were more\nlikely to seek to empower users. As poorly designed apps risk amplifying\nmistrust and undermining policing efforts, we suggest 12 design considerations\nto help ensure the development of high quality/fit for purpose Police/Citizen\napps.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 12:52:42 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 19:38:19 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Elphick", "Camilla", ""], ["Philpot", "Richard", ""], ["Zhang", "Min", ""], ["Stuart", "Avelie", ""], ["Walkington", "Zoe", ""], ["Frumkin", "Lara", ""], ["Pike", "Graham", ""], ["Gardner", "Kelly", ""], ["Lacey", "Mark", ""], ["Levine", "Mark", ""], ["Price", "Blaine", ""], ["Bandara", "Arosha", ""], ["Nuseibeh", "Bashar", ""]]}, {"id": "2006.07211", "submitter": "Kimon Kieslich", "authors": "Kimon Kieslich, Marco L\\\"unich, Frank Marcinkowski", "title": "The Threats of Artificial Intelligence Scale (TAI). Development,\n  Measurement and Test Over Three Application Domains", "comments": null, "journal-ref": null, "doi": "10.1007/s12369-020-00734-w", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years Artificial Intelligence (AI) has gained much popularity, with\nthe scientific community as well as with the public. AI is often ascribed many\npositive impacts for different social domains such as medicine and the economy.\nOn the other side, there is also growing concern about its precarious impact on\nsociety and individuals. Several opinion polls frequently query the public fear\nof autonomous robots and artificial intelligence (FARAI), a phenomenon coming\nalso into scholarly focus. As potential threat perceptions arguably vary with\nregard to the reach and consequences of AI functionalities and the domain of\napplication, research still lacks necessary precision of a respective\nmeasurement that allows for wide-spread research applicability. We propose a\nfine-grained scale to measure threat perceptions of AI that accounts for four\nfunctional classes of AI systems and is applicable to various domains of AI\napplications. Using a standardized questionnaire in a survey study (N=891), we\nevaluate the scale over three distinct AI domains (loan origination, job\nrecruitment and medical treatment). The data support the dimensional structure\nof the proposed Threats of AI (TAI) scale as well as the internal consistency\nand factoral validity of the indicators. Implications of the results and the\nempirical application of the scale are discussed in detail. Recommendations for\nfurther empirical use of the TAI scale are provided.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:15:02 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kieslich", "Kimon", ""], ["L\u00fcnich", "Marco", ""], ["Marcinkowski", "Frank", ""]]}, {"id": "2006.07294", "submitter": "Rebecca Friesen", "authors": "Rebecca Fenton Friesen, Roberta L. Klatzky, Michael A. Peshkin, J.\n  Edward Colgate", "title": "Building a navigable fine texture design space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Friction modulation technology enables the creation of textural effects on\nflat haptic displays. However, an intuitive and manageably small design space\nfor construction of such haptic textures remains an unfulfilled goal for user\ninterface designers. In this paper, we explore perceptually relevant features\nof fine texture for use in texture construction and modification. Beginning\nwith simple sinusoidal patterns of friction force that vary in frequency and\namplitude, we define irregularity as a third building block of a texture\npattern and show it to be a scalable feature distinct from the others using\nmultidimensional scaling. Additionally, subjects' verbal descriptions of this\n3-dimensional design space provide insight into their intuitive interpretation\nof the physical parameter changes.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:22:09 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Friesen", "Rebecca Fenton", ""], ["Klatzky", "Roberta L.", ""], ["Peshkin", "Michael A.", ""], ["Colgate", "J. Edward", ""]]}, {"id": "2006.07301", "submitter": "Neda Navidi", "authors": "Neda Navidi, Francoi Chabo, Saga Kurandwa, Iv Lutigma, Vincent Robt,\n  Gregry Szrftgr, Andea Schuh", "title": "Human and Multi-Agent collaboration in a human-MARL teaming framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning provides effective results with agents learning from\ntheir observations, received rewards, and internal interactions between agents.\nThis study proposes a new open-source MARL framework, called COGMENT, to\nefficiently leverage human and agent interactions as a source of learning. We\ndemonstrate these innovations by using a designed real-time environment with\nunmanned aerial vehicles driven by RL agents, collaborating with a human. The\nresults of this study show that the proposed collaborative paradigm and the\nopen-source framework leads to significant reductions in both human effort and\nexploration costs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:32:42 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 21:24:19 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Navidi", "Neda", ""], ["Chabo", "Francoi", ""], ["Kurandwa", "Saga", ""], ["Lutigma", "Iv", ""], ["Robt", "Vincent", ""], ["Szrftgr", "Gregry", ""], ["Schuh", "Andea", ""]]}, {"id": "2006.07508", "submitter": "Joe Booth", "authors": "Joe Booth, Vladimir Ivanov", "title": "Realistic Physics Based Character Controller", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the course of the last several years there was a strong interest in\napplication of modern optimal control techniques to the field of character\nanimation. This interest was fueled by introduction of efficient learning based\nalgorithms for policy optimization, growth in computation power, and game\nengine improvements. It was shown that it is possible to generate natural\nlooking control of a character by using two ingredients. First, the simulated\nagent must adhere to a motion capture dataset. And second, the character aims\nto track the control input from the user. The paper aims at closing the gap\nbetween the researchers and users by introducing an open source implementation\nof physics based character control in Unity framework that has a low entry\nbarrier and a steep learning curve.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 23:13:16 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Booth", "Joe", ""], ["Ivanov", "Vladimir", ""]]}, {"id": "2006.07519", "submitter": "Kyle Dent", "authors": "Kyle Dent and Kalai Ramea", "title": "Conversational User Interfaces for Blind Knowledge Workers: A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern trends in interface design for office equipment using controls on\ntouch surfaces create greater obstacles for blind and visually impaired users\nand contribute to an environment of dependency in work settings. We believe\nthat \\textit{conversational user interfaces} (CUIs) offer a reasonable\nalternative to touchscreen interactions enabling more access and most\nimportantly greater independence for blind knowledge workers. We present a case\nstudy of our work to develop a conversational user interface for accessibility\nfor multifunction printers. We also describe our approach to conversational\ninterfaces in general, which emphasizes task-based collaborative interactions\nbetween people and intelligent agents, and we detail the specifics of the\nsolution we created for multifunction printers. To guide our design, we worked\nwith a group of blind and visually impaired individuals starting with focus\ngroup sessions to ascertain the challenges our target users face in their\nprofessional lives. We followed our technology development with a user study to\nassess the solution and direct our future efforts. We present our findings and\nconclusions from the study.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 00:27:14 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 21:06:00 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Dent", "Kyle", ""], ["Ramea", "Kalai", ""]]}, {"id": "2006.07604", "submitter": "Cheng Zhang", "authors": "Cheng Zhang", "title": "Dynamic gesture retrieval: searching videos by human pose sequence", "comments": "The problem proposed in this article should be classified as \"gesture\n  retrieval\" or \"gesture detection\", and there are already better algorithms to\n  deal with the proposed problem, for example Dynamic Time Warping (DTW) based\n  methods. The solution in this work gives little contribution to the field, so\n  I decided to withdraw it", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of static human poses is limited, it is hard to retrieve the exact\nvideos using one single pose as the clue. However, with a pose sequence or a\ndynamic gesture as the keyword, retrieving specific videos becomes more\nfeasible. We propose a novel method for querying videos containing a designated\nsequence of human poses, whereas previous works only designate a single static\npose. The proposed method takes continuous 3d human poses from keyword gesture\nvideo and video candidates, then converts each pose in individual frames into\nbone direction descriptors, which describe the direction of each natural\nconnection in articulated pose. A temporal pyramid sliding window is then\napplied to find matches between designated gesture and video candidates, which\nensures that same gestures with different duration can be matched.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 10:11:22 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 03:53:15 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhang", "Cheng", ""]]}, {"id": "2006.07777", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen and Hal Daum\\'e III", "title": "Active Imitation Learning from Multiple Non-Deterministic Teachers:\n  Formulation, Challenges, and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate the problem of learning to imitate multiple, non-deterministic\nteachers with minimal interaction cost. Rather than learning a specific policy\nas in standard imitation learning, the goal in this problem is to learn a\ndistribution over a policy space. We first present a general framework that\nefficiently models and estimates such a distribution by learning continuous\nrepresentations of the teacher policies. Next, we develop Active\nPerformance-Based Imitation Learning (APIL), an active learning algorithm for\nreducing the learner-teacher interaction cost in this framework. By making\nquery decisions based on predictions of future progress, our algorithm avoids\nthe pitfalls of traditional uncertainty-based approaches in the face of teacher\nbehavioral uncertainty. Results on both toy and photo-realistic navigation\ntasks show that APIL significantly reduces the numbers of interactions with\nteachers without compromising on performance. Moreover, it is robust to various\ndegrees of teacher behavioral uncertainty.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 03:06:27 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Nguyen", "Khanh", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "2006.08108", "submitter": "Derek Lim", "authors": "Derek Lim, Austin R. Benson", "title": "Expertise and Dynamics within Crowdsourced Musical Knowledge Curation: A\n  Case Study of the Genius Platform", "comments": "12 pages. 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many platforms collect crowdsourced information primarily from volunteers. As\nthis type of knowledge curation has become widespread, contribution formats\nvary substantially and are driven by diverse processes across differing\nplatforms. Thus, models for one platform are not necessarily applicable to\nothers. Here, we study the temporal dynamics of Genius, a platform primarily\ndesigned for user-contributed annotations of song lyrics. A unique aspect of\nGenius is that the annotations are extremely local -- an annotated lyric may\njust be a few lines of a song -- but also highly related, e.g., by song, album,\nartist, or genre. We analyze several dynamical processes associated with lyric\nannotations and their edits, which differ substantially from models for other\nplatforms. For example, expertise on song annotations follows a \"U shape\" where\nexperts are both early and late contributors with non-experts contributing\nintermediately; we develop a user utility model that captures such behavior. We\nalso find several contribution traits appearing early in a user's lifespan of\ncontributions that distinguish (eventual) experts from non-experts. Combining\nour findings, we develop a model for early prediction of user expertise.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 03:19:47 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 02:45:43 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Lim", "Derek", ""], ["Benson", "Austin R.", ""]]}, {"id": "2006.08224", "submitter": "Medha Atre", "authors": "Medha Atre, Anand Deshpande, Reshma Godse, Pooja Deokar, Sandip\n  Moharir, Dhruva Ray, Akshay Chitlangia, Trupti Phadnis, Yugansh Goyal", "title": "Needles in the 'Sheet'stack: Augmented Analytics to get Insights from\n  Spreadsheets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business intelligence (BI) tools for database analytics have come a long way\nand nowadays also provide ready insights or visual query explorations, e.g.\nQuickInsights by Microsoft Power BI, SpotIQ by ThoughtSpot, Zenvisage, etc. In\nthis demo, we focus on providing insights by examining periodic spreadsheets of\ndifferent reports (aka views), without prior knowledge of the schema of the\ndatabase or reports, or data information. Such a solution is targeted at users\nwithout the familiarity with the database schema or resources to conduct\nanalytics in the contemporary way.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 08:54:22 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Atre", "Medha", ""], ["Deshpande", "Anand", ""], ["Godse", "Reshma", ""], ["Deokar", "Pooja", ""], ["Moharir", "Sandip", ""], ["Ray", "Dhruva", ""], ["Chitlangia", "Akshay", ""], ["Phadnis", "Trupti", ""], ["Goyal", "Yugansh", ""]]}, {"id": "2006.08481", "submitter": "Ahmet-Serdar Karakaya", "authors": "Ahmet-Serdar Karakaya, Jonathan Hasenburg and David Bermbach", "title": "SimRa: Using Crowdsourcing to Identify Near Miss Hotspots in Bicycle\n  Traffic", "comments": "Accepted for publication in Elsevier Pervasive and Mobile Computing", "journal-ref": null, "doi": "10.1016/j.pmcj.2020.101197", "report-no": null, "categories": "cs.CY cs.DC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An increased modal share of bicycle traffic is a key mechanism to reduce\nemissions and solve traffic-related problems. However, a lack of (perceived)\nsafety keeps people from using their bikes more frequently. To improve safety\nin bicycle traffic, city planners need an overview of accidents, near miss\nincidents, and bike routes. Such information, however, is currently not\navailable. In this paper, we describe SimRa, a platform for collecting data on\nbicycle routes and near miss incidents using smartphone-based crowdsourcing. We\nalso describe how we identify dangerous near miss hotspots based on the\ncollected data and propose a scoring model.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 15:29:52 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 07:46:40 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Karakaya", "Ahmet-Serdar", ""], ["Hasenburg", "Jonathan", ""], ["Bermbach", "David", ""]]}, {"id": "2006.08701", "submitter": "Jake Rhodes", "authors": "Jake S. Rhodes, Adele Cutler, Guy Wolf, Kevin R. Moon", "title": "Supervised Visualization for Data Exploration", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is often used as an initial step in data\nexploration, either as preprocessing for classification or regression or for\nvisualization. Most dimensionality reduction techniques to date are\nunsupervised; they do not take class labels into account (e.g., PCA, MDS,\nt-SNE, Isomap). Such methods require large amounts of data and are often\nsensitive to noise that may obfuscate important patterns in the data. Various\nattempts at supervised dimensionality reduction methods that take into account\nauxiliary annotations (e.g., class labels) have been successfully implemented\nwith goals of increased classification accuracy or improved data visualization.\nMany of these supervised techniques incorporate labels in the loss function in\nthe form of similarity or dissimilarity matrices, thereby creating\nover-emphasized separation between class clusters, which does not realistically\nrepresent the local and global relationships in the data. In addition, these\napproaches are often sensitive to parameter tuning, which may be difficult to\nconfigure without an explicit quantitative notion of visual superiority. In\nthis paper, we describe a novel supervised visualization technique based on\nrandom forest proximities and diffusion-based dimensionality reduction. We\nshow, both qualitatively and quantitatively, the advantages of our approach in\nretaining local and global structures in data, while emphasizing important\nvariables in the low-dimensional embedding. Importantly, our approach is robust\nto noise and parameter tuning, thus making it simple to use while producing\nreliable visualizations for data exploration.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 19:10:17 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Rhodes", "Jake S.", ""], ["Cutler", "Adele", ""], ["Wolf", "Guy", ""], ["Moon", "Kevin R.", ""]]}, {"id": "2006.08792", "submitter": "Emiel Van Miltenburg", "authors": "Emiel van Miltenburg", "title": "On the use of human reference data for evaluating automatic image\n  descriptions", "comments": "Originally presented as a (non-archival) poster at the VizWiz 2020\n  workshop, collocated with CVPR 2020. See:\n  https://vizwiz.org/workshops/2020-workshop/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatic image description systems are commonly trained and evaluated using\ncrowdsourced, human-generated image descriptions. The best-performing system is\nthen determined using some measure of similarity to the reference data (BLEU,\nMeteor, CIDER, etc). Thus, both the quality of the systems as well as the\nquality of the evaluation depends on the quality of the descriptions. As\nSection 2 will show, the quality of current image description datasets is\ninsufficient. I argue that there is a need for more detailed guidelines that\ntake into account the needs of visually impaired users, but also the\nfeasibility of generating suitable descriptions. With high-quality data,\nevaluation of image description systems could use reference descriptions, but\nwe should also look for alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 21:57:27 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["van Miltenburg", "Emiel", ""]]}, {"id": "2006.08805", "submitter": "Oznur Alkan", "authors": "Oznur Alkan and Elizabeth Daly", "title": "User Profiling from Reviews for Accurate Time-Based Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are a valuable way to engage users in a system, increase\nparticipation and show them resources they may not have found otherwise. One\nsignificant challenge is that user interests may change over time and certain\nitems have an inherently temporal aspect. As a result, a recommender system\nshould try and take into account the time-dependant user-item relationships.\nHowever, temporal aspects of a user profile may not always be explicitly\navailable and so we may need to infer this information from available\nresources. Product reviews on sites, such as Amazon, represent a valuable data\nsource to understand why someone bought an item and potentially who the item is\nfor. This information can then be used to construct a dynamic user profile. In\nthis paper, we demonstrate utilising reviews to extract temporal information to\ninfer the \\textit{age category preference} of users, and leverage this feature\nto generate time-dependent recommendations. Given the predictable and yet\nshifting nature of age and time, we show that, recommendations generated using\nthis dynamic aspect lead to higher accuracy compared with techniques from state\nof art. Mining temporally related content in reviews can enable the recommender\nto go beyond finding similar items or users to potentially predict a future\nneed of a user.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 22:23:17 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Alkan", "Oznur", ""], ["Daly", "Elizabeth", ""]]}, {"id": "2006.08818", "submitter": "Ingrid Nunes", "authors": "Ingrid Nunes, Phillip Taylor, Lina Barakat, Nathan Griffiths, Simon\n  Miles", "title": "Explaining reputation assessments", "comments": null, "journal-ref": "International Journal of Human-Computer Studies, 123, 1-17 (2019)", "doi": "10.1016/j.ijhcs.2018.10.007", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reputation is crucial to enabling human or software agents to select among\nalternative providers. Although several effective reputation assessment methods\nexist, they typically distil reputation into a numerical representation, with\nno accompanying explanation of the rationale behind the assessment. Such\nexplanations would allow users or clients to make a richer assessment of\nproviders, and tailor selection according to their preferences and current\ncontext. In this paper, we propose an approach to explain the rationale behind\nassessments from quantitative reputation models, by generating arguments that\nare combined to form explanations. Our approach adapts, extends and combines\nexisting approaches for explaining decisions made using multi-attribute\ndecision models in the context of reputation. We present example argument\ntemplates, and describe how to select their parameters using explanation\nalgorithms. Our proposal was evaluated by means of a user study, which followed\nan existing protocol. Our results give evidence that although explanations\npresent a subset of the information of trust scores, they are sufficient to\nequally evaluate providers recommended based on their trust score. Moreover,\nwhen explanation arguments reveal implicit model information, they are less\npersuasive than scores.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 23:19:35 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Nunes", "Ingrid", ""], ["Taylor", "Phillip", ""], ["Barakat", "Lina", ""], ["Griffiths", "Nathan", ""], ["Miles", "Simon", ""]]}, {"id": "2006.08899", "submitter": "Brian Keegan", "authors": "Brian C. Keegan, Chenhao Tan", "title": "A Quantitative Portrait of Wikipedia's High-Tempo Collaborations during\n  the 2020 Coronavirus Pandemic", "comments": "25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The 2020 coronavirus pandemic was a historic social disruption with\nsignificant consequences felt around the globe. Wikipedia is a\nfreely-available, peer-produced encyclopedia with a remarkable ability to\ncreate and revise content following current events. Using 973,940 revisions\nfrom 134,337 editors to 4,238 articles, this study examines the dynamics of the\nEnglish Wikipedia's response to the coronavirus pandemic through the first five\nmonths of 2020 as a \"quantitative portrait\" describing the emergent\ncollaborative behavior at three levels of analysis: article revision, editor\ncontributions, and network dynamics. Across multiple data sources, quantitative\nmethods, and levels of analysis, we find four consistent themes characterizing\nWikipedia's unique large-scale, high-tempo, and temporary online\ncollaborations: external events as drivers of activity, spillovers of activity,\ncomplex patterns of editor engagement, and the shadows of the future. In light\nof increasing concerns about online social platforms' abilities to govern the\nconduct and content of their users, we identify implications from Wikipedia's\ncoronavirus collaborations for improving the resilience of socio-technical\nsystems during a crisis.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 03:28:04 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Keegan", "Brian C.", ""], ["Tan", "Chenhao", ""]]}, {"id": "2006.09019", "submitter": "Justinas Miseikis", "authors": "Justinas Miseikis, Pietro Caroni, Patricia Duchamp, Alina Gasser,\n  Rastislav Marko, Nelija Miseikiene, Frederik Zwilling, Charles de\n  Castelbajac, Lucas Eicher, Michael Fruh, Hansruedi Fruh", "title": "Lio -- A Personal Robot Assistant for Human-Robot Interaction and Care\n  Applications", "comments": "Accepted submission at IEEE Robotics and Automation Letters (RA-L),\n  submitted to IEEE IROS 2020", "journal-ref": null, "doi": "10.1109/LRA.2020.3007462", "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lio is a mobile robot platform with a multi-functional arm explicitly\ndesigned for human-robot interaction and personal care assistant tasks. The\nrobot has already been deployed in several health care facilities, where it is\nfunctioning autonomously, assisting staff and patients on an everyday basis.\nLio is intrinsically safe by having full coverage in soft artificial-leather\nmaterial as well as having collision detection, limited speed and forces.\nFurthermore, the robot has a compliant motion controller. A combination of\nvisual, audio, laser, ultrasound and mechanical sensors are used for safe\nnavigation and environment understanding. The ROS-enabled setup allows\nresearchers to access raw sensor data as well as have direct control of the\nrobot. The friendly appearance of Lio has resulted in the robot being well\naccepted by health care staff and patients. Fully autonomous operation is made\npossible by a flexible decision engine, autonomous navigation and automatic\nrecharging. Combined with time-scheduled task triggers, this allows Lio to\noperate throughout the day, with a battery life of up to 8 hours and recharging\nduring idle times. A combination of powerful on-board computing units provides\nenough processing power to deploy artificial intelligence and deep\nlearning-based solutions on-board the robot without the need to send any\nsensitive data to cloud services, guaranteeing compliance with privacy\nrequirements. During the COVID-19 pandemic, Lio was rapidly adjusted to perform\nadditional functionality like disinfection and remote elevated body temperature\ndetection. It complies with ISO13482 - Safety requirements for personal care\nrobots, meaning it can be directly tested and deployed in care facilities.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:37:44 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Miseikis", "Justinas", ""], ["Caroni", "Pietro", ""], ["Duchamp", "Patricia", ""], ["Gasser", "Alina", ""], ["Marko", "Rastislav", ""], ["Miseikiene", "Nelija", ""], ["Zwilling", "Frederik", ""], ["de Castelbajac", "Charles", ""], ["Eicher", "Lucas", ""], ["Fruh", "Michael", ""], ["Fruh", "Hansruedi", ""]]}, {"id": "2006.09090", "submitter": "Walter Morales-Alvarez", "authors": "Walter Morales Alvarez, Miguel \\'Angel de Miguel, Fernando Garc\\'ia,\n  Cristina Olaverri-Monreal", "title": "Response of Vulnerable Road Users to Visual Information from Autonomous\n  Vehicles in Shared Spaces", "comments": "Published paper in the IEEE Intelligent Transportation Systems\n  Conference - ITSC 2019", "journal-ref": "2019 IEEE Intelligent Transportation Systems Conference (ITSC),\n  Auckland, New Zealand, 2019, pp. 3714-3719", "doi": "10.1109/ITSC.2019.8917501", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completely unmanned autonomous vehicles have been anticipated for a while.\nInitially, these are expected to drive only under certain conditions on some\nroads, and advanced functionality is required to cope with the ever-increasing\nchallenges of safety. To enhance the public's perception of road safety and\ntrust in new vehicular technologies, we investigate in this paper the effect of\nseveral interaction paradigms with vulnerable road users by developing and\napplying algorithms for the automatic analysis of pedestrian body language. We\nassess behavioral patterns and determine the impact of the coexistence of AVs\nand other road users on general road safety in a shared space for VRUs and\nvehicles. Results showed that the implementation of visual communication cues\nfor interacting with VRUs is not necessarily required for a shared space in\nwhich informal traffic rules apply.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 11:54:16 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 10:07:33 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 09:48:54 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Alvarez", "Walter Morales", ""], ["de Miguel", "Miguel \u00c1ngel", ""], ["Garc\u00eda", "Fernando", ""], ["Olaverri-Monreal", "Cristina", ""]]}, {"id": "2006.09342", "submitter": "Yoones Sekhavat", "authors": "Yoones A. Sekhavat, Samad Roohi, Hesam Sakian Mohammadi, and Georgios\n  N. Yannakakis", "title": "Play with One's Feelings: A Study on Emotion Awareness for Player\n  Experience", "comments": "Accepted and to appear in IEEE Transactions on Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective interaction between players of video games can elicit rich and\nvarying patterns of emotions. In multiplayer activities that take place in a\ncommon space (such as sports and board games), players are generally aware of\nthe emotions of their teammates or opponents as they can directly observe their\nbehavioral patterns, facial expressions, head pose, body stance and so on.\nPlayers of online video games, however, are not generally aware of the other\nplayers' emotions given the limited channels of direct interaction among them\n(e.g. via emojis or chat boxes). It also turns out that the impact of real-time\nemotionawareness on play is still unexplored in the space of online digital\ngames. Motivated by this lack of empirical knowledge on the role of the affect\nof others to one's gameplay performance in this paper we investigate the\ndegrees to which the expression of manifested emotions of an opponent can\naffect the emotions of the player and consequently his gameplay behavior. In\nthis initial study, we test our hypothesis on a two-player adversarial car\nracing game. We perform a comprehensive user study to evaluate the emotions,\nbehaviors, and attitudes of players in emotion aware versus emotion agnostic\ngame versions. Our findings suggest that expressing the emotional state of the\nopponent through an emoji in real-time affects the emotional state and behavior\nof players that can consequently affect their playing experience.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:23:43 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 13:55:01 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Sekhavat", "Yoones A.", ""], ["Roohi", "Samad", ""], ["Mohammadi", "Hesam Sakian", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "2006.09487", "submitter": "Zhibin Niu", "authors": "Junqi Wu, Zhibin Niu, Jing Wu, Xiufeng Liu, Jiawan Zhang", "title": "$E^3$: Visual Exploration of Spatiotemporal Energy Demand", "comments": "5 Pages, with 1 page reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding demand-side energy behaviour is critical for making efficiency\nresponses for energy demand management. We worked closely with energy experts\nand identified the key elements of the energy demand problem including temporal\nand spatial demand and shifts in spatiotemporal demand. To our knowledge, no\nprevious research has investigated the shifts in spatiotemporal demand. To fill\nthis research gap, we propose a unified visual analytics approach to support\nexploratory demand analysis; we developed E3, a highly interactive tool that\nsupport users in making and verifying hypotheses through human-client-server\ninteractions. A novel potential flow based approach was formalized to model\nshifts in energy demand and integrated into a server-side engine. Experts then\nevaluated and affirmed the usefulness of this approach through case studies of\nreal-world electricity data. In the future, we will improve the modelling\nalgorithm, enhance visualisation, and expand the process to support more forms\nof energy data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 19:59:28 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Wu", "Junqi", ""], ["Niu", "Zhibin", ""], ["Wu", "Jing", ""], ["Liu", "Xiufeng", ""], ["Zhang", "Jiawan", ""]]}, {"id": "2006.09501", "submitter": "Rajesh Kumar", "authors": "Vishaal Udandarao and Mohit Agrawal and Rajesh Kumar and Rajiv Ratn\n  Shah", "title": "On the Inference of Soft Biometrics from Typing Patterns Collected in a\n  Multi-device Environment", "comments": "The first two authors contributed equally. The code is available upon\n  request. Please contact the last author", "journal-ref": "The Sixth IEEE International Conference on Multimedia Big Data,\n  August 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the inference of gender, major/minor (computer\nscience, non-computer science), typing style, age, and height from the typing\npatterns collected from 117 individuals in a multi-device environment. The\ninference of the first three identifiers was considered as classification\ntasks, while the rest as regression tasks. For classification tasks, we\nbenchmark the performance of six classical machine learning (ML) and four deep\nlearning (DL) classifiers. On the other hand, for regression tasks, we\nevaluated three ML and four DL-based regressors. The overall experiment\nconsisted of two text-entry (free and fixed) and four device (Desktop, Tablet,\nPhone, and Combined) configurations. The best arrangements achieved accuracies\nof 96.15%, 93.02%, and 87.80% for typing style, gender, and major/minor,\nrespectively, and mean absolute errors of 1.77 years and 2.65 inches for age\nand height, respectively. The results are promising considering the variety of\napplication scenarios that we have listed in this work.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 20:25:58 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Udandarao", "Vishaal", ""], ["Agrawal", "Mohit", ""], ["Kumar", "Rajesh", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2006.09521", "submitter": "Kai Lukoff", "authors": "Kai Lukoff, Ulrik Lyngs, Stefania Gueorguieva, Erika S. Dillman,\n  Alexis Hiniker, Sean A. Munson", "title": "From Ancient Contemplative Practice to the App Store: Designing a\n  Digital Container for Mindfulness", "comments": "10 pages (excluding references), 4 figures. To appear in the\n  Proceedings of DIS '20: Designing Interactive Systems Conference 2020", "journal-ref": "Proceedings of DIS '20: Designing Interactive Systems Conference\n  2020", "doi": "10.1145/3357236.3395444", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hundreds of popular mobile apps today market their ties to mindfulness. What\nactivities do these apps support and what benefits do they claim? How do\nmindfulness teachers, as domain experts, view these apps? We first conduct an\nexploratory review of 370 mindfulness-related apps on Google Play, finding that\nmindfulness is presented primarily as a tool for relaxation and stress\nreduction. We then interviewed 15 U.S. mindfulness teachers from the\ntherapeutic, Buddhist, and Yogic traditions about their perspectives on these\napps. Teachers expressed concern that apps that introduce mindfulness only as a\ntool for relaxation neglect its full potential. We draw upon the experiences of\nthese teachers to suggest design implications for linking mindfulness with\nfurther contemplative practices like the cultivation of compassion. Our\nfindings speak to the importance of coherence in design: that the metaphors and\nmechanisms of a technology align with the underlying principles it follows.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 21:21:09 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Lukoff", "Kai", ""], ["Lyngs", "Ulrik", ""], ["Gueorguieva", "Stefania", ""], ["Dillman", "Erika S.", ""], ["Hiniker", "Alexis", ""], ["Munson", "Sean A.", ""]]}, {"id": "2006.09645", "submitter": "Atsuya Kobayashi", "authors": "Atsuya Kobayashi, Reo Anzai, Nao Tokui", "title": "ExSampling: a system for the real-time ensemble performance of\n  field-recorded environmental sounds", "comments": "The International Conference on New Interfaces for Musical Expression\n  2020 poster presentation. 4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose ExSampling: an integrated system of recording application and Deep\nLearning environment for a real-time music performance of environmental sounds\nsampled by field recording. Automated sound mapping to Ableton Live tracks by\nDeep Learning enables field recording to be applied to real-time performance,\nand create interactions among sound recorders, composers and performers.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 04:07:13 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Kobayashi", "Atsuya", ""], ["Anzai", "Reo", ""], ["Tokui", "Nao", ""]]}, {"id": "2006.09736", "submitter": "Casper Hansen", "authors": "Christian Hansen and Casper Hansen and Jakob Grue Simonsen and Birger\n  Larsen and Stephen Alstrup and Christina Lioma", "title": "Factuality Checking in News Headlines with Eye Tracking", "comments": "Accepted to SIGIR 2020", "journal-ref": null, "doi": "10.1145/3397271.3401221", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study whether it is possible to infer if a news headline is true or false\nusing only the movement of the human eyes when reading news headlines. Our\nstudy with 55 participants who are eye-tracked when reading 108 news headlines\n(72 true, 36 false) shows that false headlines receive statistically\nsignificantly less visual attention than true headlines. We further build an\nensemble learner that predicts news headline factuality using only eye-tracking\nmeasurements. Our model yields a mean AUC of 0.688 and is better at detecting\nfalse than true headlines. Through a model analysis, we find that eye-tracking\n25 users when reading 3-6 headlines is sufficient for our ensemble learner.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 09:24:21 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Hansen", "Christian", ""], ["Hansen", "Casper", ""], ["Simonsen", "Jakob Grue", ""], ["Larsen", "Birger", ""], ["Alstrup", "Stephen", ""], ["Lioma", "Christina", ""]]}, {"id": "2006.09888", "submitter": "Patrik Jonell", "authors": "Patrik Jonell, Taras Kucherenko, Gustav Eje Henter, Jonas Beskow", "title": "Let's Face It: Probabilistic Multi-modal Interlocutor-aware Generation\n  of Facial Gestures in Dyadic Settings", "comments": "Best Paper Award. 8 pages, 4 figures, IVA '20: Proceedings of the\n  20th ACM International Conference on Intelligent Virtual Agent", "journal-ref": null, "doi": "10.1145/3383652.3423911", "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.SD eess.AS eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enable more natural face-to-face interactions, conversational agents need\nto adapt their behavior to their interlocutors. One key aspect of this is\ngeneration of appropriate non-verbal behavior for the agent, for example facial\ngestures, here defined as facial expressions and head movements. Most existing\ngesture-generating systems do not utilize multi-modal cues from the\ninterlocutor when synthesizing non-verbal behavior. Those that do, typically\nuse deterministic methods that risk producing repetitive and non-vivid motions.\nIn this paper, we introduce a probabilistic method to synthesize\ninterlocutor-aware facial gestures - represented by highly expressive FLAME\nparameters - in dyadic conversations. Our contributions are: a) a method for\nfeature extraction from multi-party video and speech recordings, resulting in a\nrepresentation that allows for independent control and manipulation of\nexpression and speech articulation in a 3D avatar; b) an extension to MoGlow, a\nrecent motion-synthesis method based on normalizing flows, to also take\nmulti-modal signals from the interlocutor as input and subsequently output\ninterlocutor-aware facial gestures; and c) a subjective evaluation assessing\nthe use and relative importance of the input modalities. The results show that\nthe model successfully leverages the input from the interlocutor to generate\nmore appropriate behavior. Videos, data, and code available at:\nhttps://jonepatr.github.io/lets_face_it.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 14:11:51 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 21:22:20 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Jonell", "Patrik", ""], ["Kucherenko", "Taras", ""], ["Henter", "Gustav Eje", ""], ["Beskow", "Jonas", ""]]}, {"id": "2006.10110", "submitter": "Satish Reddy Bethi", "authors": "Satish Reddy Bethi", "title": "Exergames for telerehabilitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in technology have improved the connectivity between\nhumans enhancing the transfer of information. Leveraging these technological\nmarvels in the healthcare industry has led to the development of telehealth\nallowing patients and clinicians to receive and administer treatment remotely.\nTelerehabilitation is a subset of telehealth that facilitates remote\nrehabilitation treatment for patients. Providing rehabilitative services to the\naging baby boomer population requires tech-savvy solutions to augment the\ntherapists and clinicians for effective remote monitoring and tele-medicine.\nHence, this thesis develops easy-to-use exergames for low-cost mechatronic\ndevices targeting rehabilitation of post-stroke patients. Specifically, it\ndemonstrates wearable inertial sensors for exergames consisting of an animated\nvirtual coach for providing patients with instructions for performing range of\nmotion exercises. Next, a gaming environment is developed for task-specific\nrehabilitation such as eating. Finally, exergames are developed for\nrehabilitation of pincer grasping. In addition to gamified interfaces providing\nan engaging rehabilitation experience to the user, the data acquired from the\nmechatronic devices facilitate data-driven telerehabilitation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:24:47 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Bethi", "Satish Reddy", ""]]}, {"id": "2006.10191", "submitter": "Tiffany Do", "authors": "Tiffany D. Do, Dylan S. Yu, Salman Anwer, Seong Ioi Wang", "title": "Using Collaborative Filtering to Recommend Champions in League of\n  Legends", "comments": "4 pages, in proceedings of 2020 IEEE Conference on Games, COG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  League of Legends (LoL), one of the most widely played computer games in the\nworld, has over 140 playable characters known as champions that have highly\nvarying play styles. However, there is not much work on providing champion\nrecommendations to a player in LoL. In this paper, we propose that a\nrecommendation system based on a collaborative filtering approach using\nsingular value decomposition provides champion recommendations that players\nenjoy. We discuss the implementation behind our recommendation system and also\nevaluate the practicality of our system using a preliminary user study. Our\nresults indicate that players significantly preferred recommendations from our\nsystem over random recommendations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 22:57:41 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Do", "Tiffany D.", ""], ["Yu", "Dylan S.", ""], ["Anwer", "Salman", ""], ["Wang", "Seong Ioi", ""]]}, {"id": "2006.10228", "submitter": "Chang-Shing Lee", "authors": "Chang-Shing Lee, Mei-Hui Wang, Wen-Kai Kuan, Zong-Han Ciou, Yi-Lin\n  Tsai, Wei-Shan Chang, Lian-Chao Li, Naoyuki Kubota, Tzong-Xiang Huang, Eri\n  Sato-Shimokawara, and Toru Yamaguchi", "title": "A Study on AI-FML Robotic Agent for Student Learning Behavior Ontology\n  Construction", "comments": "This article has been accepted as a conference paper at CcS 2020 and\n  will be published in IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an AI-FML robotic agent for student learning\nbehavior ontology construction which can be applied in English speaking and\nlistening domain. The AI-FML robotic agent with the ontology contains the\nperception intelligence, computational intelligence, and cognition intelligence\nfor analyzing student learning behavior. In addition, there are three\nintelligent agents, including a perception agent, a computational agent, and a\ncognition agent in the AI-FML robotic agent. We deploy the perception agent and\nthe cognition agent on the robot Kebbi Air. Moreover, the computational agent\nwith the Deep Neural Network (DNN) model is performed in the cloud and can\ncommunicate with the perception agent and cognition agent via the Internet. The\nproposed AI-FML robotic agent is applied in Taiwan and tested in Japan. The\nexperimental results show that the agents can be utilized in the human and\nmachine co-learning model for the future education.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 01:45:30 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 11:56:54 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Lee", "Chang-Shing", ""], ["Wang", "Mei-Hui", ""], ["Kuan", "Wen-Kai", ""], ["Ciou", "Zong-Han", ""], ["Tsai", "Yi-Lin", ""], ["Chang", "Wei-Shan", ""], ["Li", "Lian-Chao", ""], ["Kubota", "Naoyuki", ""], ["Huang", "Tzong-Xiang", ""], ["Sato-Shimokawara", "Eri", ""], ["Yamaguchi", "Toru", ""]]}, {"id": "2006.10333", "submitter": "Ilia Iuskevich", "authors": "Ilya Yuskevich (IRT SystemX, LGI), A. Hein (LGI), Kahina\n  Amokrane-Ferka (IRT SystemX), Abdelkrim Doufene (IRT SystemX), Marija\n  Jankovic (LGI)", "title": "A discrete-event simulation model for driver performance assessment:\n  application to autonomous vehicle cockpit design optimization", "comments": null, "journal-ref": "Proceedings of the Design Society: DESIGN Conference, Oct 2020,\n  Dubrovnik, Croatia. pp.2521-2530", "doi": "10.1017/dsd.2020.157", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest advances in the design of vehicles with the adaptive level of\nautomation pose new challenges in the vehicle-driver interaction. Safety\nrequirements underline the need to explore optimal cockpit architectures with\nregard to driver cognitive and perceptual workload, eyes-off-the-road time and\nsituation awareness. We propose to integrate existing task analysis approaches\ninto system architecture evaluation for the early-stage design optimization. We\nbuilt the discrete-event simulation tool and applied it within the\nmulti-sensory (sight, sound, touch) cockpit design industrial project.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 07:40:53 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Yuskevich", "Ilya", "", "IRT SystemX, LGI"], ["Hein", "A.", "", "LGI"], ["Amokrane-Ferka", "Kahina", "", "IRT SystemX"], ["Doufene", "Abdelkrim", "", "IRT SystemX"], ["Jankovic", "Marija", "", "LGI"]]}, {"id": "2006.10635", "submitter": "Matthew Lee", "authors": "Matthew Lee", "title": "Detecting Affective Flow States of Knowledge Workers Using Physiological\n  Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flow-like experiences at work are important for productivity and worker\nwell-being. However, it is difficult to objectively detect when workers are\nexperiencing flow in their work. In this paper, we investigate how to predict a\nworker's focus state based on physiological signals. We conducted a lab study\nto collect physiological data from knowledge workers experienced different\nlevels of flow while performing work tasks. We used the nine characteristics of\nflow to design tasks that would induce different focus states. A manipulation\ncheck using the Flow Short Scale verified that participants experienced three\ndistinct flow states, one overly challenging non-flow state, and two types of\nflow states, balanced flow, and automatic flow. We built machine learning\nclassifiers that can distinguish between non-flow and flow states with 0.889\naverage AUC and rest states from working states with 0.98 average AUC. The\nresults show that physiological sensing can detect focused flow states of\nknowledge workers and can enable ways to for individuals and organizations to\nimprove both productivity and worker satisfaction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:59:57 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Lee", "Matthew", ""]]}, {"id": "2006.10681", "submitter": "Paula Lago", "authors": "Paula Lago, Shingo Takeda, Sayeda Shamma Alia, Kohei Adachi, Brahim\n  Bennai, Francois Charpillet, Sozo Inoue", "title": "A dataset for complex activity recognition withmicro and macro\n  activities in a cooking scenario", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex activity recognition can benefit from understanding the steps that\ncompose them. Current datasets, however, are annotated with one label only,\nhindering research in this direction. In this paper, we describe a new dataset\nfor sensor-based activity recognition featuring macro and micro activities in a\ncooking scenario. Three sensing systems measured simultaneously, namely a\nmotion capture system, tracking 25 points on the body; two smartphone\naccelerometers, one on the hip and the other one on the forearm; and two\nsmartwatches one on each wrist. The dataset is labeled for both the recipes\n(macro activities) and the steps (micro activities). We summarize the results\nof a baseline classification using traditional activity recognition pipelines.\nThe dataset is designed to be easily used to test and develop activity\nrecognition approaches.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:09:42 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Lago", "Paula", ""], ["Takeda", "Shingo", ""], ["Alia", "Sayeda Shamma", ""], ["Adachi", "Kohei", ""], ["Bennai", "Brahim", ""], ["Charpillet", "Francois", ""], ["Inoue", "Sozo", ""]]}, {"id": "2006.10793", "submitter": "Magy Seif El-Nasr", "authors": "Jennifer Villareale, Colan F. Biemer, Magy Seif El-Nasr, and Jichen\n  Zhu", "title": "Reflection in Game-Based Learning: A Survey of Programming Games", "comments": null, "journal-ref": "Foundations of Digital Games 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflection is a critical aspect of the learning process. However, educational\ngames tend to focus on supporting learning concepts rather than supporting\nreflection. While reflection occurs in educational games, the educational game\ndesign and research community can benefit from more knowledge of how to\nfacilitate player reflection through game design. In this paper, we examine\neducational programming games and analyze how reflection is currently\nsupported. We find that current approaches prioritize accuracy over the\nindividual learning process and often only support reflection post-gameplay.\nOur analysis identifies common reflective features, and we develop a set of\nopen areas for future work. We discuss these promising directions towards\nengaging the community in developing more mechanics for reflection in\neducational games.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 18:21:48 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Villareale", "Jennifer", ""], ["Biemer", "Colan F.", ""], ["El-Nasr", "Magy Seif", ""], ["Zhu", "Jichen", ""]]}, {"id": "2006.10808", "submitter": "Magy Seif El-Nasr", "authors": "Magy Seif El-Nasr and Erica Kleinman", "title": "Data-Driven Game Development: Ethical Considerations", "comments": null, "journal-ref": "Foundations of Digital Games 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the games industry has made a major move towards data-driven\ndevelopment, using data analytics and player modeling to inform design\ndecisions. Data-driven techniques are beneficial as they allow for the study of\nplayer behavior at scale, making them very applicable to modern digital game\ndevelopment. However, with this move towards data driven decision-making comes\na number of ethical concerns. Previous work in player modeling as well as work\nin the fields of AI and machine learning have demonstrated several ways in\nwhich algorithmic decision-making can be flawed due to data or algorithmic bias\nor lack of data from specific groups. Further, black box algorithms create a\ntrust problem due to lack of interpretability and transparency of the results\nor models developed based on the data, requiring blind faith in the results. In\nthis position paper, we discuss several factors affecting the use of game data\nin the development cycle. In addition to issues raised by previous work, we\nalso raise issues with algorithms marginalizing certain player groups and flaws\nin the resulting models due to their inability to reason about situational\nfactors affecting players' decisions. Further, we outline some work that seeks\nto address these problems and identify some open problems concerning ethics and\ngame data science.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 19:01:33 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["El-Nasr", "Magy Seif", ""], ["Kleinman", "Erica", ""]]}, {"id": "2006.10823", "submitter": "Magy Seif El-Nasr", "authors": "Erica Kleinman, Sabbir Ahmad, Zhaoqing Teng, Andy Bryant, Truong-Huy\n  D. Nguyen, Casper Harteveld, Magy Seif El-Nasr", "title": "\"And then they died\": Using Action Sequences for Data Driven,Context\n  Aware Gameplay Analysis", "comments": null, "journal-ref": "Foundations of Digital Games 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many successful games rely heavily on data analytics to understand players\nand inform design. Popular methodologies focus on machine learning and\nstatistical analysis of aggregated data. While effective in extracting\ninformation regarding player action, much of the context regarding when and how\nthose actions occurred is lost. Qualitative methods allow researchers to\nexamine context and derive meaningful explanations about the goals and\nmotivations behind player behavior, but are difficult to scale. In this paper,\nwe build on previous work by combining two existing methodologies: Interactive\nBehavior Analytics (IBA) and sequence analysis (SA), in order to create a\nnovel, mixed methods, human-in-the-loop data analysis methodology that uses\nbehavioral labels and visualizations to allow analysts to examine player\nbehavior in a way that is context sensitive, scalable, and generalizable. We\npresent the methodology along with a case study demonstrating how it can be\nused to analyze behavioral patterns of teamwork in the popular multiplayer game\nDefense of the Ancients 2 (DotA 2).\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 19:38:46 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Kleinman", "Erica", ""], ["Ahmad", "Sabbir", ""], ["Teng", "Zhaoqing", ""], ["Bryant", "Andy", ""], ["Nguyen", "Truong-Huy D.", ""], ["Harteveld", "Casper", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "2006.11043", "submitter": "Lachlan Urquhart Ph.D", "authors": "Lachlan Urquhart and Jiahong Chen", "title": "On the Principle of Accountability: Challenges for Smart Homes &\n  Cybersecurity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter introduces the Accountability Principle and its role in data\nprotection governance. We focus on what accountability means in the context of\ncybersecurity management in smart homes, considering the EU General Data\nProtection Law requirements to secure personal data. This discussion sits\nagainst the backdrop of two key new developments in data protection law.\nFirstly, the law is moving into the home, due to narrowing of the so called\nhousehold exemption. Concurrently, household occupants may now have legal\nresponsibilities to comply with the GDPR, as they find themselves jointly\nresponsible for compliance, as they are possibly held to determine the means\nand purposes of data collection with IoT device vendors. As a complex\nsocio-technical space, we consider the interactions between accountability\nrequirements and the competencies of this new class of domestic data\ncontrollers (DDCs). Specifically, we consider the value and limitations of\nedge-based security analytics to manage smart home cybersecurity risks,\nreviewing a range of prototypes and studies of their use. We also reflect on\ninterpersonal power dynamics in the domestic setting e.g. device control;\nexisting social practices around privacy and security management in smart\nhomes; and usability issues that may hamper DDCs ability to rely on such\nsolutions. We conclude by reflecting on 1) the need for collective security\nmanagement in homes and 2) the increasingly complex divisions of responsibility\nin smart homes between device users, account holders, IoT\ndevice/software/firmware vendors, and third parties.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 09:50:21 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Urquhart", "Lachlan", ""], ["Chen", "Jiahong", ""]]}, {"id": "2006.11044", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Ardavan Bidgoli, Hans Kellner", "title": "V-Dream: Immersive Exploration of Generative Design Solution Space", "comments": "Accepted to HCI International 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Design workflows have introduced alternative paradigms in the\ndomain of computational design, allowing designers to generate large pools of\nvalid solutions by defining a set of goals and constraints. However, analyzing\nand narrowing down the generated solution space, which usually consists of\nvarious high-dimensional properties, has been a major challenge in current\ngenerative workflows. By taking advantage of the interactive unbounded spatial\nexploration, and the visual immersion offered in virtual reality platforms, we\npropose V-Dream, a virtual reality generative analysis framework for exploring\nlarge-scale solution spaces. V-Dream proposes a hybrid search workflow in which\na spatial stochastic search approach is combined with a recommender system\nallowing users to pick desired candidates and eliminate the undesired ones\niteratively. In each cycle, V-Dream reorganizes the remaining options in\nclusters based on the defined features. Moreover, our framework allows users to\ninspect design solutions and evaluate their performance metrics in various\nhierarchical levels, assisting them in narrowing down the solution space\nthrough iterative cycles of search/select/re-clustering of the solutions in an\nimmersive fashion. Finally, we present a prototype of our proposed framework,\nillustrating how users can navigate and narrow down desired solutions from a\npool of over 16000 monitor stands generated by Autodesk's Dreamcatcher\nsoftware.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 09:51:27 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Bidgoli", "Ardavan", ""], ["Kellner", "Hans", ""]]}, {"id": "2006.11158", "submitter": "Max Pellert", "authors": "Max Pellert, Jana Lasser, Hannah Metzler and David Garcia", "title": "Dashboard of sentiment in Austrian social media during COVID-19", "comments": "23 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To track online emotional expressions of the Austrian population close to\nreal-time during the COVID-19 pandemic, we build a self-updating monitor of\nemotion dynamics using digital traces from three different data sources. This\nenables decision makers and the interested public to assess issues such as the\nattitude towards counter-measures taken during the pandemic and the possible\nemergence of a (mental) health crisis early on. We use web scraping and API\naccess to retrieve data from the news platform derstandard.at, Twitter and a\nchat platform for students. We document the technical details of our workflow\nin order to provide materials for other researchers interested in building a\nsimilar tool for different contexts. Automated text analysis allows us to\nhighlight changes of language use during COVID-19 in comparison to a neutral\nbaseline. We use special word clouds to visualize that overall difference.\nLongitudinally, our time series show spikes in anxiety that can be linked to\nseveral events and media reporting. Additionally, we find a marked decrease in\nanger. The changes last for remarkably long periods of time (up to 12 weeks).\nWe discuss these and more patterns and connect them to the emergence of\ncollective emotions. The interactive dashboard showcasing our data is available\nonline under http://www.mpellert.at/covid19_monitor_austria/. Our work has\nattracted media attention and is part of an web archive of resources on\nCOVID-19 collected by the Austrian National Library.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:42:38 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Pellert", "Max", ""], ["Lasser", "Jana", ""], ["Metzler", "Hannah", ""], ["Garcia", "David", ""]]}, {"id": "2006.11199", "submitter": "Magy Seif El-Nasr", "authors": "Sabbir Ahmad, Andy Bryant, Erica Kleinman, Zhaoqing Teng, Truong-Huy\n  D. Nguyen, and Magy Seif El-Nasr", "title": "Modeling Individual and Team Behavior through Spatio-temporal Analysis", "comments": null, "journal-ref": "CHI Play 2019", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling players' behaviors in games has gained increased momentum in the\npast few years. This area of research has wide applications, including modeling\nlearners and understanding player strategies, to mention a few. In this paper,\nwe present a new methodology, called Interactive Behavior Analytics (IBA),\ncomprised of two visualization systems, a labeling mechanism, and abstraction\nalgorithms that use Dynamic Time Warping and clustering algorithms. The\nmethodology is packaged in a seamless interface to facilitate knowledge\ndiscovery from game data. We demonstrate the use of this methodology with data\nfrom two multiplayer team-based games: BoomTown, a game developed by Gallup,\nand DotA 2. The results of this work show the effectiveness of this method in\nmodeling, and developing human-interpretable models of team and individual\nbehavior.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 15:59:46 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Ahmad", "Sabbir", ""], ["Bryant", "Andy", ""], ["Kleinman", "Erica", ""], ["Teng", "Zhaoqing", ""], ["Nguyen", "Truong-Huy D.", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "2006.11218", "submitter": "Yusuf Aydin", "authors": "Yusuf Aydin and Ozan Tokatli and Volkan Patoglu and Cagatay Basdogan", "title": "A Computational Multi-Criteria Optimization Approach to Controller\n  Design for Physical Human-Robot Interaction", "comments": "13 pages, 13 figures. Accepted to IEEE Transaction on Robotics", "journal-ref": null, "doi": "10.1109/TRO.2020.2998606", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical human-robot interaction (pHRI) integrates the benefits of human\noperator and a collaborative robot in tasks involving physical interaction,\nwith the aim of increasing the task performance. However, the design of\ninteraction controllers that achieve safe and transparent operations is\nchallenging, mainly due to the contradicting nature of these objectives.\nKnowing that attaining perfect transparency is practically unachievable,\ncontrollers that allow better compromise between these objectives are\ndesirable. In this paper, we propose a multi-criteria optimization framework,\nwhich jointly optimizes the stability robustness and transparency of a\nclosed-loop pHRI system for a given interaction controller. In particular, we\npropose a Pareto optimization framework that allows the designer to make\ninformed decisions by thoroughly studying the trade-off between stability\nrobustness and transparency. The proposed framework involves a search over the\ndiscretized controller parameter space to compute the Pareto front curve and a\nselection of controller parameters that yield maximum attainable transparency\nand stability robustness by studying this trade-off curve. The proposed\nframework not only leads to the design of an optimal controller, but also\nenables a fair comparison among different interaction controllers. In order to\ndemonstrate the practical use of the proposed approach, integer and fractional\norder admittance controllers are studied as a case study and compared both\nanalytically and experimentally. The experimental results validate the proposed\ndesign framework and show that the achievable transparency under fractional\norder admittance controller is higher than that of integer order one, when both\ncontrollers are designed to ensure the same level of stability robustness.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 16:46:45 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Aydin", "Yusuf", ""], ["Tokatli", "Ozan", ""], ["Patoglu", "Volkan", ""], ["Basdogan", "Cagatay", ""]]}, {"id": "2006.11398", "submitter": "Mark Whiting", "authors": "Abdullah Almaatouq, Joshua Becker, James P. Houghton, Nicolas Paton,\n  Duncan J. Watts, Mark E. Whiting", "title": "Empirica: a virtual lab for high-throughput macro-level experiments", "comments": "36 pages, 6 figures. Accepted to Behavioral Research Methods. Behav\n  Res (2021)", "journal-ref": null, "doi": "10.3758/s13428-020-01535-9", "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual labs allow researchers to design high-throughput and macro-level\nexperiments that are not feasible in traditional in-person physical lab\nsettings. Despite the increasing popularity of online research, researchers\nstill face many technical and logistical barriers when designing and deploying\nvirtual lab experiments. While several platforms exist to facilitate the\ndevelopment of virtual lab experiments, they typically present researchers with\na stark trade-off between usability and functionality. We introduce Empirica: a\nmodular virtual lab that offers a solution to the usability-functionality\ntrade-off by employing a \"flexible defaults\" design strategy. This strategy\nenables us to maintain complete \"build anything\" flexibility while offering a\ndevelopment platform that is accessible to novice programmers. Empirica's\narchitecture is designed to allow for parameterizable experimental designs,\nreusable protocols, and rapid development. These features will increase the\naccessibility of virtual lab experiments, remove barriers to innovation in\nexperiment design, and enable rapid progress in the understanding of\ndistributed human computation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 21:28:07 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 15:57:28 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Almaatouq", "Abdullah", ""], ["Becker", "Joshua", ""], ["Houghton", "James P.", ""], ["Paton", "Nicolas", ""], ["Watts", "Duncan J.", ""], ["Whiting", "Mark E.", ""]]}, {"id": "2006.11684", "submitter": "Yuan Shen", "authors": "Yuan Shen, Shanduojiao Jiang, Yanlin Chen, Eileen Yang, Xilun Jin,\n  Yuliang Fan, Katie Driggs Campbell", "title": "To Explain or Not to Explain: A Study on the Necessity of Explanations\n  for Autonomous Vehicles", "comments": "9.5 pages, 7 figures, submitted to UIST2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable AI, in the context of autonomous systems, like self driving cars,\nhas drawn broad interests from researchers. Recent studies have found that\nproviding explanations for an autonomous vehicle actions has many benefits,\ne.g., increase trust and acceptance, but put little emphasis on when an\nexplanation is needed and how the content of explanation changes with context.\nIn this work, we investigate which scenarios people need explanations and how\nthe critical degree of explanation shifts with situations and driver types.\nThrough a user experiment, we ask participants to evaluate how necessary an\nexplanation is and measure the impact on their trust in the self driving cars\nin different contexts. We also present a self driving explanation dataset with\nfirst person explanations and associated measure of the necessity for 1103\nvideo clips, augmenting the Berkeley Deep Drive Attention dataset.\nAdditionally, we propose a learning based model that predicts how necessary an\nexplanation for a given situation in real time, using camera data inputs. Our\nresearch reveals that driver types and context dictates whether or not an\nexplanation is necessary and what is helpful for improved interaction and\nunderstanding.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 00:38:24 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Shen", "Yuan", ""], ["Jiang", "Shanduojiao", ""], ["Chen", "Yanlin", ""], ["Yang", "Eileen", ""], ["Jin", "Xilun", ""], ["Fan", "Yuliang", ""], ["Campbell", "Katie Driggs", ""]]}, {"id": "2006.11904", "submitter": "Jakob Bardram", "authors": "Jakob E. Bardram", "title": "The CARP Mobile Sensing Framework -- A Cross-platform, Reactive,\n  Programming Framework and Runtime Environment for Digital Phenotyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile sensing - i.e., the ability to unobtrusively collect sensor data from\nbuilt-in phone sensors - has long been a core research topic in Ubicomp. A\nnumber of technological platforms for mobile sensing have been presented over\nthe years and a lot of knowledge on how to facilitate mobile sensing has been\naccumulated. This paper presents the CARP Mobile Sensing (CAMS) framework,\nwhich is a modern cross-platform (Android / iOS) software architecture\nproviding a reactive and unified programming model that emphasizes\nextensibility, maintainability, and adaptability. Moreover, the CAMS framework\nsupports sensing from wearable devices such as an electrocardiography (ECG)\nmonitor, and configuring data transformers. The latter allows to transform\ncollected data to a standardized data format and to implement\nprivacy-preserving data transformations. The paper presents the design,\narchitecture, implementation, and evaluation of CAMS, and shows how the\nframework has been used in two real-world mobile sensing and mobile health\n(mHealth) applications. We conclude that CAMS provides a novel cross-platform\napplication programming framework which has proved mature, stable, scalable,\nand flexible in the design of digital phenotyping and mHealth applications\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 20:28:24 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bardram", "Jakob E.", ""]]}, {"id": "2006.11929", "submitter": "Lynsay Shepherd", "authors": "Harjinder Singh Lallie, Lynsay A. Shepherd, Jason R. C. Nurse, Arnau\n  Erola, Gregory Epiphaniou, Carsten Maple, Xavier Bellekens", "title": "Cyber Security in the Age of COVID-19: A Timeline and Analysis of\n  Cyber-Crime and Cyber-Attacks during the Pandemic", "comments": "20 pages, 6 figures", "journal-ref": "Computers & Security 2021", "doi": "10.1016/j.cose.2021.102248", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic was a remarkable unprecedented event which altered the\nlives of billions of citizens globally resulting in what became commonly\nreferred to as the new-normal in terms of societal norms and the way we live\nand work. Aside from the extraordinary impact on society and business as a\nwhole, the pandemic generated a set of unique cyber-crime related circumstances\nwhich also affected society and business. The increased anxiety caused by the\npandemic heightened the likelihood of cyber-attacks succeeding corresponding\nwith an increase in the number and range of cyber-attacks.\n  This paper analyses the COVID-19 pandemic from a cyber-crime perspective and\nhighlights the range of cyber-attacks experienced globally during the pandemic.\nCyber-attacks are analysed and considered within the context of key global\nevents to reveal the modus-operandi of cyber-attack campaigns. The analysis\nshows how following what appeared to be large gaps between the initial outbreak\nof the pandemic in China and the first COVID-19 related cyber-attack, attacks\nsteadily became much more prevalent to the point that on some days, 3 or 4\nunique cyber-attacks were being reported. The analysis proceeds to utilise the\nUK as a case study to demonstrate how cyber-criminals leveraged key events and\ngovernmental announcements to carefully craft and design cyber-crime campaigns.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 22:53:47 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lallie", "Harjinder Singh", ""], ["Shepherd", "Lynsay A.", ""], ["Nurse", "Jason R. C.", ""], ["Erola", "Arnau", ""], ["Epiphaniou", "Gregory", ""], ["Maple", "Carsten", ""], ["Bellekens", "Xavier", ""]]}, {"id": "2006.12246", "submitter": "Matthew Lee", "authors": "Matthew Lee, Lyndon Kennedy, Andreas Girgensohn, Lynn Wilcox, John\n  Song En Lee, Chin Wen Tan, Ban Leong Sng", "title": "Pain Intensity Estimation from Mobile Video Using 2D and 3D Facial\n  Keypoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing post-surgical pain is critical for successful surgical outcomes. One\nof the challenges of pain management is accurately assessing the pain level of\npatients. Self-reported numeric pain ratings are limited because they are\nsubjective, can be affected by mood, and can influence the patient's perception\nof pain when making comparisons. In this paper, we introduce an approach that\nanalyzes 2D and 3D facial keypoints of post-surgical patients to estimate their\npain intensity level. Our approach leverages the previously unexplored\ncapabilities of a smartphone to capture a dense 3D representation of a person's\nface as input for pain intensity level estimation. Our contributions are adata\ncollection study with post-surgical patients to collect ground-truth labeled\nsequences of 2D and 3D facial keypoints for developing a pain estimation\nalgorithm, a pain estimation model that uses multiple instance learning to\novercome inherent limitations in facial keypoint sequences, and the preliminary\nresults of the pain estimation model using 2D and 3D features with comparisons\nof alternate approaches.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 00:18:29 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Lee", "Matthew", ""], ["Kennedy", "Lyndon", ""], ["Girgensohn", "Andreas", ""], ["Wilcox", "Lynn", ""], ["Lee", "John Song En", ""], ["Tan", "Chin Wen", ""], ["Sng", "Ban Leong", ""]]}, {"id": "2006.12349", "submitter": "Miguel Altamirano Cabrera", "authors": "Miguel Altamirano Cabrera, Juan Heredia, and Dzmitry Tsetserukou", "title": "Tactile Perception of Objects by the User's Palm for the Development of\n  Multi-contact Wearable Tactile Displays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The user's palm plays an important role in object detection and manipulation.\nThe design of a robust multi-contact tactile display must consider the\nsensation and perception of of the stimulated area aiming to deliver the right\nstimuli at the correct location. To the best of our knowledge, there is no\nstudy to obtain the human palm data for this purpose. The objective of this\nwork is to introduce the method to investigate the user's palm sensations\nduring the interaction with objects. An array of fifteen Force Sensitive\nResistors (FSRs) was located at the user's palm to get the area of interaction,\nand the normal force delivered to four different convex surfaces. Experimental\nresults showed the active areas at the palm during the interaction with each of\nthe surfaces at different forces. The obtained results can be applied in the\ndevelopment of multi-contact wearable tactile and haptic displays for the palm,\nand in training a machine-learning algorithm to predict stimuli aiming to\nachieve a highly immersive experience in Virtual Reality.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 15:47:01 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Cabrera", "Miguel Altamirano", ""], ["Heredia", "Juan", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "2006.12453", "submitter": "D Bayani", "authors": "David Bayani (1), Stefan Mitsch (1) ((1) Carnegie Mellon University)", "title": "Fanoos: Multi-Resolution, Multi-Strength, Interactive Explanations for\n  Learned Systems", "comments": "52 pages, 19 pages main body, 90 references, 3 figures, 5 tables, 12\n  pseudocode blocks Update 24 Sep. 2020 : Added a pointer to further, external\n  content: Append Section E. Update 20 Mar. 2021: Substantial additions.\n  Further explanations of process, with far more pseudocode. Some corrections\n  to a previous description; see errata section. Also briefly describe a few\n  implemented extensions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning becomes increasingly important to control the behavior of\nsafety and financially critical components in sophisticated environments, where\nthe inability to understand learned components in general, and neural nets in\nparticular, poses serious obstacles to their adoption. Explainability and\ninterpretability methods for learned systems have gained considerable academic\nattention, but the focus of current approaches on only one aspect of\nexplanation, at a fixed level of abstraction, and limited if any formal\nguarantees, prevents those explanations from being digestible by the relevant\nstakeholders (e.g., end users, certification authorities, engineers) with their\ndiverse backgrounds and situation-specific needs. We introduce Fanoos, a\nflexible framework for combining formal verification techniques, heuristic\nsearch, and user interaction to explore explanations at the desired level of\ngranularity and fidelity. We demonstrate the ability of Fanoos to produce and\nadjust the abstractness of explanations in response to user requests on a\nlearned controller for an inverted double pendulum and on a learned CPU usage\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:35:53 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 05:13:25 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 04:12:45 GMT"}, {"version": "v4", "created": "Sat, 20 Mar 2021 07:30:47 GMT"}, {"version": "v5", "created": "Wed, 14 Apr 2021 01:03:32 GMT"}, {"version": "v6", "created": "Tue, 27 Apr 2021 07:49:17 GMT"}, {"version": "v7", "created": "Wed, 28 Apr 2021 08:59:42 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Bayani", "David", "", "Carnegie Mellon University"], ["Mitsch", "Stefan", "", "Carnegie Mellon University"]]}, {"id": "2006.12683", "submitter": "Hongyan Gu", "authors": "Hongyan Gu, Yifan Xu, Mohammad Haeri Haeri, Xiang 'Anthony' Chen", "title": "CrossPath: Top-down, Cross Data Type, Multi-Criterion Histological\n  Analysis by Shepherding Mixed AI Models", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven AI promises support for pathologists to discover sparse tumor\npatterns in high-resolution histological images. However, three limitations\nprevent AI from being adopted into clinical practice: (i) a lack of\ncomprehensiveness where most AI algorithms only rely on single\ncriteria/examination; (ii) a lack of explainability where AI models work as\n'black-boxes' with little transparency; (iii) a lack of integrability where it\nis unclear how AI can become part of pathologists' existing workflow. To\naddress these limitations, we propose CrossPath: a brain tumor grading tool\nthat supports top-down, cross data type, multi-criterion histological analysis,\nwhere pathologists can shepherd mixed AI models. CrossPath first uses AI to\ndiscover multiple histological criteria with H and E and Ki-67 slides based on\nWHO guidelines. Second, CrossPath demonstrates AI findings with multi-level\nexplainable supportive evidence. Finally, CrossPath provides a top-down\nshepherding workflow to help pathologists derive an evidence-based, precise\ngrading result. To validate CrossPath, we conducted a user study with\npathologists in a local medical center. The result shows that CrossPath\nachieves a high level of comprehensiveness, explainability, and integrability\nwhile reducing about one-third time consumption compared to using a traditional\noptical microscope.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 01:02:15 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 00:45:56 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Gu", "Hongyan", ""], ["Xu", "Yifan", ""], ["Haeri", "Mohammad Haeri", ""], ["Chen", "Xiang 'Anthony'", ""]]}, {"id": "2006.12695", "submitter": "Hongyan Gu", "authors": "Hongyan Gu, Jingbin Huang, Lauren Hung, Xiang 'Anthony' Chen", "title": "Lessons Learned from Designing an AI-Enabled Diagnosis Tool for\n  Pathologists", "comments": "25 pages, 5 figures. To appear in the 24th ACM Conference on\n  Computer-Supported Cooperative Work and Social Computing (CSCW 2021)", "journal-ref": null, "doi": "10.1145/3449084", "report-no": null, "categories": "cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the promises of data-driven artificial intelligence (AI), little is\nknown about how we can bridge the gulf between traditional physician-driven\ndiagnosis and a plausible future of medicine automated by AI. Specifically, how\ncan we involve AI usefully in physicians' diagnosis workflow given that most AI\nis still nascent and error-prone (e.g., in digital pathology)? To explore this\nquestion, we first propose a series of collaborative techniques to engage human\npathologists with AI given AI's capabilities and limitations, based on which we\nprototype Impetus - a tool where an AI takes various degrees of initiatives to\nprovide various forms of assistance to a pathologist in detecting tumors from\nhistological slides. We summarize observations and lessons learned from a study\nwith eight pathologists and discuss recommendations for future work on\nhuman-centered medical AI systems.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 01:49:45 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 01:25:58 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 22:01:10 GMT"}, {"version": "v4", "created": "Thu, 11 Feb 2021 02:01:02 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Gu", "Hongyan", ""], ["Huang", "Jingbin", ""], ["Hung", "Lauren", ""], ["Chen", "Xiang 'Anthony'", ""]]}, {"id": "2006.12718", "submitter": "Siwei Fu", "authors": "Siwei Fu, Jian Zhao, Linping Yuan, Zhicheng Liu, Kwan-Liu Ma, Huamin\n  Qu", "title": "ICE: Identify and Compare Event Sequence Sets through Multi-Scale Matrix\n  and Unit Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparative analysis of event sequence data is essential in many application\ndomains, such as website design and medical care. However, analysts often face\ntwo challenges: they may not always know which sets of event sequences in the\ndata are useful to compare, and the comparison needs to be achieved at\ndifferent granularity, due to the volume and complexity of the data. This paper\npresents, ICE, an interactive visualization that allows analysts to explore an\nevent sequence dataset, and identify promising sets of event sequences to\ncompare at both the pattern and sequence levels. More specifically, ICE\nincorporates a multi-level matrix-based visualization for browsing the entire\ndataset based on the prefixes and suffixes of sequences. To support comparison\nat multiple levels, ICE employs the unit visualization technique, and we\nfurther explore the design space of unit visualizations for event sequence\ncomparison tasks. Finally, we demonstrate the effectiveness of ICE with three\nreal-world datasets from different domains.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 03:31:35 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Fu", "Siwei", ""], ["Zhao", "Jian", ""], ["Yuan", "Linping", ""], ["Liu", "Zhicheng", ""], ["Ma", "Kwan-Liu", ""], ["Qu", "Huamin", ""]]}, {"id": "2006.12719", "submitter": "Shikib Mehri", "authors": "Shikib Mehri and Maxine Eskenazi", "title": "Unsupervised Evaluation of Interactive Dialog with DialoGPT", "comments": "Published at to SIGdial 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is important to define meaningful and interpretable automatic evaluation\nmetrics for open-domain dialog research. Standard language generation metrics\nhave been shown to be ineffective for dialog. This paper introduces the FED\nmetric (fine-grained evaluation of dialog), an automatic evaluation metric\nwhich uses DialoGPT, without any fine-tuning or supervision. It also introduces\nthe FED dataset which is constructed by annotating a set of human-system and\nhuman-human conversations with eighteen fine-grained dialog qualities. The FED\nmetric (1) does not rely on a ground-truth response, (2) does not require\ntraining data and (3) measures fine-grained dialog qualities at both the turn\nand whole dialog levels. FED attains moderate to strong correlation with human\njudgement at both levels.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 03:36:09 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Mehri", "Shikib", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "2006.13208", "submitter": "Andreea Bobu", "authors": "Andreea Bobu, Marius Wiggert, Claire Tomlin, Anca D. Dragan", "title": "Feature Expansive Reward Learning: Rethinking Human Input", "comments": "13 pages, 14 figures", "journal-ref": null, "doi": "10.1145/3434073.3444667", "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a person is not satisfied with how a robot performs a task, they can\nintervene to correct it. Reward learning methods enable the robot to adapt its\nreward function online based on such human input, but they rely on handcrafted\nfeatures. When the correction cannot be explained by these features, recent\nwork in deep Inverse Reinforcement Learning (IRL) suggests that the robot could\nask for task demonstrations and recover a reward defined over the raw state\nspace. Our insight is that rather than implicitly learning about the missing\nfeature(s) from demonstrations, the robot should instead ask for data that\nexplicitly teaches it about what it is missing. We introduce a new type of\nhuman input in which the person guides the robot from states where the feature\nbeing taught is highly expressed to states where it is not. We propose an\nalgorithm for learning the feature from the raw state space and integrating it\ninto the reward function. By focusing the human input on the missing feature,\nour method decreases sample complexity and improves generalization of the\nlearned reward over the above deep IRL baseline. We show this in experiments\nwith a physical 7DOF robot manipulator, as well as in a user study conducted in\na simulated environment.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 17:59:34 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 18:59:50 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Bobu", "Andreea", ""], ["Wiggert", "Marius", ""], ["Tomlin", "Claire", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2006.13259", "submitter": "Lana Yarosh", "authors": "Lana Yarosh, Suzanne Bakken, Alan Borning, Munmun De Choudhury, Cliff\n  Lampe, Elizabeth Mynatt, Stephen Schueller, and Tiffany Veinot", "title": "Computational Support for Substance Use Disorder Prevention, Detection,\n  Treatment, and Recovery", "comments": "A Computing Community Consortium (CCC) workshop report, 28 pages", "journal-ref": null, "doi": null, "report-no": "ccc2020report_3", "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substance Use Disorders (SUDs) involve the misuse of any or several of a wide\narray of substances, such as alcohol, opioids, marijuana, and methamphetamine.\nSUDs are characterized by an inability to decrease use despite severe social,\neconomic, and health-related consequences to the individual. A 2017 national\nsurvey identified that 1 in 12 US adults have or have had a substance use\ndisorder. The National Institute on Drug Abuse estimates that SUDs relating to\nalcohol, prescription opioids, and illicit drug use cost the United States over\n$520 billion annually due to crime, lost work productivity, and health care\nexpenses. Most recently, the US Department of Health and Human Services has\ndeclared the national opioid crisis a public health emergency to address the\ngrowing number of opioid overdose deaths in the United States. In this\ninterdisciplinary workshop, we explored how computational support - digital\nsystems, algorithms, and sociotechnical approaches (which consider how\ntechnology and people interact as complex systems) - may enhance and enable\ninnovative interventions for prevention, detection, treatment, and long-term\nrecovery from SUDs.\n  The Computing Community Consortium (CCC) sponsored a two-day workshop titled\n\"Computational Support for Substance Use Disorder Prevention, Detection,\nTreatment, and Recovery\" on November 14-15, 2019 in Washington, DC. As outcomes\nfrom this visioning process, we identified three broad opportunity areas for\ncomputational support in the SUD context:\n  1. Detecting and mitigating risk of SUD relapse, 2. Establishing and\nempowering social support networks, and 3. Collecting and sharing data\nmeaningfully across ecologies of formal and informal care.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:30:20 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Yarosh", "Lana", ""], ["Bakken", "Suzanne", ""], ["Borning", "Alan", ""], ["De Choudhury", "Munmun", ""], ["Lampe", "Cliff", ""], ["Mynatt", "Elizabeth", ""], ["Schueller", "Stephen", ""], ["Veinot", "Tiffany", ""]]}, {"id": "2006.13386", "submitter": "Maneesh Bilalpur", "authors": "Maneesh Bilalpur, Seyed Mostafa Kia, Mohan Kankanhalli, and Ramanathan\n  Subramanian", "title": "Gender and Emotion Recognition from Implicit User Behavior Signals", "comments": "Under consideration for publication in IEEE Trans. Affective\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the utility of implicit behavioral cues, namely,\nElectroencephalogram (EEG) signals and eye movements for gender recognition\n(GR) and emotion recognition (ER) from psychophysical behavior. Specifically,\nthe examined cues are acquired via low-cost, off-the-shelf sensors. 28 users\n(14 male) recognized emotions from unoccluded (no mask) and partially occluded\n(eye or mouth masked) emotive faces; their EEG responses contained\ngender-specific differences, while their eye movements were characteristic of\nthe perceived facial emotions. Experimental results reveal that (a) reliable GR\nand ER is achievable with EEG and eye features, (b) differential cognitive\nprocessing of negative emotions is observed for females and (c) eye gaze-based\ngender differences manifest under partial face occlusion, as typified by the\neye and mouth mask conditions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 23:41:13 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Bilalpur", "Maneesh", ""], ["Kia", "Seyed Mostafa", ""], ["Kankanhalli", "Mohan", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "2006.13390", "submitter": "Siqian Zhao", "authors": "Siqian Zhao, Chunpai Wang, Shaghayegh Sahebi", "title": "Modeling Knowledge Acquisition from Multiple Learning Resource Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Students acquire knowledge as they interact with a variety of learning\nmaterials, such as video lectures, problems, and discussions. Modeling student\nknowledge at each point during their learning period and understanding the\ncontribution of each learning material to student knowledge are essential for\ndetecting students' knowledge gaps and recommending learning materials to them.\nCurrent student knowledge modeling techniques mostly rely on one type of\nlearning material, mainly problems, to model student knowledge growth. These\napproaches ignore the fact that students also learn from other types of\nmaterial. In this paper, we propose a student knowledge model that can capture\nknowledge growth as a result of learning from a diverse set of learning\nresource types while unveiling the association between the learning materials\nof different types. Our multi-view knowledge model (MVKM) incorporates a\nflexible knowledge increase objective on top of a multi-view tensor\nfactorization to capture occasional forgetting while representing student\nknowledge and learning material concepts in a lower-dimensional latent space.\nWe evaluate our model in different experiments toshow that it can accurately\npredict students' future performance, differentiate between knowledge gain in\ndifferent student groups and concepts, and unveil hidden similarities across\nlearning materials of different types.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 23:52:33 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 21:36:50 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Zhao", "Siqian", ""], ["Wang", "Chunpai", ""], ["Sahebi", "Shaghayegh", ""]]}, {"id": "2006.13499", "submitter": "Shahryar Baki", "authors": "Shahryar Baki and Rakesh M. Verma and Arjun Mukherjee and Omprakash\n  Gnawali", "title": "Less is More: Exploiting Social Trust to Increase the Effectiveness of a\n  Deception Attack", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber attacks such as phishing, IRS scams, etc., still are successful in\nfooling Internet users. Users are the last line of defense against these\nattacks since attackers seem to always find a way to bypass security systems.\nUnderstanding users' reason about the scams and frauds can help security\nproviders to improve users security hygiene practices. In this work, we study\nthe users' reasoning and the effectiveness of several variables within the\ncontext of the company representative fraud. Some of the variables that we\nstudy are: 1) the effect of using LinkedIn as a medium for delivering the\nphishing message instead of using email, 2) the effectiveness of natural\nlanguage generation techniques in generating phishing emails, and 3) how some\nsimple customizations, e.g., adding sender's contact info to the email, affect\nparticipants perception. The results obtained from the within-subject study\nshow that participants are not prepared even for a well-known attack - company\nrepresentative fraud. Findings include: approximately 65% mean detection rate\nand insights into how the success rate changes with the facade and\ncorrespondent (sender/receiver) information. A significant finding is that a\nsmaller set of well-chosen strategies is better than a large `mess' of\nstrategies. We also find significant differences in how males and females\napproach the same company representative fraud. Insights from our work could\nhelp defenders in developing better strategies to evaluate their defenses and\nin devising better training strategies.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 05:57:57 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Baki", "Shahryar", ""], ["Verma", "Rakesh M.", ""], ["Mukherjee", "Arjun", ""], ["Gnawali", "Omprakash", ""]]}, {"id": "2006.13503", "submitter": "Ali HeydariGorji", "authors": "Ali HeydariGorji, Seyede Mahya Safavi, Cheng-Ting Lee, Pai H. Chou", "title": "Head-mouse: A simple cursor controller based on optical measurement of\n  head tilt", "comments": null, "journal-ref": "2017 IEEE Signal Processing in Medicine and Biology Symposium\n  (SPMB), Philadelphia, PA, 2017, pp. 1-5", "doi": "10.1109/SPMB.2017.8257058", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a wearable wireless mouse-cursor controller that\noptically tracks the degree of tilt of the user's head to move the mouse\nrelative distances and therefore the degrees of tilt. The raw data can be\nprocessed locally on the wearable device before wirelessly transmitting the\nmouse-movement reports over Bluetooth Low Energy (BLE) protocol to the host\ncomputer; but for exploration of algorithms, the raw data can also be processed\non the host. The use of standard Human-Interface Device (HID) profile enables\nplug-and-play of the proposed mouse device on modern computers without\nrequiring separate driver installation. It can be used in two different modes\nto move the cursor, the joystick mode and the direct mapped mode. Experimental\nresults show that this head-controlled mouse to be intuitive and effective in\noperating the mouse cursor with fine-grained control of the cursor even by\nuntrained users.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 06:06:57 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["HeydariGorji", "Ali", ""], ["Safavi", "Seyede Mahya", ""], ["Lee", "Cheng-Ting", ""], ["Chou", "Pai H.", ""]]}, {"id": "2006.13633", "submitter": "Charith Perera", "authors": "Bayan Al Muhander, Jason Wiese, Omer Rana, Charith Perera", "title": "Privacy-Aware Internet of Things Notices in Shared Spaces: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The balance between protecting users' privacy while providing cost-effective\ndevices that are functional and usable is a key challenge in the burgeoning\nInternet of Things (IoT) industry. While in traditional desktop and mobile\ncontexts the primary user interface is a screen, in IoT screens are rare or\nvery small, which invalidate most of the traditional approaches. We examine how\nend-users interact with IoT products and how those products convey information\nback to the users, particularly `what is going on' with regards to their data.\nWe focus on understanding what the breadth of IoT, privacy, and ubiquitous\ncomputing literature tells us about how individuals with average technical\nexpertise can be notified about the privacy-related information of the spaces\nthey inhabit in an easily understandable way. In this survey, we present a\nreview of the various methods available to notify the end-users while taking\ninto consideration the factors that should be involved in the notification\nalerts within the physical domain. We identify five main factors: (1) data\ntype, (2) data usage, (3) data storage, (4) data retention period, and (5)\nnotification method. The survey also includes literature discussing\nindividuals' reactions and their potentials to provide feedback about their\nprivacy choices as a response to the received notification. The results of this\nsurvey highlight the most effective mechanisms for providing awareness of\nprivacy and data-use-practices in the context of IoT in shared spaces.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 11:15:00 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 08:42:53 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Muhander", "Bayan Al", ""], ["Wiese", "Jason", ""], ["Rana", "Omer", ""], ["Perera", "Charith", ""]]}, {"id": "2006.13660", "submitter": "Aleksey Fedoseev", "authors": "Aleksey Fedoseev, Akerke Tleugazy, Luiza Labazanova and Dzmitry\n  Tsetserukou", "title": "TeslaMirror: Multistimulus Encounter-Type Haptic Display for Shape and\n  Texture Rendering in VR", "comments": "Accepted to the ACM SIGGRAPH 2020 conference (Emerging Technologies\n  section), ACM copyright, 2 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.1145/3388534.3407300", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel concept of a hybrid tactile display with\nmultistimulus feedback, allowing the real-time experience of the position,\nshape, and texture of the virtual object. The key technology of the TeslaMirror\nis that we can deliver the sensation of object parameters (pressure, vibration,\nand electrotactile feedback) without any wearable haptic devices. We developed\nthe full digital twin of the 6 DOF UR robot in the virtual reality (VR)\nenvironment, allowing the adaptive surface simulation and control of the hybrid\ndisplay in real-time. The preliminary user study was conducted to evaluate the\nability of TeslaMirror to reproduce shape sensations with the under-actuated\nend-effector. The results revealed that potentially this approach can be used\nin the virtual systems for rendering versatile VR shapes with high fidelity\nhaptic experience.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 14:33:17 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 15:51:49 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fedoseev", "Aleksey", ""], ["Tleugazy", "Akerke", ""], ["Labazanova", "Luiza", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "2006.13763", "submitter": "Ahmad Beirami", "authors": "Sofia M Nikolakaki and Ogheneovo Dibie and Ahmad Beirami and Nicholas\n  Peterson and Navid Aghdaie and Kazi Zaman", "title": "Competitive Balance in Team Sports Games", "comments": "2020 IEEE Conference in Games (COG 2020), 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competition is a primary driver of player satisfaction and engagement in\nmultiplayer online games. Traditional matchmaking systems aim at creating\nmatches involving teams of similar aggregated individual skill levels, such as\nElo score or TrueSkill. However, team dynamics cannot be solely captured using\nsuch linear predictors. Recently, it has been shown that nonlinear predictors\nthat target to learn probability of winning as a function of player and team\nfeatures significantly outperforms these linear skill-based methods. In this\npaper, we show that using final score difference provides yet a better\nprediction metric for competitive balance. We also show that a linear model\ntrained on a carefully selected set of team and individual features achieves\nalmost the performance of the more powerful neural network model while offering\ntwo orders of magnitude inference speed improvement. This shows significant\npromise for implementation in online matchmaking systems.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:19:07 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Nikolakaki", "Sofia M", ""], ["Dibie", "Ogheneovo", ""], ["Beirami", "Ahmad", ""], ["Peterson", "Nicholas", ""], ["Aghdaie", "Navid", ""], ["Zaman", "Kazi", ""]]}, {"id": "2006.13796", "submitter": "Michael Hind", "authors": "John Richards, David Piorkowski, Michael Hind, Stephanie Houde,\n  Aleksandra Mojsilovi\\'c", "title": "A Methodology for Creating AI FactSheets", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As AI models and services are used in a growing number of highstakes areas, a\nconsensus is forming around the need for a clearer record of how these models\nand services are developed to increase trust. Several proposals for higher\nquality and more consistent AI documentation have emerged to address ethical\nand legal concerns and general social impacts of such systems. However, there\nis little published work on how to create this documentation. This is the first\nwork to describe a methodology for creating the form of AI documentation we\ncall FactSheets. We have used this methodology to create useful FactSheets for\nnearly two dozen models. This paper describes this methodology and shares the\ninsights we have gathered. Within each step of the methodology, we describe the\nissues to consider and the questions to explore with the relevant people in an\norganization who will be creating and consuming the AI facts in a FactSheet.\nThis methodology will accelerate the broader adoption of transparent AI\ndocumentation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:08:59 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 01:47:46 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Richards", "John", ""], ["Piorkowski", "David", ""], ["Hind", "Michael", ""], ["Houde", "Stephanie", ""], ["Mojsilovi\u0107", "Aleksandra", ""]]}, {"id": "2006.13883", "submitter": "Ekaterina Svikhnushina", "authors": "Ekaterina Svikhnushina, Pearl Pu", "title": "Social and Emotional Etiquette of Chatbots: A Qualitative Approach to\n  Understanding User Needs and Expectations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As chatbots are becoming increasingly popular, we often wonder what users\nperceive as natural and socially accepted manners of interacting with them.\nSome researchers maintain that humans should avoid engaging in emotional\nconversations with chatbots, while others have started building empathetic\nchatting machines using the latest deep learning techniques. To understand if\nchatbots should comprehend and display emotions, we conducted semi-structured\ninterviews with 18 participants. Our analysis revealed their overall enthusiasm\ntowards emotionally aware agents. More importantly, users' intention to accept\nemotional chatbots seem to hinge on how these agents respond to our specific\nemotions, rather than just the ability to detect human emotions. Our findings\nalso disclosed the specific application domains where emotionally intelligent\ntechnology could improve user experience. To conclude, we summarized a set of\nemotion interaction patterns that inspire users' intention to adopt such\ntechnology as well as guidelines useful for the development of emotionally\nintelligent chatbots.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:08:20 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 11:50:17 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Svikhnushina", "Ekaterina", ""], ["Pu", "Pearl", ""]]}, {"id": "2006.13898", "submitter": "Yefim Shulman", "authors": "Yefim Shulman, Thao Ngo, Joachim Meyer", "title": "Order of Control and Perceived Control over Personal Information", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-42504-3_23", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Focusing on personal information disclosure, we apply control theory and the\nnotion of the Order of Control to study people's understanding of the\nimplications of information disclosure and their tendency to consent to\ndisclosure. We analyzed the relevant literature and conducted a preliminary\nonline study (N = 220) to explore the relationship between the Order of Control\nand perceived control over personal information. Our analysis of existing\nresearch suggests that the notion of the Order of Control can help us\nunderstand people's decisions regarding the control over their personal\ninformation. We discuss limitations and future directions for research\nregarding the application of the idea of the Order of Control to online\nprivacy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:34:12 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Shulman", "Yefim", ""], ["Ngo", "Thao", ""], ["Meyer", "Joachim", ""]]}, {"id": "2006.13985", "submitter": "Marija Slavkovik", "authors": "Than Htut Soe, Oda Elise Nordberg, Frode Guribye, and Marija Slavkovik", "title": "Circumvention by design -- dark patterns in cookie consents for online\n  news outlets", "comments": "Accepted for publication at NordiCHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure that users of online services understand what data are collected\nand how they are used in algorithmic decision-making, the European Union's\nGeneral Data Protection Regulation (GDPR) specifies informed consent as a\nminimal requirement. For online news outlets consent is commonly elicited\nthrough interface design elements in the form of a pop-up. We have manually\nanalyzed 300 data collection consent notices from news outlets that are built\nto ensure compliance with GDPR. The analysis uncovered a variety of strategies\nor dark patterns that circumvent the intent of GDPR by design. We further study\nthe presence and variety of these dark patterns in these \"cookie consents\" and\nuse our observations to specify the concept of dark pattern in the context of\nconsent elicitation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 18:35:31 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Soe", "Than Htut", ""], ["Nordberg", "Oda Elise", ""], ["Guribye", "Frode", ""], ["Slavkovik", "Marija", ""]]}, {"id": "2006.14054", "submitter": "Roman Samarev", "authors": "Alberto Mastrotto (1), Anderson Nelson (1), Dev Sharma (1), Ergeta\n  Muca (1), Kristina Liapchin (1), Luis Losada (1), Mayur Bansal (1), Roman S.\n  Samarev (2 and 3) ((1) Columbia University, 116th St and Broadway, New York,\n  NY 10027, USA, (2) dotin Inc, Francisco Ln. 194, 94539, Fremont CA, USA, (3)\n  Bauman Moscow State Technical University, ul. Baumanskaya 2-ya, 5/1, 105005,\n  Moscow, Russia)", "title": "Validating psychometric survey responses", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to classify user validity in survey responses by using\na machine learning techniques. The approach is based on collecting user mouse\nactivity on web-surveys and fast predicting validity of the survey in general\nwithout analysis of specific answers. Rule based approach, LSTM and HMM models\nare considered. The approach might be used in web-survey applications to detect\nsuspicious users behaviour and request from them proper answering instead of\nfalse data recording.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:33:10 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Mastrotto", "Alberto", "", "2 and 3"], ["Nelson", "Anderson", "", "2 and 3"], ["Sharma", "Dev", "", "2 and 3"], ["Muca", "Ergeta", "", "2 and 3"], ["Liapchin", "Kristina", "", "2 and 3"], ["Losada", "Luis", "", "2 and 3"], ["Bansal", "Mayur", "", "2 and 3"], ["Samarev", "Roman S.", "", "2 and 3"]]}, {"id": "2006.14120", "submitter": "Yalong Yang", "authors": "Yalong Yang, Tim Dwyer, Kim Marriott, Bernhard Jenny and Sarah Goodwin", "title": "Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and\n  Bar Chart in Immersive Environments", "comments": "IEEE Transactions on Visualization and Computer Graphics (TVCG), to\n  appear", "journal-ref": null, "doi": "10.1109/TVCG.2020.3004137", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Tilt Map, a novel interaction technique for intuitively\ntransitioning between 2D and 3D map visualisations in immersive environments.\nOur focus is visualising data associated with areal features on maps, for\nexample, population density by state. Tilt Map transitions from 2D choropleth\nmaps to 3D prism maps to 2D bar charts to overcome the limitations of each. Our\npaper includes two user studies. The first study compares subjects' task\nperformance interpreting population density data using 2D choropleth maps and\n3D prism maps in virtual reality (VR). We observed greater task accuracy with\nprism maps, but faster response times with choropleth maps. The complementarity\nof these views inspired our hybrid Tilt Map design. Our second study compares\nTilt Map to: a side-by-side arrangement of the various views; and interactive\ntoggling between views. The results indicate benefits for Tilt Map in user\npreference; and accuracy (versus side-by-side) and time (versus toggle).\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 00:52:57 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Marriott", "Kim", ""], ["Jenny", "Bernhard", ""], ["Goodwin", "Sarah", ""]]}, {"id": "2006.14245", "submitter": "Noe Elisa Nnko", "authors": "Noe Elisa", "title": "Usability, Accessibility and Web Security Assessment of E-government\n  Websites in Tanzania", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of the fact that e-government agency (ega) in Tanzania emphasize on\nthe use of ICT within public institutions in Tanzania, accessibility, usability\nand web security vulnerabilities are still not considered by the majority of\nweb developers. The main objective of this study is to assess the usability,\naccessibility and web security vulnerabilities of selected Tanzania\ne-government websites. Using several automatic diagnostic (evaluation) tools\nsuch as pingdom, google speed insight, wave, w3c checker and acunetix, this\nstudy assess the usability, accessibility and web security vulnerabilities of\n79 selected e-government websites in Tanzania. The results reveal several\nissues on usability, accessibility and security of Tanzania e-government\nwebsites. There is high number of usability problems where 100% of websites\nwere found to have broken links and 52 out of 79 websites have loading time of\nmore than five (5) seconds for their main page. The accessibility results show\nthat all 79 selected websites have accessibility errors and violate w3c Web\nContent Accessibility Guidelines (WCAG) 1.0. The results on web security\nvulnerabilities indicate that 40 out of 79 (50.6%) assessed websites have one\nor more high-severity vulnerability (SQL injection or cross site scripting-XSS)\nwhile 51 out of 79 (64.5%) have one or more medium-severity vulnerabilities\n(Cross site request forgery or Denial of Service). Based on these results, this\nstudy provides some recommendations for improving the usability, accessibility\nand web security vulnerabilities of public institutions in Tanzania.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 08:23:45 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Elisa", "Noe", ""]]}, {"id": "2006.14279", "submitter": "Erion \\c{C}ano", "authors": "Erion \\c{C}ano, Riccardo Coppola, Eleonora Gargiulo, Marco Marengo,\n  Maurizio Morisio", "title": "Mood-based On-Car Music Recommendations", "comments": "11 pages, 5 figures. Published in proceedings of INISCOM 2016, the\n  2nd International Conference on Industrial Networks and Intelligent Systems,\n  Leicester, UK", "journal-ref": null, "doi": "10.1007/978-3-319-52569-3_14", "report-no": null, "categories": "cs.HC cs.IR cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving and music listening are two inseparable everyday activities for\nmillions of people today in the world. Considering the high correlation between\nmusic, mood and driving comfort and safety, it makes sense to use appropriate\nand intelligent music recommendations based on the mood of drivers and songs in\nthe context of car driving. The objective of this paper is to present the\nproject of a contextual mood-based music recommender system capable of\nregulating the driver's mood and trying to have a positive influence on her\ndriving behaviour. Here we present the proof of concept of the system and\ndescribe the techniques and technologies that are part of it. Further possible\nfuture improvements on each of the building blocks are also presented.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 09:50:26 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["\u00c7ano", "Erion", ""], ["Coppola", "Riccardo", ""], ["Gargiulo", "Eleonora", ""], ["Marengo", "Marco", ""], ["Morisio", "Maurizio", ""]]}, {"id": "2006.14291", "submitter": "Zhuochen Jin", "authors": "Yi Guo, Shunan Guo, Zhuochen Jin, Smiti Kaul, David Gotz, Nan Cao", "title": "Survey on Visual Analysis of Event Sequence Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event sequence data record series of discrete events in the time order of\noccurrence. They are commonly observed in a variety of applications ranging\nfrom electronic health records to network logs, with the characteristics of\nlarge-scale, high-dimensional, and heterogeneous. This high complexity of event\nsequence data makes it difficult for analysts to manually explore and find\npatterns, resulting in ever-increasing needs for computational and perceptual\naids from visual analytics techniques to extract and communicate insights from\nevent sequence datasets. In this paper, we review the state-of-the-art visual\nanalytics approaches, characterize them with our proposed design space, and\ncategorize them based on analytical tasks and applications. From our review of\nrelevant literature, we have also identified several remaining research\nchallenges and future research opportunities.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 10:22:11 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Guo", "Yi", ""], ["Guo", "Shunan", ""], ["Jin", "Zhuochen", ""], ["Kaul", "Smiti", ""], ["Gotz", "David", ""], ["Cao", "Nan", ""]]}, {"id": "2006.14654", "submitter": "Ahmed Arif", "authors": "Di \"Chelsea\" Sun, Vaishnavi Melkote, Ahmed Sabbir Arif", "title": "Exploratory Study of Young Children's Social Media Needs and\n  Requirements", "comments": null, "journal-ref": "Extended Abstracts of the 19th ACM International Conference on\n  Interaction Design and Children (IDC 2020)", "doi": "10.1145/3397617.3397836", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As social media are becoming increasingly popular among young children, it is\nimportant to explore this population's needs and requirements from these\nplatforms. As a first step to this, we conducted an exploratory design workshop\nwith children aged between ten and eleven years to find out about their social\nmedia needs and requirements. Through an analysis of the paper prototypes\nsolicited from the workshop, here we discuss the social media features that are\nthe most desired by this population.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 18:31:51 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Sun", "Di \"Chelsea\"", ""], ["Melkote", "Vaishnavi", ""], ["Arif", "Ahmed Sabbir", ""]]}, {"id": "2006.14666", "submitter": "Pranav Sharma", "authors": "Pranav Sharma", "title": "LPar -- A Distributed Multi Agent platform for building Polyglot, Omni\n  Channel and Industrial grade Natural Language Interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of serving and delighting customers in a personal and near human\nlike manner is very high on automation agendas of most Enterprises. Last few\nyears, have seen huge progress in Natural Language Processing domain which has\nled to deployments of conversational agents in many enterprises. Most of the\ncurrent industrial deployments tend to use Monolithic Single Agent designs that\nmodel the entire knowledge and skill of the Domain. While this approach is one\nof the fastest to market, the monolithic design makes it very hard to scale\nbeyond a point. There are also challenges in seamlessly leveraging many tools\noffered by sub fields of Natural Language Processing and Information Retrieval\nin a single solution. The sub fields that can be leveraged to provide relevant\ninformation are, Question and Answer system, Abstractive Summarization,\nSemantic Search, Knowledge Graph etc. Current deployments also tend to be very\ndependent on the underlying Conversational AI platform (open source or\ncommercial) , which is a challenge as this is a fast evolving space and no one\nplatform can be considered future proof even in medium term of 3-4 years.\nLately,there is also work done to build multi agent solutions that tend to\nleverage a concept of master agent. While this has shown promise, this approach\nstill makes the master agent in itself difficult to scale. To address these\nchallenges, we introduce LPar, a distributed multi agent platform for large\nscale industrial deployment of polyglot, diverse and inter-operable agents. The\nasynchronous design of LPar supports dynamically expandable domain. We also\nintroduce multiple strategies available in the LPar system to elect the most\nsuitable agent to service a customer query.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 19:20:07 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Sharma", "Pranav", ""]]}, {"id": "2006.14779", "submitter": "Gagan Bansal", "authors": "Gagan Bansal, Tongshuang Wu, Joyce Zhou, Raymond Fok, Besmira Nushi,\n  Ece Kamar, Marco Tulio Ribeiro, Daniel S. Weld", "title": "Does the Whole Exceed its Parts? The Effect of AI Explanations on\n  Complementary Team Performance", "comments": "CHI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many researchers motivate explainable AI with studies showing that human-AI\nteam performance on decision-making tasks improves when the AI explains its\nrecommendations. However, prior studies observed improvements from explanations\nonly when the AI, alone, outperformed both the human and the best team. Can\nexplanations help lead to complementary performance, where team accuracy is\nhigher than either the human or the AI working solo? We conduct mixed-method\nuser studies on three datasets, where an AI with accuracy comparable to humans\nhelps participants solve a task (explaining itself in some conditions). While\nwe observed complementary improvements from AI augmentation, they were not\nincreased by explanations. Rather, explanations increased the chance that\nhumans will accept the AI's recommendation, regardless of its correctness. Our\nresult poses new challenges for human-centered AI: Can we develop explanatory\napproaches that encourage appropriate trust in AI, and therefore help generate\n(or improve) complementary performance?\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 03:34:04 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 21:23:55 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2021 22:50:34 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Bansal", "Gagan", ""], ["Wu", "Tongshuang", ""], ["Zhou", "Joyce", ""], ["Fok", "Raymond", ""], ["Nushi", "Besmira", ""], ["Kamar", "Ece", ""], ["Ribeiro", "Marco Tulio", ""], ["Weld", "Daniel S.", ""]]}, {"id": "2006.14782", "submitter": "Gurpriya Kaur Bhatia", "authors": "Gurpriya Kaur Bhatia and Shubham Gupta and Alpana Dubey and\n  Ponnurangam Kumaraguru", "title": "WorkerRep: Immutable Reputation System For Crowdsourcing Platform Based\n  on Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowdsourcing is a process wherein an individual or an organisation utilizes\nthe talent pool present over the Internet to accomplish their task. The\nexisting crowdsourcing platforms and their reputation computation are\ncentralised and hence prone to various attacks or malicious manipulation of the\ndata by the central entity. A few distributed crowdsourcing platforms have been\nproposed but they lack a robust reputation mechanism. So we propose a\ndecentralised crowdsourcing platform having an immutable reputation mechanism\nto tackle these problems. It is built on top of Ethereum network and does not\nrequire the user to trust a third party for a non malicious experience. It also\nutilizes IOTAs consensus mechanism which reduces the cost for task evaluation\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 03:37:41 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Bhatia", "Gurpriya Kaur", ""], ["Gupta", "Shubham", ""], ["Dubey", "Alpana", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2006.14882", "submitter": "Fan Zuo", "authors": "Fan Zuo, Jingxing Wang, Jingqin Gao, Kaan Ozbay, Xuegang Jeff Ban,\n  Yubin Shen, Hong Yang, Shri Iyer", "title": "An Interactive Data Visualization and Analytics Tool to Evaluate\n  Mobility and Sociability Trends During COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 outbreak has dramatically changed travel behavior in affected\ncities. The C2SMART research team has been investigating the impact of COVID-19\non mobility and sociability. New York City (NYC) and Seattle, two of the cities\nmost affected by COVID-19 in the U.S. were included in our initial study. An\nall-in-one dashboard with data mining and cloud computing capabilities was\ndeveloped for interactive data analytics and visualization to facilitate the\nunderstanding of the impact of the outbreak and corresponding policies such as\nsocial distancing on transportation systems. This platform is updated regularly\nand continues to evolve with the addition of new data, impact metrics, and\nvisualizations to assist public and decision-makers to make informed decisions.\nThis paper presents the architecture of the COVID related mobility data\ndashboard and preliminary mobility and sociability metrics for NYC and Seattle.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 09:27:53 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Zuo", "Fan", ""], ["Wang", "Jingxing", ""], ["Gao", "Jingqin", ""], ["Ozbay", "Kaan", ""], ["Ban", "Xuegang Jeff", ""], ["Shen", "Yubin", ""], ["Yang", "Hong", ""], ["Iyer", "Shri", ""]]}, {"id": "2006.15285", "submitter": "Yunlong Wang", "authors": "Yunlong Wang and Harald Reiterer", "title": "Promoting the Research of Health Behavior Change in Chinese HCI\n  Community", "comments": "CHI'19 Workshop: HCI in China: Research Agenda, Education Curriculum,\n  Industry Partnership, and Communities Building. Glasgow, United Kingdom. May\n  4, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unhealthy lifestyles largely contribute to many chronic diseases, which makes\nthe research on health behavior change crucial for both individuals and the\nwhole society. As an interdisciplinary research field, health behavior change\nresearch in the HCI community is still in the early stage. This research field\nis notably less developed in Chinese HCI community. In this position paper, we\nwill first illustrate the research of health behavior change in the HCI\ncommunity based on our previous systematic review. According to the unique\nproperties of Chinese society, we will then discuss both the potential\nadvantages and challenges of conducting health behavior change research in\nChina. Lastly, we will briefly introduce the SMARTACT project in Germany to\nprovide a reference for future related research. This paper aims to draw more\nattention to this research field and promote its development in China.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 04:58:24 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Yunlong", ""], ["Reiterer", "Harald", ""]]}, {"id": "2006.15292", "submitter": "Alex Mariakakis", "authors": "Alex Mariakakis, Sifang Chen, Bichlien Nguyen, Kirsten Bray, Molly\n  Blank, Jonathan Lester, Lauren Ryan, Paul Johns, Gonzalo Ramos, Asta Roseway", "title": "Project Calico: Wearable Chemical Sensors for Environmental Monitoring", "comments": "9 pages, 6 figures 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Environmental hazards often go unnoticed because they are invisible to the\nnaked eye, posing risks to our health over time. Project Calico aims to raise\nawareness of these risks by augmenting everyday fashion with color-changing\nchemical sensors that can be observed at a glance or captured by a smartphone\ncamera. Project Calico leverages existing cosmetic and fabrication processes to\ndemocratize environmental sensing, enabling creators to make their own\naccessories. We present two fashionable instantiations of Project Calico\ninvolving UV irradiation. EcoHair, created by hair treatment, is UV-sensitive\nhair that intensifies in color saturation depending on the UV intensity.\nEcoPatches, created by inkjet printing, can be worn as temporary tattoos that\nchange their color to reflect cumulative UV exposure over time. We present\nfindings from two focus groups regarding the Project Calico vision and gathered\ninsights from their overall impressions and projected use patterns.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 06:22:18 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 19:28:31 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Mariakakis", "Alex", ""], ["Chen", "Sifang", ""], ["Nguyen", "Bichlien", ""], ["Bray", "Kirsten", ""], ["Blank", "Molly", ""], ["Lester", "Jonathan", ""], ["Ryan", "Lauren", ""], ["Johns", "Paul", ""], ["Ramos", "Gonzalo", ""], ["Roseway", "Asta", ""]]}, {"id": "2006.15432", "submitter": "Thiago Porcino", "authors": "Thiago Porcino, Esteban Clua, Daniela Trevisan, \\'Erick Rodrigues,\n  Alexandre Silva", "title": "Automatic Recommendation of Strategies for Minimizing Discomfort in\n  Virtual Environments", "comments": "Accepted at the IEEE 8th International Conference on Serious Games\n  and Applications for Health (SeGAH) - SeGAH 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual reality (VR) is an imminent trend in games, education, entertainment,\nmilitary, and health applications, as the use of head-mounted displays is\nbecoming accessible to the mass market. Virtual reality provides immersive\nexperiences but still does not offer an entirely perfect situation, mainly due\nto Cybersickness (CS) issues. In this work, we first present a detailed review\nabout possible causes of CS. Following, we propose a novel CS prediction\nsolution. Our system is able to suggest if the user may be entering in the next\nmoments of the application into an illness situation. We use Random Forest\nclassifiers, based on a dataset we have produced. The CSPQ (Cybersickness\nProfile Questionnaire) is also proposed, which is used to identify the player's\nsusceptibility to CS and the dataset construction. In addition, we designed two\nimmersive environments for empirical studies where participants are asked to\ncomplete the questionnaire and describe (orally) the degree of discomfort\nduring their gaming experience. Our data was achieved through 84 individuals on\ndifferent days, using VR devices. Our proposal also allows us to identify which\nare the most frequent attributes (causes) in the observed discomfort\nsituations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 19:28:48 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Porcino", "Thiago", ""], ["Clua", "Esteban", ""], ["Trevisan", "Daniela", ""], ["Rodrigues", "\u00c9rick", ""], ["Silva", "Alexandre", ""]]}, {"id": "2006.15449", "submitter": "Kovila  P.L. Coopamootoo", "authors": "Magdalene Ng, Kovila P.L. Coopamootoo, Ehsan Toreini, Mhairi Aitken,\n  Karen Elliot, Aad van Moorsel", "title": "Simulating the Effects of Social Presence on Trust, Privacy Concerns &\n  Usage Intentions in Automated Bots for Finance", "comments": "In Publication for 5th IEEE European Symposium on Security & Privacy\n  Workshops (EuroSPW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FinBots are chatbots built on automated decision technology, aimed to\nfacilitate accessible banking and to support customers in making financial\ndecisions. Chatbots are increasing in prevalence, sometimes even equipped to\nmimic human social rules, expectations and norms, decreasing the necessity for\nhuman-to-human interaction. As banks and financial advisory platforms move\ntowards creating bots that enhance the current state of consumer trust and\nadoption rates, we investigated the effects of chatbot vignettes with and\nwithout socio-emotional features on intention to use the chatbot for financial\nsupport purposes. We conducted a between-subject online experiment with N = 410\nparticipants. Participants in the control group were provided with a vignette\ndescribing a secure and reliable chatbot called XRO23, whereas participants in\nthe experimental group were presented with a vignette describing a secure and\nreliable chatbot that is more human-like and named Emma. We found that Vignette\nEmma did not increase participants' trust levels nor lowered their privacy\nconcerns even though it increased perception of social presence. However, we\nfound that intention to use the presented chatbot for financial support was\npositively influenced by perceived humanness and trust in the bot. Participants\nwere also more willing to share financially-sensitive information such as\naccount number, sort code and payments information to XRO23 compared to Emma -\nrevealing a preference for a technical and mechanical FinBot in information\nsharing. Overall, this research contributes to our understanding of the\nintention to use chatbots with different features as financial technology, in\nparticular that socio-emotional support may not be favoured when designed\nindependently of financial function.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 21:31:53 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 16:15:45 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Ng", "Magdalene", ""], ["Coopamootoo", "Kovila P. L.", ""], ["Toreini", "Ehsan", ""], ["Aitken", "Mhairi", ""], ["Elliot", "Karen", ""], ["van Moorsel", "Aad", ""]]}, {"id": "2006.15545", "submitter": "Hee-Seung Moon", "authors": "Hee-Seung Moon and Jiwon Seo", "title": "Dynamic Difficulty Adjustment via Fast User Adaptation", "comments": "Submitted to ACM UIST 2020 (Poster)", "journal-ref": "UIST '20 Adjunct: Adjunct Publication of the 33rd Annual ACM\n  Symposium on User Interface Software and Technology, 2020, pp. 13-15", "doi": "10.1145/3379350.3418578", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic difficulty adjustment (DDA) is a technology that adapts a game's\nchallenge to match the player's skill. It is a key element in game development\nthat provides continuous motivation and immersion to the player. However,\nconventional DDA methods require tuning in-game parameters to generate the\nlevels for various players. Recent DDA approaches based on deep learning can\nshorten the time-consuming tuning process, but require sufficient user demo\ndata for adaptation. In this paper, we present a fast user adaptation method\nthat can adjust the difficulty of the game for various players using only a\nsmall amount of demo data by applying a meta-learning algorithm. In the video\ngame environment user test (n=9), our proposed DDA method outperformed a\ntypical deep learning-based baseline method.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 08:53:46 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Moon", "Hee-Seung", ""], ["Seo", "Jiwon", ""]]}, {"id": "2006.15647", "submitter": "Chayan Sarkar", "authors": "Hrishav Bakul Barua, Chayan Sarkar, Achanna Anil Kumar, Arpan Pal,\n  Balamuralidhar P", "title": "I can attend a meeting too! Towards a human-like telepresence avatar\n  robot to attend meeting on your behalf", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telepresence robots are used in various forms in various use-cases that helps\nto avoid physical human presence at the scene of action. In this work, we focus\non a telepresence robot that can be used to attend a meeting remotely with a\ngroup of people. Unlike a one-to-one meeting, participants in a group meeting\ncan be located at a different part of the room, especially in an informal\nsetup. As a result, all of them may not be at the viewing angle of the robot,\na.k.a. the remote participant. In such a case, to provide a better meeting\nexperience, the robot should localize the speaker and bring the speaker at the\ncenter of the viewing angle. Though sound source localization can easily be\ndone using a microphone-array, bringing the speaker or set of speakers at the\nviewing angle is not a trivial task. First of all, the robot should react only\nto a human voice, but not to the random noises. Secondly, if there are multiple\nspeakers, to whom the robot should face or should it rotate continuously with\nevery new speaker? Lastly, most robotic platforms are resource-constrained and\nto achieve a real-time response, i.e., avoiding network delay, all the\nalgorithms should be implemented within the robot itself. This article presents\na study and implementation of an attention shifting scheme in a telepresence\nmeeting scenario which best suits the needs and expectations of the collocated\nand remote attendees. We define a policy to decide when a robot should rotate\nand how much based on real-time speaker localization. Using user satisfaction\nstudy, we show the efficacy and usability of our system in the meeting\nscenario. Moreover, our system can be easily adapted to other scenarios where\nmultiple people are located.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 16:43:04 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Barua", "Hrishav Bakul", ""], ["Sarkar", "Chayan", ""], ["Kumar", "Achanna Anil", ""], ["Pal", "Arpan", ""], ["P", "Balamuralidhar", ""]]}, {"id": "2006.15768", "submitter": "Alexandros Papangelis", "authors": "Alexandros Papangelis and Stefan Ultes", "title": "Towards meaningful, grounded conversations with intelligent agents", "comments": "Published at RoboDial at SIGDIAL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As conversational agents become integral parts of many aspects of our lives,\ncurrent approaches are reaching bottlenecks of performance that require\nincreasing amounts of data or increasingly powerful models. It is also becoming\nclear that such agents are here to stay and accompany us for long periods of\ntime. If we are, therefore, to design agents that can deeply understand our\nworld and evolve with it, we need to take a step back and revisit the\ntrade-offs we have made in the current state of the art models. This paper\nargues that a) we need to shift from slot filling into a more realistic\nconversation paradigm; and b) that, to realize that paradigm, we need models\nthat are able to handle concrete and abstract entities as well as evolving\nrelations between them.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 01:18:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Papangelis", "Alexandros", ""], ["Ultes", "Stefan", ""]]}, {"id": "2006.15948", "submitter": "Hendry F Chame", "authors": "Hendry F. Chame, Ahmadreza Ahmadi, Jun Tani", "title": "Towards hybrid primary intersubjectivity: a neural robotics library for\n  human science", "comments": null, "journal-ref": "Frontiers in psychology, 11 (2020)", "doi": "10.3389/fpsyg.2020.584869", "report-no": null, "categories": "cs.RO cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot interaction is becoming an interesting area of research in\ncognitive science, notably, for the study of social cognition. Interaction\ntheorists consider primary intersubjectivity a non-mentalist, pre-theoretical,\nnon-conceptual sort of processes that ground a certain level of communication\nand understanding, and provide support to higher-level cognitive skills. We\nargue this sort of low level cognitive interaction, where control is shared in\ndyadic encounters, is susceptible of study with neural robots. Hence, in this\nwork we pursue three main objectives. Firstly, from the concept of active\ninference we study primary intersubjectivity as a second person perspective\nexperience characterized by predictive engagement, where perception, cognition,\nand action are accounted for an hermeneutic circle in dyadic interaction.\nSecondly, we propose an open-source methodology named \\textit{neural robotics\nlibrary} (NRL) for experimental human-robot interaction, and a demonstration\nprogram for interacting in real-time with a virtual Cartesian robot (VCBot).\nLastly, through a study case, we discuss some ways human-robot (hybrid)\nintersubjectivity can contribute to human science research, such as to the\nfields of developmental psychology, educational technology, and cognitive\nrehabilitation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 11:35:46 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Chame", "Hendry F.", ""], ["Ahmadi", "Ahmadreza", ""], ["Tani", "Jun", ""]]}, {"id": "2006.15955", "submitter": "Jean-Benoit Delbrouck", "authors": "Jean-Benoit Delbrouck and No\\'e Tits and Mathilde Brousmiche and\n  St\\'ephane Dupont", "title": "A Transformer-based joint-encoding for Emotion Recognition and Sentiment\n  Analysis", "comments": "Winner of the ACL20: Second Grand-Challenge on Multimodal Language", "journal-ref": null, "doi": "10.18653/v1/2020.challengehml-1.1", "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding expressed sentiment and emotions are two crucial factors in\nhuman multimodal language. This paper describes a Transformer-based\njoint-encoding (TBJE) for the task of Emotion Recognition and Sentiment\nAnalysis. In addition to use the Transformer architecture, our approach relies\non a modular co-attention and a glimpse layer to jointly encode one or more\nmodalities. The proposed solution has also been submitted to the ACL20: Second\nGrand-Challenge on Multimodal Language to be evaluated on the CMU-MOSEI\ndataset. The code to replicate the presented experiments is open-source:\nhttps://github.com/jbdel/MOSEI_UMONS.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 11:51:46 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Delbrouck", "Jean-Benoit", ""], ["Tits", "No\u00e9", ""], ["Brousmiche", "Mathilde", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "2006.16077", "submitter": "Miguel Ribeiro", "authors": "Bruno Cardoso, Miguel Ribeiro, Catia Prandi, Nuno Nunes", "title": "Gamification and engagement of tourists and residents in public\n  transportation exploiting location-based technologies", "comments": null, "journal-ref": "Proceedings of TRA2020, the 8th Transport Research Arena:\n  Rethinking transport towards clean and inclusive mobility", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cities are becoming very congested. There is a need to reduce the number of\nprivate cars on the roads, by maximising the potential for local public\ntransport. With the increasing awareness of transport that is sustainable in\nthe sense of environmental impact, but also climate and social, there is the\nneed to create engagement into public transportation. Gamification, which is\nthe use of game elements in non-game contexts, has proven to deliver very\npositive results, by turning regular activities into engaging ones, which are\nfun to perform. We have designed a mobile application, that interacts with\nshort-range wireless communication technologies, inviting people to use public\ntransport. To evaluate the solution, we have created a questionnaire based on\nthe System Usability Scale, but also using usability testing with specific\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 14:31:57 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 11:02:37 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Cardoso", "Bruno", ""], ["Ribeiro", "Miguel", ""], ["Prandi", "Catia", ""], ["Nunes", "Nuno", ""]]}, {"id": "2006.16353", "submitter": "Kumar Akash", "authors": "Kumar Akash, Griffon McMahon, Tahira Reid, Neera Jain", "title": "Human Trust-based Feedback Control: Dynamically varying automation\n  transparency to optimize human-machine interactions", "comments": "21 pages", "journal-ref": null, "doi": "10.1109/MCS.2020.3019151", "report-no": null, "categories": "cs.HC cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human trust in automation plays an essential role in interactions between\nhumans and automation. While a lack of trust can lead to a human's disuse of\nautomation, over-trust can result in a human trusting a faulty autonomous\nsystem which could have negative consequences for the human. Therefore, human\ntrust should be calibrated to optimize human-machine interactions with respect\nto context-specific performance objectives. In this article, we present a\nprobabilistic framework to model and calibrate a human's trust and workload\ndynamics during his/her interaction with an intelligent decision-aid system.\nThis calibration is achieved by varying the automation's transparency---the\namount and utility of information provided to the human. The parameterization\nof the model is conducted using behavioral data collected through human-subject\nexperiments, and three feedback control policies are experimentally validated\nand compared against a non-adaptive decision-aid system. The results show that\nhuman-automation team performance can be optimized when the transparency is\ndynamically updated based on the proposed control policy. This framework is a\nfirst step toward widespread design and implementation of real-time adaptive\nautomation for use in human-machine interactions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 20:17:55 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Akash", "Kumar", ""], ["McMahon", "Griffon", ""], ["Reid", "Tahira", ""], ["Jain", "Neera", ""]]}, {"id": "2006.16508", "submitter": "Ruoyan Kong", "authors": "Ruoyan Kong, Haiyi Zhu and Joseph A. Konstan", "title": "Learning to Ignore: A Case Study of Organization-Wide Bulk Email\n  Effectiveness", "comments": "This is a pre-print version of a paper accepted to CSCW 2021, the\n  24th ACM Conference on Computer-Supported Cooperative Work and Social\n  Computing. 23 pages", "journal-ref": null, "doi": "10.1145/3449154", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bulk email is a primary communication channel within organizations, with\nall-company emails and regular newsletters serving as a mechanism for making\nemployees aware of policies and events. Ineffective communication could result\nin wasted employee time and a lack of compliance or awareness. Previous studies\non organizational emails focused mostly on recipients. However, organizational\nbulk email system is a multi-stakeholder problem including recipients,\ncommunicators, and the organization itself. We studied the effectiveness,\npractice, and assessments of the organizational bulk email system of a large\nuniversity from multi-stakeholders' perspectives. We conducted a qualitative\nstudy with the university's communicators, recipients, and managers. We delved\ninto the organizational bulk email's distributing mechanisms of the\ncommunicators, the reading behaviors of recipients, and the perspectives on\nemails' values of communicators, managers, and recipients. We found that the\norganizational bulk email system as a whole was strained, and communicators are\ncaught in the middle of this multi-stakeholder problem. First, though the\ncommunicators had an interest in preserving the effectiveness of channels in\nreaching employees, they had high-level clients whose interests might outweigh\njudgment about whether a message deserves widespread circulation. Second,\nthough communicators thought they were sending important information,\nrecipients viewed most of the organizational bulk emails as not relevant to\nthem. Third, this disagreement was amplified by the success metric used by\ncommunicators. They viewed their bulk emails as successful if they had a high\nopen rate. But recipients often opened and then rapidly discarded emails\nwithout reading the details. Last, while the communicators in general\nunderstood the challenge, they had a limited set of targeting and feedback\ntools to support their task.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 03:31:04 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2021 00:56:25 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Kong", "Ruoyan", ""], ["Zhu", "Haiyi", ""], ["Konstan", "Joseph A.", ""]]}, {"id": "2006.16572", "submitter": "Niek Beckers", "authors": "Timo Melman, Niek Beckers and David Abbink", "title": "Mitigating undesirable emergent behavior arising between driver and\n  semi-automated vehicle", "comments": "RSS 2020 Emergent behavior workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emergent behavior arising in a joint human-robot system cannot be fully\npredicted based on an understanding of the individual agents. Typically, robot\nbehavior is governed by algorithms that optimize a reward function that should\nquantitatively capture the joint system's goal. Although reward functions can\nbe updated to better match human needs, this is no guarantee that no\nmisalignment with the complex and variable human needs will occur. Algorithms\nmay learn undesirable behavior when interacting with the human and the\nintrinsically unpredictable human-inhabited world, thereby producing further\nmisalignment with human users or bystanders. As a result, humans might behave\ndifferently than anticipated, causing robots to learn differently and\nundesirable behavior to emerge. With this short paper, we state that to design\nfor Human-Robot Interaction that mitigates such undesirable emergent behavior,\nwe need to complement advancements in human-robot interaction algorithms with\nhuman factors knowledge and expertise. More specifically, we advocate a\nthree-pronged approach that we illustrate using a particularly challenging\nexample of safety-critical human-robot interaction: a driver interacting with a\nsemi-automated vehicle. Undesirable emergent behavior should be mitigated by a\ncombination of 1) including driver behavioral mechanisms in the vehicle's\nalgorithms and reward functions, 2) model-based approaches that account for\ninteraction-induced driver behavioral adaptations and 3) driver-centered\ninteraction design that promotes driver engagement with the semi-automated\nvehicle, and the transparent communication of each agent's actions that allows\nmutual support and adaptation. We provide examples from recent empirical work\nin our group, in the hope this proves to be fruitful for discussing emergent\nhuman-robot interaction.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 07:10:51 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2020 07:15:44 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 06:51:45 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Melman", "Timo", ""], ["Beckers", "Niek", ""], ["Abbink", "David", ""]]}, {"id": "2006.16614", "submitter": "Niek Beckers", "authors": "Niek Beckers, Edwin van Asseldonk and Herman van der Kooij", "title": "Haptic human-human interaction does not improve individual visuomotor\n  adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Haptic interaction between two humans, for example, a physiotherapist\nassisting a patient regaining the ability to grasp a cup, likely facilitates\nmotor skill acquisition. Haptic human-human interaction has been shown to\nenhance individual performance improvement in a tracking task with a visuomotor\nrotation perturbation. These results are remarkable given that haptically\nassisting or guiding an individual rarely benefits their individual improvement\nwhen the assistance is removed. We, therefore, replicated a study that reported\nthat haptic interaction between humans was beneficial for individual\nimprovement for tracking a target in a visuomotor rotation perturbation. In\naddition, we tested the effect of more interaction time and a stronger haptic\ncoupling between the partners on individual improvement in the same task. We\nfound no benefits of haptic interaction on individual improvement compared to\nindividuals who practised the task alone, independent of interaction time or\ninteraction strength.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 09:10:23 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 10:00:26 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Beckers", "Niek", ""], ["van Asseldonk", "Edwin", ""], ["van der Kooij", "Herman", ""]]}, {"id": "2006.16743", "submitter": "Pengyu Nie", "authors": "Pengyu Nie, Karl Palmskog, Junyi Jessy Li, Milos Gligoric", "title": "Learning to Format Coq Code Using Language Models", "comments": "Accepted in the Coq Workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Should the final right bracket in a record declaration be on a separate line?\nShould arguments to the rewrite tactic be separated by a single space? Coq code\ntends to be written in distinct manners by different people and teams. The\nexpressiveness, flexibility, and extensibility of Coq's languages and notations\nmeans that Coq projects have a wide variety of recognizable coding styles,\nsometimes explicitly documented as conventions on naming and formatting. In\nparticular, even inexperienced users can distinguish vernacular using the\nstandard library and plain Ltac from idiomatic vernacular using the\nMathematical Components (MathComp) library and SSReflect.\n  While coding conventions are important for comprehension and maintenance,\nthey are costly to document and enforce. Rule-based formatters, such as Coq's\nbeautifier, have limited flexibility and only capture small fractions of\ndesired conventions in large verification projects. We believe that application\nof language models - a class of Natural Language Processing (NLP) techniques\nfor capturing regularities in corpora - can provide a solution to this\nconundrum. More specifically, we believe that an approach based on\nautomatically learning conventions from existing Coq code, and then suggesting\nidiomatic code to users in the proper context, can be superior to manual\napproaches and static analysis tools - both in terms of effort and results.\n  As a first step, we here outline initial models to learn and suggest space\nformatting in Coq files, with a preliminary implementation for Coq 8.10, and\nevaluated on a corpus based on MathComp 1.9.0 which comprises 164k lines of Coq\ncode from four core projects.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:46:15 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Nie", "Pengyu", ""], ["Palmskog", "Karl", ""], ["Li", "Junyi Jessy", ""], ["Gligoric", "Milos", ""]]}, {"id": "2006.16870", "submitter": "Namwoo Kang", "authors": "Sunghee Lee, Soyoung Yoo, Seongsin Kim, Eunji Kim, Namwoo Kang", "title": "The Effect of Robo-taxi User Experience on User Acceptance: Field Test\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement of self-driving technology, the commercialization of\nRobo-taxi services is just a matter of time. However, there is some skepticism\nregarding whether such taxi services will be successfully accepted by real\ncustomers due to perceived safety-related concerns; therefore, studies focused\non user experience have become more crucial. Although many studies\nstatistically analyze user experience data obtained by surveying individuals'\nperceptions of Robo-taxi or indirectly through simulators, there is a lack of\nresearch that statistically analyzes data obtained directly from actual\nRobo-taxi service experiences. Accordingly, based on the user experience data\nobtained by implementing a Robo-taxi service in the downtown of Seoul and\nDaejeon in South Korea, this study quantitatively analyzes the effect of user\nexperience on user acceptance through structural equation modeling and path\nanalysis. We also obtained balanced and highly valid insights by reanalyzing\nmeaningful causal relationships obtained through statistical models based on\nin-depth interview results. Results revealed that the experience of the\ntraveling stage had the greatest effect on user acceptance, and the cutting\nedge of the service and apprehension of technology were emotions that had a\ngreat effect on user acceptance. Based on these findings, we suggest guidelines\nfor the design and marketing of future Robo-taxi services.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 14:58:26 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 13:39:59 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Lee", "Sunghee", ""], ["Yoo", "Soyoung", ""], ["Kim", "Seongsin", ""], ["Kim", "Eunji", ""], ["Kang", "Namwoo", ""]]}, {"id": "2006.16925", "submitter": "Soaad Hossain Mr", "authors": "Soaad Hossain, Syed Ishtiaque Ahmed", "title": "Ethical Analysis on the Application of Neurotechnology for Human\n  Augmentation in Physicians and Surgeons", "comments": "24 pages, 2 figures, accepted to Future Technologies Conference (FTC)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the shortage of physicians and surgeons and increase in demand worldwide\ndue to situations such as the COVID-19 pandemic, there is a growing interest in\nfinding solutions to help address the problem. A solution to this problem would\nbe to use neurotechnology to provide them augmented cognition, senses and\naction for optimal diagnosis and treatment. Consequently, doing so can\nnegatively impact them and others. We argue that applying neurotechnology for\nhuman enhancement in physicians and surgeons can cause injustices, and harm to\nthem and patients. In this paper, we will first describe the augmentations and\nneurotechnologies that can be used to achieve the relevant augmentations for\nphysicians and surgeons. We will then review selected ethical concerns\ndiscussed within literature, discuss the neuroengineering behind using\nneurotechnology for augmentation purposes, then conclude with an analysis on\noutcomes and ethical issues of implementing human augmentation via\nneurotechnology in medical and surgical practice.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 07:46:22 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 16:58:58 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Hossain", "Soaad", ""], ["Ahmed", "Syed Ishtiaque", ""]]}]