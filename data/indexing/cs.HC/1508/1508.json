[{"id": "1508.00375", "submitter": "Hamed Haddadi", "authors": "Hamed Haddadi, Ferda Ofli, Yelena Mejova, Ingmar Weber, Jaideep\n  Srivastava", "title": "360 Quantified Self", "comments": "QCRI Technical Report", "journal-ref": null, "doi": null, "report-no": "QCRI-TR-2015-004", "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable devices with a wide range of sensors have contributed to the rise of\nthe Quantified Self movement, where individuals log everything ranging from the\nnumber of steps they have taken, to their heart rate, to their sleeping\npatterns. Sensors do not, however, typically sense the social and ambient\nenvironment of the users, such as general life style attributes or information\nabout their social network. This means that the users themselves, and the\nmedical practitioners, privy to the wearable sensor data, only have a narrow\nview of the individual, limited mainly to certain aspects of their physical\ncondition.\n  In this paper we describe a number of use cases for how social media can be\nused to complement the check-up data and those from sensors to gain a more\nholistic view on individuals' health, a perspective we call the 360 Quantified\nSelf. Health-related information can be obtained from sources as diverse as\nfood photo sharing, location check-ins, or profile pictures. Additionally,\ninformation from a person's ego network can shed light on the social dimension\nof wellbeing which is widely acknowledged to be of utmost importance, even\nthough they are currently rarely used for medical diagnosis. We articulate a\nlong-term vision describing the desirable list of technical advances and\nvariety of data to achieve an integrated system encompassing Electronic Health\nRecords (EHR), data from wearable devices, alongside information derived from\nsocial media data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 11:11:15 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2015 08:22:10 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Haddadi", "Hamed", ""], ["Ofli", "Ferda", ""], ["Mejova", "Yelena", ""], ["Weber", "Ingmar", ""], ["Srivastava", "Jaideep", ""]]}, {"id": "1508.00413", "submitter": "Tingshao Zhu", "authors": "Liqing Cui, Shun Li, Wan Zhang, Zhan Zhang, Tingshao Zhu", "title": "Identifying Emotion from Natural Walking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion identification from gait aims to automatically determine persons\naffective state, it has attracted a great deal of interests and offered immense\npotential value in action tendency, health care, psychological detection and\nhuman-computer(robot) interaction.In this paper, we propose a new method of\nidentifying emotion from natural walking, and analyze the relevance between the\ntraits of walking and affective states. After obtaining the pure acceleration\ndata of wrist and ankle, we set a moving average filter window with different\nsizes w, then extract 114 features including time-domain, frequency-domain,\npower and distribution features from each data slice, and run principal\ncomponent analysis (PCA) to reduce dimension. In experiments, we train SVM,\nDecision Tree, multilayerperception, Random Tree and Random Forest\nclassification models, and compare the classification accuracy on data of wrist\nand ankle with respect to different w. The performance of emotion\nidentification on acceleration data of ankle is better than wrist.Comparing\ndifferent classification models' results, SVM has best accuracy of identifying\nanger and happy could achieve 90:31% and 89:76% respectively, and\nidentification ratio of anger-happy is 87:10%.The anger-neutral-happy\nclassification reaches 85%-78%-78%.The results show that it is capable of\nidentifying personal emotional states through the gait of walking.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2015 13:48:46 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 06:38:36 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Cui", "Liqing", ""], ["Li", "Shun", ""], ["Zhang", "Wan", ""], ["Zhang", "Zhan", ""], ["Zhu", "Tingshao", ""]]}, {"id": "1508.00761", "submitter": "Tingshao Zhu", "authors": "Shun Li, Changye Zhu, Liqing Cui, Nan Zhao, Baobin Li and Tingshao Zhu", "title": "Recognition of Emotions using Kinects", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychological studies indicate that emotional states are expressed in the way\npeople walk and the human gait is investigated in terms of its ability to\nreveal a person's emotional state. And Microsoft Kinect is a rapidly\ndeveloping, inexpensive, portable and no-marker motion capture system. This\npaper gives a new referable method to do emotion recognition, by using\nMicrosoft Kinect to do gait pattern analysis, which has not been reported. $59$\nsubjects are recruited in this study and their gait patterns are record by two\nKinect cameras. Significant joints selecting, Coordinate system transforming,\nSlider window gauss filter, Differential operation, and Data segmentation are\nused in data preprocessing. Feature extracting is based on Fourier\ntransformation. By using the NaiveBayes, RandomForests, libSVM and SMO\nclassification, the recognition rate of natural and unnatural emotions can\nreach above 70%.It is concluded that using the Kinect system can be a new\nmethod in recognition of emotions.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 13:03:27 GMT"}], "update_date": "2015-08-05", "authors_parsed": [["Li", "Shun", ""], ["Zhu", "Changye", ""], ["Cui", "Liqing", ""], ["Zhao", "Nan", ""], ["Li", "Baobin", ""], ["Zhu", "Tingshao", ""]]}, {"id": "1508.00883", "submitter": "Todd Davies", "authors": "Eric Showers and Nathan Tindall, Todd Davies", "title": "Equality of Participation Online Versus Face to Face: Condensed Analysis\n  of the Community Forum Deliberative Methods Demonstration", "comments": "14 pages, 10 tables, to appear in Efthimios Tambouris, Panos\n  Panagiotopoulos, {\\O}ystein S{\\ae}b{\\o}, Konstantinos Tarabanis, Michela\n  Milano, Theresa Pardo, and Maria Wimmer (Editors), Electronic Participation:\n  Proceedings of the 7th IFIP WG 8.5 International Conference, ePart 2015\n  (Thessaloniki, August 30-September 2), Springer LNCS Vol. 9249, 2015", "journal-ref": "Lecture Notes in Computer Science 9249:53-67, 2015", "doi": "10.1007/978-3-319-22500-5_5", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online deliberation may provide a more cost-effective and/or less inhibiting\nenvironment for public participation than face to face (F2F). But do online\nmethods bias participation toward certain individuals or groups? We compare F2F\nversus online participation in an experiment affording within-participants and\ncross-modal comparisons. For English speakers required to have Internet access\nas a condition of participation, we find no negative effects of online modes on\nequality of participation (EoP) related to gender, age, or educational level.\nAsynchronous online discussion appears to improve EoP for gender relative to\nF2F. Data suggest a dampening effect of online environments on black\nparticipants, as well as amplification for whites. Synchronous online voice\ncommunication EoP is on par with F2F across individuals. But individual-level\nEoP is much lower in the online forum, and greater online forum participation\npredicts greater F2F participation for individuals. Measured rates of\nparticipation are compared to self-reported experiences, and other findings are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2015 19:43:16 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Showers", "Eric", ""], ["Tindall", "Nathan", ""], ["Davies", "Todd", ""]]}, {"id": "1508.02024", "submitter": "Zhihan Lv", "authors": "Weixi Wang, Zhihan Lv, Xiaoming Li, Weiping Xu, Baoyun Zhang", "title": "Preprint Virtual Reality Based GIS Analysis Platform", "comments": "This is the preprint version of our paper on ICONIP2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on ICONIP2015. The proposed\nplatform supports the integrated VRGIS functions including 3D spatial analysis\nfunctions, 3D visualization for spatial process and serves for 3D globe and\ndigital city. The 3D analysis and visualization of the concerned city massive\ninformation are conducted in the platform. The amount of information that can\nbe visualized with this platform is overwhelming, and the GIS based\nnavigational scheme allows to have great flexibility to access the different\navailable data sources.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2015 14:14:33 GMT"}], "update_date": "2015-08-11", "authors_parsed": [["Wang", "Weixi", ""], ["Lv", "Zhihan", ""], ["Li", "Xiaoming", ""], ["Xu", "Weiping", ""], ["Zhang", "Baoyun", ""]]}, {"id": "1508.02823", "submitter": "Adish Singla", "authors": "Adish Singla, Eric Horvitz, Pushmeet Kohli, Andreas Krause", "title": "Learning to Hire Teams", "comments": "Short version of this paper will appear in HCOMP'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing and human computation has been employed in increasingly\nsophisticated projects that require the solution of a heterogeneous set of\ntasks. We explore the challenge of building or hiring an effective team, for\nperforming tasks required for such projects on an ongoing basis, from an\navailable pool of applicants or workers who have bid for the tasks. The\nrecruiter needs to learn workers' skills and expertise by performing online\ntests and interviews, and would like to minimize the amount of budget or time\nspent in this process before committing to hiring the team. How can one\noptimally spend budget to learn the expertise of workers as part of recruiting\na team? How can one exploit the similarities among tasks as well as underlying\nsocial ties or commonalities among the workers for faster learning? We tackle\nthese decision-theoretic challenges by casting them as an instance of online\nlearning for best action selection. We present algorithms with PAC bounds on\nthe required budget to hire a near-optimal team with high confidence.\nFurthermore, we consider an embedding of the tasks and workers in an underlying\ngraph that may arise from task similarities or social ties, and that can\nprovide additional side-observations for faster learning. We then quantify the\nimprovement in the bounds that we can achieve depending on the characteristic\nproperties of this graph structure. We evaluate our methodology on simulated\nproblem instances as well as on real-world crowdsourcing data collected from\nthe oDesk platform. Our methodology and results present an interesting\ndirection of research to tackle the challenges faced by a recruiter for\ncontract-based crowdsourcing.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2015 06:35:38 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Singla", "Adish", ""], ["Horvitz", "Eric", ""], ["Kohli", "Pushmeet", ""], ["Krause", "Andreas", ""]]}, {"id": "1508.02982", "submitter": "Jeffrey Bigham", "authors": "Michael Nebeling, Anhong Guo, Kyle Murray, Annika Tostengard, Angelos\n  Giannopoulos, Martin Mihajlov, Steven Dow, Jaime Teevan, Jeffrey P. Bigham", "title": "WearWrite: Orchestrating the Crowd to Complete Complex Tasks from\n  Wearables (We Wrote This Paper on a Watch)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a paradigm for completing complex tasks from\nwearable devices by leveraging crowdsourcing, and demonstrate its validity for\nacademic writing. We explore this paradigm using a collaborative authoring\nsystem, called WearWrite, which is designed to enable authors and crowd workers\nto work together using an Android smartwatch and Google Docs to produce\nacademic papers, including this one. WearWrite allows expert authors who do not\nhave access to large devices to contribute bits of expertise and big picture\ndirection from their watch, while freeing them of the obligation of integrating\ntheir contributions into the overall document. Crowd workers on desktop\ncomputers actually write the document. We used this approach to write several\nsimple papers, and found it was effective at producing reasonable drafts.\nHowever, the workers often needed more structure and the authors more context.\nWearWrite addresses these issues by focusing workers on specific tasks and\nproviding select context to authors on the watch. We demonstrate the system's\nfeasibility by writing this paper using it.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2015 21:15:15 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Nebeling", "Michael", ""], ["Guo", "Anhong", ""], ["Murray", "Kyle", ""], ["Tostengard", "Annika", ""], ["Giannopoulos", "Angelos", ""], ["Mihajlov", "Martin", ""], ["Dow", "Steven", ""], ["Teevan", "Jaime", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1508.03276", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan and Mehul Bhatt and Harshita Jhavar", "title": "Talking about the Moving Image: A Declarative Model for Image Schema\n  Based Embodied Perception Grounding and Language Generation", "comments": "19 pages. Unpublished report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general theory and corresponding declarative model for the\nembodied grounding and natural language based analytical summarisation of\ndynamic visuo-spatial imagery. The declarative model ---ecompassing\nspatio-linguistic abstractions, image schemas, and a spatio-temporal feature\nbased language generator--- is modularly implemented within Constraint Logic\nProgramming (CLP). The implemented model is such that primitives of the theory,\ne.g., pertaining to space and motion, image schemata, are available as\nfirst-class objects with `deep semantics' suited for inference and query. We\ndemonstrate the model with select examples broadly motivated by areas such as\nfilm, design, geography, smart environments where analytical natural language\nbased externalisations of the moving image are central from the viewpoint of\nhuman interaction, evidence-based qualitative analysis, and sensemaking.\n  Keywords: moving image, visual semantics and embodiment, visuo-spatial\ncognition and computation, cognitive vision, computational models of narrative,\ndeclarative spatial reasoning\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2015 17:34:07 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Jhavar", "Harshita", ""]]}, {"id": "1508.03593", "submitter": "Justin Hsu", "authors": "Sepehr Assadi, Justin Hsu, Shahin Jabbari", "title": "Online Assignment of Heterogeneous Tasks in Crowdsourcing Markets", "comments": "Extended version of paper in HCOMP 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of heterogeneous task assignment in crowdsourcing\nmarkets from the point of view of the requester, who has a collection of tasks.\nWorkers arrive online one by one, and each declare a set of feasible tasks they\ncan solve, and desired payment for each feasible task. The requester must\ndecide on the fly which task (if any) to assign to the worker, while assigning\nworkers only to feasible tasks. The goal is to maximize the number of assigned\ntasks with a fixed overall budget.\n  We provide an online algorithm for this problem and prove an upper bound on\nthe competitive ratio of this algorithm against an arbitrary (possibly\nworst-case) sequence of workers who want small payments relative to the\nrequester's total budget. We further show an almost matching lower bound on the\ncompetitive ratio of any algorithm in this setting. Finally, we propose a\ndifferent algorithm that achieves an improved competitive ratio in the random\npermutation model, where the order of arrival of the workers is chosen\nuniformly at random. Apart from these strong theoretical guarantees, we carry\nout experiments on simulated data which demonstrates the practical\napplicability of our algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2015 18:24:37 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Assadi", "Sepehr", ""], ["Hsu", "Justin", ""], ["Jabbari", "Shahin", ""]]}, {"id": "1508.03653", "submitter": "Kazutaka Kurihara", "authors": "Koji Tsukada, Maho Oki, Kazutaka Kurihara, Yuko Furudate", "title": "AnimalCatcher: a digital camera to capture various reactions of animals", "comments": "Written in Japanese, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People often have difficulty to take pictures of animals, since animals\nusually do not react with cameras nor understand verbal directions. To solve\nthis problem, we developed a new interaction technique, AnimalCatcher, which\ncan attract animals' attention easily. The AnimalCatcher shoots various sounds\nusing directional speaker to capture various reactions of animals. This paper\ndescribes concepts, implementation, and example pictures taken in a zoo.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2015 01:08:32 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Tsukada", "Koji", ""], ["Oki", "Maho", ""], ["Kurihara", "Kazutaka", ""], ["Furudate", "Yuko", ""]]}, {"id": "1508.03722", "submitter": "Mirco Musolesi", "authors": "Veljko Pejovic, Abhinav Mehrotra, Mirco Musolesi", "title": "Anticipatory Mobile Digital Health: Towards Personalised Proactive\n  Therapies and Prevention Strategies", "comments": "2 figures, 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last two centuries saw groundbreaking advances in the field of\nhealthcare: from the invention of the vaccine to organ transplant, and\neradication of numerous deadly diseases. Yet, these breakthroughs have only\nilluminated the role that individual traits and behaviours play in the health\nstate of a person. Continuous patient monitoring and individually-tailored\ntherapies can help in early detection and efficient tackling of health issues.\nHowever, even the most developed nations cannot afford proactive personalised\nhealthcare at scale. Mobile computing devices, nowadays equipped with an array\nof sensors, high-performance computing power, and carried by their owners at\nall time, promise to revolutionise modern healthcare. These devices can enable\ncontinuous patient monitoring, and, with the help of machine learning, can\nbuild predictive models of patient's health and behaviour. Finally, through\ntheir close integration with a user's lifestyle mobiles can be used to deliver\npersonalised proactive therapies. In this article, we develop the concept of\nanticipatory mobile-based healthcare - anticipatory mobile digital health - and\nexamine the opportunities and challenges associated with its practical\nrealisation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 11:29:22 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Pejovic", "Veljko", ""], ["Mehrotra", "Abhinav", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1508.03725", "submitter": "Mirco Musolesi", "authors": "Veljko Pejovic, Neal Lathia, Cecilia Mascolo, Mirco Musolesi", "title": "Mobile-Based Experience Sampling for Behaviour Research", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Experience Sampling Method (ESM) introduces in-situ sampling of human\nbehaviour, and provides researchers and behavioural therapists with\necologically valid and timely assessments of a person's psychological state.\nThis, in turn, opens up new opportunities for understanding behaviour at a\nscale and granularity that was not possible just a few years ago. The practical\napplications are many, such as the delivery of personalised and agile behaviour\ninterventions. Mobile computing devices represent a revolutionary platform for\nimproving ESM. They are an inseparable part of our daily lives, context-aware,\nand can interact with people at suitable moments. Furthermore, these devices\nare equipped with sensors, and can thus take part of the reporting burden off\nthe participant, and collect data automatically. The goal of this survey is to\ndiscuss recent advancements in using mobile technologies for ESM (mESM), and\npresent our vision of the future of mobile experience sampling.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2015 12:15:38 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Pejovic", "Veljko", ""], ["Lathia", "Neal", ""], ["Mascolo", "Cecilia", ""], ["Musolesi", "Mirco", ""]]}, {"id": "1508.03895", "submitter": "EPTCS", "authors": "Carlo A. Furia, Christopher M. Poskitt, Julian Tschannen", "title": "The AutoProof Verifier: Usability by Non-Experts and on Standard Code", "comments": "In Proceedings F-IDE 2015, arXiv:1508.03388", "journal-ref": "EPTCS 187, 2015, pp. 42-55", "doi": "10.4204/EPTCS.187.4", "report-no": null, "categories": "cs.SE cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal verification tools are often developed by experts for experts; as a\nresult, their usability by programmers with little formal methods experience\nmay be severely limited. In this paper, we discuss this general phenomenon with\nreference to AutoProof: a tool that can verify the full functional correctness\nof object-oriented software. In particular, we present our experiences of using\nAutoProof in two contrasting contexts representative of non-expert usage.\nFirst, we discuss its usability by students in a graduate course on software\nverification, who were tasked with verifying implementations of various sorting\nalgorithms. Second, we evaluate its usability in verifying code developed for\nprogramming assignments of an undergraduate course. The first scenario\nrepresents usability by serious non-experts; the second represents usability on\n\"standard code\", developed without full functional verification in mind. We\nreport our experiences and lessons learnt, from which we derive some general\nsuggestions for furthering the development of verification tools with respect\nto improving their usability.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 01:36:56 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Furia", "Carlo A.", ""], ["Poskitt", "Christopher M.", ""], ["Tschannen", "Julian", ""]]}, {"id": "1508.03896", "submitter": "EPTCS", "authors": "Nabil M. Kabbani (Clemson University), Daniel Welch (Clemson\n  University), Caleb Priester (Clemson University), Stephen Schaub (Clemson\n  University), Blair Durkee (Clemson University), Yu-Shan Sun (Clemson\n  University), Murali Sitaraman (Clemson University)", "title": "Formal Reasoning Using an Iterative Approach with an Integrated Web IDE", "comments": "In Proceedings F-IDE 2015, arXiv:1508.03388", "journal-ref": "EPTCS 187, 2015, pp. 56-71", "doi": "10.4204/EPTCS.187.5", "report-no": null, "categories": "cs.SE cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes our experience in communicating the elements of\nreasoning about correctness, and the central role of formal specifications in\nreasoning about modular, component-based software using a language and an\nintegrated Web IDE designed for the purpose. Our experience in using such an\nIDE, supported by a 'push-button' verifying compiler in a classroom setting,\nreveals the highly iterative process learners use to arrive at suitably\nspecified, automatically provable code. We explain how the IDE facilitates\nreasoning at each step of this process by providing human readable verification\nconditions (VCs) and feedback from an integrated prover that clearly indicates\nunprovable VCs to help identify obstacles to completing proofs. The paper\ndiscusses the IDE's usage in verified software development using several\nexamples drawn from actual classroom lectures and student assignments to\nillustrate principles of design-by-contract and the iterative process of\ncreating and subsequently refining assertions, such as loop invariants in\nobject-based code.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 01:37:08 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Kabbani", "Nabil M.", "", "Clemson University"], ["Welch", "Daniel", "", "Clemson\n  University"], ["Priester", "Caleb", "", "Clemson University"], ["Schaub", "Stephen", "", "Clemson\n  University"], ["Durkee", "Blair", "", "Clemson University"], ["Sun", "Yu-Shan", "", "Clemson\n  University"], ["Sitaraman", "Murali", "", "Clemson University"]]}, {"id": "1508.04028", "submitter": "Lex Fridman", "authors": "Lex Fridman, Joonbum Lee, Bryan Reimer, Trent Victor", "title": "Owl and Lizard: Patterns of Head Pose and Eye Pose in Driver Gaze\n  Classification", "comments": "Accepted for Publication in IET Computer Vision. arXiv admin note:\n  text overlap with arXiv:1507.04760", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate, robust, inexpensive gaze tracking in the car can help keep a driver\nsafe by facilitating the more effective study of how to improve (1) vehicle\ninterfaces and (2) the design of future Advanced Driver Assistance Systems. In\nthis paper, we estimate head pose and eye pose from monocular video using\nmethods developed extensively in prior work and ask two new interesting\nquestions. First, how much better can we classify driver gaze using head and\neye pose versus just using head pose? Second, are there individual-specific\ngaze strategies that strongly correlate with how much gaze classification\nimproves with the addition of eye pose information? We answer these questions\nby evaluating data drawn from an on-road study of 40 drivers. The main insight\nof the paper is conveyed through the analogy of an \"owl\" and \"lizard\" which\ndescribes the degree to which the eyes and the head move when shifting gaze.\nWhen the head moves a lot (\"owl\"), not much classification improvement is\nattained by estimating eye pose on top of head pose. On the other hand, when\nthe head stays still and only the eyes move (\"lizard\"), classification accuracy\nincreases significantly from adding in eye pose. We characterize how that\naccuracy varies between people, gaze strategies, and gaze regions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 13:54:05 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 18:10:49 GMT"}, {"version": "v3", "created": "Sun, 20 Nov 2016 04:46:26 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Fridman", "Lex", ""], ["Lee", "Joonbum", ""], ["Reimer", "Bryan", ""], ["Victor", "Trent", ""]]}, {"id": "1508.04131", "submitter": "Pushkar Joshi", "authors": "Pushkar Joshi", "title": "Contextually proximate approach to develop smart user interface", "comments": "4 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Researchers and experts are taking efforts in delivering an optimal user\nexperience from a long time. Computer interfaces are being developed to keep\nuser 'in the flow' as well as for making users more connected to the real world\nwile using virtual environment. Developing ubiquitous user interfaces for\nnovices and experts at the same time is crucial work for interaction designers.\nThis paper molds the designing approach of user interfaces in bit different\nparameters by reviewing the existing literature and proposing a different way\nto develop a smart user interface to make user more familiar with the design\nand to keep user 'in the flow'. Contextually proximate approach (CPA) will help\nusers to minimize their feeling of insecurity as designing process includes\nlocal resources of users to develop the user interfaces. These various\nresources and parameters are explained further in the paper by giving different\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2015 07:32:33 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Joshi", "Pushkar", ""]]}, {"id": "1508.04153", "submitter": "J\\'er\\'emie Boulanger", "authors": "J\\'er\\'emie Boulanger, Ludovic Seifert, Romain H\\'erault,\n  Jean-Francois Coeurjolly", "title": "Automatic sensor-based detection and classification of climbing\n  activities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a method to automatically detect and classify climbing\nactivities using inertial measurement units (IMUs) attached to the wrists, feet\nand pelvis of the climber. The IMUs record limb acceleration and angular\nvelocity. Detection requires a learning phase with manual annotation to\nconstruct the statistical models used in the cusum algorithm. Full-body\nactivity is then classified based on the detection of each IMU.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 13:08:10 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Boulanger", "J\u00e9r\u00e9mie", ""], ["Seifert", "Ludovic", ""], ["H\u00e9rault", "Romain", ""], ["Coeurjolly", "Jean-Francois", ""]]}, {"id": "1508.04603", "submitter": "Serena Ivaldi", "authors": "Serena Ivaldi and Sebastien Lefort and Jan Peters and Mohamed\n  Chetouani and Joelle Provasi and Elisabetta Zibetti", "title": "Towards engagement models that consider individual factors in HRI: on\n  the relation of extroversion and negative attitude towards robots to gaze and\n  speech during a human-robot assembly task", "comments": "24 pages, submitted to IJSR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the engagement is critical for human - robot interaction.\nEngagement measures typically rely on the dynamics of the social signals\nexchanged by the partners, especially speech and gaze. However, the dynamics of\nthese signals is likely to be influenced by individual and social factors, such\nas personality traits, as it is well documented that they critically influence\nhow two humans interact with each other. Here, we assess the influence of two\nfactors, namely extroversion and negative attitude toward robots, on speech and\ngaze during a cooperative task, where a human must physically manipulate a\nrobot to assemble an object. We evaluate if the scores of extroversion and\nnegative attitude towards robots co-variate with the duration and frequency of\ngaze and speech cues. The experiments were carried out with the humanoid robot\niCub and N=56 adult participants. We found that the more people are extrovert,\nthe more and longer they tend to talk with the robot; and the more people have\na negative attitude towards robots, the less they will look at the robot face\nand the more they will look at the robot hands where the assembly and the\ncontacts occur. Our results confirm and provide evidence that the engagement\nmodels classically used in human-robot interaction should take into account\nattitudes and personality traits.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 11:30:00 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Ivaldi", "Serena", ""], ["Lefort", "Sebastien", ""], ["Peters", "Jan", ""], ["Chetouani", "Mohamed", ""], ["Provasi", "Joelle", ""], ["Zibetti", "Elisabetta", ""]]}, {"id": "1508.04789", "submitter": "Luz Rello", "authors": "Luz Rello, Clara Bayarri, Yolanda Otal, and Martin Pielot", "title": "A Computer-Based Method to Improve the Spelling of Children with\n  Dyslexia", "comments": "8 pages, ASSETS'14, October 20-22, 2014, Rochester, NY, USA", "journal-ref": null, "doi": "10.1145/2661334.2661373", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method which aims to improve the spelling of\nchildren with dyslexia through playful and targeted exercises. In contrast to\nprevious approaches, our method does not use correct words or positive examples\nto follow, but presents the child a misspelled word as an exercise to solve. We\ncreated these training exercises on the basis of the linguistic knowledge\nextracted from the errors found in texts written by children with dyslexia. To\ntest the effectiveness of this method in Spanish, we integrated the exercises\nin a game for iPad, DysEggxia (Piruletras in Spanish), and carried out a\nwithin-subject experiment. During eight weeks, 48 children played either\nDysEggxia or Word Search, which is another word game. We conducted tests and\nquestionnaires at the beginning of the study, after four weeks when the games\nwere switched, and at the end of the study. The children who played DysEggxia\nfor four weeks in a row had significantly less writing errors in the tests that\nafter playing Word Search for the same time. This provides evidence that\nerror-based exercises presented in a tablet help children with dyslexia improve\ntheir spelling skills.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 20:38:28 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Rello", "Luz", ""], ["Bayarri", "Clara", ""], ["Otal", "Yolanda", ""], ["Pielot", "Martin", ""]]}, {"id": "1508.04819", "submitter": "Brian Keegan", "authors": "Brian C. Keegan, Shakked Lev, Ofer Arazy", "title": "Analyzing Organizational Routines in Online Knowledge Collaborations: A\n  Case for Sequence Analysis in CSCW", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research into socio-technical systems like Wikipedia has overlooked important\nstructural patterns in the coordination of distributed work. This paper argues\nfor a conceptual reorientation towards sequences as a fundamental unit of\nanalysis for understanding work routines in online knowledge collaboration. We\noutline a research agenda for researchers in computer-supported cooperative\nwork (CSCW) to understand the relationships, patterns, antecedents, and\nconsequences of sequential behavior using methods already developed in fields\nlike bio-informatics. Using a data set of 37,515 revisions from 16,616 unique\neditors to 96 Wikipedia articles as a case study, we analyze the prevalence and\nsignificance of different sequences of editing patterns. We illustrate the\nmixed method potential of sequence approaches by interpreting the frequent\npatterns as general classes of behavioral motifs. We conclude by discussing the\nmethodological opportunities for using sequence analysis for expanding existing\napproaches to analyzing and theorizing about co-production routines in online\nknowledge collaboration.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2015 22:36:21 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 15:17:09 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Keegan", "Brian C.", ""], ["Lev", "Shakked", ""], ["Arazy", "Ofer", ""]]}, {"id": "1508.05626", "submitter": "Joseph Maguire", "authors": "Joseph Maguire and Karen Renaud", "title": "You Only Live Twice or \"The Years We Wasted Caring about\n  Shoulder-Surfing\"", "comments": "Proceedings of the BCS HCI 2012", "journal-ref": null, "doi": "10.1145/2377916.2377975", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passwords are a good idea, in theory. They have the potential to act as a\nfairly strong gateway. In practice though, passwords are plagued with problems.\nThey are (1) easily shared, (2) trivial to observe and (3) maddeningly elusive\nwhen forgotten. While alternatives to passwords have been proposed, none, as\nyet, have been adopted widely. There seems to be a reluctance to switch from\ntried and tested passwords to novel alternatives, even if the most glaring\nflaws of passwords can be mitigated. One argument is that there is not enough\ninvestigation into the feasibility of many password alternatives. Graphical\nauthentication mechanisms are a case in point. Therefore, in this paper, we\ndetail the design of two prototype applications that utilise graphical\nauthentication mechanisms. However, when forced to consider the design of such\nprototypes, we find that pertinent password problems eg. observation of entry,\nare just that: password problems. We conclude that effective, alternative\nauthentication mechanisms should target authentication scenarios rather than\nthe well-known problems of passwords. This is the only route to wide-spread\nadoption of alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2015 15:55:49 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Maguire", "Joseph", ""], ["Renaud", "Karen", ""]]}, {"id": "1508.05782", "submitter": "Jaan Aru", "authors": "Madis Vasser, Markus K\\\"angsepp, Jaan Aru", "title": "Change Blindness in 3D Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present change blindness study subjects explored stereoscopic three\ndimensional (3D) environments through a virtual reality (VR) headset. A novel\nmethod that tracked the subjects' head movements was used for inducing changes\nin the scene whenever the changing object was out of the field of view. The\neffect of change location (foreground or background in 3D depth) on change\nblindness was investigated. Two experiments were conducted, one in the lab (n =\n50) and the other online (n = 25). Up to 25% of the changes were undetected and\nthe mean overall search time was 27 seconds in the lab study. Results indicated\nsignificantly lower change detection success and more change cycles if the\nchanges occurred in the background, with no differences in overall search\ntimes. The results confirm findings from previous studies and extend them to 3D\nenvironments. The study also demonstrates the feasibility of online VR\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2015 12:33:10 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Vasser", "Madis", ""], ["K\u00e4ngsepp", "Markus", ""], ["Aru", "Jaan", ""]]}, {"id": "1508.06161", "submitter": "Scott Bronikowski", "authors": "Daniel Paul Barrett, Scott Alan Bronikowski, Haonan Yu, and Jeffrey\n  Mark Siskind", "title": "Robot Language Learning, Generation, and Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified framework which supports grounding natural-language\nsemantics in robotic driving. This framework supports acquisition (learning\ngrounded meanings of nouns and prepositions from human annotation of robotic\ndriving paths), generation (using such acquired meanings to generate sentential\ndescription of new robotic driving paths), and comprehension (using such\nacquired meanings to support automated driving to accomplish navigational goals\nspecified in natural language). We evaluate the performance of these three\ntasks by having independent human judges rate the semantic fidelity of the\nsentences associated with paths, achieving overall average correctness of 94.6%\nand overall average completeness of 85.6%.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 14:10:21 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Barrett", "Daniel Paul", ""], ["Bronikowski", "Scott Alan", ""], ["Yu", "Haonan", ""], ["Siskind", "Jeffrey Mark", ""]]}, {"id": "1508.06195", "submitter": "Luiz Capretz Dr.", "authors": "Arif Raza, Luiz Fernando Capretz", "title": "Usability as a Dominant Quality Attribute", "comments": "International Conference on Software Engineering Research and\n  Practice, Las Vegas, pp. 571-575, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whenever an architect or a team of architects begins an architectural design,\nthere are certain goals set to achieve. There are many factors involved in\nsetting up goals for the architecture design such as type of the project, end\nuser perspective, functional and non-functional requirements and so on. This\npaper reviews and further elaborates strategy for the usability characteristics\nof software architecture. Although user centered designs are tremendously\ngaining popularity, still in many design scenarios, usability is barely even\nconsidered as one of the primary goals. This work provides an opportunity to\ncompare different strategies and evaluate their pros and cons.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2015 15:50:50 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Raza", "Arif", ""], ["Capretz", "Luiz Fernando", ""]]}, {"id": "1508.07053", "submitter": "Kenji Hata", "authors": "Kenji Hata, Sherman Leung, Ranjay Krishna, Michael S. Bernstein, Li\n  Fei-Fei", "title": "SentenceRacer: A Game with a Purpose for Image Sentence Annotation", "comments": "2 pages, 2 figures, 2 tables, potential CSCW poster submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently datasets that contain sentence descriptions of images have enabled\nmodels that can automatically generate image captions. However, collecting\nthese datasets are still very expensive. Here, we present SentenceRacer, an\nonline game that gathers and verifies descriptions of images at no cost.\nSimilar to the game hangman, players compete to uncover words in a sentence\nthat ultimately describes an image. SentenceRacer both generates and verifies\nthat the sentences are accurate descriptions. We show that SentenceRacer\ngenerates annotations of higher quality than those generated on Amazon\nMechanical Turk (AMT).\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2015 23:03:17 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Hata", "Kenji", ""], ["Leung", "Sherman", ""], ["Krishna", "Ranjay", ""], ["Bernstein", "Michael S.", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1508.07097", "submitter": "Nguyen Huynh", "authors": "Hoai Nguyen Huynh, Erika Fille Legara, Christopher Monterola", "title": "A Dynamical Model of Twitter Activity Profiles", "comments": "10 pages, 5 figures", "journal-ref": "Hypertext '15, 49-57 (2015)", "doi": "10.1145/2700171.2791029", "report-no": null, "categories": "cs.SI cs.CY cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of the era of Big Data has allowed many researchers to dig into\nvarious socio-technical systems, including social media platforms. In\nparticular, these systems have provided them with certain verifiable means to\nlook into certain aspects of human behavior. In this work, we are specifically\ninterested in the behavior of individuals on social media platforms---how they\nhandle the information they get, and how they share it. We look into Twitter to\nunderstand the dynamics behind the users' posting activities---tweets and\nretweets---zooming in on topics that peaked in popularity. Three mechanisms are\nconsidered: endogenous stimuli, exogenous stimuli, and a mechanism that\ndictates the decay of interest of the population in a topic. We propose a model\ninvolving two parameters $\\eta^\\star$ and $\\lambda$ describing the tweeting\nbehaviour of users, which allow us to reconstruct the findings of Lehmann et\nal. (2012) on the temporal profiles of popular Twitter hashtags. With this\nmodel, we are able to accurately reproduce the temporal profile of user\nengagements on Twitter. Furthermore, we introduce an alternative in classifying\nthe collective activities on the socio-technical system based on the model.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2015 05:24:14 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Huynh", "Hoai Nguyen", ""], ["Legara", "Erika Fille", ""], ["Monterola", "Christopher", ""]]}]