[{"id": "1802.00029", "submitter": "Mawulolo Ameko", "authors": "Mawulolo K. Ameko, Lihua Cai, Mehdi Boukhechba, Alexander Daros,\n  Philip I. Chow, Bethany A. Teachman, Matthew S. Gerber, Laura E. Barnes", "title": "Cluster-based Approach to Improve Affect Recognition from Passively\n  Sensed Data", "comments": "BHI2018", "journal-ref": null, "doi": "10.1109/BHI.2018.8333461", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative affect is a proxy for mental health in adults. By being able to\npredict participants' negative affect states unobtrusively, researchers and\nclinicians will be better positioned to deliver targeted, just-in-time mental\nhealth interventions via mobile applications. This work attempts to personalize\nthe passive recognition of negative affect states via group-based modeling of\nuser behavior patterns captured from mobility, communication, and activity\npatterns. Results show that group models outperform generalized models in a\ndataset based on two weeks of users' daily lives.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 19:26:27 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Ameko", "Mawulolo K.", ""], ["Cai", "Lihua", ""], ["Boukhechba", "Mehdi", ""], ["Daros", "Alexander", ""], ["Chow", "Philip I.", ""], ["Teachman", "Bethany A.", ""], ["Gerber", "Matthew S.", ""], ["Barnes", "Laura E.", ""]]}, {"id": "1802.00264", "submitter": "Kang Li", "authors": "Kang Li, Xiaoguang Zhao, Jiang Bian, and Min Tan", "title": "Automatic Safety Helmet Wearing Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance is essential for the safety of power substation. The detection\nof whether wearing safety helmets or not for perambulatory workers is the key\ncomponent of overall intelligent surveillance system in power substation. In\nthis paper, a novel and practical safety helmet detection framework based on\ncomputer vision, machine learning and image processing is proposed. In order to\nascertain motion objects in power substation, the ViBe background modelling\nalgorithm is employed. Moreover, based on the result of motion objects\nsegmentation, real-time human classification framework C4 is applied to locate\npedestrian in power substation accurately and quickly. Finally, according to\nthe result of pedestrian detection, the safety helmet wearing detection is\nimplemented using the head location, the color space transformation and the\ncolor feature discrimination. Extensive compelling experimental results in\npower substation illustrate the efficiency and effectiveness of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 12:41:25 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Li", "Kang", ""], ["Zhao", "Xiaoguang", ""], ["Bian", "Jiang", ""], ["Tan", "Min", ""]]}, {"id": "1802.00272", "submitter": "Kang Li", "authors": "Kang Li, Jinting Wu, Xiaoguang Zhao and Min Tan", "title": "Real-Time Human-Robot Interaction for a Service Robot Based on 3D Human\n  Activity Recognition and Human-mimicking Decision Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the development of a real-time Human-Robot Interaction\n(HRI) system for a service robot based on 3D human activity recognition and\nhuman-like decision mechanism. The Human-Robot Interactive (HRI) system, which\nallows one person to interact with a service robot using natural body language,\ncollects sequences of 3D skeleton joints comprising rich human movement\ninformation about the user via Microsoft Kinect. This information is used to\ntrain a three-layer Long-Short-Term Memory (LSTM) network for human action\nrecognition. The robot understands user intent based on an online LSTM network\ntest, and responds to the user via movements of the robotic arm or chassis.\nFurthermore, the human-like decision mechanism is also fused into this process,\nwhich allows the robot to instinctively decide whether to interrupt the current\ntask according to task priority. The framework of the overall system is\nestablished on the Robot Operating System (ROS) platform. The real-life\nactivity interaction between our service robot and the user was conducted to\ndemonstrate the effectiveness of developed HRI system.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 12:58:06 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 11:38:42 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Li", "Kang", ""], ["Wu", "Jinting", ""], ["Zhao", "Xiaoguang", ""], ["Tan", "Min", ""]]}, {"id": "1802.00373", "submitter": "Cassie Meeker", "authors": "Cassie Meeker, Sangwoo Park, Lauri Bishop, Joel Stein, Matei Ciocarlie", "title": "EMG Pattern Classification to Control a Hand Orthosis for Functional\n  Grasp Assistance after Stroke", "comments": "ICORR 2017, 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable orthoses can function both as assistive devices, which allow the\nuser to live independently, and as rehabilitation devices, which allow the user\nto regain use of an impaired limb. To be fully wearable, such devices must have\nintuitive controls, and to improve quality of life, the device should enable\nthe user to perform Activities of Daily Living. In this context, we explore the\nfeasibility of using electromyography (EMG) signals to control a wearable\nexotendon device to enable pick and place tasks. We use an easy to don,\ncommodity forearm EMG band with 8 sensors to create an EMG pattern\nclassification control for an exotendon device. With this control, we are able\nto detect a user's intent to open, and can thus enable extension and pick and\nplace tasks. In experiments with stroke survivors, we explore the accuracy of\nthis control in both non-functional and functional tasks. Our results support\nthe feasibility of developing wearable devices with intuitive controls which\nprovide a functional context for rehabilitation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 16:14:55 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Meeker", "Cassie", ""], ["Park", "Sangwoo", ""], ["Bishop", "Lauri", ""], ["Stein", "Joel", ""], ["Ciocarlie", "Matei", ""]]}, {"id": "1802.00507", "submitter": "Manuel Ortega-Rodr\\'iguez", "authors": "Diana Valverde-M\\'endez, Manuel Ortega-Rodr\\'iguez, Hugo\n  Sol\\'is-S\\'anchez, Ariadna Venegas-Li", "title": "The effects of anger on automated long-term-spectra based\n  speaker-identification", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forensic speaker identification has traditionally considered approaches based\non long term spectra analysis as especially robust, given that they work well\nfor short recordings, are not sensitive to changes in the intensity of the\nsample, and continue to function in the presence of noise and limited passband.\nWe find, however, that anger induces a significant distortion of the acoustic\nsignal for long term spectra analysis purposes. Even moderate anger offsets\nspeaker identification results by 33% in the direction of a different speaker\naltogether. Thus, caution should be exercised when applying this tool.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 03:51:30 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Valverde-M\u00e9ndez", "Diana", ""], ["Ortega-Rodr\u00edguez", "Manuel", ""], ["Sol\u00eds-S\u00e1nchez", "Hugo", ""], ["Venegas-Li", "Ariadna", ""]]}, {"id": "1802.00613", "submitter": "Jens Grubert", "authors": "Jens Grubert and Lukas Witzani and Eyal Ofek and Michel Pahud and\n  Matthias Kranz and Per Ola Kristensson", "title": "Effects of Hand Representations for Typing in Virtual Reality", "comments": "IEEE VR 2018 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alphanumeric text entry is a challenge for Virtual Reality (VR) applications.\nVR enables new capabilities, impossible in the real world, such as an\nunobstructed view of the keyboard, without occlusion by the user's physical\nhands. Several hand representations have been proposed for typing in VR on\nstandard physical keyboards. However, to date, these hand representations have\nnot been compared regarding their performance and effects on presence for VR\ntext entry. Our work addresses this gap by comparing existing hand\nrepresentations with minimalistic fingertip visualization. We study the effects\nof four hand representations (no hand representation, inverse kinematic model,\nfingertip visualization using spheres and video inlay) on typing in VR using a\nstandard physical keyboard with 24 participants. We found that the fingertip\nvisualization and video inlay both resulted in statistically significant lower\ntext entry error rates compared to no hand or inverse kinematic model\nrepresentations. We found no statistical differences in text entry speed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 09:39:13 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Grubert", "Jens", ""], ["Witzani", "Lukas", ""], ["Ofek", "Eyal", ""], ["Pahud", "Michel", ""], ["Kranz", "Matthias", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "1802.00626", "submitter": "Jens Grubert", "authors": "Jens Grubert and Lukas Witzani and Eyal Ofek and Michel Pahud and\n  Matthias Kranz and Per Ola Kristensson", "title": "Text Entry in Immersive Head-Mounted Display-based Virtual Reality using\n  Standard Keyboards", "comments": "IEEE VR 2018. arXiv admin note: text overlap with arXiv:1802.00613", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance and user experience of two popular mainstream text\nentry devices, desktop keyboards and touchscreen keyboards, for use in Virtual\nReality (VR) applications. We discuss the limitations arising from limited\nvisual feedback, and examine the efficiency of different strategies of use. We\nanalyze a total of 24 hours of typing data in VR from 24 participants and find\nthat novice users are able to retain about 60% of their typing speed on a\ndesktop keyboard and about 40-45\\% of their typing speed on a touchscreen\nkeyboard. We also find no significant learning effects, indicating that users\ncan transfer their typing skills fast into VR. Besides investigating baseline\nperformances, we study the position in which keyboards and hands are rendered\nin space. We find that this does not adversely affect performance for desktop\nkeyboard typing and results in a performance trade-off for touchscreen keyboard\ntyping.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 10:28:44 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Grubert", "Jens", ""], ["Witzani", "Lukas", ""], ["Ofek", "Eyal", ""], ["Pahud", "Michel", ""], ["Kranz", "Matthias", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "1802.01029", "submitter": "Michael Veale", "authors": "Michael Veale, Max Van Kleek, Reuben Binns", "title": "Fairness and Accountability Design Needs for Algorithmic Support in\n  High-Stakes Public Sector Decision-Making", "comments": "14 pages, 0 figures, ACM Conference on Human Factors in Computing\n  Systems (CHI'18), April 21--26, Montreal, Canada", "journal-ref": "Proceedings of the 2018 CHI Conference on Human Factors in\n  Computing Systems (2018) 440", "doi": "10.1145/3173574.3174014", "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Calls for heightened consideration of fairness and accountability in\nalgorithmically-informed public decisions---like taxation, justice, and child\nprotection---are now commonplace. How might designers support such human\nvalues? We interviewed 27 public sector machine learning practitioners across 5\nOECD countries regarding challenges understanding and imbuing public values\ninto their work. The results suggest a disconnect between organisational and\ninstitutional realities, constraints and needs, and those addressed by current\nresearch into usable, transparent and 'discrimination-aware' machine\nlearning---absences likely to undermine practical initiatives unless addressed.\nWe see design opportunities in this disconnect, such as in supporting the\ntracking of concept drift in secondary data sources, and in building usable\ntransparency tools to identify risks and incorporate domain knowledge, aimed\nboth at managers and at the 'street-level bureaucrats' on the frontlines of\npublic service. We conclude by outlining ethical challenges and future\ndirections for collaboration in these high-stakes applications.\n", "versions": [{"version": "v1", "created": "Sat, 3 Feb 2018 20:57:13 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Veale", "Michael", ""], ["Van Kleek", "Max", ""], ["Binns", "Reuben", ""]]}, {"id": "1802.01096", "submitter": "Nathalia Moraes do Nascimento", "authors": "Nathalia Nascimento, Carlos Lucena, Paulo Alencar and Donald Cowan", "title": "Software Engineers vs. Machine Learning Algorithms: An Empirical Study\n  Assessing Performance and Reuse Tasks", "comments": "22 pages. To be submitted to IEEE Transactions on Software\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several papers have recently contained reports on applying machine learning\n(ML) to the automation of software engineering (SE) tasks, such as project\nmanagement, modeling and development. However, there appear to be no approaches\ncomparing how software engineers fare against machine-learning algorithms as\napplied to specific software development tasks. Such a comparison is essential\nto gain insight into which tasks are better performed by humans and which by\nmachine learning and how cooperative work or human-in-the-loop processes can be\nimplemented more effectively. In this paper, we present an empirical study that\ncompares how software engineers and machine-learning algorithms perform and\nreuse tasks. The empirical study involves the synthesis of the control\nstructure of an autonomous streetlight application. Our approach consists of\nfour steps. First, we solved the problem using machine learning to determine\nspecific performance and reuse tasks. Second, we asked software engineers with\ndifferent domain knowledge levels to provide a solution to the same tasks.\nThird, we compared how software engineers fare against machine-learning\nalgorithms when accomplishing the performance and reuse tasks based on criteria\nsuch as energy consumption and safety. Finally, we analyzed the results to\nunderstand which tasks are better performed by either humans or algorithms so\nthat they can work together more effectively. Such an understanding and the\nresulting human-in-the-loop approaches, which take into account the strengths\nand weaknesses of humans and machine-learning algorithms, are fundamental not\nonly to provide a basis for cooperative work in support of software\nengineering, but also, in other areas.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 09:38:48 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 21:32:02 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Nascimento", "Nathalia", ""], ["Lucena", "Carlos", ""], ["Alencar", "Paulo", ""], ["Cowan", "Donald", ""]]}, {"id": "1802.01186", "submitter": "Oggi Rudovic", "authors": "Ognjen Rudovic, Jaeryoung Lee, Miles Dai, Bjorn Schuller and Rosalind\n  Picard", "title": "Personalized Machine Learning for Robot Perception of Affect and\n  Engagement in Autism Therapy", "comments": "The paper has undergone a major revision and its content is outdated", "journal-ref": null, "doi": "10.1126/scirobotics.aao6760", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots have great potential to facilitate future therapies for children on\nthe autism spectrum. However, existing robots lack the ability to automatically\nperceive and respond to human affect, which is necessary for establishing and\nmaintaining engaging interactions. Moreover, their inference challenge is made\nharder by the fact that many individuals with autism have atypical and\nunusually diverse styles of expressing their affective-cognitive states. To\ntackle the heterogeneity in behavioral cues of children with autism, we use the\nlatest advances in deep learning to formulate a personalized machine learning\n(ML) framework for automatic perception of the childrens affective states and\nengagement during robot-assisted autism therapy. The key to our approach is a\nnovel shift from the traditional ML paradigm - instead of using\n'one-size-fits-all' ML models, our personalized ML framework is optimized for\neach child by leveraging relevant contextual information (demographics and\nbehavioral assessment scores) and individual characteristics of each child. We\ndesigned and evaluated this framework using a dataset of multi-modal audio,\nvideo and autonomic physiology data of 35 children with autism (age 3-13) and\nfrom 2 cultures (Asia and Europe), participating in a 25-minute child-robot\ninteraction (~500k datapoints). Our experiments confirm the feasibility of the\nrobot perception of affect and engagement, showing clear improvements due to\nthe model personalization. The proposed approach has potential to improve\nexisting therapies for autism by offering more efficient monitoring and\nsummarization of the therapy progress.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 20:05:26 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 01:21:12 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Rudovic", "Ognjen", ""], ["Lee", "Jaeryoung", ""], ["Dai", "Miles", ""], ["Schuller", "Bjorn", ""], ["Picard", "Rosalind", ""]]}, {"id": "1802.01489", "submitter": "David Burns", "authors": "David Burns, Nathan Leung, Michael Hardisty, Cari Whyne, Patrick\n  Henry, Stewart McLachlin", "title": "Shoulder Physiotherapy Exercise Recognition: Machine Learning the\n  Inertial Signals from a Smartwatch", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6579/aacfd9", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Participation in a physical therapy program is considered one of\nthe greatest predictors of successful conservative management of common\nshoulder disorders. However, adherence to these protocols is often poor and\ntypically worse for unsupervised home exercise programs. Currently, there are\nlimited tools available for objective measurement of adherence in the home\nsetting. The goal of this study was to develop and evaluate the potential for\nperforming home shoulder physiotherapy monitoring using a commercial\nsmartwatch.\n  Approach: Twenty healthy adult subjects with no prior shoulder disorders\nperformed seven exercises from an evidence-based rotator cuff physiotherapy\nprotocol, while 6-axis inertial sensor data was collected from the active\nextremity. Within an activity recognition chain (ARC) framework, four\nsupervised learning algorithms were trained and optimized to classify the\nexercises: k-nearest neighbor (k-NN), random forest (RF), support vector\nmachine classifier (SVC), and a convolutional recurrent neural network (CRNN).\nAlgorithm performance was evaluated using 5-fold cross-validation stratified\nfirst temporally and then by subject.\n  Main Results: Categorical classification accuracy was above 94% for all\nalgorithms on the temporally stratified cross validation, with the best\nperformance achieved by the CRNN algorithm (99.4%). The subject stratified\ncross validation, which evaluated classifier performance on unseen subjects,\nyielded lower accuracies scores again with CRNN performing best (88.9%).\n  Significance: This proof of concept study demonstrates the technical\nfeasibility of a smartwatch device and supervised machine learning approach to\nmore easily monitor and assess the at-home adherence of shoulder physiotherapy\nexercise protocols.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 16:06:09 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 21:56:15 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Burns", "David", ""], ["Leung", "Nathan", ""], ["Hardisty", "Michael", ""], ["Whyne", "Cari", ""], ["Henry", "Patrick", ""], ["McLachlin", "Stewart", ""]]}, {"id": "1802.01636", "submitter": "Chandrayee Basu", "authors": "Chandrayee Basu, Qian Yang, David Hungerman, Mukesh Singhal, Anca D.\n  Dragan", "title": "Do You Want Your Autonomous Car To Drive Like You?", "comments": "8 pages, 7 figures, HRI 2017", "journal-ref": null, "doi": "10.1145/1235", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With progress in enabling autonomous cars to drive safely on the road, it is\ntime to start asking how they should be driving. A common answer is that they\nshould be adopting their users' driving style. This makes the assumption that\nusers want their autonomous cars to drive like they drive - aggressive drivers\nwant aggressive cars, defensive drivers want defensive cars. In this paper, we\nput that assumption to the test. We find that users tend to prefer a\nsignificantly more defensive driving style than their own. Interestingly, they\nprefer the style they think is their own, even though their actual driving\nstyle tends to be more aggressive. We also find that preferences do depend on\nthe specific driving scenario, opening the door for new ways of learning\ndriving style preference.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 20:24:40 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Basu", "Chandrayee", ""], ["Yang", "Qian", ""], ["Hungerman", "David", ""], ["Singhal", "Mukesh", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1802.01713", "submitter": "Sneha Mehta", "authors": "Sneha Mehta", "title": "Spot that Bird: A Location Based Bird Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's age of pervasive computing and social media people make extensive\nuse of technology for communicating, sharing media and learning. Yet while in\nthe outdoors, on a hike or a trail we find ourselves inept of information about\nthe natural world surrounding us. In this paper I present in detail the design\nand technological considerations required to build a location based mobile\napplication for learning about the avian taxonomy present in the user's\nsurroundings. It is designed to be a game for better engagement and learning.\nThe application makes suggestions for birds likely to be sighted in the\nvicinity of the user and requires the user to spot those birds and upload a\nphotograph to the system. If spotted correctly the user scores points. I also\ndiscuss some design methods and evaluation approaches for the application.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 22:17:36 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Mehta", "Sneha", ""]]}, {"id": "1802.01744", "submitter": "Siddharth Reddy", "authors": "Siddharth Reddy, Anca D. Dragan, Sergey Levine", "title": "Shared Autonomy via Deep Reinforcement Learning", "comments": "Accepted to the Robotics: Science and Systems (RSS) 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In shared autonomy, user input is combined with semi-autonomous control to\nachieve a common goal. The goal is often unknown ex-ante, so prior work enables\nagents to infer the goal from user input and assist with the task. Such methods\ntend to assume some combination of knowledge of the dynamics of the\nenvironment, the user's policy given their goal, and the set of possible goals\nthe user might target, which limits their application to real-world scenarios.\nWe propose a deep reinforcement learning framework for model-free shared\nautonomy that lifts these assumptions. We use human-in-the-loop reinforcement\nlearning with neural network function approximation to learn an end-to-end\nmapping from environmental observation and user input to agent action values,\nwith task reward as the only form of supervision. This approach poses the\nchallenge of following user commands closely enough to provide the user with\nreal-time action feedback and thereby ensure high-quality user input, but also\ndeviating from the user's actions when they are suboptimal. We balance these\ntwo needs by discarding actions whose values fall below some threshold, then\nselecting the remaining action closest to the user's input. Controlled studies\nwith users (n = 12) and synthetic pilots playing a video game, and a pilot\nstudy with users (n = 4) flying a real quadrotor, demonstrate the ability of\nour algorithm to assist users with real-time control tasks in which the agent\ncannot directly access the user's private information through observations, but\nreceives a reward signal and user input that both depend on the user's intent.\nThe agent learns to assist the user without access to this private information,\nimplicitly inferring it from the user's input. This paper is a proof of concept\nthat illustrates the potential for deep reinforcement learning to enable\nflexible and practical assistive systems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 00:45:12 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 03:12:34 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Reddy", "Siddharth", ""], ["Dragan", "Anca D.", ""], ["Levine", "Sergey", ""]]}, {"id": "1802.01780", "submitter": "Jaime Fisac", "authors": "Chang Liu, Jessica B. Hamrick, Jaime F. Fisac, Anca D. Dragan, J. Karl\n  Hedrick, S. Shankar Sastry, Thomas L. Griffiths", "title": "Goal Inference Improves Objective and Perceived Performance in\n  Human-Robot Collaboration", "comments": "Published at the International Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS 2016)", "journal-ref": "C. Liu, J. Hamrick, J. Fisac, A. Dragan, J. K. Hedrick, S. Sastry,\n  T. Griffiths. \"Goal Inference Improves Objective and Perceived Performance in\n  Human-Robot Collaboration\". Autonomous Agents and Multiagent Systems (AAMAS),\n  2016", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of human-robot interaction is fundamental to the design and use of\nrobotics in real-world applications. Robots will need to predict and adapt to\nthe actions of human collaborators in order to achieve good performance and\nimprove safety and end-user adoption. This paper evaluates a human-robot\ncollaboration scheme that combines the task allocation and motion levels of\nreasoning: the robotic agent uses Bayesian inference to predict the next goal\nof its human partner from his or her ongoing motion, and re-plans its own\nactions in real time. This anticipative adaptation is desirable in many\npractical scenarios, where humans are unable or unwilling to take on the\ncognitive overhead required to explicitly communicate their intent to the\nrobot. A behavioral experiment indicates that the combination of goal inference\nand dynamic task planning significantly improves both objective and perceived\nperformance of the human-robot team. Participants were highly sensitive to the\ndifferences between robot behaviors, preferring to work with a robot that\nadapted to their actions over one that did not.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 03:31:23 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Liu", "Chang", ""], ["Hamrick", "Jessica B.", ""], ["Fisac", "Jaime F.", ""], ["Dragan", "Anca D.", ""], ["Hedrick", "J. Karl", ""], ["Sastry", "S. Shankar", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1802.01924", "submitter": "Nikolaos K Tselios", "authors": "Christos Katsanos, Michalis Xenos and Nikolaos Tselios", "title": "Tool-mediated HCI Modeling Instruction in a Campus_based Software\n  Quality Course", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Keystroke Level Model (KLM) and Fitts Law constitute core teaching\nsubjects in most HCI courses, as well as many courses on software design and\nevaluation. The KLM Form Analyzer (KLM_FA) has been introduced as a\npractitioner s tool to facilitate web form design and evaluation, based on\nthese established HCI predictive models. It was also hypothesized that KLMFA\ncan also be used for educational purposes, since it provides step by step\ntracing of the KLM modeling for any web form filling task, according to various\ninteraction strategies or users characteristics. In our previous work, we found\nthat KLM-FA supports teaching and learning of HCI modeling in the context of\ndistance education. This paper reports a study investigating the learning\neffectiveness of KLM-FA in the context of campus-based higher education.\nStudents of a software quality course completed a knowledge test after the\nlecture- based instruction (pre-test condition) and after being involved in a\nKLMFA mediated learning activity (post-test condition). They also provided\nposttest ratings for their educational experience and the tool s usability.\nResults showed that KLM-FA can significantly improve learning of the HCI\nmodeling. In addition, participating students rated their perceived educational\nexperience as very satisfactory and the perceived usability of KLM-FA as good\nto excellent.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 13:07:09 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Katsanos", "Christos", ""], ["Xenos", "Michalis", ""], ["Tselios", "Nikolaos", ""]]}, {"id": "1802.02223", "submitter": "Song-Hwa Kwon", "authors": "Song-Hwa Kwon and Hyeong In Choi and Sung Jin Lee and Nam-Sook Wee", "title": "Seeded Ising Model and Statistical Natures of Human Iris Templates", "comments": "7 pages", "journal-ref": "Phys. Rev. E 98, 032115 (2018)", "doi": "10.1103/PhysRevE.98.032115", "report-no": null, "categories": "stat.AP cs.CV cs.HC physics.data-an q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a variant of Ising model, called the Seeded Ising Model, to model\nprobabilistic nature of human iris templates. This model is an Ising model in\nwhich the values at certain lattice points are held fixed throughout Ising\nmodel evolution. Using this we show how to reconstruct the full iris template\nfrom partial information, and we show that about 1/6 of the given template is\nneeded to recover almost all information content of the original one in the\nsense that the resulting Hamming distance is well within the range to assert\ncorrectly the identity of the subject. This leads us to propose the concept of\neffective statistical degree of freedom of iris templates and show it is about\n1/6 of the total number of bits. In particular, for a template of $2048$ bits,\nits effective statistical degree of freedom is about $342$ bits, which\ncoincides very well with the degree of freedom computed by the completely\ndifferent method proposed by Daugman.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 02:32:18 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Kwon", "Song-Hwa", ""], ["Choi", "Hyeong In", ""], ["Lee", "Sung Jin", ""], ["Wee", "Nam-Sook", ""]]}, {"id": "1802.02561", "submitter": "Hamza Harkous", "authors": "Hamza Harkous, Kassem Fawaz, R\\'emi Lebret, Florian Schaub, Kang G.\n  Shin, Karl Aberer", "title": "Polisis: Automated Analysis and Presentation of Privacy Policies Using\n  Deep Learning", "comments": "Published at USENIX Security 2018; associated website:\n  https://pribot.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy policies are the primary channel through which companies inform users\nabout their data collection and sharing practices. These policies are often\nlong and difficult to comprehend. Short notices based on information extracted\nfrom privacy policies have been shown to be useful but face a significant\nscalability hurdle, given the number of policies and their evolution over time.\nCompanies, users, researchers, and regulators still lack usable and scalable\ntools to cope with the breadth and depth of privacy policies. To address these\nhurdles, we propose an automated framework for privacy policy analysis\n(Polisis). It enables scalable, dynamic, and multi-dimensional queries on\nnatural language privacy policies. At the core of Polisis is a privacy-centric\nlanguage model, built with 130K privacy policies, and a novel hierarchy of\nneural-network classifiers that accounts for both high-level aspects and\nfine-grained details of privacy practices. We demonstrate Polisis' modularity\nand utility with two applications supporting structured and free-form querying.\nThe structured querying application is the automated assignment of privacy\nicons from privacy policies. With Polisis, we can achieve an accuracy of 88.4%\non this task. The second application, PriBot, is the first freeform\nquestion-answering system for privacy policies. We show that PriBot can produce\na correct answer among its top-3 results for 82% of the test questions. Using\nan MTurk user study with 700 participants, we show that at least one of\nPriBot's top-3 answers is relevant to users for 89% of the test questions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:36:38 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 09:27:17 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Harkous", "Hamza", ""], ["Fawaz", "Kassem", ""], ["Lebret", "R\u00e9mi", ""], ["Schaub", "Florian", ""], ["Shin", "Kang G.", ""], ["Aberer", "Karl", ""]]}, {"id": "1802.02565", "submitter": "Johannes Wagner", "authors": "Johannes Wagner, Tobias Baur, Yue Zhang, Michel F. Valstar, Bj\\\"orn\n  Schuller, Elisabeth Andr\\'e", "title": "Applying Cooperative Machine Learning to Speed Up the Annotation of\n  Social Signals in Large Multi-modal Corpora", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific disciplines, such as Behavioural Psychology, Anthropology and\nrecently Social Signal Processing are concerned with the systematic exploration\nof human behaviour. A typical work-flow includes the manual annotation (also\ncalled coding) of social signals in multi-modal corpora of considerable size.\nFor the involved annotators this defines an exhausting and time-consuming task.\nIn the article at hand we present a novel method and also provide the tools to\nspeed up the coding procedure. To this end, we suggest and evaluate the use of\nCooperative Machine Learning (CML) techniques to reduce manual labelling\nefforts by combining the power of computational capabilities and human\nintelligence. The proposed CML strategy starts with a small number of labelled\ninstances and concentrates on predicting local parts first. Afterwards, a\nsession-independent classification model is created to finish the remaining\nparts of the database. Confidence values are computed to guide the manual\ninspection and correction of the predictions. To bring the proposed approach\ninto application we introduce NOVA - an open-source tool for collaborative and\nmachine-aided annotations. In particular, it gives labellers immediate access\nto CML strategies and directly provides visual feedback on the results. Our\nexperiments show that the proposed method has the potential to significantly\nreduce human labelling efforts.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 18:47:49 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Wagner", "Johannes", ""], ["Baur", "Tobias", ""], ["Zhang", "Yue", ""], ["Valstar", "Michel F.", ""], ["Schuller", "Bj\u00f6rn", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "1802.02952", "submitter": "Shoaib Ahmed Siddiqui", "authors": "Shoaib Ahmed Siddiqui, Dominik Mercier, Mohsin Munir, Andreas Dengel,\n  Sheraz Ahmed", "title": "TSViz: Demystification of Deep Learning Models for Time-Series Analysis", "comments": "7 Pages (6 + 1 for references), 7 figures", "journal-ref": "IEEE Access 2019 PP(99):1-1", "doi": "10.1109/ACCESS.2019.2912823", "report-no": "ACCESS.2019.2912823", "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel framework for demystification of convolutional\ndeep learning models for time-series analysis. This is a step towards making\ninformed/explainable decisions in the domain of time-series, powered by deep\nlearning. There have been numerous efforts to increase the interpretability of\nimage-centric deep neural network models, where the learned features are more\nintuitive to visualize. Visualization in time-series domain is much more\ncomplicated as there is no direct interpretation of the filters and inputs as\ncompared to the image modality. In addition, little or no concentration has\nbeen devoted for the development of such tools in the domain of time-series in\nthe past. TSViz provides possibilities to explore and analyze a network from\ndifferent dimensions at different levels of abstraction which includes\nidentification of parts of the input that were responsible for a prediction\n(including per filter saliency), importance of different filters present in the\nnetwork for a particular prediction, notion of diversity present in the network\nthrough filter clustering, understanding of the main sources of variation\nlearnt by the network through inverse optimization, and analysis of the\nnetwork's robustness against adversarial noise. As a sanity check for the\ncomputed influence values, we demonstrate results regarding pruning of neural\nnetworks based on the computed influence information. These representations\nallow to understand the network features so that the acceptability of deep\nnetworks for time-series data can be enhanced. This is extremely important in\ndomains like finance, industry 4.0, self-driving cars, health-care,\ncounter-terrorism etc., where reasons for reaching a particular prediction are\nequally important as the prediction itself. We assess the proposed framework\nfor interpretability with a set of desirable properties essential for any\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 16:29:49 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 23:22:53 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 07:53:44 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Siddiqui", "Shoaib Ahmed", ""], ["Mercier", "Dominik", ""], ["Munir", "Mohsin", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "1802.03051", "submitter": "Garrett Goodman", "authors": "Garrett Goodman, Tanvi Banerjee, William Romine, Cogan Shimizu,\n  Jennifer Hughes", "title": "Caregiver Assessment Using Smart Gaming Technology: A Preliminary\n  Approach", "comments": "7 pages, 1 figures, 6 tables", "journal-ref": null, "doi": "10.1109/FUZZ-IEEE.2019.8858868", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As pre-diagnostic technologies are becoming increasingly accessible, using\nthem to improve the quality of care available to dementia patients and their\ncaregivers is of increasing interest. Specifically, we aim to develop a tool\nfor non-invasively assessing task performance in a simple gaming application.\nTo address this, we have developed Caregiver Assessment using Smart Gaming\nTechnology (CAST), a mobile application that personalizes a traditional word\nscramble game. Its core functionality uses a Fuzzy Inference System (FIS)\noptimized via a Genetic Algorithm (GA) to provide customized performance\nmeasures for each user of the system. With CAST, we match the relative level of\ndifficulty of play using the individual's ability to solve the word scramble\ntasks. We provide an analysis of the preliminary results for determining task\ndifficulty, with respect to our current participant cohort.\n", "versions": [{"version": "v1", "created": "Thu, 8 Feb 2018 21:25:29 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 18:29:21 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Goodman", "Garrett", ""], ["Banerjee", "Tanvi", ""], ["Romine", "William", ""], ["Shimizu", "Cogan", ""], ["Hughes", "Jennifer", ""]]}, {"id": "1802.03109", "submitter": "Carlos Toxtli", "authors": "Carlos Toxtli, Andr\\'es Monroy-Hern\\'andez, Justin Cranshaw", "title": "Understanding Chatbot-mediated Task Management", "comments": "5 pages, 2 figures, CHI 2018", "journal-ref": null, "doi": "10.1145/3173574.3173632", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Effective task management is essential to successful team collaboration.\nWhile the past decade has seen considerable innovation in systems that track\nand manage group tasks, these innovations have typically been outside of the\nprincipal communication channels: email, instant messenger, and group chat.\nTeams formulate, discuss, refine, assign, and track the progress of their\ncollaborative tasks over electronic communication channels, yet they must leave\nthese channels to update their task-tracking tools, creating a source of\nfriction and inefficiency. To address this problem, we explore how bots might\nbe used to mediate task management for individuals and teams. We deploy a\nprototype bot to eight different teams of information workers to help them\ncreate, assign, and keep track of tasks, all within their main communication\nchannel. We derived seven insights for the design of future bots for\ncoordinating work.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 03:15:33 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Toxtli", "Carlos", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""], ["Cranshaw", "Justin", ""]]}, {"id": "1802.03113", "submitter": "Araz Taeihagh", "authors": "Araz Taeihagh", "title": "Crowdsourcing: a new tool for policy-making?", "comments": null, "journal-ref": "Policy Sciences Journal, 50(4):629-647 (2017)", "doi": "10.1007/s11077-017-9303-3", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is rapidly evolving and applied in situations where ideas,\nlabour, opinion or expertise of large groups of people are used. Crowdsourcing\nis now used in various policy-making initiatives; however, this use has usually\nfocused on open collaboration platforms and specific stages of the policy\nprocess, such as agenda-setting and policy evaluations. Other forms of\ncrowdsourcing have been neglected in policy-making, with a few exceptions. This\narticle examines crowdsourcing as a tool for policy-making, and explores the\nnuances of the technology and its use and implications for different stages of\nthe policy process. The article addresses questions surrounding the role of\ncrowdsourcing and whether it can be considered as a policy tool or as a\ntechnological enabler and investigates the current trends and future directions\nof crowdsourcing.\n  Keywords: Crowdsourcing, Public Policy, Policy Instrument, Policy Tool,\nPolicy Process, Policy Cycle, Open Collaboration, Virtual Labour Markets,\nTournaments, Competition.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 03:34:15 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Taeihagh", "Araz", ""]]}, {"id": "1802.03486", "submitter": "Ziyi Chen", "authors": "Ziyi Chen", "title": "An LSTM Recurrent Network for Step Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones with sensors such as accelerometer and gyroscope can be used as\npedometers and navigators. In this paper, we propose to use an LSTM recurrent\nnetwork for counting the number of steps taken by both blind and sighted users,\nbased on an annotated smartphone sensor dataset, WeAllWork. The models were\ntrained separately for sighted people, blind people with a long cane or a guide\ndog for Leave-One-Out training modality. It achieved 5% overcount and\nundercount rate.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 00:44:20 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Chen", "Ziyi", ""]]}, {"id": "1802.03852", "submitter": "Ruck Thawonmas", "authors": "Yunshi Liu, Pujana Paliyawan, Takahiro Kusano, Tomohiro Harada and\n  Ruck Thawonmas", "title": "A Personalized Method for Calorie Consumption Assessment", "comments": "The AAAI 2018 Spring Symposium on Beyond Machine Intelligence:\n  Understanding Cognitive Bias and Humanity for Well-Being, March 26-28, 2018,\n  Stanford University, Palo Alto, California USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an image-processing-based method for personalization of\ncalorie consumption assessment during exercising. An experiment is carried out\nwhere several actions are required in an exercise called broadcast gymnastics,\nespecially popular in Japan and China. We use Kinect, which captures body\nactions by separating the body into joints and segments that contain them, to\nmonitor body movements to test the velocity of each body joint and capture the\nsubject's image for calculating the mass of each body joint that differs for\neach subject. By a kinetic energy formula, we obtain the kinetic energy of each\nbody joint, and calories consumed during exercise are calculated in this\nprocess. We evaluate the performance of our method by benchmarking it to\nFitbit, a smart watch well-known for health monitoring during exercise. The\nexperimental results in this paper show that our method outperforms a\nstate-of-the-art calorie assessment method, which we base on and improve, in\nterms of the error rate from Fitbit's ground-truth values.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 00:54:30 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Liu", "Yunshi", ""], ["Paliyawan", "Pujana", ""], ["Kusano", "Takahiro", ""], ["Harada", "Tomohiro", ""], ["Thawonmas", "Ruck", ""]]}, {"id": "1802.03996", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Xianzhi Wang, Wenjie Zhang, Shuai Zhang, Yunhao\n  Liu", "title": "Know Your Mind: Adaptive Brain Signal Classification with Reinforced\n  Attentive Convolutional Neural Networks", "comments": "10 pages, Accepted by ICDM 2019 as full paper", "journal-ref": "International Conference on Data Mining (ICDM 2019) as full paper", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) signals reflect activities on certain brain\nareas. Effective classification of time-varying EEG signals is still\nchallenging. First, EEG signal processing and feature engineering are\ntime-consuming and highly rely on expert knowledge. In addition, most existing\nstudies focus on domain-specific classification algorithms which may not be\napplicable to other domains. Moreover, the EEG signal usually has a low\nsignal-to-noise ratio and can be easily corrupted. In this regard, we propose a\ngeneric EEG signal classification framework that accommodates a wide range of\napplications to address the aforementioned issues. The proposed framework\ndevelops a reinforced selective attention model to automatically choose the\ndistinctive information among the raw EEG signals. A convolutional mapping\noperation is employed to dynamically transform the selected information to an\nover-complete feature space, wherein implicit spatial dependency of EEG samples\ndistribution is able to be uncovered. We demonstrate the effectiveness of the\nproposed framework using three representative scenarios: intention recognition\nwith motor imagery EEG, person identification, and neurological diagnosis.\nThree widely used public datasets and a local dataset are used for our\nevaluation. The experiments show that our framework outperforms the\nstate-of-the-art baselines and achieves the accuracy of more than 97% on all\nthe datasets with low latency and good resilience of handling complex EEG\nsignals across various domains. These results confirm the suitability of the\nproposed generic approach for a range of problems in the realm of\nBrain-Computer Interface applications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 11:59:40 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 05:13:58 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 10:30:50 GMT"}, {"version": "v4", "created": "Mon, 19 Aug 2019 07:07:11 GMT"}, {"version": "v5", "created": "Sat, 24 Aug 2019 02:52:34 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Zhang", "Wenjie", ""], ["Zhang", "Shuai", ""], ["Liu", "Yunhao", ""]]}, {"id": "1802.04090", "submitter": "Christos Katsanos", "authors": "Alexandros Liapis, Christos Katsanos, Michalis Xenos", "title": "Don't Leave Me Alone: Retrospective Think Aloud supported by Real-time\n  Monitoring of Participant's Physiology", "comments": "International Conference on Human-Computer Interaction (HCII), 11\n  pages, 3 figures", "journal-ref": null, "doi": "10.1007/978-3-319-91238-7_10", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Think aloud protocols are widely applied in user experience studies. In this\npaper, the effect of two different applications of the Retrospective Think\nAloud (RTA) protocol on the number of user-reported usability issues is\nexamined. To this end, 30 users were asked to use the National Cadastre and\nMapping Agency web application and complete a set of tasks, such as measuring\nthe land area of a square in their hometown. The order of tasks was randomized\nper participant. Next, participants were involved in RTA sessions. Each\nparticipant was involved in two different RTA modes: (a) the strict guidance,\nin which the facilitator stayed in the background and prompted participants to\nkeep thinking aloud based on his judgement and experience, and (b) the\nphysiology-supported interventions, in which the facilitator intervened based\non real-time monitoring of user's physiological signals. During each session,\nthree participant's physiological signals were recorded: skin conductance, skin\ntemperature and blood volume pulse. Participants were also asked to provide\nvalence-arousal ratings for each self-reported usability issue. Analysis of the\ncollected data showed that participants in the physiology-supported RTA mode\nreported significantly more usability issues. No significant effect of the RTA\nmode was found on the va-lence-arousal ratings for the reported usability\nissues. Participants' physiological signals during the RTA sessions did not\nalso differ significantly between the two modes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 14:58:52 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Liapis", "Alexandros", ""], ["Katsanos", "Christos", ""], ["Xenos", "Michalis", ""]]}, {"id": "1802.04143", "submitter": "Araz Taeihagh", "authors": "John Prpic, Araz Taeihagh, and James Melton", "title": "The Fundamentals of Policy Crowdsourcing", "comments": "arXiv admin note: substantial text overlap with arXiv:1702.04213", "journal-ref": "Policy & Internet, 7: 340-361 (2015)", "doi": "10.1002/poi3.102", "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the state of the research on crowdsourcing for policy making? This\narticle begins to answer this question by collecting, categorizing, and\nsituating an extensive body of the extant research investigating policy\ncrowdsourcing, within a new framework built on fundamental typologies from each\nfield. We first define seven universal characteristics of the three general\ncrowdsourcing techniques (virtual labor markets, tournament crowdsourcing, open\ncollaboration), to examine the relative trade-offs of each modality. We then\ncompare these three types of crowdsourcing to the different stages of the\npolicy cycle, in order to situate the literature spanning both domains. We\nfinally discuss research trends in crowdsourcing for public policy, and\nhighlight the research gaps and overlaps in the literature.\n  KEYWORDS: crowdsourcing, policy cycle, crowdsourcing trade-offs, policy\nprocesses, policy stages, virtual labor markets, tournament crowdsourcing, open\ncollaboration\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 03:41:41 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Prpic", "John", ""], ["Taeihagh", "Araz", ""], ["Melton", "James", ""]]}, {"id": "1802.04236", "submitter": "Shayan Eskandari", "authors": "Shayan Eskandari, Jeremy Clark, Abdelwahab Hamou-Lhadj", "title": "Buy your coffee with bitcoin: Real-world deployment of a bitcoin point\n  of sale terminal", "comments": "Advanced and Trusted Computing 2016 Intl IEEE Conferences, 8 pages", "journal-ref": null, "doi": "10.1109/UIC-ATC-ScalCom-CBDCom-IoP-SmartWorld.2016.0073", "report-no": null, "categories": "cs.CR cs.CY cs.ET cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss existing approaches for Bitcoin payments, as\nsuitable for a small business for small-value transactions. We develop an\nevaluation framework utilizing security, usability, deployability criteria,,\nexamine several existing systems, tools. Following a requirements engineering\napproach, we designed, implemented a new Point of Sale (PoS) system that\nsatisfies an optimal set of criteria within our evaluation framework. Our open\nsource system, Aunja PoS, has been deployed in a real world cafe since October\n2014.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 18:38:35 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Eskandari", "Shayan", ""], ["Clark", "Jeremy", ""], ["Hamou-Lhadj", "Abdelwahab", ""]]}, {"id": "1802.04286", "submitter": "Iacopo Pozzana", "authors": "Iacopo Pozzana, Emilio Ferrara", "title": "Measuring bot and human behavioral dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bots, social media accounts controlled by software rather than by humans,\nhave recently been under the spotlight for their association with various forms\nof online manipulation. To date, much work has focused on social bot detection,\nbut little attention has been devoted to the characterization and measurement\nof the behavior and activity of bots, as opposed to humans'. Over the course of\nthe years, bots have become more sophisticated, and capable to reflect some\nshort-term behavior, emulating that of human users. The goal of this paper is\nto study the behavioral dynamics that bots exhibit over the course of one\nactivity session, and highlight if and how these differ from human activity\nsignatures. By using a large Twitter dataset associated with recent political\nevents, we first separate bots and humans, then isolate their activity\nsessions. We compile a list of quantities to be measured, like the propensity\nof users to engage in social interactions or to produce content. Our analysis\nhighlights the presence of short-term behavioral trends in humans, which can be\nassociated with a cognitive origin, that are absent in bots, intuitively due to\ntheir automated activity. These findings are finally codified to create and\nevaluate a machine learning algorithm to detect activity sessions produced by\nbots and humans, to allow for more nuanced bot detection strategies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 19:00:12 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 17:55:13 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Pozzana", "Iacopo", ""], ["Ferrara", "Emilio", ""]]}, {"id": "1802.04351", "submitter": "Shayan Eskandari", "authors": "Shayan Eskandari, Jeremy Clark, David Barrera, Elizabeth Stobert", "title": "A first look at the usability of bitcoin key management", "comments": "10 Pages, USEC 15: NDSS Workshop on Usable Security (USEC) 2015, San\n  Diego, CA, USA, February 8, 2015, Internet Society", "journal-ref": null, "doi": "10.14722/usec.2015.23015", "report-no": null, "categories": "cs.CR cs.CY cs.ET cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin users are directly or indirectly forced to deal with public key\ncryptography, which has a number of security and usability challenges that\ndiffer from the password-based authentication underlying most online banking\nservices. Users must ensure that keys are simultaneously accessible, resistant\nto digital theft and resilient to loss. In this paper, we contribute an\nevaluation framework for comparing Bitcoin key management approaches, and\nconduct a broad usability evaluation of six representative Bitcoin clients. We\nfind that Bitcoin shares many of the fundamental challenges of key management\nknown from other domains, but that Bitcoin may present a unique opportunity to\nrethink key management for end users.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 20:38:04 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Eskandari", "Shayan", ""], ["Clark", "Jeremy", ""], ["Barrera", "David", ""], ["Stobert", "Elizabeth", ""]]}, {"id": "1802.04480", "submitter": "Eduardo Castell\\'o Ferrer", "authors": "Eduardo Castell\\'o Ferrer, Ognjen Rudovic, Thomas Hardjono, Alex\n  Pentland", "title": "RoboChain: A Secure Data-Sharing Framework for Human-Robot Interaction", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Robots have potential to revolutionize the way we interact with the world\naround us. One of their largest potentials is in the domain of mobile health\nwhere they can be used to facilitate clinical interventions. However, to\naccomplish this, robots need to have access to our private data in order to\nlearn from these data and improve their interaction capabilities. Furthermore,\nto enhance this learning process, the knowledge sharing among multiple robot\nunits is the natural step forward. However, to date, there is no\nwell-established framework which allows for such data sharing while preserving\nthe privacy of the users (e.g., the hospital patients). To this end, we\nintroduce RoboChain - the first learning framework for secure, decentralized\nand computationally efficient data and model sharing among multiple robot units\ninstalled at multiple sites (e.g., hospitals). RoboChain builds upon and\ncombines the latest advances in open data access and blockchain technologies,\nas well as machine learning. We illustrate this framework using the example of\na clinical intervention conducted in a private network of hospitals.\nSpecifically, we lay down the system architecture that allows multiple robot\nunits, conducting the interventions at different hospitals, to perform\nefficient learning without compromising the data privacy.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 06:56:30 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 20:36:02 GMT"}, {"version": "v3", "created": "Mon, 26 Mar 2018 19:27:11 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ferrer", "Eduardo Castell\u00f3", ""], ["Rudovic", "Ognjen", ""], ["Hardjono", "Thomas", ""], ["Pentland", "Alex", ""]]}, {"id": "1802.04551", "submitter": "Hideaki Imamura", "authors": "Hideaki Imamura and Issei Sato and Masashi Sugiyama", "title": "Analysis of Minimax Error Rate for Crowdsourcing and Its Application to\n  Worker Clustering Model", "comments": "Accepted to ICML2018 (International Conference on Machine Learning)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While crowdsourcing has become an important means to label data, there is\ngreat interest in estimating the ground truth from unreliable labels produced\nby crowdworkers. The Dawid and Skene (DS) model is one of the most well-known\nmodels in the study of crowdsourcing. Despite its practical popularity,\ntheoretical error analysis for the DS model has been conducted only under\nrestrictive assumptions on class priors, confusion matrices, or the number of\nlabels each worker provides. In this paper, we derive a minimax error rate\nunder more practical setting for a broader class of crowdsourcing models\nincluding the DS model as a special case. We further propose the worker\nclustering model, which is more practical than the DS model under real\ncrowdsourcing settings. The wide applicability of our theoretical analysis\nallows us to immediately investigate the behavior of this proposed model, which\ncan not be analyzed by existing studies. Experimental results showed that there\nis a strong similarity between the lower bound of the minimax error rate\nderived by our theoretical analysis and the empirical error of the estimated\nvalue.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 10:47:14 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 05:35:35 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Imamura", "Hideaki", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1802.04877", "submitter": "Natasha Jaques", "authors": "Natasha Jaques, Jennifer McCleary, Jesse Engel, David Ha, Fred\n  Bertsch, Rosalind Picard, Douglas Eck", "title": "Learning via social awareness: Improving a deep generative sketching\n  model with facial feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the quest towards general artificial intelligence (AI), researchers have\nexplored developing loss functions that act as intrinsic motivators in the\nabsence of external rewards. This paper argues that such research has\noverlooked an important and useful intrinsic motivator: social interaction. We\nposit that making an AI agent aware of implicit social feedback from humans can\nallow for faster learning of more generalizable and useful representations, and\ncould potentially impact AI safety. We collect social feedback in the form of\nfacial expression reactions to samples from Sketch RNN, an LSTM-based\nvariational autoencoder (VAE) designed to produce sketch drawings. We use a\nLatent Constraints GAN (LC-GAN) to learn from the facial feedback of a small\ngroup of viewers, by optimizing the model to produce sketches that it predicts\nwill lead to more positive facial expressions. We show in multiple independent\nevaluations that the model trained with facial feedback produced sketches that\nare more highly rated, and induce significantly more positive facial\nexpressions. Thus, we establish that implicit social feedback can improve the\noutput of a deep learning model.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:19:10 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 18:45:48 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Jaques", "Natasha", ""], ["McCleary", "Jennifer", ""], ["Engel", "Jesse", ""], ["Ha", "David", ""], ["Bertsch", "Fred", ""], ["Picard", "Rosalind", ""], ["Eck", "Douglas", ""]]}, {"id": "1802.04915", "submitter": "Shayan Eskandari", "authors": "Shayan Eskandari, Jeremy Clark, Vignesh Sundaresan, Moe Adham", "title": "On the Feasibility of Decentralized Derivatives Markets", "comments": "15 pages, 1st Workshop on Trusted Smart Contracts In Association with\n  Financial Cryptography 17 April 07, 2017", "journal-ref": "International Conference on Financial Cryptography and Data\n  Security FC 2017: Financial Cryptography and Data Security pp 553-567", "doi": "10.1007/978-3-319-70278-0_35", "report-no": null, "categories": "cs.CR cs.CY cs.ET cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Velocity, a decentralized market deployed on\nEthereum for trading a custom type of derivative option. To enable the smart\ncontract to work, we also implement a price fetching tool called PriceGeth. We\npresent this as a case study, noting challenges in development of the system\nthat might be of independent interest to whose working on smart contract\nimplementations. We also apply recent academic results on the security of the\nSolidity smart contract language in validating our codes security. Finally, we\ndiscuss more generally the use of smart contracts in modelling financial\nderivatives.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 01:27:41 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Eskandari", "Shayan", ""], ["Clark", "Jeremy", ""], ["Sundaresan", "Vignesh", ""], ["Adham", "Moe", ""]]}, {"id": "1802.04995", "submitter": "Jeremy Frey", "authors": "J\\'er\\'emy Frey (IDC), May Grabli, Ronit Slyper (IDC), Jessica\n  Cauchard (IDC)", "title": "Breeze: Sharing Biofeedback Through Wearable Technologies", "comments": null, "journal-ref": "CHI '18 - SIGCHI Conference on Human Factors in Computing System,\n  Apr 2018, Montreal, Canada. 2018, https://chi2018.acm.org/", "doi": "10.1145/3173574.3174219", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitally presenting physiological signals as biofeedback to users raises\nawareness of both body and mind. This paper describes the effectiveness of\nconveying a physiological signal often overlooked for communication: breathing.\nWe present the design and development of digital breathing patterns and their\nevaluation along three output modalities: visual, audio, and haptic. We also\npresent Breeze, a wearable pendant placed around the neck that measures\nbreathing and sends biofeedback in real-time. We evaluated how the breathing\npatterns were interpreted in a fixed environment and gathered qualitative data\non the wearable device's design. We found that participants intentionally\nmodified their own breathing to match the biofeedback, as a technique for\nunderstanding the underlying emotion. Our results describe how the features of\nthe breathing patterns and the feedback modalities influenced participants'\nperception. We include guidelines and suggested use cases, such as Breeze being\nused by loved ones to increase connectedness and empathy.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 09:12:31 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "IDC"], ["Grabli", "May", "", "IDC"], ["Slyper", "Ronit", "", "IDC"], ["Cauchard", "Jessica", "", "IDC"]]}, {"id": "1802.05101", "submitter": "James Bagrow", "authors": "James P. Bagrow", "title": "Democratizing AI: Non-expert design of prediction tasks", "comments": "17 pages, 6 figures", "journal-ref": "PeerJ Computer Science, 6: e296, 2020", "doi": "10.7717/peerj-cs.296", "report-no": null, "categories": "cs.HC cs.AI cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-experts have long made important contributions to machine learning (ML)\nby contributing training data, and recent work has shown that non-experts can\nalso help with feature engineering by suggesting novel predictive features.\nHowever, non-experts have only contributed features to prediction tasks already\nposed by experienced ML practitioners. Here we study how non-experts can design\nprediction tasks themselves, what types of tasks non-experts will design, and\nwhether predictive models can be automatically trained on data sourced for\ntheir tasks. We use a crowdsourcing platform where non-experts design\npredictive tasks that are then categorized and ranked by the crowd.\nCrowdsourced data are collected for top-ranked tasks and predictive models are\nthen trained and evaluated automatically using those data. We show that\nindividuals without ML experience can collectively construct useful datasets\nand that predictive models can be learned on these datasets, but challenges\nremain. The prediction tasks designed by non-experts covered a broad range of\ndomains, from politics and current events to health behavior, demographics, and\nmore. Proper instructions are crucial for non-experts, so we also conducted a\nrandomized trial to understand how different instructions may influence the\ntypes of prediction tasks being proposed. In general, understanding better how\nnon-experts can contribute to ML can further leverage advances in Automatic ML\nand has important implications as ML continues to drive workplace automation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 14:16:13 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 20:04:50 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Bagrow", "James P.", ""]]}, {"id": "1802.05316", "submitter": "Nathan Hilliard", "authors": "Meg Pirrung, Nathan Hilliard, Art\\\"em Yankov, Nancy O'Brien, Paul\n  Weidert, Courtney D Corley, Nathan O Hodas", "title": "Sharkzor: Interactive Deep Learning for Image Triage, Sort and Summary", "comments": "ICML 2017 human-in-the-loop workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharkzor is a web application for machine-learning assisted image sort and\nsummary. Deep learning algorithms are leveraged to infer, augment, and automate\nthe user's mental model. Initially, images uploaded by the user are spread out\non a canvas. The user then interacts with the images to impute their mental\nmodel into the application's algorithmic underpinnings. Methods of interaction\nwithin Sharkzor's user interface and user experience support three primary user\ntasks; triage, organize and automate. The user triages the large pile of\noverlapping images by moving images of interest into proximity. The user then\norganizes said images into meaningful groups. After interacting with the images\nand groups, deep learning helps to automate the user's interactions. The loop\nof interaction, automation, and response by the user allows the system to\nquickly make sense of large amounts of data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 20:48:28 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Pirrung", "Meg", ""], ["Hilliard", "Nathan", ""], ["Yankov", "Art\u00ebm", ""], ["O'Brien", "Nancy", ""], ["Weidert", "Paul", ""], ["Corley", "Courtney D", ""], ["Hodas", "Nathan O", ""]]}, {"id": "1802.05534", "submitter": "Lindah Kotut", "authors": "Lindah Kotut, Michael Horning, Steve Harrison, D. Scott McCrickard", "title": "Opportunity in Conflict: Understanding Tension Among Key Groups on the\n  Trail", "comments": "Workshop Paper Submitted to CHI HCI Outdoors: Understanding\n  Human-Computer Interaction in the Outdoors (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the question of who technology users on the trail are,\nwhat their technological uses and needs are, and what conflicts exist between\ndifferent trail users regarding technology use and experience, toward\nunderstanding how experiences of trail users contribute to designers. We argue\nthat exploring these tensions provide opportunities for design that can be used\nto both mitigate conflicts and improve community on the trail.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 22:07:27 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Kotut", "Lindah", ""], ["Horning", "Michael", ""], ["Harrison", "Steve", ""], ["McCrickard", "D. Scott", ""]]}, {"id": "1802.05568", "submitter": "Bin Guo", "authors": "Yi Ouyang, Bin Guo, Xinjiang Lu, Qi Han, Tong Guo, Zhiwen Yu", "title": "CompetitiveBike: Competitive Prediction of Bike-Sharing Apps Using\n  Heterogeneous Crowdsourced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, bike-sharing systems have been deployed in many cities,\nwhich provide an economical lifestyle. With the prevalence of bike-sharing\nsystems, a lot of companies join the market, leading to increasingly fierce\ncompetition. To be competitive, bike-sharing companies and app developers need\nto make strategic decisions for mobile apps development. Therefore, it is\nsignificant to predict and compare the popularity of different bike-sharing\napps. However, existing works mostly focus on predicting the popularity of a\nsingle app, the popularity contest among different apps has not been explored\nyet. In this paper, we aim to forecast the popularity contest between Mobike\nand Ofo, two most popular bike-sharing apps in China. We develop\nCompetitiveBike, a system to predict the popularity contest among bike-sharing\napps. Moreover, we conduct experiments on real-world datasets collected from 11\napp stores and Sina Weibo, and the experiments demonstrate the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 14:36:09 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Ouyang", "Yi", ""], ["Guo", "Bin", ""], ["Lu", "Xinjiang", ""], ["Han", "Qi", ""], ["Guo", "Tong", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1802.05735", "submitter": "Seyed Ali Cheraghi", "authors": "Seyed Ali Cheraghi, Vinod Namboodiri, Kaushik Sinha", "title": "IBeaconMap: Automated Indoor Space Representation for Beacon-Based\n  Wayfinding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, there have been few options for navigational aids for the\nblind and visually impaired (BVI) in large indoor spaces. Some recent indoor\nnavigation systems allow users equipped with smartphones to interact with low\ncost Bluetoothbased beacons deployed strategically within the indoor space of\ninterest to navigate their surroundings. A major challenge in deploying such\nbeacon-based navigation systems is the need to employ a time and\nlabor-expensive beacon planning process to identify potential beacon placement\nlocations and arrive at a topological structure representing the indoor space.\nThis work presents a technique called IBeaconMap for creating such topological\nstructures to use with beacon-based navigation that only needs the floor plans\nof the indoor spaces of interest. IBeaconMap employs a combination of computer\nvision and machine learning techniques to arrive at the required set of beacon\nlocations and a weighted connectivity graph (with directional orientations) for\nsubsequent navigational needs. Evaluations show IBeaconMap to be both fast and\nreasonably accurate, potentially proving to be an essential tool to be utilized\nbefore mass deployments of beacon-based indoor wayfinding systems of the\nfuture.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 19:58:17 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Cheraghi", "Seyed Ali", ""], ["Namboodiri", "Vinod", ""], ["Sinha", "Kaushik", ""]]}, {"id": "1802.05797", "submitter": "Jaybie de Guzman", "authors": "Jaybie A. de Guzman, Kanchana Thilakarathna, and Aruna Seneviratne", "title": "Security and Privacy Approaches in Mixed Reality: A Literature Survey", "comments": "41 pages, 11 figures, 2 tables (3 tables at the appendix); updated\n  references in page 14", "journal-ref": "ACM Comput. Surv. 52, 6, Article 110 (October 2019), 37 pages", "doi": "10.1145/3359626", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed reality (MR) technology development is now gaining momentum due to\nadvances in computer vision, sensor fusion, and realistic display technologies.\nWith most of the research and development focused on delivering the promise of\nMR, there is only barely a few working on the privacy and security implications\nof this technology. This survey paper aims to put in to light these risks, and\nto look into the latest security and privacy work on MR. Specifically, we list\nand review the different protection approaches that have been proposed to\nensure user and data security and privacy in MR. We extend the scope to include\nwork on related technologies such as augmented reality (AR), virtual reality\n(VR), and human-computer interaction (HCI) as crucial components, if not the\norigins, of MR, as well as numerous related work from the larger area of mobile\ndevices, wearables, and Internet-of-Things (IoT). We highlight the lack of\ninvestigation, implementation, and evaluation of data protection approaches in\nMR. Further challenges and directions on MR security and privacy are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Feb 2018 23:33:45 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 01:55:30 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2020 04:48:27 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["de Guzman", "Jaybie A.", ""], ["Thilakarathna", "Kanchana", ""], ["Seneviratne", "Aruna", ""]]}, {"id": "1802.05892", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg, Sergej Sizov", "title": "Neuroscientific User Models: The Source of Uncertain User Feedback and\n  Potentials for Improving Web Personalisation", "comments": "Bayesian Brain, Neural Coding, Human Uncertainty, Noise, User Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the neuroscientific theory of the Bayesian brain in\nthe light of adaptive web systems and content personalisation. In particular,\nwe elaborate on neural mechanisms of human decision-making and the origin of\nlacking reliability of user feedback, often denoted as noise or human\nuncertainty. To this end, we first introduce an adaptive model of cognitive\nagency in which populations of neurons provide an estimation for states of the\nworld. Subsequently, we present various so-called decoder functions with which\nneuronal activity can be translated into quantitative decisions. The interplay\nof the underlying cognition model and the chosen decoder function leads to\ndifferent model-based properties of decision processes. The goal of this paper\nis to promote novel user models and exploit them to naturally associate users\nto different clusters on the basis of their individual neural characteristics\nand thinking patterns. These user models might be able to turn the variability\nof user behaviour into additional information for improving web personalisation\nand its experience.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 11:08:30 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1802.05895", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg, Sergej Sizov", "title": "Dealing with Uncertainties in User Feedback: Strategies Between Denying\n  and Accepting", "comments": "Human Uncertainty, Noise, Magic Barrier, Ranking Error, User Feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest research revealed a considerable lack of reliability within user\nfeedback and discussed striking impacts for the assessment of adaptive web\nsystems and content personalisation approaches, e.g. ranking errors, systematic\nbiases to accuracy metrics as well as its natural offset (the magic barrier).\nIn order to perform holistic assessments and to improve web systems, a variety\nof strategies have been proposed to deal with this so-called human uncertainty.\nIn this contribution we discuss the most relevant strategies to handle\nuncertain feedback and demonstrate that these approaches are more or less\nineffective to fulfil their objectives. In doing so, we consider human\nuncertainty within a purely probabilistic framework and utilise hypothesis\ntesting as well as a generalisation of the magic barrier to compare the effects\nof recently proposed algorithms. On this basis we recommend a novel strategy of\nacceptance which turns away from mere filtering and discuss potential benefits\nfor the community of the WWW.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 11:15:01 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1802.06024", "submitter": "Sahisnu Mazumder", "authors": "Sahisnu Mazumder, Nianzu Ma and Bing Liu", "title": "Towards a Continuous Knowledge Learning Engine for Chatbots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although chatbots have been very popular in recent years, they still have\nsome serious weaknesses which limit the scope of their applications. One major\nweakness is that they cannot learn new knowledge during the conversation\nprocess, i.e., their knowledge is fixed beforehand and cannot be expanded or\nupdated during conversation. In this paper, we propose to build a general\nknowledge learning engine for chatbots to enable them to continuously and\ninteractively learn new knowledge during conversations. As time goes by, they\nbecome more and more knowledgeable and better and better at learning and\nconversation. We model the task as an open-world knowledge base completion\nproblem and propose a novel technique called lifelong interactive learning and\ninference (LiLi) to solve it. LiLi works by imitating how humans acquire\nknowledge and perform inference during an interactive conversation. Our\nexperimental results show LiLi is highly promising.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 16:50:27 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 06:50:11 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Mazumder", "Sahisnu", ""], ["Ma", "Nianzu", ""], ["Liu", "Bing", ""]]}, {"id": "1802.06180", "submitter": "Bilal Farooq", "authors": "Bilal Farooq, Elisabetta Cherchi, Anae Sobhani", "title": "Virtual Immersive Reality for Stated Preference Travel Behaviour\n  Experiments: A Case Study of Autonomous Vehicles on Urban Roads", "comments": "Accepted in Journal of Transportation Research Record on 16-02-2016", "journal-ref": null, "doi": "10.1177/0361198118776810", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stated preference experiments have been known to suffer from the lack of\nrealism. This issue is particularly visible when the scenario doesn't have a\nwell understood prior reference e.g. in case of the autonomous vehicles related\nscenarios. We present Virtual Immersive Reality Environment (VIRE) that is\ncapable of developing highly realistic, immersive, and interactive choice\nscenario. We demonstrate the use of VIRE in the pedestrian preferences related\nto autonomous vehicles and associated infrastructure changes on urban streets\nof Montr\\'eal. The results are compared with predominantly used approaches i.e.\ntext-only and visual aid. We show that VIRE results in better understanding of\nthe scenario and consistent results.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 03:20:27 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Farooq", "Bilal", ""], ["Cherchi", "Elisabetta", ""], ["Sobhani", "Anae", ""]]}, {"id": "1802.06306", "submitter": "Ziming Li", "authors": "Ziming Li, Julia Kiseleva, Alekh Agarwal, Maarten de Rijke", "title": "Learning Data-Driven Objectives to Optimize Interactive Systems", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective optimization is essential for interactive systems to provide a\nsatisfactory user experience. However, it is often challenging to find an\nobjective to optimize for. Generally, such objectives are manually crafted and\nrarely capture complex user needs in an accurate manner. We propose an approach\nthat infers the objective directly from observed user interactions. These\ninferences can be made regardless of prior knowledge and across different types\nof user behavior. We introduce interactive system optimization, a novel\nalgorithm that uses these inferred objectives for optimization. Our main\ncontribution is a new general principled approach to optimizing interactive\nsystems using data-driven objectives. We demonstrate the high effectiveness of\ninteractive system optimization over several simulations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Feb 2018 23:04:15 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 09:09:57 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 17:24:19 GMT"}, {"version": "v4", "created": "Wed, 2 May 2018 14:45:08 GMT"}, {"version": "v5", "created": "Tue, 8 May 2018 15:33:21 GMT"}, {"version": "v6", "created": "Tue, 16 Oct 2018 15:54:49 GMT"}, {"version": "v7", "created": "Wed, 17 Oct 2018 10:25:51 GMT"}, {"version": "v8", "created": "Fri, 13 Dec 2019 22:11:21 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Li", "Ziming", ""], ["Kiseleva", "Julia", ""], ["Agarwal", "Alekh", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1802.06521", "submitter": "Chang-Shing Lee", "authors": "Chang-Shing Lee, Mei-Hui Wang, Li-Wei Ko, Naoyuki Kubota, Lu-An Lin,\n  Shinya Kitaoka, Yu-Te Wang, and Shun-Feng Su", "title": "Human and Smart Machine Co-Learning with Brain Computer Interface", "comments": "This article will be published in IEEE SMC Magazine, vol. 4, no. 2,\n  2018", "journal-ref": null, "doi": "10.1109/MSMC.2017.2785441", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has become a very popular approach for cybernetics systems,\nand it has always been considered important research in the Computational\nIntelligence area. Nevertheless, when it comes to smart machines, it is not\njust about the methodologies. We need to consider systems and cybernetics as\nwell as include human in the loop. The purpose of this article is as follows:\n(1) To integrate the open source Facebook AI Research (FAIR) DarkForest program\nof Facebook with Item Response Theory (IRT), to the new open learning system,\nnamely, DDF learning system; (2) To integrate DDF Go with Robot namely Robotic\nDDF Go system; (3) To invite the professional Go players to attend the activity\nto play Go games on site with a smart machine. The research team will apply\nthis technology to education, such as, playing games to enhance the children\nconcentration on learning mathematics, languages, and other topics. With the\ndetected brainwaves, the robot will be able to speak some words that are very\nmuch to the point for the students and to assist the teachers in classroom in\nthe future.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 05:42:09 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lee", "Chang-Shing", ""], ["Wang", "Mei-Hui", ""], ["Ko", "Li-Wei", ""], ["Kubota", "Naoyuki", ""], ["Lin", "Lu-An", ""], ["Kitaoka", "Shinya", ""], ["Wang", "Yu-Te", ""], ["Su", "Shun-Feng", ""]]}, {"id": "1802.07292", "submitter": "Massimo Stella", "authors": "Massimo Stella, Emilio Ferrara and Manlio De Domenico", "title": "Bots increase exposure to negative and inflammatory content in online\n  social systems", "comments": "8 pages, 5 figures", "journal-ref": "PNAS 115 (49) 12435-12440 (2018)", "doi": "10.1073/pnas.1803470115", "report-no": null, "categories": "physics.soc-ph cs.CY cs.HC cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Societies are complex systems which tend to polarize into sub-groups of\nindividuals with dramatically opposite perspectives. This phenomenon is\nreflected -- and often amplified -- in online social networks where, however,\nhumans are no more the only players, and co-exist alongside with social bots,\ni.e., software-controlled accounts. Analyzing large-scale social data collected\nduring the Catalan referendum for independence on October 1, 2017, consisting\nof nearly 4 millions Twitter posts generated by almost 1 million users, we\nidentify the two polarized groups of Independentists and Constitutionalists and\nquantify the structural and emotional roles played by social bots. We show that\nbots act from peripheral areas of the social system to target influential\nhumans of both groups, bombarding Independentists with violent contents,\nincreasing their exposure to negative and inflammatory narratives and\nexacerbating social conflict online. Our findings stress the importance of\ndeveloping countermeasures to unmask these forms of automated social\nmanipulation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 19:17:19 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 09:05:00 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Stella", "Massimo", ""], ["Ferrara", "Emilio", ""], ["De Domenico", "Manlio", ""]]}, {"id": "1802.07557", "submitter": "Aljoscha P\\\"ortner", "authors": "Aljoscha P\\\"ortner, Lilian Schr\\\"oder, Robin Rasch, Dennis Sprute,\n  Martin Hoffmann, Matthias K\\\"onig", "title": "The Power of Color: A Study on the Effective Use of Colored Light in\n  Human-Robot Interaction", "comments": null, "journal-ref": null, "doi": "10.1109/IROS.2018.8594231", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In times of more and more complex interaction techniques, we point out the\npowerfulness of colored light as a simple and cheap feedback mechanism. Since\nit is visible over a distance and does not interfere with other modalities, it\nis especially interesting for mobile robots. In an online survey, we asked 56\nparticipants to choose the most appropriate colors for scenarios that were\npresented in the form of videos. In these scenarios a mobile robot accomplished\ntasks, in some with success, in others it failed because the task is not\nfeasible, in others it stopped because it waited for help. We analyze in what\nway the color preferences differ between these three categories. The results\nshow a connection between colors and meanings and that it depends on the\nparticipants' technical affinity, experience with robots and gender how clear\nthe color preference is for a certain category. Finally, we found out that the\nparticipants' favorite color is not related to color preferences.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 13:21:34 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["P\u00f6rtner", "Aljoscha", ""], ["Schr\u00f6der", "Lilian", ""], ["Rasch", "Robin", ""], ["Sprute", "Dennis", ""], ["Hoffmann", "Martin", ""], ["K\u00f6nig", "Matthias", ""]]}, {"id": "1802.07578", "submitter": "Tobias Schnabel", "authors": "Tobias Schnabel, Paul N. Bennett, Thorsten Joachims", "title": "Improving Recommender Systems Beyond the Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems rely heavily on the predictive accuracy of the learning\nalgorithm. Most work on improving accuracy has focused on the learning\nalgorithm itself. We argue that this algorithmic focus is myopic. In\nparticular, since learning algorithms generally improve with more and better\ndata, we propose shaping the feedback generation process as an alternate and\ncomplementary route to improving accuracy. To this effect, we explore how\nchanges to the user interface can impact the quality and quantity of feedback\ndata -- and therefore the learning accuracy. Motivated by information foraging\ntheory, we study how feedback quality and quantity are influenced by interface\ndesign choices along two axes: information scent and information access cost.\nWe present a user study of these interface factors for the common task of\npicking a movie to watch, showing that these factors can effectively shape and\nimprove the implicit feedback data that is generated while maintaining the user\nexperience.\n", "versions": [{"version": "v1", "created": "Wed, 21 Feb 2018 14:13:49 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Schnabel", "Tobias", ""], ["Bennett", "Paul N.", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1802.07852", "submitter": "Siddharth Siddharth", "authors": "Siddharth, Aashish Patel, Tzyy-Ping Jung, and Terrence J. Sejnowski", "title": "An Affordable Bio-Sensing and Activity Tagging Platform for HCI Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel multi-modal bio-sensing platform capable of integrating\nmultiple data streams for use in real-time applications. The system is composed\nof a central compute module and a companion headset. The compute node collects,\ntime-stamps and transmits the data while also providing an interface for a wide\nrange of sensors including electroencephalogram, photoplethysmogram,\nelectrocardiogram, and eye gaze among others. The companion headset contains\nthe gaze tracking cameras. By integrating many of the measurements systems into\nan accessible package, we are able to explore previously unanswerable questions\nranging from open-environment interactions to emotional response studies.\nThough some of the integrated sensors are designed from the ground-up to fit\ninto a compact form factor, we validate the accuracy of the sensors and find\nthat they perform similarly to, and in some cases better than, alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 00:08:42 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Siddharth", "", ""], ["Patel", "Aashish", ""], ["Jung", "Tzyy-Ping", ""], ["Sejnowski", "Terrence J.", ""]]}, {"id": "1802.07954", "submitter": "Fabrice Rossi", "authors": "A. Endert, W. Ribarsky, C. Turkay, W Wong, I. Nabney, I D\\'iaz Blanco,\n  Fabrice Rossi (SAMM)", "title": "The State of the Art in Integrating Machine Learning into Visual\n  Analytics", "comments": null, "journal-ref": "Computer Graphics Forum, Wiley, 2017, 36 (8), pp.458 - 486", "doi": "10.1111/cgf.13092", "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analytics systems combine machine learning or other analytic\ntechniques with interactive data visualization to promote sensemaking and\nanalytical reasoning. It is through such techniques that people can make sense\nof large, complex data. While progress has been made, the tactful combination\nof machine learning and data visualization is still under-explored. This\nstate-of-the-art report presents a summary of the progress that has been made\nby highlighting and synthesizing select research advances. Further, it presents\nopportunities and challenges to enhance the synergy between machine learning\nand visual analytics for impactful future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 09:48:56 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Endert", "A.", "", "SAMM"], ["Ribarsky", "W.", "", "SAMM"], ["Turkay", "C.", "", "SAMM"], ["Wong", "W", "", "SAMM"], ["Nabney", "I.", "", "SAMM"], ["Blanco", "I D\u00edaz", "", "SAMM"], ["Rossi", "Fabrice", "", "SAMM"]]}, {"id": "1802.08182", "submitter": "Nick Feamster", "authors": "Serena Zheng, Noah Apthorpe, Marshini Chetty, Nick Feamster", "title": "User Perceptions of Smart Home IoT Privacy", "comments": "20 pages, 1 table", "journal-ref": "Proceedings of the ACM on Human-Computer Interaction, ACM\n  Conference on Computer-Supported Cooperative Work and Social Computing\n  (CSCW), Volume 2, Article 200. November 2018", "doi": "10.1145/3274469", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart home Internet of Things (IoT) devices are rapidly increasing in\npopularity, with more households including Internet-connected devices that\ncontinuously monitor user activities. In this study, we conduct eleven\nsemi-structured interviews with smart home owners, investigating their reasons\nfor purchasing IoT devices, perceptions of smart home privacy risks, and\nactions taken to protect their privacy from those external to the home who\ncreate, manage, track, or regulate IoT devices and/or their data. We note\nseveral recurring themes. First, users' desires for convenience and\nconnectedness dictate their privacy-related behaviors for dealing with external\nentities, such as device manufacturers, Internet Service Providers,\ngovernments, and advertisers. Second, user opinions about external entities\ncollecting smart home data depend on perceived benefit from these entities.\nThird, users trust IoT device manufacturers to protect their privacy but do not\nverify that these protections are in place. Fourth, users are unaware of\nprivacy risks from inference algorithms operating on data from non-audio/visual\ndevices. These findings motivate several recommendations for device designers,\nresearchers, and industry standards to better match device privacy features to\nthe expectations and preferences of smart home owners.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 17:11:25 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 13:44:28 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Zheng", "Serena", ""], ["Apthorpe", "Noah", ""], ["Chetty", "Marshini", ""], ["Feamster", "Nick", ""]]}, {"id": "1802.08218", "submitter": "Danna Gurari", "authors": "Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen\n  Grauman, Jiebo Luo, and Jeffrey P. Bigham", "title": "VizWiz Grand Challenge: Answering Visual Questions from Blind People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of algorithms to automatically answer visual questions currently is\nmotivated by visual question answering (VQA) datasets constructed in artificial\nVQA settings. We propose VizWiz, the first goal-oriented VQA dataset arising\nfrom a natural VQA setting. VizWiz consists of over 31,000 visual questions\noriginating from blind people who each took a picture using a mobile phone and\nrecorded a spoken question about it, together with 10 crowdsourced answers per\nvisual question. VizWiz differs from the many existing VQA datasets because (1)\nimages are captured by blind photographers and so are often poor quality, (2)\nquestions are spoken and so are more conversational, and (3) often visual\nquestions cannot be answered. Evaluation of modern algorithms for answering\nvisual questions and deciding if a visual question is answerable reveals that\nVizWiz is a challenging dataset. We introduce this dataset to encourage a\nlarger community to develop more generalized algorithms that can assist blind\npeople.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 18:16:53 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 19:52:08 GMT"}, {"version": "v3", "created": "Mon, 2 Apr 2018 15:53:07 GMT"}, {"version": "v4", "created": "Wed, 9 May 2018 17:26:40 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Gurari", "Danna", ""], ["Li", "Qing", ""], ["Stangl", "Abigale J.", ""], ["Guo", "Anhong", ""], ["Lin", "Chi", ""], ["Grauman", "Kristen", ""], ["Luo", "Jiebo", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1802.08345", "submitter": "Xiao Ma", "authors": "Xiao Ma, Megan Cackett, Leslie Park, Eric Chien, and Mor Naaman", "title": "Web-Based VR Experiments Powered by the Crowd", "comments": "The Web Conference 2018 (WWW 2018); update citation format", "journal-ref": "Proceedings of the WWW 2018", "doi": "10.1145/3178876.3186034", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We build on the increasing availability of Virtual Reality (VR) devices and\nWeb technologies to conduct behavioral experiments in VR using crowdsourcing\ntechniques. A new recruiting and validation method allows us to create a panel\nof eligible experiment participants recruited from Amazon Mechanical Turk.\nUsing this panel, we ran three different crowdsourced VR experiments, each\nreproducing one of three VR illusions: place illusion, embodiment illusion, and\nplausibility illusion. Our experience and worker feedback on these experiments\nshow that conducting Web-based VR experiments using crowdsourcing is already\nfeasible, though some challenges---including scale---remain. Such crowdsourced\nVR experiments on the Web have the potential to finally support replicable VR\nexperiments with diverse populations at a low cost.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 23:13:54 GMT"}, {"version": "v2", "created": "Mon, 26 Feb 2018 15:35:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Ma", "Xiao", ""], ["Cackett", "Megan", ""], ["Park", "Leslie", ""], ["Chien", "Eric", ""], ["Naaman", "Mor", ""]]}, {"id": "1802.08452", "submitter": "Massimo Quadrana", "authors": "Massimo Quadrana, Paolo Cremonesi and Dietmar Jannach", "title": "Sequence-Aware Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems are one of the most successful applications of data\nmining and machine learning technology in practice. Academic research in the\nfield is historically often based on the matrix completion problem formulation,\nwhere for each user-item-pair only one interaction (e.g., a rating) is\nconsidered. In many application domains, however, multiple user-item\ninteractions of different types can be recorded over time. And, a number of\nrecent works have shown that this information can be used to build richer\nindividual user models and to discover additional behavioral patterns that can\nbe leveraged in the recommendation process. In this work we review existing\nworks that consider information from such sequentially-ordered user- item\ninteraction logs in the recommendation process. Based on this review, we\npropose a categorization of the corresponding recommendation tasks and goals,\nsummarize existing algorithmic solutions, discuss methodological approaches\nwhen benchmarking what we call sequence-aware recommender systems, and outline\nopen challenges in the area.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 09:19:26 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Quadrana", "Massimo", ""], ["Cremonesi", "Paolo", ""], ["Jannach", "Dietmar", ""]]}, {"id": "1802.08612", "submitter": "Libby Hemphill", "authors": "Libby Hemphill", "title": "More Specificity, More Attention to Social Context: Reframing How We\n  Address \"Bad Actors\"", "comments": "Paper submitted Workshop Paper Submitted to CHI 2018: Understanding\n  \"Bad Actors\" Online", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To address \"bad actors\" online, I argue for more specific definitions of\nacceptable and unacceptable behaviors and explicit attention to the social\nstructures in which behaviors occur.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 15:48:56 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Hemphill", "Libby", ""]]}, {"id": "1802.08621", "submitter": "Zhe Cui", "authors": "Zhe Cui, Sriram Karthik Badam, Adil Yal\\c{c}in, Niklas Elmqvist", "title": "DataSite: Proactive Visual Data Exploration with Computation of\n  Insight-based Recommendations", "comments": "Databases, Information Visualization, Human Computer Interaction;\n  Accepted at Information Visualization Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective data analysis ideally requires the analyst to have high expertise\nas well as high knowledge of the data. Even with such familiarity, manually\npursuing all potential hypotheses and exploring all possible views is\nimpractical. We present DataSite, a proactive visual analytics system where the\nburden of selecting and executing appropriate computations is shared by an\nautomatic server-side computation engine. Salient features identified by these\nautomatic background processes are surfaced as notifications in a feed\ntimeline. DataSite effectively turns data analysis into a conversation between\nanalyst and computer, thereby reducing the cognitive load and domain knowledge\nrequirements. We validate the system with a user study comparing it to a recent\nvisualization recommendation system, yielding significant improvement,\nparticularly for complex analyses that existing analytics systems do not\nsupport well.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 16:27:52 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 16:11:20 GMT"}, {"version": "v3", "created": "Sat, 22 Sep 2018 21:11:20 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Cui", "Zhe", ""], ["Badam", "Sriram Karthik", ""], ["Yal\u00e7in", "Adil", ""], ["Elmqvist", "Niklas", ""]]}, {"id": "1802.09012", "submitter": "Min Chen", "authors": "Min Chen, Kelly Gaither, Nigel W. John, and Brian McCann", "title": "Cost-benefit Analysis of Visualization in Virtual Environments", "comments": "Submitted to SciVis 2017 on 31 March 2017 and was not accepted.\n  Authors' feedback about the SciVis 2017 reviews can be found in the cover\n  letter accompanying the EuroVis submission. Revised with a major extension\n  and submitted to EuroVis 2018 on 13 December 2017, but was not accepted", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 25(1),\n  2019", "doi": "10.1109/TVCG.2018.2865025", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization and virtual environments (VEs) have been two interconnected\nparallel strands in visual computing for decades. Some VEs have been purposely\ndeveloped for visualization applications, while many visualization applications\nare exemplary showcases in general-purpose VEs. Because of the development and\noperation costs of VEs, the majority of visualization applications in practice\nare yet to benefit from the capacity of VEs. In this paper, we examine this\nperplexity from an information-theoretic perspective. Our objectives are to\nconduct cost-benefit analysis on typical VE systems (including augmented and\nmixed reality, theatre-based systems, and large powerwalls), to explain why\nsome visualization applications benefit more from VEs than others, and to\nsketch out pathways for the future development of visualization applications in\nVEs. We support our theoretical propositions and analysis using theories and\ndiscoveries in the literature of cognitive sciences and the practical evidence\nreported in the literatures of visualization and VEs.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 14:14:42 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 09:30:32 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Chen", "Min", ""], ["Gaither", "Kelly", ""], ["John", "Nigel W.", ""], ["McCann", "Brian", ""]]}, {"id": "1802.09055", "submitter": "Ahmed Fadhil", "authors": "Ahmed Fadhil", "title": "Domain Specific Design Patterns: Designing For Conversational User\n  Interfaces", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Designing conversational user interface experience is complicated because\nconversation comes with many expectations. When these expectations are met, we\nfeel the interface is natural, but once violated, we feel something is amiss.\nThe last decade witnessed human language technologies and behaviours to enable\nhumans converse with software using spoken dialogue to access, create and\nprocess information. Less is known about the practicalities of designing\nchatbot interactions. In this paper, we introduce the nature of conversational\nuser interfaces (CUIs) and describe the underlying technologies they are based\non. Moreover, we define guidelines for designing conversational interfaces in\nvarious domains. This paper particularly focuses on classifying the elements\nand techniques used in CUI design patterns. After concluding certain challenges\nwith CUI, we discuss important features and chatbot states to be considered in\nCUI design for specific domain. We envisage this study to support CUI\nresearchers to design tailored chatbots applicable into certain domain and\nimprove the current state of research challenges in the field of Artificial\nIntelligence and conversational agents.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 18:26:45 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Fadhil", "Ahmed", ""]]}, {"id": "1802.09100", "submitter": "Ahmed Fadhil", "authors": "Ahmed Fadhil", "title": "Can a Chatbot Determine My Diet?: Addressing Challenges of Chatbot\n  Application for Meal Recommendation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Poor nutrition can lead to reduced immunity, increased susceptibility to\ndisease, impaired physical and mental development, and reduced productivity. A\nconversational agent can support people as a virtual coach, however building\nsuch systems still have its associated challenges and limitations. This paper\ndescribes the background and motivation for chatbot systems in the context of\nhealthy nutrition recommendation. We discuss current challenges associated with\nchatbot application, we tackled technical, theoretical, behavioural, and social\naspects of the challenges. We then propose a pipeline to be used as guidelines\nby developers to implement theoretically and technically robust chatbot\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Feb 2018 22:30:10 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Fadhil", "Ahmed", ""]]}, {"id": "1802.09172", "submitter": "Bo Han", "authors": "Bo Han, Quanming Yao, Yuangang Pan, Ivor W. Tsang, Xiaokui Xiao, Qiang\n  Yang and Masashi Sugiyama", "title": "Millionaire: A Hint-guided Approach for Crowdsourcing", "comments": "Machine Learning; Artificial Intelligence; Game Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning is migrating to the era of complex models, which\nrequires a plethora of well-annotated data. While crowdsourcing is a promising\ntool to achieve this goal, existing crowdsourcing approaches barely acquire a\nsufficient amount of high-quality labels. In this paper, motivated by the\n\"Guess-with-Hints\" answer strategy from the Millionaire game show, we introduce\nthe hint-guided approach into crowdsourcing to deal with this challenge. Our\napproach encourages workers to get help from hints when they are unsure of\nquestions. Specifically, we propose a hybrid-stage setting, consisting of the\nmain stage and the hint stage. When workers face any uncertain question on the\nmain stage, they are allowed to enter the hint stage and look up hints before\nmaking any answer. A unique payment mechanism that meets two important design\nprinciples for crowdsourcing is developed. Besides, the proposed mechanism\nfurther encourages high-quality workers less using hints, which helps identify\nand assigns larger possible payment to them. Experiments are performed on\nAmazon Mechanical Turk, which show that our approach ensures a sufficient\nnumber of high-quality labels with low expenditure and detects high-quality\nworkers.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 06:07:12 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 08:59:24 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Han", "Bo", ""], ["Yao", "Quanming", ""], ["Pan", "Yuangang", ""], ["Tsang", "Ivor W.", ""], ["Xiao", "Xiaokui", ""], ["Yang", "Qiang", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1802.09307", "submitter": "Konrad Hinsen", "authors": "Konrad Hinsen", "title": "Digital Scientific Notations as a Human-Computer Interface in\n  Computer-Aided Research", "comments": null, "journal-ref": null, "doi": "10.7717/peerj-cs.158", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of today's scientific research relies on computers and software not only\nfor administrational tasks, but also for processing scientific information.\nExamples of such computer-aided research are the analysis of experimental data\nor the simulation of phenomena based on theoretical models. With the rapid\nincrease of computational power, scientific software has integrated more and\nmore complex scientific knowledge in a black-box fashion. As a consequence, its\nusers do not know, and don't even have a chance of finding out, which models or\nassumptions their computations are based on. The black-box nature of scientific\nsoftware has thereby become a major cause of mistakes. The present work starts\nwith an analysis of this situation from the point of view of human-computer\ninteraction in scientific research. It identifies the key role of digital\nscientific notations at the human-computer interface, and describes a\nproof-of-concept implementation of such a digital scientific notation for\nscientific models formulated as mathematical equations.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 15:51:43 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Hinsen", "Konrad", ""]]}, {"id": "1802.09522", "submitter": "Hugo Valle", "authors": "M. Saleem and Hugo Valle and Stephen Brown and Veronica Winters and\n  Akhtar Mahmood", "title": "The Hiperwall Visualization Platform for Big Data Research", "comments": "16 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of Big Data, with the increasing use of large-scale data-driven\napplications, the visualization of very large high-resolution images and\nextracting useful information (searching for specific targets or rare signal\nevents) from these images can pose challenges to the current video-wall display\ntechnologies. At Bellarmine University, we have set up an Advanced\nVisualization and Computational Lab (AVCL) using a state-of-the-art next\ngeneration video-wall technology, called Hiperwall (Highly Interactive\nParallelized Display Wall). The 16 feet wide by 4.5 feet high Hiperwall\nvisualization system consists of eight display tiles that are arranged in a 4x2\ntile format and has an effective resolution of 16.5 Megapixels. Using\nHiperwall, we can perform interactive visual data analytics of large images by\nconducting comparative views of multiple large images in Astronomy and multiple\ndata events in experimental High Energy Physics (HEP). Users can display a\nsingle large image across all the display tiles, or view many different images\nsimultaneously on multiple display tiles. Hiperwall enables simultaneous\nvisualization of multiple high resolution images and its contents on the entire\ndisplay wall without loss of clarity. Hiperwall's middleware also allows\nresearchers in geographically diverse locations to collaborate on large\nscientific experiments. In this paper we will provide a description of a new\ngeneration of display wall setup at Bellarmine University that is based on the\nHiperwall technology, which is a robust visualization system for Big Data\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 15:03:28 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Saleem", "M.", ""], ["Valle", "Hugo", ""], ["Brown", "Stephen", ""], ["Winters", "Veronica", ""], ["Mahmood", "Akhtar", ""]]}, {"id": "1802.09566", "submitter": "Mudasir Ahmad Wani", "authors": "Mudasir Ahmad Wani, Nancy Agarwal, Suraiya Jabin, Syed Zeeshan Hussai", "title": "Design of iMacros-based Data Crawler and the Behavioral Analysis of\n  Facebook Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining the desired dataset is still a prime challenge faced by researchers\nwhile analyzing Online Social Network (OSN) sites. Application Programming\nInterfaces (APIs) provided by OSN service providers for retrieving data impose\nseveral unavoidable restrictions which make it difficult to get a desirable\ndataset. In this paper, we present an iMacros technology-based data crawler\ncalled IMcrawler, capable of collecting every piece of information which is\naccessible through a browser from the Facebook website within the legal\nframework which permits access to publicly shared user content on OSNs. The\nproposed crawler addresses most of the challenges allied with web data\nextraction approaches and most of the APIs provided by OSN service providers.\nTwo broad sections have been extracted from Facebook user profiles, namely,\nPersonal Information and Wall Activities. The present work is the first attempt\ntowards providing the detailed description of crawler design for the Facebook\nwebsite.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 18:36:46 GMT"}, {"version": "v2", "created": "Tue, 26 Nov 2019 11:16:01 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Wani", "Mudasir Ahmad", ""], ["Agarwal", "Nancy", ""], ["Jabin", "Suraiya", ""], ["Hussai", "Syed Zeeshan", ""]]}, {"id": "1802.09597", "submitter": "Manish Raghavan", "authors": "Manish Raghavan, Ashton Anderson, Jon Kleinberg", "title": "Mapping the Invocation Structure of Online Political Interaction", "comments": "The Web Conference 2018 (WWW 2018)", "journal-ref": null, "doi": "10.1145/3178876.3186129", "report-no": null, "categories": "cs.SI cs.CY cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The surge in political information, discourse, and interaction has been one\nof the most important developments in social media over the past several years.\nThere is rich structure in the interaction among different viewpoints on the\nideological spectrum. However, we still have only a limited analytical\nvocabulary for expressing the ways in which these viewpoints interact.\n  In this paper, we develop network-based methods that operate on the ways in\nwhich users share content; we construct \\emph{invocation graphs} on Web domains\nshowing the extent to which pages from one domain are invoked by users to reply\nto posts containing pages from other domains. When we locate the domains on a\npolitical spectrum induced from the data, we obtain an embedded graph showing\nhow these interaction links span different distances on the spectrum. The\nstructure of this embedded network, and its evolution over time, helps us\nderive macro-level insights about how political interaction unfolded through\n2016, leading up to the US Presidential election. In particular, we find that\nthe domains invoked in replies spanned increasing distances on the spectrum\nover the months approaching the election, and that there was clear asymmetry\nbetween the left-to-right and right-to-left patterns of linkage.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:43:18 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Raghavan", "Manish", ""], ["Anderson", "Ashton", ""], ["Kleinberg", "Jon", ""]]}, {"id": "1802.10079", "submitter": "George Leu", "authors": "George Leu and Jiangjun Tang and Hussein Abbass", "title": "On the role of working memory in trading-off skills and situation\n  awareness in Sudoku", "comments": null, "journal-ref": "Lecture Notes in Computer Science, vol 8836, Springer, 2014", "doi": "10.1007/978-3-319-12643-2_69", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Working memory accounts for the ability of humans to perform cognitive\nprocessing, by handling both the representation of information (the mental\npicture forming the situation awareness) and the space required for processing\nthese information (skill processing). The more complex the skills are, the more\nprocessing space they require, the less space becomes available for storage of\ninformation. This interplay between situation awareness and skills is critical\nin many applications. Theoretically, it is less understood in cognition and\nneuroscience. In the meantime, and practically, it is vital when analysing the\nmental processes involved in safety-critical domains.\n  In this paper, we use the Sudoku game as a vehicle to study this trade-off.\nThis game combines two features that are present during a user interaction with\na software in many safety critical domains: scanning for information and\nprocessing of information. We use a society of agents for investigating how\nthis trade-off influences player's proficiency.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 00:16:45 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Leu", "George", ""], ["Tang", "Jiangjun", ""], ["Abbass", "Hussein", ""]]}, {"id": "1802.10121", "submitter": "Ismael Figueroa", "authors": "Cristhy Jimenez and Hector Allende-Cid and Ismael Figueroa", "title": "PROMETHEUS: PROcedural METhodology for developing HEuristics of\n  USability", "comments": "The original version of this work appears in Spanish in the IEEE\n  Latin American Transactions journal. This is a faithful translation to\n  english, in order to further disseminate our work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usability is used to assess the effectiveness of a software product from the\nuser point of view. Hence, proper methodologies and techniques to perform this\nassessment are very relevant. Heuristic evaluation is probably the most\ncommonly used method for usability assessment. Developed by Nielsen and Molich\nin the '90s, traditional heuristic evaluations rely on Nielsen's 10 usability\nheuristics. However, recent evidence suggests that such heuristics are not\nsufficiently complete for dealing with new domains such as interactive\ntelevision, virtual worlds, and many others. In addition to the lack of\nsuitability of traditional heuristics, in the past years the lack of a robust\nmethodology or process to effectively develop and validate these new\ndomain-specific heuristics has been documented. In this paper we summarize\ncurrent evidence on the lack of suitability of traditional heuristics, as well\nas the need to develop new domain-specific heuristics. After identifying and\nacknowledging existing gaps in heuristics for state-of-the-art technology, we\npresent PROMETHEUS, a PROcedural METhodology for developing HEuristics of\nUSability. PROMETHEUS refines the methodology of Rusu et. al. (2011), and is\ncomposed of 8 stages. PROMETHEUS clearly defines the artifacts that are\nrequired and produced by each stage, and also presents a set of quality\nindicators in order to assess the need for further refinement in the\ndevelopment of new heuristics. As an initial validation of PROMETHEUS, we apply\na questionnaire to several researchers that have used the methodology of Rusu\net. al., and we have also performed a small retrospective study, computing the\nquality indicators of several previous studies. Our results suggest that\nPROMETHEUS is a very promising methodology, and that the metrics and indicators\nare indeed pertinent with respect to the conclusions of previous studies.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 19:08:24 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Jimenez", "Cristhy", ""], ["Allende-Cid", "Hector", ""], ["Figueroa", "Ismael", ""]]}, {"id": "1802.10237", "submitter": "Ali Moin", "authors": "Ali Moin, Andy Zhou, Abbas Rahimi, Simone Benatti, Alisha Menon, Senam\n  Tamakloe, Jonathan Ting, Natasha Yamamoto, Yasser Khan, Fred Burghardt, Luca\n  Benini, Ana C. Arias, Jan M. Rabaey", "title": "An EMG Gesture Recognition System with Flexible High-Density Sensors and\n  Brain-Inspired High-Dimensional Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  EMG-based gesture recognition shows promise for human-machine interaction.\nSystems are often afflicted by signal and electrode variability which degrades\nperformance over time. We present an end-to-end system combating this\nvariability using a large-area, high-density sensor array and a robust\nclassification algorithm. EMG electrodes are fabricated on a flexible substrate\nand interfaced to a custom wireless device for 64-channel signal acquisition\nand streaming. We use brain-inspired high-dimensional (HD) computing for\nprocessing EMG features in one-shot learning. The HD algorithm is tolerant to\nnoise and electrode misplacement and can quickly learn from few gestures\nwithout gradient descent or back-propagation. We achieve an average\nclassification accuracy of 96.64% for five gestures, with only 7% degradation\nwhen training and testing across different days. Our system maintains this\naccuracy when trained with only three trials of gestures; it also demonstrates\ncomparable accuracy with the state-of-the-art when trained with one trial.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 02:37:01 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 17:38:05 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Moin", "Ali", ""], ["Zhou", "Andy", ""], ["Rahimi", "Abbas", ""], ["Benatti", "Simone", ""], ["Menon", "Alisha", ""], ["Tamakloe", "Senam", ""], ["Ting", "Jonathan", ""], ["Yamamoto", "Natasha", ""], ["Khan", "Yasser", ""], ["Burghardt", "Fred", ""], ["Benini", "Luca", ""], ["Arias", "Ana C.", ""], ["Rabaey", "Jan M.", ""]]}, {"id": "1802.10408", "submitter": "German I. Parisi", "authors": "German I. Parisi, Pablo Barros, Di Fu, Sven Magg, Haiyan Wu, Xun Liu,\n  Stefan Wermter", "title": "A Neurorobotic Experiment for Crossmodal Conflict Resolution in Complex\n  Environments", "comments": "Accepted at the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2018), Madrid, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crossmodal conflict resolution is crucial for robot sensorimotor coupling\nthrough the interaction with the environment, yielding swift and robust\nbehaviour also in noisy conditions. In this paper, we propose a neurorobotic\nexperiment in which an iCub robot exhibits human-like responses in a complex\ncrossmodal environment. To better understand how humans deal with multisensory\nconflicts, we conducted a behavioural study exposing 33 subjects to congruent\nand incongruent dynamic audio-visual cues. In contrast to previous studies\nusing simplified stimuli, we designed a scenario with four animated avatars and\nobserved that the magnitude and extension of the visual bias are related to the\nsemantics embedded in the scene, i.e., visual cues that are congruent with\nenvironmental statistics (moving lips and vocalization) induce the strongest\nbias. We implement a deep learning model that processes stereophonic sound,\nfacial features, and body motion to trigger a discrete behavioural response.\nAfter training the model, we exposed the iCub to the same experimental\nconditions as the human subjects, showing that the robot can replicate similar\nresponses in real time. Our interdisciplinary work provides important insights\ninto how crossmodal conflict resolution can be modelled in robots and\nintroduces future research directions for the efficient combination of sensory\nobservations with internally generated knowledge and expectations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 13:49:18 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 09:39:47 GMT"}, {"version": "v3", "created": "Mon, 24 Sep 2018 20:22:52 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Parisi", "German I.", ""], ["Barros", "Pablo", ""], ["Fu", "Di", ""], ["Magg", "Sven", ""], ["Wu", "Haiyan", ""], ["Liu", "Xun", ""], ["Wermter", "Stefan", ""]]}, {"id": "1802.10503", "submitter": "Paul Schydlo", "authors": "Paul Schydlo, Mirko Rakovic, Lorenzo Jamone and Jos\\'e Santos-Victor", "title": "Anticipation in Human-Robot Cooperation: A Recurrent Neural Network\n  Approach for Multiple Action Sequences Prediction", "comments": "IEEE International Conference on Robotics and Automation (ICRA) 2018,\n  Accepted", "journal-ref": null, "doi": "10.1109/ICRA.2018.8460924", "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Close human-robot cooperation is a key enabler for new developments in\nadvanced manufacturing and assistive applications. Close cooperation require\nrobots that can predict human actions and intent, and understand human\nnon-verbal cues. Recent approaches based on neural networks have led to\nencouraging results in the human action prediction problem both in continuous\nand discrete spaces. Our approach extends the research in this direction. Our\ncontributions are three-fold. First, we validate the use of gaze and body pose\ncues as a means of predicting human action through a feature selection method.\nNext, we address two shortcomings of existing literature: predicting multiple\nand variable-length action sequences. This is achieved by introducing an\nencoder-decoder recurrent neural network topology in the discrete action\nprediction problem. In addition, we theoretically demonstrate the importance of\npredicting multiple action sequences as a means of estimating the stochastic\nreward in a human robot cooperation scenario. Finally, we show the ability to\neffectively train the prediction model on a action prediction dataset,\ninvolving human motion data, and explore the influence of the model's\nparameters on its performance. Source code repository:\nhttps://github.com/pschydlo/ActionAnticipation\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:10:20 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 23:54:08 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 10:37:01 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Schydlo", "Paul", ""], ["Rakovic", "Mirko", ""], ["Jamone", "Lorenzo", ""], ["Santos-Victor", "Jos\u00e9", ""]]}]