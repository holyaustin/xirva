[{"id": "1605.00284", "submitter": "Heba Abdelnasser", "authors": "Heba Abdelnasser, Moustafa Youssef, Khaled A. Harras", "title": "MagBoard: Magnetic-based Ubiquitous Homomorphic Off-the-shelf Keyboard", "comments": "Accepted for publication in SECON 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main methods for interacting with mobile devices today is the\nerror-prone and inflexible touch-screen keyboard. This paper proposes MagBoard:\na homomorphic ubiquitous keyboard for mobile devices. MagBoard allows\napplication developers and users to design and print different custom keyboards\nfor the same applications to fit different user's needs. The core idea is to\nleverage the triaxial magnetometer embedded in standard mobile phones to\naccurately localize the location of a magnet on a virtual grid superimposed on\nthe printed keyboard. This is achieved through a once in a lifetime\nfingerprint. MagBoard also provides a number of modules that allow it to cope\nwith background magnetic noise, heterogeneous devices, different magnet shapes,\nsizes, and strengths, as well as changes in magnet polarity. Our implementation\nof MagBoard on Android phones with extensive evaluation in different scenarios\ndemonstrates that it can achieve a key detection accuracy of more than 91% for\nkeys as small as 2cm*2cm, reaching 100% for 4cm*4cm keys. This accuracy is\nrobust with different phones and magnets, highlighting MagBoard promise as a\nhomomorphic ubiquitous keyboard for mobile devices.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2016 18:06:28 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Abdelnasser", "Heba", ""], ["Youssef", "Moustafa", ""], ["Harras", "Khaled A.", ""]]}, {"id": "1605.00807", "submitter": "Sheng-Yi Hsu", "authors": "Sheng-yi Hsu, Yuan-fu Lou, Chuen-tsai Sun", "title": "Block Shelves for Visual Programming Languages", "comments": "6 pages, 6 figures, short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blocks editor, such as the editor in Scratch, is widely applied for\nvisual programming languages (VPL) nowadays. Despite it's friendly for\nnon-programmers, it exists three main limitations while displaying block codes:\n(1) the readability, (2) the program structure, and (3) the re-use. To cope\nwith these issues, we introduce a novel formatting tool, block shelves, into\nthe editor for organizing blocks. A user could utilize shelves to constitute a\nuser-defined structure for the VPL projects. Based on the experiment results,\nblock shelves improves the block code navigating and searching significantly.\nBesides, for achieving code re-use, users could use shelf export/import to\nshare/re-use their block codes between projects in the file format of\neXtensible Markup Language (xml.) All functions were demonstrated on MIT App\ninventor 2, while all modifications were made in Google Blockly.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 09:27:55 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Hsu", "Sheng-yi", ""], ["Lou", "Yuan-fu", ""], ["Sun", "Chuen-tsai", ""]]}, {"id": "1605.00910", "submitter": "Poonam Yadav Dr", "authors": "Poonam Yadav and John Darlington", "title": "Design Guidelines for the User-Centred Collaborative Citizen Science\n  Platforms", "comments": "In Review process", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Citizen Science platforms are good examples of socio-technical systems\nwhere technology-enabled interactions occur between scientists and the general\npublic (volunteers). Citizen Science platforms usually host multiple Citizen\nScience projects, and allow volunteers to choose the ones to participate in.\nRecent work in the area has demonstrated a positive feedback loop between\nparticipation and learning and creativity in Citizen Science projects, which is\none of the motivating factors both for scientists and the volunteers. This\nemphasises the importance of creating successful Citizen Science platforms,\nwhich support this feedback process, and enable enhanced learning and\ncreativity to occur through knowledge sharing and diverse participation. In\nthis paper, we discuss how scientists' and volunteers' motivation and\nparticipation influence the design of Citizen Science platforms. We present our\nsummary as guidelines for designing these platforms as user-inspired\nsocio-technical systems. We also present the case-studies on popular Citizen\nScience platforms, including our own CitizenGrid platform, developed as part of\nthe CCL EU project, as well as Zooniverse, World Community Grid, CrowdCrafting\nand EpiCollect+ to see how closely these platforms follow our proposed\nguidelines and how these may be further improved to incorporate the creativity\nenabled by the collective knowledge sharing.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2016 13:51:54 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Yadav", "Poonam", ""], ["Darlington", "John", ""]]}, {"id": "1605.01148", "submitter": "Viirj Kan", "authors": "Viirj Kan, Emma Vargo, Noa Machover, Hiroshi Ishii, Serena Pan,\n  Weixuan Chen, Yasuaki Kakehi", "title": "Organic Primitives: Synthesis and Design of pH-Reactive Materials using\n  Molecular I/O for Sensing, Actuation, and Interaction", "comments": "Updated paper", "journal-ref": null, "doi": "10.1145/3025453.3025952", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present Organic Primitives, an enabling toolbox that expands\nupon the library of input-output devices in HCI and facilitates the design of\ninteractions with organic, fluid-based systems. We formulated color, odor and\nshape changing material primitives which act as sensor-actuators that convert\npH signals into human-readable outputs. Food-grade organic molecules\nanthocyanin, vanillin, and chitosan were employed as dopants to synthesize\nmaterials which output a spectrum of colors, degrees of shape deformation, and\nswitch between odorous and non-odorous states. We evaluated the individual\noutput properties of our sensor-actuators to assess the rate, range, and\nreversibility of the changes as a function of pH 2-10. We present a design\nspace with techniques for enhancing the functionality of the material\nprimitives, and offer passive and computational methods for controlling the\nmaterial interfaces. Finally, we explore applications enabled by Organic\nPrimitives under four contexts: environmental, cosmetic, edible, and\ninterspecies.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2016 05:33:44 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 02:23:43 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Kan", "Viirj", ""], ["Vargo", "Emma", ""], ["Machover", "Noa", ""], ["Ishii", "Hiroshi", ""], ["Pan", "Serena", ""], ["Chen", "Weixuan", ""], ["Kakehi", "Yasuaki", ""]]}, {"id": "1605.01600", "submitter": "Fabien Ringeval", "authors": "Michel Valstar, Jonathan Gratch, Bjorn Schuller, Fabien Ringeval,\n  Denis Lalanne, Mercedes Torres Torres, Stefan Scherer, Guiota Stratou, Roddy\n  Cowie and Maja Pantic", "title": "AVEC 2016 - Depression, Mood, and Emotion Recognition Workshop and\n  Challenge", "comments": "Proceedings of the 6th International Workshop on Audio/Visual Emotion\n  Challenge, AVEC'16, co-located with the 24th ACM International Conference on\n  Multimedia, MM 2016, pages 3-10, Amsterdam, The Netherlands, October 2016.\n  ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Audio/Visual Emotion Challenge and Workshop (AVEC 2016) \"Depression, Mood\nand Emotion\" will be the sixth competition event aimed at comparison of\nmultimedia processing and machine learning methods for automatic audio, visual\nand physiological depression and emotion analysis, with all participants\ncompeting under strictly the same conditions. The goal of the Challenge is to\nprovide a common benchmark test set for multi-modal information processing and\nto bring together the depression and emotion recognition communities, as well\nas the audio, video and physiological processing communities, to compare the\nrelative merits of the various approaches to depression and emotion recognition\nunder well-defined and strictly comparable conditions and establish to what\nextent fusion of the approaches is possible and beneficial. This paper presents\nthe challenge guidelines, the common data used, and the performance of the\nbaseline system on the two tasks.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2016 14:04:50 GMT"}, {"version": "v2", "created": "Fri, 6 May 2016 12:34:51 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 08:02:32 GMT"}, {"version": "v4", "created": "Tue, 22 Nov 2016 15:19:24 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Valstar", "Michel", ""], ["Gratch", "Jonathan", ""], ["Schuller", "Bjorn", ""], ["Ringeval", "Fabien", ""], ["Lalanne", "Denis", ""], ["Torres", "Mercedes Torres", ""], ["Scherer", "Stefan", ""], ["Stratou", "Guiota", ""], ["Cowie", "Roddy", ""], ["Pantic", "Maja", ""]]}, {"id": "1605.01919", "submitter": "Scott A. Hale", "authors": "Scott A. Hale", "title": "User Reviews and Language: How Language Influences Ratings", "comments": null, "journal-ref": "Proceedings of the 2016 CHI Conference on Human Factors in\n  Computing Systems, CHI'16 Extended Abstracts", "doi": "10.1145/2851581.2892466", "report-no": null, "categories": "cs.HC cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of user reviews of tourist attractions, restaurants, mobile apps,\netc. is increasing for all languages; yet, research is lacking on how reviews\nin multiple languages should be aggregated and displayed. Speakers of different\nlanguages may have consistently different experiences, e.g., different\ninformation available in different languages at tourist attractions or\ndifferent user experiences with software due to\ninternationalization/localization choices. This paper assesses the similarity\nin the ratings given by speakers of different languages to London tourist\nattractions on TripAdvisor. The correlations between different languages are\ngenerally high, but some language pairs are more correlated than others. The\nresults question the common practice of computing average ratings from reviews\nin many languages.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 12:52:09 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Hale", "Scott A.", ""]]}, {"id": "1605.03389", "submitter": "Markus Oberweger", "authors": "Markus Oberweger, Gernot Riegler, Paul Wohlhart, Vincent Lepetit", "title": "Efficiently Creating 3D Training Data for Fine Hand Pose Estimation", "comments": "added link to source https://github.com/moberweger/semi-auto-anno.\n  Appears in Proc. of CVPR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many recent hand pose estimation methods critically rely on a training\nset of labelled frames, the creation of such a dataset is a challenging task\nthat has been overlooked so far. As a result, existing datasets are limited to\na few sequences and individuals, with limited accuracy, and this prevents these\nmethods from delivering their full potential. We propose a semi-automated\nmethod for efficiently and accurately labeling each frame of a hand depth video\nwith the corresponding 3D locations of the joints: The user is asked to provide\nonly an estimate of the 2D reprojections of the visible joints in some\nreference frames, which are automatically selected to minimize the labeling\nwork by efficiently optimizing a sub-modular loss function. We then exploit\nspatial, temporal, and appearance constraints to retrieve the full 3D poses of\nthe hand over the complete sequence. We show that this data can be used to\ntrain a recent state-of-the-art hand pose estimation method, leading to\nincreased accuracy. The code and dataset can be found on our website\nhttps://cvarlab.icg.tugraz.at/projects/hand_detection/\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 11:40:27 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 15:45:38 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Oberweger", "Markus", ""], ["Riegler", "Gernot", ""], ["Wohlhart", "Paul", ""], ["Lepetit", "Vincent", ""]]}, {"id": "1605.03478", "submitter": "Graeme Craig Jenkinson", "authors": "Jeunese Payne, Graeme Jenkinson, Frank Stajano, M. Angela Sasse and\n  Max Spencer", "title": "Responsibility and Tangible Security: Towards a Theory of User\n  Acceptance of Security Tokens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security and usability issues with passwords suggest a need for a new\nauthentication scheme. Several alternatives involve a physical device or token.\nWe investigate one such alternative, Pico: an authentication scheme that\nutilizes multiple wearable devices. We present the grounded theory results of a\nseries of semi-structured interviews for exploring perceptions of this scheme.\nWe found that the idea of carrying physical devices increases perceived\npersonal responsibility for secure authentication, making the risks and\ninconvenience associated with loss and theft salient for participants. Although\nour work is focused on Pico, the results of the study contribute to a broader\nunderstanding of user perception and concerns of responsibility for any\ntoken-based authentication schemes.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2016 15:28:19 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Payne", "Jeunese", ""], ["Jenkinson", "Graeme", ""], ["Stajano", "Frank", ""], ["Sasse", "M. Angela", ""], ["Spencer", "Max", ""]]}, {"id": "1605.03757", "submitter": "David Garcia", "authors": "David Garcia, Arvid Kappas, Dennis K\\\"uster, Frank Schweitzer", "title": "The Dynamics of Emotions in Online Interaction", "comments": null, "journal-ref": "Royal Society Open Science, 2016 3 160059", "doi": "10.1098/rsos.160059", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the changes in emotional states induced by reading and participating\nin online discussions, empirically testing a computational model of online\nemotional interaction. Using principles of dynamical systems, we quantify\nchanges in valence and arousal through subjective reports, as recorded in three\nindependent studies including 207 participants (110 female). In the context of\nonline discussions, the dynamics of valence and arousal are composed of two\nforces: an internal relaxation towards baseline values independent of the\nemotional charge of the discussion, and a driving force of emotional states\nthat depends on the content of the discussion. The dynamics of valence show the\nexistence of positive and negative tendencies, while arousal increases when\nreading emotional content regardless of its polarity. The tendency of\nparticipants to take part in the discussion increases with positive arousal.\nWhen participating in an online discussion, the content of participants'\nexpression depends on their valence, and their arousal significantly decreases\nafterwards as a regulation mechanism. We illustrate how these results allow the\ndesign of agent-based models to reproduce and analyze emotions in online\ncommunities. Our work empirically validates the microdynamics of a model of\nonline collective emotions, bridging online data analysis with research in the\nlaboratory.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 11:02:48 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Garcia", "David", ""], ["Kappas", "Arvid", ""], ["K\u00fcster", "Dennis", ""], ["Schweitzer", "Frank", ""]]}, {"id": "1605.03783", "submitter": "Pariya Kashfi", "authors": "Pariya Kashfi, Agneta Nilsson, Robert Feldt", "title": "Integrating User eXperience Practices into Software Development\n  Processes: Implications of Subjectivity and Emergent Nature of UX", "comments": "The paper is submitted to PeerJ Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many software companies face challenges in their work with User eXperience\n(UX) and how to integrate UX practices into existing development processes. A\nbetter understanding of these challenges can help researchers and practitioners\nbetter address them. Existing research does not analyse UX challenges in\nrelation to other software quality characteristics including usability. In this\nempirical study, we have interviewed 17 practitioners from eight software\ndevelopment companies. Their responses are coded and analysed with thematic\nanalysis. We report 11 challenges that practitioners face in their work with\nUX. Some of these challenges partly overlap with those reported in existing\nliterature about usability or software quality characteristics. In contrast to\nthese overlaps, the participants of our study either view many of the\nchallenges unique to UX, or more severe than for usability or other quality\ncharacteristics. Although at a superficial level challenges with UX and other\nquality characteristics overlap, we differentiate these challenges at a deeper\nlevel through two main aspects of UX: subjectivity and emergent nature. In\nparticular, we identify at least five issues that are essential to the very\nnature of UX, and add at least seven extra difficulties to the work of\npractitioners. These difficulties can explain why practitioners perceive the\nchallenges to be more severe than for other quality characteristics. Our\nfindings can be useful for researchers in identifying industrially relevant\nresearch areas and for practitioners to learn from empirically investigated\nchallenges and base their improvement efforts on such knowledge. Investigating\nthe overlaps can help finding research areas not only for enhancing practice of\nUX but also software quality in general. It also makes it easier for\npractitioners to spot, better understand as well as find mitigation strategies\nfor UX challenges.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 12:38:22 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Kashfi", "Pariya", ""], ["Nilsson", "Agneta", ""], ["Feldt", "Robert", ""]]}, {"id": "1605.03883", "submitter": "Pariya Kashfi", "authors": "Pariya Kashfi, Robert Feldt, Agneta Nilsson, Richard Berntsson\n  Svensson", "title": "Cross-Section Evidence-based Timelines for Software Process Improvement\n  Retrospectives: A Case Study of User eXperience Integration", "comments": "A short version of this paper is accepted for presentation in, and\n  will appear in proceeding of The Euromicro Conference series on Software\n  Engineering and Advanced Applications (SEAA) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although integrating UX practices into software development processes is a\ntype of Software Process Improvement (SPI) activity, this has not yet been\ntaken into account in UX publications. In this study, we approach UX\nintegration in a software development company in Sweden from a SPI perspective.\nFollowing the guidelines in SPI literature, we performed a retrospective\nmeeting at the company to reflect on their decade of SPI activities for\nenhancing UX integration. The aim of the meeting was to reflect on, learn from,\nand coordinate various activities spanned across various organizational units\nand projects. We therefore supported the meeting by a pre- generated timeline\nof the main activities in the organization that is different from common\nproject retrospective meetings in SPI. This approach is a refinement of a\nsimilar approach that is used in Agile projects, and is shown to improve\neffectiveness of, and decrease memory bias. We hypothesized that this method\ncan be useful in the context of UX integration, and in this broader scope. To\nevaluate the method we gathered practitioners' view through a questionnaire.\nThe findings showed our hypothesis to be plausible. Here, we present that UX\nintegration research and practice can benefit from the SPI body of knowledge;\nWe also show that such cross-section evidence-based timeline retrospective\nmeetings are useful for UX integration, and in a larger scale than one project,\nespecially for identifying and reflecting on 'organizational issues'. This\napproach also provides a cross- section longitudinal overview of the SPI\nactivities that cannot easily be gained in other common SPI learning\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 16:43:16 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Kashfi", "Pariya", ""], ["Feldt", "Robert", ""], ["Nilsson", "Agneta", ""], ["Svensson", "Richard Berntsson", ""]]}, {"id": "1605.03915", "submitter": "Hang Ren", "authors": "Hang Ren, Weiqun Xu and Yonghong Yan", "title": "Optimizing human-interpretable dialog management policy using Genetic\n  Algorithm", "comments": "This technical report is an updated version of the conference paper:\n  \"H. Ren, W. Xu, and Y. Yan, Optimizing human-interpretable dialog management\n  policy using genetic algorithm, in 2015 IEEE Workshop on Automatic Speech\n  Recognition and Understanding (ASRU), 2015, 791-797\". Experiments on policy\n  training via user simulator have been enriched and the reward function is\n  updated", "journal-ref": null, "doi": "10.1109/ASRU.2015.7404869", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic optimization of spoken dialog management policies that are robust\nto environmental noise has long been the goal for both academia and industry.\nApproaches based on reinforcement learning have been proved to be effective.\nHowever, the numerical representation of dialog policy is\nhuman-incomprehensible and difficult for dialog system designers to verify or\nmodify, which limits its practical application. In this paper we propose a\nnovel framework for optimizing dialog policies specified in domain language\nusing genetic algorithm. The human-interpretable representation of policy makes\nthe method suitable for practical employment. We present learning algorithms\nusing user simulation and real human-machine dialogs respectively.Empirical\nexperimental results are given to show the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2016 18:03:38 GMT"}, {"version": "v2", "created": "Fri, 13 May 2016 01:28:52 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ren", "Hang", ""], ["Xu", "Weiqun", ""], ["Yan", "Yonghong", ""]]}, {"id": "1605.04072", "submitter": "Pascale Fung Prof.", "authors": "Pascale Fung, Dario Bertero, Yan Wan, Anik Dey, Ricky Ho Yin Chan,\n  Farhad Bin Siddique, Yang Yang, Chien-Sheng Wu, Ruixi Lin", "title": "Towards Empathetic Human-Robot Interactions", "comments": "23 pages. Keynote at 17th International Conference on Intelligent\n  Text Processing and Computational Linguistics. To appear in Lecture Notes in\n  Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the late 1990s when speech companies began providing their\ncustomer-service software in the market, people have gotten used to speaking to\nmachines. As people interact more often with voice and gesture controlled\nmachines, they expect the machines to recognize different emotions, and\nunderstand other high level communication features such as humor, sarcasm and\nintention. In order to make such communication possible, the machines need an\nempathy module in them which can extract emotions from human speech and\nbehavior and can decide the correct response of the robot. Although research on\nempathetic robots is still in the early stage, we described our approach using\nsignal processing techniques, sentiment analysis and machine learning\nalgorithms to make robots that can \"understand\" human emotion. We propose Zara\nthe Supergirl as a prototype system of empathetic robots. It is a software\nbased virtual android, with an animated cartoon character to present itself on\nthe screen. She will get \"smarter\" and more empathetic through its deep\nlearning algorithms, and by gathering more data and learning from it. In this\npaper, we present our work so far in the areas of deep learning of emotion and\nsentiment recognition, as well as humor recognition. We hope to explore the\nfuture direction of android development and how it can help improve people's\nlives.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2016 07:31:50 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Fung", "Pascale", ""], ["Bertero", "Dario", ""], ["Wan", "Yan", ""], ["Dey", "Anik", ""], ["Chan", "Ricky Ho Yin", ""], ["Siddique", "Farhad Bin", ""], ["Yang", "Yang", ""], ["Wu", "Chien-Sheng", ""], ["Lin", "Ruixi", ""]]}, {"id": "1605.04533", "submitter": "Andreea Ioana Sburlea", "authors": "Andreea Ioana Sburlea, Luis Montesano, Javier Minguez", "title": "Advantages of EEG phase patterns for the detection of gait intention in\n  healthy and stroke subjects", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One use of EEG-based brain-computer interfaces (BCIs) in rehabilitation is\nthe detection of movement intention. In this paper we investigate for the first\ntime the instantaneous phase of movement related cortical potential (MRCP) and\nits application to the detection of gait intention. We demonstrate the utility\nof MRCP phase in two independent datasets, in which 10 healthy subjects and 9\nchronic stroke patients executed a self-initiated gait task in three sessions.\nPhase features were compared to more conventional amplitude and power features.\nThe neurophysiology analysis showed that phase features have higher\nsignal-to-noise ratio than the other features. Also, BCI detectors of gait\nintention based on phase, amplitude, and their combination were evaluated under\nthree conditions: session specific calibration, intersession transfer, and\nintersubject transfer. Results show that the phase based detector is the most\naccurate for session specific calibration (movement intention was correctly\ndetected in 66.5% of trials in healthy subjects, and in 63.3% in stroke\npatients). However, in intersession and intersubject transfer, the detector\nthat combines amplitude and phase features is the most accurate one and the\nonly that retains its accuracy (62.5% in healthy subjects and 59% in stroke\npatients) w.r.t. session specific calibration. Thus, MRCP phase features\nimprove the detection of gait intention and could be used in practice to remove\ntime-consuming BCI recalibration.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2016 12:21:33 GMT"}], "update_date": "2016-05-17", "authors_parsed": [["Sburlea", "Andreea Ioana", ""], ["Montesano", "Luis", ""], ["Minguez", "Javier", ""]]}, {"id": "1605.04984", "submitter": "Nikita Gordienko", "authors": "Nikita Gordienko", "title": "Multi-Parametric Statistical Method for Estimation of Accumulated\n  Fatigue by Sensors in Ordinary Gadgets", "comments": "7 pages, 6 figures; Proc. International Conference \"Science in XXI\n  century: Current Problems in Physics\" (May 17-19, 2016) Kyiv, Ukraine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new method is proposed to monitor the level of currently accumulated\nfatigue and estimate it by the several statistical methods. The experimental\nsoftware application was developed and used to get data from sensors\n(accelerometer, GPS, gyroscope, magnetometer, and camera), conducted\nexperiments, collected data, calculated parameters of their distributions\n(mean, standard deviation, skewness, kurtosis), and analyzed them by\nstatistical methods (moment analysis, cluster analysis, bootstrapping,\nperiodogram and spectrogram analyses). The hypothesis 1 (physical activity can\nbe estimated and classified by moment and cluster analysis) and hypothesis 2\n(fatigue can be estimated by moment analysis, bootstrapping analysis,\nperiodogram, and spectrogram) were proposed and proved. Several \"fatigue\nmetrics\" were proposed: location, size, shape of clouds of points on\nbootstrapping plot. The most promising fatigue metrics is the distance from the\n\"rest\" state point to the \"fatigue\" state point (sum of 3 squared non-normal\ndistribution of non-correlated acceleration values) on the skewness-kurtosis\nplot. These hypotheses were verified on several persons of various age, gender,\nfitness level and improved standard statistical methods in similar researches.\nThe method can be used in practice for ordinary people in everyday situations\n(to estimate their fatigue, give tips about it and advice on context-related\ninformation).\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2016 23:25:40 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Gordienko", "Nikita", ""]]}, {"id": "1605.05120", "submitter": "Manuel Aiple", "authors": "Manuel Aiple, Andr\\'e Schiele", "title": "Pushing the limits of the CyberGrasp for haptic rendering", "comments": "7 pages, 12 figures", "journal-ref": "Robotics and Automation (ICRA), 2013 IEEE International Conference\n  on, Karlsruhe, 2013, pp. 3541-3546", "doi": "10.1109/ICRA.2013.6631073", "report-no": null, "categories": "cs.HC cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CyberGrasp is a well known dataglove-exoskeleton device combination that\nallows to render haptic feedback to the human fingers. Its design, however,\nrestricts its usability for teleoperation through a limited control bandwidth\nand position sensor resolution. Therefore the system is restricted to low\nachievable contact stiffness and feedback gain magnitudes in haptic rendering.\nMoreover, the system prohibits simple adaption of its controller\nimplementation. In this paper, the ExHand Box is presented, a newly designed\nback-end to widen the CyberGrasp's bandwidth restrictions and to open it up for\nfully customized controller implementations. The ExHand Box provides a new\ncomputer, interface electronics and motor controllers for the otherwise\nunmodified CyberGlove and CyberGrasp hand systems. The loop frequency of the\nnew system can be freely varied up to 2 kHz and custom controllers can be\nimplemented through an automatic code generation interface. System performance\nidentification experiments are presented that demonstrate improved behavior in\nhard contact situations over a range of sampling periods. Maximum contact\nstiffnesses of up to 50kN/m in a stable condition are demonstrated, which is\nsignificantly higher than what could be achieved with the non-customized\noriginal system version. Moreover, a bilateral control experiment is conducted\nto demonstrate the new system's usability for generic teleoperation research.\nIn this experiment a raycasting algorithm is introduced for pre-contact\ndetection in order to compensate for high delay and jitter communication links\nbetween master and slave as they appear in an Ethernet network. It is\ndemonstrated that the contact stiffness can be maintained in the order of\nmagnitude of the system performance identification with a demonstrated\nstiffness of 41kN/m in a stable condition.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 11:43:55 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Aiple", "Manuel", ""], ["Schiele", "Andr\u00e9", ""]]}, {"id": "1605.05122", "submitter": "Kursad Agpak", "authors": "Kursad Agpak, Huseyin Karateke, Suleyman Mete", "title": "Two-Finger Keyboard Layout Problem: An Application On Turkish Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart phone and tablet usage has sharply increased for the last decade. While\nentering test on these devices, virtual keyboards are generally used instead of\nconventional hardware keyboards. In this study, a new problem which is\ntwo-finger keyboard layout problem and solution approach is presented for\nincreasing user test entrance performance, especially on virtual keyboards.\nDefined two-finger keyboard layout problem is modeled as Quadratic Assignment\nProblem. Because of combinatorial structure of the problem a genetic algorithm\nis developed. Its result is given to mathematical model as initial solution for\nfinding better solutions with mathematical model. Proposed approach is applied\non Turkish language. The new two finger keyboard layout for Turkish language is\ncompared with F and QWERTY keyboard layouts based on certain performance\nmeasurement techniques.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 11:55:38 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Agpak", "Kursad", ""], ["Karateke", "Huseyin", ""], ["Mete", "Suleyman", ""]]}, {"id": "1605.05224", "submitter": "Laurel Riek", "authors": "Michael J. Gonzales, Joshua M. Henry, Aaron W. Calhoun, and Laurel D.\n  Riek", "title": "Visual TASK: A Collaborative Cognitive Aid for Acute Care Resuscitation", "comments": "8 pages, 5 figures", "journal-ref": "10th EAI International Conference on Pervasive Computing\n  Technologies for Healthcare (Pervasive Health), pp. 1-8, May, 2016", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preventable medical errors are a severe problem in healthcare, causing over\n400,000 deaths per year in the US in hospitals alone. In acute care, the branch\nof medicine encompassing the emergency department (ED) and intensive care units\n(ICU), error rates may be higher to due low situational awareness among\nclinicians performing resuscitation on patients. To support cognition, novice\nteam leaders may rely on reference guides to direct and anticipate future\nsteps. However, guides often act as a fixation point, diverting the leader's\nattention away from the team. To address this issue, we conducted a qualitative\nstudy that evaluates a collaborative cognitive aid co-designed with clinicians\ncalled Visual TASK. Our study explored the use of Visual TASK in three\nsimulations employing a projected shared display with two different interaction\nmodalities: the Microsoft Kinect and a touchscreen. Our results suggest that\ntools like the Kinect, while useful in other areas of acute care like the OR,\nare unsuitable for use in high-stress situations like resuscitation. We also\nobserved that fixation may not be constrained to reference guides alone, and\nmay extend to other objects in the room. We present our findings, and a\ndiscussion regarding future avenues in which collaborative cognitive aids may\nhelp in improving situational awareness in resuscitation.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 16:12:40 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Gonzales", "Michael J.", ""], ["Henry", "Joshua M.", ""], ["Calhoun", "Aaron W.", ""], ["Riek", "Laurel D.", ""]]}, {"id": "1605.05291", "submitter": "Jing Wu", "authors": "Jing Wu, Benjamin R. Shuman, Bingni W. Brunton, Katherine M. Steele,\n  Jared D. Olson, Rajesh P.N. Rao, Jeffrey G. Ojemann", "title": "Multistep Model for Predicting Upper-Limb 3D Isometric Force Application\n  from Pre-Movement Electrocorticographic Features", "comments": "4 pages, 3 figures, accepted to EMBC 2016 (38th Annual International\n  Conference of the IEEE Engineering in Medicine and Biology Society)", "journal-ref": null, "doi": "10.1109/EMBC.2016.7591010", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural correlates of movement planning onset and direction may be present in\nhuman electrocorticography in the signal dynamics of both motor and non-motor\ncortical regions. We use a three-stage model of jPCA reduced-rank hidden Markov\nmodel (jPCA-RR-HMM), regularized shrunken-centroid discriminant analysis (RDA),\nand LASSO regression to extract direction-sensitive planning information and\nmovement onset in an upper-limb 3D isometric force task in a human subject.\nThis mode achieves a relatively high true positive force-onset prediction rate\nof 60% within 250ms, and an above-chance 36% accuracy (17% chance) in\npredicting one of six planned 3D directions of isometric force using\npre-movement signals. We also find direction-distinguishing information up to\n400ms before force onset in the pre-movement signals, captured by electrodes\nplaced over the limb-ipsilateral dorsal premotor regions. This approach can\ncontribute to more accurate decoding of higher-level movement goals, at earlier\ntimescales, and inform sensor placement. Our results also contribute to further\nunderstanding of the spatiotemporal features of human motor planning.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2016 19:14:03 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Wu", "Jing", ""], ["Shuman", "Benjamin R.", ""], ["Brunton", "Bingni W.", ""], ["Steele", "Katherine M.", ""], ["Olson", "Jared D.", ""], ["Rao", "Rajesh P. N.", ""], ["Ojemann", "Jeffrey G.", ""]]}, {"id": "1605.05528", "submitter": "Charith Perera", "authors": "Tommy Nilsson, Carl Hogsden, Charith Perera, Saeed Aghaee, David\n  Scruton, Andreas Lund, Alan F. Blackwell", "title": "Applying Seamful Design in Location-based Mobile Museum Applications", "comments": "Accepted for ACM Transactions on Multimedia Computing Communications\n  and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of mobile computing is currently altering patterns of our\nbehavior to a greater degree than perhaps any other invention. In combination\nwith the introduction of power efficient wireless communication technologies,\nsuch as Bluetooth Low Energy (BLE), designers are today increasingly empowered\nto shape the way we interact with our physical surroundings and thus build\nentirely new experiences. However, our evaluations of BLE and its abilities to\nfacilitate mobile location-based experiences in public environments revealed a\nnumber of potential problems. Most notably, the position and orientation of the\nuser in combination with various environmental factors, such as crowds of\npeople traversing the space, were found to cause major fluctuations of the\nreceived BLE signal strength. These issues are rendering a seamless functioning\nof any location-based application practically impossible. Instead of achieving\nseamlessness by eliminating these technical issues, we thus choose to advocate\nthe use of a seamful approach, i.e. to reveal and exploit these problems and\nturn them into a part of the actual experience. In order to demonstrate the\nviability of this approach, we designed, implemented and evaluated the Ghost\nDetector - an educational location-based museum game for children. By\npresenting a qualitative evaluation of this game and by motivating our design\ndecisions, this paper provides insight into some of the challenges and possible\nsolutions connected to the process of developing location-based BLE-enabled\nexperiences for public cultural spaces.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2016 11:40:43 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 10:59:12 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Nilsson", "Tommy", ""], ["Hogsden", "Carl", ""], ["Perera", "Charith", ""], ["Aghaee", "Saeed", ""], ["Scruton", "David", ""], ["Lund", "Andreas", ""], ["Blackwell", "Alan F.", ""]]}, {"id": "1605.06043", "submitter": "Andres Ledesma", "authors": "Andres Ledesma, Mohammed Al-Musawi, Hannu Nieminen", "title": "Health Figures: An Open Source JavaScript Library for Health Data\n  Visualization", "comments": "BMC Medical Informatics and Decision Making 16.1 (2016)", "journal-ref": null, "doi": "10.1186/s12911-016-0275-6", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The way we look at data has a great impact on how we can understand it,\nparticularly when the data is related to health and wellness. Due to the\nincreased use of self-tracking devices and the ongoing shift towards preventive\nmedicine, better understanding of our health data is an important part of\nimproving the general welfare of the citizens. Electronic Health Records,\nself-tracking devices and mobile applications provide a rich variety of data\nbut it often becomes difficult to understand. We implemented the hFigures\nlibrary inspired on the hGraph visualization with additional improvements. The\npurpose of the library is to provide a visual representation of the evolution\nof health measurements in a complete and useful manner. We researched the\nusefulness and usability of the library by building an application for health\ndata visualization in a health coaching program. We performed a user evaluation\nwith Heuristic Evaluation, Controlled User Testing and Usability\nQuestionnaires. In the Heuristics Evaluation the average response was 6.3 out\nof 7 points and the Cognitive Walkthrough done by usability experts indicated\nno design or mismatch errors. In the CSUQ usability test the system obtained an\naverage score of 6.13 out of 7, and in the ASQ usability test the overall\nsatisfaction score was 6.64 out of 7. We developed hFigures, an open source\nlibrary for visualizing a complete, accurate and normalized graphical\nrepresentation of health data. The idea is based on the concept of the hGraph\nbut it provides additional key features, including a comparison of multiple\nhealth measurements over time. We conducted a usability evaluation of the\nlibrary as a key component of an application for health and wellness\nmonitoring. The results indicate that the data visualization library was\nhelpful in assisting users in understanding health data and its evolution over\ntime.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2016 16:34:29 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 07:46:00 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Ledesma", "Andres", ""], ["Al-Musawi", "Mohammed", ""], ["Nieminen", "Hannu", ""]]}, {"id": "1605.07722", "submitter": "Longqi Yang", "authors": "Longqi Yang, Cheng-Kang Hsieh, Hongjian Yang, Nicola Dell, Serge\n  Belongie, Curtis Cole, Deborah Estrin", "title": "Yum-me: A Personalized Nutrient-based Meal Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nutrient-based meal recommendations have the potential to help individuals\nprevent or manage conditions such as diabetes and obesity. However, learning\npeople's food preferences and making recommendations that simultaneously appeal\nto their palate and satisfy nutritional expectations are challenging. Existing\napproaches either only learn high-level preferences or require a prolonged\nlearning period. We propose Yum-me, a personalized nutrient-based meal\nrecommender system designed to meet individuals' nutritional expectations,\ndietary restrictions, and fine-grained food preferences. Yum-me enables a\nsimple and accurate food preference profiling procedure via a visual quiz-based\nuser interface, and projects the learned profile into the domain of\nnutritionally appropriate food options to find ones that will appeal to the\nuser. We present the design and implementation of Yum-me, and further describe\nand evaluate two innovative contributions. The first contriution is an open\nsource state-of-the-art food image analysis model, named FoodDist. We\ndemonstrate FoodDist's superior performance through careful benchmarking and\ndiscuss its applicability across a wide array of dietary applications. The\nsecond contribution is a novel online learning framework that learns food\npreference from item-wise and pairwise image comparisons. We evaluate the\nframework in a field study of 227 anonymous users and demonstrate that it\noutperforms other baselines by a significant margin. We further conducted an\nend-to-end validation of the feasibility and effectiveness of Yum-me through a\n60-person user study, in which Yum-me improves the recommendation acceptance\nrate by 42.63%.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 04:13:49 GMT"}, {"version": "v2", "created": "Wed, 21 Dec 2016 14:48:18 GMT"}, {"version": "v3", "created": "Sun, 30 Apr 2017 17:43:02 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Yang", "Longqi", ""], ["Hsieh", "Cheng-Kang", ""], ["Yang", "Hongjian", ""], ["Dell", "Nicola", ""], ["Belongie", "Serge", ""], ["Cole", "Curtis", ""], ["Estrin", "Deborah", ""]]}, {"id": "1605.07733", "submitter": "Radoslava Kraleva Dr.", "authors": "Radoslava Kraleva, Velin Kralev", "title": "On model architecture for a children's speech recognition interactive\n  dialog system", "comments": "6 pages, 2 figures, in proc. of conference FMNS 2009, Blagoevgrad,\n  Bulgaria", "journal-ref": "Third International Scientific Conference \"Mathematics and Natural\n  Sciences\", Vol. (1), pp. 106-111, 2009", "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report presents a general model of the architecture of information\nsystems for the speech recognition of children. It presents a model of the\nspeech data stream and how it works. The result of these studies and presented\nveins architectural model shows that research needs to be focused on\nacoustic-phonetic modeling in order to improve the quality of children's speech\nrecognition and the sustainability of the systems to noise and changes in\ntransmission environment. Another important aspect is the development of more\naccurate algorithms for modeling of spontaneous child speech.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 04:57:42 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Kraleva", "Radoslava", ""], ["Kralev", "Velin", ""]]}, {"id": "1605.07735", "submitter": "Radoslava Kraleva Dr.", "authors": "Radoslava Kraleva", "title": "Design and development a children's speech database", "comments": "8 pages, 2 figures, 1 table, conference FMNS 2011, Blagoevgrad,\n  Bulgaria", "journal-ref": "Fourth International Scientific Conference \"Mathematics and\n  Natural Sciences\" 2011, Bulgaria, Vol. (2), pp. 41-48", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The report presents the process of planning, designing and the development of\na database of spoken children's speech whose native language is Bulgarian. The\nproposed model is designed for children between the age of 4 and 6 without\nspeech disorders, and reflects their specific capabilities. At this age most\nchildren cannot read, there is no sustained concentration, they are emotional,\netc. The aim is to unite all the media information accompanying the recording\nand processing of spoken speech, thereby to facilitate the work of researchers\nin the field of speech recognition. This database will be used for the\ndevelopment of systems for children's speech recognition, children's speech\nsynthesis systems, games which allow voice control, etc. As a result of the\nproposed model a prototype system for speech recognition is presented.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 05:04:11 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Kraleva", "Radoslava", ""]]}, {"id": "1605.07760", "submitter": "Jens Grubert", "authors": "Jens Grubert, Matthias Kranz, Aaron Quigley", "title": "Challenges in Mobile Multi-Device Ecosystems", "comments": null, "journal-ref": "mUX: The Journal of Mobile User Experience, 5(1), 1-22, 2016", "doi": "10.1186/s13678-016-0007-y", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coordinated multi-display environments from the desktop, second-screen to\ngigapixel display walls are increasingly common. Personal and intimate mobile\nand wearable devices such as head-mounted displays, smartwatches, smartphones\nand tablets are rarely part of such multi-device ecosystems. With this paper,\nwe contribute to a better understanding about factors that impede the creation\nand use of such mobile multi-device ecosystems. We base our findings on\nliterature research and an expert survey. Specifically, we present grounded\nchallenges relevant for the design, development and use of mobile multi-device\nenvironments.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 07:26:02 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Grubert", "Jens", ""], ["Kranz", "Matthias", ""], ["Quigley", "Aaron", ""]]}, {"id": "1605.08035", "submitter": "Luis Valente", "authors": "Luis Valente, Bruno Feijo, Alexandre Ribeiro Silva, Esteban Clua", "title": "Notes on Pervasive Virtuality", "comments": "Tech report MCC01/16 (Monografias em Ci\\^encia da Computa\\c{c}\\~ao,\n  May 2016, PUC-Rio, ISSN 0103-9741), discussion paper in progress to IFIP ICEC\n  2016", "journal-ref": null, "doi": null, "report-no": "MCC01/16", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes current notes about a new mixed-reality paradigm that\nwe named as \"pervasive virtuality\". This paradigm has emerged recently in\nindustry and academia through different initiatives. In this paper we intend to\nexplore this new area by proposing a set of features that we identified as\nimportant or helpful to realize pervasive virtuality in games and entertainment\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2016 12:01:46 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Valente", "Luis", ""], ["Feijo", "Bruno", ""], ["Silva", "Alexandre Ribeiro", ""], ["Clua", "Esteban", ""]]}, {"id": "1605.08117", "submitter": "Jitao Sang", "authors": "Jitao Sang, Huaiwen Zhang, Changsheng Xu", "title": "Visual BFI: an Exploratory Study for Image-based Personality Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper positions and explores the topic of image-based personality test.\nInstead of responding to text-based questions, the subjects will be provided a\nset of \"choose-your-favorite-image\" visual questions. With the image options of\neach question belonging to the same concept, the subjects' personality traits\nare estimated by observing their preferences of images under several unique\nconcepts. The solution to design such an image-based personality test consists\nof concept-question identification and image-option selection. We have\npresented a preliminary framework to regularize these two steps in this\nexploratory study. A demo version of the designed image-based personality test\nis available at http://www.visualbfi.org/. Subjective as well as objective\nevaluations have demonstrated the feasibility of image-based personality test\nin limited questions.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 01:28:29 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Sang", "Jitao", ""], ["Zhang", "Huaiwen", ""], ["Xu", "Changsheng", ""]]}, {"id": "1605.08261", "submitter": "Emilio Leonardi", "authors": "A. Tarable and A. Nordio and E. Leonardi and M. Ajmone Marsan", "title": "The Importance of Worker Reputation Information in Microtask-Based Crowd\n  Work Systems", "comments": "To appear in IEEE TPDS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first systematic investigation of the potential\nperformance gains for crowd work systems, deriving from available information\nat the requester about individual worker reputation. In particular, we first\nformalize the optimal task assignment problem when workers' reputation\nestimates are available, as the maximization of a monotone (submodular)\nfunction subject to Matroid constraints. Then, being the optimal problem\nNP-hard, we propose a simple but efficient greedy heuristic task allocation\nalgorithm. We also propose a simple \"maximum a-posteriori\" decision rule and a\ndecision algorithm based on message passing. Finally, we test and compare\ndifferent solutions, showing that system performance can greatly benefit from\ninformation about workers' reputation. Our main findings are that: i) even\nlargely inaccurate estimates of workers' reputation can be effectively\nexploited in the task assignment to greatly improve system performance; ii) the\nperformance of the maximum a-posteriori decision rule quickly degrades as\nworker reputation estimates become inaccurate; iii) when workers' reputation\nestimates are significantly inaccurate, the best performance can be obtained by\ncombining our proposed task assignment algorithm with the message-passing\ndecision algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 13:14:33 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Tarable", "A.", ""], ["Nordio", "A.", ""], ["Leonardi", "E.", ""], ["Marsan", "M. Ajmone", ""]]}, {"id": "1605.08313", "submitter": "Anvesha Amaravati", "authors": "Anvesha A, Shaojie Xu, Ningyuan Cao, Justin Romberg and Arijit\n  Raychowdhury", "title": "A Light-powered, Always-On, Smart Camera with Compressed Domain Gesture\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an energy-efficient camera-based gesture recognition\nsystem powered by light energy for \"always on\" applications. Low energy\nconsumption is achieved by directly extracting gesture features from the\ncompressed measurements, which are the block averages and the linear\ncombinations of the image sensor's pixel values. The gestures are recognized\nusing a nearest-neighbour (NN) classifier followed by Dynamic Time Warping\n(DTW). The system has been implemented on an Analog Devices Black Fin ULP\nvision processor and powered by PV cells whose output is regulated by TI's\nDC-DC buck converter with Maximum Power Point Tracking (MPPT). Measured data\nreveals that with only 400 compressed measurements (768x compression ratio) per\nframe, the system is able to recognize key wake-up gestures with greater than\n80% accuracy and only 95mJ of energy per frame. Owing to its fully self-powered\noperation, the proposed system can find wide applications in \"always-on\" vision\nsystems such as in surveillance, robotics and consumer electronics with\ntouch-less operation.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 14:52:19 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 06:38:45 GMT"}], "update_date": "2016-10-05", "authors_parsed": [["A", "Anvesha", ""], ["Xu", "Shaojie", ""], ["Cao", "Ningyuan", ""], ["Romberg", "Justin", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "1605.08464", "submitter": "Vivek Sharma", "authors": "Vivek Sharma and Sule Yildirim-Yayilgan and Luc Van Gool", "title": "Low-Cost Scene Modeling using a Density Function Improves Segmentation\n  Performance", "comments": "accepted for publication at 25th IEEE International Symposium on\n  Robot and Human Interactive Communication (RO-MAN), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a low cost and effective way to combine a free simulation software\nand free CAD models for modeling human-object interaction in order to improve\nhuman & object segmentation. It is intended for research scenarios related to\nsafe human-robot collaboration (SHRC) and interaction (SHRI) in the industrial\ndomain. The task of human and object modeling has been used for detecting\nactivity, and for inferring and predicting actions, different from those works,\nwe do human and object modeling in order to learn interactions in RGB-D data\nfor improving segmentation. For this purpose, we define a novel density\nfunction to model a three dimensional (3D) scene in a virtual environment\n(VREP). This density function takes into account various possible\nconfigurations of human-object and object-object relationships and interactions\ngoverned by their affordances. Using this function, we synthesize a large,\nrealistic and highly varied synthetic RGB-D dataset that we use for training.\nWe train a random forest classifier, and the pixelwise predictions obtained is\nintegrated as a unary term in a pairwise conditional random fields (CRF). Our\nevaluation shows that modeling these interactions improves segmentation\nperformance by ~7\\% in mean average precision and recall over state-of-the-art\nmethods that ignore these interactions in real-world data. Our approach is\ncomputationally efficient, robust and can run real-time on consumer hardware.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2016 22:34:37 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Sharma", "Vivek", ""], ["Yildirim-Yayilgan", "Sule", ""], ["Van Gool", "Luc", ""]]}, {"id": "1605.08485", "submitter": "Sabrina Nusrat", "authors": "Sabrina Nusrat and Stephen Kobourov", "title": "The State of the Art in Cartograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartograms combine statistical and geographical information in thematic maps,\nwhere areas of geographical regions (e.g., countries, states) are scaled in\nproportion to some statistic (e.g., population, income). Cartograms make it\npossible to gain insight into patterns and trends in the world around us and\nhave been very popular visualizations for geo-referenced data for over a\ncentury. This work surveys cartogram research in visualization, cartography and\ngeometry, covering a broad spectrum of different cartogram types: from the\ntraditional rectangular and table cartograms, to Dorling and diffusion\ncartograms. A particular focus is the study of the major cartogram dimensions:\nstatistical accuracy, geographical accuracy, and topological accuracy. We\nreview the history of cartograms, describe the algorithms for generating them,\nand consider task taxonomies. We also review quantitative and qualitative\nevaluations, and we use these to arrive at design guidelines and research\nchallenges.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 00:58:30 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 06:45:40 GMT"}, {"version": "v3", "created": "Tue, 6 Sep 2016 05:46:45 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Nusrat", "Sabrina", ""], ["Kobourov", "Stephen", ""]]}, {"id": "1605.08548", "submitter": "Andres Monroy-Hernandez", "authors": "Justin Cranshaw, Andr\\'es Monroy-Hern\\'andez, S.A. Needham", "title": "Journeys & Notes: Designing Social Computing for Non-Places", "comments": "CHI '16 Proceedings of the 2016 CHI Conference on Human Factors in\n  Computing Systems", "journal-ref": null, "doi": "10.1145/2858036.2858573", "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a mobile application we designed and engineered to\nenable people to log their travels near and far, leave notes behind, and build\na community around spaces in between destinations. Our design explores new\nground for location-based social computing systems, identifying opportunities\nwhere these systems can foster the growth of on-line communities rooted at\nnon-places. In our work we develop, explore, and evaluate several innovative\nfeatures designed around four usage scenarios: daily commuting, long-distance\ntraveling, quantified traveling, and journaling. We present the results of two\nsmall-scale user studies, and one large-scale, world-wide deployment,\nsynthesizing the results as potential opportunities and lessons learned in\ndesigning social computing for non-places.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 09:03:22 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Cranshaw", "Justin", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""], ["Needham", "S. A.", ""]]}, {"id": "1605.08749", "submitter": "David Gotz", "authors": "David Gotz, Brandon A. Price, Annie T. Chen", "title": "Visual Model Validation via Inline Replication", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": "10.1177/1473871618821747", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualizations typically show retrospective views of an existing dataset\nwith little or no focus on repeatability. However, consumers of these tools\noften use insights gleaned from retrospective visualizations as the basis for\ndecisions about future events. In this way, visualizations often serve as\nvisual predictive models despite the fact that they are typically designed to\npresent historical views of the data. This \"visual predictive model\" approach,\nhowever, can lead to invalid inferences. In this paper, we describe an approach\nto visual model validation called Inline Replication (IR) which, similar to the\ncross-validation technique used widely in machine learning, provides a\nnonparametric and broadly applicable technique for visual model assessment and\nrepeatability. This paper describes the overall IR process and outlines how it\ncan be integrated into both traditional and emerging \"big data\" visualization\npipelines. Examples are provided showing IR integrated within common\nvisualization techniques (such as bar charts and linear regression lines) as\nwell as a more fully-featured visualization system designed for complex\nexploratory analysis tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 18:38:09 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Gotz", "David", ""], ["Price", "Brandon A.", ""], ["Chen", "Annie T.", ""]]}, {"id": "1605.08766", "submitter": "Andres Monroy-Hernandez", "authors": "Sayamindu Dasgupta, William Hale, Andr\\'es Monroy-Hern\\'andez, and\n  Benjamin Mako Hill", "title": "Remixing as a Pathway to Computational Thinking", "comments": "In Proceedings of the 19th ACM Conference on Computer-Supported\n  Cooperative Work & Social Computing (CSCW '16)", "journal-ref": null, "doi": "10.1145/2818048.2819984", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Theorists and advocates of \"remixing\" have suggested that appropriation can\nact as a pathway for learning. We test this theory quantitatively using data\nfrom more than 2.4 million multimedia programming projects shared by more than\n1 million users in the Scratch online community. First, we show that users who\nremix more often have larger repertoires of programming commands even after\ncontrolling for the numbers of projects and amount of code shared. Second, we\nshow that exposure to computational thinking concepts through remixing is\nassociated with increased likelihood of using those concepts. Our results\nsupport theories that young people learn through remixing, and have important\nimplications for designers of social computing systems.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 19:34:17 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Dasgupta", "Sayamindu", ""], ["Hale", "William", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""], ["Hill", "Benjamin Mako", ""]]}, {"id": "1605.08817", "submitter": "Kerstin Eder", "authors": "Adriana Hamacher, Nadia Bianchi-Berthouze, Anthony G. Pipe and Kerstin\n  Eder", "title": "Believing in BERT: Using expressive communication to enhance trust and\n  counteract operational error in physical Human-Robot Interaction", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": "10.1109/ROMAN.2016.7745163", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategies are necessary to mitigate the impact of unexpected behavior in\ncollaborative robotics, and research to develop solutions is lacking. Our aim\nhere was to explore the benefits of an affective interaction, as opposed to a\nmore efficient, less error prone but non-communicative one. The experiment took\nthe form of an omelet-making task, with a wide range of participants\ninteracting directly with BERT2, a humanoid robot assistant. Having significant\nimplications for design, results suggest that efficiency is not the most\nimportant aspect of performance for users; a personable, expressive robot was\nfound to be preferable over a more efficient one, despite a considerable trade\noff in time taken to perform the task. Our findings also suggest that a robot\nexhibiting human-like characteristics may make users reluctant to 'hurt its\nfeelings'; they may even lie in order to avoid this.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2016 23:32:47 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2016 10:03:59 GMT"}, {"version": "v3", "created": "Wed, 22 Jun 2016 16:16:32 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Hamacher", "Adriana", ""], ["Bianchi-Berthouze", "Nadia", ""], ["Pipe", "Anthony G.", ""], ["Eder", "Kerstin", ""]]}, {"id": "1605.08841", "submitter": "Andres Monroy-Hernandez", "authors": "Charles Kiene, Andr\\'es Monroy-Hern\\'andez, Benjamin Mako Hill", "title": "Surviving an \"Eternal September\" - How an Online Community Managed a\n  Surge of Newcomers", "comments": "In Proceedings of the 34th Annual ACM Conference on Human Factors in\n  Computing Systems (CHI 2016). ACM, New York, NY, USA", "journal-ref": null, "doi": "10.1145/2858036.2858356", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a qualitative analysis of interviews with participants in the\nNoSleep community within Reddit where millions of fans and writers of horror\nfiction congregate. We explore how the community handled a massive, sudden, and\nsustained increase in new members. Although existing theory and stories like\nUsenet's infamous \"Eternal September\" suggest that large influxes of newcomers\ncan hurt online communities, our interviews suggest that NoSleep survived\nwithout major incident. We propose that three features of NoSleep allowed it to\nmanage the rapid influx of newcomers gracefully: (1) an active and\nwell-coordinated group of administrators, (2) a shared sense of community which\nfacilitated community moderation, and (3) technological systems that mitigated\nnorm violations. We also point to several important trade-offs and limitations.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 03:58:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kiene", "Charles", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""], ["Hill", "Benjamin Mako", ""]]}, {"id": "1605.08928", "submitter": "Yang Cheng", "authors": "Yang Cheng", "title": "Virtual Reality based Learning Systems", "comments": "Computer Communication Technology, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is based on studies of the existing literature, focusing on the\nstates-of-the-arts on virtual reality (VR) and its potential uses in learning.\nDifferent platforms have been used to improve the learning effects of VR that\noffers exciting opportunities in various fields. As more and more students want\nin a distance, part-time, or want to continue their education, VR has attracted\nconsiderable attention in learning, training, and traditional education. VR\nbased learning enables operators to bring together all disciplinary resources\nin a common playground. The VR base multimedia platform has successfully\ndemonstrated great potential of education and training. In this paper, we will\ndiscuss existing systems and their uses and address the technical challenges\nand future directions.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2016 19:29:13 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Cheng", "Yang", ""]]}, {"id": "1605.09432", "submitter": "Ramanathan Subramanian", "authors": "Ramanathan Subramanian, Romer Rosales, Glenn Fung, Jennifer Dy", "title": "Evaluating Crowdsourcing Participants in the Absence of Ground-Truth", "comments": "4 pages, 5 figures, Workshop on Human Computation for Science and\n  Computational Sustainability, NIPS 2012, Lake Tahoe, NV. 7 Dec 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a supervised/semi-supervised learning scenario where multiple\nannotators are available, we consider the problem of identification of\nadversarial or unreliable annotators.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2016 22:05:36 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Subramanian", "Ramanathan", ""], ["Rosales", "Romer", ""], ["Fung", "Glenn", ""], ["Dy", "Jennifer", ""]]}, {"id": "1605.09505", "submitter": "Sarit Kraus", "authors": "Moshe Bitan, Galit Nahari, Zvi Nisin, Ariel Roth, Sarit Kraus", "title": "Psychologically based Virtual-Suspect for Interrogative Interview\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a Virtual-Suspect system which can be used to train\ninexperienced law enforcement personnel in interrogation strategies. The system\nsupports different scenario configurations based on historical data. The\nresponses presented by the Virtual-Suspect are selected based on the\npsychological state of the suspect, which can be configured as well.\nFurthermore, each interrogator's statement affects the Virtual-Suspect's\ncurrent psychological state, which may lead the interrogation in different\ndirections. In addition, the model takes into account the context in which the\nstatements are made. Experiments with 24 subjects demonstrate that the\nVirtual-Suspect's behavior is similar to that of a human who plays the role of\nthe suspect.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 06:43:51 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Bitan", "Moshe", ""], ["Nahari", "Galit", ""], ["Nisin", "Zvi", ""], ["Roth", "Ariel", ""], ["Kraus", "Sarit", ""]]}, {"id": "1605.09678", "submitter": "Umar Ruhi", "authors": "Umar Ruhi", "title": "Level Up Your Strategy: Towards a Descriptive Framework for Meaningful\n  Enterprise Gamification", "comments": "12 pages, 3 figure, 2 tables", "journal-ref": "Technology Innovation Management Review, 2015, 5(8): 5-16", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gamification initiatives are currently top-of-mind for many organizations\nseeking to engage their employees in creative ways, improve their productivity,\nand drive positive behavioural outcomes in their workforce - ultimately leading\nto positive business outcomes on the whole. Despite its touted benefits, little\nempirical research has been done to date to investigate technological and\nindividual personal factors that determine the success or failure of enterprise\ngamification initiatives. In this article, we provide a summary of our\npreliminary research findings from three case studies of gamification\ninitiatives across different business contexts and present an empirically\nvalidated descriptive framework that details the key success factors for\nenterprise gamification. Our adaptation of the mechanics, dynamics, and\naesthetics (MDA) framework for enterprise gamification aims to explicate the\nconnections between end-user motivations, interactive gameplay elements, and\ntechnology features and functions that constitute effective gamification\ninterventions in the enterprise. Following a discussion of the core elements in\nthe framework and their interrelationships, the implications of our research\nare presented in the form of guidelines for the management and design of\ngamification initiatives and applications. The research findings presented in\nthis article can potentially aid in the development of game mechanics that\ntranslate into positive user experiences and foster higher levels of employee\nengagement. Additionally, our research findings provide insights on key success\nfactors for the effective adoption and institutionalization of enterprise\ngamification initiatives in organizations, and subsequently help them enhance\nthe performance of their employees and drive positive business outcomes.\n", "versions": [{"version": "v1", "created": "Sun, 29 May 2016 20:41:54 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ruhi", "Umar", ""]]}, {"id": "1605.09769", "submitter": "Pariya Kashfi", "authors": "Pariya Hajar Kashfi", "title": "Towards Usable openEHR-aware Clinical Decision Support: A User-centered\n  Design Approach", "comments": "Thesis for the degree of Licentiate of Engineering, Chalmers\n  University of Technology", "journal-ref": null, "doi": null, "report-no": "ISSN 1651-4769;7", "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis addresses the question of how usable openEHR-aware clinical\ndecision support can be designed and developed in order to improve the quality\nof health care. To answer this research question, several sub-questions were\nidentified and investigated. This included analyzing state of the art in two\ndifferent aspects of design and development and evaluation of Clinical Decision\nSupport (CDS) and also investigating application of a customized user-centered\ndesign (UCD) process in developing openEHR-based clinical applications.\nAnalysis of state of the art in interplay between Human-Computer Interaction\n(HCI) and CDS and also the intersection between CDS and Electronic Health\nRecords (EHR) revealed that consideration of both HCI and integration of CDS\ninto EHR is more appreciated in theory than in practice and there is still a\nlong way to go before reaching an acceptable level in these two success factors\nof CDS. Moreover, the experience in designing an openEHR-based clinical\napplication revealed that apart from benefits offered by openEHR approach, such\nas specifying different roles and involvement of domain experts in defining\ndomain concepts, there are various shortcomings that need to be improved, for\ninstance the limited support for openEHR application developers. Additionally,\nthis study revealed that there are characteristics of the domain, tasks and\nusers in the domain that developers should be informed about while applying UCD\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2016 19:03:33 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Kashfi", "Pariya Hajar", ""]]}]