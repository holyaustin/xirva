[{"id": "2001.00062", "submitter": "Hiba Arnout", "authors": "Hiba Arnout and Johannes Kehrer and Johanna Bronner and Thomas Runkler", "title": "Visual Evaluation of Generative Adversarial Networks for Time Series\n  Data", "comments": "To appear in the Proceedings of the Human-Centered AI:\n  Trustworthiness of AI Models & Data (HAI) track at AAAI Fall Symposium, DC,\n  November 7-9, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A crucial factor to trust Machine Learning (ML) algorithm decisions is a good\nrepresentation of its application field by the training dataset. This is\nparticularly true when parts of the training data have been artificially\ngenerated to overcome common training problems such as lack of data or\nimbalanced dataset. Over the last few years, Generative Adversarial Networks\n(GANs) have shown remarkable results in generating realistic data. However,\nthis ML approach lacks an objective function to evaluate the quality of the\ngenerated data. Numerous GAN applications focus on generating image data mostly\nbecause they can be easily evaluated by a human eye. Less efforts have been\nmade to generate time series data. Assessing their quality is more complicated,\nparticularly for technical data. In this paper, we propose a human-centered\napproach supporting a ML or domain expert to accomplish this task using Visual\nAnalytics (VA) techniques. The presented approach consists of two views, namely\na GAN Iteration View showing similarity metrics between real and generated data\nover the iterations of the generation process and a Detailed Comparative View\nequipped with different time series visualizations such as TimeHistograms, to\ncompare the generated data at different iteration steps. Starting from the GAN\nIteration View, the user can choose suitable iteration steps for detailed\ninspection. We evaluate our approach with a usage scenario that enabled an\nefficient comparison of two different GAN models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 13:59:33 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Arnout", "Hiba", ""], ["Kehrer", "Johannes", ""], ["Bronner", "Johanna", ""], ["Runkler", "Thomas", ""]]}, {"id": "2001.00453", "submitter": "Supriya Sarker", "authors": "Supriya Sarker, Md. Sajedur Rahman, Mohammad Nazmus Sakib", "title": "An Approach Towards Intelligent Accident Detection, Location Tracking\n  and Notification System", "comments": "The 3rd IEEE International Conference on Telecommunications and\n  Photonics (ICTP) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Advancement in transportation system has boosted speed of our lives.\nMeantime, road traffic accident is a major global health issue resulting huge\nloss of lives, properties and valuable time. It is considered as one of the\nreasons of highest rate of death nowadays. Accident creates catastrophic\nsituation for victims, especially accident occurs in highways imposes great\nadverse impact on large numbers of victims. In this paper, we develop an\nintelligent accident detection, location tracking and notification system that\ndetects an accident immediately when it takes place. Global Positioning System\n(GPS) device finds the exact location of accident. Global System for Mobile\n(GSM) module sends a notification message including the link of location in the\ngoogle map to the nearest police control room and hospital so that they can\nvisit the link, find out the shortest route of the accident spot and take\ninitiatives to speed up the rescue process.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 14:15:18 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Sarker", "Supriya", ""], ["Rahman", "Md. Sajedur", ""], ["Sakib", "Mohammad Nazmus", ""]]}, {"id": "2001.00470", "submitter": "Jianzhu Huai", "authors": "Jianzhu Huai, Yujia Zhang, Alper Yilmaz", "title": "The Mobile AR Sensor Logger for Android and iOS Devices", "comments": "4 pages, 4 figures, submitted to IEEE Sensors 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, commodity mobile devices equipped with cameras and inertial\nmeasurement units (IMUs) have attracted much research and design effort for\naugmented reality (AR) and robotics applications. Based on such sensors, many\ncommercial AR toolkits and public benchmark datasets have been made available\nto accelerate hatching and validating new ideas. To lower the difficulty and\nenhance the flexibility in accessing the rich raw data of typical AR sensors on\nmobile devices, this paper present the mobile AR sensor (MARS) logger for two\nof the most popular mobile operating systems, Android and iOS. The logger\nhighlights the best possible synchronization between the camera and the IMU\nallowed by a mobile device, and efficient saving of images at about 30Hz, and\nrecording the metadata relevant to AR applications. This logger has been tested\non a relatively large spectrum of mobile devices, and the collected data has\nbeen used for analyzing the sensor characteristics. We see that this\napplication will facilitate research and development related to AR and\nrobotics, so it has been open sourced at\nhttps://github.com/OSUPCVLab/mobile-ar-sensor-logger.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2019 13:41:02 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Huai", "Jianzhu", ""], ["Zhang", "Yujia", ""], ["Yilmaz", "Alper", ""]]}, {"id": "2001.00471", "submitter": "Haruna Isah", "authors": "Kennedy Ralston, Yuhao Chen, Haruna Isah, Farhana Zulkernine", "title": "A Voice Interactive Multilingual Student Support System using IBM Watson", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems powered by artificial intelligence are being developed to be more\nuser-friendly by communicating with users in a progressively human-like\nconversational way. Chatbots, also known as dialogue systems, interactive\nconversational agents, or virtual agents are an example of such systems used in\na wide variety of applications ranging from customer support in the business\ndomain to companionship in the healthcare sector. It is becoming increasingly\nimportant to develop chatbots that can best respond to the personalized needs\nof their users so that they can be as helpful to the user as possible in a real\nhuman way. This paper investigates and compares three popular existing chatbots\nAPI offerings and then propose and develop a voice interactive and multilingual\nchatbot that can effectively respond to users mood, tone, and language using\nIBM Watson Assistant, Tone Analyzer, and Language Translator. The chatbot was\nevaluated using a use case that was targeted at responding to users needs\nregarding exam stress based on university students survey data generated using\nGoogle Forms. The results of measuring the chatbot effectiveness at analyzing\nresponses regarding exam stress indicate that the chatbot responding\nappropriately to the user queries regarding how they are feeling about exams\n76.5%. The chatbot could also be adapted for use in other application areas\nsuch as student info-centers, government kiosks, and mental health support\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 18:58:25 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Ralston", "Kennedy", ""], ["Chen", "Yuhao", ""], ["Isah", "Haruna", ""], ["Zulkernine", "Farhana", ""]]}, {"id": "2001.00575", "submitter": "Singamsetti Mohan Sai", "authors": "Mona teja K, Mohan Sai. S, H S S S Raviteja D, Sai Kushagra P V", "title": "Smart Summarizer for Blind People", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's world, time is a very important resource. In our busy lives, most\nof us hardly have time to read the complete news so what we have to do is just\ngo through the headlines and satisfy ourselves with that. As a result, we might\nmiss a part of the news or misinterpret the complete thing. The situation is\neven worse for the people who are visually impaired or have lost their ability\nto see. The inability of these people to read text has a huge impact on their\nlives. There are a number of methods for blind people to read the text. Braille\nscript, in particular, is one of the examples, but it is a highly inefficient\nmethod as it is really time taking and requires a lot of practice. So, we\npresent a method for visually impaired people based on the sense of sound which\nis obviously better and more accurate than the sense of touch. This paper deals\nwith an efficient method to summarize news into important keywords so as to\nsave the efforts to go through the complete text every single time. This paper\ndeals with many API's and modules like the tesseract, GTTS, and many algorithms\nthat have been discussed and implemented in detail such as Luhn's Algorithm,\nLatent Semantic Analysis Algorithm, Text Ranking Algorithm. And the other\nfunctionality that this paper deals with is converting the summarized text to\nspeech so that the system can aid even the blind people.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jan 2020 20:39:22 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["K", "Mona teja", ""], ["S", "Mohan Sai.", ""], ["D", "H S S S Raviteja", ""], ["P", "Sai Kushagra", "V"]]}, {"id": "2001.00580", "submitter": "Thomas Drugman", "authors": "Thomas Drugman, Jerome Urbain, Thierry Dutoit", "title": "Assessment of Audio Features for Automatic Cough Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the issue of cough detection using only audio\nrecordings, with the ultimate goal of quantifying and qualifying the degree of\npathology for patients suffering from respiratory diseases, notably\nmucoviscidosis. A large set of audio features describing various aspects of the\naudio signal is proposed. These features are assessed in two steps. First,\ntheir intrisic potential and redundancy are evaluated using mutual\ninformation-based measures. Secondly, their efficiency is confirmed relying on\nthree classifiers: Artificial Neural Network, Gaussian Mixture Model and\nSupport Vector Machine. The influence of both the feature dimension and the\nclassifier complexity are also investigated.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 09:30:23 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Drugman", "Thomas", ""], ["Urbain", "Jerome", ""], ["Dutoit", "Thierry", ""]]}, {"id": "2001.00728", "submitter": "Lei Zhang", "authors": "Lei Zhang, Weihai Chen, Yuan Chai, Jianhua Wang, Jianbin Zhang", "title": "Gait Graph Optimization: Generate Variable Gaits from One Base Gait for\n  Lower-limb Rehabilitation Exoskeleton Robots", "comments": "8 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most concentrated application of lower-limb rehabilitation exoskeleton\n(LLE) robot is that it can help paraplegics \"re-walk\". However, \"walking\" in\ndaily life is more than just walking on flat ground with fixed gait. This paper\nfocuses on variable gaits generation for LLE robot to adapt complex walking\nenvironment. Different from traditional gaits generator for biped robot, the\ngenerated gaits for LLEs should be comfortable to patients. Inspired by the\npose graph optimization algorithm in SLAM, we propose a graph-based gait\ngeneration algorithm called gait graph optimization (GGO) to generate variable,\nfunctional and comfortable gaits from one base gait collected from healthy\nindividuals to adapt the walking environment. Variants of walking problem,\ne.g., stride adjustment, obstacle avoidance, and stair ascent and descent, help\nverify the proposed approach in simulation and experimentation. We open source\nour implementation.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 05:31:51 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 09:00:45 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Zhang", "Lei", ""], ["Chen", "Weihai", ""], ["Chai", "Yuan", ""], ["Wang", "Jianhua", ""], ["Zhang", "Jianbin", ""]]}, {"id": "2001.00892", "submitter": "Adrien Coppens", "authors": "Adrien Coppens and Berat Bicer and Naz Yilmaz and Serhat Aras", "title": "Exploration of Interaction Techniques for Graph-based Modelling in\n  Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Editing and manipulating graph-based models within immersive environments is\nlargely unexplored and certain design activities could benefit from using those\ntechnologies. For example, in the case study of architectural modelling, the 3D\ncontext of Virtual Reality naturally matches the intended output product, i.e.\na 3D architectural geometry. Since both the state of the art and the state of\nthe practice are lacking, we explore the field of VR-based interactive\nmodelling, and provide insights as to how to implement proper interactions in\nthat context, with broadly available devices. We consequently produce several\nopen-source software prototypes for manipulating graph-based models in VR.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 17:06:58 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Coppens", "Adrien", ""], ["Bicer", "Berat", ""], ["Yilmaz", "Naz", ""], ["Aras", "Serhat", ""]]}, {"id": "2001.00991", "submitter": "Marc Killpack", "authors": "Erich Mielke, Eric Townsend, David Wingate, and Marc D. Killpack", "title": "Human-robot co-manipulation of extended objects: Data-driven models and\n  control from analysis of human-human dyads", "comments": "Paper has been in submission to IJRR since November 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human teams are able to easily perform collaborative manipulation tasks.\nHowever, for a robot and human to simultaneously manipulate an extended object\nis a difficult task using existing methods from the literature. Our approach in\nthis paper is to use data from human-human dyad experiments to determine motion\nintent which we use for a physical human-robot co-manipulation task. We first\npresent and analyze data from human-human dyads performing co-manipulation\ntasks. We show that our human-human dyad data has interesting trends including\nthat interaction forces are non-negligible compared to the force required to\naccelerate an object and that the beginning of a lateral movement is\ncharacterized by distinct torque triggers from the leader of the dyad. We also\nexamine different metrics to quantify performance of different dyads. We also\ndevelop a deep neural network based on motion data from human-human trials to\npredict human intent based on past motion. We then show how force and motion\ndata can be used as a basis for robot control in a human-robot dyad. Finally,\nwe compare the performance of two controllers for human-robot co-manipulation\nto human-human dyad performance.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 21:23:12 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Mielke", "Erich", ""], ["Townsend", "Eric", ""], ["Wingate", "David", ""], ["Killpack", "Marc D.", ""]]}, {"id": "2001.01073", "submitter": "Blagoj Nenovski", "authors": "Blagoj Nenovski and Igor Nedelkovski", "title": "Recognizing and tracking outdoor objects by using ARToolKit markers", "comments": null, "journal-ref": "International Journal of Computer Science & Information Technology\n  (IJCSIT) Vol 11, No 6, December 2019", "doi": "10.5121/ijcsit.2019.11603", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We created an augmented reality platform for spatial exploration that\nrecognizes buildings facades and displays various multimedia for different time\npoints. In order to provide the user with the best user experience fast\nrecognition and stable tracking are the key elements of any augmented reality\napp. In an outdoor environment, lighting, reflective surfaces and occlusion can\ndrastically affect the user experience. In a setup where these conditions are\nsimilar, marker creation methodology and the app parameters are key. In this\npaper we focus on resizing the photo prior marker creating and the importance\nof camera calibration and resolution and their effect on the recognition speed\nand quality of tracking outdoor objects.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 12:56:59 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Nenovski", "Blagoj", ""], ["Nedelkovski", "Igor", ""]]}, {"id": "2001.01215", "submitter": "Shital Shah", "authors": "Shital Shah, Roland Fernandez, Steven Drucker", "title": "A System for Real-Time Interactive Analysis of Deep Learning Training", "comments": "Accepted at ACM SIGCHI Symposium on Engineering Interactive Computing\n  Systems (EICS 2019). Code available as TensorWatch project at\n  https://github.com/microsoft/tensorwatch", "journal-ref": null, "doi": "10.1145/3319499.3328231", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing diagnosis or exploratory analysis during the training of deep\nlearning models is challenging but often necessary for making a sequence of\ndecisions guided by the incremental observations. Currently available systems\nfor this purpose are limited to monitoring only the logged data that must be\nspecified before the training process starts. Each time a new information is\ndesired, a cycle of stop-change-restart is required in the training process.\nThese limitations make interactive exploration and diagnosis tasks difficult,\nimposing long tedious iterations during the model development. We present a new\nsystem that enables users to perform interactive queries on live processes\ngenerating real-time information that can be rendered in multiple formats on\nmultiple surfaces in the form of several desired visualizations simultaneously.\nTo achieve this, we model various exploratory inspection and diagnostic tasks\nfor deep learning training processes as specifications for streams using a\nmap-reduce paradigm with which many data scientists are already familiar. Our\ndesign achieves generality and extensibility by defining composable primitives\nwhich is a fundamentally different approach than is used by currently available\nsystems. The open source implementation of our system is available as\nTensorWatch project at https://github.com/microsoft/tensorwatch.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 11:33:31 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 08:57:16 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Shah", "Shital", ""], ["Fernandez", "Roland", ""], ["Drucker", "Steven", ""]]}, {"id": "2001.01222", "submitter": "Adwait Naik", "authors": "Adwait Naik", "title": "HMM-based phoneme speech recognition system for control and command of\n  industrial robots", "comments": "Some improvements to be done in the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speech recognition is a prominent technology, which helps us to develop a\nNatural language interface through speech for the Human-Robot Interaction\n(HRI). It allows the computer to take the spoken instructions, interpret it,\nand generate text from it. In this paper, we propose a phoneme based speech\nrecognition system to control industrial robots. Speech recognition has become\none of the popular interfaces when it comes to reducing robot operator's\nefforts to control and command the robot. This paper intends to investigate the\npotential of Linear Predictive coding technique to develop a stable and robust\nphoneme speech recognition system for robotics applications. Our system is\ndivided into three segments: a microphone array, a voice module, and a 3-DOF\nrobotic arm. To validate our approach, we have performed tests with simple and\ncomplex sentences for various robotics activities like manipulating a cube and\npick and place tasks. Moreover, we also analyzed the test result to rectify the\nproblems and limitations in our approach. The paper presents all the test\nresults which we have achieved through conducting experiments on our project.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 12:10:19 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 05:50:17 GMT"}, {"version": "v3", "created": "Sat, 18 Apr 2020 11:18:19 GMT"}, {"version": "v4", "created": "Wed, 3 Jun 2020 04:21:44 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Naik", "Adwait", ""]]}, {"id": "2001.01223", "submitter": "Leonel Merino", "authors": "Leonel Merino, Mircea Lungu, Christoph Seidl", "title": "Unleashing the Potentials of Immersive Augmented Reality for Software\n  Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In immersive augmented reality (IAR), users can wear a head-mounted display\nto see computer-generated images superimposed to their view of the world. IAR\nwas shown to be beneficial across several domains, e.g., automotive, medicine,\ngaming and engineering, with positive impacts on, e.g., collaboration and\ncommunication. We think that IAR bears a great potential for software\nengineering but, as of yet, this research area has been neglected. In this\nvision paper, we elicit potentials and obstacles for the use of IAR in software\nengineering. We identify possible areas that can be supported with IAR\ntechnology by relating commonly discussed IAR improvements to typical software\nengineering tasks. We further demonstrate how innovative use of IAR technology\nmay fundamentally improve typical activities of a software engineer through a\ncomprehensive series of usage scenarios outlining practical application.\nFinally, we reflect on current limitations of IAR technology based on our\nscenarios and sketch research activities necessary to make our vision a\nreality. We consider this paper to be relevant to academia and industry alike\nin guiding the steps to innovative research and applications for IAR in\nsoftware engineering.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 12:22:28 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Merino", "Leonel", ""], ["Lungu", "Mircea", ""], ["Seidl", "Christoph", ""]]}, {"id": "2001.01340", "submitter": "HaiLong Liu", "authors": "Hailong Liu, Takatsugu Hirayama, Luis Yoichi Morales, Hiroshi Murase", "title": "What Is the Gaze Behavior of Pedestrians in Interactions with an\n  Automated Vehicle When They Do Not Understand Its Intentions?", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactions between pedestrians and automated vehicles (AVs) will increase\nsignificantly with the popularity of AV. However, pedestrians often have not\nenough trust on the AVs , particularly when they are confused about an AV's\nintention in a interaction. This study seeks to evaluate if pedestrians clearly\nunderstand the driving intentions of AVs in interactions and presents\nexperimental research on the relationship between gaze behaviors of pedestrians\nand their understanding of the intentions of the AV. The hypothesis\ninvestigated in this study was that the less the pedestrian understands the\ndriving intentions of the AV, the longer the duration of their gazing behavior\nwill be. A pedestrian--vehicle interaction experiment was designed to verify\nthe proposed hypothesis. A robotic wheelchair was used as the manual driving\nvehicle (MV) and AV for interacting with pedestrians while pedestrians' gaze\ndata and their subjective evaluation of the driving intentions were recorded.\nThe experimental results supported our hypothesis as there was a negative\ncorrelation between the pedestrians' gaze duration on the AV and their\nunderstanding of the driving intentions of the AV. Moreover, the gaze duration\nof most of the pedestrians on the MV was shorter than that on an AV. Therefore,\nwe conclude with two recommendations to designers of external human-machine\ninterfaces (eHMI): (1) when a pedestrian is engaged in an interaction with an\nAV, the driving intentions of the AV should be provided; (2) if the pedestrian\nstill gazes at the AV after the AV displays its driving intentions, the AV\nshould provide clearer information about its driving intentions.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 00:24:34 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 06:03:23 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Liu", "Hailong", ""], ["Hirayama", "Takatsugu", ""], ["Morales", "Luis Yoichi", ""], ["Murase", "Hiroshi", ""]]}, {"id": "2001.01441", "submitter": "Orestis Georgiou", "authors": "Ted Romanus, Sam Frish, Mykola Maksymenko, William Frier, Lo\\\"ic\n  Corenthy, Orestis Georgiou", "title": "Mid-Air Haptic Bio-Holograms in Mixed Reality", "comments": "5 pages, 4 figures, In Proceedings of the IEEE and ACM International\n  Symposium on Mixed and Augmented Reality, (ISMAR), (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a prototype demonstrator that integrates three technologies, mixed\nreality head-mounted displays, wearable bio-sensors, and mid-air haptic\nprojectors to deliver an interactive tactile experience with a bio-hologram.\nUsers of this prototype are able to see, touch and feel a hologram of a heart\nthat is beating at the same rhythm as their own. The demo uses an Ultrahaptics\ndevice, a Magic Leap One Mixed Reality headset, and an Apple Watch that\nmeasures the wearer's heart rate, all synchronized and networked together such\nthat updates from the wristband dynamically change the haptic feedback and the\nanimation speed of the beating heart thus creating a more personalised\nexperience.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 09:04:51 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Romanus", "Ted", ""], ["Frish", "Sam", ""], ["Maksymenko", "Mykola", ""], ["Frier", "William", ""], ["Corenthy", "Lo\u00efc", ""], ["Georgiou", "Orestis", ""]]}, {"id": "2001.01445", "submitter": "Orestis Georgiou", "authors": "Alex Girdler, Orestis Georgiou", "title": "Mid-Air Haptics in Aviation -- creating the sensation of touch where\n  there is nothing but thin air", "comments": "13 pages, 8 figures, In Proceedings of Interservice/Industry\n  Training, Simulation, and Education Conference (I/ITSEC) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exciting new technology known as mid-air haptics has been adopted by\nseveral industries including Automotive and Entertainment, however it has yet\nto emerge in simulated pilot training or in real-life flight decks. Full-flight\nsimulators are expensive to manufacture, maintain and operate. Not only that,\neach simulator is limited to one aircraft type, which is inefficient for the\nmajority of airlines that have several in service. With the growing trend in\ntouchscreen instrumentation, cockpit displays require the pilot's attention to\nbe drawn away from their view out of the window. But by using gesture\nrecognition interfaces combined with mid-air haptic feedback, we can mitigate\nthis shortcoming while also adding another dimension to the existing technology\nfor pilots already familiar with using legacy cockpits, complete with\ntraditional instrumentation. Meanwhile, simulation environments using augmented\nand virtual reality technology offers quality immersive training to the extent\nthat pilots can go from hundreds of hours of simulated training to being\nresponsible for hundreds of lives on their very first flight. The software\nre-programmability and dynamic richness afforded by mid-air haptic technologies\ncombined with a basic full-motion platform could allow for an interchange of\ninstrumentation layouts thus enhancing simulation immersiveness and\nenvironments. Finally, by borrowing and exploring concepts within the\nautomotive sector, this concept paper presents how flight deck design could\nevolve by adopting this technology. If pilot testimony suggests that they can\nadapt to virtual objects, can this replace physical controls?\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 09:14:31 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Girdler", "Alex", ""], ["Georgiou", "Orestis", ""]]}, {"id": "2001.01824", "submitter": "Bijan Fakhri", "authors": "Bijan Fakhri, Troy McDaniel, Heni Ben Amor, Hemanth Venkateswara,\n  Abhik Chowdhury, and Sethuraman Panchanathan", "title": "Foveated Haptic Gaze", "comments": "Accepted to ICSM 2019. For a demonstration of Foveated Haptic Gaze,\n  see https://youtu.be/Xp7B8UqtVFw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As digital worlds become ubiquitous via video games, simulations, virtual and\naugmented reality, people with disabilities who cannot access those worlds are\nbecoming increasingly disenfranchised. More often than not the design of these\nenvironments focuses on vision, making them inaccessible in whole or in part to\npeople with visual impairments. Accessible games and visual aids have been\ndeveloped but their lack of prevalence or unintuitive interfaces make them\nimpractical for daily use. To address this gap, we present Foveated Haptic\nGaze, a method for conveying visual information via haptics that is intuitive\nand designed for interacting with real-time 3-dimensional environments. To\nvalidate our approach we developed a prototype of the system along with a\nsimplified first-person shooter game. Lastly we present encouraging user study\nresults of both sighted and blind participants using our system to play the\ngame with no visual feedback.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 00:35:23 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 18:25:53 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 20:33:18 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Fakhri", "Bijan", ""], ["McDaniel", "Troy", ""], ["Amor", "Heni Ben", ""], ["Venkateswara", "Hemanth", ""], ["Chowdhury", "Abhik", ""], ["Panchanathan", "Sethuraman", ""]]}, {"id": "2001.01840", "submitter": "Waqas Ahmed", "authors": "Sheikh Muhammad Hizam, Waqas Ahmed", "title": "A Conceptual Paper on SERVQUAL-Framework for Assessing Quality of\n  Internet of Things (IoT) Services", "comments": "11 pages, 01 figure, 01 table", "journal-ref": "International Journal of Financial Research, Vol. 10, No. 5,\n  Special Issue; 2019", "doi": "10.5430/ijfr.v10n5p387", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Service quality possesses the vital prominence in usability of innovative\nproducts and services. As technological innovation has made the life\nsynchronized and effective, Internet of Things (IoT) is matter of discussion\neverywhere. From users' perspective, IoT services are always embraced by\nvarious system characteristics of security and performance. A service quality\nmodel can better present the preference of such technology customers. the study\nintends to project theoretical model of service quality for internet of things\n(IoT). Based on the existing models of service quality and the literature in\ninternet of things, a framework is proposed to conceptualize and measure\nservice quality for internet of things.This study established the IoT-Servqual\nmodel with four dimensions (i.e., Privacy, Functionality, Efficiency, and\nTangibility) of multiple service quality models. These dimensions are essential\nand inclined towards the users' leaning of IoT Services. This paper contributes\nto research on internet of things services by development of a comprehensive\nframework for customers' quality apprehension. This model will previse the\nexpression of information secrecy of users related with internet of things\n(IoT). This research will advance understanding of service quality in modern\nday technology and assist firms to devise the fruitful service structure.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 12:13:53 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Hizam", "Sheikh Muhammad", ""], ["Ahmed", "Waqas", ""]]}, {"id": "2001.01868", "submitter": "Roman Grigorii", "authors": "Roman V. Grigorii and J. Edward Colgate", "title": "Closed loop application of electroadhesion for increased precision in\n  texture rendering", "comments": null, "journal-ref": null, "doi": "10.1109/TOH.2020.2972350", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile displays based on friction modulation offer wide-bandwidth forces\nrendered directly on the fingertip. However, due to a number of touch\nconditions (e.g., normal force, skin hydration) that result in variations in\nthe friction force and the strength of modulation effect, the precision of the\nforce rendering remains limited. In this paper we demonstrate a closed-loop\nelectroadhesion method for precise playback of friction force profiles on a\nhuman finger and we apply this method to the tactile rendering of several\ntextiles encountered in everyday life.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 03:04:26 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Grigorii", "Roman V.", ""], ["Colgate", "J. Edward", ""]]}, {"id": "2001.01902", "submitter": "Yiru Chen", "authors": "Yiru Chen, Eugene Wu", "title": "Monte Carlo Tree Search for Generating Interactive Data Analysis\n  Interfaces", "comments": "4 pages, 6 figures, The AAAI-20 Workshop on Intelligent Process\n  Automation (IPA-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive tools like user interfaces help democratize data access for\nend-users by hiding underlying programming details and exposing the necessary\nwidget interface to users. Since customized interfaces are costly to build,\nautomated interface generation is desirable. SQL is the dominant way to analyze\ndata and there already exists logs to analyze data. Previous work proposed a\nsyntactic approach to analyze structural changes in SQL query logs and\nautomatically generates a set of widgets to express the changes. However, they\ndo not consider layout usability and the sequential order of queries in the\nlog. We propose to adopt Monte Carlo Tree Search(MCTS) to search for the\noptimal interface that accounts for hierarchical layout as well as the\nusability in terms of how easy to express the query log.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 06:01:30 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 03:18:57 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Chen", "Yiru", ""], ["Wu", "Eugene", ""]]}, {"id": "2001.02004", "submitter": "Zijie Wang", "authors": "Zijie J. Wang, Robert Turko, Omar Shaikh, Haekyu Park, Nilaksh Das,\n  Fred Hohman, Minsuk Kahng, Duen Horng Chau", "title": "CNN 101: Interactive Visual Learning for Convolutional Neural Networks", "comments": "CHI'20 Late-Breaking Work (April 25-30, 2020), 7 pages, 3 figures", "journal-ref": null, "doi": "10.1145/3334480.3382899", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of deep learning solving previously-thought hard problems has\ninspired many non-experts to learn and understand this exciting technology.\nHowever, it is often challenging for learners to take the first steps due to\nthe complexity of deep learning models. We present our ongoing work, CNN 101,\nan interactive visualization system for explaining and teaching convolutional\nneural networks. Through tightly integrated interactive views, CNN 101 offers\nboth overview and detailed descriptions of how a model works. Built using\nmodern web technologies, CNN 101 runs locally in users' web browsers without\nrequiring specialized hardware, broadening the public's education access to\nmodern deep learning techniques.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 12:46:41 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 15:16:57 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 16:38:32 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Wang", "Zijie J.", ""], ["Turko", "Robert", ""], ["Shaikh", "Omar", ""], ["Park", "Haekyu", ""], ["Das", "Nilaksh", ""], ["Hohman", "Fred", ""], ["Kahng", "Minsuk", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2001.02114", "submitter": "Yunfeng Zhang", "authors": "Yunfeng Zhang, Q. Vera Liao, Rachel K. E. Bellamy", "title": "Effect of Confidence and Explanation on Accuracy and Trust Calibration\n  in AI-Assisted Decision Making", "comments": null, "journal-ref": null, "doi": "10.1145/3351095.3372852", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, AI is being increasingly used to help human experts make decisions in\nhigh-stakes scenarios. In these scenarios, full automation is often\nundesirable, not only due to the significance of the outcome, but also because\nhuman experts can draw on their domain knowledge complementary to the model's\nto ensure task success. We refer to these scenarios as AI-assisted decision\nmaking, where the individual strengths of the human and the AI come together to\noptimize the joint decision outcome. A key to their success is to appropriately\n\\textit{calibrate} human trust in the AI on a case-by-case basis; knowing when\nto trust or distrust the AI allows the human expert to appropriately apply\ntheir knowledge, improving decision outcomes in cases where the model is likely\nto perform poorly. This research conducts a case study of AI-assisted decision\nmaking in which humans and AI have comparable performance alone, and explores\nwhether features that reveal case-specific model information can calibrate\ntrust and improve the joint performance of the human and AI. Specifically, we\nstudy the effect of showing confidence score and local explanation for a\nparticular prediction. Through two human experiments, we show that confidence\nscore can help calibrate people's trust in an AI model, but trust calibration\nalone is not sufficient to improve AI-assisted decision making, which may also\ndepend on whether the human can bring in enough unique knowledge to complement\nthe AI's errors. We also highlight the problems in using local explanation for\nAI-assisted decision making scenarios and invite the research community to\nexplore new approaches to explainability for calibrating human trust in AI.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 15:33:48 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zhang", "Yunfeng", ""], ["Liao", "Q. Vera", ""], ["Bellamy", "Rachel K. E.", ""]]}, {"id": "2001.02271", "submitter": "Jichen Zhu", "authors": "Chelsea M. Myers, Evan Freed, Luis Fernando Laris Pardo, Anushay\n  Furqan, Sebastian Risi, Jichen Zhu", "title": "Revealing Neural Network Bias to Non-Experts Through Interactive\n  Counterfactual Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI algorithms are not immune to biases. Traditionally, non-experts have\nlittle control in uncovering potential social bias (e.g., gender bias) in the\nalgorithms that may impact their lives. We present a preliminary design for an\ninteractive visualization tool CEB to reveal biases in a commonly used AI\nmethod, Neural Networks (NN). CEB combines counterfactual examples and\nabstraction of an NN decision process to empower non-experts to detect bias.\nThis paper presents the design of CEB and initial findings of an expert panel\n(n=6) with AI, HCI, and Social science experts.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 20:24:10 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2020 19:09:37 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Myers", "Chelsea M.", ""], ["Freed", "Evan", ""], ["Pardo", "Luis Fernando Laris", ""], ["Furqan", "Anushay", ""], ["Risi", "Sebastian", ""], ["Zhu", "Jichen", ""]]}, {"id": "2001.02306", "submitter": "Muhammad Amith", "authors": "Muhammad Amith, Rebecca Lin, Rachel Cunningham, Qiwei Luna Wu, Lara S.\n  Savas, Yang Gong, Julie A. Boom, Lu Tang, Cui Tao", "title": "Examining Potential Usability and Health Beliefs Among Young Adults\n  Using a Conversational Agent for HPV Vaccine Counseling", "comments": "To appear in: Proceedings of the AMIA 2020 Informatics Summit\n  23rd-26th March Houston, TX, USA (2020). Please visit and cite the canonical\n  version once available; M. Amith and R. Lin contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human papillomavirus (HPV) vaccine is the most effective way to prevent\nHPV-related cancers. Integrating provider vaccine counseling is crucial to\nimproving HPV vaccine completion rates. Automating the counseling experience\nthrough a conversational agent could help improve HPV vaccine coverage and\nreduce the burden of vaccine counseling for providers. In a previous study, we\ntested a simulated conversational agent that provided HPV vaccine counseling\nfor parents using the Wizard of OZ protocol. In the current study, we assessed\nthe conversational agent among young college adults (n=24), a population that\nmay have missed the HPV vaccine during their adolescence when vaccination is\nrecommended. We also administered surveys for system and voice usability, and\nfor health beliefs concerning the HPV vaccine. Participants perceived the agent\nto have high usability that is slightly better or equivalent to other voice\ninteractive interfaces, and there is some evidence that the agent impacted\ntheir beliefs concerning the harms, uncertainty, and risk denials for the HPV\nvaccine. Overall, this study demonstrates the potential for conversational\nagents to be an impactful tool for health promotion endeavors.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 22:33:09 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Amith", "Muhammad", ""], ["Lin", "Rebecca", ""], ["Cunningham", "Rachel", ""], ["Wu", "Qiwei Luna", ""], ["Savas", "Lara S.", ""], ["Gong", "Yang", ""], ["Boom", "Julie A.", ""], ["Tang", "Lu", ""], ["Tao", "Cui", ""]]}, {"id": "2001.02316", "submitter": "Michael Correll", "authors": "Andrew McNutt, Gordon Kindlmann, Michael Correll", "title": "Surfacing Visualization Mirages", "comments": null, "journal-ref": null, "doi": "10.1145/3313831.3376420", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirty data and deceptive design practices can undermine, invert, or\ninvalidate the purported messages of charts and graphs. These failures can\narise silently: a conclusion derived from a particular visualization may look\nplausible unless the analyst looks closer and discovers an issue with the\nbacking data, visual specification, or their own assumptions. We term such\nsilent but significant failures \"visualization mirages\". We describe a\nconceptual model of mirages and show how they can be generated at every stage\nof the visual analytics process. We adapt a methodology from software testing,\n\"metamorphic testing\", as a way of automatically surfacing potential mirages at\nthe visual encoding stage of analysis through modifications to the underlying\ndata and chart specification. We show that metamorphic testing can reliably\nidentify mirages across a variety of chart types with relatively little prior\nknowledge of the data or the domain.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 00:11:45 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["McNutt", "Andrew", ""], ["Kindlmann", "Gordon", ""], ["Correll", "Michael", ""]]}, {"id": "2001.02329", "submitter": "Anup Anand Deshmukh", "authors": "Anup Anand Deshmukh, Catherine Soladie, Renaud Seguier", "title": "Emo-CNN for Perceiving Stress from Audio Signals: A Brain Chemistry\n  Approach", "comments": "2 pages, 2 tables and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion plays a key role in many applications like healthcare, to gather\npatients emotional behavior. There are certain emotions which are given more\nimportance due to their effectiveness in understanding human feelings. In this\npaper, we propose an approach that models human stress from audio signals. The\nresearch challenge in speech emotion detection is defining the very meaning of\nstress and being able to categorize it in a precise manner. Supervised Machine\nLearning models, including state of the art Deep Learning classification\nmethods, rely on the availability of clean and labelled data. One of the\nproblems in affective computation and emotion detection is the limited amount\nof annotated data of stress. The existing labelled stress emotion datasets are\nhighly subjective to the perception of the annotator.\n  We address the first issue of feature selection by exploiting the use of\ntraditional MFCC features in Convolutional Neural Network. Our experiments show\nthat Emo-CNN consistently and significantly outperforms the popular existing\nmethods over multiple datasets. It achieves 90.2% categorical accuracy on the\nEmo-DB dataset. To tackle the second and the more significant problem of\nsubjectivity in stress labels, we use Lovheim's cube, which is a 3-dimensional\nprojection of emotions. The cube aims at explaining the relationship between\nthese neurotransmitters and the positions of emotions in 3D space. The learnt\nemotion representations from the Emo-CNN are mapped to the cube using three\ncomponent PCA (Principal Component Analysis) which is then used to model human\nstress. This proposed approach not only circumvents the need for labelled\nstress data but also complies with the psychological theory of emotions given\nby Lovheim's cube. We believe that this work is the first step towards creating\na connection between Artificial Intelligence and the chemistry of human\nemotions.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 01:01:48 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Deshmukh", "Anup Anand", ""], ["Soladie", "Catherine", ""], ["Seguier", "Renaud", ""]]}, {"id": "2001.02382", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Ryosuke Nakayama, Dan Liu, Yasuaki Kakehi, Mark D. Gross,\n  Daniel Leithinger", "title": "LiftTiles: Constructive Building Blocks for Prototyping Room-scale\n  Shape-changing Interfaces", "comments": "TEI 2020", "journal-ref": null, "doi": "10.1145/3374920.3374941", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale shape-changing interfaces have great potential, but creating such\nsystems requires substantial time, cost, space, and efforts, which hinders the\nresearch community to explore interactions beyond the scale of human hands. We\nintroduce modular inflatable actuators as building blocks for prototyping\nroom-scale shape-changing interfaces. Each actuator can change its height from\n15cm to 150cm, actuated and controlled by air pressure. Each unit is low-cost\n(8 USD), lightweight (10 kg), compact (15 cm), and robust, making it\nwell-suited for prototyping room-scale shape transformations. Moreover, our\nmodular and reconfigurable design allows researchers and designers to quickly\nconstruct different geometries and to explore various applications. This paper\ncontributes to the design and implementation of highly extendable inflatable\nactuators, and demonstrates a range of scenarios that can leverage this modular\nbuilding block.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 05:23:32 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Suzuki", "Ryo", ""], ["Nakayama", "Ryosuke", ""], ["Liu", "Dan", ""], ["Kakehi", "Yasuaki", ""], ["Gross", "Mark D.", ""], ["Leithinger", "Daniel", ""]]}, {"id": "2001.02478", "submitter": "Q.Vera Liao", "authors": "Q. Vera Liao, Daniel Gruen, Sarah Miller", "title": "Questioning the AI: Informing Design Practices for Explainable AI User\n  Experiences", "comments": "Working draft. To appear in the ACM CHI Conference on Human Factors\n  in Computing Systems (CHI 2020)", "journal-ref": null, "doi": "10.1145/3313831.3376590", "report-no": null, "categories": "cs.HC cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A surge of interest in explainable AI (XAI) has led to a vast collection of\nalgorithmic work on the topic. While many recognize the necessity to\nincorporate explainability features in AI systems, how to address real-world\nuser needs for understanding AI remains an open question. By interviewing 20 UX\nand design practitioners working on various AI products, we seek to identify\ngaps between the current XAI algorithmic work and practices to create\nexplainable AI products. To do so, we develop an algorithm-informed XAI\nquestion bank in which user needs for explainability are represented as\nprototypical questions users might ask about the AI, and use it as a study\nprobe. Our work contributes insights into the design space of XAI, informs\nefforts to support design practices in this space, and identifies opportunities\nfor future XAI work. We also provide an extended XAI question bank and discuss\nhow it can be used for creating user-centered XAI.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 12:34:51 GMT"}, {"version": "v2", "created": "Sat, 8 Feb 2020 21:44:57 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Liao", "Q. Vera", ""], ["Gruen", "Daniel", ""], ["Miller", "Sarah", ""]]}, {"id": "2001.02479", "submitter": "Michael Veale", "authors": "Midas Nouwens, Ilaria Liccardi, Michael Veale, David Karger, Lalana\n  Kagal", "title": "Dark Patterns after the GDPR: Scraping Consent Pop-ups and Demonstrating\n  their Influence", "comments": "13 pages, 3 figures. To appear in the Proceedings of CHI '20 CHI\n  Conference on Human Factors in Computing Systems, April 25--30, 2020,\n  Honolulu, HI, USA", "journal-ref": null, "doi": "10.1145/3313831.3376321", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New consent management platforms (CMPs) have been introduced to the web to\nconform with the EU's General Data Protection Regulation, particularly its\nrequirements for consent when companies collect and process users' personal\ndata. This work analyses how the most prevalent CMP designs affect people's\nconsent choices. We scraped the designs of the five most popular CMPs on the\ntop 10,000 websites in the UK (n=680). We found that dark patterns and implied\nconsent are ubiquitous; only 11.8% meet the minimal requirements that we set\nbased on European law. Second, we conducted a field experiment with 40\nparticipants to investigate how the eight most common designs affect consent\nchoices. We found that notification style (banner or barrier) has no effect;\nremoving the opt-out button from the first page increases consent by 22--23\npercentage points; and providing more granular controls on the first page\ndecreases consent by 8--20 percentage points. This study provides an empirical\nbasis for the necessary regulatory action to enforce the GDPR, in particular\nthe possibility of focusing on the centralised, third-party CMP services as an\neffective way to increase compliance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 12:36:29 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Nouwens", "Midas", ""], ["Liccardi", "Ilaria", ""], ["Veale", "Michael", ""], ["Karger", "David", ""], ["Kagal", "Lalana", ""]]}, {"id": "2001.02553", "submitter": "Marvin Wyrich", "authors": "Marvin Wyrich, Regina Hebig, Stefan Wagner, Riccardo Scandariato", "title": "Perception and Acceptance of an Autonomous Refactoring Bot", "comments": "8 pages, 2 figures. To be published at 12th International Conference\n  on Agents and Artificial Intelligence (ICAART 2020)", "journal-ref": null, "doi": "10.5220/0009168803030310", "report-no": null, "categories": "cs.SE cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of autonomous bots for automatic support in software development\ntasks is increasing. In the past, however, they were not always perceived\npositively and sometimes experienced a negative bias compared to their human\ncounterparts. We conducted a qualitative study in which we deployed an\nautonomous refactoring bot for 41 days in a student software development\nproject. In between and at the end, we conducted semi-structured interviews to\nfind out how developers perceive the bot and whether they are more or less\ncritical when reviewing the contributions of a bot compared to human\ncontributions. Our findings show that the bot was perceived as a useful and\nunobtrusive contributor, and developers were no more critical of it than they\nwere about their human colleagues, but only a few team members felt responsible\nfor the bot.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 14:47:54 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Wyrich", "Marvin", ""], ["Hebig", "Regina", ""], ["Wagner", "Stefan", ""], ["Scandariato", "Riccardo", ""]]}, {"id": "2001.02639", "submitter": "Deborah Ferreira", "authors": "Deborah Ferreira, Julia Rozanova, Krishna Dubba, Dell Zhang, Andre\n  Freitas", "title": "On the Evaluation of Intelligent Process Automation", "comments": "Submitted to the AAAI-20 Workshop on Intelligent Process Automation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent Process Automation (IPA) is emerging as a sub-field of AI to\nsupport the automation of long-tail processes which requires the coordination\nof tasks across different systems. So far, the field of IPA has been largely\ndriven by systems and use cases, lacking a more formal definition of the task\nand its assessment. This paper aims to address this gap by providing a\nformalisation of IPA and by proposing specific metrics to support the empirical\nevaluation of IPA systems. This work also compares and contrasts IPA against\nrelated tasks such as end-user programming and program synthesis.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 17:20:54 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 09:43:15 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Ferreira", "Deborah", ""], ["Rozanova", "Julia", ""], ["Dubba", "Krishna", ""], ["Zhang", "Dell", ""], ["Freitas", "Andre", ""]]}, {"id": "2001.02705", "submitter": "Yixuan Zhang", "authors": "Yixuan Zhang, Nurul Suhaimi, Rana Azghandi, Mary Amulya Joseph, Miso\n  Kim, Jacqueline Griffin, Andrea G. Parker", "title": "Understanding the Use of Crisis Informatics Technology among Older\n  Adults", "comments": "10 pages", "journal-ref": null, "doi": "10.1145/3313831.3376862", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass emergencies increasingly pose significant threats to human life, with a\ndisproportionate burden being incurred by older adults. Research has explored\nhow mobile technology can mitigate the effects of mass emergencies. However,\nless work has examined how mobile technologies support older adults during\nemergencies, considering their unique needs. To address this research gap, we\ninterviewed 16 older adults who had recent experience with an emergency\nevacuation to understand the perceived value of using mobile technology during\nemergencies. We found that there was a lack of awareness and engagement with\nexisting crisis apps. Our findings characterize the ways in which our\nparticipants did and did not feel crisis informatics tools address human\nvalues, including basic needs and esteem needs. We contribute an understanding\nof how older adults used mobile technology during emergencies and their\nperspectives on how well such tools address human values.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 19:09:46 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 15:03:33 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zhang", "Yixuan", ""], ["Suhaimi", "Nurul", ""], ["Azghandi", "Rana", ""], ["Joseph", "Mary Amulya", ""], ["Kim", "Miso", ""], ["Griffin", "Jacqueline", ""], ["Parker", "Andrea G.", ""]]}, {"id": "2001.02731", "submitter": "Xumeng Chen", "authors": "Xumeng Chen, Leo Yu-Ho Lo, Huamin Qu", "title": "SirenLess: reveal the intention behind news", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News articles tend to be increasingly misleading nowadays, preventing readers\nfrom making subjective judgments towards certain events. While some machine\nlearning approaches have been proposed to detect misleading news, most of them\nare black boxes that provide limited help for humans in decision making. In\nthis paper, we present SirenLess, a visual analytical system for misleading\nnews detection by linguistic features. The system features article explorer, a\nnovel interactive tool that integrates news metadata and linguistic features to\nreveal semantic structures of news articles and facilitate textual analysis. We\nuse SirenLess to analyze 18 news articles from different sources and summarize\nsome helpful patterns for misleading news detection. A user study with\njournalism professionals and university students is conducted to confirm the\nusefulness and effectiveness of our system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 20:36:17 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Chen", "Xumeng", ""], ["Lo", "Leo Yu-Ho", ""], ["Qu", "Huamin", ""]]}, {"id": "2001.02746", "submitter": "Suyun Bae", "authors": "Suyun \"Sandra\" Bae, Oh-Hyun Kwon, Senthil Chandrasegaran, Kwan-Liu Ma", "title": "Spinneret: Aiding Creative Ideation through Non-Obvious Concept\n  Associations", "comments": "ACM CHI 2020", "journal-ref": null, "doi": "10.1145/3313831.3376746", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mind mapping is a popular way to explore a design space in creative thinking\nexercises, allowing users to form associations between concepts. Yet, most\nexisting digital tools for mind mapping focus on authoring and organization,\nwith little support for addressing the challenges of mind mapping such as\nstagnation and design fixation. We present Spinneret, a functional approach to\naid mind mapping by providing suggestions based on a knowledge graph. Spinneret\nuses biased random walks to explore the knowledge graph in the neighborhood of\nan existing concept node in the mind map, and provides \"suggestions\" for the\nuser to add to the mind map. A comparative study with a baseline mind-mapping\ntool reveals that participants created more diverse and distinct concepts with\nSpinneret, and reported that the suggestions inspired them to think of ideas\nthey would otherwise not have explored.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 21:28:06 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Bae", "Suyun \"Sandra\"", ""], ["Kwon", "Oh-Hyun", ""], ["Chandrasegaran", "Senthil", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2001.02807", "submitter": "Yashaswini Murthy", "authors": "Ioannis C. Konstantakopoulos, Kristy A. Hamilton, Yashaswini Murthy,\n  Tanya Veeravalli, Costas Spanos, Roy Dong", "title": "smartSDH: An Experimental Study of Mechanism Based Building Control", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Internet of Things (IoT) technologies are increasingly being deployed,\nsituations frequently arise where multiple stakeholders must reconcile\npreferences to control a shared resource. We perform a 5-month long experiment\ndubbed 'smartSDH' (carried out in 27 employees' office space) where users\nreport their preferences for the brightness of overhead lighting. smartSDH\nimplements a modified Vickrey-Clarke-Groves (VCG) mechanism; assuming users are\nrational, it incentivizes truthful reporting, implements the socially desirable\noutcome, and compensates participants to ensure higher payoffs under smartSDH\nwhen compared with the default outside option(i.e., the option chosen in the\nabsence of such a mechanism). smartSDH assesses the feasibility of the VCG\nmechanism in the context of smart building control and evaluated smartSDH's\neffect using metrics such as light level satisfaction, incentive satisfaction,\nand energy consumption. Although previous studies on the theoretical aspects of\nthe mechanism indicate user satisfaction, our experiments indicate quite the\ncontrary. We found that the participants were significantly less satisfied with\nlight brightness and incentives determined by the VCG mechanism over time.\nThese data suggest the need for more realistic behavioral models to design IoT\ntechnologies and highlights difficulties in estimating preferences from\nobservable external factors such as atmospheric conditions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 01:54:59 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 13:27:42 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 03:42:24 GMT"}, {"version": "v4", "created": "Sat, 26 Jun 2021 23:51:27 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Konstantakopoulos", "Ioannis C.", ""], ["Hamilton", "Kristy A.", ""], ["Murthy", "Yashaswini", ""], ["Veeravalli", "Tanya", ""], ["Spanos", "Costas", ""], ["Dong", "Roy", ""]]}, {"id": "2001.02912", "submitter": "Laure Soulier", "authors": "Sharon Oviatt and Laure Soulier", "title": "Conversational Search for Learning Technologies", "comments": "Dagstuhl Report on Conversational Search (ID 19461) - This document\n  is a report of the breaking group \"Conversational Search for Learning\n  Technologies\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search is based on a user-system cooperation with the\nobjective to solve an information-seeking task. In this report, we discuss the\nimplication of such cooperation with the learning perspective from both user\nand system side. We also focus on the stimulation of learning through a key\ncomponent of conversational search, namely the multimodality of communication\nway, and discuss the implication in terms of information retrieval. We end with\na research road map describing promising research directions and perspectives.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 10:35:27 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Oviatt", "Sharon", ""], ["Soulier", "Laure", ""]]}, {"id": "2001.02921", "submitter": "Kashyap Todi", "authors": "Niraj Dayama, Kashyap Todi, Taru Saarelainen, Antti Oulasvirta", "title": "GRIDS: Interactive Layout Design with Integer Programming", "comments": "13 pages, 10 figures, ACM CHI 2020 Full Paper", "journal-ref": null, "doi": "10.1145/3313831.3376553", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grid layouts are used by designers to spatially organise user interfaces when\nsketching and wireframing. However, their design is largely time consuming\nmanual work. This is challenging due to combinatorial explosion and complex\nobjectives, such as alignment, balance, and expectations regarding positions.\nThis paper proposes a novel optimisation approach for the generation of diverse\ngrid-based layouts. Our mixed integer linear programming (MILP) model offers a\nrigorous yet efficient method for grid generation that ensures packing,\nalignment, grouping, and preferential positioning of elements. Further, we\npresent techniques for interactive diversification, enhancement, and completion\nof grid layouts (Figure 1). These capabilities are demonstrated using GRIDS1, a\nwireframing tool that provides designers with real-time layout suggestions. We\nreport findings from a ratings study (N = 13) and a design study (N = 16),\nlending evidence for the benefit of computational grid generation during early\nstages of design.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 11:08:15 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Dayama", "Niraj", ""], ["Todi", "Kashyap", ""], ["Saarelainen", "Taru", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "2001.02938", "submitter": "Filip Krumpe", "authors": "Filip Krumpe and Thomas Mendel", "title": "Computing Curved Area Labels in Near-Real Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Area Labeling Problem one is after placing the label of a geographic\narea. Given the outer boundary of the area and an optional set of holes. The\ngoal is to find a label position such that the label spans the area and is\nconform to its shape.\n  The most recent research in this field from Barrault in 2001 proposes an\nalgorithm to compute label placements based on curved support lines. His\nsolution has some drawbacks as he is evaluating many very similar solutions.\nFurthermore he needs to restrict the search space due to performance issues and\ntherefore might miss interesting solutions.\n  We propose a solution that evaluates the search space more broadly and much\nmore efficient. To achieve this we compute a skeleton of the polygon. The\nskeleton is pruned such that edges close to the boundary polygon are removed.\nIn the so pruned skeleton we choose a set of candidate paths to be longest\ndistinct subpaths of the graph. Based on these candidates the label support\nlines are computed and the label positions evaluated.\n  Keywords: Area lettering \\and Automated label placement \\and Digital\ncartography \\and Geographic information sciences \\and Geometric Optimization.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 12:05:17 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Krumpe", "Filip", ""], ["Mendel", "Thomas", ""]]}, {"id": "2001.02985", "submitter": "Pengcheng An", "authors": "Pengcheng An, Kenneth Holstein, Bernice d'Anjou, Berry Eggen, Saskia\n  Bakker", "title": "The TA Framework: Designing Real-time Teaching Augmentation for K-12\n  Classrooms", "comments": "to be published in Proceedings of the 2020 CHI Conference on Human\n  Factors in Computing Systems, 17 pages, 10 figures", "journal-ref": null, "doi": "10.1145/3313831.3376277", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the HCI community has seen increased interest in the design of\nteaching augmentation (TA): tools that extend and complement teachers'\npedagogical abilities during ongoing classroom activities. Examples of TA\nsystems are emerging across multiple disciplines, taking various forms: e.g.,\nambient displays, wearables, or learning analytics dashboards. However, these\ndiverse examples have not been analyzed together to derive more fundamental\ninsights into the design of teaching augmentation. Addressing this opportunity,\nwe broadly synthesize existing cases to propose the TA framework. Our framework\nspecifies a rich design space in five dimensions, to support the design and\nanalysis of teaching augmentation. We contextualize the framework using\nexisting designs cases, to surface underlying design trade-offs: for example,\nbalancing actionability of presented information with teachers' needs for\nprofessional autonomy, or balancing unobtrusiveness with informativeness in the\ndesign of TA systems. Applying the TA framework, we identify opportunities for\nfuture research and design.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 13:59:20 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["An", "Pengcheng", ""], ["Holstein", "Kenneth", ""], ["d'Anjou", "Bernice", ""], ["Eggen", "Berry", ""], ["Bakker", "Saskia", ""]]}, {"id": "2001.03012", "submitter": "Huan Wei", "authors": "Huan Wei, Haotian Li, Meng Xia, Yong Wang, Huamin Qu", "title": "Predicting Student Performance in Interactive Online Question Pools\n  Using Mouse Interaction Features", "comments": "10 pages, 7 figures, conference lak20, has been accepted, proceeding\n  now. link: https://lak20.solaresearch.org/list-of-accepted-papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling student learning and further predicting the performance is a\nwell-established task in online learning and is crucial to personalized\neducation by recommending different learning resources to different students\nbased on their needs. Interactive online question pools (e.g., educational game\nplatforms), an important component of online education, have become\nincreasingly popular in recent years. However, most existing work on student\nperformance prediction targets at online learning platforms with a\nwell-structured curriculum, predefined question order and accurate knowledge\ntags provided by domain experts. It remains unclear how to conduct student\nperformance prediction in interactive online question pools without such\nwell-organized question orders or knowledge tags by experts. In this paper, we\npropose a novel approach to boost student performance prediction in interactive\nonline question pools by further considering student interaction features and\nthe similarity between questions. Specifically, we introduce new features\n(e.g., think time, first attempt, and first drag-and-drop) based on student\nmouse movement trajectories to delineate students' problem-solving details. In\naddition, heterogeneous information network is applied to integrating students'\nhistorical problem-solving information on similar questions, enhancing student\nperformance predictions on a new question. We evaluate the proposed approach on\nthe dataset from a real-world interactive question pool using four typical\nmachine learning models.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 14:29:12 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Wei", "Huan", ""], ["Li", "Haotian", ""], ["Xia", "Meng", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2001.03021", "submitter": "Martin Feick", "authors": "Martin Feick, Scott Bateman, Anthony Tang, Andr\\'e Miede, Nicolai\n  Marquardt", "title": "TanGi: Tangible Proxies for Embodied Object Exploration and Manipulation\n  in Virtual Reality", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring and manipulating complex virtual objects is challenging due to\nlimitations of conventional controllers and free-hand interaction techniques.\nWe present the TanGi toolkit which enables novices to rapidly build physical\nproxy objects using Composable Shape Primitives. TanGi also provides\nManipulators allowing users to build objects including movable parts, making\nthem suitable for rich object exploration and manipulation in VR. With a set of\ndifferent use cases and applications we show the capabilities of the TanGi\ntoolkit, and evaluate its use. In a study with 16 participants, we demonstrate\nthat novices can quickly build physical proxy objects using the Composable\nShape Primitives, and explore how different levels of object embodiment affect\nvirtual object exploration. In a second study with 12 participants we evaluate\nTanGi's Manipulators, and investigate the effectiveness of embodied\ninteraction. Findings from this study show that TanGi's proxies outperform\ntraditional controllers, and were generally favored by participants.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 14:36:30 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Feick", "Martin", ""], ["Bateman", "Scott", ""], ["Tang", "Anthony", ""], ["Miede", "Andr\u00e9", ""], ["Marquardt", "Nicolai", ""]]}, {"id": "2001.03093", "submitter": "Boris Ivanovic", "authors": "Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, Marco Pavone", "title": "Trajectron++: Dynamically-Feasible Trajectory Forecasting With\n  Heterogeneous Data", "comments": "23 pages, 6 figures, 5 tables. All code, models, and data can be\n  found at https://github.com/StanfordASL/Trajectron-plus-plus . European\n  Conference on Computer Vision (ECCV) 2020. Fixed a few typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning about human motion is an important prerequisite to safe and\nsocially-aware robotic navigation. As a result, multi-agent behavior prediction\nhas become a core component of modern human-robot interactive systems, such as\nself-driving cars. While there exist many methods for trajectory forecasting,\nmost do not enforce dynamic constraints and do not account for environmental\ninformation (e.g., maps). Towards this end, we present Trajectron++, a modular,\ngraph-structured recurrent model that forecasts the trajectories of a general\nnumber of diverse agents while incorporating agent dynamics and heterogeneous\ndata (e.g., semantic maps). Trajectron++ is designed to be tightly integrated\nwith robotic planning and control frameworks; for example, it can produce\npredictions that are optionally conditioned on ego-agent motion plans. We\ndemonstrate its performance on several challenging real-world trajectory\nforecasting datasets, outperforming a wide array of state-of-the-art\ndeterministic and generative methods.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 16:47:17 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2020 21:42:44 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 07:41:13 GMT"}, {"version": "v4", "created": "Sat, 21 Nov 2020 00:14:52 GMT"}, {"version": "v5", "created": "Wed, 13 Jan 2021 18:53:02 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Salzmann", "Tim", ""], ["Ivanovic", "Boris", ""], ["Chakravarty", "Punarjay", ""], ["Pavone", "Marco", ""]]}, {"id": "2001.03231", "submitter": "Mariah Schrum", "authors": "Mariah L. Schrum, Michael Johnson, Muyleng Ghuy, Matthew C. Gombolay", "title": "Four Years in Review: Statistical Practices of Likert Scales in\n  Human-Robot Interaction Studies", "comments": null, "journal-ref": null, "doi": "10.1145/3319502.3378178", "report-no": null, "categories": "cs.HC cs.RO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots become more prevalent, the importance of the field of human-robot\ninteraction (HRI) grows accordingly. As such, we should endeavor to employ the\nbest statistical practices. Likert scales are commonly used metrics in HRI to\nmeasure perceptions and attitudes. Due to misinformation or honest mistakes,\nmost HRI researchers do not adopt best practices when analyzing Likert data. We\nconduct a review of psychometric literature to determine the current standard\nfor Likert scale design and analysis. Next, we conduct a survey of four years\nof the International Conference on Human-Robot Interaction (2016 through 2019)\nand report on incorrect statistical practices and design of Likert scales.\nDuring these years, only 3 of the 110 papers applied proper statistical testing\nto correctly-designed Likert scales. Our analysis suggests there are areas for\nmeaningful improvement in the design and testing of Likert scales. Lastly, we\nprovide recommendations to improve the accuracy of conclusions drawn from\nLikert data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 21:45:25 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 01:50:53 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Schrum", "Mariah L.", ""], ["Johnson", "Michael", ""], ["Ghuy", "Muyleng", ""], ["Gombolay", "Matthew C.", ""]]}, {"id": "2001.03271", "submitter": "Alireza Karduni", "authors": "Alireza Karduni, Ryan Wesslen, Isaac Cho, Wenwen Dou", "title": "Du Bois Wrapped Bar Chart: Visualizing categorical data with\n  disproportionate values", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a visualization technique, Du Bois wrapped bar chart, inspired by\nwork of W.E.B Du Bois. Du Bois wrapped bar charts enable better large-to-small\nbar comparison by wrapping large bars over a certain threshold. We first\npresent two crowdsourcing experiments comparing wrapped and standard bar charts\nto evaluate (1) the benefit of wrapped bars in helping participants identify\nand compare values; (2) the characteristics of data most suitable for wrapped\nbars. In the first study (n=98) using real-world datasets, we find that wrapped\nbar charts lead to higher accuracy in identifying and estimating ratios between\nbars. In a follow-up study (n=190) with 13 simulated datasets, we find\nparticipants were consistently more accurate with wrapped bar charts when\ncertain category values are disproportionate as measured by entropy and\nH-spread. Finally, in an in-lab study, we investigate participants' experience\nand strategies, leading to guidelines for when and how to use wrapped bar\ncharts.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 01:18:07 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 19:35:34 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Karduni", "Alireza", ""], ["Wesslen", "Ryan", ""], ["Cho", "Isaac", ""], ["Dou", "Wenwen", ""]]}, {"id": "2001.03352", "submitter": "Sunjun Kim", "authors": "Sunjun Kim, Byungjoo Lee, Thomas van Gemert, Antti Oulasvirta", "title": "Optimal Sensor Position for a Computer Mouse", "comments": "13 pages, CHI 2020", "journal-ref": null, "doi": "10.1145/3313831.3376735", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer mice have their displacement sensors in various locations (center,\nfront, and rear). However, there has been little research into the effects of\nsensor position or on engineering approaches to exploit it. This paper first\ndiscusses the mechanisms via which sensor position affects mouse movement and\nreports the results from a study of a pointing task in which the sensor\nposition was systematically varied. Placing the sensor in the center turned out\nto be the best compromise: improvements over front and rear were in the 11--14%\nrange for throughput and 20--23% for path deviation. However, users varied in\ntheir personal optima. Accordingly, variable-sensor-position mice are then\npresented, with a demonstration that high accuracy can be achieved with two\nstatic optical sensors. A virtual sensor model is described that allows\nsoftware-side repositioning of the sensor. Individual-specific calibration\nshould yield an added 4% improvement in throughput over the default center\nposition.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 09:03:45 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Kim", "Sunjun", ""], ["Lee", "Byungjoo", ""], ["van Gemert", "Thomas", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "2001.03542", "submitter": "Glenn Van Wallendael", "authors": "Jolien De Letter, Anissa All, Lieven De Marez, Vasileios Avramelos,\n  Peter Lambert, Glenn Van Wallendael", "title": "Exploratory Study on User's Dynamic Visual Acuity and Quality Perception\n  of Impaired Images", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we assess the impact of head movement on user's visual acuity\nand their quality perception of impaired images. There are physical limitations\non the amount of visual information a person can perceive and physical\nlimitations regarding the speed at which our body, and as a consequence our\nhead, can explore a scene. In these limitations lie fundamental solutions for\nthe communication of multimedia systems. As such, subjects were asked to\nevaluate the perceptual quality of static images presented on a TV screen while\ntheir head was in a dynamic (moving) state. The idea is potentially applicable\nto virtual reality applications and therefore, we also measured the image\nquality perception of each subject on a head mounted display. Experiments show\nthe significant decrease in visual acuity and quality perception when the\nuser's head is not static, and give an indication on how much the quality can\nbe reduced without the user noticing any impairments.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 16:15:06 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["De Letter", "Jolien", ""], ["All", "Anissa", ""], ["De Marez", "Lieven", ""], ["Avramelos", "Vasileios", ""], ["Lambert", "Peter", ""], ["Van Wallendael", "Glenn", ""]]}, {"id": "2001.03687", "submitter": "Naina Dhingra", "authors": "Naina Dhingra, Eugenio Valli and Andreas Kunz", "title": "Recognition and Localisation of Pointing Gestures using a RGB-D Camera", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-verbal communication is part of our regular conversation, and multiple\ngestures are used to exchange information. Among those gestures, pointing is\nthe most important one. If such gestures cannot be perceived by other team\nmembers, e.g. by blind and visually impaired people (BVIP), they lack important\ninformation and can hardly participate in a lively workflow. Thus, this paper\ndescribes a system for detecting such pointing gestures to provide input for\nsuitable output modalities to BVIP. Our system employs an RGB-D camera to\nrecognize the pointing gestures performed by the users. The system also locates\nthe target of pointing e.g. on a common workspace. We evaluated the system by\nconducting a user study with 26 users. The results show that the system has a\nsuccess rate of 89.59 and 79.92 % for a 2 x 3 matrix using the left and right\narm respectively, and 73.57 and 68.99 % for 3 x 4 matrix using the left and\nright arm respectively.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 23:02:21 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Dhingra", "Naina", ""], ["Valli", "Eugenio", ""], ["Kunz", "Andreas", ""]]}, {"id": "2001.03849", "submitter": "Ankit Agrawal", "authors": "Ankit Agrawal (1), Sophia Abraham (1), Benjamin Burger (1), Chichi\n  Christine (1), Luke Fraser (1), John Hoeksema (1), Sara Hwang (1), Elizabeth\n  Travnik (1), Shreya Kumar (1), Walter Scheirer (1), Jane Cleland-Huang (1),\n  Michael Vierhauser (2), Ryan Bauer (3), Steve Cox (3) ((1) University of\n  Notre Dame, USA, (2) Johannes Keppler University, Linz, Austria, (3) South\n  Bend Fire Department, South Bend, IN, USA)", "title": "The Next Generation of Human-Drone Partnerships: Co-Designing an\n  Emergency Response System", "comments": "10 Pages, 5 Figures, 2 Tables. This article is publishing in CHI2020", "journal-ref": null, "doi": "10.1145/3313831.3376825", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The use of semi-autonomous Unmanned Aerial Vehicles (UAV) to support\nemergency response scenarios, such as fire surveillance and search and rescue,\noffers the potential for huge societal benefits. However, designing an\neffective solution in this complex domain represents a \"wicked design\" problem,\nrequiring a careful balance between trade-offs associated with drone autonomy\nversus human control, mission functionality versus safety, and the diverse\nneeds of different stakeholders. This paper focuses on designing for\nsituational awareness (SA) using a scenario-driven, participatory design\nprocess. We developed SA cards describing six common design-problems, known as\nSA demons, and three new demons of importance to our domain. We then used these\nSA cards to equip domain experts with SA knowledge so that they could more\nfully engage in the design process. We designed a potentially reusable solution\nfor achieving SA in multi-stakeholder, multi-UAV, emergency response\napplications.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 04:54:39 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Agrawal", "Ankit", ""], ["Abraham", "Sophia", ""], ["Burger", "Benjamin", ""], ["Christine", "Chichi", ""], ["Fraser", "Luke", ""], ["Hoeksema", "John", ""], ["Hwang", "Sara", ""], ["Travnik", "Elizabeth", ""], ["Kumar", "Shreya", ""], ["Scheirer", "Walter", ""], ["Cleland-Huang", "Jane", ""], ["Vierhauser", "Michael", ""], ["Bauer", "Ryan", ""], ["Cox", "Steve", ""]]}, {"id": "2001.03899", "submitter": "Tommaso Lisini Baldi Dr.", "authors": "Tommaso Lisini Baldi, Gianluca Paolocci, Davide Barcelli, and Domenico\n  Prattichizzo", "title": "Wearable Haptics for Remote Social Walking", "comments": null, "journal-ref": "IEEE Transactions on Haptics, 2020", "doi": "10.1109/TOH.2020.2967049", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walking is an essential activity for a healthy life, which becomes less\ntiring and more enjoyable if done together. Common difficulties we have in\nperforming sufficient physical exercise, for instance the lack of motivation,\ncan be overcome by exploiting its social aspect. However, our lifestyle\nsometimes makes it very difficult to find time together with others who live\nfar away from us to go for a walk. In this paper we propose a novel system\nenabling people to have a 'remote social walk' by streaming the gait cadence\nbetween two persons walking in different places, increasing the sense of mutual\npresence. Vibrations provided at the users' ankles display the partner's\nsensation perceived during the heel-strike. In order to achieve the\naforementioned goal in a two users experiment, we envisaged a four-step\nincremental validation process: i) a single walker has to adapt the cadence\nwith a virtual reference generated by a software; ii) a single user is tasked\nto follow a predefined time varying gait cadence; iii) a leader-follower\nscenario in which the haptic actuation is mono-directional; iv) a peer-to-peer\ncase with bi-directional haptic communication. Careful experimental validation\nwas conducted involving a total of 50 people, which confirmed the efficacy of\nour system in perceiving the partners' gait cadence in each of the proposed\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 10:05:33 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 09:38:59 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Baldi", "Tommaso Lisini", ""], ["Paolocci", "Gianluca", ""], ["Barcelli", "Davide", ""], ["Prattichizzo", "Domenico", ""]]}, {"id": "2001.04180", "submitter": "Ulrik Lyngs", "authors": "Ulrik Lyngs, Kai Lukoff, Petr Slovak, William Seymour, Helena Webb,\n  Marina Jirotka, Jun Zhao, Max Van Kleek, Nigel Shadbolt", "title": "'I Just Want to Hack Myself to Not Get Distracted': Evaluating Design\n  Interventions for Self-Control on Facebook", "comments": "10 pages (excluding references), 6 figures. To appear in the\n  Proceedings of CHI '20 CHI Conference on Human Factors in Computing Systems,\n  April 25--30, 2020, Honolulu, HI, USA", "journal-ref": null, "doi": "10.1145/3313831.3376672", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Beyond being the world's largest social network, Facebook is for many also\none of its greatest sources of digital distraction. For students, problematic\nuse has been associated with negative effects on academic achievement and\ngeneral wellbeing. To understand what strategies could help users regain\ncontrol, we investigated how simple interventions to the Facebook UI affect\nbehaviour and perceived control. We assigned 58 university students to one of\nthree interventions: goal reminders, removed newsfeed, or white background\n(control). We logged use for 6 weeks, applied interventions in the middle\nweeks, and administered fortnightly surveys. Both goal reminders and removed\nnewsfeed helped participants stay on task and avoid distraction. However, goal\nreminders were often annoying, and removing the newsfeed made some fear missing\nout on information. Our findings point to future interventions such as controls\nfor adjusting types and amount of available information, and flexible blocking\nwhich matches individual definitions of 'distraction'.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 12:19:48 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 15:51:14 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Lyngs", "Ulrik", ""], ["Lukoff", "Kai", ""], ["Slovak", "Petr", ""], ["Seymour", "William", ""], ["Webb", "Helena", ""], ["Jirotka", "Marina", ""], ["Zhao", "Jun", ""], ["Van Kleek", "Max", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "2001.04352", "submitter": "Yi-Chi Liao", "authors": "Yi-Chi Liao, Sunjun Kim, Byungjoo Lee, Antti Oulasvirta", "title": "Button Simulation and Design via FDVV Models", "comments": "10 pages, 16 figures, CHI'20", "journal-ref": null, "doi": "10.1145/3313831.3376262", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a push-button with desired sensation and performance is challenging\nbecause the mechanical construction must have the right response\ncharacteristics. Physical simulation of a button's force-displacement (FD)\nresponse has been studied to facilitate prototyping; however, the simulations'\nscope and realism have been limited. In this paper, we extend FD modeling to\ninclude vibration (V) and velocity-dependence characteristics (V). The\nresulting FDVV models better capture tactility characteristics of buttons,\nincluding snap. They increase the range of simulated buttons and the perceived\nrealism relative to FD models. The paper also demonstrates methods for\nobtaining these models, editing them, and simulating accordingly. This\nend-to-end approach enables the analysis, prototyping, and optimization of\nbuttons, and supports exploring designs that would be hard to implement\nmechanically.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 15:40:12 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 15:18:57 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Liao", "Yi-Chi", ""], ["Kim", "Sunjun", ""], ["Lee", "Byungjoo", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "2001.04461", "submitter": "Anelise Newman", "authors": "Anelise Newman, Barry McNamara, Camilo Fosco, Yun Bin Zhang, Pat\n  Sukhum, Matthew Tancik, Nam Wook Kim, Zoya Bylinskii", "title": "TurkEyes: A Web-Based Toolbox for Crowdsourcing Attention Data", "comments": "To appear in CHI 2020. Code available at http://turkeyes.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements provide insight into what parts of an image a viewer finds most\nsalient, interesting, or relevant to the task at hand. Unfortunately, eye\ntracking data, a commonly-used proxy for attention, is cumbersome to collect.\nHere we explore an alternative: a comprehensive web-based toolbox for\ncrowdsourcing visual attention. We draw from four main classes of\nattention-capturing methodologies in the literature. ZoomMaps is a novel\n\"zoom-based\" interface that captures viewing on a mobile phone. CodeCharts is a\n\"self-reporting\" methodology that records points of interest at precise viewing\ndurations. ImportAnnots is an \"annotation\" tool for selecting important image\nregions, and \"cursor-based\" BubbleView lets viewers click to deblur a small\narea. We compare these methodologies using a common analysis framework in order\nto develop appropriate use cases for each interface. This toolbox and our\nanalyses provide a blueprint for how to gather attention data at scale without\nan eye tracker.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:55:14 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Newman", "Anelise", ""], ["McNamara", "Barry", ""], ["Fosco", "Camilo", ""], ["Zhang", "Yun Bin", ""], ["Sukhum", "Pat", ""], ["Tancik", "Matthew", ""], ["Kim", "Nam Wook", ""], ["Bylinskii", "Zoya", ""]]}, {"id": "2001.04465", "submitter": "Andreea Bobu", "authors": "Andreea Bobu, Dexter R.R. Scobee, Jaime F. Fisac, S. Shankar Sastry,\n  Anca D. Dragan", "title": "LESS is More: Rethinking Probabilistic Models of Human Behavior", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3319502.3374811", "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots need models of human behavior for both inferring human goals and\npreferences, and predicting what people will do. A common model is the\nBoltzmann noisily-rational decision model, which assumes people approximately\noptimize a reward function and choose trajectories in proportion to their\nexponentiated reward. While this model has been successful in a variety of\nrobotics domains, its roots lie in econometrics, and in modeling decisions\namong different discrete options, each with its own utility or reward. In\ncontrast, human trajectories lie in a continuous space, with continuous-valued\nfeatures that influence the reward function. We propose that it is time to\nrethink the Boltzmann model, and design it from the ground up to operate over\nsuch trajectory spaces. We introduce a model that explicitly accounts for\ndistances between trajectories, rather than only their rewards. Rather than\neach trajectory affecting the decision independently, similar trajectories now\naffect the decision together. We start by showing that our model better\nexplains human behavior in a user study. We then analyze the implications this\nhas for robot inference, first in toy environments where we have ground truth\nand find more accurate inference, and finally for a 7DOF robot arm learning\nfrom user demonstrations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:59:01 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Bobu", "Andreea", ""], ["Scobee", "Dexter R. R.", ""], ["Fisac", "Jaime F.", ""], ["Sastry", "S. Shankar", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2001.04509", "submitter": "Lionel Robert", "authors": "Na Du, Feng Zhou, Elizabeth Pulver, Dawn M. Tilbury, Lionel P. Robert,\n  Anuj K. Pradhan, X. Jessie Yang", "title": "Examining the Effects of Emotional Valence and Arousal on Takeover\n  Performance in Conditionally Automated Driving", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In conditionally automated driving, drivers have difficulty in takeover\ntransitions as they become increasingly decoupled from the operational level of\ndriving. Factors influencing takeover performance, such as takeover lead time\nand the engagement of non-driving related tasks, have been studied in the past.\nHowever, despite the important role emotions play in human-machine interaction\nand in manual driving, little is known about how emotions influence drivers\ntakeover performance. This study, therefore, examined the effects of emotional\nvalence and arousal on drivers takeover timeliness and quality in conditionally\nautomated driving. We conducted a driving simulation experiment with 32\nparticipants. Movie clips were played for emotion induction. Participants with\ndifferent levels of emotional valence and arousal were required to take over\ncontrol from automated driving, and their takeover time and quality were\nanalyzed. Results indicate that positive valence led to better takeover quality\nin the form of a smaller maximum resulting acceleration and a smaller maximum\nresulting jerk. However, high arousal did not yield an advantage in takeover\ntime. This study contributes to the literature by demonstrating how emotional\nvalence and arousal affect takeover performance. The benefits of positive\nemotions carry over from manual driving to conditionally automated driving\nwhile the benefits of arousal do not.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 19:28:15 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Du", "Na", ""], ["Zhou", "Feng", ""], ["Pulver", "Elizabeth", ""], ["Tilbury", "Dawn M.", ""], ["Robert", "Lionel P.", ""], ["Pradhan", "Anuj K.", ""], ["Yang", "X. Jessie", ""]]}, {"id": "2001.04519", "submitter": "Chieh-Yang Huang", "authors": "Chieh-Yang Huang, Shih-Hong Huang, Ting-Hao 'Kenneth' Huang", "title": "Heteroglossia: In-Situ Story Ideation with the Crowd", "comments": "Accepted by CHI 2020. Video Promotion:\n  https://www.youtube.com/watch?v=i0G-tq3d8c0", "journal-ref": null, "doi": "10.1145/3313831.3376715", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ideation is essential for creative writing. Many authors struggle to come up\nwith ideas throughout the writing process, yet modern writing tools fail to\nprovide on-the-spot assistance for writers when they get stuck. This paper\nintroduces Heteroglossia, an add-on for Google Docs that allows writers to\nelicit story ideas from the online crowd using their text editors. Writers can\nshare snippets of their working drafts and ask the crowd to provide follow-up\nstory ideas based on it. Heteroglossia employs a strategy called \"role play\",\nwhere each worker is assigned a fictional character in a story and asked to\nbrainstorm plot ideas from that character's perspective. Our deployment with\ntwo experienced story writers shows that Heteroglossia is easy to use and can\ngenerate interesting ideas. Heteroglossia allows us to gain insight into how\nfuture technologies can be developed to support ideation in creative writing\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 20:00:57 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 20:59:43 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Huang", "Chieh-Yang", ""], ["Huang", "Shih-Hong", ""], ["Huang", "Ting-Hao 'Kenneth'", ""]]}, {"id": "2001.04564", "submitter": "Weiyan Shi", "authors": "Weiyan Shi, Xuewei Wang, Yoo Jung Oh, Jingwen Zhang, Saurav Sahay,\n  Zhou Yu", "title": "Effects of Persuasive Dialogues: Testing Bot Identities and Inquiry\n  Strategies", "comments": "15 pages, 10 figures. Full paper to appear at ACM CHI 2020", "journal-ref": null, "doi": "10.1145/3313831.3376843", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent conversational agents, or chatbots, can take on various\nidentities and are increasingly engaging in more human-centered conversations\nwith persuasive goals. However, little is known about how identities and\ninquiry strategies influence the conversation's effectiveness. We conducted an\nonline study involving 790 participants to be persuaded by a chatbot for\ncharity donation. We designed a two by four factorial experiment (two chatbot\nidentities and four inquiry strategies) where participants were randomly\nassigned to different conditions. Findings showed that the perceived identity\nof the chatbot had significant effects on the persuasion outcome (i.e.,\ndonation) and interpersonal perceptions (i.e., competence, confidence, warmth,\nand sincerity). Further, we identified interaction effects among perceived\nidentities and inquiry strategies. We discuss the findings for theoretical and\npractical implications for developing ethical and effective persuasive\nchatbots. Our published data, codes, and analyses serve as the first step\ntowards building competent ethical persuasive chatbots.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 23:41:55 GMT"}, {"version": "v2", "created": "Sat, 18 Jan 2020 19:13:23 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Shi", "Weiyan", ""], ["Wang", "Xuewei", ""], ["Oh", "Yoo Jung", ""], ["Zhang", "Jingwen", ""], ["Sahay", "Saurav", ""], ["Yu", "Zhou", ""]]}, {"id": "2001.04568", "submitter": "Zhenqiang Ying", "authors": "Zhenqiang Ying and Alan Bovik", "title": "180-degree Outpainting from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Presenting context images to a viewer's peripheral vision is one of the most\neffective techniques to enhance immersive visual experiences. However, most\nimages only present a narrow view, since the field-of-view (FoV) of standard\ncameras is small. To overcome this limitation, we propose a deep learning\napproach that learns to predict a 180{\\deg} panoramic image from a narrow-view\nimage. Specifically, we design a foveated framework that applies different\nstrategies on near-periphery and mid-periphery regions. Two networks are\ntrained separately, and then are employed jointly to sequentially perform\nnarrow-to-90{\\deg} generation and 90{\\deg}-to-180{\\deg} generation. The\ngenerated outputs are then fused with their aligned inputs to produce expanded\nequirectangular images for viewing. Our experimental results show that\nsingle-view-to-panoramic image generation using deep learning is both feasible\nand promising.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 23:50:34 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Ying", "Zhenqiang", ""], ["Bovik", "Alan", ""]]}, {"id": "2001.04612", "submitter": "Simon Perrault", "authors": "Sanju Menon and Weiyu Zhang and Simon T. Perrault", "title": "Nudge for Deliberativeness: How Interface Features Influence Online\n  Discourse", "comments": "CHI 2020, 10 pages", "journal-ref": null, "doi": "10.1145/3313831.3376646", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive load is a significant challenge to users for being deliberative.\nInterface design has been used to mitigate this cognitive state. This paper\nsurveys literature on the anchoring effect, partitioning effect and\npoint-of-choice effect, based on which we propose three interface nudges,\nnamely, the word-count anchor, partitioning text fields, and reply choice\nprompt. We then conducted a 2*2*2 factorial experiment with 80 participants (10\nfor each condition), testing how these nudges affect deliberativeness. The\nresults showed a significant positive impact of the word-count anchor. There\nwas also a significant positive impact of the partitioning text fields on the\nword count of response. The reply choice prompt showed a surprisingly negative\naffect on the quantity of response, hinting at the possibility that the reply\nchoice prompt induces a fear of evaluation, which could in turn dampen the\nwillingness to reply.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 03:52:31 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Menon", "Sanju", ""], ["Zhang", "Weiyu", ""], ["Perrault", "Simon T.", ""]]}, {"id": "2001.04809", "submitter": "Joshua Kim", "authors": "Joshua Y. Kim, Greyson Y. Kim and Kalina Yacef", "title": "Detecting depression in dyadic conversations with multimodal narratives\n  and visualizations", "comments": "12 pages", "journal-ref": "AI 2019: Advances in Artificial Intelligence. AI 2019 vol 11919", "doi": "10.1007/978-3-030-35288-2_25", "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversations contain a wide spectrum of multimodal information that gives us\nhints about the emotions and moods of the speaker. In this paper, we developed\na system that supports humans to analyze conversations. Our main contribution\nis the identification of appropriate multimodal features and the integration of\nsuch features into verbatim conversation transcripts. We demonstrate the\nability of our system to take in a wide range of multimodal information and\nautomatically generated a prediction score for the depression state of the\nindividual. Our experiments showed that this approach yielded better\nperformance than the baseline model. Furthermore, the multimodal narrative\napproach makes it easy to integrate learnings from other disciplines, such as\nconversational analysis and psychology. Lastly, this interdisciplinary and\nautomated approach is a step towards emulating how practitioners record the\ncourse of treatment as well as emulating how conversational analysts have been\nanalyzing conversations by hand.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 10:47:13 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 23:16:48 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Kim", "Joshua Y.", ""], ["Kim", "Greyson Y.", ""], ["Yacef", "Kalina", ""]]}, {"id": "2001.04839", "submitter": "Yi Guo", "authors": "Yi Guo, Li Mao, Gongsen Zhang, Zhi Chen, Xi Pei and X. George Xu", "title": "Conceptual Design and Preliminary Results of a VR-based Radiation Safety\n  Training System for Interventional Radiologists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have reported an increased risk of developing brain and neck\ntumors, as well as cataracts, in practitioners in interventional radiology\n(IR). Occupational radiation protection in IR has been a top concern for\nregulatory agencies and professional societies. To help minimize occupational\nradiation exposure in IR, we conceptualized a virtual reality (VR) based\nradiation safety training system to help operators understand complex radiation\nfields and to avoid high radiation areas through game-like interactive\nsimulations. The preliminary development of the system has yielded results\nsuggesting that the training system can calculate and report the radiation\nexposure after each training session based on a database precalculated from\ncomputational phantoms and Monte Carlo simulations and the position information\nprovided in real-time by the MS Hololens headset worn by trainee. In addition,\nreal-time dose rate and cumulative dose will be displayed to the trainee by MS\nHololens to help them adjust their practice. This paper presents the conceptual\ndesign of the overall hardware and software design, as well as preliminary\nresults to combine MS HoloLens headset and complex 3D X-ray field spatial\ndistribution data to create a mixed reality environment for safety training\npurpose in IR.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 15:02:47 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Guo", "Yi", ""], ["Mao", "Li", ""], ["Zhang", "Gongsen", ""], ["Chen", "Zhi", ""], ["Pei", "Xi", ""], ["Xu", "X. George", ""]]}, {"id": "2001.04879", "submitter": "C. Estelle Smith", "authors": "C. Estelle Smith, Bowen Yu, Anjali Srivastava, Aaron Halfaker, Loren\n  Terveen, Haiyi Zhu", "title": "Keeping Community in the Loop: Understanding Wikipedia Stakeholder\n  Values for Machine Learning-Based Systems", "comments": "10 pages, 1 table, accepted paper to CHI 2020 conference", "journal-ref": null, "doi": "10.1145/3313831.3376783", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  On Wikipedia, sophisticated algorithmic tools are used to assess the quality\nof edits and take corrective actions. However, algorithms can fail to solve the\nproblems they were designed for if they conflict with the values of communities\nwho use them. In this study, we take a Value-Sensitive Algorithm Design\napproach to understanding a community-created and -maintained machine\nlearning-based algorithm called the Objective Revision Evaluation System\n(ORES)---a quality prediction system used in numerous Wikipedia applications\nand contexts. Five major values converged across stakeholder groups that ORES\n(and its dependent applications) should: (1) reduce the effort of community\nmaintenance, (2) maintain human judgement as the final authority, (3) support\ndiffering peoples' differing workflows, (4) encourage positive engagement with\ndiverse editor groups, and (5) establish trustworthiness of people and\nalgorithms within the community. We reveal tensions between these values and\ndiscuss implications for future research to improve algorithms like ORES.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 16:30:25 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Smith", "C. Estelle", ""], ["Yu", "Bowen", ""], ["Srivastava", "Anjali", ""], ["Halfaker", "Aaron", ""], ["Terveen", "Loren", ""], ["Zhu", "Haiyi", ""]]}, {"id": "2001.04883", "submitter": "C. Estelle Smith", "authors": "C. Estelle Smith, Eduardo Nevarez, Haiyi Zhu", "title": "Disseminating Research News in HCI: Perceived Hazards, How-To's, and\n  Opportunities for Innovation", "comments": "10 pages, 2 figures, accepted paper to CHI 2020 conference", "journal-ref": null, "doi": "10.1145/3313831.3376744", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass media afford researchers critical opportunities to disseminate research\nfindings and trends to the general public. Yet researchers also perceive that\ntheir work can be miscommunicated in mass media, thus generating unintended\nunderstandings of HCI research by the general public. We conduct a Grounded\nTheory analysis of interviews with 12 HCI researchers and find that\nmiscommunication can occur at four origins along the socio-technical\ninfrastructure known as the Media Production Pipeline (MPP) for science news.\nResults yield researchers' perceived hazards of disseminating their work\nthrough mass media, as well as strategies for fostering effective communication\nof research. We conclude with implications for augmenting or innovating new MPP\ntechnologies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 16:34:24 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Smith", "C. Estelle", ""], ["Nevarez", "Eduardo", ""], ["Zhu", "Haiyi", ""]]}, {"id": "2001.05149", "submitter": "Yao Xie", "authors": "Yao Xie, Melody Chen, David Kao, Ge Gao, and Xiang 'Anthony' Chen", "title": "CheXplain: Enabling Physicians to Explore and UnderstandData-Driven,\n  AI-Enabled Medical Imaging Analysis", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3313831.3376807", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent development of data-driven AI promises to automate medical\ndiagnosis; however, most AI functions as 'black boxes' to physicians with\nlimited computational knowledge. Using medical imaging as a point of departure,\nwe conducted three iterations of design activities to formulate CheXplain---a\nsystem that enables physicians to explore and understand AI-enabled chest X-ray\nanalysis: (1) a paired survey between referring physicians and radiologists\nreveals whether, when, and what kinds of explanations are needed; (2) a\nlow-fidelity prototype co-designed with three physicians formulates eight key\nfeatures; and (3) a high-fidelity prototype evaluated by another six physicians\nprovides detailed summative insights on how each feature enables the\nexploration and understanding of AI. We summarize by discussing recommendations\nfor future work to design and implement explainable medical AI systems that\nencompass four recurring themes: motivation, constraint, explanation, and\njustification.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 06:44:52 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 06:04:44 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Xie", "Yao", ""], ["Chen", "Melody", ""], ["Kao", "David", ""], ["Gao", "Ge", ""], ["Chen", "Xiang 'Anthony'", ""]]}, {"id": "2001.05152", "submitter": "Nilavra Bhattacharya", "authors": "Nilavra Bhattacharya, Somnath Rakshit, Jacek Gwizdka, Paul Kogut", "title": "Relevance Prediction from Eye-movements Using Semi-interpretable\n  Convolutional Neural Networks", "comments": null, "journal-ref": "2020 Conference on Human Information Interaction and Retrieval\n  (CHIIR '20), March 14--18, 2020, Vancouver, BC, Canada", "doi": "10.1145/3343413.3377960", "report-no": null, "categories": "cs.HC cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an image-classification method to predict the perceived-relevance\nof text documents from eye-movements. An eye-tracking study was conducted where\nparticipants read short news articles, and rated them as relevant or irrelevant\nfor answering a trigger question. We encode participants' eye-movement\nscanpaths as images, and then train a convolutional neural network classifier\nusing these scanpath images. The trained classifier is used to predict\nparticipants' perceived-relevance of news articles from the corresponding\nscanpath images. This method is content-independent, as the classifier does not\nrequire knowledge of the screen-content, or the user's information-task. Even\nwith little data, the image classifier can predict perceived-relevance with up\nto 80% accuracy. When compared to similar eye-tracking studies from the\nliterature, this scanpath image classification method outperforms previously\nreported metrics by appreciable margins. We also attempt to interpret how the\nimage classifier differentiates between scanpaths on relevant and irrelevant\ndocuments.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 07:02:14 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Bhattacharya", "Nilavra", ""], ["Rakshit", "Somnath", ""], ["Gwizdka", "Jacek", ""], ["Kogut", "Paul", ""]]}, {"id": "2001.05166", "submitter": "Nupur Kumari", "authors": "Nupur Kumari, Siddarth R., Akash Rupela, Piyush Gupta, Balaji\n  Krishnamurthy", "title": "ShapeVis: High-dimensional Data Visualization at Scale", "comments": "Accepted at WWW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present ShapeVis, a scalable visualization technique for point cloud data\ninspired from topological data analysis. Our method captures the underlying\ngeometric and topological structure of the data in a compressed graphical\nrepresentation. Much success has been reported by the data visualization\ntechnique Mapper, that discreetly approximates the Reeb graph of a filter\nfunction on the data. However, when using standard dimensionality reduction\nalgorithms as the filter function, Mapper suffers from considerable\ncomputational cost. This makes it difficult to scale to high-dimensional data.\nOur proposed technique relies on finding a subset of points called landmarks\nalong the data manifold to construct a weighted witness-graph over it. This\ngraph captures the structural characteristics of the point cloud, and its\nweights are determined using a Finite Markov Chain. We further compress this\ngraph by applying induced maps from standard community detection algorithms.\nUsing techniques borrowed from manifold tearing, we prune and reinstate edges\nin the induced graph based on their modularity to summarize the shape of data.\nWe empirically demonstrate how our technique captures the structural\ncharacteristics of real and synthetic data sets. Further, we compare our\napproach with Mapper using various filter functions like t-SNE, UMAP, LargeVis\nand show that our algorithm scales to millions of data points while preserving\nthe quality of data visualization.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 07:59:13 GMT"}, {"version": "v2", "created": "Tue, 21 Jan 2020 16:12:47 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Kumari", "Nupur", ""], ["R.", "Siddarth", ""], ["Rupela", "Akash", ""], ["Gupta", "Piyush", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "2001.05171", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Xiong Zhang and Jonathan Engel and Sara Evensen and Yuliang Li and\n  \\c{C}a\\u{g}atay Demiralp and Wang-Chiew Tan", "title": "Teddy: A System for Interactive Review Analysis", "comments": "CHI'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reviews are integral to e-commerce services and products. They contain a\nwealth of information about the opinions and experiences of users, which can\nhelp better understand consumer decisions and improve user experience with\nproducts and services. Today, data scientists analyze reviews by developing\nrules and models to extract, aggregate, and understand information embedded in\nthe review text. However, working with thousands of reviews, which are\ntypically noisy incomplete text, can be daunting without proper tools. Here we\nfirst contribute results from an interview study that we conducted with fifteen\ndata scientists who work with review text, providing insights into their\npractices and challenges. Results suggest data scientists need interactive\nsystems for many review analysis tasks. In response we introduce Teddy, an\ninteractive system that enables data scientists to quickly obtain insights from\nreviews and improve their extraction and modeling pipelines.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 08:19:01 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Zhang", "Xiong", ""], ["Engel", "Jonathan", ""], ["Evensen", "Sara", ""], ["Li", "Yuliang", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Tan", "Wang-Chiew", ""]]}, {"id": "2001.05308", "submitter": "Yang Li", "authors": "Yang Li, Julien Amelot, Xin Zhou, Samy Bengio, Si Si", "title": "Auto Completion of User Interface Layout Design Using Transformer-Based\n  Tree Decoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been of increasing interest in the field to develop automatic\nmachineries to facilitate the design process. In this paper, we focus on\nassisting graphical user interface (UI) layout design, a crucial task in app\ndevelopment. Given a partial layout, which a designer has entered, our model\nlearns to complete the layout by predicting the remaining UI elements with a\ncorrect position and dimension as well as the hierarchical structures. Such\nautomation will significantly ease the effort of UI designers and developers.\nWhile we focus on interface layout prediction, our model can be generally\napplicable for other layout prediction problems that involve tree structures\nand 2-dimensional placements. Particularly, we design two versions of\nTransformer-based tree decoders: Pointer and Recursive Transformer, and\nexperiment with these models on a public dataset. We also propose several\nmetrics for measuring the accuracy of tree prediction and ground these metrics\nin the domain of user experience. These contribute a new task and methods to\ndeep learning research.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:24:41 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Li", "Yang", ""], ["Amelot", "Julien", ""], ["Zhou", "Xin", ""], ["Bengio", "Samy", ""], ["Si", "Si", ""]]}, {"id": "2001.05424", "submitter": "Amanda Swearngin", "authors": "Amanda Swearngin, Chenglong Wang, Alannah Oleson, James Fogarty, Amy\n  J. Ko", "title": "Scout: Rapid Exploration of Interface Layout Alternatives through\n  High-Level Design Constraints", "comments": "13 pages, 8 figures, ACM CHI Full Paper (2020)", "journal-ref": "Proceedings of the 2019 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3313831.3376593", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although exploring alternatives is fundamental to creating better interface\ndesigns, current processes for creating alternatives are generally manual,\nlimiting the alternatives a designer can explore. We present Scout, a system\nthat helps designers rapidly explore alternatives through mixed-initiative\ninteraction with high-level constraints and design feedback. Prior\nconstraint-based layout systems use low-level spatial constraints and generally\nproduce a single design. Tosupport designer exploration of alternatives, Scout\nintroduces high-level constraints based on design concepts (e.g.,~semantic\nstructure, emphasis, order) and formalizes them into low-level spatial\nconstraints that a solver uses to generate potential layouts. In an evaluation\nwith 18 interface designers, we found that Scout: (1) helps designers create\nmore spatially diverse layouts with similar quality to those created with a\nbaseline tool and (2) can help designers avoid a linear design process and\nquickly ideate layouts they do not believe they would have thought of on their\nown.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 16:49:26 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Swearngin", "Amanda", ""], ["Wang", "Chenglong", ""], ["Oleson", "Alannah", ""], ["Fogarty", "James", ""], ["Ko", "Amy J.", ""]]}, {"id": "2001.05621", "submitter": "Yuan Liang", "authors": "Yuan Liang, Hsuan-Wei Fan, Zhujun Fang, Leiying Miao, Wen Li, Xuan\n  Zhang, Weibin Sun, Kun Wang, Lei He, Xiang Anthony Chen", "title": "OralCam: Enabling Self-Examination and Awareness of Oral Health Using a\n  Smartphone Camera", "comments": "13 pages, CHI2020 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to a lack of medical resources or oral health awareness, oral diseases\nare often left unexamined and untreated, affecting a large population\nworldwide. With the advent of low-cost, sensor-equipped smartphones, mobile\napps offer a promising possibility for promoting oral health. However, to the\nbest of our knowledge, no mobile health (mHealth) solutions can directly\nsupport a user to self-examine their oral health condition. This paper presents\nOralCam, the first interactive app that enables end-users' self-examination of\nfive common oral conditions (diseases or early disease signals) by taking\nsmartphone photos of one's oral cavity. OralCam allows a user to annotate\nadditional information (e.g. living habits, pain, and bleeding) to augment the\ninput image, and presents the output hierarchically, probabilistically and with\nvisual explanations to help a laymen user understand examination results.\nDeveloped on our in-house dataset that consists of 3,182 oral photos annotated\nby dental experts, our deep learning based framework achieved an average\ndetection sensitivity of 0.787 over five conditions with high localization\naccuracy. In a week-long in-the-wild user study (N=18), most participants had\nno trouble using OralCam and interpreting the examination results. Two expert\ninterviews further validate the feasibility of OralCam for promoting users'\nawareness of oral health.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 02:54:47 GMT"}, {"version": "v2", "created": "Wed, 22 Jan 2020 22:20:28 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Liang", "Yuan", ""], ["Fan", "Hsuan-Wei", ""], ["Fang", "Zhujun", ""], ["Miao", "Leiying", ""], ["Li", "Wen", ""], ["Zhang", "Xuan", ""], ["Sun", "Weibin", ""], ["Wang", "Kun", ""], ["He", "Lei", ""], ["Chen", "Xiang Anthony", ""]]}, {"id": "2001.05684", "submitter": "Chunggi Lee", "authors": "Chunggi Lee, Sanghoon Kim, Dongyun Han, Hongjun Yang, Young-Woo Park,\n  Bum Chul Kwon, Sungahn Ko", "title": "GUIComp: A GUI Design Assistant with Real-Time, Multi-Faceted Feedback", "comments": "10 pages, 3 figures, accepted paper to CHI 2020 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users may face challenges while designing graphical user interfaces, due to a\nlack of relevant experience and guidance. This paper aims to investigate the\nissues that users with no experience face during the design process, and how to\nresolve them. To this end, we conducted semi-structured interviews, based on\nwhich we built a GUI prototyping assistance tool called GUIComp. This tool can\nbe connected to GUI design software as an extension, and it provides real-time,\nmulti-faceted feedback on a user's current design. Additionally, we conducted\ntwo user studies, in which we asked participants to create mobile GUIs with or\nwithout GUIComp, and requested online workers to assess the created GUIs. The\nexperimental results show that GUIComp facilitated iterative design and the\nparticipants with GUIComp had better a user experience and produced more\nacceptable designs than those who did not.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 07:22:56 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Lee", "Chunggi", ""], ["Kim", "Sanghoon", ""], ["Han", "Dongyun", ""], ["Yang", "Hongjun", ""], ["Park", "Young-Woo", ""], ["Kwon", "Bum Chul", ""], ["Ko", "Sungahn", ""]]}, {"id": "2001.05745", "submitter": "Ali Asadipour", "authors": "A. Asadipour, K. Debattista, V. Patel, A. Chalmers", "title": "A Technology-aided Multi-modal Training Approach to Assist Abdominal\n  Palpation Training and its Assessment in Medical Education", "comments": "In Press", "journal-ref": null, "doi": "10.1016/j.ijhcs.2020.102394", "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer-assisted multimodal training is an effective way of learning complex\nmotor skills in various applications. In particular disciplines (eg.\nhealthcare) incompetency in performing dexterous hands-on examinations\n(clinical palpation) may result in misdiagnosis of symptoms, serious injuries\nor even death. Furthermore, a high quality clinical examination can help to\nexclude significant pathology, and reduce time and cost of diagnosis by\neliminating the need for unnecessary medical imaging. Medical palpation is used\nregularly as an effective preliminary diagnosis method all around the world but\nyears of training are required currently to achieve competency. This paper\nfocuses on a multimodal palpation training system to teach and improve clinical\nexamination skills in relation to the abdomen. It is our aim to shorten\nsignificantly the palpation training duration by increasing the frequency of\nrehearsals as well as providing essential augmented feedback on how to perform\nvarious abdominal palpation techniques which has been captured and modelled\nfrom medical experts. Twenty three first year medical students divided into a\ncontrol group (n=8), a semi-visually trained group (n=8), and a fully visually\ntrained group (n=7) were invited to perform three palpation tasks (superficial,\ndeep and liver). The medical students performances were assessed using both\ncomputer-based and human-based methods where a positive correlation was shown\nbetween the generated scores, r=.62, p(one-tailed)<.05. The visually-trained\ngroup significantly outperformed the control group in which abstract\nvisualisation of applied forces and their palmar locations were provided to the\nstudents during each palpation examination (p<.05). Moreover, a positive trend\nwas observed between groups when visual feedback was presented, J=132, z=2.62,\nr=0.55.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 11:31:46 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Asadipour", "A.", ""], ["Debattista", "K.", ""], ["Patel", "V.", ""], ["Chalmers", "A.", ""]]}, {"id": "2001.05780", "submitter": "Aske Mottelson", "authors": "Aske Mottelson, Kasper Hornb{\\ae}k", "title": "Emotional Avatars: The Interplay between Affect and Ownership of a\n  Virtual Body", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human bodies influence the owners' affect through posture, facial\nexpressions, and movement. It remains unclear whether similar links between\nvirtual bodies and affect exist. Such links could present design opportunities\nfor virtual environments and advance our understanding of fundamental concepts\nof embodied VR.\n  An initial outside-the-lab between-subjects study using commodity equipment\npresented 207 participants with seven avatar manipulations, related to posture,\nfacial expression, and speed. We conducted a lab-based between-subjects study\nusing high-end VR equipment with 41 subjects to clarify affect's impact on body\nownership.\n  The results show that some avatar manipulations can subtly influence affect.\nStudy I found that facial manipulations emerged as most effective in this\nregard, particularly for positive affect. Also, body ownership showed a\nmoderating influence on affect: in Study I body ownership varied with valence\nbut not with arousal, and Study II showed body ownership to vary with positive\nbut not with negative affect.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 13:09:56 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 08:12:05 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Mottelson", "Aske", ""], ["Hornb\u00e6k", "Kasper", ""]]}, {"id": "2001.05863", "submitter": "Richard Savery", "authors": "Richard Savery, Ryan Rose, Gil Weinberg", "title": "Establishing Human-Robot Trust through Music-Driven Robotic Emotion\n  Prosody and Gesture", "comments": null, "journal-ref": "The 28th IEEE International Conference on Robot & Human\n  Interactive Communication 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As human-robot collaboration opportunities continue to expand, trust becomes\never more important for full engagement and utilization of robots. Affective\ntrust, built on emotional relationship and interpersonal bonds is particularly\ncritical as it is more resilient to mistakes and increases the willingness to\ncollaborate. In this paper we present a novel model built on music-driven\nemotional prosody and gestures that encourages the perception of a robotic\nidentity, designed to avoid uncanny valley. Symbolic musical phrases were\ngenerated and tagged with emotional information by human musicians. These\nphrases controlled a synthesis engine playing back pre-rendered audio samples\ngenerated through interpolation of phonemes and electronic instruments.\nGestures were also driven by the symbolic phrases, encoding the emotion from\nthe musical phrase to low degree-of-freedom movements. Through a user study we\nshowed that our system was able to accurately portray a range of emotions to\nthe user. We also showed with a significant result that our non-linguistic\naudio generation achieved an 8% higher mean of average trust than using a\nstate-of-the-art text-to-speech system.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 20:36:12 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Savery", "Richard", ""], ["Rose", "Ryan", ""], ["Weinberg", "Gil", ""]]}, {"id": "2001.05871", "submitter": "Vivian Lai", "authors": "Vivian Lai, Han Liu, Chenhao Tan", "title": "\"Why is 'Chicago' deceptive?\" Towards Building Model-Driven Tutorials\n  for Humans", "comments": "26 pages, 48 figures, CHI 2020", "journal-ref": null, "doi": "10.1145/10.1145/3313831.3376873", "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To support human decision making with machine learning models, we often need\nto elucidate patterns embedded in the models that are unsalient, unknown, or\ncounterintuitive to humans. While existing approaches focus on explaining\nmachine predictions with real-time assistance, we explore model-driven\ntutorials to help humans understand these patterns in a training phase. We\nconsider both tutorials with guidelines from scientific papers, analogous to\ncurrent practices of science communication, and automatically selected examples\nfrom training data with explanations. We use deceptive review detection as a\ntestbed and conduct large-scale, randomized human-subject experiments to\nexamine the effectiveness of such tutorials. We find that tutorials indeed\nimprove human performance, with and without real-time assistance. In\nparticular, although deep learning provides superior predictive performance\nthan simple models, tutorials and explanations from simple models are more\nuseful to humans. Our work suggests future directions for human-centered\ntutorials and explanations towards a synergy between humans and AI.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 19:00:00 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Lai", "Vivian", ""], ["Liu", "Han", ""], ["Tan", "Chenhao", ""]]}, {"id": "2001.05952", "submitter": "Patrick Rodler", "authors": "Patrick Rodler", "title": "On Expert Behaviors and Question Types for Efficient Query-Based\n  Ontology Fault Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LO cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We challenge existing query-based ontology fault localization methods wrt.\nassumptions they make, criteria they optimize, and interaction means they use.\nWe find that their efficiency depends largely on the behavior of the\ninteracting expert, that performed calculations can be inefficient or\nimprecise, and that used optimization criteria are often not fully realistic.\nAs a remedy, we suggest a novel (and simpler) interaction approach which\novercomes all identified problems and, in comprehensive experiments on faulty\nreal-world ontologies, enables a successful fault localization while requiring\nfewer expert interactions in 66 % of the cases, and always at least 80 % less\nexpert waiting time, compared to existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 17:23:07 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Rodler", "Patrick", ""]]}, {"id": "2001.06007", "submitter": "Nicolas Lair", "authors": "Nicolas Lair, Cl\\'ement Delgrange, David Mugisha, Jean-Michel Dussoux,\n  Pierre-Yves Oudeyer, and Peter Ford Dominey", "title": "User-in-the-loop Adaptive Intent Detection for Instructable Digital\n  Assistant", "comments": "To be published as a conference paper in the proceedings of IUI'20", "journal-ref": "25th International Conference on Intelligent User Interfaces (IUI\n  '20), March 17--20, 2020, Cagliari, Italy", "doi": "10.1145/3377325.3377490", "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are becoming increasingly comfortable using Digital Assistants (DAs)\nto interact with services or connected objects. However, for non-programming\nusers, the available possibilities for customizing their DA are limited and do\nnot include the possibility of teaching the assistant new tasks. To make the\nmost of the potential of DAs, users should be able to customize assistants by\ninstructing them through Natural Language (NL). To provide such\nfunctionalities, NL interpretation in traditional assistants should be\nimproved: (1) The intent identification system should be able to recognize new\nforms of known intents, and to acquire new intents as they are expressed by the\nuser. (2) In order to be adaptive to novel intents, the Natural Language\nUnderstanding module should be sample efficient, and should not rely on a\npretrained model. Rather, the system should continuously collect the training\ndata as it learns new intents from the user. In this work, we propose AidMe\n(Adaptive Intent Detection in Multi-Domain Environments), a user-in-the-loop\nadaptive intent detection framework that allows the assistant to adapt to its\nuser by learning his intents as their interaction progresses. AidMe builds its\nrepertoire of intents and collects data to train a model of semantic similarity\nevaluation that can discriminate between the learned intents and autonomously\ndiscover new forms of known intents. AidMe addresses two major issues - intent\nlearning and user adaptation - for instructable digital assistants. We\ndemonstrate the capabilities of AidMe as a standalone system by comparing it\nwith a one-shot learning system and a pretrained NLU module through simulations\nof interactions with a user. We also show how AidMe can smoothly integrate to\nan existing instructable digital assistant.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 18:06:43 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Lair", "Nicolas", ""], ["Delgrange", "Cl\u00e9ment", ""], ["Mugisha", "David", ""], ["Dussoux", "Jean-Michel", ""], ["Oudeyer", "Pierre-Yves", ""], ["Dominey", "Peter Ford", ""]]}, {"id": "2001.06047", "submitter": "Assaf Marron", "authors": "Assaf Marron, Lior Limonad, Sarah Pollack, and David Harel", "title": "Expecting the Unexpected: Developing Autonomous-System Design Principles\n  for Reacting to Unpredicted Events and Conditions", "comments": "6 pages; 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When developing autonomous systems, engineers and other stakeholders make\ngreat effort to prepare the system for all foreseeable events and conditions.\nHowever, these systems are still bound to encounter events and conditions that\nwere not considered at design time. For reasons like safety, cost, or ethics,\nit is often highly desired that these new situations be handled correctly upon\nfirst encounter. In this paper we first justify our position that there will\nalways exist unpredicted events and conditions, driven among others by: new\ninventions in the real world; the diversity of world-wide system deployments\nand uses; and, the non-negligible probability that multiple seemingly unlikely\nevents, which may be neglected at design time, will not only occur, but occur\ntogether. We then argue that despite this unpredictability property, handling\nthese events and conditions is indeed possible. Hence, we offer and exemplify\ndesign principles that when applied in advance, can enable systems to deal, in\nthe future, with unpredicted circumstances. We conclude with a discussion of\nhow this work and a broader theoretical study of the unexpected can contribute\ntoward a foundation of engineering principles for developing trustworthy\nnext-generation autonomous systems.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 19:39:01 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 12:30:23 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 13:39:32 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Marron", "Assaf", ""], ["Limonad", "Lior", ""], ["Pollack", "Sarah", ""], ["Harel", "David", ""]]}, {"id": "2001.06067", "submitter": "Jinghui Cheng", "authors": "Wenting Wang, Deeksha Arya, Nicole Novielli, Jinghui Cheng, Jin L.C.\n  Guo", "title": "ArguLens: Anatomy of Community Opinions On Usability Issues Using\n  Argumentation Models", "comments": "14 pages, 7 figures, ACM CHI Full Paper (2020). For codebook, data,\n  and code for ArguLens, see https://github.com/HCDLab/ArguLens", "journal-ref": null, "doi": "10.1145/3313831.3376218", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open-source software (OSS), the design of usability is often influenced by\nthe discussions among community members on platforms such as issue tracking\nsystems (ITSs). However, digesting the rich information embedded in issue\ndiscussions can be a major challenge due to the vast number and diversity of\nthe comments. We propose and evaluate ArguLens, a conceptual framework and\nautomated technique leveraging an argumentation model to support effective\nunderstanding and consolidation of community opinions in ITSs. Through content\nanalysis, we anatomized highly discussed usability issues from a large, active\nOSS project, into their argumentation components and standpoints. We then\nexperimented with supervised machine learning techniques for automated argument\nextraction. Finally, through a study with experienced ITS users, we show that\nthe information provided by ArguLens supported the digestion of\nusability-related opinions and facilitated the review of lengthy issues.\nArguLens provides the direction of designing valuable tools for high-level\nreasoning and effective discussion about usability.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 20:50:39 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Wang", "Wenting", ""], ["Arya", "Deeksha", ""], ["Novielli", "Nicole", ""], ["Cheng", "Jinghui", ""], ["Guo", "Jin L. C.", ""]]}, {"id": "2001.06229", "submitter": "Ali Nassif", "authors": "Mariam AlAbboudi, Maitha Majed, Fatima Hassan, Ali Bou Nassif", "title": "EEG Wheelchair for People of Determination", "comments": "Paper accepted at IEH-2020 ASET Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to design and construct an electroencephalograph\n(EEG) based brain-controlled wheelchair to provide a communication bridge from\nthe nervous system to the external technical device for people of determination\nor individuals suffering from partial or complete paralysis. EEG is a technique\nthat reads the activity of the brain by capturing brain signals non-invasively\nusing a special EEG headset. The signals acquired go through pre-processing,\nfeature extraction and classification. This technique allows human thoughts\nalone to be converted to control the wheelchair. The commands used are moving\nto the right, left, forward, and backward and stop. The brain signals are\nacquired using the Emotiv Epoc headset. Discrete Wavelet Transform is used for\nfeature extraction and Support Vector Machine (SVM) is used for classification.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 10:35:43 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["AlAbboudi", "Mariam", ""], ["Majed", "Maitha", ""], ["Hassan", "Fatima", ""], ["Nassif", "Ali Bou", ""]]}, {"id": "2001.06347", "submitter": "Xuesu Xiao", "authors": "Xuesu Xiao, Jan Dufek, Robin R. Murphy", "title": "Tethered Aerial Visual Assistance", "comments": "Submitted to special issue of \"Field and Service Robotics\" of the\n  Journal of Field Robotics (JFR). arXiv admin note: text overlap with\n  arXiv:1904.00078", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an autonomous tethered Unmanned Aerial Vehicle (UAV) is\ndeveloped into a visual assistant in a marsupial co-robots team, collaborating\nwith a tele-operated Unmanned Ground Vehicle (UGV) for robot operations in\nunstructured or confined environments. These environments pose extreme\nchallenges to the remote tele-operator due to the lack of sufficient\nsituational awareness, mostly caused by the unstructuredness and confinement,\nstationary and limited field-of-view and lack of depth perception from the\nrobot's onboard cameras. To overcome these problems, a secondary tele-operated\nrobot is used in current practices, who acts as a visual assistant and provides\nexternal viewpoints to overcome the perceptual limitations of the primary\nrobot's onboard sensors. However, a second tele-operated robot requires extra\nmanpower and teamwork demand between primary and secondary operators. The\nmanually chosen viewpoints tend to be subjective and sub-optimal. Considering\nthese intricacies, we develop an autonomous tethered aerial visual assistant in\nplace of the secondary tele-operated robot and operator, to reduce human robot\nratio from 2:2 to 1:2. Using a fundamental viewpoint quality theory, a formal\nrisk reasoning framework, and a newly developed tethered motion suite, our\nvisual assistant is able to autonomously navigate to good-quality viewpoints in\na risk-aware manner through unstructured or confined spaces with a tether. The\ndeveloped marsupial co-robots team could improve tele-operation efficiency in\nnuclear operations, bomb squad, disaster robots, and other domains with novel\ntasks or highly occluded environments, by reducing manpower and teamwork\ndemand, and achieving better visual assistance quality with trustworthy\nrisk-aware motion.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 06:41:04 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Xiao", "Xuesu", ""], ["Dufek", "Jan", ""], ["Murphy", "Robin R.", ""]]}, {"id": "2001.06423", "submitter": "Arjun Srinivasan", "authors": "Arjun Srinivasan, Bongshin Lee, Nathalie Henry Riche, Steven M.\n  Drucker, Ken Hinckley", "title": "InChorus: Designing Consistent Multimodal Interactions for Data\n  Visualization on Tablet Devices", "comments": "To appear in ACM CHI 2020 Conference on Human Factors in Computing\n  Systems; 13 pages (10 content + 3 references); 4 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While tablet devices are a promising platform for data visualization,\nsupporting consistent interactions across different types of visualizations on\ntablets remains an open challenge. In this paper, we present multimodal\ninteractions that function consistently across different visualizations,\nsupporting common operations during visual data analysis. By considering\nstandard interface elements (e.g., axes, marks) and grounding our design in a\nset of core concepts including operations, parameters, targets, and\ninstruments, we systematically develop interactions applicable to different\nvisualization types. To exemplify how the proposed interactions collectively\nfacilitate data exploration, we employ them in a tablet-based system, InChorus\nthat supports pen, touch, and speech input. Based on a study with 12\nparticipants performing replication and fact-checking tasks with InChorus, we\ndiscuss how participants adapted to using multimodal input and highlight\nconsiderations for future multimodal visualization systems.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 16:46:17 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Srinivasan", "Arjun", ""], ["Lee", "Bongshin", ""], ["Riche", "Nathalie Henry", ""], ["Drucker", "Steven M.", ""], ["Hinckley", "Ken", ""]]}, {"id": "2001.06462", "submitter": "Oh-Hyun Kwon", "authors": "Joseph Kotlarek, Oh-Hyun Kwon, Kwan-Liu Ma, Peter Eades, Andreas\n  Kerren, Karsten Klein, Falk Schreiber", "title": "A Study of Mental Maps in Immersive Network Visualization", "comments": "IEEE Pacific Visualization Symposium 2020", "journal-ref": null, "doi": "10.1109/PacificVis48177.2020.4722", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visualization of a network influences the quality of the mental map that\nthe viewer develops to understand the network. In this study, we investigate\nthe effects of a 3D immersive visualization environment compared to a\ntraditional 2D desktop environment on the comprehension of a network's\nstructure. We compare the two visualization environments using three\ntasks--interpreting network structure, memorizing a set of nodes, and\nidentifying the structural changes--commonly used for evaluating the quality of\na mental map in network visualization. The results show that participants were\nable to interpret network structure more accurately when viewing the network in\nan immersive environment, particularly for larger networks. However, we found\nthat 2D visualizations performed better than immersive visualization for tasks\nthat required spatial memory.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:18:51 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Kotlarek", "Joseph", ""], ["Kwon", "Oh-Hyun", ""], ["Ma", "Kwan-Liu", ""], ["Eades", "Peter", ""], ["Kerren", "Andreas", ""], ["Klein", "Karsten", ""], ["Schreiber", "Falk", ""]]}, {"id": "2001.06463", "submitter": "Alexandros Papangelis", "authors": "Alexandros Papangelis, Mahdi Namazifar, Chandra Khatri, Yi-Chia Wang,\n  Piero Molino, Gokhan Tur", "title": "Plato Dialogue System: A Flexible Conversational AI Research Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the field of Spoken Dialogue Systems and Conversational AI grows, so does\nthe need for tools and environments that abstract away implementation details\nin order to expedite the development process, lower the barrier of entry to the\nfield, and offer a common test-bed for new ideas. In this paper, we present\nPlato, a flexible Conversational AI platform written in Python that supports\nany kind of conversational agent architecture, from standard architectures to\narchitectures with jointly-trained components, single- or multi-party\ninteractions, and offline or online training of any conversational agent\ncomponent. Plato has been designed to be easy to understand and debug and is\nagnostic to the underlying learning frameworks that train each component.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:27:29 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Papangelis", "Alexandros", ""], ["Namazifar", "Mahdi", ""], ["Khatri", "Chandra", ""], ["Wang", "Yi-Chia", ""], ["Molino", "Piero", ""], ["Tur", "Gokhan", ""]]}, {"id": "2001.06509", "submitter": "Dakuo Wang", "authors": "Jaimie Drozdal, Justin Weisz, Dakuo Wang, Gaurav Dass, Bingsheng Yao,\n  Changruo Zhao, Michael Muller, Lin Ju, Hui Su", "title": "Trust in AutoML: Exploring Information Needs for Establishing Trust in\n  Automated Machine Learning Systems", "comments": "IUI 2020", "journal-ref": null, "doi": "10.1145/3377325.3377501", "report-no": null, "categories": "cs.LG cs.CY cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore trust in a relatively new area of data science: Automated Machine\nLearning (AutoML). In AutoML, AI methods are used to generate and optimize\nmachine learning models by automatically engineering features, selecting\nmodels, and optimizing hyperparameters. In this paper, we seek to understand\nwhat kinds of information influence data scientists' trust in the models\nproduced by AutoML? We operationalize trust as a willingness to deploy a model\nproduced using automated methods. We report results from three studies --\nqualitative interviews, a controlled experiment, and a card-sorting task -- to\nunderstand the information needs of data scientists for establishing trust in\nAutoML systems. We find that including transparency features in an AutoML tool\nincreased user trust and understandability in the tool; and out of all proposed\nfeatures, model performance metrics and visualizations are the most important\ninformation to data scientists when establishing their trust with an AutoML\ntool.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 19:50:54 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Drozdal", "Jaimie", ""], ["Weisz", "Justin", ""], ["Wang", "Dakuo", ""], ["Dass", "Gaurav", ""], ["Yao", "Bingsheng", ""], ["Zhao", "Changruo", ""], ["Muller", "Michael", ""], ["Ju", "Lin", ""], ["Su", "Hui", ""]]}, {"id": "2001.06578", "submitter": "Francisco Ortega Ph.D.", "authors": "Catherine Angelini, Adam S. Williams, Mathew Kress, Edgar Ramos\n  Vieira, Newton D'Souza, Naphtali D. Rishe, Joseph Medina, Francisco R. Ortega", "title": "City Planning with Augmented Reality", "comments": "This was accepted by Graphics Interface 2019 (GI '2019) HCI track as\n  a poster paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an early study designed to analyze how city planning and the\nhealth of senior citizens can benefit from the use of augmented reality (AR)\nusing Microsoft's HoloLens. We also explore whether AR and VR can be used to\nhelp city planners receive real-time feedback from citizens, such as the\nelderly, on virtual plans, allowing for informed decisions to be made before\nany construction begins.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 02:22:45 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Angelini", "Catherine", ""], ["Williams", "Adam S.", ""], ["Kress", "Mathew", ""], ["Vieira", "Edgar Ramos", ""], ["D'Souza", "Newton", ""], ["Rishe", "Naphtali D.", ""], ["Medina", "Joseph", ""], ["Ortega", "Francisco R.", ""]]}, {"id": "2001.06579", "submitter": "Francisco Ortega Ph.D.", "authors": "Drew Johnston, Jarret Flack, Indrakshi Ray, Francisco R. Ortega", "title": "Towards a Virtual Reality Home IoT Network Visualizer", "comments": "This paper was accepted at the Graphics Interface 2019 (GI' 19) HCI\n  Track for a poster paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an IoT home network visualizer that utilizes virtual reality (VR).\nThis prototype demonstrates the potential that VR has to aid in the\nunderstanding of home IoT networks. This is particularly important due the\nincreased number of household devices now connected to the Internet. This\nprototype is able to function in a standard display or a VR headset. A\nprototype was developed to aid in the understanding of home IoT networks for\nhomeowners.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 02:31:30 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Johnston", "Drew", ""], ["Flack", "Jarret", ""], ["Ray", "Indrakshi", ""], ["Ortega", "Francisco R.", ""]]}, {"id": "2001.06684", "submitter": "Dakuo Wang", "authors": "Amy X. Zhang, Michael Muller, Dakuo Wang", "title": "How do Data Science Workers Collaborate? Roles, Workflows, and Tools", "comments": "CSCW'2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, the prominence of data science within organizations has given rise to\nteams of data science workers collaborating on extracting insights from data,\nas opposed to individual data scientists working alone. However, we still lack\na deep understanding of how data science workers collaborate in practice. In\nthis work, we conducted an online survey with 183 participants who work in\nvarious aspects of data science. We focused on their reported interactions with\neach other (e.g., managers with engineers) and with different tools (e.g.,\nJupyter Notebook). We found that data science teams are extremely collaborative\nand work with a variety of stakeholders and tools during the six common steps\nof a data science workflow (e.g., clean data and train model). We also found\nthat the collaborative practices workers employ, such as documentation, vary\naccording to the kinds of tools they use. Based on these findings, we discuss\ndesign implications for supporting data science team collaborations and future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 15:11:56 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 08:38:00 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 16:38:43 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Zhang", "Amy X.", ""], ["Muller", "Michael", ""], ["Wang", "Dakuo", ""]]}, {"id": "2001.06737", "submitter": "Anahita Sanandaji", "authors": "Anahita Sanandaji, Cindy Grimm, Ruth West, Christopher Sanchez", "title": "Developing and Validating an Interactive Training Tool for Inferring 2D\n  Cross-Sections of Complex 3D Structures", "comments": "17 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding 2D cross-sections of 3D structures is a crucial skill in many\ndisciplines, from geology to medical imaging. Cross-section inference in the\ncontext of 3D structures requires a complex set of spatial/visualization skills\nincluding mental rotation, spatial structure understanding, and viewpoint\nprojection. Prior studies show that experts differ from novices in these, and\nother, skill dimensions. Building on a previously developed model that\nhierarchically characterizes the specific spatial sub-skills needed for this\ntask, we have developed the first domain-agnostic, computer-based training tool\nfor cross-section understanding of complex 3D structures. We demonstrate, in an\nevaluation with 60 participants, that this interactive tool is effective for\nincreasing cross-section inference skills for a variety of structures, from\nsimple primitive ones to more complex biological structures.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 23:34:41 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Sanandaji", "Anahita", ""], ["Grimm", "Cindy", ""], ["West", "Ruth", ""], ["Sanchez", "Christopher", ""]]}, {"id": "2001.06765", "submitter": "Amit Kumar Jaiswal", "authors": "Amit Kumar Jaiswal, Haiming Liu, Ingo Frommholz", "title": "Information Foraging for Enhancing Implicit Feedback in Content-based\n  Image Recommendation", "comments": "FIRE '19: Proceedings of the 11th Forum for Information Retrieval\n  Evaluation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User implicit feedback plays an important role in recommender systems.\nHowever, finding implicit features is a tedious task. This paper aims to\nidentify users' preferences through implicit behavioural signals for image\nrecommendation based on the Information Scent Model of Information Foraging\nTheory. In the first part, we hypothesise that the users' perception is\nimproved with visual cues in the images as behavioural signals that provide\nusers' information scent during information seeking. We designed a\ncontent-based image recommendation system to explore which image attributes\n(i.e., visual cues or bookmarks) help users find their desired image. We found\nthat users prefer recommendations predicated by visual cues and therefore\nconsider the visual cues as good information scent for their information\nseeking. In the second part, we investigated if visual cues in the images\ntogether with the images itself can be better perceived by the users than each\nof them on its own. We evaluated the information scent artifacts in image\nrecommendation on the Pinterest image collection and the WikiArt dataset. We\nfind our proposed image recommendation system supports the implicit signals\nthrough Information Foraging explanation of the information scent model.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 03:36:33 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Jaiswal", "Amit Kumar", ""], ["Liu", "Haiming", ""], ["Frommholz", "Ingo", ""]]}, {"id": "2001.06798", "submitter": "Jonas Oppenlaender", "authors": "Jonas Oppenlaender, Kristy Milland, Aku Visuri, Panos Ipeirotis, Simo\n  Hosio", "title": "Creativity on Paid Crowdsourcing Platforms", "comments": "10 pages, 2 figures, 1 table, CHI 2020 accepted", "journal-ref": null, "doi": "10.1145/3313831.3376677", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General-purpose crowdsourcing platforms are increasingly being harnessed for\ncreative work. The platforms' potential for creative work is clearly\nidentified, but the workers' perspectives on such work have not been\nextensively documented. In this paper, we uncover what the workers have to say\nabout creative work on paid crowdsourcing platforms. Through a quantitative and\nqualitative analysis of a questionnaire launched on two different crowdsourcing\nplatforms, our results revealed clear differences between the workers on the\nplatforms in both preferences and prior experience with creative work. We\nidentify common pitfalls with creative work on crowdsourcing platforms, provide\nrecommendations for requesters of creative work, and discuss the meaning of our\nfindings within the broader scope of creativity-oriented research. To the best\nof our knowledge, we contribute the first extensive worker-oriented study of\ncreative work on paid crowdsourcing platforms.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 09:55:12 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Oppenlaender", "Jonas", ""], ["Milland", "Kristy", ""], ["Visuri", "Aku", ""], ["Ipeirotis", "Panos", ""], ["Hosio", "Simo", ""]]}, {"id": "2001.07142", "submitter": "Diogo Rato", "authors": "Diogo Rato, Samuel Mascarenhas, and Rui Prada", "title": "Towards Social Identity in Socio-Cognitive Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current architectures for social agents are designed around some specific\nunits of social behaviour that address particular challenges. Although their\nperformance might be adequate for controlled environments, deploying these\nagents in the wild is difficult. Moreover, the increasing demand for autonomous\nagents capable of living alongside humans calls for the design of more robust\nsocial agents that can cope with diverse social situations. We believe that to\ndesign such agents, their sociality and cognition should be conceived as one.\nThis includes creating mechanisms for constructing social reality as an\ninterpretation of the physical world with social meanings and selective\ndeployment of cognitive resources adequate to the situation. We identify\nseveral design principles that should be considered while designing agent\narchitectures for socio-cognitive systems. Taking these remarks into account,\nwe propose a socio-cognitive agent model based on the concept of Cognitive\nSocial Frames that allow the adaptation of an agent's cognition based on its\ninterpretation of its surroundings, its Social Context. Our approach supports\nan agent's reasoning about other social actors and its relationship with them.\nCognitive Social Frames can be built around social groups, and form the basis\nfor social group dynamics mechanisms and construct of Social Identity.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 15:27:26 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Rato", "Diogo", ""], ["Mascarenhas", "Samuel", ""], ["Prada", "Rui", ""]]}, {"id": "2001.07416", "submitter": "Kaixuan Chen", "authors": "Kaixuan Chen, Dalin Zhang, Lina Yao, Bin Guo, Zhiwen Yu, Yunhao Liu", "title": "Deep Learning for Sensor-based Human Activity Recognition: Overview,\n  Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast proliferation of sensor devices and Internet of Things enables the\napplications of sensor-based activity recognition. However, there exist\nsubstantial challenges that could influence the performance of the recognition\nsystem in practical scenarios. Recently, as deep learning has demonstrated its\neffectiveness in many areas, plenty of deep methods have been investigated to\naddress the challenges in activity recognition. In this study, we present a\nsurvey of the state-of-the-art deep learning methods for sensor-based human\nactivity recognition. We first introduce the multi-modality of the sensory data\nand provide information for public datasets that can be used for evaluation in\ndifferent challenge tasks. We then propose a new taxonomy to structure the deep\nmethods by challenges. Challenges and challenge-related deep methods are\nsummarized and analyzed to form an overview of the current research progress.\nAt the end of this work, we discuss the open issues and provide some insights\nfor future directions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 09:55:59 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 14:27:27 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Chen", "Kaixuan", ""], ["Zhang", "Dalin", ""], ["Yao", "Lina", ""], ["Guo", "Bin", ""], ["Yu", "Zhiwen", ""], ["Liu", "Yunhao", ""]]}, {"id": "2001.07455", "submitter": "Martin Lindvall", "authors": "Martin Lindvall and Jesper Molin", "title": "Designing for the Long Tail of Machine Learning", "comments": "Accepted for presentation in poster format for the ACM CHI'19\n  Workshop <Emerging Perspectives in Human-Centered Machine Learning>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technical advances has made machine learning (ML) a promising\ncomponent to include in end user facing systems. However, user experience (UX)\npractitioners face challenges in relating ML to existing user-centered design\nprocesses and how to navigate the possibilities and constraints of this design\nspace. Drawing on our own experience, we characterize designing within this\nspace as navigating trade-offs between data gathering, model development and\ndesigning valuable interactions for a given model performance. We suggest that\nthe theoretical description of how machine learning performance scales with\ntraining data can guide designers in these trade-offs as well as having\nimplications for prototyping. We exemplify the learning curve's usage by\narguing that a useful pattern is to design an initial system in a bootstrap\nphase that aims to exploit the training effect of data collected at increasing\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 11:53:28 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Lindvall", "Martin", ""], ["Molin", "Jesper", ""]]}, {"id": "2001.07501", "submitter": "Wen Wang", "authors": "Wen Wang, Xiaojiang Peng, Yu Qiao, Jian Cheng", "title": "A Comprehensive Study on Temporal Modeling for Online Action Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online action detection (OAD) is a practical yet challenging task, which has\nattracted increasing attention in recent years. A typical OAD system mainly\nconsists of three modules: a frame-level feature extractor which is usually\nbased on pre-trained deep Convolutional Neural Networks (CNNs), a temporal\nmodeling module, and an action classifier. Among them, the temporal modeling\nmodule is crucial which aggregates discriminative information from historical\nand current features. Though many temporal modeling methods have been developed\nfor OAD and other topics, their effects are lack of investigation on OAD\nfairly. This paper aims to provide a comprehensive study on temporal modeling\nfor OAD including four meta types of temporal modeling methods, \\ie temporal\npooling, temporal convolution, recurrent neural networks, and temporal\nattention, and uncover some good practices to produce a state-of-the-art OAD\nsystem. Many of them are explored in OAD for the first time, and extensively\nevaluated with various hyper parameters. Furthermore, based on our\ncomprehensive study, we present several hybrid temporal modeling methods, which\noutperform the recent state-of-the-art methods with sizable margins on\nTHUMOS-14 and TVSeries.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 13:12:58 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Wen", ""], ["Peng", "Xiaojiang", ""], ["Qiao", "Yu", ""], ["Cheng", "Jian", ""]]}, {"id": "2001.07546", "submitter": "Mira Park Dr.", "authors": "Yiming Zhong, Yuan Tian, Mira Park, Soonja Yeom", "title": "Exploring an Application of Virtual Reality for Early Detection of\n  Dementia", "comments": "11 pages, 4 tables, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facing the severe global dementia problem, an exploration was conducted\nadopting the technology of virtual reality (VR). This report lays a technical\nfoundation for further research project \"Early Detection of Dementia Using\nTesting Tools in VR Environment\", which illustrates the process of developing a\nVR application using Unity 3D software on Oculus Go. This preliminary\nexploration is composed of three steps, including 3D virtual scene\nconstruction, VR interaction design and monitoring. The exploration was\nrecorded to provide basic technical guidance and detailed method for subsequent\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 03:27:36 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Zhong", "Yiming", ""], ["Tian", "Yuan", ""], ["Park", "Mira", ""], ["Yeom", "Soonja", ""]]}, {"id": "2001.07549", "submitter": "Paul Rosen", "authors": "Zachariah Beasley and Alon Friedman and Les Piegl and Paul Rosen", "title": "Leveraging Peer Feedback to Improve Visualization Education", "comments": null, "journal-ref": "2020 IEEE Pacific Visualization Symposium (PacificVis)", "doi": "10.1109/PacificVis48177.2020.1261", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peer review is a widely utilized pedagogical feedback mechanism for engaging\nstudents, which has been shown to improve educational outcomes. However, we\nfind limited discussion and empirical measurement of peer review in\nvisualization coursework. In addition to engagement, peer review provides\ndirect and diverse feedback and reinforces recently-learned course concepts\nthrough critical evaluation of others' work. In this paper, we discuss the\nconstruction and application of peer review in a computer science visualization\ncourse, including: projects that reuse code and visualizations in a\nfeedback-guided, continual improvement process and a peer review rubric to\nreinforce key course concepts. To measure the effectiveness of the approach, we\nevaluate student projects, peer review text, and a post-course questionnaire\nfrom 3 semesters of mixed undergraduate and graduate courses. The results\nindicate that course concepts are reinforced with peer review---82% reported\nlearning more because of peer review, and 75% of students recommended\ncontinuing it. Finally, we provide a road-map for adapting peer review to other\nvisualization courses to produce more highly engaged students.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 21:46:58 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 16:04:30 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Beasley", "Zachariah", ""], ["Friedman", "Alon", ""], ["Piegl", "Les", ""], ["Rosen", "Paul", ""]]}, {"id": "2001.07630", "submitter": "Irawan Nurhas", "authors": "Irawan Nurhas, Bayu Rima Aditya, Stefan Geisler, Jan Pawlowski", "title": "Why Does Cultural Diversity Foster Technology-enabled Intergenerational\n  Collaboration?", "comments": "8 Pages, 5th Information System International Conference (ISICO)", "journal-ref": "Procedia Computer Science, 2020", "doi": "10.1016/j.procs.2019.11.094", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Globalization and information technology enable people to join the movement\nof global citizenship and work without borders. However, different type of\nbarriers existed that could affect collaboration in todays work environment, in\nwhich different generations are involved. Although researchers have identified\nseveral technical barriers to intergenerational collaboration (iGOAL), the\ninfluence of cultural diversity on iGOAL has rarely been studied. Therefore,\nusing a quantitative study approach, this paper investigates the impact of\ndifferences in cultural background on perceived technical and operational\nbarriers to iGOAL. Our study reveals six barriers to IGC that are perceived\ndifferently by culturally diverse people (CDP) and non-CDP. Furthermore, CDP\ncan foster IGC because CDP consider the barriers to be of less of a reason to\navoid working with different generations than do non-CDP.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 16:21:13 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Nurhas", "Irawan", ""], ["Aditya", "Bayu Rima", ""], ["Geisler", "Stefan", ""], ["Pawlowski", "Jan", ""]]}, {"id": "2001.07876", "submitter": "Xingbo Wang", "authors": "Xingbo Wang, Haipeng Zeng, Yong Wang, Aoyu Wu, Zhida Sun, Xiaojuan Ma,\n  Huamin Qu", "title": "VoiceCoach: Interactive Evidence-based Training for Voice Modulation\n  Skills in Public Speaking", "comments": "Accepted by CHI '20", "journal-ref": "Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3313831.3376726", "report-no": null, "categories": "cs.HC cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modulation of voice properties, such as pitch, volume, and speed, is\ncrucial for delivering a successful public speech. However, it is challenging\nto master different voice modulation skills. Though many guidelines are\navailable, they are often not practical enough to be applied in different\npublic speaking situations, especially for novice speakers. We present\nVoiceCoach, an interactive evidence-based approach to facilitate the effective\ntraining of voice modulation skills. Specifically, we have analyzed the voice\nmodulation skills from 2623 high-quality speeches (i.e., TED Talks) and use\nthem as the benchmark dataset. Given a voice input, VoiceCoach automatically\nrecommends good voice modulation examples from the dataset based on the\nsimilarity of both sentence structures and voice modulation skills. Immediate\nand quantitative visual feedback is provided to guide further improvement. The\nexpert interviews and the user study provide support for the effectiveness and\nusability of VoiceCoach.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 04:52:06 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wang", "Xingbo", ""], ["Zeng", "Haipeng", ""], ["Wang", "Yong", ""], ["Wu", "Aoyu", ""], ["Sun", "Zhida", ""], ["Ma", "Xiaojuan", ""], ["Qu", "Huamin", ""]]}, {"id": "2001.07944", "submitter": "Luke Storry", "authors": "Luke Storry", "title": "augKlimb: Interactive Data-Led Augmentation of Bouldering Training", "comments": "Written as a MEng Masters thesis at the University of Bristol.\n  Received an award for the highest mark of the year. (49 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climbing is a popular and growing sport, especially indoors, where climbers\ncan train on man-made routes using artificial holds. Both strength and good\ntechnique is required to successfully reach the top of a climb, and often\ncoaches work to improve technique so less strength is required, enabling a\nclimber to ascent more difficult climbs.\n  Various aspects of adding computer-interaction to climbing have been studied\nin recent years, but there is a large space for research into lightweight tools\nto aid recreational intermediate climbers, both with trickier climbs and to\nimprove their own technique.\n  In this CS Masters final project, I explored which form of data-capture and\noutput-features could improve a climber's training, and analysed how climbers\nresponded to viewing their data throughout a climbing session, then conducted a\nuser-centred design to build a lightweight mobile application for intermediate\nclimbers. A variety of hardware and software solutions were explored, tested\nand developed through series of surveys, discussions, wizard-of-oz studies and\nprototyping, resulting in a system that most closely meets the needs of local\nindoor boulderers given the project's time scope.\n  This consists of an iteratively developed interactive mobile app that: can\nrecord, graph, and score the acceleration of a climber, as both a training tool\nand gamification incentive for good technique; can link a video recording to\nthe acceleration graph, to enable frame-by-frame inspection of weaknesses; is\nfully approved and distributed on the Google play Store and currently being\nregularly used by 15 local climbers. Then I conducted a final usability study,\ncomprising a thematic analysis of forty minutes's worth of interview\ntranscripts, to gain a deep understanding of the app's impact on the climbers\nusing it, along with its benefits and limitations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 10:26:59 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Storry", "Luke", ""]]}, {"id": "2001.08047", "submitter": "Nicholas Santavas Mr", "authors": "Nicholas Santavas, Ioannis Kansizoglou, Loukas Bampis, Evangelos\n  Karakasis and Antonios Gasteratos", "title": "Attention! A Lightweight 2D Hand Pose Estimation Approach", "comments": "updated version with ablation studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Vision based human pose estimation is an non-invasive technology for\nHuman-Computer Interaction (HCI). Direct use of the hand as an input device\nprovides an attractive interaction method, with no need for specialized sensing\nequipment, such as exoskeletons, gloves etc, but a camera. Traditionally, HCI\nis employed in various applications spreading in areas including manufacturing,\nsurgery, entertainment industry and architecture, to mention a few. Deployment\nof vision based human pose estimation algorithms can give a breath of\ninnovation to these applications. In this letter, we present a novel\nConvolutional Neural Network architecture, reinforced with a Self-Attention\nmodule that it can be deployed on an embedded system, due to its lightweight\nnature, with just 1.9 Million parameters. The source code and qualitative\nresults are publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 15:05:00 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 01:24:50 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Santavas", "Nicholas", ""], ["Kansizoglou", "Ioannis", ""], ["Bampis", "Loukas", ""], ["Karakasis", "Evangelos", ""], ["Gasteratos", "Antonios", ""]]}, {"id": "2001.08166", "submitter": "Alwin Hoffmann", "authors": "Michael Filipenko and Andreas Angerer and Alwin Hoffmann and Wolfgang\n  Reif", "title": "Opportunities and Limitations of Mixed Reality Holograms in Industrial\n  Robotics", "comments": "Workshop on \"Factory of the Future - How to digitalize the\n  robot-aided manufacturing process in Industry 4.0?\"; 2019 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems, Macao", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces two case studies combining the field of industrial\nrobotics with Mixed Reality (MR). The goal of those case studies is to get a\nbetter understanding of how MR can be useful and what are the limitations. The\nfirst case study describes an approach to visualize the digital twin of a robot\narm. The second case study aims at facilitating the commissioning of industrial\nrobots. Furthermore, this paper reports the experiences gained by implementing\nthose two scenarios and discusses the limitations.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 17:30:45 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Filipenko", "Michael", ""], ["Angerer", "Andreas", ""], ["Hoffmann", "Alwin", ""], ["Reif", "Wolfgang", ""]]}, {"id": "2001.08194", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Jun Kato, Koji Yatani", "title": "ClassCode: An Interactive Teaching and Learning Environment for\n  Programming Education in Classrooms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programming education is becoming important as demands on computer literacy\nand coding skills are growing. Despite the increasing popularity of interactive\nonline learning systems, many programming courses in schools have not changed\ntheir teaching format from the conventional classroom setting. We see two\nresearch opportunities here. Students may have diverse expertise and experience\nin programming. Thus, particular content and teaching speed can be disengaging\nfor experienced students or discouraging for novice learners. In a large\nclassroom, instructors cannot oversee the learning progress of each student,\nand have difficulty matching teaching materials with the comprehension level of\nindividual students. We present ClassCode, a web-based environment tailored to\nprogramming education in classrooms. Students can take online tutorials\nprepared by instructors at their own pace. They can then deepen their\nunderstandings by performing interactive coding exercises interleaved within\ntutorials. ClassCode tracks all interactions by each student, and summarizes\nthem to instructors. This serves as a progress report, facilitating the\ninstructors to provide additional explanations in-situ or revise course\nmaterials. Our user evaluation through a small lecture and expert review by\ninstructors and teaching assistants confirm the potential of ClassCode by\nuncovering how it could address issues in existing programming courses at\nuniversities.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 18:28:16 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Suzuki", "Ryo", ""], ["Kato", "Jun", ""], ["Yatani", "Koji", ""]]}, {"id": "2001.08298", "submitter": "Zana Bu\\c{c}inca", "authors": "Zana Bu\\c{c}inca, Phoebe Lin, Krzysztof Z. Gajos, Elena L. Glassman", "title": "Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating\n  Explainable AI Systems", "comments": null, "journal-ref": null, "doi": "10.1145/3377325.3377498", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable artificially intelligent (XAI) systems form part of\nsociotechnical systems, e.g., human+AI teams tasked with making decisions. Yet,\ncurrent XAI systems are rarely evaluated by measuring the performance of\nhuman+AI teams on actual decision-making tasks. We conducted two online\nexperiments and one in-person think-aloud study to evaluate two currently\ncommon techniques for evaluating XAI systems: (1) using proxy, artificial tasks\nsuch as how well humans predict the AI's decision from the given explanations,\nand (2) using subjective measures of trust and preference as predictors of\nactual performance. The results of our experiments demonstrate that evaluations\nwith proxy tasks did not predict the results of the evaluations with the actual\ndecision-making tasks. Further, the subjective measures on evaluations with\nactual decision-making tasks did not predict the objective performance on those\nsame tasks. Our results suggest that by employing misleading evaluation\nmethods, our field may be inadvertently slowing its progress toward developing\nhuman+AI teams that can reliably perform better than humans or AIs alone.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 22:14:28 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Bu\u00e7inca", "Zana", ""], ["Lin", "Phoebe", ""], ["Gajos", "Krzysztof Z.", ""], ["Glassman", "Elena L.", ""]]}, {"id": "2001.08301", "submitter": "Malin Eiband", "authors": "Malin Eiband, Daniel Buschek, Heinrich Hussmann", "title": "How to Support Users in Understanding Intelligent Systems? Structuring\n  the Discussion", "comments": "19 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The opaque nature of many intelligent systems violates established usability\nprinciples and thus presents a challenge for human-computer interaction.\nResearch in the field therefore highlights the need for transparency,\nscrutability, intelligibility, interpretability and explainability, among\nothers. While all of these terms carry a vision of supporting users in\nunderstanding intelligent systems, the underlying notions and assumptions about\nusers and their interaction with the system often remain unclear. We review the\nliterature in HCI through the lens of implied user questions to synthesise a\nconceptual framework integrating user mindsets, user involvement, and knowledge\noutcomes to reveal, differentiate and classify current notions in prior work.\nThis framework aims to resolve conceptual ambiguity in the field and enables\nresearchers to clarify their assumptions and become aware of those made in\nprior work. We thus hope to advance and structure the dialogue in the HCI\nresearch community on supporting users in understanding intelligent systems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 22:32:37 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 14:08:50 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 16:46:05 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2020 16:16:06 GMT"}, {"version": "v5", "created": "Thu, 18 Feb 2021 11:37:03 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Eiband", "Malin", ""], ["Buschek", "Daniel", ""], ["Hussmann", "Heinrich", ""]]}, {"id": "2001.08328", "submitter": "Yuwei Tu", "authors": "Yuwei Tu, Weiyu Chen, Christopher G. Brinton", "title": "A Deep Learning Approach to Behavior-Based Learner Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of e-learning has created demand for improving\nonline education through techniques such as predictive analytics and content\nrecommendations. In this paper, we study learner outcome predictions, i.e.,\npredictions of how they will perform at the end of a course. We propose a novel\nTwo Branch Decision Network for performance prediction that incorporates two\nimportant factors: how learners progress through the course and how the content\nprogresses through the course. We combine clickstream features which log every\naction the learner takes while learning, and textual features which are\ngenerated through pre-trained GloVe word embeddings. To assess the performance\nof our proposed network, we collect data from a short online course designed\nfor corporate training and evaluate both neural network and non-neural network\nbased algorithms on it. Our proposed algorithm achieves 95.7% accuracy and\n0.958 AUC score, which outperforms all other models. The results also indicate\nthe combination of behavior features and text features are more predictive than\nbehavior features only and neural network models are powerful in capturing the\njoint relationship between user behavior and course content.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 01:26:52 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Tu", "Yuwei", ""], ["Chen", "Weiyu", ""], ["Brinton", "Christopher G.", ""]]}, {"id": "2001.08379", "submitter": "Chuan Wang", "authors": "Chuan Wang, Xumeng Wang, Kwan-Liu Ma", "title": "Visual Summary of Value-level Feature Attribution in Prediction Classes\n  with Recurrent Neural Networks", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Recurrent Neural Networks (RNN) is increasingly used in decision-making\nwith temporal sequences. However, understanding how RNN models produce final\npredictions remains a major challenge. Existing work on interpreting RNN models\nfor sequence predictions often focuses on explaining predictions for individual\ndata instances (e.g., patients or students). Because state-of-the-art\npredictive models are formed with millions of parameters optimized over\nmillions of instances, explaining predictions for single data instances can\neasily miss a bigger picture. Besides, many outperforming RNN models use\nmulti-hot encoding to represent the presence/absence of features, where the\ninterpretability of feature value attribution is missing. We present ViSFA, an\ninteractive system that visually summarizes feature attribution over time for\ndifferent feature values. ViSFA scales to large data such as the MIMIC dataset\ncontaining the electronic health records of 1.2 million high-dimensional\ntemporal events. We demonstrate that ViSFA can help us reason RNN prediction\nand uncover insights from data by distilling complex attribution into compact\nand easy-to-interpret visualizations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 05:38:30 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 19:22:38 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Wang", "Chuan", ""], ["Wang", "Xumeng", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2001.08444", "submitter": "Jon Vadillo Jueguen", "authors": "Jon Vadillo and Roberto Santana", "title": "On the human evaluation of audio adversarial examples", "comments": "Preprint. 17 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG cs.SD stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Human-machine interaction is increasingly dependent on speech communication.\nMachine Learning models are usually applied to interpret human speech commands.\nHowever, these models can be fooled by adversarial examples, which are inputs\nintentionally perturbed to produce a wrong prediction without being noticed.\nWhile much research has been focused on developing new techniques to generate\nadversarial perturbations, less attention has been given to aspects that\ndetermine whether and how the perturbations are noticed by humans. This\nquestion is relevant since high fooling rates of proposed adversarial\nperturbation strategies are only valuable if the perturbations are not\ndetectable. In this paper we investigate to which extent the distortion metrics\nproposed in the literature for audio adversarial examples, and which are\ncommonly applied to evaluate the effectiveness of methods for generating these\nattacks, are a reliable measure of the human perception of the perturbations.\nUsing an analytical framework, and an experiment in which 18 subjects evaluate\naudio adversarial examples, we demonstrate that the metrics employed by\nconvention are not a reliable measure of the perceptual similarity of\nadversarial examples in the audio domain.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 10:56:50 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 14:27:20 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Vadillo", "Jon", ""], ["Santana", "Roberto", ""]]}, {"id": "2001.08578", "submitter": "Mohammed Abuhamad", "authors": "Mohammed Abuhamad, Ahmed Abusnaina, DaeHun Nyang, and David Mohaisen", "title": "Sensor-based Continuous Authentication of Smartphones' Users Using\n  Behavioral Biometrics: A Contemporary Survey", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices and technologies have become increasingly popular, offering\ncomparable storage and computational capabilities to desktop computers allowing\nusers to store and interact with sensitive and private information. The\nsecurity and protection of such personal information are becoming more and more\nimportant since mobile devices are vulnerable to unauthorized access or theft.\nUser authentication is a task of paramount importance that grants access to\nlegitimate users at the point-of-entry and continuously through the usage\nsession. This task is made possible with today's smartphones' embedded sensors\nthat enable continuous and implicit user authentication by capturing behavioral\nbiometrics and traits. In this paper, we survey more than 140 recent behavioral\nbiometric-based approaches for continuous user authentication, including\nmotion-based methods (28 studies), gait-based methods (19 studies), keystroke\ndynamics-based methods (20 studies), touch gesture-based methods (29 studies),\nvoice-based methods (16 studies), and multimodal-based methods (34 studies).\nThe survey provides an overview of the current state-of-the-art approaches for\ncontinuous user authentication using behavioral biometrics captured by\nsmartphones' embedded sensors, including insights and open challenges for\nadoption, usability, and performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 15:07:28 GMT"}, {"version": "v2", "created": "Sun, 10 May 2020 17:31:29 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Abuhamad", "Mohammed", ""], ["Abusnaina", "Ahmed", ""], ["Nyang", "DaeHun", ""], ["Mohaisen", "David", ""]]}, {"id": "2001.08656", "submitter": "David Melhart", "authors": "David Melhart, Georgios N. Yannakakis, Antonios Liapis", "title": "I Feel I Feel You: A Theory of Mind Experiment in Games", "comments": "Accepted manuscript for the KI-Kunstliche Intelligenz special issue\n  on Artificial Intelligence in Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study into the player's emotional theory of mind of gameplaying\nagents, we investigate how an agent's behaviour and the player's own\nperformance and emotions shape the recognition of a frustrated behaviour. We\nfocus on the perception of frustration as it is a prevalent affective\nexperience in human-computer interaction. We present a testbed game tailored\ntowards this end, in which a player competes against an agent with a\nfrustration model based on theory. We collect gameplay data, an annotated\nground truth about the player's appraisal of the agent's frustration, and apply\nface recognition to estimate the player's emotional state. We examine the\ncollected data through correlation analysis and predictive machine learning\nmodels, and find that the player's observable emotions are not correlated\nhighly with the perceived frustration of the agent. This suggests that our\nsubject's theory of mind is a cognitive process based on the gameplay context.\nOur predictive models---using ranking support vector machines---corroborate\nthese results, yielding moderately accurate predictors of players' theory of\nmind.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 16:49:39 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Melhart", "David", ""], ["Yannakakis", "Georgios N.", ""], ["Liapis", "Antonios", ""]]}, {"id": "2001.08703", "submitter": "Guangliang Li", "authors": "Guangliang Li, Hamdi Dibeklio\\u{g}lu, Shimon Whiteson and Hayley Hung", "title": "Facial Feedback for Reinforcement Learning: A Case Study and Offline\n  Analysis Using the TAMER Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive reinforcement learning provides a way for agents to learn to\nsolve tasks from evaluative feedback provided by a human user. Previous\nresearch showed that humans give copious feedback early in training but very\nsparsely thereafter. In this article, we investigate the potential of agent\nlearning from trainers' facial expressions via interpreting them as evaluative\nfeedback. To do so, we implemented TAMER which is a popular interactive\nreinforcement learning method in a reinforcement-learning benchmark problem ---\nInfinite Mario, and conducted the first large-scale study of TAMER involving\n561 participants. With designed CNN-RNN model, our analysis shows that telling\ntrainers to use facial expressions and competition can improve the accuracies\nfor estimating positive and negative feedback using facial expressions. In\naddition, our results with a simulation experiment show that learning solely\nfrom predicted feedback based on facial expressions is possible and using\nstrong/effective prediction models or a regression method, facial responses\nwould significantly improve the performance of agents. Furthermore, our\nexperiment supports previous studies demonstrating the importance of\nbi-directional feedback and competitive elements in the training interface.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 17:50:57 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Li", "Guangliang", ""], ["Dibeklio\u011flu", "Hamdi", ""], ["Whiteson", "Shimon", ""], ["Hung", "Hayley", ""]]}, {"id": "2001.08791", "submitter": "Brian Quanz", "authors": "Brian Quanz, Wei Sun, Ajay Deshpande, Dhruv Shah, Jae-eun Park", "title": "Machine learning based co-creative design framework", "comments": "Thirty-third Conference on Neural Information Processing Systems\n  (NeurIPS) 2019 Workshop on Machine Learning for Creativity and Design,\n  December 14th, 2019, Vancouver, Canada\n  (https://neurips2019creativity.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible, co-creative framework bringing together multiple\nmachine learning techniques to assist human users to efficiently produce\neffective creative designs. We demonstrate its potential with a perfume bottle\ndesign case study, including human evaluation and quantitative and qualitative\nanalyses.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 20:18:44 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Quanz", "Brian", ""], ["Sun", "Wei", ""], ["Deshpande", "Ajay", ""], ["Shah", "Dhruv", ""], ["Park", "Jae-eun", ""]]}, {"id": "2001.08927", "submitter": "Langtao Chen", "authors": "Langtao Chen", "title": "The Impact of Content Commenting on User Continuance in Online Q&A\n  Communities: An Affordance Perspective", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online question-and-answer (Q&A) communities provide convenient and\ninnovative ways for participants to share information and collaboratively solve\nproblems with others. A growing challenge for those Q&A communities is to\nencourage and maintain ongoing user participation. From the perspective of\nmotivational affordances, this study proposes a research framework to explain\nthe effect of content commenting on user continuance behavior in online Q&A\ncommunities. The moderating role of participant's tenure in the relationship\nbetween content commenting and user continuance is also explored. Using a\nlongitudinal panel dataset collected from a large online Q&A community, this\nresearch empirically tests the effect of content commenting on continued user\nparticipation in the Q&A community. The results show that both comment receipt\nand comment provisioning are important motivating factors for user continuance\nin the community. Specifically, received comments on questions submitted by a\nparticipant have a positive effect on the participant's continuance of posting\nquestions, while answer comments both received and posted by a participant have\npositive impact on user continuance of posting answers in the community. In\naddition, tenure in the community is indeed found to have a significant\nnegative moderating effect on the relationship between content commenting and\nuser continuance. This research not only offers a more nuanced theoretical\nunderstanding of how content commenting affects continued user involvement and\nhow participants' tenure in the community moderates the impact of content\ncommenting, but also provides implications for improving user continuance in\nonline Q&A communities.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 09:31:49 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Chen", "Langtao", ""]]}, {"id": "2001.09077", "submitter": "William Seymour", "authors": "William Seymour, Martin J. Kraemer, Reuben Binns, Max Van Kleek", "title": "Informing the Design of Privacy-Empowering Tools for the Connected Home", "comments": "10 pages, 2 figures. To appear in the Proceedings of the 2020 CHI\n  Conference on Human Factors in Computing Systems (CHI '20)", "journal-ref": null, "doi": "10.1145/3313831.3376264", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected devices in the home represent a potentially grave new privacy\nthreat due to their unfettered access to the most personal spaces in people's\nlives. Prior work has shown that despite concerns about such devices, people\noften lack sufficient awareness, understanding, or means of taking effective\naction. To explore the potential for new tools that support such needs directly\nwe developed Aretha, a privacy assistant technology probe that combines a\nnetwork disaggregator, personal tutor, and firewall, to empower end-users with\nboth the knowledge and mechanisms to control disclosures from their homes. We\ndeployed Aretha in three households over six weeks, with the aim of\nunderstanding how this combination of capabilities might enable users to gain\nawareness of data disclosures by their devices, form educated privacy\npreferences, and to block unwanted data flows. The probe, with its novel\naffordances-and its limitations-prompted users to co-adapt, finding new control\nmechanisms and suggesting new approaches to address the challenge of regaining\nprivacy in the connected home.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 16:30:52 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Seymour", "William", ""], ["Kraemer", "Martin J.", ""], ["Binns", "Reuben", ""], ["Van Kleek", "Max", ""]]}, {"id": "2001.09134", "submitter": "Rajesh Kumar", "authors": "Shivam Rustagi and Aakash Garg and Pranay Raj Anand and Rajesh Kumar\n  and Yaman Kumar and Rajiv Ratn Shah", "title": "Touchless Typing Using Head Movement-based Gestures", "comments": "*The two lead authors contributed equally. More details are available\n  at https://sites.google.com/iiitd.ac.in/touchless-typing/home", "journal-ref": "The Sixth IEEE International Conference on Multimedia Big Data,\n  August 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel touchless typing interface that makes use\nof an on-screen QWERTY keyboard and a smartphone camera. The keyboard was\ndivided into nine color-coded clusters. The user moved their head toward\nclusters, which contained the letters that they wanted to type. A front-facing\nsmartphone camera recorded the head movements. A bidirectional GRU based model\nwhich used pre-trained embedding rich in head pose features was employed to\ntranslate the recordings into cluster sequences. The model achieved an accuracy\nof 96.78% and 86.81% under intra- and inter-user scenarios, respectively, over\na dataset of 2234 video sequences collected from 22 users.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 18:32:08 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 06:00:39 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Rustagi", "Shivam", ""], ["Garg", "Aakash", ""], ["Anand", "Pranay Raj", ""], ["Kumar", "Rajesh", ""], ["Kumar", "Yaman", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2001.09219", "submitter": "Q.Vera Liao", "authors": "Bhavya Ghai, Q. Vera Liao, Yunfeng Zhang, Rachel Bellamy, Klaus\n  Mueller", "title": "Explainable Active Learning (XAL): An Empirical Study of How Local\n  Explanations Impact Annotator Experience", "comments": "replacing with a draft accepted to CSCW2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide adoption of Machine Learning technologies has created a rapidly\ngrowing demand for people who can train ML models. Some advocated the term\n\"machine teacher\" to refer to the role of people who inject domain knowledge\ninto ML models. One promising learning paradigm is Active Learning (AL), by\nwhich the model intelligently selects instances to query the machine teacher\nfor labels. However, in current AL settings, the human-AI interface remains\nminimal and opaque. We begin considering AI explanations as a core element of\nthe human-AI interface for teaching machines. When a human student learns, it\nis a common pattern to present one's own reasoning and solicit feedback from\nthe teacher. When a ML model learns and still makes mistakes, the human teacher\nshould be able to understand the reasoning underlying the mistakes. When the\nmodel matures, the machine teacher should be able to recognize its progress in\norder to trust and feel confident about their teaching outcome. Toward this\nvision, we propose a novel paradigm of explainable active learning (XAL), by\nintroducing techniques from the recently surging field of explainable AI (XAI)\ninto an AL setting. We conducted an empirical study comparing the model\nlearning outcomes, feedback content and experience with XAL, to that of\ntraditional AL and coactive learning (providing the model's prediction without\nthe explanation). Our study shows benefits of AI explanation as interfaces for\nmachine teaching--supporting trust calibration and enabling rich forms of\nteaching feedback, and potential drawbacks--anchoring effect with the model\njudgment and cognitive workload. Our study also reveals important individual\nfactors that mediate a machine teacher's reception to AI explanations,\nincluding task knowledge, AI experience and need for cognition. By reflecting\non the results, we suggest future directions and design implications for XAL.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 22:52:18 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 00:18:11 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 16:47:14 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 12:43:28 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Ghai", "Bhavya", ""], ["Liao", "Q. Vera", ""], ["Zhang", "Yunfeng", ""], ["Bellamy", "Rachel", ""], ["Mueller", "Klaus", ""]]}, {"id": "2001.09326", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Patrik Jonell, Sanne van Waveren, Gustav Eje Henter,\n  Simon Alexanderson, Iolanda Leite, Hedvig Kjellstr\\\"om", "title": "Gesticulator: A framework for semantically-aware speech-driven gesture\n  generation", "comments": "ICMI 2020 Best Paper Award. Code is available. 9 pages, 6 figures", "journal-ref": "Proceedings of the 2020 International Conference on Multimodal\n  Interaction (ICMI '20)", "doi": "10.1145/3382507.3418815", "report-no": null, "categories": "cs.HC cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During speech, people spontaneously gesticulate, which plays a key role in\nconveying information. Similarly, realistic co-speech gestures are crucial to\nenable natural and smooth interactions with social agents. Current end-to-end\nco-speech gesture generation systems use a single modality for representing\nspeech: either audio or text. These systems are therefore confined to producing\neither acoustically-linked beat gestures or semantically-linked gesticulation\n(e.g., raising a hand when saying \"high\"): they cannot appropriately learn to\ngenerate both gesture types. We present a model designed to produce arbitrary\nbeat and semantic gestures together. Our deep-learning based model takes both\nacoustic and semantic representations of speech as input, and generates\ngestures as a sequence of joint angle rotations as output. The resulting\ngestures can be applied to both virtual agents and humanoid robots. Subjective\nand objective evaluations confirm the success of our approach. The code and\nvideo are available at the project page\nhttps://svito-zar.github.io/gesticulator .\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 14:42:23 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 12:27:15 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 07:44:47 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 11:13:33 GMT"}, {"version": "v5", "created": "Thu, 14 Jan 2021 16:29:20 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kucherenko", "Taras", ""], ["Jonell", "Patrik", ""], ["van Waveren", "Sanne", ""], ["Henter", "Gustav Eje", ""], ["Alexanderson", "Simon", ""], ["Leite", "Iolanda", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2001.09388", "submitter": "Ning Yu", "authors": "Ning Yu, Zachary Tuttle, Carl Jake Thurnau, Emmanuel Mireku", "title": "AI-Powered GUI Attack and Its Defensive Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the first Graphical User Interface (GUI) prototype was invented in the\n1970s, GUI systems have been deployed into various personal computer systems\nand server platforms. Recently, with the development of artificial intelligence\n(AI) technology, malicious malware powered by AI is emerging as a potential\nthreat to GUI systems. This type of AI-based cybersecurity attack, targeting at\nGUI systems, is explored in this paper. It is twofold: (1) A malware is\ndesigned to attack the existing GUI system by using AI-based object recognition\ntechniques. (2) Its defensive methods are discovered by generating adversarial\nexamples and other methods to alleviate the threats from the intelligent GUI\nattack. The results have shown that a generic GUI attack can be implemented and\nperformed in a simple way based on current AI techniques and its\ncountermeasures are temporary but effective to mitigate the threats of GUI\nattack so far.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 02:33:52 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Yu", "Ning", ""], ["Tuttle", "Zachary", ""], ["Thurnau", "Carl Jake", ""], ["Mireku", "Emmanuel", ""]]}, {"id": "2001.09395", "submitter": "Shixia Liu", "authors": "Kelei Cao, Mengchen Liu, Hang Su, Jing Wu, Jun Zhu, Shixia Liu", "title": "Analyzing the Noise Robustness of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial examples, generated by adding small but intentionally\nimperceptible perturbations to normal examples, can mislead deep neural\nnetworks (DNNs) to make incorrect predictions. Although much work has been done\non both adversarial attack and defense, a fine-grained understanding of\nadversarial examples is still lacking. To address this issue, we present a\nvisual analysis method to explain why adversarial examples are misclassified.\nThe key is to compare and analyze the datapaths of both the adversarial and\nnormal examples. A datapath is a group of critical neurons along with their\nconnections. We formulate the datapath extraction as a subset selection problem\nand solve it by constructing and training a neural network. A multi-level\nvisualization consisting of a network-level visualization of data flows, a\nlayer-level visualization of feature maps, and a neuron-level visualization of\nlearned features, has been designed to help investigate how datapaths of\nadversarial and normal examples diverge and merge in the prediction process. A\nquantitative evaluation and a case study were conducted to demonstrate the\npromise of our method to explain the misclassification of adversarial examples.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 03:39:10 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Cao", "Kelei", ""], ["Liu", "Mengchen", ""], ["Su", "Hang", ""], ["Wu", "Jing", ""], ["Zhu", "Jun", ""], ["Liu", "Shixia", ""]]}, {"id": "2001.09399", "submitter": "Suraj Padmanaban Kesavan", "authors": "Suraj P. Kesavan, Takanori Fujiwara, Jianping Kelvin Li, Caitlin Ross,\n  Misbah Mubarak, Christopher D. Carothers, Robert B. Ross, Kwan-Liu Ma", "title": "A Visual Analytics Framework for Reviewing Streaming Performance Data", "comments": "This is the author's preprint version that will be published in\n  Proceedings of IEEE Pacific Visualization Symposium, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and tuning the performance of extreme-scale parallel computing\nsystems demands a streaming approach due to the computational cost of applying\noffline algorithms to vast amounts of performance log data. Analyzing large\nstreaming data is challenging because the rate of receiving data and limited\ntime to comprehend data make it difficult for the analysts to sufficiently\nexamine the data without missing important changes or patterns. To support\nstreaming data analysis, we introduce a visual analytic framework comprising of\nthree modules: data management, analysis, and interactive visualization. The\ndata management module collects various computing and communication performance\nmetrics from the monitored system using streaming data processing techniques\nand feeds the data to the other two modules. The analysis module automatically\nidentifies important changes and patterns at the required latency. In\nparticular, we introduce a set of online and progressive analysis methods for\nnot only controlling the computational costs but also helping analysts better\nfollow the critical aspects of the analysis results. Finally, the interactive\nvisualization module provides the analysts with a coherent view of the changes\nand patterns in the continuously captured performance data. Through a\nmulti-faceted case study on performance analysis of parallel discrete-event\nsimulation, we demonstrate the effectiveness of our framework for identifying\nbottlenecks and locating outliers.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 04:34:22 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Kesavan", "Suraj P.", ""], ["Fujiwara", "Takanori", ""], ["Li", "Jianping Kelvin", ""], ["Ross", "Caitlin", ""], ["Mubarak", "Misbah", ""], ["Carothers", "Christopher D.", ""], ["Ross", "Robert B.", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2001.09455", "submitter": "Michael Ekstrand", "authors": "Mucun Tian, Michael D. Ekstrand", "title": "Estimating Error and Bias in Offline Evaluation Results", "comments": "Published in CHIIR 2020", "journal-ref": null, "doi": "10.1145/3343413.3378004", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline evaluations of recommender systems attempt to estimate users'\nsatisfaction with recommendations using static data from prior user\ninteractions. These evaluations provide researchers and developers with first\napproximations of the likely performance of a new system and help weed out bad\nideas before presenting them to users. However, offline evaluation cannot\naccurately assess novel, relevant recommendations, because the most novel items\nwere previously unknown to the user, so they are missing from the historical\ndata and cannot be judged as relevant.\n  We present a simulation study to estimate the error that such missing data\ncauses in commonly-used evaluation metrics in order to assess its prevalence\nand impact. We find that missing data in the rating or observation process\ncauses the evaluation protocol to systematically mis-estimate metric values,\nand in some cases erroneously determine that a popularity-based recommender\noutperforms even a perfect personalized recommender. Substantial breakthroughs\nin recommendation quality, therefore, will be difficult to assess with existing\noffline techniques.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 14:00:56 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Tian", "Mucun", ""], ["Ekstrand", "Michael D.", ""]]}, {"id": "2001.09551", "submitter": "Joseph Bakarji", "authors": "Joseph Bakarji", "title": "Machine Learning for a Music Glove Instrument", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A music glove instrument equipped with force sensitive, flex and IMU sensors\nis trained on an electric piano to learn note sequences based on a time series\nof sensor inputs. Once trained, the glove is used on any surface to generate\nthe sequence of notes most closely related to the hand motion. The data is\ncollected manually by a performer wearing the glove and playing on an electric\nkeyboard. The feature space is designed to account for the key hand motion,\nsuch as the thumb-under movement. Logistic regression along with bayesian\nbelief networks are used learn the transition probabilities from one note to\nanother. This work demonstrates a data-driven approach for digital musical\ninstruments in general.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 01:08:11 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Bakarji", "Joseph", ""]]}, {"id": "2001.09604", "submitter": "Ruotong Wang", "authors": "Ruotong Wang, F. Maxwell Harper and Haiyi Zhu", "title": "Factors Influencing Perceived Fairness in Algorithmic Decision-Making:\n  Algorithm Outcomes, Development Procedures, and Individual Differences", "comments": "To appear at CHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic decision-making systems are increasingly used throughout the\npublic and private sectors to make important decisions or assist humans in\nmaking these decisions with real social consequences. While there has been\nsubstantial research in recent years to build fair decision-making algorithms,\nthere has been less research seeking to understand the factors that affect\npeople's perceptions of fairness in these systems, which we argue is also\nimportant for their broader acceptance. In this research, we conduct an online\nexperiment to better understand perceptions of fairness, focusing on three sets\nof factors: algorithm outcomes, algorithm development and deployment\nprocedures, and individual differences. We find that people rate the algorithm\nas more fair when the algorithm predicts in their favor, even surpassing the\nnegative effects of describing algorithms that are very biased against\nparticular demographic groups. We find that this effect is moderated by several\nvariables, including participants' education level, gender, and several aspects\nof the development procedure. Our findings suggest that systems that evaluate\nalgorithmic fairness through users' feedback must consider the possibility of\noutcome favorability bias.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 06:51:53 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Wang", "Ruotong", ""], ["Harper", "F. Maxwell", ""], ["Zhu", "Haiyi", ""]]}, {"id": "2001.09746", "submitter": "Nuno Henriques", "authors": "Nuno A. C. Henriques, Helder Coelho, Leonel Garcia-Marques", "title": "SensAI+Expanse Emotional Valence Prediction Studies with Cognition and\n  Memory Integration", "comments": "Accepted as regular paper in COGNITIVE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The humans are affective and cognitive beings relying on memories for their\nindividual and social identities. Also, human dyadic bonds require some common\nbeliefs such as empathetic behaviour for better interaction. In this sense,\nresearch studies involving human-agent interaction should resource on affect,\ncognition, and memory integration. The developed artificial agent system\n(SensAI+Expanse) includes machine learning algorithms, heuristics, and memory\nas cognition aids towards emotional valence prediction on the interacting\nhuman. Further, an adaptive empathy score is always present in order to engage\nthe human in a recognisable interaction outcome. [...] The agent is resilient\non collecting data, adapts its cognitive processes to each human individual in\na learning best effort for proper contextualised prediction. The current study\nmake use of an achieved adaptive process. Also, the use of individual\nprediction models with specific options of the learning algorithm and\nevaluation metric from a previous research study. The accomplished solution\nincludes a highly performant prediction ability, an efficient energy use, and\nfeature importance explanation for predicted probabilities. Results of the\npresent study show evidence of significant emotional valence behaviour\ndifferences between some age ranges and gender combinations. Therefore, this\nwork contributes with an artificial intelligent agent able to assist on\ncognitive science studies. This ability is about affective disturbances by\nmeans of predicting human emotional valence contextualised in space and time.\nMoreover, contributes with learning processes and heuristics fit to the task\nincluding economy of cognition and memory to cope with the environment.\nFinally, these contributions include an achieved age and gender neutrality on\npredicting emotional valence states in context and with very good performance\nfor each individual.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 18:17:57 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 16:27:19 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2020 15:11:24 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Henriques", "Nuno A. C.", ""], ["Coelho", "Helder", ""], ["Garcia-Marques", "Leonel", ""]]}, {"id": "2001.09766", "submitter": "Lok Chan", "authors": "Lok Chan, Kenzie Doyle, Duncan McElfresh, Vincent Conitzer, John P.\n  Dickerson, Jana Schaich Borg, Walter Sinnott-Armstrong", "title": "Artificial Artificial Intelligence: Measuring Influence of AI\n  'Assessments' on Moral Decision-Making", "comments": null, "journal-ref": "Proceedings of the 2020 AAAI/ACM Conference on AI, Ethics, and\n  Society (AIES '20)", "doi": "10.1145/3375627.3375870", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given AI's growing role in modeling and improving decision-making, how and\nwhen to present users with feedback is an urgent topic to address. We\nempirically examined the effect of feedback from false AI on moral\ndecision-making about donor kidney allocation. We found some evidence that\njudgments about whether a patient should receive a kidney can be influenced by\nfeedback about participants' own decision-making perceived to be given by AI,\neven if the feedback is entirely random. We also discovered different effects\nbetween assessments presented as being from human experts and assessments\npresented as being from AI.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 14:15:18 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Chan", "Lok", ""], ["Doyle", "Kenzie", ""], ["McElfresh", "Duncan", ""], ["Conitzer", "Vincent", ""], ["Dickerson", "John P.", ""], ["Borg", "Jana Schaich", ""], ["Sinnott-Armstrong", "Walter", ""]]}, {"id": "2001.09830", "submitter": "Manikandan Ravikiran", "authors": "Manikandan Ravikiran", "title": "What's happened in MOOC Posts Analysis, Knowledge Tracing and Peer\n  Feedbacks? A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning Management Systems (LMS) and Educational Data Mining (EDM) are two\nimportant parts of online educational environment with the former being a\ncentralised web-based information systems where the learning content is managed\nand learning activities are organised (Stone and Zheng,2014) and latter\nfocusing on using data mining techniques for the analysis of data so generated.\nAs part of this work, we present a literature review of three major tasks of\nEDM (See section 2), by identifying shortcomings and existing open problems,\nand a Blumenfield chart (See section 3). The consolidated set of papers and\nresources so used are released in\nhttps://github.com/manikandan-ravikiran/cs6460-Survey. The coverage statistics\nand review matrix of the survey are as shown in Figure 1 & Table 1\nrespectively. Acronym expansions are added in the Appendix Section 4.1.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 14:45:55 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ravikiran", "Manikandan", ""]]}, {"id": "2001.09925", "submitter": "Zachary Eberhart", "authors": "Zachary Eberhart, Aakash Bansal, Collin McMillan", "title": "The Apiza Corpus: API Usage Dialogues with a Simulated Virtual Assistant", "comments": "Preview of an upcoming TSE submission. 3 Pages + references. The\n  Apiza corpus and relevant data can be found at\n  https://github.com/ApizaCorpus/ApizaCorpus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual assistant technology has the potential to make a significant impact\nin the field of software engineering. However, few SE-related datasets exist\nthat would be suitable for the design or training of a virtual assistant. To\nhelp lay the groundwork for a hypothetical virtual assistant for API usage, we\ndesigned and conducted a Wizard-of-Oz study to gather this crucial data. We\nhired 30 professional programmers to complete a series of programming tasks by\ninteracting with a simulated virtual assistant. Unbeknownst to the programmers,\nthe virtual assistant was actually operated by another human expert. In this\nreport, we describe our experimental methodology and summarize the results of\nthe study.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 17:30:23 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Eberhart", "Zachary", ""], ["Bansal", "Aakash", ""], ["McMillan", "Collin", ""]]}, {"id": "2001.09963", "submitter": "Vinoth Pandian Sermuga Pandian", "authors": "Vinoth Pandian Sermuga Pandian and Sarah Suleri", "title": "NASA-TLX Web App: An Online Tool to Analyse Subjective Workload", "comments": "4 pages including references with 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  NASA Task Load Index (NASA-TLX) is a widely used assessment technique to\ncompute subjective workload experienced during a task. It evaluates workload\nusing six dimensions: mental demand, physical demand, temporal demand,\nfrustration, effort, and performance. This paper presents a web app to assist\nexperimenters in using NASA-TLX to commute subjective workload. The web app\nenables the experimenter to conduct various experiments simultaneously and\noffers the participants a concise interface to provide their subjective\nevaluation. It performs the calculations at the backend and provides the\ncomputed results comprehensively. The web app provides a dashboard for the\nexperimenter to visualize and export the summary of results. Qualitative\nfeedback from 12 experimenters indicated that the NASA-TLX web app is relevant,\nhelpful, and easy to use.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 18:34:09 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Pandian", "Vinoth Pandian Sermuga", ""], ["Suleri", "Sarah", ""]]}, {"id": "2001.10223", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Javier\n  Ortega-Garcia", "title": "BioTouchPass2: Touchscreen Password Biometrics Using Time-Aligned\n  Recurrent Neural Networks", "comments": null, "journal-ref": "IEEE Transactions on Information Forensics and Security, 2020", "doi": "10.1109/TIFS.2020.2973832", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Passwords are still used on a daily basis for all kind of applications.\nHowever, they are not secure enough by themselves in many cases. This work\nenhances password scenarios through two-factor authentication asking the users\nto draw each character of the password instead of typing them as usual. The\nmain contributions of this study are as follows: i) We present the novel\nMobileTouchDB public database, acquired in an unsupervised mobile scenario with\nno restrictions in terms of position, posture, and devices. This database\ncontains more than 64K on-line character samples performed by 217 users, with\n94 different smartphone models, and up to 6 acquisition sessions. ii) We\nperform a complete analysis of the proposed approach considering both\ntraditional authentication systems such as Dynamic Time Warping (DTW) and novel\napproaches based on Recurrent Neural Networks (RNNs). In addition, we present a\nnovel approach named Time-Aligned Recurrent Neural Networks (TA-RNNs). This\napproach combines the potential of DTW and RNNs to train more robust systems\nagainst attacks.\n  A complete analysis of the proposed approach is carried out using both\nMobileTouchDB and e-BioDigitDB databases. Our proposed TA-RNN system\noutperforms the state of the art, achieving a final 2.38% Equal Error Rate,\nusing just a 4-digit password and one training sample per character. These\nresults encourage the deployment of our proposed approach in comparison with\ntraditional typed-based password systems where the attack would have 100%\nsuccess rate under the same impostor scenario.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 09:25:06 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Fierrez", "Julian", ""], ["Ortega-Garcia", "Javier", ""]]}, {"id": "2001.10284", "submitter": "Prashan Madumal", "authors": "Prashan Madumal, Tim Miller, Liz Sonenberg, Frank Vetere", "title": "Distal Explanations for Model-free Explainable Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce and evaluate a distal explanation model for\nmodel-free reinforcement learning agents that can generate explanations for\n`why' and `why not' questions. Our starting point is the observation that\ncausal models can generate opportunity chains that take the form of `A enables\nB and B causes C'. Using insights from an analysis of 240 explanations\ngenerated in a human-agent experiment, we define a distal explanation model\nthat can analyse counterfactuals and opportunity chains using decision trees\nand causal models. A recurrent neural network is employed to learn opportunity\nchains, and decision trees are used to improve the accuracy of task prediction\nand the generated counterfactuals. We computationally evaluate the model in 6\nreinforcement learning benchmarks using different reinforcement learning\nalgorithms. From a study with 90 human participants, we show that our distal\nexplanation model results in improved outcomes over three scenarios compared\nwith two baseline explanation models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 11:57:38 GMT"}, {"version": "v2", "created": "Sat, 12 Sep 2020 09:46:38 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Madumal", "Prashan", ""], ["Miller", "Tim", ""], ["Sonenberg", "Liz", ""], ["Vetere", "Frank", ""]]}, {"id": "2001.10464", "submitter": "Vignesh Prasad", "authors": "Ruth Stock-Homburg, Jan Peters, Katharina Schneider, Vignesh Prasad,\n  Lejla Nukovic", "title": "Evaluation of the Handshake Turing Test for anthropomorphic Robots", "comments": "Accepted as a Late Breaking Report in The 15th Annual ACM/IEEE\n  International Conference on Human Robot Interaction (HRI) 2020", "journal-ref": null, "doi": "10.1145/3371382.3378260", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Handshakes are fundamental and common greeting and parting gestures among\nhumans. They are important in shaping first impressions as people tend to\nassociate character traits with a person's handshake. To widen the social\nacceptability of robots and make a lasting first impression, a good handshaking\nability is an important skill for social robots. Therefore, to test the\nhuman-likeness of a robot handshake, we propose an initial Turing-like test,\nprimarily for the hardware interface to future AI agents. We evaluate the test\non an android robot's hand to determine if it can pass for a human hand. This\nis an important aspect of Turing tests for motor intelligence where humans have\nto interact with a physical device rather than a virtual one. We also propose\nsome modifications to the definition of a Turing test for such scenarios taking\ninto account that a human needs to interact with a physical medium.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 16:52:42 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Stock-Homburg", "Ruth", ""], ["Peters", "Jan", ""], ["Schneider", "Katharina", ""], ["Prasad", "Vignesh", ""], ["Nukovic", "Lejla", ""]]}, {"id": "2001.10608", "submitter": "Noah Apthorpe", "authors": "Noah Apthorpe, Pardis Emami-Naeini, Arunesh Mathur, Marshini Chetty,\n  Nick Feamster", "title": "You, Me, and IoT: How Internet-Connected Consumer Devices Affect\n  Interpersonal Relationships", "comments": "26 pages, 5 figures, 5 tables. Updated version with additional\n  examples and minor revisions. Original title: \"You, Me, and IoT: How\n  Internet-Connected Home Devices Affect Interpersonal Relationships\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-connected consumer devices have rapidly increased in popularity;\nhowever, relatively little is known about how these technologies are affecting\ninterpersonal relationships in multi-occupant households. In this study, we\nconduct 13 semi-structured interviews and survey 508 individuals from a variety\nof backgrounds to discover and categorize how consumer IoT devices are\naffecting interpersonal relationships in the United States. We highlight\nseveral themes, providing large-scale exploratory data about the pervasiveness\nof interpersonal costs and benefits of consumer IoT devices. These results also\ninform follow-up studies and design priorities for future IoT technologies to\namplify positive and reduce negative interpersonal effects.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 22:03:56 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 19:27:54 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Apthorpe", "Noah", ""], ["Emami-Naeini", "Pardis", ""], ["Mathur", "Arunesh", ""], ["Chetty", "Marshini", ""], ["Feamster", "Nick", ""]]}, {"id": "2001.10617", "submitter": "Manikandan Ravikiran", "authors": "Manikandan Ravikiran", "title": "Systematic Review of Approaches to Improve Peer Assessment at Scale", "comments": "This is a review assignment, work on progress. Expected to be updated\n  regularly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Peer Assessment is a task of analysis and commenting on student's writing by\npeers, is core of all educational components both in campus and in MOOC's.\nHowever, with the sheer scale of MOOC's & its inherent personalised open ended\nlearning, automatic grading and tools assisting grading at scale is highly\nimportant. Previously we presented survey on tasks of post classification,\nknowledge tracing and ended with brief review on Peer Assessment (PA), with\nsome initial problems. In this review we shall continue review on PA from\nperspective of improving the review process itself. As such rest of this review\nfocus on three facets of PA namely Auto grading and Peer Assessment Tools (we\nshall look only on how peer reviews/auto-grading is carried), strategies to\nhandle Rogue Reviews, Peer Review Improvement using Natural Language\nProcessing. The consolidated set of papers and resources so used are released\nin https://github.com/manikandan-ravikiran/cs6460-Survey-2.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 15:59:24 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Ravikiran", "Manikandan", ""]]}, {"id": "2001.10685", "submitter": "Joseph Bullock", "authors": "Tomaz Logar, Joseph Bullock, Edoardo Nemni, Lars Bromley, John A.\n  Quinn, Miguel Luengo-Oroz", "title": "PulseSatellite: A tool using human-AI feedback loops for satellite image\n  analysis in humanitarian contexts", "comments": "2 pages, 2 figures", "journal-ref": "Proceedings of the AAAI Conference on Artificial Intelligence, New\n  York, United States, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanitarian response to natural disasters and conflicts can be assisted by\nsatellite image analysis. In a humanitarian context, very specific satellite\nimage analysis tasks must be done accurately and in a timely manner to provide\noperational support. We present PulseSatellite, a collaborative satellite image\nanalysis tool which leverages neural network models that can be retrained\non-the fly and adapted to specific humanitarian contexts and geographies. We\npresent two case studies, in mapping shelters and floods respectively, that\nillustrate the capabilities of PulseSatellite.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 04:09:51 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Logar", "Tomaz", ""], ["Bullock", "Joseph", ""], ["Nemni", "Edoardo", ""], ["Bromley", "Lars", ""], ["Quinn", "John A.", ""], ["Luengo-Oroz", "Miguel", ""]]}, {"id": "2001.10898", "submitter": "Donghan Hu", "authors": "Donghan Hu, Sang Won Lee", "title": "ScreenTrack: Using a Visual History of a Computer Screen to Retrieve\n  Documents and Web Pages", "comments": "CHI 2020, 10 pages, 7 figures", "journal-ref": null, "doi": "10.1145/3313831.3376753", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computers are used for various purposes, so frequent context switching is\ninevitable. In this setting, retrieving the documents, files, and web pages\nthat have been used for a task can be a challenge. While modern applications\nprovide a history of recent documents for users to resume work, this is not\nsufficient to retrieve all the digital resources relevant to a given primary\ndocument. The histories currently available do not take into account the\ncomplex dependencies among resources across applications. To address this\nproblem, we tested the idea of using a visual history of a computer screen to\nretrieve digital resources within a few days of their use through the\ndevelopment of ScreenTrack. ScreenTrack is software that captures screenshots\nof a computer at regular intervals. It then generates a time-lapse video from\nthe captured screenshots and lets users retrieve a recently opened document or\nweb page from a screenshot after recognizing the resource by its appearance. A\ncontrolled user study found that participants were able to retrieve requested\ninformation more quickly with ScreenTrack than under the baseline condition\nwith existing tools. A follow-up study showed that the participants used\nScreenTrack to retrieve previously used resources and to recover the context\nfor task resumption.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 15:30:54 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 01:09:18 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Hu", "Donghan", ""], ["Lee", "Sang Won", ""]]}, {"id": "2001.11131", "submitter": "Jason R.C. Nurse Dr", "authors": "Meredydd Williams, Kelvin K. K. Yao, Jason R. C. Nurse", "title": "Developing an Augmented Reality Tourism App through User-Centred Design\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.GR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) bridges the gap between the physical and virtual\nworld. Through overlaying graphics on natural environments, users can immerse\nthemselves in a tailored environment. This offers great benefits to mobile\ntourism, where points of interest (POIs) can be annotated on a smartphone\nscreen. While a variety of apps currently exist, usability issues can\ndiscourage users from embracing AR. Interfaces can become cluttered with icons,\nwith POI occlusion posing further challenges. In this paper, we use\nuser-centred design (UCD) to develop an AR tourism app. We solicit requirements\nthrough a synthesis of domain analysis, tourist observation and semi-structured\ninterviews. Whereas previous user-centred work has designed mock-ups, we\niteratively develop a full Android app. This includes overhead maps and route\nnavigation, in addition to a detailed AR browser. The final product is\nevaluated by 20 users, who participate in a tourism task in a UK city. Users\nregard the system as usable and intuitive, and suggest the addition of further\ncustomisation. We finish by critically analysing the challenges of a\nuser-centred methodology.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 23:35:32 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Williams", "Meredydd", ""], ["Yao", "Kelvin K. K.", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "2001.11163", "submitter": "Wei Li", "authors": "Wei Li, Mathias Funk, Jasper Eikelboom, and Aarnout Brombacher", "title": "Visual Exploration of Movement Relatedness for Multi-species Ecology\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Advances in GPS telemetry technology have enabled analysis of animal movement\nin open areas. Ecologists today are utilizing modern analytic tools to study\nanimal behaviors from large quantity of GPS coordinates. Analytic tools with\nautomatic event extraction functionality can be used to investigate potential\ninteractions between animals by locating relevant segments in movement\ntrajectories. However, such automation can easily overlook the spatial,\ntemporal, social context as well as potential data problems. To this end, this\npaper explores the visual presentations that also clarify the spatial-temporal\ncontexts, social surroudings, as well as underlying data uncertainties of\nmulti-species animal interactions. The outcome system presents the\nproximity-based, time-varying relatedness between animal entities through\npairwise (PW) or individual-to-group (i-G) perspectives. Focusing on the\nrelational aspects, we employ both static depictions and animations to\ncommunicate the travelling of individuals. Our contributions are a novel\nvisualization system that helps investigate the subtle variations of long term\nspatial-temporal relatedness while considering small group patterns. Our\nevaluation with movement ecologists shows that the system gives them quick\naccess to valuable clues in discovering insights into multi-species movements\nand signs of potential interactions.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 03:35:20 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Li", "Wei", ""], ["Funk", "Mathias", ""], ["Eikelboom", "Jasper", ""], ["Brombacher", "Aarnout", ""]]}, {"id": "2001.11274", "submitter": "Alfonso White", "authors": "Alfonso White, Daniela M. Romano", "title": "Scalable Psychological Momentum Forecasting in Esports", "comments": "8 pages, 8 figures", "journal-ref": "Proceedings of Workshop SUM '20: State-based User Modelling, The\n  13th ACM International Conference on Web Search and Data Mining (WSDM '20),\n  2020", "doi": "10.13140/RG.2.2.21224.21769", "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world of competitive Esports and video gaming has seen and continues to\nexperience steady growth in popularity and complexity. Correspondingly, more\nresearch on the topic is being published, ranging from social network analyses\nto the benchmarking of advanced artificial intelligence systems in playing\nagainst humans. In this paper, we present ongoing work on an intelligent agent\nrecommendation engine that suggests actions to players in order to maximise\nsuccess and enjoyment, both in the space of in-game choices, as well as\ndecisions made around play session timing in the broader context. By leveraging\ntemporal data and appropriate models, we show that a learned representation of\nplayer psychological momentum, and of tilt, can be used, in combination with\nplayer expertise, to achieve state-of-the-art performance in pre- and\npost-draft win prediction. Our progress toward fulfilling the potential for\nderiving optimal recommendations is documented.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 11:57:40 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 01:16:19 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["White", "Alfonso", ""], ["Romano", "Daniela M.", ""]]}, {"id": "2001.11337", "submitter": "Zehong Cao Prof.", "authors": "Xiaotong Gu, Zehong Cao, Alireza Jolfaei, Peng Xu, Dongrui Wu,\n  Tzyy-Ping Jung, Chin-Teng Lin", "title": "EEG-based Brain-Computer Interfaces (BCIs): A Survey of Recent Studies\n  on Signal Sensing Technologies and Computational Intelligence Approaches and\n  their Applications", "comments": "Submitting to IEEE/ACM Transactions on Computational Biology and\n  Bioinformatics", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interface (BCI) is a powerful communication tool between users\nand systems, which enhances the capability of the human brain in communicating\nand interacting with the environment directly. Advances in neuroscience and\ncomputer science in the past decades have led to exciting developments in BCI,\nthereby making BCI a top interdisciplinary research area in computational\nneuroscience and intelligence. Recent technological advances such as wearable\nsensing devices, real-time data streaming, machine learning, and deep learning\napproaches have increased interest in electroencephalographic (EEG) based BCI\nfor translational and healthcare applications. Many people benefit from\nEEG-based BCIs, which facilitate continuous monitoring of fluctuations in\ncognitive states under monotonous tasks in the workplace or at home. In this\nstudy, we survey the recent literature of EEG signal sensing technologies and\ncomputational intelligence approaches in BCI applications, compensated for the\ngaps in the systematic summary of the past five years (2015-2019). In specific,\nwe first review the current status of BCI and its significant obstacles. Then,\nwe present advanced signal sensing and enhancement technologies to collect and\nclean EEG signals, respectively. Furthermore, we demonstrate state-of-art\ncomputational intelligence techniques, including interpretable fuzzy models,\ntransfer learning, deep learning, and combinations, to monitor, maintain, or\ntrack human cognitive states and operating performance in prevalent\napplications. Finally, we deliver a couple of innovative BCI-inspired\nhealthcare applications and discuss some future research directions in\nEEG-based BCIs.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 10:36:26 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Gu", "Xiaotong", ""], ["Cao", "Zehong", ""], ["Jolfaei", "Alireza", ""], ["Xu", "Peng", ""], ["Wu", "Dongrui", ""], ["Jung", "Tzyy-Ping", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "2001.11401", "submitter": "Ali Asadipour", "authors": "Ali Asadipour, Kurt Debattista, and Alan Chalmers", "title": "Visuohaptic augmented feedback for enhancing motor skills acquisition", "comments": "11 pages, 12 figures", "journal-ref": null, "doi": "10.1007/s00371-016-1275-3", "report-no": null, "categories": "cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Serious games are accepted as an effective approach to deliver augmented\nfeedback in motor (re-) learning processes. The multi-modal nature of the\nconventional computer games (e.g. audiovisual representation) plus the ability\nto interact via haptic-enabled inputs provides a more immersive experience.\nThus, particular disciplines such as medical education in which frequent hands\non rehearsals play a key role in learning core motor skills (e.g. physical\npalpations) may benefit from this technique. Challenges such as the\nimpracticality of verbalising palpation experience by tutors and ethical\nconsiderations may prevent the medical students from correctly learning core\npalpation skills. This work presents a new data glove, built from off-the-shelf\ncomponents which captures pressure sensitivity designed to provide feedback for\npalpation tasks. In this work the data glove is used to control a serious game\nadapted from the infinite runner genre to improve motor skill acquisition. A\ncomparative evaluation on usability and effectiveness of the method using\nmultimodal visualisations, as part of a larger study to enhance pressure\nsensitivity, is presented. Thirty participants divided into a game-playing\ngroup (n = 15) and a control group (n = 15) were invited to perform a simple\npalpation task. The game-playing group significantly outperformed the control\ngroup in which abstract visualisation of force was provided to the users in a\nblind-folded transfer test. The game-based training approach was positively\ndescribed by the game-playing group as enjoyable and engaging.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 15:31:52 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Asadipour", "Ali", ""], ["Debattista", "Kurt", ""], ["Chalmers", "Alan", ""]]}, {"id": "2001.11569", "submitter": "Dongrui Wu", "authors": "Xiao Zhang, Dongrui Wu, Lieyun Ding, Hanbin Luo, Chin-Teng Lin,\n  Tzyy-Ping Jung, Ricardo Chavarriaga", "title": "Tiny noise, big mistakes: Adversarial perturbations induce errors in\n  Brain-Computer Interface spellers", "comments": null, "journal-ref": "National Science Review, 2020", "doi": "10.1093/nsr/nwaa233", "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An electroencephalogram (EEG) based brain-computer interface (BCI) speller\nallows a user to input text to a computer by thought. It is particularly useful\nto severely disabled individuals, e.g., amyotrophic lateral sclerosis patients,\nwho have no other effective means of communication with another person or a\ncomputer. Most studies so far focused on making EEG-based BCI spellers faster\nand more reliable; however, few have considered their security. This study, for\nthe first time, shows that P300 and steady-state visual evoked potential BCI\nspellers are very vulnerable, i.e., they can be severely attacked by\nadversarial perturbations, which are too tiny to be noticed when added to EEG\nsignals, but can mislead the spellers to spell anything the attacker wants. The\nconsequence could range from merely user frustration to severe misdiagnosis in\nclinical applications. We hope our research can attract more attention to the\nsecurity of EEG-based BCI spellers, and more broadly, EEG-based BCIs, which has\nreceived little attention before.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 21:18:46 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 02:58:43 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 15:47:20 GMT"}, {"version": "v4", "created": "Thu, 16 Jul 2020 23:14:34 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zhang", "Xiao", ""], ["Wu", "Dongrui", ""], ["Ding", "Lieyun", ""], ["Luo", "Hanbin", ""], ["Lin", "Chin-Teng", ""], ["Jung", "Tzyy-Ping", ""], ["Chavarriaga", "Ricardo", ""]]}, {"id": "2001.11571", "submitter": "Alvaro Pastor", "authors": "Alvaro Pastor, Laia Pujol", "title": "Analysis of the Bergen-Belsen VR/AR application by means of the Virtual\n  Subjectiveness Model", "comments": "Shortcomings found worthy of its withdrawal. There is no way to\n  update it at the present time to address the issues raised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We test the usefulness of Virtual Subjectiveness (Par\\'es and Par\\'es, 2006)\nas an analytical model for AMVR projects by means of the evaluation of the\nBergen-Belsen VR/AR application. This application allows users to retrieve\ngeolocated historical data through 3D architectural reconstructions, while\nexploring the site of the former concentration camp. Having analyzed the\ncontext of development of this application, its interface, mappings and\ninteraction behaviors, and their interrelation in time, we found the Virtual\nSubjectiveness model to be an adequate paradigm to make a structured evaluation\nof the technical and conceptual levels of the chosen VR/AR application.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 21:23:31 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 13:47:13 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Pastor", "Alvaro", ""], ["Pujol", "Laia", ""]]}, {"id": "2001.11604", "submitter": "Qi Wu", "authors": "Qi Wu (1), Tyson Neuroth (1), Oleg Igouchkine (1), Konduri Aditya (2),\n  Jacqueline H. Chen (3), Kwan-Liu Ma (1) ((1) UC Davis, (2) Indian Institute\n  of Science, (3) Sandia National Laboratories)", "title": "Diva: A Declarative and Reactive Language for In-Situ Visualization", "comments": "11 pages, 5 figures, 6 listings, 1 table, to be published in LDAV\n  2020. The article has gone through 2 major revisions: Emphasized\n  contributions, features and examples. Addressed connections between DIVA and\n  FRP. In sec. 3, we fixed a design flaw and addressed it in sec. 3.3-3.4.\n  Re-designed sec. 5 with a more concrete example and benchmark results.\n  Simplified the syntax of DIVA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of adaptive workflow management for in situ visualization and\nanalysis has been a growing trend in large-scale scientific simulations.\nHowever, coordinating adaptive workflows with traditional procedural\nprogramming languages can be difficult because system flow is determined by\nunpredictable scientific phenomena, which often appear in an unknown order and\ncan evade event handling. This makes the implementation of adaptive workflows\ntedious and error-prone. Recently, reactive and declarative programming\nparadigms have been recognized as well-suited solutions to similar problems in\nother domains. However, there is a dearth of research on adapting these\napproaches to in situ visualization and analysis. With this paper, we present a\nlanguage design and runtime system for developing adaptive systems through a\ndeclarative and reactive programming paradigm. We illustrate how an adaptive\nworkflow programming system is implemented using our approach and demonstrate\nit with a use case from a combustion simulation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 23:28:20 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 19:38:21 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wu", "Qi", ""], ["Neuroth", "Tyson", ""], ["Igouchkine", "Oleg", ""], ["Aditya", "Konduri", ""], ["Chen", "Jacqueline H.", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2001.11717", "submitter": "Evgeny Tsykunov", "authors": "Evgeny Tsykunov, Ruslan Agishev, Roman Ibrahimov, Taha Moriyama, Luiza\n  Labazanova, Hiroyuki Kajimoto, and Dzmitry Tsetserukou", "title": "SwarmCloak: Landing of Two Micro-Quadrotors on Human Hands Using\n  Wearable Tactile Interface Driven by Light Intensity", "comments": "IEEE Haptics Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the human operator, it is often easier and faster to catch a small size\nquadrotor right in the midair instead of landing it on a surface. However,\ninteraction strategies for such cases have not yet been considered properly,\nespecially when more than one drone has to be landed at the same time. In this\npaper, we propose a novel interaction strategy to land multiple robots on the\nhuman hands using vibrotactile feedback. We developed a wearable tactile\ndisplay that is activated by the intensity of light emitted from an LED ring on\nthe bottom of the quadcopter. We conducted experiments, where participants were\nasked to adjust the position of the palm to land one or two\nvertically-descending drones with different landing speeds, by having only\nvisual feedback, only tactile feedback or visual-tactile feedback. We conducted\nstatistical analysis of the drone landing positions, landing pad and human head\ntrajectories. Two-way ANOVA showed a statistically significant difference\nbetween the feedback conditions. Experimental analysis proved that with an\nincreasing number of drones, tactile feedback plays a more important role in\naccurate hand positioning and operator's convenience. The most precise landing\nof one and two drones was achieved with the combination of tactile and visual\nfeedback.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 09:01:10 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Tsykunov", "Evgeny", ""], ["Agishev", "Ruslan", ""], ["Ibrahimov", "Roman", ""], ["Moriyama", "Taha", ""], ["Labazanova", "Luiza", ""], ["Kajimoto", "Hiroyuki", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "2001.11761", "submitter": "Milad Mozafari", "authors": "Milad Mozafari, Leila Reddy, Rufin VanRullen", "title": "Reconstructing Natural Scenes from fMRI Patterns using BigBiGAN", "comments": "Accepted to IEEE IJCNN2020", "journal-ref": null, "doi": "10.1109/IJCNN48605.2020.9206960", "report-no": null, "categories": "cs.CV cs.HC eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding and reconstructing images from brain imaging data is a research area\nof high interest. Recent progress in deep generative neural networks has\nintroduced new opportunities to tackle this problem. Here, we employ a recently\nproposed large-scale bi-directional generative adversarial network, called\nBigBiGAN, to decode and reconstruct natural scenes from fMRI patterns. BigBiGAN\nconverts images into a 120-dimensional latent space which encodes class and\nattribute information together, and can also reconstruct images based on their\nlatent vectors. We computed a linear mapping between fMRI data, acquired over\nimages from 150 different categories of ImageNet, and their corresponding\nBigBiGAN latent vectors. Then, we applied this mapping to the fMRI activity\npatterns obtained from 50 new test images from 50 unseen categories in order to\nretrieve their latent vectors, and reconstruct the corresponding images.\nPairwise image decoding from the predicted latent vectors was highly accurate\n(84%). Moreover, qualitative and quantitative assessments revealed that the\nresulting image reconstructions were visually plausible, successfully captured\nmany attributes of the original images, and had high perceptual similarity with\nthe original content. This method establishes a new state-of-the-art for\nfMRI-based natural image reconstruction, and can be flexibly updated to take\ninto account any future improvements in generative models of natural scene\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 10:46:59 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 12:12:57 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 20:15:46 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Mozafari", "Milad", ""], ["Reddy", "Leila", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2001.11777", "submitter": "Lionel Robert", "authors": "Lionel P. Robert, Rasha Alahmad, Connor Esterwood, Sangmi Kim,\n  Sangseok You, Qiaoning Zhang", "title": "A Review of Personality in Human Robot Interactions", "comments": "70 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality has been identified as a vital factor in understanding the\nquality of human robot interactions. Despite this the research in this area\nremains fragmented and lacks a coherent framework. This makes it difficult to\nunderstand what we know and identify what we do not. As a result our knowledge\nof personality in human robot interactions has not kept pace with the\ndeployment of robots in organizations or in our broader society. To address\nthis shortcoming, this paper reviews 83 articles and 84 separate studies to\nassess the current state of human robot personality research. This review: (1)\nhighlights major thematic research areas, (2) identifies gaps in the\nliterature, (3) derives and presents major conclusions from the literature and\n(4) offers guidance for future research.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 11:28:37 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 21:57:37 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Robert", "Lionel P.", ""], ["Alahmad", "Rasha", ""], ["Esterwood", "Connor", ""], ["Kim", "Sangmi", ""], ["You", "Sangseok", ""], ["Zhang", "Qiaoning", ""]]}, {"id": "2001.11782", "submitter": "Zhengxiong Jia", "authors": "Zhengxiong Jia and Xirong Li", "title": "iCap: Interactive Image Captioning with Predictive Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a brand new topic of interactive image captioning with\nhuman in the loop. Different from automated image captioning where a given test\nimage is the sole input in the inference stage, we have access to both the test\nimage and a sequence of (incomplete) user-input sentences in the interactive\nscenario. We formulate the problem as Visually Conditioned Sentence Completion\n(VCSC). For VCSC, we propose asynchronous bidirectional decoding for image\ncaption completion (ABD-Cap). With ABD-Cap as the core module, we build iCap, a\nweb-based interactive image captioning system capable of predicting new text\nwith respect to live input from a user. A number of experiments covering both\nautomated evaluations and real user studies show the viability of our\nproposals.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 11:33:12 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 08:36:31 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 04:15:55 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Jia", "Zhengxiong", ""], ["Li", "Xirong", ""]]}, {"id": "2001.11913", "submitter": "Mohammad Aliannejadi", "authors": "Luca Costa and Mohammad Aliannejadi and Fabio Crestani", "title": "A Tool for Conducting User Studies on Mobile Devices", "comments": "To appear in ACM CHIIR 2020, Vancouver, BC, Canada", "journal-ref": null, "doi": "10.1145/3343413.3377985", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ever-growing interest in the area of mobile information retrieval\nand the ongoing fast development of mobile devices and, as a consequence,\nmobile apps, an active research area lies in studying users' behavior and\nsearch queries users submit on mobile devices. However, many researchers\nrequire to develop an app that collects useful information from users while\nthey search on their phones or participate in a user study. In this paper, we\naim to address this need by providing a comprehensive Android app, called\nOmicron, which can be used to collect mobile query logs and perform user\nstudies on mobile devices. Omicron, at its current version, can collect users'\nmobile queries, relevant documents, sensor data as well as user activity and\ninteraction data in various study settings. Furthermore, we designed Omicron in\nsuch a way that it is conveniently extendable to conduct more specific studies\nand collect other types of sensor data. Finally, we provide a tool to monitor\nthe participants and their data both during and after the collection process.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 15:41:27 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Costa", "Luca", ""], ["Aliannejadi", "Mohammad", ""], ["Crestani", "Fabio", ""]]}]