[{"id": "1703.00037", "submitter": "Peter Darch", "authors": "Peter T. Darch", "title": "Managing the Public to Manage Data: Citizen Science and Astronomy", "comments": "16 pages, 0 figures, published in International Journal of Digital\n  Curation", "journal-ref": "International Journal of Digital Curation, 2014, 9(1), 25-40", "doi": "10.2218/ijdc.v9i1.298", "report-no": null, "categories": "astro-ph.IM cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Citizen science projects recruit members of the public as volunteers to\nprocess and produce datasets. These datasets must win the trust of the\nscientific community. The task of securing credibility involves, in part,\napplying standard scientific procedures to clean these datasets. However,\neffective management of volunteer behavior also makes a significant\ncontribution to enhancing data quality. Through a case study of Galaxy Zoo, a\ncitizen science project set up to generate datasets based on volunteer\nclassifications of galaxy morphologies, this paper explores how those involved\nin running the project manage volunteers. The paper focuses on how methods for\ncrediting volunteer contributions motivate volunteers to provide higher quality\ncontributions and to behave in a way that better corresponds to statistical\nassumptions made when combining volunteer contributions into datasets. These\nmethods have made a significant contribution to the success of the project in\nsecuring trust in these datasets, which have been well used by other\nscientists. Implications for practice are then presented for citizen science\nprojects, providing a list of considerations to guide choices regarding how to\ncredit volunteer contributions to improve the quality and trustworthiness of\ncitizen science-produced datasets.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 20:00:26 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Darch", "Peter T.", ""]]}, {"id": "1703.00050", "submitter": "Manolis Savva", "authors": "Angel X. Chang, Mihail Eric, Manolis Savva, Christopher D. Manning", "title": "SceneSeer: 3D Scene Design with Natural Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing 3D scenes is currently a creative task that requires significant\nexpertise and effort in using complex 3D design interfaces. This effortful\ndesign process starts in stark contrast to the easiness with which people can\nuse language to describe real and imaginary environments. We present SceneSeer:\nan interactive text to 3D scene generation system that allows a user to design\n3D scenes using natural language. A user provides input text from which we\nextract explicit constraints on the objects that should appear in the scene.\nGiven these explicit constraints, the system then uses a spatial knowledge base\nlearned from an existing database of 3D scenes and 3D object models to infer an\narrangement of the objects forming a natural scene matching the input\ndescription. Using textual commands the user can then iteratively refine the\ncreated scene by adding, removing, replacing, and manipulating objects. We\nevaluate the quality of 3D scenes generated by SceneSeer in a perceptual\nevaluation experiment where we compare against manually designed scenes and\nsimpler baselines for 3D scene generation. We demonstrate how the generated\nscenes can be iteratively refined through simple natural language commands.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 20:47:47 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Chang", "Angel X.", ""], ["Eric", "Mihail", ""], ["Savva", "Manolis", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1703.00061", "submitter": "Manolis Savva", "authors": "Manolis Savva, Angel X. Chang, Maneesh Agrawala", "title": "SceneSuggest: Context-driven 3D Scene Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SceneSuggest: an interactive 3D scene design system providing\ncontext-driven suggestions for 3D model retrieval and placement. Using a\npoint-and-click metaphor we specify regions in a scene in which to\nautomatically place and orient relevant 3D models. Candidate models are ranked\nusing a set of static support, position, and orientation priors learned from 3D\nscenes. We show that our suggestions enable rapid assembly of indoor scenes. We\nperform a user study comparing suggestions to manual search and selection, as\nwell as to suggestions with no automatic orientation. We find that suggestions\nreduce total modeling time by 32%, that orientation priors reduce time spent\nre-orienting objects by 27%, and that context-driven suggestions reduce the\nnumber of text queries by 50%.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 21:21:03 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Savva", "Manolis", ""], ["Chang", "Angel X.", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "1703.00099", "submitter": "Zhou Yu", "authors": "Zhou Yu, Alan W Black and Alexander I. Rudnicky", "title": "Learning Conversational Systems that Interleave Task and Non-Task\n  Content", "comments": "Dialog Systems, Reinforcement Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task-oriented dialog systems have been applied in various tasks, such as\nautomated personal assistants, customer service providers and tutors. These\nsystems work well when users have clear and explicit intentions that are\nwell-aligned to the systems' capabilities. However, they fail if users\nintentions are not explicit. To address this shortcoming, we propose a\nframework to interleave non-task content (i.e. everyday social conversation)\ninto task conversations. When the task content fails, the system can still keep\nthe user engaged with the non-task content. We trained a policy using\nreinforcement learning algorithms to promote long-turn conversation coherence\nand consistency, so that the system can have smooth transitions between task\nand non-task content. To test the effectiveness of the proposed framework, we\ndeveloped a movie promotion dialog system. Experiments with human users\nindicate that a system that interleaves social and task content achieves a\nbetter task success rate and is also rated as more engaging compared to a pure\ntask-oriented system.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 01:27:32 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Yu", "Zhou", ""], ["Black", "Alan W", ""], ["Rudnicky", "Alexander I.", ""]]}, {"id": "1703.00492", "submitter": "Yong Lu", "authors": "Shaohe Lv, Yong Lu, Mianxiong Dong, Xiaodong Wang, Yong Dou, Weihua\n  Zhuang", "title": "Qualitative Action Recognition by Wireless Radio Signals in\n  Human-Machine Systems", "comments": "12 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-machine systems required a deep understanding of human behaviors. Most\nexisting research on action recognition has focused on discriminating between\ndifferent actions, however, the quality of executing an action has received\nlittle attention thus far. In this paper, we study the quality assessment of\ndriving behaviors and present WiQ, a system to assess the quality of actions\nbased on radio signals. This system includes three key components, a deep\nneural network based learning engine to extract the quality information from\nthe changes of signal strength, a gradient based method to detect the signal\nboundary for an individual action, and an activitybased fusion policy to\nimprove the recognition performance in a noisy environment. By using the\nquality information, WiQ can differentiate a triple body status with an\naccuracy of 97%, while for identification among 15 drivers, the average\naccuracy is 88%. Our results show that, via dedicated analysis of radio\nsignals, a fine-grained action characterization can be achieved, which can\nfacilitate a large variety of applications, such as smart driving assistants.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 20:55:38 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 20:46:02 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Lv", "Shaohe", ""], ["Lu", "Yong", ""], ["Dong", "Mianxiong", ""], ["Wang", "Xiaodong", ""], ["Dou", "Yong", ""], ["Zhuang", "Weihua", ""]]}, {"id": "1703.00521", "submitter": "Andrew Reach", "authors": "Andrew McCaleb Reach and Chris North", "title": "The Signals and Systems Approach to Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Animation is ubiquitous in visualization systems, and a common technique for\ncreating these animations is the transition. In the transition approach,\nanimations are created by smoothly interpolating a visual attribute between a\nstart and end value, reaching the end value after a specified duration. This\napproach works well when each transition for an attribute is allowed to finish\nbefore the next is triggered, but performs poorly when a new transition is\ntriggered before the current transition has finished. In particular,\ninterruptions introduce velocity discontinuities, and frequent interruptions\ncan slow down the resulting animation. To solve these problems, we model the\nproblem of animation as a signal processing problem. In our technique,\nanimations are produced by transformations of signals, or functions over time.\nIn particular, an animation is produced by transforming an input signal, a\nfunction from time to target attribute value, into an output signal, a function\nfrom time to displayed attribute value. We show that well-known\nsignal-processing techniques can be applied to produce animations that are free\nfrom velocity discontinuities even when interrupted.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 21:40:16 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Reach", "Andrew McCaleb", ""], ["North", "Chris", ""]]}, {"id": "1703.00549", "submitter": "Xuhai Xu", "authors": "Xuhai Xu, Justine Cassell", "title": "Statistical Verification of Computational Rapport Model", "comments": "Incomplete project", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapport plays an important role during communication because it can help\npeople understand each other's feelings or ideas and leads to a smooth\ncommunication. Computational rapport model has been proposed based on theory in\nprevious work. But there lacks solid verification. In this paper, we apply\nstructural equation model (SEM) to the theoretical model on both dyads of\nfriend and stranger. The results indicate some unfavorable paths. Based on the\nresults and more literature, we modify the original model to integrate more\nnonverbal behaviors, including gaze and smile. Fit indices and other\nexamination show the goodness of our new models, which can give us more insight\ninto rapport management during conversation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 23:41:57 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 03:52:00 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Xu", "Xuhai", ""], ["Cassell", "Justine", ""]]}, {"id": "1703.00556", "submitter": "Risto Miikkulainen", "authors": "Risto Miikkulainen, Neil Iscoe, Aaron Shagrin, Ron Cordell, Sam\n  Nazari, Cory Schoolland, Myles Brundage, Jonathan Epstein, Randy Dean,\n  Gurmeet Lamba", "title": "Conversion Rate Optimization through Evolutionary Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversion optimization means designing a web interface so that as many users\nas possible take a desired action on it, such as register or purchase. Such\ndesign is usually done by hand, testing one change at a time through A/B\ntesting, or a limited number of combinations through multivariate testing,\nmaking it possible to evaluate only a small fraction of designs in a vast\ndesign space. This paper describes Sentient Ascend, an automatic conversion\noptimization system that uses evolutionary optimization to create effective web\ninterface designs. Ascend makes it possible to discover and utilize\ninteractions between the design elements that are difficult to identify\notherwise. Moreover, evaluation of design candidates is done in parallel\nonline, i.e. with a large number of real users interacting with the system. A\ncase study on an existing media site shows that significant improvements (i.e.\nover 43%) are possible beyond human design. Ascend can therefore be seen as an\napproach to massively multivariate conversion optimization, based on a\nmassively parallel interactive evolution.\n", "versions": [{"version": "v1", "created": "Wed, 1 Mar 2017 23:54:28 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 21:16:02 GMT"}, {"version": "v3", "created": "Mon, 27 Mar 2017 18:30:16 GMT"}, {"version": "v4", "created": "Sun, 30 Apr 2017 20:51:21 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Miikkulainen", "Risto", ""], ["Iscoe", "Neil", ""], ["Shagrin", "Aaron", ""], ["Cordell", "Ron", ""], ["Nazari", "Sam", ""], ["Schoolland", "Cory", ""], ["Brundage", "Myles", ""], ["Epstein", "Jonathan", ""], ["Dean", "Randy", ""], ["Lamba", "Gurmeet", ""]]}, {"id": "1703.00818", "submitter": "Matthew Guzdial", "authors": "Kristin Siu, Matthew Guzdial, and Mark O. Riedl", "title": "Evaluating Singleplayer and Multiplayer in Human Computation Games", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human computation games (HCGs) can provide novel solutions to intractable\ncomputational problems, help enable scientific breakthroughs, and provide\ndatasets for artificial intelligence. However, our knowledge about how to\ndesign and deploy HCGs that appeal to players and solve problems effectively is\nincomplete. We present an investigatory HCG based on Super Mario Bros. We used\nthis game in a human subjects study to investigate how different social\nconditions---singleplayer and multiplayer---and scoring\nmechanics---collaborative and competitive---affect players' subjective\nexperiences, accuracy at the task, and the completion rate. In doing so, we\ndemonstrate a novel design approach for HCGs, and discuss the benefits and\ntradeoffs of these mechanics in HCG design.\n", "versions": [{"version": "v1", "created": "Thu, 2 Mar 2017 15:01:59 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Siu", "Kristin", ""], ["Guzdial", "Matthew", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1703.01274", "submitter": "Kory W Mathewson", "authors": "Kory W. Mathewson and Patrick M. Pilarski", "title": "Actor-Critic Reinforcement Learning with Simultaneous Human Control and\n  Feedback", "comments": "10 pages, 2 pages of references, 8 figures. Under review for the 34th\n  International Conference on Machine Learning, Sydney, Australia, 2017.\n  Copyright 2017 by the authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes a first study into how different human users deliver\nsimultaneous control and feedback signals during human-robot interaction. As\npart of this work, we formalize and present a general interactive learning\nframework for online cooperation between humans and reinforcement learning\nagents. In many human-machine interaction settings, there is a growing gap\nbetween the degrees-of-freedom of complex semi-autonomous systems and the\nnumber of human control channels. Simple human control and feedback mechanisms\nare required to close this gap and allow for better collaboration between\nhumans and machines on complex tasks. To better inform the design of concurrent\ncontrol and feedback interfaces, we present experimental results from a\nhuman-robot collaborative domain wherein the human must simultaneously deliver\nboth control and feedback signals to interactively train an actor-critic\nreinforcement learning robot. We compare three experimental conditions: 1)\nhuman delivered control signals, 2) reward-shaping feedback signals, and 3)\nsimultaneous control and feedback. Our results suggest that subjects provide\nless feedback when simultaneously delivering feedback and control signals and\nthat control signal quality is not significantly diminished. Our data suggest\nthat subjects may also modify when and how they provide feedback. Through\nalgorithmic development and tuning informed by this study, we expect\nsemi-autonomous actions of robotic agents can be better shaped by human\nfeedback, allowing for seamless collaboration and improved performance in\ndifficult interactive domains.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 18:15:32 GMT"}, {"version": "v2", "created": "Wed, 15 Mar 2017 15:15:14 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Mathewson", "Kory W.", ""], ["Pilarski", "Patrick M.", ""]]}, {"id": "1703.01333", "submitter": "Steve Jan", "authors": "Steve T.K. Jan and Chun Wang and Qing Zhang and Gang Wang", "title": "Towards Monetary Incentives in Social Q&A Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community-based question answering (CQA) services are facing key challenges\nto motivate domain experts to provide timely answers. Recently, CQA services\nare exploring new incentive models to engage experts and celebrities by\nallowing them to set a price on their answers. In this paper, we perform a\ndata-driven analysis on two emerging payment-based CQA systems: Fenda (China)\nand Whale (US). By analyzing a large dataset of 220K questions (worth 1 million\nUSD collectively), we examine how monetary incentives affect different players\nin the system. We find that, while monetary incentive enables quick answers\nfrom experts, it also drives certain users to aggressively game the system for\nprofits. In addition, in this supplier-driven marketplace, users need to\nproactively adjust their price to make profits. Famous people are unwilling to\nlower their price, which in turn hurts their income and engagement over time.\nFinally, we discuss the key implications to future CQA design.\n", "versions": [{"version": "v1", "created": "Fri, 3 Mar 2017 20:36:38 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 01:48:18 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Jan", "Steve T. K.", ""], ["Wang", "Chun", ""], ["Zhang", "Qing", ""], ["Wang", "Gang", ""]]}, {"id": "1703.01377", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "Farah Bouassida, {\\L}ukasz Kidzi\\'nski, Pierre Dillenbourg", "title": "Learning styles: Literature versus machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every teacher understands that different students benefit from different\nactivities. Recent advances in data processing allow us to detect and use\nbehavioral variability for adapting to a student. This approach allows us to\noptimize learning process but does not focus on understanding it. Conversely,\nclassical findings in educational sciences allow us to understand the learner\nbut are hard to embed in a large scale adaptive system. In this study we design\nand build a framework to investigate when the two approaches coincide.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 01:46:52 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Bouassida", "Farah", ""], ["Kidzi\u0144ski", "\u0141ukasz", ""], ["Dillenbourg", "Pierre", ""]]}, {"id": "1703.01460", "submitter": "Xingjun Ma", "authors": "Xingjun Ma, Sudanthi Wijewickrema, Shuo Zhou, Yun Zhou, Zakaria\n  Mhammedi, Stephen O'Leary, James Bailey", "title": "Adversarial Generation of Real-time Feedback with Neural Networks for\n  Simulation-based Training", "comments": "Appeared in the Proceedings of the 26th International Joint\n  Conference on Artificial Intelligence (IJCAI), Melbourne, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based training (SBT) is gaining popularity as a low-cost and\nconvenient training technique in a vast range of applications. However, for a\nSBT platform to be fully utilized as an effective training tool, it is\nessential that feedback on performance is provided automatically in real-time\nduring training. It is the aim of this paper to develop an efficient and\neffective feedback generation method for the provision of real-time feedback in\nSBT. Existing methods either have low effectiveness in improving novice skills\nor suffer from low efficiency, resulting in their inability to be used in\nreal-time. In this paper, we propose a neural network based method to generate\nfeedback using the adversarial technique. The proposed method utilizes a\nbounded adversarial update to minimize a L1 regularized loss via\nback-propagation. We empirically show that the proposed method can be used to\ngenerate simple, yet effective feedback. Also, it was observed to have high\neffectiveness and efficiency when compared to existing methods, thus making it\na promising option for real-time feedback generation in SBT.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 14:24:27 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 07:54:58 GMT"}, {"version": "v3", "created": "Tue, 23 May 2017 14:16:20 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Ma", "Xingjun", ""], ["Wijewickrema", "Sudanthi", ""], ["Zhou", "Shuo", ""], ["Zhou", "Yun", ""], ["Mhammedi", "Zakaria", ""], ["O'Leary", "Stephen", ""], ["Bailey", "James", ""]]}, {"id": "1703.01500", "submitter": "Ah Reum Kang", "authors": "Ah Reum Kang, Jeremy Blackburn, Haewoon Kwak, Huy Kang Kim", "title": "I Would Not Plant Apple Trees If the World Will Be Wiped: Analyzing\n  Hundreds of Millions of Behavioral Records of Players During an MMORPG Beta\n  Test", "comments": "10 pages, 9 figures, In Proceedings of the 26th International World\n  Wide Web Conference (WWW) 2017", "journal-ref": null, "doi": "10.1145/3038912.3038914", "report-no": null, "categories": "cs.CY cs.HC cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we use player behavior during the closed beta test of the\nMMORPG ArcheAge as a proxy for an extreme situation: at the end of the closed\nbeta test, all user data is deleted, and thus, the outcome (or penalty) of\nplayers' in-game behaviors in the last few days loses its meaning. We analyzed\n270 million records of player behavior in the 4th closed beta test of ArcheAge.\nOur findings show that there are no apparent pandemic behavior changes, but\nsome outliers were more likely to exhibit anti-social behavior (e.g., player\nkilling). We also found that contrary to the reassuring adage that \"Even if I\nknew the world would go to pieces tomorrow, I would still plant my apple tree,\"\nplayers abandoned character progression, showing a drastic decrease in quest\ncompletion, leveling, and ability changes at the end of the beta test.\n", "versions": [{"version": "v1", "created": "Sat, 4 Mar 2017 17:50:04 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Kang", "Ah Reum", ""], ["Blackburn", "Jeremy", ""], ["Kwak", "Haewoon", ""], ["Kim", "Huy Kang", ""]]}, {"id": "1703.02103", "submitter": "Gourav Ganesh Shenoy", "authors": "Gourav G. Shenoy, Mangirish A. Wagle, Kay Connelly", "title": "Leveling the playing field for Visually Impaired using Transport\n  Assistant", "comments": "8 pages, 7 figures, ubiquitous computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually impaired people face numerous challenges when it comes to\ntransportation. Not only must they circumvent obstacles while navigating, but\nthey also need access to essential information related to available public\ntransport, up-to-date weather forecast, and convenient method for booking\nprivate taxis. In this paper we introduce Transport Assistant - a voice based\nassistive technology prototype, built with a goal of leveling the playing field\nfor the visually impaired to solve these problems that they face in their day\nto day life. Being voice enabled makes it seamlessly integrate into the\nenvironment, and can be invoked by saying a hotword - hello assistant. The\npaper explores this research question, followed by investigating existing\ntechnologies, explains the methodology and design, then concludes by presenting\nthe prototype and results.\n", "versions": [{"version": "v1", "created": "Mon, 6 Mar 2017 20:43:42 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Shenoy", "Gourav G.", ""], ["Wagle", "Mangirish A.", ""], ["Connelly", "Kay", ""]]}, {"id": "1703.02227", "submitter": "He Jiang", "authors": "He Jiang, Hongjing Ma, Zhilei Ren, Jingxuan Zhang, Xiaochen Li", "title": "What Makes a Good App Description?", "comments": "9 pages, 8 figures", "journal-ref": "Proceedings of the 6th Asia-Pacific Symposium on Internetware,\n  2014", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Google Play store, an introduction page is associated with every\nmobile application (app) for users to acquire its details, including\nscreenshots, description, reviews, etc. However, it remains a challenge to\nidentify what items influence users most when downloading an app. To explore\nusers' perspective, we conduct a survey to inquire about this question. The\nresults of survey suggest that the participants pay most attention to the app\ndescription which gives users a quick overview of the app. Although there exist\nsome guidelines about how to write a good app description to attract more\ndownloads, it is hard to define a high quality app description. Meanwhile,\nthere is no tool to evaluate the quality of app description. In this paper, we\nemploy the method of crowdsourcing to extract the attributes that affect the\napp descriptions' quality. First, we download some app descriptions from Google\nPlay, then invite some participants to rate their quality with the score from\none (very poor) to five (very good). The participants are also requested to\nexplain every score's reasons. By analyzing the reasons, we extract the\nattributes that the participants consider important during evaluating the\nquality of app descriptions. Finally, we train the supervised learning models\non a sample of 100 app descriptions. In our experiments, the support vector\nmachine model obtains up to 62% accuracy. In addition, we find that the\npermission, the number of paragraphs and the average number of words in one\nfeature play key roles in defining a good app description.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 06:00:05 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Jiang", "He", ""], ["Ma", "Hongjing", ""], ["Ren", "Zhilei", ""], ["Zhang", "Jingxuan", ""], ["Li", "Xiaochen", ""]]}, {"id": "1703.02256", "submitter": "Daniel Martens", "authors": "Daniel Martens and Timo Johann", "title": "On the Emotion of Users in App Reviews", "comments": "7 pages. To be presented at the Second International Workshop on\n  Emotion Awareness in Software Engineering, colocated with the 39th\n  International Conference on Software Engineering (ICSE'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  App store analysis has become an important discipline in recent software\nengineering research. It empirically studies apps using information mined from\ntheir distribution platforms. Information provided by users, such as app\nreviews, are of high interest to developers. Commercial providers such as App\nAnnie analyzing this information became an important source for companies\ndeveloping and marketing mobile apps. In this paper, we perform an exploratory\nstudy, which analyzes over seven million reviews from the Apple AppStore\nregarding their emotional sentiment. Since recent research in this field used\nsentiments to detail and refine their results, we aim to gain deeper insights\ninto the nature of sentiments in user reviews. In this study we try to evaluate\nwhether or not the emotional sentiment can be an informative feature for\nsoftware engineers, as well as pitfalls of its usage. We present our initial\nresults and discuss how they can be interpreted from the software engineering\nperspective.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 07:51:46 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Martens", "Daniel", ""], ["Johann", "Timo", ""]]}, {"id": "1703.02335", "submitter": "Dejanira Araiza-Illan", "authors": "Satragni Sarkar, Dejanira Araiza-Illan, Kerstin Eder", "title": "Effects of Faults, Experience, and Personality on Trust in a Robot\n  Co-Worker", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To design trustworthy robots, we need to understand the impact factors of\ntrust: people's attitudes, experience, and characteristics; the robot's\nphysical design, reliability, and performance; a task's specification and the\ncircumstances under which it is to be performed, e.g. at leisure or under time\npressure. As robots are used for a wide variety of tasks and applications,\nrobot designers ought to be provided with evidence and guidance, to inform\ntheir decisions to achieve safe, trustworthy and efficient human-robot\ninteractions. In this work, the impact factors of trust in a collaborative\nmanufacturing scenario are studied by conducting an experiment with a real\nrobot and participants where a physical object was assembled and then\ndisassembled. Objective and subjective measures were employed to evaluate the\ndevelopment of trust, under faulty and non-faulty robot conditions, and the\neffect of previous experience with robots, and personality traits. Our findings\nhighlight differences when compared to other, more social, scenarios with\nrobotic assistants (such as a home care assistant), in that the condition\n(faulty or not) does not have a significant impact on the human's perception of\nthe robot in terms of human-likeliness, likeability, trustworthiness, and even\ncompetence. However, personality and previous experience do have an effect on\nhow the robot is perceived by participants, even though that is relatively\nsmall.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 11:30:10 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 10:23:20 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Sarkar", "Satragni", ""], ["Araiza-Illan", "Dejanira", ""], ["Eder", "Kerstin", ""]]}, {"id": "1703.02365", "submitter": "Jeremy Frey", "authors": "J\\'er\\'emy Frey (Potioc), Renaud Gervais (Potioc), Thibault Lain\\'e\n  (Potioc), Maxime Duluc (Potioc), Hugo Germain (Potioc), St\\'ephanie Fleck\n  (PERSEUS, UL), Fabien Lotte (Potioc), Martin Hachet (LaBRI)", "title": "Scientific Outreach with Teegi, a Tangible EEG Interface to Talk about\n  Neurotechnologies", "comments": null, "journal-ref": "CHI '17 Interactivity - SIGCHI Conference on Human Factors in\n  Computing System, May 2017, Denver, United States", "doi": "10.1145/3027063.3052971", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teegi is an anthropomorphic and tangible avatar exposing a users' brain\nactivity in real time. It is connected to a device sensing the brain by means\nof electroencephalog-raphy (EEG). Teegi moves its hands and feet and closes its\neyes along with the person being monitored. It also displays on its scalp the\nassociated EEG signals, thanks to a semi-spherical display made of LEDs.\nAttendees can interact directly with Teegi -- e.g. move its limbs -- to\ndiscover by themselves the underlying brain processes. Teegi can be used for\nscientific outreach to introduce neurotechnologies in general and\nbrain-computer interfaces (BCI) in particular.\n", "versions": [{"version": "v1", "created": "Tue, 7 Mar 2017 13:12:31 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "Potioc"], ["Gervais", "Renaud", "", "Potioc"], ["Lain\u00e9", "Thibault", "", "Potioc"], ["Duluc", "Maxime", "", "Potioc"], ["Germain", "Hugo", "", "Potioc"], ["Fleck", "St\u00e9phanie", "", "PERSEUS, UL"], ["Lotte", "Fabien", "", "Potioc"], ["Hachet", "Martin", "", "LaBRI"]]}, {"id": "1703.02744", "submitter": "The-Hien Dang-Ha", "authors": "Anh-Vu Dinh-Duc, The-Hien Dang-Ha, Ngoc-An Lam", "title": "Nviz - A General Purpse Visualization tool for Wireless Sensor Networks", "comments": "4 pages, 7 figures, Published in: IEEE International Conference on\n  Electrical Engineering/Electronics, Computer, Telecommunications and\n  Information Technology (ECTI-CON) 2012", "journal-ref": "9th International Conference on Electrical\n  Engineering/Electronics, Computer, Telecommunications and Information\n  Technology (ECTI-CON), IEEE, 2012", "doi": "10.1109/CTS.2013.6567285", "report-no": null, "categories": "cs.NI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Wireless Sensor Network (WSN), data manipulation and representation is a\ncrucial part and can take a lot of time to be developed from scratch. Although\nvarious visualization tools have been created for certain projects so far,\nthese tools can only be used in certain scenarios, due to their hard-coded\npacket formats and network's properties. To speed up the development process, a\nvisualization tool which can adapt to any kind of WSN is essentially necessary.\nFor this purpose, a general-purpose visualization tool - NViz, which can\nrepresent and visualize data for any kind of WSN, is proposed. NViz allows\nusers to set their network's properties and packet formats through XML files.\nBased on properties defined, users can choose the meaning of them and let NViz\nrepresents the data respectively. Furthermore, a better Replay mechanism, which\nlets researchers and developers debug their WSN easily, is also integrated in\nthis tool. NViz is designed based on a layered architecture which allows for\nclear and well-defined interrelationships and interfaces between each\ncomponent.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 08:21:19 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Dinh-Duc", "Anh-Vu", ""], ["Dang-Ha", "The-Hien", ""], ["Lam", "Ngoc-An", ""]]}, {"id": "1703.02860", "submitter": "Tianran Hu", "authors": "Tianran Hu, Han Guo, Hao Sun, Thuy-vy Thi Nguyen, Jiebo Luo", "title": "Spice up Your Chat: The Intentions and Sentiment Effects of Using Emoji", "comments": "10 pages, published at ICWSM'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emojis, as a new way of conveying nonverbal cues, are widely adopted in\ncomputer-mediated communications. In this paper, first from a message sender\nperspective, we focus on people's motives in using four types of emojis --\npositive, neutral, negative, and non-facial. We compare the willingness levels\nof using these emoji types for seven typical intentions that people usually\napply nonverbal cues for in communication. The results of extensive statistical\nhypothesis tests not only report the popularities of the intentions, but also\nuncover the subtle differences between emoji types in terms of intended uses.\nSecond, from a perspective of message recipients, we further study the\nsentiment effects of emojis, as well as their duplications, on verbal messages.\nDifferent from previous studies in emoji sentiment, we study the sentiments of\nemojis and their contexts as a whole. The experiment results indicate that the\npowers of conveying sentiment are different between four emoji types, and the\nsentiment effects of emojis vary in the contexts of different valences.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 14:57:10 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Hu", "Tianran", ""], ["Guo", "Han", ""], ["Sun", "Hao", ""], ["Nguyen", "Thuy-vy Thi", ""], ["Luo", "Jiebo", ""]]}, {"id": "1703.02929", "submitter": "Seyed Sadegh Mohseni Salehi", "authors": "Seyed Sadegh Mohseni Salehi, Mohammad Moghadamfalahi, Fernando\n  Quivira, Alexander Piers, Hooman Nezamfar, and Deniz Erdogmus", "title": "Decoding Complex Imagery Hand Gestures", "comments": "This work has been submitted to EMBC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain computer interfaces (BCIs) offer individuals suffering from major\ndisabilities an alternative method to interact with their environment.\nSensorimotor rhythm (SMRs) based BCIs can successfully perform control tasks;\nhowever, the traditional SMR paradigms intuitively disconnect the control and\nreal task, making them non-ideal for complex control scenarios. In this study,\nwe design a new, intuitively connected motor imagery (MI) paradigm using\nhierarchical common spatial patterns (HCSP) and context information to\neffectively predict intended hand grasps from electroencephalogram (EEG) data.\nExperiments with 5 participants yielded an aggregate classification\naccuracy--intended grasp prediction probability--of 64.5\\% for 8 different hand\ngestures, more than 5 times the chance level.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 17:35:00 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Salehi", "Seyed Sadegh Mohseni", ""], ["Moghadamfalahi", "Mohammad", ""], ["Quivira", "Fernando", ""], ["Piers", "Alexander", ""], ["Nezamfar", "Hooman", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1703.02938", "submitter": "Seyed Sadegh Mohseni Salehi", "authors": "Seyed Sadegh Mohseni Salehi, Mohammad Moghadamfalahi, Hooman Nezamfar,\n  Marzieh Haghighi and Deniz Erdogmus", "title": "Context-Aware Recursive Bayesian Graph Traversal in BCIs", "comments": "This work has been submitted to EMBC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noninvasive brain computer interfaces (BCI), and more specifically\nElectroencephalography (EEG) based systems for intent detection need to\ncompensate for the low signal to noise ratio of EEG signals. In many\napplications, the temporal dependency information from consecutive decisions\nand contextual data can be used to provide a prior probability for the upcoming\ndecision. In this study we proposed two probabilistic graphical models (PGMs),\nusing context information and previously observed EEG evidences to estimate a\nprobability distribution over the decision space in graph based decision-making\nmechanism. In this approach, user moves a pointer to the desired vertex in the\ngraph in which each vertex represents an action. To select a vertex, a Select\ncommand, or a proposed probabilistic Selection criterion (PSC) can be used to\nautomatically detect the user intended vertex. Performance of different PGMs\nand Selection criteria combinations are compared over a keyboard based on a\ngraph layout. Based on the simulation results, probabilistic Selection\ncriterion along with the probabilistic graphical model provides the highest\nperformance boost for individuals with pour calibration performance and\nachieving the same performance for individuals with high calibration\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 17:49:35 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Salehi", "Seyed Sadegh Mohseni", ""], ["Moghadamfalahi", "Mohammad", ""], ["Nezamfar", "Hooman", ""], ["Haghighi", "Marzieh", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1703.02968", "submitter": "Daniele Bernardini", "authors": "Andrea Barillari (1), Daniele Bernardini (1), Pierluigi Crescenzi (2)\n  ((1) Intranet Standard GmbH, Munich, Germany (2) Universit\\`a di Firenze,\n  Italy)", "title": "Sigil3D: A Crowdsourcing Platform for Interactive 3D Content", "comments": "translated from the paper published in the conference proceedings for\n  GARR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose applying the crowdsourcing approach to a software\nplatform that uses a modern and state-of-the-art 3D game engine. This platform\ncould facilitate the generation and manipulation of interactive 3D environments\nby a community of users producing different content such as cultural heritage,\nscientific virtual labs, games, novel art forms and virtual museums.\n", "versions": [{"version": "v1", "created": "Wed, 8 Mar 2017 18:16:21 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Barillari", "Andrea", ""], ["Bernardini", "Daniele", ""], ["Crescenzi", "Pierluigi", ""]]}, {"id": "1703.03156", "submitter": "Ingmar Weber", "authors": "Enes Kocabey, Mustafa Camurcu, Ferda Ofli, Yusuf Aytar, Javier Marin,\n  Antonio Torralba, Ingmar Weber", "title": "Face-to-BMI: Using Computer Vision to Infer Body Mass Index on Social\n  Media", "comments": "This is a preprint of a short paper accepted at ICWSM'17. Please cite\n  that version instead", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A person's weight status can have profound implications on their life,\nranging from mental health, to longevity, to financial income. At the societal\nlevel, \"fat shaming\" and other forms of \"sizeism\" are a growing concern, while\nincreasing obesity rates are linked to ever raising healthcare costs. For these\nreasons, researchers from a variety of backgrounds are interested in studying\nobesity from all angles. To obtain data, traditionally, a person would have to\naccurately self-report their body-mass index (BMI) or would have to see a\ndoctor to have it measured. In this paper, we show how computer vision can be\nused to infer a person's BMI from social media images. We hope that our tool,\nwhich we release, helps to advance the study of social aspects related to body\nweight.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 06:48:47 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Kocabey", "Enes", ""], ["Camurcu", "Mustafa", ""], ["Ofli", "Ferda", ""], ["Aytar", "Yusuf", ""], ["Marin", "Javier", ""], ["Torralba", "Antonio", ""], ["Weber", "Ingmar", ""]]}, {"id": "1703.03437", "submitter": "Jakob Eg Larsen", "authors": "Jakob Eg Larsen, Kasper Eskelund, Thomas Blomseth Christiansen", "title": "Active Self-Tracking of Subjective Experience with a One-Button\n  Wearable: A Case Study in Military PTSD", "comments": "5 pages, 4 figures, 2nd Symposium Computing and Mental Health at ACM\n  CHI Conference on Human Factors in Computing Systems 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a case study with the participation of a Danish veteran suffering\nfrom post-traumatic stress disorder (PTSD). As part of psychotherapeutic\ntreatment the participant and therapist have used our novel technique for\ninstrumenting self-tracking of select aspects of subjective experience using a\none-button wearable device. The instrumentation system is described along with\nthe specific self-track- ing protocol which defined the participant's\nself-tracking of a single symptom, namely the occurrences of a bodily\nexperienced precursor to hyperarousal. Results from the case study demonstrate\nhow self-tracking data on a single symptom collected by a patient can provide\nvaluable input to the therapeutic process. Specifically, it facilitated\nidentification of crucial details otherwise unavailable from the clinical\nassessment and even became decisive in disentangling different symptoms and\ntheir causes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Mar 2017 19:40:27 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Larsen", "Jakob Eg", ""], ["Eskelund", "Kasper", ""], ["Christiansen", "Thomas Blomseth", ""]]}, {"id": "1703.03714", "submitter": "Matthew Marge", "authors": "Matthew Marge, Claire Bonial, Brendan Byrne, Taylor Cassidy, A.\n  William Evans, Susan G. Hill, Clare Voss", "title": "Applying the Wizard-of-Oz Technique to Multimodal Human-Robot Dialogue", "comments": "Presented at the 2016 IEEE International Symposium on Robot and Human\n  Interactive Communication (RO-MAN), Interactive Session, August 26-31, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our overall program objective is to provide more natural ways for soldiers to\ninteract and communicate with robots, much like how soldiers communicate with\nother soldiers today. We describe how the Wizard-of-Oz (WOz) method can be\napplied to multimodal human-robot dialogue in a collaborative exploration task.\nWhile the WOz method can help design robot behaviors, traditional approaches\nplace the burden of decisions on a single wizard. In this work, we consider two\nwizards to stand in for robot navigation and dialogue management software\ncomponents. The scenario used to elicit data is one in which a human-robot team\nis tasked with exploring an unknown environment: a human gives verbal\ninstructions from a remote location and the robot follows them, clarifying\npossible misunderstandings as needed via dialogue. We found the division of\nlabor between wizards to be workable, which holds promise for future software\ndevelopment.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 15:27:45 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Marge", "Matthew", ""], ["Bonial", "Claire", ""], ["Byrne", "Brendan", ""], ["Cassidy", "Taylor", ""], ["Evans", "A. William", ""], ["Hill", "Susan G.", ""], ["Voss", "Clare", ""]]}, {"id": "1703.03831", "submitter": "Giacomo Cabri", "authors": "Daniele Grassi, Giacomo Barigazzi, Giacomo Cabri", "title": "User Longevity and Engagement in Mobile Multiplayer Sports Management\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile games are extremely popular and engage millions of people every day.\nEven if they are often quite simple, their development features a high degree\nof difficulty and requires close attention to both achieve a high satisfaction\nfrom users and grant a return to the developers. In this paper we propose a\nmodel that analyzes users' playing session time in order to evaluate and\nmaximize the longevity of games, even during the first phases of their\ndevelopment.\n", "versions": [{"version": "v1", "created": "Fri, 10 Mar 2017 20:10:41 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Grassi", "Daniele", ""], ["Barigazzi", "Giacomo", ""], ["Cabri", "Giacomo", ""]]}, {"id": "1703.04150", "submitter": "Charith Perera", "authors": "Charith Perera, Saeed Aghaee, Ramsey Faragher, Robert Harle, Alan\n  Blackwell", "title": "A Contextual Investigation of Location in the Home Using Bluetooth Low\n  Energy Beacons", "comments": "Technical Report (6 Pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location sensing is a key enabling technology for Ubicomp to support\ncontextual interaction. However, the laboratories where calibrated testing of\nlocation technologies is done are very different to the domestic situations\nwhere `context' is a problematic social construct. This study reports\nmeasurements of Bluetooth beacons, informed by laboratory studies, but done in\ndiverse domestic settings. The design of these surveys has been motivated by\nthe natural environment implied in the Bluetooth beacon standards - relating\nthe technical environment of the beacon to the function of spaces within the\nhome. This research method can be considered as a situated, `ethnographic'\ntechnical response to the study of physical infrastructure that arises through\nsocial processes. The results offer insights for the future design of `seamful'\napproaches to indoor location sensing, and to the ways that context might be\nconstructed and interpreted in a seamful manner.\n", "versions": [{"version": "v1", "created": "Sun, 12 Mar 2017 17:51:12 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Perera", "Charith", ""], ["Aghaee", "Saeed", ""], ["Faragher", "Ramsey", ""], ["Harle", "Robert", ""], ["Blackwell", "Alan", ""]]}, {"id": "1703.04416", "submitter": "Robert Obryk", "authors": "Jyrki Alakuijala, Robert Obryk, Zoltan Szabadka, and Jan Wassenberg", "title": "Users prefer Guetzli JPEG over same-sized libjpeg", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on pairwise comparisons by human raters of JPEG images from libjpeg\nand our new Guetzli encoder. Although both files are size-matched, 75% of\nratings are in favor of Guetzli. This implies the Butteraugli psychovisual\nimage similarity metric which guides Guetzli is reasonably close to human\nperception at high quality levels. We provide access to the raw ratings and\nsource images for further analysis and study.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 14:33:47 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Alakuijala", "Jyrki", ""], ["Obryk", "Robert", ""], ["Szabadka", "Zoltan", ""], ["Wassenberg", "Jan", ""]]}, {"id": "1703.04421", "submitter": "Robert Obryk", "authors": "Jyrki Alakuijala, Robert Obryk, Ostap Stoliarchuk, Zoltan Szabadka,\n  Lode Vandevenne, and Jan Wassenberg", "title": "Guetzli: Perceptually Guided JPEG Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Guetzli is a new JPEG encoder that aims to produce visually indistinguishable\nimages at a lower bit-rate than other common JPEG encoders. It optimizes both\nthe JPEG global quantization tables and the DCT coefficient values in each JPEG\nblock using a closed-loop optimizer. Guetzli uses Butteraugli, our perceptual\ndistance metric, as the source of feedback in its optimization process. We\nreach a 29-45% reduction in data size for a given perceptual distance,\naccording to Butteraugli, in comparison to other compressors we tried.\nGuetzli's computation is currently extremely slow, which limits its\napplicability to compressing static content and serving as a proof- of-concept\nthat we can achieve significant reductions in size by combining advanced\npsychovisual models with lossy compression techniques.\n", "versions": [{"version": "v1", "created": "Mon, 13 Mar 2017 14:40:08 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Alakuijala", "Jyrki", ""], ["Obryk", "Robert", ""], ["Stoliarchuk", "Ostap", ""], ["Szabadka", "Zoltan", ""], ["Vandevenne", "Lode", ""], ["Wassenberg", "Jan", ""]]}, {"id": "1703.04574", "submitter": "Miles Hansard", "authors": "Kasim Terzic and Miles Hansard", "title": "Causes of discomfort in stereoscopic content: a review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the causes of discomfort in viewing stereoscopic content.\nThese include objective factors, such as misaligned images, as well as\nsubjective factors, such as excessive disparity. Different approaches to the\nmeasurement of visual discomfort are also reviewed, in relation to the\nunderlying physiological and psychophysical processes. The importance of\nunderstanding these issues, in the context of new display technologies, is\nemphasized.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 14:07:19 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Terzic", "Kasim", ""], ["Hansard", "Miles", ""]]}, {"id": "1703.04791", "submitter": "Changtao Zhong", "authors": "Changtao Zhong, Hau-wen Chan, Dmytro Karamshuk, Dongwon Lee, Nishanth\n  Sastry", "title": "Wearing Many (Social) Hats: How Different are Your Different Social\n  Network Personae?", "comments": "Accepted at the 11th International AAAI Conference on Web and Social\n  Media (ICWSM17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates when users create profiles in different social\nnetworks, whether they are redundant expressions of the same persona, or they\nare adapted to each platform. Using the personal webpages of 116,998 users on\nAbout.me, we identify and extract matched user profiles on several major social\nnetworks including Facebook, Twitter, LinkedIn, and Instagram. We find evidence\nfor distinct site-specific norms, such as differences in the language used in\nthe text of the profile self-description, and the kind of picture used as\nprofile image. By learning a model that robustly identifies the platform given\na user's profile image (0.657--0.829 AUC) or self-description (0.608--0.847\nAUC), we confirm that users do adapt their behaviour to individual platforms in\nan identifiable and learnable manner. However, different genders and age groups\nadapt their behaviour differently from each other, and these differences are,\nin general, consistent across different platforms. We show that differences in\nsocial profile construction correspond to differences in how formal or informal\nthe platform is.\n", "versions": [{"version": "v1", "created": "Tue, 14 Mar 2017 22:37:01 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 21:31:04 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Zhong", "Changtao", ""], ["Chan", "Hau-wen", ""], ["Karamshuk", "Dmytro", ""], ["Lee", "Dongwon", ""], ["Sastry", "Nishanth", ""]]}, {"id": "1703.04874", "submitter": "Jovonni Pharr", "authors": "Jovonni L. Pharr", "title": "Hacker Combat: A Competitive Sport from Programmatic Dueling &\n  Cyberwarfare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The history of humanhood has included competitive activities of many\ndifferent forms. Sports have offered many benefits beyond that of\nentertainment. At the time of this article, there exists not a competitive\necosystem for cyber security beyond that of conventional capture the flag\ncompetitions, and the like. This paper introduces a competitive framework with\na foundation on computer science, and hacking. This proposed competitive\nlandscape encompasses the ideas underlying information security, software\nengineering, and cyber warfare. We also demonstrate the opportunity to rank,\nscore, & categorize actionable skill levels into tiers of capability.\nPhysiological metrics are analyzed from participants during gameplay. These\nanalyses provide support regarding the intricacies required for competitive\nplay, and analysis of play. We use these intricacies to build a case for an\norganized competitive ecosystem. Using previous player behavior from gameplay,\nwe also demonstrate the generation of an artificial agent purposed with\ngameplay at a competitive level.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 01:38:16 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Pharr", "Jovonni L.", ""]]}, {"id": "1703.04900", "submitter": "Chris Holdgraf", "authors": "Chris Holdgraf, Aaron Culich, Ariel Rokem, Fatma Deniz, Maryana\n  Alegro, Dani Ushizima", "title": "Portable learning environments for hands-on computational instruction:\n  Using container- and cloud-based technology to teach data science", "comments": "Accepted at the PEARC 2017 conference in New Orleans, LA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in learning outside of the traditional\nclassroom setting. This is especially true for topics covering computational\ntools and data science, as both are challenging to incorporate in the standard\ncurriculum. These atypical learning environments offer new opportunities for\nteaching, particularly when it comes to combining conceptual knowledge with\nhands-on experience/expertise with methods and skills. Advances in cloud\ncomputing and containerized environments provide an attractive opportunity to\nimprove the efficiency and ease with which students can learn. This manuscript\ndetails recent advances towards using commonly-available cloud computing\nservices and advanced cyberinfrastructure support for improving the learning\nexperience in bootcamp-style events. We cover the benefits (and challenges) of\nusing a server hosted remotely instead of relying on student laptops, discuss\nthe technology that was used in order to make this possible, and give\nsuggestions for how others could implement and improve upon this model for\npedagogy and reproducibility.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 02:57:57 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 23:12:25 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Holdgraf", "Chris", ""], ["Culich", "Aaron", ""], ["Rokem", "Ariel", ""], ["Deniz", "Fatma", ""], ["Alegro", "Maryana", ""], ["Ushizima", "Dani", ""]]}, {"id": "1703.05267", "submitter": "Tim Weninger PhD", "authors": "Maria Glenski, Corey Pennycuff, Tim Weninger", "title": "Consumers and Curators: Browsing and Voting Patterns on Reddit", "comments": "16 pages, 12 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As crowd-sourced curation of news and information become the norm, it is\nimportant to understand not only how individuals consume information through\nsocial news Web sites, but also how they contribute to their ranking systems.\nIn the present work, we introduce and make available a new dataset containing\nthe activity logs that recorded all activity for 309 Reddit users for one year.\nUsing this newly collected data, we present findings that highlight the\nbrowsing and voting behavior of the study's participants. We find that most\nusers do not read the article that they vote on, and that, in total, 73% of\nposts were rated (ie, upvoted or downvoted) without first viewing the content.\nWe also show evidence of cognitive fatigue in the browsing sessions of users\nthat are most likely to vote.\n", "versions": [{"version": "v1", "created": "Wed, 15 Mar 2017 17:06:31 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Glenski", "Maria", ""], ["Pennycuff", "Corey", ""], ["Weninger", "Tim", ""]]}, {"id": "1703.05462", "submitter": "Hsiang-Ting Chen", "authors": "Avinash Kumar Singh, Hsiang-Ting Chen, Jung-Tai King, Chin-Teng Lin", "title": "Measuring Cognitive Conflict in Virtual Reality with Feedback-Related\n  Negativity", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2018.2832089", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As virtual reality (VR) emerges as a mainstream platform, designers have\nstarted to experiment new interaction techniques to enhance the user\nexperience. This is a challenging task because designers not only strive to\nprovide designs with good performance but also carefully ensure not to disrupt\nusers' immersive experience. There is a dire need for a new evaluation tool\nthat extends beyond traditional quantitative measurements to assist designers\nin the design process. We propose an EEG-based experiment framework that\nevaluates interaction techniques in VR by measuring intentionally elicited\ncognitive conflict. Through the analysis of the feedback-related negativity\n(FRN) as well as other quantitative measurements, this framework allows\ndesigners to evaluate the effect of the variables of interest. We studied the\nframework by applying it to the fundamental task of 3D object selection using\ndirect 3D input, i.e. tracked hand in VR. The cognitive conflict is\nintentionally elicited by manipulating the selection radius of the target\nobject. Our first behavior experiment validated the framework in line with the\nfindings of conflict-induced behavior adjustments like those reported in other\nclassical psychology experiment paradigms. Our second EEG-based experiment\nexamines the effect of the appearance of virtual hands. We found that the\namplitude of FRN correlates with the level of realism of the virtual hands,\nwhich concurs with the Uncanny Valley theory.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 02:56:34 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Singh", "Avinash Kumar", ""], ["Chen", "Hsiang-Ting", ""], ["King", "Jung-Tai", ""], ["Lin", "Chin-Teng", ""]]}, {"id": "1703.05616", "submitter": "Arianna D'Ulizia", "authors": "Fernando Ferri, Arianna D'Ulizia, Patrizia Grifoni", "title": "Multimodal Language Specification for Human Adaptive Mechatronics", "comments": "11 pages, 4 figures", "journal-ref": "Journal of Next Generation Information Technology (JNIT), Volume\n  3, Number 1, November 2012", "doi": "10.4156/jnit.vol3.issue1.6", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing and building automated systems with which people can interact\nnaturally is one of the emerging objective of Mechatronics. In this perspective\nmultimodality and adaptivity represent focal issues, enabling users to\ncommunicate more freely and naturally with automated systems. One of the basic\nproblem of multimodal interaction is the fusion process. Current approaches to\nfusion are mainly two: the former implements the multimodal fusion at dialogue\nmanagement level, whereas the latter at grammar level. In this paper, we\npropose a multimodal attribute grammar, that provides constructions both for\nrepresenting input symbols from different modalities and for modeling semantic\nand temporal features of multimodal input symbols, enabling the specification\nof multimodal languages. Moreover, an application of the proposed approach in\nthe context of a multimodal language specification to control a driver\nassistance system, as robots using different integrated interaction modalities,\nis given.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 13:38:02 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Ferri", "Fernando", ""], ["D'Ulizia", "Arianna", ""], ["Grifoni", "Patrizia", ""]]}, {"id": "1703.05700", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Tom Yeh, Koji Yatani, Mark D. Gross", "title": "Autocomplete Textures for 3D Printing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture is an essential property of physical objects that affects aesthetics,\nusability, and functionality. However, designing and applying textures to 3D\nobjects with existing tools remains difficult and time-consuming; it requires\nproficient 3D modeling skills. To address this, we investigated an\nauto-completion approach for efficient texture creation that automates the\ntedious, repetitive process of applying texture while allowing flexible\ncustomization. We developed techniques for users to select a target surface,\nsketch and manipulate a texture with 2D drawings, and then generate 3D\nprintable textures onto an arbitrary curved surface. In a controlled experiment\nour tool sped texture creation by 80% over conventional tools, a performance\ngain that is higher with more complex target surfaces. This result confirms\nthat auto-completion is powerful for creating 3D textures.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 16:24:01 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Suzuki", "Ryo", ""], ["Yeh", "Tom", ""], ["Yatani", "Koji", ""], ["Gross", "Mark D.", ""]]}, {"id": "1703.05834", "submitter": "Long Qian", "authors": "Ehsan Azimi, Long Qian, Nassir Navab, Peter Kazanzides", "title": "Alignment of the Virtual Scene to the Tracking Space of a Mixed Reality\n  Head-Mounted Display", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the mounting global interest for optical see-through head-mounted\ndisplays (OST-HMDs) across medical, industrial and entertainment settings, many\nsystems with different capabilities are rapidly entering the market. Despite\nsuch variety, they all require display calibration to create a proper mixed\nreality environment. With the aid of tracking systems, it is possible to\nregister rendered graphics with tracked objects in the real world. We propose a\ncalibration procedure to properly align the coordinate system of a 3D virtual\nscene that the user sees with that of the tracker. Our method takes a blackbox\napproach towards the HMD calibration, where the tracker's data is its input and\nthe 3D coordinates of a virtual object in the observer's eye is the output; the\nobjective is thus to find the 3D projection that aligns the virtual content\nwith its real counterpart. In addition, a faster and more intuitive version of\nthis calibration is introduced in which the user simultaneously aligns multiple\npoints of a single virtual 3D object with its real counterpart; this reduces\nthe number of required repetitions in the alignment from 20 to only 4, which\nleads to a much easier calibration task for the user. In this paper, both\ninternal (HMD camera) and external tracking systems are studied. We perform\nexperiments with Microsoft HoloLens, taking advantage of its self localization\nand spatial mapping capabilities to eliminate the requirement for line of sight\nfrom the HMD to the object or external tracker. The experimental results\nindicate an accuracy of up to 4 mm in the average reprojection error based on\ntwo separate evaluation methods. We further perform experiments with the\ninternal tracking on the Epson Moverio BT-300 to demonstrate that the method\ncan provide similar results with other HMDs.\n", "versions": [{"version": "v1", "created": "Thu, 16 Mar 2017 21:51:23 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 03:34:01 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 21:55:54 GMT"}, {"version": "v4", "created": "Wed, 27 Mar 2019 19:04:16 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Azimi", "Ehsan", ""], ["Qian", "Long", ""], ["Navab", "Nassir", ""], ["Kazanzides", "Peter", ""]]}, {"id": "1703.06161", "submitter": "Valery Vilisov", "authors": "Valery Vilisov", "title": "Risk Proneness Estimation Method Developed in Relation to the Decision\n  Taker that Controls the Robotic System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work suggests the estimation method developed in relation to the\nposition of the robotic system (RS) operator, showing his degree of risk\nproneness. The base models are: Hurwitz pessimism/optimism criterion and\ndecision trees. The problem is solved using the reverse setting: we estimate\npessimism/optimism parameter of the operator (decision taker) by observing what\ndecisions he makes when controlling the RS. The solution context of such\ndecision taker position estimation problems can be: using RS in emergency\nsituations, in military actions and other situations connected with the\nuncertainty of the situation.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 18:39:23 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Vilisov", "Valery", ""]]}, {"id": "1703.06169", "submitter": "Mark Whiting", "authors": "Dilrukshi Gamage, Mark Whiting, Thejan Rajapakshe, Haritha\n  Thilakarathne, Indika Perera, Shantha Fernando", "title": "Improving Assessment on MOOCs Through Peer Identification and Aligned\n  Incentives", "comments": "To apear at Learning@Scale 2017", "journal-ref": null, "doi": "10.1145/3051457.3054013", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive Open Online Courses (MOOCs) use peer assessment to grade open ended\nquestions at scale, allowing students to provide feedback. Relative to teacher\nbased grading, peer assessment on MOOCs traditionally delivers lower quality\nfeedback and fewer learner interactions. We present the identified peer review\n(IPR) framework, which provides non-blind peer assessment and incentives\ndriving high quality feedback. We show that, compared to traditional peer\nassessment methods, IPR leads to significantly longer and more useful feedback\nas well as more discussion between peers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Mar 2017 19:00:52 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gamage", "Dilrukshi", ""], ["Whiting", "Mark", ""], ["Rajapakshe", "Thejan", ""], ["Thilakarathne", "Haritha", ""], ["Perera", "Indika", ""], ["Fernando", "Shantha", ""]]}, {"id": "1703.06317", "submitter": "Marcos Baez", "authors": "Leysan Nurgalieva, Juan Jose Jara Laconich, Marcos Baez, Fabio Casati,\n  Maurizio Marchese", "title": "Designing for older adults: review of touchscreen design guidelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distinct abilities of older adults to interact with computers has\nmotivated a wide range of contributions in the the form of design guidelines\nfor making technologies usable and accessible for the elderly population.\nHowever, despite the growing effort by the research community, the adoption of\nguidelines by developers and designers has been scant or not properly\ntranslated into more accessible interaction systems. In this paper we explore\nthis issue by reporting on a qualitative outcomes of a systematic review of 204\nresearch-derived design guidelines for touchscreen applications. We report\nfirst on the different definitions of \"elderly\" and assess the reliability,\norganization and accessibility of the guidelines. Then we present our early\nattempt at facilitating the reporting and access of such guidelines to\nresearchers and practitioners.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 16:33:58 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Nurgalieva", "Leysan", ""], ["Laconich", "Juan Jose Jara", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""], ["Marchese", "Maurizio", ""]]}, {"id": "1703.06537", "submitter": "Varvara Kollia", "authors": "Varvara Kollia, Noureddine Tayebi", "title": "A Controlled Set-Up Experiment to Establish Personalized Baselines for\n  Real-Life Emotion Recognition", "comments": "15 pages, 2 figures, 9 tables, Statistics-Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design, conduct and present the results of a highly personalized baseline\nemotion recognition experiment, which aims to set reliable ground-truth\nestimates for the subject's emotional state for real-life prediction under\nsimilar conditions using a small number of physiological sensors. We also\npropose an adaptive stimuli-selection mechanism that would use the user's\nfeedback as guide for future stimuli selection in the controlled-setup\nexperiment and generate optimal ground-truth personalized sessions\nsystematically. Initial results are very promising (85% accuracy) and variable\nimportance analysis shows that only a few features, which are easy-to-implement\nin portable devices, would suffice to predict the subject's emotional state.\n", "versions": [{"version": "v1", "created": "Sun, 19 Mar 2017 23:28:39 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Kollia", "Varvara", ""], ["Tayebi", "Noureddine", ""]]}, {"id": "1703.06637", "submitter": "Jichang Zhao", "authors": "Zhenkun Zhou, Ke Xu and Jichang Zhao", "title": "Extroverts Tweet Differently from Introverts in Weibo", "comments": "Datasets of this study can be freely downloaded through:\n  https://doi.org/10.6084/m9.figshare.4765150.v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being dominant factors driving the human actions, personalities can be\nexcellent indicators in predicting the offline and online behavior of different\nindividuals. However, because of the great expense and inevitable subjectivity\nin questionnaires and surveys, it is challenging for conventional studies to\nexplore the connection between personality and behavior and gain insights in\nthe context of large amount individuals. Considering the more and more\nimportant role of the online social media in daily communications, we argue\nthat the footprint of massive individuals, like tweets in Weibo, can be the\ninspiring proxy to infer the personality and further understand its functions\nin shaping the online human behavior. In this study, a map from self-reports of\npersonalities to online profiles of 293 active users in Weibo is established to\ntrain a competent machine learning model, which then successfully identifies\nover 7,000 users as extroverts or introverts. Systematical comparisons from\nperspectives of tempo-spatial patterns, online activities, emotion expressions\nand attitudes to virtual honor surprisingly disclose that the extrovert indeed\nbehaves differently from the introvert in Weibo. Our findings provide solid\nevidence to justify the methodology of employing machine learning to\nobjectively study personalities of massive individuals and shed lights on\napplications of probing personalities and corresponding behaviors solely\nthrough online profiles.\n", "versions": [{"version": "v1", "created": "Mon, 20 Mar 2017 09:00:00 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Zhou", "Zhenkun", ""], ["Xu", "Ke", ""], ["Zhao", "Jichang", ""]]}, {"id": "1703.07187", "submitter": "Nicolai Brodersen Hansen", "authors": "Nicolai Brodersen Hansen", "title": "Materials in Participatory Design Processes", "comments": "89 pages, PhD Dissertation - contains introductory comments, the\n  overview article, abstracts and links to all papers appended in the final\n  doctoral work", "journal-ref": "PhD Dissertation. Department of Culture and Communication, Aarhus\n  University, 2016", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This dissertation presents three years of academic inquiry into the question\nof what role materials play in interaction design and participatory design\nprocesses. The dissertation aims at developing conceptual tools, based on\nDeweys pragmatism, for understanding how materials aid design reflection.\n  It has been developed using a research-through-design approach in which the\nauthor has conducted practical design work in order to investigate and\nexperiment with using materials to scaffold design inquiry. The results of the\nPhD work is submitted as seven separate papers, submitted to esteemed journals\nand conferences within the field of interaction design and HCI.\n  The work is motivated both by the growing interest in materials in\ninteraction design and HCI and the interest in design processes and\ncollaboration within those fields. At the core of the dissertation lies an\ninterest in the many different materials used during the design process:\nsketches, prototypes as well as the materials we shape products out of:\nphysical and digital materials now form a unity of computation and physical\nmaterials that has given rise to a new research interest in design and\nmateriality.\n  The main results from the dissertation are an understanding of design\nmaterials that draws on pragmatist philosophy. The papers and overview article\nhighlights how materials in a pragmatist perspective are more than the matter\nout of which we shape an idea. Rather they structure the entire process of\ninquiry, helping us frame problems, inspire solutions and try out these\nsolutions in practice. This framework, developed in several of the submitted\npapers, is tested and illustrated through a series of experimental design\ncases.\n", "versions": [{"version": "v1", "created": "Tue, 21 Mar 2017 12:50:21 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Hansen", "Nicolai Brodersen", ""]]}, {"id": "1703.07534", "submitter": "Dong Liu", "authors": "Jingxian Zhang and Dong Liu", "title": "Visual Analyses of Music History: A User-Centric Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music history, referring to the records of users' listening or downloading\nhistory in online music services, is the primary source for music service\nproviders to analyze users' preferences on music and thus to provide\npersonalized recommendations to users. In order to engage users into the\nservice and to improve user experience, it would be beneficial to provide\nvisual analyses of one user's music history as well as visualized\nrecommendations to that user. In this paper, we take a user-centric approach to\nthe design of such visual analyses. We start by investigating user needs on\nsuch visual analyses and recommendations, then propose several different\nvisualization schemes, and perform a pilot study to collect user feedback on\nthe designed schemes. We further conduct user studies to verify the utility of\nthe proposed schemes, and the results not only demonstrate the effectiveness of\nour proposed visualization, but also provide important insights to guide the\nvisualization design in the future.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 05:37:19 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Zhang", "Jingxian", ""], ["Liu", "Dong", ""]]}, {"id": "1703.07555", "submitter": "Pierre De", "authors": "Landy Rajaonarivo (ENIB), Matthieu Courgeon (ENIB), Eric Maisel\n  (ENIB), Pierre De Loor (ENIB)", "title": "Inline Co-Evolution between Users and Information Presentation for Data\n  Exploration", "comments": null, "journal-ref": "22nd International Conference on Intelligent User Interfaces , Mar\n  2017, Limassol, Cyprus. ACM, IUI '17 Proceedings of the 22nd International\n  Conference on Intelligent User Interfaces pp.215 - 219, 2017, IUI '17\n  Proceedings of the 22nd International Conference on Intelligent User\n  Interfaces", "doi": "10.1145/3025171.3025226", "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an intelligent user interface model dedicated to the\nexploration of complex databases. This model is implemented on a 3D metaphor :\na virtual museum. In this metaphor, the database elements are embodied as\nmuseum objects. The objects are grouped in rooms according to their semantic\nproperties and relationships and the rooms organization forms the museum. Rooms\norganization is not predefi-ned but defined incrementally by taking into\naccount not only the relationships between objects, but also the users centers\nof interest. The latter are evaluated in real-time through user interactions\nwithin the virtual museum. This interface allows for a personal reading and\nfavors the discovery of unsuspec-ted links between data. In this paper, we\npresent our model's formalization as well as its application to the context of\ncultural heritage.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 08:05:30 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Rajaonarivo", "Landy", "", "ENIB"], ["Courgeon", "Matthieu", "", "ENIB"], ["Maisel", "Eric", "", "ENIB"], ["De Loor", "Pierre", "", "ENIB"]]}, {"id": "1703.07729", "submitter": "Ethan Kerzner", "authors": "Ethan Kerzner, Alexander Lex, Crystal Lynn Sigulinsky, Timothy Urness,\n  Bryan William Jones, Robert E. Marc, Miriah Meyer", "title": "Graffinity: Visualizing Connectivity In Large Graphs", "comments": "The definitive version is available at http://diglib.eg.org/ and\n  http://onlinelibrary.wiley.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate graphs are prolific across many fields, including transportation\nand neuroscience. A key task in graph analysis is the exploration of\nconnectivity, to, for example, analyze how signals flow through neurons, or to\nexplore how well different cities are connected by flights. While standard\nnode-link diagrams are helpful in judging connectivity, they do not scale to\nlarge networks. Adjacency matrices also do not scale to large networks and are\nonly suitable to judge connectivity of adjacent nodes. A key approach to\nrealize scalable graph visualization are queries: instead of displaying the\nwhole network, only a relevant subset is shown. Query-based techniques for\nanalyzing connectivity in graphs, however, can also easily suffer from\ncluttering if the query result is big enough. To remedy this, we introduce\ntechniques that provide an overview of the connectivity and reveal details on\ndemand. We have two main contributions: (1) two novel visualization techniques\nthat work in concert for summarizing graph connectivity; and (2) Graffinity, an\nopen-source implementation of these visualizations supplemented by detail views\nto enable a complete analysis workflow. Graffinity was designed in a close\ncollaboration with neuroscientists and is optimized for connectomics data\nanalysis, yet the technique is applicable across domains. We validate the\nconnectivity overview and our open-source tool with illustrative examples using\nflight and connectomics data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 16:23:32 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Kerzner", "Ethan", ""], ["Lex", "Alexander", ""], ["Sigulinsky", "Crystal Lynn", ""], ["Urness", "Timothy", ""], ["Jones", "Bryan William", ""], ["Marc", "Robert E.", ""], ["Meyer", "Miriah", ""]]}, {"id": "1703.07869", "submitter": "Jens Grubert", "authors": "Peter Mohr, Markus Tatzgern, Jens Grubert, Dieter Schmalstieg, Denis\n  Kalkofen", "title": "Adaptive User Perspective Rendering for Handheld Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handheld Augmented Reality commonly implements some variant of magic lens\nrendering, which turns only a fraction of the user's real environment into AR\nwhile the rest of the environment remains unaffected. Since handheld AR devices\nare commonly equipped with video see-through capabilities, AR magic lens\napplications often suffer from spatial distortions, because the AR environment\nis presented from the perspective of the camera of the mobile device. Recent\napproaches counteract this distortion based on estimations of the user's head\nposition, rendering the scene from the user's perspective. To this end,\napproaches usually apply face-tracking algorithms on the front camera of the\nmobile device. However, this demands high computational resources and therefore\ncommonly affects the performance of the application beyond the already high\ncomputational load of AR applications. In this paper, we present a method to\nreduce the computational demands for user perspective rendering by applying\nlightweight optical flow tracking and an estimation of the user's motion before\nhead tracking is started. We demonstrate the suitability of our approach for\ncomputationally limited mobile devices and we compare it to device perspective\nrendering, to head tracked user perspective rendering, as well as to fixed\npoint of view user perspective rendering.\n", "versions": [{"version": "v1", "created": "Wed, 22 Mar 2017 21:56:58 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["Mohr", "Peter", ""], ["Tatzgern", "Markus", ""], ["Grubert", "Jens", ""], ["Schmalstieg", "Dieter", ""], ["Kalkofen", "Denis", ""]]}, {"id": "1703.08288", "submitter": "Beat Signer", "authors": "Beat Signer and Timothy J. Curtin", "title": "Tangible Holograms: Towards Mobile Physical Augmentation of Virtual\n  Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": "WISE-2017-01", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last two decades have seen the emergence and steady development of\ntangible user interfaces. While most of these interfaces are applied for input\n- with output still on traditional computer screens - the goal of programmable\nmatter and actuated shape-changing materials is to directly use the physical\nobjects for visual or tangible feedback. Advances in material sciences and\nflexible display technologies are investigated to enable such reconfigurable\nphysical objects. While existing solutions aim for making physical objects more\ncontrollable via the digital world, we propose an approach where holograms\n(virtual objects) in a mixed reality environment are augmented with physical\nvariables such as shape, texture or temperature. As such, the support for\nmobility forms an important contribution of the proposed solution since it\nenables users to freely move within and across environments. Furthermore, our\naugmented virtual objects can co-exist in a single environment with\nprogrammable matter and other actuated shape-changing solutions. The future\npotential of the proposed approach is illustrated in two usage scenarios and we\nhope that the presentation of our work in progress on a novel way to realise\ntangible holograms will foster some lively discussions in the CHI community.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 05:31:56 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Signer", "Beat", ""], ["Curtin", "Timothy J.", ""]]}, {"id": "1703.08365", "submitter": "Manoel Miranda", "authors": "Priscila Martins, Manoel Miranda, Fabr\\'icio Benevenuto, Jussara\n  Almeida", "title": "The Emergence of Crowdsourcing among Pok\\'emon Go Players", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its launching, Pok{\\'e}mon Go has been pointed as the largest gaming\nphenomenon of the smartphone age. As the game requires the user to walk in the\nreal world to see and capture Pok{\\'e}mons, a new wave of crowdsourcing apps\nhave emerged to allow users to collaborate with each other, sharing where and\nwhen Pok{\\'e}mons were found. In this paper we characterize one of such\ninitiatives, called PokeCrew. Our analyses uncover a set of aspects of user\nbehavior and system usage in such emerging crowdsourcing task, helping unveil\nsome problems and benefits. We hope our effort can inspire the design of new\ncrowdsourcing systems.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 11:31:20 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Martins", "Priscila", ""], ["Miranda", "Manoel", ""], ["Benevenuto", "Fabr\u00edcio", ""], ["Almeida", "Jussara", ""]]}, {"id": "1703.08428", "submitter": "Justin Cranshaw", "authors": "Justin Cranshaw, Emad Elwany, Todd Newman, Rafal Kocielnik, Bowen Yu,\n  Sandeep Soni, Jaime Teevan, Andr\\'es Monroy-Hern\\'andez", "title": "Calendar.help: Designing a Workflow-Based Scheduling Agent with Humans\n  in the Loop", "comments": "10 pages", "journal-ref": null, "doi": "10.1145/3025453.3025780", "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although information workers may complain about meetings, they are an\nessential part of their work life. Consequently, busy people spend a\nsignificant amount of time scheduling meetings. We present Calendar.help, a\nsystem that provides fast, efficient scheduling through structured workflows.\nUsers interact with the system via email, delegating their scheduling needs to\nthe system as if it were a human personal assistant. Common scheduling\nscenarios are broken down using well-defined workflows and completed as a\nseries of microtasks that are automated when possible and executed by a human\notherwise. Unusual scenarios fall back to a trained human assistant who\nexecutes them as unstructured macrotasks. We describe the iterative approach we\nused to develop Calendar.help, and share the lessons learned from scheduling\nthousands of meetings during a year of real-world deployments. Our findings\nprovide insight into how complex information tasks can be broken down into\nrepeatable components that can be executed efficiently to improve productivity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Mar 2017 14:40:31 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Cranshaw", "Justin", ""], ["Elwany", "Emad", ""], ["Newman", "Todd", ""], ["Kocielnik", "Rafal", ""], ["Yu", "Bowen", ""], ["Soni", "Sandeep", ""], ["Teevan", "Jaime", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""]]}, {"id": "1703.08717", "submitter": "Tommy Nilsson", "authors": "Tommy Nilsson", "title": "Smart Spaces: Challenges and Opportunities of BLE-Centered Mobile\n  Systems for Public Environments", "comments": "Nilsson, T., 2014. Smart spaces: challenges and opportunities of\n  BLE-centered mobile systems for public environments. arXiv admin note:\n  substantial text overlap with arXiv:1605.05528", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of mobile computing is currently altering patterns of our\nbehavior to a greater degree than perhaps any other invention. In combination\nwith the introduction of BLE (Bluetooth Low Energy) and similar technologies\nenabling context-awareness, designers are today finding themselves empowered to\nbuild experiences and facilitate interactions with our physical surroundings in\nways not possible before. The aim of this thesis is to present a research\nproject, currently underway at the University of Cambridge, which is dealing\nwith implementation of a BLE system into a museum environment. By assessing the\ntechnology, describing the design decisions as well as presenting a qualitative\nevaluation, this paper seeks to provide insight into some of the challenges and\npossible solutions connected to the process of developing ubiquitous BLE\ncomputing systems for public spaces. The project outcome revealed the potential\nuse of BLE to engage whole new groups of audiences as well as made me argue in\nfavor of a more seamful approach to the design of these systems.\n", "versions": [{"version": "v1", "created": "Sat, 25 Mar 2017 17:27:14 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Nilsson", "Tommy", ""]]}, {"id": "1703.08803", "submitter": "Arnaud Blouin", "authors": "Val\\'eria Lelli and Arnaud Blouin and Benoit Baudry and Fabien Coulon\n  and Olivier Beaudoux", "title": "Automatic Detection of GUI Design Smells: The Case of Blob Listener", "comments": null, "journal-ref": "Proceedings of the 8th ACM SIGCHI Symposium on Engineering\n  Interactive Computing Systems (EICS'16), pp.263-274, 2016", "doi": "10.1145/2933242.2933260", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical User Interfaces (GUIs) intensively rely on event-driven\nprogramming: widgets send GUI events, which capture users' interactions, to\ndedicated objects called controllers. Controllers implement several GUI\nlisteners that handle these events to produce GUI commands. In this work, we\nconducted an empirical study on 13 large Java Swing open-source software\nsystems. We study to what extent the number of GUI commands that a GUI listener\ncan produce has an impact on the change-and fault-proneness of the GUI listener\ncode. We identify a new type of design smell, called Blob listener that\ncharacterizes GUI listeners that can produce more than two GUI commands. We\nshow that 21 % of the analyzed GUI controllers are Blob listeners. We propose a\nsystematic static code analysis procedure that searches for Blob listener that\nwe implement in InspectorGuidget. We conducted experiments on six software\nsystems for which we manually identified 37 instances of Blob listener.\nInspectorGuidget successfully detected 36 Blob listeners out of 37. The results\nexhibit a precision of 97.37 % and a recall of 97.59 %. Finally, we propose\ncoding practices to avoid the use of Blob listeners.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 10:40:21 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Lelli", "Val\u00e9ria", ""], ["Blouin", "Arnaud", ""], ["Baudry", "Benoit", ""], ["Coulon", "Fabien", ""], ["Beaudoux", "Olivier", ""]]}, {"id": "1703.08862", "submitter": "Michael Everett", "authors": "Yu Fan Chen, Michael Everett, Miao Liu, and Jonathan P. How", "title": "Socially Aware Motion Planning with Deep Reinforcement Learning", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robotic vehicles to navigate safely and efficiently in pedestrian-rich\nenvironments, it is important to model subtle human behaviors and navigation\nrules (e.g., passing on the right). However, while instinctive to humans,\nsocially compliant navigation is still difficult to quantify due to the\nstochasticity in people's behaviors. Existing works are mostly focused on using\nfeature-matching techniques to describe and imitate human paths, but often do\nnot generalize well since the feature values can vary from person to person,\nand even run to run. This work notes that while it is challenging to directly\nspecify the details of what to do (precise mechanisms of human navigation), it\nis straightforward to specify what not to do (violations of social norms).\nSpecifically, using deep reinforcement learning, this work develops a\ntime-efficient navigation policy that respects common social norms. The\nproposed method is shown to enable fully autonomous navigation of a robotic\nvehicle moving at human walking speed in an environment with many pedestrians.\n", "versions": [{"version": "v1", "created": "Sun, 26 Mar 2017 19:39:50 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 23:35:25 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Chen", "Yu Fan", ""], ["Everett", "Michael", ""], ["Liu", "Miao", ""], ["How", "Jonathan P.", ""]]}, {"id": "1703.08930", "submitter": "Sarath Sreedharan", "authors": "Tathagata Chakraborti, Sarath Sreedharan, Anagha Kulkarni and Subbarao\n  Kambhampati", "title": "Alternative Modes of Interaction in Proximal Human-in-the-Loop Operation\n  of Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ambiguity and noise in natural language instructions create a significant\nbarrier towards adopting autonomous systems into safety critical workflows\ninvolving humans and machines. In this paper, we propose to build on recent\nadvances in electrophysiological monitoring methods and augmented reality\ntechnologies, to develop alternative modes of communication between humans and\nrobots involved in large-scale proximal collaborative tasks. We will first\nintroduce augmented reality techniques for projecting a robot's intentions to\nits human teammate, who can interact with these cues to engage in real-time\ncollaborative plan execution with the robot. We will then look at how\nelectroencephalographic (EEG) feedback can be used to monitor human response to\nboth discrete events, as well as longer term affective states while execution\nof a plan. These signals can be used by a learning agent, a.k.a an affective\nrobot, to modify its policy. We will present an end-to-end system capable of\ndemonstrating these modalities of interaction. We hope that the proposed system\nwill inspire research in augmenting human-robot interactions with alternative\nforms of communications in the interests of safety, productivity, and fluency\nof teaming, particularly in engineered settings such as the factory floor or\nthe assembly line in the manufacturing industry where the use of such wearables\ncan be enforced.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 05:08:02 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Chakraborti", "Tathagata", ""], ["Sreedharan", "Sarath", ""], ["Kulkarni", "Anagha", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1703.09167", "submitter": "Ioannis Rigas", "authors": "Ioannis Rigas, Lee Friedman, Oleg Komogortsev", "title": "A Study on the Extraction and Analysis of a Large Set of Eye Movement\n  Features during Reading", "comments": "38 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a study on the extraction and analysis of a set of 101\ncategories of eye movement features from three types of eye movement events:\nfixations, saccades, and post-saccadic oscillations. The eye movements were\nrecorded during a reading task. For the categories of features with multiple\ninstances in a recording we extract corresponding feature subtypes by\ncalculating descriptive statistics on the distributions of these instances. A\nunified framework of detailed descriptions and mathematical formulas are\nprovided for the extraction of the feature set. The analysis of feature values\nis performed using a large database of eye movement recordings from a normative\npopulation of 298 subjects. We demonstrate the central tendency and overall\nvariability of feature values over the experimental population, and more\nimportantly, we quantify the test-retest reliability (repeatability) of each\nseparate feature. The described methods and analysis can provide valuable tools\nin fields exploring the eye movements, such as in behavioral studies, attention\nand cognition research, medical research, biometric recognition, and\nhuman-computer interaction.\n", "versions": [{"version": "v1", "created": "Mon, 27 Mar 2017 16:21:26 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Rigas", "Ioannis", ""], ["Friedman", "Lee", ""], ["Komogortsev", "Oleg", ""]]}, {"id": "1703.09468", "submitter": "Manuel Neurauter Mag.", "authors": "Stefan Zugal, Jakob Pinggera, Manuel Neurauter, Thomas Maran, Barbara\n  Weber", "title": "Cheetah Experimental Platform Web 1.0: Cleaning Pupillary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers started using cognitive load in various settings, e.g.,\neducational psychology, cognitive load theory, or human-computer interaction.\nCognitive load characterizes a tasks' demand on the limited information\nprocessing capacity of the brain. The widespread adoption of eye-tracking\ndevices led to increased attention for objectively measuring cognitive load via\npupil dilation. However, this approach requires a standardized data processing\nroutine to reliably measure cognitive load. This technical report presents\nCEP-Web, an open source platform to providing state of the art data processing\nroutines for cleaning pupillary data combined with a graphical user interface,\nenabling the management of studies and subjects. Future developments will\ninclude the support for analyzing the cleaned data as well as support for\nTask-Evoked Pupillary Response (TEPR) studies.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 09:15:08 GMT"}, {"version": "v2", "created": "Sat, 21 Apr 2018 07:12:39 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zugal", "Stefan", ""], ["Pinggera", "Jakob", ""], ["Neurauter", "Manuel", ""], ["Maran", "Thomas", ""], ["Weber", "Barbara", ""]]}, {"id": "1703.09662", "submitter": "Dorna Bandari Ph.D.", "authors": "Dorna Bandari, Shuo Xiang, Jure Leskovec", "title": "Categorizing User Sessions at Pinterest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different users can use a given Internet application in many different ways.\nThe ability to record detailed event logs of user in-application activity\nallows us to discover ways in which the application is being used. This enables\npersonalization and also leads to important insights with actionable business\nand product outcomes.\n  Here we study the problem of user session categorization, where the goal is\nto automatically discover categories/classes of user in-session behavior using\nevent logs, and then consistently categorize each user session into the\ndiscovered classes. We develop a three stage approach which uses clustering to\ndiscover categories of sessions, then builds classifiers to classify new\nsessions into the discovered categories, and finally performs daily\nclassification in a distributed pipeline. An important innovation of our\napproach is selecting a set of events as long-tail features, and replacing them\nwith a new feature that is less sensitive to product experimentation and\nlogging changes. This allows for robust and stable identification of session\ntypes even though the underlying application is constantly changing. We deploy\nthe approach to Pinterest and demonstrate its effectiveness. We discover\ninsights that have consequences for product monetization, growth, and design.\nOur solution classifies millions of user sessions daily and leads to actionable\ninsights.\n", "versions": [{"version": "v1", "created": "Tue, 28 Mar 2017 16:32:56 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 01:29:26 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 06:31:08 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Bandari", "Dorna", ""], ["Xiang", "Shuo", ""], ["Leskovec", "Jure", ""]]}, {"id": "1703.09847", "submitter": "Awanthika Senarath", "authors": "Awanthika Senarath, Nalin A.G. Arachchilage, and Jill Slay", "title": "Designing Privacy for You : A User Centric Approach For Privacy", "comments": "14 pages, HCI International 2017 Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy directly concerns the user as the data owner (data- subject) and\nhence privacy in systems should be implemented in a manner which concerns the\nuser (user-centered). There are many concepts and guidelines that support\ndevelopment of privacy and embedding privacy into systems. However, none of\nthem approaches privacy in a user- centered manner. Through this research we\npropose a framework that would enable developers and designers to grasp privacy\nin a user-centered manner and implement it along with the software development\nlife cycle.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 00:27:01 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 03:50:19 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Senarath", "Awanthika", ""], ["Arachchilage", "Nalin A. G.", ""], ["Slay", "Jill", ""]]}, {"id": "1703.09934", "submitter": "Manuel Aiple", "authors": "Manuel Aiple and Andr\\'e Schiele", "title": "Towards Teleoperation with Human-like Dynamics: Human Use of Elastic\n  Tools", "comments": "6 pages, 13 figures, accepted to the World Haptics Conference 2017", "journal-ref": "2017 IEEE World Haptics Conference (WHC), Munich, Germany, 2017,\n  pp. 171-176", "doi": "10.1109/WHC.2017.7989896", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable stiffness actuators undergo lower peak force in contacts compared to\ntheir rigid counterparts, and are thus safer for human-robot interaction.\nFurthermore, they can store energy in their elastic element and can release it\nlater to achieve human-like dynamic movements. However, it is not clear how to\nintegrate them in teleoperator systems so that they can be controlled\nintuitively by a human.\n  We performed an experiment to study human use of elastic tools to determine\nhow a teleoperator system with an elastic slave would need to be designed. For\nthis, we had 13 untrained participants hammer with an elastic tool under\ndifferent stiffness conditions, asking them to try to find the best timing for\na backward-forward swing motion in order to achieve the strongest impact.\n  We found that the participants generally executed the task efficiently after\na few trials and they converged to very similar solutions. The stiffness\ninfluenced the performance slightly, a stiffness between 2.3 Nm/rad and 4.1\nNm/rad showing the best results.\n  We conclude that humans intuitively know how to efficiently use elastic tools\nfor hammering type tasks. This could facilitate the control of teleoperator\nsystems with elastic slave manipulators for tasks requiring explosive movements\nlike hammering.\n", "versions": [{"version": "v1", "created": "Wed, 29 Mar 2017 08:44:09 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Aiple", "Manuel", ""], ["Schiele", "Andr\u00e9", ""]]}, {"id": "1703.10405", "submitter": "Mengqi Peng", "authors": "Mengqi Peng, Jun Xing, Li-Yi Wei", "title": "Autocomplete 3D Sculpting", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital sculpting is a popular means to create 3D models but remains a\nchallenging task for many users. This can be alleviated by recent advances in\ndata-driven and procedural modeling, albeit bounded by the underlying data and\nprocedures. We propose a 3D sculpting system that assists users in freely\ncreating models without predefined scope. With a brushing interface similar to\ncommon sculpting tools, our system silently records and analyzes users'\nworkflows, and predicts what they might or should do in the future to reduce\ninput labor or enhance output quality. Users can accept, ignore, or modify the\nsuggestions and thus maintain full control and individual style. They can also\nexplicitly select and clone past workflows over output model regions. Our key\nidea is to consider how a model is authored via dynamic workflows in addition\nto what it is shaped in static geometry, for more accurate analysis of user\nintentions and more general synthesis of shape structures. The workflows\ncontain potential repetitions for analysis and synthesis, including user inputs\n(e.g. pen strokes on a pressure sensing tablet), model outputs (e.g. extrusions\non an object surface), and camera viewpoints. We evaluate our method via user\nfeedbacks and authored models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 11:06:02 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Peng", "Mengqi", ""], ["Xing", "Jun", ""], ["Wei", "Li-Yi", ""]]}, {"id": "1703.10579", "submitter": "Lingyu Lyu", "authors": "Lingyu Lyu, Mehmed Kantardzic", "title": "Evaluating Complex Task through Crowdsourcing: Multiple Views Approach", "comments": "8 pages, 13 figures, the paper is accepted by ICCSE 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularity of massive open online courses, grading through\ncrowdsourcing has become a prevalent approach towards large scale classes.\nHowever, for getting grades for complex tasks, which require specific skills\nand efforts for grading, crowdsourcing encounters a restriction of insufficient\nknowledge of the workers from the crowd. Due to knowledge limitation of the\ncrowd graders, grading based on partial perspectives becomes a big challenge\nfor evaluating complex tasks through crowdsourcing. Especially for those tasks\nwhich not only need specific knowledge for grading, but also should be graded\nas a whole instead of being decomposed into smaller and simpler subtasks. We\npropose a framework for grading complex tasks via multiple views, which are\ndifferent grading perspectives defined by experts for the task, to provide\nuniformity. Aggregation algorithm based on graders variances are used to\ncombine the grades for each view. We also detect bias patterns of the graders,\nand debias them regarding each view of the task. Bias pattern determines how\nthe behavior is biased among graders, which is detected by a statistical\ntechnique. The proposed approach is analyzed on a synthetic data set. We show\nthat our model gives more accurate results compared to the grading approaches\nwithout different views and debiasing algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 17:25:47 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["Lyu", "Lingyu", ""], ["Kantardzic", "Mehmed", ""]]}, {"id": "1703.10674", "submitter": "Arnaud Blouin", "authors": "Arnaud Blouin and Val\\'eria Lelli and Benoit Baudry and Fabien Coulon", "title": "User Interface Design Smell: Automatic Detection and Refactoring of Blob\n  Listeners", "comments": "18 pages. arXiv admin note: text overlap with arXiv:1703.08803", "journal-ref": "Information and Software Technology, 2018", "doi": "10.1016/j.infsof.2018.05.005", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User Interfaces (UIs) intensively rely on event-driven programming: widgets\nsend UI events, which capture users' interactions, to dedicated objects called\ncontrollers. Controllers use several UI listeners that handle these events to\nproduce UI commands. First, we reveal the presence of design smells in the code\nthat describes and controls UIs. Second, we demonstrate that specific code\nanalyses are necessary to analyze and refactor UI code, because of its coupling\nwith the rest of the code. We conducted an empirical study on four large Java\nSwing and SWT open-source software systems. We study to what extent the number\nof UI commands that a UI listener can produce has an impact on the change- and\nfault-proneness of the UI listener code. We develop a static code analysis for\ndetecting UI commands in the code. We identify a new type of design smell,\ncalled Blob Listener that characterizes UI listeners that can produce more than\ntwo UI commands. We propose a systematic static code analysis procedure that\nsearches for Blob Listeners that we implement in InspectorGuidget. We conducted\nexperiments on the four software systems for which we manually identified 53\ninstances of Blob Listener. InspectorGuidget successfully detected 52 Blob\nListeners out of 53. The results exhibit a precision of 81.25% and a recall of\n98.11%. We then developed a semi-automatically and behavior-preserving\nrefactoring process to remove Blob Listeners. 49.06% of the 53 Blob Listeners\nwere automatically refactored. Patches for JabRef, and FreeCol have been\naccepted and merged. Discussions with developers of the four software systems\nassess the relevance of the Blob Listener. This work shows that UI code also\nsuffers from design smells that have to be identified and characterized. We\nargue that studies have to be conducted to find other UI design smells and\ntools that analyze UI code must be developed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Mar 2017 21:01:22 GMT"}, {"version": "v2", "created": "Tue, 18 Apr 2017 08:31:05 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 07:36:50 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Blouin", "Arnaud", ""], ["Lelli", "Val\u00e9ria", ""], ["Baudry", "Benoit", ""], ["Coulon", "Fabien", ""]]}]