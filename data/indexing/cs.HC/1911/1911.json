[{"id": "1911.00046", "submitter": "Maryam Arab", "authors": "Thomas D. LaToza, Maryam Arab, Dastyni Loksa, Amy J. Ko", "title": "Explicit Programming Strategies", "comments": "48 pages, 8 figures, To appear in the proceedings of Empirical\n  Software Engineering Journal", "journal-ref": null, "doi": "10.1007/s10664-020-09810-1", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software developers solve a diverse and wide range of problems. While\nsoftware engineering research often focuses on tools to support this problem\nsolving, the strategies that developers use to solve problems are at least as\nimportant. In this paper, we offer a novel approach for enabling developers to\nfollow explicit programming strategies that describe how an expert tackles a\ncommon programming problem. We define explicit programming strategies,\ngrounding our definition in prior work both within software engineering and in\nother professions which have adopted more explicit procedures for problem\nsolving. We then present a novel notation called Roboto and a novel\nStrategyTracker tool that explicitly represents programming strategies and\nframe executing strategies as a collaborative effort between human abilities to\nmake decisions and computer abilities to structure process and persist\ninformation. Ina formative evaluation, 28 software developers of varying\nexpertise completed a design task and a debugging task. We found that, compared\nto developers who are free to choose their strategies, developers gave explicit\nstrategies experienced their work as more organized, systematic, and\npredictable, but also more constrained. Developers using explicit strategies\nwere objectively more successful at the design and debugging tasks. We discuss\nthe implications of Roboto and these findings, envisioning a thriving ecosystem\nof explicit strategies that accelerate and improve developers programming\nproblem solving.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 18:31:03 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 15:03:30 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["LaToza", "Thomas D.", ""], ["Arab", "Maryam", ""], ["Loksa", "Dastyni", ""], ["Ko", "Amy J.", ""]]}, {"id": "1911.00071", "submitter": "Mohammad Eslami", "authors": "Mohammad Eslami, Mahdi Karami, Sedigheh Eslami, Solale Tabarestani,\n  Farah Torkamani-Azar, Christoph Meinel", "title": "SignCol: Open-Source Software for Collecting Sign Language Gestures", "comments": "The paper is presented at ICSESS conference but the published version\n  by them on the IEEE Xplore is impaired and the quality of figures is\n  inappropriate!! This is the preprint version which had appropriate format and\n  figures", "journal-ref": null, "doi": "10.1109/ICSESS.2018.8663952", "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign(ed) languages use gestures, such as hand or head movements, for\ncommunication. Sign language recognition is an assistive technology for\nindividuals with hearing disability and its goal is to improve such\nindividuals' life quality by facilitating their social involvement. Since sign\nlanguages are vastly varied in alphabets, as known as signs, a sign recognition\nsoftware should be capable of handling eight different types of sign\ncombinations, e.g. numbers, letters, words and sentences. Due to the intrinsic\ncomplexity and diversity of symbolic gestures, recognition algorithms need a\ncomprehensive visual dataset to learn by. In this paper, we describe the design\nand implementation of a Microsoft Kinect-based open source software, called\nSignCol, for capturing and saving the gestures used in sign languages. Our work\nsupports a multi-language database and reports the recorded items statistics.\nSignCol can capture and store colored(RGB) frames, depth frames, infrared\nframes, body index frames, coordinate mapped color-body frames, skeleton\ninformation of each frame and camera parameters simultaneously.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2019 19:36:18 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Eslami", "Mohammad", ""], ["Karami", "Mahdi", ""], ["Eslami", "Sedigheh", ""], ["Tabarestani", "Solale", ""], ["Torkamani-Azar", "Farah", ""], ["Meinel", "Christoph", ""]]}, {"id": "1911.00310", "submitter": "Emna Rejaibi", "authors": "Alice Othmani, Daoud Kadoch, Kamil Bentounes, Emna Rejaibi, Romain\n  Alfred, and Abdenour Hadid", "title": "Towards Robust Deep Neural Networks for Affect and Depression\n  Recognition from Speech", "comments": "16 pages, 2 figures, 1 algorithm and 6 tables", "journal-ref": "ICPR CAIHA 2020 workshop", "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Intelligent monitoring systems and affective computing applications have\nemerged in recent years to enhance healthcare. Examples of these applications\ninclude assessment of affective states such as Major Depressive Disorder (MDD).\nMDD describes the constant expression of certain emotions: negative emotions\n(low Valence) and lack of interest (low Arousal). High-performing intelligent\nsystems would enhance MDD diagnosis in its early stages. In this paper, we\npresent a new deep neural network architecture, called EmoAudioNet, for emotion\nand depression recognition from speech. Deep EmoAudioNet learns from the\ntime-frequency representation of the audio signal and the visual representation\nof its spectrum of frequencies. Our model shows very promising results in\npredicting affect and depression. It works similarly or outperforms the\nstate-of-the-art methods according to several evaluation metrics on RECOLA and\non DAIC-WOZ datasets in predicting arousal, valence, and depression. Code of\nEmoAudioNet is publicly available on GitHub:\nhttps://github.com/AliceOTHMANI/EmoAudioNet\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 11:38:58 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 16:23:15 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 08:49:25 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2020 16:17:03 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Othmani", "Alice", ""], ["Kadoch", "Daoud", ""], ["Bentounes", "Kamil", ""], ["Rejaibi", "Emna", ""], ["Alfred", "Romain", ""], ["Hadid", "Abdenour", ""]]}, {"id": "1911.00466", "submitter": "Wieslaw Kopec", "authors": "Wies{\\l}aw Kope\\'c, Marcin Wichrowski, Krzysztof Kalinowski, Anna\n  Jaskulska, Kinga Skorupska, Daniel Cnotkowski, Jakub Tyszka, Agata Popieluch,\n  Anna Voitenkova, Rafa{\\l} Mas{\\l}yk, Piotr Gago, Maciej Krzywicki, Monika\n  Kornacka, Cezary Biele, Pawe{\\l} Kobyli\\'nski, Jaros{\\l}aw Kowalski,\n  Katarzyna Abramczuk, Aldona Zdrodowska, Grzegorz Pochwatko, Jakub Mo\\.zaryn,\n  Krzysztof Marasek", "title": "VR with Older Adults: Participatory Design of a Virtual ATM Training\n  Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we report on a study conducted with a group of older adults in\nwhich they engaged in participatory design workshops to create a VR ATM\ntraining simulation. Based on observation, recordings and the developed VR\napplication we present the results of the workshops and offer considerations\nand recommendations for organizing opportunities for end users, in this case\nolder adults, to directly engage in co-creation of cutting-edge ICT solutions.\nThese include co-designing interfaces and interaction schemes for emerging\ntechnologies like VR and AR. We discuss such aspects as user engagement and\nhardware and software tools suitable for participatory prototyping of VR\napplications. Finally, we present ideas for further research in the area of VR\nparticipatory prototyping with users of various proficiency levels, taking\nsteps towards developing a unified framework for co-design in AR and VR.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 17:08:35 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Kope\u0107", "Wies\u0142aw", ""], ["Wichrowski", "Marcin", ""], ["Kalinowski", "Krzysztof", ""], ["Jaskulska", "Anna", ""], ["Skorupska", "Kinga", ""], ["Cnotkowski", "Daniel", ""], ["Tyszka", "Jakub", ""], ["Popieluch", "Agata", ""], ["Voitenkova", "Anna", ""], ["Mas\u0142yk", "Rafa\u0142", ""], ["Gago", "Piotr", ""], ["Krzywicki", "Maciej", ""], ["Kornacka", "Monika", ""], ["Biele", "Cezary", ""], ["Kobyli\u0144ski", "Pawe\u0142", ""], ["Kowalski", "Jaros\u0142aw", ""], ["Abramczuk", "Katarzyna", ""], ["Zdrodowska", "Aldona", ""], ["Pochwatko", "Grzegorz", ""], ["Mo\u017caryn", "Jakub", ""], ["Marasek", "Krzysztof", ""]]}, {"id": "1911.00523", "submitter": "Chenhao Tan", "authors": "David Atkinson, Kumar Bhargav Srinivasan, Chenhao Tan", "title": "What Gets Echoed? Understanding the \"Pointers\" in Explanations of\n  Persuasive Arguments", "comments": "19 pages, 3 figures, EMNLP 2019, the code and dataset are available\n  at https://chenhaot.com/papers/explanation-pointers.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanations are central to everyday life, and are a topic of growing\ninterest in the AI community. To investigate the process of providing natural\nlanguage explanations, we leverage the dynamics of the /r/ChangeMyView\nsubreddit to build a dataset with 36K naturally occurring explanations of why\nan argument is persuasive. We propose a novel word-level prediction task to\ninvestigate how explanations selectively reuse, or echo, information from what\nis being explained (henceforth, explanandum). We develop features to capture\nthe properties of a word in the explanandum, and show that our proposed\nfeatures not only have relatively strong predictive power on the echoing of a\nword in an explanation, but also enhance neural methods of generating\nexplanations. In particular, while the non-contextual properties of a word\nitself are more valuable for stopwords, the interaction between the constituent\nparts of an explanandum is crucial in predicting the echoing of content words.\nWe also find intriguing patterns of a word being echoed. For example, although\nnouns are generally less likely to be echoed, subjects and objects can,\ndepending on their source, be more likely to be echoed in the explanations.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 18:00:05 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Atkinson", "David", ""], ["Srinivasan", "Kumar Bhargav", ""], ["Tan", "Chenhao", ""]]}, {"id": "1911.00568", "submitter": "Yang Liu", "authors": "Kanit Wongsuphasawat, Yang Liu, Jeffrey Heer", "title": "Goals, Process, and Challenges of Exploratory Data Analysis: An\n  Interview Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do analysis goals and context affect exploratory data analysis (EDA)? To\ninvestigate this question, we conducted semi-structured interviews with 18 data\nanalysts. We characterize common exploration goals: profiling (assessing data\nquality) and discovery (gaining new insights). Though the EDA literature\nprimarily emphasizes discovery, we observe that discovery only reliably occurs\nin the context of open-ended analyses, whereas all participants engage in\nprofiling across all of their analyses. We describe the process and challenges\nof EDA highlighted by our interviews. We find that analysts must perform\nrepetitive tasks (e.g., examine numerous variables), yet they may have limited\ntime or lack domain knowledge to explore data. Analysts also often have to\nconsult other stakeholders and oscillate between exploration and other tasks,\nsuch as acquiring and wrangling additional data. Based on these observations,\nwe identify design opportunities for exploratory analysis tools, such as\naugmenting exploration with automation and guidance.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 19:49:55 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Wongsuphasawat", "Kanit", ""], ["Liu", "Yang", ""], ["Heer", "Jeffrey", ""]]}, {"id": "1911.00665", "submitter": "Kyoko Sugisaki", "authors": "Kyoko Sugisaki", "title": "Chat-Bot-Kit: A web-based tool to simulate text-based interactions\n  between humans and with computers", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe Chat-Bot-Kit, a web-based tool for text-based\nchats that we designed for research purposes in computer-mediated communication\n(CMC). Chat-Bot-Kit enables to carry out language studies on text-based\nreal-time chats for the purpose of research: The generated messages are\nstructured with language performance data such as pause and speed of\nkeyboard-handling and the movement of the mouse. The tool provides two modes of\nchat communications - quasi-synchron and synchron modes - and various typing\nindicators. The tool is also designed to be used in wizard-of-oz studies in\nHuman-Computer Interaction (HCI) and for the evaluation of chatbots (dialogue\nsystems) in Natural Language Processing (NLP).\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 06:44:32 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Sugisaki", "Kyoko", ""]]}, {"id": "1911.00737", "submitter": "Jason Kelly", "authors": "Jason D Kelly, Ashley Petersen, Thomas S Lendvay, Timothy M Kowalewski", "title": "The Effect of Video Playback Speed on Surgeon Technical Skill Perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Finding effective methods of discriminating surgeon technical skill\nis a complex problem to solve computationally. Previous research has shown\nnon-expert crowd evaluations of surgical performances are as accurate as the\ngold standard, expert surgeon review. The aim of this research is to learn\nwhether crowdsourced evaluators give higher ratings of technical skill to video\nof performances with increased playback speed, the use in discriminating skill\nlevels, and if this increase is related to the evaluator consciously being\naware that the video is being manually edited. Methods: A set of ten peg\ntransfer videos (5 novices, 5 experts), were used to evaluate the perceived\nskill of the performers at each video playback speed used (0.4x-3.6x).\nObjective metrics used for measuring technical skill were also computed for\ncomparison by manipulating the corresponding kinematic data of each\nperformance. Two videos of an expert and novice performing dry lab laparoscopic\ntrials of peg transfer tasks were used to obtain evaluations at each playback\nspeed (0.2x-3.0x) of perception of whether a video is played at real-time\nplayback speed or not. Results: We found that while both novices and experts\nhad increased perceived technical skill as the video playback was increased,\nthe amount of increase was significantly greater for experts. Each increase in\nplayback speed by 0.4x was associated with, on average, a 0.72-point increase\nin the GOALS score (95% CI: 0.60-0.84 point increase; p < 0.001) for expert\nvideos and only a 0.24-point increase in the GOALS score (95% CI: 0.13-0.36\npoint increase; p < 0.001) for novice videos. Conclusion: Due to the\ndifferential increase in perceived technical skill due to increased playback\nspeed for experts, the difference between novice and expert skill levels of\nsurgical performances may be more easily discerned by manually increasing the\nvideo playback speed.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 15:31:09 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Kelly", "Jason D", ""], ["Petersen", "Ashley", ""], ["Lendvay", "Thomas S", ""], ["Kowalewski", "Timothy M", ""]]}, {"id": "1911.00879", "submitter": "Beril Sirmacek", "authors": "Sheona M.M.D.P. Sequeira, Beril Sirmacek", "title": "A low-cost real-time 3D imaging system for contactless asthma\n  observation", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asthma is becoming a very serious problem with every passing day, especially\nin children. However, it is very difficult to detect this disorder in them,\nsince the breathing motion of children tends to change when they reach an age\nof 6. This, thus makes it very difficult to monitor their respiratory state\neasily. In this paper, we present a cheap non-contact alternative to the\ncurrent methods that are available. This is using a stereo camera, that\ncaptures a video of the patient breathing at a frame rate of 30Hz. For further\nprocessing, the captured video has to be rectified and converted into a point\ncloud. The obtained point clouds need to be aligned in order to have the output\nwith respect to a common plane. They are then converted into a surface mesh.\nThe depth is further estimated by subtracting every point cloud from the\nreference point cloud (the first frame). The output data, however, when plotted\nwith respect to real time produces a very noisy plot. This is filtered by\ndetermining the signal frequency by taking the Fast Fourier Transform of the\nbreathing signal. The system was tested under 4 different breathing conditions:\ndeep, shallow and normal breathing and while coughing. On its success, it was\ntested with mixed breathing (combination of normal and shallow breathing) and\nwas lastly compared with the output of the expensive 3dMD system. The\ncomparison showed that using the stereo camera, we can reach to similar\nsensitivity for respiratory motion observation. The experimental results show\nthat, the proposed method provides a major step towards development of low-cost\nhome-based observation systems for asthma patients and care-givers.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 12:54:02 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Sequeira", "Sheona M. M. D. P.", ""], ["Sirmacek", "Beril", ""]]}, {"id": "1911.00914", "submitter": "Saturnino Luz", "authors": "Bridget Kane, Jing Su, Saturnino Luz", "title": "Potential Applications of Machine Learning at Multidisciplinary Medical\n  Team Meetings", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While machine learning (ML) systems have produced great advances in several\ndomains, their use in support of complex cooperative work remains a research\nchallenge. A particularly challenging setting, and one that may benefit from ML\nsupport is the work of multidisciplinary medical teams (MDTs). This paper\nfocuses on the activities performed during the multidisciplinary medical team\nmeeting (MDTM), reviewing their main characteristics in light of a longitudinal\nanalysis of several MDTs in a large teaching hospital over a period of ten\nyears and of our development of ML methods to support MDTMs, and identifying\nopportunities and possible pitfalls for the use of ML to support MDTMs.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 15:51:14 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Kane", "Bridget", ""], ["Su", "Jing", ""], ["Luz", "Saturnino", ""]]}, {"id": "1911.00988", "submitter": "Bahador Saket", "authors": "Bahador Saket, Subhajit Das, Bum Chul Kwon, Alex Endert", "title": "Geono-Cluster: Interactive Visual Cluster Analysis for Biologists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biologists often perform clustering analysis to derive meaningful patterns,\nrelationships, and structures from data instances and attributes. Though\nclustering plays a pivotal role in biologists' data exploration, it takes\nnon-trivial efforts for biologists to find the best grouping in their data\nusing existing tools. Visual cluster analysis is currently performed either\nprogrammatically or through menus and dialogues in many tools, which require\nparameter adjustments over several steps of trial-and-error. In this paper, we\nintroduce Geono-Cluster, a novel visual analysis tool designed to support\ncluster analysis for biologists who do not have formal data science training.\nGeono-Cluster enables biologists to apply their domain expertise into\nclustering results by visually demonstrating how their expected clustering\noutputs should look like with a small sample of data instances. The system then\npredicts users' intentions and generates potential clustering results. Our\nstudy follows the design study protocol to derive biologists' tasks and\nrequirements, design the system, and evaluate the system with experts on their\nown dataset. Results of our study with six biologists provide initial evidence\nthat Geono-Cluster enables biologists to create, refine, and evaluate\nclustering results to effectively analyze their data and gain data-driven\ninsights. At the end, we discuss lessons learned and the implications of our\nstudy.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 23:10:31 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Saket", "Bahador", ""], ["Das", "Subhajit", ""], ["Kwon", "Bum Chul", ""], ["Endert", "Alex", ""]]}, {"id": "1911.01003", "submitter": "Saad Alqithami", "authors": "Saad Alqithami, Musaad Alzahrani, Abdulkareem Alzahrani, and Ahmed\n  Mostafa", "title": "Modeling an Augmented Reality Game Environment to Enhance Behavior of\n  ADHD Patients", "comments": "The 12th International Conference on Brain Informatics (BI 2019) ---\n  Brain Science meets Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper generically models an augmented reality game-based environment to\nproject the gamification of an online cognitive behavioral therapist that\nperforms instant measurements for patients with a predefined Attention Deficit\nHyperactivity Disorder (ADHD). ADHD is one of the most common\nneurodevelopmental disorders in which patients have difficulties related to\ninattention, hyperactivity, and impulsivity. Those patients are in need for a\npsychological therapy; the use of cognitive behavioral therapy as a\nfirmly-established treatment is to help in enhancing the way they think and\nbehave. A major limitation in traditional cognitive behavioral therapies is\nthat therapists may face difficulty to optimize patients' neuropsychological\nstimulus following a specified treatment plan, i.e., therapists struggle to\ndraw clear images when stimulating patients' mindset to a point where they\nshould be. Other limitations recognized here include availability,\naccessibility and level-of-experience of the therapists. Therefore, the paper\npresent a gamification model, we term as \"AR-Therapist,\" in order to take\nadvantages of augmented reality developments to engage patients in both real\nand virtual game-based environments. The model provides an on-time measurements\nof patients' progress throughout the treatment sessions which, in result,\novercomes limitations observed in traditional cognitive behavioral therapies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 01:57:13 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Alqithami", "Saad", ""], ["Alzahrani", "Musaad", ""], ["Alzahrani", "Abdulkareem", ""], ["Mostafa", "Ahmed", ""]]}, {"id": "1911.01072", "submitter": "Byung Hyung Kim", "authors": "Byung Hyung Kim and Sungho Jo", "title": "Wearable Affective Life-Log System for Understanding Emotion Dynamics in\n  Daily Life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Past research on recognizing human affect has made use of a variety of\nphysiological sensors in many ways. Nonetheless, how affective dynamics are\ninfluenced in the context of human daily life has not yet been explored. In\nthis work, we present a wearable affective life-log system (ALIS), that is\nrobust as well as easy to use in daily life to detect emotional changes and\ndetermine their cause-and-effect relationship on users' lives. The proposed\nsystem records how a user feels in certain situations during long-term\nactivities with physiological sensors. Based on the long-term monitoring, the\nsystem analyzes how the contexts of the user's life affect his/her emotion\nchanges. Furthermore, real-world experimental results demonstrate that the\nproposed wearable life-log system enables us to build causal structures to find\neffective stress relievers suited to every stressful situation in school life.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 08:35:51 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 20:31:00 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Kim", "Byung Hyung", ""], ["Jo", "Sungho", ""]]}, {"id": "1911.01158", "submitter": "Byung Hyung Kim", "authors": "Byung Hyung Kim and Sungho Jo", "title": "An Affective Situation Labeling System from Psychological Behaviors in\n  Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a computational framework for providing affective labels\nto real-life situations, called A-Situ. We first define an affective situation,\nas a specific arrangement of affective entities relevant to emotion elicitation\nin a situation. Then, the affective situation is represented as a set of labels\nin the valence-arousal emotion space. Based on physiological behaviors in\nresponse to a situation, the proposed framework quantifies the expected emotion\nevoked by the interaction with a stimulus event. The accumulated result in a\nspatiotemporal situation is represented as a polynomial curve called the\naffective curve, which bridges the semantic gap between cognitive and affective\nperception in real-world situations. We show the efficacy of the curve for\nreliable emotion labeling in real-world experiments, respectively concerning 1)\na comparison between the results from our system and existing explicit\nassessments for measuring emotion, 2) physiological distinctiveness in\nemotional states, and 3) physiological characteristics correlated to continuous\nlabels. The efficiency of affective curves to discriminate emotional states is\nevaluated through subject-dependent classification performance using\nbicoherence features to represent discrete affective states in the\nvalence-arousal space. Furthermore, electroencephalography-based statistical\nanalysis revealed the physiological correlates of the affective curves.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 12:32:10 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 20:26:04 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Kim", "Byung Hyung", ""], ["Jo", "Sungho", ""]]}, {"id": "1911.01281", "submitter": "Jie Hua", "authors": "Jie Hua, Chenguang Liu, Tomasz Kalbarczyk, Catherine Wright,\n  Gruia-Catalin Roman, Christine Julien", "title": "rIoT: Enabling Seamless Context-Aware Automation in the Internet of\n  Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in mobile computing capabilities and an increasing number of\nInternet of Things (IoT) devices have enriched the possibilities of the IoT but\nhave also increased the cognitive load required of IoT users. Existing\ncontext-aware systems provide various levels of automation in the IoT. Many of\nthese systems adaptively take decisions on how to provide services based on\nassumptions made a priori. The approaches are difficult to personalize to an\nindividual's dynamic environment, and thus today's smart IoT spaces often\ndemand complex and specialized interactions with the user in order to provide\ntailored services. We propose rIoT, a framework for seamless and personalized\nautomation of human-device interaction in the IoT. rIoT leverages existing\ntechnologies to operate across heterogeneous devices and networks to provide a\none-stop solution for device interaction in the IoT. We show how rIoT exploits\nsimilarities between contexts and employs a decision-tree like method to\nadaptively capture a user's preferences from a small number of interactions\nwith the IoT space. We measure the performance of rIoT on two real-world data\nsets and a real mobile device in terms of accuracy, learning speed, and latency\nin comparison to two state-of-the-art machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 00:07:04 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 16:53:09 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Hua", "Jie", ""], ["Liu", "Chenguang", ""], ["Kalbarczyk", "Tomasz", ""], ["Wright", "Catherine", ""], ["Roman", "Gruia-Catalin", ""], ["Julien", "Christine", ""]]}, {"id": "1911.01318", "submitter": "Michael Segundo Ortiz", "authors": "Michael Segundo Ortz", "title": "Sequential/Spatial, a Survey of Interactive Information Retrieval\n  Methods for Controlled Experimentation and Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey presents studies that investigated non-spatial (sequential) and\nspatial information retrieval systems in parallel during a battery of\ninformation-seeking tasks with respect to user navigational behaviors,\nincidental learning, retrieval performance, cognitive abilities & load, direct\nmanipulation of 2D & 3D interfaces, and satisfaction. I consider how\ninformation theory has contributed to the concepts of foraging, sense-making,\nexploration, and how the applied areas of interactive information retrieval\n(IIR) and cognitive/behavioral psychology have implemented these concepts into\narchitecture, interface design, experimental design, user study, and evaluation\nmethodology.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 16:30:39 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Ortz", "Michael Segundo", ""]]}, {"id": "1911.01371", "submitter": "Giuliano Tortoreto", "authors": "Giuliano Tortoreto, Evgeny A. Stepanov, Alessandra Cervone, Mateusz\n  Dubiel, Giuseppe Riccardi", "title": "Affective Behaviour Analysis of On-line User Interactions: Are On-line\n  Support Groups more Therapeutic than Twitter?", "comments": null, "journal-ref": null, "doi": "10.18653/v1/W19-3211", "report-no": null, "categories": "cs.HC cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increase in the prevalence of mental health problems has coincided with a\ngrowing popularity of health related social networking sites. Regardless of\ntheir therapeutic potential, On-line Support Groups (OSGs) can also have\nnegative effects on patients. In this work we propose a novel methodology to\nautomatically verify the presence of therapeutic factors in social networking\nwebsites by using Natural Language Processing (NLP) techniques. The methodology\nis evaluated on On-line asynchronous multi-party conversations collected from\nan OSG and Twitter. The results of the analysis indicate that therapeutic\nfactors occur more frequently in OSG conversations than in Twitter\nconversations. Moreover, the analysis of OSG conversations reveals that the\nusers of that platform are supportive, and interactions are likely to lead to\nthe improvement of their emotional state. We believe that our method provides a\nstepping stone towards automatic analysis of emotional states of users of\nonline platforms. Possible applications of the method include provision of\nguidelines that highlight potential implications of using such platforms on\nusers' mental health, and/or support in the analysis of their impact on\nspecific individuals.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 17:59:18 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Tortoreto", "Giuliano", ""], ["Stepanov", "Evgeny A.", ""], ["Cervone", "Alessandra", ""], ["Dubiel", "Mateusz", ""], ["Riccardi", "Giuseppe", ""]]}, {"id": "1911.01474", "submitter": "Alborz Rezazadeh Sereshkeh", "authors": "Alborz Rezazadeh Sereshkeh, Gary Leung, Krish Perumal, Caleb Phillips,\n  Minfan Zhang, Afsaneh Fazly, Iqbal Mohomed", "title": "VASTA: A Vision and Language-assisted Smartphone Task Automation System", "comments": "Submitted to ACM IUI'20, 10 figures, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present VASTA, a novel vision and language-assisted Programming By\nDemonstration (PBD) system for smartphone task automation. Development of a\nrobust PBD automation system requires overcoming three key challenges: first,\nhow to make a particular demonstration robust to positional and visual changes\nin the user interface (UI) elements; secondly, how to recognize changes in the\nautomation parameters to make the demonstration as generalizable as possible;\nand thirdly, how to recognize from the user utterance what automation the user\nwishes to carry out. To address the first challenge, VASTA leverages\nstate-of-the-art computer vision techniques, including object detection and\noptical character recognition, to accurately label interactions demonstrated by\na user, without relying on the underlying UI structures. To address the second\nand third challenges, VASTA takes advantage of advanced natural language\nunderstanding algorithms for analyzing the user utterance to trigger the VASTA\nautomation scripts, and to determine the automation parameters for\ngeneralization. We run an initial user study that demonstrates the\neffectiveness of VASTA at clustering user utterances, understanding changes in\nthe automation parameters, detecting desired UI elements, and, most\nimportantly, automating various tasks. A demo video of the system is available\nhere: http://y2u.be/kr2xE-FixjI\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 20:21:32 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Sereshkeh", "Alborz Rezazadeh", ""], ["Leung", "Gary", ""], ["Perumal", "Krish", ""], ["Phillips", "Caleb", ""], ["Zhang", "Minfan", ""], ["Fazly", "Afsaneh", ""], ["Mohomed", "Iqbal", ""]]}, {"id": "1911.01542", "submitter": "Florian Heimerl", "authors": "Florian Heimerl, Christoph Kralj, Torsten M\\\"oller, Michael Gleicher", "title": "embComp: Visual Interactive Comparison of Vector Embeddings", "comments": "published in IEEE Transactions on Visualization and Computer Graphics\n  (2020)", "journal-ref": null, "doi": "10.1109/TVCG.2020.3045918", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces embComp, a novel approach for comparing two embeddings\nthat capture the similarity between objects, such as word and document\nembeddings. We survey scenarios where comparing these embedding spaces is\nuseful. From those scenarios, we derive common tasks, introduce visual analysis\nmethods that support these tasks, and combine them into a comprehensive system.\nOne of embComp's central features are overview visualizations that are based on\nmetrics for measuring differences in the local structure around objects.\nSummarizing these local metrics over the embeddings provides global overviews\nof similarities and differences. Detail views allow comparison of the local\nstructure around selected objects and relating this local information to the\nglobal views. Integrating and connecting all of these components, embComp\nsupports a range of analysis workflows that help understand similarities and\ndifferences between embedding spaces. We assess our approach by applying it in\nseveral use cases, including understanding corpora differences via word vector\nembeddings, and understanding algorithmic differences in generating embeddings.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 00:06:41 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 00:44:34 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Heimerl", "Florian", ""], ["Kralj", "Christoph", ""], ["M\u00f6ller", "Torsten", ""], ["Gleicher", "Michael", ""]]}, {"id": "1911.01753", "submitter": "Hendry F Chame", "authors": "Hendry Ferreira Chame and Jun Tani", "title": "Cognitive and motor compliance in intentional human-robot interaction", "comments": "\"\\c{opyright} 20XX IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works.\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodiment and subjective experience in human-robot interaction are important\naspects to consider when studying both natural cognition and adaptive robotics\nto human environments. Although several researches have focused on nonverbal\ncommunication and collaboration, the study of autonomous physical interaction\nhas obtained less attention. From the perspective of neurorobotics, we\ninvestigate the relation between intentionality, motor compliance, cognitive\ncompliance, and behavior emergence. We propose a variational model inspired by\nthe principles of predictive coding and active inference to study\nintentionality and cognitive compliance, and an intermittent control concept\nfor motor deliberation and compliance based on torque feed-back. Our\nexperiments with the humanoid Torobo portrait interesting perspectives for the\nbio-inspired study of developmental and social processes.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 13:03:10 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 09:44:27 GMT"}, {"version": "v3", "created": "Sat, 16 Nov 2019 12:07:28 GMT"}, {"version": "v4", "created": "Tue, 30 Jun 2020 12:55:44 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chame", "Hendry Ferreira", ""], ["Tani", "Jun", ""]]}, {"id": "1911.02104", "submitter": "Mine Sarac Stroppa", "authors": "Mine Sarac, Allison M. Okamura and Massimiliano Di Luca", "title": "Effects of Haptic Feedback on the Wrist during Virtual Manipulation", "comments": "7 pages, submitted conference paper for IEEE Haptics Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an alternative to thimble devices for the fingertips, we investigate\nhaptic systems that apply stimulus to the user's forearm. Our aim is to provide\neffective interaction with virtual objects, despite the lack of co-location of\nvirtual and real-world contacts, while taking advantage of relatively large\nskin area and ease of mounting on the forearm. We developed prototype wearable\nhaptic devices that provide skin deformation in the normal and shear\ndirections, and performed a user study to determine the effects of haptic\nfeedback in different directions and at different locations near the wrist\nduring virtual manipulation. Participants performed significantly better while\ndiscriminating stiffness values of virtual objects with normal forces compared\nto shear forces. We found no differences in performance or participant\npreferences with regard to stimulus on the dorsal, ventral, or both sides of\nthe forearm.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 21:52:49 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 22:45:43 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Sarac", "Mine", ""], ["Okamura", "Allison M.", ""], ["Di Luca", "Massimiliano", ""]]}, {"id": "1911.02320", "submitter": "Sandy Huang", "authors": "Sandy H. Huang, Isabella Huang, Ravi Pandya, Anca D. Dragan", "title": "Nonverbal Robot Feedback for Human Teachers", "comments": "CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots can learn preferences from human demonstrations, but their success\ndepends on how informative these demonstrations are. Being informative is\nunfortunately very challenging, because during teaching, people typically get\nno transparency into what the robot already knows or has learned so far. In\ncontrast, human students naturally provide a wealth of nonverbal feedback that\nreveals their level of understanding and engagement. In this work, we study how\na robot can similarly provide feedback that is minimally disruptive, yet gives\nhuman teachers a better mental model of the robot learner, and thus enables\nthem to teach more effectively. Our idea is that at any point, the robot can\nindicate what it thinks the correct next action is, shedding light on its\ncurrent estimate of the human's preferences. We analyze how useful this\nfeedback is, both in theory and with two user studies---one with a virtual\ncharacter that tests the feedback itself, and one with a PR2 robot that uses\ngaze as the feedback mechanism. We find that feedback can be useful for\nimproving both the quality of teaching and teachers' understanding of the\nrobot's capability.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 11:26:31 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Huang", "Sandy H.", ""], ["Huang", "Isabella", ""], ["Pandya", "Ravi", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1911.02391", "submitter": "Florian Pfisterer", "authors": "Florian Pfisterer, Janek Thomas, Bernd Bischl", "title": "Towards Human Centered AutoML", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building models from data is an integral part of the majority of data science\nworkflows. While data scientists are often forced to spend the majority of the\ntime available for a given project on data cleaning and exploratory analysis,\nthe time available to practitioners to build actual models from data is often\nrather short due to time constraints for a given project. AutoML systems are\ncurrently rising in popularity, as they can build powerful models without human\noversight. In this position paper, we aim to discuss the impact of the rising\npopularity of such systems and how a user-centered interface for such systems\ncould look like. More importantly, we also want to point out features that are\ncurrently missing in those systems and start to explore better usability of\nsuch systems from a data-scientists perspective.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 13:55:23 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Pfisterer", "Florian", ""], ["Thomas", "Janek", ""], ["Bischl", "Bernd", ""]]}, {"id": "1911.02455", "submitter": "Agathe Balayn", "authors": "Agathe Balayn, Alessandro Bozzon, Zoltan Szlavik", "title": "Unfairness towards subjective opinions in Machine Learning", "comments": "Human-Centered Machine Learning Perspectives (HCML) workshop at the\n  CHI conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the high interest for Machine Learning (ML) in academia and industry,\nmany issues related to the application of ML to real-life problems are yet to\nbe addressed. Here we put forward one limitation which arises from a lack of\nadaptation of ML models and datasets to specific applications. We formalise a\nnew notion of unfairness as exclusion of opinions. We propose ways to quantify\nthis unfairness, and aid understanding its causes through visualisation. These\ninsights into the functioning of ML-based systems hint at methods to mitigate\nunfairness.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 16:11:41 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Balayn", "Agathe", ""], ["Bozzon", "Alessandro", ""], ["Szlavik", "Zoltan", ""]]}, {"id": "1911.02524", "submitter": "Georgiy Platonov", "authors": "Georgiy Platonov, Benjamin Kane, Aaron Gindi, Lenhart K. Schubert", "title": "A Spoken Dialogue System for Spatial Question Answering in a Physical\n  Blocks World", "comments": "9 pages (with references), 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blocks world is a classic toy domain that has long been used to build and\ntest spatial reasoning systems. Despite its relative simplicity, tackling this\ndomain in its full complexity requires the agent to exhibit a rich set of\nfunctional capabilities, ranging from vision to natural language understanding.\nThere is currently a resurgence of interest in solving problems in such limited\ndomains using modern techniques. In this work we tackle spatial question\nanswering in a holistic way, using a vision system, speech input and output\nmediated by an animated avatar, a dialogue system that robustly interprets\nspatial queries, and a constraint solver that derives answers based on 3-D\nspatial modeling. The contributions of this work include a semantic parser that\nmaps spatial questions into logical forms consistent with a general approach to\nmeaning representation, a dialog manager based on a schema representation, and\na constraint solver for spatial questions that provides answers in agreement\nwith human perception. These and other components are integrated into a\nmulti-modal human-computer interaction pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 18:05:13 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Platonov", "Georgiy", ""], ["Kane", "Benjamin", ""], ["Gindi", "Aaron", ""], ["Schubert", "Lenhart K.", ""]]}, {"id": "1911.02637", "submitter": "Jun Rekimoto", "authors": "Jun Rekimoto", "title": "Homo Cyberneticus: The Era of Human-AI Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is submitted and accepted as ACM UIST 2019 Visions. UIST Visions\nis a venue for forward thinking ideas to inspire the community. The goal is not\nto report research but to project and propose new research directions. This\narticle, entitled \"Homo Cyberneticus: The Era of Human-AI Integration\",\nproposes HCI research directions, namely human-augmentation and\nhuman-AI-integration.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2019 12:30:17 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Rekimoto", "Jun", ""]]}, {"id": "1911.02695", "submitter": "Zhou Fang", "authors": "Zhou Fang, Pujana Paliyawan, Ruck Thawonmas and Tomohiro Harada", "title": "Towards An Angry-Birds-like Game System for Promoting Mental Well-being\n  of Players Using Art-Therapy-embedded PCG", "comments": "2019 IEEE 8th Global Conference on Consumer Electronics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an integration of a game system and the art therapy\nconcept for promoting the mental well-being of video game players. In the\nproposed game system, the player plays an Angry-Birds-like game in which levels\nin the game are generated based on images they draw. Upon finishing a game\nlevel, the player also receives positive feedback (praising words) toward their\ndrawing and the generated level from an Art Therapy AI. The proposed system is\ncomposed of three major parts: (1) a drawing recognizer that identifies what\nobject is drawn by the player (Sketcher), (2) a level generator that converts\nthe drawing image into a pixel image, then a set of blocks representing a game\nlevel (PCG AI), and (3) the Art Therapy AI that encourages the player and\nimproves their emotion. This paper describes an overview of the system and\nexplains how its major components function.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 01:06:04 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Fang", "Zhou", ""], ["Paliyawan", "Pujana", ""], ["Thawonmas", "Ruck", ""], ["Harada", "Tomohiro", ""]]}, {"id": "1911.02725", "submitter": "Daphne Chen", "authors": "M. Asif Rana, Daphne Chen, S. Reza Ahmadzadeh, Jacob Williams, Vivian\n  Chu, and Sonia Chernova", "title": "Benchmark for Skill Learning from Demonstration: Impact of User\n  Experience, Task Complexity, and Start Configuration on Performance", "comments": "8 pages, 8 figures, submitted to IEEE Robotics and Automation\n  Letters, videos and website can be found at\n  https://sites.google.com/view/rail-lfd", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we contribute a large-scale study benchmarking the performance\nof multiple motion-based learning from demonstration approaches. Given the\nnumber and diversity of existing methods, it is critical that comprehensive\nempirical studies be performed comparing the relative strengths of these\nlearning techniques. In particular, we evaluate four different approaches based\non properties an end user may desire for real-world tasks. To perform this\nevaluation, we collected data from nine participants, across four different\nmanipulation tasks with varying starting conditions. The resulting\ndemonstrations were used to train 180 task models and evaluated on 720 task\nreproductions on a physical robot. Our results detail how i) complexity of the\ntask, ii) the expertise of the human demonstrator, and iii) the starting\nconfiguration of the robot affect task performance. The collected dataset of\ndemonstrations, robot executions, and evaluations are being made publicly\navailable. Research insights and guidelines are also provided to guide future\nresearch and deployment choices about these approaches.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:43:01 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Rana", "M. Asif", ""], ["Chen", "Daphne", ""], ["Ahmadzadeh", "S. Reza", ""], ["Williams", "Jacob", ""], ["Chu", "Vivian", ""], ["Chernova", "Sonia", ""]]}, {"id": "1911.02726", "submitter": "Hamed Alqahtani Mr", "authors": "Hamed Alqahtani, Charles Z. Liu, Manolya Kavakli-Thorne and Yuzhi Kang", "title": "An Agent-Based Intelligent HCI Information System in Mixed Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a design of agent-based intelligent HCI (iHCI) system\nusing collaborative information for MR to improve user experience and\ninformation security based on context-aware computing. In order to implement\ntarget awareness system, we propose the use of non-parameter stochastic\nadaptive learning and a kernel learning strategy for improving the adaptivity\nof the recognition. The proposed design involves the use of a context-aware\ncomputing strategy to recognize patterns for simulating human awareness and\nprocessing of stereo pattern analysis. It provides a flexible customization\nmethod for scene creation and manipulation. It also enables several types of\nawareness related to the interactive target, user-experience, system\nperformance, confidentiality, and agent identification by applying several\nstrategies, such as context pattern analysis, scalable learning, data-aware\nconfidential computing.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:46:55 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Alqahtani", "Hamed", ""], ["Liu", "Charles Z.", ""], ["Kavakli-Thorne", "Manolya", ""], ["Kang", "Yuzhi", ""]]}, {"id": "1911.02789", "submitter": "Guoxian Yu", "authors": "Jinzheng Tu, Guoxian Yu, Carlotta Domeniconi, Jun Wang, Xiangliang\n  Zhang", "title": "Active Multi-Label Crowd Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is an economic and efficient strategy aimed at collecting\nannotations of data through an online platform. Crowd workers with different\nexpertise are paid for their service, and the task requester usually has a\nlimited budget. How to collect reliable annotations for multi-label data and\nhow to compute the consensus within budget is an interesting and challenging,\nbut rarely studied, problem. In this paper, we propose a novel approach to\naccomplish Active Multi-label Crowd Consensus (AMCC). AMCC accounts for the\ncommonality and individuality of workers, and assumes that workers can be\norganized into different groups. Each group includes a set of workers who share\na similar annotation behavior and label correlations. To achieve an effective\nmulti-label consensus, AMCC models workers' annotations via a linear\ncombination of commonality and individuality, and reduces the impact of\nunreliable workers by assigning smaller weights to the group. To collect\nreliable annotations with reduced cost, AMCC introduces an active crowdsourcing\nlearning strategy that selects sample-label-worker triplets. In a triplet, the\nselected sample and label are the most informative for the consensus model, and\nthe selected worker can reliably annotate the sample with low cost. Our\nexperimental results on multi-label datasets demonstrate the advantages of AMCC\nover state-of-the-art solutions on computing crowd consensus and on reducing\nthe budget by choosing cost-effective triplets.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 07:59:46 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Tu", "Jinzheng", ""], ["Yu", "Guoxian", ""], ["Domeniconi", "Carlotta", ""], ["Wang", "Jun", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "1911.02985", "submitter": "Yoichi Ochiai Prof.", "authors": "Satoshi Hashizume, Amy Koike, Takayuki Hoshi, Yoichi Ochiai", "title": "Sonovortex: Aerial Haptic Layer Rendering by Aerodynamic Vortex and\n  Focused Ultrasound", "comments": "10 pages,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method of rendering aerial haptics that uses an aerodynamic\nvortex and focused ultrasound is presented. Significant research has been\nconducted on haptic applications based on multiple phenomena such as magnetic\nand electric fields, focused ultrasound, and laser plasma. By combining\nmultiple physical quantities; the resolution, distance, and magnitude of force\nare enhanced. To combine multiple tactile technologies, basic experiments on\nresolution and discrimination threshold are required. Separate user studies\nwere conducted using aerodynamic and ultrasonic haptics. Moreover, the\nperception of their superposition, in addition to their resolution, was tested.\nAlthough these fields cause no direct interference, the system enables the\nsimultaneous perception of the tactile feedback of both stimuli. The results of\nthis study are expected to contribute to expanding the expression of aerial\nhaptic displays based on several principles.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 02:05:39 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 05:20:30 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Hashizume", "Satoshi", ""], ["Koike", "Amy", ""], ["Hoshi", "Takayuki", ""], ["Ochiai", "Yoichi", ""]]}, {"id": "1911.03166", "submitter": "Nami Ogawa", "authors": "Rebecca Fribourg, Nami Ogawa, Ludovic Hoyet, Ferran Argelaguet, Takuji\n  Narumi, Michitaka Hirose, Anatole L\\'ecuyer", "title": "Virtual Co-Embodiment: Evaluation of the Sense of Agency while Sharing\n  the Control of a Virtual Body among Two Individuals", "comments": "Accepted for publication in IEEE Transactions on Visualization and\n  Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.2999197", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a concept called ''virtual co-embodiment'', which\nenables a user to share their virtual avatar with another entity (e.g., another\nuser, robot, or autonomous agent). We describe a proof-of-concept in which two\nusers can be immersed from a first-person perspective in a virtual environment\nand can have complementary levels of control (total, partial, or none) over a\nshared avatar. In addition, we conducted an experiment to investigate the\ninfluence of users' level of control over the shared avatar and prior knowledge\nof their actions on the users' sense of agency and motor actions. The results\nshowed that participants are good at estimating their real level of control but\nsignificantly overestimate their sense of agency when they can anticipate the\nmotion of the avatar. Moreover, participants performed similar body motions\nregardless of their real control over the avatar. The results also revealed\nthat the internal dimension of the locus of control, which is a personality\ntrait, is negatively correlated with the user's perceived level of control. The\ncombined results unfold a new range of applications in the fields of\nvirtual-reality-based training and collaborative teleoperation, where users\nwould be able to share their virtual body.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 10:22:00 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 08:48:24 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Fribourg", "Rebecca", ""], ["Ogawa", "Nami", ""], ["Hoyet", "Ludovic", ""], ["Argelaguet", "Ferran", ""], ["Narumi", "Takuji", ""], ["Hirose", "Michitaka", ""], ["L\u00e9cuyer", "Anatole", ""]]}, {"id": "1911.03287", "submitter": "Katerin Romeo-Pakker", "authors": "Katerine Romeo (LITIS), E Pissaloux (UNIROUEN), F Serin", "title": "Accessible tables in digital documents", "comments": "in French", "journal-ref": "CNRIUT'2019 Congr{\\`e}s National de la Recherche des IUT, IUT de\n  Toulon, Jun 2019, Toulon, France. pp.138-140", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessibility of tables on websites for Visually Impaired Persons (VIP) is\nnot optimal with screen readers which are not always effective for the recovery\nof visual information (2D). Actual Web/Multimedia technologies are not taking\nin account the difference of the visual perception with respect to the vocal\nperception which is linear. This paper analyses the difficulties for accessing\nto spatial information with the existing recommendations for the conception of\naccessible (for all) websites. Different solutions to facilitate accessible\ntable creation are presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 14:36:44 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Romeo", "Katerine", "", "LITIS"], ["Pissaloux", "E", "", "UNIROUEN"], ["Serin", "F", ""]]}, {"id": "1911.03598", "submitter": "Lili Yu", "authors": "Lili Yu, Howard Chen, Sida Wang, Tao Lei, Yoav Artzi", "title": "Interactive Classification by Asking Informative Questions", "comments": "Accepted at ACL 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the potential for interaction in natural language classification. We\nadd a limited form of interaction for intent classification, where users\nprovide an initial query using natural language, and the system asks for\nadditional information using binary or multi-choice questions. At each turn,\nour system decides between asking the most informative question or making the\nfinal classification prediction.The simplicity of the model allows for\nbootstrapping of the system without interaction data, instead relying on simple\ncrowdsourcing tasks. We evaluate our approach on two domains, showing the\nbenefit of interaction and the advantage of learning to balance between asking\nadditional questions and making the final prediction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2019 03:05:50 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 19:47:51 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yu", "Lili", ""], ["Chen", "Howard", ""], ["Wang", "Sida", ""], ["Lei", "Tao", ""], ["Artzi", "Yoav", ""]]}, {"id": "1911.03871", "submitter": "Magda Friedjungov\\'a", "authors": "Petra Kubern\\'atov\\'a, Magda Friedjungov\\'a, Max van Duijn", "title": "Constructing a Data Visualization Recommender System", "comments": "Conference paper, DATA 2018, part of the Communications in Computer\n  and Information Science book series", "journal-ref": "Communications in Computer and Information Science 862 (2019) 1-25", "doi": "10.1007/978-3-030-26636-3_1", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing a suitable visualization for data is a difficult task. Current data\nvisualization recommender systems exist to aid in choosing a visualization, yet\nsuffer from issues such as low accessibility and indecisiveness. In this study,\nwe first define a step-by-step guide on how to build a data visualization\nrecommender system. We then use this guide to create a model for a data\nvisualization recommender system for non-experts that aims to resolve the\nissues of current solutions. The result is a question-based model that uses a\ndecision tree and a data visualization classification hierarchy in order to\nrecommend a visualization. Furthermore, it incorporates both task-driven and\ndata characteristics-driven perspectives, whereas existing solutions seem to\neither convolute these or focus on one of the two exclusively. Based on testing\nagainst existing solutions, it is shown that the new model reaches similar\nresults while being simpler, clearer, more versatile, extendable and\ntransparent. The presented guide can be used as a manual for anyone building a\ndata visualization recommender system. The resulting model can be applied in\nthe development of new data visualization software or as part of a learning\ntool.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2019 07:24:39 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Kubern\u00e1tov\u00e1", "Petra", ""], ["Friedjungov\u00e1", "Magda", ""], ["van Duijn", "Max", ""]]}, {"id": "1911.04052", "submitter": "Ajay Mandlekar", "authors": "Ajay Mandlekar, Jonathan Booher, Max Spero, Albert Tung, Anchit Gupta,\n  Yuke Zhu, Animesh Garg, Silvio Savarese, Li Fei-Fei", "title": "Scaling Robot Supervision to Hundreds of Hours with RoboTurk: Robotic\n  Manipulation Dataset through Human Reasoning and Dexterity", "comments": "Published at IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, richly annotated datasets have accelerated progress in fields such as\ncomputer vision and natural language processing, but replicating these\nsuccesses in robotics has been challenging. While prior data collection\nmethodologies such as self-supervision have resulted in large datasets, the\ndata can have poor signal-to-noise ratio. By contrast, previous efforts to\ncollect task demonstrations with humans provide better quality data, but they\ncannot reach the same data magnitude. Furthermore, neither approach places\nguarantees on the diversity of the data collected, in terms of solution\nstrategies. In this work, we leverage and extend the RoboTurk platform to scale\nup data collection for robotic manipulation using remote teleoperation. The\nprimary motivation for our platform is two-fold: (1) to address the\nshortcomings of prior work and increase the total quantity of manipulation data\ncollected through human supervision by an order of magnitude without\nsacrificing the quality of the data and (2) to collect data on challenging\nmanipulation tasks across several operators and observe a diverse set of\nemergent behaviors and solutions. We collected over 111 hours of robot\nmanipulation data across 54 users and 3 challenging manipulation tasks in 1\nweek, resulting in the largest robot dataset collected via remote\nteleoperation. We evaluate the quality of our platform, the diversity of\ndemonstrations in our dataset, and the utility of our dataset via quantitative\nand qualitative analysis. For additional results, supplementary videos, and to\ndownload our dataset, visit http://roboturk.stanford.edu/realrobotdataset .\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 03:13:46 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Mandlekar", "Ajay", ""], ["Booher", "Jonathan", ""], ["Spero", "Max", ""], ["Tung", "Albert", ""], ["Gupta", "Anchit", ""], ["Zhu", "Yuke", ""], ["Garg", "Animesh", ""], ["Savarese", "Silvio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1911.04255", "submitter": "Abhiram Singh", "authors": "Abhiram Singh, Ashwin Gumaste", "title": "Decoding Imagined Speech and Computer Control using Brain Waves", "comments": "Published in the Journal of Neuroscience Methods", "journal-ref": "Journal of Neuroscience Methods, Volume 358, 1 July 2021, 109196", "doi": "10.1016/j.jneumeth.2021.109196", "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore the possibility of decoding Imagined Speech brain\nwaves using machine learning techniques. We propose a covariance matrix of\nElectroencephalogram channels as input features, projection to tangent space of\ncovariance matrices for obtaining vectors from covariance matrices, principal\ncomponent analysis for dimension reduction of vectors, an artificial\nfeed-forward neural network as a classification model and bootstrap aggregation\nfor creating an ensemble of neural network models. After the classification,\ntwo different Finite State Machines are designed that create an interface for\ncontrolling a computer system using an Imagined Speech-based BCI system. The\nproposed approach is able to decode the Imagined Speech signal with a maximum\nmean classification accuracy of 85% on binary classification task of one long\nword and a short word. We also show that our proposed approach is able to\ndifferentiate between imagined speech brain signals and rest state brain\nsignals with maximum mean classification accuracy of 94%. We compared our\nproposed method with other approaches for decoding imagined speech and show\nthat our approach performs equivalent to the state of the art approach on\ndecoding long vs. short words and outperforms it significantly on the other two\ntasks of decoding three short words and three vowels with an average margin of\n11% and 9%, respectively. We also obtain an information transfer rate of\n21-bits-per-minute when using an IS based system to operate a computer. These\nresults show that the proposed approach is able to decode a wide variety of\nimagined speech signals without any human-designed features.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 12:18:36 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 11:41:29 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 13:13:21 GMT"}, {"version": "v4", "created": "Fri, 30 Apr 2021 05:42:49 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Singh", "Abhiram", ""], ["Gumaste", "Ashwin", ""]]}, {"id": "1911.04338", "submitter": "Dongrui Wu", "authors": "Xue Jiang and Xiao Zhang and Dongrui Wu", "title": "Active Learning for Black-Box Adversarial Attacks in EEG-Based\n  Brain-Computer Interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has made significant breakthroughs in many fields, including\nelectroencephalogram (EEG) based brain-computer interfaces (BCIs). However,\ndeep learning models are vulnerable to adversarial attacks, in which\ndeliberately designed small perturbations are added to the benign input samples\nto fool the deep learning model and degrade its performance. This paper\nconsiders transferability-based black-box attacks, where the attacker trains a\nsubstitute model to approximate the target model, and then generates\nadversarial examples from the substitute model to attack the target model.\nLearning a good substitute model is critical to the success of these attacks,\nbut it requires a large number of queries to the target model. We propose a\nnovel framework which uses query synthesis based active learning to improve the\nquery efficiency in training the substitute model. Experiments on three\nconvolutional neural network (CNN) classifiers and three EEG datasets\ndemonstrated that our method can improve the attack success rate with the same\nnumber of queries, or, in other words, our method requires fewer queries to\nachieve a desired attack performance. To our knowledge, this is the first work\nthat integrates active learning and adversarial attacks for EEG-based BCIs.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 15:00:24 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Jiang", "Xue", ""], ["Zhang", "Xiao", ""], ["Wu", "Dongrui", ""]]}, {"id": "1911.04395", "submitter": "Daria Trinitatova", "authors": "Daria Trinitatova and Dzmitry Tsetserukou", "title": "TouchVR: a Wearable Haptic Interface for VR Aimed at Delivering\n  Multi-modal Stimuli at the User's Palm", "comments": "2 pages, Accepted to SIGGRAPH Asia 2019 XR", "journal-ref": null, "doi": "10.1145/3355355.3361896", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TouchVR is a novel wearable haptic interface which can deliver multimodal\ntactile stimuli on the palm by DeltaTouch haptic display and vibrotactile\nfeedback on the fingertips by vibration motors for the Virtual Reality (VR)\nuser. DeltaTouch display is capable of generating 3D force vector at the\ncontact point and presenting multimodal tactile sensation of weight, slippage,\nencounter, softness, and texture. The VR system consists of HTC Vive Pro base\nstations and head-mounted display (HMD), and Leap Motion controller for\ntracking the user's hands motion in VR. The MatrixTouch, BallFeel, and RoboX\napplications have been developed to demonstrate the capabilities of the\nproposed technology. A novel haptic interface can potentially bring a new level\nof immersion of the user in VR and make it more interactive and tangible.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 17:06:18 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Trinitatova", "Daria", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "1911.04446", "submitter": "Ufuk Celikcan", "authors": "Emre Avan, Ufuk Celikcan, Tolga K. Capin, and Hasmet Gurcay", "title": "Enhancing User Experience in Virtual Reality with Radial Basis Function\n  Interpolation Based Stereoscopic Camera Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing a depth-rich Virtual Reality (VR) experience to users without\ncausing discomfort remains to be a challenge with today's commercially\navailable head-mounted displays (HMDs), which enforce strict measures on\nstereoscopic camera parameters for the sake of keeping visual discomfort to a\nminimum. However, these measures often lead to an unimpressive VR experience\nwith shallow depth feeling. We propose the first method ready to be used with\nexisting consumer HMDs for automated stereoscopic camera control in virtual\nenvironments (VEs). Using radial basis function interpolation and projection\nmatrix manipulations, our method makes it possible to significantly enhance\nuser experience in terms of overall perceived depth while maintaining visual\ndiscomfort on a par with the default arrangement. In our implementation, we\nalso introduce the first immersive interface for authoring a unique 3D\nstereoscopic cinematography for any VE to be experienced with consumer HMDs. We\nconducted a user study that demonstrates the benefits of our approach in terms\nof superior picture quality and perceived depth. We also investigated the\neffects of using depth of field (DoF) in combination with our approach and\nobserved that the addition of our DoF implementation was seen as a degraded\nexperience, if not similar.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 18:49:49 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Avan", "Emre", ""], ["Celikcan", "Ufuk", ""], ["Capin", "Tolga K.", ""], ["Gurcay", "Hasmet", ""]]}, {"id": "1911.04638", "submitter": "David Abramov", "authors": "David Abramov, Jasmine Otto, Mahika Dubey, Cassia Artanegara, Pierre\n  Boutillier, Walter Fontana, Angus G. Forbes", "title": "RuleVis: Constructing Patterns and Rules for Rule-Based Models", "comments": "4 pages, 6 figures, presented at IEEE VIS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce RuleVis, a web-based application for defining and editing\n\"correct-by-construction\" executable rules that model biochemical\nfunctionality, which can be used to simulate the behavior of protein-protein\ninteraction networks and other complex systems. Rule-based models involve\nemergent effects based on the interactions between rules, which can vary\nconsiderably with regard to the scale of a model, requiring the user to inspect\nand edit individual rules. RuleVis bridges the graph rewriting and systems\nbiology research communities by providing an external visual representation of\nsalient patterns that experts can use to determine the appropriate level of\ndetail for a particular modeling context. We describe the visualization and\ninteraction features available in RuleVisand provide a detailed example\ndemonstrating how RuleVis can be used to reason about intracellular\ninteractions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 02:24:58 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Abramov", "David", ""], ["Otto", "Jasmine", ""], ["Dubey", "Mahika", ""], ["Artanegara", "Cassia", ""], ["Boutillier", "Pierre", ""], ["Fontana", "Walter", ""], ["Forbes", "Angus G.", ""]]}, {"id": "1911.04643", "submitter": "Matthew Rueben", "authors": "Matthew Rueben, Frank J. Bernieri, Cindy M. Grimm, William D. Smart", "title": "Framing Effects on Privacy Concerns about a Home Telepresence Robot", "comments": "Revised version was later accepted to the 2017 ACM/IEEE International\n  Conference on Human-Robot Interaction (HRI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy-sensitive robotics is an emerging area of HRI research. Judgments\nabout privacy would seem to be context-dependent, but none of the promising\nwork on contextual \"frames\" has focused on privacy concerns. This work studies\nthe impact of contextual \"frames\" on local users' privacy judgments in a home\ntelepresence setting. Our methodology consists of using an online questionnaire\nto collect responses to animated videos of a telepresence robot after framing\npeople with an introductory paragraph.\n  The results of four studies indicate a large effect of manipulating the robot\noperator's identity between a stranger and a close confidante. It also appears\nthat this framing effect persists throughout several videos. These findings\nserve to caution HRI researchers that a change in frame could cause their\nresults to fail to replicate or generalize. We also recommend that robots be\ndesigned to encourage or discourage certain frames.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 02:56:47 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Rueben", "Matthew", ""], ["Bernieri", "Frank J.", ""], ["Grimm", "Cindy M.", ""], ["Smart", "William D.", ""]]}, {"id": "1911.04667", "submitter": "Evgeny Tsykunov", "authors": "Evgeny Tsykunov and Dzmitry Tsetserukou", "title": "WiredSwarm: High Resolution Haptic Feedback Provided by a Swarm of\n  Drones to the User's Fingers for VR interaction", "comments": "ACM VRST 2019 conference", "journal-ref": null, "doi": "10.1145/3359996.3364789", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a concept of a novel interaction strategy for providing rich\nhaptic feedback in Virtual Reality (VR), when each user's finger is connected\nto micro-quadrotor with a wire. Described technology represents the first\nflying wearable haptic interface. The solution potentially is able to deliver\nhigh resolution force feedback to each finger during fine motor interaction in\nVR. The tips of tethers are connected to the centers of quadcopters under their\nbottom. Therefore, flight stability is increasing and the interaction forces\nare becoming stronger which allows to use smaller drones.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 04:25:37 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Tsykunov", "Evgeny", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "1911.04680", "submitter": "Evgeny Tsykunov", "authors": "Evgeny Tsykunov, Roman Ibrahimov, Derek Vasquez, Dzmitry Tsetserukou", "title": "SlingDrone: Mixed Reality System for Pointing and Interaction Using a\n  Single Drone", "comments": "ACM VRST 2019 conference", "journal-ref": null, "doi": "10.1145/3359996.3364271", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose SlingDrone, a novel Mixed Reality interaction paradigm that\nutilizes a micro-quadrotor as both pointing controller and interactive robot\nwith a slingshot motion type. The drone attempts to hover at a given position\nwhile the human pulls it in desired direction using a hand grip and a leash.\nBased on the displacement, a virtual trajectory is defined. To allow for\nintuitive and simple control, we use virtual reality (VR) technology to trace\nthe path of the drone based on the displacement input. The user receives force\nfeedback propagated through the leash. Force feedback from SlingDrone coupled\nwith visualized trajectory in VR creates an intuitive and user friendly\npointing device. When the drone is released, it follows the trajectory that was\nshown in VR. Onboard payload (e.g. magnetic gripper) can perform various\nscenarios for real interaction with the surroundings, e.g. manipulation or\nsensing. Unlike HTC Vive controller, SlingDrone does not require handheld\ndevices, thus it can be used as a standalone pointing technology in VR.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 05:30:24 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Tsykunov", "Evgeny", ""], ["Ibrahimov", "Roman", ""], ["Vasquez", "Derek", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "1911.04787", "submitter": "David Paulus", "authors": "David Paulus, Gerdien de Vries, Bartel Van de Walle", "title": "Effects of data ambiguity and cognitive biases on the interpretability\n  of machine learning models in humanitarian decision making", "comments": "3 pager, 1 figure, AAAI Fall Symposium - AI for Social Good, November\n  7-9, 2019, Arlington, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The effectiveness of machine learning algorithms depends on the quality and\namount of data and the operationalization and interpretation by the human\nanalyst. In humanitarian response, data is often lacking or overburdening, thus\nambiguous, and the time-scarce, volatile, insecure environments of humanitarian\nactivities are likely to inflict cognitive biases. This paper proposes to\nresearch the effects of data ambiguity and cognitive biases on the\ninterpretability of machine learning algorithms in humanitarian decision\nmaking.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 10:50:42 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Paulus", "David", ""], ["de Vries", "Gerdien", ""], ["Van de Walle", "Bartel", ""]]}, {"id": "1911.04794", "submitter": "Kirill Shatilov", "authors": "Kirill A. Shatilov, Dimitris Chatzopoulos, Lik-Hang Lee, Pan Hui", "title": "Emerging Natural User Interfaces in Mobile Computing: A Bottoms-Up\n  Survey", "comments": "33 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile and wearable interfaces and interaction paradigms are highly\nconstrained by the available screen real estate, and the computational and\npower resources. Although there exist many ways of displaying information to\nmobile users, inputting data to a mobile device is, usually, limited to a\nconventional touch based interaction, that distracts users from their ongoing\nactivities. Furthermore, emerging applications, like augmented, mixed and\nvirtual reality (AR/MR/VR), require new types of input methods in order to\ninteract with complex virtual worlds, challenging the traditional techniques of\nHuman-Computer Interaction (HCI). Leveraging of Natural User Interfaces (NUIs),\nas a paradigm of using natural intuitive actions to interact with computing\nsystems, is one of many ways to meet these challenges in mobile computing and\nits modern applications. Brain-Machine Interfaces that enable thought-only\nhands-free interaction, Myoelectric input methods that track body gestures and\ngaze-tracking input interfaces - are the examples of NUIs applicable to mobile\nand wearable interactions. The wide adoption of wearable devices and the\npenetration of mobile technologies, alongside with the growing market of\nAR/MR/VR, motivates the exploration and implementation of new interaction\nparadigms. The concurrent development of bio-signal acquisition techniques and\naccompanying ecosystems offers a useful toolbox to address open challenges. In\nthis survey, we present state-of-the-art bio-signal acquisition methods,\nsummarize and evaluate recent developments in the area of NUIs and outline\npotential application in mobile scenarios. The survey will provide a bottoms-up\noverview starting from (i) underlying biological aspects and signal acquisition\ntechniques, (ii) portable NUI hardware solutions, (iii) NUI-enabled\napplications, as well as (iv) research challenges and open problems.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 11:13:59 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 04:07:17 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Shatilov", "Kirill A.", ""], ["Chatzopoulos", "Dimitris", ""], ["Lee", "Lik-Hang", ""], ["Hui", "Pan", ""]]}, {"id": "1911.04848", "submitter": "Manolis Chiou", "authors": "Manolis Chiou, Nick Hawes, Rustam Stolkin", "title": "Mixed-Initiative variable autonomy for remotely operated mobile robots", "comments": "Submitted for journal publication, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an Expert-guided Mixed-Initiative Control Switcher\n(EMICS) for remotely operated mobile robots. The EMICS enables switching\nbetween different levels of autonomy during task execution initiated by either\nthe human operator and/or the EMICS. The EMICS is evaluated in two disaster\nresponse inspired experiments, one with a simulated robot and test arena, and\none with a real robot in a realistic environment.\n  Analyses from the two experiments provide evidence that: a) Human-Initiative\n(HI) systems outperform systems with single modes of operation, such as pure\nteleoperation, in navigation tasks; b) in the context of the simulated robot\nexperiment, Mixed-Initiative (MI) systems provide improved performance in\nnavigation tasks, improved operator performance in cognitive demanding\nsecondary tasks, and improved operator workload compared to HI. Results also\nreinforce previous human-robot interaction evidence regarding the importance of\nthe operator's personality traits and their trust in the autonomous system.\nLastly, our experiment on a physical robot provides empirical evidence that\nidentify two major challenges for MI control: a) the design of context-aware MI\ncontrol systems; and b) the conflict for control between the robot's MI control\nsystem and the operator. Insights regarding these challenges are discussed and\nways to tackle them are proposed.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 13:48:04 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 11:12:59 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Chiou", "Manolis", ""], ["Hawes", "Nick", ""], ["Stolkin", "Rustam", ""]]}, {"id": "1911.04930", "submitter": "Weiguo Zhou Mr", "authors": "Weiguo Zhou, Xin Jiang, Chen Chen, Sijia Mei, and Yun-Hui Liu", "title": "HMTNet:3D Hand Pose Estimation from Single Depth Image Based on Hand\n  Morphological Topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to the rapid development of CNNs and depth sensors, great progress has\nbeen made in 3D hand pose estimation. Nevertheless, it is still far from being\nsolved for its cluttered circumstance and severe self-occlusion of hand. In\nthis paper, we propose a method that takes advantage of human hand\nmorphological topology (HMT) structure to improve the pose estimation\nperformance. The main contributions of our work can be listed as below.\nFirstly, in order to extract more powerful features, we concatenate original\nand last layer of initial feature extraction module to preserve hand\ninformation better. Next, regression module inspired from hand morphological\ntopology is proposed. In this submodule, we design a tree-like network\nstructure according to hand joints distribution to make use of high order\ndependency of hand joints. Lastly, we conducted sufficient ablation experiments\nto verify our proposed method on each dataset. Experimental results on three\npopular hand pose dataset show superior performance of our method compared with\nthe state-of-the-art methods. On ICVL and NYU dataset, our method outperforms\ngreat improvement over 2D state-of-the-art methods. On MSRA dataset, our method\nachieves comparable accuracy with the state-of-the-art methods. To summarize,\nour method is the most efficient method which can run at 220:7 fps on a single\nGPU compared with approximate accurate methods at present. The code will be\navailable at.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2019 15:26:43 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Zhou", "Weiguo", ""], ["Jiang", "Xin", ""], ["Chen", "Chen", ""], ["Mei", "Sijia", ""], ["Liu", "Yun-Hui", ""]]}, {"id": "1911.05248", "submitter": "Sara Hooker", "authors": "Sara Hooker, Aaron Courville, Gregory Clark, Yann Dauphin, Andrea\n  Frome", "title": "What Do Compressed Deep Neural Networks Forget?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network pruning and quantization techniques have demonstrated it\nis possible to achieve high levels of compression with surprisingly little\ndegradation to test set accuracy. However, this measure of performance conceals\nsignificant differences in how different classes and images are impacted by\nmodel compression techniques. We find that models with radically different\nnumbers of weights have comparable top-line performance metrics but diverge\nconsiderably in behavior on a narrow subset of the dataset. This small subset\nof data points, which we term Pruning Identified Exemplars (PIEs) are\nsystematically more impacted by the introduction of sparsity. Compression\ndisproportionately impacts model performance on the underrepresented long-tail\nof the data distribution. PIEs over-index on atypical or noisy images that are\nfar more challenging for both humans and algorithms to classify. Our work\nprovides intuition into the role of capacity in deep neural networks and the\ntrade-offs incurred by compression. An understanding of this disparate impact\nis critical given the widespread deployment of compressed models in the wild.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 02:02:19 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 18:24:24 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Hooker", "Sara", ""], ["Courville", "Aaron", ""], ["Clark", "Gregory", ""], ["Dauphin", "Yann", ""], ["Frome", "Andrea", ""]]}, {"id": "1911.05305", "submitter": "Muhammad Shihab Rashid", "authors": "Muhammad Shihab Rashid, Zubayet Zaman, Hasan Mahmud, Md. Kamrul Hasan", "title": "Emotion Recognition with Forearm-based Electromyography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electromyography is an unexplored field of study when it comes to alternate\ninput modality while interacting with a computer. However, to make computers\nunderstand human emotions is pivotal in the area of human-computer interaction\nand in assistive technology. Traditional input devices used currently have\nlimitations and restrictions when it comes to express human emotions. The\napplications regarding computers and emotions are vast. In this paper we\nanalyze EMG signals recorded from a low cost MyoSensor and classify them into\ntwo classes - Relaxed and Angry. In order to perform this classification we\nhave created a dataset collected from 10 users, extracted 8 significant\nfeatures and classified them using Support Vector Machine algorithm. We show\nuniquely that forearm-based EMG signal can express emotions. Experimental\nresults show an accuracy of 88.1% after 300 iterations.This shows significant\nopportunities in various fields of computer science such as gaming and\ne-learning tools where EMG signals can be used to detect human emotions and\nmake the system provide feedback based on it. We discuss further applications\nof the method that seeks to expand the range of human-computer interaction\nbeyond the button box.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 05:57:34 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Rashid", "Muhammad Shihab", ""], ["Zaman", "Zubayet", ""], ["Mahmud", "Hasan", ""], ["Hasan", "Md. Kamrul", ""]]}, {"id": "1911.05564", "submitter": "Lawrence Kim", "authors": "Lawrence H. Kim, Sean Follmer", "title": "Interaction with Ubiquitous Robots and Autonomous IoT", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/14", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Robotics have been slowly permeating Internet of Things (IoT) where the\npreviously ubiquitous but static sensors are now given the power to actively\nnavigate the environment and even interact with users. Emergence of these\nubiquitous swarms of robots not only opens up the range of possible\napplications, but also increases the number of elements to study and design\nfor. We do not yet understand how, when, and where these robots should move,\nmanipulate, and touch around people. Through user-centered studies, we aim to\nbetter understand how to best design for interaction with Autonomous IoT or a\nswarm of ubiquitous robots.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 00:06:15 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Kim", "Lawrence H.", ""], ["Follmer", "Sean", ""]]}, {"id": "1911.05629", "submitter": "Dennis N\\'u\\~nez Fern\\'andez", "authors": "Dennis N\\'u\\~nez-Fern\\'andez, Franklin Porras-Barrientos, Macarena\n  Vittet-Mondo\\~nedo, Robert H. Gilman, Mirko Zimic", "title": "Prediction of gaze direction using Convolutional Neural Networks for\n  Autism diagnosis", "comments": "LatinX in AI Research at NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism is a developmental disorder that affects social interaction and\ncommunication of children. The gold standard diagnostic tools are very\ndifficult to use and time consuming. However, diagnostic could be deduced from\nchild gaze preferences by looking a video with social and abstract scenes. In\nthis work, we propose an algorithm based on convolutional neural networks to\npredict gaze direction for a fast and effective autism diagnosis. Early results\nshow that our algorithm achieves real-time response and robust high accuracy\nfor prediction of gaze direction.\n", "versions": [{"version": "v1", "created": "Fri, 25 Oct 2019 15:06:56 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["N\u00fa\u00f1ez-Fern\u00e1ndez", "Dennis", ""], ["Porras-Barrientos", "Franklin", ""], ["Vittet-Mondo\u00f1edo", "Macarena", ""], ["Gilman", "Robert H.", ""], ["Zimic", "Mirko", ""]]}, {"id": "1911.05661", "submitter": "Dongdong Zhang", "authors": "Dongdong Zhang, Dong Cao, Haibo Chen", "title": "Deep Learning Decoding of Mental State in Non-invasive Brain Computer\n  Interface", "comments": "5 pages, 2 figures, published in AIIPCC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain computer interface (BCI) has been popular as a key approach to monitor\nour brains recent year. Mental states monitoring is one of the most important\nBCI applications and becomes increasingly accessible. However, the mental state\nprediction accuracy and generality through encephalogram (EEG) are not good\nenough for everyday use. Here in this paper we present a deep learning-based\nEEG decoding method to read mental states. We propose a novel 1D convolutional\nneural network with different filter lengths to capture different frequency\nbands information. To improve the prediction accuracy, we also used a\nresnet-like structure to train a relatively deep convolutional neural network\nto promote feature extraction. Compared with traditional ways of predictions\nsuch as KNN and SVM, we achieved a significantly better result with an accuracy\nof 96.40%. Also, in contrast with some already published open source deep\nneural network structures, our methods achieved the state of art prediction\naccuracy on a mental state recognition dataset. Our results demonstrate using\nonly 1D convolution could extract the features of EEG and the possibility of\nmental state prediction using portable EEG devices.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 01:25:51 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Zhang", "Dongdong", ""], ["Cao", "Dong", ""], ["Chen", "Haibo", ""]]}, {"id": "1911.05683", "submitter": "Leon Gatys", "authors": "Jonas Rauber, Emily B. Fox, Leon A. Gatys", "title": "Modeling patterns of smartphone usage and their relationship to\n  cognitive health", "comments": "Machine Learning for Health (ML4H) at NeurIPS 2019 - Extended\n  Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of smartphone usage in many people's lives make it a rich source\nof information about a person's mental and cognitive state. In this work we\nanalyze 12 weeks of phone usage data from 113 older adults, 31 with diagnosed\ncognitive impairment and 82 without. We develop structured models of users'\nsmartphone interactions to reveal differences in phone usage patterns between\npeople with and without cognitive impairment. In particular, we focus on\ninferring specific types of phone usage sessions that are predictive of\ncognitive impairment. Our model achieves an AUROC of 0.79 when discriminating\nbetween healthy and symptomatic subjects, and its interpretability enables\nnovel insights into which aspects of phone usage strongly relate with cognitive\nhealth in our dataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 18:04:18 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Rauber", "Jonas", ""], ["Fox", "Emily B.", ""], ["Gatys", "Leon A.", ""]]}, {"id": "1911.05824", "submitter": "Baichen Li", "authors": "Baichen Li, Scott R. Downen, Quan Dong, Nam Tran, Maxine LeSaux,\n  Andrew C. Meltzer, Zhenyu Li", "title": "A Discreet Wearable IoT Sensor for Continuous Transdermal Alcohol\n  Monitoring -- Challenges and Opportunities", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.med-ph q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive continuous alcohol monitoring has potential applications in both\npopulation research and in clinical management of acute alcohol intoxication or\nchronic alcoholism. Current wearable monitors based on transdermal alcohol\ncontent (TAC) sensing are relatively bulky and have limited quantification\naccuracy. Here we describe the development of a discreet wearable transdermal\nalcohol (TAC) sensor in the form of a wristband or armband. This novel sensor\ncan detect vapor-phase alcohol in perspiration from 0.09 ppm (equivalent to\n0.09 mg/dL sweat alcohol concentration at 25 {\\deg}C under Henry's Law\nequilibrium) to over 500 ppm at one-minute time resolution. The TAC sensor is\npowered by a 110 mAh lithium battery that lasts for over 7 days. In addition,\nthe sensor can function as a medical \"internet-of-things\" (IoT) device by\nconnecting to an Android smartphone gateway via Bluetooth Low Energy (BLE) and\nupload data to a cloud informatics system. Such wearable IoT sensors may enable\nlarge-scale alcohol-related research and personalized management. We also\npresent evidence suggesting a hypothesis that perspiration rate is the dominant\nfactor leading to TAC measurement variabilities, which may inform more\nreproducible and accurate TAC sensor designs in the future.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 21:47:42 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Li", "Baichen", ""], ["Downen", "Scott R.", ""], ["Dong", "Quan", ""], ["Tran", "Nam", ""], ["LeSaux", "Maxine", ""], ["Meltzer", "Andrew C.", ""], ["Li", "Zhenyu", ""]]}, {"id": "1911.05849", "submitter": "Juan Esteban Heredia Mena", "authors": "Juan Heredia, Jonathan Tirado, Vladislav Panov, Miguel Altamirano,\n  Youcef Kamal-Toumi, Dzmitry Tsetserukou", "title": "RecyGlide : A Forearm-worn Multi-modal Haptic Display aimed to Improve\n  User VR Immersion", "comments": "ACM VRST 2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haptic devices have been employed to immerse users in VR environments. In\nparticular, hand and finger haptic devices have been deeply developed. However,\nthis type of device occlude the hand detection by some tracking systems, or in\nother tracking systems, it is uncomfortable for the users to wear two-hand\ndevices (haptic and tracking device). We introduce RecyGlide, which is a novel\nwearable forearm multimodal display at the forearm. The RecyGlide is composed\nof inverted five-bar linkages and vibration motors. The device provides\nmultimodal tactile feedback such as slippage, a force vector, pressure, and\nvibration. We tested the discrimination ability of monomodal and multimodal\nstimuli patterns in the forearm and confirmed that the multimodal stimuli\npatterns are more recognizable. This haptic device was used in VR applications,\nand we proved that it enhances VR experience and makes it more interactive.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 22:48:57 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 07:13:50 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Heredia", "Juan", ""], ["Tirado", "Jonathan", ""], ["Panov", "Vladislav", ""], ["Altamirano", "Miguel", ""], ["Kamal-Toumi", "Youcef", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "1911.05996", "submitter": "Mohammad Malekzadeh", "authors": "Mohammad Malekzadeh, Richard G. Clegg, Andrea Cavallaro, Hamed Haddadi", "title": "Privacy and Utility Preserving Sensor-Data Transformations", "comments": "Accepted to appear in Pervasive and Mobile computing (PMC) Journal,\n  Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitive inferences and user re-identification are major threats to privacy\nwhen raw sensor data from wearable or portable devices are shared with\ncloud-assisted applications. To mitigate these threats, we propose mechanisms\nto transform sensor data before sharing them with applications running on\nusers' devices. These transformations aim at eliminating patterns that can be\nused for user re-identification or for inferring potentially sensitive\nactivities, while introducing a minor utility loss for the target application\n(or task). We show that, on gesture and activity recognition tasks, we can\nprevent inference of potentially sensitive activities while keeping the\nreduction in recognition accuracy of non-sensitive activities to less than 5\npercentage points. We also show that we can reduce the accuracy of user\nre-identification and of the potential inference of gender to the level of a\nrandom guess, while keeping the accuracy of activity recognition comparable to\nthat obtained on the original data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 08:47:29 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Malekzadeh", "Mohammad", ""], ["Clegg", "Richard G.", ""], ["Cavallaro", "Andrea", ""], ["Haddadi", "Hamed", ""]]}, {"id": "1911.06408", "submitter": "Mine Sarac Stroppa", "authors": "Mine Sarac, and Massimiliano Solazzi, and Antonio Frisoli", "title": "Design Requirements of Generic Hand Exoskeletons and Survey of Hand\n  Exoskeletons for Rehabilitation, Assistive or Haptic Use", "comments": "15 pages", "journal-ref": "IEEE Transactions on Haptics, 2019", "doi": "10.1109/TOH.2019.2924881", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current hand exoskeletons have been designed specifically for\nrehabilitation, assistive or haptic applications to simplify the design\nrequirements. Clinical studies on post-stroke rehabilitation have shown that\nadapting assistive or haptic applications into physical therapy sessions\nsignificantly improves the motor learning and treatment process. The recent\ntechnology can lead to the creation of generic hand exoskeletons that are\napplication-agnostic. In this paper, our motivation is to create guidelines and\nbest practices for generic exoskeletons by reviewing the literature of current\ndevices. First, we describe each application and briefly explain their design\nrequirements, and then list the design selections to achieve these\nrequirements. Then, we detail each selection by investigating the existing\nexoskeletons based on their design choices, and by highlighting their impact on\napplication types. With the motivation of creating efficient generic\nexoskeletons in the future, we finally summarize the best practices in the\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 22:42:35 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Sarac", "Mine", ""], ["Solazzi", "Massimiliano", ""], ["Frisoli", "Antonio", ""]]}, {"id": "1911.06541", "submitter": "Jacek Matulewski", "authors": "J. Matulewski, B. Ba{\\l}aj, I. Mo\\'scichowska, A. Ignaczewska, R.\n  Linowiecki, J. Dreszer, W. Duch", "title": "The Markup Language for Designing Gaze Controlled Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaze Interaction Markup Language (GIML) is presented, which is new\nlanguage for designing the gaze-controlled application and psychological\nexperiments (also by non-programmers).\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 09:50:43 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 01:53:18 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Matulewski", "J.", ""], ["Ba\u0142aj", "B.", ""], ["Mo\u015bcichowska", "I.", ""], ["Ignaczewska", "A.", ""], ["Linowiecki", "R.", ""], ["Dreszer", "J.", ""], ["Duch", "W.", ""]]}, {"id": "1911.06607", "submitter": "Charith Perera", "authors": "Yasar Majib, Charith Perera", "title": "Context Aware Family Dynamics based Internet of Things Access Control\n  Towards Better Child Safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, children are increasingly connected to the Internet and consume\ncontent and services through various means. It has been a challenge for less\ntech-savvy parents to protect children from harmful content and services.\nInternet of Things (IoT) has made the situation much worse as IoT devices allow\nchildren to connect to the Internet in novel ways (e.g., connected\nrefrigerators, TVs, and so on). In this paper, we propose mySafeHome, an\napproach which utilises family dynamics to provide a more natural, and\nintuitive access control mechanism to protect children from harmful content and\nservices in the context of IoT. In mySafeHome, access control dynamically\nadapts based on the physical distance between family members. For example, a\nparticular type of content can only be consumed, through TV, by children if the\nparents are in the same room (or hearing distance). mySafeHome allows parents\nto assess a given content by themselves. Our approach also aims to create\ngranular levels of access control (e.g., block / limit certain content,\nfeatures, services, on certain devices when the parents are not in the\nvicinity). We developed a prototype using OpenHAB and several smart home\ndevices to demonstrate the proposed approach. We believe that our approach also\nfacilitates the creation of better relationships between family members. A demo\ncan be viewed here: http://safehome.technology/demo.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 19:44:42 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Majib", "Yasar", ""], ["Perera", "Charith", ""]]}, {"id": "1911.06727", "submitter": "Katerine Romeo", "authors": "Katerine Romeo (LITIS), Edwige Pissaloux (LITIS), Fr\\'ed\\'eric Serin\n  (LITIS)", "title": "Accessibility to textual and visual information on websites for visually\n  impaired persons", "comments": "in French. Handicap 2018, Jun 2018, Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to textual and visual information for visually impaired persons\nbecomes very difficult with screen readers which are not adapted to different\nwebsites.This paper analyses the use of different technologies for access\ndigital content and to establish some ameliorations to the existing\nrecommendations to accessible website conception for all. The preliminary\nevaluation results with visually impaired people of our website ACCESSPACE\nwhich is constructed with the existing recommendations, confirm the project's\nrelevance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:23:46 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Romeo", "Katerine", "", "LITIS"], ["Pissaloux", "Edwige", "", "LITIS"], ["Serin", "Fr\u00e9d\u00e9ric", "", "LITIS"]]}, {"id": "1911.06747", "submitter": "Maryam Fazel-Zarandi", "authors": "Maryam Fazel-Zarandi, Sampat Biswas, Ryan Summers, Ahmed Elmalt, Andy\n  McCraw, Michael McPhilips, John Peach", "title": "Towards Personalized Dialog Policies for Conversational Skill Discovery", "comments": "The 3rd Conversational AI workshop - today's practice and tomorrow's\n  potential", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many businesses and consumers are extending the capabilities of voice-based\nservices such as Amazon Alexa, Google Home, Microsoft Cortana, and Apple Siri\nto create custom voice experiences (also known as skills). As the number of\nthese experiences increases, a key problem is the discovery of skills that can\nbe used to address a user's request. In this paper, we focus on conversational\nskill discovery and present a conversational agent which engages in a dialog\nwith users to help them find the skills that fulfill their needs. To this end,\nwe start with a rule-based agent and improve it by using reinforcement\nlearning. In this way, we enable the agent to adapt to different user\nattributes and conversational styles as it interacts with users. We evaluate\nour approach in a real production setting by deploying the agent to interact\nwith real users, and show the effectiveness of the conversational agent in\nhelping users find the skills that serve their request.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:56:17 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Fazel-Zarandi", "Maryam", ""], ["Biswas", "Sampat", ""], ["Summers", "Ryan", ""], ["Elmalt", "Ahmed", ""], ["McCraw", "Andy", ""], ["McPhilips", "Michael", ""], ["Peach", "John", ""]]}, {"id": "1911.06877", "submitter": "Zhenyi He", "authors": "Zhenyi He, Karl Rosenberg, Ken Perlin", "title": "Exploring Configurations for Multi-user Communication in Virtual Reality", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) enables users to collaborate while exploring scenarios\nnot realizable in the physical world. We propose CollabVR, a distributed\nmulti-user collaboration environment, to explore how digital content improves\nexpression and understanding of ideas among groups. To achieve this, we\ndesigned and examined three possible configurations for participants and shared\nmanipulable objects. In configuration (1), participants stand side-by-side. In\n(2), participants are positioned across from each other, mirrored face-to-face.\nIn (3), called \"eyes-free,\" participants stand side-by-side looking at a shared\ndisplay, and draw upon a horizontal surface. We also explored a \"telepathy\"\nmode, in which participants could see from each other's point of view. We\nimplemented \"3DSketch\" visual objects for participants to manipulate and move\nbetween virtual content boards in the environment. To evaluate the system, we\nconducted a study in which four people at a time used each of the three\nconfigurations to cooperate and communicate ideas with each other. We have\nprovided experimental results and interview responses.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 21:15:46 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["He", "Zhenyi", ""], ["Rosenberg", "Karl", ""], ["Perlin", "Ken", ""]]}, {"id": "1911.07179", "submitter": "Shibo Zhang", "authors": "Shibo Zhang, Yuqi Zhao, Dzung Tri Nguyen, Runsheng Xu, Sougata Sen,\n  Josiah Hester, Nabil Alshurafa", "title": "NeckSense: A Multi-Sensor Necklace for Detecting Eating Activities in\n  Free-Living Conditions", "comments": "21 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design, implementation, and evaluation of a multi-sensor\nlow-power necklace 'NeckSense' for automatically and unobtrusively capturing\nfine-grained information about an individual's eating activity and eating\nepisodes, across an entire waking-day in a naturalistic setting. The NeckSense\nfuses and classifies the proximity of the necklace from the chin, the ambient\nlight, the Lean Forward Angle, and the energy signals to determine chewing\nsequences, a building block of the eating activity. It then clusters the\nidentified chewing sequences to determine eating episodes. We tested NeckSense\nwith 11 obese and 9 non-obese participants across two studies, where we\ncollected more than 470 hours of data in naturalistic setting. Our result\ndemonstrates that NeckSense enables reliable eating-detection for an entire\nwaking-day, even in free-living environments. Overall, our system achieves an\nF1-score of 81.6% in detecting eating episodes in an exploratory study.\nMoreover, our system can achieve a F1-score of 77.1% for episodes even in an\nall-day-around free-living setting. With more than 15.8 hours of battery-life\nNeckSense will allow researchers and dietitians to better understand natural\nchewing and eating behaviors, and also enable real-time interventions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 08:13:11 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 20:41:33 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Zhang", "Shibo", ""], ["Zhao", "Yuqi", ""], ["Nguyen", "Dzung Tri", ""], ["Xu", "Runsheng", ""], ["Sen", "Sougata", ""], ["Hester", "Josiah", ""], ["Alshurafa", "Nabil", ""]]}, {"id": "1911.07328", "submitter": "Christine Bauer", "authors": "Christine Bauer", "title": "The Potential of the Confluence of Theoretical and Algorithmic Modeling\n  in Music Recommendation", "comments": "6 pages; 1st ACM CHI 2019 Workshop on Computational Modeling in\n  Human-Computer Interaction; workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The task of a music recommender system is to predict what music item a\nparticular user would like to listen to next. This position paper discusses the\nmain challenges of the music preference prediction task: the lack of\ninformation on the many contextual factors influencing a user's music\npreferences in existing open datasets, the lack of clarity of what the right\nchoice of music is and whether a right choice exists at all; the multitude of\ncriteria (beyond accuracy) that have to be met for a \"good\" music item\nrecommendation; and the need for explanations on relationships to identify (and\npotentially counteract) unwanted biases in recommendation approaches. The paper\nsubstantiates the position that the confluence of theoretical modeling (which\nseeks to explain behaviors) and algorithmic modeling (which seeks to predict\nbehaviors) seems to be an effective avenue to take in computational modeling\nfor music recommender systems.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 20:23:41 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Bauer", "Christine", ""]]}, {"id": "1911.07340", "submitter": "Sina Masnadi", "authors": "Sina Masnadi, Joseph J. LaViola Jr., Xiaofan Zhu, Karthik Desingh and\n  Odest Chadwicke Jenkins", "title": "A Sketch-Based System for Human-Guided Constrained Object Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an easy to use sketch-based interface to extract\ngeometries and generate affordance files from 3D point clouds for robot-object\ninteraction tasks. Using our system, even novice users can perform robot task\nplanning by employing such sketch tools. Our focus in this paper is employing\nhuman-in-the-loop approach to assist in the generation of more accurate\naffordance templates and guidance of robot through the task execution process.\nSince we do not employ any unsupervised learning to generate affordance\ntemplates, our system performs much faster and is more versatile for template\ngeneration. Our system is based on the extraction of geometries for generalized\ncylindrical and cuboid shapes, after extracting the geometries, affordances are\ngenerated for objects by applying simple sketches. We evaluated our technique\nby asking users to define affordances by employing sketches on the 3D scenes of\na door handle and a drawer handle and used the resulting extracted affordance\ntemplate files to perform the tasks of turning a door handle and opening a\ndrawer by the robot.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 21:11:56 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 18:25:18 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Masnadi", "Sina", ""], ["LaViola", "Joseph J.", "Jr."], ["Zhu", "Xiaofan", ""], ["Desingh", "Karthik", ""], ["Jenkins", "Odest Chadwicke", ""]]}, {"id": "1911.07380", "submitter": "Elif Surer", "authors": "Elif Surer, Mustafa Erkayao\\u{g}lu, Zeynep Nur \\\"Ozt\\\"urk, Furkan\n  Y\\\"ucel, Emin Alp B{\\i}y{\\i}k, Burak Altan, B\\\"u\\c{s}ra \\c{S}enderin, Zeliha\n  O\\u{g}uz, Servet G\\\"urer, H. \\c{S}ebnem D\\\"uzg\\\"un", "title": "Developing a Scenario-Based Video Game Generation Framework: Preliminary\n  Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emergency training and planning provide structured curricula, rule-based\naction items, and interdisciplinary collaborative entities to imitate and teach\nreal-life tasks. This rule-based structure enables the curricula to be\ntransferred into other systematic learning platforms such as serious games\n---games that have additional purposes rather than only entertainment. Serious\ngames aim to educate, cure, and plan several real-life tasks and circumstances\nin an interactive, efficient, and user-friendly way. Although emergency\ntraining includes these highly structured and repetitive action responses, a\ngeneral framework to map the training scenarios' actions, roles, and\ncollaborative structures to game mechanics and game dialogues, is still not\navailable. To address this issue, in this study, a scenario-based game\ngenerator, which maps domain-oriented tasks to game rules and game mechanics,\nwas developed. Also, two serious games (i.e., Hospital game and BioGarden game)\naddressing the training mechanisms of Chemical, Biological, Radiological,\nNuclear, and Explosives (CBRNe) domain, were developed by both the game\ndevelopers and the scenario-based game generator for comparative analysis. The\nresults show that although the game generator uses higher CPU time, memory\nusage, and rendering time, it highly outperforms the game development pipeline\nperformance of the developers. Thus, this study is an initial attempt of a game\ngenerator which bridges the CBRNe practitioners and game developers to\ntransform real-life training scenarios into video games efficiently and\nquickly.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:43:06 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Surer", "Elif", ""], ["Erkayao\u011flu", "Mustafa", ""], ["\u00d6zt\u00fcrk", "Zeynep Nur", ""], ["Y\u00fccel", "Furkan", ""], ["B\u0131y\u0131k", "Emin Alp", ""], ["Altan", "Burak", ""], ["\u015eenderin", "B\u00fc\u015fra", ""], ["O\u011fuz", "Zeliha", ""], ["G\u00fcrer", "Servet", ""], ["D\u00fczg\u00fcn", "H. \u015eebnem", ""]]}, {"id": "1911.07447", "submitter": "Klaus Mueller", "authors": "Bing Wang, Klaus Mueller", "title": "Subspace Shapes: Enhancing High-Dimensional Subspace Structures via\n  Ambient Occlusion Shading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We test the hypothesis whether transforming a data matrix into a 3D shaded\nsurface or even a volumetric display can be more appealing to humans than a\nscatterplot since it makes direct use of the innate 3D scene understanding\ncapabilities of the human visual system. We also test whether 3D shaded\ndisplays can add a significant amount of information to the visualization of\nhigh-dimensional data, especially when enhanced with proper tools to navigate\nthe various 3D subspaces. Our experiments suggest that mainstream users prefer\nshaded displays over scatterplots for visual cluster analysis tasks after\nreceiving training for both. Our experiments also provide evidence that 3D\ndisplays can better communicate spatial relationships, size, and shape of\nclusters.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 06:27:48 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Wang", "Bing", ""], ["Mueller", "Klaus", ""]]}, {"id": "1911.07565", "submitter": "Ronivon Dias Silva", "authors": "Ronivon Dias, Pedro Neto, Irvayne Ibiapina, Guilherme Avelino e Otavio\n  Castro", "title": "Effects of Visualizing Technical Debts on a Software Maintenance Project", "comments": "in Portuguese, Aceito no XVIII Brazilian Symposium on Software\n  Quality (SBQS'19), October 28-November 1, 2019, Fortaleza, Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technical debt (TD) metaphor is widely used to encapsulate numerous\nsoftware quality problems. She describes the trade-off between the short term\nbenefit of taking a shortcut during the design or implementation phase of a\nsoftware product (for example, in order to meet a deadline) and the long term\nconsequences of taking said shortcut, which may affect the quality of the\nsoftware product. TDs must be managed to guarantee the software quality and\nalso reduce its maintenance and evolution costs. However, the tools for TD\ndetection usually provide results only considering the files perspective (class\nand methods), that is not usual during the project management. In this work, a\ntechnique is proposed to identify/visualize TD on a new perspective: software\nfeatures. The proposed technique adopts Mining Software Repository (MRS) tools\nto identify the software features and after the technical debts that affect\nthese features. Additionally, we also proposed an approach to support\nmaintenance tasks guided by TD visualization at the feature level aiming to\nevaluate its applicability on real software projects. The results indicate that\nthe approach can be useful to decrease the existent TDs, as well as avoid the\nintroduction of new TDs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 11:51:58 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Dias", "Ronivon", ""], ["Neto", "Pedro", ""], ["Ibiapina", "Irvayne", ""], ["Castro", "Guilherme Avelino e Otavio", ""]]}, {"id": "1911.07624", "submitter": "Misbahu Sharfuddeen Zubair", "authors": "Misbahu S. Zubair, David J. Brown, Matthew Bates and Thomas\n  Hughes-Roberts", "title": "Designing Accessible Visual Programming Tools for Children with Autism\n  Spectrum Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Programming Tools (VPTs) allow users to create interactive media\nprojects such as games and animations using visual representations of\nprogramming concepts. Although VPTs have been shown to have huge potential for\nteaching children with cognitive impairments including those with Autism\nSpectrum Condition (ASC), research has shown that existing VPTs may not be\naccessible to them. Therefore, this study proposes a set of recommendations for\nthe design of accessible VPTs for children with ASC. Recommendations were\ninitially gathered and validated by interviewing experts (n=7). The interviews\nwere thematically analysed to identify recommendations. A second set of\ninterviews with a subset of the initial experts (n=3) was then conducted to\nvalidate the gathered recommendations. An examination of the available\nliterature was then conducted to identify additional recommendations for the\ndesign of VPTs. These recommendations arose from those used for the design of\nother interactive applications for children with ASC (e.g. virtual\nenvironments, serious games) and not identified as part of those the initially\ngathered from interviews. A novel set of recommendations for the design of VPTs\nfor children with ASC and additional cognitive impairments has been defined as\nthe result of this study.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 13:40:10 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zubair", "Misbahu S.", ""], ["Brown", "David J.", ""], ["Bates", "Matthew", ""], ["Hughes-Roberts", "Thomas", ""]]}, {"id": "1911.07625", "submitter": "Ahmed Ben Said", "authors": "Ahmed Ben Said and Abdelkarim Erradi", "title": "Deep-Gap: A deep learning framework for forecasting crowdsourcing\n  supply-demand gap based on imaging time series and residual learning", "comments": "Accepted at CloudCom 2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile crowdsourcing has become easier thanks to the widespread of\nsmartphones capable of seamlessly collecting and pushing the desired data to\ncloud services. However, the success of mobile crowdsourcing relies on\nbalancing the supply and demand by first accurately forecasting spatially and\ntemporally the supply-demand gap, and then providing efficient incentives to\nencourage participant movements to maintain the desired balance. In this paper,\nwe propose Deep-Gap, a deep learning approach based on residual learning to\npredict the gap between mobile crowdsourced service supply and demand at a\ngiven time and space. The prediction can drive the incentive model to achieve a\ngeographically balanced service coverage in order to avoid the case where some\nareas are over-supplied while other areas are under-supplied. This allows\nanticipating the supply-demand gap and redirecting crowdsourced service\nproviders towards target areas. Deep-Gap relies on historical supply-demand\ntime series data as well as available external data such as weather conditions\nand day type (e.g., weekday, weekend, holiday). First, we roll and encode the\ntime series of supply-demand as images using the Gramian Angular Summation\nField (GASF), Gramian Angular Difference Field (GADF) and the Recurrence Plot\n(REC). These images are then used to train deep Convolutional Neural Networks\n(CNN) to extract the low and high-level features and forecast the crowdsourced\nservices gap. We conduct comprehensive comparative study by establishing two\nsupply-demand gap forecasting scenarios: with and without external data.\nCompared to state-of-art approaches, Deep-Gap achieves the lowest forecasting\nerrors in both scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 17:32:39 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Said", "Ahmed Ben", ""], ["Erradi", "Abdelkarim", ""]]}, {"id": "1911.07692", "submitter": "Christian Tiefenau", "authors": "Christian Tiefenau, Maximilian H\\\"aring, Mohamed Khamis, Emanuel von\n  Zezschwitz", "title": "\"Please enter your PIN\" -- On the Risk of Bypass Attacks on Biometric\n  Authentication on Mobile Devices", "comments": "Poster Session SOUPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Nowadays, most mobile devices support biometric authentication schemes like\nfingerprint or face unlock. However, these probabilistic mechanisms can only be\nactivated in combination with a second alternative factor, usually\nknowledge-based authentication. In this paper, we show that this aspect can be\nexploited in a bypass attack. In this bypass attack, the attacker forces the\nuser to \"bypass\" the biometric authentication by, for example, resetting the\nphone. This forces the user to enter an easy-to-observe passcode instead. We\npresent the threat model and provide preliminary results of an online survey.\nBased on our results, we discuss potential countermeasures. We conclude that\nbetter feedback design and security-optimized fallback mechanisms can help\nfurther improve the overall security of mobile unlock mechanisms while\npreserving usability.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:15:22 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Tiefenau", "Christian", ""], ["H\u00e4ring", "Maximilian", ""], ["Khamis", "Mohamed", ""], ["von Zezschwitz", "Emanuel", ""]]}, {"id": "1911.07701", "submitter": "Christian Tiefenau", "authors": "Christian Tiefenau, Maximilian H\\\"aring, Eva Gerlitz, Emanuel von\n  Zezschwitz", "title": "Making Privacy Graspable: Can we Nudge Users to use Privacy Enhancing\n  Techniques?", "comments": "SOUPS 2019 Poster Session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Smart speakers are gaining popularity. However, such devices can put the\nuser's privacy at risk whenever hot-words are misinterpreted and voice data is\nrecorded without the user's consent. To mitigate such risks, smart speakers\nprovide privacy control mechanisms like the build-in mute button.\nUnfortunately, previous work indicated that such mute buttons are rarely used.\nIn this paper, we present the Privacy Hat, a tangible device which can be\nplaced on the smart speaker to prevent the device from listening. We designed\nthe Privacy Hat based on the results of a focus group and developed a working\nprototype. We hypothesize that the specific user experience of this physical\nand tangible token makes the use of privacy-enhancing technology more graspable\nfor the user. As a consequence, we expect that the Privacy Hat nudges users to\nmore actively use privacy-enhancing features like the mute button. In addition,\nwe propose the Privacy Hat as a study tool as we hypothesize that the artifact\nsupports participants in reflecting their behaviour. We report on the concept,\nthe prototype and our preliminary results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:30:08 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Tiefenau", "Christian", ""], ["H\u00e4ring", "Maximilian", ""], ["Gerlitz", "Eva", ""], ["von Zezschwitz", "Emanuel", ""]]}, {"id": "1911.07936", "submitter": "Efe Bozkir", "authors": "Efe Bozkir, Ali Burak \\\"Unal, Mete Akg\\\"un, Enkelejda Kasneci, Nico\n  Pfeifer", "title": "Privacy Preserving Gaze Estimation using Synthetic Images via a\n  Randomized Encoding Based Framework", "comments": "In Symposium on Eye Tracking Research and Applications (ETRA '20).\n  Authors' copy of the published paper, refer to the doi for the definitive\n  version", "journal-ref": null, "doi": "10.1145/3379156.3391364", "report-no": null, "categories": "cs.CV cs.CR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye tracking is handled as one of the key technologies for applications that\nassess and evaluate human attention, behavior, and biometrics, especially using\ngaze, pupillary, and blink behaviors. One of the challenges with regard to the\nsocial acceptance of eye tracking technology is however the preserving of\nsensitive and personal information. To tackle this challenge, we employ a\nprivacy-preserving framework based on randomized encoding to train a Support\nVector Regression model using synthetic eye images privately to estimate the\nhuman gaze. During the computation, none of the parties learn about the data or\nthe result that any other party has. Furthermore, the party that trains the\nmodel cannot reconstruct pupil, blinks or visual scanpath. The experimental\nresults show that our privacy-preserving framework is capable of working in\nreal-time, with the same accuracy as compared to non-private version and could\nbe extended to other eye tracking related problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 12:52:09 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 13:57:04 GMT"}, {"version": "v3", "created": "Mon, 6 Apr 2020 21:09:16 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 13:04:07 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bozkir", "Efe", ""], ["\u00dcnal", "Ali Burak", ""], ["Akg\u00fcn", "Mete", ""], ["Kasneci", "Enkelejda", ""], ["Pfeifer", "Nico", ""]]}, {"id": "1911.07983", "submitter": "Todd Murphey", "authors": "Kathleen Fitzsimons, Aleksandra Kalinowska, Julius P.A. Dewald, and\n  Todd Murphey", "title": "Task-Based Hybrid Shared Control for Training Through Forceful\n  Interaction", "comments": "16 pages, submitted to the International Journal of Robotics Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the fact that robotic platforms can provide both consistent practice\nand objective assessments of users over the course of their training, there are\nrelatively few instances where physical human robot interaction has been\nsignificantly more effective than unassisted practice or human-mediated\ntraining. This paper describes a hybrid shared control robot, which enhances\ntask learning through kinesthetic feedback. The assistance assesses user\nactions using a task-specific evaluation criterion and selectively accepts or\nrejects them at each time instant. Through two human subject studies (total\nn=68), we show that this hybrid approach of switching between full transparency\nand full rejection of user inputs leads to increased skill acquisition and\nshort-term retention compared to unassisted practice. Moreover, we show that\nthe shared control paradigm exhibits features previously shown to promote\nsuccessful training. It avoids user passivity by only rejecting user actions\nand allowing failure at the task. It improves performance during assistance,\nproviding meaningful task-specific feedback. It is sensitive to initial skill\nof the user and behaves as an `assist-as-needed' control scheme---adapting its\nengagement in real time based on the performance and needs of the user. Unlike\nother successful algorithms, it does not require explicit modulation of the\nlevel of impedance or error amplification during training and it is permissive\nto a range of strategies because of its evaluation criterion. We demonstrate\nthat the proposed hybrid shared control paradigm with a task-based minimal\nintervention criterion significantly enhances task-specific training.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 22:22:30 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Fitzsimons", "Kathleen", ""], ["Kalinowska", "Aleksandra", ""], ["Dewald", "Julius P. A.", ""], ["Murphey", "Todd", ""]]}, {"id": "1911.07985", "submitter": "Jean-Francois Boujut", "authors": "Maud Poulin (G-SCOP_CC), Jean-Fran\\c{c}ois Boujut (G-SCOP_CC),\n  C\\'edric Masclet (G-SCOP_CC)", "title": "Analysis of the co-design activity: influence of a mixed artifact and\n  contribution of the gestural function in a spatial augmented reality\n  environment", "comments": "INCOSE conference on Human System Integration, Sep 2019, Biarritz,\n  France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality provides new possibilities to propose environments where\nthe designers can take advantage of the physicality of the artifacts while\nkeeping the versatility of digital environments. Mixed objects can therefore\nprovide new media in the interactions between stakeholders. Besides, the\nincreasing interest in user participation in early design phases is limited by\nthe poor representations or the expensive mock ups to be provided in design\nmeetings. Therefore, understanding the role of these mixed artifacts by\nanalyzing and characterizing the interactions is crucial to the development of\nboth design methods and environments. By focusing on multimodal interactions,\nwe aim at providing new results in terms of the design process, in particular\nby studying the contribution of the gesture in collaborative product\nco-creativity sessions but also by understanding the role of these multiple\ninteractions in an augmented reality environment.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:07:35 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Poulin", "Maud", "", "G-SCOP_CC"], ["Boujut", "Jean-Fran\u00e7ois", "", "G-SCOP_CC"], ["Masclet", "C\u00e9dric", "", "G-SCOP_CC"]]}, {"id": "1911.08089", "submitter": "Joseph Futoma", "authors": "Mark Sendak, Madeleine Elish, Michael Gao, Joseph Futoma, William\n  Ratliff, Marshall Nichols, Armando Bedoya, Suresh Balu, Cara O'Brien", "title": "\"The Human Body is a Black Box\": Supporting Clinical Decision-Making\n  with Deep Learning", "comments": "To appear at ACM FAT* 2020, Barcelona. Updated to camera-ready\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning technologies are increasingly developed for use in\nhealthcare. While research communities have focused on creating\nstate-of-the-art models, there has been less focus on real world implementation\nand the associated challenges to accuracy, fairness, accountability, and\ntransparency that come from actual, situated use. Serious questions remain\nunder examined regarding how to ethically build models, interpret and explain\nmodel output, recognize and account for biases, and minimize disruptions to\nprofessional expertise and work cultures. We address this gap in the literature\nand provide a detailed case study covering the development, implementation, and\nevaluation of Sepsis Watch, a machine learning-driven tool that assists\nhospital clinicians in the early diagnosis and treatment of sepsis. We, the\nteam that developed and evaluated the tool, discuss our conceptualization of\nthe tool not as a model deployed in the world but instead as a socio-technical\nsystem requiring integration into existing social and professional contexts.\nRather than focusing on model interpretability to ensure a fair and accountable\nmachine learning, we point toward four key values and practices that should be\nconsidered when developing machine learning to support clinical\ndecision-making: rigorously define the problem in context, build relationships\nwith stakeholders, respect professional discretion, and create ongoing feedback\nloops with stakeholders. Our work has significant implications for future\nresearch regarding mechanisms of institutional accountability and\nconsiderations for designing machine learning systems. Our work underscores the\nlimits of model interpretability as a solution to ensure transparency,\naccuracy, and accountability in practice. Instead, our work demonstrates other\nmeans and goals to achieve FATML values in design and in practice.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 04:28:47 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 03:42:06 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Sendak", "Mark", ""], ["Elish", "Madeleine", ""], ["Gao", "Michael", ""], ["Futoma", "Joseph", ""], ["Ratliff", "William", ""], ["Nichols", "Marshall", ""], ["Bedoya", "Armando", ""], ["Balu", "Suresh", ""], ["O'Brien", "Cara", ""]]}, {"id": "1911.08202", "submitter": "Christian Meske", "authors": "Christian Meske, Ireti Amojo", "title": "Status Quo, Critical Reflection and Road Ahead of Digital Nudging in\n  Information Systems Research -- A Discussion with Markus Weinmann and Alexey\n  Voinov", "comments": null, "journal-ref": "Communications of the Association for Information Systems (Volume\n  46, Article 17), 04/2020", "doi": "10.17705/1CAIS.04617", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Research on Digital Nudging has become increasingly popular in the\nInformation Systems (IS) community. This paper presents an overview of the\ncurrent progress, a critical reflection and an outlook to further research\nregarding Digital Nudging in IS. For this purpose, we conducted a comprehensive\nliterature review as well as an interview with Markus Weinmann from Rotterdam\nSchool of Management at Erasmus University, one of the first scholars who\nintroduced Digital Nudging to the IS community, and Alexey Voinov, director of\nthe Centre on Persuasive Systems for Wise Adaptive Living at University of\nTechnology Sydney. The findings uncover a gap between what we know about what\nconstitutes Digital Nudging and how consequent requirements can actually be put\ninto practice. In this context, the original concept of Nudging bears inherent\nchallenges, e.g. regarding the focus on the individuals' welfare, which hence\nalso apply to Digital Nudging. Moreover, we need a better understanding of how\nNudging in digital choice environments differs from that in the offline world.\nTo further distinguish itself from other disciplines that already tested\nvarious nudges in many different domains, Digital Nudging Research in IS may\nbenefit from a strong Design Science perspective, going beyond the test of\neffectiveness and providing specific design principles for the different types\nof digital nudges.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 10:44:27 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 04:38:28 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Meske", "Christian", ""], ["Amojo", "Ireti", ""]]}, {"id": "1911.08277", "submitter": "Bas R. J. Bolmer", "authors": "Bas R.J. Bolmer, Monique Taverne, Marco Scherer", "title": "Exploring the added value of blockchain technology for the healthcare\n  domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, the University Medical Center Groningen (UMCG) has written\ndown lessons learned on how blockchain technology can have an impact on the\nhealthcare domain. By looking at two use-cases, the hospital challenged several\nteams, participating in an open innovation program and blockchain hackathon, to\nfind a solution that showed the added value of the technology for patient care\nand scientific research. Besides this practical perspective, the report also\nconsiders literature discussing the current state of blockchain technology in\nregard to developments in the healthcare domain (touching on patient\nempowerment, data management, regulations, and interoperability between\nhealthcare systems).\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 17:00:00 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Bolmer", "Bas R. J.", ""], ["Taverne", "Monique", ""], ["Scherer", "Marco", ""]]}, {"id": "1911.08293", "submitter": "Michael Hind", "authors": "Michael Hind, Stephanie Houde, Jacquelyn Martino, Aleksandra\n  Mojsilovic, David Piorkowski, John Richards, Kush R. Varshney", "title": "Experiences with Improving the Transparency of AI Models and Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI models and services are used in a growing number of highstakes areas,\nresulting in a need for increased transparency. Consistent with this, several\nproposals for higher quality and more consistent documentation of AI data,\nmodels, and systems have emerged. Little is known, however, about the needs of\nthose who would produce or consume these new forms of documentation. Through\nsemi-structured developer interviews, and two document creation exercises, we\nhave assembled a clearer picture of these needs and the various challenges\nfaced in creating accurate and useful AI documentation. Based on the\nobservations from this work, supplemented by feedback received during multiple\ndesign explorations and stakeholder conversations, we make recommendations for\neasing the collection and flexible presentation of AI facts to promote\ntransparency.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 23:30:58 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Hind", "Michael", ""], ["Houde", "Stephanie", ""], ["Martino", "Jacquelyn", ""], ["Mojsilovic", "Aleksandra", ""], ["Piorkowski", "David", ""], ["Richards", "John", ""], ["Varshney", "Kush R.", ""]]}, {"id": "1911.08480", "submitter": "Michalis Xenos", "authors": "Michalis Xenos, Catherine Christodoulopoulou, Andreas Mallas, and John\n  Garofalakis", "title": "The Future Time Traveller Project: Career Guidance on Future Skills,\n  Jobs and Career Prospects of Generation Z through a Game-Based Virtual World\n  Environment", "comments": "Published at the 10th International IEEE Conference on Information\n  Intelligence Systems & Applications, IISA 2019, Patras, Greece, 15-17 July\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future Time Traveller is a European project that aims at transforming career\nguidance of generation Z through an innovative, games-based scenario approach\nand to prepare the next generation for the jobs of the future. The pro-ject\nobjective is to foster innovative thinking and future-oriented mindset of young\npeople, through an innovative game-based virtual world environment. This\nenvironment helps them explore the future world, understand the trends that\nshape the future world of work, the emerging jobs, and the skills they will\nrequire. The Future Time Traveller project is implemented by a team of experts\nin 7 European countries (Bulgaria, Germany, Greece, Italy, Poland, Portugal,\nand United Kingdom). The project target groups include young people\n(genera-tion Z), career guidance practitioners and experts, and policymakers.\nThis paper presents, in brief, the Future Time Traveller project and introduces\nthe reader to the main features and functionalities of the 3-dimensional\nvirtual world and the games developed in this environment.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 07:02:55 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Xenos", "Michalis", ""], ["Christodoulopoulou", "Catherine", ""], ["Mallas", "Andreas", ""], ["Garofalakis", "John", ""]]}, {"id": "1911.08567", "submitter": "Praveen Kumar Bodigutla", "authors": "Praveen Kumar Bodigutla, Lazaros Polymenakos, Spyros Matsoukas", "title": "Multi-domain Conversation Quality Evaluation via User Satisfaction\n  Estimation", "comments": "The 3rd Conversational AI workshop: Today's practice and tomorrow's\n  potential at NeurIPS 2019. arXiv admin note: substantial text overlap with\n  arXiv:1908.07064", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automated metric to evaluate dialogue quality is vital for optimizing data\ndriven dialogue management. The common approach of relying on explicit user\nfeedback during a conversation is intrusive and sparse. Current models to\nestimate user satisfaction use limited feature sets and employ annotation\nschemes with limited generalizability to conversations spanning multiple\ndomains. To address these gaps, we created a new Response Quality annotation\nscheme, introduced five new domain-independent feature sets and experimented\nwith six machine learning models to estimate User Satisfaction at both turn and\ndialogue level.\n  Response Quality ratings achieved significantly high correlation (0.76) with\nexplicit turn-level user ratings. Using the new feature sets we introduced,\nGradient Boosting Regression model achieved best (rating [1-5]) prediction\nperformance on 26 seen (linear correlation ~0.79) and one new multi-turn domain\n(linear correlation 0.67). We observed a 16% relative improvement (68% -> 79%)\nin binary (\"satisfactory/dissatisfactory\") class prediction accuracy of a\ndomain-independent dialogue-level satisfaction estimation model after including\npredicted turn-level satisfaction ratings as features.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 00:11:47 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Bodigutla", "Praveen Kumar", ""], ["Polymenakos", "Lazaros", ""], ["Matsoukas", "Spyros", ""]]}, {"id": "1911.08608", "submitter": "Michele Rossi", "authors": "Riccardo Bonetto, Mattia Soldan, Alberto Lanaro, Simone Milani,\n  Michele Rossi", "title": "Seq2Seq RNN based Gait Anomaly Detection from Smartphone Acquired\n  Multimodal Motion Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones and wearable devices are fast growing technologies that, in\nconjunction with advances in wireless sensor hardware, are enabling ubiquitous\nsensing applications. Wearables are suitable for indoor and outdoor scenarios,\ncan be placed on many parts of the human body and can integrate a large number\nof sensors capable of gathering physiological and behavioral biometric\ninformation. Here, we are concerned with gait analysis systems that extract\nmeaningful information from a user's movements to identify anomalies and\nchanges in their walking style. The solution that is put forward is\nsubject-specific, as the designed feature extraction and classification tools\nare trained on the subject under observation. A smartphone mounted on an ad-hoc\nmade chest support is utilized to gather inertial data and video signals from\nits built-in sensors and rear-facing camera. The collected video and inertial\ndata are preprocessed, combined and then classified by means of a Recurrent\nNeural Network (RNN) based Sequence-to-Sequence (Seq2Seq) model, which is used\nas a feature extractor, and a following Convolutional Neural Network (CNN)\nclassifier. This architecture provides excellent results, being able to\ncorrectly assess anomalies in 100% of the cases, for the considered tests,\nsurpassing the performance of support vector machine classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 21:57:42 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Bonetto", "Riccardo", ""], ["Soldan", "Mattia", ""], ["Lanaro", "Alberto", ""], ["Milani", "Simone", ""], ["Rossi", "Michele", ""]]}, {"id": "1911.08657", "submitter": "Diego Casado-Mansilla", "authors": "D. Casado-Mansilla, P. Garaizar, A. Irizar-Arrieta, D.\n  L\\'opez-de-Ipi\\~na", "title": "On the Side Effects of Automation in IoT: Complacency and Comfort vs.\n  Relapse and Distrust", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/03", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation through IoT brings with it a whole new set of philosophical and\nethical implications that we barely began to address. However, it is widely\nconsidered by many scholars as the panacea to overcoming the majority of\nsocietal issues. The case of energy efficiency as an action for tackling\nclimate change is not different: demand-response proposals or occupancy-driven\nenergy management systems crowd the current research agenda on energy\nefficiency. However, there are still very few studies that have reported the\neffects of automation in the mid or long term beyond energy reduction (e.g.\nemotional feelings derived to interact with automation, complacency to the\ndevices or perceived value of the automation throughout the time). In this\nworkshop article, we report scientific evidence of a study conducted in ten\nworkplaces during more than one year where we found that automating some\nelectronic devices of common use (i.e. moving away or preventing subjects from\nthe control of these devices) in favour of comfort and energy efficiency, is\nassociated with a reduction of the users' confidence in science and technology\nas a mean to solve all environmental current problems and reduce the\nwillingness of people to act in favor of the environment.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 13:12:00 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Casado-Mansilla", "D.", ""], ["Garaizar", "P.", ""], ["Irizar-Arrieta", "A.", ""], ["L\u00f3pez-de-Ipi\u00f1a", "D.", ""]]}, {"id": "1911.09041", "submitter": "Laura Koesten", "authors": "Laura Koesten, Kathleen Gregory, Paul Groth, Elena Simperl", "title": "Talking datasets: Understanding data sensemaking behaviours", "comments": "26 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sharing and reuse of data are seen as critical to solving the most\ncomplex problems of today. Despite this potential, relatively little is known\nabout a key step in data reuse: people's behaviours involved in data-centric\nsensemaking. We aim to address this gap by presenting a mixed-methods study\ncombining in-depth interviews, a think-aloud task and a screen recording\nanalysis with 31 researchers as they summarised and interacted with both\nfamiliar and unfamiliar data. We use our findings to identify and detail common\nactivity patterns and necessary data attributes across three clusters of\nsensemaking activities: inspecting data, engaging with content, and placing\ndata within broader contexts. We conclude by proposing design recommendations\nfor tools and documentation practices which can be used to facilitate\nsensemaking and subsequent data reuse.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 17:14:18 GMT"}, {"version": "v2", "created": "Fri, 24 Jan 2020 17:17:03 GMT"}, {"version": "v3", "created": "Sat, 18 Jul 2020 16:36:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Koesten", "Laura", ""], ["Gregory", "Kathleen", ""], ["Groth", "Paul", ""], ["Simperl", "Elena", ""]]}, {"id": "1911.09219", "submitter": "Matthew Guzdial", "authors": "Andrew Hoyt, Matthew Guzdial, Yalini Kumar, Gillian Smith, and Mark O.\n  Riedl", "title": "Integrating Automated Play in Level Co-Creation", "comments": "2 pages, 2 figures, AIIDE Workshop on Experimental AI in Games", "journal-ref": "AIIDE Workshop on Experimental AI in Games 2019", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In level co-creation an AI and human work together to create a video game\nlevel. One open challenge in level co-creation is how to empower human users to\nensure particular qualities of the final level, such as challenge. There has\nbeen significant prior research into automated pathing and automated\nplaytesting for video game levels, but not in how to incorporate these into\ntools. In this demonstration we present an improvement of the Morai Maker\nmixed-initiative level editor for Super Mario Bros. that includes automated\npathing and challenge approximation features.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 23:36:42 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Hoyt", "Andrew", ""], ["Guzdial", "Matthew", ""], ["Kumar", "Yalini", ""], ["Smith", "Gillian", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1911.09279", "submitter": "Yunlong Wang", "authors": "Guang Jiang, Mengzhen Shi, Ying Su, Pengcheng An, Brian Y. Lim,\n  Yunlong Wang", "title": "NaMemo: Enhancing Lecturers' Interpersonal Competence of Remembering\n  Students' Names", "comments": "DIS '20 Companion", "journal-ref": null, "doi": "10.1145/3393914.3395860", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Addressing students by their names helps a teacher to start building rapport\nwith students and thus facilitates their classroom participation. However, this\nbasic yet effective skill has become rather challenging for university\nlecturers, who have to handle large-sized (sometimes exceeding 100) groups in\ntheir daily teaching. To enhance lecturers' competence in delivering\ninterpersonal interaction, we developed NaMemo, a real-time name-indicating\nsystem based on a dedicated face-recognition pipeline. This paper presents the\nsystem design, the pilot feasibility test, and our plan for the following\nstudy, which aims to evaluate NaMemo's impacts on learning and teaching, as\nwell as to probe design implications including privacy considerations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 04:13:55 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 04:10:58 GMT"}, {"version": "v3", "created": "Tue, 17 Mar 2020 01:07:42 GMT"}, {"version": "v4", "created": "Sun, 19 Apr 2020 06:45:07 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Jiang", "Guang", ""], ["Shi", "Mengzhen", ""], ["Su", "Ying", ""], ["An", "Pengcheng", ""], ["Lim", "Brian Y.", ""], ["Wang", "Yunlong", ""]]}, {"id": "1911.09284", "submitter": "Klaus Mueller", "authors": "Nafees Ahmed and Klaus Mueller", "title": "EnergyScout: A Consumer Oriented Dashboard for Smart Meter Data\n  Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing popularity of smart meters provides energy consumers in\nhouseholds with unprecedented opportunities for understanding and modifying\ntheir energy use. However, while a variety of solutions, both commercial and\nacademic,have been proposed, research on effective visual analysis tools is\nstill needed to achieve widespread adoption of smart meters. In this paper we\nexplore an interface that seeks to balance the tradeoff between complexity and\nusability. We worked with real household data and in close collaboration with\nconsumer experts of a large local utility company. Based on their continued\nfeedback we designed EnergyScout - a dashboard with a versatile set of highly\ninteractive visual tools with which consumers can understand the energy\nconsumption of their household devices, discover the impact of their usage\npatterns, compare them with usage patterns of the past, and see via what-if\nanalysis what effects a modification of these patterns may have, also in the\ncontext of modulated incentivized pricing, social and personal events, outside\ntemperature, and weather. All of these are events which could explain certain\nusage patterns and help motivate a modification of behavior. We tested\nEnergyScout with various groups of people, households, and energy bill\nresponsibilities in order to gauge the merits of this system.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 04:26:42 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Ahmed", "Nafees", ""], ["Mueller", "Klaus", ""]]}, {"id": "1911.09386", "submitter": "Jana Korunovska", "authors": "Jana Korunovska, Bernadette Kamleitner and Sarah Spiekermann", "title": "The Power and Pitfalls of Transparent Privacy Policies in Social\n  Networking Service Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Users disclose ever-increasing amounts of personal data on Social Network\nService platforms (SNS). Unless SNSs' policies are privacy friendly, this\nleaves them vulnerable to privacy risks because they ignore the privacy\npolicies. Designers and regulators have pushed for shorter, simpler and more\nprominent privacy policies, however the evidence that transparent policies\nincrease informed consent is lacking. To answer this question, we conducted an\nonline experiment with 214 regular Facebook users asked to join a fictitious\nSNS. We experimentally manipulated the privacy-friendliness of SNS's policy and\nvaried threats of secondary data use and data visibility. Half of our\nparticipants incorrectly recalled even the most formally \"perfect\" and\neasy-to-read privacy policies. Mostly, users recalled policies as more privacy\nfriendly than they were. Moreover, participants self-censored their disclosures\nwhen aware that visibility threats were present, but were less sensitive to\nthreats of secondary data use. We present design recommendations to increase\ninformed consent.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 10:24:12 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 10:55:42 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Korunovska", "Jana", ""], ["Kamleitner", "Bernadette", ""], ["Spiekermann", "Sarah", ""]]}, {"id": "1911.09605", "submitter": "Aryabrata Basu", "authors": "Aryabrata Basu", "title": "A brief chronology of Virtual Reality", "comments": "Preprint to other suitable (potential) journals", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we are going to review a brief history of the field of\nVirtual Reality (VR), VR systems, and applications and discuss how they\nevolved. After that, we will familiarize ourselves with the essential\ncomponents of VR experiences and common VR terminology. Finally, we discuss the\nevolution of ubiquitous VR as a subfield of VR and its current trends.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 17:01:14 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 16:22:11 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Basu", "Aryabrata", ""]]}, {"id": "1911.09668", "submitter": "Chenglong Wang", "authors": "Chenglong Wang, Yu Feng, Rastislav Bodik, Alvin Cheung, Isil Dillig", "title": "Visualization by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While visualizations play a crucial role in gaining insights from data,\ngenerating useful visualizations from a complex dataset is far from an easy\ntask. Besides understanding the functionality provided by existing\nvisualization libraries, generating the desired visualization also requires\nreshaping and aggregating the underlying data as well as composing different\nvisual elements to achieve the intended visual narrative. This paper aims to\nsimplify visualization tasks by automatically synthesizing the required program\nfrom simple visual sketches provided by the user. Specifically, given an input\ndata set and a visual sketch that demonstrates how to visualize a very small\nsubset of this data, our technique automatically generates a program that can\nbe used to visualize the entire data set.\n  Automating visualization poses several challenges. First, because many\nvisualization tasks require data wrangling in addition to generating plots, we\nneed to decompose the end-to-end synthesis task into two separate sub-problems.\nSecond, because the intermediate specification that results from the\ndecomposition is necessarily imprecise, this makes the data wrangling task\nparticularly challenging in our context. In this paper, we address these\nproblems by developing a new compositional visualization-by-example technique\nthat (a) decomposes the end-to-end task into two different synthesis problems\nover different DSLs and (b) leverages bi-directional program analysis to deal\nwith the complexity that arises from having an imprecise intermediate\nspecification.\n  We implemented our visualization-by-example algorithm and evaluate it on 83\nvisualization tasks collected from on-line forums and tutorials. Viser can\nsolve 84% of these benchmarks within a 600 second time limit, and, for those\ntasks that can be solved, the desired visualization is among the top-5\ngenerated by Viser in 70% of the cases.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:55:35 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Wang", "Chenglong", ""], ["Feng", "Yu", ""], ["Bodik", "Rastislav", ""], ["Cheung", "Alvin", ""], ["Dillig", "Isil", ""]]}, {"id": "1911.09713", "submitter": "Thomas Groechel", "authors": "Thomas R. Groechel, Zhonghao Shi, Roxanna Pakkar, and Maja J.\n  Matari\\'c", "title": "Using Socially Expressive Mixed Reality Arms for Enhancing\n  Low-Expressivity Robots", "comments": "8 pages, 9 figures, 3 tables, In 2019 IEEE International Symposium on\n  Robot and Human Interactactive Communication (RO-MAN '19), New Delhi, India,\n  Oct-2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expressivity--the use of multiple modalities to convey internal state and\nintent of a robot--is critical for interaction. Yet, due to cost, safety, and\nother constraints, many robots lack high degrees of physical expressivity. This\npaper explores using mixed reality to enhance a robot with limited expressivity\nby adding virtual arms that extend the robot's expressiveness. The arms,\ncapable of a range of non-physically-constrained gestures, were evaluated in a\nbetween-subject study ($n=34$) where participants engaged in a mixed reality\nmathematics task with a socially assistive robot. The study results indicate\nthat the virtual arms added a higher degree of perceived emotion, helpfulness,\nand physical presence to the robot. Users who reported a higher perceived\nphysical presence also found the robot to have a higher degree of social\npresence, ease of use, usefulness, and had a positive attitude toward using the\nrobot with mixed reality. The results also demonstrate the users' ability to\ndistinguish the virtual gestures' valence and intent.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 19:23:04 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 18:57:39 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Groechel", "Thomas R.", ""], ["Shi", "Zhonghao", ""], ["Pakkar", "Roxanna", ""], ["Matari\u0107", "Maja J.", ""]]}, {"id": "1911.09923", "submitter": "Claudia Savina Bianchini", "authors": "Claudia S. Bianchini (FORELLIS), Fabrizio Borgia (UPS), Maria de\n  Marsico", "title": "SWift -- A SignWriting editor to bridge between deaf world and\n  e-learning", "comments": null, "journal-ref": "Proceedings of the International Conference on Advanced Learning\n  Technologies (ICALT), IEEE - Institute of Electrical and Electronics\n  Engineers, pp.526-530, 2012, 978-0-7695-4702-2", "doi": "10.1109/ICALT.2012.235", "report-no": "pubblicazione #010", "categories": "cs.HC cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SWift (SignWriting improved fast transcriber) is an advanced editor for\nSignWriting (SW). At present, SW is a promising alternative to provide\ndocuments in an easy-to-grasp written form of (any) Sign Language, the gestural\nway of communication which is widely adopted by the deaf community. SWift was\ndeveloped SW users, either deaf or not, to support collaboration and exchange\nof ideas. The application allows composing and saving desired signs using\nelementary components, called glyphs. The procedure that was devised guides and\nsimplifies the editing process. SWift aims at breaking the \"electronic\"\nbarriers that keep the deaf community away from ICT in general, and from\ne-learning in particular. The editor can be contained in a pluggable module;\ntherefore, it can be integrated everywhere the use of SW is an advisable\nalternative to written \"verbal\" language, which often hinders information\ngrasping by deaf users.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 08:44:23 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Bianchini", "Claudia S.", "", "FORELLIS"], ["Borgia", "Fabrizio", "", "UPS"], ["de Marsico", "Maria", ""]]}, {"id": "1911.09958", "submitter": "Slawomir Konrad Tadeja", "authors": "Slawomir Konrad Tadeja and Wojciech Rydlewicz and Yupu Lu and Per Ola\n  Kristensson and Tomasz Bubas and Maciej Rydlewicz", "title": "PhotoTwinVR: An Immersive System for Manipulation, Inspection and\n  Dimension Measurements of the 3D Photogrammetric Models of Real-Life\n  Structures in Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photogrammetry is a science dealing with obtaining reliable information about\nphysical objects using their imagery description. Recent advancements in the\ndevelopment of Virtual Reality (VR) can help to unlock the full potential\noffered by the digital 3D-reality models generated using the state-of-art\nphotogrammetric technologies. These models are becoming a viable alternative\nfor providing high-quality content for such immersive environment.\nSimultaneously, their analyses in VR could bring added-value to professionals\nworking in various engineering and non-engineering settings and help in\nextracting useful information about physical objects. However, there is little\nresearch published to date on feasible interaction methods in the VR-based\nsystems augmented with the 3D photogrammetric models, especially concerning\ngestural input interfaces. Consequently, this paper presents the PhotoTwinVR --\nan immersive, gesture-controlled system for manipulation and inspection of 3D\nphotogrammetric models of physical objects in VR. Our system allows the user to\nperform basic engineering operations on the model subjected to the off-line\ninspection process. An observational study with a group of three domain-expert\nparticipants was completed to verify its feasibility. The system was populated\nwith a 3D photogrammetric model of an existing pipe-rack generated using a\ncommercial software package. The participants were asked to carry out a survey\nmeasurement of the object using the measurement toolbox offered by PhotoTwinVR.\nThe study revealed a potential of such immersive tool to be applied in\npractical real-words cases of off-line inspections of pipelines.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 10:30:12 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Tadeja", "Slawomir Konrad", ""], ["Rydlewicz", "Wojciech", ""], ["Lu", "Yupu", ""], ["Kristensson", "Per Ola", ""], ["Bubas", "Tomasz", ""], ["Rydlewicz", "Maciej", ""]]}, {"id": "1911.10044", "submitter": "Christian Tominski", "authors": "Sven Kluge and Stefan Gladisch and Uwe Freiherr von Lukas and Oliver\n  Staadt and Christian Tominski", "title": "Virtual Lenses as Embodied Tools for Immersive Analytics", "comments": null, "journal-ref": "Proceedings of the GI VR/AR Workshop (VAR), 2020", "doi": "10.18420/vrar2020_8", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive lenses are useful tools for supporting the analysis of data in\ndifferent ways. Most existing lenses are designed for 2D visualization and are\noperated using standard mouse and keyboard interaction. On the other hand,\nresearch on virtual lenses for novel 3D immersive visualization environments is\nscarce. Our work aims to narrow this gap in the literature. We focus\nparticularly on the interaction with lenses. Inspired by natural interaction\nwith magnifying glasses in the real world, our lenses are designed as graspable\ntools that can be created and removed as needed, manipulated and parameterized\ndepending on the task, and even combined to flexibly create new views on the\ndata. We implemented our ideas in a system for the visual analysis of 3D sonar\ndata. Informal user feedback from more than a hundred people suggests that the\ndesigned lens interaction is easy to use for the task of finding a hidden wreck\nin sonar data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 13:53:45 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kluge", "Sven", ""], ["Gladisch", "Stefan", ""], ["von Lukas", "Uwe Freiherr", ""], ["Staadt", "Oliver", ""], ["Tominski", "Christian", ""]]}, {"id": "1911.10098", "submitter": "Alex Raymond", "authors": "Alex Raymond, Hatice Gunes, Amanda Prorok", "title": "Culture-Based Explainable Human-Agent Deconfliction", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.HC cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Law codes and regulations help organise societies for centuries, and as AI\nsystems gain more autonomy, we question how human-agent systems can operate as\npeers under the same norms, especially when resources are contended. We posit\nthat agents must be accountable and explainable by referring to which rules\njustify their decisions. The need for explanations is associated with user\nacceptance and trust. This paper's contribution is twofold: i) we propose an\nargumentation-based human-agent architecture to map human regulations into a\nculture for artificial agents with explainable behaviour. Our architecture\nleans on the notion of argumentative dialogues and generates explanations from\nthe history of such dialogues; and ii) we validate our architecture with a user\nstudy in the context of human-agent path deconfliction. Our results show that\nexplanations provide a significantly higher improvement in human performance\nwhen systems are more complex. Consequently, we argue that the criteria\ndefining the need of explanations should also consider the complexity of a\nsystem. Qualitative findings show that when rules are more complex,\nexplanations significantly reduce the perception of challenge for humans.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 15:51:18 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Raymond", "Alex", ""], ["Gunes", "Hatice", ""], ["Prorok", "Amanda", ""]]}, {"id": "1911.10176", "submitter": "Daniel Roth Dr.", "authors": "Daniel Roth and Marc Erich Latoschik", "title": "Construction of a Validated Virtual Embodiment Questionnaire", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3023603", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User embodiment is important for many virtual reality (VR) applications, for\nexample, in the context of social interaction, therapy, training, or\nentertainment. However, there is no validated instrument to empirically measure\nthe perception of embodiment, necessary to reliably evaluate this important\nquality of user experience (UX). To assess components of virtual embodiment in\na valid, reliable, and consistent fashion, we develped a Virtual Embodiment\nQuestionnaire (VEQ). We reviewed previous literature to identify applicable\nconstructs and items, and performed a confirmatory factor analysis (CFA) on the\ndata from three experiments (N = 196). Each experiment modified a distinct\nsimulation property, namely, the level of immersion, the level of\npersonalization, and the level of behavioral realism. The analysis confirmed\nthree factors: (1) ownership of a virtual body, (2) agency over a virtual body,\nand (3) change in the perceived body schema. A fourth study (N = 22) further\nconfirmed the reliability and validity of the scale and investigated the\nimpacts of latency jitter of avatar movements presented in the simulation\ncompared to linear latencies and a baseline. We present the final scale and\nfurther insights from the studies regarding related constructs.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 18:23:19 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 07:12:48 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Roth", "Daniel", ""], ["Latoschik", "Marc Erich", ""]]}, {"id": "1911.10629", "submitter": "Davinderjit Kaur", "authors": "Ashish Verma, Ankush Goyal, Davinderjit Kaur", "title": "Fatigue Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, there are many fatigue detection methods and the majority of them\nare tracking eye in real-time using one or two cameras to detect the physical\nresponses in eyes. It is indicated that the responses in eyes have high\nrelativity with driver fatigue. As part of this project, We will propose a\nfatigue detection system based on pose estimation. Using pose estimation, We\nplan to mark the body joints in the upper body for shoulders and neck. Then, we\nplan to compare the location of the joints of the current posture with the\nideal posture.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2019 22:36:15 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Verma", "Ashish", ""], ["Goyal", "Ankush", ""], ["Kaur", "Davinderjit", ""]]}, {"id": "1911.10742", "submitter": "Yu Li", "authors": "Yu Li, Kun Qian, Weiyan Shi, Zhou Yu", "title": "End-to-End Trainable Non-Collaborative Dialog System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end task-oriented dialog models have achieved promising performance on\ncollaborative tasks where users willingly coordinate with the system to\ncomplete a given task. While in non-collaborative settings, for example,\nnegotiation and persuasion, users and systems do not share a common goal. As a\nresult, compared to collaborate tasks, people use social content to build\nrapport and trust in these non-collaborative settings in order to advance their\ngoals. To handle social content, we introduce a hierarchical intent annotation\nscheme, which can be generalized to different non-collaborative dialog tasks.\nBuilding upon TransferTransfo (Wolf et al. 2019), we propose an end-to-end\nneural network model to generate diverse coherent responses. Our model utilizes\nintent and semantic slots as the intermediate sentence representation to guide\nthe generation process. In addition, we design a filter to select appropriate\nresponses based on whether these intermediate representations fit the designed\ntask and conversation constraints. Our non-collaborative dialog model guides\nusers to complete the task while simultaneously keeps them engaged. We test our\napproach on our newly proposed ANTISCAM dataset and an existing\nPERSUASIONFORGOOD dataset. Both automatic and human evaluations suggest that\nour model outperforms multiple baselines in these two non-collaborative tasks.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 07:34:37 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Li", "Yu", ""], ["Qian", "Kun", ""], ["Shi", "Weiyan", ""], ["Yu", "Zhou", ""]]}, {"id": "1911.10882", "submitter": "Claudia Savina Bianchini", "authors": "Claudia S. Bianchini (Poitiers UFR LL), Fabrizio Borgia (UPS), Paolo\n  Bottoni, Maria de Marsico", "title": "SWift -- A SignWriting improved fast transcriber", "comments": null, "journal-ref": "Proceedings of the International Working Conference on Advanced\n  Visual Interfaces, ACM - Association for Computer Machinery, pp.390 - 393,\n  2012, ACM AVI2012, 978-1-4503-1287-5", "doi": "10.1145/2254556.2254631", "report-no": "pubblicazione #008", "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SWift (SignWriting improved fast transcriber), an advanced editor\nfor computer-aided writing and transcribing using SignWriting (SW). SW is\ndevised to allow deaf people and linguists alike to exploit an easy-to-grasp\nwritten form of (any) sign language. Similarly, SWift has been developed for\neveryone who masters SW, and is not exclusively deaf-oriented. Using SWift, it\nis possible to compose and save any sign, using elementary components called\nglyphs. A guided procedure facilitates the composition process. SWift is aimed\nat helping to break down the \"electronic\" barriers that keep the deaf community\naway from Information and Communication Technology (ICT). The editor has been\ndeveloped modularly and can be integrated everywhere the use of SW, as an\nalternative to written vocal language, may be advisable.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 12:54:58 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Bianchini", "Claudia S.", "", "Poitiers UFR LL"], ["Borgia", "Fabrizio", "", "UPS"], ["Bottoni", "Paolo", ""], ["de Marsico", "Maria", ""]]}, {"id": "1911.10920", "submitter": "Sophie Le Page", "authors": "Kelly Bronson, Sophie Le Page, Katherine-Marie Robinson, Ajung Moon,\n  Shalaleh Rismani, Jason Millar", "title": "Drivers' Awareness, Knowledge, and Use of Autonomous Driving Assistance\n  Systems (ADAS) and Vehicle Automation", "comments": "There is a problem with the data analysis and we are reanalyzing the\n  data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advanced driver assistance systems (ADAS) technologies in vehicles (e.g. park\nassist, lane change assist, emergency braking, etc.), which take over parts of\nthe driving task of human drivers, are advancing at a disruptive pace and hold\nthe potential to deliver many benefits to society. However, public\nunderstanding of ADAS systems, and driver training and licensing for using\nthem, are lagging behind the fast-paced technological development, which could\nraise safety issues or slow the deployment of ADAS, thus offsetting their\npotential benefits. There is, therefore, a need to investigate issues related\nto public perception of ADAS in order to develop appropriate policies and\ngovernance structures which support innovation, and result in the smooth\ndeployment and acceptance of appropriate ADAS for society. In this work we\nperform a quantitative public survey to better understand how the public's\nawareness and knowledge of ADAS technologies in their vehicles correlate to\ntheir use or engagement of those technologies. We find that up to 67% of\nparticipants never or rarely use optional ADAS in their vehicles (e.g. adaptive\ncruise control), where women were less likely than men to use ADAS even though\nwomen reported more awareness of ADAS in their vehicles, better training, and\nmore willingness to pay for ADAS. By performing this analysis we hope to raise\nawareness around the public perception of current state-of-the-art in ADAS\ntechnologies. We also hope to flag concerns that answers to these questions\nmight raise for the regulatory agencies, and manufacturers, responsible for\nbringing these technologies to market.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 13:55:19 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 13:48:44 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Bronson", "Kelly", ""], ["Page", "Sophie Le", ""], ["Robinson", "Katherine-Marie", ""], ["Moon", "Ajung", ""], ["Rismani", "Shalaleh", ""], ["Millar", "Jason", ""]]}, {"id": "1911.11356", "submitter": "Viet Trinh", "authors": "Viet Trinh, Roberto Manduchi", "title": "Semantic Interior Mapology: A Toolbox For Indoor Scene Description From\n  Architectural Floor Plans", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Semantic Interior Mapology (SIM) toolbox for the conversion\nof a floor plan and its room contents (such as furnitures) to a vectorized\nform. The toolbox is composed of the Map Conversion toolkit and the Map\nPopulation toolkit. The Map Conversion toolkit allows one to quickly trace the\nlayout of a floor plan, and to generate a GeoJSON file that can be rendered in\n3D using web applications such as Mapbox. The Map Population toolkit takes the\n3D scan of a room in the building (acquired from an RGB-D camera), and, through\na semi-automatic process, populates individual objects of interest with a\ncorrect dimension and position in the GeoJSON representation of the building.\nSIM is easy to use and produces accurate results even in the case of complex\nbuilding layouts.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 05:56:43 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Trinh", "Viet", ""], ["Manduchi", "Roberto", ""]]}, {"id": "1911.11743", "submitter": "Vinoj Yasanga Jayasundara Magalle Hewa", "authors": "Vinoj Jayasundara, Hirunima Jayasekara, Tharaka Samarasinghe, Kasun T.\n  Hemachandra", "title": "Device-Free User Authentication, Activity Classification and Tracking\n  using Passive Wi-Fi Sensing: A Deep Learning Based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy issues related to video camera feeds have led to a growing need for\nsuitable alternatives that provide functionalities such as user authentication,\nactivity classification and tracking in a noninvasive manner. Existing\ninfrastructure makes Wi-Fi a possible candidate, yet, utilizing traditional\nsignal processing methods to extract information necessary to fully\ncharacterize an event by sensing weak ambient Wi-Fi signals is deemed to be\nchallenging. This paper introduces a novel end to-end deep learning framework\nthat simultaneously predicts the identity, activity and the location of a user\nto create user profiles similar to the information provided through a video\ncamera. The system is fully autonomous and requires zero user intervention\nunlike systems that require user-initiated initialization, or a user held\ntransmitting device to facilitate the prediction. The system can also predict\nthe trajectory of the user by predicting the location of a user over\nconsecutive time steps. The performance of the system is evaluated through\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:27:50 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Jayasundara", "Vinoj", ""], ["Jayasekara", "Hirunima", ""], ["Samarasinghe", "Tharaka", ""], ["Hemachandra", "Kasun T.", ""]]}, {"id": "1911.11751", "submitter": "Gyanendra Sharma", "authors": "Gyanendra Sharma, Richard J Radke", "title": "Multi-person Spatial Interaction in a Large Immersive Display Using\n  Smartphones as Touchpads", "comments": "8 pages with references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a multi-user interaction interface for a large\nimmersive space that supports simultaneous screen interactions by combining (1)\nuser input via personal smartphones and Bluetooth microphones, (2) spatial\ntracking via an overhead array of Kinect sensors, and (3) WebSocket interfaces\nto a webpage running on the large screen. Users are automatically, dynamically\nassigned personal and shared screen sub-spaces based on their tracked location\nwith respect to the screen, and use a webpage on their personal smartphone for\ntouchpad-type input. We report user experiments using our interaction framework\nthat involve image selection and placement tasks, with the ultimate goal of\nrealizing display-wall environments as viable, interactive workspaces with\nnatural multimodal interfaces.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 18:39:24 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Sharma", "Gyanendra", ""], ["Radke", "Richard J", ""]]}, {"id": "1911.11787", "submitter": "Goran Muric", "authors": "Goran Muric, Andres Abeliuk, Kristina Lerman, Emilio Ferrara", "title": "Collaboration Drives Individual Productivity", "comments": "Proc. ACM Hum.-Comput. Interact., Vol. 3, No. CSCW", "journal-ref": null, "doi": "10.1145/3359176", "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does the number of collaborators affect individual productivity? Results\nof prior research have been conflicting, with some studies reporting an\nincrease in individual productivity as the number of collaborators grows, while\nother studies showing that the {free-rider effect} skews the effort invested by\nindividuals, making larger groups less productive. The difference between these\nschools of thought is substantial: if a super-scaling effect exists, as\nsuggested by former studies, then as groups grow, their productivity will\nincrease even faster than their size, super-linearly improving their\nefficiency. We address this question by studying two planetary-scale\ncollaborative systems: GitHub and Wikipedia. By analyzing the activity of over\n2 million users on these platforms, we discover that the interplay between\ngroup size and productivity exhibits complex, previously-unobserved dynamics:\nthe productivity of smaller groups scales super-linearly with group size, but\nsaturates at larger sizes. This effect is not an artifact of the heterogeneity\nof productivity: the relation between group size and productivity holds at the\nindividual level. People tend to do more when collaborating with more people.\nWe propose a generative model of individual productivity that captures the\nnon-linearity in collaboration effort. The proposed model is able to explain\nand predict group work dynamics in GitHub and Wikipedia by capturing their\nmaximally informative behavioral features, and it paves the way for a\nprincipled, data-driven science of collaboration.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 19:00:06 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Muric", "Goran", ""], ["Abeliuk", "Andres", ""], ["Lerman", "Kristina", ""], ["Ferrara", "Emilio", ""]]}, {"id": "1911.11920", "submitter": "Limeng Cui", "authors": "Limeng Cui", "title": "Warning Signs in Communicating the Machine Learning Detection Results of\n  Misinformation with Individuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the prevalence of misinformation online, researchers have focused on\ndeveloping various machine learning algorithms to detect fake news. However,\nusers' perception of machine learning outcomes and related behaviors have been\nwidely ignored. Hence, this paper proposed to bridge this gap by studying how\nto pass the detection results of machine learning to the users, and aid their\ndecisions in handling misinformation. An online experiment was conducted, to\nevaluate the effect of the proposed machine learning warning sign against a\ncontrol condition. We examined participants' detection and sharing of news. The\ndata showed that warning sign's effects on participants' trust toward the fake\nnews were not significant. However, we found that people's uncertainty about\nthe authenticity of the news dropped with the presence of the machine learning\nwarning sign. We also found that social media experience had effects on users'\ntrust toward the fake news, and age and social media experience had effects on\nusers' sharing decision. Therefore, the results indicate that there are many\nfactors worth studying that affect people's trust in the news. Moreover, the\nwarning sign in communicating machine learning detection results is different\nfrom ordinary warnings and needs more detailed research and design. These\nfindings hold important implications for the design of machine learning\nwarnings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 02:33:24 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Cui", "Limeng", ""]]}, {"id": "1911.12119", "submitter": "Hans-J\\\"urgen Profitlich", "authors": "Hans-J\\\"urgen Profitlich and Daniel Sonntag", "title": "Interactivity and Transparency in Medical Risk Assessment with\n  Supersparse Linear Integer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring systems are linear classification models that only require users to\nadd or subtract a few small numbers in order to make a prediction. They are\nused for example by clinicians to assess the risk of medical conditions. This\nwork focuses on our approach to implement an intuitive user interface to allow\na clinician to generate such scoring systems interactively, based on the\nRiskSLIM machine learning library. We describe the technical architecture which\nallows a medical professional who is not specialised in developing and applying\nmachine learning algorithms to create competitive transparent supersparse\nlinear integer models in an interactive way. We demonstrate our prototype\nmachine learning system in the nephrology domain, where doctors can\ninteractively sub-select datasets to compute models, explore scoring tables\nthat correspond to the learned models, and check the quality of the transparent\nsolutions from a medical perspective.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 14:58:35 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 10:10:19 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Profitlich", "Hans-J\u00fcrgen", ""], ["Sonntag", "Daniel", ""]]}, {"id": "1911.12135", "submitter": "Tomasz Rutkowski", "authors": "Tomasz M. Rutkowski, Masato S. Abe, Marcin Koculak, Mihoko\n  Otake-Matsuura", "title": "Cognitive Assessment Estimation from Behavioral Responses in Emotional\n  Faces Evaluation Task -- AI Regression Approach for Dementia Onset Prediction\n  in Aging Societies", "comments": "4 pages, 2 figures, accepted for a presentation at AI for Social Good\n  workshop at NeurIPS (2019), Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a practical health-theme machine learning (ML) application\nconcerning `AI for social good' domain for `Producing Good Outcomes' track. In\nparticular, the solution is concerning the problem of a potential elderly adult\ndementia onset prediction in aging societies. The paper discusses our attempt\nand encouraging preliminary study results of behavioral responses analysis in a\nworking memory-based emotional evaluation experiment. We focus on the\ndevelopment of digital biomarkers for dementia progress detection and\nmonitoring. We present a behavioral data collection concept for a subsequent\nAI-based application together with a range of regression encouraging results of\nMontreal Cognitive Assessment (MoCA) scores in the leave-one-subject-out\ncross-validation setup. The regressor input variables include experimental\nsubject's emotional valence and arousal recognition responses, as well as\nreaction times, together with self-reported education levels and ages, obtained\nfrom a group of twenty older adults taking part in the reported data collection\nproject. The presented results showcase the potential social benefits of\nartificial intelligence application for elderly and establish a step forward to\ndevelop ML approaches, for the subsequent application of simple behavioral\nobjective testing for dementia onset diagnostics replacing subjective MoCA.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 05:47:25 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Rutkowski", "Tomasz M.", ""], ["Abe", "Masato S.", ""], ["Koculak", "Marcin", ""], ["Otake-Matsuura", "Mihoko", ""]]}, {"id": "1911.12383", "submitter": "Jiayi Xu", "authors": "Jiayi Xu, Soumya Dutta, Wenbin He, Joachim Moortgat, Han-Wei Shen", "title": "Geometry-Driven Detection, Tracking and Visual Analysis of Viscous and\n  Gravitational Fingers", "comments": "Published at IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.3017568", "report-no": null, "categories": "cs.GR cs.CG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Viscous and gravitational flow instabilities cause a displacement front to\nbreak up into finger-like fluids. The detection and evolutionary analysis of\nthese fingering instabilities are critical in multiple scientific disciplines\nsuch as fluid mechanics and hydrogeology. However, previous detection methods\nof the viscous and gravitational fingers are based on density thresholding,\nwhich provides limited geometric information of the fingers. The geometric\nstructures of fingers and their evolution are important yet little studied in\nthe literature. In this work, we explore the geometric detection and evolution\nof the fingers in detail to elucidate the dynamics of the instability. We\npropose a ridge voxel detection method to guide the extraction of finger cores\nfrom three-dimensional (3D) scalar fields. After skeletonizing finger cores\ninto skeletons, we design a spanning tree based approach to capture how fingers\nbranch spatially from the finger skeletons. Finally, we devise a novel\ngeometric-glyph augmented tracking graph to study how the fingers and their\nbranches grow, merge, and split over time. Feedback from earth scientists\ndemonstrates the usefulness of our approach to performing spatio-temporal\ngeometric analyses of fingers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 19:02:58 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 19:06:53 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 20:34:17 GMT"}, {"version": "v4", "created": "Wed, 19 Aug 2020 21:08:54 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Xu", "Jiayi", ""], ["Dutta", "Soumya", ""], ["He", "Wenbin", ""], ["Moortgat", "Joachim", ""], ["Shen", "Han-Wei", ""]]}, {"id": "1911.12482", "submitter": "Basit Ayantunde", "authors": "Basit Ayantunde, Jane Odum, Fadlullah Olawumi, and Joshua Olalekan", "title": "Designing the Next Generation of Intelligent Personal Robotic Assistants\n  for the Physically Impaired", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The physically impaired commonly have difficulties performing simple routine\ntasks without relying on other individuals who are not always readily available\nand thus make them strive for independence. While their impaired abilities can\nin many cases be augmented (to certain degrees) with the use of assistive\ntechnologies, there has been little attention to their applications in embodied\nAI with assistive technologies. This paper presents the modular framework,\narchitecture, and design of the mid-fidelity prototype of MARVIN: an\nartificial-intelligence-powered robotic assistant designed to help the\nphysically impaired in performing simple day-to-day tasks. The prototype\nfeatures a trivial locomotion unit and also utilizes various state-of-the-art\nneural network architectures for specific modular components of the system.\nThese components perform specialized functions, such as automatic speech\nrecognition, object detection, natural language understanding, speech\nsynthesis, etc. We also discuss the constraints, challenges encountered,\npotential future applications and improvements towards succeeding prototypes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 02:00:20 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Ayantunde", "Basit", ""], ["Odum", "Jane", ""], ["Olawumi", "Fadlullah", ""], ["Olalekan", "Joshua", ""]]}, {"id": "1911.12793", "submitter": "Rafael Henkin", "authors": "Rafael Henkin, Cagatay Turkay", "title": "Words of Estimative Correlation: Studying Verbalizations of Scatterplots", "comments": "Accepted and to be published in IEEE Transactions on Visualization\n  and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.3023537", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language and visualization are being increasingly deployed together\nfor supporting data analysis in different ways, from multimodal interaction to\nenriched data summaries and insights. Yet, researchers still lack systematic\nknowledge on how viewers verbalize their interpretations of visualizations, and\nhow they interpret verbalizations of visualizations in such contexts. We\ndescribe two studies aimed at identifying characteristics of data and charts\nthat are relevant in such tasks. The first study asks participants to verbalize\nwhat they see in scatterplots that depict various levels of correlations. The\nsecond study then asks participants to choose visualizations that match a given\nverbal description of correlation. We extract key concepts from responses,\norganize them in a taxonomy and analyze the categorized responses. We observe\nthat participants use a wide range of vocabulary across all scatterplots, but\nparticular concepts are preferred for higher levels of correlation. A\ncomparison between the studies reveals the ambiguity of some of the concepts.\nWe discuss how the results could inform the design of multimodal\nrepresentations aligned with the data and analytical tasks, and present a\nresearch roadmap to deepen the understanding about visualizations and natural\nlanguage.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2019 17:09:57 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 12:30:18 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 16:05:10 GMT"}, {"version": "v4", "created": "Tue, 29 Sep 2020 08:49:43 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Henkin", "Rafael", ""], ["Turkay", "Cagatay", ""]]}, {"id": "1911.12918", "submitter": "Yuxuan Zhao", "authors": "Yuxuan Zhao, Xinyan Cao, Jinlong Lin, Dunshan Yu, Xixin Cao", "title": "Multimodal Emotion Recognition Model using Physiological Signals", "comments": "10 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important field of research in Human-Machine Interactions, emotion\nrecognition based on physiological signals has become research hotspots.\nMotivated by the outstanding performance of deep learning approaches in\nrecognition tasks, we proposed a Multimodal Emotion Recognition Model that\nconsists of a 3D convolutional neural network model, a 1D convolutional neural\nnetwork model and a biologically inspired multimodal fusion model which\nintegrates multimodal information on the decision level for emotion\nrecognition. We use this model to classify four emotional regions from the\narousal valence plane, i.e., low arousal and low valence (LALV), high arousal\nand low valence (HALV), low arousal and high valence (LAHV) and high arousal\nand high valence (HAHV) in the DEAP and AMIGOS dataset. The 3D CNN model and 1D\nCNN model are used for emotion recognition based on electroencephalogram (EEG)\nsignals and peripheral physiological signals respectively, and get the accuracy\nof 93.53% and 95.86% with the original EEG signals in these two datasets.\nCompared with the single-modal recognition, the multimodal fusion model\nimproves the accuracy of emotion recognition by 5% ~ 25%, and the fusion result\nof EEG signals (decomposed into four frequency bands) and peripheral\nphysiological signals get the accuracy of 95.77%, 97.27% and 91.07%, 99.74% in\nthese two datasets respectively. Integrated EEG signals and peripheral\nphysiological signals, this model could reach the highest accuracy about 99% in\nboth datasets which shows that our proposed method demonstrates certain\nadvantages in solving the emotion recognition tasks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 01:35:23 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Zhao", "Yuxuan", ""], ["Cao", "Xinyan", ""], ["Lin", "Jinlong", ""], ["Yu", "Dunshan", ""], ["Cao", "Xixin", ""]]}, {"id": "1911.13032", "submitter": "Joaquim Jorge", "authors": "Maur\\'icio Sousa, Daniel Mendes, and Joaquim Jorge", "title": "Safe Walking In VR using Augmented Virtuality", "comments": "10 pages, 10 figures, VRCAI 2019 Poster; The Authors would like to\n  thank Francisco Venda for his work and contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New technologies allow ordinary people to access Virtual Reality at\naffordable prices in their homes. One of the most important tasks when\ninteracting with immersive Virtual Reality is to navigate the virtual\nenvironments (VEs). Arguably, the best methods to accomplish this use of direct\ncontrol interfaces. Among those, natural walking (NW) makes for enjoyable user\nexperience. However, common techniques to support direct control interfaces in\nVEs feature constraints that make it difficult to use those methods in cramped\nhome environments. Indeed, NW requires unobstructed and open space. To approach\nthis problem, we propose a new virtual locomotion technique, Combined Walking\nin Place (CWIP). CWIP allows people to take advantage of the available physical\nspace and empowers them to use NW to navigate in the virtual world. For longer\ndistances, we adopt Walking in Place (WIP) to enable them to move in the\nvirtual world beyond the confines of a cramped real room. However, roaming in\nimmersive alternate reality, while moving in the confines of a cluttered\nenvironment can lead people to stumble and fall. To approach these problems, we\ndeveloped Augmented Virtual Reality (AVR), to inform users about real-world\nhazards, such as chairs, drawers, walls via proxies and signs placed in the\nvirtual world. We propose thus CWIP-AVR as a way to safely explore VR in the\ncramped confines of your own home. To our knowledge, this is the first approach\nto combined different locomotion modalities in a safe manner. We evaluated it\nin a user study with 20 participants to validate their ability to navigate a\nvirtual world while walking in a confined and cluttered real space. Our results\nshow that CWIP-AVR allows people to navigate VR safely, switching between\nlocomotion modes flexibly while maintaining a good immersion.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 10:09:19 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Sousa", "Maur\u00edcio", ""], ["Mendes", "Daniel", ""], ["Jorge", "Joaquim", ""]]}, {"id": "1911.13207", "submitter": "Claudia Savina Bianchini", "authors": "Claudia Bianchini (FORELLIS), Fabrizio Borgia, Maria de Marsico", "title": "A concrete example of inclusive design: deaf-oriented accessibility", "comments": "SCOPUS - ISI?", "journal-ref": "The Wiley Handbook of Human Computer Interaction, 2 (chapter 33),\n  Wiley (Blackwell), pp.731-756, 2018, 978-1-118-97613-5", "doi": "10.1002/9781118976005.ch33", "report-no": "pubblicazione #17", "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the continuing challenges of Human Computer Interaction research is\nthe full inclusion of people with special needs into the digital world. In\nparticular, this crucial category includes people that experiences some kind of\nlimitation in exploiting traditional information communication channels. One\nimmediately thinks about blind people, and several researches aim at addressing\ntheir needs. On the contrary, limitations suffered by deaf people are often\nunderestimated. This often the result of a kind of ignorance or\nmisunderstanding of the real nature of their communication difficulties. This\nchapter aims at both increasing the awareness of deaf problems in the digital\nworld, and at proposing the project of a comprehensive solution for their\nbetter inclusion. As for the former goal, we will provide a bird's-eye\npresentation of history and evolution of understanding of deafness issues, and\nof strategies to address them. As for the latter, we will present the design,\nimplementation and evaluation of the first nucleus of a comprehensive digital\nframework to facilitate the access of deaf people into the digital world.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 15:01:57 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Bianchini", "Claudia", "", "FORELLIS"], ["Borgia", "Fabrizio", ""], ["de Marsico", "Maria", ""]]}, {"id": "1911.13231", "submitter": "Claudia Savina Bianchini", "authors": "Fabrizio Borgia (PCL, UPS), Claudia Bianchini (Poitiers UFR LL,\n  FORELLIS), Maria de Marsico", "title": "Towards improving the e-learning experience for deaf students: e-LUX", "comments": "anche ISBN 978-3-319-07436-9", "journal-ref": "Lecture Notes in Computer Science, Springer, 2014, Universal\n  access in human-computer interaction: universal access to information and\n  knowledge, 8514 (2), pp.221-232", "doi": "10.1007/978-3-319-07440-5", "report-no": "pubblicazione #014", "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deaf people are more heavily affected by the digital divide than many would\nexpect. Moreover, most accessibility guidelines addressing their needs just\ndeal with captioning and audio-content transcription. However, this approach to\nthe problem does not consider that deaf people have big troubles with vocal\nlanguages, even in their written form. At present, only a few organizations,\nlike W3C, produced guidelines dealing with one of their most distinctive\nexpressions: Sign Language (SL). SL is, in fact, the visual-gestural language\nused by many deaf people to communicate with each other. The present work aims\nat supporting e-learning user experience (e-LUX) for these specific users by\nenhancing the accessibility of content and container services. In particular,\nwe propose preliminary solutions to tailor activities which can be more\nfruitful when performed in one's own \"native\" language, which for most deaf\npeople, especially younger ones, is represented by national SL.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 14:55:16 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Borgia", "Fabrizio", "", "PCL, UPS"], ["Bianchini", "Claudia", "", "Poitiers UFR LL,\n  FORELLIS"], ["de Marsico", "Maria", ""]]}, {"id": "1911.13248", "submitter": "Chee Wee Leong", "authors": "Chee Wee Leong, Katrina Roohr, Vikram Ramanarayanan, Michelle P.\n  Martin-Raugh, Harrison Kell, Rutuja Ubale, Yao Qian, Zydrune Mladineo, Laura\n  McCulla", "title": "To Trust, or Not to Trust? A Study of Human Bias in Automated Video\n  Interview Assessments", "comments": "ICCV Workshop on Interpreting and Explaining Visual Artificial\n  Intelligence Models, Seoul, South Korea, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised systems require human labels for training. But, are humans\nthemselves always impartial during the annotation process? We examine this\nquestion in the context of automated assessment of human behavioral tasks.\nSpecifically, we investigate whether human ratings themselves can be trusted at\ntheir face value when scoring video-based structured interviews, and whether\nsuch ratings can impact machine learning models that use them as training data.\nWe present preliminary empirical evidence that indicates there might be biases\nin such annotations, most of which are visual in nature.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 05:03:04 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Leong", "Chee Wee", ""], ["Roohr", "Katrina", ""], ["Ramanarayanan", "Vikram", ""], ["Martin-Raugh", "Michelle P.", ""], ["Kell", "Harrison", ""], ["Ubale", "Rutuja", ""], ["Qian", "Yao", ""], ["Mladineo", "Zydrune", ""], ["McCulla", "Laura", ""]]}, {"id": "1911.13250", "submitter": "Anush Sankaran", "authors": "Raunak Sinha, Anush Sankaran, Mayank Vatsa, Richa Singh", "title": "AuthorGAN: Improving GAN Reproducibility using a Modular GAN Framework", "comments": "NeurIPS 2019, MLSys: Workshop on Systems for ML", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models are becoming increasingly popular in the literature, with\nGenerative Adversarial Networks (GAN) being the most successful variant, yet.\nWith this increasing demand and popularity, it is becoming equally difficult\nand challenging to implement and consume GAN models. A qualitative user survey\nconducted across 47 practitioners show that expert level skill is required to\nuse GAN model for a given task, despite the presence of various open source\nlibraries. In this research, we propose a novel system called AuthorGAN, aiming\nto achieve true democratization of GAN authoring. A highly modularized library\nagnostic representation of GAN model is defined to enable interoperability of\nGAN architecture across different libraries such as Keras, Tensorflow, and\nPyTorch. An intuitive drag-and-drop based visual designer is built using\nnode-red platform to enable custom architecture designing without the need for\nwriting any code. Five different GAN models are implemented as a part of this\nframework and the performance of the different GAN models are shown using the\nbenchmark MNIST dataset.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 08:29:03 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Sinha", "Raunak", ""], ["Sankaran", "Anush", ""], ["Vatsa", "Mayank", ""], ["Singh", "Richa", ""]]}]