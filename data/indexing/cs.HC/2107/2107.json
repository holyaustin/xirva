[{"id": "2107.00045", "submitter": "Nathan George", "authors": "Samuel Kuhn and Nathan George", "title": "Performance of OpenBCI EEG Binary Intent Classification with Laryngeal\n  Imagery", "comments": "https://github.com/nateGeorge/openbci_laryngeal_imagery", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  One of the greatest goals of neuroscience in recent decades has been to\nrehabilitate individuals who no longer have a functional relationship between\ntheir mind and their body. Although neuroscience has produced technologies\nwhich allow the brains of paralyzed patients to accomplish tasks such as spell\nwords or control a motorized wheelchair, these technologies utilize parts of\nthe brain which may not be optimal for simultaneous use. For example, if you\nneeded to look at flashing lights to spell words for communication, it would be\ndifficult to simultaneously look at where you are moving. To improve upon this\nissue, this study developed and tested the foundation for a speech prosthesis\nparadigm which would utilize the innate neurophysiology of the human brain's\nspeech system. In this experiment, two participants were asked to respond to a\nyes or no question via an EEG-based BCI of three different types; SSVEP-based,\nmotor imagery-based, and laryngeal-imagery-based. By comparing the accuracy of\nthe two established BCI paradigms to the novel laryngeal-imagery paradigm, we\ncan establish the relative effectiveness of the novel paradigm. Machine\nlearning algorithms were used to classify the EEG signals which had been\ntransformed into frequency space (spectrograms) and common spatial pattern\n(CSP) dimensions. The SSVEP control task was able to be classified with better\naccuracy (62.5\\%) than the no information rate of 50\\% on the test set, but\nmotor activity/imagery and laryngeal activity/imagery control tasks were not.\nAlthough the laryngeal methods did not produce accuracies above the no\ninformation rate, it is possible that with a larger amount of higher-quality\ndata, this could prove otherwise. In the future, similar research should focus\non reproducing the methods used here with better quality and more data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 18:08:06 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Kuhn", "Samuel", ""], ["George", "Nathan", ""]]}, {"id": "2107.00065", "submitter": "Connor Scully-Allison", "authors": "Connor Scully-Allison, Katherine E. Isaacs", "title": "Design and Evaluation of Scalable Representations of Communication in\n  Gantt Charts for Large-scale Execution Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gantt charts are frequently used to explore execution traces of large-scale\nparallel programs found in high-performance computing (HPC). In these\nvisualizations, each parallel processor is assigned a row showing the\ncomputation state of a processor at a particular time. Lines are drawn between\nrows to show communication between these processors. When drawn to align\nequivalent calls across rows, structures can emerge reflecting communication\npatterns employed by the executing code. However, though these structures have\nthe same definition at any scale, they are obscured by the density of rendered\nlines when displaying more than a few hundred processors. A more scalable\nmetaphor is necessary to aid HPC experts in understanding communication in\nlarge-scale traces. To address this issue, we first conduct an exploratory\nstudy to identify what visual features are critical for determining similarity\nbetween structures shown at different scales. Based on these findings, we\ndesign a set of glyphs for displaying these structures in dense charts. We then\nconduct a pre-registered user study evaluating how well people interpret\ncommunication using our new representation versus their base depictions in\nlarge-scale Gantt charts. Through our evaluation, we find that our\nrepresentation enables users to more accurately identify communication patterns\ncompared to full renderings of dense charts. We discuss the results of our\nevaluation and findings regarding the design of metaphors for extensible\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 19:19:14 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Scully-Allison", "Connor", ""], ["Isaacs", "Katherine E.", ""]]}, {"id": "2107.00123", "submitter": "Ali Ayub", "authors": "Ali Ayub, Huiqing Hu, Guangwei Zhou, Carter Fendley, Crystal Ramsay,\n  Kathy Lou Jackson, Alan R. Wagner", "title": "If you Cheat, I Cheat: Cheating on a Collaborative Task with a Social\n  Robot", "comments": "Accepted at IEEE International Conference on Robot and Human\n  Interactive Communication (ROMAN), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots may soon play a role in higher education by augmenting learning\nenvironments and managing interactions between instructors and learners.\nLittle, however, is known about how the presence of robots in the learning\nenvironment will influence academic integrity. This study therefore\ninvestigates if and how college students cheat while engaged in a collaborative\nsorting task with a robot. We employed a 2x2 factorial design to examine the\neffects of cheating exposure (exposure to cheating or no exposure) and task\nclarity (clear or vague rules) on college student cheating behaviors while\ninteracting with a robot. Our study finds that prior exposure to cheating on\nthe task significantly increases the likelihood of cheating. Yet, the tendency\nto cheat was not impacted by the clarity of the task rules. These results\nsuggest that normative behavior by classmates may strongly influence the\ndecision to cheat while engaged in an instructional experience with a robot.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 22:11:59 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Ayub", "Ali", ""], ["Hu", "Huiqing", ""], ["Zhou", "Guangwei", ""], ["Fendley", "Carter", ""], ["Ramsay", "Crystal", ""], ["Jackson", "Kathy Lou", ""], ["Wagner", "Alan R.", ""]]}, {"id": "2107.00227", "submitter": "Wei Zeng", "authors": "Chi Zhang, Wei Zeng, Ligang Liu", "title": "UrbanVR: An immersive analytics system for context-aware urban design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Urban design is a highly visual discipline that requires visualization for\ninformed decision making. However, traditional urban design tools are mostly\nlimited to representations on 2D displays that lack intuitive awareness. The\npopularity of head-mounted displays (HMDs) promotes a promising alternative\nwith consumer-grade 3D displays. We introduce UrbanVR, an immersive analytics\nsystem with effective visualization and interaction techniques, to enable\narchitects to assess designs in a virtual reality (VR) environment.\nSpecifically, UrbanVR incorporates 1) a customized parallel coordinates plot\n(PCP) design to facilitate quantitative assessment of high-dimensional design\nmetrics, 2) a series of egocentric interactions, including gesture interactions\nand handle-bar metaphors, to facilitate user interactions, and 3) a viewpoint\noptimization algorithm to help users explore both the PCP for quantitative\nanalysis, and objects of interest for context awareness. Effectiveness and\nfeasibility of the system are validated through quantitative user studies and\nqualitative expert feedbacks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 05:49:19 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Zhang", "Chi", ""], ["Zeng", "Wei", ""], ["Liu", "Ligang", ""]]}, {"id": "2107.00377", "submitter": "Enes Yigitbas", "authors": "Enes Yigitbas, Jonas Klauke, Sebastian Gottschalk, Gregor Engels", "title": "VREUD -- An End-User Development Tool to Simplify the Creation of\n  Interactive VR Scenes", "comments": "Preprint - accepted at IEEE Symposium on Visual Languages and\n  Human-Centric Computing (VL/HCC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in Virtual Reality (VR) technology and the increased\navailability of VR-equipped devices enable a wide range of consumer-oriented\napplications. For novice developers, however, creating interactive scenes for\nVR applications is a complex and cumbersome task that requires high technical\nknowledge which is often missing. This hinders the potential of enabling\nnovices to create, modify, and execute their own interactive VR scenes.\nAlthough recent authoring tools for interactive VR scenes are promising, most\nof them focus on expert professionals as the target group and neglect the\nnovices with low programming knowledge. To lower the entry barrier, we provide\nan open-source web-based End-User Development (EUD) tool, called VREUD, that\nsupports the rapid construction and execution of interactive VR scenes.\nConcerning construction, VREUD enables the specification of the VR scene\nincluding interactions and tasks. Furthermore, VREUD supports the execution and\nimmersive experience of the created interactive VR scenes on VR head-mounted\ndisplays. Based on a user study, we have analyzed the effectiveness,\nefficiency, and user satisfaction of VREUD which shows promising results to\nempower novices in creating their interactive VR scenes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 11:30:42 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Yigitbas", "Enes", ""], ["Klauke", "Jonas", ""], ["Gottschalk", "Sebastian", ""], ["Engels", "Gregor", ""]]}, {"id": "2107.00389", "submitter": "Nan Gao", "authors": "Nan Gao, Mohammad Saiedur Rahaman, Wei Shao, Flora D. Salim", "title": "Investigating the Reliability of Self-report Survey in the Wild: The\n  Quest for Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring human mental state (e.g., emotion, depression, engagement) with\nsensing technology is one of the most valuable challenges in the affective\ncomputing area, which has a profound impact in all industries interacting with\nhumans. The self-report survey is the most common way to quantify how people\nthink, but prone to subjectivity and various responses bias. It is usually used\nas the ground truth for human mental state prediction. In recent years, many\ndata-driven machine learning models are built based on self-report annotations\nas the target value. In this research, we investigate the reliability of\nself-report surveys in the wild by studying the confidence level of responses\nand survey completion time. We conduct a case study (i.e., student engagement\ninference) by recruiting 23 students in a high school setting over a period of\n4 weeks. Our participants volunteered 488 self-reported responses and data from\ntheir wearable sensors. We also find the physiologically measured student\nengagement and perceived student engagement are not always consistent. The\nfindings from this research have great potential to benefit future studies in\npredicting engagement, depression, stress, and other emotion-related states in\nthe field of affective computing and sensing technologies.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:01:34 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Gao", "Nan", ""], ["Rahaman", "Mohammad Saiedur", ""], ["Shao", "Wei", ""], ["Salim", "Flora D.", ""]]}, {"id": "2107.00456", "submitter": "Xiaotian Lu", "authors": "Xiaotian Lu, Arseny Tolmachev, Tatsuya Yamamoto, Koh Takeuchi, Seiji\n  Okajima, Tomoyoshi Takebayashi, Koji Maruhashi, Hisashi Kashima", "title": "Crowdsourcing Evaluation of Saliency-based XAI Methods", "comments": "16 pages, 7 figures, 2 tables, Accepted for ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the reasons behind the predictions made by deep neural networks\nis critical for gaining human trust in many important applications, which is\nreflected in the increasing demand for explainability in AI (XAI) in recent\nyears. Saliency-based feature attribution methods, which highlight important\nparts of images that contribute to decisions by classifiers, are often used as\nXAI methods, especially in the field of computer vision. In order to compare\nvarious saliency-based XAI methods quantitatively, several approaches for\nautomated evaluation schemes have been proposed; however, there is no guarantee\nthat such automated evaluation metrics correctly evaluate explainability, and a\nhigh rating by an automated evaluation scheme does not necessarily mean a high\nexplainability for humans. In this study, instead of the automated evaluation,\nwe propose a new human-based evaluation scheme using crowdsourcing to evaluate\nXAI methods. Our method is inspired by a human computation game, \"Peek-a-boom\",\nand can efficiently compare different XAI methods by exploiting the power of\ncrowds. We evaluate the saliency maps of various XAI methods on two datasets\nwith automated and crowd-based evaluation schemes. Our experiments show that\nthe result of our crowd-based evaluation scheme is different from those of\nautomated evaluation schemes. In addition, we regard the crowd-based evaluation\nresults as ground truths and provide a quantitative performance measure to\ncompare different automated evaluation schemes. We also discuss the impact of\ncrowd workers on the results and show that the varying ability of crowd workers\ndoes not significantly impact the results.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 17:37:53 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Lu", "Xiaotian", ""], ["Tolmachev", "Arseny", ""], ["Yamamoto", "Tatsuya", ""], ["Takeuchi", "Koh", ""], ["Okajima", "Seiji", ""], ["Takebayashi", "Tomoyoshi", ""], ["Maruhashi", "Koji", ""], ["Kashima", "Hisashi", ""]]}, {"id": "2107.00480", "submitter": "Nadejda Roubtsova", "authors": "Nadejda Roubtsova, Martin Parsons, Nicola Binetti, Isabelle Mareschal,\n  Essi Viding and Darren Cosker", "title": "EmoGen: Quantifiable Emotion Generation and Analysis for Experimental\n  Psychology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D facial modelling and animation in computer vision and graphics\ntraditionally require either digital artist's skill or complex pipelines with\nobjective-function-based solvers to fit models to motion capture. This\ninaccessibility of quality modelling to a non-expert is an impediment to\neffective quantitative study of facial stimuli in experimental psychology. The\nEmoGen methodology we present in this paper solves the issue democratising\nfacial modelling technology. EmoGen is a robust and configurable framework\nletting anyone author arbitrary quantifiable facial expressions in 3D through a\nuser-guided genetic algorithm search. Beyond sample generation, our methodology\nis made complete with techniques to analyse distributions of these expressions\nin a principled way. This paper covers the technical aspects of expression\ngeneration, specifically our production-quality facial blendshape model,\nautomatic corrective mechanisms of implausible facial configurations in the\nabsence of artist's supervision and the genetic algorithm implementation\nemployed in the model space search. Further, we provide a comparative\nevaluation of ways to quantify generated facial expressions in the blendshape\nand geometric domains and compare them theoretically and empirically. The\npurpose of this analysis is 1. to define a similarity cost function to simulate\nmodel space search for convergence and parameter dependence assessment of the\ngenetic algorithm and 2. to inform the best practices in the data distribution\nanalysis for experimental psychology.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 14:23:37 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Roubtsova", "Nadejda", ""], ["Parsons", "Martin", ""], ["Binetti", "Nicola", ""], ["Mareschal", "Isabelle", ""], ["Viding", "Essi", ""], ["Cosker", "Darren", ""]]}, {"id": "2107.00690", "submitter": "Manolis Chiou", "authors": "Manolis Chiou, Faye McCabe, Markella Grigoriou, Rustam Stolkin", "title": "Trust, Shared Understanding and Locus of Control in Mixed-Initiative\n  Robotic Systems", "comments": "Pre-print of the accepted paper to appear in IEEE RO-MAN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how trust, shared understanding between a human\noperator and a robot, and the Locus of Control (LoC) personality trait, evolve\nand affect Human-Robot Interaction (HRI) in mixed-initiative robotic systems.\nAs such systems become more advanced and able to instigate actions alongside\nhuman operators, there is a shift from robots being perceived as a tool to\nbeing a team-mate. Hence, the team-oriented human factors investigated in this\npaper (i.e. trust, shared understanding, and LoC) can play a crucial role in\nefficient HRI. Here, we present the results from an experiment inspired by a\ndisaster response scenario in which operators remotely controlled a mobile\nrobot in navigation tasks, with either human-initiative or mixed-initiative\ncontrol, switching dynamically between two different levels of autonomy:\nteleoperation and autonomous navigation. Evidence suggests that operators\ntrusted and developed an understanding of the robotic systems, especially in\nmixed-initiative control, where trust and understanding increased over time, as\noperators became more familiar with the system and more capable of performing\nthe task. Lastly, evidence and insights are presented on how LoC affects HRI.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 18:33:41 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chiou", "Manolis", ""], ["McCabe", "Faye", ""], ["Grigoriou", "Markella", ""], ["Stolkin", "Rustam", ""]]}, {"id": "2107.00862", "submitter": "Yuanbang Li", "authors": "Yuanbang Li", "title": "User Role Discovery and Optimization Method based on K-means +\n  Reinforcement learning in Mobile Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the widespread use of mobile phones, users can share their location and\nactivity anytime, anywhere, as a form of check in data. These data reflect user\nfeatures. Long term stable, and a set of user shared features can be abstracted\nas user roles. The role is closely related to the user's social background,\noccupation, and living habits. This study provides four main contributions.\nFirstly, user feature models from different views for each user are constructed\nfrom the analysis of check in data. Secondly, K Means algorithm is used to\ndiscover user roles from user features. Thirdly, a reinforcement learning\nalgorithm is proposed to strengthen the clustering effect of user roles and\nimprove the stability of the clustering result. Finally, experiments are used\nto verify the validity of the method, the results of which show the\neffectiveness of the method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 06:40:12 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Li", "Yuanbang", ""]]}, {"id": "2107.00868", "submitter": "Yuanbang Li", "authors": "Yuanbang Li", "title": "Construction and Adaptability Analysis of User's Preference Models Based\n  on Check-in Data in LBSN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the widespread use of mobile phones, users can share their location\nanytime, anywhere, as a form of check-in data. These data reflect user\npreferences. Furthermore, the preference rules for different users vary. How to\ndiscover a user's preference from their related information and how to validate\nwhether a preference model is suited to a user is important for providing a\nsuitable service to the user. This study provides four main contributions.\nFirst, multiple preference models from different views for each user are\nconstructed. Second, an algorithm is proposed to validate whether a preference\nmodel is applicable to the user by calculating the stability value of the\nuser's long-term check-in data for each model. Third, a unified model, i.e., a\nmulti-channel convolutional neural network is used to characterize this\napplicability. Finally, three datasets from multiple sources are used to verify\nthe validity of the method, the results of which show the effectiveness of the\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 06:50:29 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 03:36:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Li", "Yuanbang", ""]]}, {"id": "2107.00887", "submitter": "Shreyas Hampali", "authors": "Shreyas Hampali, Sayan Deb Sarkar, Vincent Lepetit", "title": "HO-3D_v3: Improving the Accuracy of Hand-Object Annotations of the HO-3D\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HO-3D is a dataset providing image sequences of various hand-object\ninteraction scenarios annotated with the 3D pose of the hand and the object and\nwas originally introduced as HO-3D_v2. The annotations were obtained\nautomatically using an optimization method, 'HOnnotate', introduced in the\noriginal paper. HO-3D_v3 provides more accurate annotations for both the hand\nand object poses thus resulting in better estimates of contact regions between\nthe hand and the object. In this report, we elaborate on the improvements to\nthe HOnnotate method and provide evaluations to compare the accuracy of\nHO-3D_v2 and HO-3D_v3. HO-3D_v3 results in 4mm higher accuracy compared to\nHO-3D_v2 for hand poses while exhibiting higher contact regions with the object\nsurface.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 08:06:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Hampali", "Shreyas", ""], ["Sarkar", "Sayan Deb", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2107.00948", "submitter": "Haoyi Xiong", "authors": "Zhiyuan Wang, Haoyi Xiong, Jie Zhang, Sijia Yang, Mehdi Boukhechba,\n  Laura E. Barnes, Daqing Zhang", "title": "From Personalized Medicine to Population Health: A Survey of mHealth\n  Sensing Techniques", "comments": "Submitted to a journal for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile Sensing Apps have been widely used as a practical approach to collect\nbehavioral and health-related information from individuals and provide timely\nintervention to promote health and well-beings, such as mental health and\nchronic cares. As the objectives of mobile sensing could be either \\emph{(a)\npersonalized medicine for individuals} or \\emph{(b) public health for\npopulations}, in this work we review the design of these mobile sensing apps,\nand propose to categorize the design of these apps/systems in two paradigms --\n\\emph{(i) Personal Sensing} and \\emph{(ii) Crowd Sensing} paradigms. While both\nsensing paradigms might incorporate with common ubiquitous sensing\ntechnologies, such as wearable sensors, mobility monitoring, mobile data\noffloading, and/or cloud-based data analytics to collect and process sensing\ndata from individuals, we present a novel taxonomy system with two major\ncomponents that can specify and classify apps/systems from aspects of the\nlife-cycle of mHealth Sensing: \\emph{(1) Sensing Task Creation \\&\nParticipation}, \\emph{(2) Health Surveillance \\& Data Collection}, and\n\\emph{(3) Data Analysis \\& Knowledge Discovery}. With respect to different\ngoals of the two paradigms, this work systematically reviews this field, and\nsummarizes the design of typical apps/systems in the view of the configurations\nand interactions between these two components. In addition to summarization,\nthe proposed taxonomy system also helps figure out the potential directions of\nmobile sensing for health from both personalized medicines and population\nhealth perspectives.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:16:21 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Wang", "Zhiyuan", ""], ["Xiong", "Haoyi", ""], ["Zhang", "Jie", ""], ["Yang", "Sijia", ""], ["Boukhechba", "Mehdi", ""], ["Barnes", "Laura E.", ""], ["Zhang", "Daqing", ""]]}, {"id": "2107.00949", "submitter": "Christian Guckelsberger", "authors": "Christian Guckelsberger, Anna Kantosalo, Santiago Negrete-Yankelevich\n  and Tapio Takala", "title": "Embodiment and Computational Creativity", "comments": "10 pages, 1 Table, 1 Figure. Accepted as full paper at the\n  International Conference on Computational Creativity (ICCC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We conjecture that creativity and the perception of creativity are, at least\nto some extent, shaped by embodiment. This makes embodiment highly relevant for\nComputational Creativity (CC) research, but existing research is scarce and the\nuse of the concept highly ambiguous. We overcome this situation by means of a\nsystematic review and a prescriptive analysis of publications at the\nInternational Conference on Computational Creativity. We adopt and extend an\nestablished typology of embodiment to resolve ambiguity through identifying and\ncomparing different usages of the concept. We collect, contextualise and\nhighlight opportunities and challenges in embracing embodiment in CC as a\nreference for research, and put forward important directions to further the\nembodied CC research programme.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 10:18:55 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Guckelsberger", "Christian", ""], ["Kantosalo", "Anna", ""], ["Negrete-Yankelevich", "Santiago", ""], ["Takala", "Tapio", ""]]}, {"id": "2107.01001", "submitter": "Peng Yang", "authors": "Peng Yang, Tony Q. S. Quek, Jingxuan Chen, Chaoqun You, and Xianbin\n  Cao", "title": "Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets\n  Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the problem of providing ultra-reliable and\nenergy-efficient virtual reality (VR) experiences for wireless mobile users. To\nensure reliable ultra-high-definition (UHD) video frame delivery to mobile\nusers and enhance their immersive visual experiences, a coordinated multipoint\n(CoMP) transmission technique and millimeter wave (mmWave) communications are\nexploited. Owing to user movement and time-varying wireless channels, the\nwireless VR experience enhancement problem is formulated as a\nsequence-dependent and mixed-integer problem with a goal of maximizing users'\nfeeling of presence (FoP) in the virtual world, subject to power consumption\nconstraints on access points (APs) and users' head-mounted displays (HMDs). The\nproblem, however, is hard to be directly solved due to the lack of users'\naccurate tracking information and the sequence-dependent and mixed-integer\ncharacteristics. To overcome this challenge, we develop a parallel echo state\nnetwork (ESN) learning method to predict users' tracking information by\ntraining fresh and historical tracking samples separately collected by APs.\nWith the learnt results, we propose a deep reinforcement learning (DRL) based\noptimization algorithm to solve the formulated problem. In this algorithm, we\nimplement deep neural networks (DNNs) as a scalable solution to produce integer\ndecision variables and solving a continuous power control problem to criticize\nthe integer decision variables. Finally, the performance of the proposed\nalgorithm is compared with various benchmark algorithms, and the impact of\ndifferent design parameters is also discussed. Simulation results demonstrate\nthat the proposed algorithm is more 4.14% energy-efficient than the benchmark\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:35:10 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 13:19:43 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Yang", "Peng", ""], ["Quek", "Tony Q. S.", ""], ["Chen", "Jingxuan", ""], ["You", "Chaoqun", ""], ["Cao", "Xianbin", ""]]}, {"id": "2107.01068", "submitter": "Shahab Jalalvand", "authors": "Shahab Jalalvand and Srinivas Bangalore", "title": "Unsupervised Spoken Utterance Classification", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  An intelligent virtual assistant (IVA) enables effortless conversations in\ncall routing through spoken utterance classification (SUC) which is a special\nform of spoken language understanding (SLU). Building a SUC system requires a\nlarge amount of supervised in-domain data that is not always available. In this\npaper, we introduce an unsupervised spoken utterance classification approach\n(USUC) that does not require any in-domain data except for the intent labels\nand a few para-phrases per intent. USUC is consisting of a KNN classifier (K=1)\nand a complex embedding model trained on a large amount of unsupervised\ncustomer service corpus. Among all embedding models, we demonstrate that Elmo\nworks best for USUC. However, an Elmo model is too slow to be used at run-time\nfor call routing. To resolve this issue, first, we compute the uni- and bi-gram\nembedding vectors offline and we build a lookup table of n-grams and their\ncorresponding embedding vector. Then we use this table to compute sentence\nembedding vectors at run-time, along with back-off techniques for unseen\nn-grams. Experiments show that USUC outperforms the traditional utterance\nclassification methods by reducing the classification error rate from 32.9% to\n27.0% without requiring supervised data. Moreover, our lookup and back-off\ntechnique increases the processing speed from 16 utterances per second to 118\nutterances per second.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 13:22:15 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Jalalvand", "Shahab", ""], ["Bangalore", "Srinivas", ""]]}, {"id": "2107.01086", "submitter": "Manu Airaksinen", "authors": "Manu Airaksinen, Sampsa Vanhatalo, Okko R\\\"as\\\"anen", "title": "Comparison of end-to-end neural network architectures and data\n  augmentation methods for automatic infant motility assessment using wearable\n  sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Infant motility assessment using intelligent wearables is a promising new\napproach for assessment of infant neurophysiological development, and where\nefficient signal analysis plays a central role. This study investigates the use\nof different end-to-end neural network architectures for processing infant\nmotility data from wearable sensors. We focus on the performance and\ncomputational burden of alternative sensor encoder and time-series modelling\nmodules and their combinations. In addition, we explore the benefits of data\naugmentation methods in ideal and non-ideal recording conditions. The\nexperiments are conducted using a data-set of multi-sensor movement recordings\nfrom 7-month-old infants, as captured by a recently proposed smart jumpsuit for\ninfant motility assessment. Our results indicate that the choice of the encoder\nmodule has a major impact on classifier performance. For sensor encoders, the\nbest performance was obtained with parallel 2-dimensional convolutions for\nintra-sensor channel fusion with shared weights for all sensors. The results\nalso indicate that a relatively compact feature representation is obtainable\nfor within-sensor feature extraction without a drastic loss to classifier\nperformance. Comparison of time-series models revealed that feed-forward\ndilated convolutions with residual and skip connections outperformed all\nRNN-based models in performance, training time, and training stability. The\nexperiments also indicate that data augmentation improves model robustness in\nsimulated packet loss or sensor dropout scenarios. In particular, signal- and\nsensor-dropout-based augmentation strategies provided considerable boosts to\nperformance without negatively affecting the baseline performance. Overall the\nresults provide tangible suggestions on how to optimize end-to-end neural\nnetwork training for multi-channel movement sensor data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 14:02:05 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Airaksinen", "Manu", ""], ["Vanhatalo", "Sampsa", ""], ["R\u00e4s\u00e4nen", "Okko", ""]]}, {"id": "2107.01091", "submitter": "Nikiita Pavlichenko", "authors": "Nikita Pavlichenko, Ivan Stelmakh, Dmitry Ustalov", "title": "Vox Populi, Vox DIY: Benchmark Dataset for Crowdsourced Audio\n  Transcription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domain-specific data is the crux of the successful transfer of machine\nlearning systems from benchmarks to real life. Crowdsourcing has become one of\nthe standard tools for cheap and time-efficient data collection for simple\nproblems such as image classification: thanks in large part to advances in\nresearch on aggregation methods. However, the applicability of crowdsourcing to\nmore complex tasks (e.g., speech recognition) remains limited due to the lack\nof principled aggregation methods for these modalities. The main obstacle\ntowards designing advanced aggregation methods is the absence of training data,\nand in this work, we focus on bridging this gap in speech recognition. For\nthis, we collect and release CrowdSpeech -- the first publicly available\nlarge-scale dataset of crowdsourced audio transcriptions. Evaluation of\nexisting aggregation methods on our data shows room for improvement, suggesting\nthat our work may entail the design of better algorithms. At a higher level, we\nalso contribute to the more general challenge of collecting high-quality\ndatasets using crowdsourcing: we develop a principled pipeline for constructing\ndatasets of crowdsourced audio transcriptions in any novel domain. We show its\napplicability on an under-resourced language by constructing VoxDIY -- a\ncounterpart of CrowdSpeech for the Russian language. We also release the code\nthat allows a full replication of our data collection pipeline and share\nvarious insights on best practices of data collection via crowdsourcing.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 14:05:28 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pavlichenko", "Nikita", ""], ["Stelmakh", "Ivan", ""], ["Ustalov", "Dmitry", ""]]}, {"id": "2107.01126", "submitter": "Laila Melkas", "authors": "Laila Melkas, Rafael Savvides, Suyog Chandramouli, Jarmo M\\\"akel\\\"a,\n  Tuomo Nieminen, Ivan Mammarella and Kai Puolam\\\"aki", "title": "Interactive Causal Structure Discovery in Earth System Sciences", "comments": "23 pages, 8 figures, to be published in Proceedings of the 2021 KDD\n  Workshop on Causal Discovery", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal structure discovery (CSD) models are making inroads into several\ndomains, including Earth system sciences. Their widespread adaptation is\nhowever hampered by the fact that the resulting models often do not take into\naccount the domain knowledge of the experts and that it is often necessary to\nmodify the resulting models iteratively. We present a workflow that is required\nto take this knowledge into account and to apply CSD algorithms in Earth system\nsciences. At the same time, we describe open research questions that still need\nto be addressed. We present a way to interactively modify the outputs of the\nCSD algorithms and argue that the user interaction can be modelled as a greedy\nfinding of the local maximum-a-posteriori solution of the likelihood function,\nwhich is composed of the likelihood of the causal model and the prior\ndistribution representing the knowledge of the expert user. We use a real-world\ndata set for examples constructed in collaboration with our co-authors, who are\nthe domain area experts. We show that finding maximally usable causal models in\nthe Earth system sciences or other similar domains is a difficult task which\ncontains many interesting open research questions. We argue that taking the\ndomain knowledge into account has a substantial effect on the final causal\nmodels discovered.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 09:23:08 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Melkas", "Laila", ""], ["Savvides", "Rafael", ""], ["Chandramouli", "Suyog", ""], ["M\u00e4kel\u00e4", "Jarmo", ""], ["Nieminen", "Tuomo", ""], ["Mammarella", "Ivan", ""], ["Puolam\u00e4ki", "Kai", ""]]}, {"id": "2107.01277", "submitter": "Mukund Telukunta", "authors": "Mukund Telukunta, Venkata Sriram Siddhardh Nadendla", "title": "Non-Comparative Fairness for Human-Auditing and Its Relation to\n  Traditional Fairness Notions", "comments": "arXiv admin note: substantial text overlap with arXiv:2009.04383", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bias evaluation in machine-learning based services (MLS) based on traditional\nalgorithmic fairness notions that rely on comparative principles is practically\ndifficult, making it necessary to rely on human auditor feedback. However, in\nspite of taking rigorous training on various comparative fairness notions,\nhuman auditors are known to disagree on various aspects of fairness notions in\npractice, making it difficult to collect reliable feedback. This paper offers a\nparadigm shift to the domain of algorithmic fairness via proposing a new\nfairness notion based on the principle of non-comparative justice. In contrary\nto traditional fairness notions where the outcomes of two individuals/groups\nare compared, our proposed notion compares the MLS' outcome with a desired\noutcome for each input. This desired outcome naturally describes a human\nauditor's expectation, and can be easily used to evaluate MLS on crowd-auditing\nplatforms. We show that any MLS can be deemed fair from the perspective of\ncomparative fairness (be it in terms of individual fairness, statistical\nparity, equal opportunity or calibration) if it is non-comparatively fair with\nrespect to a fair auditor. We also show that the converse holds true in the\ncontext of individual fairness. Given that such an evaluation relies on the\ntrustworthiness of the auditor, we also present an approach to identify fair\nand reliable auditors by estimating their biases with respect to a given set of\nsensitive attributes, as well as quantify the uncertainty in the estimation of\nbiases within a given MLS. Furthermore, all of the above results are also\nvalidated on COMPAS, German credit and Adult Census Income datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 20:05:22 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Telukunta", "Mukund", ""], ["Nadendla", "Venkata Sriram Siddhardh", ""]]}, {"id": "2107.01570", "submitter": "Reuben Kirkham", "authors": "Reuben Kirkham, Benjamin Tannert", "title": "Using Computer Simulations to Investigate the Potential Performance of\n  'A to B' Routing Systems for People with Mobility Impairments", "comments": "Accepted to Mobile HCI 2021, 21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigating from 'A to B' remains a serious problem for many people with\nmobility impairments, due to the need to avoid accessibility barriers. Yet\nthere is currently no effective routing tool that is regularly used by people\nwith disabilities in order to effectively avoid accessibility barriers in the\nbuilt environment. To explore what is required to produce an effective routing\ntool, we have conducted Monte-Carlo simulations, simulating over 460 million\njourneys. This work illustrates the need to focus on barrier minimization,\ninstead of barrier avoidance, due to the limitations of what can be achieved by\nany accessibility documentation tool. We also make a substantial contribution\nto the concern of meaningful performance metrics for activity recognition,\nillustrating how simulations can operate as useful real-world performance\nmetrics for information sources utilized by navigation systems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 08:21:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kirkham", "Reuben", ""], ["Tannert", "Benjamin", ""]]}, {"id": "2107.01669", "submitter": "Ariel Caputo", "authors": "Ariel Caputo, Andrea Giachetti, Salwa Abkal, Chiara Marchesini,\n  Massimo Zancanaro", "title": "Real vs Simulated Foveated Rendering to Reduce Visual Discomfort in\n  Virtual Reality", "comments": "9 pages, 2 figures, 1 table, to be published in proceedings of the\n  18th International Conference promoted by the IFIP Technical Committee 13 on\n  Human Computer Interaction, INTERACT 2021. August 30th September 3rd, 2021,\n  Bari, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, a study aimed at investigating the effects of real (using eye\ntracking to determine the fixation) and simulated foveated blurring in\nimmersive Virtual Reality is presented. Techniques to reduce the optical flow\nperceived at the visual field margins are often employed in immersive Virtual\nReality environments to alleviate discomfort experienced when the visual motion\nperception does not correspond to the body's acceleration. Although still\npreliminary, our results suggest that for participants with higher\nself-declared sensitivity to sickness, there might be an improvement for nausea\nwhen using blurring. The (perceived) difficulty of the task seems to improve\nwhen the real foveated method is used.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 15:54:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Caputo", "Ariel", ""], ["Giachetti", "Andrea", ""], ["Abkal", "Salwa", ""], ["Marchesini", "Chiara", ""], ["Zancanaro", "Massimo", ""]]}, {"id": "2107.01763", "submitter": "Alisa Koegel Ms", "authors": "A. Koegel, C. Furet, T. Suzuki, Y. Klebanov, J. Hu, T. Kappeler, D.\n  Okazaki, K. Matsui, T. Hiraoka, K. Shimono, K. Nakano, K. Honma, M.\n  Pennington", "title": "Exploration of increasing drivers trust in a semi-autonomous vehicle\n  through real time visualizations of collaborative driving dynamic", "comments": "8 pages, 11 figures, 2021 IEEE Intelligent Vehicles Symposium (IV21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Thinking Wave is an ongoing development of visualization concepts showing\nthe real-time effort and confidence of semi-autonomous vehicle (AV) systems.\nOffering drivers access to this information can inform their decision making,\nand enable them to handle the situation accordingly and takeover when\nnecessary. Two different visualizations have been designed, Concept one, Tidal,\ndemonstrates the AV systems effort through intensified activity of a simple\ngraphic which fluctuates in speed and frequency. Concept two, Tandem, displays\nthe effort of the AV system as well as the handling dynamic and shared\nresponsibility between the driver and the vehicle system. Working\ncollaboratively with mobility research teams at the University of Tokyo, we are\nprototyping and refining the Thinking Wave and its embodiments as we work\ntowards building a testable version integrated into a driving simulator. The\ndevelopment of the thinking wave aims to calibrate trust by increasing the\ndrivers knowledge and understanding of vehicle handling capacity. By enabling\ntransparent communication of the AV systems capacity, we hope to empower\nAV-skeptic drivers and keep over-trusting drivers on alert in the case of an\nemergency takeover situation, in order to create a safer autonomous driving\nexperience.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 02:37:07 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Koegel", "A.", ""], ["Furet", "C.", ""], ["Suzuki", "T.", ""], ["Klebanov", "Y.", ""], ["Hu", "J.", ""], ["Kappeler", "T.", ""], ["Okazaki", "D.", ""], ["Matsui", "K.", ""], ["Hiraoka", "T.", ""], ["Shimono", "K.", ""], ["Nakano", "K.", ""], ["Honma", "K.", ""], ["Pennington", "M.", ""]]}, {"id": "2107.01829", "submitter": "Jim Mainprice", "authors": "Yoojin Oh, Marc Toussaint, Jim Mainprice", "title": "A System for Traded Control Teleoperation of Manipulation Tasks using\n  Intent Prediction from Hand Gestures", "comments": "Accepted to IEEE-RoMAN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a teleoperation system that includes robot perception and\nintent prediction from hand gestures. The perception module identifies the\nobjects present in the robot workspace and the intent prediction module which\nobject the user likely wants to grasp. This architecture allows the approach to\nrely on traded control instead of direct control: we use hand gestures to\nspecify the goal objects for a sequential manipulation task, the robot then\nautonomously generates a grasping or a retrieving motion using trajectory\noptimization. The perception module relies on the model-based tracker to\nprecisely track the 6D pose of the objects and makes use of a state of the art\nlearning-based object detection and segmentation method, to initialize the\ntracker by automatically detecting objects in the scene. Goal objects are\nidentified from user hand gestures using a trained a multi-layer perceptron\nclassifier. After presenting all the components of the system and their\nempirical evaluation, we present experimental results comparing our pipeline to\na direct traded control approach (i.e., one that does not use prediction) which\nshows that using intent prediction allows to bring down the overall task\nexecution time.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 07:37:17 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Oh", "Yoojin", ""], ["Toussaint", "Marc", ""], ["Mainprice", "Jim", ""]]}, {"id": "2107.01995", "submitter": "Soheil Habibian", "authors": "Soheil Habibian, Ananth Jonnavittula, Dylan P. Losey", "title": "Here's What I've Learned: Asking Questions that Reveal Reward Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots can learn from humans by asking questions. In these questions the\nrobot demonstrates a few different behaviors and asks the human for their\nfavorite. But how should robots choose which questions to ask? Today's robots\noptimize for informative questions that actively probe the human's preferences\nas efficiently as possible. But while informative questions make sense from the\nrobot's perspective, human onlookers often find them arbitrary and misleading.\nIn this paper we formalize active preference-based learning from the human's\nperspective. We hypothesize that -- from the human's point-of-view -- the\nrobot's questions reveal what the robot has and has not learned. Our insight\nenables robots to use questions to make their learning process transparent to\nthe human operator. We develop and test a model that robots can leverage to\nrelate the questions they ask to the information these questions reveal. We\nthen introduce a trade-off between informative and revealing questions that\nconsiders both human and robot perspectives: a robot that optimizes for this\ntrade-off actively gathers information from the human while simultaneously\nkeeping the human up to date with what it has learned. We evaluate our approach\nacross simulations, online surveys, and in-person user studies. Videos of our\nuser studies and results are available here: https://youtu.be/tC6y_jHN7Vw.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 14:58:35 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Habibian", "Soheil", ""], ["Jonnavittula", "Ananth", ""], ["Losey", "Dylan P.", ""]]}, {"id": "2107.01996", "submitter": "Chao Wang Senior Scientist", "authors": "Chao Wang, Pengcheng An", "title": "Explainability via Interactivity? Supporting Nonexperts' Sensemaking of\n  Pretrained CNN by Interacting with Their Daily Surroundings", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research on Explainable AI (XAI) heavily targets on expert users\n(data scientists or AI developers). However, increasing importance has been\nargued for making AI more understandable to nonexperts, who are expected to\nleverage AI techniques, but have limited knowledge about AI. We present a\nmobile application to support nonexperts to interactively make sense of\nConvolutional Neural Networks (CNN); it allows users to play with a pretrained\nCNN by taking pictures of their surrounding objects. We use an up-to-date XAI\ntechnique (Class Activation Map) to intuitively visualize the model's decision\n(the most important image regions that lead to a certain result). Deployed in a\nuniversity course, this playful learning tool was found to support design\nstudents to gain vivid understandings about the capabilities and limitations of\npretrained CNNs in real-world environments. Concrete examples of students'\nplayful explorations are reported to characterize their sensemaking processes\nreflecting different depths of thought.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 19:22:53 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Chao", ""], ["An", "Pengcheng", ""]]}, {"id": "2107.02030", "submitter": "Michael Winter", "authors": "Michael Winter and R\\\"udiger Pryss and Thomas Probst and Winfried\n  Schlee and Miles Tallon and Ulrich Frick and Manfred Reichert", "title": "Are Non-Experts Able to Comprehend Business Process Models -- Study\n  Insights Involving Novices and Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The comprehension of business process models is crucial for enterprises.\nPrior research has shown that children as well as adolescents perceive and\ninterpret graphical representations in a different manner compared to\ngrown-ups. To evaluate this, observations in the context of business process\nmodels are presented in this paper obtained from a study on visual literacy in\ncultural education. We demonstrate that adolescents without expertise in\nprocess model comprehension are able to correctly interpret business process\nmodels expressed in terms of BPMN 2.0. In a comprehensive study, n = 205\nlearners (i.e., pupils at the age of 15) needed to answer questions related to\nprocess models they were confronted with, reflecting different levels of\ncomplexity. In addition, process models were created with varying styles of\nelement labels. Study results indicate that an abstract description (i.e.,\nusing only alphabetic letters) of process models is understood more easily\ncompared to concrete or pseudo} descriptions. As benchmark, results are\ncompared with the ones of modeling experts (n = 40). Amongst others, study\nfindings suggest using abstract descriptions in order to introduce novices to\nprocess modeling notations. With the obtained insights, we highlight that\nprocess models can be properly comprehended by novices.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jul 2021 05:36:06 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 05:32:13 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Winter", "Michael", ""], ["Pryss", "R\u00fcdiger", ""], ["Probst", "Thomas", ""], ["Schlee", "Winfried", ""], ["Tallon", "Miles", ""], ["Frick", "Ulrich", ""], ["Reichert", "Manfred", ""]]}, {"id": "2107.02033", "submitter": "Felix Biessmann", "authors": "Felix Biessmann and Dionysius Refiano", "title": "Quality Metrics for Transparent Machine Learning With and Without Humans\n  In the Loop Are Not Correlated", "comments": "Proceedings of the ICML Workshop on Theoretical Foundations,\n  Criticism, and Application Trends of Explainable AI held in conjunction with\n  the 38th International Conference on Machine Learning (ICML), a\n  non-peer-reviewed longer version was previously published as preprint here\n  arXiv:1912.05011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The field explainable artificial intelligence (XAI) has brought about an\narsenal of methods to render Machine Learning (ML) predictions more\ninterpretable. But how useful explanations provided by transparent ML methods\nare for humans remains difficult to assess. Here we investigate the quality of\ninterpretable computer vision algorithms using techniques from psychophysics.\nIn crowdsourced annotation tasks we study the impact of different\ninterpretability approaches on annotation accuracy and task time. We compare\nthese quality metrics with classical XAI, automated quality metrics. Our\nresults demonstrate that psychophysical experiments allow for robust quality\nassessment of transparency in machine learning. Interestingly the quality\nmetrics computed without humans in the loop did not provide a consistent\nranking of interpretability methods nor were they representative for how useful\nan explanation was for humans. These findings highlight the potential of\nmethods from classical psychophysics for modern machine learning applications.\nWe hope that our results provide convincing arguments for evaluating\ninterpretability in its natural habitat, human-ML interaction, if the goal is\nto obtain an authentic assessment of interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:30:51 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Biessmann", "Felix", ""], ["Refiano", "Dionysius", ""]]}, {"id": "2107.02221", "submitter": "Raz Saremi", "authors": "Razieh Saremi, Hamid Shamszare, Marzieh Lotfalian Saremi, Ye Yang", "title": "Nobody of the Crowd: An Empirical Evaluation on Worker Clustering in\n  Topcoder", "comments": "11 pages, 9 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Software crowdsourcing platforms typically employ extrinsic rewards\nsuch as rating or ranking systems to motivate workers. Such rating systems are\nnoisy and only provide limited knowledge about worker's preferences and\nperformance. Goal: The objective of this study is to empirically investigate\npatterns and effects of worker behavior in software crowdsourcing platforms in\norder to improve the success and efficiency of software crowdsourcing. Method:\nFirst, we create the bipartite network of active workers based on common\nregistration for tasks. Then, we use the Clauset-Newman-Moore graph clustering\nalgorithm to identify developer clusters in the network. Finally, we conduct an\nempirical evaluation to measure and analyze workers' behavior per identified\ncluster in the platform by workers' ranking. More specifically, workers'\nbehavior is analyzed based on worker reliability, worker trustworthiness, and\nworker success as measures for workers' performance, worker efficiency, and\nworker elasticity to represent workers' preferences, and worker contest, worker\nconfidence, and worker deceitfulness to understand workers' strategies. The\nempirical study is conducted on more than one year's real-world data from\ntopcoder, one of the leading software crowdsourcing platforms. Results: We\nidentify four clusters of active workers: mixed-ranked, high-ranked,\nmid-ranked, and low-ranked. Based on statistical analysis, this study can only\nsupport that the low ranked group associates with the highest reliable workers\nwith an average reliability of 25%, while the mixed-ranked group contains the\nmost trustworthy workers with average trustworthiness of 16%. Conclusions:\nThese findings are helpful for task requesters to understand preferences and\nrelations among unknown resources in the platform and plan for task success in\na more effective and efficient manner in a software crowdsourcing platform.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 18:41:01 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Saremi", "Razieh", ""], ["Shamszare", "Hamid", ""], ["Saremi", "Marzieh Lotfalian", ""], ["Yang", "Ye", ""]]}, {"id": "2107.02334", "submitter": "Ryan Wesslen", "authors": "Ryan Wesslen, Alireza Karduni, Douglas Markant, Wenwen Dou", "title": "Effect of uncertainty visualizations on myopic loss aversion and equity\n  premium puzzle in retirement investment decisions", "comments": "To be published in TVCG Special Issue on the 2021 IEEE Visualization\n  Conference (VIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For many households, investing for retirement is one of the most significant\ndecisions and is fraught with uncertainty. In a classic study in behavioral\neconomics, Benartzi and Thaler (1999) found evidence using bar charts that\ninvestors exhibit myopic loss aversion in retirement decisions: Investors\noverly focus on the potential for short-term losses, leading them to invest\nless in riskier assets and miss out on higher long-term returns. Recently,\nadvances in uncertainty visualizations have shown improvements in\ndecision-making under uncertainty in a variety of tasks. In this paper, we\nconduct a controlled and incentivized crowdsourced experiment replicating\nBenartzi and Thaler (1999) and extending it to measure the effect of different\nuncertainty representations on myopic loss aversion. Consistent with the\noriginal study, we find evidence of myopic loss aversion with bar charts and\nfind that participants make better investment decisions with longer evaluation\nperiods. We also find that common uncertainty representations such as interval\nplots and bar charts achieve the highest mean expected returns while other\nuncertainty visualizations lead to poorer long-term performance and strong\neffects on the equity premium. Qualitative feedback further suggests that\ndifferent uncertainty representations lead to visual reasoning heuristics that\ncan either mitigate or encourage a focus on potential short-term losses. We\ndiscuss implications of our results on using uncertainty visualizations for\nretirement decisions in practice and possible extensions for future work.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 01:06:27 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 13:37:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Wesslen", "Ryan", ""], ["Karduni", "Alireza", ""], ["Markant", "Douglas", ""], ["Dou", "Wenwen", ""]]}, {"id": "2107.02359", "submitter": "Shruthi Chari", "authors": "Shruthi Chari, Prithwish Chakraborty, Mohamed Ghalwash, Oshani\n  Seneviratne, Elif K. Eyigoz, Daniel M. Gruen, Fernando Suarez Saiz, Ching-Hua\n  Chen, Pablo Meyer Rojas, Deborah L. McGuinness", "title": "Leveraging Clinical Context for User-Centered Explainability: A Diabetes\n  Use Case", "comments": "4 pages, 4 tables, 3 figures, 2.5 pages appendices To appear and\n  accepted at: KDD Workshop on Applied Data Science for Healthcare (DSHealth),\n  2021, Virtual", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Academic advances of AI models in high-precision domains, like healthcare,\nneed to be made explainable in order to enhance real-world adoption. Our past\nstudies and ongoing interactions indicate that medical experts can use AI\nsystems with greater trust if there are ways to connect the model inferences\nabout patients to explanations that are tied back to the context of use.\nSpecifically, risk prediction is a complex problem of diagnostic and\ninterventional importance to clinicians wherein they consult different sources\nto make decisions. To enable the adoption of the ever improving AI risk\nprediction models in practice, we have begun to explore techniques to\ncontextualize such models along three dimensions of interest: the patients'\nclinical state, AI predictions about their risk of complications, and\nalgorithmic explanations supporting the predictions. We validate the importance\nof these dimensions by implementing a proof-of-concept (POC) in type-2 diabetes\n(T2DM) use case where we assess the risk of chronic kidney disease (CKD) - a\ncommon T2DM comorbidity. Within the POC, we include risk prediction models for\nCKD, post-hoc explainers of the predictions, and other natural-language modules\nwhich operationalize domain knowledge and CPGs to provide context. With primary\ncare physicians (PCP) as our end-users, we present our initial results and\nclinician feedback in this paper. Our POC approach covers multiple knowledge\nsources and clinical scenarios, blends knowledge to explain data and\npredictions to PCPs, and received an enthusiastic response from our medical\nexpert.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 02:44:40 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 01:19:16 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 18:35:40 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Chari", "Shruthi", ""], ["Chakraborty", "Prithwish", ""], ["Ghalwash", "Mohamed", ""], ["Seneviratne", "Oshani", ""], ["Eyigoz", "Elif K.", ""], ["Gruen", "Daniel M.", ""], ["Saiz", "Fernando Suarez", ""], ["Chen", "Ching-Hua", ""], ["Rojas", "Pablo Meyer", ""], ["McGuinness", "Deborah L.", ""]]}, {"id": "2107.02364", "submitter": "Zhe Liu", "authors": "Yuhui Su, Zhe Liu, Chunyang Chen, Junjie Wang, Qing Wang", "title": "OwlEyes-Online: A Fully Automated Platform for Detecting and Localizing\n  UI Display Issues", "comments": "Accepted to 29th International Conference on ESEC/FSE '21", "journal-ref": null, "doi": "10.1145/3468264.3473109", "report-no": "fse21demo-p3-p", "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graphical User Interface (GUI) provides visual bridges between software apps\nand end users. However, due to the compatibility of software or hardware, UI\ndisplay issues such as text overlap, blurred screen, image missing always occur\nduring GUI rendering on different devices. Because these UI display issues can\nbe found directly by human eyes, in this paper, we implement an online UI\ndisplay issue detection tool OwlEyes-Online, which provides a simple and\neasy-to-use platform for users to realize the automatic detection and\nlocalization of UI display issues. The OwlEyes-Online can automatically run the\napp and get its screenshots and XML files, and then detect the existence of\nissues by analyzing the screenshots. In addition, OwlEyes-Online can also find\nthe detailed area of the issue in the given screenshots to further remind\ndevelopers. Finally, OwlEyes-Online will automatically generate test reports\nwith UI display issues detected in app screenshots and send them to users. The\nOwlEyes-Online was evaluated and proved to be able to accurately detect UI\ndisplay issues. Tool Link: http://www.owleyes.online:7476 Github Link:\nhttps://github.com/franklinbill/owleyes Demo Video Link:\nhttps://youtu.be/002nHZBxtCY\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 02:55:35 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Su", "Yuhui", ""], ["Liu", "Zhe", ""], ["Chen", "Chunyang", ""], ["Wang", "Junjie", ""], ["Wang", "Qing", ""]]}, {"id": "2107.02472", "submitter": "Marco Guerini", "authors": "Yi-Ling Chung, Serra Sinem Tekiroglu, Sara Tonelli, Marco Guerini", "title": "Empowering NGOs in Countering Online Hate Messages", "comments": "Preprint of the paper published in Online Social Networks and Media\n  Journal (OSNEM)", "journal-ref": null, "doi": "10.1016/j.osnem.2021.100150", "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies on online hate speech have mostly focused on the automated detection\nof harmful messages. Little attention has been devoted so far to the\ndevelopment of effective strategies to fight hate speech, in particular through\nthe creation of counter-messages. While existing manual scrutiny and\nintervention strategies are time-consuming and not scalable, advances in\nnatural language processing have the potential to provide a systematic approach\nto hatred management. In this paper, we introduce a novel ICT platform that NGO\noperators can use to monitor and analyze social media data, along with a\ncounter-narrative suggestion tool. Our platform aims at increasing the\nefficiency and effectiveness of operators' activities against islamophobia. We\ntest the platform with more than one hundred NGO operators in three countries\nthrough qualitative and quantitative evaluation. Results show that NGOs favor\nthe platform solution with the suggestion tool, and that the time required to\nproduce counter-narratives significantly decreases.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 08:36:24 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chung", "Yi-Ling", ""], ["Tekiroglu", "Serra Sinem", ""], ["Tonelli", "Sara", ""], ["Guerini", "Marco", ""]]}, {"id": "2107.02543", "submitter": "Hasan Mahmud", "authors": "Hasan Mahmud, Mashrur Mahmud Morshed, Md. Kamrul Hasan", "title": "A deep-learning--based multimodal depth-aware dynamic hand gesture\n  recognition system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Any spatio-temporal movement or reorientation of the hand, done with the\nintention of conveying a specific meaning, can be considered as a hand gesture.\nInputs to hand gesture recognition systems can be in several forms, such as\ndepth images, monocular RGB, or skeleton joint points. We observe that raw\ndepth images possess low contrasts in the hand regions of interest (ROI). They\ndo not highlight important details to learn, such as finger bending information\n(whether a finger is overlapping the palm, or another finger). Recently, in\ndeep-learning--based dynamic hand gesture recognition, researchers are tying to\nfuse different input modalities (e.g. RGB or depth images and hand skeleton\njoint points) to improve the recognition accuracy. In this paper, we focus on\ndynamic hand gesture (DHG) recognition using depth quantized image features and\nhand skeleton joint points. In particular, we explore the effect of using\ndepth-quantized features in Convolutional Neural Network (CNN) and Recurrent\nNeural Network (RNN) based multi-modal fusion networks. We find that our method\nimproves existing results on the SHREC-DHG-14 dataset. Furthermore, using our\nmethod, we show that it is possible to reduce the resolution of the input\nimages by more than four times and still obtain comparable or better accuracy\nto that of the resolutions used in previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 11:18:53 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mahmud", "Hasan", ""], ["Morshed", "Mashrur Mahmud", ""], ["Hasan", "Md. Kamrul", ""]]}, {"id": "2107.02624", "submitter": "Yanou Ramon", "authors": "Yanou Ramon, Tom Vermeire, Olivier Toubia, David Martens, Theodoros\n  Evgeniou", "title": "Understanding Consumer Preferences for Explanations Generated by XAI\n  Algorithms", "comments": "18 pages, 1 appendix, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining firm decisions made by algorithms in customer-facing applications\nis increasingly required by regulators and expected by customers. While the\nemerging field of Explainable Artificial Intelligence (XAI) has mainly focused\non developing algorithms that generate such explanations, there has not yet\nbeen sufficient consideration of customers' preferences for various types and\nformats of explanations. We discuss theoretically and study empirically\npeople's preferences for explanations of algorithmic decisions. We focus on\nthree main attributes that describe automatically-generated explanations from\nexisting XAI algorithms (format, complexity, and specificity), and capture\ndifferences across contexts (online targeted advertising vs. loan applications)\nas well as heterogeneity in users' cognitive styles. Despite their popularity\namong academics, we find that counterfactual explanations are not popular among\nusers, unless they follow a negative outcome (e.g., loan application was\ndenied). We also find that users are willing to tolerate some complexity in\nexplanations. Finally, our results suggest that preferences for specific (vs.\nmore abstract) explanations are related to the level at which the decision is\nconstrued by the user, and to the deliberateness of the user's cognitive style.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 13:59:34 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Ramon", "Yanou", ""], ["Vermeire", "Tom", ""], ["Toubia", "Olivier", ""], ["Martens", "David", ""], ["Evgeniou", "Theodoros", ""]]}, {"id": "2107.02823", "submitter": "Yante Li", "authors": "Yante Li, Jinsheng Wei, Seyednavid Mohammadifoumani, Yang Liu, Guoying\n  Zhao", "title": "Deep Learning based Micro-expression Recognition: A Survey", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Micro-expressions (MEs) are involuntary facial movements revealing people's\nhidden feelings in high-stake situations and have practical importance in\nmedical treatment, national security, interrogations and many human-computer\ninteraction systems. Early methods for MER mainly based on traditional\nappearance and geometry features. Recently, with the success of deep learning\n(DL) in various fields, neural networks have received increasing interests in\nMER. Different from macro-expressions, MEs are spontaneous, subtle, and rapid\nfacial movements, leading to difficult data collection, thus have small-scale\ndatasets. DL based MER becomes challenging due to above ME characters. To data,\nvarious DL approaches have been proposed to solve the ME issues and improve MER\nperformance. In this survey, we provide a comprehensive review of deep\nmicro-expression recognition (MER), including datasets, deep MER pipeline, and\nthe bench-marking of most influential methods. This survey defines a new\ntaxonomy for the field, encompassing all aspects of MER based on DL. For each\naspect, the basic approaches and advanced developments are summarized and\ndiscussed. In addition, we conclude the remaining challenges and and potential\ndirections for the design of robust deep MER systems. To the best of our\nknowledge, this is the first survey of deep MER methods, and this survey can\nserve as a reference point for future MER research.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 18:05:52 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Li", "Yante", ""], ["Wei", "Jinsheng", ""], ["Mohammadifoumani", "Seyednavid", ""], ["Liu", "Yang", ""], ["Zhao", "Guoying", ""]]}, {"id": "2107.02890", "submitter": "Raz Saremi", "authors": "Hamid Shamszare, Razieh Saremi, Sanam Jena", "title": "From Zero to The Hero: A Collaborative Market Aware Recommendation\n  System for Crowd Workers", "comments": "11 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of software crowdsourcing depends on active and trustworthy pool\nof worker supply. The uncertainty of crowd workers' behaviors makes it\nchallenging to predict workers' success and plan accordingly. In a competitive\ncrowdsourcing marketplace, competition for success over shared tasks adds\nanother layer of uncertainty in crowd workers' decision-making process.\nPreliminary analysis on software worker behaviors reveals an alarming task\ndropping rate of 82.9%. These factors lead to the need for an automated\nrecommendation system for CSD workers to improve the visibility and\npredictability of their success in the competition. To that end, this paper\nproposes a collaborative recommendation system for crowd workers. The proposed\nrecommendation system method uses five input metrics based on workers'\ncollaboration history in the pool, workers' preferences in taking tasks in\nterms of monetary prize and duration, workers' specialty, and workers'\nproficiency. The proposed method then recommends the most suitable tasks for a\nworker to compete on based on workers' probability of success in the task.\nExperimental results on 260 active crowd workers demonstrate that just\nfollowing the top three success probabilities of task recommendations, workers\ncan achieve success up to 86%\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 21:02:36 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Shamszare", "Hamid", ""], ["Saremi", "Razieh", ""], ["Jena", "Sanam", ""]]}, {"id": "2107.02965", "submitter": "Jason Orlosky", "authors": "Jason Orlosky, Misha Sra, Kenan Bekta\\c{s}, Huaishu Peng, Jeeeun Kim,\n  Nataliya Kos'myna, Tobias Hollerer, Anthony Steed, Kiyoshi Kiyokawa, Kaan\n  Ak\\c{s}it", "title": "Telelife: The Future of Remote Living", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In recent years, everyday activities such as work and socialization have\nsteadily shifted to more remote and virtual settings. With the COVID-19\npandemic, the switch from physical to virtual has been accelerated, which has\nsubstantially affected various aspects of our lives, including business,\neducation, commerce, healthcare, and personal life. This rapid and large-scale\nswitch from in-person to remote interactions has revealed that our current\ntechnologies lack functionality and are limited in their ability to recreate\ninterpersonal interactions. To help address these limitations in the future, we\nintroduce \"Telelife,\" a vision for the near future that depicts the potential\nmeans to improve remote living better aligned with how we interact, live and\nwork in the physical world. Telelife encompasses novel synergies of\ntechnologies and concepts such as digital twins, virtual prototyping, and\nattention and context-aware user interfaces with innovative hardware that can\nsupport ultrarealistic graphics, user state detection, and more. These ideas\nwill guide the transformation of our daily lives and routines soon, targeting\nthe year 2035. In addition, we identify opportunities across high-impact\napplications in domains related to this vision of Telelife. Along with a recent\nsurvey of relevant fields such as human-computer interaction, pervasive\ncomputing, and virtual reality, the directions outlined in this paper will\nguide future research on remote living.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 01:05:27 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Orlosky", "Jason", ""], ["Sra", "Misha", ""], ["Bekta\u015f", "Kenan", ""], ["Peng", "Huaishu", ""], ["Kim", "Jeeeun", ""], ["Kos'myna", "Nataliya", ""], ["Hollerer", "Tobias", ""], ["Steed", "Anthony", ""], ["Kiyokawa", "Kiyoshi", ""], ["Ak\u015fit", "Kaan", ""]]}, {"id": "2107.03172", "submitter": "Kailun Yang", "authors": "Jiaming Zhang, Kailun Yang, Angela Constantinescu, Kunyu Peng, Karin\n  M\\\"uller, Rainer Stiefelhagen", "title": "Trans4Trans: Efficient Transformer for Transparent Object Segmentation\n  to Help Visually Impaired People Navigate in the Real World", "comments": "8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common fully glazed facades and transparent objects present architectural\nbarriers and impede the mobility of people with low vision or blindness, for\ninstance, a path detected behind a glass door is inaccessible unless it is\ncorrectly perceived and reacted. However, segmenting these safety-critical\nobjects is rarely covered by conventional assistive technologies. To tackle\nthis issue, we construct a wearable system with a novel dual-head Transformer\nfor Transparency (Trans4Trans) model, which is capable of segmenting general\nand transparent objects and performing real-time wayfinding to assist people\nwalking alone more safely. Especially, both decoders created by our proposed\nTransformer Parsing Module (TPM) enable effective joint learning from different\ndatasets. Besides, the efficient Trans4Trans model composed of symmetric\ntransformer-based encoder and decoder, requires little computational expenses\nand is readily deployed on portable GPUs. Our Trans4Trans model outperforms\nstate-of-the-art methods on the test sets of Stanford2D3D and Trans10K-v2\ndatasets and obtains mIoU of 45.13% and 75.14%, respectively. Through various\npre-tests and a user study conducted in indoor and outdoor scenarios, the\nusability and reliability of our assistive system have been extensively\nverified.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:06:27 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zhang", "Jiaming", ""], ["Yang", "Kailun", ""], ["Constantinescu", "Angela", ""], ["Peng", "Kunyu", ""], ["M\u00fcller", "Karin", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2107.03178", "submitter": "Francisco Cruz", "authors": "Richard Dazeley, Peter Vamplew, Cameron Foale, Charlotte Young, Sunil\n  Aryal, Francisco Cruz", "title": "Levels of explainable artificial intelligence for human-aligned\n  conversational explanations", "comments": "35 pages, 13 figures", "journal-ref": "Artificial Intelligence, 299, 103525 (2021)", "doi": "10.1016/j.artint.2021.103525", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the last few years there has been rapid research growth into eXplainable\nArtificial Intelligence (XAI) and the closely aligned Interpretable Machine\nLearning (IML). Drivers for this growth include recent legislative changes and\nincreased investments by industry and governments, along with increased concern\nfrom the general public. People are affected by autonomous decisions every day\nand the public need to understand the decision-making process to accept the\noutcomes. However, the vast majority of the applications of XAI/IML are focused\non providing low-level `narrow' explanations of how an individual decision was\nreached based on a particular datum. While important, these explanations rarely\nprovide insights into an agent's: beliefs and motivations; hypotheses of other\n(human, animal or AI) agents' intentions; interpretation of external cultural\nexpectations; or, processes used to generate its own explanation. Yet all of\nthese factors, we propose, are essential to providing the explanatory depth\nthat people require to accept and trust the AI's decision-making. This paper\naims to define levels of explanation and describe how they can be integrated to\ncreate a human-aligned conversational explanation system. In so doing, this\npaper will survey current approaches and discuss the integration of different\ntechnologies to achieve these levels with Broad eXplainable Artificial\nIntelligence (Broad-XAI), and thereby move towards high-level `strong'\nexplanations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:19:16 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Dazeley", "Richard", ""], ["Vamplew", "Peter", ""], ["Foale", "Cameron", ""], ["Young", "Charlotte", ""], ["Aryal", "Sunil", ""], ["Cruz", "Francisco", ""]]}, {"id": "2107.03180", "submitter": "Kailun Yang", "authors": "Huayao Liu, Ruiping Liu, Kailun Yang, Jiaming Zhang, Kunyu Peng,\n  Rainer Stiefelhagen", "title": "HIDA: Towards Holistic Indoor Understanding for the Visually Impaired\n  via Semantic Instance Segmentation with a Wearable Solid-State LiDAR Sensor", "comments": "10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independently exploring unknown spaces or finding objects in an indoor\nenvironment is a daily but challenging task for visually impaired people.\nHowever, common 2D assistive systems lack depth relationships between various\nobjects, resulting in difficulty to obtain accurate spatial layout and relative\npositions of objects. To tackle these issues, we propose HIDA, a lightweight\nassistive system based on 3D point cloud instance segmentation with a\nsolid-state LiDAR sensor, for holistic indoor detection and avoidance. Our\nentire system consists of three hardware components, two interactive\nfunctions~(obstacle avoidance and object finding) and a voice user interface.\nBased on voice guidance, the point cloud from the most recent state of the\nchanging indoor environment is captured through an on-site scanning performed\nby the user. In addition, we design a point cloud segmentation model with dual\nlightweight decoders for semantic and offset predictions, which satisfies the\nefficiency of the whole system. After the 3D instance segmentation, we\npost-process the segmented point cloud by removing outliers and projecting all\npoints onto a top-view 2D map representation. The system integrates the\ninformation above and interacts with users intuitively by acoustic feedback.\nThe proposed 3D instance segmentation model has achieved state-of-the-art\nperformance on ScanNet v2 dataset. Comprehensive field tests with various tasks\nin a user study verify the usability and effectiveness of our system for\nassisting visually impaired people in holistic indoor understanding, obstacle\navoidance and object search.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 12:23:53 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Liu", "Huayao", ""], ["Liu", "Ruiping", ""], ["Yang", "Kailun", ""], ["Zhang", "Jiaming", ""], ["Peng", "Kunyu", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2107.03272", "submitter": "Jesse David Dinneen", "authors": "Jesse David Dinneen and Ba Xuan Nguyen", "title": "How Big Are Peoples' Computer Files? File Size Distributions Among\n  User-managed Collections", "comments": "Final version to appear in ASIS&T `21: Proceedings of the 84th Annual\n  Meeting of the Association for Information Science & Technology, 58", "journal-ref": "ASIS&T 2021: Proceedings of the 84th Annual Meeting of the\n  Association for Information Science & Technology, 58", "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Improving file management interfaces and optimising system performance\nrequires current data about users' digital collections and particularly about\nthe file size distributions of such collections. However, prior works have\nexamined only the sizes of system files and users' work files in varied\ncontexts, and there has been no such study since 2013; it therefore remains\nunclear how today's file sizes are distributed, particularly personal files,\nand further if distributions differ among the major operating systems or common\noccupations. Here we examine such differences among 49 million files in 348\nuser collections. We find that the average file size has grown more than\nten-fold since the mid-2000s, though most files are still under 8 MB, and that\nthere are demographic and technological influences in the size distributions.\nWe discuss the implications for user interfaces, system optimisation, and PIM\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:05:35 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Dinneen", "Jesse David", ""], ["Nguyen", "Ba Xuan", ""]]}, {"id": "2107.03291", "submitter": "Jesse David Dinneen", "authors": "William Jones and Jesse David Dinneen and Robert Capra and Anne R.\n  Diekema and Manuel A. P\\'erez-Qui\\~nones", "title": "Personal Information Management", "comments": "Final version available at\n  https://www.routledgehandbooks.com/doi/10.1081/E-ELIS4-120053695", "journal-ref": "Chapter in Levine-Clark, M., & McDonald, J. (Eds.), Encyclopedia\n  of Library and Information Science, Fourth Edition, 2017, pp. 3584-3605. New\n  York, NY: Taylor & Francis", "doi": "10.1081/E-ELIS4-120053695", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Personal Information Management (PIM) refers to the practice and the study of\nthe activities a person performs in order to acquire or create, store,\norganize, maintain, retrieve, use, and distribute information in each of its\nmany forms (paper and digital, in e-mails, files, Web pages, text messages,\ntweets, posts, etc.) as needed to meet life's many goals (everyday and\nlong-term, work-related and not) and to fulfill life's many roles and\nresponsibilities (as parent, spouse, friend, employee, member of community,\netc.). PIM activities are an effort to establish, use, and maintain a mapping\nbetween information and need. Activities of finding (and re-finding) move from\na current need toward information while activities of keeping move from\nencountered information toward anticipated need. Meta-level activities such as\nmaintaining, organizing, and managing the flow of information focus on the\nmapping itself. Tools and techniques of PIM can promote information integration\nwith benefits for each kind of PIM activity and across the life cycle of\npersonal information. Understanding how best to accomplish this integration\nwithout inadvertently creating problems along the way is a key challenge of\nPIM.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:27:28 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jones", "William", ""], ["Dinneen", "Jesse David", ""], ["Capra", "Robert", ""], ["Diekema", "Anne R.", ""], ["P\u00e9rez-Qui\u00f1ones", "Manuel A.", ""]]}, {"id": "2107.03487", "submitter": "Devansh Saxena", "authors": "Devansh Saxena, Karla Badillo-Urquiola, Pamela Wisniewski, Shion Guha", "title": "A Framework of High-Stakes Algorithmic Decision-Making for the Public\n  Sector Developed through a Case Study of Child-Welfare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms have permeated throughout civil government and society, where they\nare being used to make high-stakes decisions about human lives. In this paper,\nwe first develop a cohesive framework of algorithmic decision-making adapted\nfor the public sector (ADMAPS) that reflects the complex socio-technical\ninteractions between \\textit{human discretion}, \\textit{bureaucratic\nprocesses}, and \\textit{algorithmic decision-making} by synthesizing disparate\nbodies of work in the fields of Human-Computer Interaction (HCI), Science and\nTechnology Studies (STS), and Public Administration (PA). We then applied the\nADMAPS framework to conduct a qualitative analysis of an in-depth, eight-month\nethnographic case study of the algorithms in daily use within a child-welfare\nagency that serves approximately 900 families and 1300 children in the\nmid-western United States. Overall, we found there is a need to focus on\nstrength-based algorithmic outcomes centered in social ecological frameworks.\nIn addition, algorithmic systems need to support existing bureaucratic\nprocesses and augment human discretion, rather than replace it. Finally,\ncollective buy-in in algorithmic systems requires trust in the target outcomes\nat both the practitioner and bureaucratic levels. As a result of our study, we\npropose guidelines for the design of high-stakes algorithmic decision-making\ntools in the child-welfare system, and more generally, in the public sector. We\nempirically validate the theoretically derived ADMAPS framework to demonstrate\nhow it can be useful for systematically making pragmatic decisions about the\ndesign of algorithms for the public sector.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 21:24:35 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Saxena", "Devansh", ""], ["Badillo-Urquiola", "Karla", ""], ["Wisniewski", "Pamela", ""], ["Guha", "Shion", ""]]}, {"id": "2107.03761", "submitter": "Akhila Sri Manasa Venigalla", "authors": "Akhila Sri Manasa Venigalla, Kowndinya Boyalakunta and Sridhar\n  Chimalakonda", "title": "GitQ- Towards Using Badges as Visual Cues for GitHub Projects", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  GitHub hosts millions of software repositories, facilitating developers to\ncontribute to many projects in multiple ways. Most of the information about the\nrepositories are text-based in the form of stars, forks, commits, and so on.\nHowever, developers willing to contribute to projects on GitHub often find it\nchallenging to select appropriate projects to contribute or reuse due to the\nlarge number of repositories present on GitHub. Adding visual cues such as\nbadges, to the repositories might reduce the search space for developers.\nHence, we present GitQ, to automatically augment GitHub repositories with\nbadges representing information about source code and project maintenance.\nPresenting GitQ as a browser plugin to GitHub, could make it easily accessible\nto developers using GitHub. GitQ is evaluated with 15 developers based on the\nUTAUT model to understand developer perception towards its usefulness. We\nobserved that 11 out of 15 developers perceived GitQ to be useful in\nidentifying the right set of repositories using visual cues generated by GitQ.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:11:48 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Venigalla", "Akhila Sri Manasa", ""], ["Boyalakunta", "Kowndinya", ""], ["Chimalakonda", "Sridhar", ""]]}, {"id": "2107.03783", "submitter": "Berkay Kopru", "authors": "Berkay K\\\"opr\\\"u, Engin Erzin", "title": "Use of Affective Visual Information for Summarization of Human-Centric\n  Videos", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing volume of user-generated human-centric video content and their\napplications, such as video retrieval and browsing, require compact\nrepresentations that are addressed by the video summarization literature.\nCurrent supervised studies formulate video summarization as a\nsequence-to-sequence learning problem and the existing solutions often neglect\nthe surge of human-centric view, which inherently contains affective content.\nIn this study, we investigate the affective-information enriched supervised\nvideo summarization task for human-centric videos. First, we train a visual\ninput-driven state-of-the-art continuous emotion recognition model (CER-NET) on\nthe RECOLA dataset to estimate emotional attributes. Then, we integrate the\nestimated emotional attributes and the high-level representations from the\nCER-NET with the visual information to define the proposed affective video\nsummarization architectures (AVSUM). In addition, we investigate the use of\nattention to improve the AVSUM architectures and propose two new architectures\nbased on temporal attention (TA-AVSUM) and spatial attention (SA-AVSUM). We\nconduct video summarization experiments on the TvSum database. The proposed\nAVSUM-GRU architecture with an early fusion of high level GRU embeddings and\nthe temporal attention based TA-AVSUM architecture attain competitive video\nsummarization performances by bringing strong performance improvements for the\nhuman-centric videos compared to the state-of-the-art in terms of F-score and\nself-defined face recall metrics.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 11:46:04 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["K\u00f6pr\u00fc", "Berkay", ""], ["Erzin", "Engin", ""]]}, {"id": "2107.03823", "submitter": "Alexander Sch\\\"afer", "authors": "Alexander Sch\\\"afer, Gerd Reis, Didier Stricker", "title": "Investigating the Sense of Presence Between Handcrafted and Panorama\n  Based Virtual Environments", "comments": null, "journal-ref": null, "doi": "10.1145/3473856.3474024", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality applications are becoming increasingly mature. The\nrequirements and complexity of such systems is steadily increasing. Realistic\nand detailed environments are often omitted in order to concentrate on the\ninteraction possibilities within the application. Creating an accurate and\nrealistic virtual environment is not a task for laypeople, but for experts in\n3D design and modeling. To save costs and avoid hiring experts, panorama images\nare often used to create realistic looking virtual environments. These images\ncan be captured and provided by non-experts. Panorama images are an alternative\nto handcrafted 3D models in many cases because they offer immersion and a scene\ncan be captured in great detail with the touch of a button. This work\ninvestigates whether it is advisable to recreate an environment in detail by\nhand or whether it is recommended to use panorama images for virtual\nenvironments in certain scenarios. For this purpose, an interactive virtual\nenvironment was created in which a handmade 3D environment is almost\nindistinguishable from an environment created with panorama images. Interactive\nelements were added and a user study was conducted to investigate the effect of\nboth environments to the user. The study conducted indicates that panorama\nimages can be a useful substitute for 3D modeled environments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:09:54 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Sch\u00e4fer", "Alexander", ""], ["Reis", "Gerd", ""], ["Stricker", "Didier", ""]]}, {"id": "2107.03833", "submitter": "Alexander Sch\\\"afer", "authors": "Alexander Sch\\\"afer, Gerd Reis, Didier Stricker", "title": "Towards Collaborative Photorealistic VR Meeting Rooms", "comments": null, "journal-ref": null, "doi": "10.1145/3340764.3344466", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When designing 3D applications it is necessary to find a compromise between\ncost (e.g. money, time) and achievable realism of the virtual environment.\nReusing existing assets has an impact on the uniqueness of the application and\ncreating high quality 3D assets is very time consuming and expensive. We aim\nfor a low cost, high quality and minimal time effort solution to create virtual\nenvironments. This paper's main contribution is a novel way of creating a\nvirtual meeting application by utilizing augmented spherical images for photo\nrealistic virtual environments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 13:22:20 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 05:29:05 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Sch\u00e4fer", "Alexander", ""], ["Reis", "Gerd", ""], ["Stricker", "Didier", ""]]}, {"id": "2107.03886", "submitter": "Geesung Oh", "authors": "Geesung Oh, Euiseok Jeong and Sejoon Lim", "title": "Causal affect prediction model using a facial image sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among human affective behavior research, facial expression recognition\nresearch is improving in performance along with the development of deep\nlearning. However, for improved performance, not only past images but also\nfuture images should be used along with corresponding facial images, but there\nare obstacles to the application of this technique to real-time environments.\nIn this paper, we propose the causal affect prediction network (CAPNet), which\nuses only past facial images to predict corresponding affective valence and\narousal. We train CAPNet to learn causal inference between past images and\ncorresponding affective valence and arousal through supervised learning by\npairing the sequence of past images with the current label using the Aff-Wild2\ndataset. We show through experiments that the well-trained CAPNet outperforms\nthe baseline of the second challenge of the Affective Behavior Analysis\nin-the-wild (ABAW2) Competition by predicting affective valence and arousal\nonly with past facial images one-third of a second earlier. Therefore, in\nreal-time application, CAPNet can reliably predict affective valence and\narousal only with past data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:13:50 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Oh", "Geesung", ""], ["Jeong", "Euiseok", ""], ["Lim", "Sejoon", ""]]}, {"id": "2107.03907", "submitter": "Jason R.C. Nurse Dr", "authors": "Jason R. C. Nurse and Nikki Williams and Emily Collins and Niki\n  Panteli and John Blythe and Ben Koppelman", "title": "Remote Working Pre- and Post-COVID-19: An Analysis of New Threats and\n  Risks to Security and Privacy", "comments": "HCI International 2021 (HCII 2021)", "journal-ref": null, "doi": "10.1007/978-3-030-78645-8_74", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 has radically changed society as we know it. To reduce the spread of\nthe virus, millions across the globe have been forced to work remotely, often\nin make-shift home offices, and using a plethora of new, unfamiliar digital\ntechnologies. In this article, we critically analyse cyber security and privacy\nconcerns arising due to remote working during the coronavirus pandemic. Through\nour work, we discover a series of security risks emerging because of the\nrealities of this period. For instance, lack of remote-working security\ntraining, heightened stress and anxiety, rushed technology deployment, and the\npresence of untrusted individuals in a remote-working environment (e.g., in\nflatshares), can result in new cyber-risk. Simultaneously, we find that as\norganisations look to manage these and other risks posed by their remote\nworkforces, employee's privacy (including personal information and activities)\nis often compromised. This is apparent in the significant adoption of remote\nworkplace monitoring, management and surveillance technologies. Such\ntechnologies raise several privacy and ethical questions, and further highlight\nthe tension between security and privacy going forward.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 15:39:56 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Nurse", "Jason R. C.", ""], ["Williams", "Nikki", ""], ["Collins", "Emily", ""], ["Panteli", "Niki", ""], ["Blythe", "John", ""], ["Koppelman", "Ben", ""]]}, {"id": "2107.03924", "submitter": "Mahmoud Nasr Mr", "authors": "Mahmoud Nasr, MD. Milon Islam, Shady Shehata, Fakhri Karray and Yuri\n  Quintana", "title": "Smart Healthcare in the Age of AI: Recent Advances, Challenges, and\n  Future Prospects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant increase in the number of individuals with chronic ailments\n(including the elderly and disabled) has dictated an urgent need for an\ninnovative model for healthcare systems. The evolved model will be more\npersonalized and less reliant on traditional brick-and-mortar healthcare\ninstitutions such as hospitals, nursing homes, and long-term healthcare\ncenters. The smart healthcare system is a topic of recently growing interest\nand has become increasingly required due to major developments in modern\ntechnologies, especially in artificial intelligence (AI) and machine learning\n(ML). This paper is aimed to discuss the current state-of-the-art smart\nhealthcare systems highlighting major areas like wearable and smartphone\ndevices for health monitoring, machine learning for disease diagnosis, and the\nassistive frameworks, including social robots developed for the ambient\nassisted living environment. Additionally, the paper demonstrates software\nintegration architectures that are very significant to create smart healthcare\nsystems, integrating seamlessly the benefit of data analytics and other tools\nof AI. The explained developed systems focus on several facets: the\ncontribution of each developed framework, the detailed working procedure, the\nperformance as outcomes, and the comparative merits and limitations. The\ncurrent research challenges with potential future directions are addressed to\nhighlight the drawbacks of existing systems and the possible methods to\nintroduce novel frameworks, respectively. This review aims at providing\ncomprehensive insights into the recent developments of smart healthcare systems\nto equip experts to contribute to the field.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 05:10:47 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Nasr", "Mahmoud", ""], ["Islam", "MD. Milon", ""], ["Shehata", "Shady", ""], ["Karray", "Fakhri", ""], ["Quintana", "Yuri", ""]]}, {"id": "2107.03959", "submitter": "Jason R.C. Nurse Dr", "authors": "Rahime Belen Saglam and Jason R.C. Nurse and Duncan Hodges", "title": "Privacy Concerns in Chatbot Interactions: When to Trust and When to\n  Worry", "comments": null, "journal-ref": "23rd International Conference on Human-Computer Interaction (HCII\n  2021)", "doi": "10.1007/978-3-030-78642-7_53", "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through advances in their conversational abilities, chatbots have started to\nrequest and process an increasing variety of sensitive personal information.\nThe accurate disclosure of sensitive information is essential where it is used\nto provide advice and support to users in the healthcare and finance sectors.\nIn this study, we explore users' concerns regarding factors associated with the\nuse of sensitive data by chatbot providers. We surveyed a representative sample\nof 491 British citizens. Our results show that the user concerns focus on\ndeleting personal information and concerns about their data's inappropriate\nuse. We also identified that individuals were concerned about losing control\nover their data after a conversation with conversational agents. We found no\neffect from a user's gender or education but did find an effect from the user's\nage, with those over 45 being more concerned than those under 45. We also\nconsidered the factors that engender trust in a chatbot. Our respondents'\nprimary focus was on the chatbot's technical elements, with factors such as the\nresponse quality being identified as the most critical factor. We again found\nno effect from the user's gender or education level; however, when we\nconsidered some social factors (e.g. avatars or perceived 'friendliness'), we\nfound those under 45 years old rated these as more important than those over\n45. The paper concludes with a discussion of these results within the context\nof designing inclusive, digital systems that support a wide range of users.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 16:31:58 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Saglam", "Rahime Belen", ""], ["Nurse", "Jason R. C.", ""], ["Hodges", "Duncan", ""]]}, {"id": "2107.04007", "submitter": "Melissa Roemmele", "authors": "Melissa Roemmele", "title": "Inspiration through Observation: Demonstrating the Influence of\n  Automatically Generated Text on Creative Writing", "comments": "Accepted at ICCC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Getting machines to generate text perceived as creative is a long-pursued\ngoal. A growing body of research directs this goal towards augmenting the\ncreative writing abilities of human authors. In this paper, we pursue this\nobjective by analyzing how observing examples of automatically generated text\ninfluences writing. In particular, we examine a task referred to as sentence\ninfilling, which involves transforming a list of words into a complete\nsentence. We emphasize \"storiability\" as a desirable feature of the resulting\nsentences, where \"storiable\" sentences are those that suggest a story a reader\nwould be curious to hear about. Both humans and an automated system (based on a\nneural language model) performed this sentence infilling task. In one setting,\npeople wrote sentences on their own; in a different setting, people observed\nthe sentences produced by the model while writing their own sentences. Readers\nthen assigned storiability preferences to the resulting sentences in a\nsubsequent evaluation. We find that human-authored sentences were judged as\nmore storiable when authors observed the generated examples, and that\nstoriability increased as authors derived more semantic content from the\nexamples. This result gives evidence of an \"inspiration through observation\"\nparadigm for human-computer collaborative writing, through which human writing\ncan be enhanced by text generation models without directly copying their\noutput.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 17:53:22 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Roemmele", "Melissa", ""]]}, {"id": "2107.04014", "submitter": "Mitja Kulczynski", "authors": "Pamela Fleischmann and Mitja Kulczynski and Dirk Nowotka", "title": "The Show Must Go On -- Examination During a Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When unexpected incidents occur, new innovative and flexible solutions are\nrequired. If this event is something such radical and dramatic like the\nCOVID-19 pandemic, these solutions must aim to guarantee as much normality as\npossible while protecting lives. After a moment of shock our university decided\nthat the students have to be able to pursue their studies for guaranteeing a\ndegree in the expected time since most of them faced immediate financial\nproblems due to the loss of their student jobs. This implied, for us as\nteachers, that we had to reorganise not only the teaching methods from nearly\none day to the next, but we also had to come up with an adjusted way of\nexaminations which had to take place in person with pen and paper under strict\nhygiene rules. On the other hand the correction should avoid personal contacts.\nWe developed a framework which allowed us to correct the digitalised exams\nsafely at home while providing the high standards given by the general data\nprotection regulation of our country. Moreover, the time spent in the offices\ncould be reduced to a minimum thanks to automatically generated exam sheets,\nautomatically re-digitalised and sorted worked-on exams.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:22:52 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Fleischmann", "Pamela", ""], ["Kulczynski", "Mitja", ""], ["Nowotka", "Dirk", ""]]}, {"id": "2107.04117", "submitter": "Evangelos Pournaras", "authors": "Evangelos Pournaras, Atif Nabi Ghulam, Renato Kunz, Regula H\\\"anggli", "title": "Crowd Sensing and Living Lab Outdoor Experimentation Made Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outdoor `living lab' experimentation using pervasive computing provides new\nopportunities: higher realism, external validity and large-scale\nsocio-spatio-temporal observations. However, experimentation `in the wild' is\nhighly complex and costly. Noise, biases, privacy concerns to comply with\nstandards of ethical review boards, remote moderation, control of experimental\nconditions and equipment perplex the collection of high-quality data for causal\ninference. This article introduces Smart Agora, a novel open-source software\nplatform for rigorous systematic outdoor experimentation. Without writing a\nsingle line of code, highly complex experimental scenarios are visually\ndesigned and automatically deployed to smart phones. Novel geolocated survey\nand sensor data are collected subject of participants verifying desired\nexperimental conditions, for instance. their presence at certain urban spots.\nThis new approach drastically improves the quality and purposefulness of crowd\nsensing, tailored to conditions that confirm/reject hypotheses. The features\nthat support this innovative functionality and the broad spectrum of its\napplicability are demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 21:49:32 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Pournaras", "Evangelos", ""], ["Ghulam", "Atif Nabi", ""], ["Kunz", "Renato", ""], ["H\u00e4nggli", "Regula", ""]]}, {"id": "2107.04135", "submitter": "Congyu Wu", "authors": "Congyu Wu, Megan McMahon, Hagen Fritz, David M. Schnyer", "title": "Circadian Rhythms are Not Captured Equal: Exploring Circadian Metrics\n  Extracted by Different Computational Methods from Smartphone Accelerometer\n  and GPS Sensors in Daily Life Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circadian rhythm is the natural biological cycle manifested in human daily\nroutines. A regular and stable rhythm is found to be correlated with good\nphysical and mental health. With the wide adoption of mobile and wearable\ntechnology, many types of sensor data, such as GPS and actigraphy, provide\nevidence for researchers to objectively quantify the circadian rhythm of a user\nand further use these quantified metrics of circadian rhythm to infer the\nuser's health status. Researchers in computer science and psychology have\ninvestigated circadian rhythm using various mobile and wearable sensors in\necologically valid human sensing studies, but questions remain whether and how\ndifferent data types produce different circadian rhythm results when\nsimultaneously used to monitor a user. We hypothesize that different sensor\ndata reveal different aspects of the user's daily behavior, thus producing\ndifferent circadian rhythm patterns. In this paper we focus on two data types:\nGPS and accelerometer data from smartphones. We used smartphone data from 225\ncollege student participants and applied four circadian rhythm characterization\nmethods. We found significant and interesting discrepancies in the rhythmic\npatterns discovered among sensors, which suggests circadian rhythms discovered\nfrom different personal tracking sensors have different levels of sensitivity\nto device usage and aspects of daily behavior.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 22:30:40 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wu", "Congyu", ""], ["McMahon", "Megan", ""], ["Fritz", "Hagen", ""], ["Schnyer", "David M.", ""]]}, {"id": "2107.04137", "submitter": "Congyu Wu", "authors": "Congyu Wu, Hagen Fritz, Cameron Craddock, Kerry Kinney, Darla\n  Castelli, David M. Schnyer", "title": "Exploring Post COVID-19 Outbreak Intradaily Mobility Pattern Change in\n  College Students: a GPS-focused Smartphone Sensing Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the outbreak of the COVID-19 pandemic in 2020, most colleges and\nuniversities move to restrict campus activities, reduce indoor gatherings and\nmove instruction online. These changes required that students adapt and alter\ntheir daily routines accordingly. To investigate patterns associated with these\nbehavioral changes, we collected smartphone sensing data using the Beiwe\nplatform from two groups of undergraduate students at a major North American\nuniversity, one from January to March of 2020 (74 participants), the other from\nMay to August (52 participants), to observe the differences in students' daily\nlife patterns before and after the start of the pandemic. In this paper, we\nfocus on the mobility patterns evidenced by GPS signal tracking from the\nstudents' smartphones and report findings using several analytical methods\nincluding principal component analysis, circadian rhythm analysis, and\npredictive modeling of perceived sadness levels using mobility-based digital\nmetrics. Our findings suggest that compared to the pre-COVID group, students in\nthe mid-COVID group generally (1) registered a greater amount of midday\nmovement than movement in the morning (8-10am) and in the evening (7-9pm), as\nopposed to the other way around; (2) exhibited significantly less intradaily\nvariability in their daily movement, and (3) had a significant lower\ncorrelation between their mobility patterns and negative mood.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 22:39:03 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Wu", "Congyu", ""], ["Fritz", "Hagen", ""], ["Craddock", "Cameron", ""], ["Kinney", "Kerry", ""], ["Castelli", "Darla", ""], ["Schnyer", "David M.", ""]]}, {"id": "2107.04225", "submitter": "Lingfeng Wang", "authors": "Lingfeng Wang, Shisen Wang", "title": "A Multi-task Mean Teacher for Semi-supervised Facial Affective Behavior\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective Behavior Analysis is an important part in human-computer\ninteraction. Existing successful affective behavior analysis method such as\nTSAV[9] suffer from challenge of incomplete labeled datasets. To boost its\nperformance, this paper presents a multi-task mean teacher model for\nsemi-supervised Affective Behavior Analysis to learn from missing labels and\nexploring the learning of multiple correlated task simultaneously. To be\nspecific, we first utilize TSAV as baseline model to simultaneously recognize\nthe three tasks. We have modified the preprocessing method of rendering mask to\nprovide better semantics information. After that, we extended TSAV model to\nsemi-supervised model using mean teacher, which allow it to be benefited from\nunlabeled data. Experimental results on validation datasets show that our\nmethod achieves better performance than TSAV model, which verifies that the\nproposed network can effectively learn additional unlabeled data to boost the\naffective behavior analysis performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 05:48:22 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 05:15:32 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Lingfeng", ""], ["Wang", "Shisen", ""]]}, {"id": "2107.04452", "submitter": "Xiaoxue Zang", "authors": "Xiaoxue Zang, Ying Xu, Jindong Chen", "title": "Multimodal Icon Annotation For Mobile Applications", "comments": "11 pages, MobileHCI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Annotating user interfaces (UIs) that involves localization and\nclassification of meaningful UI elements on a screen is a critical step for\nmany mobile applications such as screen readers and voice control of devices.\nAnnotating object icons, such as menu, search, and arrow backward, is\nespecially challenging due to the lack of explicit labels on screens, their\nsimilarity to pictures, and their diverse shapes. Existing studies either use\nview hierarchy or pixel based methods to tackle the task. Pixel based\napproaches are more popular as view hierarchy features on mobile platforms are\noften incomplete or inaccurate, however it leaves out instructional information\nin the view hierarchy such as resource-ids or content descriptions. We propose\na novel deep learning based multi-modal approach that combines the benefits of\nboth pixel and view hierarchy features as well as leverages the\nstate-of-the-art object detection techniques. In order to demonstrate the\nutility provided, we create a high quality UI dataset by manually annotating\nthe most commonly used 29 icons in Rico, a large scale mobile design dataset\nconsisting of 72k UI screenshots. The experimental results indicate the\neffectiveness of our multi-modal approach. Our model not only outperforms a\nwidely used object classification baseline but also pixel based object\ndetection models. Our study sheds light on how to combine view hierarchy with\npixel features for annotating UI elements.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 13:57:37 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Zang", "Xiaoxue", ""], ["Xu", "Ying", ""], ["Chen", "Jindong", ""]]}, {"id": "2107.04506", "submitter": "Jason R.C. Nurse Dr", "authors": "Alice Jaffray and Conor Finn and Jason R.C. Nurse", "title": "SherLOCKED: A Detective-themed Serious Game for Cyber Security Education", "comments": null, "journal-ref": "15th IFIP International Symposium on Human Aspects of Information\n  Security & Assurance (HAISA 2021)", "doi": "10.1007/978-3-030-81111-2_4", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gamification and Serious Games are progressively being used over a host of\nfields, particularly to support education. Such games provide a new way to\nengage students with content and can complement more traditional approaches to\nlearning. This article proposes SherLOCKED, a new serious game created in the\nstyle of a 2D top-down puzzle adventure. The game is situated in the context of\nan undergraduate cyber security course, and is used to consolidate students'\nknowledge of foundational security concepts (e.g. the CIA triad, security\nthreats and attacks and risk management). SherLOCKED was built based on a\nreview of existing serious games and a study of common gamification principles.\nIt was subsequently implemented within an undergraduate course, and evaluated\nwith 112 students. We found the game to be an effective, attractive and fun\nsolution for allowing further engagement with content that students were\nintroduced to during lectures. This research lends additional evidence to the\nuse of serious games in supporting learning about cyber security.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 15:46:47 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Jaffray", "Alice", ""], ["Finn", "Conor", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "2107.04566", "submitter": "Naimul Mefraz Khan", "authors": "Zeeshan Ahmad, Suha Rabbani, Muhammad Rehman Zafar, Syem Ishaque,\n  Sridhar Krishnan, Naimul Khan", "title": "Multi-level Stress Assessment from ECG in a Virtual Reality Environment\n  using Multimodal Fusion", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ECG is an attractive option to assess stress in serious Virtual Reality (VR)\napplications due to its non-invasive nature. However, the existing Machine\nLearning (ML) models perform poorly. Moreover, existing studies only perform a\nbinary stress assessment, while to develop a more engaging biofeedback-based\napplication, multi-level assessment is necessary. Existing studies annotate and\nclassify a single experience (e.g. watching a VR video) to a single stress\nlevel, which again prevents design of dynamic experiences where real-time\nin-game stress assessment can be utilized. In this paper, we report our\nfindings on a new study on VR stress assessment, where three stress levels are\nassessed. ECG data was collected from 9 users experiencing a VR roller coaster.\nThe VR experience was then manually labeled in 10-seconds segments to three\nstress levels by three raters. We then propose a novel multimodal deep fusion\nmodel utilizing spectrogram and 1D ECG that can provide a stress prediction\nfrom just a 1-second window. Experimental results demonstrate that the proposed\nmodel outperforms the classical HRV-based ML models (9% increase in accuracy)\nand baseline deep learning models (2.5% increase in accuracy). We also report\nresults on the benchmark WESAD dataset to show the supremacy of the model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:34:42 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Ahmad", "Zeeshan", ""], ["Rabbani", "Suha", ""], ["Zafar", "Muhammad Rehman", ""], ["Ishaque", "Syem", ""], ["Krishnan", "Sridhar", ""], ["Khan", "Naimul", ""]]}, {"id": "2107.04681", "submitter": "Amit Kumar Nath", "authors": "Amit Kumar Nath, Andy Wang", "title": "A Survey on Personal Image Retrieval Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The number of photographs taken worldwide is growing rapidly and steadily.\nWhile a small subset of these images is annotated and shared by users through\nsocial media platforms, due to the sheer number of images in personal photo\nrepositories (shared or not shared), finding specific images remains\nchallenging. This survey explores existing image retrieval techniques as well\nas photo-organizer applications to highlight their relative strengths in\naddressing this challenge.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 21:06:04 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Nath", "Amit Kumar", ""], ["Wang", "Andy", ""]]}, {"id": "2107.04711", "submitter": "Jan Smeddinck", "authors": "Rosanna Bellini, Alexander Wilson, Jan David Smeddinck", "title": "Fragments of the Past: Curating Peer Support with Perpetrators of\n  Domestic Violence", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445611", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing evidence that digital peer-support networks can have a\npositive influence on behaviour change and wellbeing outcomes for people who\nharm themselves and others. However, making and sustaining such networks are\nsubject to ethical and pragmatic challenges, particularly for perpetrators of\ndomestic violence whom pose unique risks when brought together. In this work we\nreport on a ten-month study where we worked with six support workers and\neighteen perpetrators in the design and deployment of Fragments of the Past; a\nsocio-material system that connects audio messages with tangible artefacts. We\nshare how crafting digitally-augmented artefacts - 'fragments' - of experiences\nof desisting from violence can translate messages for motivation and rapport\nbetween peers, without subjecting the process to risks inherent with direct\ninter-personal communication. These insights provide the basis for practical\nconsiderations for future network design with challenging populations.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 22:57:43 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Bellini", "Rosanna", ""], ["Wilson", "Alexander", ""], ["Smeddinck", "Jan David", ""]]}, {"id": "2107.04799", "submitter": "Shah Rukh Humayoun", "authors": "Shah Rukh Humayoun, Ibrahim Mansour, Ragaad AlTarawneh", "title": "TEVISE: An Interactive Visual Analytics Tool to Explore Evolution of\n  Keywords' Relations in Tweet Data", "comments": "21 pages, 6 figures, INTERACT 2021 full paper pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a new window to explore tweet data has been opened in TExVis tool\nthrough visualizing the relations between the frequent keywords. However,\ntimeline exploration of tweet data, not present in TExVis, could play a\ncritical factor in understanding the changes in people's feedback and reaction\nover time. Targeting this, we present our visual analytics tool, called TEVisE.\nIt uses an enhanced adjacency matrix diagram to overcome the cluttering problem\nin TExVis and visualizes the evolution of frequent keywords and the relations\nbetween these keywords over time. We conducted two user studies to find answers\nof our two formulated research questions. In the first user study, we focused\non evaluating the used visualization layouts in both tools from the\nperspectives of common usability metrics and cognitive load theory. We found\nbetter accuracy in our TEVisE tool for tasks related to reading exploring\nrelations between frequent keywords. In the second study, we collected users'\nfeedback towards exploring the summary view and the new timeline evolution view\ninside TEVisE. In the second study, we collected users' feedback towards\nexploring the summary view and the new timeline evolution view inside TEVisE.\nWe found that participants preferred both view, one to get overall glance while\nthe other to get the trends changes over time.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 09:28:43 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Humayoun", "Shah Rukh", ""], ["Mansour", "Ibrahim", ""], ["AlTarawneh", "Ragaad", ""]]}, {"id": "2107.04875", "submitter": "Manos Kamarianakis", "authors": "Manos Kamarianakis, Nick Lydatakis and George Papagiannakis", "title": "Never 'Drop the Ball' in the Operating Room: An efficient hand-based VR\n  HMD controller interpolation algorithm, for collaborative, networked virtual\n  environments", "comments": "11 pages, 11 figures, Initial paper submitted and accepted to CGI2021\n  - ENGAGE Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose two algorithms that can be applied in the context of\na networked virtual environment to efficiently handle the interpolation of\ndisplacement data for hand-based VR HMDs. Our algorithms, based on the use of\ndual-quaternions and multivectors respectively, impact the network consumption\nrate and are highly effective in scenarios involving multiple users. We\nillustrate convincing results in a modern game engine and a medical VR\ncollaborative training scenario.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 16:48:14 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Kamarianakis", "Manos", ""], ["Lydatakis", "Nick", ""], ["Papagiannakis", "George", ""]]}, {"id": "2107.05016", "submitter": "Abdallah Lakhdari", "authors": "Ke Wang, Waheeb Yaqub, Abdallah Lakhdari and Basem Suleiman", "title": "Combating fake news by empowering fact-checked news spread via\n  topology-based interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid information diffusion and large-scaled information cascades can enable\nthe undesired spread of false information. A small-scaled false information\noutbreak may potentially lead to an infodemic. We propose a novel information\ndiffusion and intervention technique to combat the spread of false news. As\nfalse information is often spreading faster in a social network, the proposed\ndiffusion methodology inhibits the spread of false news by proactively\ndiffusing the fact-checked information. Our methodology mainly relies on\ndefining the potential super-spreaders in a social network based on their\ncentrality metrics. We run an extensive set of experiments on different\nnetworks to investigate the impact of centrality metrics on the performance of\nthe proposed diffusion and intervention models. The obtained results\ndemonstrate that empowering the diffusion of fact-checked news combats the\nspread of false news further and deeper in social networks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 10:42:58 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wang", "Ke", ""], ["Yaqub", "Waheeb", ""], ["Lakhdari", "Abdallah", ""], ["Suleiman", "Basem", ""]]}, {"id": "2107.05078", "submitter": "Zheyi Ma", "authors": "Zheyi Ma, Hao Li, Wen Fang, Qingwen Liu, Bin Zhou and Zhiyong Bu", "title": "A Cloud-Edge-Terminal Collaborative System for Temperature Measurement\n  in COVID-19 Prevention", "comments": "6 pages, 8 figures, INFOCOMW ICCN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To prevent the spread of coronavirus disease 2019 (COVID-19), preliminary\ntemperature measurement and mask detection in public areas are conducted.\nHowever, the existing temperature measurement methods face the problems of\nsafety and deployment. In this paper, to realize safe and accurate temperature\nmeasurement even when a person's face is partially obscured, we propose a\ncloud-edge-terminal collaborative system with a lightweight infrared\ntemperature measurement model. A binocular camera with an RGB lens and a\nthermal lens is utilized to simultaneously capture image pairs. Then, a mobile\ndetection model based on a multi-task cascaded convolutional network (MTCNN) is\nproposed to realize face alignment and mask detection on the RGB images. For\naccurate temperature measurement, we transform the facial landmarks on the RGB\nimages to the thermal images by an affine transformation and select a more\naccurate temperature measurement area on the forehead. The collected\ninformation is uploaded to the cloud in real time for COVID-19 prevention.\nExperiments show that the detection model is only 6.1M and the average\ndetection speed is 257ms. At a distance of 1m, the error of indoor temperature\nmeasurement is about 3%. That is, the proposed system can realize real-time\ntemperature measurement in public areas.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 16:15:15 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ma", "Zheyi", ""], ["Li", "Hao", ""], ["Fang", "Wen", ""], ["Liu", "Qingwen", ""], ["Zhou", "Bin", ""], ["Bu", "Zhiyong", ""]]}, {"id": "2107.05104", "submitter": "Mohit Chandra", "authors": "Mohit Chandra, Manvith Reddy, Shradha Sehgal, Saurabh Gupta, Arun\n  Balaji Buduru, Ponnurangam Kumaraguru", "title": "\"A Virus Has No Religion\": Analyzing Islamophobia on Twitter During the\n  COVID-19 Outbreak", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has disrupted people's lives driving them to act in\nfear, anxiety, and anger, leading to worldwide racist events in the physical\nworld and online social networks. Though there are works focusing on Sinophobia\nduring the COVID-19 pandemic, less attention has been given to the recent surge\nin Islamophobia. A large number of positive cases arising out of the religious\nTablighi Jamaat gathering has driven people towards forming anti-Muslim\ncommunities around hashtags like #coronajihad, #tablighijamaatvirus on Twitter.\nIn addition to the online spaces, the rise in Islamophobia has also resulted in\nincreased hate crimes in the real world. Hence, an investigation is required to\ncreate interventions. To the best of our knowledge, we present the first\nlarge-scale quantitative study linking Islamophobia with COVID-19.\n  In this paper, we present CoronaBias dataset which focuses on anti-Muslim\nhate spanning four months, with over 410,990 tweets from 244,229 unique users.\nWe use this dataset to perform longitudinal analysis. We find the relation\nbetween the trend on Twitter with the offline events that happened over time,\nmeasure the qualitative changes in the context associated with the Muslim\ncommunity, and perform macro and micro topic analysis to find prevalent topics.\nWe also explore the nature of the content, focusing on the toxicity of the URLs\nshared within the tweets present in the CoronaBias dataset. Apart from the\ncontent-based analysis, we focus on user analysis, revealing that the portrayal\nof religion as a symbol of patriotism played a crucial role in deciding how the\nMuslim community was perceived during the pandemic. Through these experiments,\nwe reveal the existence of anti-Muslim rhetoric around COVID-19 in the Indian\nsub-continent.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jul 2021 17:56:17 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 16:50:26 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chandra", "Mohit", ""], ["Reddy", "Manvith", ""], ["Sehgal", "Shradha", ""], ["Gupta", "Saurabh", ""], ["Buduru", "Arun Balaji", ""], ["Kumaraguru", "Ponnurangam", ""]]}, {"id": "2107.05194", "submitter": "Yiming Luo", "authors": "Yiming Luo, Jialin Wang, Hai-Ning Liang, Shan Luo, and Eng Gee Lim", "title": "Monoscopic vs. Stereoscopic Views and Display Types in the Teleoperation\n  of Unmanned Ground Vehicles for Object Avoidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual reality (VR) head-mounted displays (HMD) have recently been used to\nprovide an immersive, first-person vision/view in real-time for manipulating\nremotely-controlled unmanned ground vehicles (UGV). The teleoperation of UGV\ncan be challenging for operators when it is done in real time. One big\nchallenge is for operators to perceive quickly and rapidly the distance of\nobjects that are around the UGV while it is moving. In this research, we\nexplore the use of monoscopic and stereoscopic views and display types\n(immersive and non-immersive VR) for operating vehicles remotely. We conducted\ntwo user studies to explore their feasibility and advantages. Results show a\nsignificantly better performance when using an immersive display with\nstereoscopic view for dynamic, real-time navigation tasks that require avoiding\nboth moving and static obstacles. The use of stereoscopic view in an immersive\ndisplay in particular improved user performance and led to better usability.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 04:53:37 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Luo", "Yiming", ""], ["Wang", "Jialin", ""], ["Liang", "Hai-Ning", ""], ["Luo", "Shan", ""], ["Lim", "Eng Gee", ""]]}, {"id": "2107.05383", "submitter": "Jesse David Dinneen", "authors": "Jesse David Dinneen and Helen Bubinger", "title": "Not Quite 'Ask a Librarian': AI on the Nature, Value, and Future of LIS", "comments": "Final version to appear in ASIS&T '21: Proceedings of the 84th Annual\n  Meeting of the Association for Information Science & Technology, 58", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CY cs.HC cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  AI language models trained on Web data generate prose that reflects human\nknowledge and public sentiments, but can also contain novel insights and\npredictions. We asked the world's best language model, GPT-3, fifteen difficult\nquestions about the nature, value, and future of library and information\nscience (LIS), topics that receive perennial attention from LIS scholars. We\npresent highlights from its 45 different responses, which range from platitudes\nand caricatures to interesting perspectives and worrisome visions of the\nfuture, thus providing an LIS-tailored demonstration of the current performance\nof AI language models. We also reflect on the viability of using AI to forecast\nor generate research ideas in this way today. Finally, we have shared the full\nresponse log online for readers to consider and evaluate for themselves.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 15:20:17 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Dinneen", "Jesse David", ""], ["Bubinger", "Helen", ""]]}, {"id": "2107.05408", "submitter": "Kieran Waugh", "authors": "Kieran Waugh and Judy Robertson", "title": "Don't Touch Me! A Comparison of Usability on Touch and Non-Touch Inputs", "comments": "To appear in INTERACT 2021 (Lecture Notes in Computer Science). 4\n  pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Public touchscreens are filthy and, regardless of how often they are cleaned,\nthey pose a considerable risk in the transmission of bacteria and viruses.\nWhile we rely on their use, we should find a feasible alternative to touch\ndevices. Non-touch (touchless) interaction, via the use of mid-air gestures,\nhas been previously labelled as not user friendly and unsuitable. However,\nprevious works have extensively compared such interaction to precise mouse\nmovements. In this paper, we investigate and compare the usability of an\ninterface controlled via a touchscreen and a non-touch device. Participants\n(N=22) using a touchscreen and the Leap Motion Controller, performed tasks on a\nmock-up ticketing machine, later evaluating their experience using the System\nUsability and Gesture Usability scales. Results show that, in contrast to the\nprevious works, the non-touch method was usable and quickly learnable. We\nconclude with recommendations for future work on making a non-touch interface\nmore user-friendly.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 13:24:42 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 13:29:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Waugh", "Kieran", ""], ["Robertson", "Judy", ""]]}, {"id": "2107.05666", "submitter": "Ramesh Sah", "authors": "Ramesh Kumar Sah and Hassan Ghasemzadeh", "title": "Stress Classification and Personalization: Getting the most out of the\n  least", "comments": "4 pages, 4 figures, IEEE International Conference on Wearable and\n  Implantable Body Sensor Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stress detection and monitoring is an active area of research with important\nimplications for the personal, professional, and social health of an\nindividual. Current approaches for affective state classification use\ntraditional machine learning algorithms with features computed from multiple\nsensor modalities. These methods are data-intensive and rely on hand-crafted\nfeatures which impede the practical applicability of these sensor systems in\ndaily lives. To overcome these shortcomings, we propose a novel Convolutional\nNeural Network (CNN) based stress detection and classification framework\nwithout any feature computation using data from only one sensor modality. Our\nmethod is competitive and outperforms current state-of-the-art techniques and\nachieves a classification accuracy of $92.85\\%$ and an $f1$ score of $0.89$.\nThrough our leave-one-subject-out analysis, we also show the importance of\npersonalizing stress models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 18:14:10 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sah", "Ramesh Kumar", ""], ["Ghasemzadeh", "Hassan", ""]]}, {"id": "2107.05704", "submitter": "Reuben Binns Dr", "authors": "Reuben Binns, Reuben Kirkham", "title": "How Could Equality and Data Protection Law Shape AI Fairness for People\n  with Disabilities?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article examines the concept of 'AI fairness' for people with\ndisabilities from the perspective of data protection and equality law. This\nexamination demonstrates that there is a need for a distinctive approach to AI\nfairness that is fundamentally different to that used for other protected\ncharacteristics, due to the different ways in which discrimination and data\nprotection law applies in respect of Disability. We articulate this new agenda\nfor AI fairness for people with disabilities, explaining how combining data\nprotection and equality law creates new opportunities for disabled people's\norganisations and assistive technology researchers alike to shape the use of\nAI, as well as to challenge potential harmful uses.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 19:41:01 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Binns", "Reuben", ""], ["Kirkham", "Reuben", ""]]}, {"id": "2107.05783", "submitter": "Franklin Mingzhe Li", "authors": "Franklin Mingzhe Li, Jamie Dorst, Peter Cederberg, Patrick Carrington", "title": "Non-Visual Cooking: Exploring Practices and Challenges of Meal\n  Preparation by People with Visual Impairments", "comments": "18 pages", "journal-ref": null, "doi": "10.1145/3441852.3471215", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The reliance on vision for tasks related to cooking and eating healthy can\npresent barriers to cooking for oneself and achieving proper nutrition. There\nhas been little research exploring cooking practices and challenges faced by\npeople with visual impairments. We present a content analysis of 122 YouTube\nvideos to highlight the cooking practices of visually impaired people, and we\ndescribe detailed practices for 12 different cooking activities (e.g., cutting\nand chopping, measuring, testing food for doneness). Based on the cooking\npractices, we also conducted semi-structured interviews with 12 visually\nimpaired people who have cooking experience and show existing challenges,\nconcerns, and risks in cooking (e.g., tracking the status of tasks in progress,\nverifying whether things are peeled or cleaned thoroughly). We further discuss\nopportunities to support the current practices and improve the independence of\npeople with visual impairments in cooking (e.g., zero-touch interactions for\ncooking). Overall, our findings provide guidance for future research exploring\nvarious assistive technologies to help people cook without relying on vision.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jul 2021 23:53:57 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Franklin Mingzhe", ""], ["Dorst", "Jamie", ""], ["Cederberg", "Peter", ""], ["Carrington", "Patrick", ""]]}, {"id": "2107.05866", "submitter": "Shuang Peng", "authors": "Shuang Peng, Mengdi Zhou, Minghui Yang, Haitao Mi, Shaosheng Cao,\n  Zujie Wen, Teng Xu, Hongbin Wang, Lei Liu", "title": "A Dialogue-based Information Extraction System for Medical Insurance\n  Assessment", "comments": "To be published in the Findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Chinese medical insurance industry, the assessor's role is essential\nand requires significant efforts to converse with the claimant. This is a\nhighly professional job that involves many parts, such as identifying personal\ninformation, collecting related evidence, and making a final insurance report.\nDue to the coronavirus (COVID-19) pandemic, the previous offline insurance\nassessment has to be conducted online. However, for the junior assessor often\nlacking practical experience, it is not easy to quickly handle such a complex\nonline procedure, yet this is important as the insurance company needs to\ndecide how much compensation the claimant should receive based on the\nassessor's feedback. In order to promote assessors' work efficiency and speed\nup the overall procedure, in this paper, we propose a dialogue-based\ninformation extraction system that integrates advanced NLP technologies for\nmedical insurance assessment. With the assistance of our system, the average\ntime cost of the procedure is reduced from 55 minutes to 35 minutes, and the\ntotal human resources cost is saved 30% compared with the previous offline\nprocedure. Until now, the system has already served thousands of online claim\ncases.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 06:14:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Peng", "Shuang", ""], ["Zhou", "Mengdi", ""], ["Yang", "Minghui", ""], ["Mi", "Haitao", ""], ["Cao", "Shaosheng", ""], ["Wen", "Zujie", ""], ["Xu", "Teng", ""], ["Wang", "Hongbin", ""], ["Liu", "Lei", ""]]}, {"id": "2107.05962", "submitter": "Sumit Shekhar", "authors": "Ulrike Bath, Sumit Shekhar, J\\\"urgen D\\\"ollner, Matthias Trapp", "title": "COLiER: Collaborative Editing of Raster Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various web-based image-editing tools and web-based collaborative tools exist\nin isolation. Research focusing to bridge the gap between these two domains is\nsparse. We respond to the above and develop prototype groupware for real-time\ncollaborative editing of raster images in a web browser. To better understand\nthe requirements, we conduct a preliminary user study and establish\ncommunication and synchronization as key elements. The existing groupware for\ntext documents, presentations, and vector graphics handles the above through\nwell-established techniques. However, those cannot be extended as it is for\nraster graphics manipulation. To this end, we develop a document model that is\nmaintained by a server and is delivered and synchronized to multiple clients.\nOur prototypical implementation is based on a scalable client-server\narchitecture: using WebGL for interactive browser-based rendering and WebSocket\nconnections to maintain synchronization. We evaluate our work qualitatively\nthrough a post-deployment user study for three different scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 10:09:13 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 10:53:23 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bath", "Ulrike", ""], ["Shekhar", "Sumit", ""], ["D\u00f6llner", "J\u00fcrgen", ""], ["Trapp", "Matthias", ""]]}, {"id": "2107.06097", "submitter": "Mike Merrill", "authors": "Mike A. Merrill and Tim Althoff", "title": "Transformer-Based Behavioral Representation Learning Enables Transfer\n  Learning for Mobile Sensing in Small Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning has revolutionized research and applications in NLP and\ncomputer vision, this has not yet been the case for behavioral modeling and\nbehavioral health applications. This is because the domain's datasets are\nsmaller, have heterogeneous datatypes, and typically exhibit a large degree of\nmissingness. Therefore, off-the-shelf deep learning models require significant,\noften prohibitive, adaptation. Accordingly, many research applications still\nrely on manually coded features with boosted tree models, sometimes with\ntask-specific features handcrafted by experts. Here, we address these\nchallenges by providing a neural architecture framework for mobile sensing data\nthat can learn generalizable feature representations from time series and\ndemonstrates the feasibility of transfer learning on small data domains through\nfinetuning. This architecture combines benefits from CNN and Trans-former\narchitectures to (1) enable better prediction performance by learning directly\nfrom raw minute-level sensor data without the need for handcrafted features by\nup to 0.33 ROC AUC, and (2) use pretraining to outperform simpler neural models\nand boosted decision trees with data from as few a dozen participants.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 22:26:50 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Merrill", "Mike A.", ""], ["Althoff", "Tim", ""]]}, {"id": "2107.06206", "submitter": "Sridhar Chimalakonda", "authors": "Shruti Priya, Shubhankar Bhadra and Sridhar Chimalakonda", "title": "ML-Quest: A Game for Introducing Machine Learning Concepts to K-12\n  Students", "comments": "13 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Today, Machine Learning (ML) is of a great importance to society due to the\navailability of huge data and high computational resources. This ultimately led\nto the introduction of ML concepts at multiple levels of education including\nK-12 students to promote computational thinking. However, teaching these\nconcepts to K-12 through traditional methodologies such as video lectures and\nbooks is challenging. Many studies in the literature have reported that using\ninteractive environments such as games to teach computational thinking and\nprogramming improves retention capacity and motivation among students.\nTherefore, introducing ML concepts using a game might enhance students'\nunderstanding of the subject and motivate them to learn further. However, we\nare not aware of any existing game which explicitly focuses on introducing ML\nconcepts to students using game play. Hence, in this paper, we propose\nML-Quest, a 3D video game to provide conceptual overview of three ML concepts:\nSupervised Learning, Gradient Descent and K-Nearest Neighbor (KNN)\nClassification. The crux of the game is to introduce the definition and working\nof these concepts, which we call conceptual overview, in a simulated scenario\nwithout overwhelming students with the intricacies of ML. The game has been\npredominantly evaluated for its usefulness and player experience using the\nTechnology Acceptance Model (TAM) model with the help of 23 higher-secondary\nschool students. The survey result shows that around 70% of the participants\neither agree or strongly agree that the ML-Quest is quite interactive and\nuseful in introducing them to ML concepts.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 16:05:01 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Priya", "Shruti", ""], ["Bhadra", "Shubhankar", ""], ["Chimalakonda", "Sridhar", ""]]}, {"id": "2107.06265", "submitter": "Zhenyi He", "authors": "Zhenyi He, Ruofei Du, Ken Perlin", "title": "Who is Looking at Whom? Visualizing Gaze Awareness for Remote\n  Small-Group Conversations", "comments": "One column, 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video conferences play a vital role in our daily lives. However, many\nnonverbal cues are missing, including gaze and spatial information. We\nintroduce LookAtChat, a web-based video conferencing system, which empowers\nremote users to identify gaze awareness and spatial relationships in\nsmall-group conversations. Leveraging real-time eye-tracking technology\navailable with ordinary webcams, LookAtChat tracks each user's gaze direction,\nidentifies who is looking at whom, and provides corresponding spatial cues.\nInformed by formative interviews with 5 participants who regularly use\nvideoconferencing software, we explored the design space of gaze visualization\nin both 2D and 3D layouts. We further conducted an exploratory user study\n(N=20) to evaluate LookAtChat in three conditions: baseline layout, 2D\ndirectional layout, and 3D perspective layout. Our findings demonstrate how\nLookAtChat engages participants in small-group conversations, how gaze and\nspatial information improve conversation quality, and the potential benefits\nand challenges to incorporating gaze awareness visualization into existing\nvideoconferencing systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 17:48:37 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 15:47:17 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["He", "Zhenyi", ""], ["Du", "Ruofei", ""], ["Perlin", "Ken", ""]]}, {"id": "2107.06302", "submitter": "Lakmal Meegahapola", "authors": "Lakmal Meegahapola, Florian Labhart, Thanh-Trung Phan, Daniel\n  Gatica-Perez", "title": "Examining the Social Context of Alcohol Drinking in Young Adults with\n  Smartphone Sensing", "comments": "to appear in the proceedings of the ACM on Interactive, Mobile,\n  Wearable and Ubiquitous Technologies (IMWUT) - ACM UbiComp 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to prior work, the type of relationship between the person\nconsuming alcohol and others in the surrounding (friends, family, spouse,\netc.), and the number of those people (alone, with one person, with a group,\netc.) are related to many aspects of alcohol consumption, such as the drinking\namount, location, motives, and mood. Even though the social context is\nrecognized as an important aspect that influences the drinking behavior of\nyoung adults in alcohol research, relatively little work has been conducted in\nsmartphone sensing research on this topic. In this study, we analyze the\nweekend nightlife drinking behavior of 241 young adults in Switzerland, using a\ndataset consisting of self-reports and passive smartphone sensing data over a\nperiod of three months. Using multiple statistical analyses, we show that\nfeatures from modalities such as accelerometer, location, application usage,\nbluetooth, and proximity could be informative about different social contexts\nof drinking. We define and evaluate seven social context inference tasks using\nsmartphone sensing data, obtaining accuracies of the range 75%-86% in four\ntwo-class and three three-class inferences. Further, we discuss the possibility\nof identifying the sex composition of a group of friends using smartphone\nsensor data with accuracies over 70%. The results are encouraging towards (a)\nsupporting future interventions on alcohol consumption that incorporate users'\nsocial context more meaningfully, and (b) reducing the need for user\nself-reports when creating drink logs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 18:01:12 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 10:01:03 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Meegahapola", "Lakmal", ""], ["Labhart", "Florian", ""], ["Phan", "Thanh-Trung", ""], ["Gatica-Perez", "Daniel", ""]]}, {"id": "2107.06495", "submitter": "Peter Xenopoulos", "authors": "Peter Xenopoulos, Joao Rulff, Claudio Silva", "title": "GgViz: Accelerating Large-Scale Esports Game Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Game review is crucial for teams, players and media staff in sports. Despite\nits importance, game review is work-intensive and hard to scale. Recent\nadvances in sports data collection have introduced systems that couple video\nwith clustering techniques to allow for users to query sports situations of\ninterest through sketching. However, due to data limitations, as well as\ndifferences in the sport itself, esports has seen a dearth of such systems. In\nthis paper, we leverage emerging data for Counter-Strike: Global Offensive\n(CSGO) to develop ggViz, a novel visual analytics system that allows users to\nquery a large esports data set for similar plays by drawing situations of\ninterest. Along with ggViz, we also present a performant retrieval algorithm\nthat can easily scale to hundreds of millions of game situations. We\ndemonstrate ggViz's utility through detailed cases studies and interviews with\nstaff from professional esports teams.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 05:48:26 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 16:47:07 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xenopoulos", "Peter", ""], ["Rulff", "Joao", ""], ["Silva", "Claudio", ""]]}, {"id": "2107.06799", "submitter": "Akhila Sri Manasa Venigalla", "authors": "Kowndinya Boyalakuntla, Akhila Sri Manasa Venigalla and Sridhar\n  Chimalakonda", "title": "WAccess -- A Web Accessibility Tool based on the latest WCAG 2.2\n  guidelines", "comments": "11 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The vision of providing access to all web content equally for all users makes\nweb accessibility a fundamental goal of todays internet. Web accessibility is\nthe practice of removing barriers from websites that could hinder functionality\nfor users with various disabilities. Web accessibility is measured against the\naccessibility guidelines such as WCAG, GIGW and so on. WCAG 2.2 is the latest\nset of guidelines for web accessibility that helps in making websites\naccessible. The web accessibility tools available in the World Wide Web\nConsortium (W3C), only conform up to WCAG 2.1 guidelines. No tools exist for\nthe latest set of guidelines. Despite the availability of several tools to\ncheck conformity of websites with WCAG 2.1 guidelines, there is scarcity of\ntools that are both open source and scalable. To support automated\naccessibility evaluation of numerous websites against WCAG 2.2 and 2.1, we\npresent here a tool, WAccess. WAccess highlights violations of 9 guidelines\nfrom WCAG 2.1 and 7 guidelines from WCAG 2.2 of a specific web page on the web\nconsole and suggests the fix for violations while specifying violating code\nsnippet simultaneously. We evaluated WAccess against 2246 government websites\nof India, and observed a total of about 2 million violations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 15:56:38 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Boyalakuntla", "Kowndinya", ""], ["Venigalla", "Akhila Sri Manasa", ""], ["Chimalakonda", "Sridhar", ""]]}, {"id": "2107.06875", "submitter": "Amir Yazdani", "authors": "Amir Yazdani, Roya Sabbagh Novin, Andrew Merryweather, Tucker Hermans", "title": "DULA: A Differentiable Ergonomics Model for Postural Optimization in\n  Physical HRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ergonomics and human comfort are essential concerns in physical human-robot\ninteraction applications. Defining an accurate and easy-to-use ergonomic\nassessment model stands as an important step in providing feedback for postural\ncorrection to improve operator health and comfort. In order to enable efficient\ncomputation, previously proposed automated ergonomic assessment and correction\ntools make approximations or simplifications to gold-standard assessment tools\nused by ergonomists in practice. In order to retain assessment quality, while\nimproving computational considerations, we introduce DULA, a differentiable and\ncontinuous ergonomics model learned to replicate the popular and scientifically\nvalidated RULA assessment. We show that DULA provides assessment comparable to\nRULA while providing computational benefits. We highlight DULA's strength in a\ndemonstration of gradient-based postural optimization for a simulated\nteleoperation task.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:39:45 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Yazdani", "Amir", ""], ["Novin", "Roya Sabbagh", ""], ["Merryweather", "Andrew", ""], ["Hermans", "Tucker", ""]]}, {"id": "2107.06886", "submitter": "Sujeong Kim", "authors": "Sujeong Kim, Amir Tamrakar", "title": "\"How to best say it?\" : Translating Directives in Machine Language into\n  Natural Language in the Blocks World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to generate optimal natural language for block placement\ndirectives generated by a machine's planner during human-agent interactions in\nthe blocks world. A non user-friendly machine directive, e.g., move(ObjId,\ntoPos), is transformed into visually and contextually grounded referring\nexpressions that are much easier for the user to comprehend. We describe an\nalgorithm that progressively and generatively transforms the machine's\ndirective in ECI (Elementary Composable Ideas)-space, generating many\nalternative versions of the directive. We then define a cost function to\nevaluate the ease of comprehension of these alternatives and select the best\noption. The parameters for this cost function were derived empirically from a\nuser study that measured utterance-to-action timings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 17:59:08 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Kim", "Sujeong", ""], ["Tamrakar", "Amir", ""]]}, {"id": "2107.06970", "submitter": "Nathan TeBlunthuis", "authors": "Nathan TeBlunthuis, Benjamin Mako Hill", "title": "Identifying Competition and Mutualism Between Online Groups", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Platforms often host multiple online groups with highly overlapping topics\nand members. How can researchers and designers understand how interactions\nbetween related groups affect measures of group health? Inspired by population\necology, prior social computing research has studied competition and mutualism\namong related groups by correlating group size with degrees of overlap in\ncontent and membership. The resulting body of evidence is puzzling as overlaps\nseem sometimes to help and other times to hurt. We suggest that this confusion\nresults from aggregating inter-group relationships into an overall\nenvironmental effect instead of focusing on networks of competition and\nmutualism among groups. We propose a theoretical framework based on community\necology and a method for inferring competitive and mutualistic interactions\nfrom time series participation data. We compare population and community\necology analyses of online community growth by analyzing clusters of subreddits\nwith high user overlap but varying degrees of competition and mutualism.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 20:14:31 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["TeBlunthuis", "Nathan", ""], ["Hill", "Benjamin Mako", ""]]}, {"id": "2107.07015", "submitter": "Kailas Vodrahalli", "authors": "Kailas Vodrahalli, Tobias Gerstenberg, James Zou", "title": "Do Humans Trust Advice More if it Comes from AI? An Analysis of Human-AI\n  Interactions", "comments": "34 pages, 6 figures + 18 full page figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of AI, the algorithm's output is framed as a suggestion\nto a human user. The user may ignore the advice or take it into consideration\nto modify his/her decisions. With the increasing prevalence of such human-AI\ninteractions, it is important to understand how users act (or do not act) upon\nAI advice, and how users regard advice differently if they believe the advice\ncome from an \"AI\" versus another human. In this paper, we characterize how\nhumans use AI suggestions relative to equivalent suggestions from a group of\npeer humans across several experimental settings. We find that participants'\nbeliefs about the human versus AI performance on a given task affects whether\nor not they heed the advice. When participants decide to use the advice, they\ndo so similarly for human and AI suggestions. These results provide insights\ninto factors that affect human-AI interactions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 21:33:14 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Vodrahalli", "Kailas", ""], ["Gerstenberg", "Tobias", ""], ["Zou", "James", ""]]}, {"id": "2107.07018", "submitter": "Tazin Afrin", "authors": "Tazin Afrin, Omid Kashefi, Christopher Olshefski, Diane Litman,\n  Rebecca Hwa, Amanda Godley", "title": "Effective Interfaces for Student-Driven Revision Sessions for\n  Argumentative Writing", "comments": "13 pages, The 2021 ACM CHI Virtual Conference on Human Factors in\n  Computing Systems", "journal-ref": null, "doi": "10.1145/3411764.3445683", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the design and evaluation of a web-based intelligent writing\nassistant that helps students recognize their revisions of argumentative\nessays. To understand how our revision assistant can best support students, we\nhave implemented four versions of our system with differences in the unit span\n(sentence versus sub-sentence) of revision analysis and the level of feedback\nprovided (none, binary, or detailed revision purpose categorization). We first\ndiscuss the design decisions behind relevant components of the system, then\nanalyze the efficacy of the different versions through a Wizard of Oz study\nwith university students. Our results show that while a simple interface with\nno revision feedback is easier to use, an interface that provides a detailed\ncategorization of sentence-level revisions is the most helpful based on user\nsurvey data, as well as the most effective based on improvement in writing\noutcomes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 21:52:39 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Afrin", "Tazin", ""], ["Kashefi", "Omid", ""], ["Olshefski", "Christopher", ""], ["Litman", "Diane", ""], ["Hwa", "Rebecca", ""], ["Godley", "Amanda", ""]]}, {"id": "2107.07023", "submitter": "Denae Ford", "authors": "Souti Chattopadhyay, Thomas Zimmermann, Denae Ford", "title": "Reel Life vs. Real Life: How Software Developers Share Their Daily Life\n  through Vlogs", "comments": "12 pages, 2 figures, 3 tables", "journal-ref": null, "doi": "10.1145/3468264.3468599", "report-no": null, "categories": "cs.SE cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software developers are turning to vlogs (video blogs) to share what a day is\nlike to walk in their shoes. Through these vlogs developers share a rich\nperspective of their technical work as well their personal lives. However, does\nthe type of activities portrayed in vlogs differ from activities developers in\nthe industry perform? Would developers at a software company prefer to show\nactivities to different extents if they were asked to share about their day\nthrough vlogs? To answer these questions, we analyzed 130 vlogs by software\ndevelopers on YouTube and conducted a survey with 335 software developers at a\nlarge software company. We found that although vlogs present traditional\ndevelopment activities such as coding and code peripheral activities (11%),\nthey also prominently feature wellness and lifestyle related activities (47.3%)\nthat have not been reflected in previous software engineering literature. We\nalso found that developers at the software company were inclined to share more\nnon-coding tasks (e.g., personal projects, time spent with family and friends,\nand health) when asked to create a mock-up vlog to promote diversity. These\nfindings demonstrate a shift in our understanding of how software developers\nare spending their time and find valuable to share publicly. We discuss how\nvlogs provide a more complete perspective of software development work and\nserve as a valuable source of data for empirical research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 22:16:53 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 16:56:46 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Chattopadhyay", "Souti", ""], ["Zimmermann", "Thomas", ""], ["Ford", "Denae", ""]]}, {"id": "2107.07064", "submitter": "Dae-Hyeok Lee", "authors": "Dae-Hyeok Lee, Sung-Jin Kim, Seong-Whan Lee", "title": "DAL: Feature Learning from Overt Speech to Decode Imagined Speech-based\n  EEG Signals with Convolutional Autoencoder", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interface (BCI) is one of the tools which enables the\ncommunication between humans and devices by reflecting intention and status of\nhumans. With the development of artificial intelligence, the interest in\ncommunication between humans and drones using electroencephalogram (EEG) is\nincreased. Especially, in the case of controlling drone swarms such as\ndirection or formation, there are many advantages compared with controlling a\ndrone unit. Imagined speech is one of the endogenous BCI paradigms, which can\nidentify intentions of users. When conducting imagined speech, the users\nimagine the pronunciation as if actually speaking. In contrast, overt speech is\na task in which the users directly pronounce the words. When controlling drone\nswarms using imagined speech, complex commands can be delivered more\nintuitively, but decoding performance is lower than that of other endogenous\nBCI paradigms. We proposed the Deep-autoleaner (DAL) to learn EEG features of\novert speech for imagined speech-based EEG signals classification. To the best\nof our knowledge, this study is the first attempt to use EEG features of overt\nspeech to decode imagined speech-based EEG signals with an autoencoder. A total\nof eight subjects participated in the experiment. When classifying four words,\nthe average accuracy of the DAL was 48.41%. In addition, when comparing the\nperformance between w/o and w/ EEG features of overt speech, there was a\nperformance improvement of 7.42% when including EEG features of overt speech.\nHence, we demonstrated that EEG features of overt speech could improve the\ndecoding performance of imagined speech.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 01:13:19 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Lee", "Dae-Hyeok", ""], ["Kim", "Sung-Jin", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2107.07331", "submitter": "Runze Chen", "authors": "Runze Chen and Haiyong Luo and Fang Zhao and Xuechun Meng and Zhiqing\n  Xie and Yida Zhu", "title": "Modeling Accurate Human Activity Recognition for Embedded Devices Using\n  Multi-level Distillation", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human activity recognition (HAR) based on IMU sensors is an essential domain\nin ubiquitous computing. Because of the improving trend to deploy artificial\nintelligence into IoT devices or smartphones, more researchers design the HAR\nmodels for embedded devices. We propose a plug-and-play HAR modeling pipeline\nwith multi-level distillation to build deep convolutional HAR models with\nnative support of embedded devices. SMLDist consists of stage distillation,\nmemory distillation, and logits distillation, which covers all the information\nflow of the deep models. Stage distillation constrains the learning direction\nof the intermediate features. Memory distillation teaches the student models\nhow to explain and store the inner relationship between high-dimensional\nfeatures based on Hopfield networks. Logits distillation constructs distilled\nlogits by a smoothed conditional rule to keep the probable distribution and\nimprove the correctness of the soft target. We compare the performance of\naccuracy, F1 macro score, and energy cost on the embedded platform of various\nstate-of-the-art HAR frameworks with a MobileNet V3 model built by SMLDist. The\nproduced model has well balance with robustness, efficiency, and accuracy.\nSMLDist can also compress the models with minor performance loss in an equal\ncompression rate than other state-of-the-art knowledge distillation methods on\nseven public datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 09:01:41 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Chen", "Runze", ""], ["Luo", "Haiyong", ""], ["Zhao", "Fang", ""], ["Meng", "Xuechun", ""], ["Xie", "Zhiqing", ""], ["Zhu", "Yida", ""]]}, {"id": "2107.07334", "submitter": "L\\^e Nguy\\^en Hoang", "authors": "L\\^e-Nguy\\^en Hoang, Louis Faucon, Aidan Jungo, Sergei Volodin, Dalia\n  Papuc, Orfeas Liossatos, Ben Crulis, Mariame Tighanimine, Isabela Constantin,\n  Anastasiia Kucherenko, Alexandre Maurer, Felix Grimberg, Vlad Nitu, Chris\n  Vossen, S\\'ebastien Rouault and El-Mahdi El-Mhamdi", "title": "Tournesol: A quest for a large, secure and trustworthy database of\n  reliable human judgments", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Today's large-scale algorithms have become immensely influential, as they\nrecommend and moderate the content that billions of humans are exposed to on a\ndaily basis. They are the de-facto regulators of our societies' information\ndiet, from shaping opinions on public health to organizing groups for social\nmovements. This creates serious concerns, but also great opportunities to\npromote quality information. Addressing the concerns and seizing the\nopportunities is a challenging, enormous and fabulous endeavor, as intuitively\nappealing ideas often come with unwanted {\\it side effects}, and as it requires\nus to think about what we deeply prefer.\n  Understanding how today's large-scale algorithms are built is critical to\ndetermine what interventions will be most effective. Given that these\nalgorithms rely heavily on {\\it machine learning}, we make the following key\nobservation: \\emph{any algorithm trained on uncontrolled data must not be\ntrusted}. Indeed, a malicious entity could take control over the data, poison\nit with dangerously manipulative fabricated inputs, and thereby make the\ntrained algorithm extremely unsafe. We thus argue that the first step towards\nsafe and ethical large-scale algorithms must be the collection of a large,\nsecure and trustworthy dataset of reliable human judgments.\n  To achieve this, we introduce \\emph{Tournesol}, an open source platform\navailable at \\url{https://tournesol.app}. Tournesol aims to collect a large\ndatabase of human judgments on what algorithms ought to widely recommend (and\nwhat they ought to stop widely recommending). We outline the structure of the\nTournesol database, the key features of the Tournesol platform and the main\nhurdles that must be overcome to make it a successful project. Most\nimportantly, we argue that, if successful, Tournesol may then serve as the\nessential foundation for any safe and ethical large-scale algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 19:21:35 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Hoang", "L\u00ea-Nguy\u00ean", ""], ["Faucon", "Louis", ""], ["Jungo", "Aidan", ""], ["Volodin", "Sergei", ""], ["Papuc", "Dalia", ""], ["Liossatos", "Orfeas", ""], ["Crulis", "Ben", ""], ["Tighanimine", "Mariame", ""], ["Constantin", "Isabela", ""], ["Kucherenko", "Anastasiia", ""], ["Maurer", "Alexandre", ""], ["Grimberg", "Felix", ""], ["Nitu", "Vlad", ""], ["Vossen", "Chris", ""], ["Rouault", "S\u00e9bastien", ""], ["El-Mhamdi", "El-Mahdi", ""]]}, {"id": "2107.07335", "submitter": "Hyungju Ahn", "authors": "Hyung-Ju Ahn, Dae-Hyeok Lee, Ji-Hoon Jeong, Seong-Whan Lee", "title": "Towards Natural Brain-Machine Interaction using Endogenous Potentials\n  based on Deep Neural Networks", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot collaboration has the potential to maximize the efficiency of the\noperation of autonomous robots. Brain-machine interface (BMI) would be a\ndesirable technology to collaborate with robots since the intention or state of\nusers can be translated from the neural activities. However, the\nelectroencephalogram (EEG), which is one of the most popularly used\nnon-invasive BMI modalities, has low accuracy and a limited degree of freedom\n(DoF) due to a low signal-to-noise ratio. Thus, improving the performance of\nmulti-class EEG classification is crucial to develop more flexible BMI-based\nhuman-robot collaboration. In this study, we investigated the possibility for\ninter-paradigm classification of multiple endogenous BMI paradigms, such as\nmotor imagery (MI), visual imagery (VI), and speech imagery (SI), to enhance\nthe limited DoF while maintaining robust accuracy. We conducted the statistical\nand neurophysiological analyses on MI, VI, and SI and classified three\nparadigms using the proposed temporal information-based neural network (TINN).\nWe confirmed that statistically significant features could be extracted on\ndifferent brain regions when classifying three endogenous paradigms. Moreover,\nour proposed TINN showed the highest accuracy of 0.93 compared to the previous\nmethods for classifying three different types of mental imagery tasks (MI, VI,\nand SI).\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 05:34:15 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ahn", "Hyung-Ju", ""], ["Lee", "Dae-Hyeok", ""], ["Jeong", "Ji-Hoon", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2107.07336", "submitter": "Shital Desai", "authors": "Shital Desai, Arlene Astell", "title": "Mixed reality technologies for people with dementia: Participatory\n  evaluation methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Technologies can support people with early onset dementia (PwD) to aid them\nin Instrumental Activities of Daily Living (IADL). The integration of physical\nand virtual realities in Mixed reality technologies (MRTs) could provide\nscalable and deployable options in developing prompting systems for PwD.\nHowever, these emerging technologies should be evaluated and investigated for\nfeasibility with PwD. Survey instruments such as SUS, SUPR-Q and ethnographic\nmethods that are used for usability evaluation of websites and apps are used to\nevaluate and study MRTs. However, PwD who cannot provide written and verbal\nfeedback are unable to participate in these studies. MRTs also present\nchallenges due to different ways in which physical and virtual realities could\nbe coupled. Experiences with physical, virtual and the couplings between the\ntwo are to be considered in evaluating MRTs.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 00:00:15 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Desai", "Shital", ""], ["Astell", "Arlene", ""]]}, {"id": "2107.07341", "submitter": "Rutwik Shah", "authors": "Rutwik Shah, Bruno Astuto, Tyler Gleason, Will Fletcher, Justin\n  Banaga, Kevin Sweetwood, Allen Ye, Rina Patel, Kevin McGill, Thomas Link,\n  Jason Crane, Valentina Pedoia, Sharmila Majumdar", "title": "Leveraging wisdom of the crowds to improve consensus among radiologists\n  by real time, blinded collaborations on a digital swarm platform", "comments": "24 pages, 2 tables, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.DC cs.LG cs.NE cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radiologists today play a key role in making diagnostic decisions and\nlabeling images for training A.I. algorithms. Low inter-reader reliability\n(IRR) can be seen between experts when interpreting challenging cases. While\nteams-based decisions are known to outperform individual decisions,\ninter-personal biases often creep up in group interactions which limit\nnon-dominant participants from expressing true opinions. To overcome the dual\nproblems of low consensus and inter-personal bias, we explored a solution\nmodeled on biological swarms of bees. Two separate cohorts; three radiologists\nand five radiology residents collaborated on a digital swarm platform in real\ntime and in a blinded fashion, grading meniscal lesions on knee MR exams. These\nconsensus votes were benchmarked against clinical (arthroscopy) and\nradiological (senior-most radiologist) observations. The IRR of the consensus\nvotes was compared to the IRR of the majority and most confident votes of the\ntwo cohorts.The radiologist cohort saw an improvement of 23% in IRR of swarm\nvotes over majority vote. Similar improvement of 23% in IRR in 3-resident swarm\nvotes over majority vote, was observed. The 5-resident swarm had an even higher\nimprovement of 32% in IRR over majority vote. Swarm consensus votes also\nimproved specificity by up to 50%. The swarm consensus votes outperformed\nindividual and majority vote decisions in both the radiologists and resident\ncohorts. The 5-resident swarm had higher IRR than 3-resident swarm indicating\npositive effect of increased swarm size. The attending and resident swarms also\noutperformed predictions from a state-of-the-art A.I. algorithm. Utilizing a\ndigital swarm platform improved agreement and allows participants to express\njudgement free intent, resulting in superior clinical performance and robust\nA.I. training labels.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 06:52:06 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Shah", "Rutwik", ""], ["Astuto", "Bruno", ""], ["Gleason", "Tyler", ""], ["Fletcher", "Will", ""], ["Banaga", "Justin", ""], ["Sweetwood", "Kevin", ""], ["Ye", "Allen", ""], ["Patel", "Rina", ""], ["McGill", "Kevin", ""], ["Link", "Thomas", ""], ["Crane", "Jason", ""], ["Pedoia", "Valentina", ""], ["Majumdar", "Sharmila", ""]]}, {"id": "2107.07344", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "Framework for A Personalized Intelligent Assistant to Elderly People for\n  Activities of Daily Living", "comments": "arXiv admin note: text overlap with arXiv:2106.15599", "journal-ref": "International Journal of Recent Trends in Human Computer\n  Interaction (IJHCI), Volume 9, Issue 1, 2019, pp. 1-22", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing population of elderly people is associated with the need to\nmeet their increasing requirements and to provide solutions that can improve\ntheir quality of life in a smart home. In addition to fear and anxiety towards\ninterfacing with systems; cognitive disabilities, weakened memory, disorganized\nbehavior and even physical limitations are some of the problems that elderly\npeople tend to face with increasing age. The essence of providing\ntechnology-based solutions to address these needs of elderly people and to\ncreate smart and assisted living spaces for the elderly; lies in developing\nsystems that can adapt by addressing their diversity and can augment their\nperformances in the context of their day to day goals. Therefore, this work\nproposes a framework for development of a Personalized Intelligent Assistant to\nhelp elderly people perform Activities of Daily Living (ADLs) in a smart and\nconnected Internet of Things (IoT) based environment. This Personalized\nIntelligent Assistant can analyze different tasks performed by the user and\nrecommend activities by considering their daily routine, current affective\nstate and the underlining user experience. To uphold the efficacy of this\nproposed framework, it has been tested on a couple of datasets for modelling an\naverage user and a specific user respectively. The results presented show that\nthe model achieves a performance accuracy of 73.12% when modelling a specific\nuser, which is considerably higher than its performance while modelling an\naverage user, this upholds the relevance for development and implementation of\nthis proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:36:07 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2107.07360", "submitter": "Sebastian L\\\"obbers", "authors": "Sebastian L\\\"obbers, Mathieu Barthet, Gy\\\"orgy Fazekas", "title": "Sketching sounds: an exploratory study on sound-shape associations", "comments": "accepted for International Computer Music Conference (ICMC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sound synthesiser controls typically correspond to technical parameters of\nsignal processing algorithms rather than intuitive sound descriptors that\nrelate to human perception of sound. This makes it difficult to realise sound\nideas in a straightforward way. Cross-modal mappings, for example between\ngestures and sound, have been suggested as a more intuitive control mechanism.\nA large body of research shows consistency in human associations between sounds\nand shapes. However, the use of drawings to drive sound synthesis has not been\nexplored to its full extent. This paper presents an exploratory study that\nasked participants to sketch visual imagery of sounds with a monochromatic\ndigital drawing interface, with the aim to identify different representational\napproaches and determine whether timbral sound characteristics can be\ncommunicated reliably through visual sketches. Results imply that the\ndevelopment of a synthesiser exploiting sound-shape associations is feasible,\nbut a larger and more focused dataset is needed in followup studies.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:37:53 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["L\u00f6bbers", "Sebastian", ""], ["Barthet", "Mathieu", ""], ["Fazekas", "Gy\u00f6rgy", ""]]}, {"id": "2107.07374", "submitter": "X. Jessie Yang", "authors": "X. Jessie Yang, Christopher Schemanske, Christine Searle", "title": "Toward quantifying trust dynamics: How people adjust their trust after\n  moment-to-moment interaction with automation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Objective: We examine how human operators adjust their trust in automation as\na result of their moment-to-moment interaction with automation. Background:\nMost existing studies measured trust by administering questionnaires at the end\nof an experiment. Only a limited number of studies viewed trust as a dynamic\nvariable that can strengthen or decay over time. Method: Seventy-five\nparticipants took part in an aided memory recognition task. In the task,\nparticipants viewed a series of images and later on performed 40 trials of the\nrecognition task to identify a target image when it was presented with a\ndistractor. In each trial, participants performed the initial recognition by\nthemselves, received a recommendation from an automated decision aid, and\nperformed the final recognition. After each trial, participants reported their\ntrust on a visual analog scale. Results: Outcome bias and contrast effect\nsignificantly influence human operators' trust adjustments. An automation\nfailure leads to a larger trust decrement if the final outcome is undesirable,\nand a marginally larger trust decrement if the human operator succeeds the task\nby him-/her-self. An automation success engenders a greater trust increment if\nthe human operator fails the task. Additionally, automation failures have a\nlarger effect on trust adjustment than automation successes. Conclusion: Human\noperators adjust their trust in automation as a result of their\nmoment-to-moment interaction with automation. Their trust adjustments are\nsignificantly influenced by decision-making heuristics/biases. Application:\nUnderstanding the trust adjustment process enables accurate prediction of the\noperators' moment-to-moment trust in automation and informs the design of\ntrust-aware adaptive automation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:57:44 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Yang", "X. Jessie", ""], ["Schemanske", "Christopher", ""], ["Searle", "Christine", ""]]}, {"id": "2107.07385", "submitter": "Dilrukshi Gamage", "authors": "Dilrukshi Gamage, Mark E Whiting", "title": "Together we learn better: leveraging communities of practice for MOOC\n  learners", "comments": "Asian CHI Symposium 2021", "journal-ref": null, "doi": "10.1145/3429360.3468176", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  MOOC participants often feel isolated and disconnected from their peers.\nNavigating meaningful peer interactions, generating a sense of belonging, and\nachieving social presence are all major challenges for MOOC platforms. MOOC\nusers often rely on external social platforms for such connection and peer\ninteraction, however, off-platform networking often distracts participants from\ntheir learning. With the intention of resolving this issue, we introduce\nPeerCollab, a web-based platform that provides affordances to create\ncommunities and supports meaningful peer interactions, building close-knit\ngroups of learners. We present an initial evaluation through a field study\n(n=56) over 6 weeks and a controlled experiment (n=22). The result indicates\ninsights on how learners build a sense of belonging and develop peer\ninteractions leading to close-knit learning circles. We find that PeerCollab\ncan provide more meaningful interactions and create a community to bring a\nculture of social learning to decentralized, and isolated MOOC learners.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 04:44:27 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Gamage", "Dilrukshi", ""], ["Whiting", "Mark E", ""]]}, {"id": "2107.07477", "submitter": "Paul Rosen", "authors": "Ghulam Jilani Quadri and Paul Rosen", "title": "A Survey of Perception-Based Visualization Studies by Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge of human perception has long been incorporated into visualizations\nto enhance their quality and effectiveness. The last decade, in particular, has\nshown an increase in perception-based visualization research studies. With all\nof this recent progress, the visualization community lacks a comprehensive\nguide to contextualize their results. In this report, we provide a systematic\nand comprehensive review of research studies on perception related to\nvisualization. This survey reviews perception-focused visualization studies\nsince 1980 and summarizes their research developments focusing on low-level\ntasks, further breaking techniques down by visual encoding and visualization\ntype. In particular, we focus on how perception is used to evaluate the\neffectiveness of visualizations, to help readers understand and apply the\nprinciples of perception of their visualization designs through a\ntask-optimized approach. We concluded our report with a summary of the\nweaknesses and open research questions in the area.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:28:39 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Quadri", "Ghulam Jilani", ""], ["Rosen", "Paul", ""]]}, {"id": "2107.07603", "submitter": "Joshua Scurll", "authors": "Joshua M. Scurll", "title": "Measuring inter-cluster similarities with Alpha Shape TRIangulation in\n  loCal Subspaces (ASTRICS) facilitates visualization and clustering of\n  high-dimensional data", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.HC cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering and visualizing high-dimensional (HD) data are important tasks in\na variety of fields. For example, in bioinformatics, they are crucial for\nanalyses of single-cell data such as mass cytometry (CyTOF) data. Some of the\nmost effective algorithms for clustering HD data are based on representing the\ndata by nodes in a graph, with edges connecting neighbouring nodes according to\nsome measure of similarity or distance. However, users of graph-based\nalgorithms are typically faced with the critical but challenging task of\nchoosing the value of an input parameter that sets the size of neighbourhoods\nin the graph, e.g. the number of nearest neighbours to which to connect each\nnode or a threshold distance for connecting nodes. The burden on the user could\nbe alleviated by a measure of inter-node similarity that can have value 0 for\ndissimilar nodes without requiring any user-defined parameters or thresholds.\nThis would determine the neighbourhoods automatically while still yielding a\nsparse graph. To this end, I propose a new method called ASTRICS to measure\nsimilarity between clusters of HD data points based on local dimensionality\nreduction and triangulation of critical alpha shapes. I show that my ASTRICS\nsimilarity measure can facilitate both clustering and visualization of HD data\nby using it in Stage 2 of a three-stage pipeline: Stage 1 = perform an initial\nclustering of the data by any method; Stage 2 = let graph nodes represent\ninitial clusters instead of individual data points and use ASTRICS to\nautomatically define edges between nodes; Stage 3 = use the graph for further\nclustering and visualization. This trades the critical task of choosing a graph\nneighbourhood size for the easier task of essentially choosing a resolution at\nwhich to view the data. The graph and consequently downstream clustering and\nvisualization are then automatically adapted to the chosen resolution.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 20:51:06 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Scurll", "Joshua M.", ""]]}, {"id": "2107.07630", "submitter": "Ho Chit Siu", "authors": "Ho Chit Siu, Jaime D. Pena, Kimberlee C. Chang, Edenna Chen, Yutai\n  Zhou, Victor J. Lopez, Kyle Palko, Ross E. Allen", "title": "Evaluation of Human-AI Teams for Learned and Rule-Based Agents in Hanabi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep reinforcement learning has generated superhuman AI in competitive games\nsuch as Go and StarCraft. Can similar learning techniques create a superior AI\nteammate for human-machine collaborative games? Will humans prefer AI teammates\nthat improve objective team performance or those that improve subjective\nmetrics of trust? In this study, we perform a single-blind evaluation of teams\nof humans and AI agents in the cooperative card game Hanabi, with both\nrule-based and learning-based agents. In addition to the game score, used as an\nobjective metric of the human-AI team performance, we also quantify subjective\nmeasures of the human's perceived performance, teamwork, interpretability,\ntrust, and overall preference of AI teammate. We find that humans have a clear\npreference toward a rule-based AI teammate (SmartBot) over a state-of-the-art\nlearning-based AI teammate (Other-Play) across nearly all subjective metrics,\nand generally view the learning-based agent negatively, despite no statistical\ndifference in the game score. This result has implications for future AI design\nand reinforcement learning benchmarking, highlighting the need to incorporate\nsubjective metrics of human-AI teaming rather than a singular focus on\nobjective task performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 22:19:15 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 03:15:47 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Siu", "Ho Chit", ""], ["Pena", "Jaime D.", ""], ["Chang", "Kimberlee C.", ""], ["Chen", "Edenna", ""], ["Zhou", "Yutai", ""], ["Lopez", "Victor J.", ""], ["Palko", "Kyle", ""], ["Allen", "Ross E.", ""]]}, {"id": "2107.07823", "submitter": "Aoyu Wu", "authors": "Aoyu Wu, Yun Wang, Mengyu Zhou, Xinyi He, Haidong Zhang, Huamin Qu and\n  Dongmei Zhang", "title": "MultiVision: Designing Analytical Dashboards with Deep Learning Based\n  Recommendation", "comments": "Accepted at the IEEE Visualization Conference (IEEE VIS 2021). 11\n  pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We contribute a deep-learning-based method that assists in designing\nanalytical dashboards for analyzing a data table. Given a data table, data\nworkers usually need to experience a tedious and time-consuming process to\nselect meaningful combinations of data columns for creating charts. This\nprocess is further complicated by the need of creating dashboards composed of\nmultiple views that unveil different perspectives of data. Existing automated\napproaches for recommending multiple-view visualizations mainly build on\nmanually crafted design rules, producing sub-optimal or irrelevant suggestions.\nTo address this gap, we present a deep learning approach for selecting data\ncolumns and recommending multiple charts. More importantly, we integrate the\ndeep learning models into a mixed-initiative system. Our model could make\nrecommendations given optional user-input selections of data columns. The\nmodel, in turn, learns from provenance data of authoring logs in an offline\nmanner. We compare our deep learning model with existing methods for\nvisualization recommendation and conduct a user study to evaluate the\nusefulness of the system.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 11:14:24 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Wu", "Aoyu", ""], ["Wang", "Yun", ""], ["Zhou", "Mengyu", ""], ["He", "Xinyi", ""], ["Zhang", "Haidong", ""], ["Qu", "Huamin", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2107.07851", "submitter": "Antonyo Musabini", "authors": "Antonyo Musabini, Evin Bozbayir, Herv\\'e Marcasuzaa, Omar Adair Islas\n  Ram\\'irez", "title": "Park4U Mate: Context-Aware Digital Assistant for Personalized Autonomous\n  Parking", "comments": "Accepted at 2021 IEEE Intelligent Vehicles Symposium - IV (matching\n  camera-ready version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People park their vehicle depending on interior and exterior contexts. They\ndo it naturally, even unconsciously. For instance, with a baby seat on the\nrear, the driver might leave more space on one side to be able to get the baby\nout easily; or when grocery shopping, s/he may position the vehicle to remain\nthe trunk accessible. Autonomous vehicles are becoming technically effective at\ndriving from A to B and parking in a proper spot, with a default way. However,\nin order to satisfy users' expectations and to become trustworthy, they will\nalso need to park or make a temporary stop, appropriate to the given situation.\nIn addition, users want to understand better the capabilities of their driving\nassistance features, such as automated parking systems. A voice-based interface\ncan help with this and even ease the adoption of these features. Therefore, we\ndeveloped a voice-based in-car assistant (Park4U Mate), that is aware of\ninterior and exterior contexts (thanks to a variety of sensors), and that is\nable to park autonomously in a smart way (with a constraints minimization\nstrategy). The solution was demonstrated to thirty-five users in test-drives\nand their feedback was collected on the system's decision-making capability as\nwell as on the human-machine-interaction. The results show that: (1) the\nproposed optimization algorithm is efficient at deciding the best parking\nstrategy; hence, autonomous vehicles can adopt it; (2) a voice-based digital\nassistant for autonomous parking is perceived as a clear and effective\ninteraction method. However, the interaction speed remained the most important\ncriterion for users. In addition, they clearly wish not to be limited on only\nvoice-interaction, to use the automated parking function and rather appreciate\na multi-modal interaction.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 12:36:07 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Musabini", "Antonyo", ""], ["Bozbayir", "Evin", ""], ["Marcasuzaa", "Herv\u00e9", ""], ["Ram\u00edrez", "Omar Adair Islas", ""]]}, {"id": "2107.07859", "submitter": "Hyeon Jeon", "authors": "Hyeon Jeon, Hyung-Kwon Ko, Jaemin Jo, Youngtaek Kim, and Jinwook Seo", "title": "Measuring and Explaining the Inter-Cluster Reliability of\n  Multidimensional Projections", "comments": "IEEE Transactions of Visualization and Computer Graphics (TVCG, Proc.\n  VIS 2021), to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose Steadiness and Cohesiveness, two novel metrics to measure the\ninter-cluster reliability of multidimensional projection (MDP), specifically\nhow well the inter-cluster structures are preserved between the original\nhigh-dimensional space and the low-dimensional projection space. Measuring\ninter-cluster reliability is crucial as it directly affects how well\ninter-cluster tasks (e.g., identifying cluster relationships in the original\nspace from a projected view) can be conducted; however, despite the importance\nof inter-cluster tasks, we found that previous metrics, such as Trustworthiness\nand Continuity, fail to measure inter-cluster reliability. Our metrics consider\ntwo aspects of the inter-cluster reliability: Steadiness measures the extent to\nwhich clusters in the projected space form clusters in the original space, and\nCohesiveness measures the opposite. They extract random clusters with arbitrary\nshapes and positions in one space and evaluate how much the clusters are\nstretched or dispersed in the other space. Furthermore, our metrics can\nquantify pointwise distortions, allowing for the visualization of inter-cluster\nreliability in a projection, which we call a reliability map. Through\nquantitative experiments, we verify that our metrics precisely capture the\ndistortions that harm inter-cluster reliability while previous metrics have\ndifficulty capturing the distortions. A case study also demonstrates that our\nmetrics and the reliability map 1) support users in selecting the proper\nprojection techniques or hyperparameters and 2) prevent misinterpretation while\nperforming inter-cluster tasks, thus allow an adequate identification of\ninter-cluster structure.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 12:52:13 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 04:49:15 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 05:27:38 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jeon", "Hyeon", ""], ["Ko", "Hyung-Kwon", ""], ["Jo", "Jaemin", ""], ["Kim", "Youngtaek", ""], ["Seo", "Jinwook", ""]]}, {"id": "2107.07893", "submitter": "Stephan Schl\\\"ogl PhD", "authors": "Christian Voigt, Stephan Schl\\\"ogl, Aleksander Groth", "title": "Dark Patterns in Online Shopping: Of Sneaky Tricks, Perceived Annoyance\n  and Respective Brand Trust", "comments": "13 pages", "journal-ref": null, "doi": "10.1007/978-3-030-77750-0_10", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dark patterns utilize interface elements to trick users into performing\nunwanted actions. Online shopping websites often employ these manipulative\nmechanisms so as to increase their potential customer base, to boost their\nsales, or to optimize their advertising efforts. Although dark patterns are\noften successful, they clearly inhibit positive user experiences. Particularly,\nwith respect to customers' perceived annoyance and trust put into a given\nbrand, they may have negative effects. To investigate respective connections\nbetween the use of dark patterns, users' perceived level of annoyance and their\nexpressed brand trust, we conducted an experiment-based survey. We implemented\ntwo versions of a fictitious online shop; i.e. one which used five different\ntypes of dark patterns and a similar one without such manipulative user\ninterface elements. A total of $n=204$ participants were then forwarded to one\nof the two shops (approx. $2/3$ to the shop which used the dark patterns) and\nasked to buy a specific product. Subsequently, we measured participants'\nperceived annoyance level, their expressed brand trust and their affinity for\ntechnology. Results show a higher level of perceived annoyance with those who\nused the dark pattern version of the online shop. Also, we found a significant\nconnection between perceived annoyance and participants' expressed brand trust.\nA connection between participants' affinity for technology and their ability to\nrecognize and consequently counter dark patterns, however, is not supported by\nour data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 13:25:06 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Voigt", "Christian", ""], ["Schl\u00f6gl", "Stephan", ""], ["Groth", "Aleksander", ""]]}, {"id": "2107.07986", "submitter": "Fernando Alonso-Fernandez", "authors": "Elias Josse, Amanda Nerborg, Kevin Hernandez-Diaz, Fernando\n  Alonso-Fernandez", "title": "In-Bed Person Monitoring Using Thermal Infrared Sensors", "comments": "Accepted for publication at FedCSIS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world is expecting an aging population and shortage of healthcare\nprofessionals. This poses the problem of providing a safe and dignified life\nfor the elderly. Technological solutions involving cameras can contribute to\nsafety, comfort and efficient emergency responses, but they are invasive of\nprivacy. We use 'Griddy', a prototype with a Panasonic Grid-EYE, a\nlow-resolution infrared thermopile array sensor, which offers more privacy.\nMounted over a bed, it can determine if the user is on the bed or not without\nhuman interaction. For this purpose, two datasets were captured, one (480\nimages) under constant conditions, and a second one (200 images) under\ndifferent variations such as use of a duvet, sleeping with a pet, or increased\nroom temperature. We test three machine learning algorithms: Support Vector\nMachines (SVM), k-Nearest Neighbors (k-NN) and Neural Network (NN). With\n10-fold cross validation, the highest accuracy in the main dataset is for both\nSVM and k-NN (99%). The results with variable data show a lower reliability\nunder certain circumstances, highlighting the need of extra work to meet the\nchallenge of variations in the environment.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 15:59:07 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Josse", "Elias", ""], ["Nerborg", "Amanda", ""], ["Hernandez-Diaz", "Kevin", ""], ["Alonso-Fernandez", "Fernando", ""]]}, {"id": "2107.08034", "submitter": "Tim Weninger PhD", "authors": "Pamela Bilo Thomas, Clark Hogan-Taylor, Michael Yankoski, Tim Weninger", "title": "Pilot Study Suggests Online Media Literacy Programming Reduces Belief in\n  False News in Indonesia", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Amidst the threat of digital misinformation, we offer a pilot study regarding\nthe efficacy of an online social media literacy campaign aimed at empowering\nindividuals in Indonesia with skills to help them identify misinformation. We\nfound that users who engaged with our online training materials and educational\nvideos were more likely to identify misinformation than those in our control\ngroup (total $N$=1000). Given the promising results of our preliminary study,\nwe plan to expand efforts in this area, and build upon lessons learned from\nthis pilot study.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 17:56:27 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Thomas", "Pamela Bilo", ""], ["Hogan-Taylor", "Clark", ""], ["Yankoski", "Michael", ""], ["Weninger", "Tim", ""]]}, {"id": "2107.08141", "submitter": "Hyeok Kim", "authors": "Hyeok Kim, Ryan Rossi, Abhraneel Sarma, Dominik Moritz, and Jessica\n  Hullman", "title": "An Automated Approach to Reasoning About Task-Oriented Insights in\n  Responsive Visualization", "comments": "9 pages, 11 figures, 2 tables. Accepted at IEEE VIS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Authors often transform a large screen visualization for smaller displays\nthrough rescaling, aggregation and other techniques when creating\nvisualizations for both desktop and mobile devices (i.e., responsive\nvisualization). However, transformations can alter relationships or patterns\nimplied by the large screen view, requiring authors to reason carefully about\nwhat information to preserve while adjusting their design for the smaller\ndisplay. We propose an automated approach to approximating the loss of support\nfor task-oriented visualization insights (identification, comparison, and\ntrend) in responsive transformation of a source visualization. We\noperationalize identification, comparison, and trend loss as objective\nfunctions calculated by comparing properties of the rendered source\nvisualization to each realized target (small screen) visualization. To evaluate\nthe utility of our approach, we train machine learning models on human ranked\nsmall screen alternative visualizations across a set of source visualizations.\nWe find that our approach achieves an accuracy of 84% (random forest model) in\nranking visualizations. We demonstrate this approach in a prototype responsive\nvisualization recommender that enumerates responsive transformations using\nAnswer Set Programming and evaluates the preservation of task-oriented insights\nusing our loss measures. We discuss implications of our approach for the\ndevelopment of automated and semi-automated responsive visualization\nrecommendation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 22:59:16 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kim", "Hyeok", ""], ["Rossi", "Ryan", ""], ["Sarma", "Abhraneel", ""], ["Moritz", "Dominik", ""], ["Hullman", "Jessica", ""]]}, {"id": "2107.08264", "submitter": "Xingbo Wang", "authors": "Xingbo Wang, Jianben He, Zhihua Jin, Muqiao Yang, Yong Wang, Huamin Qu", "title": "M2Lens: Visualizing and Explaining Multimodal Models for Sentiment\n  Analysis", "comments": "11 pages, 7 figures. This paper is accepted by IEEE VIS, 2021. To\n  appear in IEEE Transactions on Visualization and Computer Graphics (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal sentiment analysis aims to recognize people's attitudes from\nmultiple communication channels such as verbal content (i.e., text), voice, and\nfacial expressions. It has become a vibrant and important research topic in\nnatural language processing. Much research focuses on modeling the complex\nintra- and inter-modal interactions between different communication channels.\nHowever, current multimodal models with strong performance are often\ndeep-learning-based techniques and work like black boxes. It is not clear how\nmodels utilize multimodal information for sentiment predictions. Despite recent\nadvances in techniques for enhancing the explainability of machine learning\nmodels, they often target unimodal scenarios (e.g., images, sentences), and\nlittle research has been done on explaining multimodal models. In this paper,\nwe present an interactive visual analytics system, M2Lens, to visualize and\nexplain multimodal models for sentiment analysis. M2Lens provides explanations\non intra- and inter-modal interactions at the global, subset, and local levels.\nSpecifically, it summarizes the influence of three typical interaction types\n(i.e., dominance, complement, and conflict) on the model predictions. Moreover,\nM2Lens identifies frequent and influential multimodal features and supports the\nmulti-faceted exploration of model behaviors from language, acoustic, and\nvisual modalities. Through two case studies and expert interviews, we\ndemonstrate our system can help users gain deep insights into the multimodal\nmodels for sentiment analysis.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 15:54:27 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 02:20:19 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Xingbo", ""], ["He", "Jianben", ""], ["Jin", "Zhihua", ""], ["Yang", "Muqiao", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2107.08268", "submitter": "Kyros Jalife", "authors": "Kyros Jalife, Casper Harteveld, Christoffer Holmgard", "title": "From Flow to Fuse: A Cognitive Perspective", "comments": "CHI PLAY'21 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of flow is used extensively in HCI, video games, and many other\nfields, but its prevalent definition is conceptually vague and alternative\ninterpretations have contributed to ambiguity in the literature. To address\nthis, we use cognitive science theory to expose inconsistencies in flow's\nprevalent definition, and introduce fuse, a concept related to flow but\nconsistent with cognitive science, and defined as the \"fusion of\nactivity-related sensory stimuli and awareness\". Based on this definition, we\ndevelop a preliminary model that hypothesizes fuse's underlying cognitive\nprocesses. To illustrate the model's practical value, we derive a set of design\nheuristics that we exemplify in the context of video games. Together, the fuse\ndefinition, model and design heuristics form our theoretical framework, and are\na product of rethinking flow from a cognitive perspective with the purpose of\nimproving conceptual clarity and theoretical robustness in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jul 2021 16:08:41 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Jalife", "Kyros", ""], ["Harteveld", "Casper", ""], ["Holmgard", "Christoffer", ""]]}, {"id": "2107.08356", "submitter": "Xingbo Wang", "authors": "Xingbo Wang, Yao Ming, Tongshuang Wu, Haipeng Zeng, Yong Wang, Huamin\n  Qu", "title": "DeHumor: Visual Analytics for Decomposing Humor", "comments": "15 pages. A preprint version of a publication at IEEE Transactions on\n  Visualization and Computer Graphics (TVCG), 2021", "journal-ref": null, "doi": "10.1109/TVCG.2021.3097709", "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite being a critical communication skill, grasping humor is challenging\n-- a successful use of humor requires a mixture of both engaging content\nbuild-up and an appropriate vocal delivery (e.g., pause). Prior studies on\ncomputational humor emphasize the textual and audio features immediately next\nto the punchline, yet overlooking longer-term context setup. Moreover, the\ntheories are usually too abstract for understanding each concrete humor\nsnippet. To fill in the gap, we develop DeHumor, a visual analytical system for\nanalyzing humorous behaviors in public speaking. To intuitively reveal the\nbuilding blocks of each concrete example, DeHumor decomposes each humorous\nvideo into multimodal features and provides inline annotations of them on the\nvideo script. In particular, to better capture the build-ups, we introduce\ncontent repetition as a complement to features introduced in theories of\ncomputational humor and visualize them in a context linking graph. To help\nusers locate the punchlines that have the desired features to learn, we\nsummarize the content (with keywords) and humor feature statistics on an\naugmented time matrix. With case studies on stand-up comedy shows and TED\ntalks, we show that DeHumor is able to highlight various building blocks of\nhumor examples. In addition, expert interviews with communication coaches and\nhumor researchers demonstrate the effectiveness of DeHumor for multimodal humor\nanalysis of speech content and vocal delivery.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 04:01:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Xingbo", ""], ["Ming", "Yao", ""], ["Wu", "Tongshuang", ""], ["Zeng", "Haipeng", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2107.08432", "submitter": "Diego Monteiro", "authors": "Diego Monteiro, Hao Chen, Hai-Ning Liang, Huawei Tu, Henry Dub", "title": "Evaluating Performance and Gameplay of Virtual Reality Sickness\n  Techniques in a First-Person Shooter Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In virtual reality (VR) games, playability and immersion levels are important\nbecause they affect gameplay, enjoyment, and performance. However, they can be\nadversely affected by VR sickness (VRS) symptoms. VRS can be minimized by\nmanipulating users' perception of the virtual environment via the head-mounted\ndisplay (HMD). One extreme example is the Teleport mitigation technique, which\nlets users navigate discretely, skipping sections of the virtual space. Other\ntechniques are less extreme but still rely on controlling what and how much\nusers see via the HMD. This research examines the effect on players'\nperformance and gameplay of these mitigation techniques in fast-paced VR games.\nOur focus is on two types of visual reduction techniques. This study aims to\nidentify specifically the trade-offs these techniques have in a first-person\nshooter game regarding immersion, performance, and VRS. The main contributions\nin this paper are (1) a deeper understanding of one of the most popular\ntechniques (Teleport) when it comes to gameplay; (2) the replication and\nvalidation of a novel VRS mitigation technique based on visual reduction; and\n(3) a comparison of their effect on players' performance and gameplay.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 13:00:22 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Monteiro", "Diego", ""], ["Chen", "Hao", ""], ["Liang", "Hai-Ning", ""], ["Tu", "Huawei", ""], ["Dub", "Henry", ""]]}, {"id": "2107.08434", "submitter": "Sebastian Cmentowski", "authors": "Sebastian Cmentowski, Andrey Krekhov, Jens Kr\\\"uger", "title": "\"I Packed My Bag and in It I Put...\": A Taxonomy of Inventory Systems\n  for Virtual Reality Games", "comments": "to appear: accepted IEEE COG 2021 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a journey, a backpack is a perfect place to store and organize the\nnecessary provisions and tools. Similarly, carrying and managing items is a\ncentral part of most digital games, providing significant prospects for the\nplayer experience. Even though VR games are gradually becoming more mature,\nmost of them still avoid this essential feature. Some of the reasons for this\ndeficit are the additional requirements and challenges that VR imposes on\ndevelopers to achieve a compelling user experience. We structure the ample\ndesign space of VR inventories by analyzing popular VR games and developing a\nstructural taxonomy. We combine our insights with feedback from game developers\nto identify the essential building blocks and design choices. Finally, we\npropose meaningful design implications and demonstrate the practical use of our\nwork in action.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 13:02:06 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Cmentowski", "Sebastian", ""], ["Krekhov", "Andrey", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "2107.08437", "submitter": "Diego Monteiro", "authors": "Yiwen Zhang, Diego Monteiro, Hai-Ning Liang, Jieming Ma, Nilufar\n  Baghaei", "title": "Effect of Input-output Randomness on Gameplay Satisfaction in\n  Collectable Card Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Randomness is an important factor in games, so much so that some games rely\nalmost purely on it for its outcomes and increase players' engagement with\nthem. However, randomness can affect the game experience depending on when it\noccurs in a game, altering the chances of planning for a player. In this paper,\nwe refer to it as \"input-output randomness\". Input-output randomness is a\ncornerstone of collectable card games like Hearthstone, in which cards are\ndrawn randomly (input randomness) and have random effects when played (output\nrandomness). While the topic might have been commonly discussed by game\ndesigners and be present in many games, few empirical studies have been\nperformed to evaluate the effects of these different kinds of randomness on the\nplayers' satisfaction. This research investigates the effects of input-output\nrandomness on collectable card games across four input-output randomness\nconditions. We have developed our own collectable card game and experimented\nwith the different kinds of randomness with the game. Our results suggest that\ninput randomness can significantly impact game satisfaction negatively.\nOverall, our results present helpful considerations on how and when to apply\nrandomness in game design when aiming for players' satisfaction.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 13:02:56 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhang", "Yiwen", ""], ["Monteiro", "Diego", ""], ["Liang", "Hai-Ning", ""], ["Ma", "Jieming", ""], ["Baghaei", "Nilufar", ""]]}, {"id": "2107.08439", "submitter": "Sebastian Cmentowski", "authors": "Sebastian Cmentowski, Jens Kr\\\"uger", "title": "Effects of Task Type and Wall Appearance on Collision Behavior in\n  Virtual Environments", "comments": "to appear: accepted IEEE COG 2021 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by the games community, virtual reality setups have lately evolved\ninto affordable and consumer-ready mobile headsets. However, despite these\npromising improvements, it remains challenging to convey immersive and engaging\nVR games as players are usually limited to experience the virtual world by\nvision and hearing only. One prominent example of such open challenges is the\ndisparity between the real surroundings and the virtual environment. As virtual\nobstacles usually do not have a physical counterpart, players might walk\nthrough walls enclosing the level. Thus, past research mainly focussed on\nmultisensory collision feedback to deter players from ignoring obstacles.\nHowever, the underlying causative reasons for such unwanted behavior have\nmostly remained unclear.\n  Our work investigates how task types and wall appearances influence the\nplayers' incentives to walk through virtual walls. Therefore, we conducted a\nuser study, confronting the participants with different task motivations and\nwalls of varying opacity and realism. Our evaluation reveals that players\ngenerally adhere to realistic behavior, as long as the experience feels\ninteresting and diverse. Furthermore, we found that opaque walls excel in\ndeterring subjects from cutting short, whereas different degrees of realism had\nno significant influence on walking trajectories. Finally, we use collected\nplayer feedback to discuss individual reasons for the observed behavior.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 13:07:40 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Cmentowski", "Sebastian", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "2107.08514", "submitter": "Amit Joshi Dr", "authors": "Pranali Kokate, Sidharth Pancholi, Amit M. Joshi", "title": "Classification of Upper Arm Movements from EEG signals using Machine\n  Learning with ICA Analysis", "comments": "41 Pages, Figures 32, Table 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Brain-Computer Interface system is a profoundly developing area of\nexperimentation for Motor activities which plays vital role in decoding\ncognitive activities. Classification of Cognitive-Motor Imagery activities from\nEEG signals is a critical task. Hence proposed a unique algorithm for\nclassifying left/right-hand movements by utilizing Multi-layer Perceptron\nNeural Network. Handcrafted statistical Time domain and Power spectral density\nfrequency domain features were extracted and obtained a combined accuracy of\n96.02%. Results were compared with the deep learning framework. In addition to\naccuracy, Precision, F1-Score, and recall was considered as the performance\nmetrics. The intervention of unwanted signals contaminates the EEG signals\nwhich influence the performance of the algorithm. Therefore, a novel approach\nwas approached to remove the artifacts using Independent Components Analysis\nwhich boosted the performance. Following the selection of appropriate feature\nvectors that provided acceptable accuracy. The same method was used on all nine\nsubjects. As a result, intra-subject accuracy was obtained for 9 subjects\n94.72%. The results show that the proposed approach would be useful to classify\nthe upper limb movements accurately.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 18:56:28 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Kokate", "Pranali", ""], ["Pancholi", "Sidharth", ""], ["Joshi", "Amit M.", ""]]}, {"id": "2107.08573", "submitter": "Paul Rosen", "authors": "Hamza Elhamdadi and Shaun Canavan and Paul Rosen", "title": "AffectiveTDA: Using Topological Data Analysis to Improve Analysis and\n  Explainability in Affective Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach utilizing Topological Data Analysis to study the\nstructure of face poses used in affective computing, i.e., the process of\nrecognizing human emotion. The approach uses a conditional comparison of\ndifferent emotions, both respective and irrespective of time, with multiple\ntopological distance metrics, dimension reduction techniques, and face\nsubsections (e.g., eyes, nose, mouth, etc.). The results confirm that our\ntopology-based approach captures known patterns, distinctions between emotions,\nand distinctions between individuals, which is an important step towards more\nrobust and explainable emotion recognition by machines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 01:28:50 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Elhamdadi", "Hamza", ""], ["Canavan", "Shaun", ""], ["Rosen", "Paul", ""]]}, {"id": "2107.08882", "submitter": "Alfie Abdul-Rahman", "authors": "Saiful Khan and Phong H. Nguyen and Alfie Abdul-Rahman and Benjamin\n  Bach and Min Chen and Euan Freeman and Cagatay Turkay", "title": "Propagating Visual Designs to Numerous Plots and Dashboards", "comments": "This paper was submitted to IEEE VIS 2021, and it was revised after\n  receiving the first round reviews. The authors would like to thank the\n  anonymous reviewers for their effort, comments, and suggestions. The work is\n  part of the big effort by many RAMPVIS colleagues [arXiv:2012.04757]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of developing an infrastructure for providing visualization\nand visual analytics (VIS) tools to epidemiologists and modeling scientists, we\nencountered a technical challenge for applying a number of visual designs to\nnumerous datasets rapidly and reliably with limited development resources. In\nthis paper, we present a technical solution to address this challenge.\nOperationally, we separate the tasks of data management, visual designs, and\nplots and dashboard deployment in order to streamline the development workflow.\nTechnically, we utilize: an ontology to bring datasets, visual designs, and\ndeployable plots and dashboards under the same management framework;\nmulti-criteria search and ranking algorithms for discovering potential datasets\nthat match a visual design; and a purposely-design user interface for\npropagating each visual design to appropriate datasets (often in tens and\nhundreds) and quality-assuring the propagation before the deployment. This\ntechnical solution has been used in the development of the RAMPVIS\ninfrastructure for supporting a consortium of epidemiologists and modeling\nscientists through visualization.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 13:57:45 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Khan", "Saiful", ""], ["Nguyen", "Phong H.", ""], ["Abdul-Rahman", "Alfie", ""], ["Bach", "Benjamin", ""], ["Chen", "Min", ""], ["Freeman", "Euan", ""], ["Turkay", "Cagatay", ""]]}, {"id": "2107.08946", "submitter": "Kimon Kieslich", "authors": "Marco L\\\"unich, Kimon Kieslich", "title": "Using automated decision-making (ADM) to allocate Covid-19 vaccinations?\n  Exploring the roles of trust and social group preference on the legitimacy of\n  ADM vs. human decision-making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In combating the ongoing global health threat of the Covid-19 pandemic,\ndecision-makers have to take actions based on a multitude of relevant health\ndata with severe potential consequences for the affected patients. Because of\ntheir presumed advantages in handling and analyzing vast amounts of data,\ncomputer systems of automated decision-making (ADM) are implemented and\nsubstitute humans in decision-making processes. In this study, we focus on a\nspecific application of ADM in contrast to human decision-making (HDM), namely\nthe allocation of Covid-19 vaccines to the public. In particular, we elaborate\non the role of trust and social group preference on the legitimacy of vaccine\nallocation. We conducted a survey with a 2x2 randomized factorial design among\nn=1602 German respondents, in which we utilized distinct decision-making agents\n(HDM vs. ADM) and prioritization of a specific social group (teachers vs.\nprisoners) as design factors. Our findings show that general trust in ADM\nsystems and preference for vaccination of a specific social group influence the\nlegitimacy of vaccine allocation. However, contrary to our expectations, trust\nin the agent making the decision did not moderate the link between social group\npreference and legitimacy. Moreover, the effect was also not moderated by the\ntype of decision-maker (human vs. algorithm). We conclude that trustworthy ADM\nsystems must not necessarily lead to the legitimacy of ADM systems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 15:00:14 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["L\u00fcnich", "Marco", ""], ["Kieslich", "Kimon", ""]]}, {"id": "2107.09008", "submitter": "Tanvi Bajpai", "authors": "Tanvi Bajpai, Drshika Asher, Anwesa Goswami, Eshwar Chandrasekharan", "title": "Harmonizing the Cacophony: An Affordance-aware Framework of Audio-Based\n  Social Platform Moderation", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clubhouse is an audio-based social platform that launched in April 2020 and\nrose to popularity amidst the global COVID-19 pandemic. Unlike other platforms\nsuch as Discord, Clubhouse is entirely audio-based, and is not organized by\nspecific communities. Following Clubhouse's surge in popularity, there has been\na rise in the development of other audio-based platforms, as well as the\ninclusion of audio-calling features to existing platforms. In this paper, we\npresent a framework (MIC) for analyzing audio-based social platforms that\naccounts for unique platform affordances, the challenges they provide to both\nusers and moderators, and how these affordances relate to one another using MIC\ndiagrams. Next, we demonstrate how to apply the framework to preexisting\naudio-based platforms and Clubhouse, highlighting key similarities and\ndifferences in affordances across these platforms. Using MIC as a lens to\nexamine observational data from Clubhouse members we uncover user perceptions\nand challenges in moderating audio on the platform.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:35:38 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bajpai", "Tanvi", ""], ["Asher", "Drshika", ""], ["Goswami", "Anwesa", ""], ["Chandrasekharan", "Eshwar", ""]]}, {"id": "2107.09015", "submitter": "Matthew Brehmer", "authors": "Matthew Brehmer, Robert Kosara, Carmen Hull", "title": "Generative Design Inspiration for Glyphs with Diatoms", "comments": "Accepted to IEEE VIS 2021; to appear in IEEE Transactions on\n  Visualization & Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Diatoms, a technique that generates design inspiration for\nglyphs by sampling from palettes of mark shapes, encoding channels, and glyph\nscaffold shapes. Diatoms allows for a degree of randomness while respecting\nconstraints imposed by columns in a data table: their data types and domains as\nwell as semantic associations between columns as specified by the designer. We\npair this generative design process with two forms of interactive design\nexternalization that enable comparison and critique of the design alternatives.\nFirst, we incorporate a familiar small multiples configuration in which every\ndata point is drawn according to a single glyph design, coupled with the\nability to page between alternative glyph designs. Second, we propose a small\npermutables design gallery, in which a single data point is drawn according to\neach alternative glyph design, coupled with the ability to page between data\npoints. We demonstrate an implementation of our technique as an extension to\nTableau featuring three example palettes, and to better understand how Diatoms\ncould fit into existing design workflows, we conducted interviews and\nchauffeured demos with 12 designers. Finally, we reflect on our process and the\ndesigners' reactions, discussing the potential of our technique in the context\nof visualization authoring systems. Ultimately, our approach to glyph design\nand comparison can kickstart and inspire visualization design, allowing for the\nserendipitous discovery of shape and channel combinations that would have\notherwise been overlooked.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 16:47:05 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Brehmer", "Matthew", ""], ["Kosara", "Robert", ""], ["Hull", "Carmen", ""]]}, {"id": "2107.09038", "submitter": "Christopher Anand", "authors": "Maryam Hosseinkord and Gurleen Dulai and Narges Osmani and Christopher\n  K. Anand", "title": "Code and Structure Editing for Teaching: A Case Study in using\n  Bibliometrics to Guide Computer Science Research", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Structure or projectional editors are a well-studied concept among\nresearchers and some practitioners. They have the huge advantage of preventing\nsyntax and in some cases type errors, and aid the discovery of syntax by users\nunfamiliar with a language. This begs the question: why are they not widely\nused in education? To answer this question we performed a systematic review of\n57 papers and performed a bibliometric analysis which extended to 381 papers.\nFrom these we generated two hypotheses: (1) a lack of empirical evidence\nprevents educators from committing to this technology, and (2) existing tools\nhave not been designed based on actual user needs as they would be if\nhuman-centered design principles were used. Given problems we encountered with\nexisting resources to support a systematic review, and the role of bibliometric\ntools in overcoming those obstacles, we also detail our methods so that they\nmay be used as a guide for researchers or graduate students unfamiliar with\nbibliometrics. In particular, we report on which tools provide reliable and\nplentiful information in the field of computer science, and which have\ninsufficient coverage and interoperability issues.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 17:36:18 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Hosseinkord", "Maryam", ""], ["Dulai", "Gurleen", ""], ["Osmani", "Narges", ""], ["Anand", "Christopher K.", ""]]}, {"id": "2107.09042", "submitter": "Matthew Brehmer", "authors": "Matthew Brehmer and Robert Kosara", "title": "From Jam Session to Recital: Synchronous Communication and Collaboration\n  Around Data in Organizations", "comments": "Accepted to IEEE VIS 2021; to appear in IEEE Transactions on\n  Visualization & Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior research on communicating with visualization has focused on public\npresentation and asynchronous individual consumption, such as in the domain of\njournalism. The visualization research community knows comparatively little\nabout synchronous and multimodal communication around data within\norganizations, from team meetings to executive briefings. We conducted two\nqualitative interview studies with individuals who prepare and deliver\npresentations about data to audiences in organizations. In contrast to prior\nwork, we did not limit our interviews to those who self-identify as data\nanalysts or data scientists. Both studies examined aspects of speaking about\ndata with visual aids such as charts, dashboards, and tables. One study was a\nretrospective examination of current practices and difficulties, from which we\nidentified three scenarios involving presentations of data. We describe these\nscenarios using an analogy to musical performance: small collaborative team\nmeetings are akin to jam session, while more structured presentations can range\nfrom semi-improvisational performances among peers to formal recitals given to\nexecutives or customers. In our second study, we grounded the discussion around\nthree design probes, each examining a different aspect of presenting data: the\nprogressive reveal of visualization to direct attention and advance a\nnarrative, visualization presentation controls that are hidden from the\naudience's view, and the coordination of a presenter's video with interactive\nvisualization. Our distillation of interviewees' responses surfaced twelve\nthemes, from ways of authoring presentations to creating accessible and\nengaging audience experiences.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 17:51:41 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Brehmer", "Matthew", ""], ["Kosara", "Robert", ""]]}, {"id": "2107.09163", "submitter": "Maria De-Arteaga", "authors": "Sina Fazelpour, Maria De-Arteaga", "title": "Diversity in Sociotechnical Machine Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a surge of recent interest in sociocultural diversity in\nmachine learning (ML) research, with researchers (i) examining the benefits of\ndiversity as an organizational solution for alleviating problems with\nalgorithmic bias, and (ii) proposing measures and methods for implementing\ndiversity as a design desideratum in the construction of predictive algorithms.\nCurrently, however, there is a gap between discussions of measures and benefits\nof diversity in ML, on the one hand, and the broader research on the underlying\nconcepts of diversity and the precise mechanisms of its functional benefits, on\nthe other. This gap is problematic because diversity is not a monolithic\nconcept. Rather, different concepts of diversity are based on distinct\nrationales that should inform how we measure diversity in a given context.\nSimilarly, the lack of specificity about the precise mechanisms underpinning\ndiversity's potential benefits can result in uninformative generalities,\ninvalid experimental designs, and illicit interpretations of findings. In this\nwork, we draw on research in philosophy, psychology, and social and\norganizational sciences to make three contributions: First, we introduce a\ntaxonomy of different diversity concepts from philosophy of science, and\nexplicate the distinct epistemic and political rationales underlying these\nconcepts. Second, we provide an overview of mechanisms by which diversity can\nbenefit group performance. Third, we situate these taxonomies--of concepts and\nmechanisms--in the lifecycle of sociotechnical ML systems and make a case for\ntheir usefulness in fair and accountable ML. We do so by illustrating how they\nclarify the discourse around diversity in the context of ML systems, promote\nthe formulation of more precise research questions about diversity's impact,\nand provide conceptual tools to further advance research and practice.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jul 2021 21:26:38 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Fazelpour", "Sina", ""], ["De-Arteaga", "Maria", ""]]}, {"id": "2107.09373", "submitter": "Manoranjan Mohanty", "authors": "Waheeb Yaqub, Manoranjan Mohanty, Basem Suleiman", "title": "Image-Hashing-Based Anomaly Detection for Privacy-Preserving Online\n  Proctoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online proctoring has become a necessity in online teaching. Video-based\ncrowd-sourced online proctoring solutions are being used, where an exam-taking\nstudent's video is monitored by third parties, leading to privacy concerns. In\nthis paper, we propose a privacy-preserving online proctoring system. The\nproposed image-hashing-based system can detect the student's excessive face and\nbody movement (i.e., anomalies) that is resulted when the student tries to\ncheat in the exam. The detection can be done even if the student's face is\nblurred or masked in video frames. Experiment with an in-house dataset shows\nthe usability of the proposed system.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:45:05 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Yaqub", "Waheeb", ""], ["Mohanty", "Manoranjan", ""], ["Suleiman", "Basem", ""]]}, {"id": "2107.09509", "submitter": "Himanshu Thapliyal", "authors": "Rajdeep Kumar Nath and Himanshu Thapliyal", "title": "Wearable Health Monitoring System for Older Adults in a Smart Home\n  Environment", "comments": "6 Pages, 2021 IEEE Computer Society Annual Symposium on VLSI", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CY cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of IoT has enabled the design of connected and integrated smart\nhealth monitoring systems. These smart health monitoring systems could be\nrealized in a smart home context to render long-term care to the elderly\npopulation. In this paper, we present the design of a wearable health\nmonitoring system suitable for older adults in a smart home context. The\nproposed system offers solutions to monitor the stress, blood pressure, and\nlocation of an individual within a smart home environment. The stress detection\nmodel proposed in this work uses Electrodermal Activity (EDA),\nPhotoplethysmogram (PPG), and Skin Temperature (ST) sensors embedded in a smart\nwristband for detecting physiological stress. The stress detection model is\ntrained and tested using stress labels obtained from salivary cortisol which is\na clinically established biomarker for physiological stress. A voice-based\nprototype is also implemented and the feasibility of the proposed system for\nintegration in a smart home environment is analyzed by simulating a data\nacquisition and streaming scenario. We have also proposed a blood pressure\nestimation model using PPG signal and advanced regression techniques for\nintegration with the stress detection model in the wearable health monitoring\nsystem. Finally, the design of a voice-assisted indoor location system is\nproposed for integration with the proposed system within a smart home\nenvironment. The proposed wearable health monitoring system is an important\ndirection to realize a smart home environment with extensive diagnostic\ncapabilities so that such a system could be useful for rendering long-term and\npersonalized care to the aging population in the comfort of their home.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 03:16:54 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Nath", "Rajdeep Kumar", ""], ["Thapliyal", "Himanshu", ""]]}, {"id": "2107.09545", "submitter": "Feng Zhou", "authors": "Jackie Ayoub, Na Du, X. Jessie Yang, Feng Zhou", "title": "Predicting Driver Takeover Time in Conditionally Automated Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is extremely important to ensure a safe takeover transition in\nconditionally automated driving. One of the critical factors that quantifies\nthe safe takeover transition is takeover time. Previous studies identified the\neffects of many factors on takeover time, such as takeover lead time,\nnon-driving tasks, modalities of the takeover requests (TORs), and scenario\nurgency. However, there is a lack of research to predict takeover time by\nconsidering these factors all at the same time. Toward this end, we used\neXtreme Gradient Boosting (XGBoost) to predict the takeover time using a\ndataset from a meta-analysis study [1]. In addition, we used SHAP (SHapley\nAdditive exPlanation) to analyze and explain the effects of the predictors on\ntakeover time. We identified seven most critical predictors that resulted in\nthe best prediction performance. Their main effects and interaction effects on\ntakeover time were examined. The results showed that the proposed approach\nprovided both good performance and explainability. Our findings have\nimplications on the design of in-vehicle monitoring and alert systems to\nfacilitate the interaction between the drivers and the automated vehicle.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:01:49 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Ayoub", "Jackie", ""], ["Du", "Na", ""], ["Yang", "X. Jessie", ""], ["Zhou", "Feng", ""]]}, {"id": "2107.09615", "submitter": "Zoya Bylinskii", "authors": "Sofie Beier, Sam Berlow, Esat Boucaud, Zoya Bylinskii, Tianyuan Cai,\n  Jenae Cohn, Kathy Crowley, Stephanie L. Day, Tilman Dingler, Jonathan Dobres,\n  Jennifer Healey, Rajiv Jain, Marjorie Jordan, Bernard Kerr, Qisheng Li, Dave\n  B. Miller, Susanne Nobles, Alexandra Papoutsaki, Jing Qian, Tina Rezvanian,\n  Shelley Rodrigo, Ben D. Sawyer, Shannon M. Sheppard, Bram Stein, Rick\n  Treitman, Jen Vanek, Shaun Wallace, Benjamin Wolfe", "title": "Readability Research: An Interdisciplinary Approach", "comments": "This paper was generated collaboratively over the course of a series\n  of online workshops, the results of which were extensively edited by Dr. Zoya\n  Bylinskii, Dr. Ben D. Sawyer, and Dr. Benjamin Wolfe. Original illustrations\n  by Bernard Kerr. Corresponding Author: Dr. Ben D. Sawyer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Readability is on the cusp of a revolution. Fixed text is becoming fluid as a\nproliferation of digital reading devices rewrite what a document can do. As\npast constraints make way for more flexible opportunities, there is great need\nto understand how reading formats can be tuned to the situation and the\nindividual. We aim to provide a firm foundation for readability research, a\ncomprehensive framework for modern, multi-disciplinary readability research.\nReadability refers to aspects of visual information design which impact\ninformation flow from the page to the reader. Readability can be enhanced by\nchanges to the set of typographical characteristics of a text. These aspects\ncan be modified on-demand, instantly improving the ease with which a reader can\nprocess and derive meaning from text. We call on a multi-disciplinary research\ncommunity to take up these challenges to elevate reading outcomes and provide\nthe tools to do so effectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 16:52:17 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Beier", "Sofie", ""], ["Berlow", "Sam", ""], ["Boucaud", "Esat", ""], ["Bylinskii", "Zoya", ""], ["Cai", "Tianyuan", ""], ["Cohn", "Jenae", ""], ["Crowley", "Kathy", ""], ["Day", "Stephanie L.", ""], ["Dingler", "Tilman", ""], ["Dobres", "Jonathan", ""], ["Healey", "Jennifer", ""], ["Jain", "Rajiv", ""], ["Jordan", "Marjorie", ""], ["Kerr", "Bernard", ""], ["Li", "Qisheng", ""], ["Miller", "Dave B.", ""], ["Nobles", "Susanne", ""], ["Papoutsaki", "Alexandra", ""], ["Qian", "Jing", ""], ["Rezvanian", "Tina", ""], ["Rodrigo", "Shelley", ""], ["Sawyer", "Ben D.", ""], ["Sheppard", "Shannon M.", ""], ["Stein", "Bram", ""], ["Treitman", "Rick", ""], ["Vanek", "Jen", ""], ["Wallace", "Shaun", ""], ["Wolfe", "Benjamin", ""]]}, {"id": "2107.09667", "submitter": "Nicolas Michael M\\\"uller", "authors": "Nicolas M. M\\\"uller, Karla Markert, Konstantin B\\\"ottinger", "title": "Human Perception of Audio Deepfakes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent emergence of deepfakes, computerized realistic multimedia fakes,\nbrought the detection of manipulated and generated content to the forefront.\nWhile many machine learning models for deepfakes detection have been proposed,\nthe human detection capabilities have remained far less explored. This is of\nspecial importance as human perception differs from machine perception and\ndeepfakes are generally designed to fool the human. So far, this issue has only\nbeen addressed in the area of images and video.\n  To compare the ability of humans and machines in detecting audio deepfakes,\nwe conducted an online gamified experiment in which we asked users to discern\nbonda-fide audio samples from spoofed audio, generated with a variety of\nalgorithms. 200 users competed for 8976 game rounds with an artificial\nintelligence (AI) algorithm trained for audio deepfake detection. With the\ncollected data we found that the machine generally outperforms the humans in\ndetecting audio deepfakes, but that the converse holds for a certain attack\ntype, for which humans are still more accurate. Furthermore, we found that\nyounger participants are on average better at detecting audio deepfakes than\nolder participants, while IT-professionals hold no advantage over laymen. We\nconclude that it is important to combine human and machine knowledge in order\nto improve audio deepfake detection.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:19:42 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["M\u00fcller", "Nicolas M.", ""], ["Markert", "Karla", ""], ["B\u00f6ttinger", "Konstantin", ""]]}, {"id": "2107.09917", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer", "title": "Audit, Don't Explain -- Recommendations Based on a Socio-Technical\n  Understanding of ML-Based Systems", "comments": "This paper will be presented at the Workshop on User-Centered\n  Artificial Intelligence (UCAI '21) at Mensch und Computer 2021", "journal-ref": null, "doi": "10.18420/muc2021-mci-ws02-232", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this position paper, I provide a socio-technical perspective on machine\nlearning-based systems. I also explain why systematic audits may be preferable\nto explainable AI systems. I make concrete recommendations for how institutions\ngoverned by public law akin to the German T\\\"UV and Stiftung Warentest can\nensure that ML systems operate in the interest of the public.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 07:36:01 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Heuer", "Hendrik", ""]]}, {"id": "2107.09922", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer, Hendrik Hoch, Andreas Breiter, Yannis Theocharis", "title": "Auditing the Biases Enacted by YouTube for Political Topics in Germany", "comments": "To appear in Mensch und Computer '21, September 05-08, 2021,\n  Ingolstadt. Full Reference: Hendrik Heuer, Hendrik Hoch, Andreas Breiter, and\n  Yannis Theocharis. 2021. Auditing the Biases Enacted by YouTube for Political\n  Topics in Germany. In Mensch und Computer '21, September 05-08, 2021,\n  Ingolstadt. ACM, New York, NY, USA, 21 pages. https:\n  //doi.org/10.1145/1122445.1122456", "journal-ref": null, "doi": "10.1145/3473856.3473864", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With YouTube's growing importance as a news platform, its recommendation\nsystem came under increased scrutiny. Recognizing YouTube's recommendation\nsystem as a broadcaster of media, we explore the applicability of laws that\nrequire broadcasters to give important political, ideological, and social\ngroups adequate opportunity to express themselves in the broadcasted program of\nthe service. We present audits as an important tool to enforce such laws and to\nensure that a system operates in the public's interest. To examine whether\nYouTube is enacting certain biases, we collected video recommendations about\npolitical topics by following chains of ten recommendations per video. Our\nfindings suggest that YouTube's recommendation system is enacting important\nbiases. We find that YouTube is recommending increasingly popular but topically\nunrelated videos. The sadness evoked by the recommended videos decreases while\nthe happiness increases. We discuss the strong popularity bias we identified\nand analyze the link between the popularity of content and emotions. We also\ndiscuss how audits empower researchers and civic hackers to monitor complex\nmachine learning (ML)-based systems like YouTube's recommendation system.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 07:53:59 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Heuer", "Hendrik", ""], ["Hoch", "Hendrik", ""], ["Breiter", "Andreas", ""], ["Theocharis", "Yannis", ""]]}, {"id": "2107.09952", "submitter": "Huey Eng Chua", "authors": "Zifeng Yuan, Huey Eng Chua, Sourav S Bhowmick, Zekun Ye, Wook-Shin\n  Han, Byron Choi", "title": "Towards Plug-and-Play Visual Graph Query Interfaces: Data-driven Canned\n  Pattern Selection for Large Networks", "comments": "19 pages, 26 figures, 3 tables, VLDB conference", "journal-ref": null, "doi": "10.14778/3476249.3476256", "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canned patterns (i.e. small subgraph patterns) in visual graph query\ninterfaces (a.k.a GUI) facilitate efficient query formulation by enabling\npattern-at-a-time construction mode. However, existing GUIs for querying large\nnetworks either do not expose any canned patterns or if they do then they are\ntypically selected manually based on domain knowledge. Unfortunately, manual\ngeneration of canned patterns is not only labor intensive but may also lack\ndiversity for supporting efficient visual formulation of a wide range of\nsubgraph queries. In this paper, we present a novel generic and extensible\nframework called TATTOO that takes a data-driven approach to automatically\nselecting canned patterns for a GUI from large networks. Specifically, it first\ndecomposes the underlying network into truss-infested and truss-oblivious\nregions. Then candidate canned patterns capturing different real-world query\ntopologies are generated from these regions. Canned patterns based on a\nuser-specified plug are then selected for the GUI from these candidates by\nmaximizing coverage and diversity, and by minimizing the cognitive load of the\npattern set. Experimental studies with real-world datasets demonstrate the\nbenefits of TATTOO. Importantly, this work takes a concrete step towards\nrealizing plug-and-play visual graph query interfaces for large networks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 09:01:00 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Yuan", "Zifeng", ""], ["Chua", "Huey Eng", ""], ["Bhowmick", "Sourav S", ""], ["Ye", "Zekun", ""], ["Han", "Wook-Shin", ""], ["Choi", "Byron", ""]]}, {"id": "2107.10204", "submitter": "Shruti Phadke", "authors": "Shruti Phadke, Mattia Samory, Tanushree Mitra", "title": "Characterizing Social Imaginaries and Self-Disclosures of Dissonance in\n  Online Conspiracy Discussion Communities", "comments": "Accepted at CSCW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Online discussion platforms offer a forum to strengthen and propagate belief\nin misinformed conspiracy theories. Yet, they also offer avenues for conspiracy\ntheorists to express their doubts and experiences of cognitive dissonance. Such\nexpressions of dissonance may shed light on who abandons misguided beliefs and\nunder which circumstances. This paper characterizes self-disclosures of\ndissonance about QAnon, a conspiracy theory initiated by a mysterious leader Q\nand popularized by their followers, anons in conspiracy theory subreddits. To\nunderstand what dissonance and disbelief mean within conspiracy communities, we\nfirst characterize their social imaginaries, a broad understanding of how\npeople collectively imagine their social existence. Focusing on 2K posts from\ntwo image boards, 4chan and 8chan, and 1.2 M comments and posts from 12\nsubreddits dedicated to QAnon, we adopt a mixed methods approach to uncover the\nsymbolic language representing the movement, expectations, practices, heroes\nand foes of the QAnon community. We use these social imaginaries to create a\ncomputational framework for distinguishing belief and dissonance from general\ndiscussion about QAnon. Further, analyzing user engagement with QAnon\nconspiracy subreddits, we find that self-disclosures of dissonance correlate\nwith a significant decrease in user contributions and ultimately with their\ndeparture from the community. We contribute a computational framework for\nidentifying dissonance self-disclosures and measuring the changes in user\nengagement surrounding dissonance. Our work can provide insights into designing\ndissonance-based interventions that can potentially dissuade conspiracists from\nonline conspiracy discussion communities.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 16:49:21 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Phadke", "Shruti", ""], ["Samory", "Mattia", ""], ["Mitra", "Tanushree", ""]]}, {"id": "2107.10249", "submitter": "Feng Zhou", "authors": "Sue Bai, Dakota Drake Legge, Ashley Young, Shan Bao, Feng Zhou", "title": "Investigating External Interaction Modality and Design Between Automated\n  Vehicles and Pedestrians at Crossings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we investigated the effectiveness and user acceptance of three\nexternal interaction modalities (i.e., visual, auditory, and visual+auditory)\nin promoting communications between automated vehicle systems (AVS) and\npedestrians at a crosswalk through a large number of combined designs. For this\npurpose, an online survey was designed and distributed to 68 participants. All\nparticipants reported their overall preferences for safety, comfort, trust,\nease of understanding, usability, and acceptance towards the systems. Results\nshowed that the visual+auditory interaction modality was the mostly preferred,\nfollowed by the visual interaction modality and then the auditory one. We also\ntested different visual and auditory interaction methods, and found that\n\"Pedestrian silhouette on the front of the vehicle\" was the best preferred\noption while middle-aged participants liked \"Chime\" much better than young\nparticipants though it was overall better preferred than others. Finally,\ncommunication between the AVS and pedestrians' phones was not well received due\nto privacy concerns. These results provided important interface design\nrecommendations in identifying better combination of visual and auditory\ndesigns and therefore improving AVS communicating their intention with\npedestrians.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 17:53:47 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Bai", "Sue", ""], ["Legge", "Dakota Drake", ""], ["Young", "Ashley", ""], ["Bao", "Shan", ""], ["Zhou", "Feng", ""]]}, {"id": "2107.10309", "submitter": "David Gotz", "authors": "Smiti Kaul, David Borland, Nan Cao, David Gotz", "title": "Improving Visualization Interpretation Using Counterfactuals", "comments": "To Appear in IEEE TVCG (and be presented at IEEE VIS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex, high-dimensional data is used in a wide range of domains to explore\nproblems and make decisions. Analysis of high-dimensional data, however, is\nvulnerable to the hidden influence of confounding variables, especially as\nusers apply ad hoc filtering operations to visualize only specific subsets of\nan entire dataset. Thus, visual data-driven analysis can mislead users and\nencourage mistaken assumptions about causality or the strength of relationships\nbetween features. This work introduces a novel visual approach designed to\nreveal the presence of confounding variables via counterfactual possibilities\nduring visual data analysis. It is implemented in CoFact, an interactive\nvisualization prototype that determines and visualizes \\textit{counterfactual\nsubsets} to better support user exploration of feature relationships. Using\npublicly available datasets, we conducted a controlled user study to\ndemonstrate the effectiveness of our approach; the results indicate that users\nexposed to counterfactual visualizations formed more careful judgments about\nfeature-to-outcome relationships.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 19:00:51 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Kaul", "Smiti", ""], ["Borland", "David", ""], ["Cao", "Nan", ""], ["Gotz", "David", ""]]}, {"id": "2107.10449", "submitter": "Zhendong Chu", "authors": "Zhendong Chu, Hongning Wang", "title": "Improve Learning from Crowds via Generative Augmentation", "comments": "KDD 2021", "journal-ref": null, "doi": "10.1145/3447548.3467409", "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowdsourcing provides an efficient label collection schema for supervised\nmachine learning. However, to control annotation cost, each instance in the\ncrowdsourced data is typically annotated by a small number of annotators. This\ncreates a sparsity issue and limits the quality of machine learning models\ntrained on such data. In this paper, we study how to handle sparsity in\ncrowdsourced data using data augmentation. Specifically, we propose to directly\nlearn a classifier by augmenting the raw sparse annotations. We implement two\nprinciples of high-quality augmentation using Generative Adversarial Networks:\n1) the generated annotations should follow the distribution of authentic ones,\nwhich is measured by a discriminator; 2) the generated annotations should have\nhigh mutual information with the ground-truth labels, which is measured by an\nauxiliary network. Extensive experiments and comparisons against an array of\nstate-of-the-art learning from crowds methods on three real-world datasets\nproved the effectiveness of our data augmentation framework. It shows the\npotential of our algorithm for low-budget crowdsourcing in general.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 04:14:30 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Chu", "Zhendong", ""], ["Wang", "Hongning", ""]]}, {"id": "2107.10552", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, David Melhart, Antonios Liapis, Georgios N.\n  Yannakakis", "title": "Privileged Information for Modeling Affect In The Wild", "comments": "8 pages, 4 figures, 2021 9th International Conference on Affective\n  Computing and Intelligent Interaction (ACII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge of affective computing research is discovering ways to\nreliably transfer affect models that are built in the laboratory to real world\nsettings, namely in the wild. The existing gap between in vitro and in vivo\naffect applications is mainly caused by limitations related to affect sensing\nincluding intrusiveness, hardware malfunctions, availability of sensors, but\nalso privacy and security. As a response to these limitations in this paper we\nare inspired by recent advances in machine learning and introduce the concept\nof privileged information for operating affect models in the wild. The presence\nof privileged information enables affect models to be trained across multiple\nmodalities available in a lab setting and ignore modalities that are not\navailable in the wild with no significant drop in their modeling performance.\nThe proposed privileged information framework is tested in a game arousal\ncorpus that contains physiological signals in the form of heart rate and\nelectrodermal activity, game telemetry, and pixels of footage from two\ndissimilar games that are annotated with arousal traces. By training our\narousal models using all modalities (in vitro) and using solely pixels for\ntesting the models (in vivo), we reach levels of accuracy obtained from models\nthat fuse all modalities both for training and testing. The findings of this\npaper make a decisive step towards realizing affect interaction in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 10:09:16 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Melhart", "David", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "2107.10788", "submitter": "Mohit Singhala", "authors": "Mohit Singhala and Jeremy D. Brown", "title": "Towards an Understanding of the Role Operator Limb Dynamics Plays in\n  Haptic Perception of Stiffness", "comments": "Accepted as Works-in-Progress paper in Haptics Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating haptic interfaces capable of rendering the rich sensation needed for\ndexterous manipulation is crucial for the advancement of human-in-the-loop\ntelerobotic systems (HiLTS). One limiting factor has been the absence of\ndetailed knowledge of the effect of operator limb dynamics and haptic\nexploration dynamics on haptic perception. We propose to begin investigations\nof these effects with single-joint haptic exploration and feedback of physical\nand virtual environments. Here, we present our experimental apparatus, a 1-DoF\nrotational kinesthetic haptic device and electromyography (EMG) system, along\nwith preliminary findings from our efforts to investigate the change in\nstiffness discrimination thresholds for differing exploration velocities.\nResult trends indicate a possible relationship between exploration velocity and\ndiscrimination thresholds, as well as a complex interaction between muscle\nactivation, exploration velocity, and haptic feedback.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 16:43:04 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Singhala", "Mohit", ""], ["Brown", "Jeremy D.", ""]]}, {"id": "2107.10790", "submitter": "Juan Manuel Mayor Torres", "authors": "Juan Manuel Mayor-Torres, Mirco Ravanelli, Sara E. Medina-DeVilliers,\n  Matthew D. Lerner and Giuseppe Riccardi", "title": "Interpretable SincNet-based Deep Learning for Emotion Recognition from\n  EEG brain activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning methods, such as deep learning, show promising results in\nthe medical domain. However, the lack of interpretability of these algorithms\nmay hinder their applicability to medical decision support systems. This paper\nstudies an interpretable deep learning technique, called SincNet. SincNet is a\nconvolutional neural network that efficiently learns customized band-pass\nfilters through trainable sinc-functions. In this study, we use SincNet to\nanalyze the neural activity of individuals with Autism Spectrum Disorder (ASD),\nwho experience characteristic differences in neural oscillatory activity. In\nparticular, we propose a novel SincNet-based neural network for detecting\nemotions in ASD patients using EEG signals. The learned filters can be easily\ninspected to detect which part of the EEG spectrum is used for predicting\nemotions. We found that our system automatically learns the high-$\\alpha$ (9-13\nHz) and $\\beta$ (13-30 Hz) band suppression often present in individuals with\nASD. This result is consistent with recent neuroscience studies on emotion\nrecognition, which found an association between these band suppressions and the\nbehavioral deficits observed in individuals with ASD. The improved\ninterpretability of SincNet is achieved without sacrificing performance in\nemotion recognition.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 14:44:53 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Mayor-Torres", "Juan Manuel", ""], ["Ravanelli", "Mirco", ""], ["Medina-DeVilliers", "Sara E.", ""], ["Lerner", "Matthew D.", ""], ["Riccardi", "Giuseppe", ""]]}, {"id": "2107.10807", "submitter": "Mohit Singhala", "authors": "Mohit Singhala and Jeremy D. Brown", "title": "A novel teleoperator testbed to understand the effects of master-slave\n  dynamics on embodiment and kinesthetic perception *", "comments": "Accepted as Works-in-Progress paper at Haptics Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rising popularity of telerobotic systems, the focus on transparency\nwith regards to haptic perception is also increasing. Transparency, however,\nrepresents a theoretical ideal as most bilateral force-reflecting telerobots\nintroduce dynamics (stiffness and damping) between the operator and the\nenvironment. To achieve true dexterity, it will be essential to understand how\nhumans embody the dynamics of these telerobots and thereby distinguish them\nfrom the environment they are exploring. In this short manuscript, we introduce\na novel single degree-of-freedom testbed designed to perform psychophysical and\ntask performance assessments of kinesthetic perception during telerobotic\nexploration. The system is capable of being configured as a rigid mechanical\nteleoperator, a dynamic mechanical teleoperator, and an electromechanicaal\nteleoperator. We performed prefatory system identification and found that the\nsystem is capable of simulating telerobotic exploration necessary to understand\nthe impact of master-slave dynamics on kinesthetic perception.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:04:27 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Singhala", "Mohit", ""], ["Brown", "Jeremy D.", ""]]}, {"id": "2107.10869", "submitter": "Nathaniel Strawn", "authors": "Nate Strawn", "title": "Filament Plots for Data Visualization", "comments": "33 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We construct a computationally inexpensive 3D extension of Andrew's plots by\nconsidering curves generated by Frenet-Serret equations and induced by\noptimally smooth 2D Andrew's plots. We consider linear isometries from a\nEuclidean data space to infinite dimensional spaces of 2D curves, and\nparametrize the linear isometries that produce (on average) optimally smooth\ncurves over a given dataset. This set of optimal isometries admits many degrees\nof freedom, and (using recent results on generalized Gauss sums) we identify a\nparticular a member of this set which admits an asymptotic projective \"tour\"\nproperty. Finally, we consider the unit-length 3D curves (filaments) induced by\nthese 2D Andrew's plots, where the linear isometry property preserves distances\nas \"relative total square curvatures\". This work concludes by illustrating\nfilament plots for several datasets. Code is available at\nhttps://github.com/n8epi/filaments\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 18:20:33 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Strawn", "Nate", ""]]}, {"id": "2107.10910", "submitter": "Pengcheng An", "authors": "Zhaoyi Yang, Pengcheng An, Jinchen Yang, Samuel Strojny, Zihui Zhang,\n  Dongsheng Sun, Jian Zhao", "title": "Designing Mobile EEG Neurofeedback Games for Children with Autism:\n  Implications from Industry Practice", "comments": null, "journal-ref": null, "doi": "10.1145/3447527.3477522", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Neurofeedback games are an effective and playful approach to enhance certain\nsocial and attentional capabilities in children with autism, which are\npromising to become widely accessible along with the commercialization of\nmobile EEG modules. However, little industry-based experiences are shared,\nregarding how to better design neurofeedback games to fine-tune their\nplayability and user experiences for autistic children. In this paper, we\nreview the experiences we gained from industry practice, in which a series of\nmobile EEG neurofeedback games have been developed for preschool autistic\nchildren. We briefly describe our design and development in a one-year\ncollaboration with a special education center involving a group of\nstakeholders: children with autism and their caregivers and parents. We then\nsummarize four concrete implications we learnt concerning the design of game\ncharacters, game narratives, as well as gameplay elements, which aim to support\nfuture work in creating better neurofeedback games for preschool children with\nautism.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 20:14:46 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Yang", "Zhaoyi", ""], ["An", "Pengcheng", ""], ["Yang", "Jinchen", ""], ["Strojny", "Samuel", ""], ["Zhang", "Zihui", ""], ["Sun", "Dongsheng", ""], ["Zhao", "Jian", ""]]}, {"id": "2107.11029", "submitter": "Pradipta Biswas", "authors": "Priyam Rajkhowa and Pradipta Biswas", "title": "User Perception of Privacy with Ubiquitous Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Privacy is important for all individuals in everyday life. With emerging\ntechnologies, smartphones with AR, various social networking applications and\nartificial intelligence driven modes of surveillance, they tend to intrude\nprivacy. This study aimed to explore and discover various concerns related to\nperception of privacy in this era of ubiquitous technologies. It employed\nonline survey questionnaire to study user perspectives of privacy. Purposive\nsampling was used to collect data from 60 participants. Inductive thematic\nanalysis was used to analyze data. Our study discovered key themes like\nattitude towards privacy in public and private spaces, privacy awareness,\nconsent seeking, dilemmas/confusions related to various technologies, impact of\nattitude and beliefs on individuals actions regarding how to protect oneself\nfrom invasion of privacy in both public and private spaces. These themes\ninteracted amongst themselves and influenced formation of various actions. They\nwere like core principles that molded actions that prevented invasion of\nprivacy for both participant and bystander. Findings of this study would be\nhelpful to improve privacy and personalization of various emerging\ntechnologies. This study contributes to privacy by design and positive design\nby considering psychological needs of users. This is suggestive that the\nfindings can be applied in the areas of experience design, positive\ntechnologies, social computing and behavioral interventions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 05:01:44 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Rajkhowa", "Priyam", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2107.11095", "submitter": "Anna-Pia Lohfink", "authors": "Anna-Pia Lohfink, Simon D. Duque Anton, Heike Leitte, Christoph Garth", "title": "Knowledge Rocks:Adding Knowledge Assistance to Visualization Systems", "comments": "IEEE Vis 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Knowledge Rocks, an implementation strategy and guideline for\naugmenting visualization systems to knowledge-assisted visualization systems,\nas defined by the KAVA model. Visualization systems become more and more\nsophisticated. Hence, it is increasingly important to support users with an\nintegrated knowledge base in making constructive choices and drawing the right\nconclusions. We support the effective reactivation of visualization software\nresources by augmenting them with knowledge-assistance. To provide a general\nand yet supportive implementation strategy, we propose an implementation\nprocess that bases on an application-agnostic architecture. This architecture\nis derived from existing knowledge-assisted visualization systems and the KAVA\nmodel. Its centerpiece is an ontology that is able to automatically analyze and\nclassify input data, linked to a database to store classified instances. We\ndiscuss design decisions and advantages of the KR framework and illustrate its\nbroad area of application in diverse integration possibilities of this\narchitecture into an existing visualization system. In addition, we provide a\ndetailed case study by augmenting an it-security system with\nknowledge-assistance facilities.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 09:26:31 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Lohfink", "Anna-Pia", ""], ["Anton", "Simon D. Duque", ""], ["Leitte", "Heike", ""], ["Garth", "Christoph", ""]]}, {"id": "2107.11172", "submitter": "Mohit Singhala", "authors": "Mohit Singhala, Jacob Carducci and Jeremy D. Brown", "title": "Towards an understanding of how humans perceive stiffness during\n  bimanual exploration", "comments": "Accepted as Works-in-Progress paper at Haptics Symposium 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an experimental testbed and associated psychophysical paradigm\nare presented for understanding how people discriminate torsional stiffness\nusing wrist rotation about their forearm. Featured in the testbed are two 1-DoF\nrotary kinesthetic haptic devices. An adaptive staircase was used to evaluate\nJNDs for a stiffness discrimination task where participants explored virtual\ntorsion springs by rotating their forearms. The JNDs were evaluated across\nseven different conditions, under four different exploration modes: bimanual,\nunimanual, bimanual feedback for unimanual displacement, and unimanual feedback\nfor bimanual displacement. The discrimination results will inform future\ninvestigation into understanding how stiffness percepts vary.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 16:50:59 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Singhala", "Mohit", ""], ["Carducci", "Jacob", ""], ["Brown", "Jeremy D.", ""]]}, {"id": "2107.11174", "submitter": "Mohit Singhala", "authors": "Mohit Singhala, Amy Chi, Maria Coleman and Jeremy D. Brown", "title": "Preliminary investigation into how limb choice affects kinesthetic\n  perception", "comments": "Accepted as Works-in-Progress paper to World Haptics 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have a limited understanding of how we integrate haptic information in\nreal-time from our upper limbs to perform complex bimanual tasks, an ability\nthat humans routinely employ to perform tasks of varying levels of difficulty.\nIn order to understand how information from both limbs is used to create a\nunified percept, it is important to study both the limbs separately first.\nPrevalent theories highlighting the role of central nervous system (CNS) in\naccounting for internal body dynamics seem to suggest that both upper limbs\nshould be equally sensitive to external stimuli. However, there is empirical\nproof demonstrating a perceptual difference in our upper limbs for tasks like\nshape discrimination, prompting the need to study effects of limb choice on\nkinesthetic perception. In this manuscript, we start evaluating Just Noticeable\nDifference (JND) for stiffness for both forearms separately. Early results\nvalidate the need for a more thorough investigation of limb choice on\nkinesthetic perception.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 16:56:43 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Singhala", "Mohit", ""], ["Chi", "Amy", ""], ["Coleman", "Maria", ""], ["Brown", "Jeremy D.", ""]]}, {"id": "2107.11181", "submitter": "Huyen N. Nguyen", "authors": "Huyen N. Nguyen, Jake Gonzalez, Jian Guo, Ngan V.T. Nguyen, and Tommy\n  Dang", "title": "VisMCA: A Visual Analytics System for Misclassification Correction and\n  Analysis. VAST Challenge 2020, Mini-Challenge 2 Award: Honorable Mention for\n  Detailed Analysis of Patterns of Misclassification", "comments": null, "journal-ref": "IEEE Conference on Visual Analytics Science and Technology (VAST)\n  2020", "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents VisMCA, an interactive visual analytics system that\nsupports deepening understanding in ML results, augmenting users' capabilities\nin correcting misclassification, and providing an analysis of underlying\npatterns, in response to the VAST Challenge 2020 Mini-Challenge 2. VisMCA\nfacilitates tracking provenance and provides a comprehensive view of object\ndetection results, easing re-labeling, and producing reliable, corrected data\nfor future training. Our solution implements multiple analytical views on\nvisual analysis to offer a deep insight for underlying pattern discovery.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 09:02:57 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Nguyen", "Huyen N.", ""], ["Gonzalez", "Jake", ""], ["Guo", "Jian", ""], ["Nguyen", "Ngan V. T.", ""], ["Dang", "Tommy", ""]]}, {"id": "2107.11367", "submitter": "Caitlyn McColeman PhD", "authors": "Caitlyn M. McColeman, Fumeng Yang, Steven Franconeri, Timothy F. Brady", "title": "Rethinking the Ranks of Visual Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Data can be visually represented using visual channels like position, length\nor luminance. An existing ranking of these visual channels is based on how\naccurately participants could report the ratio between two depicted values.\nThere is an assumption that this ranking should hold for different tasks and\nfor different numbers of marks. However, there is little existing work testing\nassumption, especially given that visually computing ratios is relatively\nunimportant in real-world visualizations, compared to seeing, remembering, and\ncomparing trends and motifs, across displays that almost universally depict\nmore than two values.\n  We asked participants to immediately reproduce a set of values from memory.\nWith a Bayesian multilevel modeling approach, we observed how the relevant rank\npositions of visual channels shift across different numbers of marks (2, 4 or\n8) and for bias, precision, and error measures. The ranking did not hold, even\nfor reproductions of only 2 marks, and the new ranking was highly inconsistent\nfor reproductions of different numbers of marks. Other factors besides channel\nchoice far more influence on performance, such as the number of values in the\nseries (e.g. more marks led to larger errors), or the value of each mark (e.g.\nsmall values are systematically overestimated).\n  Recall was worse for displays with 8 marks than 4, consistent with\nestablished limits on visual memory. These results show that we must move\nbeyond two-value ratio judgments as a baseline for ranking the quality of a\nvisual channel, including testing new tasks (detection of trends or motifs),\ntimescales (immediate computation, or later comparison), and the number of\nvalues (from a handful, to thousands).\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 17:40:35 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["McColeman", "Caitlyn M.", ""], ["Yang", "Fumeng", ""], ["Franconeri", "Steven", ""], ["Brady", "Timothy F.", ""]]}, {"id": "2107.11413", "submitter": "Dong Yin", "authors": "Keren Gu, Xander Masotto, Vandana Bachani, Balaji Lakshminarayanan,\n  Jack Nikodem, Dong Yin", "title": "A Realistic Simulation Framework for Learning with Label Noise", "comments": "Datasets released at\n  https://github.com/deepmind/deepmind-research/tree/master/noisy_label", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simulation framework for generating realistic instance-dependent\nnoisy labels via a pseudo-labeling paradigm. We show that this framework\ngenerates synthetic noisy labels that exhibit important characteristics of the\nlabel noise in practical settings via comparison with the CIFAR10-H dataset.\nEquipped with controllable label noise, we study the negative impact of noisy\nlabels across a few realistic settings to understand when label noise is more\nproblematic. We also benchmark several existing algorithms for learning with\nnoisy labels and compare their behavior on our synthetic datasets and on the\ndatasets with independent random label noise. Additionally, with the\navailability of annotator information from our simulation framework, we propose\na new technique, Label Quality Model (LQM), that leverages annotator features\nto predict and correct against noisy labels. We show that by adding LQM as a\nlabel correction step before applying existing noisy label techniques, we can\nfurther improve the models' performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 18:53:53 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Gu", "Keren", ""], ["Masotto", "Xander", ""], ["Bachani", "Vandana", ""], ["Lakshminarayanan", "Balaji", ""], ["Nikodem", "Jack", ""], ["Yin", "Dong", ""]]}, {"id": "2107.11441", "submitter": "Jimmy Moore", "authors": "Jimmy Moore and Pascal Goffin and Jason Wiese and Miriah Meyer", "title": "An interview method for engagement with personal sensor data", "comments": "17 pages, 3 figures, submitted to The Proceedings of the ACM on\n  Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) 2022", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Whether investigating research questions or designing systems, many\nresearchers and designers need to engage users with their personal data.\nHowever, it is difficult to successfully design user-facing tools for\ninteracting with personal data without first understanding what users want to\ndo with their data. Techniques for raw data exploration, sketching, or\nphysicalization can avoid the perils of tool development, but prevent direct\nanalytical access to users' rich personal data streams. We present a new method\nthat directly tackles this challenge: the data engagement interview. This\ninterview method incorporates an analyst to provide real-time analysis of\npersonal data streams, granting interview participants opportunities to\ndirectly engage their data, and interviewers to observe and ask questions\nthroughout this engagement. We describe the method's development through a case\nstudy with asthmatic participants, share insights and guidance from our\nexperience, and report a broad set of insights from these interviews.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jul 2021 19:52:51 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Moore", "Jimmy", ""], ["Goffin", "Pascal", ""], ["Wiese", "Jason", ""], ["Meyer", "Miriah", ""]]}, {"id": "2107.11633", "submitter": "Paul Rosen", "authors": "Rifat Ara Proma and Matthew Sumpter and Humberto Lugo and Elizabeth\n  Friedman and Khandaker Tasnim Huq and Paul Rosen", "title": "CleanAirNowKC: Building Community Power by Improving Data Accessibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As cities continue to grow globally, air pollution is increasing at an\nalarming rate, causing a significant negative impact on public health. One way\nto affect the negative impact is to regulate the producers of such pollution\nthrough policy implementation and enforcement. CleanAirNowKC (CAN-KC) is an\nenvironmental justice organization based in Kansas City (KC), Kansas. As part\nof their organizational objectives, they have to date deployed nine PurpleAir\nair quality sensors in different locations about which the community has\nexpressed concern. In this paper, we have implemented an interactive map that\ncan help the community members to monitor air quality efficiently. The system\nalso allows for reporting and tracking industrial emissions or toxic releases,\nwhich will further help identify major contributors to pollution. These\nresources can serve an important role as evidence that will assist in\nadvocating for community-driven just policies to improve the air quality\nregulation in Kansas City.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 15:31:24 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Proma", "Rifat Ara", ""], ["Sumpter", "Matthew", ""], ["Lugo", "Humberto", ""], ["Friedman", "Elizabeth", ""], ["Huq", "Khandaker Tasnim", ""], ["Rosen", "Paul", ""]]}, {"id": "2107.11637", "submitter": "Allan Wang", "authors": "Allan Wang, Christoforos Mavrogiannis, Aaron Steinfeld", "title": "Group-based Motion Prediction for Navigation in Crowded Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We focus on the problem of planning the motion of a robot in a dynamic\nmultiagent environment such as a pedestrian scene. Enabling the robot to\nnavigate safely and in a socially compliant fashion in such scenes requires a\nrepresentation that accounts for the unfolding multiagent dynamics. Existing\napproaches to this problem tend to employ microscopic models of motion\nprediction that reason about the individual behavior of other agents. While\nsuch models may achieve high tracking accuracy in trajectory prediction\nbenchmarks, they often lack an understanding of the group structures unfolding\nin crowded scenes. Inspired by the Gestalt theory from psychology, we build a\nModel Predictive Control framework (G-MPC) that leverages group-based\nprediction for robot motion planning. We conduct an extensive simulation study\ninvolving a series of challenging navigation tasks in scenes extracted from two\nreal-world pedestrian datasets. We illustrate that G-MPC enables a robot to\nachieve statistically significantly higher safety and lower number of group\nintrusions than a series of baselines featuring individual pedestrian motion\nprediction models. Finally, we show that G-MPC can handle noisy lidar-scan\nestimates without significant performance losses.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 15:51:43 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wang", "Allan", ""], ["Mavrogiannis", "Christoforos", ""], ["Steinfeld", "Aaron", ""]]}, {"id": "2107.11963", "submitter": "Murtuza Shergadwala", "authors": "Murtuza N. Shergadwala and Zhaoqing Teng and Magy Seif El-Nasr", "title": "Can we infer player behavior tendencies from a player's decision-making\n  data? Integrating Theory of Mind to Player Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Game AI systems need the theory of mind, which is the humanistic ability to\ninfer others' mental models, preferences, and intent. Such systems would enable\ninferring players' behavior tendencies that contribute to the variations in\ntheir decision-making behaviors. To that end, in this paper, we propose the use\nof inverse Bayesian inference to infer behavior tendencies given a descriptive\ncognitive model of a player's decision making. The model embeds behavior\ntendencies as weight parameters in a player's decision-making. Inferences on\nsuch parameters provide intuitive interpretations about a player's cognition\nwhile making in-game decisions. We illustrate the use of inverse Bayesian\ninference with synthetically generated data in a game called \\textit{BoomTown}\ndeveloped by Gallup. We use the proposed model to infer a player's behavior\ntendencies for moving decisions on a game map. Our results indicate that our\nmodel is able to infer these parameters towards uncovering not only a player's\ndecision making but also their behavior tendencies for making such decisions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 05:19:49 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Shergadwala", "Murtuza N.", ""], ["Teng", "Zhaoqing", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "2107.12061", "submitter": "Christian Guckelsberger", "authors": "Shaghayegh Roohi and Christian Guckelsberger and Asko Relas and Henri\n  Heiskanen and Jari Takatalo and Perttu H\\\"am\\\"al\\\"ainen", "title": "Predicting Game Engagement and Difficulty Using AI Players", "comments": "18 pages, 5 figures, 2 tables. In Proceedings ACM Human-Computer\n  Interaction, Vol. 5, CHIPLAY, Article 231. Publication date: September 2021", "journal-ref": null, "doi": "10.1145/3474658", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach to automated playtesting for the\nprediction of human player behavior and experience. It has previously been\ndemonstrated that Deep Reinforcement Learning (DRL) game-playing agents can\npredict both game difficulty and player engagement, operationalized as average\npass and churn rates. We improve this approach by enhancing DRL with Monte\nCarlo Tree Search (MCTS). We also motivate an enhanced selection strategy for\npredictor features, based on the observation that an AI agent's best-case\nperformance can yield stronger correlations with human data than the agent's\naverage performance. Both additions consistently improve the prediction\naccuracy, and the DRL-enhanced MCTS outperforms both DRL and vanilla MCTS in\nthe hardest levels. We conclude that player modelling via automated playtesting\ncan benefit from combining DRL and MCTS. Moreover, it can be worthwhile to\ninvestigate a subset of repeated best AI agent runs, if AI gameplay does not\nyield good predictions on average.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 09:31:57 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Roohi", "Shaghayegh", ""], ["Guckelsberger", "Christian", ""], ["Relas", "Asko", ""], ["Heiskanen", "Henri", ""], ["Takatalo", "Jari", ""], ["H\u00e4m\u00e4l\u00e4inen", "Perttu", ""]]}, {"id": "2107.12108", "submitter": "Lars Moormann", "authors": "J. van Hegelsom, J.M. van de Mortel-Fronczak, L. Moormann, D.A. van\n  Beek, J.E. Rooda", "title": "Development of a 3D Digital Twin of the Swalmen Tunnel in the\n  Rijkswaterstaat Project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an ongoing project, a cooperation between the TU/e and the Dutch\nDepartment of Waterways and Public Works (Rijkswaterstaat in Dutch, abbreviated\nto RWS) is established. The project focuses on investigating applicability of\nsynthesis-based engineering in the design of supervisory controllers for\nbridges, waterways and tunnels. Supervisory controllers ensure correct\ncooperation between components in a system. The design process of these\ncontrollers partly relies on simulation with models of the plant (the physical\nsystem). A possible addition to this design process is digital twin technology.\nA digital twin is a virtual copy of a system that is generally much more\nrealistic than the 2D simulation models that are currently used for supervisory\ncontroller validation. In this report, the development of a digital twin of the\nSwalmen tunnel that is suitable for supervisory control validation is\ndescribed. The Swalmen tunnel is a highway tunnel in Limburg, the Netherlands.\nThis case study is relevant, because the Swalmen tunnel will be renovated in\n2023 and 2028. These renovation projects include updating controlled subsystems\nin the tunnel, such as boom barriers and traffic lights, and updating the\nsupervisory controller of the tunnel. The digital twin might be useful to aid\nthe supervisory controller design process in these renovation projects.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 11:03:16 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["van Hegelsom", "J.", ""], ["van de Mortel-Fronczak", "J. M.", ""], ["Moormann", "L.", ""], ["van Beek", "D. A.", ""], ["Rooda", "J. E.", ""]]}, {"id": "2107.12143", "submitter": "Alara Zindanc{\\i}o\\u{g}lu", "authors": "Alara Zindanc{\\i}o\\u{g}lu and T. Metin Sezgin", "title": "Perceptually Validated Precise Local Editing for Facial Action Units\n  with StyleGAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to edit facial expressions has a wide range of applications in\ncomputer graphics. The ideal facial expression editing algorithm needs to\nsatisfy two important criteria. First, it should allow precise and targeted\nediting of individual facial actions. Second, it should generate high fidelity\noutputs without artifacts. We build a solution based on StyleGAN, which has\nbeen used extensively for semantic manipulation of faces. As we do so, we add\nto our understanding of how various semantic attributes are encoded in\nStyleGAN. In particular, we show that a naive strategy to perform editing in\nthe latent space results in undesired coupling between certain action units,\neven if they are conceptually distinct. For example, although brow lowerer and\nlip tightener are distinct action units, they appear correlated in the training\ndata. Hence, StyleGAN has difficulty in disentangling them. We allow\ndisentangled editing of such action units by computing detached regions of\ninfluence for each action unit, and restrict editing to these regions. We\nvalidate the effectiveness of our local editing method through perception\nexperiments conducted with 23 subjects. The results show that our method\nprovides higher control over local editing and produces images with superior\nfidelity compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:21:37 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 09:05:22 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zindanc\u0131o\u011flu", "Alara", ""], ["Sezgin", "T. Metin", ""]]}, {"id": "2107.12167", "submitter": "Abdul Rafey Aftab", "authors": "Abdul Rafey Aftab, Michael von der Beeck, Steven Rohrhirsch, Benoit\n  Diotte, Michael Feld", "title": "Multimodal Fusion Using Deep Learning Applied to Driver's Referencing of\n  Outside-Vehicle Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is a growing interest in more intelligent natural user interaction with\nthe car. Hand gestures and speech are already being applied for driver-car\ninteraction. Moreover, multimodal approaches are also showing promise in the\nautomotive industry. In this paper, we utilize deep learning for a multimodal\nfusion network for referencing objects outside the vehicle. We use features\nfrom gaze, head pose and finger pointing simultaneously to precisely predict\nthe referenced objects in different car poses. We demonstrate the practical\nlimitations of each modality when used for a natural form of referencing,\nspecifically inside the car. As evident from our results, we overcome the\nmodality specific limitations, to a large extent, by the addition of other\nmodalities. This work highlights the importance of multimodal sensing,\nespecially when moving towards natural user interaction. Furthermore, our user\nbased analysis shows noteworthy differences in recognition of user behavior\ndepending upon the vehicle pose.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 12:37:06 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Aftab", "Abdul Rafey", ""], ["von der Beeck", "Michael", ""], ["Rohrhirsch", "Steven", ""], ["Diotte", "Benoit", ""], ["Feld", "Michael", ""]]}, {"id": "2107.12244", "submitter": "George Boateng", "authors": "Prince Steven Annor, Samuel Boateng, Edwin Pelpuo Kayang, George\n  Boateng", "title": "AutoGrad: Automated Grading Software for Mobile Game Assignments in\n  SuaCode Courses", "comments": "7 pages. Under review at the Computer Science Education Research\n  Conference (CSERC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automatic grading systems have been in existence since the turn of the\nhalf-century. Several systems have been developed in the literature with either\nstatic analysis and dynamic analysis or a hybrid of both methodologies for\ncomputer science courses. This paper presents AutoGrad, a novel portable\ncross-platform automatic grading system for graphical Processing programs\ndeveloped on Android smartphones during an online course. AutoGrad uses\nProcessing, which is used in the emerging Interactive Media Arts, and pioneers\ngrading systems utilized outside the sciences to assist tuition in the Arts. It\nalso represents the first system built and tested in an African context across\nover thirty-five countries across the continent. This paper first explores the\ndesign and implementation of AutoGrad. AutoGrad employs APIs to download the\nassignments from the course platform, performs static and dynamic analysis on\nthe assignment to evaluate the graphical output of the program, and returns the\ngrade and feedback to the student. It then evaluates AutoGrad by analyzing data\ncollected from the two online cohorts of 1000+ students of our SuaCode\nsmartphone-based course. From the analysis and students' feedback, AutoGrad is\nshown to be adequate for automatic assessment, feedback provision to students,\nand easy integration for both cloud and standalone usage by reducing the time\nand effort required in grading the 4 assignments required to complete the\ncourse.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 14:46:13 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Annor", "Prince Steven", ""], ["Boateng", "Samuel", ""], ["Kayang", "Edwin Pelpuo", ""], ["Boateng", "George", ""]]}, {"id": "2107.12257", "submitter": "George Boateng", "authors": "George Boateng, Prince Steven Annor, Victor Kumbol", "title": "SuaCode Africa: Teaching Coding Online to Africans using Smartphones", "comments": "7 pages. Under review at the Computer Science Education Research\n  Conference (CSERC) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is a burgeoning trend of smartphone ownership in Africa due to the low\ncosts of Android smartphones and the global increase in social media usage.\nBuilding upon previous works that introduced a smartphone-based coding course\nto secondary and tertiary students in Ghana via an in-person program and an\nonline course, this work introduced Africans in 37 countries to our online\nsmartphone-based course in 2019. Students in this 8-week course read lesson\nnotes, submitted assignments, collaborated with peers, and facilitators in an\nonline forum and completed open and closed-ended surveys after the course. We\nperformed qualitative and quantitative analyses on the data from the course.\n  Out of the 709 students that applied, 210 were officially admitted to the\ncourse after passing the preliminary assignments. And at the end of the course,\n72% of the 210 students completed the course. Additionally, students'\nassignment submissions and self-reports showed an understanding of the\nprogramming concepts, with comparable performance between males and females and\nacross educational levels. Also, students mentioned that the lesson notes were\neasy to understand and they enjoyed the experience of writing code on their\nsmartphones. Moreover, students adequately received help from peers and\nfacilitators in the course forum. Lastly, results of a survey sent to students\na year after completing this program showed that they had developed various\napplications, wrote online tutorials, and learned several tools and\ntechnologies. We were successful at introducing coding skills to Africans using\nsmartphones through SuaCode Africa.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 15:03:09 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Boateng", "George", ""], ["Annor", "Prince Steven", ""], ["Kumbol", "Victor", ""]]}, {"id": "2107.12317", "submitter": "Zachary Eberhart", "authors": "Zachary Eberhart and Collin McMillan", "title": "Dialogue Management for Interactive API Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  API search involves finding components in an API that are relevant to a\nprogramming task. For example, a programmer may need a function in a C library\nthat opens a new network connection, then another function that sends data\nacross that connection. Unfortunately, programmers often have trouble finding\nthe API components that they need. A strong scientific consensus is emerging\ntowards developing interactive tool support that responds to conversational\nfeedback, emulating the experience of asking a fellow human programmer for\nhelp. A major barrier to creating these interactive tools is implementing\ndialogue management for API search. Dialogue management involves determining\nhow a system should respond to user input, such as whether to ask a\nclarification question or to display potential results. In this paper, we\npresent a dialogue manager for interactive API search that considers search\nresults and dialogue history to select efficient actions. We implement two\ndialogue policies: a hand-crafted policy and a policy optimized via\nreinforcement learning. We perform a synthetics evaluation and a human\nevaluation comparing the policies to a generic single-turn, top-N policy used\nby source code search engines.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 16:38:50 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Eberhart", "Zachary", ""], ["McMillan", "Collin", ""]]}, {"id": "2107.12443", "submitter": "Niklas Stoehr", "authors": "Raphael Lepuschitz and Niklas Stoehr", "title": "SeismographAPI: Visualising Temporal-Spatial Crisis Data", "comments": "Published in the KDD Workshop on Data-driven Humanitarian Mapping\n  held with the 27th ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD '21), August 14, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective decision-making for crisis mitigation increasingly relies on\nvisualisation of large amounts of data. While interactive dashboards are more\ninformative than static visualisations, their development is far more\ntime-demanding and requires a range of technical and financial capabilities.\nThere are few open-source libraries available, which is blocking contributions\nfrom low-resource environments and impeding rapid crisis responses. To address\nthese limitations, we present SeismographAPI, an open-source library for\nvisualising temporal-spatial crisis data on the country- and sub-country level\nin two use cases: Conflict Monitoring Map and Pandemic Monitoring Map. The\nlibrary provides easy-to-use data connectors, broad functionality, clear\ndocumentation and run time-efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2021 19:20:19 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Lepuschitz", "Raphael", ""], ["Stoehr", "Niklas", ""]]}, {"id": "2107.12548", "submitter": "Haotian Li", "authors": "Haotian Li, Yong Wang, Songheng Zhang, Yangqiu Song, Huamin Qu", "title": "KG4Vis: A Knowledge Graph-Based Approach for Visualization\n  Recommendation", "comments": "11 pages, 8 figures. Accepted to IEEE VIS 2021 and will appear in\n  IEEE Transactions on Visualization & Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization recommendation or automatic visualization generation can\nsignificantly lower the barriers for general users to rapidly create effective\ndata visualizations, especially for those users without a background in data\nvisualizations. However, existing rule-based approaches require tedious manual\nspecifications of visualization rules by visualization experts. Other machine\nlearning-based approaches often work like black-box and are difficult to\nunderstand why a specific visualization is recommended, limiting the wider\nadoption of these approaches. This paper fills the gap by presenting KG4Vis, a\nknowledge graph (KG)-based approach for visualization recommendation. It does\nnot require manual specifications of visualization rules and can also guarantee\ngood explainability. Specifically, we propose a framework for building\nknowledge graphs, consisting of three types of entities (i.e., data features,\ndata columns and visualization design choices) and the relations between them,\nto model the mapping rules between data and effective visualizations. A\nTransE-based embedding technique is employed to learn the embeddings of both\nentities and relations of the knowledge graph from existing\ndataset-visualization pairs. Such embeddings intrinsically model the desirable\nvisualization rules. Then, given a new dataset, effective visualizations can be\ninferred from the knowledge graph with semantically meaningful rules. We\nconducted extensive evaluations to assess the proposed approach, including\nquantitative comparisons, case studies and expert interviews. The results\ndemonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 01:47:35 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Li", "Haotian", ""], ["Wang", "Yong", ""], ["Zhang", "Songheng", ""], ["Song", "Yangqiu", ""], ["Qu", "Huamin", ""]]}, {"id": "2107.12567", "submitter": "Yuka Ikarashi", "authors": "Yuka Ikarashi, Jonathan Ragan-Kelley, Tsukasa Fukusato, Jun Kato,\n  Takeo Igarashi", "title": "Guided Optimization for Image Processing Pipelines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Writing high-performance image processing code is challenging and\nlabor-intensive. The Halide programming language simplifies this task by\ndecoupling high-level algorithms from \"schedules\" which optimize their\nimplementation. However, even with this abstraction, it is still challenging\nfor Halide programmers to understand complicated scheduling strategies and\nproductively write valid, optimized schedules. To address this, we propose a\nprogramming support method called \"guided optimization.\" Guided optimization\nprovides programmers a set of valid optimization options and interactive\nfeedback about their current choices, which enables them to comprehend and\nefficiently optimize image processing code without the time-consuming\ntrial-and-error process of traditional text editors. We implemented a\nproof-of-concept system, Roly-poly, which integrates guided optimization,\nprogram visualization, and schedule cost estimation to support the\ncomprehension and development of efficient Halide image processing code. We\nconducted a user study with novice Halide programmers and confirmed that\nRoly-poly and its guided optimization was informative, increased productivity,\nand resulted in higher-performing schedules in less time.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 03:02:17 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 03:00:11 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ikarashi", "Yuka", ""], ["Ragan-Kelley", "Jonathan", ""], ["Fukusato", "Tsukasa", ""], ["Kato", "Jun", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2107.12597", "submitter": "Maximilian Altmeyer", "authors": "Maximilian Altmeyer, Marc Schubhan, Antonio Kr\\\"uger, Pascal Lessel", "title": "A Long-Term Investigation on the Effects of (Personalized) Gamification\n  on Course Participation in a Gym", "comments": null, "journal-ref": "Proceedings of the 5th International GamiFIN Conference\n  (GamiFIN-2021)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gamification is frequently used to motivate people getting more physically\nactive. However, most systems follow a one-size-fits-all gamification approach,\nalthough past research has shown that interpersonal differences exist in the\nperception of gamification elements. Also, most studies investigating the\neffects of gamification are rather short, although it has been shown that\ngamification can suffer from novelty effects. In this paper, we address both\nthese issues by investigating whether gamification elements, integrated into a\nfitness course booking system, have an effect on how frequently users\nparticipate in fitness courses in a gym (N=52) over a duration of 275 days (548\ndays including baseline). Also, the gamification elements that we implemented\nare tailored to specific Hexad user types, which allows us to investigate\nwhether using suitable gamification elements leads to an increased course\nparticipation. Our results show that gamification increased the participation\nin fitness courses significantly and that users who received a suitable set of\ngamification elements - according to their Hexad user type - increased their\nparticipation significantly more than others.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 04:54:19 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Altmeyer", "Maximilian", ""], ["Schubhan", "Marc", ""], ["Kr\u00fcger", "Antonio", ""], ["Lessel", "Pascal", ""]]}, {"id": "2107.12599", "submitter": "Maximilian Altmeyer", "authors": "Maximilian Altmeyer, Pascal Lessel, Atiq Ur Rehman Waqar, Antonio\n  Kr\\\"uger", "title": "Design Guidelines to Increase the Persuasiveness of Achievement Goals\n  for Physical Activity", "comments": null, "journal-ref": "Proceedings of the 5th International GamiFIN Conference\n  (GamiFIN-2021)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Achievement goals are frequently used to support behavior change. However,\nthey are often not specifically designed for this purpose nor account for the\ndegree to which a user is already intending to perform the target behavior. In\nthis paper, we investigate the perceived persuasiveness of different goal types\nas defined by the 3x2 Achievement Goal Model, what people like and dislike\nabout them and the role that behavior change intentions play when aiming at\nincreasing step counts. We created visualizations for each goal type based on a\nqualitative pre-study (N=18) and ensured their comprehensibility (N=18). In an\nonline experiment (N=118), we show that there are differences in the perception\nof these goal types and that behavior change intentions should be considered to\nmaximize their persuasiveness as goals evolve. Next, we derive design\nguidelines on when to use which type of achievement goal and what to consider\nwhen using them\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 04:56:52 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Altmeyer", "Maximilian", ""], ["Lessel", "Pascal", ""], ["Waqar", "Atiq Ur Rehman", ""], ["Kr\u00fcger", "Antonio", ""]]}, {"id": "2107.12682", "submitter": "Anna-Pia Lohfink", "authors": "Anna-Pia Lohfink, Frederike Gartzky, Florian Wetzels, Luisa Vollmer,\n  Christoph Garth", "title": "Time-Varying Fuzzy Contour Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a holistic, topology-based visualization technique for spatial\ntime series data based on an adaptation of Fuzzy Contour Trees. Common analysis\napproaches for time dependent scalar fields identify and track specific\nfeatures. To give a more general overview of the data, we extend Fuzzy Contour\nTrees, from the visualization and simultaneous analysis of the topology of\nmultiple scalar fields, to time dependent scalar fields. The resulting\ntime-varying Fuzzy Contour Trees allow the comparison of multiple time steps\nthat are not required to be consecutive. We provide specific interaction and\nnavigation possibilities that allow the exploration of individual time steps\nand time windows in addition to the behavior of the contour trees over all time\nsteps. To achieve this, we reduce an existing alignment to multiple\nsub-alignments and adapt the Fuzzy Contour Tree-layout to continuously reflect\nchanges and similarities in the sub-alignments. We apply time-varying Fuzzy\nContour Trees to different real-world data sets and demonstrate their\nusefulness.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 09:10:10 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Lohfink", "Anna-Pia", ""], ["Gartzky", "Frederike", ""], ["Wetzels", "Florian", ""], ["Vollmer", "Luisa", ""], ["Garth", "Christoph", ""]]}, {"id": "2107.12696", "submitter": "Staas De Jong", "authors": "Staas de Jong", "title": "A tactile closed-loop device for musical interaction", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2006", "journal-ref": null, "doi": "10.5281/zenodo.1176935", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a device implementing a closed tactile loop for musical\ninteraction, based on a small freely held magnet which serves as the medium for\nboth input and output. The component parts as well as an example of its\nprogrammable behaviour are described.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 09:54:14 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["de Jong", "Staas", ""]]}, {"id": "2107.12704", "submitter": "Staas De Jong", "authors": "Staas de Jong", "title": "The cyclotactor: towards a tactile platform for musical interaction", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2008", "journal-ref": null, "doi": "10.5281/zenodo.1179571", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reports on work in progress on a finger-based tactile I/O device\nfor musical interaction. Central to the device is the ability to set up\ncyclical relationships between tactile input and output. A direct practical\napplication of this to musical interaction is given, using the idea to\nmultiplex two degrees of freedom on a single tactile loop.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:02:57 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["de Jong", "Staas", ""]]}, {"id": "2107.12709", "submitter": "Staas De Jong", "authors": "Staas de Jong", "title": "Developing the cyclotactor", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2009", "journal-ref": null, "doi": "10.5281/zenodo.1177591", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents developments in the technology underlying the\ncyclotactor, a finger-based tactile I/O device for musical interaction. These\ninclude significant improvements both in the basic characteristics of tactile\ninteraction and in the related (vibro)tactile sample rates, latencies, and\ntiming precision. After presenting the new prototype's tactile output force\nlandscape, some of the new possibilities for interaction are discussed,\nespecially those for musical interaction with zero audio/tactile latency.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:10:25 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["de Jong", "Staas", ""]]}, {"id": "2107.12714", "submitter": "Staas De Jong", "authors": "Staas de Jong", "title": "Making grains tangible: microtouch for microsound", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2011", "journal-ref": null, "doi": "10.5281/zenodo.1178055", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new research direction for the large family of\ninstrumental musical interfaces where sound is generated using digital granular\nsynthesis, and where interaction and control involve the (fine) operation of\nstiff, flat contact surfaces. First, within a historical context, a general\nabsence of, and clear need for, tangible output that is dynamically\ninstantiated by the grain-generating process itself is identified. Second, to\nfill this gap, a concrete general approach is proposed based on the careful\nconstruction of non-vibratory and vibratory force pulses, in a one-to-one\nrelationship with sonic grains. An informal pilot psychophysics experiment\ninitiating the approach was conducted, which took into account the two main\ncases for applying forces to the human skin: perpendicular, and lateral.\nInitial results indicate that the force pulse approach can enable perceivably\nmultidimensional, tangible display of the ongoing grain-generating process.\nMoreover, it was found that this can be made to meaningfully happen (in real\ntime) in the same timescale of basic sonic grain generation. This is not a\ntrivial property, and provides an important and positive fundament for further\ndeveloping this type of enhanced display. It also leads to the exciting\nprospect of making arbitrary sonic grains actual physical manipulanda.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:18:56 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["de Jong", "Staas", ""]]}, {"id": "2107.12716", "submitter": "Staas De Jong", "authors": "Staas de Jong", "title": "Ghostfinger: a novel platform for fully computational fingertip\n  controllers", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2017", "journal-ref": null, "doi": "10.5281/zenodo.1176292", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Ghostfinger, a technology for highly dynamic up/down fingertip\nhaptics and control. The overall user experience offered by the technology can\nbe described as that of tangibly and audibly interacting with a small hologram.\nMore specifically, Ghostfinger implements automatic visualization of the\ndynamic instantiation/parametrization of algorithmic primitives that together\ndetermine the current haptic conditions for fingertip action. Some aspects of\nthis visualization are visuospatial: A floating see-through cursor provides\nreal-time, to-scale display of the fingerpad transducer, as it is being moved\nby the user. Simultaneously, each haptic primitive instance is represented by a\nfloating block shape, type-colored, variably transparent, and possibly\noverlapping with other such block shapes. Further aspects of visualization are\nsymbolic: Each instance is also represented by a type symbol, lighting up\nwithin a grid if the instance is providing output to the user. We discuss the\nsystem's user interface, programming interface, and potential applications.\nThis is done from a general perspective that articulates and emphasizes the\nuniquely enabling role of the principle of computation in the implementation of\nnew forms of instrumental control of musical sound. Beyond the currently\npresented technology, this also reflects more broadly on the role of Digital\nMusical Instruments (DMIs) in NIME.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 10:28:13 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["de Jong", "Staas", ""]]}, {"id": "2107.12734", "submitter": "Ralf Raumanns", "authors": "Ralf Raumanns, Gerard Schouten, Max Joosten, Josien P. W. Pluim and\n  Veronika Cheplygina", "title": "ENHANCE (ENriching Health data by ANnotations of Crowd and Experts): A\n  case study for skin lesion classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present ENHANCE, an open dataset with multiple annotations to complement\nthe existing ISIC and PH2 skin lesion classification datasets. This dataset\ncontains annotations of visual ABC (asymmetry, border, colour) features from\nnon-expert annotation sources: undergraduate students, crowd workers from\nAmazon MTurk and classic image processing algorithms. In this paper we first\nanalyse the correlations between the annotations and the diagnostic label of\nthe lesion, as well as study the agreement between different annotation\nsources. Overall we find weak correlations of non-expert annotations with the\ndiagnostic label, and low agreement between different annotation sources. We\nthen study multi-task learning (MTL) with the annotations as additional labels,\nand show that non-expert annotations can improve (ensembles of)\nstate-of-the-art convolutional neural networks via MTL. We hope that our\ndataset can be used in further research into multiple annotations and/or MTL.\nAll data and models are available on Github:\nhttps://github.com/raumannsr/ENHANCE.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 11:23:33 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Raumanns", "Ralf", ""], ["Schouten", "Gerard", ""], ["Joosten", "Max", ""], ["Pluim", "Josien P. W.", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "2107.12772", "submitter": "Enes Yigitbas", "authors": "Enes Yigitbas, Simon Gorissen, Nils Weidmann, Gregor Engels", "title": "Collaborative Software Modeling in Virtual Reality", "comments": "preprint, accepted at ACM/IEEE 24th International Conference on Model\n  Driven Engineering Languages and Systems (MODELS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling is a key activity in conceptual design and system design. Through\ncollaborative modeling, end-users, stakeholders, experts, and entrepreneurs are\nable to create a shared understanding of a system representation. While the\nUnified Modeling Language (UML) is one of the major conceptual modeling\nlanguages in object-oriented software engineering, more and more concerns arise\nfrom the modeling quality of UML and its tool support. Among them, the\nlimitation of the two-dimensional presentation of its notations and lack of\nnatural collaborative modeling tools are reported to be significant. In this\npaper, we explore the potential of using Virtual Reality (VR) technology for\ncollaborative UML software design by comparing it with classical collaborative\nsoftware design using conventional devices (Desktop PC, Laptop). For this\npurpose, we have developed a VR modeling environment that offers a natural\ncollaborative modeling experience for UML Class Diagrams. Based on a user study\nwith 24 participants, we have compared collaborative VR modeling with\nconventional modeling with regard to efficiency, effectiveness, and user\nsatisfaction. Results show that the use of VR has some disadvantages concerning\nefficiency and effectiveness, but the user's fun, the feeling of being in the\nsame room with a remote collaborator, and the naturalness of collaboration were\nincreased.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 12:34:54 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Yigitbas", "Enes", ""], ["Gorissen", "Simon", ""], ["Weidmann", "Nils", ""], ["Engels", "Gregor", ""]]}, {"id": "2107.12845", "submitter": "Antonio Lieto", "authors": "Agnese Augello, Giuseppe Citt\\`a, Manuel Gentile, Antonio Lieto", "title": "A Storytelling Robot managing Persuasive and Ethical Stances via ACT-R:\n  an Exploratory Study", "comments": "20 pages, 7 figures", "journal-ref": "International Journal of Social Robotics, 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a storytelling robot, controlled via the ACT-R cognitive\narchitecture, able to adopt different persuasive techniques and ethical stances\nwhile conversing about some topics concerning COVID-19. The main contribution\nof the paper consists in the proposal of a needs-driven model that guides and\nevaluates, during the dialogue, the use (if any) of persuasive techniques\navailable in the agent procedural memory. The portfolio of persuasive\ntechniques tested in such a model ranges from the use of storytelling, to\nframing techniques and rhetorical-based arguments. To the best of our\nknowledge, this represents the first attempt of building a persuasive agent\nable to integrate a mix of explicitly grounded cognitive assumptions about\ndialogue management, storytelling and persuasive techniques as well as ethical\nattitudes. The paper presents the results of an exploratory evaluation of the\nsystem on 63 participants\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 14:27:58 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Augello", "Agnese", ""], ["Citt\u00e0", "Giuseppe", ""], ["Gentile", "Manuel", ""], ["Lieto", "Antonio", ""]]}, {"id": "2107.12895", "submitter": "Roman Klinger", "authors": "Felix Casel and Amelie Heindl and Roman Klinger", "title": "Emotion Recognition under Consideration of the Emotion Component Process\n  Model", "comments": "accepted at KONVENS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Emotion classification in text is typically performed with neural network\nmodels which learn to associate linguistic units with emotions. While this\noften leads to good predictive performance, it does only help to a limited\ndegree to understand how emotions are communicated in various domains. The\nemotion component process model (CPM) by Scherer (2005) is an interesting\napproach to explain emotion communication. It states that emotions are a\ncoordinated process of various subcomponents, in reaction to an event, namely\nthe subjective feeling, the cognitive appraisal, the expression, a\nphysiological bodily reaction, and a motivational action tendency. We\nhypothesize that these components are associated with linguistic realizations:\nan emotion can be expressed by describing a physiological bodily reaction (\"he\nwas trembling\"), or the expression (\"she smiled\"), etc. We annotate existing\nliterature and Twitter emotion corpora with emotion component classes and find\nthat emotions on Twitter are predominantly expressed by event descriptions or\nsubjective reports of the feeling, while in literature, authors prefer to\ndescribe what characters do, and leave the interpretation to the reader. We\nfurther include the CPM in a multitask learning model and find that this\nsupports the emotion categorization. The annotated corpora are available at\nhttps://www.ims.uni-stuttgart.de/data/emotion.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 15:53:25 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Casel", "Felix", ""], ["Heindl", "Amelie", ""], ["Klinger", "Roman", ""]]}, {"id": "2107.13074", "submitter": "Sebastiaan De Peuter", "authors": "Sebastiaan De Peuter (1), Antti Oulasvirta (2), Samuel Kaski (1 and 3)\n  ((1) Department of Computer Science, Aalto University, Finland, (2)\n  Department of Communications and Networking, Aalto University, Finland, (3)\n  Department of Computer Science, University of Manchester, UK)", "title": "Toward AI Assistants That Let Designers Design", "comments": "9 pages, 3 figures, submitted to IEEE Computer magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AI for supporting designers needs to be rethought. It should aim to\ncooperate, not automate, by supporting and leveraging the creativity and\nproblem-solving of designers. The challenge for such AI is how to infer\ndesigners' goals and then help them without being needlessly disruptive. We\npresent AI-assisted design: a framework for creating such AI, built around\ngenerative user models which enable reasoning about designers' goals,\nreasoning, and capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 10:29:36 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["De Peuter", "Sebastiaan", "", "1 and 3"], ["Oulasvirta", "Antti", "", "1 and 3"], ["Kaski", "Samuel", "", "1 and 3"]]}, {"id": "2107.13076", "submitter": "Sondess Missaoui Dr.", "authors": "ennifer Chubba, Sondess Missaouib, Shauna Concannonc, Liam Maloneyb,\n  James Alfred Walker", "title": "Interactive Storytelling for Children: A Case-study of Design and\n  Development Considerations for Ethical Conversational AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conversational Artificial Intelligence (CAI) systems and Intelligent Personal\nAssistants (IPA), such as Alexa, Cortana, Google Home and Siri are becoming\nubiquitous in our lives, including those of children, the implications of which\nis receiving increased attention, specifically with respect to the effects of\nthese systems on children's cognitive, social and linguistic development.\nRecent advances address the implications of CAI with respect to privacy,\nsafety, security, and access. However, there is a need to connect and embed the\nethical and technical aspects in the design. Using a case-study of a research\nand development project focused on the use of CAI in storytelling for children,\nthis paper reflects on the social context within a specific case of technology\ndevelopment, as substantiated and supported by argumentation from within the\nliterature. It describes the decision making process behind the recommendations\nmade on this case for their adoption in the creative industries. Further\nresearch that engages with developers and stakeholders in the ethics of\nstorytelling through CAI is highlighted as a matter of urgency.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 15:11:45 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Chubba", "ennifer", ""], ["Missaouib", "Sondess", ""], ["Concannonc", "Shauna", ""], ["Maloneyb", "Liam", ""], ["Walker", "James Alfred", ""]]}, {"id": "2107.13115", "submitter": "Lu Wang", "authors": "Lu Wang, Munif Ishad Mujib, Jake Williams, George Demiris, Jina\n  Huh-Yoo", "title": "An Evaluation of Generative Pre-Training Model-based Therapy Chatbot for\n  Caregivers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of off-the-shelf intelligent home products and broader\ninternet adoption, researchers increasingly explore smart computing\napplications that provide easier access to health and wellness resources.\nAI-based systems like chatbots have the potential to provide services that\ncould provide mental health support. However, existing therapy chatbots are\noften retrieval-based, requiring users to respond with a constrained set of\nanswers, which may not be appropriate given that such pre-determined inquiries\nmay not reflect each patient's unique circumstances. Generative-based\napproaches, such as the OpenAI GPT models, could allow for more dynamic\nconversations in therapy chatbot contexts than previous approaches. To\ninvestigate the generative-based model's potential in therapy chatbot contexts,\nwe built a chatbot using the GPT-2 model. We fine-tuned it with 306 therapy\nsession transcripts between family caregivers of individuals with dementia and\ntherapists conducting Problem Solving Therapy. We then evaluated the model's\npre-trained and the fine-tuned model in terms of basic qualities using three\nmeta-information measurements: the proportion of non-word outputs, the length\nof response, and sentiment components. Results showed that: (1) the fine-tuned\nmodel created more non-word outputs than the pre-trained model; (2) the\nfine-tuned model generated outputs whose length was more similar to that of the\ntherapists compared to the pre-trained model; (3) both the pre-trained model\nand fine-tuned model were likely to generate more negative and fewer positive\noutputs than the therapists. We discuss potential reasons for the problem, the\nimplications, and solutions for developing therapy chatbots and call for\ninvestigations of the AI-based system application.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 01:01:08 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Wang", "Lu", ""], ["Mujib", "Munif Ishad", ""], ["Williams", "Jake", ""], ["Demiris", "George", ""], ["Huh-Yoo", "Jina", ""]]}, {"id": "2107.13165", "submitter": "Kushal Chawla", "authors": "Kushal Chawla, Rene Clever, Jaysa Ramirez, Gale Lucas, Jonathan Gratch", "title": "Towards Emotion-Aware Agents For Negotiation Dialogues", "comments": "Accepted at 9th International Conference on Affective Computing &\n  Intelligent Interaction (ACII 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Negotiation is a complex social interaction that encapsulates emotional\nencounters in human decision-making. Virtual agents that can negotiate with\nhumans are useful in pedagogy and conversational AI. To advance the development\nof such agents, we explore the prediction of two important subjective goals in\na negotiation - outcome satisfaction and partner perception. Specifically, we\nanalyze the extent to which emotion attributes extracted from the negotiation\nhelp in the prediction, above and beyond the individual difference variables.\nWe focus on a recent dataset in chat-based negotiations, grounded in a\nrealistic camping scenario. We study three degrees of emotion dimensions -\nemoticons, lexical, and contextual by leveraging affective lexicons and a\nstate-of-the-art deep learning architecture. Our insights will be helpful in\ndesigning adaptive negotiation agents that interact through realistic\ncommunication interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 04:42:36 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Chawla", "Kushal", ""], ["Clever", "Rene", ""], ["Ramirez", "Jaysa", ""], ["Lucas", "Gale", ""], ["Gratch", "Jonathan", ""]]}, {"id": "2107.13403", "submitter": "Slawomir Konrad Tadeja", "authors": "S{\\l}awomir Konrad Tadeja, Krzysztof Kutt, Yupu Lu, Pranay Seshadri,\n  Grzegorz J. Nalepa, Per Ola Kristensson", "title": "Jarvis for Aeroengine Analytics: A Speech Enhanced Virtual Reality\n  Demonstrator Based on Mining Knowledge Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a Virtual Reality (VR) based environment where the\nengineer interacts with incoming data from a fleet of aeroengines. This data\ntakes the form of 3D computer-aided design (CAD) engine models coupled with\ncharacteristic plots for the subsystems of each engine. Both the plots and\nmodels can be interacted with and manipulated using speech or gestural input.\nThe characteristic data is ported to a knowledge-based system underpinned by a\nknowledge-graph storing complex domain knowledge. This permits the system to\nrespond to queries about the current state and health of each aeroengine asset.\nResponses to these questions require some degree of analysis, which is handled\nby a semantic knowledge representation layer managing information on aeroengine\nsubsystems. This paper represents a significant step forward for aeroengine\nanalysis in a bespoke VR environment and brings us a step closer to a\nJarvis-like system for aeroengine analytics.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 14:46:16 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Tadeja", "S\u0142awomir Konrad", ""], ["Kutt", "Krzysztof", ""], ["Lu", "Yupu", ""], ["Seshadri", "Pranay", ""], ["Nalepa", "Grzegorz J.", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "2107.13405", "submitter": "Emanuele Lattanzi PhD.", "authors": "Emanuele Lattanzi, Lorenzo Calisti, Valerio Freschi", "title": "Automatic Unstructured Handwashing Recognition using Smartwatch to\n  Reduce Contact Transmission of Pathogens", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Current guidelines from the World Health Organization indicate that the\nSARSCoV-2 coronavirus, which results in the novel coronavirus disease\n(COVID-19), is transmitted through respiratory droplets or by contact. Contact\ntransmission occurs when contaminated hands touch the mucous membrane of the\nmouth, nose, or eyes. Moreover, pathogens can also be transferred from one\nsurface to another by contaminated hands, which facilitates transmission by\nindirect contact. Consequently, hands hygiene is extremely important to prevent\nthe spread of the SARSCoV-2 virus. Additionally, hand washing and/or hand\nrubbing disrupts also the transmission of other viruses and bacteria that cause\ncommon colds, flu and pneumonia, thereby reducing the overall disease burden.\nThe vast proliferation of wearable devices, such as smartwatches, containing\nacceleration, rotation, magnetic field sensors, etc., together with the modern\ntechnologies of artificial intelligence, such as machine learning and more\nrecently deep-learning, allow the development of accurate applications for\nrecognition and classification of human activities such as: walking, climbing\nstairs, running, clapping, sitting, sleeping, etc. In this work we evaluate the\nfeasibility of an automatic system, based on current smartwatches, which is\nable to recognize when a subject is washing or rubbing its hands, in order to\nmonitor parameters such as frequency and duration, and to evaluate the\neffectiveness of the gesture. Our preliminary results show a classification\naccuracy of about 95% and of about 94% for respectively deep and standard\nlearning techniques.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 14:52:45 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lattanzi", "Emanuele", ""], ["Calisti", "Lorenzo", ""], ["Freschi", "Valerio", ""]]}, {"id": "2107.13485", "submitter": "Alex Kale", "authors": "Alex Kale, Yifan Wu, and Jessica Hullman", "title": "Causal Support: Modeling Causal Inferences with Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysts often make visual causal inferences about possible data-generating\nmodels. However, visual analytics (VA) software tends to leave these models\nimplicit in the mind of the analyst, which casts doubt on the statistical\nvalidity of informal visual \"insights\". We formally evaluate the quality of\ncausal inferences from visualizations by adopting causal support -- a Bayesian\ncognition model that learns the probability of alternative causal explanations\ngiven some data -- as a normative benchmark for causal inferences. We\ncontribute two experiments assessing how well crowdworkers can detect (1) a\ntreatment effect and (2) a confounding relationship. We find that chart users'\ncausal inferences tend to be insensitive to sample size such that they deviate\nfrom our normative benchmark. While interactively cross-filtering data in\nvisualizations can improve sensitivity, on average users do not perform\nreliably better with common visualizations than they do with textual\ncontingency tables. These experiments demonstrate the utility of causal support\nas an evaluation framework for inferences in VA and point to opportunities to\nmake analysts' mental models more explicit in VA software.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:50:36 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Kale", "Alex", ""], ["Wu", "Yifan", ""], ["Hullman", "Jessica", ""]]}, {"id": "2107.13498", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Jinwoo Kim, JungHo Jeon, Jinding Xing, Changbum Ahn,\n  Pingbo Tang, and Hubo Cai", "title": "Toward Integrated Human-machine Intelligence for Civil Engineering: An\n  Interdisciplinary Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The purpose of this paper is to examine the opportunities and barriers of\nIntegrated Human-Machine Intelligence (IHMI) in civil engineering. Integrating\nartificial intelligence's high efficiency and repeatability with humans'\nadaptability in various contexts can advance timely and reliable\ndecision-making during civil engineering projects and emergencies. Successful\ncases in other domains, such as biomedical science, healthcare, and\ntransportation, showed the potential of IHMI in data-driven, knowledge-based\ndecision-making in numerous civil engineering applications. However, whether\nthe industry and academia are ready to embrace the era of IHMI and maximize its\nbenefit to the industry is still questionable due to several knowledge gaps.\nThis paper thus calls for future studies in exploring the value, method, and\nchallenges of applying IHMI in civil engineering. Our systematic review of the\nliterature and motivating cases has identified four knowledge gaps in achieving\neffective IHMI in civil engineering. First, it is unknown what types of tasks\nin the civil engineering domain can be assisted by AI and to what extent.\nSecond, the interface between human and AI in civil engineering-related tasks\nneed more precise and formal definition. Third, the barriers that impede\ncollecting detailed behavioral data from humans and contextual environments\ndeserve systematic classification and prototyping. Lastly, it is unknown what\nexpected and unexpected impacts will IHMI have on the AEC industry and\nentrepreneurship. Analyzing these knowledge gaps led to a list of identified\nresearch questions. This paper will lay the foundation for identifying relevant\nstudies to form a research roadmap to address the four knowledge gaps\nidentified.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 17:10:12 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zhang", "Cheng", ""], ["Kim", "Jinwoo", ""], ["Jeon", "JungHo", ""], ["Xing", "Jinding", ""], ["Ahn", "Changbum", ""], ["Tang", "Pingbo", ""], ["Cai", "Hubo", ""]]}, {"id": "2107.13509", "submitter": "Upol Ehsan", "authors": "Upol Ehsan, Samir Passi, Q. Vera Liao, Larry Chan, I-Hsiang Lee,\n  Michael Muller, Mark O. Riedl", "title": "The Who in Explainable AI: How AI Background Shapes Perceptions of AI\n  Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability of AI systems is critical for users to take informed actions\nand hold systems accountable. While \"opening the opaque box\" is important,\nunderstanding who opens the box can govern if the Human-AI interaction is\neffective. In this paper, we conduct a mixed-methods study of how two different\ngroups of whos--people with and without a background in AI--perceive different\ntypes of AI explanations. These groups were chosen to look at how disparities\nin AI backgrounds can exacerbate the creator-consumer gap. We quantitatively\nshare what the perceptions are along five dimensions: confidence, intelligence,\nunderstandability, second chance, and friendliness. Qualitatively, we highlight\nhow the AI background influences each group's interpretations and elucidate why\nthe differences might exist through the lenses of appropriation and cognitive\nheuristics. We find that (1) both groups had unwarranted faith in numbers, to\ndifferent extents and for different reasons, (2) each group found explanatory\nvalues in different explanations that went beyond the usage we designed them\nfor, and (3) each group had different requirements of what counts as humanlike\nexplanations. Using our findings, we discuss potential negative consequences\nsuch as harmful manipulation of user trust and propose design interventions to\nmitigate them. By bringing conscious awareness to how and why AI backgrounds\nshape perceptions of potential creators and consumers in XAI, our work takes a\nformative step in advancing a pluralistic Human-centered Explainable AI\ndiscourse.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 17:32:04 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ehsan", "Upol", ""], ["Passi", "Samir", ""], ["Liao", "Q. Vera", ""], ["Chan", "Larry", ""], ["Lee", "I-Hsiang", ""], ["Muller", "Michael", ""], ["Riedl", "Mark O.", ""]]}, {"id": "2107.13519", "submitter": "Jorge Ramirez", "authors": "Jorge Ram\\'irez, Burcu Sayin, Marcos Baez, Fabio Casati, Luca\n  Cernuzzi, Boualem Benatallah, Gianluca Demartini", "title": "On the state of reporting in crowdsourcing experiments and a checklist\n  to aid current practices", "comments": "Accepted to CSCW 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is being increasingly adopted as a platform to run studies with\nhuman subjects. Running a crowdsourcing experiment involves several choices and\nstrategies to successfully port an experimental design into an otherwise\nuncontrolled research environment, e.g., sampling crowd workers, mapping\nexperimental conditions to micro-tasks, or ensure quality contributions. While\nseveral guidelines inform researchers in these choices, guidance of how and\nwhat to report from crowdsourcing experiments has been largely overlooked. If\nunder-reported, implementation choices constitute variability sources that can\naffect the experiment's reproducibility and prevent a fair assessment of\nresearch outcomes. In this paper, we examine the current state of reporting of\ncrowdsourcing experiments and offer guidance to address associated reporting\nissues. We start by identifying sensible implementation choices, relying on\nexisting literature and interviews with experts, to then extensively analyze\nthe reporting of 171 crowdsourcing experiments. Informed by this process, we\npropose a checklist for reporting crowdsourcing experiments.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 17:42:24 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ram\u00edrez", "Jorge", ""], ["Sayin", "Burcu", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""], ["Cernuzzi", "Luca", ""], ["Benatallah", "Boualem", ""], ["Demartini", "Gianluca", ""]]}, {"id": "2107.13738", "submitter": "Nathan Partlan", "authors": "Nathan Partlan, Erica Kleinman, Jim Howe, Sabbir Ahmad, Stacy\n  Marsella, Magy Seif El-Nasr", "title": "Design-Driven Requirements for Computationally Co-Creative Game AI\n  Design Tools", "comments": "12 pages, 1 figure. Accepted for publication in Foundations of\n  Digital Games (FDG) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game AI designers must manage complex interactions between the AI character,\nthe game world, and the player, while achieving their design visions.\nComputational co-creativity tools can aid them, but first, AI and HCI\nresearchers must gather requirements and determine design heuristics to build\neffective co-creative tools. In this work, we present a participatory design\nstudy that categorizes and analyzes game AI designers' workflows, goals, and\nexpectations for such tools. We evince deep connections between game AI design\nand the design of co-creative tools, and present implications for future\nco-creativity tool research and development.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 04:14:53 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Partlan", "Nathan", ""], ["Kleinman", "Erica", ""], ["Howe", "Jim", ""], ["Ahmad", "Sabbir", ""], ["Marsella", "Stacy", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "2107.13760", "submitter": "Yu Cheng Hsu Mr", "authors": "Yu Cheng Hsu, Qingpeng Zhang, Efstratios Tsougenis, Kwok-Leung Tsui", "title": "Viewpoint-Invariant Exercise Repetition Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Counting the repetition of human exercise and physical rehabilitation is a\ncommon task in rehabilitation and exercise training. The existing vision-based\nrepetition counting methods less emphasize the concurrent motions in the same\nvideo. This work presents a vision-based human motion repetition counting\napplicable to counting concurrent motions through the skeleton location\nextracted from various pose estimation methods. The presented method was\nvalidated on the University of Idaho Physical Rehabilitation Movements Data Set\n(UI-PRMD), and MM-fit dataset. The overall mean absolute error (MAE) for mm-fit\nwas 0.06 with off-by-one Accuracy (OBOA) 0.94. Overall MAE for UI-PRMD dataset\nwas 0.06 with OBOA 0.95. We have also tested the performance in a variety of\ncamera locations and concurrent motions with conveniently collected video with\noverall MAE 0.06 and OBOA 0.88. The proposed method provides a view-angle and\nmotion agnostic concurrent motion counting. This method can potentially use in\nlarge-scale remote rehabilitation and exercise training with only one camera.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 06:00:52 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Hsu", "Yu Cheng", ""], ["Zhang", "Qingpeng", ""], ["Tsougenis", "Efstratios", ""], ["Tsui", "Kwok-Leung", ""]]}, {"id": "2107.13811", "submitter": "Staas de Jong", "authors": "Staas de Jong, Jeroen Jillissen, D\\\"unya Kirkali, Alwin de Rooij,\n  Hanna Schraffenberger, Arnout Terpstra", "title": "One-press control: a tactile input method for pressure-sensitive\n  computer keyboards", "comments": "The burden of executing this research project was shared among the\n  authors, with all major choices made together in an open and democratic\n  process. The concepts and algorithms that were decided upon for publication\n  were by the first author, who also had writing duties. Further information is\n  in the Acknowledgments", "journal-ref": "Extended Abstracts of the Conference on Human Factors in Computing\n  Systems (CHI), 2010", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work presents One-press control, a tactile input method for\npressure-sensitive keyboards based on the detection and classification of\npressing movements on the already held-down key. To seamlessly integrate the\nadded control input with existing practices for ordinary computer keyboards,\nthe redefined notion of virtual modifier keys is introduced. A number of\napplication examples are given, especially to point out a potential for\nsimplifying existing interactions by replacing modifier key combinations with\nsingle key presses. Also, a new class of interaction scenarios employing the\ntechnique is proposed, based on an interaction model named \"What You Touch Is\nWhat You Get (WYTIWYG)\". Here, the proposed tactile input method is used to\nnavigate interaction options, get full previews of potential outcomes, and then\neither commit to one or abort altogether - all in the space of one key depress\n/ release cycle. The results of user testing indicate some remaining\nimplementation issues, as well as that the technique can be learned within\nabout a quarter of an hour of hands-on operating practice time.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:19:50 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["de Jong", "Staas", ""], ["Jillissen", "Jeroen", ""], ["Kirkali", "D\u00fcnya", ""], ["de Rooij", "Alwin", ""], ["Schraffenberger", "Hanna", ""], ["Terpstra", "Arnout", ""]]}, {"id": "2107.13817", "submitter": "Staas de Jong", "authors": "Staas de Jong", "title": "Presenting the cyclotactor project", "comments": null, "journal-ref": "Proceedings of the International Conference on Tangible, Embedded,\n  and embodied Interaction (TEI), 2010", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cyclotactor is a novel platform for finger-based tactile interaction\nresearch. The operating principle is to track vertical fingerpad position above\na freely approachable surface aperture, while directly projecting a force on\nthe same fingerpad. The projected force can be specified in Newtons, with high\ntemporal resolution. In combination with a relatively low overall latency\nbetween tactile input and output, this is used to work towards the ideal of\ninstant programmable haptic feedback. This enables support for output across\nthe continuum between static force levels and vibrotactile feedback, targeting\nboth the kinesthetic and cutaneous senses of touch. The current state of the\ntechnology is described, and an overview of the research goals of the\ncyclotactor project is given.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:30:27 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["de Jong", "Staas", ""]]}, {"id": "2107.13825", "submitter": "Staas de Jong", "authors": "Staas de Jong", "title": "Kinetic surface friction rendering for interactive sonification: an\n  initial exploration", "comments": null, "journal-ref": "Proceedings of the International Workshop on Interactive\n  Sonification (ISon), 2010", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inspired by the role sound and friction play in interactions with everyday\nobjects, this work aims to identify some of the ways in which kinetic surface\nfriction rendering can complement interactive sonification controlled by\nmovable objects. In order to do this, a tactile system is presented which\nimplements a movable physical object with programmable friction. Important\naspects of this system include the capacity to display high-resolution kinetic\nfriction patterns, the ability to algorithmically define interactions directly\nin terms of physical units, and the complete integration of audio and tactile\nsynthesis.\n  A prototype interaction spatially mapping arbitrary 1D signal data on a\nsurface and directly converting these to sound and friction during movements\nacross the surface is described. The results of a pilot evaluation of this\ninteraction indicate how kinetic surface friction rendering can be a means for\ngiving dynamically created virtual objects for sonification a tangible\npresence. Some specific possible roles for movement input and friction output\nare identified, as well as issues to be considered when applying and further\ndeveloping this type of haptic feedback in the context of interactive\nsonification.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 08:43:50 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["de Jong", "Staas", ""]]}, {"id": "2107.13847", "submitter": "Zhongyi Zhou", "authors": "Zhongyi Zhou, Anran Xu, Koji Yatani", "title": "SyncUp: Vision-based Practice Support for Synchronized Dancing", "comments": "In Proceedings of the ACM on Interactive, Mobile, Wearable and\n  Ubiquitous Technologies (IMWUT/UbiComp '21), Vol. 5, No. 3, Article 143", "journal-ref": null, "doi": "10.1145/3478120", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The beauty of synchronized dancing lies in the synchronization of body\nmovements among multiple dancers. While dancers utilize camera recordings for\ntheir practice, standard video interfaces do not efficiently support their\nactivities of identifying segments where they are not well synchronized. This\nthus fails to close a tight loop of an iterative practice process (i.e.,\ncapturing a practice, reviewing the video, and practicing again). We present\nSyncUp, a system that provides multiple interactive visualizations to support\nthe practice of synchronized dancing and liberate users from manual inspection\nof recorded practice videos. By analyzing videos uploaded by users, SyncUp\nquantifies two aspects of synchronization in dancing: pose similarity among\nmultiple dancers and temporal alignment of their movements. The system then\nhighlights which body parts and which portions of the dance routine require\nfurther practice to achieve better synchronization. The results of our system\nevaluations show that our pose similarity estimation and temporal alignment\npredictions were correlated well with human ratings. Participants in our\nqualitative user evaluation expressed the benefits and its potential use of\nSyncUp, confirming that it would enable quick iterative practice.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 09:26:59 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhou", "Zhongyi", ""], ["Xu", "Anran", ""], ["Yatani", "Koji", ""]]}, {"id": "2107.13936", "submitter": "Maximilian T. Fischer", "authors": "Maximilian T. Fischer, Alexander Frings, Daniel A. Keim, Daniel\n  Seebacher", "title": "Towards a Survey on Static and Dynamic Hypergraph Visualizations", "comments": "2021 IEEE Visualization Conference (VIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging hypergraph structures to model advanced processes has gained much\nattention over the last few years in many areas, ranging from\nprotein-interaction in computational biology to image retrieval using machine\nlearning. Hypergraph models can provide a more accurate representation of the\nunderlying processes while reducing the overall number of links compared to\nregular representations. However, interactive visualization methods for\nhypergraphs and hypergraph-based models have rarely been explored or\nsystematically analyzed. This paper reviews the existing research landscape for\nhypergraph and hypergraph model visualizations and assesses the currently\nemployed techniques. We provide an overview and a categorization of proposed\napproaches, focusing on performance, scalability, interaction support,\nsuccessful evaluation, and the ability to represent different underlying data\nstructures, including a recent demand for a temporal representation of\ninteraction networks and their improvements beyond graph-based methods. Lastly,\nwe discuss the strengths and weaknesses of the approaches and give an insight\ninto the future challenges arising in this emerging research field.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 12:58:26 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Fischer", "Maximilian T.", ""], ["Frings", "Alexander", ""], ["Keim", "Daniel A.", ""], ["Seebacher", "Daniel", ""]]}, {"id": "2107.14013", "submitter": "Jonathan Roberts PhD", "authors": "Jonathan C. Roberts, Peter Butcher, Ann Sherlock and Sarah Nason", "title": "Explanatory Journeys: Visualising to Understand and Explain\n  Administrative Justice Paths of Redress", "comments": "11 pages with 10 figures, accepted for publication in IEEE\n  Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Administrative justice concerns the relationships between individuals and the\nstate. It includes redress and complaints on decisions of a child's education,\nsocial care, licensing, planning, environment, housing and homelessness.\nHowever, if someone has a complaint or an issue, it is challenging for people\nto understand different possible redress paths and explore what path is\nsuitable for their situation. Explanatory visualisation has the potential to\ndisplay these paths of redress in a clear way, such that people can see,\nunderstand and explore their options. The visualisation challenge is further\ncomplicated because information is spread across many documents, laws, guidance\nand policies and requires judicial interpretation. Consequently, there is not a\nsingle database of paths of redress. In this work we present how we have\nco-designed a system to visualise administrative justice paths of redress.\nSimultaneously, we classify, collate and organise the underpinning data, from\nexpert workshops, heuristic evaluation and expert critical reflection. We make\nfour contributions: (i) an application design study of the explanatory\nvisualisation tool (Artemus), (ii) coordinated and co-design approach to\naggregating the data, (iii) two in-depth case studies in housing and education\ndemonstrating explanatory paths of redress in administrative law, and (iv)\nreflections on the expert co-design process and expert data gathering and\nexplanatory visualisation for administrative justice and law.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 14:25:58 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Roberts", "Jonathan C.", ""], ["Butcher", "Peter", ""], ["Sherlock", "Ann", ""], ["Nason", "Sarah", ""]]}, {"id": "2107.14077", "submitter": "Soaad Hossain Mr", "authors": "Soraia Oueida, Soaad Hossain, Yehia Kotb, Syed Ishtiaque Ahmed", "title": "A Fair and Ethical Healthcare Artificial Intelligence System for\n  Monitoring Driver Behavior and Preventing Road Accidents", "comments": "12 pages, 2 figures, accepted to Future Technologies Conference (FTC\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to prevent transportation accidents and\nmonitor driver's behavior using a healthcare AI system that incorporates\nfairness and ethics. Dangerous medical cases and unusual behavior of the driver\nare detected. Fairness algorithm is approached in order to improve\ndecision-making and address ethical issues such as privacy issues, and to\nconsider challenges that appear in the wild within AI in healthcare and\ndriving. A healthcare professional will be alerted about any unusual activity,\nand the driver's location when necessary, is provided in order to enable the\nhealthcare professional to immediately help to the unstable driver. Therefore,\nusing the healthcare AI system allows for accidents to be predicted and thus\nprevented and lives may be saved based on the built-in AI system inside the\nvehicle which interacts with the ER system.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:23:42 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Oueida", "Soraia", ""], ["Hossain", "Soaad", ""], ["Kotb", "Yehia", ""], ["Ahmed", "Syed Ishtiaque", ""]]}]