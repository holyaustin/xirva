[{"id": "1504.00747", "submitter": "He Jiang", "authors": "He Jiang, Xin Chen, Shuwei Zhang, Xin Zhang, Weiqiang Kong, Tao Zhang", "title": "Software for Wearable Devices: Challenges and Opportunities", "comments": "6 pages, 1 figure, for Compsac 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Wearable devices are a new form of mobile computer system that provides\nexclusive and user-personalized services. Wearable devices bring new issues and\nchallenges to computer science and technology. This paper summarizes the\ndevelopment process and the categories of wearable devices. In addition, we\npresent new key issues arising in aspects of wearable devices, including\noperating systems, database management system, network communication protocol,\napplication development platform, privacy and security, energy consumption,\nhuman-computer interaction, software engineering, and big data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 05:25:07 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Jiang", "He", ""], ["Chen", "Xin", ""], ["Zhang", "Shuwei", ""], ["Zhang", "Xin", ""], ["Kong", "Weiqiang", ""], ["Zhang", "Tao", ""]]}, {"id": "1504.01025", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Liangbing Feng, Shengzhong Feng, Haibo Li", "title": "Preprint Extending Touch-less Interaction on Vision Based Wearable\n  Device", "comments": "This is the preprint version of our paper on IEEE Virtual Reality\n  Conference 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on IEEE Virtual Reality Conference\n2015. A touch-less interaction technology on vision based wearable device is\ndesigned and evaluated. Users interact with the application with dynamic\nhands/feet gestures in front of the camera. Several proof-of-concept prototypes\nwith eleven dynamic gestures are developed based on the touch-less interaction.\nAt last, a comparing user study evaluation is proposed to demonstrate the\nusability of the touch-less approach, as well as the impact on user's emotion,\nrunning on a wearable framework or Google Glass.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 17:12:19 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 09:14:35 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Lv", "Zhihan", ""], ["Feng", "Liangbing", ""], ["Feng", "Shengzhong", ""], ["Li", "Haibo", ""]]}, {"id": "1504.01030", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Chantal Esteve, Javier Chirivella, Pablo Gagliardo", "title": "Preprint A Game Based Assistive Tool for Rehabilitation of Dysphonic\n  Patients", "comments": "This is the preprint version of our paper on 3rd International\n  Workshop on Virtual and Augmented Assistive Technology (VAAT) at IEEE Virtual\n  Reality 2015 (VR2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on 3rd International Workshop on\nVirtual and Augmented Assistive Technology (VAAT) at IEEE Virtual Reality 2015\n(VR2015). An assistive training tool for rehabilitation of dysphonic patients\nis designed and developed according to the practical clinical needs. The\nassistive tool employs a space flight game as the attractive logic part, and\nmicrophone arrays as input device, which is getting rid of ambient noise by\nsetting a specific orientation. The therapist can guide the patient to play the\ngame as well as the voice training simultaneously side by side, while not\ninterfere the patient voice. The voice information can be recorded and\nextracted for evaluating the long-time rehabilitation progress. This paper\noutlines a design science approach for the development of an initial useful\nsoftware prototype of such a tool, considering 'Intuitive', 'Entertainment',\n'Incentive' as main design factors.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 17:43:54 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 08:47:30 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Lv", "Zhihan", ""], ["Esteve", "Chantal", ""], ["Chirivella", "Javier", ""], ["Gagliardo", "Pablo", ""]]}, {"id": "1504.01049", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Tianyun Su, Xiaoming Li, Shengzhong Feng", "title": "3D visual analysis of seabed on smartphone", "comments": "2015 IEEE Pacific Visualization Symposium (PacificVis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We create a 'virtual-seabed' platform to realize the 3D visual analysis of\nseabed on smartphone. The 3D seabed platform is based on a 'section-drilling'\nmodel, implementing visualization and analysis of the integrated data of seabed\non the 3D browser on smartphone. Some 3D visual analysis functions are\ndeveloped. This work presents a thorough and interesting way of presenting\nseabed data on smartphone, which raises many application possibilities. This\nplatform is another practical proof based on our WebVRGIS platform.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 20:34:11 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Lv", "Zhihan", ""], ["Su", "Tianyun", ""], ["Li", "Xiaoming", ""], ["Feng", "Shengzhong", ""]]}, {"id": "1504.01051", "submitter": "Zhihan Lv", "authors": "Xiaoming Li, Zhihan Lv, Baoyun Zhang, Weixi Wang, Shengzhong Feng,\n  Jinxing Hu", "title": "WebVRGIS Based City Bigdata 3D Visualization and Analysis", "comments": "2015 IEEE Pacific Visualization Symposium (PacificVis)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows the WEBVRGIS platform overlying multiple types of data about\nShenzhen over a 3d globe. The amount of information that can be visualized with\nthis platform is overwhelming, and the GIS-based navigational scheme allows to\nhave great flexibility to access the different available data sources. For\nexample,visualising historical and forecasted passenger volume at stations\ncould be very helpful when overlaid with other social data.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 20:44:33 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Li", "Xiaoming", ""], ["Lv", "Zhihan", ""], ["Zhang", "Baoyun", ""], ["Wang", "Weixi", ""], ["Feng", "Shengzhong", ""], ["Hu", "Jinxing", ""]]}, {"id": "1504.01379", "submitter": "Zhihan Lv", "authors": "Zhihan Lv and Xiaoming Li and Baoyun Zhang and Weixi Wang and\n  Shengzhong Feng and Jinxing Hu", "title": "Preprint Big City 3D Visual Analysis", "comments": "This is the preprint version of our paper on EUROGRAPHICS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on EUROGRAPHICS 2015. A big city\nvisual analysis platform based on Web Virtual Reality Geographical Information\nSystem (WEBVRGIS) is presented. Extensive model editing functions and spatial\nanalysis functions are available, including terrain analysis, spatial analysis,\nsunlight analysis, traffic analysis, population analysis and community\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 15:53:09 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 20:57:06 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Lv", "Zhihan", ""], ["Li", "Xiaoming", ""], ["Zhang", "Baoyun", ""], ["Wang", "Weixi", ""], ["Feng", "Shengzhong", ""], ["Hu", "Jinxing", ""]]}, {"id": "1504.01693", "submitter": "Benjamin Holland", "authors": "Benjamin Holland, Tom Deering, Suresh Kothari, Jon Mathews, Nikhil\n  Ranade", "title": "Security Toolbox for Detecting Novel and Sophisticated Android Malware", "comments": "4 pages, 1 listing, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a demo of our Security Toolbox to detect novel malware in\nAndroid apps. This Toolbox is developed through our recent research project\nfunded by the DARPA Automated Program Analysis for Cybersecurity (APAC)\nproject. The adversarial challenge (\"Red\") teams in the DARPA APAC program are\ntasked with designing sophisticated malware to test the bounds of malware\ndetection technology being developed by the research and development (\"Blue\")\nteams. Our research group, a Blue team in the DARPA APAC program, proposed a\n\"human-in-the-loop program analysis\" approach to detect malware given the\nsource or Java bytecode for an Android app. Our malware detection apparatus\nconsists of two components: a general-purpose program analysis platform called\nAtlas, and a Security Toolbox built on the Atlas platform. This paper describes\nthe major design goals, the Toolbox components to achieve the goals, and the\nworkflow for auditing Android apps. The accompanying video\n(http://youtu.be/WhcoAX3HiNU) illustrates features of the Toolbox through a\nlive audit.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2015 18:09:11 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Holland", "Benjamin", ""], ["Deering", "Tom", ""], ["Kothari", "Suresh", ""], ["Mathews", "Jon", ""], ["Ranade", "Nikhil", ""]]}, {"id": "1504.02134", "submitter": "Chris Thomas", "authors": "Christopher Thomas and Brandon Jennings", "title": "Hand Posture's Effect on Touch Screen Text Input Behaviors: A Touch Area\n  Based Study", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices with touch keyboards have become ubiquitous, but text entry on\nthese devices remains slow and errorprone. Understanding touch patterns during\ntext entry could be useful in designing robust error-correction algorithms for\nsoft keyboards. In this paper, we present an analysis of text input behaviors\non a soft QWERTY keyboard in three different text entry postures: index finger\nonly, one thumb, and two thumb. Our work expands on the work of [1] by\nconsidering the entire surface area of digit contact with the smartphone\nkeyboard, rather than interpreting each touch as a single point. To do this, we\ncaptured touch areas for every key in a lab study with 8 participants and\ncalculated offsets, error rates, and size measurements. We then repeated the\noriginal experiment described in [1] and showed that significant differences\nexist when basing offset calculations on touch area compared to touch points\nfor two postures.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 21:39:56 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Thomas", "Christopher", ""], ["Jennings", "Brandon", ""]]}, {"id": "1504.02218", "submitter": "Sabrina Nusrat", "authors": "Sabrina Nusrat, Md. Jawaherul Alam, Stephen G. Kobourov", "title": "Evaluating Cartogram Effectiveness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartograms are maps in which areas of geographic regions (countries, states)\nappear in proportion to some variable of interest (population, income).\nCartograms are popular visualizations for geo-referenced data that have been\nused for over a century and that make it possible to gain insight into patterns\nand trends in the world around us. Despite the popularity of cartograms and the\nlarge number of cartogram types, there are few studies evaluating the\neffectiveness of cartograms in conveying information. Based on a recent task\ntaxonomy for cartograms, we evaluate four major different types of cartograms:\ncontiguous, non-contiguous, rectangular, and Dorling cartograms. Specifically,\nwe evaluate the effectiveness of these cartograms by quantitative performance\nanalysis, as well as by subjective preferences. We analyze the results of our\nstudy in the context of some prevailing assumptions in the literature of\ncartography and cognitive science. Finally, we make recommendations for the use\nof different types of cartograms for different tasks and settings.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 07:58:01 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2017 20:27:53 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Nusrat", "Sabrina", ""], ["Alam", "Md. Jawaherul", ""], ["Kobourov", "Stephen G.", ""]]}, {"id": "1504.02305", "submitter": "Haewoon Kwak", "authors": "Haewoon Kwak and Jeremy Blackburn and Seungyeop Han", "title": "Exploring Cyberbullying and Other Toxic Behavior in Team Competition\n  Online Games", "comments": "CHI'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.MM cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore cyberbullying and other toxic behavior in team\ncompetition online games. Using a dataset of over 10 million player reports on\n1.46 million toxic players along with corresponding crowdsourced decisions, we\ntest several hypotheses drawn from theories explaining toxic behavior. Besides\nproviding large-scale, empirical based understanding of toxic behavior, our\nwork can be used as a basis for building systems to detect, prevent, and\ncounter-act toxic behavior.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 13:33:58 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Kwak", "Haewoon", ""], ["Blackburn", "Jeremy", ""], ["Han", "Seungyeop", ""]]}, {"id": "1504.02356", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Eva Mohedano, Amaia Salvador, Sergi Porta, Xavier Gir\\'o-i-Nieto,\n  Graham Healy, Kevin McGuinness, Noel O'Connor and Alan F. Smeaton", "title": "Exploring EEG for Object Detection and Retrieval", "comments": "This preprint is the full version of a short paper accepted in the\n  ACM International Conference on Multimedia Retrieval (ICMR) 2015 (Shanghai,\n  China)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the potential for using Brain Computer Interfaces (BCI)\nas a relevance feedback mechanism in content-based image retrieval. We\ninvestigate if it is possible to capture useful EEG signals to detect if\nrelevant objects are present in a dataset of realistic and complex images. We\nperform several experiments using a rapid serial visual presentation (RSVP) of\nimages at different rates (5Hz and 10Hz) on 8 users with different degrees of\nfamiliarization with BCI and the dataset. We then use the feedback from the BCI\nand mouse-based interfaces to retrieve localized objects in a subset of TRECVid\nimages. We show that it is indeed possible to detect such objects in complex\nimages and, also, that users with previous knowledge on the dataset or\nexperience with the RSVP outperform others. When the users have limited time to\nannotate the images (100 seconds in our experiments) both interfaces are\ncomparable in performance. Comparing our best users in a retrieval task, we\nfound that EEG-based relevance feedback outperforms mouse-based feedback. The\nrealistic and complex image dataset differentiates our work from previous\nstudies on EEG for image retrieval.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 15:43:52 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Mohedano", "Eva", ""], ["Salvador", "Amaia", ""], ["Porta", "Sergi", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Healy", "Graham", ""], ["McGuinness", "Kevin", ""], ["O'Connor", "Noel", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "1504.02358", "submitter": "Alessandro Provetti", "authors": "Carlo Bernava, Giacomo Fiumara, Dario Maggiorini, Alessandro Provetti,\n  Laura Ripamonti", "title": "RDF annotation of Second Life objects: Knowledge Representation meets\n  Social Virtual reality", "comments": "The final publication is available at link.springer.com", "journal-ref": "Computational and Mathematical Organization Theory (2014) Vol. 20,\n  pages 20-35. ISSN 1381-298X", "doi": "10.1007/s10588-012-9148-4", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have designed and implemented an application running inside Second Life\nthat supports user annotation of graphical objects and graphical visualization\nof concept ontologies, thus providing a formal, machine-accessible description\nof objects. As a result, we offer a platform that combines the graphical\nknowledge representation that is expected from a MUVE artifact with the\nsemantic structure given by the Resource Framework Description (RDF)\nrepresentation of information.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 15:52:59 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Bernava", "Carlo", ""], ["Fiumara", "Giacomo", ""], ["Maggiorini", "Dario", ""], ["Provetti", "Alessandro", ""], ["Ripamonti", "Laura", ""]]}, {"id": "1504.03370", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Chantal Esteve, Javier Chirivella, Pablo Gagliardo", "title": "Preprint Serious Game Based Dysphonic Rehabilitation Tool", "comments": "This is the preprint version of our paper on 2015 International\n  Conference on Virtual Rehabilitation (ICVR2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on 2015 International Conference on\nVirtual Rehabilitation (ICVR2015). The purpose of this work is designing and\nimplementing a rehabilitation software for dysphonic patients. Constant\ntraining is a key factor for this type of therapy. The patient can play the\ngame as well as conduct the voice training simultaneously guided by therapists\nat clinic or exercise independently at home. The voice information can be\nrecorded and extracted for evaluating the long-time rehabilitation progress.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 21:46:32 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 08:40:22 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Lv", "Zhihan", ""], ["Esteve", "Chantal", ""], ["Chirivella", "Javier", ""], ["Gagliardo", "Pablo", ""]]}, {"id": "1504.03371", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Haibo Li", "title": "Preprint Imagining In-Air Interaction for Hemiplegia Sufferer", "comments": "This is the preprint version of our paper on 2015 International\n  Conference on Virtual Rehabilitation (ICVR2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on 2015 International Conference on\nVirtual Rehabilitation (ICVR2015). In this paper, we described the imagination\nscenarios of a touch-less interaction technology for hemiplegia, which can\nsupport either hand or foot interaction with the smartphone or head mounted\ndevice (HMD). The computer vision interaction technology is implemented in our\nprevious work, which provides a core support for gesture interaction by\naccurately detecting and tracking the hand or foot gesture. The patients\ninteract with the application using hand/foot gesture motion in the camera\nview.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 21:52:07 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 08:37:31 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Lv", "Zhihan", ""], ["Li", "Haibo", ""]]}, {"id": "1504.03425", "submitter": "Iftekhar Naim", "authors": "Iftekhar Naim, M. Iftekhar Tanveer, Daniel Gildea, Mohammed (Ehsan)\n  Hoque", "title": "Automated Analysis and Prediction of Job Interview Performance", "comments": "14 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational framework for automatically quantifying verbal and\nnonverbal behaviors in the context of job interviews. The proposed framework is\ntrained by analyzing the videos of 138 interview sessions with 69\ninternship-seeking undergraduates at the Massachusetts Institute of Technology\n(MIT). Our automated analysis includes facial expressions (e.g., smiles, head\ngestures, facial tracking points), language (e.g., word counts, topic\nmodeling), and prosodic information (e.g., pitch, intonation, and pauses) of\nthe interviewees. The ground truth labels are derived by taking a weighted\naverage over the ratings of 9 independent judges. Our framework can\nautomatically predict the ratings for interview traits such as excitement,\nfriendliness, and engagement with correlation coefficients of 0.75 or higher,\nand can quantify the relative importance of prosody, language, and facial\nexpressions. By analyzing the relative feature weights learned by the\nregression models, our framework recommends to speak more fluently, use less\nfiller words, speak as \"we\" (vs. \"I\"), use more unique words, and smile more.\nWe also find that the students who were rated highly while answering the first\ninterview question were also rated highly overall (i.e., first impression\nmatters). Finally, our MIT Interview dataset will be made available to other\nresearchers to further validate and expand our findings.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 05:49:26 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Naim", "Iftekhar", "", "Ehsan"], ["Tanveer", "M. Iftekhar", "", "Ehsan"], ["Gildea", "Daniel", "", "Ehsan"], ["Mohammed", "", "", "Ehsan"], ["Hoque", "", ""]]}, {"id": "1504.03731", "submitter": "Vincenzo De Florio", "authors": "Vincenzo De Florio and Chris Blondia", "title": "Safety enhancement through situation-aware user interfaces", "comments": "Published in the Proc. of the 7th Int.l IET System Safety Conference,\n  Edinburgh, UK, 15-18 October 2012. IET", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their privileged position halfway between the physical and the cyber\nuniverses, user interfaces may play an important role in preventing,\ntolerating, and learning from scenarios potentially affecting mission safety\nand the user's quality of experience. This vision is embodied here in the main\nideas and a proof-of-concepts implementation of user interfaces that combine\ndynamic profiling with context- and situation-awareness and autonomic software\nadaptation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 21:36:53 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["De Florio", "Vincenzo", ""], ["Blondia", "Chris", ""]]}, {"id": "1504.04309", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Chantal Esteve, Javier Chirivella, Pablo Gagliardo", "title": "Preprint Clinical Feedback and Technology Selection of Game Based\n  Dysphonic Rehabilitation Tool", "comments": "This is the preprint version of our paper on 2015 9th International\n  Conference on Pervasive Computing Technologies for Healthcare\n  (PervasiveHealth2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on 2015 9th International\nConference on Pervasive Computing Technologies for Healthcare\n(PervasiveHealth2015). An assistive training tool software for rehabilitation\nof dysphonic patients is evaluated according to the practical clinical feedback\nfrom the treatments. One stroke sufferer and one parkinson sufferer have\nprovided earnest suggestions for the improvement of our tool software. The\nassistive tool employs a serious game as the attractive logic part, and running\non the tablet with normal microphone as input device. Seven pitch estimation\nalgorithms have been evaluated and compared with selected patients voice\ndatabase. A series of benchmarks have been generated during the evaluation\nprocess for technology selection.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 17:12:48 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 08:35:27 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Lv", "Zhihan", ""], ["Esteve", "Chantal", ""], ["Chirivella", "Javier", ""], ["Gagliardo", "Pablo", ""]]}, {"id": "1504.05163", "submitter": "Walter Quattrociocchi", "authors": "Alessandro Bessi, Fabiana Zollo, Michela Del Vicario, Antonio Scala,\n  Guido Caldarelli, Walter Quattrociocchi", "title": "Trend of Narratives in the Age of Misinformation", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0134641", "report-no": null, "categories": "cs.SI cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media enabled a direct path from producer to consumer of contents\nchanging the way users get informed, debate, and shape their worldviews. Such a\n{\\em disintermediation} weakened consensus on social relevant issues in favor\nof rumors, mistrust, and fomented conspiracy thinking -- e.g., chem-trails\ninducing global warming, the link between vaccines and autism, or the New World\nOrder conspiracy.\n  In this work, we study through a thorough quantitative analysis how different\nconspiracy topics are consumed in the Italian Facebook. By means of a\nsemi-automatic topic extraction strategy, we show that the most discussed\ncontents semantically refer to four specific categories: {\\em environment},\n{\\em diet}, {\\em health}, and {\\em geopolitics}. We find similar patterns by\ncomparing users activity (likes and comments) on posts belonging to different\nsemantic categories. However, if we focus on the lifetime -- i.e., the distance\nin time between the first and the last comment for each user -- we notice a\nremarkable difference within narratives -- e.g., users polarized on geopolitics\nare more persistent in commenting, whereas the less persistent are those\nfocused on diet related topics. Finally, we model users mobility across various\ntopics finding that the more a user is active, the more he is likely to join\nall topics. Once inside a conspiracy narrative users tend to embrace the\noverall corpus.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 19:19:10 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Bessi", "Alessandro", ""], ["Zollo", "Fabiana", ""], ["Del Vicario", "Michela", ""], ["Scala", "Antonio", ""], ["Caldarelli", "Guido", ""], ["Quattrociocchi", "Walter", ""]]}, {"id": "1504.05694", "submitter": "Serge Egelman", "authors": "Linda Lee, Serge Egelman, Joong Hwa Lee, David Wagner", "title": "Risk Perceptions for Wearable Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable devices, or \"wearables,\" bring great benefits but also potential\nrisks that could expose users' activities with- out their awareness or consent.\nIn this paper, we report findings from the first large-scale survey conducted\nto investigate user security and privacy concerns regarding wearables. We\nsurveyed 1,782 Internet users in order to identify risks that are particularly\nconcerning to them; these risks are inspired by the sensor inputs and\napplications of popular wearable technologies. During this experiment, our\nquestions controlled for the effects of what data was being accessed and with\nwhom it was being shared. We also investigated how these emergent threats\ncompared to existent mobile threats, how upcoming capabilities and artifacts\ncompared to existing technologies, and how users ranked technical and\nnontechnical concerns to sketch a concrete and broad view of the wearable\ndevice landscape. We hope that this work will inform the design of future user\nnotification, permission management, and access control schemes for wearables.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 08:44:23 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Lee", "Linda", ""], ["Egelman", "Serge", ""], ["Lee", "Joong Hwa", ""], ["Wagner", "David", ""]]}, {"id": "1504.06359", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Alaa Halawani, Shengzhong Feng, Shafiq ur Rehman, Haibo Li", "title": "Preprint Touch-less Interactive Augmented Reality Game on Vision Based\n  Wearable Device", "comments": "This is the preprint version of our paper on Personal and Ubiquitous\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on Personal and Ubiquitous\nComputing. There is an increasing interest in creating pervasive games based on\nemerging interaction technologies. In order to develop touch-less, interactive\nand augmented reality games on vision-based wearable device, a touch-less\nmotion interaction technology is designed and evaluated in this work. Users\ninteract with the augmented reality games with dynamic hands/feet gestures in\nfront of the camera, which triggers the interaction event to interact with the\nvirtual object in the scene. Three primitive augmented reality games with\neleven dynamic gestures are developed based on the proposed touch-less\ninteraction technology as proof. At last, a comparing evaluation is proposed to\ndemonstrate the social acceptability and usability of the touch-less approach,\nrunning on a hybrid wearable framework or with Google Glass, as well as\nworkload assessment, user's emotions and satisfaction.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 22:55:08 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 20:04:13 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2015 08:27:44 GMT"}, {"version": "v4", "created": "Thu, 13 Aug 2015 20:27:54 GMT"}, {"version": "v5", "created": "Mon, 7 Sep 2015 15:48:19 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Lv", "Zhihan", ""], ["Halawani", "Alaa", ""], ["Feng", "Shengzhong", ""], ["Rehman", "Shafiq ur", ""], ["Li", "Haibo", ""]]}, {"id": "1504.06966", "submitter": "Daniel Hienert", "authors": "Daniel Hienert, Dennis Wegener, Siegfried Schomisch", "title": "Making sense of Open Data Statistics with Information from Wikipedia", "comments": "In Availability, reliability, and security in information systems and\n  HCI : IFIP 8.4, 8.9, TC 5 International Cross-Domain Conference, CD-ARES\n  2013, Regensburg, Germany, September 2-6, 2013 ; proceedings, edited by\n  Alfredo Cuzzocrea, Christian Kittl, Dimitris E. Simos, Edgar Weippl, and Lida\n  Xu, Lecture Notes in Computer Science 8127, 329-344", "journal-ref": null, "doi": "10.1007/978-3-642-40511-2_23", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, more and more open data statistics are published by governments,\nstatistical offices and organizations like the United Nations, The World Bank\nor Eurostat. This data is freely available and can be consumed by end users in\ninteractive visualizations. However, additional information is needed to enable\nlaymen to interpret these statistics in order to make sense of the raw data. In\nthis paper, we present an approach to combine open data statistics with\nhistorical events. In a user interface we have integrated interactive\nvisualizations of open data statistics with a timeline of thematically\nappropriate historical events from Wikipedia. This can help users to explore\nstatistical data in several views and to get related events for certain trends\nin the timeline. Events include links to Wikipedia articles, where details can\nbe found and the search process can be continued. We have conducted a user\nstudy to evaluate if users can use the interface intuitively, if relations\nbetween trends in statistics and historical events can be found and if users\nlike this approach for their exploration process.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 08:17:32 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Hienert", "Daniel", ""], ["Wegener", "Dennis", ""], ["Schomisch", "Siegfried", ""]]}, {"id": "1504.07844", "submitter": "Stefan Gladisch", "authors": "Stefan Gladisch, Ulrike Kister, Christian Tominski, Raimund Dachselt,\n  Heidrun Schumann", "title": "Mapping Tasks to Interactions for Graph Exploration and Graph Editing on\n  Interactive Surfaces", "comments": "21 pages, minor corrections (typos etc.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph exploration and editing are still mostly considered independently and\nsystems to work with are not designed for todays interactive surfaces like\nsmartphones, tablets or tabletops. When developing a system for those modern\ndevices that supports both graph exploration and graph editing, it is necessary\nto 1) identify what basic tasks need to be supported, 2) what interactions can\nbe used, and 3) how to map these tasks and interactions. This technical report\nprovides a list of basic interaction tasks for graph exploration and editing as\na result of an extensive system review. Moreover, different interaction\nmodalities of interactive surfaces are reviewed according to their interaction\nvocabulary and further degrees of freedom that can be used to make interactions\ndistinguishable are discussed. Beyond the scope of graph exploration and\nediting, we provide an approach for finding and evaluating a mapping from tasks\nto interactions, that is generally applicable. Thus, this work acts as a\nguideline for developing a system for graph exploration and editing that is\nspecifically designed for interactive surfaces.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 13:06:50 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 08:48:02 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Gladisch", "Stefan", ""], ["Kister", "Ulrike", ""], ["Tominski", "Christian", ""], ["Dachselt", "Raimund", ""], ["Schumann", "Heidrun", ""]]}, {"id": "1504.08145", "submitter": "David Sousa-Rodrigues", "authors": "David Sousa-Rodrigues, Mafalda Teixeira de Sampayo, Eug\\'enio\n  Rodrigues, Ad\\'elio Rodrigues Gaspar, \\'Alvaro Gomes, Carlos Henggeler\n  Antunes", "title": "Online survey for collective clustering of computer generated\n  architectural floor plans", "comments": "Extended abstract accepted for ICTPI'15 conference, June 17-19,\n  Milton Keynes, United Kingdom - http://www.ictpi15.info/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study is to understand what are the collective actions of\narchitecture practitioners when grouping floor plan designs. The understanding\nof how professionals and students solve this complex problem may help to\ndevelop specific programmes for the teaching of architecture. In addition, the\nfindings of this study can help in the development of query mechanisms for\ndatabase retrieval of floor plans and the implementation of clustering\nmechanisms to aggregate floor plans resulting from generative design methods.\nThe study aims to capture how practitioners define similarity between floor\nplans from a pool of available designs. A hybrid evolutionary strategy is used,\nwhich takes into account the building's functional program to generate\nalternative floor plan designs. The first step of this methodology consisted in\nan online survey to gather information on how the respondents would perform a\nclustering task. Online surveys have been used in several applications and are\na method of data collection that conveys several advantages. When properly\ndeveloped and implemented, a survey portrays the characteristics of large\ngroups of respondents on a specific topic and allows assessing its\nrepresentation. Several types of surveys are available; e.g. questionnaire and\ninterview formats, phone survey, and online surveys, which can be coupled with\ninference engines that act and direct the survey according to respondents'\nanswers. In the present study, the survey was posed as an online exercise in\nwhich respondents had to perform a pre-defined task, which makes it similar to\nrunning an experiment in an online environment. The experiment aimed to\nunderstand the perception and criteria of the target population to perform the\nclustering task by comparing the results with the respondents' answers to a\nquestionnaire presented at the end of the exercise.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 09:44:38 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Sousa-Rodrigues", "David", ""], ["de Sampayo", "Mafalda Teixeira", ""], ["Rodrigues", "Eug\u00e9nio", ""], ["Gaspar", "Ad\u00e9lio Rodrigues", ""], ["Gomes", "\u00c1lvaro", ""], ["Antunes", "Carlos Henggeler", ""]]}]