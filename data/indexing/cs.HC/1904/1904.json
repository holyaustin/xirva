[{"id": "1904.00107", "submitter": "Irawan Nurhas", "authors": "Irawan Nurhas, Jan Pawlowski, Stefan Geisler, Maria Kovtunenko, Bayu\n  Rima Aditya", "title": "Group-centered framework towards a positive design of digital\n  collaboration in global settings", "comments": "6 Pages, 3 Figures, Positive computing, International Conference on\n  Industrial Enterprise and System Engineering", "journal-ref": "Atlantis Highlights in Engineering, 2019", "doi": "10.2991/icoiese-18.2019.39", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Globally distributed groups require collaborative systems to support their\nwork. Besides being able to support the teamwork, these systems also should\npromote well-being and maximize the human potential that leads to an engaging\nsystem and joyful experience. Designing such system is a significant challenge\nand requires a thorough understanding of group work. We used the field theory\nas a lens to view the essential aspects of group motivation and then utilized\ncollaboration personas to analyze the elements of group work. We integrated\nwell-being determinants as engagement factors to develop a group-centered\nframework for digital collaboration in a global setting. Based on the outcomes,\nwe proposed a conceptual framework to design an engaging collaborative system\nand recommend system values that can be used to evaluate the system further\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 22:40:21 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 07:46:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Nurhas", "Irawan", ""], ["Pawlowski", "Jan", ""], ["Geisler", "Stefan", ""], ["Kovtunenko", "Maria", ""], ["Aditya", "Bayu Rima", ""]]}, {"id": "1904.00472", "submitter": "Irawan Nurhas", "authors": "Irawan Nurhas, Stefan Geisler, Jan Pawlowski", "title": "Positive Personas: Integrating Well-being Determinants into Personas", "comments": "4 pages, conference MuC 2017, Regensburg", "journal-ref": "Mensch und Computer 2017-Tagungsband: Spielend einfach\n  interagieren", "doi": "10.18420/muc2017-mci-0356", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  System design for well-being needs an appropriate tool to help designers to\ndetermine relevant requirements that can help human well-being to flourish.\nPersonas come as a simple yet powerful tool in the early development stage of\nthe user interface design. Considering well-being determinants in the early\ndesign process provide benefits for both the user and the development team.\nTherefore, in this short paper, we performed a literature study to provide a\nconceptual model of well-being in personas and propose positive design\ninterventions in the personas creation process.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2019 20:11:22 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 07:43:57 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Nurhas", "Irawan", ""], ["Geisler", "Stefan", ""], ["Pawlowski", "Jan", ""]]}, {"id": "1904.00609", "submitter": "Gregoire Cattan", "authors": "Gr\\'egoire Cattan (GIPSA-Services, IHMTEK), Pedro C. Rodrigues\n  (GIPSA-Services), Marco Congedo (GIPSA-Services)", "title": "Passive Head-Mounted Display Music-Listening EEG dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the experimental procedures for a dataset that we have made\npublicly available at https://doi.org/10.5281/zenodo.2617084 in mat (Mathworks,\nNatick, USA) and csv formats. This dataset contains electroencephalographic\nrecordings of 12 subjects listening to music with and without a passive\nhead-mounted display, that is, a head-mounted display which does not include\nany electronics at the exception of a smartphone. The electroencephalographic\nheadset consisted of 16 electrodes. Data were recorded during a pilot\nexperiment taking place in the GIPSA-lab, Grenoble, France, in 2017 (Cattan and\nal, 2018). Python code for manipulating the data is available at\nhttps://github.com/plcrodrigues/py.PHMDML.EEG.2017-GIPSA. The ID of this\ndataset is PHMDML.EEG.2017-GIPSA.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:42:11 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 07:27:57 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Cattan", "Gr\u00e9goire", "", "GIPSA-Services, IHMTEK"], ["Rodrigues", "Pedro C.", "", "GIPSA-Services"], ["Congedo", "Marco", "", "GIPSA-Services"]]}, {"id": "1904.00672", "submitter": "Leyang Xue", "authors": "Leyang Xue, Peng Zhang, An Zeng", "title": "Enhancing the long-term performance of recommender system", "comments": "16 pages, 10 figures", "journal-ref": "Physica A. 531.2019.121731", "doi": "10.1016/j.physa.2019.121731", "report-no": "0378-4371", "categories": "physics.soc-ph cs.HC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender system is a critically important tool in online commercial system\nand provide users with personalized recommendation on items. So far, numerous\nrecommendation algorithms have been made to further improve the recommendation\nperformance in a single-step recommendation, while the long-term recommendation\nperformance is neglected. In this paper, we proposed an approach called\nAdjustment of Recommendation List (ARL) to enhance the long-term recommendation\naccuracy. In order to observe the long-term accuracy, we developed an evolution\nmodel of network to simulate the interaction between the recommender system and\nuser's behaviour. The result shows that not only long-term recommendation\naccuracy can be enhanced significantly but the diversity of item in online\nsystem maintains healthy. Notably, an optimal parameter n* of ARL existed in\nlong-term recommendation, indicating that there is a trade-off between keeping\ndiversity of item and user's preference to maximize the long-term\nrecommendation accuracy. Finally, we confirmed that the optimal parameter n* is\nstable during evolving network, which reveals the robustness of ARL method.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 09:55:16 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Xue", "Leyang", ""], ["Zhang", "Peng", ""], ["Zeng", "An", ""]]}, {"id": "1904.01006", "submitter": "EPTCS", "authors": "Maximilian Dor\\'e (Ludwig Maximilian University Munich), Krysia Broda\n  (Imperial College London)", "title": "Towards Intuitive Reasoning in Axiomatic Geometry", "comments": "In Proceedings ThEdu'18, arXiv:1903.12402", "journal-ref": "EPTCS 290, 2019, pp. 38-55", "doi": "10.4204/EPTCS.290.4", "report-no": null, "categories": "cs.LO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proving lemmas in synthetic geometry is often a time-consuming endeavour\nsince many intermediate lemmas need to be proven before interesting results can\nbe obtained. Improvements in automated theorem provers (ATP) in recent years\nnow mean they can prove many of these intermediate lemmas. The interactive\ntheorem prover Elfe accepts mathematical texts written in fair English and\nverifies them with the help of ATP. Geometrical texts can thereby easily be\nformalized in Elfe, leaving only the cornerstones of a proof to be derived by\nthe user. This allows for teaching axiomatic geometry to students without prior\nexperience in formalized mathematics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:53:27 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Dor\u00e9", "Maximilian", "", "Ludwig Maximilian University Munich"], ["Broda", "Krysia", "", "Imperial College London"]]}, {"id": "1904.01121", "submitter": "Sharon Zhou", "authors": "Sharon Zhou, Mitchell L. Gordon, Ranjay Krishna, Austin Narcomey, Li\n  Fei-Fei, Michael S. Bernstein", "title": "HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative\n  Models", "comments": "https://hype.stanford.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models often use human evaluations to measure the perceived\nquality of their outputs. Automated metrics are noisy indirect proxies, because\nthey rely on heuristics or pretrained embeddings. However, up until now, direct\nhuman evaluation strategies have been ad-hoc, neither standardized nor\nvalidated. Our work establishes a gold standard human benchmark for generative\nrealism. We construct Human eYe Perceptual Evaluation (HYPE) a human benchmark\nthat is (1) grounded in psychophysics research in perception, (2) reliable\nacross different sets of randomly sampled outputs from a model, (3) able to\nproduce separable model performances, and (4) efficient in cost and time. We\nintroduce two variants: one that measures visual perception under adaptive time\nconstraints to determine the threshold at which a model's outputs appear real\n(e.g. 250ms), and the other a less expensive variant that measures human error\nrate on fake and real images sans time constraints. We test HYPE across six\nstate-of-the-art generative adversarial networks and two sampling techniques on\nconditional and unconditional image generation using four datasets: CelebA,\nFFHQ, CIFAR-10, and ImageNet. We find that HYPE can track model improvements\nacross training epochs, and we confirm via bootstrap sampling that HYPE\nrankings are consistent and replicable.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 21:48:41 GMT"}, {"version": "v2", "created": "Wed, 24 Apr 2019 03:58:24 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 05:35:31 GMT"}, {"version": "v4", "created": "Thu, 31 Oct 2019 23:43:11 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Zhou", "Sharon", ""], ["Gordon", "Mitchell L.", ""], ["Krishna", "Ranjay", ""], ["Narcomey", "Austin", ""], ["Fei-Fei", "Li", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1904.01629", "submitter": "Felix Hamza-Lup", "authors": "Jonathan Norman, Felix G. Hamza-Lup", "title": "Challenges in the Deployment of Visuo-Haptic Virtual Environments on the\n  Internet", "comments": null, "journal-ref": "Computer and Network Technology, 2010, pp. 33-37", "doi": "10.1109/ICCNT.2010.88", "report-no": null, "categories": "cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Haptic sensory feedback has been shown to complement the visual and auditory\nsenses, improve user performance and provide a greater sense of togetherness in\ncollaborative and interactive virtual environments. However, we are faced with\nnumerous challenges when deploying these systems over the present day Internet.\nThe most significant of these challenges are the network performance\nlimitations of the Wide Area Networks. In this paper, we offer a structured\nexamination of the current challenges in the deployment of haptic-based\ndistributed systems by analyzing the recent advances in the understanding of\nthese challenges and the progress that has been made to overcome them.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 04:27:33 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Norman", "Jonathan", ""], ["Hamza-Lup", "Felix G.", ""]]}, {"id": "1904.01664", "submitter": "Katherine Metcalf", "authors": "Katherine Metcalf, Barry-John Theobald, Garrett Weinberg, Robert Lee,\n  Ing-Marie Jonsson, Russ Webb, Nicholas Apostoloff", "title": "Mirroring to Build Trust in Digital Assistants", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe experiments towards building a conversational digital assistant\nthat considers the preferred conversational style of the user. In particular,\nthese experiments are designed to measure whether users prefer and trust an\nassistant whose conversational style matches their own. To this end we\nconducted a user study where subjects interacted with a digital assistant that\nresponded in a way that either matched their conversational style, or did not.\nUsing self-reported personality attributes and subjects' feedback on the\ninteractions, we built models that can reliably predict a user's preferred\nconversational style.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 20:51:27 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Metcalf", "Katherine", ""], ["Theobald", "Barry-John", ""], ["Weinberg", "Garrett", ""], ["Lee", "Robert", ""], ["Jonsson", "Ing-Marie", ""], ["Webb", "Russ", ""], ["Apostoloff", "Nicholas", ""]]}, {"id": "1904.01672", "submitter": "Brent Hecht", "authors": "J. Sch\\\"oning, B. Hecht, M. Raubal, A. Kr\\\"uger, M. Marsh, and M. Rohs", "title": "Improving Interaction with Virtual Globes through Spatial Thinking:\n  Helping Users Ask \"Why?\"", "comments": "Proceedings of the International Conference on Intelligent User\n  Interfaces (IUI 2008)", "journal-ref": null, "doi": "10.1145/1378773.1378790", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual globes have progressed from little-known technology to broadly\npopular software in a mere few years. We investigated this phenomenon through a\nsurvey and discovered that, while virtual globes are en vogue, their use is\nrestricted to a small set of tasks so simple that they do not involve any\nspatial thinking. Spatial thinking requires that users ask \"what is where\" and\n\"why\"; the most common virtual globe tasks only include the \"what\". Based on\nthe results of this survey, we have developed a multi-touch virtual globe\nderived from an adapted virtual globe paradigm designed to widen the potential\nuses of the technology by helping its users to inquire about both the \"what is\nwhere\" and \"why\" of spatial distribution. We do not seek to provide users with\nfull GIS (geographic information system) functionality, but rather we aim to\nfacilitate the asking and answering of simple \"why\" questions about general\ntopics that appeal to a wide virtual globe user base.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:29:33 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Sch\u00f6ning", "J.", ""], ["Hecht", "B.", ""], ["Raubal", "M.", ""], ["Kr\u00fcger", "A.", ""], ["Marsh", "M.", ""], ["Rohs", "M.", ""]]}, {"id": "1904.01673", "submitter": "Brent Hecht", "authors": "M. Soll, P. Naumann, J. Sch\\\"oning, P. Samsonov, and B. Hecht", "title": "Helping Computers Understand Geographically-Bound Activity Restrictions", "comments": null, "journal-ref": "Proceedings of the ACM Conference on Human Factors in Computing\n  Systems (CHI 2016)", "doi": "10.1145/2858036.2858053", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of certain types of geographic data prevents the development of\nlocation-aware technologies in a number of important domains. One such type of\n\"unmapped\" geographic data is space usage rules (SURs), which are defined as\ngeographically-bound activity restrictions (e.g. \"no dogs\", \"no smoking\", \"no\nfishing\", \"no skateboarding\"). Researchers in the area of human-computer\ninteraction have recently begun to develop techniques for the automated mapping\nof SURs with the aim of supporting activity planning systems (e.g. one-touch\n\"Can I Smoke Here?\" apps, SUR-aware vacation planning tools). In this paper, we\npresent a novel SUR mapping technique - SPtP - that outperforms\nstate-of-the-art approaches by 30% for one of the most important components of\nthe SUR mapping pipeline: associating a point observation of a SUR (e.g. a 'no\nsmoking' sign) with the corresponding polygon in which the SUR applies (e.g.\nthe nearby park or the entire campus on which the sign is located). This paper\nalso contributes a series of new SUR benchmark datasets to help further\nresearch in this area.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 21:30:41 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Soll", "M.", ""], ["Naumann", "P.", ""], ["Sch\u00f6ning", "J.", ""], ["Samsonov", "P.", ""], ["Hecht", "B.", ""]]}, {"id": "1904.01687", "submitter": "Jason R.C. Nurse Dr", "authors": "Helena Webb and Jason R.C. Nurse and Louise Bezuidenhout and Marina\n  Jirotka", "title": "Lab Hackathons to Overcome Laboratory Equipment Shortages in Africa:\n  Opportunities and Challenges", "comments": "2019 CHI Conference on Human Factors in Computing Systems Extended\n  Abstracts", "journal-ref": null, "doi": "10.1145/3290607.3299063", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equipment shortages in Africa undermine Science, Technology, Engineering and\nMathematics (STEM) Education. We have pioneered the LabHackathon (LabHack): a\nnovel initiative that adapts the conventional hackathon and draws on insights\nfrom the Open Hardware movement and Responsible Research and Innovation (RRI).\nLabHacks are fun, educational events that challenge student participants to\nbuild frugal and reproducible pieces of laboratory equipment. Completed designs\nare then made available to others. LabHacks can therefore facilitate the open\nand sustainable design of laboratory equipment, in situ, in Africa. In this\ncase study we describe the LabHackathon model, discuss its application in a\npilot event held in Zimbabwe and outline the opportunities and challenges it\npresents.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:15:57 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Webb", "Helena", ""], ["Nurse", "Jason R. C.", ""], ["Bezuidenhout", "Louise", ""], ["Jirotka", "Marina", ""]]}, {"id": "1904.01688", "submitter": "Brent Hecht", "authors": "H. Li, B. Alarcon, S.M. Espinosa, B and Hecht", "title": "Out of Site: Empowering a New Approach to Online Boycotts", "comments": "CSCW 2018 -- PACM Computer-Supported Cooperative Work and Social\n  Computing", "journal-ref": null, "doi": "10.1145/3274375", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GrabYourWallet, #boycottNRA and other online boycott campaigns have attracted\nsubstantial public interest in recent months. However, a number of significant\nchallenges are preventing online boycotts from reaching their potential. In\nparticular, complex webs of brands and subsidiaries can make it difficult for\nparticipants to conform to the goals of a boycott. Similarly, participants and\norganizers have limited visibility into a boycott's progress. This affects\ntheir ability to use sociotechnical innovations from social computing to\nincentivize participation. To address these challenges, this paper makes a\nsystem contribution: a new boycott tool called Out of Site. Out of Site uses\nlightweight automation to remove obstacles to successful online boycotts. We\ndescribe the design challenges associated with Out of Site and report results\nfrom two phases of deployment with the GrabYourWallet and Stop Animal Testing\nboycott communities. Our findings highlight the potential of boycott-assisting\ntechnologies and inform the design of this new class of technologies. Finally,\nlike is the case for many systems in social computing, while we designed Out of\nSite for pro-social uses, there are a number of easily predictable ways in\nwhich the system can be leveraged for anti-social purposes (e.g. exacerbating\nfilter bubble issues, empowering boycotts of businesses owned by racial,\nethnic, and religious minorities). As such, we developed for this project a\nnew, very straightforward design approach that treats preventing these\nanti-social uses as a top-tier design concern. This approach stands in contrast\nto the status quo of ignoring potential anti-social uses and/or considering\nthem to be a secondary design priority. We discuss how our simple approach may\nhelp other research projects reduce their potential negative impacts with\nminimal burden.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:22:03 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Li", "H.", ""], ["Alarcon", "B.", ""], ["Espinosa", "S. M.", ""], ["B", "", ""], ["Hecht", "", ""]]}, {"id": "1904.01689", "submitter": "Brent Hecht", "authors": "B. Hecht and D. Gergle", "title": "The Tower of Babel Meets Web 2.0: User-Generated Content and its\n  Applications in a Multilingual Context", "comments": "CHI 2010 Proceedings of the SIGCHI Conference on Human Factors in\n  Computing Systems", "journal-ref": null, "doi": "10.1145/1753326.1753370", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores language's fragmenting effect on user-generated content\nby examining the diversity of knowledge representations across 25 different\nWikipedia language editions. This diversity is measured at two levels: the\nconcepts that are included in each edition and the ways in which these concepts\nare described. We demonstrate that the diversity present is greater than has\nbeen presumed in the literature and has a significant influence on applications\nthat use Wikipedia as a source of world knowledge. We close by explicating how\nknowledge diversity can be beneficially leveraged to create \"culturally-aware\napplications\" and \"hyperlingual applications\".\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:23:05 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Hecht", "B.", ""], ["Gergle", "D.", ""]]}, {"id": "1904.01694", "submitter": "Brent Hecht", "authors": "N. Wenig, D. Wenig, S. Ernst, R. Malaka, B. Hecht, and J. Sch\\\"oning", "title": "Pharos: improving navigation instructions on smartwatches by including\n  global landmarks", "comments": "MobileHCI 2017 Proceedings of the 19th International Conference on\n  Human-Computer Interaction with Mobile Devices and Services", "journal-ref": null, "doi": "10.1145/3098279.3098529", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark-based navigation systems have proven benefits relative to\ntraditional turn-by-turn systems that use street names and distances. However,\none obstacle to the implementation of landmark-based navigation systems is the\ncomplex challenge of selecting salient local landmarks at each decision point\nfor each user. In this paper, we present Pharos, a novel system that extends\nturn-by-turn navigation instructions using a single global landmark (e.g. the\nEiffel Tower, the Burj Khalifa, municipal TV towers) rather than multiple,\nhard-to-select local landmarks. We first show that our approach is feasible in\na large number of cities around the world through the use of computer vision to\nselect global landmarks. We then present the results of a study demonstrating\nthat by including global landmarks in navigation instructions, users navigate\nmore confidently and build a more accurate mental map of the navigated area\nthan using turn-by-turn instructions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:43:55 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Wenig", "N.", ""], ["Wenig", "D.", ""], ["Ernst", "S.", ""], ["Malaka", "R.", ""], ["Hecht", "B.", ""], ["Sch\u00f6ning", "J.", ""]]}, {"id": "1904.01698", "submitter": "Xu Xie", "authors": "Xu Xie, Hangxin Liu, Zhenliang Zhang, Yuxing Qiu, Feng Gao, Siyuan Qi,\n  Yixin Zhu, Song-Chun Zhu", "title": "VRGym: A Virtual Testbed for Physical and Interactive AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose VRGym, a virtual reality testbed for realistic human-robot\ninteraction. Different from existing toolkits and virtual reality environments,\nthe VRGym emphasizes on building and training both physical and interactive\nagents for robotics, machine learning, and cognitive science. VRGym leverages\nmechanisms that can generate diverse 3D scenes with high realism through\nphysics-based simulation. We demonstrate that VRGym is able to (i) collect\nhuman interactions and fine manipulations, (ii) accommodate various robots with\na ROS bridge, (iii) support experiments for human-robot interaction, and (iv)\nprovide toolkits for training the state-of-the-art machine learning algorithms.\nWe hope VRGym can help to advance general-purpose robotics and machine learning\nagents, as well as assisting human studies in the field of cognitive science.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 22:55:43 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Xie", "Xu", ""], ["Liu", "Hangxin", ""], ["Zhang", "Zhenliang", ""], ["Qiu", "Yuxing", ""], ["Gao", "Feng", ""], ["Qi", "Siyuan", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1904.01778", "submitter": "Ramanathan Subramanian", "authors": "Abhinav Shukla, Shruti Shriya Gullapuram, Harish Katti, Mohan\n  Kankanhalli, Stefan Winkler, Ramanathan Subramanian", "title": "Recognition of Advertisement Emotions with Application to Computational\n  Advertising", "comments": "Under consideration for publication in IEEE Trans. Affective\n  Computing. arXiv admin note: text overlap with arXiv:1709.01684", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advertisements (ads) often contain strong affective content to capture viewer\nattention and convey an effective message to the audience. However, most\ncomputational affect recognition (AR) approaches examine ads via the text\nmodality, and only limited work has been devoted to decoding ad emotions from\naudiovisual or user cues. This work (1) compiles an affective ad dataset\ncapable of evoking coherent emotions across users; (2) explores the efficacy of\ncontent-centric convolutional neural network (CNN) features for AR vis-\\~a-vis\nhandcrafted audio-visual descriptors; (3) examines user-centric ad AR from\nElectroencephalogram (EEG) responses acquired during ad-viewing, and (4)\ndemonstrates how better affect predictions facilitate effective computational\nadvertising as determined by a study involving 18 users. Experiments reveal\nthat (a) CNN features outperform audiovisual descriptors for content-centric\nAR; (b) EEG features are able to encode ad-induced emotions better than\ncontent-based features; (c) Multi-task learning performs best among a slew of\nclassification algorithms to achieve optimal AR, and (d) Pursuant to (b), EEG\nfeatures also enable optimized ad insertion onto streamed video, as compared to\ncontent-based or manual insertion techniques in terms of ad memorability and\noverall user experience.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2019 05:42:23 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Shukla", "Abhinav", ""], ["Gullapuram", "Shruti Shriya", ""], ["Katti", "Harish", ""], ["Kankanhalli", "Mohan", ""], ["Winkler", "Stefan", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1904.01931", "submitter": "Lex Fridman", "authors": "Henri Schmidt, Jack Terwilliger, Dina AlAdawy, Lex Fridman", "title": "Hacking Nonverbal Communication Between Pedestrians and Vehicles in\n  Virtual Reality", "comments": "2019 Driving Assessment Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use an immersive virtual reality environment to explore the intricate\nsocial cues that underlie non-verbal communication involved in a pedestrian's\ncrossing decision. We \"hack\" non-verbal communication between pedestrian and\nvehicle by engineering a set of 15 vehicle trajectories, some of which follow\nsocial conventions and some that break them. By subverting social expectations\nof vehicle behavior we show that pedestrians may use vehicle kinematics to\ninfer social intentions and not merely as the state of a moving object. We\ninvestigate human behavior in this virtual world by conducting a study of 22\nsubjects, with each subject experiencing and responding to each of the\ntrajectories by moving their body, legs, arms, and head in both the physical\nand the virtual world. Both quantitative and qualitative responses are\ncollected and analyzed, showing that, in fact, social cues can be engineered\nthrough vehicle trajectory manipulation. In addition, we demonstrate that\nimmersive virtual worlds which allow the pedestrian to move around freely,\nprovide a powerful way to understand both the mechanisms of human perception\nand the social signaling involved in pedestrian-vehicle interaction.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 00:34:32 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Schmidt", "Henri", ""], ["Terwilliger", "Jack", ""], ["AlAdawy", "Dina", ""], ["Fridman", "Lex", ""]]}, {"id": "1904.02323", "submitter": "Fred Hohman", "authors": "Fred Hohman, Haekyu Park, Caleb Robinson, Duen Horng Chau", "title": "Summit: Scaling Deep Learning Interpretability by Visualizing Activation\n  and Attribution Summarizations", "comments": "Published in IEEE Transactions on Visualization and Computer Graphics\n  2020, and presented at IEEE VAST 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is increasingly used in decision-making tasks. However,\nunderstanding how neural networks produce final predictions remains a\nfundamental challenge. Existing work on interpreting neural network predictions\nfor images often focuses on explaining predictions for single images or\nneurons. As predictions are often computed from millions of weights that are\noptimized over millions of images, such explanations can easily miss a bigger\npicture. We present Summit, an interactive system that scalably and\nsystematically summarizes and visualizes what features a deep learning model\nhas learned and how those features interact to make predictions. Summit\nintroduces two new scalable summarization techniques: (1) activation\naggregation discovers important neurons, and (2) neuron-influence aggregation\nidentifies relationships among such neurons. Summit combines these techniques\nto create the novel attribution graph that reveals and summarizes crucial\nneuron associations and substructures that contribute to a model's outcomes.\nSummit scales to large data, such as the ImageNet dataset with 1.2M images, and\nleverages neural network feature visualization and dataset examples to help\nusers distill large, complex neural network models into compact, interactive\nvisualizations. We present neural network exploration scenarios where Summit\nhelps us discover multiple surprising insights into a prevalent, large-scale\nimage classifier's learned representations and informs future neural network\narchitecture design. The Summit visualization runs in modern web browsers and\nis open-sourced.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 03:00:40 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 14:42:39 GMT"}, {"version": "v3", "created": "Mon, 2 Sep 2019 19:42:14 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Hohman", "Fred", ""], ["Park", "Haekyu", ""], ["Robinson", "Caleb", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1904.02348", "submitter": "Yanchao Wang", "authors": "Yan-Chao Wang and Feng Lin and Hock-Soon Seah", "title": "Orthogonal Voronoi Diagram and Treemap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel space partitioning strategy for implicit\nhierarchy visualization such that the new plot not only has a tidy layout\nsimilar to the treemap, but also is flexible to data changes similar to the\nVoronoi treemap. To achieve this, we define a new distance function and\nneighborhood relationship between sites so that space will be divided by\naxis-aligned segments. Then a sweepline+skyline based heuristic algorithm is\nproposed to allocate the partitioned spaces to form an orthogonal Voronoi\ndiagram with orthogonal rectangles. To the best of our knowledge, it is the\nfirst time to use a sweepline-based strategy for the Voronoi treemap. Moreover,\nwe design a novel strategy to initialize the diagram status and modify the\nstatus update procedure so that the generation of our plot is more effective\nand efficient. We show that the proposed algorithm has an O(nlog(n)) complexity\nwhich is the same as the state-of-the-art Voronoi treemap. To this end, we show\nvia experiments on the artificial dataset and real-world dataset the\nperformance of our algorithm in terms of computation time, converge rate, and\naspect ratio. Finally, we discuss the pros and cons of our method and make a\nconclusion.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 05:05:49 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Wang", "Yan-Chao", ""], ["Lin", "Feng", ""], ["Seah", "Hock-Soon", ""]]}, {"id": "1904.02579", "submitter": "Rogerio Bonatti", "authors": "Mirko Gschwindt, Efe Camci, Rogerio Bonatti, Wenshan Wang, Erdal\n  Kayacan, Sebastian Scherer", "title": "Can a Robot Become a Movie Director? Learning Artistic Principles for\n  Aerial Cinematography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial filming is constantly gaining importance due to the recent advances in\ndrone technology. It invites many intriguing, unsolved problems at the\nintersection of aesthetical and scientific challenges. In this work, we propose\na deep reinforcement learning agent which supervises motion planning of a\nfilming drone by making desirable shot mode selections based on aesthetical\nvalues of video shots. Unlike most of the current state-of-the-art approaches\nthat require explicit guidance by a human expert, our drone learns how to make\nfavorable viewpoint selections by experience. We propose a learning scheme that\nexploits aesthetical features of retrospective shots in order to extract a\ndesirable policy for better prospective shots. We train our agent in realistic\nAirSim simulations using both a hand-crafted reward function as well as reward\nfrom direct human input. We then deploy the same agent on a real DJI M210 drone\nin order to test the generalization capability of our approach to real world\nconditions. To evaluate the success of our approach in the end, we conduct a\ncomprehensive user study in which participants rate the shot quality of our\nmethods. Videos of the system in action can be seen at\nhttps://youtu.be/qmVw6mfyEmw.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 14:30:09 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 15:45:57 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Gschwindt", "Mirko", ""], ["Camci", "Efe", ""], ["Bonatti", "Rogerio", ""], ["Wang", "Wenshan", ""], ["Kayacan", "Erdal", ""], ["Scherer", "Sebastian", ""]]}, {"id": "1904.02670", "submitter": "Sharath Chandra Guntuku", "authors": "Sharath Chandra Guntuku, Daniel Preotiuc-Pietro, Johannes C.\n  Eichstaedt, Lyle H. Ungar", "title": "What Twitter Profile and Posted Images Reveal About Depression and\n  Anxiety", "comments": "ICWSM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has found strong links between the choice of social media\nimages and users' emotions, demographics and personality traits. In this study,\nwe examine which attributes of profile and posted images are associated with\ndepression and anxiety of Twitter users. We used a sample of 28,749 Facebook\nusers to build a language prediction model of survey-reported depression and\nanxiety, and validated it on Twitter on a sample of 887 users who had taken\nanxiety and depression surveys. We then applied it to a different set of 4,132\nTwitter users to impute language-based depression and anxiety labels, and\nextracted interpretable features of posted and profile pictures to uncover the\nassociations with users' depression and anxiety, controlling for demographics.\nFor depression, we find that profile pictures suppress positive emotions rather\nthan display more negative emotions, likely because of social media\nself-presentation biases. They also tend to show the single face of the user\n(rather than show her in groups of friends), marking increased focus on the\nself, emblematic for depression. Posted images are dominated by grayscale and\nlow aesthetic cohesion across a variety of image features. Profile images of\nanxious users are similarly marked by grayscale and low aesthetic cohesion, but\nless so than those of depressed users. Finally, we show that image features can\nbe used to predict depression and anxiety, and that multitask learning that\nincludes a joint modeling of demographics improves prediction performance.\nOverall, we find that the image attributes that mark depression and anxiety\noffer a rich lens into these conditions largely congruent with the\npsychological literature, and that images on Twitter allow inferences about the\nmental health status of users.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:08:16 GMT"}], "update_date": "2019-04-05", "authors_parsed": [["Guntuku", "Sharath Chandra", ""], ["Preotiuc-Pietro", "Daniel", ""], ["Eichstaedt", "Johannes C.", ""], ["Ungar", "Lyle H.", ""]]}, {"id": "1904.02679", "submitter": "Jesse Vig", "authors": "Jesse Vig", "title": "Visualizing Attention in Transformer-Based Language Representation\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an open-source tool for visualizing multi-head self-attention in\nTransformer-based language representation models. The tool extends earlier work\nby visualizing attention at three levels of granularity: the attention-head\nlevel, the model level, and the neuron level. We describe how each of these\nviews can help to interpret the model, and we demonstrate the tool on the BERT\nmodel and the OpenAI GPT-2 model. We also present three use cases for analyzing\nGPT-2: detecting model bias, identifying recurring patterns, and linking\nneurons to model behavior.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:32:49 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 16:00:53 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Vig", "Jesse", ""]]}, {"id": "1904.02743", "submitter": "Ana Paula Chaves", "authors": "Ana Paula Chaves and Marco Aurelio Gerosa", "title": "How should my chatbot interact? A survey on human-chatbot interaction\n  design", "comments": null, "journal-ref": null, "doi": "10.1080/10447318.2020.1841438", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatbots' growing popularity has brought new challenges to HCI, having\nchanged the patterns of human interactions with computers. The increasing need\nto approximate conversational interaction styles raises expectations for\nchatbots to present social behaviors that are habitual in human-human\ncommunication. In this survey, we argue that chatbots should be enriched with\nsocial characteristics that cohere with users' expectations, ultimately\navoiding frustration and dissatisfaction. We bring together the literature on\ndisembodied, text-based chatbots to derive a conceptual model of social\ncharacteristics for chatbots. We analyzed 56 papers from various domains to\nunderstand how social characteristics can benefit human-chatbot interactions\nand identify the challenges and strategies to designing them. Additionally, we\ndiscussed how characteristics may influence one another. Our results provide\nrelevant opportunities to both researchers and designers to advance\nhuman-chatbot interactions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 18:43:31 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 22:00:31 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chaves", "Ana Paula", ""], ["Gerosa", "Marco Aurelio", ""]]}, {"id": "1904.02760", "submitter": "Rens Hoegen", "authors": "Rens Hoegen, Deepali Aneja, Daniel McDuff and Mary Czerwinski", "title": "An End-to-End Conversational Style Matching Agent", "comments": null, "journal-ref": "Proceedings of the 19th ACM International Conference on\n  Intelligent Virtual Agents (2019) 111-118", "doi": "10.1145/3308532.3329473", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an end-to-end voice-based conversational agent that is able to\nengage in naturalistic multi-turn dialogue and align with the interlocutor's\nconversational style. The system uses a series of deep neural network\ncomponents for speech recognition, dialogue generation, prosodic analysis and\nspeech synthesis to generate language and prosodic expression with qualities\nthat match those of the user. We conducted a user study (N=30) in which\nparticipants talked with the agent for 15 to 20 minutes, resulting in over 8\nhours of natural interaction data. Users with high consideration conversational\nstyles reported the agent to be more trustworthy when it matched their\nconversational style. Whereas, users with high involvement conversational\nstyles were indifferent. Finally, we provide design guidelines for multi-turn\ndialogue interactions using conversational style adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 19:29:56 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 22:04:37 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Hoegen", "Rens", ""], ["Aneja", "Deepali", ""], ["McDuff", "Daniel", ""], ["Czerwinski", "Mary", ""]]}, {"id": "1904.02805", "submitter": "Arturo Deza", "authors": "Arturo Deza, Amit Surana, Miguel P. Eckstein", "title": "Assessment of Faster R-CNN in Man-Machine collaborative search", "comments": "To be presented at CVPR 2019 in Long Beach, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of modern expert systems driven by deep learning that\nsupplement human experts (e.g. radiologists, dermatologists, surveillance\nscanners), we analyze how and when do such expert systems enhance human\nperformance in a fine-grained small target visual search task. We set up a 2\nsession factorial experimental design in which humans visually search for a\ntarget with and without a Deep Learning (DL) expert system. We evaluate human\nchanges of target detection performance and eye-movements in the presence of\nthe DL system. We find that performance improvements with the DL system\n(computed via a Faster R-CNN with a VGG16) interacts with observer's perceptual\nabilities (e.g., sensitivity). The main results include: 1) The DL system\nreduces the False Alarm rate per Image on average across observer groups of\nboth high/low sensitivity; 2) Only human observers with high sensitivity\nperform better than the DL system, while the low sensitivity group does not\nsurpass individual DL system performance, even when aided with the DL system\nitself; 3) Increases in number of trials and decrease in viewing time were\nmainly driven by the DL system only for the low sensitivity group. 4) The DL\nsystem aids the human observer to fixate at a target by the 3rd fixation. These\nresults provide insights of the benefits and limitations of deep learning\nsystems that are collaborative or competitive with humans.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 22:03:53 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Deza", "Arturo", ""], ["Surana", "Amit", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "1904.02813", "submitter": "Mitchell Gordon", "authors": "Mitchell L. Gordon, Tim Althoff, Jure Leskovec", "title": "Goal-setting And Achievement In Activity Tracking Apps: A Case Study Of\n  MyFitnessPal", "comments": null, "journal-ref": "WWW 2019: The Web Conference 2019", "doi": "10.1145/3308558.3313432", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity tracking apps often make use of goals as one of their core\nmotivational tools. There are two critical components to this tool: setting a\ngoal, and subsequently achieving that goal. Despite its crucial role in how a\nnumber of prominent self-tracking apps function, there has been relatively\nlittle investigation of the goal-setting and achievement aspects of\nself-tracking apps.\n  Here we explore this issue, investigating a particular goal setting and\nachievement process that is extensive, recorded, and crucial for both the app\nand its users' success: weight loss goals in MyFitnessPal. We present a\nlarge-scale study of 1.4 million users and weight loss goals, allowing for an\nunprecedented detailed view of how people set and achieve their goals. We find\nthat, even for difficult long-term goals, behavior within the first 7 days\npredicts those who ultimately achieve their goals, that is, those who lose at\nleast as much weight as they set out to, and those who do not. For instance,\nhigh amounts of early weight loss, which some researchers have classified as\nunsustainable, leads to higher goal achievement rates. We also show that early\nfood intake, self-monitoring motivation, and attitude towards the goal are\nimportant factors. We then show that we can use our findings to predict goal\nachievement with an accuracy of 79% ROC AUC just 7 days after a goal is set.\nFinally, we discuss how our findings could inform steps to improve goal\nachievement in self-tracking apps.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 22:46:10 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Gordon", "Mitchell L.", ""], ["Althoff", "Tim", ""], ["Leskovec", "Jure", ""]]}, {"id": "1904.02898", "submitter": "Tiago Ribeiro", "authors": "Tiago Ribeiro and Ana Paiva", "title": "Nutty-based Robot Animation -- Principles and Practices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot animation is a new form of character animation that extends the\ntraditional process by allowing the animated motion to become more interactive\nand adaptable during interaction with users in real-world settings. This paper\nreviews how this new type of character animation has evolved and been shaped\nfrom character animation principles and practices. We outline some new\nparadigms that aim at allowing character animators to become robot animators,\nand to properly take part in the development of social robots. One such\nparadigm consists of the 12 principles of robot animation, which describes\ngeneral concepts that both animators and robot developers should consider in\norder to properly understand each other. We also introduce the concept of\nKinematronics, for specifying the controllable and programmable expressive\nabilities of robots, and the Nutty Workflow and Pipeline. The Nutty Pipeline\nintroduces the concept of the Programmable Robot Animation Engine, which allows\nto generate, compose and blend various types of animation sources into a final,\ninteraction-enabled motion that can be rendered on robots in real-time during\nreal-world interactions. The Nutty Motion Filter is described and exemplified\nas a technique that allows an open-loop motion controller to apply physical\nlimits to the motion while still allowing to tweak the shape and expressivity\nof the resulting motion. Additionally, we describe some types of tools that can\nbe developed and integrated into Nutty-based workflows and pipelines, which\nallow animation artists to perform an integral part of the expressive behaviour\ndevelopment within social robots, and thus to evolve from standard (3D)\ncharacter animators, towards a full-stack type of robot animators.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 07:13:01 GMT"}, {"version": "v2", "created": "Tue, 16 Apr 2019 15:27:52 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 17:34:03 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ribeiro", "Tiago", ""], ["Paiva", "Ana", ""]]}, {"id": "1904.03285", "submitter": "Arijit Ray", "authors": "Arijit Ray, Yi Yao, Rakesh Kumar, Ajay Divakaran, Giedrius Burachas", "title": "Can You Explain That? Lucid Explanations Help Human-AI Collaborative\n  Image Retrieval", "comments": "2019 AAAI Conference on Human Computation and Crowdsourcing", "journal-ref": "2019 AAAI Conference on Human Computation and Crowdsourcing", "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there have been many proposals on making AI algorithms explainable, few\nhave attempted to evaluate the impact of AI-generated explanations on human\nperformance in conducting human-AI collaborative tasks. To bridge the gap, we\npropose a Twenty-Questions style collaborative image retrieval game,\nExplanation-assisted Guess Which (ExAG), as a method of evaluating the efficacy\nof explanations (visual evidence or textual justification) in the context of\nVisual Question Answering (VQA). In our proposed ExAG, a human user needs to\nguess a secret image picked by the VQA agent by asking natural language\nquestions to it. We show that overall, when AI explains its answers, users\nsucceed more often in guessing the secret image correctly. Notably, a few\ncorrect explanations can readily improve human performance when VQA answers are\nmostly incorrect as compared to no-explanation games. Furthermore, we also show\nthat while explanations rated as \"helpful\" significantly improve human\nperformance, \"incorrect\" and \"unhelpful\" explanations can degrade performance\nas compared to no-explanation games. Our experiments, therefore, demonstrate\nthat ExAG is an effective means to evaluate the efficacy of AI-generated\nexplanations on a human-AI collaborative task.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 21:26:39 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 16:45:38 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 21:52:10 GMT"}, {"version": "v4", "created": "Sat, 21 Sep 2019 17:13:50 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Ray", "Arijit", ""], ["Yao", "Yi", ""], ["Kumar", "Rakesh", ""], ["Divakaran", "Ajay", ""], ["Burachas", "Giedrius", ""]]}, {"id": "1904.03475", "submitter": "Marcus Scheunemann", "authors": "Marcus M. Scheunemann, Kerstin Dautenhahn, Maha Salem and Ben Robins", "title": "Utilizing Bluetooth Low Energy to recognize proximity, touch and humans", "comments": null, "journal-ref": null, "doi": "10.1109/ROMAN.2016.7745156", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interacting with humans is one of the main challenges for mobile robots in a\nhuman inhabited environment. To enable adaptive behavior, a robot needs to\nrecognize touch gestures and/or the proximity to interacting individuals.\nMoreover, a robot interacting with two or more humans usually needs to\ndistinguish between them. However, this remains both a configuration and cost\nintensive task. In this paper we utilize inexpensive Bluetooth Low Energy (BLE)\ndevices and propose an easy and configurable technique to enhance the robot's\ncapabilities to interact with surrounding people. In a noisy laboratory\nsetting, a mobile spherical robot is utilized in three proof-of-concept\nexperiments of the proposed system architecture. Firstly, we enhance the robot\nwith proximity information about the individuals in the surrounding\nenvironment. Secondly, we exploit BLE to utilize it as a touch sensor. And\nlastly, we use BLE to distinguish between interacting individuals. Results show\nthat observing the raw received signal strength (RSS) between BLE devices\nalready enhances the robot's interaction capabilities and that the provided\ninfrastructure can be facilitated to enable adaptive behavior in the future. We\nshow one and the same sensor system can be used to detect different types of\ninformation relevant in human-robot interaction (HRI) experiments.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 15:34:47 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Scheunemann", "Marcus M.", ""], ["Dautenhahn", "Kerstin", ""], ["Salem", "Maha", ""], ["Robins", "Ben", ""]]}, {"id": "1904.03495", "submitter": "Asad Sayeed", "authors": "Asad Sayeed", "title": "Usability in the Larger Reality: A Contrarian Argument for the\n  Importance of Social and Political Considerations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Usability engineering is situated in a much larger social and institutional\ncontext than is usually acknowledged by usability professionals in the way that\nthey define their field. The definitions and processes used in the improvement\nof user interfaces are subordinate to interests that often have narrow goals,\nhaving adverse effects on user awareness and autonomy that have further adverse\neffects on society as a whole. These effects are brought about by the way that\nknowledge about systems is limited by the design of the interface that limits\nthe user to tasks and goals defined by organizations, often commercial ones. It\nis the point at which the structures of the user interface are defined by\nusability professionals that sources of the limitation of knowledge can be\nidentified. These sources are defined by their reliance on a construction of\nthe user's wants and needs that cyclically reinforce, through the actual use of\nthe interface, the user's own construction of her wants and needs. To alleviate\nthis, it is necessary to come up with new processes of user interface design\nthat do not make assumptions about the user that tend to subordinate the user,\nand it is also necessary to reconstruct the user as a participant in the\ninterface.\n  NOTE: This article was written in 2003 as a final project for a Master's\ncourse in human-computer interaction at the University of Ottawa. The author\nattempted to publish it at the time, but did not really understand the process.\nThe ideas stand up fairly well, however, and he would like to contribute it as\na comment on the current state of affairs. It is only lightly edited for\nformat.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 17:07:41 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sayeed", "Asad", ""]]}, {"id": "1904.03540", "submitter": "Rokas Volkovas", "authors": "Rokas Volkovas, Michael Fairbank, John Woodward, Simon Lucas", "title": "Mek: Mechanics Prototyping Tool for 2D Tile-Based Turn-Based\n  Deterministic Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are few digital tools to help designers create game mechanics. A\ngeneral language to express game mechanics is necessary for rapid game design\niteration. The first iteration of a mechanics-focused language, together with\nits interfacing tool, are introduced in this paper. The language is restricted\nto two-dimensional, turn-based, tile-based, deterministic, complete-information\ngames. The tool is compared to the existing alternatives for game mechanics\nprototyping and shown to be capable of succinctly implementing a range of\nwell-known game mechanics.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 22:09:53 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Volkovas", "Rokas", ""], ["Fairbank", "Michael", ""], ["Woodward", "John", ""], ["Lucas", "Simon", ""]]}, {"id": "1904.03616", "submitter": "Beibin Li", "authors": "Beibin Li, Sachin Mehta, Deepali Aneja, Claire Foster, Pamela Ventola,\n  Frederick Shic, Linda Shapiro", "title": "A Facial Affect Analysis System for Autism Spectrum Disorder", "comments": "5 pages (including 1 page for reference), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an end-to-end machine learning-based system for\nclassifying autism spectrum disorder (ASD) using facial attributes such as\nexpressions, action units, arousal, and valence. Our system classifies ASD\nusing representations of different facial attributes from convolutional neural\nnetworks, which are trained on images in the wild. Our experimental results\nshow that different facial attributes used in our system are statistically\nsignificant and improve sensitivity, specificity, and F1 score of ASD\nclassification by a large margin. In particular, the addition of different\nfacial attributes improves the performance of ASD classification by about 7%\nwhich achieves a F1 score of 76%.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 10:08:35 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Li", "Beibin", ""], ["Mehta", "Sachin", ""], ["Aneja", "Deepali", ""], ["Foster", "Claire", ""], ["Ventola", "Pamela", ""], ["Shic", "Frederick", ""], ["Shapiro", "Linda", ""]]}, {"id": "1904.03656", "submitter": "Weina Jin", "authors": "Weina Jin, Alissa N. Antle, Diane Gromala", "title": "Ride N' Rhythm, Bike as an Embodied Musical Instrument to Improve Music\n  Perception for Young Children", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music plays a crucial role in young children's development. Current research\nlacks the design of an interactive system for younger children that could\ngenerate dynamic music change in response to the children's body movement. In\nthis paper, we present the design of bike as an embodied musical instrument for\nyoung children 2-5 years old to improve their music perception skills. In the\nRide N' Rhythm prototype, the rider's body position maps to the music volume;\nand the speed of the bike maps to the tempo. The design of the prototype\nincorporates the Embodied Music Cognition theory and Dalcroze Eurhythmics\npedagogy, and aims to internalize the 'intuitive' knowing and musical\nunderstanding via the combination of music and body movement.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 14:06:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Jin", "Weina", ""], ["Antle", "Alissa N.", ""], ["Gromala", "Diane", ""]]}, {"id": "1904.03713", "submitter": "Natalie Parde", "authors": "Natalie Parde and Rodney D. Nielsen", "title": "AI Meets Austen: Towards Human-Robot Discussions of Literary Metaphor", "comments": "Accepted to the 20th International Conference on Artificial\n  Intelligence in Education (AIED 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence is revolutionizing formal education, fueled by\ninnovations in learning assessment, content generation, and instructional\ndelivery. Informal, lifelong learning settings have been the subject of less\nattention. We provide a proof-of-concept for an embodied book discussion\ncompanion, designed to stimulate conversations with readers about particularly\ncreative metaphors in fiction literature. We collect ratings from 26\nparticipants, each of whom discuss Jane Austen's \"Pride and Prejudice\" with the\nrobot across one or more sessions, and find that participants rate their\ninteractions highly. This suggests that companion robots could be an\ninteresting entryway for the promotion of lifelong learning and cognitive\nexercise in future applications.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 19:01:32 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Parde", "Natalie", ""], ["Nielsen", "Rodney D.", ""]]}, {"id": "1904.03992", "submitter": "Yung-Ting Lee", "authors": "Yung-Ting Lee and Taisuke Ozaki", "title": "OpenMX Viewer: A web-based crystalline and molecular graphical user\n  interface program", "comments": "16 pages, 4 figures", "journal-ref": "J. Mol. Graph. Model. 89, 192-198 (2019)", "doi": "10.1016/j.jmgm.2019.03.013", "report-no": null, "categories": "cs.HC cond-mat.mtrl-sci", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OpenMX Viewer (Open source package for Material eXplorer Viewer) is a\nweb-based graphical user interface (GUI) program for visualization and analysis\nof crystalline and molecular structures and 3D grid data in the Gaussian cube\nformat such as electron density and molecular orbitals. The web-based GUI\nprogram enables us to quickly visualize crystalline and molecular structures by\ndragging and dropping XYZ, CIF, or OpenMX input/output files, and analyze\nstatic/dynamic structural properties conveniently in a web browser. Several\nbasic functionalities such as analysis of Mulliken charges, molecular dynamics,\ngeometry optimization and band structure are included. In addition, based on\nmarching cubes, marching tetrahedra and surface nets algorithms with Affine\ntransformation, 3D isosurface techniques are supported to visualize electron\ndensity and molecular/crystalline orbitals in the cube format with\nsuperposition of a crystalline or molecular structure. Furthermore, the Band\nStructure Viewer is implemented for showing a band structure in a web browser.\nBy accessing the website of the OpenMX Viewer, the latest OpenMX Viewer is\nalways available for users to visualize various structures and analyze their\nproperties without installations, upgrades, updates, registration, sign-in and\nterminal commands.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 06:48:00 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Lee", "Yung-Ting", ""], ["Ozaki", "Taisuke", ""]]}, {"id": "1904.04015", "submitter": "Gregoire Cattan", "authors": "Maxime Chabance (IHMTEK), Gr\\'egoire Cattan (GIPSA-Services, IHMTEK),\n  Bastien Maureille (IHMTEK)", "title": "Implementation of a Daemon for OpenBCI", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes a technical study of the electroencephalographic\n(EEG) headset OpenBCI (New York, US). In comparison to research grade EEG, the\nOpenBCI headset is affordable thus suitable for the general public use. In this\nstudy we designed a daemon, that is, a background and continuous task\ncommunicating with the headset, acquiring, filtering and analyzing the EEG\ndata. This study was promoted by the IHMTEK Company (Vienne, France) in 2016\nwithin a thesis on the integration of EEG-based brain-computer interfaces in\nvirtual reality for the general public.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:45:51 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Chabance", "Maxime", "", "IHMTEK"], ["Cattan", "Gr\u00e9goire", "", "GIPSA-Services, IHMTEK"], ["Maureille", "Bastien", "", "IHMTEK"]]}, {"id": "1904.04156", "submitter": "Saugat Bhattacharyya", "authors": "Monalisa Pal, Sanghamitra Bandyopadhyay and Saugat Bhattacharyya", "title": "A Many Objective Optimization Approach for Transfer Learning in EEG\n  Classification", "comments": "Pre-submission work", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In Brain-Computer Interfacing (BCI), due to inter-subject non-stationarities\nof electroencephalogram (EEG), classifiers are trained and tested using EEG\nfrom the same subject. When physical disabilities bottleneck the natural\nmodality of performing a task, acquisition of ample training data is difficult\nwhich practically obstructs classifier training. Previous works have tackled\nthis problem by generalizing the feature space amongst multiple subjects\nincluding the test subject. This work aims at knowledge transfer to classify\nEEG of the target subject using a classifier trained with the EEG of another\nunit source subject. A many-objective optimization framework is proposed where\noptimal weights are obtained for projecting features in another dimension such\nthat single source-trained target EEG classification performance is maximized\nwith the modified features. To validate the approach, motor imagery tasks from\nthe BCI Competition III Dataset IVa are classified using power spectral density\nbased features and linear support vector machine. Several performance metrics,\nimprovement in accuracy, sensitivity to the dimension of the projected space,\nassess the efficacy of the proposed approach. Addressing single-source training\npromotes independent living of differently-abled individuals by reducing\nassistance from others. The proposed approach eliminates the requirement of EEG\nfrom multiple source subjects and is applicable to any existing feature\nextractors and classifiers. Source code is available at\nhttp://worksupplements.droppages.com/tlbci.html.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 13:28:24 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Pal", "Monalisa", ""], ["Bandyopadhyay", "Sanghamitra", ""], ["Bhattacharyya", "Saugat", ""]]}, {"id": "1904.04188", "submitter": "Lex Fridman", "authors": "Dina AlAdawy, Michael Glazer, Jack Terwilliger, Henri Schmidt, Josh\n  Domeyer, Bruce Mehler, Bryan Reimer, Lex Fridman", "title": "Eye Contact Between Pedestrians and Drivers", "comments": "Will appear in Proceedings of 2019 Driving Assessment Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When asked, a majority of people believe that, as pedestrians, they make eye\ncontact with the driver of an approaching vehicle when making their crossing\ndecisions. This work presents evidence that this widely held belief is false.\nWe do so by showing that, in majority of cases where conflict is possible,\npedestrians begin crossing long before they are able to see the driver through\nthe windshield. In other words, we are able to circumvent the very difficult\nquestion of whether pedestrians choose to make eye contact with drivers, by\nshowing that whether they think they do or not, they can't. Specifically, we\nshow that over 90\\% of people in representative lighting conditions cannot\ndetermine the gaze of the driver at 15m and see the driver at all at 30m. This\nmeans that, for example, that given the common city speed limit of 25mph, more\nthan 99% of pedestrians would have begun crossing before being able to see\neither the driver or the driver's gaze. In other words, from the perspective of\nthe pedestrian, in most situations involving an approaching vehicle, the\ncrossing decision is made by the pedestrian solely based on the kinematics of\nthe vehicle without needing to determine that eye contact was made by\nexplicitly detecting the eyes of the driver.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:03:30 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["AlAdawy", "Dina", ""], ["Glazer", "Michael", ""], ["Terwilliger", "Jack", ""], ["Schmidt", "Henri", ""], ["Domeyer", "Josh", ""], ["Mehler", "Bruce", ""], ["Reimer", "Bryan", ""], ["Fridman", "Lex", ""]]}, {"id": "1904.04202", "submitter": "Lex Fridman", "authors": "Jack Terwilliger, Michael Glazer, Henri Schmidt, Josh Domeyer,\n  Heishiro Toyoda, Bruce Mehler, Bryan Reimer, Lex Fridman", "title": "Dynamics of Pedestrian Crossing Decisions Based on Vehicle Trajectories\n  in Large-Scale Simulated and Real-World Data", "comments": "Will appear in Proceedings of 2019 Driving Assessment Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans, as both pedestrians and drivers, generally skillfully navigate\ntraffic intersections. Despite the uncertainty, danger, and the non-verbal\nnature of communication commonly found in these interactions, there are\nsurprisingly few collisions considering the total number of interactions. As\nthe role of automation technology in vehicles grows, it becomes increasingly\ncritical to understand the relationship between pedestrian and driver behavior:\nhow pedestrians perceive the actions of a vehicle/driver and how pedestrians\nmake crossing decisions. The relationship between time-to-arrival (TTA) and\npedestrian gap acceptance (i.e., whether a pedestrian chooses to cross under a\ngiven window of time to cross) has been extensively investigated. However, the\ndynamic nature of vehicle trajectories in the context of non-verbal\ncommunication has not been systematically explored. Our work provides evidence\nthat trajectory dynamics, such as changes in TTA, can be powerful signals in\nthe non-verbal communication between drivers and pedestrians. Moreover, we\ninvestigate these effects in both simulated and real-world datasets, both\nlarger than have previously been considered in literature to the best of our\nknowledge.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 17:19:54 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Terwilliger", "Jack", ""], ["Glazer", "Michael", ""], ["Schmidt", "Henri", ""], ["Domeyer", "Josh", ""], ["Toyoda", "Heishiro", ""], ["Mehler", "Bruce", ""], ["Reimer", "Bryan", ""], ["Fridman", "Lex", ""]]}, {"id": "1904.04399", "submitter": "Forrest Huang", "authors": "Forrest Huang and John F. Canny", "title": "Sketchforme: Composing Sketched Scenes from Text Descriptions for\n  Interactive Applications", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sketching and natural languages are effective communication media for\ninteractive applications. We introduce Sketchforme, the first\nneural-network-based system that can generate sketches based on text\ndescriptions specified by users. Sketchforme is capable of gaining high-level\nand low-level understanding of multi-object sketched scenes without being\ntrained on sketched scene datasets annotated with text descriptions. The\nsketches composed by Sketchforme are expressive and realistic: we show in our\nuser study that these sketches convey descriptions better than human-generated\nsketches in multiple cases, and 36.5% of those sketches are considered to be\nhuman-generated. We develop multiple interactive applications using these\ngenerated sketches, and show that Sketchforme can significantly improve\nlanguage learning applications and support intelligent language-based sketching\nassistants.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 23:56:34 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Huang", "Forrest", ""], ["Canny", "John F.", ""]]}, {"id": "1904.04723", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Michael Wu, Michael N. Chin, Robert N. Chin,\n  Allen Y. Yang", "title": "Affordance Analysis of Virtual and Augmented Reality Mediated\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual and augmented reality communication platforms are seen as promising\nmodalities for next-generation remote face-to-face interactions. Our study\nattempts to explore non-verbal communication features in relation to their\nconversation context for virtual and augmented reality mediated communication\nsettings. We perform a series of user experiments, triggering nine conversation\ntasks in 4 settings, each containing corresponding non-verbal communication\nfeatures. Our results indicate that conversation types which involve less\nemotional engagement are more likely to be acceptable in virtual reality and\naugmented reality settings with low-fidelity avatar representation, compared to\nscenarios that involve high emotional engagement or intellectually difficult\ndiscussions. We further systematically analyze and rank the impact of\nlow-fidelity representation of micro-expressions, body scale, head pose, and\nhand gesture in affecting the user experience in one-on-one conversations, and\nvalidate that preserving micro-expression cues plays the most effective role in\nimproving bi-directional conversations in future virtual and augmented reality\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 15:07:51 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Wu", "Michael", ""], ["Chin", "Michael N.", ""], ["Chin", "Robert N.", ""], ["Yang", "Allen Y.", ""]]}, {"id": "1904.04890", "submitter": "Francis Williams", "authors": "Francis Williams, Alexander Bock, Harish Doraiswamy, Cassandra\n  Donatelli, Kayla Hall, Adam Summers, Daniele Panozzo, Cl\\'audio T. Silva", "title": "Unwind: Interactive Fish Straightening", "comments": null, "journal-ref": null, "doi": "10.1145/3313831.3376846", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ScanAllFish project is a large-scale effort to scan all the world's\n33,100 known species of fishes. It has already generated thousands of\nvolumetric CT scans of fish species which are available on open access\nplatforms such as the Open Science Framework. To achieve a scanning rate\nrequired for a project of this magnitude, many specimens are grouped together\ninto a single tube and scanned all at once. The resulting data contain many\nfish which are often bent and twisted to fit into the scanner. Our system,\nUnwind, is a novel interactive visualization and processing tool which\nextracts, unbends, and untwists volumetric images of fish with minimal user\ninteraction. Our approach enables scientists to interactively unwarp these\nvolumes to remove the undesired torque and bending using a piecewise-linear\nskeleton extracted by averaging isosurfaces of a harmonic function connecting\nthe head and tail of each fish. The result is a volumetric dataset of a\nindividual, straight fish in a canonical pose defined by the marine biologist\nexpert user. We have developed Unwind in collaboration with a team of marine\nbiologists: Our system has been deployed in their labs, and is presently being\nused for dataset construction, biomechanical analysis, and the generation of\nfigures for scientific publication.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 20:07:24 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 18:55:13 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Williams", "Francis", ""], ["Bock", "Alexander", ""], ["Doraiswamy", "Harish", ""], ["Donatelli", "Cassandra", ""], ["Hall", "Kayla", ""], ["Summers", "Adam", ""], ["Panozzo", "Daniele", ""], ["Silva", "Cl\u00e1udio T.", ""]]}, {"id": "1904.04964", "submitter": "Fei Wang", "authors": "Fei Wang, Jianwei Feng, Yinliang Zhao, Xiaobin Zhang, Shiyuan Zhang\n  and Jinsong Han", "title": "Joint Activity Recognition and Indoor Localization with WiFi\n  Fingerprints", "comments": "accepted by IEEE Access; 10 pages, 15 figures, 4 tables", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2923743", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the rapid development in the research topic of\nWiFi sensing that automatically senses human with commercial WiFi devices. This\nwork falls into two major categories, i.e., the activity recognition and the\nindoor localization. The former work utilizes WiFi devices to recognize human\ndaily activities such as smoking, walking, and dancing. The latter one, indoor\nlocalization, can be used for indoor navigation, location-based services, and\nthrough-wall surveillance. The key rationale behind this type of work is that\npeople behaviors can influence the WiFi signal propagation and introduce\nspecific patterns into WiFi signals, called WiFi fingerprints, which can be\nfurther explored to identify human activities and locations. In this paper, we\npropose a novel deep learning framework for joint activity recognition and\nindoor localization task using WiFi Channel State Information~(CSI)\nfingerprints. More precisely, we develop a system running standard IEEE 802.11n\nWiFi protocol, and collect more than 1400 CSI fingerprints on 6 activities at\n16 indoor locations. Then we propose a dual-task convolutional neural network\nwith 1-dimensional convolutional layers for the joint task of activity\nrecognition and indoor localization. Experimental results and ablation study\nshow that our approach achieves good performances in this joint WiFi sensing\ntask. Data and code have been made publicly available at\nhttps://github.com/geekfeiw/apl.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 01:22:06 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 19:29:53 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Wang", "Fei", ""], ["Feng", "Jianwei", ""], ["Zhao", "Yinliang", ""], ["Zhang", "Xiaobin", ""], ["Zhang", "Shiyuan", ""], ["Han", "Jinsong", ""]]}, {"id": "1904.05009", "submitter": "Charles Martin", "authors": "Charles P Martin and Jim Torresen", "title": "An Interactive Musical Prediction System with Mixture Density Recurrent\n  Neural Networks", "comments": "Accepted for presentation at the International Conference on New\n  Interfaces for Musical Expression (NIME), June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is about creating digital musical instruments where a predictive\nneural network model is integrated into the interactive system. Rather than\npredicting symbolic music (e.g., MIDI notes), we suggest that predicting future\ncontrol data from the user and precise temporal information can lead to new and\ninteresting interactive possibilities. We propose that a mixture density\nrecurrent neural network (MDRNN) is an appropriate model for this task. The\npredictions can be used to fill-in control data when the user stops performing,\nor as a kind of filter on the user's own input. We present an interactive MDRNN\nprediction server that allows rapid prototyping of new NIMEs featuring\npredictive musical interaction by recording datasets, training MDRNN models,\nand experimenting with interaction modes. We illustrate our system with several\nexample NIMEs applying this idea. Our evaluation shows that real-time\npredictive interaction is viable even on single-board computers and that small\nmodels are appropriate for small datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 05:50:15 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Martin", "Charles P", ""], ["Torresen", "Jim", ""]]}, {"id": "1904.05154", "submitter": "Heinrich Dinkel", "authors": "Heinrich Dinkel, Mengyue Wu and Kai Yu", "title": "Text-based depression detection on sparse data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous text-based depression detection is commonly based on large\nuser-generated data. Sparse scenarios like clinical conversations are less\ninvestigated. This work proposes a text-based multi-task BGRU network with\npretrained word embeddings to model patients' responses during clinical\ninterviews. Our main approach uses a novel multi-task loss function, aiming at\nmodeling both depression severity and binary health state. We independently\ninvestigate word- and sentence-level word-embeddings as well as the use of\nlarge-data pretraining for depression detection. To strengthen our findings, we\nreport mean-averaged results for a multitude of independent runs on sparse\ndata. First, we show that pretraining is helpful for word-level text-based\ndepression detection. Second, our results demonstrate that sentence-level\nword-embeddings should be mostly preferred over word-level ones. While the\nchoice of pooling function is less crucial, mean and attention pooling should\nbe preferred over last-timestep pooling. Our method outputs depression presence\nresults as well as predicted severity score, culminating a macro F1 score of\n0.84 and MAE of 3.48 on the DAIC-WOZ development set.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 03:47:15 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 08:58:17 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 05:23:15 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Dinkel", "Heinrich", ""], ["Wu", "Mengyue", ""], ["Yu", "Kai", ""]]}, {"id": "1904.05247", "submitter": "Linus W. Dietz", "authors": "Rinita Roy and Linus W. Dietz", "title": "A Model for Using Physiological Conditions for Proactive Tourist\n  Recommendations", "comments": null, "journal-ref": "ABIS '2019 Proceedings of the 23rd International Workshop on\n  Personalization and Recommendation on the Web and Beyond", "doi": "10.1145/3345002.3349289", "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile proactive tourist recommender systems can support tourists by\nrecommending the best choice depending on different contexts related to herself\nand the environment. In this paper, we propose to utilize wearable sensors to\ngather health information about a tourist and use them for recommending tourist\nactivities. We discuss a range of wearable devices, sensors to infer\nphysiological conditions of the users, and exemplify the feasibility using a\npopular self-quantification mobile app. Our main contribution then comprises a\ndata model to derive relations between the parameters measured by the wearable\nsensors, such as heart rate, body temperature, blood pressure, and use them to\ninfer the physiological condition of a user. This model can then be used to\nderive classes of tourist activities that determine which items should be\nrecommended.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:37:28 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Roy", "Rinita", ""], ["Dietz", "Linus W.", ""]]}, {"id": "1904.05248", "submitter": "Reshmashree Bangalore Kantharaju", "authors": "Reshmashree B. Kantharaju, Dominic De Franco, Alison Pease, Catherine\n  Pelachaud", "title": "Is Two Better than One? Effects of Multiple Agents on User Persuasion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual humans need to be persuasive in order to promote behaviour change in\nhuman users. While several studies have focused on understanding the numerous\naspects that influence the degree of persuasion, most of them are limited to\ndyadic interactions. In this paper, we present an evaluation study focused on\nunderstanding the effects of multiple agents on user's persuasion. Along with\ngender and status (authoritative & peer), we also look at type of focus\nemployed by the agent i.e., user-directed where the agent aims to persuade by\naddressing the user directly and vicarious where the agent aims to persuade the\nuser, who is an observer, indirectly by engaging another agent in the\ndiscussion. Participants were randomly assigned to one of the 12 conditions and\npresented with a persuasive message by one or several virtual agents. A\nquestionnaire was used to measure perceived interpersonal attitude, credibility\nand persuasion. Results indicate that credibility positively affects\npersuasion. In general, multiple agent setting, irrespective of the focus, was\nmore persuasive than single agent setting. Although, participants favored\nuser-directed setting and reported it to be persuasive and had an increased\nlevel of trust in the agents, the actual change in persuasion score reflects\nthat vicarious setting was the most effective in inducing behaviour change. In\naddition to this, the study also revealed that authoritative agents were the\nmost persuasive.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 15:38:30 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Kantharaju", "Reshmashree B.", ""], ["De Franco", "Dominic", ""], ["Pease", "Alison", ""], ["Pelachaud", "Catherine", ""]]}, {"id": "1904.05331", "submitter": "Nitish Nag", "authors": "Nitish Nag, Aditya Bharadwaj, Aditya Narendra Rao, Akash Kulhalli,\n  Kushal Samir Mehta, Nishant Bhattacharya, Pratul Ramkumar, Dinkar Sitaram,\n  Ramesh Jain", "title": "Flavour Enhanced Food Recommendation", "comments": "In Proceedings of 5th International Workshop on Multimedia Assisted\n  Dietary Management, Nice, France, October 21, 2019, MADiMa 2019, 6 pages", "journal-ref": null, "doi": "10.1145/3347448.3357169", "report-no": null, "categories": "cs.SI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mechanism to use the features of flavour to enhance the quality\nof food recommendations. An empirical method to determine the flavour of food\nis incorporated into a recommendation engine based on major gustatory nerves.\nSuch a system has advantages of suggesting food items that the user is more\nlikely to enjoy based upon matching with their flavour profile through use of\nthe taste biological domain knowledge. This preliminary intends to spark more\nrobust mechanisms by which flavour of food is taken into consideration as a\nmajor feature set into food recommendation systems. Our long term vision is to\nintegrate this with health factors to recommend healthy and tasty food to users\nto enhance quality of life.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2019 02:44:26 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 20:36:12 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Nag", "Nitish", ""], ["Bharadwaj", "Aditya", ""], ["Rao", "Aditya Narendra", ""], ["Kulhalli", "Akash", ""], ["Mehta", "Kushal Samir", ""], ["Bhattacharya", "Nishant", ""], ["Ramkumar", "Pratul", ""], ["Sitaram", "Dinkar", ""], ["Jain", "Ramesh", ""]]}, {"id": "1904.05382", "submitter": "Garreth Tigwell", "authors": "Garreth W. Tigwell, Zhanna Sarsenbayeva, Benjamin M. Gorman, David R.\n  Flatla, Jorge Goncalves, Yeliz Yesilada, Jacob O. Wobbrock", "title": "Proceedings of the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situationally-induced impairments and disabilities (SIIDs) make it difficult\nfor users of interactive computing systems to perform tasks due to context\n(e.g., listening to a phone call when in a noisy crowd) rather than a result of\na congenital or acquired impairment (e.g., hearing damage). SIIDs are a great\nconcern when considering the ubiquitousness of technology in a wide range of\ncontexts. Considering our daily reliance on technology, and mobile technology\nin particular, it is increasingly important that we fully understand and model\nhow SIIDs occur. Similarly, we must identify appropriate methods for sensing\nand adapting technology to reduce the effects of SIIDs. In this workshop, we\nwill bring together researchers working on understanding, sensing, modelling,\nand adapting technologies to ameliorate the effects of SIIDs. This workshop\nwill provide a venue to identify existing research gaps, new directions for\nfuture research, and opportunities for future collaboration.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 09:14:23 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Tigwell", "Garreth W.", ""], ["Sarsenbayeva", "Zhanna", ""], ["Gorman", "Benjamin M.", ""], ["Flatla", "David R.", ""], ["Goncalves", "Jorge", ""], ["Yesilada", "Yeliz", ""], ["Wobbrock", "Jacob O.", ""]]}, {"id": "1904.05387", "submitter": "Emery Berger", "authors": "Eunice Jun, Maureen Daum, Jared Roesch, Sarah E. Chasins, Emery D.\n  Berger, Rene Just, Katharina Reinecke", "title": "Tea: A High-level Language and Runtime System for Automating Statistical\n  Analysis", "comments": "11 pages", "journal-ref": null, "doi": "10.1145/3332165.3347940", "report-no": null, "categories": "cs.PL cs.HC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though statistical analyses are centered on research questions and\nhypotheses, current statistical analysis tools are not. Users must first\ntranslate their hypotheses into specific statistical tests and then perform API\ncalls with functions and parameters. To do so accurately requires that users\nhave statistical expertise. To lower this barrier to valid, replicable\nstatistical analysis, we introduce Tea, a high-level declarative language and\nruntime system. In Tea, users express their study design, any parametric\nassumptions, and their hypotheses. Tea compiles these high-level specifications\ninto a constraint satisfaction problem that determines the set of valid\nstatistical tests, and then executes them to test the hypothesis. We evaluate\nTea using a suite of statistical analyses drawn from popular tutorials. We show\nthat Tea generally matches the choices of experts while automatically switching\nto non-parametric tests when parametric assumptions are not met. We simulate\nthe effect of mistakes made by non-expert users and show that Tea automatically\navoids both false negatives and false positives that could be produced by the\napplication of incorrect statistical tests.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:44:55 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Jun", "Eunice", ""], ["Daum", "Maureen", ""], ["Roesch", "Jared", ""], ["Chasins", "Sarah E.", ""], ["Berger", "Emery D.", ""], ["Just", "Rene", ""], ["Reinecke", "Katharina", ""]]}, {"id": "1904.05681", "submitter": "Alexandre Kaspar", "authors": "Alexandre Kaspar, Liane Makatura and Wojciech Matusik", "title": "Knitting Skeletons: A Computer-Aided Design Tool for Shaping and\n  Patterning of Knitted Garments", "comments": "Project page: http://knitskel.csail.mit.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a novel interactive system for simple garment composition\nand surface patterning. Our approach makes it easier for casual users to\ncustomize machine-knitted garments, while enabling more advanced users to\ndesign their own composable templates. Our tool combines ideas from CAD\nsoftware and image editing: it allows the composition of (1) parametric knitted\nprimitives, and (2) stitch pattern layers with different resampling behaviours.\nBy leveraging the regularity of our primitives, our tool enables interactive\ncustomization with automated layout and real-time patterning feedback. We show\na variety of garments and patterns created with our tool, and highlight our\nability to transfer shape and pattern customizations between users.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 13:23:57 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 20:24:59 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Kaspar", "Alexandre", ""], ["Makatura", "Liane", ""], ["Matusik", "Wojciech", ""]]}, {"id": "1904.06117", "submitter": "Tousif Ahmed", "authors": "Tousif Ahmed, Rakibul Hasan, Kay Connelly, David Crandall, Apu Kapadia", "title": "Conveying Situational Information to People with Visual Impairments", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no01", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing who is in one's vicinity is key to managing privacy in everyday\nenvironments, but is challenging for people with visual impairments. Wearable\ncameras and other sensors may be able to detect such information, but how\nshould this complex visually-derived information be conveyed in a way that is\ndiscreet, intuitive, and unobtrusive? Motivated by previous studies on the\nspecific information that visually impaired people would like to have about\ntheir surroundings, we created three medium-fidelity prototypes: 1) a 3D\nprinted model of a watch to convey tactile information; 2) a smartwatch app for\nhaptic feedback; and 3) a smartphone app for audio feedback. A usability study\nwith 14 participants with visual impairments identified a range of practical\nissues (e.g., speed of conveying information) and design considerations (e.g.,\nconfigurable privacy bubble) for conveying privacy feedback in real-world\ncontexts.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:22:55 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Ahmed", "Tousif", ""], ["Hasan", "Rakibul", ""], ["Connelly", "Kay", ""], ["Crandall", "David", ""], ["Kapadia", "Apu", ""]]}, {"id": "1904.06118", "submitter": "Elgin Akp{\\i}nar", "authors": "Elgin Akp{\\i}nar, Yeliz Ye\\c{s}ilada, Selim Temizer", "title": "Ability and Context Based Adaptive System: A Proposal for Machine\n  Learning Approach", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no02", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we interact with small screen devices, sometimes we make errors, due to\nour abilities/disabilities, contextual factors that distract our attention or\nproblems related to the interface. Recovering from these errors may be time\nconsuming or cause frustration. Predicting and learning these errors based on\nthe previous user interaction and contextual factors, and adapting user\ninterface to prevent from these errors can improve user performance and\nsatisfaction. In this paper, we propose a system that aims to monitor user\nperformance and contextual changes and do adaptations based on the user\nperformance by using machine learning techniques. Here, we briefly present our\nsystematic literature review findings and discuss our research questions\ntowards developing such an adaptive system.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:24:34 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Akp\u0131nar", "Elgin", ""], ["Ye\u015filada", "Yeliz", ""], ["Temizer", "Selim", ""]]}, {"id": "1904.06119", "submitter": "Tigmanshu Bhatnagar", "authors": "Tigmanshu Bhatnagar, Youngjun Cho, Nicolai Marquardt, Catherine\n  Holloway", "title": "Expressive haptics for enhanced usability of mobile interfaces in\n  situations of impairments", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no03", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing for situational awareness could lead to better solutions for\ndisabled people, likewise, exploring the needs of disabled people could lead to\ninnovations that can address situational impairments. This in turn can create\nnon-stigmatising assistive technology for disabled people from which eventually\neveryone could benefit. In this paper, we investigate the potential for\nadvanced haptics to compliment the graphical user interface of mobile devices,\nthereby enhancing user experiences of all people in some situations (e.g.\nsunlight interfering with interaction) and visually impaired people. We explore\ntechnical solutions to this problem space and demonstrate our justification for\na focus on the creation of kinaesthetic force feedback. We propose initial\ndesign concepts and studies, with a view to co-create delightful and expressive\nhaptic interactions with potential users motivated by scenarios of situational\nand permanent impairments.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:26:06 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Bhatnagar", "Tigmanshu", ""], ["Cho", "Youngjun", ""], ["Marquardt", "Nicolai", ""], ["Holloway", "Catherine", ""]]}, {"id": "1904.06120", "submitter": "Nayeri Jacobo", "authors": "Nayeri Jacobo", "title": "Method Cards for Designing for Situational Impairment", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no04", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite significant research that was influenced by Situationally-Induced\nImpairments and Disabilities (SIIDs) to improve the accessibility of mobile\ntechnology, there is still lack of awareness on how to design for SIIDs.\nDesigning for situational impairments does not only affect usability for people\nwho have temporary or long-term disabilities, but also for the \"ideal\" users\nwho get impacted. Limited resources on how to design for situational\nimpairments overlook inclusive interactions and hinder the creation of\naccessible technology. Thus, I am going to create method cards that can be used\nduring the design process to figure out how to get designers to design for\nSIIDs. These method cards help us better understand how to improve the design\nprocess by addressing the subject of how to design in order to reduce SIIDs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:27:19 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Jacobo", "Nayeri", ""]]}, {"id": "1904.06122", "submitter": "Varun Jain", "authors": "Varun Jain, Ramya Hebbalaguppe", "title": "AirPen: A Touchless Fingertip Based Gestural Interface for Smartphones\n  and Head-Mounted Devices", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no05", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gestures are an intuitive, socially acceptable, and a non-intrusive\ninteraction modality in Mixed Reality (MR) and smartphone based applications.\nUnlike speech interfaces, they tend to perform well even in shared and public\nspaces. Hand gestures can also be used to interact with smartphones in\nsituations where the user's ability to physically touch the device is impaired.\nHowever, accurate gesture recognition can be achieved through state-of-the-art\ndeep learning models or with the use of expensive sensors. Despite the\nrobustness of these deep learning models, they are computationally heavy and\nmemory hungry, and obtaining real-time performance on-device without additional\nhardware is still a challenge. To address this, we propose AirPen: an analogue\nto pen on paper, but in air, for in-air writing and gestural commands that\nworks seamlessly in First and Second Person View. The models are trained on a\nGPU machine and ported on an Android smartphone. AirPen comprises of three deep\nlearning models that work in tandem: MobileNetV2 for hand localisation, our\ncustom fingertip regression architecture followed by a Bi-LSTM model for\ngesture classification. The overall framework works in real-time on mobile\ndevices and achieves a classification accuracy of 80% with an average latency\nof only 0.12 s.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:29:01 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Jain", "Varun", ""], ["Hebbalaguppe", "Ramya", ""]]}, {"id": "1904.06123", "submitter": "Gisela Reyes-Cruz", "authors": "Gisela Reyes-Cruz, Joel Fischer, Stuart Reeves", "title": "An ethnographic study of visual impairments for voice user interface\n  design", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no06", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design for Voice User Interfaces (VUIs) has become more relevant in recent\nyears due to the enormous advances of speech technologies and their growing\npresence in our everyday lives. Although modern VUIs still present interaction\nissues, reports indicate they are being adopted by people with different\ndisabilities and having a positive impact. For the first author's PhD research\nproject, an ethnographic study is currently being carried out in a local\ncharity that provides support and services to people with visual impairments.\nThe purpose is to understand people's competencies and practices, and how these\nare, or could be, related to voice technologies (assistive technology and\nmainstream VUIs). Through direct observation and contextual interviews, we aim\nto investigate the problems and solutions they encounter and the ways they cope\nwith particular situations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:30:59 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Reyes-Cruz", "Gisela", ""], ["Fischer", "Joel", ""], ["Reeves", "Stuart", ""]]}, {"id": "1904.06128", "submitter": "Zhanna Sarsenbayeva", "authors": "Zhanna Sarsenbayeva, Vassilis Kostakos, Jorge Goncalves", "title": "Situationally-Induced Impairments and Disabilities Research", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no07", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research has shown that various environmental factors impact smartphone\ninteraction and lead to Situationally-Induced Impairments and Disabilities. In\nthis work we discuss the importance of thoroughly understanding the effects of\nthese situational impairments on smartphone interaction. We argue that\nsystematic investigation of the effects of different situational impairments is\nquintessential for conducting successful research in the field of SIIDs that\nmight lead to building appropriate sensing, modelling, and adapting techniques.\nWe also provide insights for future work identifying potential directions to\nconduct research in SIIDs.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:41:33 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Sarsenbayeva", "Zhanna", ""], ["Kostakos", "Vassilis", ""], ["Goncalves", "Jorge", ""]]}, {"id": "1904.06129", "submitter": "Sidas Saulynas", "authors": "Sidas Saulynas, Ravi Kuber", "title": "Designing Mobile Interaction Guidelines to Account for Situationally\n  Induced Impairments and Disabilities (SIID) and Severely Constraining\n  Situational Impairments (SCSI)", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no08", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research investigates the variety and complexity of situational\nimpairment events (SIE) that are being experienced by users of smartphone\ntechnology of all abilities. The authors have created a classification system\nto help describe the different types of SIE as well as differentiate a certain\nsubgroup of events that were identified as severely constraining. Continuing\nresearch examined workarounds that users deploy when attempting to complete a\nmobile I/O transaction in the presence of an SIE, as well as social/cultural\nbarriers to attempting mobile interaction that users recognize but do not\nalways follow. The ultimate goal of this research arc would be the creation of\nguidelines to assist mobile designers and researchers in the accounting of SIE\nand perhaps different design considerations for those events deemed severely\nconstraining.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:42:52 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Saulynas", "Sidas", ""], ["Kuber", "Ravi", ""]]}, {"id": "1904.06131", "submitter": "Shreepriya Shreepriya", "authors": "Shreepriya Shreepriya, Danilo Gallo, Sruthi Viswanathan, Jutta\n  Willamowski", "title": "Situationally Induced Impairment in Navigation Support for Runners", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no09", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices are ubiquitous and support us in a myriad of situations. In\nthis paper, we study the support that mobile devices provide for navigation. It\npresents our findings on the Situational Induced Impairments and Disabilities\n(SIID) during running. We define the context of runners and the factors\naffecting the use of mobile devices for navigation during running. We discuss\ndesign implications and introduce early concepts to address the uncovered SIID\nissues. This work contributes to the growing body of research on SIID in using\nmobile devices.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:44:42 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Shreepriya", "Shreepriya", ""], ["Gallo", "Danilo", ""], ["Viswanathan", "Sruthi", ""], ["Willamowski", "Jutta", ""]]}, {"id": "1904.06132", "submitter": "Osian Smith", "authors": "Osian Smith, Stephen Lindsay", "title": "Looking At Situationally-Induced Impairments And Disabilities (SIIDs)\n  With People With Cognitive Brain Injury", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no10", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document, we discuss our work into a speaker recognition to support\npeople with prosopagnosia and the limitations of alerting the user of whom they\nare in discussion with. We will discuss how current research into Situationally\nInduced Impairments Disabilities (SIIDs) can assist people with disabilities\nand vice versa and how our work can support people who may find themselves in a\nsituation where they are impaired with facial recognition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:46:30 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Smith", "Osian", ""], ["Lindsay", "Stephen", ""]]}, {"id": "1904.06134", "submitter": "Aaron Steinfeld", "authors": "Aaron Steinfeld, John Zimmerman, Anthony Tomasic", "title": "Universal Design and Adaptive Interfaces as a Strategy for Induced\n  Disabilities", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no11", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is great promise in creating effective technology experiences during\nsituationally-induced impairments and disabilities through the combination of\nuniversal design and adaptive interfaces. We believe this combination is a\npowerful approach for meeting the UX needs of people with disabilities,\nincluding those which are temporary in nature. Research in each of these areas,\nand the combination, illustrates this promise.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:48:14 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Steinfeld", "Aaron", ""], ["Zimmerman", "John", ""], ["Tomasic", "Anthony", ""]]}, {"id": "1904.06138", "submitter": "Paul Whittington", "authors": "Paul Whittington, Huseyin Dogan, Nan Jiang, Keith Phalp", "title": "The development and evaluation of the SmartAbility Android Application\n  to detect users' abilities", "comments": "Presented at the CHI'19 Workshop: Addressing the Challenges of\n  Situationally-Induced Impairments and Disabilities in Mobile Interaction,\n  2019 (arXiv:1904.05382)", "journal-ref": null, "doi": null, "report-no": "SIID/2019/no12", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SmartAbility Android Application recommends Assistive Technology (AT) for\npeople with reduced physical ability, by focusing on the actions (abilities)\nthat can be performed independently. The Application utilises built-in sensor\ntechnologies in Android devices to detect user abilities, including head and\nlimb movements, speech and blowing. The Application was evaluated by 18\nparticipants with varying physical conditions and assessed through the System\nUsability Scale (SUS) and NASA Task Load Index (TLX). The Application achieved\na SUS score of 72.5 (indicating 'Good Usability') with low levels of Temporal\nDemand and Frustration and medium levels of Mental Demand, Physical Demand and\nEffort. It is anticipated that the SmartAbility Application will be\ndisseminated to the AT domain, to improve quality of life for people with\nreduced physical ability.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 09:51:20 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Whittington", "Paul", ""], ["Dogan", "Huseyin", ""], ["Jiang", "Nan", ""], ["Phalp", "Keith", ""]]}, {"id": "1904.06399", "submitter": "Leonel Merino", "authors": "Leonel Merino, Mario Hess, Alexandre Bergel, Oscar Nierstrasz, Daniel\n  Weiskopf", "title": "PerfVis: Pervasive Visualization in Immersive AugmentedReality for\n  Performance Awareness", "comments": "ICPE'19 vision, 4 pages, 2 figure, conference", "journal-ref": null, "doi": "10.1145/3302541.3313104", "report-no": null, "categories": "cs.HC cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Developers are usually unaware of the impact of code changes to the\nperformance of software systems. Although developers can analyze the\nperformance of a system by executing, for instance, a performance test to\ncompare the performance of two consecutive versions of the system, changing\nfrom a programming task to a testing task would disrupt the development flow.\nIn this paper, we propose the use of a city visualization that dynamically\nprovides developers with a pervasive view of the continuous performance of a\nsystem. We use an immersive augmented reality device (Microsoft HoloLens) to\ndisplay our visualization and extend the integrated development environment on\na computer screen to use the physical space. We report on technical details of\nthe design and implementation of our visualization tool, and discuss early\nfeedback that we collected of its usability. Our investigation explores a new\nvisual metaphor to support the exploration and analysis of possibly very large\nand multidimensional performance data. Our initial result indicates that the\ncity metaphor can be adequate to analyze dynamic performance data on a large\nand non-trivial software system.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2019 14:59:01 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Merino", "Leonel", ""], ["Hess", "Mario", ""], ["Bergel", "Alexandre", ""], ["Nierstrasz", "Oscar", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "1904.06417", "submitter": "Lionel Robert", "authors": "Lionel P. Robert Jr", "title": "The Future of Pedestrian Automated Vehicle Interactions", "comments": "4 pages", "journal-ref": "XRDS 25(3) (April 2019), 30-33. DOI:\n  https://doi.org/10.1145/3313115", "doi": "10.1145/3313115", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the increasing popularity of autonomous vehicles has garnered critical\nmedia attention, less has been written about the field of pedestrian-automated\nvehicle interactions and its challenges. Current research trends are discussed\nas well as several areas receiving much less attention, but are still vital to\nthe field.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 21:01:01 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Robert", "Lionel P.", "Jr"]]}, {"id": "1904.06427", "submitter": "Andr\\'es Monroy-Hern\\'andez", "authors": "Fannie Liu, Mario Esparza, Maria Pavlovskaia, Geoff Kaufman, Laura\n  Dabbish, Andr\\'es Monroy-Hern\\'andez", "title": "Animo: Sharing Biosignals on a Smartwatch for Lightweight Social\n  Connection", "comments": "Journal Proceedings of the ACM on Interactive, Mobile, Wearable and\n  Ubiquitous Technologies (IMWUT/UbiComp '19). Volume 3 Issue 1, March 2019.\n  Article No. 18", "journal-ref": null, "doi": "10.1145/3314405", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Animo, a smartwatch app that enables people to share and view each\nother's biosignals. We designed and engineered Animo to explore new ground for\nsmartwatch-based biosignals social computing systems: identifying opportunities\nwhere these systems can support lightweight and mood-centric interactions. In\nour work we develop, explore, and evaluate several innovative features designed\nfor dyadic communication of heart rate. We discuss the results of a two-week\nstudy (N=34), including new communication patterns participants engaged in, and\noutline the design landscape for communicating with biosignals on smartwatches.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 21:36:06 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Liu", "Fannie", ""], ["Esparza", "Mario", ""], ["Pavlovskaia", "Maria", ""], ["Kaufman", "Geoff", ""], ["Dabbish", "Laura", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""]]}, {"id": "1904.06518", "submitter": "O\\u{g}uzhan Atabek", "authors": "Oguzhan Atabek", "title": "Challenges in Integrating Technology into Education", "comments": "19 pages, 4 tables", "journal-ref": "Turkish Studies - Information Technologies and Applied Sciences\n  14(1) (2019) 1-19", "doi": "10.7827/TurkishStudies.14810", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite significant amount of investment, there seems to be obstacles to\ntechnology integration in education. In order to shed light on the nature of\nperceived obstacles to technology integration, opinions of 117 professionals,\nwho were selected by Turkish Ministry of National Education as experts in their\nrespective fields, about the obstacles to integration of technology into\neducation were investigated. After categorizing the perceived obstacles by\nfactor analysis, associations of those categories with personal and\nprofessional differences were further investigated for better contextualizing\nthe findings. Correlations were analyzed by Pearson's product moment\ncoefficient and point-biserial coefficient. The results revealed that it's not\nthe hardware itself that constitute obstacles to technology integration.\nInsufficiency of in-service and pre-service training, content support, and\nincentive system emerged as major perceived obstacles to technology\nintegration. Inadequacy of physical and technological infrastructure was also\nfound to be an important obstacle to successful integration. Novelty of the\ntechnologies compared to older ones were not found to be an obstacle to\ntechnology integration. Moreover, participants stressed the lack of education\nin teacher training institutions about current technologies that Ministry of\nNational Education officially requires teachers to use as part of their jobs to\nbe another important obstacle. There were no correlations between sex, age,\nlevel of education, job position, year of experience in other careers, and any\nof the categories of perceived obstacles. However, there was a strong negative\ncorrelation between year of experience in teaching and insufficiency of\nresources. Association between year of experience in educational administration\nand negative psychological state was also strong and negative.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2019 10:11:40 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Atabek", "Oguzhan", ""]]}, {"id": "1904.06710", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley", "title": "Towards expert-based speed-precision control in early simulator training\n  for novice surgeons", "comments": null, "journal-ref": "2018, Information, 9(12), 316", "doi": "10.3390/info9120316", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulator training for image guided surgical interventions would benefit from\nintelligent systems that detect the evolution of task performance, and take\ncontrol of individual speed precision strategies by providing effective\nautomatic performance feedback. At the earliest training stages, novices\nfrequently focus on getting faster at the task. This may, as shown here,\ncompromise the evolution of their precision scores, sometimes irreparably, if\nit is not controlled for as early as possible. Artificial intelligence could\nhelp make sure that a trainee reaches optimal individual speed accuracy\ntradeoff by monitoring individual performance criteria, detecting critical\ntrends at any given moment in time, and alerting the trainee as early as\nnecessary when to slow down and focus on precision, or when to focus on getting\nfaster. It is suggested that, for effective benchmarking, individual training\nstatistics of novices are compared with the statistics of an expert surgeon.\nThe speed accuracy functions of novices trained in a large number of\nexperimental sessions reveal differences in individual speed versus precision\nstrategies, and clarify why such strategies should be automatically detected\nand controlled for before further training on specific surgical task models, or\nclinical models, may be envisaged. How expert benchmark statistics may be\nexploited for automatic performance control is explained.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 15:36:11 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Dresp-Langley", "Birgitta", ""]]}, {"id": "1904.06722", "submitter": "Neil S. Gaikwad", "authors": "Snehalkumar (Neil) S. Gaikwad, Durim Morina, Adam Ginzberg, Catherine\n  Mullings, Shirish Goyal, Dilrukshi Gamage, Christopher Diemert, Mathias\n  Burton, Sharon Zhou, Mark Whiting, Karolina Ziulkoski, Alipta Ballav, Aaron\n  Gilbee, Senadhipathige S. Niranga, Vibhor Sehgal, Jasmine Lin, Leonardy\n  Kristianto, Angela Richmond-Fuller, Jeff Regino, Nalin Chhibber, Dinesh\n  Majeti, Sachin Sharma, Kamila Mananova, Dinesh Dhakal, William Dai, Victoria\n  Purynova, Samarth Sandeep, Varshine Chandrakanthan, Tejas Sarma, Sekandar\n  Matin, Ahmed Nasser, Rohit Nistala, Alexander Stolzoff, Kristy Milland,\n  Vinayak Mathur, Rajan Vaish, Michael S. Bernstein", "title": "Boomerang: Rebounding the Consequences of Reputation Feedback on\n  Crowdsourcing Platforms", "comments": null, "journal-ref": "Proceedings of the 29th Annual Symposium on User Interface\n  Software and Technology, 2016", "doi": "10.1145/2984511.2984542", "report-no": null, "categories": "cs.CY cs.HC econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Paid crowdsourcing platforms suffer from low-quality work and unfair\nrejections, but paradoxically, most workers and requesters have high reputation\nscores. These inflated scores, which make high-quality work and workers\ndifficult to find, stem from social pressure to avoid giving negative feedback.\nWe introduce Boomerang, a reputation system for crowdsourcing that elicits more\naccurate feedback by rebounding the consequences of feedback directly back onto\nthe person who gave it. With Boomerang, requesters find that their highly-rated\nworkers gain earliest access to their future tasks, and workers find tasks from\ntheir highly-rated requesters at the top of their task feed. Field experiments\nverify that Boomerang causes both workers and requesters to provide feedback\nthat is more closely aligned with their private opinions. Inspired by a\ngame-theoretic notion of incentive-compatibility, Boomerang opens opportunities\nfor interaction design to incentivize honest reporting over strategic\ndishonesty.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 16:40:01 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Snehalkumar", "", "", "Neil"], ["Gaikwad", "S.", ""], ["Morina", "Durim", ""], ["Ginzberg", "Adam", ""], ["Mullings", "Catherine", ""], ["Goyal", "Shirish", ""], ["Gamage", "Dilrukshi", ""], ["Diemert", "Christopher", ""], ["Burton", "Mathias", ""], ["Zhou", "Sharon", ""], ["Whiting", "Mark", ""], ["Ziulkoski", "Karolina", ""], ["Ballav", "Alipta", ""], ["Gilbee", "Aaron", ""], ["Niranga", "Senadhipathige S.", ""], ["Sehgal", "Vibhor", ""], ["Lin", "Jasmine", ""], ["Kristianto", "Leonardy", ""], ["Richmond-Fuller", "Angela", ""], ["Regino", "Jeff", ""], ["Chhibber", "Nalin", ""], ["Majeti", "Dinesh", ""], ["Sharma", "Sachin", ""], ["Mananova", "Kamila", ""], ["Dhakal", "Dinesh", ""], ["Dai", "William", ""], ["Purynova", "Victoria", ""], ["Sandeep", "Samarth", ""], ["Chandrakanthan", "Varshine", ""], ["Sarma", "Tejas", ""], ["Matin", "Sekandar", ""], ["Nasser", "Ahmed", ""], ["Nistala", "Rohit", ""], ["Stolzoff", "Alexander", ""], ["Milland", "Kristy", ""], ["Mathur", "Vinayak", ""], ["Vaish", "Rajan", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1904.06764", "submitter": "Lingheng Meng", "authors": "Lingheng Meng, Daiwei Lin, Adam Francey, Rob Gorbet, Philip Beesley,\n  Dana Kuli\\'c", "title": "Learning to Engage with Interactive Systems: A Field Study on Deep\n  Reinforcement Learning in a Public Museum", "comments": "29 pages, 19 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Physical agents that can autonomously generate engaging, life-like behaviour\nwill lead to more responsive and interesting robots and other autonomous\nsystems. Although many advances have been made for one-to-one interactions in\nwell controlled settings, future physical agents should be capable of\ninteracting with humans in natural settings, including group interaction. In\norder to generate engaging behaviours, the autonomous system must first be able\nto estimate its human partners' engagement level. In this paper, we propose an\napproach for estimating engagement during group interaction by simultaneously\ntaking into account active and passive interaction, i.e. occupancy, and use the\nmeasure as the reward signal within a reinforcement learning framework to learn\nengaging interactive behaviours. The proposed approach is implemented in an\ninteractive sculptural system in a museum setting. We compare the learning\nsystem to a baseline using pre-scripted interactive behaviours. Analysis based\non sensory data and survey data shows that adaptable behaviours within an\nexpert-designed action space can achieve higher engagement and likeability.\n", "versions": [{"version": "v1", "created": "Sun, 14 Apr 2019 21:33:53 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 17:46:13 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Meng", "Lingheng", ""], ["Lin", "Daiwei", ""], ["Francey", "Adam", ""], ["Gorbet", "Rob", ""], ["Beesley", "Philip", ""], ["Kuli\u0107", "Dana", ""]]}, {"id": "1904.07333", "submitter": "Michael Bossetta", "authors": "Michael Bossetta", "title": "The Digital Architectures of Social Media: Comparing Political\n  Campaigning on Facebook, Twitter, Instagram, and Snapchat in the 2016 U.S.\n  Election", "comments": null, "journal-ref": "Journalism & Mass Communication Quarterly 95(2) 2018", "doi": "10.1177/1077699018763307", "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The present study argues that political communication on social media is\nmediated by a platform's digital architecture, defined as the technical\nprotocols that enable, constrain, and shape user behavior in a virtual space. A\nframework for understanding digital architectures is introduced, and four\nplatforms (Facebook, Twitter, Instagram, and Snapchat) are compared along the\ntypology. Using the 2016 US election as a case, interviews with three\nRepublican digital strategists are combined with social media data to qualify\nthe studyies theoretical claim that a platform's network structure,\nfunctionality, algorithmic filtering, and datafication model affect political\ncampaign strategy on social media.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2019 21:34:31 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Bossetta", "Michael", ""]]}, {"id": "1904.07765", "submitter": "Oznur Alkan", "authors": "Oznur Alkan, Elizabeth M. Daly, Adi Botea", "title": "An Evaluation Framework for Interactive Recommender System", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional recommender systems present a relatively static list of\nrecommendations to a user where the feedback is typically limited to an\naccept/reject or a rating model. However, these simple modes of feedback may\nonly provide limited insights as to why a user likes or dislikes an item and\nwhat aspects of the item the user has considered. Interactive recommender\nsystems present an opportunity to engage the user in the process by allowing\nthem to interact with the recommendations, provide feedback and impact the\nresults in real-time. Evaluation of the impact of the user interaction\ntypically requires an extensive user study which is time consuming and gives\nresearchers limited opportunities to tune their solutions without having to\nconduct multiple rounds of user feedback. Additionally, user experience and\ndesign aspects can have a significant impact on the user feedback which may\nresult in not necessarily assessing the quality of some of the underlying\nalgorithmic decisions in the overall solution. As a result, we present an\nevaluation framework which aims to simulate the users interacting with the\nrecommender. We formulate metrics to evaluate the quality of the interactive\nrecommenders which are outputted by the framework once simulation is completed.\nWhile simulation along is not sufficient to evaluate a complete solution, the\nresults can be useful to help researchers tune their solution before moving to\nthe user study stage.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:36:48 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Alkan", "Oznur", ""], ["Daly", "Elizabeth M.", ""], ["Botea", "Adi", ""]]}, {"id": "1904.07802", "submitter": "Quentin Debard", "authors": "Quentin Debard, Jilles Steeve Dibangoye, St\\'ephane Canu, Christian\n  Wolf", "title": "Learning 3D Navigation Protocols on Touch Interfaces with Cooperative\n  Multi-Agent Reinforcement Learning", "comments": "17 pages, 8 figures. Accepted at The European Conference on Machine\n  Learning and Principles and Practice of Knowledge Discovery in Databases 2019\n  (ECMLPKDD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using touch devices to navigate in virtual 3D environments such as computer\nassisted design (CAD) models or geographical information systems (GIS) is\ninherently difficult for humans, as the 3D operations have to be performed by\nthe user on a 2D touch surface. This ill-posed problem is classically solved\nwith a fixed and handcrafted interaction protocol, which must be learned by the\nuser. We propose to automatically learn a new interaction protocol allowing to\nmap a 2D user input to 3D actions in virtual environments using reinforcement\nlearning (RL). A fundamental problem of RL methods is the vast amount of\ninteractions often required, which are difficult to come by when humans are\ninvolved. To overcome this limitation, we make use of two collaborative agents.\nThe first agent models the human by learning to perform the 2D finger\ntrajectories. The second agent acts as the interaction protocol, interpreting\nand translating to 3D operations the 2D finger trajectories from the first\nagent. We restrict the learned 2D trajectories to be similar to a training set\nof collected human gestures by first performing state representation learning,\nprior to reinforcement learning. This state representation learning is\naddressed by projecting the gestures into a latent space learned by a\nvariational auto encoder (VAE).\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 16:33:04 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 20:44:46 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Debard", "Quentin", ""], ["Dibangoye", "Jilles Steeve", ""], ["Canu", "St\u00e9phane", ""], ["Wolf", "Christian", ""]]}, {"id": "1904.07986", "submitter": "Ahmed Fadhil Dr.", "authors": "Ahmed Fadhil", "title": "Beyond Technical Motives: Perceived User Behavior in Abandoning Wearable\n  Health & Wellness Trackers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Health trackers are widely adopted to support individuals with daily health\nand wellness activity tracking. They can help increase steps taken, enhance\nsleeping pattern, improve healthy diet, and promote the overall health. Despite\nthe growth in wearable adoption, their real-life use is still questionable.\nWhile some users derive long-term values from their trackers, others face\nbarriers to integrate it into their daily routine. Studies have analysed\ntechnical aspects of these barriers. In this paper, we analyse the behavioural\nfactors of discouragement and wearable abandonment strictly tied to user habits\nand lifestyle circumstances. A data analysis was conducted on 8 of the highly\nrated wearables for 2017. The analysis collected sale posts on Kijiji and\nGumtree, the second sales online retailers for both the Italian and UK market,\nrespectively. We extracted insights from the posts about user motives,\nhighlighted technology condition and limitations, and timeframe before the\nabandonment. The findings revealed certain user behavioural patterns when\nabandoning their wearables. In addition, analysing the posts showed other\nmotives for the posts and not strictly related to wearable abandonment.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 21:12:59 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Fadhil", "Ahmed", ""]]}, {"id": "1904.08009", "submitter": "Zhijiong Huang", "authors": "Zhijiong Huang, Yu Zhang, Kathryn C. Quigley, Ramya Sankar, Clemence\n  Wormser, Xinxin Mo, Allen Y. Yang", "title": "Accessibility of Virtual Reality Locomotion Modalities to Adults and\n  Minors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) is an important new technology that is fun-damentally\nchanging the way people experience entertainment and education content. Due to\nthe fact that most currently available VR products are one size fits all, the\naccessibility of the content design and user interface design, even for healthy\nchildren is not well understood. It requires more research to ensure that\nchildren can have equally good user compared to adults in VR. In our study, we\nseek to explore accessibility of locomotion in VR between healthy adults and\nminors along both objective and subjective dimensions. We performed a user\nexperience experiment where subjects completed a simple task of moving and\ntouching underwater animals in VR using one of four different locomotion\nmodalities, as well as real-world walking without wearing VR headsets as the\nbaseline. Our results show that physical body movement that mirrors real-world\nmovement exclusively is the least preferred by both adults and minors. However,\nwithin the different modalities of controller assisted locomotion there are\nvariations between adults and minors for preference and challenge levels.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 23:12:41 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Huang", "Zhijiong", ""], ["Zhang", "Yu", ""], ["Quigley", "Kathryn C.", ""], ["Sankar", "Ramya", ""], ["Wormser", "Clemence", ""], ["Mo", "Xinxin", ""], ["Yang", "Allen Y.", ""]]}, {"id": "1904.08066", "submitter": "Roghayeh Barmaki", "authors": "Zhang Guo, Kevin Yu, Rebecca Pearlman, Nassir Navab, and Roghayeh\n  Barmaki", "title": "Collaboration Analysis Using Deep Learning", "comments": "6 pages, 4 Figures and a Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the collaborative learning process is one of the growing\nfields of education research, which has many different analytic solutions. In\nthis paper, we provided a new solution to improve automated collaborative\nlearning analyses using deep neural networks. Instead of using self-reported\nquestionnaires, which are subject to bias and noise, we automatically extract\ngroup-working information by object recognition results using Mask R-CNN\nmethod. This process is based on detecting the people and other objects from\npictures and video clips of the collaborative learning process, then evaluate\nthe mobile learning performance using the collaborative indicators. We tested\nour approach to automatically evaluate the group-work collaboration in a\ncontrolled study of thirty-three dyads while performing an anatomy body\npainting intervention. The results indicate that our approach recognizes the\ndifferences of collaborations among teams of treatment and control groups in\nthe case study. This work introduces new methods for automated quality\nprediction of collaborations among human-human interactions using computer\nvision techniques.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 03:23:17 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Guo", "Zhang", ""], ["Yu", "Kevin", ""], ["Pearlman", "Rebecca", ""], ["Navab", "Nassir", ""], ["Barmaki", "Roghayeh", ""]]}, {"id": "1904.08490", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Huiying Li, Steven Nagels, Zhijing Li, Pedro Lopes, Ben Y.\n  Zhao, and Haitao Zheng", "title": "Understanding the Effectiveness of Ultrasonic Microphone Jammer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have explained the principle of using ultrasonic transmissions\nto jam nearby microphones. These signals are inaudible to nearby users, but\nleverage \"hardware nonlinearity\" to induce a jamming signal inside microphones\nthat disrupts voice recordings. This has great implications on audio privacy\nprotection. In this work, we gain a deeper understanding on the effectiveness\nof ultrasonic jammer under practical scenarios, with the goal of disabling both\nvisible and hidden microphones in the surrounding area. We first experiment\nwith existing jammer designs (both commercial products and that proposed by\nrecent papers), and find that they all offer limited angular coverage, and can\nonly target microphones in a particular direction. We overcome this limitation\nby building a circular transducer array as a wearable bracelet. It emits\nultrasonic signals simultaneously from many directions, targeting surrounding\nmicrophones without needing to point at any. More importantly, as the bracelet\nmoves with the wearer, its motion increases jamming coverage and diminishes\nblind spots (the fundamental problem facing any transducer array). We evaluate\nthe jammer bracelet under practical scenarios, confirming that it can\neffectively disrupt visible and hidden microphones in the surrounding areas,\npreventing recognition of recorded speech. We also identify limitations and\nareas for improvement.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 20:41:39 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Chen", "Yuxin", ""], ["Li", "Huiying", ""], ["Nagels", "Steven", ""], ["Li", "Zhijing", ""], ["Lopes", "Pedro", ""], ["Zhao", "Ben Y.", ""], ["Zheng", "Haitao", ""]]}, {"id": "1904.08505", "submitter": "Clebeson Santos Msc", "authors": "Clebeson Canuto dos Santos, Jorge Leonid Aching Samatelo, Raquel\n  Frizera Vassallo", "title": "Dynamic Gesture Recognition by Using CNNs and Star RGB: a Temporal\n  Information Condensation", "comments": "19 pages, 12 figures, submitted to Neurocomputing Journal", "journal-ref": null, "doi": "10.1016/j.neucom.2020.03.038", "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the advance of technologies, machines are increasingly present in\npeople's daily lives. Thus, there has been more and more effort to develop\ninterfaces, such as dynamic gestures, that provide an intuitive way of\ninteraction. Currently, the most common trend is to use multimodal data, as\ndepth and skeleton information, to enable dynamic gesture recognition. However,\nusing only color information would be more interesting, since RGB cameras are\nusually available in almost every public place, and could be used for gesture\nrecognition without the need of installing other equipment. The main problem\nwith such approach is the difficulty of representing spatio-temporal\ninformation using just color. With this in mind, we propose a technique capable\nof condensing a dynamic gesture, shown in a video, in just one RGB image. We\ncall this technique star RGB. This image is then passed to a classifier formed\nby two Resnet CNNs, a soft-attention ensemble, and a fully connected layer,\nwhich indicates the class of the gesture present in the input video.\nExperiments were carried out using both Montalbano and GRIT datasets. For\nMontalbano dataset, the proposed approach achieved an accuracy of 94.58%. Such\nresult reaches the state-of-the-art when considering this dataset and only\ncolor information. Regarding the GRIT dataset, our proposal achieves more than\n98% of accuracy, recall, precision, and F1-score, outperforming the reference\napproach by more than 6%.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 00:39:32 GMT"}, {"version": "v2", "created": "Sun, 8 Sep 2019 15:57:22 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Santos", "Clebeson Canuto dos", ""], ["Samatelo", "Jorge Leonid Aching", ""], ["Vassallo", "Raquel Frizera", ""]]}, {"id": "1904.08593", "submitter": "David McPherson", "authors": "Jesse Paterson, Jiwoong Han, Tom Cheng, Paxtan Laker, David McPherson,\n  Joseph Menke, Allen Yang", "title": "Improving Usability, Efficiency, and Safety of UAV Path Planning through\n  a Virtual Reality Interface", "comments": "7 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the capability and complexity of UAVs continue to increase, the\nhuman-robot interface community has a responsibility to design better ways of\nspecifying the complex 3D flight paths necessary for instructing them.\nImmersive interfaces, such as those afforded by virtual reality (VR), have\nseveral unique traits which may improve the user's ability to perceive and\nspecify 3D information. These traits include stereoscopic depth cues which\ninduce a sense of physical space as well as six degrees of freedom (DoF)\nnatural head-pose and gesture interactions. This work introduces an open-source\nplatform for 3D aerial path planning in VR and compares it to existing UAV\npiloting interfaces. Our study has found statistically significant improvements\nin safety and subjective usability over a manual control interface, while\nachieving a statistically significant efficiency improvement over a 2D\ntouchscreen interface. The results illustrate that immersive interfaces provide\na viable alternative to touchscreen interfaces for UAV path planning.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 05:07:34 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Paterson", "Jesse", ""], ["Han", "Jiwoong", ""], ["Cheng", "Tom", ""], ["Laker", "Paxtan", ""], ["McPherson", "David", ""], ["Menke", "Joseph", ""], ["Yang", "Allen", ""]]}, {"id": "1904.08610", "submitter": "Jan Egger", "authors": "Daniel Wild, Maximilian Weber, Jan Egger", "title": "Client/Server Based Online Environment for Manual Segmentation of\n  Medical Images", "comments": "8 pages", "journal-ref": "Proceedings of CESCG 2019: The 23rd Central European Seminar on\n  Computer Graphics", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Segmentation is a key step in analyzing and processing medical images. Due to\nthe low fault tolerance in medical imaging, manual segmentation remains the de\nfacto standard in this domain. Besides, efforts to automate the segmentation\nprocess often rely on large amounts of manually labeled data. While existing\nsoftware supporting manual segmentation is rich in features and delivers\naccurate results, the necessary time to set it up and get comfortable using it\ncan pose a hurdle for the collection of large datasets. This work introduces a\nclient/server based online environment, referred to as Studierfenster\n(studierfenster.at), that can be used to perform manual segmentations directly\nin a web browser. The aim of providing this functionality in the form of a web\napplication is to ease the collection of ground truth segmentation datasets.\nProviding a tool that is quickly accessible and usable on a broad range of\ndevices, offers the potential to accelerate this process. The manual\nsegmentation workflow of Studierfenster consists of dragging and dropping the\ninput file into the browser window and slice-by-slice outlining the object\nunder consideration. The final segmentation can then be exported as a file\nstoring its contours and as a binary segmentation mask. In order to evaluate\nthe usability of Studierfenster, a user study was performed. The user study\nresulted in a mean of 6.3 out of 7.0 possible points given by users, when asked\nabout their overall impression of the tool. The evaluation also provides\ninsights into the results achievable with the tool in practice, by presenting\ntwo ground truth segmentations performed by physicians.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:17:05 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Wild", "Daniel", ""], ["Weber", "Maximilian", ""], ["Egger", "Jan", ""]]}, {"id": "1904.08621", "submitter": "Guangliang Li", "authors": "Guangliang Li, Randy Gomez, Keisuke Nakamura, Jinying Lin, Qilei\n  Zhang, Bo He", "title": "Improving Interactive Reinforcement Agent Planning with Human\n  Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TAMER has proven to be a powerful interactive reinforcement learning method\nfor allowing ordinary people to teach and personalize autonomous agents'\nbehavior by providing evaluative feedback. However, a TAMER agent planning with\nUCT---a Monte Carlo Tree Search strategy, can only update states along its path\nand might induce high learning cost especially for a physical robot. In this\npaper, we propose to drive the agent's exploration along the optimal path and\nreduce the learning cost by initializing the agent's reward function via\ninverse reinforcement learning from demonstration. We test our proposed method\nin the RL benchmark domain---Grid World---with different discounts on human\nreward. Our results show that learning from demonstration can allow a TAMER\nagent to learn a roughly optimal policy up to the deepest search and encourage\nthe agent to explore along the optimal path. In addition, we find that learning\nfrom demonstration can improve the learning efficiency by reducing total\nfeedback, the number of incorrect actions and increasing the ratio of correct\nactions to obtain an optimal policy, allowing a TAMER agent to converge faster.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 07:45:36 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Li", "Guangliang", ""], ["Gomez", "Randy", ""], ["Nakamura", "Keisuke", ""], ["Lin", "Jinying", ""], ["Zhang", "Qilei", ""], ["He", "Bo", ""]]}, {"id": "1904.08751", "submitter": "EPTCS", "authors": "Walther Neuper (Graz University of Technology)", "title": "Technologies for \"Complete, Transparent & Interactive Models of Math\" in\n  Education", "comments": "In Proceedings ThEdu'18, arXiv:1903.12402", "journal-ref": "EPTCS 290, 2019, pp. 76-95", "doi": "10.4204/EPTCS.290.6", "report-no": null, "categories": "math.HO cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new generation of educational mathematics software is being shaped in ThEdu\nand other academic communities on the side of computer mathematics. Respective\nconcepts and technologies have been clarified to an extent, which calls for\ncooperation with educational sciences in order to optimise the new generation's\nimpact on educational practice. The paper addresses educational scientists who\nwant to examine specific software features and estimate respective effects in\nSTEM education at universities and subsequently at high-school. The key\nfeatures are characterised as a \"complete, transparent and interactive model of\nmathematics\", which offers interactive experience in all relevant aspects in\ndoing mathematics. Interaction uses several layers of formal languages: the\nlanguage of terms, of specifications, of proofs and of program language, which\nare connected by Lucas-Interpretation providing \"next-step-guidance\" as well as\nproviding prover power to check user input. So this paper is structured from\nthe point of view of computer mathematics and thus cannot give a serious\ndescription of effects on educational practice -- this is up to collaboration\nwith educational science; such collaboration is prepared by a series of\nquestions, some of which are biased towards software usability (and mainly to\nbe solved by computer mathematicians) and some of which are biased towards\ngenuine research in educational sciences.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2019 07:54:15 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Neuper", "Walther", "", "Graz University of Technology"]]}, {"id": "1904.08842", "submitter": "Ruihan Yang", "authors": "Ruihan Yang, Tianyao Chen, Yiyi Zhang and Gus Xia", "title": "Inspecting and Interacting with Meaningful Music Representations using\n  VAE", "comments": "Accepted for poster at the International Conference on New Interfaces\n  for Musical Expression (NIME), June 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.IR cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Autoencoders(VAEs) have already achieved great results on image\ngeneration and recently made promising progress on music generation. However,\nthe generation process is still quite difficult to control in the sense that\nthe learned latent representations lack meaningful music semantics. It would be\nmuch more useful if people can modify certain music features, such as rhythm\nand pitch contour, via latent representations to test different composition\nideas. In this paper, we propose a new method to inspect the pitch and rhythm\ninterpretations of the latent representations and we name it disentanglement by\naugmentation. Based on the interpretable representations, an intuitive\ngraphical user interface is designed for users to better direct the music\ncreation process by manipulating the pitch contours and rhythmic complexity.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:22:33 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Yang", "Ruihan", ""], ["Chen", "Tianyao", ""], ["Zhang", "Yiyi", ""], ["Xia", "Gus", ""]]}, {"id": "1904.08854", "submitter": "Fernando Garcia", "authors": "Fernando Garcia, Amit Kumar Pandey and Charles Fattal", "title": "Wait for me! Towards socially assistive walk companions", "comments": "2nd Workshop on Social Robots in Therapy and Care. 14th ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI 2019)", "journal-ref": null, "doi": null, "report-no": "SREC/2019/01", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the present study involves designing a humanoid robot guide as a\nwalking trainer for elderly and rehabilitation patients. The system is based on\nthe humanoid robot Pepper with a compliance approach that allows to match the\nmotion intention of the user to the robot's pace. This feasibility study is\nbacked up by an experimental evaluation conducted in a rehabilitation centre.\nWe hypothesize that Pepper robot used as an assistive partner, can also benefit\nelderly users by motivating them to perform physical activity.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 15:56:14 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Garcia", "Fernando", ""], ["Pandey", "Amit Kumar", ""], ["Fattal", "Charles", ""]]}, {"id": "1904.09044", "submitter": "Subhashis Hazarika", "authors": "Subhashis Hazarika, Haoyu Li, Ko-Chih Wang, Han-Wei Shen and\n  Ching-Shan Chou", "title": "NNVA: Neural Network Assisted Visual Analysis of Yeast Cell Polarization\n  Simulation", "comments": "Published at IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934591", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex computational models are often designed to simulate real-world\nphysical phenomena in many scientific disciplines. However, these simulation\nmodels tend to be computationally very expensive and involve a large number of\nsimulation input parameters which need to be analyzed and properly calibrated\nbefore the models can be applied for real scientific studies. We propose a\nvisual analysis system to facilitate interactive exploratory analysis of\nhigh-dimensional input parameter space for a complex yeast cell polarization\nsimulation. The proposed system can assist the computational biologists, who\ndesigned the simulation model, to visually calibrate the input parameters by\nmodifying the parameter values and immediately visualizing the predicted\nsimulation outcome without having the need to run the original expensive\nsimulation for every instance. Our proposed visual analysis system is driven by\na trained neural network-based surrogate model as the backend analysis\nframework. Surrogate models are widely used in the field of simulation sciences\nto efficiently analyze computationally expensive simulation models. In this\nwork, we demonstrate the advantage of using neural networks as surrogate models\nfor visual analysis by incorporating some of the recent advances in the field\nof uncertainty quantification, interpretability and explainability of neural\nnetwork-based models. We utilize the trained network to perform interactive\nparameter sensitivity analysis of the original simulation at multiple\nlevels-of-detail as well as recommend optimal parameter configurations using\nthe activation maximization framework of neural networks. We also facilitate\ndetail analysis of the trained network to extract useful insights about the\nsimulation model, learned by the network, during the training process.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 00:36:27 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 03:21:57 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 20:41:28 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Hazarika", "Subhashis", ""], ["Li", "Haoyu", ""], ["Wang", "Ko-Chih", ""], ["Shen", "Han-Wei", ""], ["Chou", "Ching-Shan", ""]]}, {"id": "1904.09100", "submitter": "Garima Bajwa", "authors": "Garima Bajwa, Mohamed Fazeen and Ram Dantu", "title": "Detecting driver distraction using stimuli-response EEG analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting driver distraction is a significant concern for future intelligent\ntransportation systems. We present a new approach for identifying distracted\ndriving behavior by evaluating a stimulus and response interaction with the\nbrain signals in two ways. First, measuring the driver response through EEG by\ncreating various types of distraction stimuli such as reading, texting, calling\nand using phone camera (risk odds ratio of these activities determined by NHTSA\nstudy). Second, using a survey, comparing driver's order/perception of severity\nof distraction with the derived distraction index from EEG bands. A 14\nelectrodes headset was used to record the brain signals while driving in the\npilot study with two subjects and a single dry electrode headset with 13\nsubjects in the main study. We used a naturalistic driving study as opposed to\na virtual reality driving simulator to perform the distracted driving\nmaneuvers, consisting of over 100 short duration trials (three to five seconds)\nfor a subject. We overcame a big challenge in EEG analysis - reducing the\nnumber of electrodes by isolating one electrode (FC5) from 14 electrode\nlocations to identify certain distractions. Our machine learning methods\nachieved a mean accuracy (averaged over the subjects and tasks) of 91.54 +/-\n5.23% to detect a distracted driving event and 76.99 +/- 8.63% to distinguish\nbetween the five distraction cases in our study (read, text, call, and\nsnapshot) using a single electrode. The quantification of distracted driving\ndetailed in this paper is necessary to guide future policies in road safety.\nOur system addresses the safety concerns resulting from driver distraction and\naims to bring about behavioral changes in drivers.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 07:09:32 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Bajwa", "Garima", ""], ["Fazeen", "Mohamed", ""], ["Dantu", "Ram", ""]]}, {"id": "1904.09111", "submitter": "Gregoire Cattan", "authors": "Erwan Vaineau (GIPSA-VIBS), Alexandre Barachant (GIPSA-VIBS), Anton\n  Andreev (GIPSA-Services), Pedro C. Rodrigues (GIPSA-VIBS), Gr\\'egoire Cattan\n  (GIPSA-VIBS, IHMTEK), Marco Congedo (GIPSA-VIBS)", "title": "Brain Invaders Adaptive versus Non-Adaptive P300 Brain-Computer\n  Interface dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the experimental procedures for a dataset that we have made\npublicly available at https://doi.org/10.5281/zenodo.1494163 in mat and csv\nformats. This dataset contains electroencephalographic (EEG) recordings of 24\nsubjects doing a visual P300 Brain-Computer Interface experiment on PC. The\nvisual P300 is an event-related potential elicited by visual stimulation,\npeaking 240-600 ms after stimulus onset. The experiment was designed in order\nto compare the use of a P300-based brain-computer interface on a PC with and\nwithout adaptive calibration using Riemannian geometry. The brain-computer\ninterface is based on electroencephalography (EEG). EEG data were recorded\nthanks to 16 electrodes. Data were recorded during an experiment taking place\nin the GIPSA-lab, Grenoble, France, in 2013 (Congedo, 2013). Python code for\nmanipulating the data is available at\nhttps://github.com/plcrodrigues/py.BI.EEG.2013-GIPSA. The ID of this dataset is\nBI.EEG.2013-GIPSA.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:02:46 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Vaineau", "Erwan", "", "GIPSA-VIBS"], ["Barachant", "Alexandre", "", "GIPSA-VIBS"], ["Andreev", "Anton", "", "GIPSA-Services"], ["Rodrigues", "Pedro C.", "", "GIPSA-VIBS"], ["Cattan", "Gr\u00e9goire", "", "GIPSA-VIBS, IHMTEK"], ["Congedo", "Marco", "", "GIPSA-VIBS"]]}, {"id": "1904.09115", "submitter": "Di Hu", "authors": "Di Hu, Dong Wang, Xuelong Li, Feiping Nie, Qi Wang", "title": "Listen to the Image", "comments": "Accepted by CVPR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual-to-auditory sensory substitution devices can assist the blind in\nsensing the visual environment by translating the visual information into a\nsound pattern. To improve the translation quality, the task performances of the\nblind are usually employed to evaluate different encoding schemes. In contrast\nto the toilsome human-based assessment, we argue that machine model can be also\ndeveloped for evaluation, and more efficient. To this end, we firstly propose\ntwo distinct cross-modal perception model w.r.t. the late-blind and\ncongenitally-blind cases, which aim to generate concrete visual contents based\non the translated sound. To validate the functionality of proposed models, two\nnovel optimization strategies w.r.t. the primary encoding scheme are presented.\nFurther, we conduct sets of human-based experiments to evaluate and compare\nthem with the conducted machine-based assessments in the cross-modal generation\ntask. Their highly consistent results w.r.t. different encoding schemes\nindicate that using machine model to accelerate optimization evaluation and\nreduce experimental cost is feasible to some extent, which could dramatically\npromote the upgrading of encoding scheme then help the blind to improve their\nvisual perception ability.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:13:34 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Hu", "Di", ""], ["Wang", "Dong", ""], ["Li", "Xuelong", ""], ["Nie", "Feiping", ""], ["Wang", "Qi", ""]]}, {"id": "1904.09116", "submitter": "Fernando Garcia", "authors": "Arturo Cruz-Maya, Fernando Garcia and Amit Kumar Pandey", "title": "Enabling Socially Competent navigation through incorporating HRI", "comments": "HRI '19: ACM Workshop on Social Human-Robot Interaction of Human-care\n  Service Robots, March 11--14, 2019, Daegu, Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, social robots have been deployed in public environments\nmaking evident the need of human-aware navigation capabilities. In this regard,\nthe robotics community have made efforts to include proxemics or social\nconventions within the navigation approaches. Nevertheless, few works have\ntackled the problem of labelling humans as an interactive agent when blocking\nthe robot motion trajectory. Current state of the art navigation planners will\neither propose an alternative path or freeze the motion until the path is free.\nWe present the first prototype of a framework designed to enhance social\ncompetency of robots while navigating in indoor environments. The\nimplementation is done using Navigation and Object Detection open-source\nsoftware. Specifically, the Robot Operating System (ROS) navigation stack, and\nOpenCV with Caffe deep learning models and MobileNet Single Shot Detector\n(SSD), respectively.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 08:21:06 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Cruz-Maya", "Arturo", ""], ["Garcia", "Fernando", ""], ["Pandey", "Amit Kumar", ""]]}, {"id": "1904.09435", "submitter": "Mingfei Sun", "authors": "Mingfei Sun, Yiqing Mou, Hongwen Xie, Meng Xia, Michelle Wong, and\n  Xiaojuan Ma", "title": "Estimating Emotional Intensity from Body Poses for Human-Robot\n  Interaction", "comments": "2018 IEEE International Conference on Systems, Man, and Cybernetics\n  (SMC2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equipping social and service robots with the ability to perceive human\nemotional intensities during an interaction is in increasing demand. Most of\nexisting work focuses on determining which emotion(s) participants are\nexpressing from facial expressions but largely overlooks the emotional\nintensities spontaneously revealed by other social cues, especially body\nlanguages. In this paper, we present a real-time method for robots to capture\nfluctuations of participants' emotional intensities from their body poses.\nUnlike conventional joint-position-based approaches, our method adopts local\njoint transformations as pose descriptors which are invariant to subject body\ndifferences as well as the pose sensor positions. In addition, we use a Long\nShort-Term Memory Recurrent Neural Network (LSTM-RNN) architecture to take the\nspecific emotion context into account when estimating emotional intensities\nfrom body poses. The dataset evaluation suggests that the proposed method is\neffective and performs better than baseline method on the test dataset. Also, a\nseries of succeeding field tests on a physical robot demonstrates that the\nproposed method effectively estimates subjects emotional intensities in\nreal-time. Furthermore, the robot equipped with our method is perceived to be\nmore emotion-sensitive and more emotionally intelligent.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 11:23:43 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Sun", "Mingfei", ""], ["Mou", "Yiqing", ""], ["Xie", "Hongwen", ""], ["Xia", "Meng", ""], ["Wong", "Michelle", ""], ["Ma", "Xiaojuan", ""]]}, {"id": "1904.09529", "submitter": "Kevin Karsch", "authors": "Mark A. Livingston, Zhuming Ai, Kevin Karsch, Gregory O. Gibson", "title": "User interface design for military AR applications", "comments": null, "journal-ref": null, "doi": "10.1007/s10055-010-0179-1", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a user interface for military situation awareness presents\nchallenges for managing information in a useful and usable manner. We present\nan integrated set of functions for the presentation of and interaction with\ninformation for a mobile augmented reality application for military\napplications. Our research has concentrated on four areas. We filter\ninformation based on relevance to the user (in turn based on location),\nevaluate methods for presenting information that represents entities occluded\nfrom the user's view, enable interaction through a top-down map view metaphor\nakin to current techniques used in the military, and facilitate collaboration\nwith other mobile users and/or a command center. In addition, we refined the\nuser interface architecture to conform to requirements from subject matter\nexperts. We discuss the lessons learned in our work and directions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 02:10:15 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Livingston", "Mark A.", ""], ["Ai", "Zhuming", ""], ["Karsch", "Kevin", ""], ["Gibson", "Gregory O.", ""]]}, {"id": "1904.09612", "submitter": "Qian Yang", "authors": "Qian Yang, Aaron Steinfeld, John Zimmerman", "title": "Unremarkable AI: Fitting Intelligent Decision Support into Critical,\n  Clinical Decision-Making Processes", "comments": null, "journal-ref": "CHI Conference on Human Factors in Computing Systems Proceedings\n  2019 (CHI'19)", "doi": "10.1145/3290605.3300468", "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical decision support tools (DST) promise improved healthcare outcomes by\noffering data-driven insights. While effective in lab settings, almost all DSTs\nhave failed in practice. Empirical research diagnosed poor contextual fit as\nthe cause. This paper describes the design and field evaluation of a radically\nnew form of DST. It automatically generates slides for clinicians' decision\nmeetings with subtly embedded machine prognostics. This design took inspiration\nfrom the notion of \"Unremarkable Computing\", that by augmenting the users'\nroutines technology/AI can have significant importance for the users yet remain\nunobtrusive. Our field evaluation suggests clinicians are more likely to\nencounter and embrace such a DST. Drawing on their responses, we discuss the\nimportance and intricacies of finding the right level of unremarkableness in\nDST design, and share lessons learned in prototyping critical AI systems as a\nsituated experience.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2019 14:57:44 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Yang", "Qian", ""], ["Steinfeld", "Aaron", ""], ["Zimmerman", "John", ""]]}, {"id": "1904.09818", "submitter": "Artur Andrzejak", "authors": "Artur Andrzejak, Oliver Wenz, Diego Costa", "title": "One DSL to Rule Them All: IDE-Assisted Code Generation for Agile Data\n  Analysis", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DC cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysis is at the core of scientific studies, a prominent task that\nresearchers and practitioners typically undertake by programming their own set\nof automated scripts. While there is no shortage of tools and languages\navailable for designing data analysis pipelines, users spend substantial effort\nin learning the specifics of such languages/tools and often design solutions\ntoo project specific to be reused in future studies. Furthermore, users need to\nput further effort into making their code scalable, as parallel implementations\nare typically more complex.\n  We address these problems by proposing an advanced code recommendation tool\nwhich facilitates developing data science scripts. Users formulate their\nintentions in a human-readable Domain Specific Language (DSL) for dataframe\nmanipulation and analysis. The DSL statements can be converted into executable\nPython code during editing. To avoid the need to learn the DSL and increase\nuser-friendliness, our tool supports code completion in mainstream IDEs and\neditors. Moreover, DSL statements can generate executable code for different\ndata analysis frameworks (currently we support Pandas and PySpark). Overall,\nour approach attempts to accelerate programming of common data analysis tasks\nand to facilitate the conversion of the implementations between frameworks.\n  In a preliminary assessment based on a popular data processing tutorial, our\ntool was able to fully cover 9 out of 14 processing steps for Pandas and 10 out\nof 16 for PySpark, while partially covering 4 processing steps for each of the\nframeworks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2019 13:10:59 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Andrzejak", "Artur", ""], ["Wenz", "Oliver", ""], ["Costa", "Diego", ""]]}, {"id": "1904.09884", "submitter": "Ahmed Fadhil Dr.", "authors": "Ahmed Fadhil, Yunlong Wang", "title": "Health Behaviour Change Techniques in Diabetes Management Applications:\n  A Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The rapid growth in mobile healthcare technology could significantly help\ncontrol chronic diseases, such as diabetes. This paper presents a systematic\nreview to characterise type 1 & type 2 diabetes management applications\navailable in Apple's iTunes store. We investigated \"Health & Fitness\" and\n\"Medical\" apps following a two-step filtering process (Selection and Analysis\nphases). We firstly investigated the apps compliance to the persuasive system\ndesign (PSD) model. We then characterised the behaviour change techniques\n(BCTs) of top-ranked apps for diabetes management. Finally, we checked the apps\nregarding the stages of disease continuum. The findings revealed apps\nincorporation some PSD principles based on their configuration and behaviour\nchange techniques. Most apps miss the element of BCT and focus on measuring\nexercise and caloric intake. Few apps consider managing specific diabetes type,\nwhich raises doubts about the effectiveness of those apps in providing\nsustainable diabetes management. Moreover, people may need multiple apps to\ninitiate and maintain a healthy behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2019 14:03:03 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Fadhil", "Ahmed", ""], ["Wang", "Yunlong", ""]]}, {"id": "1904.10176", "submitter": "Songlin Xu", "authors": "Songlin Xu and Jiacheng Zhu", "title": "Estimating Risk Levels of Driving Scenarios through Analysis of Driving\n  Styles for Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to operate safely on the road, autonomous vehicles need not only to\nbe able to identify objects in front of them, but also to be able to estimate\nthe risk level of the object in front of the vehicle automatically. It is\nobvious that different objects have different levels of danger to autonomous\nvehicles. An evaluation system is needed to automatically determine the danger\nlevel of the object for the autonomous vehicle. It would be too subjective and\nincomplete if the system were completely defined by humans. Based on this, we\npropose a framework based on nonparametric Bayesian learning method -- a sticky\nhierarchical Dirichlet process hidden Markov model(sticky HDP-HMM), and\ndiscover the relationship between driving scenarios and driving styles. We use\nthe analysis of driving styles of autonomous vehicles to reflect the risk\nlevels of driving scenarios to the vehicles. In this framework, we firstly use\nsticky HDP-HMM to extract driving styles from the dataset and get different\nclusters, then an evaluation system is proposed to evaluate and rank the\nurgency levels of the clusters. Finally, we map the driving scenarios to the\nranking results and thus get clusters of driving scenarios in different risk\nlevels. More importantly, we find the relationship between driving scenarios\nand driving styles. The experiment shows that our framework can cluster and\nrank driving styles of different urgency levels and find the relationship\nbetween driving scenarios and driving styles and the conclusions also fit\npeople's common sense when driving. Furthermore, this framework can be used for\nautonomous vehicles to estimate risk levels of driving scenarios and help them\nmake precise and safe decisions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 06:55:48 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Xu", "Songlin", ""], ["Zhu", "Jiacheng", ""]]}, {"id": "1904.10351", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan, Nishant Kumar, Pankaj Kumar Sahu, Venkanna\n  Udutalapally", "title": "Drishtikon: An advanced navigational aid system for visually impaired\n  people", "comments": "Pre-print of the presented article at IEEE Conference on Information\n  and Communication Technology (CICT-2018), 6 Pages, 7 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, many of the aid systems deployed for visually impaired people are\nmostly made for a single purpose. Be it navigation, object detection, or\ndistance perceiving. Also, most of the deployed aid systems use indoor\nnavigation which requires a pre-knowledge of the environment. These aid systems\noften fail to help visually impaired people in the unfamiliar scenario. In this\npaper, we propose an aid system developed using object detection and depth\nperceivement to navigate a person without dashing into an object. The prototype\ndeveloped detects 90 different types of objects and compute their distances\nfrom the user. We also, implemented a navigation feature to get input from the\nuser about the target destination and hence, navigate the impaired person to\nhis/her destination using Google Directions API. With this system, we built a\nmulti-feature, high accuracy navigational aid system which can be deployed in\nthe wild and help the visually impaired people in their daily life by\nnavigating them effortlessly to their desired destination.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:23:49 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Kotyan", "Shashank", ""], ["Kumar", "Nishant", ""], ["Sahu", "Pankaj Kumar", ""], ["Udutalapally", "Venkanna", ""]]}, {"id": "1904.10354", "submitter": "Shashank Kotyan", "authors": "Shashank Kotyan, Nishant Kumar, Pankaj Kumar Sahu, Venkanna\n  Udutalapally", "title": "HAUAR: Home Automation Using Action Recognition", "comments": "Pre-print of the presented article at IEEE Conference on Information\n  and Communication Technology (CICT-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, many of the home automation systems deployed are mostly controlled by\nhumans. This control by humans restricts the automation of home appliances to\nan extent. Also, most of the deployed home automation systems use the Internet\nof Things technology to control the appliances. In this paper, we propose a\nsystem developed using action recognition to fully automate the home\nappliances. We recognize the three actions of a person (sitting, standing and\nlying) along with the recognition of an empty room. The accuracy of the system\nwas 90% in the real-life test experiments. With this system, we remove the\nhuman intervention in home automation systems for controlling the home\nappliances and at the same time we ensure the data privacy and reduce the\nenergy consumption by efficiently and optimally using home appliances.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 14:26:18 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 04:50:47 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Kotyan", "Shashank", ""], ["Kumar", "Nishant", ""], ["Sahu", "Pankaj Kumar", ""], ["Udutalapally", "Venkanna", ""]]}, {"id": "1904.10500", "submitter": "Eda Okur", "authors": "Eda Okur, Shachi H Kumar, Saurav Sahay, Asli Arslan Esme, Lama Nachman", "title": "Natural Language Interactions in Autonomous Vehicles: Intent Detection\n  and Slot Filling from Passenger Utterances", "comments": "Accepted and presented as a full paper at 20th International\n  Conference on Computational Linguistics and Intelligent Text Processing\n  (CICLing 2019), April 7-13, 2019, La Rochelle, France", "journal-ref": "Springer LNCS Proceedings for CICLing 2019", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding passenger intents and extracting relevant slots are important\nbuilding blocks towards developing contextual dialogue systems for natural\ninteractions in autonomous vehicles (AV). In this work, we explored AMIE\n(Automated-vehicle Multi-modal In-cabin Experience), the in-cabin agent\nresponsible for handling certain passenger-vehicle interactions. When the\npassengers give instructions to AMIE, the agent should parse such commands\nproperly and trigger the appropriate functionality of the AV system. In our\ncurrent explorations, we focused on AMIE scenarios describing usages around\nsetting or changing the destination and route, updating driving behavior or\nspeed, finishing the trip and other use-cases to support various natural\ncommands. We collected a multi-modal in-cabin dataset with multi-turn dialogues\nbetween the passengers and AMIE using a Wizard-of-Oz scheme via a realistic\nscavenger hunt game activity. After exploring various recent Recurrent Neural\nNetworks (RNN) based techniques, we introduced our own hierarchical joint\nmodels to recognize passenger intents along with relevant slots associated with\nthe action to be performed in AV scenarios. Our experimental results\noutperformed certain competitive baselines and achieved overall F1 scores of\n0.91 for utterance-level intent detection and 0.96 for slot filling tasks. In\naddition, we conducted initial speech-to-text explorations by comparing\nintent/slot models trained and tested on human transcriptions versus noisy\nAutomatic Speech Recognition (ASR) outputs. Finally, we compared the results\nwith single passenger rides versus the rides with multiple passengers.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 19:13:51 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Okur", "Eda", ""], ["Kumar", "Shachi H", ""], ["Sahay", "Saurav", ""], ["Esme", "Asli Arslan", ""], ["Nachman", "Lama", ""]]}, {"id": "1904.11007", "submitter": "Shili Sheng", "authors": "Shili Sheng, Erfan Pakdamanian, Kyungtae Han, BaekGyu Kim, Prashant\n  Tiwari, Inki Kim, Lu Feng", "title": "A Case Study of Trust on Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous vehicles have benefited the society, understanding the dynamic\nchange of humans' trust during human-autonomous vehicle interaction can help to\nimprove the safety and performance of autonomous driving. We designed and\nconducted a human subjects study involving 19 participants. Each participant\nwas asked to enter their trust level in a Likert scale in real-time during\nexperiments on a driving simulator. We also collected physiological data (e.g.,\nheart rate, pupil size) of participants as complementary indicators of trust.\nWe used analysis of variance (ANOVA) and Signal Temporal Logic (STL) to analyze\nthe experimental data. Our results show the influence of different factors\n(e.g., automation alarms, weather conditions) on trust, and the individual\nvariability in human reaction time and trust change.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 15:15:44 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 16:30:26 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Sheng", "Shili", ""], ["Pakdamanian", "Erfan", ""], ["Han", "Kyungtae", ""], ["Kim", "BaekGyu", ""], ["Tiwari", "Prashant", ""], ["Kim", "Inki", ""], ["Feng", "Lu", ""]]}, {"id": "1904.11008", "submitter": "Arash Kalatian", "authors": "Arash Kalatian, Bilal Farooq", "title": "DeepWait: Pedestrian Wait Time Estimation in Mixed Traffic Conditions\n  Using Deep Survival Analysis", "comments": "Accepted for publication in the proceedings of IEEE Intelligent\n  Transportation Systems Conference - ITSC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pedestrian's road crossing behaviour is one of the important aspects of urban\ndynamics that will be affected by the introduction of autonomous vehicles. In\nthis study we introduce DeepSurvival, a novel framework for estimating\npedestrian's waiting time at unsignalized mid-block crosswalks in mixed traffic\nconditions. We exploit the strengths of deep learning in capturing the\nnonlinearities in the data and develop a cox proportional hazard model with a\ndeep neural network as the log-risk function. An embedded feature selection\nalgorithm for reducing data dimensionality and enhancing the interpretability\nof the network is also developed. We test our framework on a dataset collected\nfrom 160 participants using an immersive virtual reality environment.\nValidation results showed that with a C-index of 0.64 our proposed framework\noutperformed the standard cox proportional hazard-based model with a C-index of\n0.58.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2019 00:04:11 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 01:39:17 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 20:19:10 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Kalatian", "Arash", ""], ["Farooq", "Bilal", ""]]}, {"id": "1904.11412", "submitter": "Ahmed Fadhil Dr.", "authors": "Ahmed Fadhil", "title": "Assistive System in Conversational Agent for Health Coaching: The\n  CoachAI Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With increasing physicians' workload and patients' needs for care, there is a\nneed for technology that facilitates physicians work and performs continues\nfollow-up with patients. Existing approaches focus merely on improving\npatient's condition, and none have considered managing physician's workload.\nThis paper presents an initial evaluation of a conversational agent assisted\ncoaching platform intended to manage physicians' fatigue and provide continuous\nfollow-up to patients. We highlight the approach adapted to build the chatbot\ndialogue and the coaching platform. We will particularly discuss the activity\nrecommender algorithms used to suggest insights about patients' condition and\nactivities based on previously collected data. The paper makes three\ncontributions: (1) present the conversational agent as an assistive virtual\ncoach, (2) decrease physicians workload and continuous follow up with patients,\nall by handling some repetitive physician tasks and performing initial follow\nup with the patient, (3) present the activity recommender that tracks previous\nactivities and patient information and provides useful insights about possible\nactivity and patient match to the coach. Future work focuses on integrating the\nrecommender model with the CoachAI platform and test the prototype with\npatient's in collaboration with an ambulatory clinic.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 15:39:49 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Fadhil", "Ahmed", ""]]}, {"id": "1904.11608", "submitter": "Yao Ma", "authors": "Yao Ma, Alex Olshevsky, Venkatesh Saligrama, Csaba Szepesvari", "title": "Gradient Descent for Sparse Rank-One Matrix Completion for Crowd-Sourced\n  Aggregation of Sparsely Interacting Workers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider worker skill estimation for the single-coin Dawid-Skene\ncrowdsourcing model. In practice, skill-estimation is challenging because\nworker assignments are sparse and irregular due to the arbitrary and\nuncontrolled availability of workers. We formulate skill estimation as a\nrank-one correlation-matrix completion problem, where the observed components\ncorrespond to observed label correlations between workers. We show that the\ncorrelation matrix can be successfully recovered and skills are identifiable if\nand only if the sampling matrix (observed components) does not have a bipartite\nconnected component. We then propose a projected gradient descent scheme and\nshow that skill estimates converge to the desired global optima for such\nsampling matrices. Our proof is original and the results are surprising in\nlight of the fact that even the weighted rank-one matrix factorization problem\nis NP-hard in general. Next, we derive sample complexity bounds in terms of\nspectral properties of the signless Laplacian of the sampling matrix. Our\nproposed scheme achieves state-of-art performance on a number of real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 22:09:58 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 10:01:19 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ma", "Yao", ""], ["Olshevsky", "Alex", ""], ["Saligrama", "Venkatesh", ""], ["Szepesvari", "Csaba", ""]]}, {"id": "1904.11630", "submitter": "Abbas Ganji", "authors": "Abbas Ganji, Negin Alimohammadi and Scott Miles", "title": "Challenges in Community Resilience Planning and Opportunities with\n  Simulation Modeling", "comments": null, "journal-ref": "ISCRAM 2019", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of community resilience has become increasingly recognized in\nemergency management and post-disaster community well-being. To this end, three\nseismic resilience planning initiatives have been conducted in the U.S. in the\nlast decade to envision the current state of community resilience. Experts who\nparticipated in these initiatives confronted challenges that must be addressed\nfor future planning initiatives. We interviewed eighteen participants to learn\nabout the community resilience planning process, its characteristics, and\nchallenges. Conducting qualitative content analysis, we identify six main\nchallenges to community resilience planning: complex network systems;\ninterdependencies among built environment systems; inter-organizational\ncollaboration; connections between the built environment and social systems;\ncommunications between built environment and social institutions' experts; and\ncommunication among decision-makers, social stakeholders, and community\nmembers. To overcome the identified challenges, we discuss the capability of\nhuman-centered simulation modeling as a combination of simulation modeling and\nhuman-centered design to facilitate community resilience planning.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 00:32:40 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Ganji", "Abbas", ""], ["Alimohammadi", "Negin", ""], ["Miles", "Scott", ""]]}, {"id": "1904.11652", "submitter": "Bum Chul Kwon", "authors": "Bum Chul Kwon, Vibha Anand, Kristen A Severson, Soumya Ghosh, Zhaonan\n  Sun, Brigitte I Frohnert, Markus Lundgren, Kenney Ng", "title": "DPVis: Visual Analytics with Hidden Markov Models for Disease\n  Progression Pathways", "comments": "to appear at IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2020.2985689", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical researchers use disease progression models to understand patient\nstatus and characterize progression patterns from longitudinal health records.\nOne approach for disease progression modeling is to describe patient status\nusing a small number of states that represent distinctive distributions over a\nset of observed measures. Hidden Markov models (HMMs) and its variants are a\nclass of models that both discover these states and make inferences of health\nstates for patients. Despite the advantages of using the algorithms for\ndiscovering interesting patterns, it still remains challenging for medical\nexperts to interpret model outputs, understand complex modeling parameters, and\nclinically make sense of the patterns. To tackle these problems, we conducted a\ndesign study with clinical scientists, statisticians, and visualization\nexperts, with the goal to investigate disease progression pathways of chronic\ndiseases, namely type 1 diabetes (T1D), Huntington's disease, Parkinson's\ndisease, and chronic obstructive pulmonary disease (COPD). As a result, we\nintroduce DPVis which seamlessly integrates model parameters and outcomes of\nHMMs into interpretable and interactive visualizations. In this study, we\ndemonstrate that DPVis is successful in evaluating disease progression models,\nvisually summarizing disease states, interactively exploring disease\nprogression patterns, and building, analyzing, and comparing clinically\nrelevant patient subgroups.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 02:30:32 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 16:26:43 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Kwon", "Bum Chul", ""], ["Anand", "Vibha", ""], ["Severson", "Kristen A", ""], ["Ghosh", "Soumya", ""], ["Sun", "Zhaonan", ""], ["Frohnert", "Brigitte I", ""], ["Lundgren", "Markus", ""], ["Ng", "Kenney", ""]]}, {"id": "1904.11676", "submitter": "Yusuke Ujitoko", "authors": "Yusuke Ujitoko, Yuki Ban, Koichi Hirota", "title": "Presenting Static Friction Sensation at Stick-slip Transition using\n  Pseudo-haptic Effect", "comments": "accepted for IEEE World Haptics Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies have aimed at creating a simple hardware implementation of\nsurface friction display. In this study, we propose a new method for presenting\nstatic frictional sensation using the pseudo-haptic effect as a first attempt,\nwhich is the simplest implementation of presenting static friction sensation.\nWe focus on the stick-slip phenomenon while users explore surfaces with an\ninput device, such as a stylus. During the stick phase, we present users with\npseudo-haptic feedback that represents static friction on the surface. In our\nmethod, users watch a virtual contact point become stuck at the contact point\non screen while users freely move the input device. We hypothesize that the\nperceived probability and intensity of static friction sensation can be\ncontrolled by changing the static friction coefficient as a visual parameter.\nUser studies were conducted, and results show the threshold value over which\nusers felt the pseudo-haptic static friction sensation at 90% probability. The\nresults also show that the perceived intensity of the sensation changed with\nrespect to the static friction coefficient. The maximum intensity change was\n23%. These results confirm the hypothesis and show that our method is a\npromising option for presenting static friction sensation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 05:32:40 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Ujitoko", "Yusuke", ""], ["Ban", "Yuki", ""], ["Hirota", "Koichi", ""]]}, {"id": "1904.11701", "submitter": "Martin L\\\"angkvist", "authors": "Martin L\\\"angkvist and Jonas Widell and Per Thunberg and Amy Loutfi\n  and Mats Lid\\'en", "title": "Interactive user interface based on Convolutional Auto-encoders for\n  annotating CT-scans", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution computed tomography (HRCT) is the most important imaging\nmodality for interstitial lung diseases, where the radiologists are interested\nin identifying certain patterns, and their volumetric and regional\ndistribution. The use of machine learning can assist the radiologists with both\nthese tasks by performing semantic segmentation. In this paper, we propose an\ninteractive annotation-tool for semantic segmentation that assists the\nradiologist in labeling CT scans. The annotation tool is evaluated by six\nradiologists and radiology residents classifying healthy lung and reticular\npattern i HRCT images. The usability of the system is evaluated with a System\nUsability Score (SUS) and interaction information from the readers that used\nthe tool for annotating the CT volumes. It was discovered that the experienced\nusability and how the users interactied with the system differed between the\nusers. A higher SUS-score was given by users that prioritized learning speed\nover model accuracy and spent less time with manual labeling and instead\nutilized the suggestions provided by the GUI. An analysis of the annotation\nvariations between the readers show substantial agreement (Cohen's kappa=0.69)\nfor classification of healthy and affected lung parenchyma in pulmonary\nfibrosis. The inter-reader variation is a challenge for the definition of\nground truth.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 07:45:48 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["L\u00e4ngkvist", "Martin", ""], ["Widell", "Jonas", ""], ["Thunberg", "Per", ""], ["Loutfi", "Amy", ""], ["Lid\u00e9n", "Mats", ""]]}, {"id": "1904.11704", "submitter": "Philippe Lepinard", "authors": "Philippe L\\'epinard (IRG), S\\'ebastien Loz\\'e", "title": "XR: Enabling training mode in the human brain XR: Enabling training mode\n  in the human brain", "comments": null, "journal-ref": "MODSIM World 2019, Apr 2019, Norfolk, United States", "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The face of simulation-based training has greatly evolved, with the most\nrecent tools giving the ability to create virtual environments that rival\nrealism. At first glance, it might appear that what the training sector needs\nis the most realistic simulators possible, but traditional simulators are not\nnecessarily the most efficient or practical training tools. With all that these\nnew technologies have to offer; the challenge is to go back to the core of\ntraining needs and identify the right vector of sensory cues that will most\neffectively enable training mode in the human brain. Bigger and Pricier doesn't\nnecessarily mean better. Simulation with cross-reality content (XR), which by\ndefinition encompasses virtual reality (VR), mixed reality (MR), and augmented\nreality (AR), is the most practical solution for deploying any kind of\nsimulation-based training. The authors of this paper (a teacher and a\ntechnology expert) share their experiences and expose XR-specific best\npractices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze :\nStarting his career in the modeling and simulation community more than 15 years\nago, S{\\'e}bastien has focused on learning about the latest simulation\ninnovations and sharing information on how experts have solved their\nchallenges. He worked on the COTS integration at CAE and the Presagis focusing\non Simulation and Visualization products. More recently, Sebastien put together\nsimulation and training teams and strategies for emerging companies like CM\nLabs and D-BOX. He is now the Simulations Industry Manager at Epic Games,\nfocusing on helping companies develop real-time solutions for simulation-based\ntraining. Philippe Lepinard: Former military helicopter pilot and simulation\nofficer, Philippe L{\\'e}pinard is now an associate professor at the University\nof Paris-Est Cr{\\'e}teil (UPEC). His research is focusing on playful learning\nand training through simulation. He is one of the founding members of the\nFrench simulation association.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 07:55:39 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["L\u00e9pinard", "Philippe", "", "IRG"], ["Loz\u00e9", "S\u00e9bastien", ""]]}, {"id": "1904.11950", "submitter": "Chuanqi Tan", "authors": "Chuanqi Tan, Fuchun Sun, Tao Kong, Bin Fang and Wenchang Zhang", "title": "Attention-based Transfer Learning for Brain-computer Interface", "comments": "In Proceedings of IEEE International Conference on Acoustics, Speech\n  and Signal Processing (ICASSP) 2019, 12 - 17 May, 2019, Brighton, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different functional areas of the human brain play different roles in brain\nactivity, which has not been paid sufficient research attention in the\nbrain-computer interface (BCI) field. This paper presents a new approach for\nelectroencephalography (EEG) classification that applies attention-based\ntransfer learning. Our approach considers the importance of different brain\nfunctional areas to improve the accuracy of EEG classification, and provides an\nadditional way to automatically identify brain functional areas associated with\nnew activities without the involvement of a medical professional. We\ndemonstrate empirically that our approach out-performs state-of-the-art\napproaches in the task of EEG classification, and the results of visualization\nindicate that our approach can detect brain functional areas related to a\ncertain task.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 06:10:09 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Tan", "Chuanqi", ""], ["Sun", "Fuchun", ""], ["Kong", "Tao", ""], ["Fang", "Bin", ""], ["Zhang", "Wenchang", ""]]}, {"id": "1904.11953", "submitter": "Fei Wang", "authors": "Fei Wang, Yunpeng Song, Jimuyang Zhang, Jinsong Han and Dong Huang", "title": "Temporal Unet: Sample Level Human Action Recognition using WiFi", "comments": "14 pages, 14 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human doing actions will result in WiFi distortion, which is widely explored\nfor action recognition, such as the elderly fallen detection, hand sign\nlanguage recognition, and keystroke estimation. As our best survey, past work\nrecognizes human action by categorizing one complete distortion series into one\naction, which we term as series-level action recognition. In this paper, we\nintroduce a much more fine-grained and challenging action recognition task into\nWiFi sensing domain, i.e., sample-level action recognition. In this task, every\nWiFi distortion sample in the whole series should be categorized into one\naction, which is a critical technique in precise action localization,\ncontinuous action segmentation, and real-time action recognition. To achieve\nWiFi-based sample-level action recognition, we fully analyze approaches in\nimage-based semantic segmentation as well as in video-based frame-level action\nrecognition, then propose a simple yet efficient deep convolutional neural\nnetwork, i.e., Temporal Unet. Experimental results show that Temporal Unet\nachieves this novel task well. Codes have been made publicly available at\nhttps://github.com/geekfeiw/WiSLAR.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 21:23:28 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Wang", "Fei", ""], ["Song", "Yunpeng", ""], ["Zhang", "Jimuyang", ""], ["Han", "Jinsong", ""], ["Huang", "Dong", ""]]}, {"id": "1904.11961", "submitter": "Ahmed Fadhil Dr.", "authors": "Ahmed Fadhil, Gianluca Schiavo, Yunlong Wang", "title": "CoachAI: A Conversational Agent Assisted Health Coaching Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Poor lifestyle represents a health risk factor and is the leading cause of\nmorbidity and chronic conditions. The impact of poor lifestyle can be\nsignificantly altered by individual behavior change. Although the current shift\nin healthcare towards a long lasting modifiable behavior, however, with\nincreasing caregiver workload and individuals' continuous needs of care, there\nis a need to ease caregiver's work while ensuring continuous interaction with\nusers. This paper describes the design and validation of CoachAI, a\nconversational agent assisted health coaching system to support health\nintervention delivery to individuals and groups. CoachAI instantiates a text\nbased healthcare chatbot system that bridges the remote human coach and the\nusers. This research provides three main contributions to the preventive\nhealthcare and healthy lifestyle promotion: (1) it presents the conversational\nagent to aid the caregiver; (2) it aims to decrease caregiver's workload and\nenhance care given to users, by handling (automating) repetitive caregiver\ntasks; and (3) it presents a domain independent mobile health conversational\nagent for health intervention delivery. We will discuss our approach and\nanalyze the results of a one month validation study on physical activity,\nhealthy diet and stress management.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 17:44:04 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Fadhil", "Ahmed", ""], ["Schiavo", "Gianluca", ""], ["Wang", "Yunlong", ""]]}, {"id": "1904.12152", "submitter": "Marco Filetti", "authors": "Marco Filetti, Hamed R. Tavakoli, Niklas Ravaja, Giulio Jacucci", "title": "PeyeDF: an Eye-Tracking Application for Reading and Self-Indexing\n  Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PeyeDF is a Portable Document Format (PDF) reader with eye tracking support,\navailable as free and open source software. It is especially useful to\nresearchers investigating reading and learning phenomena, as it integrates PDF\nreading-related behavioural data with gaze-related data. It is suitable for\nshort and long-term research and supports multiple eye tracking systems. We\nutilised it to conduct an experiment which demonstrated that features obtained\nfrom both gaze and reading data collected in the past can predict reading\ncomprehension which takes place in the future. PeyeDF also provides an\nintegrated means for data collection and indexing using the DiMe personal data\nstorage system. It is designed to collect data in the background without\ninterfering with the reading experience, behaving like a modern lightweight PDF\nreader. Moreover, it supports annotations, tagging and collaborative work. A\nmodular design allows the application to be easily modified in order to support\nadditional eye tracking protocols and run controlled experiments. We discuss\nthe implementation of the software and report on the results of the experiment\nwhich we conducted with it.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 12:27:41 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Filetti", "Marco", ""], ["Tavakoli", "Hamed R.", ""], ["Ravaja", "Niklas", ""], ["Jacucci", "Giulio", ""]]}, {"id": "1904.12225", "submitter": "Oh-Hyun Kwon", "authors": "Oh-Hyun Kwon and Kwan-Liu Ma", "title": "A Deep Generative Model for Graph Layout", "comments": "To appear in IEEE Transactions on Visualization and Computer\n  Graphics. In Proc. IEEE VIS 2019 (InfoVis)", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934396", "report-no": null, "categories": "cs.SI cs.GR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different layouts can characterize different aspects of the same graph.\nFinding a \"good\" layout of a graph is thus an important task for graph\nvisualization. In practice, users often visualize a graph in multiple layouts\nby using different methods and varying parameter settings until they find a\nlayout that best suits the purpose of the visualization. However, this\ntrial-and-error process is often haphazard and time-consuming. To provide users\nwith an intuitive way to navigate the layout design space, we present a\ntechnique to systematically visualize a graph in diverse layouts using deep\ngenerative models. We design an encoder-decoder architecture to learn a model\nfrom a collection of example layouts, where the encoder represents training\nexamples in a latent space and the decoder produces layouts from the latent\nspace. In particular, we train the model to construct a two-dimensional latent\nspace for users to easily explore and generate various layouts. We demonstrate\nour approach through quantitative and qualitative evaluations of the generated\nlayouts. The results of our evaluations show that our model is capable of\nlearning and generalizing abstract concepts of graph layouts, not just\nmemorizing the training examples. In summary, this paper presents a\nfundamentally new approach to graph visualization where a machine learning\nmodel learns to visualize a graph from examples without manually-defined\nheuristics.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2019 23:19:49 GMT"}, {"version": "v2", "created": "Sat, 13 Jul 2019 23:44:11 GMT"}, {"version": "v3", "created": "Sat, 27 Jul 2019 21:45:55 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 02:57:51 GMT"}, {"version": "v5", "created": "Thu, 29 Aug 2019 00:33:59 GMT"}, {"version": "v6", "created": "Mon, 2 Sep 2019 07:04:25 GMT"}, {"version": "v7", "created": "Tue, 15 Oct 2019 17:22:25 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Kwon", "Oh-Hyun", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1904.12268", "submitter": "Ran Gilad-Bachrach", "authors": "Oded Vainas, Ori Bar-Ilan, Yossi Ben-David, Ran Gilad-Bachrach, Galit\n  Lukin, Meitar Ronen, Roi Shillo, Daniel Sitton", "title": "E-Gotsky: Sequencing Content using the Zone of Proximal Development", "comments": "A short version of this paper was accepted for publication\n  Educational Data Mining (EDM) conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vygotsky's notions of Zone of Proximal Development and Dynamic Assessment\nemphasize the importance of personalized learning that adapts to the needs and\nabilities of the learners and enables more efficient learning. In this work we\nintroduce a novel adaptive learning engine called E-gostky that builds on these\nconcepts to personalize the learning path within an e-learning system. E-gostky\nuses machine learning techniques to select the next content item that will\nchallenge the student but will not be overwhelming, keeping students in their\nZone of Proximal Development.\n  To evaluate the system, we conducted an experiment where hundreds of students\nfrom several different elementary schools used our engine to learn fractions\nfor five months. Our results show that using E-gostky can significantly reduce\nthe time required to reach similar mastery. Specifically, in our experiment, it\ntook students who were using the adaptive learning engine $17\\%$ less time to\nreach a similar level of mastery as of those who didn't. Moreover, students\nmade greater efforts to find the correct answer rather than guessing and class\nteachers reported that even students with learning disabilities showed higher\nengagement.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 07:12:11 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Vainas", "Oded", ""], ["Bar-Ilan", "Ori", ""], ["Ben-David", "Yossi", ""], ["Gilad-Bachrach", "Ran", ""], ["Lukin", "Galit", ""], ["Ronen", "Meitar", ""], ["Shillo", "Roi", ""], ["Sitton", "Daniel", ""]]}, {"id": "1904.12568", "submitter": "Oliver Hohlfeld", "authors": "Dennis Guse and Henrique R. Orefice and Gabriel Reimers and Oliver\n  Hohlfeld", "title": "TheFragebogen: A Web Browser-based Questionnaire Framework for\n  Scientific Research", "comments": "Tools Track paper to appear at IEEE QoMEX 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality of Experience (QoE) typically involves conducting experiments in\nwhich stimuli are presented to participants and their judgments as well as\nbehavioral data are collected. Nowadays, many experiments require software for\nthe presentation of stimuli and the data collection from participants. While\ndifferent software solutions exist, these are not tailored to conduct\nexperiments on QoE. Moreover, replicating experiments or repeating the same\nexperiment in different settings (e. g., laboratory vs. crowdsourcing) can\nfurther increase the software complexity. TheFragebogen is an open-source,\nversatile, extendable software framework for the implementation of\nquestionnaires - especially for research on QoE. Implemented questionnaires can\nbe presented with a state-of-the-art web browser to support a broad range of\ndevices while the use of a web server being optional. Out-of-the-box,\nTheFragebogen provides graphical exact scales as well as free-hand input, the\nability to collect behavioral data, and playback multimedia content.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 11:54:54 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Guse", "Dennis", ""], ["Orefice", "Henrique R.", ""], ["Reimers", "Gabriel", ""], ["Hohlfeld", "Oliver", ""]]}, {"id": "1904.12581", "submitter": "Achim J. Lilienthal", "authors": "Achim J. Lilienthal, Maike Schindler", "title": "Current Trends in the Use of Eye Tracking in Mathematics Education\n  Research: A PME Survey", "comments": "It is planned to update this review yearly after the respective PME\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye tracking (ET) is a research method that receives growing interest in\nmathematics education research (MER). This paper aims to give a literature\noverview, specifically focusing on the evolution of interest in this\ntechnology, ET equipment, and analysis methods used in mathematics education.\nTo capture the current state, we focus on papers published in the proceedings\nof PME, one of the primary conferences dedicated to MER, of the last ten years.\nWe identify trends in interest, methodology, and methods of analysis that are\nused in the community, and discuss possible future developments.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 11:41:17 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 10:50:43 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Lilienthal", "Achim J.", ""], ["Schindler", "Maike", ""]]}, {"id": "1904.13037", "submitter": "Jinqiang Bai", "authors": "Jinqiang Bai, Zhaoxiang Liu, Yimin Lin, Ye Li, Shiguo Lian, Dijun Liu", "title": "Wearable Travel Aid for Environment Perception and Navigation of\n  Visually Impaired People", "comments": "7 pages, 12 figures", "journal-ref": "2019 Electronics", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a wearable assistive device with the shape of a pair of\neyeglasses that allows visually impaired people to navigate safely and quickly\nin unfamiliar environment, as well as perceive the complicated environment to\nautomatically make decisions on the direction to move. The device uses a\nconsumer Red, Green, Blue and Depth (RGB-D) camera and an Inertial Measurement\nUnit (IMU) to detect obstacles. As the device leverages the ground height\ncontinuity among adjacent image frames, it is able to segment the ground from\nobstacles accurately and rapidly. Based on the detected ground, the optimal\nwalkable direction is computed and the user is then informed via converted beep\nsound. Moreover, by utilizing deep learning techniques, the device can\nsemantically categorize the detected obstacles to improve the users' perception\nof surroundings. It combines a Convolutional Neural Network (CNN) deployed on a\nsmartphone with a depth-image-based object detection to decide what the object\ntype is and where the object is located, and then notifies the user of such\ninformation via speech. We evaluated the device's performance with different\nexperiments in which 20 visually impaired people were asked to wear the device\nand move in an office, and found that they were able to avoid obstacle\ncollisions and find the way in complicated scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 03:33:45 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Bai", "Jinqiang", ""], ["Liu", "Zhaoxiang", ""], ["Lin", "Yimin", ""], ["Li", "Ye", ""], ["Lian", "Shiguo", ""], ["Liu", "Dijun", ""]]}, {"id": "1904.13086", "submitter": "Joachim Meyer", "authors": "Nir Douer, Joachim Meyer", "title": "Theoretical, Measured and Subjective Responsibility in Aided Decision\n  Making", "comments": null, "journal-ref": "ACM Transactions on Intelligent Interactive Systems, 11 (1) ,\n  Article 5, February 2021", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When humans interact with intelligent systems, their causal responsibility\nfor outcomes becomes equivocal. We analyze the descriptive abilities of a newly\ndeveloped responsibility quantification model (ResQu) to predict actual human\nresponsibility and perceptions of responsibility in the interaction with\nintelligent systems. In two laboratory experiments, participants performed a\nclassification task. They were aided by classification systems with different\ncapabilities. We compared the predicted theoretical responsibility values to\nthe actual measured responsibility participants took on and to their subjective\nrankings of responsibility. The model predictions were strongly correlated with\nboth measured and subjective responsibility. A bias existed only when\nparticipants with poor classification capabilities relied less-than-optimally\non a system that had superior classification capabilities and assumed\nhigher-than-optimal responsibility. The study implies that when humans interact\nwith advanced intelligent systems, with capabilities that greatly exceed their\nown, their comparative causal responsibility will be small, even if formally\nthe human is assigned major roles. Simply putting a human into the loop does\nnot assure that the human will meaningfully contribute to the outcomes. The\nresults demonstrate the descriptive value of the ResQu model to predict\nbehavior and perceptions of responsibility by considering the characteristics\nof the human, the intelligent system, the environment and some systematic\nbehavioral biases. The ResQu model is a new quantitative method that can be\nused in system design and can guide policy and legal decisions regarding human\nresponsibility in events involving intelligent systems.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 07:37:33 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 07:18:58 GMT"}, {"version": "v3", "created": "Wed, 29 Apr 2020 15:28:35 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Douer", "Nir", ""], ["Meyer", "Joachim", ""]]}, {"id": "1904.13333", "submitter": "Gerard Serra", "authors": "Gerard Serra and David Miralles", "title": "Coevo: a collaborative design platform with artificial agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Coevo, an online platform that allows both humans and artificial\nagents to design shapes that solve different tasks. Our goal is to explore\ncommon shared design tools that can be used by humans and artificial agents in\na context of creation. This approach can provide a better knowledge transfer\nand interaction with artificial agents since a common language of design is\ndefined. In this paper, we outline the main components of this platform and\ndiscuss the definition of a human-centered language to enhance human-AI\ncollaboration in co-creation scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 15:55:34 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Serra", "Gerard", ""], ["Miralles", "David", ""]]}, {"id": "1904.13364", "submitter": "Shah Rukh Humayoun", "authors": "Razan N. Jaber, Ragaad AlTarawneh, Shah Rukh Humayoun", "title": "Characterizing Pairs Collaboration in a Mobile-equipped Shared-Wall\n  Display Supported Collaborative Setup", "comments": "Keywords: Computer-Supported Cooperative Work (CSCW); Tiled-Wall\n  Collaborative Environment; Coupling; Smart Devices; User Study; Collaborative\n  Relation Building. 36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in mobile devices encourage researchers to utilize them\nin collaborative environments as a medium to interact with large shared\nwall-displays. In this paper, we focus on a semi-controlled user study that we\nconducted to measure the collaborative coupling ratio between partners working\nin pairs in a collaborative setup equipped with a shared tiled-wall display and\nmultiple mobile devices. We invited 36 participants in 18 pairs to take part in\nour experiment in order to analyze how they communicate and collaborate with\neach other during the experiment. We observed their collaborative coupling by\nmeasuring how often they verbally and visually communicated. Further, we found\nfrequently used collaborative physical position patterns by observing the\npairs' physical arrangements and standing positions. Moreover, we combined\nthese factors to gain a clearer understanding of coupling in our setup, taking\ninto account the mobility factor offered by the mobile devices. Results of the\nstudy show interesting findings about the coupling factors between the partners\nmainly due to the flexibility offered by including mobile devices in our\ncollaborative setup.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:53:22 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Jaber", "Razan N.", ""], ["AlTarawneh", "Ragaad", ""], ["Humayoun", "Shah Rukh", ""]]}]