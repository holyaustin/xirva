[{"id": "1509.00159", "submitter": "Zhihan Lv", "authors": "Zhihan Lv and Xiaoming Li", "title": "Preprint Virtual Reality Assistant Technology for Learning Primary\n  Geography", "comments": "This is the preprint version of our paper on ICWL2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on ICWL2015. A virtual reality\nbased enhanced technology for learning primary geography is proposed, which\nsynthesizes several latest information technologies including virtual\nreality(VR), 3D geographical information system(GIS), 3D visualization and\nmultimodal human-computer-interaction (HCI). The main functions of the proposed\nsystem are introduced, i.e. Buffer analysis, Overlay analysis, Space convex\nhull calculation, Space convex decomposition, 3D topology analysis and 3D space\nintersection detection. The multimodal technologies are employed in the system\nto enhance the immersive perception of the users.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 07:03:35 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 15:55:27 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Lv", "Zhihan", ""], ["Li", "Xiaoming", ""]]}, {"id": "1509.00189", "submitter": "Walter Quattrociocchi", "authors": "Michela Del Vicario, Alessandro Bessi, Fabiana Zollo, Fabio Petroni,\n  Antonio Scala, Guido Caldarelli, H.Eugene Stanley, Walter Quattrociocchi", "title": "Echo chambers in the age of misinformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide availability of user-provided content in online social media\nfacilitates the aggregation of people around common interests, worldviews, and\nnarratives. Despite the enthusiastic rhetoric on the part of some that this\nprocess generates \"collective intelligence\", the WWW also allows the rapid\ndissemination of unsubstantiated conspiracy theories that often elicite rapid,\nlarge, but naive social responses such as the recent case of Jade Helm 15 --\nwhere a simple military exercise turned out to be perceived as the beginning of\nthe civil war in the US. We study how Facebook users consume information\nrelated to two different kinds of narrative: scientific and conspiracy news. We\nfind that although consumers of scientific and conspiracy stories present\nsimilar consumption patterns with respect to content, the sizes of the\nspreading cascades differ. Homogeneity appears to be the primary driver for the\ndiffusion of contents, but each echo chamber has its own cascade dynamics. To\nmimic these dynamics, we introduce a data-driven percolation model on signed\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 09:24:21 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2015 15:03:12 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Del Vicario", "Michela", ""], ["Bessi", "Alessandro", ""], ["Zollo", "Fabiana", ""], ["Petroni", "Fabio", ""], ["Scala", "Antonio", ""], ["Caldarelli", "Guido", ""], ["Stanley", "H. Eugene", ""], ["Quattrociocchi", "Walter", ""]]}, {"id": "1509.01338", "submitter": "Sumit Soman", "authors": "Sumit Soman, Siddharth Srivastava, Saurabh Srivastava, Nitendra Rajput", "title": "Brain Computer Interfaces for Mobile Apps: State-of-the-art and Future\n  Directions", "comments": "Reprint from Proceedings of the 9th International Conference on\n  Interfaces and Human Computer Interaction (http://ihci-conf.org/), 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, there have been significant advancements in utilizing the\nsensing capabilities of mobile devices for developing applications. The primary\nobjective has been to enhance the way a user interacts with the application by\nmaking it effortless and convenient. This paper explores the capabilities of\nusing Brain Computer Interfaces (BCI), an evolving subset of Human Computer\nInteraction (HCI) paradigms, to control mobile devices. We present a\ncomprehensive survey of the state-of-the-art in this area, discussing the\nchallenges and limitations in using BCI for mobile applications. Further we\npropose possible modalities that in future can benefit with BCI applications.\nThis paper consolidates research directions being pursued in this domain, and\ndraws conclusions on feasibility and benefits of using BCI systems effectively\naugmented to the mobile application development domain.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2015 04:25:25 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Soman", "Sumit", ""], ["Srivastava", "Siddharth", ""], ["Srivastava", "Saurabh", ""], ["Rajput", "Nitendra", ""]]}, {"id": "1509.01662", "submitter": "Christohper Pilson", "authors": "Christopher S. Pilson", "title": "Tightly-Held and Ephemeral Psychometrics: Password and Passphrase\n  Authentication Utilizing User-Supplied Constructs of Self", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research investigates the role of passwords and passphrases as valid\nauthentication methodologies. Specifically, this research dispels earlier work\nthat ignores information-theoretic lessons learned from cognitive and social\npsychology and psycholinguistics, and extends and enriches the current password\nsecurity model.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 04:28:39 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Pilson", "Christopher S.", ""]]}, {"id": "1509.01872", "submitter": "Burak Pak", "authors": "Burak Pak, Johan Verbeke", "title": "Design Studio 2.0: Augmenting Reflective Architectural Design Learning", "comments": null, "journal-ref": "Journal of Information Technology in Construction, 17, 502-519\n  (2012)", "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web 2.0 is beyond a jargon describing technological transformation: it refers\nto new strategies, tools and techniques that encourage and augment informed,\ncreative and social inter(actions). When considered in an educational context,\nWeb 2.0 provides various opportunities for enhanced integration and for\nimproving the learning processes in information-rich collaborative disciplines\nsuch as urban planning and architectural design. The dialogue between the\ndesign students and studio teachers can be mediated in various ways by creating\nnovel learning spaces using Web 2.0-based social software and information\naggregation services, and brought to a level where the Web 2.0 environment\nsupports, augments and enriches the reflective learning processes. We propose\nto call this new setting Design Studio 2.0. We suggest that Design Studio 2.0\ncan provide numerous opportunities which are not fully or easily available in a\nconventional design studio setting. In this context, we will introduce a\nweb-based geographic virtual environment model (GEO-VEM) and discuss how we\nreconfigured and rescaled this model with the objective of supporting an\ninternational urban design studio by encouraging students to make a\ncollaborative and location-based analysis of a project site (the\nBrussels-Charleroi Canal). Pursuing the discussion further, we will present our\nexperiences and observations of this design studio including web use\nstatistics, and the results of student attitude surveys. In conclusion, we will\nreflect the difficulties and challenges of using the GEO-VEM in the Design\nStudio in a blended learning context and develop future prospects. As a result,\nwe will introduce a set of key criteria for the development and implementation\nof an effective e-learning environment as a sustainable platform for supporting\nthe Design Studio 2.0.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 00:41:59 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Pak", "Burak", ""], ["Verbeke", "Johan", ""]]}, {"id": "1509.01874", "submitter": "Burak Pak", "authors": "Burak Pak, Johan Verbeke", "title": "Geoweb 2.0 for Participatory Urban Design: Affordances and Critical\n  Success Factors", "comments": null, "journal-ref": "International Journal of Architectural Computing 12(3) 283-305\n  (2014)", "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the affordances of open-source Geoweb 2.0 platforms\nto support the participatory design of urban projects in real-world\npractices.We first introduce the two open-source platforms used in our study\nfor testing purposes. Then, based on evidence from five different field studies\nwe identify five affordances of these platforms: conversations on alternative\nurban projects, citizen consultation, design empowerment, design studio\nlearning and design research. We elaborate on these in detail and identify a\nkey set of success factors for the facilitation of better practices in the\nfuture.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 00:58:45 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Pak", "Burak", ""], ["Verbeke", "Johan", ""]]}, {"id": "1509.01876", "submitter": "Burak Pak", "authors": "Burak Pak, Johan Verbeke", "title": "Redesigning the urban design studio: Two learning experiments", "comments": null, "journal-ref": "Journal of Learning Design 3 (2) 1-13 2013", "doi": "10.5204/jld.v6i3.160", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main aim of this paper is to discuss how the combination of Web 2.0,\nsocial media and geographic technologies can provide opportunities for learning\nand new forms of participation in an urban design studio. This discussion is\nmainly based on our recent findings from two experimental urban design studio\nsetups as well as former research and literature studies. In brief, the web\nplatform enabled us to extend the learning that took place in the design studio\nbeyond the studio hours, to represent the design information in novel ways and\nallocate multiple communication forms. We found that the student activity in\nthe introduced web platform was related to their progress up to a certain\nextent. Moreover, the students perceived the platform as a convenient medium\nand addressed it as a valuable resource for learning. This study should be\nconceived as a continuation of a series of our Design Studio 2.0 experiments\nwhich involve the exploitation of opportunities provided by novel\nsocio-geographic information and communication technologies for the improvement\nof the design learning processes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 01:10:42 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Pak", "Burak", ""], ["Verbeke", "Johan", ""]]}, {"id": "1509.02739", "submitter": "Zeon Trevor Fernando", "authors": "Jaspreet Singh, Zeon Trevor Fernando, Saniya Chawla", "title": "LearnWeb-OER: Improving Accessibility of Open Educational Resources", "comments": "LinkedUp Vici Challenge, International Semantic Web Conference, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In addition to user-generated content, Open Educational Resources are\nincreasingly made available on the Web by several institutions and\norganizations with the aim of being re-used. Nevertheless, it is still\ndifficult for users to find appropriate resources for specific learning\nscenarios among the vast amount offered on the Web. Our goal is to give users\nthe opportunity to search for authentic resources from the Web and reuse them\nin a learning context. The LearnWeb-OER platform enhances collaborative\nsearching and sharing of educational resources providing specific means and\nfacilities for education. In the following, we provide a description of the\nfunctionalities that support users in collaboratively collecting, selecting,\nannotating and discussing search results and learning resources.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 12:01:39 GMT"}], "update_date": "2015-09-10", "authors_parsed": [["Singh", "Jaspreet", ""], ["Fernando", "Zeon Trevor", ""], ["Chawla", "Saniya", ""]]}, {"id": "1509.03026", "submitter": "Joy Kim", "authors": "Joy Kim and Andres Monroy-Hernandez", "title": "Storia: Summarizing Social Media Content based on Narrative Theory using\n  Crowdsourcing", "comments": null, "journal-ref": null, "doi": "10.1145/2818048.2820072", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People from all over the world use social media to share thoughts and\nopinions about events, and understanding what people say through these channels\nhas been of increasing interest to researchers, journalists, and marketers\nalike. However, while automatically generated summaries enable people to\nconsume large amounts of data efficiently, they do not provide the context\nneeded for a viewer to fully understand an event. Narrative structure can\nprovide templates for the order and manner in which this data is presented to\ncreate stories that are oriented around narrative elements rather than\nsummaries made up of facts. In this paper, we use narrative theory as a\nframework for identifying the links between social media content. To do this,\nwe designed crowdsourcing tasks to generate summaries of events based on\ncommonly used narrative templates. In a controlled study, for certain types of\nevents, people were more emotionally engaged with stories created with\nnarrative structure and were also more likely to recommend them to others\ncompared to summaries created without narrative structure.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 06:16:19 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Kim", "Joy", ""], ["Monroy-Hernandez", "Andres", ""]]}, {"id": "1509.04360", "submitter": "Joseph Williams", "authors": "Joseph Jay Williams, Neil Heffernan", "title": "A Methodology for Discovering how to Adaptively Personalize to Users\n  using Experimental Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain and provide examples of a formalism that supports the methodology\nof discovering how to adapt and personalize technology by combining randomized\nexperiments with variables associated with user models. We characterize a\nformal relationship between the use of technology to conduct A/B experiments\nand use of technology for adaptive personalization. The MOOClet Formalism [11]\ncaptures the equivalence between experimentation and personalization in its\nconceptualization of modular components of a technology. This motivates a\nunified software design pattern that enables technology components that can be\ncompared in an experiment to also be adapted based on contextual data, or\npersonalized based on user characteristics. With the aid of a concrete use\ncase, we illustrate the potential of the MOOClet formalism for a methodology\nthat uses randomized experiments of alternative micro-designs to discover how\nto adapt technology based on user characteristics, and then dynamically\nimplements these personalized improvements in real time.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 00:45:10 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Williams", "Joseph Jay", ""], ["Heffernan", "Neil", ""]]}, {"id": "1509.04751", "submitter": "Cheng-I Wang", "authors": "Tammuz Dubnov and Cheng-i Wang", "title": "Free-body Gesture Tracking and Augmented Reality Improvisation for Floor\n  and Aerial Dance", "comments": "8 pages. Technical paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an updated interactive performance system for floor and\nAerial Dance that controls visual and sonic aspects of the presentation via a\ndepth sensing camera (MS Kinect). In order to detect, measure and track free\nmovement in space, 3 degree of freedom (3-DOF) tracking in space (on the ground\nand in the air) is performed using IR markers with a method for multi target\ntracking capabilities added and described in detail. An improved gesture\ntracking and recognition system, called Action Graph (AG), is described in the\npaper. Action Graph uses an efficient incremental construction from a single\nlong sequence of movement features and automatically captures repeated\nsub-segments in the movement from start to finish with no manual interaction\nneeded with other advanced capabilities discussed as well. By using the new\nmodel for the gesture we can unify an entire choreography piece by dynamically\ntracking and recognizing gestures and sub-portions of the piece. This gives the\nperformer the freedom to improvise based on a set of recorded gestures/portions\nof the choreography and have the system dynamically respond in relation to the\nperformer within a set of related rehearsed actions, an ability that has not\nbeen seen in any other system to date.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 21:54:21 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Dubnov", "Tammuz", ""], ["Wang", "Cheng-i", ""]]}, {"id": "1509.05238", "submitter": "Katrin H\\\"ansel", "authors": "Katrin H\\\"ansel, Natalie Wilde, Hamed Haddadi, Akram Alomainy", "title": "Wearable Computing for Health and Fitness: Exploring the Relationship\n  between Data and Human Behaviour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health and fitness wearable technology has recently advanced, making it\neasier for an individual to monitor their behaviours. Previously self generated\ndata interacts with the user to motivate positive behaviour change, but issues\narise when relating this to long term mention of wearable devices. Previous\nstudies within this area are discussed. We also consider a new approach where\ndata is used to support instead of motivate, through monitoring and logging to\nencourage reflection. Based on issues highlighted, we then make recommendations\non the direction in which future work could be most beneficial.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 12:59:24 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 15:45:25 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["H\u00e4nsel", "Katrin", ""], ["Wilde", "Natalie", ""], ["Haddadi", "Hamed", ""], ["Alomainy", "Akram", ""]]}, {"id": "1509.05301", "submitter": "Victor Schetinger", "authors": "Victor Schetinger, Manuel M. Oliveira, Roberto da Silva, Tiago J.\n  Carvalho", "title": "Humans Are Easily Fooled by Digital Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital images are ubiquitous in our modern lives, with uses ranging from\nsocial media to news, and even scientific papers. For this reason, it is\ncrucial evaluate how accurate people are when performing the task of identify\ndoctored images. In this paper, we performed an extensive user study evaluating\nsubjects capacity to detect fake images. After observing an image, users have\nbeen asked if it had been altered or not. If the user answered the image has\nbeen altered, he had to provide evidence in the form of a click on the image.\nWe collected 17,208 individual answers from 383 users, using 177 images\nselected from public forensic databases. Different from other previously\nstudies, our method propose different ways to avoid lucky guess when evaluating\nusers answers. Our results indicate that people show inaccurate skills at\ndifferentiating between altered and non-altered images, with an accuracy of\n58%, and only identifying the modified images 46.5% of the time. We also track\nuser features such as age, answering time, confidence, providing deep analysis\nof how such variables influence on the users' performance.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 15:47:25 GMT"}], "update_date": "2016-01-14", "authors_parsed": [["Schetinger", "Victor", ""], ["Oliveira", "Manuel M.", ""], ["da Silva", "Roberto", ""], ["Carvalho", "Tiago J.", ""]]}, {"id": "1509.05452", "submitter": "Anant  Baijal", "authors": "Anant Baijal, Julia Kim, Carmen Branje, Frank Russo, Deborah I. Fels", "title": "Composing vibrotactile music: A multisensory experience with the\n  Emoti-chair", "comments": "IEEE HAPTICS Symposium 2012", "journal-ref": null, "doi": "10.1109/HAPTIC.2012.6183839", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Emoti-Chair is a novel technology to enhance entertainment through\nvibrotactile stimulation. We assessed the experience of this technology in two\nworkshops. In the first workshop, deaf film-makers experimented with creating\nvibetracks for a movie clip using a professional movie editing software. In the\nsecond workshop, trained opera singers sang and felt their voice through the\nEmoti-Chair. Participants in both workshops generally found the overall\nexperience to be exciting and they were motivated to use the Chair for upcoming\nprojects.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 21:52:12 GMT"}], "update_date": "2015-09-21", "authors_parsed": [["Baijal", "Anant", ""], ["Kim", "Julia", ""], ["Branje", "Carmen", ""], ["Russo", "Frank", ""], ["Fels", "Deborah I.", ""]]}, {"id": "1509.06109", "submitter": "Dustin Freeman", "authors": "Dustin Freeman, Ricardo Jota, Daniel Vogel, Daniel Wigdor, Ravin\n  Balakrishnan", "title": "A Dataset of Naturally Occurring, Whole-Body Background Activity to\n  Reduce Gesture Conflicts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real settings, natural body movements can be erroneously recognized by\nwhole-body input systems as explicit input actions. We call body activity not\nintended as input actions \"background activity.\" We argue that understanding\nbackground activity is crucial to the success of always-available whole-body\ninput in the real world. To operationalize this argument, we contribute a\nreusable study methodology and software tools to generate standardized\nbackground activity datasets composed of data from multiple Kinect cameras, a\nVicon tracker, and two high-definition video cameras. Using our methodology, we\ncreate an example background activity dataset for a television-oriented living\nroom setting. We use this dataset to demonstrate how it can be used to redesign\na gestural interaction vocabulary to minimize conflicts with the real world.\nThe software tools and initial living room dataset are publicly available\n(http://www.dgp.toronto.edu/~dustin/backgroundactivity/).\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 04:40:31 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Freeman", "Dustin", ""], ["Jota", "Ricardo", ""], ["Vogel", "Daniel", ""], ["Wigdor", "Daniel", ""], ["Balakrishnan", "Ravin", ""]]}, {"id": "1509.06293", "submitter": "Chris Foreman PhD", "authors": "Chris Foreman, Rammohan K. Ragade, and James H. Graham", "title": "An Immersive Visualization Tool for Teaching and Simulation of Smart\n  Grid Technologies", "comments": "6 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent power grid research, i.e. smart grid, involves many simultaneous\nusers spread over a relatively large geographical area. A tool for advancing\nresearch and community education is presented utilizing large-scale\nvisualization centers, e.g. planetariums, in simulating smart grid\ninteractions. This approach immerses the user in virtual smart grid\nvisualization and allows the user, with several other users, to interact in\nreal time. This facilitates community education by demonstrating how the power\ngrid functions with smart technologies. The simulation is sophisticated enough\nto also be used as a research tool for industry and higher education to test\nsoftware algorithms, deployment strategies, communications protocols, and even\nnew hardware.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 16:34:47 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Foreman", "Chris", ""], ["Ragade", "Rammohan K.", ""], ["Graham", "James H.", ""]]}, {"id": "1509.06774", "submitter": "Zhihan Lv", "authors": "Zhihan Lv", "title": "Preprint: Bringing immersive enjoyment to hyperbaric oxygen chamber\n  users using virtual reality glasses", "comments": "This is the preprint version of our paper on REHAB2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on REHAB2015. This paper proposed a\nnovel immersive entertainment system for the users of hyperbaric oxygen therapy\nchamber. The system is a hybrid of hardware and software, the scheme is\ndescribed in this paper. The hardware is combined by a HMD (i.e. virtual\nreality glasses shell), a smartphone and a waterproof bag. The software is able\nto transfer the stereoscopic images of the 3D game to the screen of the\nsmartphone synchronously. The comparison and selection of the hardware are\ndiscussed according to the practical running scene of the clinical hyperbaric\noxygen treatment. Finally, a preliminary guideline for designing this kind of\nsystem is raised accordingly.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 20:35:54 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Lv", "Zhihan", ""]]}, {"id": "1509.06776", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Vicente Penades, Sonia Blasco, Javier Chirivella, Pablo\n  Gagliardo", "title": "Preprint: Intuitive Evaluation of Kinect2 based Balance Measurement\n  Software", "comments": "This is the preprint version of our paper on REHAB2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on REHAB2015. A balance measurement\nsoftware based on Kinect2 sensor is evaluated by comparing to golden standard\nbalance measure platform intuitively. The software analysis the tracked body\ndata from the user by Kinect2 sensor and get user's center of mass(CoM) as well\nas its motion route on a plane. The software is evaluated by several comparison\ntests, the evaluation results preliminarily prove the reliability of the\nsoftware.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 20:43:51 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Lv", "Zhihan", ""], ["Penades", "Vicente", ""], ["Blasco", "Sonia", ""], ["Chirivella", "Javier", ""], ["Gagliardo", "Pablo", ""]]}, {"id": "1509.06783", "submitter": "Zhihan Lv", "authors": "Zhihan Lv, Vicente Penades, Sonia Blasco, Javier Chirivella, Pablo\n  Gagliardo", "title": "Preprint: Comparing Kinect2 based Balance Measurement Software to Wii\n  Balance Board", "comments": "This is the preprint version of our paper on REHAB2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the preprint version of our paper on REHAB2015. A balance measurement\nsoftware based on Kinect2 sensor is evaluated by comparing to Wii balance board\nin numerical analysis level, and further improved according to the\nconsideration of BFP (Body fat percentage) values of the user. Several person\nwith different body types are involved into the test. The algorithm is improved\nby comparing the body type of the user to the 'golden- standard' body type. The\nevaluation results of the optimized algorithm preliminarily prove the\nreliability of the software.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 20:59:19 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Lv", "Zhihan", ""], ["Penades", "Vicente", ""], ["Blasco", "Sonia", ""], ["Chirivella", "Javier", ""], ["Gagliardo", "Pablo", ""]]}, {"id": "1509.06808", "submitter": "Karthik Gangavarapu", "authors": "Karthik Gangavarapu, Vyshakh Babji, Tobias Mei{\\ss}ner, Andrew I. Su,\n  and Benjamin M. Good", "title": "Branch: An interactive, web-based tool for testing hypotheses and\n  developing predictive models", "comments": null, "journal-ref": null, "doi": "10.1093/bioinformatics/btw117", "report-no": null, "categories": "stat.AP cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Branch is a web application that provides users with no programming with the\nability to interact directly with large biomedical datasets. The interaction is\nmediated through a collaborative graphical user interface for building and\nevaluating decision trees. These trees can be used to compose and test\nsophisticated hypotheses and to develop predictive models. Decision trees are\nevaluated based on a library of imported datasets and can be stored in a\ncollective area for sharing and re-use. Branch is hosted at\nhttp://biobranch.org/ and the open source code is available at\nhttp://bitbucket.org/sulab/biobranch/.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 23:15:57 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2015 20:55:14 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Gangavarapu", "Karthik", ""], ["Babji", "Vyshakh", ""], ["Mei\u00dfner", "Tobias", ""], ["Su", "Andrew I.", ""], ["Good", "Benjamin M.", ""]]}, {"id": "1509.07450", "submitter": "Yi Chen", "authors": "Yi Chen, Enyi Yao, Arindam Basu", "title": "A 128 channel Extreme Learning Machine based Neural Decoder for Brain\n  Machine Interfaces", "comments": "13 pages, 17 figures, accepted by IEEE Transactions on Biomedical\n  Circuits and Systems, 2015", "journal-ref": null, "doi": "10.1109/TBCAS.2015.2483618", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, state-of-the-art motor intention decoding algorithms in\nbrain-machine interfaces are mostly implemented on a PC and consume significant\namount of power. A machine learning co-processor in 0.35um CMOS for motor\nintention decoding in brain-machine interfaces is presented in this paper.\nUsing Extreme Learning Machine algorithm and low-power analog processing, it\nachieves an energy efficiency of 290 GMACs/W at a classification rate of 50 Hz.\nThe learning in second stage and corresponding digitally stored coefficients\nare used to increase robustness of the core analog processor. The chip is\nverified with neural data recorded in monkey finger movements experiment,\nachieving a decoding accuracy of 99.3% for movement type. The same co-processor\nis also used to decode time of movement from asynchronous neural spikes. With\ntime-delayed feature dimension enhancement, the classification accuracy can be\nincreased by 5% with limited number of input channels. Further, a sparsity\npromoting training scheme enables reduction of number of programmable weights\nby ~2X.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 06:30:16 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2015 07:39:03 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Chen", "Yi", ""], ["Yao", "Enyi", ""], ["Basu", "Arindam", ""]]}, {"id": "1509.07543", "submitter": "Andreas Veit", "authors": "Andreas Veit, Michael Wilber, Rajan Vaish, Serge Belongie, James\n  Davis, Vishal Anand, Anshu Aviral, Prithvijit Chakrabarty, Yash Chandak,\n  Sidharth Chaturvedi, Chinmaya Devaraj, Ankit Dhall, Utkarsh Dwivedi, Sanket\n  Gupte, Sharath N. Sridhar, Karthik Paga, Anuj Pahuja, Aditya Raisinghani,\n  Ayush Sharma, Shweta Sharma, Darpana Sinha, Nisarg Thakkar, K. Bala Vignesh,\n  Utkarsh Verma, Kanniganti Abhishek, Amod Agrawal, Arya Aishwarya, Aurgho\n  Bhattacharjee, Sarveshwaran Dhanasekar, Venkata Karthik Gullapalli, Shuchita\n  Gupta, Chandana G, Kinjal Jain, Simran Kapur, Meghana Kasula, Shashi Kumar,\n  Parth Kundaliya, Utkarsh Mathur, Alankrit Mishra, Aayush Mudgal, Aditya\n  Nadimpalli, Munakala Sree Nihit, Akanksha Periwal, Ayush Sagar, Ayush Shah,\n  Vikas Sharma, Yashovardhan Sharma, Faizal Siddiqui, Virender Singh, Abhinav\n  S., Anurag. D. Yadav", "title": "On Optimizing Human-Machine Task Assignments", "comments": "HCOMP 2015 Work in Progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When crowdsourcing systems are used in combination with machine inference\nsystems in the real world, they benefit the most when the machine system is\ndeeply integrated with the crowd workers. However, if researchers wish to\nintegrate the crowd with \"off-the-shelf\" machine classifiers, this deep\nintegration is not always possible. This work explores two strategies to\nincrease accuracy and decrease cost under this setting. First, we show that\nreordering tasks presented to the human can create a significant accuracy\nimprovement. Further, we show that greedily choosing parameters to maximize\nmachine accuracy is sub-optimal, and joint optimization of the combined system\nimproves performance.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 21:38:07 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Veit", "Andreas", ""], ["Wilber", "Michael", ""], ["Vaish", "Rajan", ""], ["Belongie", "Serge", ""], ["Davis", "James", ""], ["Anand", "Vishal", ""], ["Aviral", "Anshu", ""], ["Chakrabarty", "Prithvijit", ""], ["Chandak", "Yash", ""], ["Chaturvedi", "Sidharth", ""], ["Devaraj", "Chinmaya", ""], ["Dhall", "Ankit", ""], ["Dwivedi", "Utkarsh", ""], ["Gupte", "Sanket", ""], ["Sridhar", "Sharath N.", ""], ["Paga", "Karthik", ""], ["Pahuja", "Anuj", ""], ["Raisinghani", "Aditya", ""], ["Sharma", "Ayush", ""], ["Sharma", "Shweta", ""], ["Sinha", "Darpana", ""], ["Thakkar", "Nisarg", ""], ["Vignesh", "K. Bala", ""], ["Verma", "Utkarsh", ""], ["Abhishek", "Kanniganti", ""], ["Agrawal", "Amod", ""], ["Aishwarya", "Arya", ""], ["Bhattacharjee", "Aurgho", ""], ["Dhanasekar", "Sarveshwaran", ""], ["Gullapalli", "Venkata Karthik", ""], ["Gupta", "Shuchita", ""], ["G", "Chandana", ""], ["Jain", "Kinjal", ""], ["Kapur", "Simran", ""], ["Kasula", "Meghana", ""], ["Kumar", "Shashi", ""], ["Kundaliya", "Parth", ""], ["Mathur", "Utkarsh", ""], ["Mishra", "Alankrit", ""], ["Mudgal", "Aayush", ""], ["Nadimpalli", "Aditya", ""], ["Nihit", "Munakala Sree", ""], ["Periwal", "Akanksha", ""], ["Sagar", "Ayush", ""], ["Shah", "Ayush", ""], ["Sharma", "Vikas", ""], ["Sharma", "Yashovardhan", ""], ["Siddiqui", "Faizal", ""], ["Singh", "Virender", ""], ["S.", "Abhinav", ""], ["Yadav", "Anurag. D.", ""]]}, {"id": "1509.07642", "submitter": "Tingshao Zhu", "authors": "Zhen Li, Jianjun Xu, Tingshao Zhu", "title": "Prediction of Brain States of Concentration and Relaxation in Real Time\n  with Portable Electroencephalographs", "comments": "18 pages,7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our research, we attempt to help people recognize their brain state of\nconcentration or relaxation more conveniently and in real time. Considering the\ninconvenience of wearing traditional multiple electrode electroencephalographs,\nwe choose Muse to collect data which is a portable headband launched lately\nwith a number of useful functions and channels and it is much easier for the\npublic to use. Besides, traditional online analysis did not focus on the\nsynchronism between users and computers and the time delay problem did exist.\nTo solve the problem, by building the Analytic Hierarchy Model, we choose the\ntwo gamma wave channels of F7 and F8 as the data source instead of using both\nbeta and alpha channels traditionally.Using the Common Space Pattern algorithm\nand the Support Vector Machine model, the channels we choose have a higher\nrecognition accuracy rate and smaller amount of data to be dealt with by the\ncomputer than the traditional ones. Furthermore, we make use of the Feedforward\nNeural Network Model to predict subjects'brain states in half a second.\nFinally, we design a plane program in Python where a plane can be controlled to\ngo up or down when users concentrate or relax. The SVM model and the\nFeedforward Neural Network model have both been tested by 12 subjects and they\ngive an evaluation ranging from 1 to 10 points. The former gets 7.58 points\nwhile the latter gets 8.83, which proves that the time delay problem is\nimproved once more.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 09:17:31 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Li", "Zhen", ""], ["Xu", "Jianjun", ""], ["Zhu", "Tingshao", ""]]}, {"id": "1509.08037", "submitter": "Takahiro Kawabe", "authors": "Takahiro Kawabe, Taiki Fukiage, Masataka Sawayama, Shin'ya Nishida", "title": "Deformation Lamps: A Projection Technique to Make a Static Object\n  Dynamic", "comments": "21 pages, 8 figures", "journal-ref": "ACM Transactions on Applied Perception 13, 2, Article 10, 2016", "doi": "10.1145/2874358", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light projection is a powerful technique to edit appearances of objects in\nthe real world. Based on pixel-wise modification of light transport, previous\ntechniques have successfully modified static surface properties such as surface\ncolor, dynamic range, gloss and shading. Here, we propose an alternative light\nprojection technique that adds a variety of illusory, yet realistic distortions\nto a wide range of static 2D and 3D projection targets. The key idea of our\ntechnique, named Deformation Lamps, is to project only dynamic luminance\ninformation, which effectively activates the motion (and shape) processing in\nthe visual system, while preserving the color and texture of the original\nobject. Although the projected dynamic luminance information is spatially\ninconsistent with the color and texture of the target object, the observer's\nbrain automatically com- bines these sensory signals in such a way as to\ncorrect the inconsistency across visual attributes. We conducted a\npsychophysical experiment to investigate the characteristics of the\ninconsistency correction, and found that the correction was dependent\ncritically on the retinal magnitude of inconsistency. Another experiment showed\nthat perceived magnitude of image deformation by our techniques was\nunderestimated. The results ruled out the possibility that the effect by our\ntechnique stemmed simply from the physical change of object appearance by light\nprojection. Finally, we discuss how our techniques can make the observers\nperceive a vivid and natural movement, deformation, or oscillation of a variety\nof static objects, including drawn pictures, printed photographs, sculptures\nwith 3D shading, objects with natural textures including human bodies.\n", "versions": [{"version": "v1", "created": "Sun, 27 Sep 2015 00:07:07 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Kawabe", "Takahiro", ""], ["Fukiage", "Taiki", ""], ["Sawayama", "Masataka", ""], ["Nishida", "Shin'ya", ""]]}, {"id": "1509.08257", "submitter": "Tingshao Zhu", "authors": "Zhen Li, Jianjun Xu, Tingshao Zhu", "title": "Recognition of Brain Waves of Left and Right Hand Movement Imagery with\n  Portable Electroencephalographs", "comments": "13 pages,4 figures,4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1509.07642", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of the modern society, mind control applied to both the\nrecovery of disabled individuals and auxiliary control of normal people has\nobtained great attention in numerous researches. In our research, we attempt to\nrecognize the brain waves of left and right hand movement imagery with portable\nelectroencephalographs. Considering the inconvenience of wearing traditional\nmultiple-electrode electroencephalographs, we choose Muse to collect data which\nis a portable headband launched lately with a number of useful functions and\nchannels and it is much easier for the public to use. Additionally, previous\nresearches generally focused on discrimination of EEG of left and right hand\nmovement imagery by using data from C3 and C4 electrodes which locate on the\ntop of the head. However, we choose the gamma wave channels of F7 and F8 and\nobtain data when subjects imagine their left or right hand to move with their\neyeballs rotated in the corresponding direction. With the help of the Common\nSpace Pattern algorithm to extract features of brain waves between left and\nright hand movement imagery, we make use of the Support Vector Machine to\nclassify different brain waves. Traditionally, the accuracy rate of\nclassification was approximately 90% using the EEG data from C3 and C4\nelectrode poles; however, the accuracy rate reaches 95.1% by using the gamma\nwave data from F7 and F8 in our experiment. Finally, we design a plane program\nin Python where a plane can be controlled to go left or right when users\nimagine their left or right hand to move. 8 subjects are tested and all of them\ncan control the plane flexibly which reveals that our model can be applied to\ncontrol hardware which is useful for disabled individuals and normal people.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 09:58:55 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Li", "Zhen", ""], ["Xu", "Jianjun", ""], ["Zhu", "Tingshao", ""]]}]