[{"id": "2010.00339", "submitter": "Stephan Wiefling", "authors": "Stephan Wiefling, Markus D\\\"urmuth, Luigi Lo Iacono", "title": "More Than Just Good Passwords? A Study on Usability and Security\n  Perceptions of Risk-based Authentication", "comments": "16 pages, 10 figures, 6 tables", "journal-ref": "36th Annual Computer Security Applications Conference (ACSAC '20).\n  December 07-11, 2020", "doi": "10.1145/3427228.3427243", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk-based Authentication (RBA) is an adaptive security measure to strengthen\npassword-based authentication. RBA monitors additional features during login,\nand when observed feature values differ significantly from previously seen\nones, users have to provide additional authentication factors such as a\nverification code. RBA has the potential to offer more usable authentication,\nbut the usability and the security perceptions of RBA are not studied well.\n  We present the results of a between-group lab study (n=65) to evaluate\nusability and security perceptions of two RBA variants, one 2FA variant, and\npassword-only authentication. Our study shows with significant results that RBA\nis considered to be more usable than the studied 2FA variants, while it is\nperceived as more secure than password-only authentication in general and\ncomparably secure to 2FA in a variety of application types. We also observed\nRBA usability problems and provide recommendations for mitigation. Our\ncontribution provides a first deeper understanding of the users' perception of\nRBA and helps to improve RBA implementations for a broader user acceptance.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 12:11:51 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Wiefling", "Stephan", ""], ["D\u00fcrmuth", "Markus", ""], ["Iacono", "Luigi Lo", ""]]}, {"id": "2010.00488", "submitter": "Min Chen", "authors": "Darren J. Edwards and Min Chen", "title": "Developing Effective Community Network Analysis Tools According to\n  Visualization Psychology", "comments": "The first author gives a presentation at IEEE VIS2020 Workshop on\n  Visualization Psychology based on this arXiv report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization is a useful technology in health science, and especially for\ncommunity network analysis. Because visualization applications in healthcare\nare typically risk-averse, health psychologists can play a significant role in\nensuring appropriate and effective uses of visualization techniques in\nhealthcare. In this paper, we examine the role of health psychologists in the\ntriangle of \"health science\", \"visualization technology\", and \"visualization\npsychology\". We conclude that health psychologists can use visualization to aid\ndata intelligence workflows in healthcare and health psychology, while\nresearching into visualization psychology to aid the improvement and\noptimization of data visualization processes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 15:29:37 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Edwards", "Darren J.", ""], ["Chen", "Min", ""]]}, {"id": "2010.00544", "submitter": "Imani Sherman", "authors": "Imani N. Sherman, Elissa M. Redmiles, Jack W. Stokes", "title": "Designing Indicators to Combat Fake Media", "comments": "26 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth of misinformation technology necessitates the need to identify\nfake videos. One approach to preventing the consumption of these fake videos is\nprovenance which allows the user to authenticate media content to its original\nsource. This research designs and investigates the use of provenance indicators\nto help users identify fake videos. We first interview users regarding their\nexperiences with different misinformation modes (text, image, video) to guide\nthe design of indicators within users' existing perspectives. Then, we conduct\na participatory design study to develop and design fake video indicators.\nFinally, we evaluate participant-designed indicators via both expert\nevaluations and quantitative surveys with a large group of end-users. Our\nresults provide concrete design guidelines for the emerging issue of fake\nvideos. Our findings also raise concerns regarding users' tendency to\novergeneralize from misinformation warning messages, suggesting the need for\nfurther research on warning design in the ongoing fight against misinformation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 16:58:12 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Sherman", "Imani N.", ""], ["Redmiles", "Elissa M.", ""], ["Stokes", "Jack W.", ""]]}, {"id": "2010.00656", "submitter": "Rui Meng", "authors": "Rui Meng, Zhen Yue, Alyssa Glass", "title": "Predicting User Engagement Status for Online Evaluation of Intelligent\n  Assistants", "comments": "Paper has been accepted by ECIR 2021 (43rd edition of the annual\n  European Conference on Information Retrieval)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluation of intelligent assistants in large-scale and online settings\nremains an open challenge. User behavior-based online evaluation metrics have\ndemonstrated great effectiveness for monitoring large-scale web search and\nrecommender systems. Therefore, we consider predicting user engagement status\nas the very first and critical step to online evaluation for intelligent\nassistants. In this work, we first proposed a novel framework for classifying\nuser engagement status into four categories -- fulfillment, continuation,\nreformulation and abandonment. We then demonstrated how to design simple but\nindicative metrics based on the framework to quantify user engagement levels.\nWe also aim for automating user engagement prediction with machine learning\nmethods. We compare various models and features for predicting engagement\nstatus using four real-world datasets. We conducted detailed analyses on\nfeatures and failure cases to discuss the performance of current models as well\nas challenges.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 19:33:27 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 22:34:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Meng", "Rui", ""], ["Yue", "Zhen", ""], ["Glass", "Alyssa", ""]]}, {"id": "2010.00828", "submitter": "Joachim Meyer", "authors": "Joachim Meyer and James K. Kuchar", "title": "Maximal benefits and possible detrimental effects of binary decision\n  aids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary decision aids, such as alerts, are a simple and widely used form of\nautomation. The formal analysis of a user's task performance with an aid sees\nthe process as the combination of information from two detectors who both\nreceive input about an event and evaluate it. The user's decisions are based on\nthe output of the aid and on the information, the user obtains independently.\nWe present a simple method for computing the maximal benefits a user can derive\nfrom a binary aid as a function of the user's and the aid's sensitivities.\nCombining the user and the aid often adds little to the performance the better\ndetector could achieve alone. Also, if users assign non-optimal weights to the\naid, performance may drop dramatically. Thus, the introduction of a valid aid\ncan actually lower detection performance, compared to a more sensitive user\nworking alone. Similarly, adding a user to a system with high sensitivity may\nlower its performance. System designers need to consider the potential adverse\neffects of introducing users or aids into systems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 07:46:58 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Meyer", "Joachim", ""], ["Kuchar", "James K.", ""]]}, {"id": "2010.00942", "submitter": "Gian-Luca Savino", "authors": "Gian-Luca Savino", "title": "Virtual Smartphone: High Fidelity Interaction with Proxy Objects in\n  Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This workshop paper presents two proxy objects for high fidelity interaction\nin virtual reality (VR): a paper map and a smartphone. We showcase how our\nvirtual paper map can increase interactivity and orientation, while our virtual\nsmartphone extends the use of a proxy object, as it allows for actual touch\ninput on a real phone leading to an almost infinite set of possible\n(inter-)actions (e.g. snapping pictures in the virtual world). Observations\nshowed that participants were very precise in holding and interacting with both\nthe paper map and the smartphone even though they did not see their hands in\nVR. The interaction in general was very intuitive which was mostly attributed\nto the realistic size of the virtual objects. Using our findings we discuss the\ntrade off between adaptivity and high fidelity of proxy objects in VR.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 12:07:29 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Savino", "Gian-Luca", ""]]}, {"id": "2010.00995", "submitter": "Ylva Ferstl", "authors": "Ylva Ferstl, Michael Neff, Rachel McDonnell", "title": "Understanding the Predictability of Gesture Parameters from Speech and\n  their Perceptual Importance", "comments": "To be published in the Proceedings of the 20th ACM International\n  Conference on Intelligent Virtual Agents (IVA 20)", "journal-ref": null, "doi": "10.1145/3383652.3423882", "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture behavior is a natural part of human conversation. Much work has\nfocused on removing the need for tedious hand-animation to create embodied\nconversational agents by designing speech-driven gesture generators. However,\nthese generators often work in a black-box manner, assuming a general\nrelationship between input speech and output motion. As their success remains\nlimited, we investigate in more detail how speech may relate to different\naspects of gesture motion. We determine a number of parameters characterizing\ngesture, such as speed and gesture size, and explore their relationship to the\nspeech signal in a two-fold manner. First, we train multiple recurrent networks\nto predict the gesture parameters from speech to understand how well gesture\nattributes can be modeled from speech alone. We find that gesture parameters\ncan be partially predicted from speech, and some parameters, such as path\nlength, being predicted more accurately than others, like velocity. Second, we\ndesign a perceptual study to assess the importance of each gesture parameter\nfor producing motion that people perceive as appropriate for the speech.\nResults show that a degradation in any parameter was viewed negatively, but\nsome changes, such as hand shape, are more impactful than others. A video\nsummarization can be found at https://youtu.be/aw6-_5kmLjY.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 13:43:33 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ferstl", "Ylva", ""], ["Neff", "Michael", ""], ["McDonnell", "Rachel", ""]]}, {"id": "2010.01023", "submitter": "Shubham Singh", "authors": "Shubham Singh, Zengou Ma, Daniele Giunchi, Anthony Steed", "title": "Real-time Collaboration Between Mixed Reality Users in Geo-referenced\n  Virtual Environment", "comments": "7 pages, 6 figures, International Symposium of Mixed and Augmented\n  Reality Conference, IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaboration using mixed reality technology is an active area of research,\nwhere significant research is done to virtually bridge physical distances.\nThere exist a diverse set of platforms and devices that can be used for a\nmixed-reality collaboration, and is largely focused for indoor scenarios,\nwhere, a stable tracking can be assumed. We focus on supporting collaboration\nbetween VR and AR users, where AR user is mobile outdoors, and VR user is\nimmersed in true-sized digital twin. This cross-platform solution requires new\nuser experiences for interaction, accurate modelling of the real-world, and\nworking with noisy outdoor tracking sensor such as GPS. In this paper, we\npresent our results and observations of real-time collaboration between\ncross-platform users, in the context of a geo-referenced virtual environment.\nWe propose a solution for using GPS measurement in VSLAM to localize the AR\nuser in an outdoor environment. The client applications enable VR and AR user\nto collaborate across the heterogeneous platforms seamlessly. The user can\nplace or load dynamic contents tagged to a geolocation and share their\nexperience with remote users in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:23:39 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Singh", "Shubham", ""], ["Ma", "Zengou", ""], ["Giunchi", "Daniele", ""], ["Steed", "Anthony", ""]]}, {"id": "2010.01316", "submitter": "Rob Procter", "authors": "Rob Procter, Mark Rouncefield, Peter Tolmie", "title": "Accounts, Accountability and Agency for Safe and Ethical AI", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine the problem of explainable AI (xAI) and explore what delivering\nxAI means in practice, particularly in contexts that involve formal or informal\nand ad-hoc collaboration where agency and accountability in decision-making are\nachieved and sustained interactionally. We use an example from an earlier study\nof collaborative decision-making in screening mammography and the difficulties\nusers faced when trying to interpret the behavior of an AI tool to illustrate\nthe challenges of delivering usable and effective xAI. We conclude by setting\nout a study programme for future research to help advance our understanding of\nxAI requirements for safe and ethical AI.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 10:05:58 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Procter", "Rob", ""], ["Rouncefield", "Mark", ""], ["Tolmie", "Peter", ""]]}, {"id": "2010.01323", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young, Katie Crowley", "title": "The Design of Tangible Digital Musical Instruments", "comments": "MusTWork 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present guidelines that highlight the impact of haptic feedback upon\nthe experiences of computer musicians using Digital Musical Instruments (DMIs).\nIn this context, haptic feedback offers a tangible, bi-directional exchange\nbetween a musician and a DMI. We propose that by adhering to and exploring\nthese guidelines the application of haptic feedback can enhance and augment the\nphysical and affective experiences of a musician in interactions with these\ndevices. It has been previously indicated that in the design of haptic DMIs,\nthe experiences and expectations of a musician must be considered for the\ncreation of tangible DMIs and that haptic feedback can be used to address the\nphysical-digital divide that currently exists between users of such\ninstruments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 11:08:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Young", "Gareth W.", ""], ["Crowley", "Katie", ""]]}, {"id": "2010.01326", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young, Dave Murphy", "title": "Digital Musical Instrument Analysis: The Haptic Bowl", "comments": "CMMR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This experiment is a case study that applies a HCI-informed DMI Evaluation\nFramework. This framework applies existing HCI evaluation methods to the\nassessment of prototype Digital Musical Instruments (DMIs). The overall study\nwill involve a three-part analysis - a description and categorisation of the\ndevice, a functionality evaluation that included an examination of usability\nand user experience, and finally an exploration of the device's effectiveness\nas a digital instrument. Here we present the findings of the first two parts of\nthe framework, outlining the constituent components of the interface and\ntesting the functionality of the device. The final stage of analysis will\ninvolve a longitudinal study and will be carried out in order to assess the\nmusical affordances of the device.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 11:18:33 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Young", "Gareth W.", ""], ["Murphy", "Dave", ""]]}, {"id": "2010.01328", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young, Dave Murphy", "title": "HCI Models for Digital Musical Instruments: Methodologies for Rigorous\n  Testing of Digital Musical Instruments", "comments": "CMMR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present an analysis of literature relating to the evaluation\nmethodologies of Digital Musical Instruments (DMIs) derived from the field of\nHuman-Computer Interaction (HCI). We then apply choice aspects from these\nexisting evaluation models and apply them to an optimized evaluation for\nassessing new DMIs.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 11:22:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Young", "Gareth W.", ""], ["Murphy", "Dave", ""]]}, {"id": "2010.01478", "submitter": "Shruthi Chari", "authors": "Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman,\n  Amar K. Das, Deborah L. McGuinness", "title": "Explanation Ontology in Action: A Clinical Use-Case", "comments": "5 pages, 2 figures, 1 protocol", "journal-ref": "International Semantic Web Conference, Poster and Demo Track, 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We addressed the problem of a lack of semantic representation for\nuser-centric explanations and different explanation types in our Explanation\nOntology (https://purl.org/heals/eo). Such a representation is increasingly\nnecessary as explainability has become an important problem in Artificial\nIntelligence with the emergence of complex methods and an uptake in\nhigh-precision and user-facing settings. In this submission, we provide\nstep-by-step guidance for system designers to utilize our ontology, introduced\nin our resource track paper, to plan and model for explanations during the\ndesign of their Artificial Intelligence systems. We also provide a detailed\nexample with our utilization of this guidance in a clinical setting.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 03:52:39 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chari", "Shruthi", ""], ["Seneviratne", "Oshani", ""], ["Gruen", "Daniel M.", ""], ["Foreman", "Morgan A.", ""], ["Das", "Amar K.", ""], ["McGuinness", "Deborah L.", ""]]}, {"id": "2010.01479", "submitter": "Shruthi Chari", "authors": "Shruthi Chari, Oshani Seneviratne, Daniel M. Gruen, Morgan A. Foreman,\n  Amar K. Das, Deborah L. McGuinness", "title": "Explanation Ontology: A Model of Explanations for User-Centered AI", "comments": "16 pages (but 1 reference over on arxiv), 5 tables, 3 code listings,\n  1 figure", "journal-ref": "International Semantic Web Conference (ISWC), 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainability has been a goal for Artificial Intelligence (AI) systems since\ntheir conception, with the need for explainability growing as more complex AI\nmodels are increasingly used in critical, high-stakes settings such as\nhealthcare. Explanations have often added to an AI system in a non-principled,\npost-hoc manner. With greater adoption of these systems and emphasis on\nuser-centric explainability, there is a need for a structured representation\nthat treats explainability as a primary consideration, mapping end user needs\nto specific explanation types and the system's AI capabilities. We design an\nexplanation ontology to model both the role of explanations, accounting for the\nsystem and user attributes in the process, and the range of different\nliterature-derived explanation types. We indicate how the ontology can support\nuser requirements for explanations in the domain of healthcare. We evaluate our\nontology with a set of competency questions geared towards a system designer\nwho might use our ontology to decide which explanation types to include, given\na combination of users' needs and a system's capabilities, both in system\ndesign settings and in real-time operations. Through the use of this ontology,\nsystem designers will be able to make informed choices on which explanations AI\nsystems can and should provide.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 03:53:35 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chari", "Shruthi", ""], ["Seneviratne", "Oshani", ""], ["Gruen", "Daniel M.", ""], ["Foreman", "Morgan A.", ""], ["Das", "Amar K.", ""], ["McGuinness", "Deborah L.", ""]]}, {"id": "2010.01567", "submitter": "Michael Lyons", "authors": "Michael J. Lyons", "title": "Facial gesture interfaces for expression and communication", "comments": "6 pages, 8 figures", "journal-ref": "2004 IEEE International Conference on Systems, Man and Cybernetics", "doi": "10.1109/ICSMC.2004.1398365", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable effort has been devoted to the automatic extraction of\ninformation about action of the face from image sequences. Within the context\nof human-computer interaction (HCI) we may distinguish systems that allow\nexpression from those which aim at recognition. Most of the work in facial\naction processing has been directed at automatically recognizing affect from\nfacial actions. By contrast, facial gesture interfaces, which respond to\ndeliberate facial actions, have received comparatively little attention. This\npaper reviews several projects on vision-based interfaces that rely on facial\naction for intentional HCI. Applications to several domains are introduced,\nincluding text entry, artistic and musical expression and assistive technology\nfor motor-impaired users.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:51:48 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Lyons", "Michael J.", ""]]}, {"id": "2010.01569", "submitter": "Bill Verplank", "authors": "Bill Verplank, Craig Sapp, and Max Mathews", "title": "A Course on Controllers", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176380", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last four years, we have developed a series of lectures, labs and\nproject assignments aimed at introducing enough technology so that students\nfrom a mix of disciplines can design and build innovative interface devices.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:55:18 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Verplank", "Bill", ""], ["Sapp", "Craig", ""], ["Mathews", "Max", ""]]}, {"id": "2010.01570", "submitter": "Matthew Wright", "authors": "David Wessel and Matthew Wright", "title": "Problems and Prospects for Intimate Musical Control of Computers", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176382", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we describe our efforts towards the development of live\nperformance computer-based musical instrumentation. Our design criteria include\ninitial ease of use coupled with a long term potential for virtuosity, minimal\nand low variance latency, and clear and simple strategies for programming the\nrelationship between gesture and musical result. We present custom controllers\nand unique adaptations of standard gestural interfaces, a programmable\nconnectivity processor, a communications protocol called Open Sound Control\n(OSC), and a variety of metaphors for musical control. We further describe\napplications of our technology to a variety of real musical performances and\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:55:43 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wessel", "David", ""], ["Wright", "Matthew", ""]]}, {"id": "2010.01571", "submitter": "Nicola Orio", "authors": "Nicola Orio, Norbert Schnell, and Marcelo M. Wanderley", "title": "Input Devices for Musical Expression: Borrowing Tools from HCI", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176370", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper reviews the existing literature on input device evaluation and\ndesign in human-computer interaction (HCI) and discusses possible applications\nof this knowledge to the design and evaluation of new interfaces for musical\nexpression. Specifically, a set of musical tasks is suggested to allow the\nevaluation of different existing controllers.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:56:14 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Orio", "Nicola", ""], ["Schnell", "Norbert", ""], ["Wanderley", "Marcelo M.", ""]]}, {"id": "2010.01572", "submitter": "Camille Goudeseune", "authors": "Camille Goudeseune, Guy Garnett, and Timothy Johnson", "title": "Resonant Processing of Instrumental Sound Controlled by Spatial Position", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176362", "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an acoustic musical instrument played through a resonance model of\nanother sound. The resonance model is controlled in real time as part of the\ncomposite instrument. Our implementation uses an electric violin, whose spatial\nposition modifies filter parameters of the resonance model. Simplicial\ninterpolation defines the mapping from spatial position to filter parameters.\nWith some effort, pitch tracking can also control the filter parameters. The\nindividual technologies -- motion tracking, pitch tracking, resonance models --\nare easily adapted to other instruments.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:56:41 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Goudeseune", "Camille", ""], ["Garnett", "Guy", ""], ["Johnson", "Timothy", ""]]}, {"id": "2010.01574", "submitter": "Michael Gurevich", "authors": "Michael Gurevich and Stephan von Muehlen", "title": "The Accordiatron: A MIDI Controller For Interactive Music", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176364", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Accordiatron is a new MIDI controller for real-time performance based on\nthe paradigm of a conventional squeeze box or concertina. It translates the\ngestures of a performer to the standard communication protocol of MIDI,\nallowing for flexible mappings of performance data to sonic parameters. When\nused in conjunction with a realtime signal processing environment, the\nAccordiatron becomes an expressive, versatile musical instrument. A combination\nof sensory outputs providing both discrete and continuous data gives the subtle\nexpressiveness and control necessary for interactive music.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:57:08 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gurevich", "Michael", ""], ["von Muehlen", "Stephan", ""]]}, {"id": "2010.01575", "submitter": "Joseph A. Paradiso", "authors": "Joseph A. Paradiso, Kai-yuh Hsiao, and Ari Benbasat", "title": "Tangible Music Interfaces Using Passive Magnetic Tags", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176374", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The technologies behind passive resonant magnetically coupled tags are\nintroduced and their application as a musical controller is illustrated for\nsolo or group performances, interactive installations, and music toys.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:57:33 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Paradiso", "Joseph A.", ""], ["Hsiao", "Kai-yuh", ""], ["Benbasat", "Ari", ""]]}, {"id": "2010.01576", "submitter": "Kenji Mase", "authors": "Kenji Mase and Tomoko Yonezawa", "title": "Body, Clothes, Water, and Toys: Media Towards Natural Music Expressions\n  with Digital Sounds", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176368", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce our research challenges for creating new musical\ninstruments using everyday-life media with intimate interfaces, such as the\nself-body, clothes, water and stuffed toys. Various sensor technologies\nincluding image processing and general touch sensitive devices are employed to\nexploit these interaction media. The focus of our effort is to provide\nuser-friendly and enjoyable experiences for new music and sound performances.\nMultimodality of musical instruments is explored in each attempt. The degree of\ncontrollability in the performance and the richness of expressions are also\ndiscussed for each installation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:57:58 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Mase", "Kenji", ""], ["Yonezawa", "Tomoko", ""]]}, {"id": "2010.01577", "submitter": "Dan Overholt", "authors": "Dan Overholt", "title": "The MATRIX: A Novel Controller for Musical Expression", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176372", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The MATRIX (Multipurpose Array of Tactile Rods for Interactive eXpression) is\na new musical interface for amateurs and professionals alike. It gives users a\n3- dimensional tangible interface to control music using their hands, and can\nbe used in conjunction with a traditional musical instrument and a microphone,\nor as a stand-alone gestural input device. The surface of the MATRIX acts as a\nreal-time interface that can manipulate the parameters of a synthesis engine or\neffect algorithm in response to a performer's expressive gestures. One example\nis to have the rods of the MATRIX control the individual grains of a granular\nsynthesizer, thereby \"sonically sculpting\" the microstructure of a sound. In\nthis way, the MATRIX provides an intuitive method of manip\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:58:23 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Overholt", "Dan", ""]]}, {"id": "2010.01578", "submitter": "Gideon D'Arcangelo", "authors": "Gideon D'Arcangelo", "title": "Creating Contexts of Creativity: Musical Composition with Modular\n  Components", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176360", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a series of projects that explore the possibilities of\nmusical expression through the combination of pre-composed, interlocking,\nmodular components. In particular, this paper presents a modular soundtrack\nrecently composed by the author for \"Currents of Creativity,\" a permanent\ninteractive video wall installation at the Pope John Paul II Cultural Center\nwhich is slated to open Easter 2001 in Washington, DC.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:58:51 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["D'Arcangelo", "Gideon", ""]]}, {"id": "2010.01579", "submitter": "Sergi Jord\\`a", "authors": "Sergi Jord\\`a", "title": "New Musical Interfaces and New Music-making Paradigms", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176366", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The conception and design of new musical interfaces is a multidisciplinary\narea that tightly relates technology and artistic creation. In this paper, the\nauthor first exposes some of the questions he has posed himself during more\nthan a decade experience as a performer, composer, interface and software\ndesigner, and educator. Finally, he illustrates these topics with some examples\nof his work.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 12:59:20 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jord\u00e0", "Sergi", ""]]}, {"id": "2010.01651", "submitter": "Huyen N. Nguyen", "authors": "Huyen N. Nguyen, Vinh T. Nguyen, Tommy Dang", "title": "Interface Design for HCI Classroom: From Learners' Perspective", "comments": "12 pages, 4 figures, 15th International Symposium on Visual Computing\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Having a good Human-Computer Interaction (HCI) design is challenging.\nPrevious works have contributed significantly to fostering HCI, including\ndesign principle with report study from the instructor view. The questions of\nhow and to what extent students perceive the design principles are still left\nopen. To answer this question, this paper conducts a study of HCI adoption in\nthe classroom. The studio-based learning method was adapted to teach 83\ngraduate and undergraduate students in 16 weeks long with four activities. A\nstandalone presentation tool for instant online peer feedback during the\npresentation session was developed to help students justify and critique\nother's work. Our tool provides a sandbox, which supports multiple application\ntypes, including Web-applications, Object Detection, Web-based Virtual Reality\n(VR), and Augmented Reality (AR). After presenting one assignment and two\nprojects, our results showed that students acquired a better understanding of\nthe Golden Rules principle over time, which was demonstrated by the development\nof visual interface design. The Wordcloud reveals the primary focus was on the\nuser interface and shed some light on students' interest in user experience.\nThe inter-rater score indicates the agreement among students that they have the\nsame level of understanding of the principles. The results show a high level of\nguideline compliance with HCI principles, in which we witnessed variations in\nvisual cognitive styles. Regardless of diversity in visual preference, the\nstudents presented high consistency and a similar perspective on adopting HCI\ndesign principles. The results also elicited suggestions into the development\nof the HCI curriculum in the future.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 18:49:24 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Nguyen", "Huyen N.", ""], ["Nguyen", "Vinh T.", ""], ["Dang", "Tommy", ""]]}, {"id": "2010.01662", "submitter": "Hancheng Cao", "authors": "Zhilong Chen, Hancheng Cao, Yuting Deng, Xuan Gao, Jinghua Piao,\n  Fengli Xu, Yu Zhang, Yong Li", "title": "Learning from Home: A Mixed-Methods Analysis of Live Streaming Based\n  Remote Education Experience in Chinese Colleges During the COVID-19 Pandemic", "comments": "Zhilong Chen and Hancheng Cao contribute equally to this work;\n  Accepted to CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445428", "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 global pandemic and resulted lockdown policies have forced\neducation in nearly every country to switch from a traditional co-located\nparadigm to a pure online 'distance learning from home' paradigm. Lying in the\ncenter of this learning paradigm shift is the emergence and wide adoption of\ndistance communication tools and live streaming platforms for education. Here,\nwe present a mixed-methods study on live streaming based education experience\nduring the COVID-19 pandemic. We focus our analysis on Chinese higher\neducation, carried out semi-structured interviews on 30 students, and 7\ninstructors from diverse colleges and disciplines, meanwhile launched a\nlarge-scale survey covering 6291 students and 1160 instructors in one leading\nChinese university. Our study not only reveals important design guidelines and\ninsights to better support current remote learning experience during the\npandemic, but also provides valuable implications towards constructing future\ncollaborative education supporting systems and experience after pandemic.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 19:14:09 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 05:33:16 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Chen", "Zhilong", ""], ["Cao", "Hancheng", ""], ["Deng", "Yuting", ""], ["Gao", "Xuan", ""], ["Piao", "Jinghua", ""], ["Xu", "Fengli", ""], ["Zhang", "Yu", ""], ["Li", "Yong", ""]]}, {"id": "2010.01693", "submitter": "Oluwatobi Olabiyi", "authors": "Oluwatobi O. Olabiyi, Prarthana Bhattarai, C. Bayan Bruss, Zachary\n  Kulis", "title": "DLGNet-Task: An End-to-end Neural Network Framework for Modeling\n  Multi-turn Multi-domain Task-Oriented Dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG cs.NE", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Task oriented dialogue (TOD) requires the complex interleaving of a number of\nindividually controllable components with strong guarantees for explainability\nand verifiability. This has made it difficult to adopt the multi-turn\nmulti-domain dialogue generation capabilities of streamlined end-to-end\nopen-domain dialogue systems. In this paper, we present a new framework,\nDLGNet-Task, a unified task-oriented dialogue system which employs\nautoregressive transformer networks such as DLGNet and GPT-2/3 to complete user\ntasks in multi-turn multi-domain conversations. Our framework enjoys the\ncontrollable, verifiable, and explainable outputs of modular approaches, and\nthe low development, deployment and maintenance cost of end-to-end systems.\nTreating open-domain system components as additional TOD system modules allows\nDLGNet-Task to learn the joint distribution of the inputs and outputs of all\nthe functional blocks of existing modular approaches such as, natural language\nunderstanding (NLU), state tracking, action policy, as well as natural language\ngeneration (NLG). Rather than training the modules individually, as is common\nin real-world systems, we trained them jointly with appropriate module\nseparations. When evaluated on the MultiWOZ2.1 dataset, DLGNet-Task shows\ncomparable performance to the existing state-of-the-art approaches.\nFurthermore, using DLGNet-Task in conversational AI systems reduces the level\nof effort required for developing, deploying, and maintaining intelligent\nassistants at scale.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 21:43:17 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 16:31:06 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Olabiyi", "Oluwatobi O.", ""], ["Bhattarai", "Prarthana", ""], ["Bruss", "C. Bayan", ""], ["Kulis", "Zachary", ""]]}, {"id": "2010.01717", "submitter": "Nader Akoury", "authors": "Nader Akoury, Shufan Wang, Josh Whiting, Stephen Hood, Nanyun Peng,\n  Mohit Iyyer", "title": "STORIUM: A Dataset and Evaluation Platform for Machine-in-the-Loop Story\n  Generation", "comments": "Accepted as a long paper to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems for story generation are asked to produce plausible and enjoyable\nstories given an input context. This task is underspecified, as a vast number\nof diverse stories can originate from a single input. The large output space\nmakes it difficult to build and evaluate story generation models, as (1)\nexisting datasets lack rich enough contexts to meaningfully guide models, and\n(2) existing evaluations (both crowdsourced and automatic) are unreliable for\nassessing long-form creative text. To address these issues, we introduce a\ndataset and evaluation platform built from STORIUM, an online collaborative\nstorytelling community. Our author-generated dataset contains 6K lengthy\nstories (125M tokens) with fine-grained natural language annotations (e.g.,\ncharacter goals and attributes) interspersed throughout each narrative, forming\na robust source for guiding models. We evaluate language models fine-tuned on\nour dataset by integrating them onto STORIUM, where real authors can query a\nmodel for suggested story continuations and then edit them. Automatic metrics\ncomputed over these edits correlate well with both user ratings of generated\nstories and qualitative feedback from semi-structured user interviews. We\nrelease both the STORIUM dataset and evaluation platform to spur more\nprincipled research into story generation.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 23:26:09 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Akoury", "Nader", ""], ["Wang", "Shufan", ""], ["Whiting", "Josh", ""], ["Hood", "Stephen", ""], ["Peng", "Nanyun", ""], ["Iyyer", "Mohit", ""]]}, {"id": "2010.01821", "submitter": "Michael Lyons", "authors": "Laurent Prevost, Olivier Liechti, Michael J. Lyons", "title": "Design and Implementation of a Mobile Exergaming Platform", "comments": "8 pages, 3 figures, International Conference on Intelligent\n  Technologies for Interactive Entertainment, INTETAIN 2009", "journal-ref": "Lecture Notes of the Institute for Computer Sciences, Social\n  Informatics and Telecommunications Engineering, vol. 9, 2009", "doi": "10.1007/978-3-642-02315-6_22?", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design, implementation, and initial testing of a\nreusable platform for the creation of pervasive games with geo-localization\nservices. We concentrate on role-playing games built by combining several types\nof simpler mini-games having three major components: Quests; Collectables; and\nNon-player characters (NPC). Quests encourage players to be active in their\nphysical environment and take part in collaborative play; Collectables provide\nmotivation; and NPCs enable player-friendly interaction with the platform. Each\nof these elements poses different technical requirements, which were met by\nimplementing the gaming platform using the inTrack pervasive middle-ware being\ndeveloped by our group. Several sample games were implemented and tested within\nthe urban environment of Kyoto, Japan, using gaming clients running on mobile\nphones from NTT DoCoMo, Japan's largest mobile provider.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 07:16:09 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Prevost", "Laurent", ""], ["Liechti", "Olivier", ""], ["Lyons", "Michael J.", ""]]}, {"id": "2010.01944", "submitter": "Selma Rizvic", "authors": "Selma Rizvic, Dusanka Boskovic, Fabio Bruno, Barbara Davidde\n  Petriaggi, Sanda Sljivo, Marco Cozza", "title": "Actors in VR storytelling", "comments": "Pre-print version", "journal-ref": null, "doi": "10.1109/VS-Games.2019.8864520", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual Reality (VR) storytelling enhances the immersion of users into\nvirtual environments (VE). Its use in virtual cultural heritage presentations\nhelps the revival of the genius loci (the spirit of the place) of cultural\nmonuments. This paper aims to show that the use of actors in VR storytelling\nadds to the quality of user experience and improves the edutainment value of\nvirtual cultural heritage applications. We will describe the Baiae dry visit\napplication which takes us to a time travel in the city considered by the Roman\nelite as \"Little Rome (Pusilla Roma)\" and presently is only partially preserved\nunder the sea.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 12:14:37 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Rizvic", "Selma", ""], ["Boskovic", "Dusanka", ""], ["Bruno", "Fabio", ""], ["Petriaggi", "Barbara Davidde", ""], ["Sljivo", "Sanda", ""], ["Cozza", "Marco", ""]]}, {"id": "2010.01973", "submitter": "Adam Aviv", "authors": "Hirak Ray and Flynn Wolf and Ravi Kuber and Adam J. Aviv", "title": "Why Older Adults (Don't) Use Password Managers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Password managers (PMs) are considered highly effective tools for increasing\nsecurity, and a recent study by Pearman et al. (SOUPS'19) highlighted the\nmotivations and barriers to adopting PMs. We expand these findings by\nreplicating Pearman et al.'s protocol and interview instrument applied to a\nsample of strictly older adults (>60 years of age), as the prior work focused\non a predominantly younger cohort. We conducted n=26 semi-structured interviews\nwith PM users, built-in browser/operating system PM users, and non-PM users.\nThe average participant age was 70.4 years. Using the same codebook from\nPearman et al., we showcase differences and similarities in PM adoption between\nthe samples, including fears of a single point of failure and the importance of\nhaving control over one's private information. Meanwhile, older adults were\nfound to have higher mistrust of cloud storage of passwords and cross-device\nsynchronization. We also highlight PM adoption motivators for older adults,\nincluding the power of recommendations from family members and the importance\nof education and outreach to improve familiarity.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:02:47 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ray", "Hirak", ""], ["Wolf", "Flynn", ""], ["Kuber", "Ravi", ""], ["Aviv", "Adam J.", ""]]}, {"id": "2010.02002", "submitter": "Priyadarshini Kumari", "authors": "Priyadarshini Kumari and Subhasis Chaudhuri", "title": "Boosted Semantic Embedding based Discriminative Feature Generation for\n  Texture Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning discriminative features is crucial for various robotic applications\nsuch as object detection and classification. In this paper, we present a\ngeneral framework for the analysis of the discriminative properties of haptic\nsignals. Our focus is on two crucial components of a robotic perception system:\ndiscriminative feature extraction and metric-based feature transformation to\nenhance the separability of haptic signals in the projected space. We propose a\nset of hand-crafted haptic features (generated only from acceleration data),\nwhich enables discrimination of real-world textures. Since the Euclidean space\ndoes not reflect the underlying pattern in the data, we propose to learn an\nappropriate transformation function to project the feature onto the new space\nand apply different pattern recognition algorithms for texture classification\nand discrimination tasks. Unlike other existing methods, we use a triplet-based\nmethod for improved discrimination in the embedded space. We further\ndemonstrate how to build a haptic vocabulary by selecting a compact set of the\nmost distinct and representative signals in the embedded space. The\nexperimental results show that the proposed features augmented with learned\nembedding improves the performance of semantic discrimination tasks such as\nclassification and clustering and outperforms the related state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:38:23 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 07:12:00 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kumari", "Priyadarshini", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2010.02026", "submitter": "Maryam Alimardani", "authors": "Nikki Leeuwis and Maryam Alimardani", "title": "High Aptitude Motor Imagery BCI Users Have Better Visuospatial Memory", "comments": "Accepted in IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND\n  CYBERNETICS (SMC2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain computer interfaces (BCI) decode the electrophysiological signals from\nthe brain into an action that is carried out by a computer or robotic device.\nMotor imagery BCIs (MI BCI) rely on the user s imagination of bodily movements,\nhowever not all users can generate the brain activity needed to control MI BCI.\nThis difference in MI BCI performance among novice users could be due to their\ncognitive abilities. In this study, the impact of spatial abilities and\nvisuospatial memory on MI BCI performance is investigated. Fifty four novice\nusers participated in a MI BCI task and two cognitive tests. The impact of\nspatial abilities and visuospatial memory on BCI task error rate in three\nfeedback sessions was measured. Our results showed that spatial abilities, as\nassessed by the Mental Rotation Test, were not related to MI BCI performance,\nhowever visuospatial memory, assessed by the design organization test, was\nhigher in high aptitude users. Our findings can contribute to optimization of\nMI BCI training paradigms through participant screening and cognitive skill\ntraining.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 13:57:06 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Leeuwis", "Nikki", ""], ["Alimardani", "Maryam", ""]]}, {"id": "2010.02057", "submitter": "No\\'e Tits", "authors": "Jean-Benoit Delbrouck and No\\'e Tits and St\\'ephane Dupont", "title": "Modulated Fusion using Transformer for Linguistic-Acoustic Emotion\n  Recognition", "comments": "EMNLP 2020 workshop: NLP Beyond Text (NLPBT)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to bring a new lightweight yet powerful solution for the task\nof Emotion Recognition and Sentiment Analysis. Our motivation is to propose two\narchitectures based on Transformers and modulation that combine the linguistic\nand acoustic inputs from a wide range of datasets to challenge, and sometimes\nsurpass, the state-of-the-art in the field. To demonstrate the efficiency of\nour models, we carefully evaluate their performances on the IEMOCAP, MOSI,\nMOSEI and MELD dataset. The experiments can be directly replicated and the code\nis fully open for future researches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 14:46:20 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Delbrouck", "Jean-Benoit", ""], ["Tits", "No\u00e9", ""], ["Dupont", "St\u00e9phane", ""]]}, {"id": "2010.02117", "submitter": "Thomas Gross", "authors": "Thomas Gro{\\ss}", "title": "Statistical Reliability of 10 Years of Cyber Security User Studies\n  (Extended Version)", "comments": "Open Science Framework: https://osf.io/bcyte. 31 pages. This is the\n  author's copy. This work was supported by the ERC Starting Grant\n  Confidentiality-Preserving Security Assurance (CASCAde), GA no 716980", "journal-ref": "Proceedings of the 10th International Workshop on Socio-Technical\n  Aspects in Security (STAST 2020), LNCS 11739, Springer, 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background. In recent years, cyber security security user studies have been\nappraised in meta-research, mostly focusing on the completeness of their\nstatistical inferences and the fidelity of their statistical reporting.\nHowever, estimates of the field's distribution of statistical power and its\npublication bias have not received much attention. Aim. In this study, we aim\nto estimate the effect sizes and their standard errors present as well as the\nimplications on statistical power and publication bias. Method. We built upon a\npublished systematic literature review of $146$ user studies in cyber security\n(2006--2016). We took into account $431$ statistical inferences including $t$-,\n$\\chi^2$-, $r$-, one-way $F$-tests, and $Z$-tests. In addition, we coded the\ncorresponding total sample sizes, group sizes and test families. Given these\ndata, we established the observed effect sizes and evaluated the overall\npublication bias. We further computed the statistical power vis-{\\`a}-vis of\nparametrized population thresholds to gain unbiased estimates of the power\ndistribution. Results. We obtained a distribution of effect sizes and their\nconversion into comparable log odds ratios together with their standard errors.\nWe, further, gained funnel-plot estimates of the publication bias present in\nthe sample as well as insights into the power distribution and its\nconsequences. Conclusions. Through the lenses of power and publication bias, we\nshed light on the statistical reliability of the studies in the field. The\nupshot of this introspection is practical recommendations on conducting and\nevaluating studies to advance the field.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 16:02:32 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Gro\u00df", "Thomas", ""]]}, {"id": "2010.02207", "submitter": "Ryan Ulyate", "authors": "Ryan Ulyate and David Bianciardi", "title": "The Interactive Dance Club: Avoiding Chaos In A Multi Participant\n  Environment", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176378", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 1998 we designed enabling technology and a venue concept that allowed\nseveral participants to influence a shared musical and visual experience. Our\nprimary goal was to deliver musically coherent and visually satisfying results\nfrom several participants' input. The result, the Interactive Dance Club, ran\nfor four nights at the ACM SIGGRAPH 98 convention in Orlando, Florida. In this\npaper we will briefly describe the Interactive Dance Club, our \"10 Commandments\nof Interactivity,\" and what we learned from its premiere at SIGGRAPH 98.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 13:00:04 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Ulyate", "Ryan", ""], ["Bianciardi", "David", ""]]}, {"id": "2010.02263", "submitter": "Zhao Han", "authors": "Zhao Han, Alexander Wilkinson, Jenna Parrillo, Jordan Allspaw, Holly\n  A. Yanco", "title": "Projection Mapping Implementation: Enabling Direct Externalization of\n  Perception Results and Action Intent to Improve Robot Explainability", "comments": "5 pagers, 4 figures, the AI-HRI Symposium at AAAI-FSS 2020\n  (Proceedings: arXiv:2010.13830)", "journal-ref": "Proceedings of the AI-HRI Symposium at AAAI-FSS 2020, 2020", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing research on non-verbal cues, e.g., eye gaze or arm movement, may not\naccurately present a robot's internal states such as perception results and\naction intent. Projecting the states directly onto a robot's operating\nenvironment has the advantages of being direct, accurate, and more salient,\neliminating mental inference about the robot's intention. However, there is a\nlack of tools for projection mapping in robotics, compared to established\nmotion planning libraries (e.g., MoveIt). In this paper, we detail the\nimplementation of projection mapping to enable researchers and practitioners to\npush the boundaries for better interaction between robots and humans. We also\nprovide practical documentation and code for a sample manipulation projection\nmapping on GitHub: https://github.com/uml-robotics/projection_mapping.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:16:20 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 21:42:55 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 16:06:52 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Han", "Zhao", ""], ["Wilkinson", "Alexander", ""], ["Parrillo", "Jenna", ""], ["Allspaw", "Jordan", ""], ["Yanco", "Holly A.", ""]]}, {"id": "2010.02278", "submitter": "Zhao Han", "authors": "Zhao Han and Holly A. Yanco", "title": "Reasons People Want Explanations After Unrecoverable Pre-Handover\n  Failures", "comments": "Proceedings of the ICRA 2020 Workshop on Human-Robot Handovers, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most research on human-robot handovers focuses on the development of\ncomfortable and efficient HRI; few have studied handover failures. If a failure\noccurs in the beginning of the interaction, it prevents the whole handover\nprocess and destroys trust. Here we analyze the underlying reasons why people\nwant explanations in a handover scenario where a robot cannot possess the\nobject. Results suggest that participants set expectations on their request and\nthat a robot should provide explanations rather than non-verbal cues after\nfailing. Participants also expect that their handover request can be done by a\nrobot, and, if not, would like to be able to fix the robot or change the\nrequest based on the provided explanations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 18:37:23 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Han", "Zhao", ""], ["Yanco", "Holly A.", ""]]}, {"id": "2010.02401", "submitter": "Sarah Cooney", "authors": "Sarah Cooney and Barath Raghavan", "title": "Tactical Patterns for Grassroots Urban Repair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of revitalizing cities in the United States suffers from balky\nand unresponsive processes---de jure egalitarian but de facto controlled and\nmediated by city officials and powerful interests, not residents. We argue\nthat, instead, our goal should be to put city planning in the hands of the\npeople, and to that end, give ordinary residents pattern-based planning tools\nto help them redesign (and repair) their urban surrounds. Through this,\nresidents can explore many disparate ideas, try them, and, if successful,\nreplicate them, enabling bottom-up city planning through direct action. We\ndescribe a prototype for such a tool that leverages classic patterns to enable\ncity planning by residents, using case studies from Los Angeles as guides for\nboth the problem and potential solution.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 00:12:21 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Cooney", "Sarah", ""], ["Raghavan", "Barath", ""]]}, {"id": "2010.02561", "submitter": "Gian-Luca Savino", "authors": "Gian-Luca Savino, Niklas Emanuel, Steven Kowalzik, Felix A. Kroll,\n  Marvin C. Lange, Matthis Laudan, Rieke Leder, Zhanhua Liang, Dayana\n  Markhabayeva, Martin Schmei{\\ss}er, Nicolai Sch\\\"utz, Carolin Stellmacher,\n  Zihe Xu, Kerstin Bub, Thorsten Kluss, Jaime Maldonado, Ernst Kruijff,\n  Johannes Sch\\\"oning", "title": "Comparing Pedestrian Navigation Methods in Virtual Reality and Real Life", "comments": null, "journal-ref": null, "doi": "10.1145/3340555.3353741", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile navigation apps are among the most used mobile applications and are\noften used as a baseline to evaluate new mobile navigation technologies in\nfield studies. As field studies often introduce external factors that are hard\nto control for, we investigate how pedestrian navigation methods can be\nevaluated in virtual reality (VR). We present a study comparing navigation\nmethods in real life (RL) and VR to evaluate if VR environments are a viable\nalternative to RL environments when it comes to testing these. In a series of\nstudies, participants navigated a real and a virtual environment using a paper\nmap and a navigation app on a smartphone. We measured the differences in\nnavigation performance, task load and spatial knowledge acquisition between RL\nand VR. From these we formulate guidelines for the improvement of pedestrian\nnavigation systems in VR like improved legibility for small screen devices. We\nfurthermore discuss appropriate low-cost and low-space VR-locomotion techniques\nand discuss more controllable locomotion techniques.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 09:08:23 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Savino", "Gian-Luca", ""], ["Emanuel", "Niklas", ""], ["Kowalzik", "Steven", ""], ["Kroll", "Felix A.", ""], ["Lange", "Marvin C.", ""], ["Laudan", "Matthis", ""], ["Leder", "Rieke", ""], ["Liang", "Zhanhua", ""], ["Markhabayeva", "Dayana", ""], ["Schmei\u00dfer", "Martin", ""], ["Sch\u00fctz", "Nicolai", ""], ["Stellmacher", "Carolin", ""], ["Xu", "Zihe", ""], ["Bub", "Kerstin", ""], ["Kluss", "Thorsten", ""], ["Maldonado", "Jaime", ""], ["Kruijff", "Ernst", ""], ["Sch\u00f6ning", "Johannes", ""]]}, {"id": "2010.03046", "submitter": "Feng Zhou", "authors": "Feng Zhou, Yangjian Ji, Roger Jianxin Jiao", "title": "Emotional Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotional design has been well recognized in the domain of human factors and\nergonomics. In this chapter, we reviewed related models and methods of\nemotional design. We are motivated to encourage emotional designers to take\nmultiple perspectives when examining these models and methods. Then we proposed\na systematic process for emotional design, including affective-cognitive needs\nelicitation, affective-cognitive needs analysis, and affective-cognitive needs\nfulfillment to support emotional design. Within each step, we provided an\nupdated review of the representative methods to support and offer further\nguidance on emotional design. We hope researchers and industrial practitioners\ncan take a systematic approach to consider each step in the framework with\ncare. Finally, the speculations on the challenges and future directions can\npotentially help researchers across different fields to further advance\nemotional design.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:31:17 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhou", "Feng", ""], ["Ji", "Yangjian", ""], ["Jiao", "Roger Jianxin", ""]]}, {"id": "2010.03047", "submitter": "Feng Zhou", "authors": "Na Du, X. Jessie Yang, Feng Zhou", "title": "Psychophysiological responses to takeover requests in conditionally\n  automated driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In SAE Level 3 automated driving, taking over control from automation raises\nsignificant safety concerns because drivers out of the vehicle control loop\nhave difficulty negotiating takeover transitions. Existing studies on takeover\ntransitions have focused on drivers' behavioral responses to takeover requests\n(TORs). As a complement, this exploratory study aimed to examine drivers'\npsychophysiological responses to TORs as a result of varying\nnon-driving-related tasks (NDRTs), traffic density and TOR lead time. A total\nnumber of 102 drivers were recruited and each of them experienced 8 takeover\nevents in a high fidelity fixed-base driving simulator. Drivers' gaze\nbehaviors, heart rate (HR) activities, galvanic skin responses (GSRs), and\nfacial expressions were recorded and analyzed during two stages. First, during\nthe automated driving stage, we found that drivers had lower heart rate\nvariability, narrower horizontal gaze dispersion, and shorter eyes-on-road time\nwhen they had a high level of cognitive load relative to a low level of\ncognitive load. Second, during the takeover transition stage, 4s lead time led\nto inhibited blink numbers and larger maximum and mean GSR phasic activation\ncompared to 7s lead time, whilst heavy traffic density resulted in increased HR\nacceleration patterns than light traffic density. Our results showed that\npsychophysiological measures can indicate specific internal states of drivers,\nincluding their workload, emotions, attention, and situation awareness in a\ncontinuous, non-invasive and real-time manner. The findings provide additional\nsupport for the value of using psychophysiological measures in automated\ndriving and for future applications in driver monitoring systems and adaptive\nalert systems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:35:51 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Du", "Na", ""], ["Yang", "X. Jessie", ""], ["Zhou", "Feng", ""]]}, {"id": "2010.03070", "submitter": "Liam Dugan", "authors": "Liam Dugan, Daphne Ippolito, Arun Kirubarajan and Chris Callison-Burch", "title": "RoFT: A Tool for Evaluating Human Detection of Machine-Generated Text", "comments": "To be published in Annual Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, large neural networks for natural language generation (NLG)\nhave made leaps and bounds in their ability to generate fluent text. However,\nthe tasks of evaluating quality differences between NLG systems and\nunderstanding how humans perceive the generated text remain both crucial and\ndifficult. In this system demonstration, we present Real or Fake Text (RoFT), a\nwebsite that tackles both of these challenges by inviting users to try their\nhand at detecting machine-generated text in a variety of domains. We introduce\na novel evaluation task based on detecting the boundary at which a text passage\nthat starts off human-written transitions to being machine-generated. We show\npreliminary results of using RoFT to evaluate detection of machine-generated\nnews articles.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 22:47:43 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Dugan", "Liam", ""], ["Ippolito", "Daphne", ""], ["Kirubarajan", "Arun", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2010.03160", "submitter": "Xiaoyu Zeng", "authors": "Xiaoyu Zeng, Yanan Wang, Tai-Yin Chiu, Nilavra Bhattacharya, Danna\n  Gurari", "title": "Vision Skills Needed to Answer Visual Questions", "comments": "To be published on Proceedings of the ACM on Human-Computer\n  Interaction, Vol. 4, No. CSCW2, Article 149. Publication date: October 2020", "journal-ref": null, "doi": "10.1145/3415220", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The task of answering questions about images has garnered attention as a\npractical service for assisting populations with visual impairments as well as\na visual Turing test for the artificial intelligence community. Our first aim\nis to identify the common vision skills needed for both scenarios. To do so, we\nanalyze the need for four vision skills---object recognition, text recognition,\ncolor recognition, and counting---on over 27,000 visual questions from two\ndatasets representing both scenarios. We next quantify the difficulty of these\nskills for both humans and computers on both datasets. Finally, we propose a\nnovel task of predicting what vision skills are needed to answer a question\nabout an image. Our results reveal (mis)matches between aims of real users of\nsuch services and the focus of the AI community. We conclude with a discussion\nabout future directions for addressing the visual question answering task.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 04:55:49 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zeng", "Xiaoyu", ""], ["Wang", "Yanan", ""], ["Chiu", "Tai-Yin", ""], ["Bhattacharya", "Nilavra", ""], ["Gurari", "Danna", ""]]}, {"id": "2010.03190", "submitter": "Yijun Zhou", "authors": "Yijun Zhou, Yuki Koyama, Masataka Goto, Takeo Igarashi", "title": "Generative Melody Composition with Human-in-the-Loop Bayesian\n  Optimization", "comments": "10 pages, 2 figures, Proceedings of the 2020 Joint Conference on AI\n  Music Creativity (CSMC-MuMe 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models allow even novice composers to generate various\nmelodies by sampling latent vectors. However, finding the desired melody is\nchallenging since the latent space is unintuitive and high-dimensional. In this\nwork, we present an interactive system that supports generative melody\ncomposition with human-in-the-loop Bayesian optimization (BO). This system\ntakes a mixed-initiative approach; the system generates candidate melodies to\nevaluate, and the user evaluates them and provides preferential feedback (i.e.,\npicking the best melody among the candidates) to the system. This process is\niteratively performed based on BO techniques until the user finds the desired\nmelody. We conducted a pilot study using our prototype system, suggesting the\npotential of this approach.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 05:54:20 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Zhou", "Yijun", ""], ["Koyama", "Yuki", ""], ["Goto", "Masataka", ""], ["Igarashi", "Takeo", ""]]}, {"id": "2010.03213", "submitter": "Michael Lyons", "authors": "Michael J. Lyons, Michael Haehnel, Nobuji Tetsutani", "title": "Designing, Playing, and Performing with a Vision-based Mouth Interface", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2003", "journal-ref": null, "doi": "10.5281/zenodo.1176529", "report-no": null, "categories": "cs.HC cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The role of the face and mouth in speech production as well asnon-verbal\ncommunication suggests the use of facial action tocontrol musical sound. Here\nwe document work on theMouthesizer, a system which uses a headworn\nminiaturecamera and computer vision algorithm to extract shapeparameters from\nthe mouth opening and output these as MIDIcontrol changes. We report our\nexperience with variousgesture-to-sound mappings and musical applications,\nanddescribe a live performance which used the Mouthesizerinterface.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 06:47:42 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Lyons", "Michael J.", ""], ["Haehnel", "Michael", ""], ["Tetsutani", "Nobuji", ""]]}, {"id": "2010.03216", "submitter": "Zheng Wang", "authors": "Zheng Wang, Rencheng Zheng, Edric John Cruz Nacpil, Kimihiko Nakano", "title": "Modeling and analysis of driver behavior under shared control through\n  weighted visual and haptic guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the optimum design of a driver-automation shared control system, an\nunderstanding of driver behavior based on measurements and modeling is crucial\nearly in the development process. This paper presents a driver model through a\nweighting process of visual guidance from the road ahead and haptic guidance\nfrom a steering system for a lane-following task. The proposed weighting\nprocess describes the interaction of a driver with the haptic guidance steering\nand the driver reliance on it. A driving simulator experiment is conducted to\nidentify the model parameters for driving manually and with haptic guidance.\nThe proposed driver model matched the driver input torque with a satisfactory\ngoodness of fit among fourteen participants after considering the individual\ndifferences. The validation results reveal that the simulated trajectory\neffectively followed the driving course by matching the measured trajectory,\nthereby indicating that the proposed driver model is capable of predicting\ndriver behavior during haptic guidance. Furthermore, the effect of different\ndegrees of driver reliance on driving performance is evaluated considering\nvarious driver states and with system failure via numerical analysis. The model\nevaluation results reveal the potential of the proposed driver model to be\napplied in the design and evaluation of a haptic guidance system.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 06:50:17 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wang", "Zheng", ""], ["Zheng", "Rencheng", ""], ["Nacpil", "Edric John Cruz", ""], ["Nakano", "Kimihiko", ""]]}, {"id": "2010.03223", "submitter": "Michael Lyons", "authors": "Mathias Funk, Kazuhiro Kuwabara, Michael J. Lyons", "title": "Sonification of Facial Actions for Musical Expression", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2005 (NIME-05)", "journal-ref": null, "doi": "10.5281/zenodo.1176749", "report-no": null, "categories": "cs.HC cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The central role of the face in social interaction and non-verbal\ncommunication suggests we explore facial action as a means of musical\nexpression. This paper presents the design, implementation, and preliminary\nstudies of a novel system utilizing face detection and optic flow algorithms to\nassociate facial movements with sound synthesis in a topographically specific\nfashion. We report on our experience with various gesture-to-sound mappings and\napplications, and describe our preliminary experiments at musical performance\nusing the system.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:04:07 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Funk", "Mathias", ""], ["Kuwabara", "Kazuhiro", ""], ["Lyons", "Michael J.", ""]]}, {"id": "2010.03247", "submitter": "Wenge Xu", "authors": "Xueshi Lu, Difeng Yu, Hai-Ning Liang, Wenge Xu, Yuzheng Chen, Xiang\n  Li, Khalad Hasan", "title": "Exploration of Hands-free Text Entry Techniques For Virtual Reality", "comments": "Corresponding author: haining.liang@xjtlu.edu.cn", "journal-ref": "In IEEE International Symposium on Mixed and Augmented Reality\n  2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text entry is a common activity in virtual reality (VR) systems. There is a\nlimited number of available hands-free techniques, which allow users to carry\nout text entry when users' hands are busy such as holding items or hand-based\ndevices are not available. The most used hands-free text entry technique is\nDwellType, where a user selects a letter by dwelling over it for a specific\nperiod. However, its performance is limited due to the fixed dwell time for\neach character selection. In this paper, we explore two other hands-free text\nentry mechanisms in VR: BlinkType and NeckType, which leverage users' eye\nblinks and neck's forward and backward movements to select letters. With a user\nstudy, we compare the performance of the two techniques with DwellType. Results\nshow that users can achieve an average text entry rate of 13.47, 11.18 and\n11.65 words per minute with BlinkType, NeckType, and DwellType, respectively.\nUsers' subjective feedback shows BlinkType as the preferred technique for text\nentry in VR.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 07:59:31 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Lu", "Xueshi", ""], ["Yu", "Difeng", ""], ["Liang", "Hai-Ning", ""], ["Xu", "Wenge", ""], ["Chen", "Yuzheng", ""], ["Li", "Xiang", ""], ["Hasan", "Khalad", ""]]}, {"id": "2010.03256", "submitter": "Hai-Ning Liang", "authors": "Diego Monteiro, Hai-Ning Liang, Jialin Wang, Hao Chen, Nilufar Baghaei", "title": "An In-Depth Exploration of the Effect of 2D/3D Views and Controller\n  Types on First Person Shooter Games in Virtual Reality", "comments": "13 pages", "journal-ref": "Proceedings of the 2020 IEEE International Symposium on Mixed and\n  Augmented Reality (ISMAR'20)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of interest in Virtual Reality (VR) research has significantly\nincreased over the past few years, both in academia and industry. The release\nof commercial VR Head-Mounted Displays (HMDs) has been a major contributing\nfactor. However, there is still much to be learned, especially how views and\ninput techniques, as well as their interaction, affect the VR experience. There\nis little work done on First-Person Shooter (FPS) games in VR, and those few\nstudies have focused on a single aspect of VR FPS. They either focused on the\nview, e.g., comparing VR to a typical 2D display or on the controller types. To\nthe best of our knowledge, there are no studies investigating variations of\n2D/3D views in HMDs, controller types, and their interactions. As such, it is\nchallenging to distinguish findings related to the controller type from those\nrelated to the view. If a study does not control for the input method and finds\nthat 2D displays lead to higher performance than VR, we cannot generalize the\nresults because of the confounding variables. To understand their interaction,\nwe propose to analyze in more depth, whether it is the view (2D vs. 3D) or the\nway it is controlled that gives the platforms their respective advantages. To\nstudy the effects of the 2D/3D views, we created a 2D visual technique,\nPlaneFrame, that was applied inside the VR headset. Our results show that the\ncontroller type can have a significant positive impact on performance,\nimmersion, and simulator sickness when associated with a 2D view. They further\nour understanding of the interactions that controllers and views have and\ndemonstrate that comparisons are highly dependent on how both factors go\ntogether. Further, through a series of three experiments, we developed a\ntechnique that can lead to a substantial performance, a good level of\nimmersion, and can minimize the level of simulator sickness.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:17:07 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Monteiro", "Diego", ""], ["Liang", "Hai-Ning", ""], ["Wang", "Jialin", ""], ["Chen", "Hao", ""], ["Baghaei", "Nilufar", ""]]}, {"id": "2010.03265", "submitter": "Michael Lyons", "authors": "Gamhewage C. de Silva, Tamara Smyth, Michael J. Lyons", "title": "A Novel Face-tracking Mouth Controller and its Application to\n  Interacting with Bioacoustic Models", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2004 (NIME-04)", "journal-ref": null, "doi": "10.5281/zenodo.1176666", "report-no": null, "categories": "cs.HC cs.CV cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a simple, computationally light, real-time system for tracking\nthe lower face and extracting information about the shape of the open mouth\nfrom a video sequence. The system allows unencumbered control of audio\nsynthesis modules by the action of the mouth. We report work in progress to use\nthe mouth controller to interact with a physical model of sound production by\nthe avian syrinx.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 08:36:43 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["de Silva", "Gamhewage C.", ""], ["Smyth", "Tamara", ""], ["Lyons", "Michael J.", ""]]}, {"id": "2010.03360", "submitter": "Abhiram Singh", "authors": "Abhiram Singh, Ashwin Gumaste", "title": "Interpreting Imagined Speech Waves with Machine Learning techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores the possibility of decoding Imagined Speech (IS) signals\nwhich can be used to create a new design of Human-Computer Interface (HCI).\nSince the underlying process generating EEG signals is unknown, various feature\nextraction methods, along with different neural network (NN) models, are used\nto approximate data distribution and classify IS signals. Based on the\nexperimental results, feed-forward NN model with ensemble and covariance matrix\ntransformed features showed the highest performance in comparison to other\nexisting methods. For comparison, three publicly available datasets were used.\nWe report a mean classification accuracy of 80% between rest and imagined\nstate, 96% and 80% for decoding long and short words on two datasets. These\nresults show that it is possible to differentiate brain signals (generated\nduring rest state) from the IS brain signals. Based on the experimental\nresults, we suggest that the word length and complexity can be used to decode\nIS signals with high accuracy, and a BCI system can be designed with IS signals\nfor computer interaction. These ideas, and results give direction for the\ndevelopment of a commercial level IS based BCI system, which can be used for\nhuman-computer interaction in daily life.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 12:13:39 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 15:42:44 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Singh", "Abhiram", ""], ["Gumaste", "Ashwin", ""]]}, {"id": "2010.03534", "submitter": "Jonathas Costa", "authors": "Jonathas Costa, Alexander Bock, Carter Emmart, Charles Hansen, Anders\n  Ynnerman, Claudio Silva", "title": "Interactive Visualization of Atmospheric Effects for Celestial Bodies", "comments": "To appear at IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC astro-ph.EP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an atmospheric model tailored for the interactive visualization of\nplanetary surfaces. As the exploration of the solar system is progressing with\nincreasingly accurate missions and instruments, the faithful visualization of\nplanetary environments is gaining increasing interest in space research,\nmission planning, and science communication and education. Atmospheric effects\nare crucial in data analysis and to provide contextual information for\nplanetary data. Our model correctly accounts for the non-linear path of the\nlight inside the atmosphere (in Earth's case), the light absorption effects by\nmolecules and dust particles, such as the ozone layer and the Martian dust, and\na wavelength-dependent phase function for Mie scattering. The mode focuses on\ninteractivity, versatility, and customization, and a comprehensive set of\ninteractive controls make it possible to adapt its appearance dynamically. We\ndemonstrate our results using Earth and Mars as examples. However, it can be\nreadily adapted for the exploration of other atmospheres found on, for example,\nof exoplanets. For Earth's atmosphere, we visually compare our results with\npictures taken from the International Space Station and against the CIE clear\nsky model. The Martian atmosphere is reproduced based on available scientific\ndata, feedback from domain experts, and is compared to images taken by the\nCuriosity rover. The work presented here has been implemented in the OpenSpace\nsystem, which enables interactive parameter setting and real-time feedback\nvisualization targeting presentations in a wide range of environments, from\nimmersive dome theaters to virtual reality headsets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 17:28:26 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Costa", "Jonathas", ""], ["Bock", "Alexander", ""], ["Emmart", "Carter", ""], ["Hansen", "Charles", ""], ["Ynnerman", "Anders", ""], ["Silva", "Claudio", ""]]}, {"id": "2010.03667", "submitter": "Amy Pavel", "authors": "Amy Pavel, Gabriel Reyes, Jeffrey P. Bigham", "title": "Rescribe: Authoring and Automatically Editing Audio Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio descriptions make videos accessible to those who cannot see them by\ndescribing visual content in audio. Producing audio descriptions is challenging\ndue to the synchronous nature of the audio description that must fit into gaps\nof other video content. An experienced audio description author will produce\ncontent that fits narration necessary to understand, enjoy, or experience the\nvideo content into the time available. This can be especially tricky for\nnovices to do well. In this paper, we introduce a tool, Rescribe, that helps\nauthors create and refine their audio descriptions. Using Rescribe, authors\nfirst create a draft of all the content they would like to include in the audio\ndescription. Rescribe then uses a dynamic programming approach to optimize\nbetween the length of the audio description, available automatic shortening\napproaches, and source track lengthening approaches. Authors can iteratively\nvisualize and refine the audio descriptions produced by Rescribe, working in\nconcert with the tool. We evaluate the effectiveness of Rescribe through\ninterviews with blind and visually impaired audio description users who\npreferred Rescribe-edited descriptions to extended descriptions. In addition,\nwe invite novice users to create audio descriptions with Rescribe and another\ntool, finding that users produce audio descriptions with fewer placement errors\nusing Rescribe.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 21:47:03 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Pavel", "Amy", ""], ["Reyes", "Gabriel", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "2010.03747", "submitter": "Amy Fox", "authors": "Amy Rae Fox, Taylor Jackson Scott", "title": "Surfacing Misconceptions Through Visualization Critique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Students of visualization come to formal education with an abundance of\npersonal experience. However, one's exposure to graphics through media and\neducation may not be sufficiently diverse to appreciate the nuance and\ncomplexity required to design and evaluate effective representations. While\nmany introductory courses in visualization address best practices for visual\nencoding of data based on perceptual characteristics, as cognitive scientists,\nwe place equal value on representational decisions based on communicative\ncontext: how the representation is intended to be used. In this pedagogical\nactivity, we aim to surface learners' preconceived notions about what makes a\nvisualization effective. Here we describe the structure and context of an\nintroductory-level visualization activity, how it might be conducted in\nindividual or group settings, our experience with the common misconceptions the\nactivity can reveal, and conclude with recommendations on how they might be\naddressed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 03:37:23 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Fox", "Amy Rae", ""], ["Scott", "Taylor Jackson", ""]]}, {"id": "2010.03779", "submitter": "Alexander Refsum Jensenius", "authors": "Cagri Erdem, Katja Henriksen Schia, Alexander Refsum Jensenius", "title": "Vrengt: A Shared Body-Machine Instrument for Music-Dance Performance", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2019", "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2019", "doi": "10.5281/zenodo.3672917", "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the process of developing a shared instrument for\nmusic-dance performance, with a particular focus on exploring the boundaries\nbetween standstill vs motion, and silence vs sound. The piece Vrengt grew from\nthe idea of enabling a true partnership between a musician and a dancer,\ndeveloping an instrument that would allow for active co-performance. Using a\nparticipatory design approach, we worked with sonification as a tool for\nsystematically exploring the dancer's bodily expressions. The exploration used\na \"spatiotemporal matrix\", with a particular focus on sonic microinteraction.\nIn the final performance, two Myo armbands were used for capturing muscle\nactivity of the arm and leg of the dancer, together with a wireless headset\nmicrophone capturing the sound of breathing. In the paper we reflect on\nmulti-user instrument paradigms, discuss our approach to creating a shared\ninstrument using sonification as a tool for the sound design, and reflect on\nthe performers' subjective evaluation of the instrument.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 05:50:44 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Erdem", "Cagri", ""], ["Schia", "Katja Henriksen", ""], ["Jensenius", "Alexander Refsum", ""]]}, {"id": "2010.03781", "submitter": "Wenge Xu", "authors": "Wenge Xu, Hai-Ning Liang, Xiaoyue Ma, Xiang Li", "title": "VirusBoxing: A HIIT-based VR boxing game", "comments": "Corresponding author: haining.liang@xjtlu.edu.cn", "journal-ref": "In Extended Abstracts of the 2020 Annual Symposium on\n  Computer-Human Interaction in Play (CHI PLAY' 20 EA)", "doi": "10.1145/3383668.3419958", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical activity or exercise can improve people's health and reduce their\nrisk of developing several diseases; most importantly, regular activity can\nimprove the quality of life. However, lack of time is one of the major barriers\nfor people doing exercise. High-intensity interval training (HIIT) can reduce\nthe time required for a healthy exercise regime but also bring similar benefits\nof regular exercise. We present a boxing-based VR exergame called VirusBoxing\nto promote physical activity for players. VirusBoxing provides players with a\nplatform for HIIT and empowers them with additional abilities to jab a distant\nobject without the need to aim at it precisely. In this paper, we discuss how\nwe adapted the HIIT protocol and gameplay features to empower players in a VR\nexergame to give players an efficient, effective, and enjoyable exercise\nexperience.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 05:55:03 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Xu", "Wenge", ""], ["Liang", "Hai-Ning", ""], ["Ma", "Xiaoyue", ""], ["Li", "Xiang", ""]]}, {"id": "2010.03951", "submitter": "Kexin Huang", "authors": "Kexin Huang, Tianfan Fu, Dawood Khan, Ali Abid, Ali Abdalla, Abubakar\n  Abid, Lucas M. Glass, Marinka Zitnik, Cao Xiao, Jimeng Sun", "title": "MolDesigner: Interactive Design of Efficacious Drugs with Deep Learning", "comments": "NeurIPS 2020 Demonstration Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficacy of a drug depends on its binding affinity to the therapeutic\ntarget and pharmacokinetics. Deep learning (DL) has demonstrated remarkable\nprogress in predicting drug efficacy. We develop MolDesigner, a\nhuman-in-the-loop web user-interface (UI), to assist drug developers leverage\nDL predictions to design more effective drugs. A developer can draw a drug\nmolecule in the interface. In the backend, more than 17 state-of-the-art DL\nmodels generate predictions on important indices that are crucial for a drug's\nefficacy. Based on these predictions, drug developers can edit the drug\nmolecule and reiterate until satisfaction. MolDesigner can make predictions in\nreal-time with a latency of less than a second.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 21:25:25 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Huang", "Kexin", ""], ["Fu", "Tianfan", ""], ["Khan", "Dawood", ""], ["Abid", "Ali", ""], ["Abdalla", "Ali", ""], ["Abid", "Abubakar", ""], ["Glass", "Lucas M.", ""], ["Zitnik", "Marinka", ""], ["Xiao", "Cao", ""], ["Sun", "Jimeng", ""]]}, {"id": "2010.04087", "submitter": "Krishna Prasad Miyapuram", "authors": "Prashant Lawhatre, Bharatesh R Shiraguppi, Esha Sharma, Krishna Prasad\n  Miyapuram, Derek Lomas", "title": "Classifying Songs with EEG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research study aims to use machine learning methods to characterize the\nEEG response to music. Specifically, we investigate how resonance in the EEG\nresponse correlates with individual aesthetic enjoyment. Inspired by the notion\nof musical processing as resonance, we hypothesize that the intensity of an\naesthetic experience is based on the degree to which a participants EEG\nentrains to the perceptual input. To test this and other hypotheses, we have\nbuilt an EEG dataset from 20 subjects listening to 12 two minute-long songs in\nrandom order. After preprocessing and feature construction, we used this\ndataset to train and test multiple machine learning models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 14:02:11 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Lawhatre", "Prashant", ""], ["Shiraguppi", "Bharatesh R", ""], ["Sharma", "Esha", ""], ["Miyapuram", "Krishna Prasad", ""], ["Lomas", "Derek", ""]]}, {"id": "2010.04096", "submitter": "Melanie Bancilhon", "authors": "Melanie Bancilhon and Alvitta Ottley", "title": "Did You Get The Gist Of It? Understanding How Visualization Impacts\n  Decision-Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As visualization researchers evaluate the impact of visualization design on\ndecision-making, they often hold a one-dimensional perspective on the cognitive\nprocesses behind making a decision. Several psychological and economical\nresearchers have shown that to make decisions, people rely on quantitative\nreasoning as well as gist-based intuition -- two systems that operate in\nparallel. In this position paper, we discuss decision theories and provide\nsuggestions to bridge the gap between the evaluation of decision-making in\nvisualization and psychology research. The goal is to question the limits of\nour knowledge and to advocate for a more nuanced understanding of\ndecision-making with visualization.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 16:25:02 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Bancilhon", "Melanie", ""], ["Ottley", "Alvitta", ""]]}, {"id": "2010.04101", "submitter": "Ankit Agrawal", "authors": "Jane Cleland-Huang, Ankit Agrawal", "title": "Human-Drone Interactions with Semi-Autonomous Cohorts of Collaborating\n  Drones", "comments": "Proceedings of the Interdisciplinary Workshop on Human-Drone\n  Interaction co-located with the 2020 ACM CHI Conference on Human Factors in\n  Computing Systems (CHI 2020) - http://ceur-ws.org/Vol-2617/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in human-drone interactions has primarily focused on cases in which\na person interacts with a single drone as an active controller, recipient of\ninformation, or a social companion; or cases in which an individual, or a team\nof operators interacts with a swarm of drones as they perform some coordinated\nflight patterns. In this position paper we explore a third scenario in which\nmultiple humans and drones collaborate in an emergency response scenario. We\ndiscuss different types of interactions, and draw examples from current\nDroneResponse project.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 16:37:30 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Cleland-Huang", "Jane", ""], ["Agrawal", "Ankit", ""]]}, {"id": "2010.04124", "submitter": "Mehak Maniktala", "authors": "Mehak Maniktala, Christa Cody, Amy Isvik, Nicholas Lytle, Min Chi,\n  Tiffany Barnes", "title": "Extending the Hint Factory for the assistance dilemma: A novel,\n  data-driven HelpNeed Predictor for proactive problem-solving help", "comments": null, "journal-ref": "Journal of Educational Data Mining 12 (4), 24-65, 2020", "doi": "10.5281/zenodo.4399683", "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining when and whether to provide personalized support is a well-known\nchallenge called the assistance dilemma. A core problem in solving the\nassistance dilemma is the need to discover when students are unproductive so\nthat the tutor can intervene. Such a task is particularly challenging for\nopen-ended domains, even those that are well-structured with defined principles\nand goals. In this paper, we present a set of data-driven methods to classify,\npredict, and prevent unproductive problem-solving steps in the well-structured\nopen-ended domain of logic. This approach leverages and extends the Hint\nFactory, a set of methods that leverages prior student solution attempts to\nbuild data-driven intelligent tutors. We present a HelpNeed classification,\nthat uses prior student data to determine when students are likely to be\nunproductive and need help learning optimal problem-solving strategies. We\npresent a controlled study to determine the impact of an Adaptive pedagogical\npolicy that provides proactive hints at the start of each step based on the\noutcomes of our HelpNeed predictor: productive vs. unproductive. Our results\nshow that the students in the Adaptive condition exhibited better training\nbehaviors, with lower help avoidance, and higher help appropriateness (a higher\nchance of receiving help when it was likely to be needed), as measured using\nthe HelpNeed classifier, when compared to the Control. Furthermore, the results\nshow that the students who received Adaptive hints based on HelpNeed\npredictions during training significantly outperform their Control peers on the\nposttest, with the former producing shorter, more optimal solutions in less\ntime. We conclude with suggestions on how these HelpNeed methods could be\napplied in other well-structured open-ended domains.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:04:03 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Maniktala", "Mehak", ""], ["Cody", "Christa", ""], ["Isvik", "Amy", ""], ["Lytle", "Nicholas", ""], ["Chi", "Min", ""], ["Barnes", "Tiffany", ""]]}, {"id": "2010.04125", "submitter": "Kun Zhou", "authors": "Kun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang and Ji-Rong Wen", "title": "Towards Topic-Guided Conversational Recommender System", "comments": "12 pages, Accepted by Coling2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational recommender systems (CRS) aim to recommend high-quality items\nto users through interactive conversations. To develop an effective CRS, the\nsupport of high-quality datasets is essential. Existing CRS datasets mainly\nfocus on immediate requests from users, while lack proactive guidance to the\nrecommendation scenario. In this paper, we contribute a new CRS dataset named\n\\textbf{TG-ReDial} (\\textbf{Re}commendation through\n\\textbf{T}opic-\\textbf{G}uided \\textbf{Dial}og). Our dataset has two major\nfeatures. First, it incorporates topic threads to enforce natural semantic\ntransitions towards the recommendation scenario. Second, it is created in a\nsemi-automatic way, hence human annotation is more reasonable and controllable.\nBased on TG-ReDial, we present the task of topic-guided conversational\nrecommendation, and propose an effective approach to this task. Extensive\nexperiments have demonstrated the effectiveness of our approach on three\nsub-tasks, namely topic prediction, item recommendation and response\ngeneration. TG-ReDial is available at https://github.com/RUCAIBox/TG-ReDial.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:04:30 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 14:25:58 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhou", "Kun", ""], ["Zhou", "Yuanhang", ""], ["Zhao", "Wayne Xin", ""], ["Wang", "Xiaoke", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2010.04274", "submitter": "Ayman Al-Kababji", "authors": "Ayman Al-Kababji and Abdullah Alsalemi and Yassine Himeur and Faycal\n  Bensaali and Abbes Amira and Rachael Fernandez and Noora Fetais", "title": "Energy Data Visualizations on Smartphones for Triggering Behavioral\n  Change: Novel Vs. Conventional", "comments": "This work has been accepted in GPECOM2020 and it is to be published\n  in its proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper conveys the importance of using suitable data visualizations for\nelectrical energy consumption and the effect it carries on reducing said\nconsumption. Data visualization tools construct an important pillar in energy\nmicro-moments, i.e., the concept of providing the right information at the\nright time in the right way for a specific power consumer. Such behavioral\nchange can be triggered with the help of good recommendations and suitable\nvisualizations to convey the right message. A questionnaire is built as a\nmobile application to evaluate different groups of conventional and novel\nvisualizations. Conventional charts are restricted to bar, line and stacked\narea charts, while novel visualizations contain heatmap, spiral and\nappliance-level stacked bar charts. Significant findings gathered from\nparticipants' responses indicate that they are slightly inclined towards\nconventional charts. However, their understanding of the novel charts is better\nby 8% when the analysis questions are investigated. Finally, a question is\nanswered on whether a group of visualizations should be discarded completely,\nor some modifications can be applied.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 21:47:52 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Al-Kababji", "Ayman", ""], ["Alsalemi", "Abdullah", ""], ["Himeur", "Yassine", ""], ["Bensaali", "Faycal", ""], ["Amira", "Abbes", ""], ["Fernandez", "Rachael", ""], ["Fetais", "Noora", ""]]}, {"id": "2010.04276", "submitter": "Curtis Bahn", "authors": "Curtis Bahn and Dan Trueman", "title": "interface : Electronic Chamber Ensemble", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176355", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the interface developments and music of the duo\n\"interface,\" formed by Curtis Bahn and Dan Trueman. We describe gestural\ninstrument design, interactive performance interfaces for improvisational\nmusic, spherical speakers (multi-channel, outward-radiating geodesic speaker\narrays) and Sensor-Speaker-Arrays (SenSAs: combinations of various sensor\ndevices with spherical speaker arrays). We discuss the concept, design and\nconstruction of these systems, and, give examples from several newly published\nCDs of work by Bahn and Trueman.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 21:57:52 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bahn", "Curtis", ""], ["Trueman", "Dan", ""]]}, {"id": "2010.04295", "submitter": "Yang Li", "authors": "Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, Zhiwei Guan", "title": "Widget Captioning: Generating Natural Language Description for Mobile\n  User Interface Elements", "comments": "16 pages, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language descriptions of user interface (UI) elements such as\nalternative text are crucial for accessibility and language-based interaction\nin general. Yet, these descriptions are constantly missing in mobile UIs. We\npropose widget captioning, a novel task for automatically generating language\ndescriptions for UI elements from multimodal input including both the image and\nthe structural representations of user interfaces. We collected a large-scale\ndataset for widget captioning with crowdsourcing. Our dataset contains 162,859\nlanguage phrases created by human workers for annotating 61,285 UI elements\nacross 21,750 unique UI screens. We thoroughly analyze the dataset, and train\nand evaluate a set of deep model configurations to investigate how each feature\nmodality as well as the choice of learning strategies impact the quality of\npredicted captions. The task formulation and the dataset as well as our\nbenchmark models contribute a solid basis for this novel multimodal captioning\ntask that connects language and user interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 22:56:03 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Li", "Yang", ""], ["Li", "Gang", ""], ["He", "Luheng", ""], ["Zheng", "Jingjie", ""], ["Li", "Hong", ""], ["Guan", "Zhiwei", ""]]}, {"id": "2010.04430", "submitter": "Manuel Gomez Rodriguez", "authors": "Utkarsh Upadhyay and Graham Lancashire and Christoph Moser and Manuel\n  Gomez-Rodriguez", "title": "Large-scale randomized experiment reveals machine learning helps people\n  learn and remember more effectively", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has typically focused on developing models and algorithms\nthat would ultimately replace humans at tasks where intelligence is required.\nIn this work, rather than replacing humans, we focus on unveiling the potential\nof machine learning to improve how people learn and remember factual material.\nTo this end, we perform a large-scale randomized controlled trial with\nthousands of learners from a popular learning app in the area of mobility.\nAfter controlling for the length and frequency of study, we find that learners\nwhose study sessions are optimized using machine learning remember the content\nover $\\sim$67% longer than those whose study sessions are generated using two\nalternative heuristics. Our randomized controlled trial also reveals that the\nlearners whose study sessions are optimized using machine learning are\n$\\sim$50% more likely to return to the app within 4-7 days.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 08:30:15 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Upadhyay", "Utkarsh", ""], ["Lancashire", "Graham", ""], ["Moser", "Christoph", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "2010.04570", "submitter": "Daniel Hernandez Garcia", "authors": "Daniel Hern\\'andez Garc\\'ia, Yanchao Yu, Weronika Siei\\'nska, Jose L.\n  Part, Nancie Gunson, Oliver Lemon, Christian Dondrup", "title": "Explainable Representations of the Social State: A Model for Social\n  Human-Robot Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a minimum set of concepts and signals needed to\ntrack the social state during Human-Robot Interaction. We look into the problem\nof complex continuous interactions in a social context with multiple humans and\nrobots, and discuss the creation of an explainable and tractable\nrepresentation/model of their social interaction. We discuss these\nrepresentations according to their representational and communicational\nproperties, and organize them into four cognitive domains (scene-understanding,\nbehaviour-profiling, mental-state, and dialogue-grounding).\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:36:36 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Garc\u00eda", "Daniel Hern\u00e1ndez", ""], ["Yu", "Yanchao", ""], ["Siei\u0144ska", "Weronika", ""], ["Part", "Jose L.", ""], ["Gunson", "Nancie", ""], ["Lemon", "Oliver", ""], ["Dondrup", "Christian", ""]]}, {"id": "2010.04573", "submitter": "Anastasia Bolotnikova", "authors": "Anastasia Bolotnikova, Pierre Gergondet, Arnaud Tanguy, S\\'ebastien\n  Courtois, Abderrahmane Kheddar", "title": "Task-Space Control Interface for SoftBank Humanoid Robots and its\n  Human-Robot Interaction Applications", "comments": "IEEE/SICE 13th International Symposium on System Integration (SII\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an open-source software interface, called mc_naoqi, that allows to\nperform whole-body task-space Quadratic Programming based control, implemented\nin mc_rtc framework, on the SoftBank Robotics Europe humanoid robots. We\ndescribe the control interface, associated robot description packages, robot\nmodules and sample whole-body controllers. We demonstrate the use of these\ntools in simulation for a robot interacting with a human model. Finally, we\nshowcase and discuss the use of the developed open-source tools for running the\nhuman-robot close contact interaction experiments with real human subjects\ninspired from assistance scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 13:38:12 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Bolotnikova", "Anastasia", ""], ["Gergondet", "Pierre", ""], ["Tanguy", "Arnaud", ""], ["Courtois", "S\u00e9bastien", ""], ["Kheddar", "Abderrahmane", ""]]}, {"id": "2010.04736", "submitter": "Chenhao Tan", "authors": "Samuel Carton, Anirudh Rathore, Chenhao Tan", "title": "Evaluating and Characterizing Human Rationales", "comments": "14 pages, 15 figures, to appear in EMNLP 2020. Code is available at\n  https://github.com/BoulderDS/evaluating-human-rationales", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two main approaches for evaluating the quality of machine-generated\nrationales are: 1) using human rationales as a gold standard; and 2) automated\nmetrics based on how rationales affect model behavior. An open question,\nhowever, is how human rationales fare with these automatic metrics. Analyzing a\nvariety of datasets and models, we find that human rationales do not\nnecessarily perform well on these metrics. To unpack this finding, we propose\nimproved metrics to account for model-dependent baseline performance. We then\npropose two methods to further characterize rationale quality, one based on\nmodel retraining and one on using \"fidelity curves\" to reveal properties such\nas irrelevance and redundancy. Our work leads to actionable suggestions for\nevaluating and characterizing rationales.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 18:00:04 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Carton", "Samuel", ""], ["Rathore", "Anirudh", ""], ["Tan", "Chenhao", ""]]}, {"id": "2010.04880", "submitter": "Debasish Chakroborti", "authors": "Debasish Chakroborti, Banani Roy, Sristy Sumana Nath", "title": "Designing for Recommending Intermediate States in A Scientific Workflow\n  Management System", "comments": "Preprint, 13th Engineering Interactive Computing Systems (EICS)\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To process a large amount of data sequentially and systematically, proper\nmanagement of workflow components (i.e., modules, data, configurations,\nassociations among ports and links) in a Scientific Workflow Management System\n(SWfMS) is inevitable. Managing data with provenance in a SWfMS to support\nreusability of workflows, modules, and data is not a simple task. Handling such\ncomponents is even more burdensome for frequently assembled and executed\ncomplex workflows for investigating large datasets with different technologies\n(i.e., various learning algorithms or models). However, a great many studies\npropose various techniques and technologies for managing and recommending\nservices in a SWfMS, but only a very few studies consider the management of\ndata in a SWfMS for efficient storing and facilitating workflow executions.\nFurthermore, there is no study to inquire about the effectiveness and\nefficiency of such data management in a SWfMS from a user perspective. In this\npaper, we present and evaluate a GUI version of such a novel approach of\nintermediate data management with two use cases (Plant Phenotyping and\nBioinformatics). The technique we call GUI-RISPTS (Recommending Intermediate\nStates from Pipelines Considering Tool-States) can facilitate executions of\nworkflows with processed data (i.e., intermediate outcomes of modules in a\nworkflow) and can thus reduce the computational time of some modules in a\nSWfMS. We integrated GUI-RISPTS with an existing workflow management system\ncalled SciWorCS. In SciWorCS, we present an interface that users use for\nselecting the recommendation of intermediate states (i.e., modules' outcomes).\nWe investigated GUI-RISP's effectiveness from users' perspectives along with\nmeasuring its overhead in terms of storage and efficiency in workflow\nexecution.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 02:31:24 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chakroborti", "Debasish", ""], ["Roy", "Banani", ""], ["Nath", "Sristy Sumana", ""]]}, {"id": "2010.04885", "submitter": "Mengyao Li", "authors": "Mengyao Li, Areen Alsaid, Sofia I. Noejovich, Ernest V. Cross, John D.\n  Lee", "title": "Towards a Conversational Measure of Trust", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly collaborative decision-making process between humans and\nagents demands a comprehensive, continuous, and unobtrusive measure of trust in\nagents. The gold standard format for measuring trust, a Likert-style survey,\nsuffers from major limitations in dynamic human-agent interactions. We proposed\na new approach to evaluate trust in a nondirective and relational conversation.\nThe term nondirective refers to abstract word selections in open-ended prompts,\nwhich can probe respondents to freely describe their attitudes. The term\nrelational refers to interactive conversations where respondents can clarify\ntheir responses in followup questions. We propose a systematic process for\ngenerating nondirective trust-based prompts by using text analysis from\npreviously validated trust scales. This nondirective and relational approach\nprovides a complementary trust measurement, which can unobtrusively elicit rich\nand dynamic information on situational trust throughout a human-agent\ninteraction.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 03:10:40 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Mengyao", ""], ["Alsaid", "Areen", ""], ["Noejovich", "Sofia I.", ""], ["Cross", "Ernest V.", ""], ["Lee", "John D.", ""]]}, {"id": "2010.04987", "submitter": "Piyawat Lertvittayakumjorn", "authors": "Piyawat Lertvittayakumjorn, Lucia Specia, Francesca Toni", "title": "FIND: Human-in-the-Loop Debugging Deep Text Classifiers", "comments": "17 pages including appendices; To appear at EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since obtaining a perfect training dataset (i.e., a dataset which is\nconsiderably large, unbiased, and well-representative of unseen cases) is\nhardly possible, many real-world text classifiers are trained on the available,\nyet imperfect, datasets. These classifiers are thus likely to have undesirable\nproperties. For instance, they may have biases against some sub-populations or\nmay not work effectively in the wild due to overfitting. In this paper, we\npropose FIND -- a framework which enables humans to debug deep learning text\nclassifiers by disabling irrelevant hidden features. Experiments show that by\nusing FIND, humans can improve CNN text classifiers which were trained under\ndifferent types of imperfect datasets (including datasets with biases and\ndatasets with dissimilar train-test distributions).\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 12:52:53 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Lertvittayakumjorn", "Piyawat", ""], ["Specia", "Lucia", ""], ["Toni", "Francesca", ""]]}, {"id": "2010.05047", "submitter": "Ilhan Aslan", "authors": "Franziska Geiger, Michelle Martin, Monika Pichlmair, Ilhan Aslan,\n  Hannes Ritschel, Bj\\\"orn Bittner, and Elisabeth Andr\\'e", "title": "Drawing with AI -- Exploring Collaborative Inking Experiences Based on\n  Mid-air Pointing and Reinforcement Learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitalization is changing the nature of tools and materials, which are used\nin artistic practices in professional and non-professional settings. For\nexample, today it is common that even children express their ideas and explore\ntheir creativity by drawing on tablets as digital canvases. While there are\nmany software-based tools, which resemble traditional tools, such as various\nforms of virtual brushes, erasers, etc. in contrast to traditional materials\nthere is potential in augmenting software-based tools and digital canvases with\nartificial intelligence. Curious about how it would feel to interact with a\ndigital canvas, which would be in contrast to a traditional canvas dynamic,\nresponsive, and potentially able to continuously adapt to its user's input, we\ndeveloped a drawing application and conducted a qualitative study with 14\nusers. In this paper, we describe details of our design process, which lead up\nto using a k-armed bandit as a simple form of reinforcement learning and a\nLeapMotion sensor to allow people from all walks of like, old and young to draw\non pervasive displays, small and large, positioned near or far.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 17:29:38 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Geiger", "Franziska", ""], ["Martin", "Michelle", ""], ["Pichlmair", "Monika", ""], ["Aslan", "Ilhan", ""], ["Ritschel", "Hannes", ""], ["Bittner", "Bj\u00f6rn", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "2010.05115", "submitter": "Henry Chen", "authors": "Henry Chen, Robin Cohen, Kerstin Dautenhahn, Edith Law, Krzysztof\n  Czarnecki", "title": "Autonomous Vehicle Visual Signals for Pedestrians: Experiments and\n  Design Recommendations", "comments": "The 31st IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.MA cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous Vehicles (AV) will transform transportation, but also the\ninteraction between vehicles and pedestrians. In the absence of a driver, it is\nnot clear how an AV can communicate its intention to pedestrians. One option is\nto use visual signals. To advance their design, we conduct four\nhuman-participant experiments and evaluate six representative AV visual signals\nfor visibility, intuitiveness, persuasiveness, and usability at pedestrian\ncrossings. Based on the results, we distill twelve practical design\nrecommendations for AV visual signals, with focus on signal pattern design and\nplacement. Moreover, the paper advances the methodology for experimental\nevaluation of visual signals, including lab, closed-course, and public road\ntests using an autonomous vehicle. In addition, the paper also reports insights\non pedestrian crosswalk behaviours and the impacts of pedestrian trust towards\nAVs on the behaviors. We hope that this work will constitute valuable input to\nthe ongoing development of international standards for AV lamps, and thus help\nmature automated driving in general.\n", "versions": [{"version": "v1", "created": "Sat, 10 Oct 2020 22:56:46 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Henry", ""], ["Cohen", "Robin", ""], ["Dautenhahn", "Kerstin", ""], ["Law", "Edith", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "2010.05123", "submitter": "Jatin Sharma", "authors": "Jatin Sharma and Jon Campbell and Pete Ansell and Jay Beavers and\n  Christopher O'Dowd", "title": "Towards Hardware-Agnostic Gaze-Trackers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze-tracking is a novel way of interacting with computers which allows new\nscenarios, such as enabling people with motor-neuron disabilities to control\ntheir computers or doctors to interact with patient information without\ntouching screen or keyboard. Further, there are emerging applications of\ngaze-tracking in interactive gaming, user experience research, human attention\nanalysis and behavioral studies. Accurate estimation of the gaze may involve\naccounting for head-pose, head-position, eye rotation, distance from the object\nas well as operating conditions such as illumination, occlusion, background\nnoise and various biological aspects of the user. Commercially available\ngaze-trackers utilize specialized sensor assemblies that usually consist of an\ninfrared light source and camera. There are several challenges in the universal\nproliferation of gaze-tracking as accessibility technologies, specifically its\naffordability, reliability, and ease-of-use. In this paper, we try to address\nthese challenges through the development of a hardware-agnostic gaze-tracker.\nWe present a deep neural network architecture as an appearance-based method for\nconstrained gaze-tracking that utilizes facial imagery captured on an ordinary\nRGB camera ubiquitous in all modern computing devices. Our system achieved an\nerror of 1.8073cm on GazeCapture dataset without any calibration or device\nspecific fine-tuning. This research shows promise that one day soon any\ncomputer, tablet, or phone will be controllable using just your eyes due to the\nprediction capabilities of deep neutral networks.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 00:53:57 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sharma", "Jatin", ""], ["Campbell", "Jon", ""], ["Ansell", "Pete", ""], ["Beavers", "Jay", ""], ["O'Dowd", "Christopher", ""]]}, {"id": "2010.05204", "submitter": "Ilhan Aslan", "authors": "Fiona Guerin, Alice Rey, Enis Caliskan, Erik Kynast, Andreas Zimmerer,\n  Ilhan Aslan, Elisabeth Andr\\'e", "title": "Towards Somaesthetics Inspired Games: Exploring the Influence of a\n  Mirror Effect on Self-Presentation in a Public Setting", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on an initial user study, which explores how players of an\naugmented mirror game, self-style or self-present themselves when they are\nallowed to see themselves in the mirror compared to when they do not see\nthemselves. To this end, we customized an open source fruit slicing game into\nan interactive installation for an architecture museum and conducted with 36\nvisitors a field study. Based on an analysis of video recordings of\nparticipants we identified, for example significant differences in how often\nparticipants smile. Ultimately, presenting a self-image to gamers in a social\nsetting resulted in behavior change, which we argue could be utilized carefully\nfrom a Somaesthetics perspective as an experience design feature in future\ngames.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 09:46:48 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Guerin", "Fiona", ""], ["Rey", "Alice", ""], ["Caliskan", "Enis", ""], ["Kynast", "Erik", ""], ["Zimmerer", "Andreas", ""], ["Aslan", "Ilhan", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "2010.05388", "submitter": "Cheng-Zhi Anna Huang", "authors": "Cheng-Zhi Anna Huang, Hendrik Vincent Koops, Ed Newton-Rex, Monica\n  Dinculescu, Carrie J. Cai", "title": "AI Song Contest: Human-AI Co-Creation in Songwriting", "comments": "6 pages + 3 pages of references", "journal-ref": "ISMIR 2020", "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is challenging the way we make music. Although research in\ndeep generative models has dramatically improved the capability and fluency of\nmusic models, recent work has shown that it can be challenging for humans to\npartner with this new class of algorithms. In this paper, we present findings\non what 13 musician/developer teams, a total of 61 users, needed when\nco-creating a song with AI, the challenges they faced, and how they leveraged\nand repurposed existing characteristics of AI to overcome some of these\nchallenges. Many teams adopted modular approaches, such as independently\nrunning multiple smaller models that align with the musical building blocks of\na song, before re-combining their results. As ML models are not easily\nsteerable, teams also generated massive numbers of samples and curated them\npost-hoc, or used a range of strategies to direct the generation, or\nalgorithmically ranked the samples. Ultimately, teams not only had to manage\nthe \"flare and focus\" aspects of the creative process, but also juggle them\nwith a parallel process of exploring and curating multiple ML models and\noutputs. These findings reflect a need to design machine learning-powered music\ninterfaces that are more decomposable, steerable, interpretable, and adaptive,\nwhich in return will enable artists to more effectively explore how AI can\nextend their personal expression.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 01:27:41 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Huang", "Cheng-Zhi Anna", ""], ["Koops", "Hendrik Vincent", ""], ["Newton-Rex", "Ed", ""], ["Dinculescu", "Monica", ""], ["Cai", "Carrie J.", ""]]}, {"id": "2010.05468", "submitter": "Dongxu Li", "authors": "Dongxu Li, Chenchen Xu, Xin Yu, Kaihao Zhang, Ben Swift, Hanna\n  Suominen, Hongdong Li", "title": "TSPNet: Hierarchical Feature Learning via Temporal Semantic Pyramid for\n  Sign Language Translation", "comments": "NeurIPS 2020 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language translation (SLT) aims to interpret sign video sequences into\ntext-based natural language sentences. Sign videos consist of continuous\nsequences of sign gestures with no clear boundaries in between. Existing SLT\nmodels usually represent sign visual features in a frame-wise manner so as to\navoid needing to explicitly segmenting the videos into isolated signs. However,\nthese methods neglect the temporal information of signs and lead to substantial\nambiguity in translation. In this paper, we explore the temporal semantic\nstructures of signvideos to learn more discriminative features. To this end, we\nfirst present a novel sign video segment representation which takes into\naccount multiple temporal granularities, thus alleviating the need for accurate\nvideo segmentation. Taking advantage of the proposed segment representation, we\ndevelop a novel hierarchical sign video feature learning method via a temporal\nsemantic pyramid network, called TSPNet. Specifically, TSPNet introduces an\ninter-scale attention to evaluate and enhance local semantic consistency of\nsign segments and an intra-scale attention to resolve semantic ambiguity by\nusing non-local video context. Experiments show that our TSPNet outperforms the\nstate-of-the-art with significant improvements on the BLEU score (from 9.58 to\n13.41) and ROUGE score (from 31.80 to 34.96)on the largest commonly-used SLT\ndataset. Our implementation is available at\nhttps://github.com/verashira/TSPNet.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 05:58:09 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Li", "Dongxu", ""], ["Xu", "Chenchen", ""], ["Yu", "Xin", ""], ["Zhang", "Kaihao", ""], ["Swift", "Ben", ""], ["Suominen", "Hanna", ""], ["Li", "Hongdong", ""]]}, {"id": "2010.05491", "submitter": "Trenton Schulz", "authors": "Trenton Schulz, Till Halbach, Ivar Solheim", "title": "Using Social Robots to Teach Language Skills to Immigrant Children in an\n  Oslo City District", "comments": "3 pages, 1 figure", "journal-ref": null, "doi": "10.1145/3371382.3378257", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social robots have been shown to help in language education for children.\nThis can be good aid for immigrant children that need additional help to learn\na second language their parents do not understand to attend school. We present\nthe setup for a long-term study that is being carried out in blinded to aid\nimmigrant children with poor skills in the Norwegian language to improve their\nvocabulary. This includes additional tools to help parents follow along and\nprovide additional help at home.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 07:30:59 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 14:57:24 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Schulz", "Trenton", ""], ["Halbach", "Till", ""], ["Solheim", "Ivar", ""]]}, {"id": "2010.05557", "submitter": "Yelena Mejova", "authors": "Yelena Mejova, V\\'ictor Suarez-Lled\\'o", "title": "Impact of Online Health Awareness Campaign: Case of National Eating\n  Disorders Association", "comments": null, "journal-ref": "Social Informatics 2020", "doi": "10.1007/978-3-030-60975-7_15", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  National Eating Disorders Association conducts a NEDAwareness week every\nyear, during which it publishes content on social media and news aimed to raise\nawareness of eating disorders. Measuring the impact of these actions is vital\nfor maximizing the effectiveness of such interventions. This paper is an effort\nto model the change in behavior of users who engage with NEDAwareness content.\nWe find that, despite popular influencers being involved in the campaign, it is\ngovernmental and nonprofit accounts that attract the most retweets.\nFurthermore, examining the tweeting language of users engaged with this\ncontent, we find linguistic categories concerning women, family, and anxiety to\nbe mentioned more within the 15 days after the intervention, and categories\nconcerning affiliation, references to others, and positive emotion mentioned\nless. We conclude with actionable implications for future campaigns and\ndiscussion of the method's limitations.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 09:21:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Mejova", "Yelena", ""], ["Suarez-Lled\u00f3", "V\u00edctor", ""]]}, {"id": "2010.05723", "submitter": "Chen He", "authors": "Chen He, Luana Micallef, Liye He, Gopal Peddinti, Tero Aittokallio,\n  Giulio Jacucci", "title": "Characterizing the Quality of Insight by Interactions: A Case Study", "comments": "To appear in IEEE Transactions on Visualization and Computer\n  Graphics. Keywords: insight, visualization, interaction, interaction pattern,\n  insight-based evaluation", "journal-ref": null, "doi": "10.1109/TVCG.2020.2977634", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the quality of insight has become increasingly important with\nthe trend of allowing users to post comments during visual exploration, yet\napproaches for qualifying insight are rare. This paper presents a case study to\ninvestigate the possibility of characterizing the quality of insight via the\ninteractions performed. To do this, we devised the interaction of a\nvisualization tool-MediSyn-for insight generation. MediSyn supports five types\nof interactions: selecting, connecting, elaborating, exploring, and sharing. We\nevaluated MediSyn with 14 participants by allowing them to freely explore the\ndata and generate insights. We then extracted seven interaction patterns from\ntheir interaction logs and correlated the patterns to four aspects of insight\nquality. The results show the possibility of qualifying insights via\ninteractions. Among other findings, exploration actions can lead to unexpected\ninsights; the drill-down pattern tends to increase the domain values of\ninsights. A qualitative analysis shows that using domain knowledge to guide\nexploration can positively affect the domain value of derived insights. We\ndiscuss the study's implications, lessons learned, and future research\nopportunities.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 14:00:56 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["He", "Chen", ""], ["Micallef", "Luana", ""], ["He", "Liye", ""], ["Peddinti", "Gopal", ""], ["Aittokallio", "Tero", ""], ["Jacucci", "Giulio", ""]]}, {"id": "2010.05988", "submitter": "Leonel Merino", "authors": "Leonel Merino, Magdalena Schwarzl, Matthias Kraus, Michael Sedlmair,\n  Dieter Schmalstieg, Daniel Weiskopf", "title": "Evaluating Mixed and Augmented Reality: A Systematic Literature Review\n  (2009-2019)", "comments": "ISMAR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a systematic review of 458 papers that report on evaluations in\nmixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST\nover a span of 11 years (2009-2019). Our goal is to provide guidance for future\nevaluations of MR/AR approaches. To this end, we characterize publications by\npaper type (e.g., technique, design study), research topic (e.g., tracking,\nrendering), evaluation scenario (e.g., algorithm performance, user\nperformance), cognitive aspects (e.g., perception, emotion), and the context in\nwhich evaluations were conducted (e.g., lab vs. in-the-wild). We found a strong\ncoupling of types, topics, and scenarios. We observe two groups: (a)\ntechnology-centric performance evaluations of algorithms that focus on\nimproving tracking, displays, reconstruction, rendering, and calibration, and\n(b) human-centric studies that analyze implications of applications and design,\nhuman factors on perception, usability, decision making, emotion, and\nattention. Amongst the 458 papers, we identified 248 user studies that involved\n5,761 participants in total, of whom only 1,619 were identified as female. We\nidentified 43 data collection methods used to analyze 10 cognitive aspects. We\nfound nine objective methods, and eight methods that support qualitative\nanalysis. A majority (216/248) of user studies are conducted in a laboratory\nsetting. Often (138/248), such studies involve participants in a static way.\nHowever, we also found a fair number (30/248) of in-the-wild studies that\ninvolve participants in a mobile fashion. We consider this paper to be relevant\nto academia and industry alike in presenting the state-of-the-art and guiding\nthe steps to designing, conducting, and analyzing results of evaluations in\nMR/AR.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 19:30:46 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Merino", "Leonel", ""], ["Schwarzl", "Magdalena", ""], ["Kraus", "Matthias", ""], ["Sedlmair", "Michael", ""], ["Schmalstieg", "Dieter", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "2010.06035", "submitter": "Jaylin Herskovitz", "authors": "Jaylin Herskovitz, Jason Wu, Samuel White, Amy Pavel, Gabriel Reyes,\n  Anhong Guo, Jeffrey P. Bigham", "title": "Making Mobile Augmented Reality Applications Accessible", "comments": "14 pages. 6 figures. Published in The 22nd International ACM\n  SIGACCESS Conference on Computers and Accessibility (ASSETS '20)", "journal-ref": null, "doi": "10.1145/3373625.3417006", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) technology creates new immersive experiences in\nentertainment, games, education, retail, and social media. AR content is often\nprimarily visual and it is challenging to enable access to it non-visually due\nto the mix of virtual and real-world content. In this paper, we identify common\nconstituent tasks in AR by analyzing existing mobile AR applications for iOS,\nand characterize the design space of tasks that require accessible\nalternatives. For each of the major task categories, we create prototype\naccessible alternatives that we evaluate in a study with 10 blind participants\nto explore their perceptions of accessible AR. Our study demonstrates that\nthese prototypes make AR possible to use for blind users and reveals a number\nof insights to move forward. We believe our work sets forth not only exemplars\nfor developers to create accessible AR applications, but also a roadmap for\nfuture research to make AR comprehensively accessible.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 21:23:27 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Herskovitz", "Jaylin", ""], ["Wu", "Jason", ""], ["White", "Samuel", ""], ["Pavel", "Amy", ""], ["Reyes", "Gabriel", ""], ["Guo", "Anhong", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "2010.06077", "submitter": "Diego Monteiro", "authors": "Diego Monteiro, Hai-Ning Liang, Hongji Li, Yu Fu and Xian Wang", "title": "Evaluating the Effect of Audience in a Virtual Reality Presentation\n  Training Tool", "comments": "9 pages, 3 figures, 33rd International Conference on Computer\n  Animation and Social Agents", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Public speaking is an essential skill in everyone's professional or academic\ncareer. Nevertheless, honing this skill is often tricky because training in\nfront of a mirror does not give feedback or inspire the same anxiety as\npresent-ing in front of an audience. Further, most people do not always have\naccess to the place where the presentation will happen. In this research, we\ndeveloped a Virtual Reality (VR) environment to assist in improving people's\npresentation skills. Our system uses 3D scanned people to create more realistic\nscenarios. We conducted a study with twelve participants who had no prior\nexperience with VR. We validated our virtual environment by analyzing whether\nit was preferred to no VR system and accepted regardless of the existence of a\nvirtual audience. Our results show that users overwhelmingly prefer to use the\nVR system as a tool to help them improve their public speaking skills than\ntraining in an empty environment. However, the preference for an audience is\nmixed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 23:28:11 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Monteiro", "Diego", ""], ["Liang", "Hai-Ning", ""], ["Li", "Hongji", ""], ["Fu", "Yu", ""], ["Wang", "Xian", ""]]}, {"id": "2010.06152", "submitter": "Hai-Ning Liang", "authors": "Jialin Wang, Hai-Ning Liang, Diego Monteiro, Wenge Xu, Hao Chen, and\n  Qiwen Chen", "title": "Real-Time Detection of Simulator Sickness in Virtual Reality Games Based\n  on Players' Psychophysiological Data during Gameplay", "comments": "IEEE International Symposium on Mixed and Augmented Reality (ISMAR)", "journal-ref": "Adjunct Proceedings of the 2020 IEEE International Symposium on\n  Mixed and Augmented Reality (ISMAR'20)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) technology has been proliferating in the last decade,\nespecially in the last few years. However, Simulator Sickness (SS) still\nrepresents a significant problem for its wider adoption. Currently, the most\ncommon way to detect SS is using the Simulator Sickness Questionnaire (SSQ).\nSSQ is a subjective measurement and is inadequate for real-time applications\nsuch as VR games. This research aims to investigate how to use machine learning\ntechniques to detect SS based on in-game characters' and users' physiological\ndata during gameplay in VR games. To achieve this, we designed an experiment to\ncollect such data with three types of games. We trained a Long Short-Term\nMemory neural network with the dataset eye-tracking and character movement data\nto detect SS in real-time. Our results indicate that, in VR games, our model is\nan accurate and efficient way to detect SS in real-time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 03:53:07 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Wang", "Jialin", ""], ["Liang", "Hai-Ning", ""], ["Monteiro", "Diego", ""], ["Xu", "Wenge", ""], ["Chen", "Hao", ""], ["Chen", "Qiwen", ""]]}, {"id": "2010.06259", "submitter": "Marios Constantinides", "authors": "Bon Adriel Aseniero, Marios Constantinides, Sagar Joglekar, Ke Zhou,\n  Daniele Quercia", "title": "MeetCues: Supporting Online Meetings Experience", "comments": "5 pages, 2 figures, 1 table", "journal-ref": null, "doi": "10.1109/VIS47514.2020.00054", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The remote work ecosystem is transforming patterns of communication between\nteams and individuals located at distance. Particularly, the absence of certain\nsubtle cues in current communication tools may hinder an online's meeting\noutcome by negatively impacting attendees' overall experience and, often, make\nthem feeling disconnected. The problem here might be due to the fact that\ncurrent tools fall short in capturing it. To partly address this, we developed\nan online platform-MeetCues-with the aim of supporting online communication\nduring meetings. MeetCues is a companion platform for a commercial\ncommunication tool with interactive and visual UI features that support\nback-channels of communications. It allows attendees to be more engaged during\na meeting, and reflect in real-time or post-meeting. We evaluated our platform\nin a diverse set of five, real-world corporate meetings, and we found that, not\nonly people were more engaged and aware during their meetings, but they also\nfelt more connected. These findings suggest promise in the design of new\ncommunications tools, and reinforce the role of InfoVis in augmenting and\nenriching online meetings.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 09:48:34 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Aseniero", "Bon Adriel", ""], ["Constantinides", "Marios", ""], ["Joglekar", "Sagar", ""], ["Zhou", "Ke", ""], ["Quercia", "Daniele", ""]]}, {"id": "2010.06279", "submitter": "Ji\\v{r}\\'i Chmel\\'ik", "authors": "Milan Dolezal, Jiri Chmelik, and Fotis Liarokapis", "title": "An Immersive Virtual Environment for Collaborative Geovisualization", "comments": "4 pages, 7 figures", "journal-ref": null, "doi": "10.1109/VS-GAMES.2017.8056613", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an immersive virtual reality environment that can be used\nto develop collaborative educational applications. Multiple users can\ncollaborate within the virtual shared space and communicate with each other\nthrough voice. To asses the feasibility of the collaborative environment a\nnovel case-study concerned the education of a geography was developed and\nevaluated. The geovisualization experiment scenario explores the possibility of\nlearning geography in a collaborative virtual environment. A user-study with 30\nparticipants was performed. Participants evaluated and commented on the\nusability and interaction methods used within the virtual environment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:45:16 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Dolezal", "Milan", ""], ["Chmelik", "Jiri", ""], ["Liarokapis", "Fotis", ""]]}, {"id": "2010.06296", "submitter": "Sanja \\v{S}\\'cepanovi\\'c", "authors": "Wonyoung So, Edyta P. Bogucka, Sanja \\v{S}\\'cepanovi\\'c, Sagar\n  Joglekar, Ke Zhou, and Daniele Quercia", "title": "Humane Visual AI: Telling the Stories Behind a Medical Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A biological understanding is key for managing medical conditions, yet\npsychological and social aspects matter too. The main problem is that these two\naspects are hard to quantify and inherently difficult to communicate. To\nquantify psychological aspects, this work mined around half a million Reddit\nposts in the sub-communities specialised in 14 medical conditions, and it did\nso with a new deep-learning framework. In so doing, it was able to associate\nmentions of medical conditions with those of emotions. To then quantify social\naspects, this work designed a probabilistic approach that mines open\nprescription data from the National Health Service in England to compute the\nprevalence of drug prescriptions, and to relate such a prevalence to census\ndata. To finally visually communicate each medical condition's biological,\npsychological, and social aspects through storytelling, we designed a\nnarrative-style layered Martini Glass visualization. In a user study involving\n52 participants, after interacting with our visualization, a considerable\nnumber of them changed their mind on previously held opinions: 10% gave more\nimportance to the psychological aspects of medical conditions, and 27% were\nmore favourable to the use of social media data in healthcare, suggesting the\nimportance of persuasive elements in interactive visualizations.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:23:51 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["So", "Wonyoung", ""], ["Bogucka", "Edyta P.", ""], ["\u0160\u0107epanovi\u0107", "Sanja", ""], ["Joglekar", "Sagar", ""], ["Zhou", "Ke", ""], ["Quercia", "Daniele", ""]]}, {"id": "2010.06328", "submitter": "Diana C. Hernandez-Bocanegra", "authors": "D. C. Hernandez-Bocanegra, J. Ziegler", "title": "Assessing the Helpfulness of Review Content for Explaining\n  Recommendations", "comments": "4 pages, In Proceedings of SIGIR 2019 Workshop on ExplainAble\n  Recommendation and Search (EARS 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the maturity already achieved by recommender systems algorithms,\nlittle is known about how to obtain and provide users with a proper rationale\nfor a recommendation. Transparency and effectiveness of recommender systems may\nbe increased when explanations are provided. In particular, identifying of\nhelpful argumentative content from reviews can be leveraged to generate textual\nexplanations. In this paper, we investigate the reasons why a review might be\nconsidered helpful, and show that the perception of credibility and\nconvincingness mediates the relationship between helpfulness and the perception\nof objectivity and relevant aspects addressed. Our findings led us to suggest\nan argumentbased approach to automatically extracting helpful content from\nhotel reviews, a domain that differs from those that best fit classical\nargumentation theories.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 12:24:43 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Hernandez-Bocanegra", "D. C.", ""], ["Ziegler", "J.", ""]]}, {"id": "2010.06403", "submitter": "Akhila Sri Manasa Venigalla", "authors": "Akhila Sri Manasa Venigalla and Sridhar Chimalakonda", "title": "EmoG- Towards Emojifying Gmail Conversations", "comments": "9 pages, 8 figures, 1 table, To appear in Proceedings of 54th Hawaii\n  International Conference on System Sciences, HICSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Emails are one of the most frequently used medium of communication in the\npresent day across multiple domains including industry and educational\ninstitutions. Understanding sentiments being expressed in an email could have a\nconsiderable impact on the recipients' action or response to the email.\nHowever, it is difficult to interpret emotions of the sender from pure text in\nwhich emotions are not explicitly present. Researchers have tried to predict\ncustomer attrition by integrating emails in client-company environment with\nemotions. However, most of the existing works deal with static assessment of\nemail emotions. Presenting sentiments of emails dynamically to the reader could\nhelp in understanding senders' emotion and as well have an impact on readers'\naction. Hence, in this paper, we present EmoG as a Google Chrome Extension\nwhich is intended to support university students. It augments emails with\nemojis based on the sentiment being conveyed in the email, which might also\noffer faster overview of email sentiments and act as tags that could help in\nautomatic sorting and processing of emails. Currently, EmoG has been developed\nto support Gmail inbox on a Google Chrome browser, and could be extended to\nother inboxes and browsers with ease. We have conducted a user survey with 15\nuniversity students to understand the usefulness of EmoG and received positive\nfeedback.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:01:11 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 09:48:06 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Venigalla", "Akhila Sri Manasa", ""], ["Chimalakonda", "Sridhar", ""]]}, {"id": "2010.06416", "submitter": "Hideyoshi Yanagisawa", "authors": "Chang Li and Hideyoshi Yanagisawa", "title": "Intrinsic motivation in virtual assistant interaction for fostering\n  spontaneous interactions", "comments": null, "journal-ref": "PLoS ONE, 16(4), e0250326 (2021)", "doi": "10.1371/journal.pone.0250326", "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the growing utility of today's conversational virtual assistants, the\nimportance of user motivation in human-AI interaction is becoming more obvious.\nHowever, previous studies in this and related fields, such as human-computer\ninteraction and human-robot interaction, scarcely discussed intrinsic\nmotivation and its affecting factors. Those studies either treated motivation\nas an inseparable concept or focused on non-intrinsic motivation. The current\nstudy aims to cover intrinsic motivation by taking an affective-engineering\napproach. A novel motivation model is proposed, in which intrinsic motivation\nis affected by two factors that derive from user interactions with virtual\nassistants: expectation of capability and uncertainty. Experiments are\nconducted where these two factors are manipulated by making participants\nbelieve they are interacting with the smart speaker \"Amazon Echo\". Intrinsic\nmotivation is measured both by using questionnaires and by covertly monitoring\na five-minute free-choice period in the experimenter's absence, during which\nthe participants could decide for themselves whether to interact with the\nvirtual assistants. Results of the first experiment showed that high\nexpectation engenders more intrinsically motivated interaction compared with\nlow expectation. The results also suggested suppressive effects by uncertainty\non intrinsic motivation, though we had not hypothesized before experiments. We\nthen revised our hypothetical model of action selection accordingly and\nconducted a verification experiment of uncertainty's effects. Results of the\nverification experiment showed that reducing uncertainty encourages more\ninteractions and causes the motivation behind these interactions to shift from\nnon-intrinsic to intrinsic.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 14:23:57 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Chang", ""], ["Yanagisawa", "Hideyoshi", ""]]}, {"id": "2010.06524", "submitter": "Perry R. Cook", "authors": "Perry R. Cook", "title": "Principles for Designing Computer Music Controllers", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2001", "journal-ref": null, "doi": "10.5281/zenodo.1176358", "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper will present observations on the design, artistic, and human\nfactors of creating digital music controllers. Specific projects will be\npresented, and a set of design principles will be supported from those\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 17:10:13 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Cook", "Perry R.", ""]]}, {"id": "2010.06584", "submitter": "Lakmal Meegahapola", "authors": "Darshana Rathnayake, Ashen de Silva, Dasun Puwakdandawa, Lakmal\n  Meegahapola, Archan Misra, Indika Perera", "title": "Jointly Optimizing Sensing Pipelines for Multimodal Mixed Reality\n  Interaction", "comments": "17th IEEE International Conference on Mobile Ad-Hoc and Sensor\n  Systems (MASS) - 2020", "journal-ref": null, "doi": "10.1109/MASS50613.2020.00046", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural human interactions for Mixed Reality Applications are overwhelmingly\nmultimodal: humans communicate intent and instructions via a combination of\nvisual, aural and gestural cues. However, supporting low-latency and accurate\ncomprehension of such multimodal instructions (MMI), on resource-constrained\nwearable devices, remains an open challenge, especially as the state-of-the-art\ncomprehension techniques for each individual modality increasingly utilize\ncomplex Deep Neural Network models. We demonstrate the possibility of\novercoming the core limitation of latency--vs.--accuracy tradeoff by exploiting\ncross-modal dependencies -- i.e., by compensating for the inferior performance\nof one model with an increased accuracy of more complex model of a different\nmodality. We present a sensor fusion architecture that performs MMI\ncomprehension in a quasi-synchronous fashion, by fusing visual, speech and\ngestural input. The architecture is reconfigurable and supports dynamic\nmodification of the complexity of the data processing pipeline for each\nindividual modality in response to contextual changes. Using a representative\n\"classroom\" context and a set of four common interaction primitives, we then\ndemonstrate how the choices between low and high complexity models for each\nindividual modality are coupled. In particular, we show that (a) a judicious\ncombination of low and high complexity models across modalities can offer a\ndramatic 3-fold decrease in comprehension latency together with an increase\n10-15% in accuracy, and (b) the right collective choice of models is context\ndependent, with the performance of some model combinations being significantly\nmore sensitive to changes in scene context or choice of interaction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:13:24 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 17:17:00 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Rathnayake", "Darshana", ""], ["de Silva", "Ashen", ""], ["Puwakdandawa", "Dasun", ""], ["Meegahapola", "Lakmal", ""], ["Misra", "Archan", ""], ["Perera", "Indika", ""]]}, {"id": "2010.06693", "submitter": "Hamdi Yahia", "authors": "Yahia Hamdi, Hanen Akouaydi, Houcine Boubaker, Adel M. Alimi", "title": "Handwriting Quality Analysis using Online-Offline Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is part of an innovative e-learning project allowing the\ndevelopment of an advanced digital educational tool that provides feedback\nduring the process of learning handwriting for young school children (three to\neight years old). In this paper, we describe a new method for children\nhandwriting quality analysis. It automatically detects mistakes, gives\nreal-time on-line feedback for children's writing, and helps teachers\ncomprehend and evaluate children's writing skills. The proposed method adjudges\nfive main criteria shape, direction, stroke order, position respect to the\nreference lines, and kinematics of the trace. It analyzes the handwriting\nquality and automatically gives feedback based on the combination of three\nextracted models: Beta-Elliptic Model (BEM) using similarity detection (SD) and\ndissimilarity distance (DD) measure, Fourier Descriptor Model (FDM), and\nperceptive Convolutional Neural Network (CNN) with Support Vector Machine (SVM)\ncomparison engine. The originality of our work lies partly in the system\narchitecture which apprehends complementary dynamic, geometric, and visual\nrepresentation of the examined handwritten scripts and in the efficient\nselected features adapted to various handwriting styles and multiple script\nlanguages such as Arabic, Latin, digits, and symbol drawing. The application\noffers two interactive interfaces respectively dedicated to learners,\neducators, experts or teachers and allows them to adapt it easily to the\nspecificity of their disciples. The evaluation of our framework is enhanced by\na database collected in Tunisia primary school with 400 children. Experimental\nresults show the efficiency and robustness of our suggested framework that\nhelps teachers and children by offering positive feedback throughout the\nhandwriting learning process using tactile digital devices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:33:56 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Hamdi", "Yahia", ""], ["Akouaydi", "Hanen", ""], ["Boubaker", "Houcine", ""], ["Alimi", "Adel M.", ""]]}, {"id": "2010.06694", "submitter": "Qiang Ning", "authors": "Qiang Ning, Hao Wu, Pradeep Dasigi, Dheeru Dua, Matt Gardner, Robert\n  L. Logan IV, Ana Marasovic, Zhen Nie", "title": "Easy, Reproducible and Quality-Controlled Data Collection with Crowdaq", "comments": "Accepted to the demo track of EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality and large-scale data are key to success for AI systems. However,\nlarge-scale data annotation efforts are often confronted with a set of common\nchallenges: (1) designing a user-friendly annotation interface; (2) training\nenough annotators efficiently; and (3) reproducibility. To address these\nproblems, we introduce Crowdaq, an open-source platform that standardizes the\ndata collection pipeline with customizable user-interface components, automated\nannotator qualification, and saved pipelines in a re-usable format. We show\nthat Crowdaq simplifies data annotation significantly on a diverse set of data\ncollection use cases and we hope it will be a convenient tool for the\ncommunity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 03:46:03 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Ning", "Qiang", ""], ["Wu", "Hao", ""], ["Dasigi", "Pradeep", ""], ["Dua", "Dheeru", ""], ["Gardner", "Matt", ""], ["Logan", "Robert L.", "IV"], ["Marasovic", "Ana", ""], ["Nie", "Zhen", ""]]}, {"id": "2010.06850", "submitter": "Hanan Hindy", "authors": "Elochukwu Ukwandu, Mohamed Amine Ben Farah, Hanan Hindy, David\n  Brosset, Dimitris Kavallieros, Robert Atkinson, Christos Tachtatzis, Miroslav\n  Bures, Ivan Andonovic and Xavier Bellekens", "title": "A Review of Cyber-Ranges and Test-Beds: Current and Future Trends", "comments": "43 pages, 18 Figures, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber situational awareness has been proven to be of value in forming a\ncomprehensive understanding of threats and vulnerabilities within\norganisations, as the degree of exposure is governed by the prevailing levels\nof cyber-hygiene and established processes. A more accurate assessment of the\nsecurity provision informs on the most vulnerable environments that necessitate\nmore diligent management. The rapid proliferation in the automation of\ncyber-attacks is reducing the gap between information and operational\ntechnologies and the need to review the current levels of robustness against\nnew sophisticated cyber-attacks, trends, technologies and mitigation\ncountermeasures has become pressing. A deeper characterisation is also the\nbasis with which to predict future vulnerabilities in turn guiding the most\nappropriate deployment technologies. Thus, refreshing established practices and\nthe scope of the training to support the decision making of users and\noperators. The foundation of the training provision is the use of Cyber-Ranges\n(CRs) and Test-Beds (TBs), platforms/tools that help inculcate a deeper\nunderstanding of the evolution of an attack and the methodology to deploy the\nmost impactful countermeasures to arrest breaches. In this paper, an evaluation\nof documented CR and TB platforms is evaluated. CRs and TBs are segmented by\ntype, technology, threat scenarios, applications and the scope of attainable\ntraining. To enrich the analysis of documented CR and TB research and cap the\nstudy, a taxonomy is developed to provide a broader comprehension of the future\nof CRs and TBs. The taxonomy elaborates on the CRs/TBs different dimensions, as\nwell as, highlighting a diminishing differentiation between application areas.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 07:25:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Ukwandu", "Elochukwu", ""], ["Farah", "Mohamed Amine Ben", ""], ["Hindy", "Hanan", ""], ["Brosset", "David", ""], ["Kavallieros", "Dimitris", ""], ["Atkinson", "Robert", ""], ["Tachtatzis", "Christos", ""], ["Bures", "Miroslav", ""], ["Andonovic", "Ivan", ""], ["Bellekens", "Xavier", ""]]}, {"id": "2010.07022", "submitter": "Jonathan Kelly", "authors": "Alexis Morris and Hallie Siegel and Jonathan Kelly", "title": "Towards a Policy-as-a-Service Framework to Enable Compliant, Trustworthy\n  AI and HRI Systems in the Wild", "comments": "In Proceedings of the AAAI Fall Symposium on Artificial Intelligence\n  for Human-Robot Interaction: Trust & Explainability in Artificial\n  Intelligence for Human-Robot Interaction (AI-HRI'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.RO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building trustworthy autonomous systems is challenging for many reasons\nbeyond simply trying to engineer agents that 'always do the right thing.' There\nis a broader context that is often not considered within AI and HRI: that the\nproblem of trustworthiness is inherently socio-technical and ultimately\ninvolves a broad set of complex human factors and multidimensional\nrelationships that can arise between agents, humans, organizations, and even\ngovernments and legal institutions, each with their own understanding and\ndefinitions of trust. This complexity presents a significant barrier to the\ndevelopment of trustworthy AI and HRI systems---while systems developers may\ndesire to have their systems 'always do the right thing,' they generally lack\nthe practical tools and expertise in law, regulation, policy and ethics to\nensure this outcome. In this paper, we emphasize the \"fuzzy\" socio-technical\naspects of trustworthiness and the need for their careful consideration during\nboth design and deployment. We hope to contribute to the discussion of\ntrustworthy engineering in AI and HRI by i) describing the policy landscape\nthat must be considered when addressing trustworthy computing and the need for\nusable trust models, ii) highlighting an opportunity for trustworthy-by-design\nintervention within the systems engineering process, and iii) introducing the\nconcept of a \"policy-as-a-service\" (PaaS) framework that can be readily applied\nby AI systems engineers to address the fuzzy problem of trust during the\ndevelopment and (eventually) runtime process. We envision that the PaaS\napproach, which offloads the development of policy design parameters and\nmaintenance of policy standards to policy experts, will enable runtime trust\ncapabilities intelligent systems in the wild.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 18:32:31 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Morris", "Alexis", ""], ["Siegel", "Hallie", ""], ["Kelly", "Jonathan", ""]]}, {"id": "2010.07035", "submitter": "Marlesson Santana", "authors": "Marlesson R. O. Santana, Luckeciano C. Melo, Fernando H. F. Camargo,\n  Bruno Brand\\~ao, Anderson Soares, Renan M. Oliveira and Sandor Caetano", "title": "MARS-Gym: A Gym framework to model, train, and evaluate Recommender\n  Systems for Marketplaces", "comments": "15 pages, 14 figures, see\n  https://github.com/deeplearningbrasil/mars-gym", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems are especially challenging for marketplaces since they\nmust maximize user satisfaction while maintaining the healthiness and fairness\nof such ecosystems. In this context, we observed a lack of resources to design,\ntrain, and evaluate agents that learn by interacting within these environments.\nFor this matter, we propose MARS-Gym, an open-source framework to empower\nresearchers and engineers to quickly build and evaluate Reinforcement Learning\nagents for recommendations in marketplaces. MARS-Gym addresses the whole\ndevelopment pipeline: data processing, model design and optimization, and\nmulti-sided evaluation. We also provide the implementation of a diverse set of\nbaseline agents, with a metrics-driven analysis of them in the Trivago\nmarketplace dataset, to illustrate how to conduct a holistic assessment using\nthe available metrics of recommendation, off-policy estimation, and fairness.\nWith MARS-Gym, we expect to bridge the gap between academic research and\nproduction systems, as well as to facilitate the design of new algorithms and\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 16:39:31 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Santana", "Marlesson R. O.", ""], ["Melo", "Luckeciano C.", ""], ["Camargo", "Fernando H. F.", ""], ["Brand\u00e3o", "Bruno", ""], ["Soares", "Anderson", ""], ["Oliveira", "Renan M.", ""], ["Caetano", "Sandor", ""]]}, {"id": "2010.07113", "submitter": "Loris Barbieri", "authors": "Fabio Bruno, Loris Barbieri, Marino Mangeruga, Marco Cozza, Antonio\n  Lagudi, Jan \\v{C}ejka, Fotis Liarokapis, Dimitrios Skarlatos", "title": "Underwater Augmented Reality for improving the diving experience in\n  submerged archaeological sites", "comments": null, "journal-ref": "Ocean Engineering, Volume 190, 15 October 2019, 106487", "doi": "10.1016/j.oceaneng.2019.106487", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mediterranean Sea has a vast maritime heritage which exploitation is made\ndifficult because of the many limitations imposed by the submerged environment.\nArchaeological diving tours, in fact, suffer from the impossibility to provide\nunderwater an exhaustive explanation of the submerged remains. Furthermore, low\nvisibility conditions, due to water turbidity and biological colonization,\nsometimes make very confusing for tourists to find their way around in the\nunderwater archaeological site. To this end, the paper investigates the\nfeasibility and potentials of the underwater Augmented Reality (UWAR)\ntechnologies developed in the iMARECulture project for improving the experience\nof the divers that visit the Underwater Archaeological Park of Baiae (Naples).\nIn particular, the paper presents two UWAR technologies that adopt hybrid\ntracking techniques to perform an augmented visualization of the actual\nconditions and of a hypothetical 3D reconstruction of the archaeological\nremains as appeared in the past. The first one integrates a marker-based\ntracking with inertial sensors, while the second one adopts a markerless\napproach that integrates acoustic localization and visual-inertial odometry.\nThe experimentations show that the proposed UWAR technologies could contribute\nto have a better comprehension of the underwater site and its archaeological\nremains.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 14:08:00 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Bruno", "Fabio", ""], ["Barbieri", "Loris", ""], ["Mangeruga", "Marino", ""], ["Cozza", "Marco", ""], ["Lagudi", "Antonio", ""], ["\u010cejka", "Jan", ""], ["Liarokapis", "Fotis", ""], ["Skarlatos", "Dimitrios", ""]]}, {"id": "2010.07209", "submitter": "Marios Constantinides", "authors": "Chao Ying Qin, Marios Constantinides, Luca Maria Aiello, Daniele\n  Quercia", "title": "HeartBees: Visualizing Crowd Affects", "comments": "8 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective sharing within groups strengthens coordination and empathy, leads\nto better health outcomes, and increases productivity and performance. Existing\ntools for affective sharing face one main challenge: creating a representation\nof collective emotional states that is relatable and universally accessible. To\novercome this challenge, we propose HeartBees, a bio-feedback system for\nvisualizing collective emotional states, which maps a multi-dimensional emotion\nmodel into a metaphorical visualization of flocks of birds. Grounded on\nAffective Computing literature and physiological sensing, we mapped\nphysiological indicators that could be obtained from wearable devices into a\nmulti-dimensional emotion model, which, in turn, our HeartBees can make use of.\nWe evaluated our nature-inspired interactive system with 353 online\nparticipants, whose responses showed good consensus in the way they\nsubjectively perceived the visualizations. Last, we discuss practical\napplications of HeartBees.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 16:23:12 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Qin", "Chao Ying", ""], ["Constantinides", "Marios", ""], ["Aiello", "Luca Maria", ""], ["Quercia", "Daniele", ""]]}, {"id": "2010.07358", "submitter": "Ruta Desai", "authors": "Benjamin Newman, Kevin Carlberg and Ruta Desai", "title": "Optimal Assistance for Object-Rearrangement Tasks in Augmented Reality", "comments": "19 pages including supplementary. Under review for ACM IUI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented-reality (AR) glasses that will have access to onboard sensors and\nan ability to display relevant information to the user present an opportunity\nto provide user assistance in quotidian tasks. Many such tasks can be\ncharacterized as object-rearrangement tasks. We introduce a novel framework for\ncomputing and displaying AR assistance that consists of (1) associating an\noptimal action sequence with the policy of an embodied agent and (2) presenting\nthis sequence to the user as suggestions in the AR system's heads-up display.\nThe embodied agent comprises a \"hybrid\" between the AR system and the user,\nwith the AR system's observation space (i.e., sensors) and the user's action\nspace (i.e., task-execution actions); its policy is learned by minimizing the\ntask-completion time. In this initial study, we assume that the AR system's\nobservations include the environment's map and localization of the objects and\nthe user. These choices allow us to formalize the problem of computing AR\nassistance for any object-rearrangement task as a planning problem,\nspecifically as a capacitated vehicle-routing problem. Further, we introduce a\nnovel AR simulator that can enable web-based evaluation of AR-like assistance\nand associated at-scale data collection via the Habitat simulator for embodied\nartificial intelligence. Finally, we perform a study that evaluates user\nresponse to the proposed form of AR assistance on a specific quotidian\nobject-rearrangement task, house cleaning, using our proposed AR simulator on\nmechanical turk. In particular, we study the effect of the proposed AR\nassistance on users' task performance and sense of agency over a range of task\ndifficulties. Our results indicate that providing users with such assistance\nimproves their overall performance and while users report a negative impact to\ntheir agency, they may still prefer the proposed assistance to having no\nassistance at all.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 18:46:07 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Newman", "Benjamin", ""], ["Carlberg", "Kevin", ""], ["Desai", "Ruta", ""]]}, {"id": "2010.07607", "submitter": "Gionnieve Lim", "authors": "Gionnieve Lim, Simon T. Perrault", "title": "Perceptions of News Sharing and Fake News in Singapore", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news is a prevalent problem that can undermine citizen engagement and\nbecome an obstacle to the goals of civic tech. To understand consumers'\nreactions and actions towards fake news, and their trust in various news media,\nwe conducted a survey in Singapore. We found that fake news stem largely from\ninstant messaging apps and social media, and that the problem of fake news was\nattributed more to its sharing than to its creation. Verification of news was\ndone mainly by using a search engine to check and cross-reference the news.\nAmongst the top three sources to obtain news, there was low trust reported in\nsocial media, high trust in local news channels, and highest trust in\ngovernment communication platforms. The strong trust in government\ncommunication platforms suggests that top-down civic tech initiatives may have\ngreat potential to effectively manage fake news and promote citizen engagement\nin Singapore.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:08:47 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Lim", "Gionnieve", ""], ["Perrault", "Simon T.", ""]]}, {"id": "2010.07610", "submitter": "Fabrice Muhlenbach", "authors": "Fabrice Muhlenbach", "title": "A Methodology for Ethics-by-Design AI Systems: Dealing with Human Value\n  Conflicts", "comments": "Paper presented at the 2020 IEEE International Conference on Systems,\n  Man, and Cybernetics (SMC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of artificial intelligence into activities traditionally\ncarried out by human beings produces brutal changes. This is not without\nconsequences for human values. This paper is about designing and implementing\nmodels of ethical behaviors in AI-based systems, and more specifically it\npresents a methodology for designing systems that take ethical aspects into\naccount at an early stage while finding an innovative solution to prevent human\nvalues from being affected. Two case studies where AI-based innovations\ncomplement economic and social proposals with this methodology are presented:\none in the field of culture and operated by a private company, the other in the\nfield of scientific research and supported by a state organization.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 09:14:00 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Muhlenbach", "Fabrice", ""]]}, {"id": "2010.07703", "submitter": "Thomas Kosch", "authors": "Thomas Kosch", "title": "Workload-Aware Systems and Interfaces for Cognitive Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In today's society, our cognition is constantly influenced by information\nintake, attention switching, and task interruptions. This increases the\ndifficulty of a given task, adding to the existing workload and leading to\ncompromised cognitive performances. The human body expresses the use of\ncognitive resources through physiological responses when confronted with a\nplethora of cognitive workload. This temporarily mobilizes additional resources\nto deal with the workload at the cost of accelerated mental exhaustion. We\npredict that recent developments in physiological sensing will increasingly\ncreate user interfaces that are aware of the user's cognitive capacities, hence\nable to intervene when high or low states of cognitive workload are detected.\nSubsequently, we investigate suitable feedback modalities in a user-centric\ndesign process which are desirable for cognitive assistance. We then\ninvestigate different physiological sensing modalities to enable suitable\nreal-time assessments of cognitive workload. We provide evidence that the human\nbrain and eye gaze are sensitive to fluctuations in cognitive resting states.\nWe show that electroencephalography and eye tracking are reliable modalities to\nassess mental workload during user interface operation. In the end, we present\napplications that regulate cognitive workload in home and work setting,\ninvestigate how cognitive workload can be visualized to the user, and show how\ncognitive workload measurements can be used to predict the efficiency of\ninformation intake through reading interfaces. Finally, we present our vision\nof future workload-aware interfaces. Previous interfaces were limited in their\nability to utilize cognitive workload for user interaction. Together with the\ncollected data sets, this thesis paves the way for methodical and technical\ntools that integrate workload-awareness as a factor for context-aware systems.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 12:19:27 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 07:59:41 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Kosch", "Thomas", ""]]}, {"id": "2010.07798", "submitter": "Soraia Figueiredo Paulo MSc", "authors": "Soraia F. Paulo (1), Daniel Medeiros (2), Pedro Borges (1), Joaquim\n  Jorge (1), Daniel Sim\\~oes Lopes (1) ((1) INESC-ID Lisboa, Instituto Superior\n  T\\'ecnico, ULisboa, (2) CMIC, Victoria University of Wellington)", "title": "Camera Travel for Immersive Colonography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive Colonography allows medical professionals to navigate inside the\nintricate tubular geometries of subject-specific 3D colon images using Virtual\nReality displays. Typically, camera travel is performed via Fly-Through or\nFly-Over techniques that enable semi-automatic traveling through a constrained,\nwell-defined path at user controlled speeds. However, Fly-Through is known to\nlimit the visibility of lesions located behind or inside haustral folds, while\nFly-Over requires splitting the entire colon visualization into two specific\nhalves. In this paper, we study the effect of immersive Fly-Through and\nFly-Over techniques on lesion detection, and introduce a camera travel\ntechnique that maintains a fixed camera orientation throughout the entire\nmedial axis path. While these techniques have been studied in non-VR desktop\nenvironments, their performance is yet not well understood in VR setups. We\nperformed a comparative study to ascertain which camera travel technique is\nmore appropriate for constrained path navigation in Immersive Colonography. To\nthis end, we asked 18 participants to navigate inside a 3D colon to find\nspecific marks. Our results suggest that the Fly-Over technique may lead to\nenhanced lesion detection at the cost of higher task completion times, while\nthe Fly-Through method may offer a more balanced trade-off between both speed\nand effectiveness, whereas the fixed camera orientation technique provided\nseemingly inferior performance results. Our study further provides design\nguidelines and informs future work.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 14:42:50 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Paulo", "Soraia F.", ""], ["Medeiros", "Daniel", ""], ["Borges", "Pedro", ""], ["Jorge", "Joaquim", ""], ["Lopes", "Daniel Sim\u00f5es", ""]]}, {"id": "2010.07938", "submitter": "Charvi Rastogi", "authors": "Charvi Rastogi, Yunfeng Zhang, Dennis Wei, Kush R. Varshney, Amit\n  Dhurandhar, Richard Tomsett", "title": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted\n  Decision-making", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several strands of research have aimed to bridge the gap between artificial\nintelligence (AI) and human decision-makers in AI-assisted decision-making,\nwhere humans are the consumers of AI model predictions and the ultimate\ndecision-makers in high-stakes applications. However, people's perception and\nunderstanding is often distorted by their cognitive biases, like confirmation\nbias, anchoring bias, availability bias, to name a few. In this work, we use\nknowledge from the field of cognitive science to account for cognitive biases\nin the human-AI collaborative decision-making system and mitigate their\nnegative effects. To this end, we mathematically model cognitive biases and\nprovide a general framework through which researchers and practitioners can\nunderstand the interplay between cognitive biases and human-AI accuracy. We\nthen focus on anchoring bias, a bias commonly witnessed in human-AI\npartnerships. We devise a cognitive science-driven, time-based approach to\nde-anchoring. A user experiment shows the effectiveness of this approach in\nhuman-AI collaborative decision-making. Using the results from this first\nexperiment, we design a time allocation strategy for a resource constrained\nsetting so as to achieve optimal human-AI collaboration under some assumptions.\nA second user study shows that our time allocation strategy can effectively\ndebias the human when the AI model has low confidence and is incorrect.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:25:41 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Rastogi", "Charvi", ""], ["Zhang", "Yunfeng", ""], ["Wei", "Dennis", ""], ["Varshney", "Kush R.", ""], ["Dhurandhar", "Amit", ""], ["Tomsett", "Richard", ""]]}, {"id": "2010.08056", "submitter": "Yudi Dong", "authors": "Yudi Dong and Yu-Dong Yao", "title": "IoT Platform for COVID-19 Prevention and Control: A Survey", "comments": "12 pages; Submitted to IEEE Internet of Things Journal", "journal-ref": "IEEE Access 2021", "doi": "10.1109/ACCESS.2021.3068276", "report-no": null, "categories": "cs.HC cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a result of the worldwide transmission of severe acute respiratory\nsyndrome coronavirus 2 (SARS-CoV-2), coronavirus disease 2019 (COVID-19) has\nevolved into an unprecedented pandemic. Currently, with unavailable\npharmaceutical treatments and vaccines, this novel coronavirus results in a\ngreat impact on public health, human society, and global economy, which is\nlikely to last for many years. One of the lessons learned from the COVID-19\npandemic is that a long-term system with non-pharmaceutical interventions for\npreventing and controlling new infectious diseases is desirable to be\nimplemented. Internet of things (IoT) platform is preferred to be utilized to\nachieve this goal, due to its ubiquitous sensing ability and seamless\nconnectivity. IoT technology is changing our lives through smart healthcare,\nsmart home, and smart city, which aims to build a more convenient and\nintelligent community. This paper presents how the IoT could be incorporated\ninto the epidemic prevention and control system. Specifically, we demonstrate a\npotential fog-cloud combined IoT platform that can be used in the systematic\nand intelligent COVID-19 prevention and control, which involves five\ninterventions including COVID-19 Symptom Diagnosis, Quarantine Monitoring,\nContact Tracing & Social Distancing, COVID-19 Outbreak Forecasting, and\nSARS-CoV-2 Mutation Tracking. We investigate and review the state-of-the-art\nliteratures of these five interventions to present the capabilities of IoT in\ncountering against the current COVID-19 pandemic or future infectious disease\nepidemics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:43:03 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 18:04:14 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Dong", "Yudi", ""], ["Yao", "Yu-Dong", ""]]}, {"id": "2010.08100", "submitter": "Roghayeh Barmaki", "authors": "Lauren Baron, Brian Cohn, and Roghayeh Barmaki", "title": "When Virtual Therapy and Art Meet: A Case Study of Creative Drawing Game\n  in Virtual Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There have been a resurge lately on virtual therapy and other virtual- and\ntele-medicine services due to the new normal of practicing 'shelter at home'.\nIn this paper, we propose a creative drawing game for virtual therapy and\ninvestigate user's comfort and movement freedom in a pilot study. In a\nmixed-design study, healthy participants (N=16, 8 females) completed one of the\neasy or hard trajectories of the virtual therapy game in standing and seated\narrangements using a virtual-reality headset. The results from participants'\nmovement accuracy, task completion time, and usability questionnaires indicate\nthat participants had significant performance differences on two levels of the\ngame based on its difficulty (between-subjects factor), but no difference in\nseated and standing configurations (within-subjects factor). Also, the hard\nmode was more favorable among participants. This work offers implications on\nvirtual reality and 3D-interactive systems, with specific contributions to\nvirtual therapy, and serious games for healthcare applications.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:08:03 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Baron", "Lauren", ""], ["Cohn", "Brian", ""], ["Barmaki", "Roghayeh", ""]]}, {"id": "2010.08103", "submitter": "Bjorn Lutjens", "authors": "Bj\\\"orn L\\\"utjens, Brandon Leshchinskiy, Christian Requena-Mesa,\n  Farrukh Chishtie, Natalia D\\'iaz-Rodriguez, Oc\\'eane Boulais, Aaron Pi\\~na,\n  Dava Newman, Alexander Lavin, Yarin Gal, Chedy Ra\\\"issi", "title": "Physics-informed GANs for Coastal Flood Visualization", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As climate change increases the intensity of natural disasters, society needs\nbetter tools for adaptation. Floods, for example, are the most frequent natural\ndisaster, but during hurricanes the area is largely covered by clouds and\nemergency managers must rely on nonintuitive flood visualizations for mission\nplanning. To assist these emergency managers, we have created a deep learning\npipeline that generates visual satellite images of current and future coastal\nflooding. We advanced a state-of-the-art GAN called pix2pixHD, such that it\nproduces imagery that is physically-consistent with the output of an\nexpert-validated storm surge model (NOAA SLOSH). By evaluating the imagery\nrelative to physics-based flood maps, we find that our proposed framework\noutperforms baseline models in both physical-consistency and photorealism.\nWhile this work focused on the visualization of coastal floods, we envision the\ncreation of a global visualization of how climate change will shape our earth.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:15:34 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 06:26:46 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["L\u00fctjens", "Bj\u00f6rn", ""], ["Leshchinskiy", "Brandon", ""], ["Requena-Mesa", "Christian", ""], ["Chishtie", "Farrukh", ""], ["D\u00edaz-Rodriguez", "Natalia", ""], ["Boulais", "Oc\u00e9ane", ""], ["Pi\u00f1a", "Aaron", ""], ["Newman", "Dava", ""], ["Lavin", "Alexander", ""], ["Gal", "Yarin", ""], ["Ra\u00efssi", "Chedy", ""]]}, {"id": "2010.08155", "submitter": "Shayan Monadjemi", "authors": "Shayan Monadjemi, Quan Nguyen, Henry Chai, Roman Garnett, Alvitta\n  Ottley", "title": "Active Visual Analytics: Assisted Data Discovery in Interactive\n  Visualizations via Active Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data foraging is a process commonly arising in interactive data analysis\nwhere a user sifts through a large amount of potentially irrelevant information\nseeking data relevant to their task. In machine learning, the related task of\nactive search considers the automated discovery of rare, valuable items from\nlarge databases and has delivered massive speedups in discovery in areas\nincluding drug and materials discovery. However, there have yet to be any\nadvances in integrating active search with an interactive interface to make use\nof an analyst's domain knowledge in the search process. We introduce and\nevaluate a technique we call Active Visual Analytics (ActiveVA), an\naugmentation of interactive visualization with active search to accelerate data\nforaging. In this approach, underlying machine learning models automatically\nlearn a user's latent interest by observing their interactions; these models\nthen inform an active search algorithm that leads the user toward the points\njudged most promising for exploration. Using the epidemic dataset from VAST\nChallenge 2011, we design and conduct a crowd-sourced user study to evaluate\nseveral aspects of this technique. We present evidence that a human--computer\npartnership based on ActiveVA results in higher throughput and more meaningful\ninteractions during interactive visual exploration and discovery without any\nundue effect on the user experience.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 04:17:14 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 00:02:46 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 20:36:04 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Monadjemi", "Shayan", ""], ["Nguyen", "Quan", ""], ["Chai", "Henry", ""], ["Garnett", "Roman", ""], ["Ottley", "Alvitta", ""]]}, {"id": "2010.08457", "submitter": "Congyu Wu", "authors": "Congyu Wu, Hagen Fritz, Zoltan Nagy, Juan P. Maestre, Edison Thomaz,\n  Christine Julien, Darla M. Castelli, Kaya de Barbaro, Gabriella M. Harari, R.\n  Cameron Craddock, Kerry A. Kinney, Samuel D. Gosling, and David M. Schnyer", "title": "Multi-Modal Data Collection for Measuring Health, Behavior, and Living\n  Environment of Large-Scale Participant Cohorts: Conceptual Framework and\n  Findings from Deployments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As mobile technologies become ever more sensor-rich, portable, and\nubiquitous, data captured by smart devices are lending rich insights into\nusers' daily lives with unprecedented comprehensiveness, unobtrusiveness, and\necological validity. A number of human-subject studies have been conducted in\nthe past decade to examine the use of mobile sensing to uncover individual\nbehavioral patterns and health outcomes. While understanding health and\nbehavior is the focus for most of these studies, we find that minimal attention\nhas been placed on measuring personal environments, especially together with\nother human-centric data modalities. Moreover, the participant cohort size in\nmost existing studies falls well below a few hundred, leaving questions open\nabout the reliability of findings on the relations between mobile sensing\nsignals and human outcomes. To address these limitations, we developed a home\nenvironment sensor kit for continuous indoor air quality tracking and deployed\nit in conjunction with established mobile sensing and experience sampling\ntechniques in a cohort study of up to 1584 student participants per data type\nfor 3 weeks at a major research university in the United States. In this paper,\nwe begin by proposing a conceptual framework that systematically organizes\nhuman-centric data modalities by their temporal coverage and spatial freedom.\nThen we report our study design and procedure, technologies and methods\ndeployed, descriptive statistics of the collected data, and results from our\nextensive exploratory analyses. Our novel data, conceptual development, and\nanalytical findings provide important guidance for data collection and\nhypothesis generation in future human-centric sensing studies.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 15:47:30 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Wu", "Congyu", ""], ["Fritz", "Hagen", ""], ["Nagy", "Zoltan", ""], ["Maestre", "Juan P.", ""], ["Thomaz", "Edison", ""], ["Julien", "Christine", ""], ["Castelli", "Darla M.", ""], ["de Barbaro", "Kaya", ""], ["Harari", "Gabriella M.", ""], ["Craddock", "R. Cameron", ""], ["Kinney", "Kerry A.", ""], ["Gosling", "Samuel D.", ""], ["Schnyer", "David M.", ""]]}, {"id": "2010.08612", "submitter": "Hancheng Cao", "authors": "Zhilong Chen, Hancheng Cao, Fengli Xu, Mengjie Cheng, Tao Wang, Yong\n  Li", "title": "Understanding the Role of Intermediaries in Online Social E-commerce: An\n  Exploratory Study of Beidian", "comments": "In CSCW 2020", "journal-ref": null, "doi": "10.1145/3415185", "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social e-commerce, as a new form of social computing based marketing\nplatforms, utilizes existing real-world social relationships for promotions and\nsales of products. It has been growing rapidly in recent years and attracted\ntens of millions of users in China. A key group of actors who enable market\ntransactions on these platforms are intermediaries who connect producers with\nconsumers by sharing information with and recommending products to their\nreal-world social contacts. Despite their crucial role, the nature and behavior\nof these intermediaries on these social e-commerce platforms has not been\nsystematically analyzed. Here we address this knowledge gap through a mixed\nmethod study. Leveraging 9 months' all-round behavior of about 40 million users\non Beidian -- one of the largest social e-commerce sites in China, alongside\nwith qualitative evidence from online forums and interviews, we examine\ncharacteristics of intermediaries, identify their behavioral patterns and\nuncover strategies and mechanisms that make successful intermediaries. We\ndemonstrate that intermediaries on social e-commerce sites act as local trend\ndetectors and \"social grocers\". Furthermore, successful intermediaries are\nhighly dedicated whenever best sellers appear and broaden items for promotion.\nTo the best of our knowledge, this paper presents the first large-scale\nanalysis on the emerging role of intermediaries in social e-commerce platforms,\nwhich provides potential insights for the design and management of social\ncomputing marketing platforms.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 20:07:35 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Chen", "Zhilong", ""], ["Cao", "Hancheng", ""], ["Xu", "Fengli", ""], ["Cheng", "Mengjie", ""], ["Wang", "Tao", ""], ["Li", "Yong", ""]]}, {"id": "2010.08859", "submitter": "Bridger Herman", "authors": "Bridger Herman, Francesca Samsel, Annie Bares, Seth Johnson, Greg\n  Abram, Daniel F. Keefe", "title": "Printmaking, Puzzles, and Studio Closets: Using Artistic Metaphors to\n  Reimagine the User Interface for Designing Immersive Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We, as a society, need artists to help us interpret and explain science, but\nwhat does an artist's studio look like when today's science is built upon the\nlanguage of large, increasingly complex data? This paper presents a data\nvisualization design interface that lifts the barriers for artists to engage\nwith actively studied, 3D multivariate datasets. To accomplish this, the\ninterface must weave together the need for creative artistic processes and the\nchallenging constraints of real-time, data-driven 3D computer graphics. The\nresult is an interface for a technical process, but technical in the way\nartistic printmaking is technical, not in the sense of computer scripting and\nprogramming. Using metaphor, computer graphics algorithms and shader program\nparameters are reimagined as tools in an artist's printmaking studio. These\nartistic metaphors and language are merged with a puzzle-piece approach to\nvisual programming and matching iconography. Finally, artists access the\ninterface using a web browser, making it possible to design immersive\nmultivariate data visualizations that can be displayed in VR and AR\nenvironments using familiar drawing tablets and touch screens. We report on\ninsights from the interdisciplinary design of the interface and early feedback\nfrom artists.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 20:30:03 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Herman", "Bridger", ""], ["Samsel", "Francesca", ""], ["Bares", "Annie", ""], ["Johnson", "Seth", ""], ["Abram", "Greg", ""], ["Keefe", "Daniel F.", ""]]}, {"id": "2010.09040", "submitter": "Alexandra Diehl PhD", "authors": "Alexandra Diehl (1) and Matthias Kraus (2) and Alfie Abdul-Rahman (3)\n  and Mennatallah El-Assady (2) and Benjamin Bach (4) and Robert Steven Laramee\n  (5) and Daniel Keim (2), Min Chen (6) ((1) University of Zurich, (2)\n  University of Konstanz, (3) King's College London, (4) Edinburgh University,\n  (5) University of Nottingham, (6) University of Oxford)", "title": "Studying Visualization Guidelines According to Grounded Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization guidelines, if defined properly, are invaluable to both\npractical applications and the theoretical foundation of visualization. In this\npaper, we present a collection of research activities for studying\nvisualization guidelines according to Grounded Theory (GT). We used the\ndiscourses at VisGuides, which is an online discussion forum for visualization\nguidelines, as the main data source for enabling data-driven research processes\nas advocated by the grounded theory methodology. We devised a categorization\nscheme focusing on observing how visualization guidelines were featured in\ndifferent threads and posts at VisGuides, and coded all 248 posts between\nSeptember 27, 2017 (when VisGuides was first launched) and March 13, 2019. To\ncomplement manual categorization and coding, we used text analysis and\nvisualization to help reveal patterns that may have been missed by the manual\neffort and summary statistics. To facilitate theoretical sampling and negative\ncase analysis, we made an in-depth analysis of the 148 posts (with both\nquestions and replies) related to a student assignment of a visualization\ncourse. Inspired by two discussion threads at VisGuides, we conducted two\ncontrolled empirical studies to collect further data to validate specific\nvisualization guidelines. Through these activities guided by grounded theory,\nwe have obtained some new findings about visualization guidelines.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 17:35:40 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 19:13:20 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Diehl", "Alexandra", ""], ["Kraus", "Matthias", ""], ["Abdul-Rahman", "Alfie", ""], ["El-Assady", "Mennatallah", ""], ["Bach", "Benjamin", ""], ["Laramee", "Robert Steven", ""], ["Keim", "Daniel", ""], ["Chen", "Min", ""]]}, {"id": "2010.09041", "submitter": "Louis Comm\\`ere", "authors": "Louis Comm\\`ere, Sean U.N. Wood, Jean Rouat", "title": "Evaluation of a Vision-to-Audition Substitution System that Provides 2D\n  WHERE Information and Fast User Learning", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision to audition substitution devices are designed to convey visual\ninformation through auditory input. The acceptance of such systems depends\nheavily on their ease of use, training time, reliability and on the amount of\ncoverage of online auditory perception of current auditory scenes. Existing\ndevices typically require extensive training time or complex and\ncomputationally demanding technology. The purpose of this work is to\ninvestigate the learning curve for a vision to audition substitution system\nthat provides simple location features. Forty-two blindfolded users\nparticipated in experiments involving location and navigation tasks.\nParticipants had no prior experience with the system. For the location task,\nparticipants had to locate 3 objects on a table after a short familiarisation\nperiod (10 minutes). Then once they understood the manipulation of the device,\nthey proceeded to the navigation task: participants had to walk through a large\ncorridor without colliding with obstacles randomly placed on the floor.\nParticipants were asked to repeat the task 5 times. In the end of the\nexperiment, each participant had to fill out a questionnaire to provide\nfeedback. They were able to perform localisation and navigation effectively\nafter a short training time with an average of 10 minutes. Their navigation\nskills greatly improved across the trials.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 17:37:31 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Comm\u00e8re", "Louis", ""], ["Wood", "Sean U. N.", ""], ["Rouat", "Jean", ""]]}, {"id": "2010.09121", "submitter": "Akira Matsui", "authors": "Akira Matsui, Daisuke Moriwaki", "title": "Online-to-Offline Advertisements as Field Experiments", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertisements have become one of today's most widely used tools for\nenhancing businesses partly because of their compatibility with A/B testing.\nA/B testing allows sellers to find effective advertisement strategies such as\nad creatives or segmentations. Even though several studies propose a technique\nto maximize the effect of an advertisement, there is insufficient comprehension\nof the customers' offline shopping behavior invited by the online\nadvertisements. Herein, we study the difference in offline behavior between\ncustomers who received online advertisements and regular customers (i.e., the\ncustomers visits the target shop voluntary), and the duration of this\ndifference. We analyzed approximately three thousand users' offline behavior\nwith their 23.5 million location records through 31 A/B testings. We first\ndemonstrate the externality that customers with advertisements traverse larger\nareas than those without advertisements, and this spatial difference lasts\nseveral days after their shopping day. We then find a long-run effect of this\nexternality of advertising that a certain portion of the customers invited to\nthe offline shops revisit these shops. Finally, based on this revisit effect\nfindings, we utilize a causal machine learning model to propose a marketing\nstrategy to maximize the revisit ratio. Our results suggest that advertisements\ndraw customers who have different behavior traits from regular customers. This\nstudy's findings demonstrate that a simple analysis may underrate the effects\nof advertisements on businesses, and an analysis considering externality can\nattract potentially valuable customers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 22:04:56 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 03:51:49 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Matsui", "Akira", ""], ["Moriwaki", "Daisuke", ""]]}, {"id": "2010.09459", "submitter": "Leshem Choshen", "authors": "Eyal Shnarch, Leshem Choshen, Guy Moshkowich, Noam Slonim, Ranit\n  Aharonov", "title": "Unsupervised Expressive Rules Provide Explainability and Assist Human\n  Experts Grasping New Domains", "comments": "Accepted to Findings of EMNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaching new data can be quite deterrent; you do not know how your\ncategories of interest are realized in it, commonly, there is no labeled data\nat hand, and the performance of domain adaptation methods is unsatisfactory.\n  Aiming to assist domain experts in their first steps into a new task over a\nnew corpus, we present an unsupervised approach to reveal complex rules which\ncluster the unexplored corpus by its prominent categories (or facets).\n  These rules are human-readable, thus providing an important ingredient which\nhas become in short supply lately - explainability. Each rule provides an\nexplanation for the commonality of all the texts it clusters together.\n  We present an extensive evaluation of the usefulness of these rules in\nidentifying target categories, as well as a user study which assesses their\ninterpretability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 13:07:15 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Shnarch", "Eyal", ""], ["Choshen", "Leshem", ""], ["Moshkowich", "Guy", ""], ["Slonim", "Noam", ""], ["Aharonov", "Ranit", ""]]}, {"id": "2010.09807", "submitter": "Congyu Wu", "authors": "Congyu Wu, Amanda N. Barczyk, R. Cameron Craddock, Gabriella M.\n  Harari, Edison Thomaz, Jason D. Shumake, Christopher G. Beevers, Samuel D.\n  Gosling, and David M. Schnyer", "title": "Improving Prediction of Real-Time Loneliness and Companionship Type\n  Using Geosocial Features of Personal Smartphone Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loneliness is a widely affecting mental health symptom and can be mediated by\nand co-vary with patterns of social exposure. Using momentary survey and\nsmartphone sensing data collected from 129 Android-using college student\nparticipants over three weeks, we (1) investigate and uncover the relations\nbetween momentary loneliness experience and companionship type and (2) propose\nand validate novel geosocial features of smartphone-based Bluetooth and GPS\ndata for predicting loneliness and companionship type in real time. We base our\nfeatures on intuitions characterizing the quantity and spatiotemporal\npredictability of an individual's Bluetooth encounters and GPS location\nclusters to capture personal significance of social exposure scenarios\nconditional on their temporal distribution and geographic patterns. We examine\nour features' statistical correlation with momentary loneliness through\nregression analyses and evaluate their predictive power using a sliding window\nprediction procedure. Our features achieved significant performance improvement\ncompared to baseline for predicting both momentary loneliness and companionship\ntype, with the effect stronger for the loneliness prediction task. As such we\nrecommend incorporation and further evaluation of our geosocial features\nproposed in this study in future mental health sensing and context-aware\ncomputing applications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 19:36:52 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Wu", "Congyu", ""], ["Barczyk", "Amanda N.", ""], ["Craddock", "R. Cameron", ""], ["Harari", "Gabriella M.", ""], ["Thomaz", "Edison", ""], ["Shumake", "Jason D.", ""], ["Beevers", "Christopher G.", ""], ["Gosling", "Samuel D.", ""], ["Schnyer", "David M.", ""]]}, {"id": "2010.09850", "submitter": "Renata Georgia Raidou", "authors": "Marwin Schindler, Hsiang-Yun Wu, Renata Georgia Raidou", "title": "The Anatomical Edutainer", "comments": null, "journal-ref": null, "doi": "10.1109/VIS47514.2020.00007", "report-no": null, "categories": "cs.HC cs.CY cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical visualizations (i.e., data representations by means of physical\nobjects) have been used for many centuries in medical and anatomical education.\nRecently, 3D printing techniques started also to emerge. Still, other medical\nphysicalizations that rely on affordable and easy-to-find materials are\nlimited, while smart strategies that take advantage of the optical properties\nof our physical world have not been thoroughly investigated. We propose the\nAnatomical Edutainer, a workflow to guide the easy, accessible, and affordable\ngeneration of physicalizations for tangible, interactive anatomical\nedutainment. The Anatomical Edutainer supports 2D printable and 3D foldable\nphysicalizations that change their visual properties (i.e., hues of the visible\nspectrum) under colored lenses or colored lights, to reveal distinct anatomical\nstructures through user interaction.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 10:40:12 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Schindler", "Marwin", ""], ["Wu", "Hsiang-Yun", ""], ["Raidou", "Renata Georgia", ""]]}, {"id": "2010.09934", "submitter": "Chengzhi Zhang", "authors": "Yingyi Zhang and Chengzhi Zhang", "title": "Enhancing Keyphrase Extraction from Microblogs using Human Reading Time", "comments": null, "journal-ref": "Journal of the Association for Information Science and\n  Technology,2021", "doi": "10.1002/ASI.24430", "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The premise of manual keyphrase annotation is to read the corresponding\ncontent of an annotated object. Intuitively, when we read, more important words\nwill occupy a longer reading time. Hence, by leveraging human reading time, we\ncan find the salient words in the corresponding content. However, previous\nstudies on keyphrase extraction ignore human reading features. In this article,\nwe aim to leverage human reading time to extract keyphrases from microblog\nposts. There are two main tasks in this study. One is to determine how to\nmeasure the time spent by a human on reading a word. We use eye fixation\ndurations extracted from an open source eye-tracking corpus (OSEC). Moreover,\nwe propose strategies to make eye fixation duration more effective on keyphrase\nextraction. The other task is to determine how to integrate human reading time\ninto keyphrase extraction models. We propose two novel neural network models.\nThe first is a model in which the human reading time is used as the ground\ntruth of the attention mechanism. In the second model, we use human reading\ntime as the external feature. Quantitative and qualitative experiments show\nthat our proposed models yield better performance than the baseline models on\ntwo microblog datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:18:44 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 11:24:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Zhang", "Yingyi", ""], ["Zhang", "Chengzhi", ""]]}, {"id": "2010.09975", "submitter": "Danqing Shi", "authors": "Danqing Shi, Xinyue Xu, Fuling Sun, Yang Shi and Nan Cao", "title": "Calliope: Automatic Visual Data Story Generation from a Spreadsheet", "comments": null, "journal-ref": null, "doi": "10.1109/tvcg.2020.3030403", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual data stories shown in the form of narrative visualizations such as a\nposter or a data video, are frequently used in data-oriented storytelling to\nfacilitate the understanding and memorization of the story content. Although\nuseful, technique barriers, such as data analysis, visualization, and\nscripting, make the generation of a visual data story difficult. Existing\nauthoring tools rely on users' skills and experiences, which are usually\ninefficient and still difficult. In this paper, we introduce a novel visual\ndata story generating system, Calliope, which creates visual data stories from\nan input spreadsheet through an automatic process and facilities the easy\nrevision of the generated story based on an online story editor. Particularly,\nCalliope incorporates a new logic-oriented Monte Carlo tree search algorithm\nthat explores the data space given by the input spreadsheet to progressively\ngenerate story pieces (i.e., data facts) and organize them in a logical order.\nThe importance of data facts is measured based on information theory, and each\ndata fact is visualized in a chart and captioned by an automatically generated\ndescription. We evaluate the proposed technique through three example stories,\ntwo controlled experiments, and a series of interviews with 10 domain experts.\nOur evaluation shows that Calliope is beneficial to efficient visual data story\ngeneration.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 02:54:34 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Shi", "Danqing", ""], ["Xu", "Xinyue", ""], ["Sun", "Fuling", ""], ["Shi", "Yang", ""], ["Cao", "Nan", ""]]}, {"id": "2010.10135", "submitter": "Hartmut Koenitz", "authors": "Hartmut Koenitz, Mirjam Palosaari Eladhari, Sandy Louchart, Frank Nack", "title": "INDCOR white paper 1: A shared vocabulary for IDN (Interactive Digital\n  Narratives)", "comments": null, "journal-ref": null, "doi": null, "report-no": "INDCOR-DL-no: WP-01-1.0", "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COST Action 18230 INDCOR (Interactive Narrative Design for Complexity\nRepresentations) is an interdisciplinary network of researchers and\npractitioners intended to further the use of interactive digital narratives\n(IDN1) to represent highly complex topics. IDN possess crucial advantages in\nthis regard, but more knowledge is needed to realize these advantages in broad\nusage by media producers and the general public. The lack of a shared\nvocabulary is a crucial obstacle on the path to a generalized, accessible body\nof IDN knowledge. This white paper frames the situation from the perspective of\nINDCOR and describes the creation of an online encyclopedia as a means to\novercome this issue. Two similar and successful projects (The Living Handbook\nof Narratology and the Stanford Encyclopedia of Philosophy) serve as examples\nfor this effort, showing how community-authored encyclopedias can provide\nhigh-quality content. The authors introduce a taxonomy based on an overarching\nanalytical framework (SPP model) as the foundational element of the\nencyclopedia, and detail editorial procedures for the project, including a\npeer-review process, designed to assure high academic quality and relevance of\nencyclopedia entries. Also, a sample entry provides guidance for authors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:00:46 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 00:20:28 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Koenitz", "Hartmut", ""], ["Eladhari", "Mirjam Palosaari", ""], ["Louchart", "Sandy", ""], ["Nack", "Frank", ""]]}, {"id": "2010.10144", "submitter": "Alan Smeaton", "authors": "Alan F. Smeaton, Naveen Garaga Krishnamurthy, Amruth Hebbasuru\n  Suryanarayana", "title": "Keystroke Dynamics as Part of Lifelogging", "comments": "Accepted to 27th International Conference on Multimedia Modeling,\n  Prague, Czech Republic, June 2021", "journal-ref": null, "doi": "10.1007/978-3-030-67835-7_16", "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the case for including keystroke dynamics in\nlifelogging. We describe how we have used a simple keystroke logging\napplication called Loggerman, to create a dataset of longitudinal keystroke\ntiming data spanning a period of more than 6 months for 4 participants. We\nperform a detailed analysis of this data by examining the timing information\nassociated with bigrams or pairs of adjacently-typed alphabetic characters. We\nshow how there is very little day-on-day variation of the keystroke timing\namong the top-200 bigrams for some participants and for others there is a lot\nand this correlates with the amount of typing each would do on a daily basis.\nWe explore how daily variations could correlate with sleep score from the\nprevious night but find no significant relation-ship between the two. Finally\nwe describe the public release of this data as well including as a series of\npointers for future work including correlating keystroke dynamics with mood and\nfatigue during the day.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:23:45 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Smeaton", "Alan F.", ""], ["Krishnamurthy", "Naveen Garaga", ""], ["Suryanarayana", "Amruth Hebbasuru", ""]]}, {"id": "2010.10148", "submitter": "Thomas Kosch", "authors": "Matthias Hoppe, Yannick Wei{\\ss}, Marinus Burger, Thomas Kosch", "title": "Don't Drone Yourself in Work: Discussing DronOS as a Framework for\n  Human-Drone Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  More and more off-the-shelf drones provide frameworks that enable the\nprogramming of flight paths. These frameworks provide vendor-dependent\nprogramming and communication interfaces that are intended for flight path\ndefinitions. However, they are often limited to outdoor and GPS-based use only.\nA key disadvantage of such a solution is that they are complicated to use and\nrequire readjustments when changing the drone model. This is time-consuming\nsince it requires redefining the flight path for the new framework. This\nworkshop paper proposes additional features for DronOS, a community-driven\nframework that enables model-independent automatisation and programming of\ndrones. We enhanced DronOS to include additional functions to account for the\nspecific design constraints in human-drone-interaction. This paper provides a\nstarting point for discussing the requirements involved in designing a drone\nsystem with other researchers within the human-drone interaction community. We\nenvision DronOS as a community-driven framework that can be applied to generic\ndrone models, hence enabling the automatisation for any commercially available\ndrone. Our goal is to build DronOS as a software tool that can be easily used\nby researchers and practitioners to prototype novel drone-based systems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:30:34 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Hoppe", "Matthias", ""], ["Wei\u00df", "Yannick", ""], ["Burger", "Marinus", ""], ["Kosch", "Thomas", ""]]}, {"id": "2010.10150", "submitter": "Wei Ping", "authors": "Sashank Santhanam, Wei Ping, Raul Puri, Mohammad Shoeybi, Mostofa\n  Patwary, Bryan Catanzaro", "title": "Local Knowledge Powered Conversational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art conversational agents have advanced significantly in\nconjunction with the use of large transformer-based language models. However,\neven with these advancements, conversational agents still lack the ability to\nproduce responses that are informative and coherent with the local context. In\nthis work, we propose a dialog framework that incorporates both local knowledge\nas well as users' past dialogues to generate high quality conversations. We\nintroduce an approach to build a dataset based on Reddit conversations, where\noutbound URL links are widely available in the conversations and the\nhyperlinked documents can be naturally included as local external knowledge.\nUsing our framework and dataset, we demonstrate that incorporating local\nknowledge can largely improve informativeness, coherency and realisticness\nmeasures using human evaluations. In particular, our approach consistently\noutperforms the state-of-the-art conversational model on the Reddit dataset\nacross all three measures. We also find that scaling the size of our models\nfrom 117M to 8.3B parameters yields consistent improvement of validation\nperplexity as well as human evaluated metrics. Our model with 8.3B parameters\ncan generate human-like responses as rated by various human evaluations in a\nsingle-turn dialog setting.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 09:34:40 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Santhanam", "Sashank", ""], ["Ping", "Wei", ""], ["Puri", "Raul", ""], ["Shoeybi", "Mohammad", ""], ["Patwary", "Mostofa", ""], ["Catanzaro", "Bryan", ""]]}, {"id": "2010.10178", "submitter": "Filippo Gabriele Prattic\\`o", "authors": "Alberto Cannav\\`o, Davide Calandra, F. Gabriele Prattic\\`o, Valentina\n  Gatteschi and Fabrizio Lamberti", "title": "An Evaluation Testbed for Locomotion in Virtual Reality", "comments": "This paper is accepted for inclusion in a future issue of IEEE\n  Transactions on Visualization and Computer Graphics (TVCG). Copyright IEEE\n  2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3032440", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common operation performed in Virtual Reality (VR) environments is\nlocomotion. Although real walking can represent a natural and intuitive way to\nmanage displacements in such environments, its use is generally limited by the\nsize of the area tracked by the VR system (typically, the size of a room) or\nrequires expensive technologies to cover particularly extended settings. A\nnumber of approaches have been proposed to enable effective explorations in VR,\neach characterized by different hardware requirements and costs, and capable to\nprovide different levels of usability and performance. However, the lack of a\nwell-defined methodology for assessing and comparing available approaches makes\nit difficult to identify, among the various alternatives, the best solutions\nfor selected application domains. To deal with this issue, this paper\nintroduces a novel evaluation testbed which, by building on the outcomes of\nmany separate works reported in the literature, aims to support a comprehensive\nanalysis of the considered design space. An experimental protocol for\ncollecting objective and subjective measures is proposed, together with a\nscoring system able to rank locomotion approaches based on a weighted set of\nrequirements. Testbed usage is illustrated in a use case requesting to select\nthe technique to adopt in a given application scenario.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 10:21:15 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Cannav\u00f2", "Alberto", ""], ["Calandra", "Davide", ""], ["Prattic\u00f2", "F. Gabriele", ""], ["Gatteschi", "Valentina", ""], ["Lamberti", "Fabrizio", ""]]}, {"id": "2010.10180", "submitter": "Xuedan Zou", "authors": "Xuedan Zou", "title": "PIXEL: Interactive Light System Design Based On Simple Gesture\n  Recognition", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this project, by utilizing the real-time human gestures captured by\nKinect, we attempted to provide a self-made interactive light system PIXEL for\ninteracting with the visitor to play a simple SNAKE game. By paralleling the\nlow power single color LED lights and lighting up them separately or together,\nwe provided a big LED board with multiple colors while at the time safe enough\nwithout using high tension electricity. We analysed the factors that influence\nthe final visual effect of the light pixel, the novel gestures that can be\nrecognized well by current computer vision algorithms and did several\nexperiments to decide the final interactive method. We also believe this\nproject provides a way to help people reconsider the relationship between the\nold and the new and the possibility to bring old things reborn by the new\ntechnologies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 10:34:24 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zou", "Xuedan", ""]]}, {"id": "2010.10584", "submitter": "Surej Mouli PhD", "authors": "Ramaswamy Palaniappan, Surej Mouli, Evangelina Fringi, Howard Bowman\n  and Ian McLoughlin", "title": "Incandescent Bulb and LED Brake Lights:Novel Analysis of Reaction Times", "comments": "10 pages, 18 figures", "journal-ref": "For a revised version and its published version refer to IEEE\n  Access journal, 2021", "doi": "10.1109/ACCESS.2021.3058579", "report-no": null, "categories": "cs.HC cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rear-end collision accounts for around 8% of all vehicle crashes in the UK,\nwith the failure to notice or react to a brake light signal being a major\ncontributory cause. Meanwhile traditional incandescent brake light bulbs on\nvehicles are increasingly being replaced by a profusion of designs featuring\nLEDs. In this paper, we investigate the efficacy of brake light design using a\nnovel approach to recording subject reaction times in a simulation setting\nusing physical brake light assemblies. The reaction times of 22 subjects were\nmeasured for ten pairs of LED and incandescent bulb brake lights. Three events\nwere investigated for each subject, namely the latency of brake light\nactivation to accelerator release (BrakeAcc), the latency of accelerator\nrelease to brake pedal depression (AccPdl), and the cumulative time from light\nactivation to brake pedal depression (BrakePdl). To our knowledge, this is the\nfirst study in which reaction times have been split into BrakeAcc and AccPdl.\nResults indicate that the two brake lights containing incandescent bulbs led to\nsignificantly slower reaction times compared to the tested eight LED lights.\nBrakeAcc results also show that experienced subjects were quicker to respond to\nthe activation of brake lights by releasing the accelerator pedal.\nInterestingly, the analysis also revealed that the type of brake light\ninfluenced the AccPdl time, although experienced subjects did not always act\nquicker than inexperienced subjects. Overall, the study found that different\ndesigns of brake light can significantly influence driver response times.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:41:52 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Palaniappan", "Ramaswamy", ""], ["Mouli", "Surej", ""], ["Fringi", "Evangelina", ""], ["Bowman", "Howard", ""], ["McLoughlin", "Ian", ""]]}, {"id": "2010.10590", "submitter": "Puru Malhotra", "authors": "Yugam Bajaj and Puru Malhotra", "title": "American Sign Language Identification Using Hand Trackpoint Analysis", "comments": "12 Pages, 6 Images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language helps people with Speaking and Hearing Disabilities communicate\nwith others efficiently. Sign Language identification is a challenging area in\nthe field of computer vision and recent developments have been able to achieve\nnear perfect results for the task, though some challenges are yet to be solved.\nIn this paper we propose a novel machine learning based pipeline for American\nSign Language identification using hand track points. We convert a hand gesture\ninto a series of hand track point coordinates that serve as an input to our\nsystem. In order to make the solution more efficient, we experimented with 28\ndifferent combinations of pre-processing techniques, each run on three\ndifferent machine learning algorithms namely k-Nearest Neighbours, Random\nForests and a Neural Network. Their performance was contrasted to determine the\nbest pre-processing scheme and algorithm pair. Our system achieved an Accuracy\nof 95.66% to identify American sign language gestures.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 19:59:16 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 21:11:27 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 21:11:21 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Bajaj", "Yugam", ""], ["Malhotra", "Puru", ""]]}, {"id": "2010.10597", "submitter": "Aditya Kalyanpur", "authors": "Clifton McFate, Aditya Kalyanpur, Dave Ferrucci, Andrea Bradshaw,\n  Ariel Diertani, David Melville, Lori Moon", "title": "SKATE: A Natural Language Interface for Encoding Structured Knowledge", "comments": "Accepted at IAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Natural Language (NL) applications, there is often a mismatch between what\nthe NL interface is capable of interpreting and what a lay user knows how to\nexpress. This work describes a novel natural language interface that reduces\nthis mismatch by refining natural language input through successive,\nautomatically generated semi-structured templates. In this paper we describe\nhow our approach, called SKATE, uses a neural semantic parser to parse NL input\nand suggest semi-structured templates, which are recursively filled to produce\nfully structured interpretations. We also show how SKATE integrates with a\nneural rule-generation model to interactively suggest and acquire commonsense\nknowledge. We provide a preliminary coverage analysis of SKATE for the task of\nstory understanding, and then describe a current business use-case of the tool\nin a specific domain: COVID-19 policy design.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 20:13:09 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 01:01:45 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["McFate", "Clifton", ""], ["Kalyanpur", "Aditya", ""], ["Ferrucci", "Dave", ""], ["Bradshaw", "Andrea", ""], ["Diertani", "Ariel", ""], ["Melville", "David", ""], ["Moon", "Lori", ""]]}, {"id": "2010.10658", "submitter": "Peter Zelchenko", "authors": "Peter Zelchenko, Xiaohan Fu, Xiangqian Li, Alex Ivanov, Zhenyu Gu", "title": "Display object alignment may influence location recall in unexpected\n  ways", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a presumption in human-computer interaction that laying out menus\nand most other material in neat rows and columns helps users get work done. The\nrule has been so implicit in the field of design as to allow for no debate.\nHowever, the idea that perfect collinearity benefits creates an advantage for\nboth either search and or recall has rarely been tested. Drawing from separate\nbranches of cognitive literature, we tested a minimal brainstorming interface\nwith either aligned or eccentrically arranged layouts on 96 college students.\nIncidental exact recall of recently worked locations improved in the eccentric\ncondition. And in both conditions there were frequent near-miss recall errors\nto neighboring aligned objects and groups of objects. Further analysis found\nonly marginal performance advantages specifically for females with the\neccentric design. However, NASA-TLX subjective measures showed that in\neccentric, females reported higher performance, less effort, and yet also\nhigher frustration; while males reported lower performance with about the same\neffort, and lower frustration.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 22:51:53 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zelchenko", "Peter", ""], ["Fu", "Xiaohan", ""], ["Li", "Xiangqian", ""], ["Ivanov", "Alex", ""], ["Gu", "Zhenyu", ""]]}, {"id": "2010.10803", "submitter": "Michael Lyons", "authors": "Alexander Refsum Jensenius, Michael J. Lyons", "title": "Trends at NIME -- Reflections on Editing \"A NIME Reader\"", "comments": "5 pages, 1 table. Proceedings of the International Conference on New\n  Interfaces for Musical Expression, 2016", "journal-ref": null, "doi": "10.5281/zenodo.1176044", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides an overview of the process of editing the forthcoming\nanthology \"A NIME Reader - Fifteen Years of New Interfaces for Musical\nExpression.\" The selection process is presented, and we reflect on some of the\ntrends we have observed in re-discovering the collection of more than 1200 NIME\npapers published throughout the 15-year long history of the conference. An\nanthology is necessarily selective, and ours is no exception. As we present in\nthis paper, the aim has been to represent the wide range of artistic,\nscientific, and technological approaches that characterize the NIME conference.\nThe anthology also includes critical discourse, and through acknowledgment of\nthe strengths and weaknesses of the NIME community, we propose activities that\ncould further diversify and strengthen the field.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 07:43:16 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Jensenius", "Alexander Refsum", ""], ["Lyons", "Michael J.", ""]]}, {"id": "2010.10967", "submitter": "Ernie Chang", "authors": "Frederik Wiehr, Anke Hirsch, Florian Daiber, Antonio Kruger, Alisa\n  Kovtunova, Stefan Borgwardt, Ernie Chang, Vera Demberg, Marcel Steinmetz,\n  Hoffmann Jorg", "title": "Safe Handover in Mixed-Initiative Control for Cyber-Physical Systems", "comments": "In Proceedings of Workshop at CHI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For mixed-initiative control between cyber-physical systems (CPS) and its\nusers, it is still an open question how machines can safely hand over control\nto humans. In this work, we propose a concept to provide technological support\nthat uses formal methods from AI -- description logic (DL) and automated\nplanning -- to predict more reliably when a hand-over is necessary, and to\nincrease the advance notice for handovers by planning ahead of runtime. We\ncombine this with methods from human-computer interaction (HCI) and natural\nlanguage generation (NLG) to develop solutions for safe and smooth handovers\nand provide an example autonomous driving scenario. A study design is proposed\nwith the assessment of qualitative feedback, cognitive load and trust in\nautomation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 12:59:32 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Wiehr", "Frederik", ""], ["Hirsch", "Anke", ""], ["Daiber", "Florian", ""], ["Kruger", "Antonio", ""], ["Kovtunova", "Alisa", ""], ["Borgwardt", "Stefan", ""], ["Chang", "Ernie", ""], ["Demberg", "Vera", ""], ["Steinmetz", "Marcel", ""], ["Jorg", "Hoffmann", ""]]}, {"id": "2010.11025", "submitter": "Ammar Malik", "authors": "Ammar Malik, Hugo Lhachemi, and Robert Shorten", "title": "I-nteract 2.0: A Cyber-Physical System to Design 3D Models using Mixed\n  Reality Technologies and Deep Learning for Additive Manufacturing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I-nteract is a cyber-physical system that enables real-time interaction with\nboth virtual and real artifacts to design 3D models for additive manufacturing\nby leveraging on mixed reality technologies. This paper presents novel advances\nin the development of the interaction platform I-nteract to generate 3D models\nusing both constructive solid geometry and artificial intelligence. The system\nalso enables the user to adjust the dimensions of the 3D models with respect to\ntheir physical workspace. The effectiveness of the system is demonstrated by\ngenerating 3D models of furniture (e.g., chairs and tables) and fitting them\ninto the physical space in a mixed reality environment.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 14:13:21 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Malik", "Ammar", ""], ["Lhachemi", "Hugo", ""], ["Shorten", "Robert", ""]]}, {"id": "2010.11046", "submitter": "Colin M. Gray", "authors": "Colin M. Gray, Jingle Chen, Shruthi Sai Chivukula, and Liyang Qu", "title": "End User Accounts of Dark Patterns as Felt Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Manipulation defines many of our experiences as a consumer, including subtle\nnudges and overt advertising campaigns that seek to gain our attention and\nmoney. With the advent of digital services that can continuously optimize\nonline experiences to favor stakeholder requirements, increasingly designers\nand developers make use of \"dark patterns\"---forms of manipulation that prey on\nhuman psychology---to encourage certain behaviors and discourage others in ways\nthat present unequal value to the end user. In this paper, we provide an\naccount of end user perceptions of manipulation that builds on and extends\nnotions of dark patterns. We report on the results of a survey of users\nconducted in English and Mandarin Chinese (n=169), including follow-up\ninterviews from nine survey respondents. We used a card sorting method to\nsupport thematic analysis of responses from each cultural context, identifying\nboth qualitatively-supported insights to describe end users' felt experiences\nof manipulative products, and a continuum of manipulation. We further support\nthis analysis through a quantitative analysis of survey results and the\npresentation of vignettes from the interviews. We conclude with implications\nfor future research, considerations for public policy, and guidance on how to\nfurther empower and give users autonomy in their experiences with digital\nservices.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 14:55:09 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Gray", "Colin M.", ""], ["Chen", "Jingle", ""], ["Chivukula", "Shruthi Sai", ""], ["Qu", "Liyang", ""]]}, {"id": "2010.11137", "submitter": "Yan Zeng", "authors": "Yan Zeng and Jian-Yun Nie", "title": "Multi-Domain Dialogue State Tracking based on State Graph", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of multi-domain Dialogue State Tracking (DST) with\nopen vocabulary, which aims to extract the state from the dialogue. Existing\napproaches usually concatenate previous dialogue state with dialogue history as\nthe input to a bi-directional Transformer encoder. They rely on the\nself-attention mechanism of Transformer to connect tokens in them. However,\nattention may be paid to spurious connections, leading to wrong inference. In\nthis paper, we propose to construct a dialogue state graph in which domains,\nslots and values from the previous dialogue state are connected properly.\nThrough training, the graph node and edge embeddings can encode co-occurrence\nrelations between domain-domain, slot-slot and domain-slot, reflecting the\nstrong transition paths in general dialogue. The state graph, encoded with\nrelational-GCN, is fused into the Transformer encoder. Experimental results\nshow that our approach achieves a new state of the art on the task while\nremaining efficient. It outperforms existing open-vocabulary DST approaches.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 16:55:18 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zeng", "Yan", ""], ["Nie", "Jian-Yun", ""]]}, {"id": "2010.11163", "submitter": "Guy Meyer", "authors": "Guy Meyer, Alan Wassyng, Mark Lawford, Kourosh Sabri, Shahram Shirani", "title": "Literature Review of Computer Tools for the Visually Impaired: a focus\n  on Search Engines", "comments": "103 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sudden reliance on the internet has resulted in the global standardization\nof specific software and interfaces tailored for the average user. Whether it\nbe web apps or dedicated software, the methods of interaction are seemingly\nsimilar. But when the computer tool is presented with unique users,\nspecifically with a disability, the quality of interaction degrades, sometimes\nto a point of complete uselessness. This roots from one's focus on the average\nuser rather than the development of a platform for all (a golden standard).\nThis paper reviews published works and products that deal with providing\naccessibility to visually impaired online users. Due to the variety of tools\nthat are available to computer users, the paper focuses on search engines as a\nprimary tool for browsing the web. By analyzing the attributes discussed below,\nthe reader is equipped with a set of references for existing applications,\nalong with practical insight and recommendations for accessible design.\nFinally, the necessary considerations for future developments and summaries of\nimportant focal points are highlighted.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 17:29:21 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Meyer", "Guy", ""], ["Wassyng", "Alan", ""], ["Lawford", "Mark", ""], ["Sabri", "Kourosh", ""], ["Shirani", "Shahram", ""]]}, {"id": "2010.11671", "submitter": "Guoliang Liu Prof. Dr.", "authors": "Hejing Ling, Guoliang Liu, Guohui Tian", "title": "Motion Planning Combines Psychological Safety and Motion Prediction for\n  a Sense Motive Robot", "comments": "submitted to RAL/ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human safety is the most important demand for human robot interaction and\ncollaboration (HRIC), which not only refers to physical safety, but also\nincludes psychological safety. Although many robots with different\nconfigurations have entered our living and working environments, the human\nsafety problem is still an ongoing research problem in human-robot coexistence\nscenarios. This paper addresses the human safety issue by covering both the\nphysical safety and psychological safety aspects. First, we introduce an\nadaptive robot velocity control and step size adjustment method according to\nhuman facial expressions, such that the robot can adjust its movement to keep\nsafety when the human emotion is unusual. Second, we predict the human motion\nby detecting the suddenly changes of human head pose and gaze direction, such\nthat the robot can infer whether the human attention is distracted, predict the\nnext move of human and rebuild a repulsive force to avoid potential collision.\nFinally, we demonstrate our idea using a 7 DOF TIAGo robot in a dynamic HRIC\nenvironment, which shows that the robot becomes sense motive, and responds to\nhuman action and emotion changes quickly and efficiently.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 04:19:53 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 15:32:08 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ling", "Hejing", ""], ["Liu", "Guoliang", ""], ["Tian", "Guohui", ""]]}, {"id": "2010.11744", "submitter": "Gareth W. Young Dr", "authors": "Gareth W. Young and David Murphy and Jeffrey Weeter", "title": "A Qualitative Analysis of Haptic Feedback in Music Focused Exercises", "comments": "6 pages", "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2017", "doi": "10.5281/zenodo.1176222", "report-no": null, "categories": "cs.HC cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the findings of a pilot-study that analysed the role of haptic\nfeedback in a musical context. To examine the role of haptics in Digital\nMusical Instrument (DMI) design an experiment was formulated to measure the\nusers' perception of device usability across four separate feedback stages:\nfully haptic (force and tactile combined), constant force only, vibrotactile\nonly, and no feedback. The study was piloted over extended periods with the\nintention of exploring the application and integration of DMIs in real-world\nmusical contexts. Applying a music orientated analysis of this type enabled the\ninvestigative process to not only take place over a comprehensive period, but\nallowed for the exploration of DMI integration in everyday compositional\npractices. As with any investigation that involves creativity, it was important\nthat the participants did not feel rushed or restricted. That is, they were\ngiven sufficient time to explore and assess the different feedback types\nwithout constraint. This provided an accurate and representational set of\nqualitative data for validating the participants' experience with the different\nfeedback types they were presented with.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:00:14 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:34:31 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Young", "Gareth W.", ""], ["Murphy", "David", ""], ["Weeter", "Jeffrey", ""]]}, {"id": "2010.11761", "submitter": "Austin Wright", "authors": "Austin P. Wright, Zijie J. Wang, Haekyu Park, Grace Guo, Fabian\n  Sperrle, Mennatallah El-Assady, Alex Endert, Daniel Keim, Duen Horng Chau", "title": "A Comparative Analysis of Industry Human-AI Interaction Guidelines", "comments": "8 pages, 3 figures, Presented at VIS2020 Workshop on TRust and\n  EXpertise in Visual Analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent release of AI interaction guidelines from Apple, Google, and\nMicrosoft, there is clearly interest in understanding the best practices in\nhuman-AI interaction. However, industry standards are not determined by a\nsingle company, but rather by the synthesis of knowledge from the whole\ncommunity. We have surveyed all of the design guidelines from each of these\nmajor companies and developed a single, unified structure of guidelines, giving\ndevelopers a centralized reference. We have then used this framework to compare\neach of the surveyed companies to find differences in areas of emphasis.\nFinally, we encourage people to contribute additional guidelines from other\ncompanies, academia, or individuals, to provide an open and extensible\nreference of AI design guidelines at\nhttps://ai-open-guidelines.readthedocs.io/.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:33:19 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Wright", "Austin P.", ""], ["Wang", "Zijie J.", ""], ["Park", "Haekyu", ""], ["Guo", "Grace", ""], ["Sperrle", "Fabian", ""], ["El-Assady", "Mennatallah", ""], ["Endert", "Alex", ""], ["Keim", "Daniel", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2010.11884", "submitter": "James Ren Hou Lee Mr", "authors": "James Ren Hou Lee, Alexander Wong", "title": "AEGIS: A real-time multimodal augmented reality computer vision based\n  system to assist facial expression recognition for individuals with autism\n  spectrum disorder", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to interpret social cues comes naturally for most people, but for\nthose living with Autism Spectrum Disorder (ASD), some experience a deficiency\nin this area. This paper presents the development of a multimodal augmented\nreality (AR) system which combines the use of computer vision and deep\nconvolutional neural networks (CNN) in order to assist individuals with the\ndetection and interpretation of facial expressions in social settings. The\nproposed system, which we call AEGIS (Augmented-reality Expression Guided\nInterpretation System), is an assistive technology deployable on a variety of\nuser devices including tablets, smartphones, video conference systems, or\nsmartglasses, showcasing its extreme flexibility and wide range of use cases,\nto allow integration into daily life with ease. Given a streaming video camera\nsource, each real-world frame is passed into AEGIS, processed for facial\nbounding boxes, and then fed into our novel deep convolutional time windowed\nneural network (TimeConvNet). We leverage both spatial and temporal information\nin order to provide an accurate expression prediction, which is then converted\ninto its corresponding visualization and drawn on top of the original video\nframe. The system runs in real-time, requires minimal set up and is simple to\nuse. With the use of AEGIS, we can assist individuals living with ASD to learn\nto better identify expressions and thus improve their social experiences.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:20:38 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Lee", "James Ren Hou", ""], ["Wong", "Alexander", ""]]}, {"id": "2010.11886", "submitter": "K L Bhanu Moorthy", "authors": "K L Bhanu Moorthy, Moneish Kumar, Ramanathan Subramaniam, Vineet\n  Gandhi", "title": "GAZED- Gaze-guided Cinematic Editing of Wide-Angle Monocular Video\n  Recordings", "comments": "10 pages", "journal-ref": "In Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems (CHI '20). Association for Computing Machinery, New York,\n  NY, USA, 1-11", "doi": "10.1145/3313831.3376544", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present GAZED- eye GAZe-guided EDiting for videos captured by a solitary,\nstatic, wide-angle and high-resolution camera. Eye-gaze has been effectively\nemployed in computational applications as a cue to capture interesting scene\ncontent; we employ gaze as a proxy to select shots for inclusion in the edited\nvideo. Given the original video, scene content and user eye-gaze tracks are\ncombined to generate an edited video comprising cinematically valid actor shots\nand shot transitions to generate an aesthetic and vivid representation of the\noriginal narrative. We model cinematic video editing as an energy minimization\nproblem over shot selection, whose constraints capture cinematographic editing\nconventions. Gazed scene locations primarily determine the shots constituting\nthe edited video. Effectiveness of GAZED against multiple competing methods is\ndemonstrated via a psychophysical study involving 12 users and twelve\nperformance videos.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:27:03 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Moorthy", "K L Bhanu", ""], ["Kumar", "Moneish", ""], ["Subramaniam", "Ramanathan", ""], ["Gandhi", "Vineet", ""]]}, {"id": "2010.11897", "submitter": "Shehzad Afzal", "authors": "Shehzad Afzal, Sohaib Ghani, Hank C. Jenkins-Smith, David S. Ebert,\n  Markus Hadwiger, Ibrahim Hoteit", "title": "A Visual Analytics Based Decision Making Environment for COVID-19\n  Modeling and Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public health officials dealing with pandemics like COVID-19 have to evaluate\nand prepare response plans. This planning phase requires not only looking into\nthe spatiotemporal dynamics and impact of the pandemic using simulation models,\nbut they also need to plan and ensure the availability of resources under\ndifferent spread scenarios. To this end, we have developed a visual analytics\nenvironment that enables public health officials to model, simulate, and\nexplore the spread of COVID-19 by supplying county-level information such as\npopulation, demographics, and hospital beds. This environment facilitates users\nto explore spatiotemporal model simulation data relevant to COVID-19 through a\ngeospatial map with linked statistical views, apply different decision measures\nat different points in time, and understand their potential impact. Users can\ndrill-down to county-level details such as the number of sicknesses, deaths,\nneeds for hospitalization, and variations in these statistics over time. We\ndemonstrate the usefulness of this environment through a use case study and\nalso provide feedback from domain experts. We also provide details about future\nextensions and potential applications of this work.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 17:33:56 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Afzal", "Shehzad", ""], ["Ghani", "Sohaib", ""], ["Jenkins-Smith", "Hank C.", ""], ["Ebert", "David S.", ""], ["Hadwiger", "Markus", ""], ["Hoteit", "Ibrahim", ""]]}, {"id": "2010.11975", "submitter": "Anamaria Crisan", "authors": "Anamaria Crisan, Shannah Fisher, Jennifer L. Gardy, Tamara Munzner", "title": "GEViTRec: Data Reconnaissance Through Recommendation Using a\n  Domain-Specific Prevalence Visualization Design Space", "comments": "16 pages; 5 figures; Submitted to TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic Epidemiology (genEpi) is a branch of public health that uses many\ndifferent data types including tabular, network, genomic, and geographic, to\nidentify and contain outbreaks of deadly diseases. Due to the volume and\nvariety of data, it is challenging for genEpi domain experts to conduct data\nreconnaissance; that is, have an overview of the data they have and make\nassessments toward its quality, completeness, and suitability. We present an\nalgorithm for data reconnaissance through automatic visualization\nrecommendation, GEViTRec. Our approach handles a broad variety of dataset types\nand automatically generates coordinated combinations of charts, in contrast to\nexisting systems that primarily focus on singleton visual encodings of tabular\ndatasets. We automatically detect linkages across multiple input datasets by\nanalyzing non-numeric attribute fields, creating an entity graph within which\nwe analyze and rank paths. For each high-ranking path, we specify chart\ncombinations with spatial and color alignments between shared fields, using a\ngradual binding approach to transform initial partial specifications of\nsingleton charts to complete specifications that are aligned and oriented\nconsistently. A novel aspect of our approach is its combination of\ndomain-agnostic elements with domain-specific information that is captured\nthrough a domain-specific visualization prevalence design space. Our\nimplementation is applied to both synthetic data and real data from an Ebola\noutbreak. We compare GEViTRec's output to what previous visualization\nrecommendation systems would generate, and to manually crafted visualizations\nused by practitioners. We conducted formative evaluations with ten genEpi\nexperts to assess the relevance and interpretability of our results.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 18:21:40 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Crisan", "Anamaria", ""], ["Fisher", "Shannah", ""], ["Gardy", "Jennifer L.", ""], ["Munzner", "Tamara", ""]]}, {"id": "2010.11997", "submitter": "Zhanwen Chen", "authors": "Zhanwen Chen, Shiyao Li, Roxanne Rashedi, Xiaoman Zi, Morgan\n  Elrod-Erickson, Bryan Hollis, Angela Maliakal, Xinyu Shen, Simeng Zhao,\n  Maithilee Kunda", "title": "Characterizing Datasets for Social Visual Question Answering, and the\n  New TinySocial Dataset", "comments": "To appear in the Joint IEEE International Conference on Development\n  and Learning and on Epigenetic Robotics (ICDL), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CV cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern social intelligence includes the ability to watch videos and answer\nquestions about social and theory-of-mind-related content, e.g., for a scene in\nHarry Potter, \"Is the father really upset about the boys flying the car?\"\nSocial visual question answering (social VQA) is emerging as a valuable\nmethodology for studying social reasoning in both humans (e.g., children with\nautism) and AI agents. However, this problem space spans enormous variations in\nboth videos and questions. We discuss methods for creating and characterizing\nsocial VQA datasets, including 1) crowdsourcing versus in-house authoring,\nincluding sample comparisons of two new datasets that we created\n(TinySocial-Crowd and TinySocial-InHouse) and the previously existing Social-IQ\ndataset; 2) a new rubric for characterizing the difficulty and content of a\ngiven video; and 3) a new rubric for characterizing question types. We close by\ndescribing how having well-characterized social VQA datasets will enhance the\nexplainability of AI agents and can also inform assessments and educational\ninterventions for people.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 03:20:23 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chen", "Zhanwen", ""], ["Li", "Shiyao", ""], ["Rashedi", "Roxanne", ""], ["Zi", "Xiaoman", ""], ["Elrod-Erickson", "Morgan", ""], ["Hollis", "Bryan", ""], ["Maliakal", "Angela", ""], ["Shen", "Xinyu", ""], ["Zhao", "Simeng", ""], ["Kunda", "Maithilee", ""]]}, {"id": "2010.12012", "submitter": "Roghayeh Barmaki", "authors": "Zang Guo and Roghayeh Barmaki", "title": "Deep neural networks for collaborative learning analytics: Evaluating\n  team collaborations using student gaze point prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic assessment and evaluation of team performance during collaborative\ntasks is key to the learning analytics and computer-supported cooperative work\nresearch. There is a growing interest in the use of gaze-oriented cues for\nevaluating the collaboration and cooperativeness of teams. However, collecting\ngaze data using eye-trackers is not always feasible due to time and cost\nconstraints. In this paper, we introduce an automated team assessment tool\nbased on gaze points and joint visual attention (JVA) information extracted by\ncomputer vision solutions. We then evaluate team collaborations in an\nundergraduate anatomy learning activity (N=60, 30 teams) as a test user-study.\nThe results indicate that higher JVA was positively associated with student\nlearning outcomes (r(30)=0.50,p<0.005). Moreover, teams who participated in two\nexperimental groups, and used interactive 3-D anatomy models, had higher JVA\n(F(1,28)=6.65,p<0.05) and better knowledge retention (F(1,28) =7.56,p<0.05)\nthan those in the control group. Also, no significant difference was observed\nbased on JVA for different gender compositions of teams. The findings from this\nwork offer implications in learning sciences and collaborative computing by\nproviding a novel mutual attention-based measure to objectively evaluate team\ncollaboration dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 02:07:29 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Guo", "Zang", ""], ["Barmaki", "Roghayeh", ""]]}, {"id": "2010.12022", "submitter": "Adel Al-Dawood", "authors": "Adel Al-Dawood, Serene Alhajhussein, Svetlana Yarosh", "title": "Saudi Arabian Parents' Perception of Online Marital Matchmaking\n  Technologies", "comments": "31 pages, CSCW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a date or a spouse online is usually considered an individualistic\nendeavor in Western cultures. This presents a challenge for collectivist\nnon-Western cultures such as Saudi Arabia where choosing a spouse is viewed as\na union of two families with parents of both spouses being heavily involved.\nOur work aims to investigate how Saudi Arabian parents view the utilization of\ntechnology by their young adults to seek potential spouses online. We report\nour findings of interviews conducted with 16 Saudi Arabian parents (8 fathers,\n6 mothers and 1 couple). We generate qualitative themes that provide insights\nabout how parents wanted to preserve their values, integrate technology into\nthe traditional process and protect their young adults from potential harms.\nThese themes lead to implications for designing suitable marital matchmaking\ntechnologies in Saudi Arabia and opportunities for future work.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 18:35:22 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Al-Dawood", "Adel", ""], ["Alhajhussein", "Serene", ""], ["Yarosh", "Svetlana", ""]]}, {"id": "2010.12066", "submitter": "Parisa Ghane", "authors": "Parisa Ghane and Gahangir Hossain", "title": "Learning Patterns in Imaginary Vowels for an Intelligent Brain Computer\n  Interface (BCI) Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology advancements made it easy to measure non-invasive and high-quality\nelectroencephalograph (EEG) signals from human's brain. Hence, development of\nrobust and high-performance AI algorithms becomes crucial to properly process\nthe EEG signals and recognize the patterns, which lead to an appropriate\ncontrol signal. Despite the advancements in processing the motor imagery EEG\nsignals, the healthcare applications, such as emotion detection, are still in\nthe early stages of AI design. In this paper, we propose a modular framework\nfor the recognition of vowels as the AI part of a brain computer interface\nsystem. We carefully designed the modules to discriminate the English vowels\ngiven the raw EEG signals, and meanwhile avoid the typical issued with the\ndata-poor environments like most of the healthcare applications. The proposed\nframework consists of appropriate signal segmentation, filtering, extraction of\nspectral features, reducing the dimensions by means of principle component\nanalysis, and finally a multi-class classification by decision-tree-based\nsupport vector machine (DT-SVM). The performance of our framework was evaluated\nby a combination of test-set and resubstitution (also known as apparent) error\nrates. We provide the algorithms of the proposed framework to make it easy for\nfuture researchers and developers who want to follow the same workflow.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 06:10:10 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ghane", "Parisa", ""], ["Hossain", "Gahangir", ""]]}, {"id": "2010.12078", "submitter": "Anindya Maiti", "authors": "Mohd Sabra, Anindya Maiti, Murtuza Jadliwala", "title": "Zoom on the Keystrokes: Exploiting Video Calls for Keystroke Inference\n  Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to recent world events, video calls have become the new norm for both\npersonal and professional remote communication. However, if a participant in a\nvideo call is not careful, he/she can reveal his/her private information to\nothers in the call. In this paper, we design and evaluate an attack framework\nto infer one type of such private information from the video stream of a call\n-- keystrokes, i.e., text typed during the call. We evaluate our video-based\nkeystroke inference framework using different experimental settings and\nparameters, including different webcams, video resolutions, keyboards,\nclothing, and backgrounds. Our relatively high keystroke inference accuracies\nunder commonly occurring and realistic settings highlight the need for\nawareness and countermeasures against such attacks. Consequently, we also\npropose and evaluate effective mitigation techniques that can automatically\nprotect users when they type during a video call.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 21:38:17 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Sabra", "Mohd", ""], ["Maiti", "Anindya", ""], ["Jadliwala", "Murtuza", ""]]}, {"id": "2010.12324", "submitter": "Janet Rafner", "authors": "Janet Rafner, Lotte Philipsen, Sebastian Risi, Joel Simon, Jacob\n  Sherson", "title": "The power of pictures: using ML assisted image generation to engage the\n  crowd in complex socioscientific problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-computer image generation using Generative Adversarial Networks (GANs)\nis becoming a well-established methodology for casual entertainment and open\nartistic exploration. Here, we take the interaction a step further by weaving\nin carefully structured design elements to transform the activity of\nML-assisted imaged generation into a catalyst for large-scale popular dialogue\non complex socioscientific problems such as the United Nations Sustainable\nDevelopment Goals (SDGs) and as a gateway for public participation in research.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 12:09:53 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 16:31:36 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Rafner", "Janet", ""], ["Philipsen", "Lotte", ""], ["Risi", "Sebastian", ""], ["Simon", "Joel", ""], ["Sherson", "Jacob", ""]]}, {"id": "2010.12570", "submitter": "Efe Bozkir", "authors": "Efe Bozkir, Shahram Eivazi, Mete Akg\\\"un, Enkelejda Kasneci", "title": "Eye Tracking Data Collection Protocol for VR for Remotely Located\n  Subjects using Blockchain and Smart Contracts", "comments": "2020 IEEE International Conference on Artificial Intelligence and\n  Virtual Reality (AIVR). Authors' copy, refer to the doi for more information", "journal-ref": null, "doi": "10.1109/AIVR50618.2020.00083", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye tracking data collection in the virtual reality context is typically\ncarried out in laboratory settings, which usually limits the number of\nparticipants or consumes at least several months of research time. In addition,\nunder laboratory settings, subjects may not behave naturally due to being\nrecorded in an uncomfortable environment. In this work, we propose a\nproof-of-concept eye tracking data collection protocol and its implementation\nto collect eye tracking data from remotely located subjects, particularly for\nvirtual reality using Ethereum blockchain and smart contracts. With the\nproposed protocol, data collectors can collect high quality eye tracking data\nfrom a large number of human subjects with heterogeneous socio-demographic\ncharacteristics. The quality and the amount of data can be helpful for various\ntasks in data-driven human-computer interaction and artificial intelligence.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:54:38 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 11:28:56 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 12:51:12 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bozkir", "Efe", ""], ["Eivazi", "Shahram", ""], ["Akg\u00fcn", "Mete", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2010.12571", "submitter": "Keith Burghardt", "authors": "Keith Burghardt, Tad Hogg, Raissa M. D'Souza, Kristina Lerman, Marton\n  Posfai", "title": "Origins of Algorithmic Instabilities in Crowdsourced Ranking", "comments": "12 pages, 20 figures", "journal-ref": "Proc. ACM Hum.-Comput. Interact., Vol. 4, No. CSCW2, Article 166.\n  2020", "doi": "10.1145/3415237", "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing systems aggregate decisions of many people to help users\nquickly identify high-quality options, such as the best answers to questions or\ninteresting news stories. A long-standing issue in crowdsourcing is how option\nquality and human judgement heuristics interact to affect collective outcomes,\nsuch as the perceived popularity of options. We address this limitation by\nconducting a controlled experiment where subjects choose between two ranked\noptions whose quality can be independently varied. We use this data to\nconstruct a model that quantifies how judgement heuristics and option quality\ncombine when deciding between two options. The model reveals popularity-ranking\ncan be unstable: unless the quality difference between the two options is\nsufficiently high, the higher quality option is not guaranteed to be eventually\nranked on top. To rectify this instability, we create an algorithm that\naccounts for judgement heuristics to infer the best option and rank it first.\nThis algorithm is guaranteed to be optimal if data matches the model. When the\ndata does not match the model, however, simulations show that in practice this\nalgorithm performs better or at least as well as popularity-based and\nrecency-based ranking for any two-choice question. Our work suggests that\nalgorithms relying on inference of mathematical models of user behavior can\nsubstantially improve outcomes in crowdsourcing systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:55:08 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Burghardt", "Keith", ""], ["Hogg", "Tad", ""], ["D'Souza", "Raissa M.", ""], ["Lerman", "Kristina", ""], ["Posfai", "Marton", ""]]}, {"id": "2010.12606", "submitter": "Roland Zimmermann", "authors": "Judy Borowski, Roland S. Zimmermann, Judith Schepers, Robert Geirhos,\n  Thomas S. A. Wallis, Matthias Bethge, Wieland Brendel", "title": "Exemplary Natural Images Explain CNN Activations Better than\n  State-of-the-Art Feature Visualization", "comments": "Published at ICLR 2021. Joint first and last authors. Code is\n  available at https://bethgelab.github.io/testing_visualizations/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature visualizations such as synthetic maximally activating images are a\nwidely used explanation method to better understand the information processing\nof convolutional neural networks (CNNs). At the same time, there are concerns\nthat these visualizations might not accurately represent CNNs' inner workings.\nHere, we measure how much extremely activating images help humans to predict\nCNN activations. Using a well-controlled psychophysical paradigm, we compare\nthe informativeness of synthetic images by Olah et al. (2017) with a simple\nbaseline visualization, namely exemplary natural images that also strongly\nactivate a specific feature map. Given either synthetic or natural reference\nimages, human participants choose which of two query images leads to strong\npositive activation. The experiments are designed to maximize participants'\nperformance, and are the first to probe intermediate instead of final layer\nrepresentations. We find that synthetic images indeed provide helpful\ninformation about feature map activations ($82\\pm4\\%$ accuracy; chance would be\n$50\\%$). However, natural images - originally intended as a baseline -\noutperform synthetic images by a wide margin ($92\\pm2\\%$). Additionally,\nparticipants are faster and more confident for natural images, whereas\nsubjective impressions about the interpretability of the feature visualizations\nare mixed. The higher informativeness of natural images holds across most\nlayers, for both expert and lay participants as well as for hand- and\nrandomly-picked feature visualizations. Even if only a single reference image\nis given, synthetic images provide less information than natural images\n($65\\pm5\\%$ vs. $73\\pm4\\%$). In summary, synthetic images from a popular\nfeature visualization method are significantly less informative for assessing\nCNN activations than natural images. We argue that visualization methods should\nimprove over this baseline.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 18:31:13 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 10:00:47 GMT"}, {"version": "v3", "created": "Sun, 2 May 2021 19:27:06 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Borowski", "Judy", ""], ["Zimmermann", "Roland S.", ""], ["Schepers", "Judith", ""], ["Geirhos", "Robert", ""], ["Wallis", "Thomas S. A.", ""], ["Bethge", "Matthias", ""], ["Brendel", "Wieland", ""]]}, {"id": "2010.12669", "submitter": "Partha Roy Dr.", "authors": "Prasun Roy and Saumik Bhattacharya and Partha Pratim Roy and Umapada\n  Pal", "title": "Position and Rotation Invariant Sign Language Recognition from 3D Point\n  Cloud Data with Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign language is a gesture based symbolic communication medium among speech\nand hearing impaired people. It also serves as a communication bridge between\nnon-impaired population and impaired population. Unfortunately, in most\nsituations a non-impaired person is not well conversant in such symbolic\nlanguages which restricts natural information flow between these two categories\nof population. Therefore, an automated translation mechanism can be greatly\nuseful that can seamlessly translate sign language into natural language. In\nthis paper, we attempt to perform recognition on 30 basic Indian sign gestures.\nGestures are represented as temporal sequences of 3D depth maps each consisting\nof 3D coordinates of 20 body joints. A recurrent neural network (RNN) is\nemployed as classifier. To improve performance of the classifier, we use\ngeometric transformation for alignment correction of depth frames. In our\nexperiments the model achieves 84.81% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 21:07:40 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Roy", "Prasun", ""], ["Bhattacharya", "Saumik", ""], ["Roy", "Partha Pratim", ""], ["Pal", "Umapada", ""]]}, {"id": "2010.12695", "submitter": "Leif Andersen", "authors": "Leif Andersen, Michael Ballantyne, Matthias Felleisen", "title": "Adding Interactive Visual Syntax to Textual Code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many programming problems call for turning geometrical thoughts into code:\ntables, hierarchical structures, nests of objects, trees, forests, graphs, and\nso on. Linear text does not do justice to such thoughts. But, it has been the\ndominant programming medium for the past and will remain so for the foreseeable\nfuture.\n  This paper proposes a novel mechanism for conveniently extending textual\nprogramming languages with problem-specific visual syntax. It argues the\nnecessity of this language feature, demonstrates the feasibility with a robust\nprototype, and sketches a design plan for adapting the idea to other languages.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 22:42:54 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Andersen", "Leif", ""], ["Ballantyne", "Michael", ""], ["Felleisen", "Matthias", ""]]}, {"id": "2010.13035", "submitter": "Michael Lyons", "authors": "Tomohiro Tokunaga, Michael J. Lyons", "title": "Enactive Mandala: Audio-visualizing Brain Waves", "comments": "2 pages, 2 figures", "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2013", "doi": "10.5281/zenodo.1178678", "report-no": null, "categories": "cs.HC cs.MM cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are exploring the design and implementation of artificial expressions,\nkinetic audio-visual representations of real-time physiological data that\nreflect emotional and cognitive state. In this work, we demonstrate a\nprototype, the Enactive Mandala, which maps real-time EEG signals to modulate\nambient music and animated visual music. Transparent real-time audio-visual\nfeedback of brainwave qualities supports intuitive insight into the connection\nbetween thoughts and physiological states.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 04:48:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Tokunaga", "Tomohiro", ""], ["Lyons", "Michael J.", ""]]}, {"id": "2010.13080", "submitter": "Hanlu Wu", "authors": "Hanlu Wu, Tengfei Ma, Lingfei Wu, Shouling Ji", "title": "Exploiting Heterogeneous Graph Neural Networks with Latent Worker/Task\n  Correlation Information for Label Aggregation in Crowdsourcing", "comments": "Accepted by TKDD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing has attracted much attention for its convenience to collect\nlabels from non-expert workers instead of experts. However, due to the high\nlevel of noise from the non-experts, an aggregation model that learns the true\nlabel by incorporating the source credibility is required. In this paper, we\npropose a novel framework based on graph neural networks for aggregating crowd\nlabels. We construct a heterogeneous graph between workers and tasks and derive\na new graph neural network to learn the representations of nodes and the true\nlabels. Besides, we exploit the unknown latent interaction between the same\ntype of nodes (workers or tasks) by adding a homogeneous attention layer in the\ngraph neural networks. Experimental results on 13 real-world datasets show\nsuperior performance over state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 10:12:37 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 15:45:26 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Wu", "Hanlu", ""], ["Ma", "Tengfei", ""], ["Wu", "Lingfei", ""], ["Ji", "Shouling", ""]]}, {"id": "2010.13111", "submitter": "Mohammad Ali", "authors": "Mohammad Ali", "title": "Develop Health Monitoring and Management System to Track Health\n  Condition and Nutrient Balance for School Students", "comments": "10 Pages, 3 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health Monitoring and Management System (HMMS) is an emerging technology for\ndecades. Researchers are working on this field to track health conditions for\ndifferent users. Researchers emphasize tracking health conditions from an early\nstage to the human body. Therefore, different research works have been\nconducted to establish HMMS in schools. Researchers propose different\nframeworks and technologies for their HMMS to check student's health condition.\nIn this paper, we introduce a complete and scalable HMMS to track health\nconditions and nutrient balance for students from primary school. We define\nprocedures step by step to establish a robust HMMS where big data methodologies\ncan be used for further prediction for diseases.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 13:08:42 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ali", "Mohammad", ""]]}, {"id": "2010.13197", "submitter": "Sriram Krishna", "authors": "Sriram Krishna, Nishant Sinha", "title": "Gestop : Customizable Gesture Control of Computer Systems", "comments": "5 pages, 5 figures, to appear in the proceedings of the 8th ACM IKDD\n  CODS and 26th COMAD (CODS-COMAD '21)", "journal-ref": null, "doi": "10.1145/3430984.3430993", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The established way of interfacing with most computer systems is a mouse and\nkeyboard. Hand gestures are an intuitive and effective touchless way to\ninteract with computer systems. However, hand gesture based systems have seen\nlow adoption among end-users primarily due to numerous technical hurdles in\ndetecting in-air gestures accurately. This paper presents Gestop, a framework\ndeveloped to bridge this gap. The framework learns to detect gestures from\ndemonstrations, is customizable by end-users and enables users to interact in\nreal-time with computers having only RGB cameras, using gestures.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 19:13:01 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Krishna", "Sriram", ""], ["Sinha", "Nishant", ""]]}, {"id": "2010.13288", "submitter": "Hao Wang", "authors": "Hao Wang, Gonzague Henri, Chin-Woo Tan, Ram Rajagopal", "title": "Activity Detection And Modeling Using Smart Meter Data: Concept And Case\n  Studies", "comments": "2020 IEEE Power & Energy Society General Meeting", "journal-ref": null, "doi": "10.1109/PESGM41954.2020.9281746", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electricity consumed by residential consumers counts for a significant part\nof global electricity consumption and utility companies can collect\nhigh-resolution load data thanks to the widely deployed advanced metering\ninfrastructure. There has been a growing research interest toward appliance\nload disaggregation via nonintrusive load monitoring. As the electricity\nconsumption of appliances is directly associated with the activities of\nconsumers, this paper proposes a new and more effective approach, i.e.,\nactivity disaggregation. We present the concept of activity disaggregation and\ndiscuss its advantage over traditional appliance load disaggregation. We\ndevelop a framework by leverage machine learning for activity detection based\non residential load data and features. We show through numerical case studies\nto demonstrate the effectiveness of the activity detection method and analyze\nconsumer behaviors by time-dependent activity modeling. Last but not least, we\ndiscuss some potential use cases that can benefit from activity disaggregation\nand some future research directions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 02:36:35 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 12:53:33 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Wang", "Hao", ""], ["Henri", "Gonzague", ""], ["Tan", "Chin-Woo", ""], ["Rajagopal", "Ram", ""]]}, {"id": "2010.13352", "submitter": "Zhaopeng Xing", "authors": "Zhaopeng Xing, Xiaojun (Jenny) Yuan, Lisa Vizer", "title": "The Age-related Differences in Web Information Search Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Older adults' need for quality health information has never been more\ncritical as during the COVID-19 pandemic. Yet, they are susceptible to the\nwide-spread misinformation disseminated through search engines and social\nmedia. To build a search-related behavioral profile of older adults, this\narticle surveys the empirical research on age-related differences in query\nformulation, search strategies, information evaluation, and susceptibility to\nmisinformation effects. It also decomposes the mechanisms (i.e., cognitive\nchanges, development goal shift) and moderators (i.e., search task and\ninterface design) of such differences. To inform the design of information\nsystems to improve older adults' information search experience, we discuss\nopportunities for future research.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 05:37:18 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Xing", "Zhaopeng", "", "Jenny"], ["Xiaojun", "", "", "Jenny"], ["Yuan", "", ""], ["Vizer", "Lisa", ""]]}, {"id": "2010.13681", "submitter": "Jonathan Mace", "authors": "Vaastav Anand, Matheus Stolet, Thomas Davidson, Ivan Beschastnikh,\n  Tamara Munzner, and Jonathan Mace", "title": "Aggregate-Driven Trace Visualizations for Performance Debugging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance issues in cloud systems are hard to debug. Distributed tracing is\na widely adopted approach that gives engineers visibility into cloud systems.\nExisting trace analysis approaches focus on debugging single request\ncorrectness issues but not debugging single request performance issues.\nDiagnosing a performance issue in a given request requires comparing the\nperformance of the offending request with the aggregate performance of typical\nrequests. Effective and efficient debugging of such issues faces three\nchallenges: (i) identifying the correct aggregate data for diagnosis; (ii)\nvisualizing the aggregated data; and (iii) efficiently collecting, storing, and\nprocessing trace data.\n  We present TraVista, a tool designed for debugging performance issues in a\nsingle trace that addresses these challenges. TraVista extends the popular\nsingle trace Gantt chart visualization with three types of aggregate data -\nmetric, temporal, and structure data, to contextualize the performance of the\noffending trace across all traces.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 15:59:02 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Anand", "Vaastav", ""], ["Stolet", "Matheus", ""], ["Davidson", "Thomas", ""], ["Beschastnikh", "Ivan", ""], ["Munzner", "Tamara", ""], ["Mace", "Jonathan", ""]]}, {"id": "2010.13714", "submitter": "Aitik Gupta", "authors": "Aitik Gupta, Aadit Agarwal", "title": "ActiveNet: A computer-vision based approach to determine lethargy", "comments": "Accepted at The ACM India Joint International Conference on Data\n  Science and Management of Data (CoDS-COMAD) 2021", "journal-ref": null, "doi": "10.1145/3430984.3430986", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of COVID-19 has forced everyone to stay indoors, fabricating a\nsignificant drop in physical activeness. Our work is constructed upon the idea\nto formulate a backbone mechanism, to detect levels of activeness in real-time,\nusing a single monocular image of a target person. The scope can be generalized\nunder many applications, be it in an interview, online classes, security\nsurveillance, et cetera. We propose a Computer Vision based multi-stage\napproach, wherein the pose of a person is first detected, encoded with a novel\napproach, and then assessed by a classical machine learning algorithm to\ndetermine the level of activeness. An alerting system is wrapped around the\napproach to provide a solution to inhibit lethargy by sending notification\nalerts to individuals involved.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:54:03 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gupta", "Aitik", ""], ["Agarwal", "Aadit", ""]]}, {"id": "2010.13779", "submitter": "Kexin Yang", "authors": "Kexin Yang, Xiaofei Zhou, Iulian Radu", "title": "XR-Ed Framework: Designing Instruction-driven andLearner-centered\n  Extended Reality Systems for Education", "comments": "In Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the HCI community has seen an increased interest in applying\nVirtual Reality (VR), AugmentedReality (AR) and Mixed Reality (MR) into\neducational settings. Despite many literature reviews, there stilllacks a clear\nframework that reveals the different design dimensions in educational Extended\nReality (XR)systems. Addressing this gap, we synthesize a broad range of\neducational XR to propose the XR-Ed framework,which reveals design space in six\ndimensions (Physical Accessibility, Scenario, Social Interactivity,\nAgency,Virtuality Degree, Assessment). Within each dimension, we contextualize\nthe framework using existing designcases. Based on the XR-Ed Design framework,\nwe incorporated instructional design approaches to proposeXR-Ins, an\ninstruction-oriented, step-by-step guideline in educational XR instruction\ndesign. Jointly, they aimto support practitioners by revealing implicit design\nchoices, offering design inspirations as well as guide themto design\ninstructional activities for XR technologies in a more instruction-oriented and\nlearner-centered way.\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 03:18:05 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Yang", "Kexin", ""], ["Zhou", "Xiaofei", ""], ["Radu", "Iulian", ""]]}, {"id": "2010.13830", "submitter": "Shelly Bagchi", "authors": "Shelly Bagchi, Jason R. Wilson, Muneeb I. Ahmad, Christian Dondrup,\n  Zhao Han, Justin W. Hart, Matteo Leonetti, Katrin Lohan, Ross Mead, Emmanuel\n  Senft, Jivko Sinapov, Megan L. Zimmerman", "title": "Proceedings of the AI-HRI Symposium at AAAI-FSS 2020", "comments": "Symposium proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Artificial Intelligence (AI) for Human-Robot Interaction (HRI) Symposium\nhas been a successful venue of discussion and collaboration since 2014. In that\ntime, the related topic of trust in robotics has been rapidly growing, with\nmajor research efforts at universities and laboratories across the world.\nIndeed, many of the past participants in AI-HRI have been or are now involved\nwith research into trust in HRI. While trust has no consensus definition, it is\nregularly associated with predictability, reliability, inciting confidence, and\nmeeting expectations. Furthermore, it is generally believed that trust is\ncrucial for adoption of both AI and robotics, particularly when transitioning\ntechnologies from the lab to industrial, social, and consumer applications.\nHowever, how does trust apply to the specific situations we encounter in the\nAI-HRI sphere? Is the notion of trust in AI the same as that in HRI? We see a\ngrowing need for research that lives directly at the intersection of AI and HRI\nthat is serviced by this symposium. Over the course of the two-day meeting, we\npropose to create a collaborative forum for discussion of current efforts in\ntrust for AI-HRI, with a sub-session focused on the related topic of\nexplainable AI (XAI) for HRI.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 18:32:24 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 02:34:58 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 14:22:06 GMT"}, {"version": "v4", "created": "Mon, 14 Dec 2020 19:15:24 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Bagchi", "Shelly", ""], ["Wilson", "Jason R.", ""], ["Ahmad", "Muneeb I.", ""], ["Dondrup", "Christian", ""], ["Han", "Zhao", ""], ["Hart", "Justin W.", ""], ["Leonetti", "Matteo", ""], ["Lohan", "Katrin", ""], ["Mead", "Ross", ""], ["Senft", "Emmanuel", ""], ["Sinapov", "Jivko", ""], ["Zimmerman", "Megan L.", ""]]}, {"id": "2010.13901", "submitter": "Louise Hanna Miss", "authors": "Louise Hanna, David Barr, Helen Hou and Shauna McGill", "title": "An investigation of Modern Foreign Language (MFL) teachers and their\n  cognitions of Computer Assisted Language Learning (CALL) amid the COVID-19\n  health pandemic", "comments": "International Conference on Big Data, IOT and Blockchain (BIBC 2020)\n  October 24-25, 2020, Dubai, UAE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A study was performed with 33 Modern Foreign Language (MFL) teachers to\nafford insight into how classroom practitioners interact with Computer Assisted\nLanguage Learning (CALL) in Second Language (L2) pedagogy. A questionnaire with\nCALL specific statements was completed by MFL teachers who were recruited via\nUK based Facebook groups. Significantly, participants acknowledged a gap in\npractice from the expectation of CALL in the MFL classroom. Overall,\nrespondents were shown to be interested and regular consumers of CALL who\nperceived its ease and importance in L2 teaching and learning.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:11:42 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Hanna", "Louise", ""], ["Barr", "David", ""], ["Hou", "Helen", ""], ["McGill", "Shauna", ""]]}, {"id": "2010.14069", "submitter": "Melanie Bancilhon", "authors": "Melanie Bancilhon, Zhengliang Liu, Alvitta Ottley", "title": "Let's Gamble: How a Poor Visualization Can Elicit Risky Behavior", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.09725", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualizations are standard tools for assessing and communicating risks.\nHowever, it is not always clear which designs are optimal or how encoding\nchoices might influence risk perception and decision-making. In this paper, we\nreport the findings of a large-scale gambling game that immersed participants\nin an environment where their actions impacted their bonuses. Participants\nchose to either enter a lottery or receive guaranteed monetary gains based on\nfive common visualization designs. By measuring risk perception and observing\ndecision-making, we showed that icon arrays tended to elicit economically sound\nbehavior. We also found that people were more likely to gamble when presented\narea proportioned triangle and circle designs. Using our results, we model risk\nperception and discuss how our findings can improve visualization selection.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 08:39:33 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bancilhon", "Melanie", ""], ["Liu", "Zhengliang", ""], ["Ottley", "Alvitta", ""]]}, {"id": "2010.14228", "submitter": "Michael Lyons", "authors": "Ivan Poupyrev, Michael J. Lyons, Sidney Fels, Tina Blaine (Bean)", "title": "New interfaces for musical expression", "comments": "2 pages, This item describes the CHI'01 workshop which started the\n  International Conference on New Interfaces for Musical Expression", "journal-ref": "ACM CHI'01 Extended Abstracts on Human Factors in Computing\n  Systems, March 2001 Pages 491-492", "doi": "10.1145/634067.634348", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid evolution of electronics, digital media, advanced materials, and\nother areas of technology, is opening up unprecedented opportunities for\nmusical interface inventors and designers. The possibilities afforded by these\nnew technologies carry with them the challenges of a complex and often\nconfusing array of choices for musical composers and performers. New musical\ntechnologies are at least partly responsible for the current explosion of new\nmusical forms, some of which are controversial and challenge traditional\ndefinitions of music. Alternative musical controllers, currently the leading\nedge of the ongoing dialogue between technology and musical culture, involve\nmany of the issues covered at past CHI meetings. This workshop brings together\ninterface experts interested in musical controllers and musicians and composers\ninvolved in the development of new musical interfaces.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 11:59:44 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Poupyrev", "Ivan", "", "Bean"], ["Lyons", "Michael J.", "", "Bean"], ["Fels", "Sidney", "", "Bean"], ["Blaine", "Tina", "", "Bean"]]}, {"id": "2010.14245", "submitter": "Christine Utz", "authors": "Christine Utz, Steffen Becker, Theodor Schnitzler, Florian M. Farke,\n  Franziska Herbert, Leonie Schaewitz, Martin Degeling, Markus D\\\"urmuth", "title": "Apps Against the Spread: Privacy Implications and User Acceptance of\n  COVID-19-Related Smartphone Apps on Three Continents", "comments": "22 pages, 1 figure, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has fueled the development of smartphone applications\nto assist disease management. Many \"corona apps\" require widespread adoption to\nbe effective, which has sparked public debates about the privacy, security, and\nsocietal implications of government-backed health applications. We conducted a\nrepresentative online study in Germany (n = 1,003), the US (n = 1,003), and\nChina (n = 1,019) to investigate user acceptance of corona apps, using a\nvignette design based on the contextual integrity framework. We explored apps\nfor contact tracing, symptom checks, quarantine enforcement, health\ncertificates, and mere information. Our results provide insights into data\nprocessing practices that foster adoption and reveal significant differences\nbetween countries, with user acceptance being highest in China and lowest in\nthe US. Chinese participants prefer the collection of personalized data, while\nGerman and US participants favor anonymity. Across countries, contact tracing\nis viewed more positively than quarantine enforcement, and technical\nmalfunctions negatively impact user acceptance.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 12:41:34 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 11:31:08 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Utz", "Christine", ""], ["Becker", "Steffen", ""], ["Schnitzler", "Theodor", ""], ["Farke", "Florian M.", ""], ["Herbert", "Franziska", ""], ["Schaewitz", "Leonie", ""], ["Degeling", "Martin", ""], ["D\u00fcrmuth", "Markus", ""]]}, {"id": "2010.14307", "submitter": "Franziska Hann{\\ss}", "authors": "Franziska Hann{\\ss}, Esther Lapczyna, Mathias M\\\"uller, Rainer Groh", "title": "What Color is this? Explaining Art Restoration Research Methods using\n  Interactive Museum Installations", "comments": "Workshop on Visual Interface Design Methods, AVI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This case study describes an approach to designing interactive museum\ninstallations as a student project with the aim of presenting the research\nresults of the restoration process of paintings to a wide range of visitors.\nDuring one and a half years, the Chair of Media Design created five interactive\nmedia stations in two lectures to enrich the special exhibition \"Veronese: The\nCuccina Cycle. The Restored Masterpiece\". The project was realised in close\ncommunication with the conservators of the Dresden State Art Collections and\nthe employees of the Science and Archaeometric Laboratory of the Dresden\nUniversity of Fine Arts. The students had to learn about the foreign content\nand how to translate it into a media-related environment. With suitable\nteaching methods, we pushed the students towards a deeper understanding of the\nmatter.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:10:45 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Hann\u00df", "Franziska", ""], ["Lapczyna", "Esther", ""], ["M\u00fcller", "Mathias", ""], ["Groh", "Rainer", ""]]}, {"id": "2010.14389", "submitter": "Andrew Isaak PhD", "authors": "Constantin von Selasinsky and Andrew Jay Isaak", "title": "It's all in the (Sub-)title? Expanding Signal Evaluation in Crowdfunding\n  Research", "comments": "Proceedings of the Twenty-Eighth European Conference on Information\n  Systems (ECIS2020)", "journal-ref": null, "doi": null, "report-no": "56", "categories": "cs.SI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on crowdfunding success that incorporates CATA (computer-aided text\nanalysis) is quickly advancing to the big leagues (e.g., Parhankangas and\nRenko, 2017; Anglin et al., 2018; Moss et al., 2018) and is often theoretically\nbased on information asymmetry, social capital, signaling or a combination\nthereof. Yet, current papers that explore crowdfunding success criteria fail to\ntake advantage of the full breadth of signals available and only very few such\npapers examine technology projects. In this paper, we compare and contrast the\nstrength of the entrepreneur's textual success signals to project backers\nwithin this category. Based on a random sample of 1,049 technology projects\ncollected from Kickstarter, we evaluate textual information not only from\nproject titles and descriptions but also from video subtitles. We find that\nincorporating subtitle information increases the variance explained by the\nrespective models and therefore their predictive capability for funding\nsuccess. By expanding the information landscape, our work advances the field\nand paves the way for more fine-grained studies of success signals in\ncrowdfunding and therefore for an improved understanding of investor\ndecision-making in the crowd.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 15:51:31 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["von Selasinsky", "Constantin", ""], ["Isaak", "Andrew Jay", ""]]}, {"id": "2010.14443", "submitter": "Ufuk Topcu", "authors": "Ufuk Topcu, Nadya Bliss, Nancy Cooke, Missy Cummings, Ashley Llorens,\n  Howard Shrobe, and Lenore Zuck", "title": "Assured Autonomy: Path Toward Living With Autonomous Systems We Can\n  Trust", "comments": "A Computing Community Consortium (CCC) workshop report, 28 pages", "journal-ref": null, "doi": null, "report-no": "ccc2020report_5", "categories": "cs.CY cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of establishing assurance in autonomy is rapidly attracting\nincreasing interest in the industry, government, and academia. Autonomy is a\nbroad and expansive capability that enables systems to behave without direct\ncontrol by a human operator. To that end, it is expected to be present in a\nwide variety of systems and applications. A vast range of industrial sectors,\nincluding (but by no means limited to) defense, mobility, health care,\nmanufacturing, and civilian infrastructure, are embracing the opportunities in\nautonomy yet face the similar barriers toward establishing the necessary level\nof assurance sooner or later. Numerous government agencies are poised to tackle\nthe challenges in assured autonomy.\n  Given the already immense interest and investment in autonomy, a series of\nworkshops on Assured Autonomy was convened to facilitate dialogs and increase\nawareness among the stakeholders in the academia, industry, and government.\nThis series of three workshops aimed to help create a unified understanding of\nthe goals for assured autonomy, the research trends and needs, and a strategy\nthat will facilitate sustained progress in autonomy.\n  The first workshop, held in October 2019, focused on current and anticipated\nchallenges and problems in assuring autonomous systems within and across\napplications and sectors. The second workshop held in February 2020, focused on\nexisting capabilities, current research, and research trends that could address\nthe challenges and problems identified in workshop. The third event was\ndedicated to a discussion of a draft of the major findings from the previous\ntwo workshops and the recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 17:00:01 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Topcu", "Ufuk", ""], ["Bliss", "Nadya", ""], ["Cooke", "Nancy", ""], ["Cummings", "Missy", ""], ["Llorens", "Ashley", ""], ["Shrobe", "Howard", ""], ["Zuck", "Lenore", ""]]}, {"id": "2010.14831", "submitter": "Stan Z Li", "authors": "Stan Z. Li, Zelin Zang, Lirong Wu", "title": "Deep Manifold Transformation for Nonlinear Dimensionality Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold learning-based encoders have been playing important roles in\nnonlinear dimensionality reduction (NLDR) for data exploration. However,\nexisting methods can often fail to preserve geometric, topological and/or\ndistributional structures of data. In this paper, we propose a deep manifold\nlearning framework, called deep manifold transformation (DMT) for unsupervised\nNLDR and embedding learning. DMT enhances deep neural networks by using\ncross-layer local geometry-preserving (LGP) constraints. The LGP constraints\nconstitute the loss for deep manifold learning and serve as geometric\nregularizers for NLDR network training. Extensive experiments on synthetic and\nreal-world data demonstrate that DMT networks outperform existing leading\nmanifold-based NLDR methods in terms of preserving the structures of data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 09:09:41 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 09:26:44 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 15:24:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Stan Z.", ""], ["Zang", "Zelin", ""], ["Wu", "Lirong", ""]]}, {"id": "2010.15015", "submitter": "Yan Chen", "authors": "Yan Chen, Walter S. Lasecki, Tao Dong", "title": "Towards Supporting Programming Education at Scale via Live Streaming", "comments": "Accepted to ACM CSCW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Live streaming, which allows streamers to broadcast their work to live\nviewers, is an emerging practice for teaching and learning computer\nprogramming. Participation in live streaming is growing rapidly, despite\nseveral apparent challenges, such as a general lack of training in pedagogy\namong streamers and scarce signals about a stream's characteristics (e.g.,\ndifficulty, style, and usefulness) to help viewers decide what to watch. To\nunderstand why people choose to participate in live streaming for teaching or\nlearning programming, and how they cope with both apparent and non-obvious\nchallenges, we interviewed 14 streamers and 12 viewers about their experience\nwith live streaming programming. Among other results, we found that the casual\nand impromptu nature of live streaming makes it easier to prepare than\npre-recorded videos, and viewers have the opportunity to shape the content and\nlearning experience via real-time communication with both the streamer and each\nother. Nonetheless, we identified several challenges that limit the potential\nof live streaming as a learning medium. For example, streamers voiced privacy\nand harassment concerns, and existing streaming platforms do not adequately\nsupport viewer-streamer interactions, adaptive learning, and discovery and\nselection of streaming content. Based on these findings, we suggest specialized\ntools to facilitate knowledge sharing among people teaching and learning\ncomputer programming online, and we offer design recommendations that promote a\nhealthy, safe, and engaging learning environment.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 14:48:39 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Chen", "Yan", ""], ["Lasecki", "Walter S.", ""], ["Dong", "Tao", ""]]}, {"id": "2010.15075", "submitter": "Noushin Hajarolasvadi", "authors": "Noushin Hajarolasvadi, Miguel Arjona Ram\\'irez and Hasan Demirel", "title": "Generative Adversarial Networks in Human Emotion Synthesis:A Review", "comments": "46 pages, 28 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthesizing realistic data samples is of great value for both academic and\nindustrial communities. Deep generative models have become an emerging topic in\nvarious research areas like computer vision and signal processing. Affective\ncomputing, a topic of a broad interest in computer vision society, has been no\nexception and has benefited from generative models. In fact, affective\ncomputing observed a rapid derivation of generative models during the last two\ndecades. Applications of such models include but are not limited to emotion\nrecognition and classification, unimodal emotion synthesis, and cross-modal\nemotion synthesis. As a result, we conducted a review of recent advances in\nhuman emotion synthesis by studying available databases, advantages, and\ndisadvantages of the generative models along with the related training\nstrategies considering two principal human communication modalities, namely\naudio and video. In this context, facial expression synthesis, speech emotion\nsynthesis, and the audio-visual (cross-modal) emotion synthesis is reviewed\nextensively under different application scenarios. Gradually, we discuss open\nresearch problems to push the boundaries of this research area for future\nworks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 16:45:36 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 11:05:36 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Hajarolasvadi", "Noushin", ""], ["Ram\u00edrez", "Miguel Arjona", ""], ["Demirel", "Hasan", ""]]}, {"id": "2010.15229", "submitter": "Jumana Almahmoud", "authors": "Jumana Almahmoud and Kruthika Kikkeri", "title": "Speech-Based Emotion Recognition using Neural Networks and Information\n  Visualization", "comments": "IEEE Vis 2020 Abstract", "journal-ref": "IEEE Vis 2020 Abstract", "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotions recognition is commonly employed for health assessment. However, the\ntypical metric for evaluation in therapy is based on patient-doctor appraisal.\nThis process can fall into the issue of subjectivity, while also requiring\nhealthcare professionals to deal with copious amounts of information. Thus,\nmachine learning algorithms can be a useful tool for the classification of\nemotions. While several models have been developed in this domain, there is a\nlack of userfriendly representations of the emotion classification systems for\ntherapy. We propose a tool which enables users to take speech samples and\nidentify a range of emotions (happy, sad, angry, surprised, neutral, clam,\ndisgust, and fear) from audio elements through a machine learning model. The\ndashboard is designed based on local therapists' needs for intuitive\nrepresentations of speech data in order to gain insights and informative\nanalyses of their sessions with their patients.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 20:57:32 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Almahmoud", "Jumana", ""], ["Kikkeri", "Kruthika", ""]]}, {"id": "2010.15346", "submitter": "Mohammad Ali", "authors": "Mohammad Ali", "title": "Developing Augmented Reality based Gaming Model to Teach Ethical\n  Education in Primary Schools", "comments": "4 Pages, 3 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Education sector is adopting new technologies for both teaching and learning\npedagogy. Augmented Reality (AR) is a new technology that can be used in the\neducational pedagogy to enhance the engagement with students. Students interact\nwith AR-based educational material for more visualization and explanation.\nTherefore, the use of AR in education is becoming more popular. However, most\nresearches narrate the use of AR technologies in the field of English, Maths,\nScience, Culture, Arts, and History education but the absence of ethical\neducation is visible. In our paper, we design the system and develop an\nAR-based mobile game model in the field of Ethical education for pre-primary\nstudents. Students from pre-primary require more interactive lessons than\ntheoretical concepts. So, we use AR technology to develop a game which offers\ninteractive procedures where students can learn with fun and engage with the\ncontext. Finally, we develop a prototype that works with our research\nobjective. We conclude our paper with future works.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 04:01:32 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Ali", "Mohammad", ""]]}, {"id": "2010.15372", "submitter": "Zheng Wang", "authors": "Zhuoxi Liu, Zheng Wang, Bo Yang, Kimihiko Nakano", "title": "Learning Personalized Discretionary Lane-Change Initiation for Fully\n  Autonomous Driving Based on Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the authors present a novel method to learn the personalized\ntactic of discretionary lane-change initiation for fully autonomous vehicles\nthrough human-computer interactions. Instead of learning from human-driving\ndemonstrations, a reinforcement learning technique is employed to learn how to\ninitiate lane changes from traffic context, the action of a self-driving\nvehicle, and in-vehicle user feedback. The proposed offline algorithm rewards\nthe action-selection strategy when the user gives positive feedback and\npenalizes it when negative feedback. Also, a multi-dimensional driving scenario\nis considered to represent a more realistic lane-change trade-off. The results\nshow that the lane-change initiation model obtained by this method can\nreproduce the personal lane-change tactic, and the performance of the\ncustomized models (average accuracy 86.1%) is much better than that of the\nnon-customized models (average accuracy 75.7%). This method allows continuous\nimprovement of customization for users during fully autonomous driving even\nwithout human-driving experience, which will significantly enhance the user\nacceptance of high-level autonomy of self-driving vehicles.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 06:21:23 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Liu", "Zhuoxi", ""], ["Wang", "Zheng", ""], ["Yang", "Bo", ""], ["Nakano", "Kimihiko", ""]]}, {"id": "2010.15446", "submitter": "Siddharth Sigtia", "authors": "Siddharth Sigtia, John Bridle, Hywel Richards, Pascal Clark, Erik\n  Marchi, Vineet Garg", "title": "Progressive Voice Trigger Detection: Accuracy vs Latency", "comments": "Camera Ready Version: ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an architecture for voice trigger detection for virtual\nassistants. The main idea in this work is to exploit information in words that\nimmediately follow the trigger phrase. We first demonstrate that by including\nmore audio context after a detected trigger phrase, we can indeed get a more\naccurate decision. However, waiting to listen to more audio each time incurs a\nlatency increase. Progressive Voice Trigger Detection allows us to trade-off\nlatency and accuracy by accepting clear trigger candidates quickly, but waiting\nfor more context to decide whether to accept more marginal examples. Using a\ntwo-stage architecture, we show that by delaying the decision for just 3% of\ndetected true triggers in the test set, we are able to obtain a relative\nimprovement of 66% in false rejection rate, while incurring only a negligible\nincrease in latency.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 09:43:04 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 15:16:22 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Sigtia", "Siddharth", ""], ["Bridle", "John", ""], ["Richards", "Hywel", ""], ["Clark", "Pascal", ""], ["Marchi", "Erik", ""], ["Garg", "Vineet", ""]]}, {"id": "2010.16016", "submitter": "EPTCS", "authors": "Walther Neuper (Johannes Kepler University Linz, Austria)", "title": "Lucas-Interpretation on Isabelle's Functions", "comments": "In Proceedings ThEdu'20, arXiv:2010.15832", "journal-ref": "EPTCS 328, 2020, pp. 79-95", "doi": "10.4204/EPTCS.328.5", "report-no": null, "categories": "cs.PL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software tools of Automated Reasoning are too sophisticated for general use\nin mathematics education and respective reasoning, while Lucas-Interpretation\nprovides a general concept for integrating such tools into educational software\nwith the purpose to reliably and flexibly check formal input of students. This\npaper gives the first technically concise description of Lucas-Interpretation\nat the occasion of migrating a prototype implementation to the function package\nof the proof assistant Isabelle. The description shows straightforward\nadaptations of Isabelle's programming language and shows, how simple migration\nof the interpreter was, since the design (before the function package has been\nintroduced to Isabelle) recognised appropriateness of Isabelle's terms as\nmiddle end. The paper gives links into the code in an open repository as\ninvitation to readers for re-using the prototyped code or adopt the general\nconcept. And since the prototype has been designed before the function package\nwas implemented, the paper is an opportunity for recording lessons learned from\nIsabelle's development of code structure.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 01:15:44 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Neuper", "Walther", "", "Johannes Kepler University Linz, Austria"]]}, {"id": "2010.16052", "submitter": "Kamran Kowsari", "authors": "Mehrdad Fazli, Kamran Kowsari, Erfaneh Gharavi, Laura Barnes, Afsaneh\n  Doryab", "title": "HHAR-net: Hierarchical Human Activity Recognition using Neural Networks", "comments": "Accepted in IHCI2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Activity recognition using built-in sensors in smart and wearable devices\nprovides great opportunities to understand and detect human behavior in the\nwild and gives a more holistic view of individuals' health and well being.\nNumerous computational methods have been applied to sensor streams to recognize\ndifferent daily activities. However, most methods are unable to capture\ndifferent layers of activities concealed in human behavior. Also, the\nperformance of the models starts to decrease with increasing the number of\nactivities. This research aims at building a hierarchical classification with\nNeural Networks to recognize human activities based on different levels of\nabstraction. We evaluate our model on the Extrasensory dataset; a dataset\ncollected in the wild and containing data from smartphones and smartwatches. We\nuse a two-level hierarchy with a total of six mutually exclusive labels namely,\n\"lying down\", \"sitting\", \"standing in place\", \"walking\", \"running\", and\n\"bicycling\" divided into \"stationary\" and \"non-stationary\". The results show\nthat our model can recognize low-level activities (stationary/non-stationary)\nwith 95.8% accuracy and overall accuracy of 92.8% over six labels. This is 3%\nabove our best performing baseline.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 17:06:42 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 22:52:46 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Fazli", "Mehrdad", ""], ["Kowsari", "Kamran", ""], ["Gharavi", "Erfaneh", ""], ["Barnes", "Laura", ""], ["Doryab", "Afsaneh", ""]]}, {"id": "2010.16153", "submitter": "Claudia-Lavinia Ignat", "authors": "Hoai Le Nguyen (COAST), Claudia-Lavinia Ignat (COAST)", "title": "Time-position characterization of conflicts: a case study of\n  collaborative editing", "comments": null, "journal-ref": "The 26th International Conference on Collaboration Technologies\n  and Social Computing (CollabTech 2020), Sep 2020, Tartu, Estonia", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative editing (CE) became increasingly common, often compulsory in\nacademia and industry where people work in teams and are distributed across\nspace and time. We aim to study collabora-tive editing behavior in terms of\ncollaboration patterns users adopt and in terms of a characterisation of\nconflicts, i.e. edits from different users that occur close in time and\nposition in the document. The process of a CE can be split into several editing\n'sessions' which are performed by a single author ('single-authored session')\nor several authors ('co-authored session'). This fragmentation process requires\na pre-defined 'maximum time gap' between sessions which is not yet well defined\nin previous studies. In this study, we analysed CE logs of 108 collaboratively\nedited documents. We show how to establish a suitable 'maximum time gap' to\nsplit CE activities into sessions by evaluating the distribution of the time\ndistance between two adjacent sessions. We studied editing activities inside\neach 'co-author session' in order to define potential conflicts in terms of\ntime and position dimensions before they occur in the document. We also\nanalysed how many of these potential conflicts become real conflicts. Findings\nshow that potential conflicting cases are few. However, they are more likely to\nbecome real conflicts.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 09:47:39 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Nguyen", "Hoai Le", "", "COAST"], ["Ignat", "Claudia-Lavinia", "", "COAST"]]}, {"id": "2010.16201", "submitter": "Muhammad Muzammel", "authors": "Muhammad Muzammel, Hanan Salam, Yann Hoffmann, Mohamed Chetouani,\n  Alice Othmani", "title": "AudVowelConsNet: A Phoneme-Level Based Deep CNN Architecture for\n  Clinical Depression Diagnosis", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.mlwa.2020.100005", "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depression is a common and serious mood disorder that negatively affects the\npatient's capacity of functioning normally in daily tasks. Speech is proven to\nbe a vigorous tool in depression diagnosis. Research in psychiatry concentrated\non performing fine-grained analysis on word-level speech components\ncontributing to the manifestation of depression in speech and revealed\nsignificant variations at the phoneme-level in depressed speech. On the other\nhand, research in Machine Learning-based automatic recognition of depression\nfrom speech focused on the exploration of various acoustic features for the\ndetection of depression and its severity level. Few have focused on\nincorporating phoneme-level speech components in automatic assessment systems.\nIn this paper, we propose an Artificial Intelligence (AI) based application for\nclinical depression recognition and assessment from speech. We investigate the\nacoustic characteristics of phoneme units, specifically vowels and consonants\nfor depression recognition via Deep Learning. We present and compare three\nspectrogram-based Deep Neural Network architectures, trained on phoneme\nconsonant and vowel units and their fusion respectively. Our experiments show\nthat the deep learned consonant-based acoustic characteristics lead to better\nrecognition results than vowel-based ones. The fusion of vowel and consonant\nspeech characteristics through a deep network significantly outperforms the\nsingle space networks as well as the state-of-art deep learning approaches on\nthe DAIC-WOZ database.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 11:36:26 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 11:59:01 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Muzammel", "Muhammad", ""], ["Salam", "Hanan", ""], ["Hoffmann", "Yann", ""], ["Chetouani", "Mohamed", ""], ["Othmani", "Alice", ""]]}]