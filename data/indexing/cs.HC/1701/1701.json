[{"id": "1701.00248", "submitter": "Tony T. Luo", "authors": "Tony T. Luo, Salil S. Kanhere, Jianwei Huang, Sajal K. Das, Fan Wu", "title": "Sustainable Incentives for Mobile Crowdsensing: Auctions, Lotteries, and\n  Trust and Reputation Systems", "comments": "6 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proper incentive mechanisms are critical for mobile crowdsensing systems to\nmotivate people to actively and persistently participate. This article provides\nan exposition of design principles of six incentive mechanisms, drawing special\nattention to the sustainability issue. We cover three primary classes of\nincentive mechanisms: auctions, lotteries, and trust and reputation systems, as\nwell as three other frameworks of promising potential: bargaining games,\ncontract theory, and market-driven mechanisms.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 14:41:16 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 11:11:22 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Luo", "Tony T.", ""], ["Kanhere", "Salil S.", ""], ["Huang", "Jianwei", ""], ["Das", "Sajal K.", ""], ["Wu", "Fan", ""]]}, {"id": "1701.00487", "submitter": "Berrie Van Der Molen M.A.", "authors": "Berrie van der Molen, Lars Buitinck, Toine Pieters", "title": "The leveled approach. Using and evaluating text mining tools\n  AVResearcherXL and Texcavator for historical research on public perceptions\n  of drugs", "comments": "3 pages, extended abstract of a lightning talk delivered at the 2nd\n  IFIP International Workshop on Computational History and Data-driven\n  Humanities on 25 May 2016 (Trinity College Dublin, Ireland)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce our explorative historical leveled approach that we use to\nunderstand drug debates in the Royal Dutch Library's digital newspaper archive.\nIn this approach we alternate between distant reading and close reading.\nFurthermore, we use this approach to evaluate two text mining tools:\nAVResearcherXL and Texcavator.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 12:16:11 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["van der Molen", "Berrie", ""], ["Buitinck", "Lars", ""], ["Pieters", "Toine", ""]]}, {"id": "1701.00841", "submitter": "Matthew Rueben", "authors": "Matthew Rueben, Cindy M. Grimm, Frank J. Bernieri, William D. Smart", "title": "A Taxonomy of Privacy Constructs for Privacy-Sensitive Robotics", "comments": "5 pages double-column, 1 figure, PDFLaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The introduction of robots into our society will also introduce new concerns\nabout personal privacy. In order to study these concerns, we must do\nhuman-subject experiments that involve measuring privacy-relevant constructs.\nThis paper presents a taxonomy of privacy constructs based on a review of the\nprivacy literature. Future work in operationalizing privacy constructs for HRI\nstudies is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 21:42:58 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Rueben", "Matthew", ""], ["Grimm", "Cindy M.", ""], ["Bernieri", "Frank J.", ""], ["Smart", "William D.", ""]]}, {"id": "1701.01096", "submitter": "Jianbo Ye", "authors": "Jianbo Ye, Jia Li, Michelle G. Newman, Reginald B. Adams Jr. and James\n  Z. Wang", "title": "Probabilistic Multigraph Modeling for Improving the Quality of\n  Crowdsourced Affective Data", "comments": "14 pages, 6 figures, 2 tables, meta data revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a probabilistic approach to joint modeling of participants'\nreliability and humans' regularity in crowdsourced affective studies.\nReliability measures how likely a subject will respond to a question seriously;\nand regularity measures how often a human will agree with other\nseriously-entered responses coming from a targeted population.\nCrowdsourcing-based studies or experiments, which rely on human self-reported\naffect, pose additional challenges as compared with typical crowdsourcing\nstudies that attempt to acquire concrete non-affective labels of objects. The\nreliability of participants has been massively pursued for typical\nnon-affective crowdsourcing studies, whereas the regularity of humans in an\naffective experiment in its own right has not been thoroughly considered. It\nhas been often observed that different individuals exhibit different feelings\non the same test question, which does not have a sole correct response in the\nfirst place. High reliability of responses from one individual thus cannot\nconclusively result in high consensus across individuals. Instead, globally\ntesting consensus of a population is of interest to investigators. Built upon\nthe agreement multigraph among tasks and workers, our probabilistic model\ndifferentiates subject regularity from population reliability. We demonstrate\nthe method's effectiveness for in-depth robust analysis of large-scale\ncrowdsourced affective data, including emotion and aesthetic assessments\ncollected by presenting visual stimuli to human subjects.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 18:24:56 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2017 02:19:30 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Ye", "Jianbo", ""], ["Li", "Jia", ""], ["Newman", "Michelle G.", ""], ["Adams", "Reginald B.", "Jr."], ["Wang", "James Z.", ""]]}, {"id": "1701.01216", "submitter": "Tony T. Luo", "authors": "T. Luo, S. S. Kanhere, H-P. Tan, F. Wu, and H. Wu", "title": "Crowdsourcing with Tullock contests: A new perspective", "comments": "9 pages, 4 figures, 3 tables", "journal-ref": "Proc. IEEE INFOCOM, 2015, pp. 2515-2523", "doi": "10.1109/INFOCOM.2015.7218641", "report-no": null, "categories": "cs.GT cs.HC cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incentive mechanisms for crowdsourcing have been extensively studied under\nthe framework of all-pay auctions. Along a distinct line, this paper proposes\nto use Tullock contests as an alternative tool to design incentive mechanisms\nfor crowdsourcing. We are inspired by the conduciveness of Tullock contests to\nattracting user entry (yet not necessarily a higher revenue) in other domains.\nIn this paper, we explore a new dimension in optimal Tullock contest design, by\nsuperseding the contest prize---which is fixed in conventional Tullock\ncontests---with a prize function that is dependent on the (unknown) winner's\ncontribution, in order to maximize the crowdsourcer's utility. We show that\nthis approach leads to attractive practical advantages: (a) it is well-suited\nfor rapid prototyping in fully distributed web agents and smartphone apps; (b)\nit overcomes the disincentive to participate caused by players' antagonism to\nan increasing number of rivals. Furthermore, we optimize conventional,\nfixed-prize Tullock contests to construct the most superior benchmark to\ncompare against our mechanism. Through extensive evaluations, we show that our\nmechanism significantly outperforms the optimal benchmark, by over three folds\non the crowdsourcer's utility cum profit and up to nine folds on the players'\nsocial welfare.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 05:44:25 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Luo", "T.", ""], ["Kanhere", "S. S.", ""], ["Tan", "H-P.", ""], ["Wu", "F.", ""], ["Wu", "H.", ""]]}, {"id": "1701.01644", "submitter": "Lennart Br\\\"uggemann", "authors": "Lennart Br\\\"uggemann", "title": "Interaktion mit 3D-Objekten in Augmented Reality Anwendungen auf mobilen\n  Android Ger\\\"aten", "comments": "88 Pages, 14 figures, Bachelor's Thesis, in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This bachelor's thesis describes the conception and implementation of an\naugmented reality application for the Android platform. The intention is to\ndemonstrate some possibilities of interaction within an augmented reality\nenvironment on mobile devices. For that purpose, a 3D-model is displayed on the\ndevices' touchscreen using marker-based tracking. This enables the user to\ntranslate, rotate or scale the model as he wishes. He can additionally select\nand highlight preassigned parts of the model to display specific information\nfor that element. To assist developers in modifying the application for\nchanging requirements without re-writing large portions of the source code, the\ninformation for each part have been encapsulated into its own data type. After\nan introduction to augmented reality, its underlying technology and the Android\nplatform, some possible usage scenarios and the resulting functionalities are\noutlined. Finally, the design as well as the developed implementation are\ndescribed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 21:08:55 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Br\u00fcggemann", "Lennart", ""]]}, {"id": "1701.01793", "submitter": "Rajan Vaish", "authors": "Rajan Vaish, Andr\\'es Monroy-Hern\\'andez", "title": "CrowdTone: Crowd-powered tone feedback and improvement system for emails", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present CrowdTone, a system designed to help people set the\nappropriate tone in their email communication. CrowdTone utilizes the context\nand content of an email message to identify and set the appropriate tone\nthrough a consensus-building process executed by crowd workers. We evaluated\nCrowdTone with 22 participants, who provided a total of 29 emails that they had\nreceived in the past, and ran them through CrowdTone. Participants and\nprofessional writers assessed the quality of improvements finding a substantial\nincrease in the percentage of emails deemed \"appropriate\" or \"very appropriate\"\n- from 25% to more than 90% by recipients, and from 45% to 90% by professional\nwriters. Additionally, the recipients' feedback indicated that more than 90% of\nthe CrowdTone processed emails showed improvement.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2017 05:35:10 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Vaish", "Rajan", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""]]}, {"id": "1701.02185", "submitter": "Anca Dumitrache", "authors": "Anca Dumitrache, Lora Aroyo, Chris Welty", "title": "Crowdsourcing Ground Truth for Medical Relation Extraction", "comments": "Accepted for publication in ACM Transactions on Interactive\n  Intelligent Systems (TiiS) Special Issue on Human-Centered Machine Learning", "journal-ref": "ACM Transactions on Interactive Intelligent Systems (TIIS) Volume\n  8 Issue 2, July 2018", "doi": "10.1145/3152889", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Cognitive computing systems require human labeled data for evaluation, and\noften for training. The standard practice used in gathering this data minimizes\ndisagreement between annotators, and we have found this results in data that\nfails to account for the ambiguity inherent in language. We have proposed the\nCrowdTruth method for collecting ground truth through crowdsourcing, that\nreconsiders the role of people in machine learning based on the observation\nthat disagreement between annotators provides a useful signal for phenomena\nsuch as ambiguity in the text. We report on using this method to build an\nannotated data set for medical relation extraction for the $cause$ and $treat$\nrelations, and how this data performed in a supervised training experiment. We\ndemonstrate that by modeling ambiguity, labeled data gathered from crowd\nworkers can (1) reach the level of quality of domain experts for this task\nwhile reducing the cost, and (2) provide better training data at scale than\ndistant supervision. We further propose and validate new weighted measures for\nprecision, recall, and F-measure, that account for ambiguity in both human and\nmachine performance on this task.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 14:13:23 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 15:04:43 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Dumitrache", "Anca", ""], ["Aroyo", "Lora", ""], ["Welty", "Chris", ""]]}, {"id": "1701.02369", "submitter": "Kory W Mathewson", "authors": "Kory W. Mathewson and Patrick M. Pilarski", "title": "Reinforcement Learning based Embodied Agents Modelling Human Users\n  Through Interaction and Multi-Sensory Perception", "comments": "4 pages, 2 figures, Accepted at the 2017 AAAI Spring Symposium on\n  Interactive Multi-Sensory Object Perception for Embodied Agents", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends recent work in interactive machine learning (IML) focused\non effectively incorporating human feedback. We show how control and feedback\nsignals complement each other in systems which model human reward. We\ndemonstrate that simultaneously incorporating human control and feedback\nsignals can improve interactive robotic systems' performance on a self-mirrored\nmovement control task where an RL-agent controlled right arm attempts to match\nthe preprogrammed movement pattern of the left arm. We illustrate the impact of\nvarying human feedback parameters on task performance by investigating the\nprobability of giving feedback on each time step and the likelihood of given\nfeedback being correct. We further illustrate that varying the temporal decay\nwith which the agent incorporates human feedback has a significant impact on\ntask performance. We found that smearing human feedback over time steps\nimproves performance and we show varying the probability of feedback at each\ntime step, and an increased likelihood of those feedbacks being 'correct' can\nimpact agent performance. We conclude that understanding latent variables in\nhuman feedback is crucial for learning algorithms acting in human-machine\ninteraction domains.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 22:03:18 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 17:44:52 GMT"}, {"version": "v3", "created": "Thu, 26 Jan 2017 18:37:52 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Mathewson", "Kory W.", ""], ["Pilarski", "Patrick M.", ""]]}, {"id": "1701.02586", "submitter": "Walterio Mayol-Cuevas", "authors": "Teesid Leelasawassuk, Dima Damen, Walterio Mayol-Cuevas", "title": "Automated capture and delivery of assistive task guidance with an\n  eyewear computer: The GlaciAR system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe and evaluate a mixed reality system that aims to\naugment users in task guidance applications by combining automated and\nunsupervised information collection with minimally invasive video guides. The\nresult is a self-contained system that we call GlaciAR (Glass-enabled\nContextual Interactions for Augmented Reality), that operates by extracting\ncontextual interactions from observing users performing actions. GlaciAR is\nable to i) automatically determine moments of relevance based on a head motion\nattention model, ii) automatically produce video guidance information, iii)\ntrigger these video guides based on an object detection method, iv) learn\nwithout supervision from observing multiple users and v) operate fully on-board\na current eyewear computer (Google Glass). We describe the components of\nGlaciAR together with evaluations on how users are able to use the system to\nachieve three tasks. We see this work as a first step toward the development of\nsystems that aim to scale up the notoriously difficult authoring problem in\nguidance systems and where people's natural abilities are enhanced via\nminimally invasive visual guidance.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 01:10:54 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Leelasawassuk", "Teesid", ""], ["Damen", "Dima", ""], ["Mayol-Cuevas", "Walterio", ""]]}, {"id": "1701.03079", "submitter": "Lili Mou", "authors": "Chongyang Tao, Lili Mou, Dongyan Zhao, Rui Yan", "title": "RUBER: An Unsupervised Method for Automatic Evaluation of Open-Domain\n  Dialog Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain human-computer conversation has been attracting increasing\nattention over the past few years. However, there does not exist a standard\nautomatic evaluation metric for open-domain dialog systems; researchers usually\nresort to human annotation for model evaluation, which is time- and\nlabor-intensive. In this paper, we propose RUBER, a Referenced metric and\nUnreferenced metric Blended Evaluation Routine, which evaluates a reply by\ntaking into consideration both a groundtruth reply and a query (previous\nuser-issued utterance). Our metric is learnable, but its training does not\nrequire labels of human satisfaction. Hence, RUBER is flexible and extensible\nto different datasets and languages. Experiments on both retrieval and\ngenerative dialog systems show that RUBER has a high correlation with human\nannotation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 17:43:57 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 16:43:08 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Tao", "Chongyang", ""], ["Mou", "Lili", ""], ["Zhao", "Dongyan", ""], ["Yan", "Rui", ""]]}, {"id": "1701.03839", "submitter": "Steven Jens Jorgensen", "authors": "Steven Jens Jorgensen, Orion Campbell, Travis Llado, Donghyun Kim,\n  Junhyeok Ahn, and Luis Sentis", "title": "Exploring Model Predictive Control to Generate Optimal Control Policies\n  for HRI Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model Human-Robot-Interaction (HRI) scenarios as linear dynamical systems\nand use Model Predictive Control (MPC) with mixed integer constraints to\ngenerate human-aware control policies. We motivate the approach by presenting\ntwo scenarios. The first involves an assistive robot that aims to maximize\nproductivity while minimizing the human's workload, and the second involves a\nlistening humanoid robot that manages its eye contact behavior to maximize\n\"connection\" and minimize social \"awkwardness\" with the human during the\ninteraction. Our simulation results show that the robot generates useful\nbehaviors as it finds control policies to minimize the specified cost function.\nFurther, we implement the second scenario on a humanoid robot and test the eye\ncontact scenario with 48 human participants to demonstrate and evaluate the\ndesired controller behavior. The humanoid generated 25% more eye contact when\nit was told to maximize connection over when it was told to maximize\nawkwardness. However, despite showing the desired behavior, there was no\nstatistical difference between the participant's perceived connection with the\nhumanoid.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 22:05:38 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Jorgensen", "Steven Jens", ""], ["Campbell", "Orion", ""], ["Llado", "Travis", ""], ["Kim", "Donghyun", ""], ["Ahn", "Junhyeok", ""], ["Sentis", "Luis", ""]]}, {"id": "1701.03963", "submitter": "Jens Grubert", "authors": "Jens Grubert, Eyal Ofek, Michel Pahud, Matthias Kranz, Dieter\n  Schmalstieg", "title": "Towards Interaction Around Unmodified Camera-equipped Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Around-device interaction promises to extend the input space of mobile and\nwearable devices beyond the common but restricted touchscreen. So far, most\naround-device interaction approaches rely on instrumenting the device or the\nenvironment with additional sensors. We believe, that the full potential of\nordinary cameras, specifically user-facing cameras, which are integrated in\nmost mobile devices today, are not used to their full potential, yet. We To\nthis end, we present a novel approach for extending the input space around\nunmodified mobile devices using built-in front-facing cameras of unmodified\nhandheld devices. Our approach estimates hand poses and gestures through\nreflections in sunglasses, ski goggles or visors. Thereby, GlassHands creates\nan enlarged input space, rivaling input reach on large touch displays. We\ndiscuss the idea, its limitations and future work.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 20:36:58 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Grubert", "Jens", ""], ["Ofek", "Eyal", ""], ["Pahud", "Michel", ""], ["Kranz", "Matthias", ""], ["Schmalstieg", "Dieter", ""]]}, {"id": "1701.03968", "submitter": "Arturo Deza", "authors": "Arturo Deza, Jeffrey R. Peters, Grant S. Taylor, Amit Surana and\n  Miguel P. Eckstein", "title": "Attention Allocation Aid for Visual Search", "comments": "To be presented at the ACM CHI conference in Denver, Colorado in May\n  2017", "journal-ref": null, "doi": "10.1145/3025453.3025834", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines the development and testing of a novel, feedback-enabled\nattention allocation aid (AAAD), which uses real-time physiological data to\nimprove human performance in a realistic sequential visual search task. Indeed,\nby optimizing over search duration, the aid improves efficiency, while\npreserving decision accuracy, as the operator identifies and classifies targets\nwithin simulated aerial imagery. Specifically, using experimental eye-tracking\ndata and measurements about target detectability across the human visual field,\nwe develop functional models of detection accuracy as a function of search\ntime, number of eye movements, scan path, and image clutter. These models are\nthen used by the AAAD in conjunction with real time eye position data to make\nprobabilistic estimations of attained search accuracy and to recommend that the\nobserver either move on to the next image or continue exploring the present\nimage. An experimental evaluation in a scenario motivated from human\nsupervisory control in surveillance missions confirms the benefits of the AAAD.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 21:58:28 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Deza", "Arturo", ""], ["Peters", "Jeffrey R.", ""], ["Taylor", "Grant S.", ""], ["Surana", "Amit", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "1701.03981", "submitter": "Josef Faller", "authors": "Josef Faller, Brendan Z. Allison, Clemens Brunner, Reinhold Scherer,\n  Dieter Schmalstieg, Gert Pfurtscheller and Christa Neuper", "title": "A feasibility study on SSVEP-based interaction with motivating and\n  immersive virtual and augmented reality", "comments": "6 Pages, correspondence: josef.faller@gmail.com, Technical Report\n  (2010) Graz University of Technology, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive steady-state visual evoked potential (SSVEP) based\nbrain-computer interface (BCI) systems offer high bandwidth compared to other\nBCI types and require only minimal calibration and training. Virtual reality\n(VR) has been already validated as effective, safe, affordable and motivating\nfeedback modality for BCI experiments. Augmented reality (AR) enhances the\nphysical world by superimposing informative, context sensitive, computer\ngenerated content. In the context of BCI, AR can be used as a friendlier and\nmore intuitive real-world user interface, thereby facilitating a more seamless\nand goal directed interaction. This can improve practicality and usability of\nBCI systems and may help to compensate for their low bandwidth. In this\nfeasibility study, three healthy participants had to finish a complex\nnavigation task in immersive VR and AR conditions using an online SSVEP BCI.\nTwo out of three subjects were successful in all conditions. To our knowledge,\nthis is the first work to present an SSVEP BCI that operates using target\nstimuli integrated in immersive VR and AR (head-mounted display and camera).\nThis research direction can benefit patients by introducing more intuitive and\neffective real-world interaction (e.g. smart home control). It may also be\nrelevant for user groups that require or benefit from hands free operation\n(e.g. due to temporary situational disability).\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 01:58:47 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Faller", "Josef", ""], ["Allison", "Brendan Z.", ""], ["Brunner", "Clemens", ""], ["Scherer", "Reinhold", ""], ["Schmalstieg", "Dieter", ""], ["Pfurtscheller", "Gert", ""], ["Neuper", "Christa", ""]]}, {"id": "1701.04645", "submitter": "Arnaud Martin", "authors": "Hosna Ouni (IRISA, DRUID), Arnaud Martin (IRISA, UR1, DRUID), Laetitia\n  Gros, Mouloud Kharoune (IRISA, DRUID), Zoltan Miklos (IRISA, DRUID)", "title": "Une mesure d'expertise pour le crowdsourcing", "comments": "in French", "journal-ref": "Extraction et Gestion des Connaissances (EGC), Jan 2017, Grenoble,\n  France. Extraction et Gestion de Connaisasnces, 2017", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing, a major economic issue, is the fact that the firm outsources\ninternal task to the crowd. It is a form of digital subcontracting for the\ngeneral public. The evaluation of the participants work quality is a major\nissue in crowdsourcing. Indeed, contributions must be controlled to ensure the\neffectiveness and relevance of the campaign. We are particularly interested in\nsmall, fast and not automatable tasks. Several methods have been proposed to\nsolve this problem, but they are applicable when the \"golden truth\" is not\nalways known. This work has the particularity to propose a method for\ncalculating the degree of expertise in the presence of gold data in\ncrowdsourcing. This method is based on the belief function theory and proposes\na structuring of data using graphs. The proposed approach will be assessed and\napplied to the data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 12:35:36 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Ouni", "Hosna", "", "IRISA, DRUID"], ["Martin", "Arnaud", "", "IRISA, UR1, DRUID"], ["Gros", "Laetitia", "", "IRISA, DRUID"], ["Kharoune", "Mouloud", "", "IRISA, DRUID"], ["Miklos", "Zoltan", "", "IRISA, DRUID"]]}, {"id": "1701.04693", "submitter": "Sepehr Valipour", "authors": "Sepehr Valipour, Camilo Perez, Martin Jagersand", "title": "Incremental Learning for Robot Perception through HRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scene understanding and object recognition is a difficult to achieve yet\ncrucial skill for robots. Recently, Convolutional Neural Networks (CNN), have\nshown success in this task. However, there is still a gap between their\nperformance on image datasets and real-world robotics scenarios. We present a\nnovel paradigm for incrementally improving a robot's visual perception through\nactive human interaction. In this paradigm, the user introduces novel objects\nto the robot by means of pointing and voice commands. Given this information,\nthe robot visually explores the object and adds images from it to re-train the\nperception module. Our base perception module is based on recent development in\nobject detection and recognition using deep learning. Our method leverages\nstate of the art CNNs from off-line batch learning, human guidance, robot\nexploration and incremental on-line learning.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 14:29:05 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Valipour", "Sepehr", ""], ["Perez", "Camilo", ""], ["Jagersand", "Martin", ""]]}, {"id": "1701.05248", "submitter": "Daniel Hadar", "authors": "Daniel Hadar", "title": "Implicit Media Tagging and Affect Prediction from video of spontaneous\n  facial expressions, recorded with depth camera", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method that automatically evaluates emotional response from\nspontaneous facial activity recorded by a depth camera. The automatic\nevaluation of emotional response, or affect, is a fascinating challenge with\nmany applications, including human-computer interaction, media tagging and\nhuman affect prediction. Our approach in addressing this problem is based on\nthe inferred activity of facial muscles over time, as captured by a depth\ncamera recording an individual's facial activity. Our contribution is two-fold:\nFirst, we constructed a database of publicly available short video clips, which\nelicit a strong emotional response in a consistent manner across different\nindividuals. Each video was tagged by its characteristic emotional response\nalong 4 scales: \\emph{Valence, Arousal, Likability} and \\emph{Rewatch} (the\ndesire to watch again). The second contribution is a two-step prediction\nmethod, based on learning, which was trained and tested using this database of\ntagged video clips. Our method was able to successfully predict the\naforementioned 4 dimensional representation of affect, as well as to identify\nthe period of strongest emotional response in the viewing recordings, in a\nmethod that is blind to the video clip being watch, revealing a significantly\nhigh agreement between the recordings of independent viewers.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 22:23:41 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Hadar", "Daniel", ""]]}, {"id": "1701.05739", "submitter": "Daniel Graziotin", "authors": "Jan-Peter Ostberg, Daniel Graziotin, Stefan Wagner, Birgit Derntl", "title": "Towards the Assessment of Stress and Emotional Responses of a\n  Salutogenesis-Enhanced Software Tool Using Psychophysiological Measurements", "comments": "5 pages, 1 figure. To be presented at the Second International\n  Workshop on Emotion Awareness in Software Engineering, colocated with the\n  39th International Conference on Software Engineering (ICSE'17)", "journal-ref": "2017 IEEE/ACM 2nd International Workshop on Emotion Awareness in\n  Software Engineering (SEmotion), Buenos Aires, Argentina, 2017, pp. 22-25", "doi": "10.1109/SEmotion.2017.4", "report-no": null, "categories": "cs.SE cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software development is intellectual, based on collaboration, and performed\nin a highly demanding economic market. As such, it is dominated by time\npressure, stress, and emotional trauma. While studies of affect are emerging\nand rising in software engineering research, stress has yet to find its place\nin the literature despite that it is highly related to affect. In this paper,\nwe study stress coping with the affect-laden framework of Salutogenesis, which\nis a validated psychological framework for enhancing mental health through a\nfeeling of coherence. We propose a controlled experiment for testing our\nhypotheses that a static analysis tool enhanced with the Salutogenesis model\nwill bring 1) a higher number of fixed quality issues, 2) reduced cognitive\nload, 3) reduction of the overall stress, and 4) positive affect induction\neffects to developers. The experiment will make use of validated physiological\nmeasurements of stress as proxied by cortisol and alpha-amylase levels in\nsaliva samples, a psychometrically validated measurement of mood and affect\ndisposition, and stress inductors such as a cognitive load task. Our\nhypotheses, if empirically supported, will lead to the creation of\nenvironments, methods, and tools that alleviate stress among developers while\nenhancing affect on the job and task performance.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 09:53:23 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 17:18:36 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Ostberg", "Jan-Peter", ""], ["Graziotin", "Daniel", ""], ["Wagner", "Stefan", ""], ["Derntl", "Birgit", ""]]}, {"id": "1701.05768", "submitter": "Yuren Zhou", "authors": "Yuren Zhou, Jin Wang, Peng Shi, Daniel Dahlmeier, Nils Tippenhauer,\n  Erik Wilhelm", "title": "Power-saving transportation mode identification for large-scale\n  applications", "comments": "This paper has been withdrawn by the author to rewrite", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation mode detection with personal devices has been investigated for\nover ten years due to its importance in monitoring ones' activities,\nunderstanding human mobility, and assisting traffic management. However, two\nmain limitations are still preventing it from large-scale deployments: high\npower consumption, and the lack of high-volume and diverse labeled data. In\norder to reduce power consumption, existing approaches are sampling using fewer\nsensors and with lower frequency, which however lead to a lower accuracy. A\ncommon way to obtain labeled data is recording the ground truth while\ncollecting data, but such method cannot apply to large-scale deployment due to\nits inefficiency. To address these issues, we adopt a new low-frequency\nsampling manner with a hierarchical transportation mode identification\nalgorithm and propose an offline data labeling approach with its manual and\nautomatic implementations. Through a real-world large-scale experiment and\ncomparison with related works, our sampling manner and algorithm are proved to\nconsume much less energy while achieving a competitive accuracy around 85%. The\nnew offline data labeling approach is also validated to be efficient and\neffective in providing ground truth for model training and testing.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 11:44:00 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 08:52:46 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Zhou", "Yuren", ""], ["Wang", "Jin", ""], ["Shi", "Peng", ""], ["Dahlmeier", "Daniel", ""], ["Tippenhauer", "Nils", ""], ["Wilhelm", "Erik", ""]]}, {"id": "1701.05921", "submitter": "Maria Cabrera", "authors": "Maria Cabrera, Keisha Novak, Daniel Foti, Richard Voyles, Juan Wachs", "title": "What makes a gesture a gesture? Neural signatures involved in gesture\n  recognition", "comments": "This work has been submitted to a IEEE conference and is awaiting for\n  a decision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work in the area of gesture production, has made the assumption that\nmachines can replicate \"human-like\" gestures by connecting a bounded set of\nsalient points in the motion trajectory. Those inflection points were\nhypothesized to also display cognitive saliency. The purpose of this paper is\nto validate that claim using electroencephalography (EEG). That is, this paper\nattempts to find neural signatures of gestures (also referred as placeholders)\nin human cognition, which facilitate the understanding, learning and repetition\nof gestures. Further, it is discussed whether there is a direct mapping between\nthe placeholders and kinematic salient points in the gesture trajectories.\nThese are expressed as relationships between inflection points in the gestures'\ntrajectories with oscillatory mu rhythms (8-12 Hz) in the EEG. This is achieved\nby correlating fluctuations in mu power during gesture observation with salient\nmotion points found for each gesture. Peaks in the EEG signal at central\nelectrodes (motor cortex) and occipital electrodes (visual cortex) were used to\nisolate the salient events within each gesture. We found that a linear model\npredicting mu peaks from motion inflections fits the data well. Increases in\nEEG power were detected 380 and 500ms after inflection points at occipital and\ncentral electrodes, respectively. These results suggest that coordinated\nactivity in visual and motor cortices is sensitive to motion trajectories\nduring gesture observation, and it is consistent with the proposal that\ninflection points operate as placeholders in gesture recognition.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 20:52:30 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Cabrera", "Maria", ""], ["Novak", "Keisha", ""], ["Foti", "Daniel", ""], ["Voyles", "Richard", ""], ["Wachs", "Juan", ""]]}, {"id": "1701.05924", "submitter": "Maria Cabrera", "authors": "Maria Cabrera, Richard Voyles, Juan Wachs", "title": "Coherency in One-Shot Gesture Recognition", "comments": "This paper was submitted to a IEEE conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User's intentions may be expressed through spontaneous gesturing, which have\nbeen seen only a few times or never before. Recognizing such gestures involves\none shot gesture learning. While most research has focused on the recognition\nof the gestures itself, recently new approaches were proposed to deal with\ngesture perception and production as part of the same problem. The framework\npresented in this work focuses on learning the process that leads to gesture\ngeneration, rather than mining the gesture's associated features. This is\nachieved using kinematic, cognitive and biomechanic characteristics of human\ninteraction. These factors enable the artificial production of realistic\ngesture samples originated from a single observation. The generated samples are\nthen used as training sets for different state-of-the-art classifiers.\nPerformance is obtained first, by observing the machines' gesture recognition\npercentages. Then, performance is computed by the human recognition from\ngestures performed by robots. Based on these two scenarios, a composite new\nmetric of coherency is proposed relating to the amount of agreement between\nthese two conditions. Experimental results provide an average recognition\nperformance of 89.2% for the trained classifiers and 92.5% for the\nparticipants. Coherency in recognition was determined at 93.6%. While this new\nmetric is not directly comparable to raw accuracy or other pure\nperformance-based standard metrics, it provides a quantifier for validating how\nrealistic the machine generated samples are and how accurate the resulting\nmimicry is.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 20:54:10 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Cabrera", "Maria", ""], ["Voyles", "Richard", ""], ["Wachs", "Juan", ""]]}, {"id": "1701.06207", "submitter": "Akash Das Sarma", "authors": "Ayush Jain, Akash Das Sarma, Aditya Parameswaran, Jennifer Widom", "title": "Understanding Workers, Developing Effective Tasks, and Enhancing\n  Marketplace Dynamics: A Study of a Large Crowdsourcing Marketplace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an experimental analysis of a dataset comprising over 27 million\nmicrotasks performed by over 70,000 workers issued to a large crowdsourcing\nmarketplace between 2012-2016. Using this data---never before analyzed in an\nacademic context---we shed light on three crucial aspects of crowdsourcing: (1)\nTask design --- helping requesters understand what constitutes an effective\ntask, and how to go about designing one; (2) Marketplace dynamics --- helping\nmarketplace administrators and designers understand the interaction between\ntasks and workers, and the corresponding marketplace load; and (3) Worker\nbehavior --- understanding worker attention spans, lifetimes, and general\nbehavior, for the improvement of the crowdsourcing ecosystem as a whole.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 19:04:27 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Jain", "Ayush", ""], ["Sarma", "Akash Das", ""], ["Parameswaran", "Aditya", ""], ["Widom", "Jennifer", ""]]}, {"id": "1701.06270", "submitter": "Xiaodong Wu", "authors": "Xiaodong Wu, Lyn Bartram, Chris Shaw", "title": "Plexus: An Interactive Visualization Tool for Analyzing Public Emotions\n  from Twitter Data", "comments": "5 Pages, Conference ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media is often used by researchers as an approach to obtaining\nreal-time data on people's activities and thoughts. Twitter, as one of the most\npopular social networking services nowadays, provides copious information\nstreams on various topics and events. Mining and analyzing Tweets enable us to\nfind public reactions and emotions to activities or objects. This paper\npresents an interactive visualization tool that identifies and visualizes\npeople's emotions on any two related topics by streaming and processing data\nfrom Twitter. The effectiveness of this visualization was evaluated and\ndemonstrated by a feasibility study with 14 participants.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 05:45:55 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 19:24:11 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Wu", "Xiaodong", ""], ["Bartram", "Lyn", ""], ["Shaw", "Chris", ""]]}, {"id": "1701.06412", "submitter": "Mihai Nadin", "authors": "Asma Naz, Regis Kopper, Ryan P. McMahan, Mihai Nadin", "title": "Emotional Qualities of VR Space", "comments": "12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emotional response a person has to a living space is predominantly\naffected by light, color and texture as space-making elements. In order to\nverify whether this phenomenon could be replicated in a simulated environment,\nwe conducted a user study in a six-sided projected immersive display that\nutilized equivalent design attributes of brightness, color and texture in order\nto assess to which extent the emotional response in a simulated environment is\naffected by the same parameters affecting real environments. Since emotional\nresponse depends upon the context, we evaluated the emotional responses of two\ngroups of users: inactive (passive) and active (performing a typical daily\nactivity). The results from the perceptual study generated data from which\ndesign principles for a virtual living space are articulated. Such a space, as\nan alternative to expensive built dwellings, could potentially support new,\nminimalist lifestyles of occupants, defined as the neo-nomads, aligned with\ntheir work experience in the digital domain through the generation of emotional\nexperiences of spaces. Data from the experiments confirmed the hypothesis that\nperceivable emotional aspects of real-world spaces could be successfully\ngenerated through simulation of design attributes in the virtual space. The\nsubjective response to the virtual space was consistent with corresponding\nresponses from real-world color and brightness emotional perception. Our data\ncould serve the virtual reality (VR) community in its attempt to conceive of\nfurther applications of virtual spaces for well-defined activities.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 23:58:54 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Naz", "Asma", ""], ["Kopper", "Regis", ""], ["McMahan", "Ryan P.", ""], ["Nadin", "Mihai", ""]]}, {"id": "1701.06718", "submitter": "Eugene Wu", "authors": "Hamed Nilforoshan and Eugene Wu", "title": "Leveraging Quality Prediction Models for Automatic Writing Feedback", "comments": "Accepted at the 12th International Conference on Web and Social Media\n  (ICWSM), 2018", "journal-ref": "ICWSM 2018", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-generated, multi-paragraph writing is pervasive and important in many\nsocial media platforms (i.e. Amazon reviews, AirBnB host profiles, etc).\nEnsuring high-quality content is important. Unfortunately, content submitted by\nusers is often not of high quality. Moreover, the characteristics that\nconstitute high quality may even vary between domains in ways that users are\nunaware of. Automated writing feedback has the potential to immediately point\nout and suggest improvements during the writing process. Most approaches,\nhowever, focus on syntax/phrasing, which is only one characteristic of\nhigh-quality content.\n  Existing research develops accurate quality prediction models. We propose\ncombining these models with model explanation techniques to identify writing\nfeatures that, if changed, will most improve the text quality. To this end, we\ndevelop a perturbation-based explanation method for a popular class of models\ncalled tree-ensembles. Furthermore, we use a weak-supervision technique to\nadapt this method to generate feedback for specific text segments in addition\nto feedback for the entire document. Our user study finds that the\nperturbation-based approach, when combined with segment-specific feedback, can\nhelp improve writing quality on Amazon (review helpfulness) and Airbnb (host\nprofile trustworthiness) by > 14% (3X improvement over recent automated\nfeedback techniques).\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 02:52:37 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 19:25:59 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Nilforoshan", "Hamed", ""], ["Wu", "Eugene", ""]]}, {"id": "1701.06745", "submitter": "EPTCS", "authors": "Serge Autexier, Pedro Quaresma", "title": "Proceedings of the 12th Workshop on User Interfaces for Theorem Provers", "comments": null, "journal-ref": "EPTCS 239, 2017", "doi": "10.4204/EPTCS.239", "report-no": null, "categories": "cs.HC cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UITP workshop series brings together researchers interested in designing,\ndeveloping and evaluating user interfaces for automated reasoning tools, such\nas interactive proof assistants, automated theorem provers, model finders,\ntools for formal methods, and tools for visualising and manipulating logical\nformulas and proofs. The twelth edition of UITP took place in Coimbra,\nPortugal, and was part of the International Joint Conference on Automated\nReasoning (IJCAR'16). The workshop consisted of an invited talk, six\npresentations of submitted papers and lively hands-on session for reasoning\ntools and their user-interface. These post-proceedings contain four contributed\npapers accepted for publication after a second round of reviewing after the\nworkshop as well as the invited paper.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 06:26:01 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Autexier", "Serge", ""], ["Quaresma", "Pedro", ""]]}, {"id": "1701.07083", "submitter": "Tim Althoff", "authors": "Tim Althoff and Eric Horvitz and Ryen W. White and Jamie Zeitzer", "title": "Harnessing the Web for Population-Scale Physiological Sensing: A Case\n  Study of Sleep and Performance", "comments": "Published in Proceedings of WWW 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.IR q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human cognitive performance is critical to productivity, learning, and\naccident avoidance. Cognitive performance varies throughout each day and is in\npart driven by intrinsic, near 24-hour circadian rhythms. Prior research on the\nimpact of sleep and circadian rhythms on cognitive performance has typically\nbeen restricted to small-scale laboratory-based studies that do not capture the\nvariability of real-world conditions, such as environmental factors,\nmotivation, and sleep patterns in real-world settings. Given these limitations,\nleading sleep researchers have called for larger in situ monitoring of sleep\nand performance. We present the largest study to date on the impact of\nobjectively measured real-world sleep on performance enabled through a\nreframing of everyday interactions with a web search engine as a series of\nperformance tasks. Our analysis includes 3 million nights of sleep and 75\nmillion interaction tasks. We measure cognitive performance through the speed\nof keystroke and click interactions on a web search engine and correlate them\nto wearable device-defined sleep measures over time. We demonstrate that\nreal-world performance varies throughout the day and is influenced by both\ncircadian rhythms, chronotype (morning/evening preference), and prior sleep\nduration and timing. We develop a statistical model that operationalizes a\nlarge body of work on sleep and performance and demonstrates that our estimates\nof circadian rhythms, homeostatic sleep drive, and sleep inertia align with\nexpectations from laboratory-based sleep studies. Further, we quantify the\nimpact of insufficient sleep on real-world performance and show that two\nconsecutive nights with less than six hours of sleep are associated with\ndecreases in performance which last for a period of six days. This work\ndemonstrates the feasibility of using online interactions for large-scale\nphysiological sensing.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 19:49:43 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 00:07:28 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Althoff", "Tim", ""], ["Horvitz", "Eric", ""], ["White", "Ryen W.", ""], ["Zeitzer", "Jamie", ""]]}, {"id": "1701.07124", "submitter": "EPTCS", "authors": "Sylvain Conchon (LRI, Universit\\'e Paris-Sud), Mohamed Iguernlala\n  (OCamlPro SAS), Alain Mebsout (The University of Iowa)", "title": "AltGr-Ergo, a Graphical User Interface for the SMT Solver Alt-Ergo", "comments": "In Proceedings UITP 2016, arXiv:1701.06745", "journal-ref": "EPTCS 239, 2017, pp. 1-13", "doi": "10.4204/EPTCS.239.1", "report-no": null, "categories": "cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to undecidability and complexity of first-order logic, SMT solvers may\nnot terminate on some problems or require a very long time. When this happens,\none would like to find the reasons why the solver fails. To this end, we have\ndesigned AltGr-Ergo, an interactive graphical interface for the SMT solver\nAlt-Ergo which allows users and tool developers to help the solver finish some\nproofs. AltGr-Ergo gives real time feedback in order to evaluate and quantify\nprogress made by the solver, and also offers various syntactic manipulation\noptions to allow a finer grained interaction with Alt-Ergo. This paper\ndescribes these features and their implementation, and gives usage scenarios\nfor most of them.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:20:59 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Conchon", "Sylvain", "", "LRI, Universit\u00e9 Paris-Sud"], ["Iguernlala", "Mohamed", "", "OCamlPro SAS"], ["Mebsout", "Alain", "", "The University of Iowa"]]}, {"id": "1701.07125", "submitter": "EPTCS", "authors": "Emilio Jes\\'us Gallego Arias (MINES ParisTech, PSL Research\n  University, France), Beno\\^it Pin (MINES ParisTech, PSL Research University,\n  France), Pierre Jouvelot (MINES ParisTech, PSL Research University, France)", "title": "jsCoq: Towards Hybrid Theorem Proving Interfaces", "comments": "In Proceedings UITP 2016, arXiv:1701.06745", "journal-ref": "EPTCS 239, 2017, pp. 15-27", "doi": "10.4204/EPTCS.239.2", "report-no": null, "categories": "cs.PL cs.HC cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe jsCcoq, a new platform and user environment for the Coq\ninteractive proof assistant. The jsCoq system targets the HTML5-ECMAScript 2015\nspecification, and it is typically run inside a standards-compliant browser,\nwithout the need of external servers or services. Targeting educational use,\njsCoq allows the user to start interaction with proof scripts right away,\nthanks to its self-contained nature. Indeed, a full Coq environment is packed\nalong the proof scripts, easing distribution and installation. Starting to use\njsCoq is as easy as clicking on a link. The current release ships more than 10\npopular Coq libraries, and supports popular books such as Software Foundations\nor Certified Programming with Dependent Types. The new target platform has\nopened up new interaction and display possibilities. It has also fostered the\ndevelopment of some new Coq-related technology. In particular, we have\nimplemented a new serialization-based protocol for interaction with the proof\nassistant, as well as a new package format for library distribution.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 01:21:14 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Arias", "Emilio Jes\u00fas Gallego", "", "MINES ParisTech, PSL Research\n  University, France"], ["Pin", "Beno\u00eet", "", "MINES ParisTech, PSL Research University,\n  France"], ["Jouvelot", "Pierre", "", "MINES ParisTech, PSL Research University, France"]]}, {"id": "1701.07166", "submitter": "Shaowei Wang", "authors": "Shaowei Wang, Liusheng Huang, Pengzhan Wang, Hongli Xu, Wei Yang", "title": "Personalized Classifier Ensemble Pruning Framework for Mobile\n  Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning has been widely employed by mobile applications, ranging\nfrom environmental sensing to activity recognitions. One of the fundamental\nissue in ensemble learning is the trade-off between classification accuracy and\ncomputational costs, which is the goal of ensemble pruning. During\ncrowdsourcing, the centralized aggregator releases ensemble learning models to\na large number of mobile participants for task evaluation or as the\ncrowdsourcing learning results, while different participants may seek for\ndifferent levels of the accuracy-cost trade-off. However, most of existing\nensemble pruning approaches consider only one identical level of such\ntrade-off. In this study, we present an efficient ensemble pruning framework\nfor personalized accuracy-cost trade-offs via multi-objective optimization.\nSpecifically, for the commonly used linear-combination style of the trade-off,\nwe provide an objective-mixture optimization to further reduce the number of\nensemble candidates. Experimental results show that our framework is highly\nefficient for personalized ensemble pruning, and achieves much better pruning\nperformance with objective-mixture optimization when compared to state-of-art\napproaches.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 05:22:35 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Wang", "Shaowei", ""], ["Huang", "Liusheng", ""], ["Wang", "Pengzhan", ""], ["Xu", "Hongli", ""], ["Yang", "Wei", ""]]}, {"id": "1701.07213", "submitter": "David H\\\"ubner", "authors": "D H\\\"ubner, T Verhoeven, K Schmid, K-R M\\\"uller, M Tangermann, P-J\n  Kindermans", "title": "Learning from Label Proportions in Brain-Computer Interfaces: Online\n  Unsupervised Learning with Guarantees", "comments": "The EEG data of 13 subjects is freely available online at:\n  http://doi.org/10.5281/zenodo.192684", "journal-ref": null, "doi": "10.1371/journal.pone.0175856", "report-no": null, "categories": "stat.ML cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Using traditional approaches, a Brain-Computer Interface (BCI)\nrequires the collection of calibration data for new subjects prior to online\nuse. Calibration time can be reduced or eliminated e.g.~by transfer of a\npre-trained classifier or unsupervised adaptive classification methods which\nlearn from scratch and adapt over time. While such heuristics work well in\npractice, none of them can provide theoretical guarantees. Our objective is to\nmodify an event-related potential (ERP) paradigm to work in unison with the\nmachine learning decoder to achieve a reliable calibration-less decoding with a\nguarantee to recover the true class means.\n  Method: We introduce learning from label proportions (LLP) to the BCI\ncommunity as a new unsupervised, and easy-to-implement classification approach\nfor ERP-based BCIs. The LLP estimates the mean target and non-target responses\nbased on known proportions of these two classes in different groups of the\ndata. We modified a visual ERP speller to meet the requirements of the LLP. For\nevaluation, we ran simulations on artificially created data sets and conducted\nan online BCI study with N=13 subjects performing a copy-spelling task.\n  Results: Theoretical considerations show that LLP is guaranteed to minimize\nthe loss function similarly to a corresponding supervised classifier. It\nperformed well in simulations and in the online application, where 84.5% of\ncharacters were spelled correctly on average without prior calibration.\n  Significance: The continuously adapting LLP classifier is the first\nunsupervised decoder for ERP BCIs guaranteed to find the true class means. This\nmakes it an ideal solution to avoid a tedious calibration and to tackle\nnon-stationarities in the data. Additionally, LLP works on complementary\nprinciples compared to existing unsupervised methods, allowing for their\nfurther enhancement when combined with LLP.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 09:09:08 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["H\u00fcbner", "D", ""], ["Verhoeven", "T", ""], ["Schmid", "K", ""], ["M\u00fcller", "K-R", ""], ["Tangermann", "M", ""], ["Kindermans", "P-J", ""]]}, {"id": "1701.07290", "submitter": "Giuseppe Santucci", "authors": "Enrico Bertini and Giuseppe Santucci", "title": "Modelling internet based applications for designing multi-device\n  adaptive interfaces", "comments": "Keywords: adaptive interfaces, multi-device and multi-channel\n  applications", "journal-ref": "Proceedings of the Workshop on Advanced Visual Interfaces AVI\n  2004, Pages 252-256 Working Conference on Advanced Visual Interfaces, AVI\n  2004; Gallipoli; Italy; 25 May 2004 through 28 May 2004", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide spread of mobile devices in the consumer market has posed a number\nof new issues in the design of internet applications and their user interfaces.\nIn particular, applications need to adapt their interaction modalities to\ndifferent portable devices. In this paper we address the problem of defining\nmodels and techniques for designing internet based applications that\nautomatically adapt to different mobile devices. First, we define a formal\nmodel that allows for specifying the interaction in a way that is abstract\nenough to be decoupled from the presentation layer, which is to be adapted to\ndifferent contexts. The model is mainly based on the idea of describing the\nuser interaction in terms of elementary actions. Then, we provide a formal\ndevice characterization showing how to effectively implements the AIUs in a\nmultidevice context.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 12:55:45 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Bertini", "Enrico", ""], ["Santucci", "Giuseppe", ""]]}, {"id": "1701.07381", "submitter": "Daniel Sonntag", "authors": "Daniel Sonntag and Martin Huber and Manuel M\\\"oller and Alassane\n  Ndiaye and Sonja Zillner and Alexander Cavallaro", "title": "Design and Implementation of a Semantic Dialogue System for Radiologists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter describes a semantic dialogue system for radiologists in a\ncomprehensive case study within the large-scale MEDICO project. MEDICO\naddresses the need for advanced semantic technologies in the search for medical\nimage and patient data. The objectives are, first, to enable a seamless\nintegration of medical images and different user applications by providing\ndirect access to image semantics, and second, to design and implement a\nmultimodal dialogue shell for the radiologist. Speech-based semantic image\nretrieval and annotation of medical images should provide the basis for help in\nclinical decision support and computer aided diagnosis. We will describe the\nclinical workflow and interaction requirements and focus on the design and\nimplementation of a multimodal user interface for patient/image search or\nannotation and its implementation while using a speech-based dialogue shell.\nOntology modeling provides the backbone for knowledge representation in the\ndialogue shell and the specific medical application domain; ontology structures\nare the communication basis of our combined semantic search and retrieval\narchitecture which includes the MEDICO server, the triple store, the semantic\nsearch API, the medical visualization toolkit MITK, and the speech-based\ndialogue shell, amongst others. We will focus on usability aspects of\nmultimodal applications, our storyboard and the implemented speech and\ntouchscreen interaction design.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 17:16:55 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Sonntag", "Daniel", ""], ["Huber", "Martin", ""], ["M\u00f6ller", "Manuel", ""], ["Ndiaye", "Alassane", ""], ["Zillner", "Sonja", ""], ["Cavallaro", "Alexander", ""]]}, {"id": "1701.07765", "submitter": "Chris Norval", "authors": "Chris Norval, Tristan Henderson", "title": "Contextual Consent: Ethical Mining of Social Media for Health Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media are a rich source of insight for data mining and user-centred\nresearch, but the question of consent arises when studying such data without\nthe express knowledge of the creator. Case studies that mine social data from\nusers of online services such as Facebook and Twitter are becoming increasingly\ncommon. This has led to calls for an open discussion into how researchers can\nbest use these vast resources to make innovative findings while still\nrespecting fundamental ethical principles. In this position paper we highlight\nsome key considerations for this topic and argue that the conditions of\ninformed consent are often not being met, and that using social media data that\nsome deem free to access and analyse may result in undesirable consequences,\nparticularly within the domain of health research and other sensitive topics.\nWe posit that successful exploitation of online personal data, particularly for\nhealth and other sensitive research, requires new and usable methods of\nobtaining consent from the user.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 16:32:47 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Norval", "Chris", ""], ["Henderson", "Tristan", ""]]}, {"id": "1701.08269", "submitter": "Rui Liu", "authors": "Rui Liu, Xiaoli Zhang", "title": "Systems of natural-language-facilitated human-robot cooperation: A\n  review", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural-language-facilitated human-robot cooperation (NLC), in which natural\nlanguage (NL) is used to share knowledge between a human and a robot for\nconducting intuitive human-robot cooperation (HRC), is continuously developing\nin the recent decade. Currently, NLC is used in several robotic domains such as\nmanufacturing, daily assistance and health caregiving. It is necessary to\nsummarize current NLC-based robotic systems and discuss the future developing\ntrends, providing helpful information for future NLC research. In this review,\nwe first analyzed the driving forces behind the NLC research. Regarding to a\nrobot s cognition level during the cooperation, the NLC implementations then\nwere categorized into four types {NL-based control, NL-based robot training,\nNL-based task execution, NL-based social companion} for comparison and\ndiscussion. Last based on our perspective and comprehensive paper review, the\nfuture research trends were discussed.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 08:32:35 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 20:27:17 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Liu", "Rui", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "1701.08308", "submitter": "Christos Perentis", "authors": "Christos Perentis, Michele Vescovi, Chiara Leonardi, Corrado Moiso,\n  Mirco Musolesi, Fabio Pianesi, Bruno Lepri", "title": "Anonymous or not? Understanding the Factors Affecting Personal Mobile\n  Data Disclosure", "comments": "19 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide adoption of mobile devices and social media platforms have\ndramatically increased the collection and sharing of personal information. More\nand more frequently, users are called to take decisions concerning the\ndisclosure of their personal information. In this study, we investigate the\nfactors affecting users' choices toward the disclosure of their personal data,\nincluding not only their demographic and self-reported individual\ncharacteristics, but also their social interactions and their mobility patterns\ninferred from months of mobile phone data activity. We report the findings of a\nfield-study conducted with a community of 63 subjects provided with (i) a\nsmart-phone and (ii) a Personal Data Store (PDS) enabling them to control the\ndisclosure of their data. We monitor the sharing behavior of our participants\nthrough the PDS, and evaluate the contribution of different factors affecting\ntheir disclosing choices of location and social interaction data. Our analysis\nshows that social interaction inferred by mobile phones is an important factor\nrevealing willingness to share, regardless of the data type. In addition, we\nprovide further insights on the individual traits relevant to the prediction of\nsharing behavior.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 18:00:54 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Perentis", "Christos", ""], ["Vescovi", "Michele", ""], ["Leonardi", "Chiara", ""], ["Moiso", "Corrado", ""], ["Musolesi", "Mirco", ""], ["Pianesi", "Fabio", ""], ["Lepri", "Bruno", ""]]}, {"id": "1701.08465", "submitter": "EPTCS", "authors": "Camille Fayollas (ICS-IRIT, University of Toulouse, Toulouse, France),\n  C\\'elia Martinie (ICS-IRIT, University of Toulouse, Toulouse, France),\n  Philippe Palanque (ICS-IRIT, University of Toulouse, Toulouse, France), Paolo\n  Masci (HASLab/INESC TEC and Universidade do Minho, Braga, Portugal), Michael\n  D. Harrison (Newcastle University, Newcastle upon Tyne, United Kingdom),\n  Jos\\'e C. Campos (HASLab/INESC TEC and Universidade do Minho, Braga,\n  Portugal), Saulo Rodrigues e Silva (HASLab/INESC TEC and Universidade do\n  Minho, Braga, Portugal)", "title": "Evaluation of Formal IDEs for Human-Machine Interface Design and\n  Analysis: The Case of CIRCUS and PVSio-web", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 1-19", "doi": "10.4204/EPTCS.240.1", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical human-machine interfaces are present in many systems including\navionics systems and medical devices. Use error is a concern in these systems\nboth in terms of hardware panels and input devices, and the software that\ndrives the interfaces. Guaranteeing safe usability, in terms of buttons, knobs\nand displays is now a key element in the overall safety of the system. New\nintegrated development environments (IDEs) based on formal methods technologies\nhave been developed by the research community to support the design and\nanalysis of high-confidence human-machine interfaces. To date, little work has\nfocused on the comparison of these particular types of formal IDEs. This paper\ncompares and evaluates two state-of-the-art toolkits: CIRCUS, a model-based\ndevelopment and analysis tool based on Petri net extensions, and PVSio-web, a\nprototyping toolkit based on the PVS theorem proving system.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:32:07 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Fayollas", "Camille", "", "ICS-IRIT, University of Toulouse, Toulouse, France"], ["Martinie", "C\u00e9lia", "", "ICS-IRIT, University of Toulouse, Toulouse, France"], ["Palanque", "Philippe", "", "ICS-IRIT, University of Toulouse, Toulouse, France"], ["Masci", "Paolo", "", "HASLab/INESC TEC and Universidade do Minho, Braga, Portugal"], ["Harrison", "Michael D.", "", "Newcastle University, Newcastle upon Tyne, United Kingdom"], ["Campos", "Jos\u00e9 C.", "", "HASLab/INESC TEC and Universidade do Minho, Braga,\n  Portugal"], ["Silva", "Saulo Rodrigues e", "", "HASLab/INESC TEC and Universidade do\n  Minho, Braga, Portugal"]]}, {"id": "1701.08469", "submitter": "EPTCS", "authors": "Stefan Mitsch (Computer Science Department, Carnegie Mellon\n  University), Andr\\'e Platzer (Computer Science Department, Carnegie Mellon\n  University)", "title": "The KeYmaera X Proof IDE - Concepts on Usability in Hybrid Systems\n  Theorem Proving", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 67-81", "doi": "10.4204/EPTCS.240.5", "report-no": null, "categories": "cs.LO cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid systems verification is quite important for developing correct\ncontrollers for physical systems, but is also challenging. Verification\nengineers, thus, need to be empowered with ways of guiding hybrid systems\nverification while receiving as much help from automation as possible. Due to\nundecidability, verification tools need sufficient means for intervening during\nthe verification and need to allow verification engineers to provide system\ndesign insights.\n  This paper presents the design ideas behind the user interface for the hybrid\nsystems theorem prover KeYmaera X. We discuss how they make it easier to prove\nhybrid systems as well as help learn how to conduct proofs in the first place.\nUnsurprisingly, the most difficult user interface challenges come from the\ndesire to integrate automation and human guidance. We also share thoughts how\nthe success of such a user interface design could be evaluated and anecdotal\nobservations about it.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:33:24 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Mitsch", "Stefan", "", "Computer Science Department, Carnegie Mellon\n  University"], ["Platzer", "Andr\u00e9", "", "Computer Science Department, Carnegie Mellon\n  University"]]}, {"id": "1701.08470", "submitter": "EPTCS", "authors": "Lilian Burdy (Clearsy System Engineering), David D\\'eharbe (Clearsy\n  System Engineering), \\'Etienne Prun (Clearsy System Engineering)", "title": "Interfacing Automatic Proof Agents in Atelier B: Introducing \"iapa\"", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 82-90", "doi": "10.4204/EPTCS.240.6", "report-no": null, "categories": "cs.SE cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of automatic theorem provers to discharge proof obligations\nis necessary to apply formal methods in an efficient manner. Tools supporting\nformal methods, such as Atelier~B, generate proof obligations fully\nautomatically. Consequently, such proof obligations are often cluttered with\ninformation that is irrelevant to establish their validity.\n  We present iapa, an \"Interface to Automatic Proof Agents\", a new tool that is\nbeing integrated to Atelier~B, through which the user will access proof\nobligations, apply operations to simplify these proof obligations, and then\ndispatch the resulting, simplified, proof obligations to a portfolio of\nautomatic theorem provers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:33:45 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Burdy", "Lilian", "", "Clearsy System Engineering"], ["D\u00e9harbe", "David", "", "Clearsy\n  System Engineering"], ["Prun", "\u00c9tienne", "", "Clearsy System Engineering"]]}, {"id": "1701.08471", "submitter": "EPTCS", "authors": "Frank Hilken (University of Bremen), Martin Gogolla (University of\n  Bremen)", "title": "User Assistance Characteristics of the USE Model Checking Tool", "comments": "In Proceedings F-IDE 2016, arXiv:1701.07925", "journal-ref": "EPTCS 240, 2017, pp. 91-97", "doi": "10.4204/EPTCS.240.7", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Unified Modeling Language (UML) is a widely used general purpose modeling\nlanguage. Together with the Object Constraint Language (OCL), formal models can\nbe described by defining the structure and behavior with UML and additional OCL\nconstraints. In the development process for formal models, it is important to\nmake sure that these models are (a) correct, i.e. consistent and complete, and\n(b) testable in the sense that the developer is able to interactively check\nmodel properties. The USE tool (UML-based Specification Environment) allows\nboth characteristics to be studied. We demonstrate how the tool supports\nmodelers to analyze, validate and verify UML and OCL models via the use of\nseveral graphical means that assist the modeler in interpreting and visualizing\nformal model descriptions. In particular, we discuss how the so-called USE\nmodel validator plugin is integrated into the USE environment in order to allow\nnon domain experts to use it and construct object models that help to verify\nproperties like model consistency.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 03:33:58 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Hilken", "Frank", "", "University of Bremen"], ["Gogolla", "Martin", "", "University of\n  Bremen"]]}, {"id": "1701.08756", "submitter": "Rui Liu", "authors": "Rui Liu, Xiaoli Zhang", "title": "A Review of Methodologies for Natural-Language-Facilitated Human-Robot\n  Cooperation", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural-language-facilitated human-robot cooperation (NLC) refers to using\nnatural language (NL) to facilitate interactive information sharing and task\nexecutions with a common goal constraint between robots and humans. Recently,\nNLC research has received increasing attention. Typical NLC scenarios include\nrobotic daily assistance, robotic health caregiving, intelligent manufacturing,\nautonomous navigation, and robot social accompany. However, a thorough review,\nthat can reveal latest methodologies to use NL to facilitate human-robot\ncooperation, is missing. In this review, a comprehensive summary about\nmethodologies for NLC is presented. NLC research includes three main research\nfocuses: NL instruction understanding, NL-based execution plan generation, and\nknowledge-world mapping. In-depth analyses on theoretical methods,\napplications, and model advantages and disadvantages are made. Based on our\npaper review and perspective, potential research directions of NLC are\nsummarized.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 18:59:04 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 19:20:07 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 19:52:20 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Liu", "Rui", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "1701.08879", "submitter": "Zhenyi He", "authors": "Zhenyi He, Fengyuan Zhu, Aaron Gaudette, Ken Perlin", "title": "Robotic Haptic Proxies for Collaborative Virtual Reality", "comments": "7 page, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for interaction in Virtual Reality (VR) using\nmobile robots as proxies for haptic feedback. This approach allows VR users to\nhave the experience of sharing and manipulating tangible physical objects with\nremote collaborators. Because participants do not directly observe the robotic\nproxies, the mapping between them and the virtual objects is not required to be\ndirect. In this paper, we describe our implementation, various scenarios for\ninteraction, and a preliminary user study.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 00:24:10 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["He", "Zhenyi", ""], ["Zhu", "Fengyuan", ""], ["Gaudette", "Aaron", ""], ["Perlin", "Ken", ""]]}]