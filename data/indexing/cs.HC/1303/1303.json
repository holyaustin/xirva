[{"id": "1303.2292", "submitter": "Ankit Chaudhary", "authors": "Ankit Chaudhary, J. L. Raheja, Karen Das, Sonia Raheja", "title": "Intelligent Approaches to interact with Machines using Hand Gesture\n  Recognition in Natural way: A Survey", "comments": null, "journal-ref": "IJCSES, 2011", "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand gestures recognition (HGR) is one of the main areas of research for the\nengineers, scientists and bioinformatics. HGR is the natural way of Human\nMachine interaction and today many researchers in the academia and industry are\nworking on different application to make interactions more easy, natural and\nconvenient without wearing any extra device. HGR can be applied from games\ncontrol to vision enabled robot control, from virtual reality to smart home\nsystems. In this paper we are discussing work done in the area of hand gesture\nrecognition where focus is on the intelligent approaches including soft\ncomputing based methods like artificial neural network, fuzzy logic, genetic\nalgorithms etc. The methods in the preprocessing of image for segmentation and\nhand image construction also taken into study. Most researchers used fingertips\nfor hand detection in appearance based modeling. Finally the comparison of\nresults given by different researchers is also presented.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2013 08:29:48 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Chaudhary", "Ankit", ""], ["Raheja", "J. L.", ""], ["Das", "Karen", ""], ["Raheja", "Sonia", ""]]}, {"id": "1303.3134", "submitter": "Vincent Buso", "authors": "Hugo Boujut (LaBRI), Vincent Buso (LaBRI), Guillaume Bourmaud (IMS),\n  Jenny Benois-Pineau (LaBRI), R\\'emi M\\'egret (IMS), Jean-Philippe Domenger\n  (LaBRI), Yann Ga\\\"estel (ISPED), Jean-Fran\\c{c}ois Dartigues", "title": "Egocentric vision IT technologies for Alzheimer disease assessment and\n  studies", "comments": "RITS - Recherche en Imagerie et Technologies pour la Sant\\'e, France\n  (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Egocentric vision technology consists in capturing the actions of persons\nfrom their own visual point of view using wearable camera sensors. We apply\nthis new paradigm to instrumental activities monitoring with the objective of\nproviding new tools for the clinical evaluation of the impact of the disease on\npersons with dementia. In this paper, we introduce the current state of the\ndevelopment of this technology and focus on two technology modules: automatic\nlocation estimation and visual saliency estimation for content interpretation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 10:58:38 GMT"}], "update_date": "2013-03-14", "authors_parsed": [["Boujut", "Hugo", "", "LaBRI"], ["Buso", "Vincent", "", "LaBRI"], ["Bourmaud", "Guillaume", "", "IMS"], ["Benois-Pineau", "Jenny", "", "LaBRI"], ["M\u00e9gret", "R\u00e9mi", "", "IMS"], ["Domenger", "Jean-Philippe", "", "LaBRI"], ["Ga\u00ebstel", "Yann", "", "ISPED"], ["Dartigues", "Jean-Fran\u00e7ois", ""]]}, {"id": "1303.3948", "submitter": "Urmila Shrawankar Ms", "authors": "Urmila Shrawankar, Vilas Thakare", "title": "An Adaptive Methodology for Ubiquitous ASR System", "comments": "10 Pages, 05 Tables, 03 Figures", "journal-ref": "Computer and Information Science;Vol.6,No.1;2013 ISSN 1913-8989\n  E-ISSN 1913-8997 Computer and Information Science; Vol. 6, No. 1; 2013, ISSN\n  1913-8989 E-ISSN 1913-8997, Published by Canadian Center of Science and\n  Education", "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.SD", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Achieving and maintaining the performance of ubiquitous (Automatic Speech\nRecognition) ASR system is a real challenge. The main objective of this work is\nto develop a method that will improve and show the consistency in performance\nof ubiquitous ASR system for real world noisy environment. An adaptive\nmethodology has been developed to achieve an objective with the help of\nimplementing followings, -Cleaning speech signal as much as possible while\npreserving originality / intangibility using various modified filters and\nenhancement techniques. -Extracting features from speech signals using various\nsizes of parameter. -Train the system for ubiquitous environment using\nmulti-environmental adaptation training methods. -Optimize the word recognition\nrate with appropriate variable size of parameters using fuzzy technique. The\nconsistency in performance is tested using standard noise databases as well as\nin real world environment. A good improvement is noticed. This work will be\nhelpful to give discriminative training of ubiquitous ASR system for better\nHuman Computer Interaction (HCI) using Speech User Interface (SUI).\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2013 05:35:39 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Shrawankar", "Urmila", ""], ["Thakare", "Vilas", ""]]}, {"id": "1303.4293", "submitter": "Tobias Kuhn", "authors": "Kaarel Kaljurand and Tobias Kuhn", "title": "A Multilingual Semantic Wiki Based on Attempto Controlled English and\n  Grammatical Framework", "comments": "To appear in the Proceedings of the 10th Extended Semantic Web\n  Conference (ESWC 2013)", "journal-ref": null, "doi": "10.1007/978-3-642-38288-8_29", "report-no": "LNCS 7882", "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a semantic wiki system with an underlying controlled natural\nlanguage grammar implemented in Grammatical Framework (GF). The grammar\nrestricts the wiki content to a well-defined subset of Attempto Controlled\nEnglish (ACE), and facilitates a precise bidirectional automatic translation\nbetween ACE and language fragments of a number of other natural languages,\nmaking the wiki content accessible multilingually. Additionally, our approach\nallows for automatic translation into the Web Ontology Language (OWL), which\nenables automatic reasoning over the wiki content. The developed wiki\nenvironment thus allows users to build, query and view OWL knowledge bases via\na user-friendly multilingual natural language interface. As a further feature,\nthe underlying multilingual grammar is integrated into the wiki and can be\ncollaboratively edited to extend the vocabulary of the wiki or even customize\nits sentence structures. This work demonstrates the combination of the existing\ntechnologies of Attempto Controlled English and Grammatical Framework, and is\nimplemented as an extension of the existing semantic wiki engine AceWiki.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 08:10:39 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Kaljurand", "Kaarel", ""], ["Kuhn", "Tobias", ""]]}, {"id": "1303.4893", "submitter": "Marko Horvat", "authors": "Marko Horvat, Sini\\v{s}a Popovi\\'c, Kre\\v{s}imir \\'Cosi\\'c", "title": "Multimedia stimuli databases usage patterns: a survey report", "comments": "5 pages, 2 figures", "journal-ref": "Proceedings of the 36nd International ICT Convention MIPRO 2013,\n  pp. 993-997, 2013", "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia documents such as images, sounds or videos can be used to elicit\nemotional responses in exposed human subjects. These stimuli are stored in\naffective multimedia databases and successfully used for a wide variety of\nresearch in affective computing, human-computer interaction and cognitive\nsciences. Affective multimedia databases are simple repositories of multimedia\ndocuments with annotated high-level semantics and affective content. Although\nimportant all affective multimedia databases have numerous deficiencies which\nimpair their applicability. To establish a better understanding of how experts\nuse affective multimedia databases an online survey was conducted into the\nsubject. The survey results are statistically significant and indicate that\ncontemporary databases lack stimuli with rich semantic and emotional content.\n73.33% of survey participants find the databases lacking at least some\nimportant semantic or emotion content. Most of the participants consider\nstimuli descriptions to be inadequate. Overall, 1-2h or more than 24h are\ngenerally needed to construct a single stimulation sequence. Almost 84% of the\nsurvey participants would like to use real-life videos in their research.\nExperts unequivocally recognize the need for an intelligent stimuli retrieval\napplication that would assist them in experimentation. Almost all experts agree\nsuch applications could be useful in their work.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 10:18:50 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2014 23:27:22 GMT"}], "update_date": "2014-01-14", "authors_parsed": [["Horvat", "Marko", ""], ["Popovi\u0107", "Sini\u0161a", ""], ["\u0106osi\u0107", "Kre\u0161imir", ""]]}, {"id": "1303.4949", "submitter": "Claudia Picardi", "authors": "Fabio Varesano", "title": "FreeIMU: An Open Hardware Framework for Orientation and Motion Sensing", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AR cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Orientation and Motion Sensing are widely implemented on various consumer\nproducts, such as mobile phones, tablets and cameras as they enable immediate\ninteraction with virtual information. The prototyping phase of any orientation\nand motion sensing capable device is however a quite difficult process as it\nmay involve complex hardware designing, math algorithms and programming.\n  In this paper, we present FreeIMU, an Open Hardware Framework for prototyping\norientation and motion sensing capable devices. The framework consists in a\nsmall circuit board containing various sensors and a software library, built on\ntop of the Arduino platform. Both the hardware and library are released under\nopen licences and supported by an active community allowing to be implemented\ninto research and commercial projects.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 14:25:14 GMT"}], "update_date": "2013-03-21", "authors_parsed": [["Varesano", "Fabio", ""]]}, {"id": "1303.5050", "submitter": "Bernard Yannou", "authors": "Fran\\c{c}ois Cluzel (LGI), Bernard Yannou (LGI), Markus Dihlmann (US)", "title": "Using evolutionary design to interactively sketch car silhouettes and\n  stimulate designer's creativity", "comments": null, "journal-ref": "Engineering Applications of Artificial Intelligence 25 (2012)\n  1413-1424", "doi": "10.1016/j.engappai.2012.02.011", "report-no": null, "categories": "cs.NE cs.HC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Interactive Genetic Algorithm is proposed to progressively sketch the\ndesired side-view of a car profile. It adopts a Fourier decomposition of a 2D\nprofile as the genotype, and proposes a cross-over mechanism. In addition, a\nformula function of two genes' discrepancies is fitted to the perceived\ndissimilarity between two car profiles. This similarity index is intensively\nused, throughout a series of user tests, to highlight the added value of the\nIGA compared to a systematic car shape exploration, to prove its ability to\ncreate superior satisfactory designs and to stimulate designer's creativity.\nThese tests have involved six designers with a design goal defined by a\nsemantic attribute. The results reveal that if \"friendly\" is diversely\ninterpreted in terms of car shapes, \"sportive\" denotes a very conventional\nrepresentation which may be a limitation for shape renewal.\n", "versions": [{"version": "v1", "created": "Sun, 17 Mar 2013 19:33:57 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2013 07:13:14 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Cluzel", "Fran\u00e7ois", "", "LGI"], ["Yannou", "Bernard", "", "LGI"], ["Dihlmann", "Markus", "", "US"]]}, {"id": "1303.6021", "submitter": "Conrad Sanderson", "authors": "Andres Sanin, Conrad Sanderson, Mehrtash T. Harandi, Brian C. Lovell", "title": "Spatio-Temporal Covariance Descriptors for Action and Gesture\n  Recognition", "comments": null, "journal-ref": "IEEE Workshop on Applications of Computer Vision, pp. 103-110,\n  2013", "doi": "10.1109/WACV.2013.6475006", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new action and gesture recognition method based on\nspatio-temporal covariance descriptors and a weighted Riemannian locality\npreserving projection approach that takes into account the curved space formed\nby the descriptors. The weighted projection is then exploited during boosting\nto create a final multiclass classification algorithm that employs the most\nuseful spatio-temporal regions. We also show how the descriptors can be\ncomputed quickly through the use of integral video representations. Experiments\non the UCF sport, CK+ facial expression and Cambridge hand gesture datasets\nindicate superior performance of the proposed method compared to several recent\nstate-of-the-art techniques. The proposed method is robust and does not require\nadditional processing of the videos, such as foreground detection,\ninterest-point detection or tracking.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 03:16:08 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Sanin", "Andres", ""], ["Sanderson", "Conrad", ""], ["Harandi", "Mehrtash T.", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1303.6372", "submitter": "Sears Merritt", "authors": "Sears Merritt and Abigail Z. Jacobs and Winter Mason and Aaron Clauset", "title": "Detecting Friendship Within Dynamic Online Interaction Networks", "comments": "To Appear at the 7th International AAAI Conference on Weblogs and\n  Social Media (ICWSM '13), 11 pages, 1 table, 6 figures", "journal-ref": "Proc. of the 7th International AAAI Conference on Weblogs and\n  Social Media (ICWSM), 380 - 389 (2013)", "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many complex social systems, the timing and frequency of interactions\nbetween individuals are observable but friendship ties are hidden. Recovering\nthese hidden ties, particularly for casual users who are relatively less\nactive, would enable a wide variety of friendship-aware applications in domains\nwhere labeled data are often unavailable, including online advertising and\nnational security. Here, we investigate the accuracy of multiple statistical\nfeatures, based either purely on temporal interaction patterns or on the\ncooperative nature of the interactions, for automatically extracting latent\nsocial ties. Using self-reported friendship and non-friendship labels derived\nfrom an anonymous online survey, we learn highly accurate predictors for\nrecovering hidden friendships within a massive online data set encompassing 18\nbillion interactions among 17 million individuals of the popular online game\nHalo: Reach. We find that the accuracy of many features improves as more data\naccumulates, and cooperative features are generally reliable. However,\nperiodicities in interaction time series are sufficient to correctly classify\n95% of ties, even for casual users. These results clarify the nature of\nfriendship in online social environments and suggest new opportunities and new\nprivacy concerns for friendship-aware applications that do not require the\ndisclosure of private friendship information.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 02:58:02 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Merritt", "Sears", ""], ["Jacobs", "Abigail Z.", ""], ["Mason", "Winter", ""], ["Clauset", "Aaron", ""]]}]