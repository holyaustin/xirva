[{"id": "1902.00040", "submitter": "David Melhart", "authors": "David Melhart, Ahmad Azadvar, Alessandro Canossa, Antonios Liapis,\n  Georgios N. Yannakakis", "title": "Your Gameplay Says It All: Modelling Motivation in Tom Clancy's The\n  Division", "comments": "Version accepted for IEEE Conference on Games, 2019", "journal-ref": "Proceedings of the 2019 International IEEE Conference on Games\n  (CoG 2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to predict the motivation of players just by observing their\ngameplay data? Even if so, how should we measure motivation in the first place?\nTo address the above questions, on the one end, we collect a large dataset of\ngameplay data from players of the popular game Tom Clancy's The Division. On\nthe other end, we ask them to report their levels of competence, autonomy,\nrelatedness and presence using the Ubisoft Perceived Experience Questionnaire.\nAfter processing the survey responses in an ordinal fashion we employ\npreference learning methods based on support vector machines to infer the\nmapping between gameplay and the reported four motivation factors. Our key\nfindings suggest that gameplay features are strong predictors of player\nmotivation as the best obtained models reach accuracies of near certainty, from\n92% up to 94% on unseen players.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 19:15:04 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 09:04:06 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Melhart", "David", ""], ["Azadvar", "Ahmad", ""], ["Canossa", "Alessandro", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "1902.00098", "submitter": "Emily Dinan", "authors": "Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller,\n  Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan\n  Lowe, Shrimai Prabhumoye, Alan W Black, Alexander Rudnicky, Jason Williams,\n  Joelle Pineau, Mikhail Burtsev, Jason Weston", "title": "The Second Conversational Intelligence Challenge (ConvAI2)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the setting and results of the ConvAI2 NeurIPS competition that\naims to further the state-of-the-art in open-domain chatbots. Some key\ntakeaways from the competition are: (i) pretrained Transformer variants are\ncurrently the best performing models on this task, (ii) but to improve\nperformance on multi-turn conversations with humans, future systems must go\nbeyond single word metrics like perplexity to measure the performance across\nsequences of utterances (conversations) -- in terms of repetition, consistency\nand balance of dialogue acts (e.g. how many questions asked vs. answered).\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 22:14:34 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Dinan", "Emily", ""], ["Logacheva", "Varvara", ""], ["Malykh", "Valentin", ""], ["Miller", "Alexander", ""], ["Shuster", "Kurt", ""], ["Urbanek", "Jack", ""], ["Kiela", "Douwe", ""], ["Szlam", "Arthur", ""], ["Serban", "Iulian", ""], ["Lowe", "Ryan", ""], ["Prabhumoye", "Shrimai", ""], ["Black", "Alan W", ""], ["Rudnicky", "Alexander", ""], ["Williams", "Jason", ""], ["Pineau", "Joelle", ""], ["Burtsev", "Mikhail", ""], ["Weston", "Jason", ""]]}, {"id": "1902.00157", "submitter": "Ulrik Lyngs", "authors": "Ulrik Lyngs, Kai Lukoff, Petr Slovak, Reuben Binns, Adam Slack,\n  Michael Inzlicht, Max Van Kleek, Nigel Shadbolt", "title": "Self-Control in Cyberspace: Applying Dual Systems Theory to a Review of\n  Digital Self-Control Tools", "comments": "11.5 pages (excl. references), 6 figures, 1 table", "journal-ref": "CHI Conference on Human Factors in Computing Systems Proceedings\n  (CHI 2019), May 4-9, 2019, Glasgow, Scotland UK. ACM, New York, NY, USA", "doi": "10.1145/3290605.3300361", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many people struggle to control their use of digital devices. However, our\nunderstanding of the design mechanisms that support user self-control remains\nlimited. In this paper, we make two contributions to HCI research in this\nspace: first, we analyse 367 apps and browser extensions from the Google Play,\nChrome Web, and Apple App stores to identify common core design features and\nintervention strategies afforded by current tools for digital self-control.\nSecond, we adapt and apply an integrative dual systems model of self-regulation\nas a framework for organising and evaluating the design features found. Our\nanalysis aims to help the design of better tools in two ways: (i) by\nidentifying how, through a well-established model of self-regulation, current\ntools overlap and differ in how they support self-control; and (ii) by using\nthe model to reveal underexplored cognitive mechanisms that could aid the\ndesign of new tools.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 02:59:20 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Lyngs", "Ulrik", ""], ["Lukoff", "Kai", ""], ["Slovak", "Petr", ""], ["Binns", "Reuben", ""], ["Slack", "Adam", ""], ["Inzlicht", "Michael", ""], ["Van Kleek", "Max", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "1902.00570", "submitter": "Atta Norouzian", "authors": "Atta Norouzian, Bogdan Mazoure, Dermot Connolly and Daniel Willett", "title": "Exploring attention mechanism for acoustic-based classification of\n  speech utterances into system-directed and non-system-directed", "comments": "Accpeted for presentation at ICASSP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice controlled virtual assistants (VAs) are now available in smartphones,\ncars, and standalone devices in homes. In most cases, the user needs to first\n\"wake-up\" the VA by saying a particular word/phrase every time he or she wants\nthe VA to do something. Eliminating the need for saying the wake-up word for\nevery interaction could improve the user experience. This would require the VA\nto have the capability to detect the speech that is being directed at it and\nrespond accordingly. In other words, the challenge is to distinguish between\nsystem-directed and non-system-directed speech utterances. In this paper, we\npresent a number of neural network architectures for tackling this\nclassification problem based on using only acoustic features. These\narchitectures are based on using convolutional, recurrent and feed-forward\nlayers. In addition, we investigate the use of an attention mechanism applied\nto the output of the convolutional and the recurrent layers. It is shown that\nincorporating the proposed attention mechanism into the models always leads to\nsignificant improvement in classification accuracy. The best model achieved\nequal error rates of 16.25 and 15.62 percents on two distinct realistic\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 21:48:45 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Norouzian", "Atta", ""], ["Mazoure", "Bogdan", ""], ["Connolly", "Dermot", ""], ["Willett", "Daniel", ""]]}, {"id": "1902.00607", "submitter": "Eunji Chong", "authors": "Eunji Chong, Katha Chanda, Zhefan Ye, Audrey Southerland, Nataniel\n  Ruiz, Rebecca M. Jones, Agata Rozga, James M. Rehg", "title": "Detecting Gaze Towards Eyes in Natural Social Interactions and its Use\n  in Child Assessment", "comments": "Published in Proceedings of the ACM on Interactive, Mobile, Wearable\n  and Ubiquitous Technologies (IMWUT) Volume 1. Winner of IMWUT Distinguished\n  Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye contact is a crucial element of non-verbal communication that signifies\ninterest, attention, and participation in social interactions. As a result,\nmeasures of eye contact arise in a variety of applications such as the\nassessment of the social communication skills of children at risk for\ndevelopmental disorders such as autism, or the analysis of turn-taking and\nsocial roles during group meetings. However, the automated measurement of\nvisual attention during naturalistic social interactions is challenging due to\nthe difficulty of estimating a subject's looking direction from video. This\npaper proposes a novel approach to eye contact detection during adult-child\nsocial interactions in which the adult wears a point-of-view camera which\ncaptures an egocentric view of the child's behavior. By analyzing the child's\nface regions and inferring their head pose we can accurately identify the onset\nand duration of the child's looks to their social partner's eyes. We introduce\nthe Pose-Implicit CNN, a novel deep learning architecture that predicts eye\ncontact while implicitly estimating the head pose. We present a fully automated\nsystem for eye contact detection that solves the sub-problems of end-to-end\nfeature learning and pose estimation using deep neural networks. To train our\nmodels, we use a dataset comprising 22 hours of 156 play session videos from\nover 100 children, half of whom are diagnosed with Autism Spectrum Disorder. We\nreport an overall precision of 0.76, recall of 0.80, and an area under the\nprecision-recall curve of 0.79, all of which are significant improvements over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 01:13:31 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Chong", "Eunji", ""], ["Chanda", "Katha", ""], ["Ye", "Zhefan", ""], ["Southerland", "Audrey", ""], ["Ruiz", "Nataniel", ""], ["Jones", "Rebecca M.", ""], ["Rozga", "Agata", ""], ["Rehg", "James M.", ""]]}, {"id": "1902.00680", "submitter": "Charles Martin", "authors": "Charles P Martin and Jim Torresen", "title": "Data Driven Analysis of Tiny Touchscreen Performance with MicroJam", "comments": null, "journal-ref": "Computer Music Journal, 43(4), 41-57 (2020)", "doi": "10.1162/comj_a_00536", "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread adoption of mobile devices, such as smartphones and tablets,\nhas made touchscreens a common interface for musical performance. New mobile\nmusical instruments have been designed that embrace collaborative creation and\nthat explore the affordances of mobile devices, as well as their constraints.\nWhile these have been investigated from design and user experience\nperspectives, there is little examination of the performers' musical outputs.\nIn this work, we introduce a constrained touchscreen performance app, MicroJam,\ndesigned to enable collaboration between performers, and engage in a novel\ndata-driven analysis of more than 1600 performances using the app. MicroJam\nconstrains performances to five seconds, and emphasises frequent and casual\nmusic making through a social media-inspired interface. Performers collaborate\nby replying to performances, adding new musical layers that are played back at\nthe same time. Our analysis shows that users tend to focus on the centre and\ndiagonals of the touchscreen area, and tend to swirl or swipe rather than tap.\nWe also observe that while long swipes dominate the visual appearance of\nperformances, the majority of interactions are short with limited expressive\npossibilities. Our findings are summarised into a set of design recommendations\nfor MicroJam and other touchscreen apps for social musical interaction.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 09:35:37 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Martin", "Charles P", ""], ["Torresen", "Jim", ""]]}, {"id": "1902.00719", "submitter": "Miguel Alonso Jr", "authors": "Miguel Alonso Jr", "title": "Learning User Preferences via Reinforcement Learning with Spatial\n  Interface Valuing", "comments": "Submitted to HCI International 2019 Parallel Session on Spatial\n  Interaction for Universal Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive Machine Learning is concerned with creating systems that operate\nin environments alongside humans to achieve a task. A typical use is to extend\nor amplify the capabilities of a human in cognitive or physical ways, requiring\nthe machine to adapt to the users' intentions and preferences. Often, this\ntakes the form of a human operator providing some type of feedback to the user,\nwhich can be explicit feedback, implicit feedback, or a combination of both.\nExplicit feedback, such as through a mouse click, carries a high cognitive\nload. The focus of this study is to extend the current state of the art in\ninteractive machine learning by demonstrating that agents can learn a human\nuser's behavior and adapt to preferences with a reduced amount of explicit\nhuman feedback in a mixed feedback setting. The learning agent perceives a\nvalue of its own behavior from hand gestures given via a spatial interface.\nThis feedback mechanism is termed Spatial Interface Valuing. This method is\nevaluated experimentally in a simulated environment for a grasping task using a\nrobotic arm with variable grip settings. Preliminary results indicate that\nlearning agents using spatial interface valuing can learn a value function\nmapping spatial gestures to expected future rewards much more quickly as\ncompared to those same agents just receiving explicit feedback, demonstrating\nthat an agent perceiving feedback from a human user via a spatial interface can\nserve as an effective complement to existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 13:45:20 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Alonso", "Miguel", "Jr"]]}, {"id": "1902.00757", "submitter": "Cass Dykeman", "authors": "Stephen Wong and Cass Dykeman", "title": "East Asians with Internet Addiction: Prevalence Rates and Support Use\n  Patterns", "comments": "26 pages, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The issue of Internet addiction has become a serious social and health issue\nin East Asian countries. There are only a few treatment programs for Internet\naddiction, and their effectiveness with people from East Asian remains unclear.\nAs support and treatment develop, it is necessary to understand cultural\npreferences for dealing with this concern. Using data from the East Asian\nSocial Survey (EASS), this study examined preferred sources of assistance for\nhelp with internet use problems in four countries - China, Japan, South Korea,\nand Taiwan. Preferences for kin versus non-kin support, use of alternative\nmedicine, and professional mental health assistance were examined, as were\nbetween-country differences in support preferences. The results indicate a\nstrong preference for seeking assistance from close relatives, followed by\nnon-kin support (i.e., close friends and co-participants in religious\ninstitutions), alternative medicine, and professional mental health services,\nrespectively. While there is a strong preference for family support, over 80%\nof survey respondents were open to seeking formal or informal mental health\nsupport outside the family. There were some significant differences between\ncountries, with South Koreans being more likely to seek non-kin support and\nprofessional support for internet addiction concerns compared to Chinese. These\ndifferences are discussed in the context of cultural and policy developments in\nEast Asian countries. Findings suggest the need for a more holistic approach to\ntreating low mental health concerns.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 17:34:54 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Wong", "Stephen", ""], ["Dykeman", "Cass", ""]]}, {"id": "1902.01117", "submitter": "Elena Sibirtseva", "authors": "Elena Sibirtseva, Ali Ghadirzadeh, Iolanda Leite, M{\\aa}rten\n  Bj\\\"orkman, Danica Kragic", "title": "Exploring Temporal Dependencies in Multimodal Referring Expressions with\n  Mixed Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In collaborative tasks, people rely both on verbal and non-verbal cues\nsimultaneously to communicate with each other. For human-robot interaction to\nrun smoothly and naturally, a robot should be equipped with the ability to\nrobustly disambiguate referring expressions. In this work, we propose a model\nthat can disambiguate multimodal fetching requests using modalities such as\nhead movements, hand gestures, and speech. We analysed the acquired data from\nmixed reality experiments and formulated a hypothesis that modelling temporal\ndependencies of events in these three modalities increases the model's\npredictive power. We evaluated our model on a Bayesian framework to interpret\nreferring expressions with and without exploiting a temporal prior.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 10:44:50 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Sibirtseva", "Elena", ""], ["Ghadirzadeh", "Ali", ""], ["Leite", "Iolanda", ""], ["Bj\u00f6rkman", "M\u00e5rten", ""], ["Kragic", "Danica", ""]]}, {"id": "1902.01320", "submitter": "Vasanth Sarathy", "authors": "Vasanth Sarathy, Thomas Arnold, Matthias Scheutz", "title": "When Exceptions are the Norm: Exploring the Role of Consent in HRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HRI researchers have made major strides in developing robotic architectures\nthat are capable of reading a limited set of social cues and producing\nbehaviors that enhance their likeability and feeling of comfort amongst humans.\nHowever, the cues in these models are fairly direct and the interactions\nlargely dyadic. To capture the normative qualities of interaction more\nrobustly, we propose consent as a distinct, critical area for HRI research.\nConvening important insights in existing HRI work around topics like touch,\nproxemics, gaze, and moral norms, the notion of consent reveals key\nexpectations that can shape how a robot acts in social space. By sorting\nvarious kinds of consent through social and legal doctrine, we delineate\nempirical and technical questions to meet consent challenges faced in major\napplication domains and robotic roles. Attention to consent could show, for\nexample, how extraordinary, norm-violating actions can be justified by agents\nand accepted by those around them. We argue that operationalizing ideas from\nlegal scholarship can better guide how robotic systems might cultivate and\nsustain proper forms of consent.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 17:17:12 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Sarathy", "Vasanth", ""], ["Arnold", "Thomas", ""], ["Scheutz", "Matthias", ""]]}, {"id": "1902.01348", "submitter": "Michael Ekstrand", "authors": "Michael D. Ekstrand and Joseph A. Konstan", "title": "Recommender Systems Notation: Proposed Common Notation for Teaching and\n  Research", "comments": null, "journal-ref": "Boise State University Computer Science Faculty Publications and\n  Presentations 177", "doi": "10.18122/cs_facpubs/177/boisestate", "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the field of recommender systems has developed, authors have used a myriad\nof notations for describing the mathematical workings of recommendation\nalgorithms. These notations ap-pear in research papers, books, lecture notes,\nblog posts, and software documentation. The dis-ciplinary diversity of the\nfield has not contributed to consistency in notation; scholars whose home base\nis in information retrieval have different habits and expectations than those\nin ma-chine learning or human-computer interaction.\n  In the course of years of teaching and research on recommender systems, we\nhave seen the val-ue in adopting a consistent notation across our work. This\nhas been particularly highlighted in our development of the Recommender Systems\nMOOC on Coursera (Konstan et al. 2015), as we need to explain a wide variety of\nalgorithms and our learners are not well-served by changing notation between\nalgorithms.\n  In this paper, we describe the notation we have adopted in our work, along\nwith its justification and some discussion of considered alternatives. We\npresent this in hope that it will be useful to others writing and teaching\nabout recommender systems. This notation has served us well for some time now,\nin research, online education, and traditional classroom instruction. We feel\nit is ready for broad use.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:11:23 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Ekstrand", "Michael D.", ""], ["Konstan", "Joseph A.", ""]]}, {"id": "1902.01564", "submitter": "Benjamin Renoust", "authors": "Benjamin Renoust, Haolin Ren, Guy Melan\\c{c}on", "title": "Animated Drag and Drop Interaction for Dynamic Multidimensional Graphs", "comments": "2 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new drag and drop interaction technique for\ngraphs. We designed this interaction to support analysis in complex\nmultidimensional and temporal graphs. The drag and drop interaction is enhanced\nwith an intuitive and controllable animation, in support of comparison tasks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 06:07:49 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Renoust", "Benjamin", ""], ["Ren", "Haolin", ""], ["Melan\u00e7on", "Guy", ""]]}, {"id": "1902.01721", "submitter": "Nadia Boukhelifa", "authors": "Nadia Boukhelifa, Anastasia Bezerianos, Ioan Cristian Trelea, Nathalie\n  Mejean Perrot, Evelyne Lutton", "title": "An Exploratory Study on Visual Exploration of Model Simulations by\n  Multiple Types of Experts", "comments": "This paper will be published in Proceedings of the ACM Conference on\n  Human Factors in Computing Systems (CHI 2019)", "journal-ref": null, "doi": "10.1145/3290605.3300874", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experts in different domains rely increasingly on simulation models of\ncomplex processes to reach insights, make decisions, and plan future projects.\nThese models are often used to study possible trade-offs, as experts try to\noptimise multiple conflicting objectives in a single investigation.\nUnderstanding all the model intricacies, however, is challenging for a single\ndomain expert. We propose a simple approach to support multiple experts when\nexploring complex model results. First, we reduce the model exploration space,\nthen present the results on a shared interactive surface, in the form of a\nscatterplot matrix and linked views. To explore how multiple experts analyse\ntrade-offs using this setup, we carried out an observational study focusing on\nthe link between expertise and insight generation during the analysis process.\nOur results reveal the different exploration strategies and multi-storyline\napproaches that domain experts adopt during trade-off analysis, and inform our\nrecommendations for collaborative model exploration systems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 14:53:42 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Boukhelifa", "Nadia", ""], ["Bezerianos", "Anastasia", ""], ["Trelea", "Ioan Cristian", ""], ["Perrot", "Nathalie Mejean", ""], ["Lutton", "Evelyne", ""]]}, {"id": "1902.01790", "submitter": "Fabio Crestani Prof.", "authors": "Fabio Crestani, Stefano Mizzaro, Ivan Scagnetto", "title": "Mobile Information Retrieval", "comments": "116 pages, published in 2017", "journal-ref": null, "doi": "10.1007/978-3-319-60777-1", "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Information Retrieval (Mobile IR) is a relatively recent branch of\nInformation Retrieval (IR) that is concerned with enabling users to carry out,\nusing a mobile device, all the classical IR operations that they were used to\ncarry out on a desktop. This includes finding content available on local\nrepositories or on the web in response to a user query, interacting with the\nsystem in an explicit or implicit way, reformulate the query and/or visualise\nthe content of the retrieved documents, as well as providing relevance\njudgments to improve the retrieval process.\n  This book is structured as follows. Chapter 2 provides a very brief overview\nof IR and of Mobile IR, briefly outlining what in Mobile IR is different from\nIR. Chapter 3 provides the foundations of Mobile IR, looking at the\ncharacteristics of mobile devices and what they bring to IR, but also looking\nat how the concept of relevance changed from standard IR to Mobile IR. Chapter\n4 presents an overview of the document collections that are searchable by a\nMobile IR system, and that are somehow different from classical IR ones;\navailable for experimentation, including collections of data that have become\ncomplementary to Mobile IR. Similarly, Chapter 5 reviews mobile information\nneeds studies and users log analysis. Chapter 6 reviews studies aimed at\nadapting and improving the users interface to the needs of Mobile IR. Chapter\n7, instead, reviews work on context awareness, which studies the many aspects\nof the user context that Mobile IR employs. Chapter 8 reviews some of\nevaluation work done in Mobile IR, highlighting the distinctions with classical\nIR from the perspectives of two main IR evaluation methodologies: users studies\nand test collections. Finally, Chapter 9 reports the conclusions of this\nreview, highlighting briefly some trends in Mobile IR that we believe will\ndrive research in the next few years.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 17:00:08 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Crestani", "Fabio", ""], ["Mizzaro", "Stefano", ""], ["Scagnetto", "Ivan", ""]]}, {"id": "1902.01799", "submitter": "Seyedroohollah Hosseini", "authors": "Seyedroohollah Hosseini, Xuan Guo", "title": "Deep Convolutional Neural Network for Automated Detection of Mind\n  Wandering using EEG Signals", "comments": "4 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mind wandering (MW) is a ubiquitous phenomenon which reflects a shift in\nattention from task-related to task-unrelated thoughts. There is a need for\nintelligent interfaces that can reorient attention when MW is detected due to\nits detrimental effects on performance and productivity. In this paper, we\npropose a deep learning model for MW detection using Electroencephalogram (EEG)\nsignals. Specifically, we develop a channel-wise deep convolutional neural\nnetwork (CNN) model to classify the features of focusing state and MW extracted\nfrom EEG signals. This is the first study that employs CNN to automatically\ndetect MW using only EEG data. The experimental results on the collected\ndataset demonstrate promising performance with 91.78% accuracy, 92.84%\nsensitivity, and 90.73% specificity.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 17:13:06 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Hosseini", "Seyedroohollah", ""], ["Guo", "Xuan", ""]]}, {"id": "1902.01800", "submitter": "Ginevra Castellano", "authors": "Patricia Alves-Oliveira, Pedro Sequeira, Francisco S. Melo, Ginevra\n  Castellano, Ana Paiva", "title": "Empathic Robot for Group Learning: A Field Study", "comments": "ACM Transactions on Human-Robot Interaction, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work explores a group learning scenario with an autonomous empathic\nrobot. We address two research questions: (1) Can an autonomous robot designed\nwith empathic competencies foster collaborative learning in a group context?\n(2) Can an empathic robot sustain positive educational outcomes in long-term\ncollaborative learning interactions with groups of students? To answer these\nquestions, we developed an autonomous robot with empathic competencies that is\nable to interact with a group of students in a learning activity about\nsustainable development. Two studies were conducted. The first study compares\nlearning outcomes in children across 3 conditions: learning with an empathic\nrobot; learning with a robot without empathic capabilities; and learning\nwithout a robot. The results show that the autonomous robot with empathy\nfosters meaningful discussions about sustainability, which is a learning\noutcome in sustainability education. The second study features groups of\nstudents who interact with the robot in a school classroom for two months. The\nlong-term educational interaction did not seem to provide significant learning\ngains, although there was a change in game-actions to achieve more\nsustainability during game-play. This result reflects the need to perform more\nlong-term research in the field of educational robots for group learning.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 17:15:29 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Alves-Oliveira", "Patricia", ""], ["Sequeira", "Pedro", ""], ["Melo", "Francisco S.", ""], ["Castellano", "Ginevra", ""], ["Paiva", "Ana", ""]]}, {"id": "1902.01827", "submitter": "David Glowacki", "authors": "Michael O'Connor, Simon J. Bennie, Helen M. Deeks, Alexander\n  Jamieson-Binnie, Alex J. Jones, Robin J. Shannon, Rebecca Walters, Thomas J.\n  Mitchell, Adrian J. Mulholland, and David R. Glowacki", "title": "Interactive molecular dynamics in virtual reality from quantum chemistry\n  to drug binding: An open-source multi-person framework", "comments": null, "journal-ref": null, "doi": "10.1063/1.5092590", "report-no": null, "categories": "physics.chem-ph cs.HC physics.bio-ph physics.ed-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As molecular scientists have made progress in their ability to engineer\nnano-scale molecular structure, we are facing new challenges in our ability to\nengineer molecular dynamics (MD) and flexibility. Dynamics at the molecular\nscale differs from the familiar mechanics of everyday objects, because it\ninvolves a complicated, highly correlated, and three-dimensional many-body\ndynamical choreography which is often non-intuitive even for highly trained\nresearchers. We recently described how interactive molecular dynamics in\nvirtual reality (iMD-VR) can help to meet this challenge, enabling researchers\nto manipulate real-time MD simulations of flexible structures in 3D. In this\narticle, we outline efforts to extend immersive technologies to the molecular\nsciences, and we introduce 'Narupa', a flexible, open-source, multi-person\niMD-VR software framework which enables groups of researchers to simultaneously\ncohabit real-time simulation environments to interactively visualize and\nmanipulate the dynamics of molecular structures with atomic-level precision. We\noutline several application domains where iMD-VR is facilitating research,\ncommunication, and creative approaches within the molecular sciences, including\ntraining machines to learn reactive potential energy surfaces (PESs),\nbiomolecular conformational sampling, protein-ligand binding, reaction\ndiscovery using 'on-the-fly' quantum chemistry, and transport dynamics in\nmaterials. We touch on iMD-VR's various cognitive and perceptual affordances,\nand how these provide research insight for molecular systems. By\nsynergistically combining human spatial reasoning and design insight with\ncomputational automation, technologies like iMD-VR have the potential to\nimprove our ability to understand, engineer, and communicate microscopic\ndynamical behavior, offering the potential to usher in a new paradigm for\nengineering molecules and nano-architectures.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 17:57:58 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 14:38:37 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 16:51:12 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["O'Connor", "Michael", ""], ["Bennie", "Simon J.", ""], ["Deeks", "Helen M.", ""], ["Jamieson-Binnie", "Alexander", ""], ["Jones", "Alex J.", ""], ["Shannon", "Robin J.", ""], ["Walters", "Rebecca", ""], ["Mitchell", "Thomas J.", ""], ["Mulholland", "Adrian J.", ""], ["Glowacki", "David R.", ""]]}, {"id": "1902.02635", "submitter": "Jun Zhao Dr", "authors": "Ge Wang, Jun Zhao and Nigel Shadbolt", "title": "Are Children Fully Aware of Online Privacy Risks and How Can We Improve\n  Their Coping Ability?", "comments": "9 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1901.10245", "journal-ref": null, "doi": null, "report-no": "KOALA.03", "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The age of children adopting digital technologies, such as tablets or\nsmartphones, is increasingly young. However, children under 11 are often\nregarded as too young to comprehend the concept of online privacy. Limited\nresearch studies have focused on children of this age group. In the summer of\n2018, we conducted 12 focus group studies with 29 children aged 6-10 from\nOxfordshire primary schools. Our research has shown that children have a good\nunderstanding of certain privacy risks, such as information oversharing or\navoiding revealing real identities online. They could use a range of\ndescriptions to articulate the risks and describe their risk coping strategies.\nHowever, at the same time, we identified that children had less awareness\nconcerning other risks, such as online tracking or game promotions.\n  Inspired by Vygotsky's Zone of Proximal Development (ZPD), this study has\nidentified critical knowledge gaps in children's understanding of online\nprivacy, and several directions for future education and technology\ndevelopment. We call for attention to the needs of raising children's awareness\nand understanding of risks related to online recommendations and data tracking,\nwhich are becoming ever more prevalent in the games and content children\nencounter. We also call for attention to children's use of language to describe\nrisks, which may be appropriate but not necessarily indicate a full\nunderstanding of the threats.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 10:09:50 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Wang", "Ge", ""], ["Zhao", "Jun", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "1902.02757", "submitter": "M. Hanefi Calp", "authors": "M. Hanefi Calp, M. Ali Akcayol", "title": "The importance of human computer interaction in the development process\n  of software projects", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, software industry has a rapid growth. In order to resist the\ncompetition increased by this growth, software projects need to be developed\nwith higher quality and especially user friendly. Therefore, the importance of\nhuman-computer interaction emerges clearly. In design and development phases of\nsoftware projects, the properties of human which is an important agent for\ninteraction -- such as behavioral, cognitive, perceptive, efficiency and\nphysical factors have to be considered. This study aims to express the\nimportance of developing softwares by taking into consideration the\nhuman-computer interaction applications. In this context, firstly a wide\nliterature review is made to examine software development process and\nhuman-computer interaction in detail, the results obtained by using design\nmethods in this process are explicated and the importance of said interaction\nis openly expressed with the exemplary applications in the literature.\nAccording to the results of the research, especially in software life cycle, it\nis observed that rules of interaction must be implemented before software\ndevelopment, however, these methods are usually included in software life cycle\nin the latter stages of software development process. This situation causes the\ndeveloped softwares to be user unfriendly and of low quality. Furthermore, it\nis observed that when the design methods used in the scope of human-computer\ninteraction are integrated into software development process during the life\ncycle, the developed projects are more successful, have better quality and are\nmore user friendly.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:28:59 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Calp", "M. Hanefi", ""], ["Akcayol", "M. Ali", ""]]}, {"id": "1902.02842", "submitter": "Jomara Sandbulte", "authors": "Jomara Sandbulte and Jessica Kropczynski and John M. Carroll", "title": "Community Animation: Exploring a design space that leverages geosocial\n  networking to increase community engagement", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores a design study of a smartphone enabled meet-up app meant\nto inspire engagement in community innovation. Community hubs such as\nco-working spaces, incubators, and maker spaces attract community members with\ndiverse interests. This paper presents these spaces as a design opportunity for\nan application that helps host community-centered meet-ups in smart and\nconnected communities. Our design study explores three scenarios of use,\ninspired by previous literature, for organizing meet-ups and compares them by\nsurveying potential users. Based on the results of our survey, we propose\nseveral design implications and implement them in the Community Animator\ngeosocial networking application, which identifies nearby individuals that are\nwilling to chat or perform community-centered activities. We present the\nresults of both our survey and our prototype, discuss our design goals, and\nprovide design implications for civic-minded, geosocial networking\napplications. Our contribution in this work is the development process,\nproposed design of a mobile application to support community-centered meet-ups,\nand insights for future work.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 20:57:03 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Sandbulte", "Jomara", ""], ["Kropczynski", "Jessica", ""], ["Carroll", "John M.", ""]]}, {"id": "1902.02865", "submitter": "Matteo Varvello", "authors": "Matteo Varvello, Jeremy Blackburn, David Naylor, Kostantina\n  Papagiannaki", "title": "EYEORG: A Platform For Crowdsourcing Web Quality Of Experience\n  Measurements", "comments": "14 pages, CONEXT2016", "journal-ref": null, "doi": "10.1145/2999572.2999590", "report-no": null, "categories": "cs.NI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tremendous effort has gone into the ongoing battle to make webpages load\nfaster. This effort has culminated in new protocols (QUIC, SPDY, and HTTP/2) as\nwell as novel content delivery mechanisms. In addition, companies like Google\nand SpeedCurve investigated how to measure \"page load time\" (PLT) in a way that\ncaptures human perception. In this paper we present Eyeorg, a platform for\ncrowdsourcing web quality of experience measurements. Eyeorg overcomes the\nscaling and automation challenges of recruiting users and collecting consistent\nuser-perceived quality measurements. We validate Eyeorg's capabilities via a\nset of 100 trusted participants. Next, we showcase its functionalities via\nthree measurement campaigns, each involving 1,000 paid participants, to 1)\nstudy the quality of several PLT metrics, 2) compare HTTP/1.1 and HTTP/2\nperformance, and 3) assess the impact of online advertisements and ad blockers\non user experience. We find that commonly used, and even novel and\nsophisticated PLT metrics fail to represent actual human perception of PLT,\nthat the performance gains from HTTP/2 are imperceivable in some circumstances,\nand that not all ad blockers are created equal.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 22:03:31 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Varvello", "Matteo", ""], ["Blackburn", "Jeremy", ""], ["Naylor", "David", ""], ["Papagiannaki", "Kostantina", ""]]}, {"id": "1902.02960", "submitter": "Carrie Cai", "authors": "Carrie J. Cai, Emily Reif, Narayan Hegde, Jason Hipp, Been Kim, Daniel\n  Smilkov, Martin Wattenberg, Fernanda Viegas, Greg S. Corrado, Martin C.\n  Stumpe, Michael Terry", "title": "Human-Centered Tools for Coping with Imperfect Algorithms during Medical\n  Decision-Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) is increasingly being used in image retrieval systems\nfor medical decision making. One application of ML is to retrieve visually\nsimilar medical images from past patients (e.g. tissue from biopsies) to\nreference when making a medical decision with a new patient. However, no\nalgorithm can perfectly capture an expert's ideal notion of similarity for\nevery case: an image that is algorithmically determined to be similar may not\nbe medically relevant to a doctor's specific diagnostic needs. In this paper,\nwe identified the needs of pathologists when searching for similar images\nretrieved using a deep learning algorithm, and developed tools that empower\nusers to cope with the search algorithm on-the-fly, communicating what types of\nsimilarity are most important at different moments in time. In two evaluations\nwith pathologists, we found that these refinement tools increased the\ndiagnostic utility of images found and increased user trust in the algorithm.\nThe tools were preferred over a traditional interface, without a loss in\ndiagnostic accuracy. We also observed that users adopted new strategies when\nusing refinement tools, re-purposing them to test and understand the underlying\nalgorithm and to disambiguate ML errors from their own errors. Taken together,\nthese findings inform future human-ML collaborative systems for expert\ndecision-making.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 07:18:34 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Cai", "Carrie J.", ""], ["Reif", "Emily", ""], ["Hegde", "Narayan", ""], ["Hipp", "Jason", ""], ["Kim", "Been", ""], ["Smilkov", "Daniel", ""], ["Wattenberg", "Martin", ""], ["Viegas", "Fernanda", ""], ["Corrado", "Greg S.", ""], ["Stumpe", "Martin C.", ""], ["Terry", "Michael", ""]]}, {"id": "1902.02996", "submitter": "Charles-Alexandre Delestage", "authors": "Willy Yvart (DeVisu), Charles-Alexandre Delestage, Sylvie\n  Leleu-Merviel (DeVisu)", "title": "SYM: Toward a New Tool in User's Mood Determination", "comments": null, "journal-ref": "EmoVis 2016, ACM IUI 2016 Workshop on Emotion and Visualization,\n  Mar 2016, Sonoma, United States. pp.23-28, 2016, Proceedings of EmoVis 2016", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though the emotional state is increasingly taken into account in\nscientific studies aimed at determining user experience of user acceptance,\nthere are still only a few normalized tools. In this article, we decided to\nfocus on mood determination as we consider this affective state to be more\npervasive and more understandable by the person who is experiencing it. Thus,\nwe propose a prototypical tool called SYM (Spot Your Mood) as a new tool in\nuser mood determination to be used in many different situations.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 09:53:53 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Yvart", "Willy", "", "DeVisu"], ["Delestage", "Charles-Alexandre", "", "DeVisu"], ["Leleu-Merviel", "Sylvie", "", "DeVisu"]]}, {"id": "1902.03043", "submitter": "Ross Harper", "authors": "Ross Harper and Joshua Southern", "title": "A Bayesian Deep Learning Framework for End-To-End Prediction of Emotion\n  from Heartbeat", "comments": "8 pages, 2 tables", "journal-ref": null, "doi": "10.1109/TAFFC.2020.2981610", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic prediction of emotion promises to revolutionise human-computer\ninteraction. Recent trends involve fusion of multiple data modalities - audio,\nvisual, and physiological - to classify emotional state. However, in practice,\ncollection of physiological data `in the wild' is currently limited to\nheartbeat time series of the kind generated by affordable wearable heart\nmonitors. Furthermore, real-world applications of emotion prediction often\nrequire some measure of uncertainty over model output, in order to inform\ndownstream decision-making. We present here an end-to-end deep learning model\nfor classifying emotional valence from unimodal heartbeat time series. We\nfurther propose a Bayesian framework for modelling uncertainty over these\nvalence predictions, and describe a probabilistic procedure for choosing to\naccept or reject model output according to the intended application. We\nbenchmarked our framework against two established datasets and achieved peak\nclassification accuracy of 90%. These results lay the foundation for\napplications of affective computing in real-world domains such as healthcare,\nwhere a high premium is placed on non-invasive collection of data, and\npredictive certainty.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 12:10:45 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2020 10:31:46 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Harper", "Ross", ""], ["Southern", "Joshua", ""]]}, {"id": "1902.03184", "submitter": "Michinari Kono", "authors": "Michinari Kono and Jun Rekimoto", "title": "wavEMS: Improving Signal Variation Freedom of Electrical Muscle\n  Stimulation", "comments": "Accepted to IEEE VR 2019 Workshop on Human Augmentation and its\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a long history in electrical muscle stimulation (EMS), which\nhas been used for medical and interaction purposes. Human-computer interaction\n(HCI) researchers are now working on various applications, including virtual\nreality (VR), notification, and learning. For the electric signals applied to\nthe human body, various types of waveforms have been considered and tested. In\ntypical applications, pulses with short duration are applied, however, many\nperspectives are required to be considered. In addition to the duration and\npolarity of the pulse/waves, the wave shapes can also be an essential factor to\nconsider. A problem of conventional EMS toolkits and systems are that they have\na limitation to the variety of signals that it can produce. For example, some\nmay be limited to monophonic pulses. Furthermore, they are usually limited to\nrectangular pulses and a limited range of frequencies, and other waveforms\ncannot be produced. These kinds of limitations make us challenging to consider\nvariations of EMS signals in HCI research and applications. The purpose of\n\"{\\it wavEMS}\" is to encourage testing of a variety of waveforms for EMS, which\ncan be manipulated through audio output. We believe that this can help improve\nHCI applications, and to open up new application areas.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 16:46:43 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Kono", "Michinari", ""], ["Rekimoto", "Jun", ""]]}, {"id": "1902.03239", "submitter": "Ben Sawyer", "authors": "B. D. Sawyer, Sean Seaman, Linda Angell, Jon Dobres, Bruce Mehler, and\n  Bryan Reimer", "title": "A Description of a Subtask Dataset with Glances", "comments": "Paper with two (2) json databases and two (2) csv data dictionaries", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a set of data made available that contains detailed\nsubtask coding of interactions with several production vehicle human machine\ninterfaces (HMIs) on open roadways, along with accompanying eyeglance data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 22:12:55 GMT"}], "update_date": "2019-08-10", "authors_parsed": [["Sawyer", "B. D.", ""], ["Seaman", "Sean", ""], ["Angell", "Linda", ""], ["Dobres", "Jon", ""], ["Mehler", "Bruce", ""], ["Reimer", "Bryan", ""]]}, {"id": "1902.03245", "submitter": "Chenhao Tan", "authors": "Brian Lubars and Chenhao Tan", "title": "Ask Not What AI Can Do, But What AI Should Do: Towards a Framework of\n  Task Delegability", "comments": "19 pages, 3 figures, 5 tables, NeurIPS 2019, dataset available at\n  https://delegability.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While artificial intelligence (AI) holds promise for addressing societal\nchallenges, issues of exactly which tasks to automate and to what extent to do\nso remain understudied. We approach this problem of task delegability from a\nhuman-centered perspective by developing a framework on human perception of\ntask delegation to AI. We consider four high-level factors that can contribute\nto a delegation decision: motivation, difficulty, risk, and trust. To obtain an\nempirical understanding of human preferences in different tasks, we build a\ndataset of 100 tasks from academic papers, popular media portrayal of AI, and\neveryday life, and administer a survey based on our proposed framework. We find\nlittle preference for full AI control and a strong preference for\nmachine-in-the-loop designs, in which humans play the leading role. Among the\nfour factors, trust is the most correlated with human preferences of optimal\nhuman-machine delegation. This framework represents a first step towards\ncharacterizing human preferences of AI automation across tasks. We hope this\nwork encourages future efforts towards understanding such individual attitudes;\nour goal is to inform the public and the AI research community rather than\ndictating any direction in technology development.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 19:00:02 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2019 18:00:00 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Lubars", "Brian", ""], ["Tan", "Chenhao", ""]]}, {"id": "1902.03322", "submitter": "Stephen Bottos", "authors": "Stephen Bottos, Balakumar Balasingam", "title": "An Approach to Track Reading Progression Using Eye-Gaze Fixation Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of tracking the eye-gaze of\nindividuals while they engage in reading. Particularly, we develop ways to\naccurately track the line being read by an individual using commercially\navailable eye tracking devices. Such an approach will enable futuristic\nfunctionalities such as comprehension evaluation, interest level detection, and\nuser-assisting applications like hands-free navigation and automatic scrolling.\nExisting commercial eye trackers provide an estimated location of the eye-gaze\nfixations every few milliseconds. However, this estimated data is found to be\nvery noisy. As such, commercial eye-trackers are unable to accurately track\nlines while reading. In this paper we propose several statistical models to\nbridge the commercial gaze tracker outputs and eye-gaze patterns while reading.\nWe then employ hidden Markov models to parametrize these statistical models and\nto accurately detect the line being read. The proposed approach is shown to\nyield an improvement of over 20% in line detection accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 23:03:15 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Bottos", "Stephen", ""], ["Balasingam", "Balakumar", ""]]}, {"id": "1902.03501", "submitter": "Dylan Slack", "authors": "Dylan Slack, Sorelle A. Friedler, Carlos Scheidegger, and Chitradeep\n  Dutta Roy", "title": "Assessing the Local Interpretability of Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing adoption of machine learning tools has led to calls for\naccountability via model interpretability. But what does it mean for a machine\nlearning model to be interpretable by humans, and how can this be assessed? We\nfocus on two definitions of interpretability that have been introduced in the\nmachine learning literature: simulatability (a user's ability to run a model on\na given input) and \"what if\" local explainability (a user's ability to\ncorrectly determine a model's prediction under local changes to the input,\ngiven knowledge of the model's original prediction). Through a user study with\n1,000 participants, we test whether humans perform well on tasks that mimic the\ndefinitions of simulatability and \"what if\" local explainability on models that\nare typically considered locally interpretable. To track the relative\ninterpretability of models, we employ a simple metric, the runtime operation\ncount on the simulatability task. We find evidence that as the number of\noperations increases, participant accuracy on the local interpretability tasks\ndecreases. In addition, this evidence is consistent with the common intuition\nthat decision trees and logistic regression models are interpretable and are\nmore interpretable than neural networks.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 21:49:36 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 23:17:38 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Slack", "Dylan", ""], ["Friedler", "Sorelle A.", ""], ["Scheidegger", "Carlos", ""], ["Roy", "Chitradeep Dutta", ""]]}, {"id": "1902.03527", "submitter": "Zhenjie Zhao", "authors": "Zhenjie Zhao", "title": "Engaging Audiences in Virtual Museums by Interactively Prompting Guiding\n  Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual museums aim to promote access to cultural artifacts. However, they\noften face the challenge of getting audiences to read and understand a large\namount of information in an uncontrolled online environment. Inspired by\nsuccessful practices in physical museums, we investigated the possible use of\nguiding questions to engage audiences in virtual museums. To this end, we first\nidentified how to construct questions that are likely to attract audiences\nthrough domain expert interviews and mining cultural-related posts in a popular\nquestion and answer community. Then in terms of the proactive level for\nattracting users' attention, we designed two mechanisms to interactively prompt\nquestions: active and passive. Through an online experiment with 150\nparticipants, we showed that having interactive guiding questions encourages\nbrowsing and improves content comprehension. We discuss reasons why they are\nuseful by conducting another qualitative comparison and obtained insights about\nthe influence of question category and interaction mechanism.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 02:24:45 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zhao", "Zhenjie", ""]]}, {"id": "1902.03529", "submitter": "Zhenjie Zhao", "authors": "Zhenjie Zhao", "title": "Live Emoji: Semantic Emotional Expressiveness of 2D Live Animation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Live animation of 2D characters has recently become a popular way for\nstorytelling, and has potential application scenarios like tele-present agents\nor robots. As an extension of human-human communication, there is a need for\naugmenting the emotional communication experience of live animation. In this\npaper, we explore the emotional expressiveness issue of 2D live animation. In\nparticular, we propose a descriptive emotion command model to bind a triggering\naction, the semantic meaning, psychology measurements, and behaviors of an\nemotional expression. Based on the model, we designed and implemented a\nproof-of-concept 2D live animation system, where a novel visual programming\ntool for editing the behaviors of 2D digital characters, and an emotion command\nrecommendation algorithm are proposed. Through a user evaluation, we showcase\nthe usability of our system and its potential for boosting creativity and\nenhancing the emotional communication experience.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 02:34:58 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 01:59:30 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Zhao", "Zhenjie", ""]]}, {"id": "1902.03541", "submitter": "Matthias G\\\"orges", "authors": "Pu Liu, Sidney Fels, Nicholas West, Matthias G\\\"orges", "title": "Human Computer Interaction Design for Mobile Devices Based on a Smart\n  Healthcare Architecture", "comments": null, "journal-ref": "In: Yurish SY, ed. Advances in Computers and Software Engineering:\n  Reviews. 2019, Volume 2, p. 99-131", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart and IoT-enabled mobile devices have the potential to enhance healthcare\nservices for both patients and healthcare providers. Human computer interaction\ndesign is key to realizing a useful and usable connection between the users and\nthese smart healthcare technologies. Appropriate design of such devices\nenhances the usability, improves effective operation in an integrated\nhealthcare system, and facilitates the collaboration and information sharing\nbetween patients, healthcare providers, and institutions. In this paper, the\nconcept of smart healthcare is introduced, including its four-layer information\narchitecture of sensing, communication, data integration, and application.\nHuman Computer Interaction design principles for smart healthcare mobile\ndevices are outlined, based on user-centered design. These include: ensuring\nsafety, providing error-resistant displays and alarms, supporting the unique\nrelationship between patients and healthcare providers, distinguishing end-user\ngroups, accommodating legacy devices, guaranteeing low latency, allowing for\npersonalization, and ensuring patient privacy. Results are synthesized in\ndesign suggestions ranging from personas, scenarios, workflow, and information\narchitecture, to prototyping, testing and iterative development. Finally,\nfuture developments in smart healthcare and Human Computer Interaction design\nfor mobile health devices are outlined.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 05:49:53 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Liu", "Pu", ""], ["Fels", "Sidney", ""], ["West", "Nicholas", ""], ["G\u00f6rges", "Matthias", ""]]}, {"id": "1902.03722", "submitter": "Vibert Thio", "authors": "Vibert Thio, Hao-Min Liu, Yin-Cheng Yeh, Yi-Hsuan Yang", "title": "A Minimal Template for Interactive Web-based Demonstrations of Musical\n  Machine Learning", "comments": "6 pages. Published in 2nd Workshop on Intelligent Music Interfaces\n  for Listening and Creation, co-located with IUI 2019, Los Angeles, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New machine learning algorithms are being developed to solve problems in\ndifferent areas, including music. Intuitive, accessible, and understandable\ndemonstrations of the newly built models could help attract the attention of\npeople from different disciplines and evoke discussions. However, we notice\nthat it has not been a common practice for researchers working on musical\nmachine learning to demonstrate their models in an interactive way. To address\nthis issue, we present in this paper an template that is specifically designed\nto demonstrate symbolic musical machine learning models on the web. The\ntemplate comes with a small codebase, is open source, and is meant to be easy\nto use by any practitioners to implement their own demonstrations. Moreover,\nits modular design facilitates the reuse of the musical components and\naccelerates the implementation. We use the template to build interactive\ndemonstrations of four exemplary music generation models. We show that the\nbuilt-in interactivity and real-time audio rendering of the browser make the\ndemonstration easier to understand and to play with. It also helps researchers\nto gain insights into different models and to A/B test them.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 04:20:20 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Thio", "Vibert", ""], ["Liu", "Hao-Min", ""], ["Yeh", "Yin-Cheng", ""], ["Yang", "Yi-Hsuan", ""]]}, {"id": "1902.04250", "submitter": "Michinari Kono", "authors": "Kohei Toyoda, Michinari Kono, Jun Rekimoto", "title": "Post-Data Augmentation to Improve Deep Pose Estimation of Extreme and\n  Wild Motions", "comments": "Accepted to IEEE VR 2019 Workshop on Human Augmentation and its\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contributions of recent deep-neural-network (DNN) based techniques have been\nplaying a significant role in human-computer interaction (HCI) and user\ninterface (UI) domains. One of the commonly used DNNs is human pose estimation.\nThis kind of technique is widely used for motion capturing of humans, and to\ngenerate or modify virtual avatars. However, in order to gain accuracy and to\nuse such systems, large and precise datasets are required for the machine\nlearning (ML) procedure. This can be especially difficult for extreme/wild\nmotions such as acrobatic movements or motions in specific sports, which are\ndifficult to estimate in typically provided training models. In addition,\ntraining may take a long duration, and will require a high-grade GPU for\nsufficient speed. To address these issues, we propose a method to improve the\npose estimation accuracy for extreme/wild motions by using pre-trained models,\ni.e., without performing the training procedure by yourselves. We assume our\nmethod to encourage usage of these DNN techniques for users in application\nareas that are out of the ML field, and to help users without high-end\ncomputers to apply them for personal and end use cases.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 06:14:01 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Toyoda", "Kohei", ""], ["Kono", "Michinari", ""], ["Rekimoto", "Jun", ""]]}, {"id": "1902.04262", "submitter": "Daniel Hienert", "authors": "Daniel Hienert, Dagmar Kern, Matthew Mitsui, Chirag Shah, Nicholas J.\n  Belkin", "title": "Reading Protocol: Understanding what has been Read in Interactive\n  Information Retrieval Tasks", "comments": "To appear in CHIIR '19 Proceedings of the 2019 ACM on Conference on\n  Human Information Interaction and Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Interactive Information Retrieval (IIR) experiments the user's gaze motion\non web pages is often recorded with eye tracking. The data is used to analyze\ngaze behavior or to identify Areas of Interest (AOI) the user has looked at. So\nfar, tools for analyzing eye tracking data have certain limitations in\nsupporting the analysis of gaze behavior in IIR experiments. Experiments often\nconsist of a huge number of different visited web pages. In existing analysis\ntools the data can only be analyzed in videos or images and AOIs for every\nsingle web page have to be specified by hand, in a very time consuming process.\nIn this work, we propose the reading protocol software which breaks eye\ntracking data down to the textual level by considering the HTML structure of\nthe web pages. This has a lot of advantages for the analyst. First and\nforemost, it can easily be identified on a large scale what has actually been\nviewed and read on the stimuli pages by the subjects. Second, the web page\nstructure can be used to filter to AOIs. Third, gaze data of multiple users can\nbe presented on the same page, and fourth, fixation times on text can be\nexported and further processed in other tools. We present the software, its\nvalidation, and example use cases with data from three existing IIR\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 07:22:17 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Hienert", "Daniel", ""], ["Kern", "Dagmar", ""], ["Mitsui", "Matthew", ""], ["Shah", "Chirag", ""], ["Belkin", "Nicholas J.", ""]]}, {"id": "1902.04393", "submitter": "Libby Ferland", "authors": "Libby Ferland and Thomas Huffstutler and Jacob Rice and Joan Zheng and\n  Shi Ni and Maria Gini", "title": "Evaluating Older Users' Experiences with Commercial Dialogue Systems:\n  Implications for Future Design and Development", "comments": "In DEEP-DIAL19 workshop at AAAI-19, Honolulu, HI, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the needs of a variety of distinct user groups is vital in\ndesigning effective, desirable dialogue systems that will be adopted by the\nlargest possible segment of the population. Despite the increasing popularity\nof dialogue systems in both mobile and home formats, user studies remain\nrelatively infrequent and often sample a segment of the user population that is\nnot representative of the needs of the potential user population as a whole.\nThis is especially the case for users who may be more reluctant adopters, such\nas older adults.\n  In this paper we discuss the results of a recent user study performed over a\nlarge population of age 50 and over adults in the Midwestern United States that\nhave experience using a variety of commercial dialogue systems. We show the\ncommon preferences, use cases, and feature gaps identified by older adult users\nin interacting with these systems. Based on these results, we propose a new,\nrobust user modeling framework that addresses common issues facing older adult\nusers, which can then be generalized to the wider user population.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 00:21:30 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Ferland", "Libby", ""], ["Huffstutler", "Thomas", ""], ["Rice", "Jacob", ""], ["Zheng", "Joan", ""], ["Ni", "Shi", ""], ["Gini", "Maria", ""]]}, {"id": "1902.04394", "submitter": "Alex B\\\"auerle", "authors": "Alex B\\\"auerle, Christian van Onzenoodt, and Timo Ropinski", "title": "Net2Vis -- A Visual Grammar for Automatically Generating\n  Publication-Tailored CNN Architecture Visualizations", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2021.3057483", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To convey neural network architectures in publications, appropriate\nvisualizations are of great importance. While most current deep learning papers\ncontain such visualizations, these are usually handcrafted just before\npublication, which results in a lack of a common visual grammar, significant\ntime investment, errors, and ambiguities. Current automatic network\nvisualization tools focus on debugging the network itself and are not ideal for\ngenerating publication visualizations. Therefore, we present an approach to\nautomate this process by translating network architectures specified in Keras\ninto visualizations that can directly be embedded into any publication. To do\nso, we propose a visual grammar for convolutional neural networks (CNNs), which\nhas been derived from an analysis of such figures extracted from all ICCV and\nCVPR papers published between 2013 and 2019. The proposed grammar incorporates\nvisual encoding, network layout, layer aggregation, and legend generation. We\nhave further realized our approach in an online system available to the\ncommunity, which we have evaluated through expert feedback, and a quantitative\nstudy. It not only reduces the time needed to generate network visualizations\nfor publications, but also enables a unified and unambiguous visualization\ndesign.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 15:13:58 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 12:36:21 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 13:28:54 GMT"}, {"version": "v4", "created": "Mon, 9 Mar 2020 12:01:15 GMT"}, {"version": "v5", "created": "Fri, 26 Jun 2020 07:10:22 GMT"}, {"version": "v6", "created": "Wed, 10 Feb 2021 09:46:51 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["B\u00e4uerle", "Alex", ""], ["van Onzenoodt", "Christian", ""], ["Ropinski", "Timo", ""]]}, {"id": "1902.04573", "submitter": "Tommy Nilsson", "authors": "Emily Shaw, Tessa Roper, Tommy Nilsson, Glyn Lawson, Sue V.G. Cobb,\n  Daniel Miller", "title": "The Heat is On: Exploring User Behaviour in a Multisensory Virtual\n  Environment for Fire Evacuation", "comments": "CHI Conference on Human Factors in Computing Systems", "journal-ref": null, "doi": "10.1145/3290605.3300856", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding validity of user behaviour in Virtual Environments (VEs) is\ncritical as they are increasingly being used for serious Health and Safety\napplications such as predicting human behaviour and training in hazardous\nsituations. This paper presents a comparative study exploring user behaviour in\nVE-based fire evacuation and investigates whether this is affected by the\naddition of thermal and olfactory simulation. Participants (N=43) were exposed\nto a virtual fire in an office building. Quantitative and qualitative analyses\nof participant attitudes and behaviours found deviations from those we would\nexpect in real life (e.g. pre-evacuation actions), but also valid behaviours\nlike fire avoidance. Potentially important differences were found between\nmultisensory and audiovisual-only conditions (e.g. perceived urgency). We\nconclude VEs have significant potential in safety-related applications, and\nthat multimodality may afford additional uses in this context, but the\nidentified limitations of behavioural validity must be carefully considered to\navoid misapplication of the technology.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 15:35:16 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Shaw", "Emily", ""], ["Roper", "Tessa", ""], ["Nilsson", "Tommy", ""], ["Lawson", "Glyn", ""], ["Cobb", "Sue V. G.", ""], ["Miller", "Daniel", ""]]}, {"id": "1902.04752", "submitter": "Huang Yanpei", "authors": "Yanpei Huang, Etienne Burdet, Lin Cao, Phuoc Thien Phan, Anthony Meng\n  Huat Tiong, and Soo Jay Phee", "title": "A Subject-Specific Four-Degree-of-Freedom Foot Interface to Control a\n  Robot Arm", "comments": "11 pages,10 figures, submit to the journal of IEEE/ASME Transactions\n  on Mechatronics with the status of under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robotic surgery, the surgeon controls robotic instruments using dedicated\ninterfaces. One critical limitation of current interfaces is that they are\ndesigned to be operated by only the hands. This means that the surgeon can only\ncontrol at most two robotic instruments at one time while many interventions\nrequire three instruments. This paper introduces a novel four-degree-of-freedom\nfoot-machine interface which allows the surgeon to control a third robotic\ninstrument using the foot, giving the surgeon a \"third hand\". This interface is\nessentially a parallel-serial hybrid mechanism with springs and force sensors.\nUnlike existing switch-based interfaces that can only un-intuitively generate\nmotion in discrete directions, this interface allows intuitive control of a\nslave robotic arm in continuous directions and speeds, naturally matching the\nfoot movements with dynamic force & position feedbacks. An experiment with ten\nnaive subjects was conducted to test the system. In view of the significant\nvariance of motion patterns between subjects, a subject-specific mapping from\nfoot movements to command outputs was developed using Independent Component\nAnalysis (ICA). Results showed that the ICA method could accurately identify\nsubjects' foot motion patterns and significantly improve the prediction\naccuracy of motion directions from 68% to 88% as compared with the forward\nkinematics-based approach. This foot-machine interface can be applied for the\nteleoperation of industrial/surgical robots independently or in coordination\nwith hands in the future.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 05:51:06 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Huang", "Yanpei", ""], ["Burdet", "Etienne", ""], ["Cao", "Lin", ""], ["Phan", "Phuoc Thien", ""], ["Tiong", "Anthony Meng Huat", ""], ["Phee", "Soo Jay", ""]]}, {"id": "1902.04861", "submitter": "Andrea Alamia", "authors": "Andrea Alamia, Victor Gauducheau, Dimitri Paisios, Rufin VanRullen", "title": "Which Neural Network Architecture matches Human Behavior in Artificial\n  Grammar Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years artificial neural networks achieved performance close to or\nbetter than humans in several domains: tasks that were previously human\nprerogatives, such as language processing, have witnessed remarkable\nimprovements in state of the art models. One advantage of this technological\nboost is to facilitate comparison between different neural networks and human\nperformance, in order to deepen our understanding of human cognition. Here, we\ninvestigate which neural network architecture (feed-forward vs. recurrent)\nmatches human behavior in artificial grammar learning, a crucial aspect of\nlanguage acquisition. Prior experimental studies proved that artificial\ngrammars can be learnt by human subjects after little exposure and often\nwithout explicit knowledge of the underlying rules. We tested four grammars\nwith different complexity levels both in humans and in feedforward and\nrecurrent networks. Our results show that both architectures can 'learn' (via\nerror back-propagation) the grammars after the same number of training\nsequences as humans do, but recurrent networks perform closer to humans than\nfeedforward ones, irrespective of the grammar complexity level. Moreover,\nsimilar to visual processing, in which feedforward and recurrent architectures\nhave been related to unconscious and conscious processes, our results suggest\nthat explicit learning is best modeled by recurrent architectures, whereas\nfeedforward networks better capture the dynamics involved in implicit learning.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 11:02:39 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Alamia", "Andrea", ""], ["Gauducheau", "Victor", ""], ["Paisios", "Dimitri", ""], ["VanRullen", "Rufin", ""]]}, {"id": "1902.04929", "submitter": "Alexander Trende", "authors": "Werner Damm, Martin Fr\\\"anzle, Andreas L\\\"udtke, Jochem W. Rieger,\n  Alexander Trende, Anirudh Unni", "title": "Integrating Neurophysiological Sensors and Driver Models for Safe and\n  Performant Automated Vehicle Control in Mixed Traffic", "comments": "8 pages, 6 Figures, submitted to HFIV'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In future mixed traffic Highly Automated Vehicles (HAV) will have to resolve\ninteractions with human operated traffic. A particular problem for HAVs is\ndetection of human states influencing safety critical decisions and driving\nbehavior of humans. We demonstrate the value proposition of neurophysiological\nsensors and driver models for optimizing performance of HAVs under safety\nconstraints in mixed traffic applications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 14:40:08 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Damm", "Werner", ""], ["Fr\u00e4nzle", "Martin", ""], ["L\u00fcdtke", "Andreas", ""], ["Rieger", "Jochem W.", ""], ["Trende", "Alexander", ""], ["Unni", "Anirudh", ""]]}, {"id": "1902.05009", "submitter": "Dongyu Liu", "authors": "Qianwen Wang, Yao Ming, Zhihua Jin, Qiaomu Shen, Dongyu Liu, Micah J.\n  Smith, Kalyan Veeramachaneni, Huamin Qu", "title": "ATMSeer: Increasing Transparency and Controllability in Automated\n  Machine Learning", "comments": "Published in the ACM Conference on Human Factors in Computing Systems\n  (CHI), 2019, Glasgow, Scotland UK", "journal-ref": "In Proceedings of the 2019 CHI Conference on Human Factors in\n  Computing Systems (CHI '19). Association for Computing Machinery, New York,\n  NY, USA, Paper 681, 1-12", "doi": "10.1145/3290605.3300911", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To relieve the pain of manually selecting machine learning algorithms and\ntuning hyperparameters, automated machine learning (AutoML) methods have been\ndeveloped to automatically search for good models. Due to the huge model search\nspace, it is impossible to try all models. Users tend to distrust automatic\nresults and increase the search budget as much as they can, thereby undermining\nthe efficiency of AutoML. To address these issues, we design and implement\nATMSeer, an interactive visualization tool that supports users in refining the\nsearch space of AutoML and analyzing the results. To guide the design of\nATMSeer, we derive a workflow of using AutoML based on interviews with machine\nlearning experts. A multi-granularity visualization is proposed to enable users\nto monitor the AutoML process, analyze the searched models, and refine the\nsearch space in real time. We demonstrate the utility and usability of ATMSeer\nthrough two case studies, expert interviews, and a user study with 13 end\nusers.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 17:03:33 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Wang", "Qianwen", ""], ["Ming", "Yao", ""], ["Jin", "Zhihua", ""], ["Shen", "Qiaomu", ""], ["Liu", "Dongyu", ""], ["Smith", "Micah J.", ""], ["Veeramachaneni", "Kalyan", ""], ["Qu", "Huamin", ""]]}, {"id": "1902.05361", "submitter": "Chun-Wei Chiang", "authors": "Anna Kasunic, Chun-Wei Chiang, Geoff Kaufman and Saiph Savage", "title": "Crowd Work on a CV? Understanding How AMT Fits into Turkers' Career\n  Goals and Professional Profiles", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2013, scholars laid out a framework for a sustainable, ethical future of\ncrowd work, recommending career ladders so that crowd work can lead to career\nadvancement and more economic mobility. Five years later, we consider this\nvision in the context of Amazon Mechanical Turk (AMT). To understand how\nworkers currently view their experiences on AMT, and how they publicly present\nand share these experiences in their professional lives, we conducted a survey\nstudy with workers on AMT (n=98). The survey we administered included a\ncombination of multiple choice, binary, and open-ended (short paragraph) items\ngauging Turkers' perceptions of their experiences on AMT within the context of\ntheir broader work experience and career goals. This work extends existing\nunderstandings of who crowd workers are and why they crowd work by seeking to\nbetter understand how crowd work factors into Turkers' professional profiles,\nand how we can subsequently better support crowd workers in their career\nadvancement. Our survey results can inform the design of better tools to\nempower crowd workers in their professional development both inside and outside\nof AMT.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 17:22:41 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Kasunic", "Anna", ""], ["Chiang", "Chun-Wei", ""], ["Kaufman", "Geoff", ""], ["Savage", "Saiph", ""]]}, {"id": "1902.05446", "submitter": "Jorge Davila-Chacon", "authors": "Jorge, Davila-Chacon and Jindong, Liu and Stefan, Wermter", "title": "Enhanced Robot Speech Recognition Using Biomimetic Binaural Sound Source\n  Localization", "comments": null, "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems (Volume:\n  30, Issue: 1, Jan. 2019)", "doi": "10.1109/TNNLS.2018.2830119", "report-no": null, "categories": "cs.SD cs.HC cs.LG cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the behavior of humans talking in noisy environments, we propose\nan embodied embedded cognition approach to improve automatic speech recognition\n(ASR) systems for robots in challenging environments, such as with ego noise,\nusing binaural sound source localization (SSL). The approach is verified by\nmeasuring the impact of SSL with a humanoid robot head on the performance of an\nASR system. More specifically, a robot orients itself toward the angle where\nthe signal-to-noise ratio (SNR) of speech is maximized for one microphone\nbefore doing an ASR task. First, a spiking neural network inspired by the\nmidbrain auditory system based on our previous work is applied to calculate the\nsound signal angle. Then, a feedforward neural network is used to handle high\nlevels of ego noise and reverberation in the signal. Finally, the sound signal\nis fed into an ASR system. For ASR, we use a system developed by our group and\ncompare its performance with and without the support from SSL. We test our SSL\nand ASR systems on two humanoid platforms with different structural and\nmaterial properties. With our approach we halve the sentence error rate with\nrespect to the common downmixing of both channels. Surprisingly, the ASR\nperformance is more than two times better when the angle between the humanoid\nhead and the sound source allows sound waves to be reflected most intensely\nfrom the pinna to the ear microphone, rather than when sound waves arrive\nperpendicularly to the membrane.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 14:09:11 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Jorge", "", ""], ["Davila-Chacon", "", ""], ["Jindong", "", ""], ["Liu", "", ""], ["Stefan", "", ""], ["Wermter", "", ""]]}, {"id": "1902.06019", "submitter": "Yao Xie", "authors": "Yao Xie, Ge Gao and Xiang 'Anthony' Chen", "title": "Outlining the Design Space of Explainable Intelligent Systems for\n  Medical Diagnosis", "comments": "6 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adoption of intelligent systems creates opportunities as well as\nchallenges for medical work. On the positive side, intelligent systems have the\npotential to compute complex data from patients and generate automated\ndiagnosis recommendations for doctors. However, medical professionals often\nperceive such systems as black boxes and, therefore, feel concerned about\nrelying on system generated results to make decisions. In this paper, we\ncontribute to the ongoing discussion of explainable artificial intelligence\n(XAI) by exploring the concept of explanation from a human-centered\nperspective. We hypothesize that medical professionals would perceive a system\nas explainable if the system was designed to think and act like doctors. We\nreport a preliminary interview study that collected six medical professionals'\nreflection of how they interact with data for diagnosis and treatment purposes.\nOur data reveals when and how doctors prioritize among various types of data as\na central part of their diagnosis process. Based on these findings, we outline\nfuture directions regarding the design of XAI systems in the medical context.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 01:11:38 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Xie", "Yao", ""], ["Gao", "Ge", ""], ["Chen", "Xiang 'Anthony'", ""]]}, {"id": "1902.06193", "submitter": "Mohammad Amin Alipour", "authors": "Soodeh Atefi, Mohammad Amin Alipour", "title": "An Automated Testing Framework for Conversational Agents", "comments": "work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Conversational agents are systems with a conversational interface that afford\ninteraction in spoken language. These systems are becoming prevalent and are\npreferred in various contexts and for many users. Despite their increasing\nsuccess, the automated testing infrastructure to support the effective and\nefficient development of such systems compared to traditional software systems\nis still limited. Automated testing framework for conversational systems can\nimprove the quality of these systems by assisting developers to write, execute,\nand maintain test cases. In this paper, we introduce our work-in-progress\nautomated testing framework, and its realization in the Python programming\nlanguage. We discuss some research problems in the development of such an\nautomated testing framework for conversational agents. In particular, we point\nout the problems of the specification of the expected behavior, known as test\noracles, and semantic comparison of utterances.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 03:17:03 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Atefi", "Soodeh", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "1902.06384", "submitter": "Mohammad Amin Alipour", "authors": "Andrew Truelove, Farah Naz Chowdhury, Omprakash Gnawali, Mohammad Amin\n  Alipour", "title": "Topics of Concern: Identifying User Issues in Reviews of IoT Apps and\n  Devices", "comments": "1st International Workshop on Software Engineering Research &\n  Practices for the Internet of Things (SERP4IoT 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) systems are bundles of networked sensors and\nactuators that are deployed in an environment and act upon the sensory data\nthat they receive. These systems, especially consumer electronics, have two\nmain cooperating components: a device and a mobile app. The unique combination\nof hardware and software in IoT systems presents challenges that are lesser\nknown to mainstream software developers. They might require innovative\nsolutions to support the development and integration of such systems. In this\npaper, we analyze more than 90,000 reviews of ten IoT devices and their\ncorresponding apps and extract the issues that users encountered while using\nthese systems. Our results indicate that issues with connectivity, timing, and\nupdates are particularly prevalent in the reviews. Our results call for a new\nsoftware-hardware development framework to assist the development of reliable\nIoT systems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 03:11:18 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 20:34:29 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Truelove", "Andrew", ""], ["Chowdhury", "Farah Naz", ""], ["Gnawali", "Omprakash", ""], ["Alipour", "Mohammad Amin", ""]]}, {"id": "1902.06442", "submitter": "Jon McCormack", "authors": "Jon McCormack, Toby Gifford, Patrick Hutchings, Maria Teresa Llano\n  Rodriguez, Matthew Yee-King, Mark d'Inverno", "title": "In a Silent Way: Communication Between AI and Improvising Musicians\n  Beyond Sound", "comments": "11 pages, accepted at ACM CHI 2019, Glasgow Scotland, UK 4-9 May 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaboration is built on trust, and establishing trust with a creative\nArtificial Intelligence is difficult when the decision process or internal\nstate driving its behaviour isn't exposed. When human musicians improvise\ntogether, a number of extra-musical cues are used to augment musical\ncommunication and expose mental or emotional states which affect musical\ndecisions and the effectiveness of the collaboration. We developed a\ncollaborative improvising AI drummer that communicates its confidence through\nan emoticon-based visualisation. The AI was trained on musical performance\ndata, as well as real-time skin conductance, of musicians improvising with\nprofessional drummers, exposing both musical and extra-musical cues to inform\nits generative process. Uni- and bi-directional extra-musical communication\nwith real and false values were tested by experienced improvising musicians.\nEach condition was evaluated using the FSS-2 questionnaire, as a proxy for\nmusical engagement. The results show a positive correlation between\nextra-musical communication of machine internal state and human musical\nengagement.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 08:03:07 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["McCormack", "Jon", ""], ["Gifford", "Toby", ""], ["Hutchings", "Patrick", ""], ["Rodriguez", "Maria Teresa Llano", ""], ["Yee-King", "Matthew", ""], ["d'Inverno", "Mark", ""]]}, {"id": "1902.06671", "submitter": "Cori Faklaris", "authors": "Cori Faklaris, Asa Blevins, Matthew O'Haver, Neha Singhal, and\n  Francesco Cafaro", "title": "An Exploration of User and Bystander Attitudes About Mobile\n  Live-Streaming Video", "comments": "12 pages", "journal-ref": null, "doi": "10.13140/RG.2.2.14052.22406", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to mobile apps such as Periscope and Facebook Live, live-streaming\nvideo is having a moment again. It has not been clear, however, to what extent\nthe current ubiquity of smartphones is impacting this technology's acceptance\nin everyday social situations and how mobile contexts or affordances will\naffect and be affected by shifts in social norms and policy debates regarding\nprivacy, surveillance and intellectual property. This ethnographic-style\nresearch explores familiarity with and attitudes about mobile live-streaming\nvideo and related legal and ethical issues among a sample of \"Middle America\"\nparticipants at two typical outdoor social events: sports tailgating and a\nrooftop party. In situ observations of n=110 bystanders to the use of a\nsmartphone, including interviews with n=20, revealed that many are not fully\naware of when their image or speech is being live-streamed in a casual context\nand want stronger notifications of and ability to consent to such broadcasting.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 17:45:31 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Faklaris", "Cori", ""], ["Blevins", "Asa", ""], ["O'Haver", "Matthew", ""], ["Singhal", "Neha", ""], ["Cafaro", "Francesco", ""]]}, {"id": "1902.06961", "submitter": "Jason R.C. Nurse Dr", "authors": "Mariam Nouh and Jason R.C. Nurse and Helena Webb and Michael Goldsmith", "title": "Cybercrime Investigators are Users Too! Understanding the\n  Socio-Technical Challenges Faced by Law Enforcement", "comments": "11 pages, Proceedings of the 2019 Workshop on Usable Security (USEC)\n  at Network and Distributed System Security Symposium (NDSS)", "journal-ref": null, "doi": "10.14722/usec.2019.23032", "report-no": null, "categories": "cs.HC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybercrime investigators face numerous challenges when policing online\ncrimes. Firstly, the methods and processes they use when dealing with\ntraditional crimes do not necessarily apply in the cyber-world. Additionally,\ncyber criminals are usually technologically-aware and constantly adapting and\ndeveloping new tools that allow them to stay ahead of law enforcement\ninvestigations. In order to provide adequate support for cybercrime\ninvestigators, there needs to be a better understanding of the challenges they\nface at both technical and socio-technical levels. In this paper, we\ninvestigate this problem through an analysis of current practices and workflows\nof investigators. We use interviews with experts from government and private\nsectors who investigate cybercrimes as our main data gathering process. From an\nanalysis of the collected data, we identify several outstanding challenges\nfaced by investigators. These pertain to practical, technical, and social\nissues such as systems availability, usability, and in computer-supported\ncollaborative work. Importantly, we use our findings to highlight research\nareas where user-centric workflows and tools are desirable. We also define a\nset of recommendations that can aid in providing a better foundation for future\nresearch in the field and allow more effective combating of cybercrimes.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 09:25:10 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Nouh", "Mariam", ""], ["Nurse", "Jason R. C.", ""], ["Webb", "Helena", ""], ["Goldsmith", "Michael", ""]]}, {"id": "1902.07057", "submitter": "Zhenyu Yan", "authors": "Zhenyu Yan, Qun Song, Rui Tan, Yang Li and Adams Wai Kin Kong", "title": "Towards Touch-to-Access Device Authentication Using Induced Body\n  Electric Potentials", "comments": "16 pages, accepted to the 25th Annual International Conference on\n  Mobile Computing and Networking (MobiCom 2019), October 21-25, 2019, Los\n  Cabos, Mexico", "journal-ref": null, "doi": "10.1145/3300061.3300118", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents TouchAuth, a new touch-to-access device authentication\napproach using induced body electric potentials (iBEPs) caused by the indoor\nambient electric field that is mainly emitted from the building's electrical\ncabling. The design of TouchAuth is based on the electrostatics of iBEP\ngeneration and a resulting property, i.e., the iBEPs at two close locations on\nthe same human body are similar, whereas those from different human bodies are\ndistinct. Extensive experiments verify the above property and show that\nTouchAuth achieves high-profile receiver operating characteristics in\nimplementing the touch-to-access policy. Our experiments also show that a range\nof possible interfering sources including appliances' electromagnetic\nemanations and noise injections into the power network do not affect the\nperformance of TouchAuth. A key advantage of TouchAuth is that the iBEP sensing\nrequires a simple analog-to-digital converter only, which is widely available\non microcontrollers. Compared with existing approaches including intra-body\ncommunication and physiological sensing, TouchAuth is a low-cost, lightweight,\nand convenient approach for authorized users to access the smart objects found\nin indoor environments.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 04:02:09 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Yan", "Zhenyu", ""], ["Song", "Qun", ""], ["Tan", "Rui", ""], ["Li", "Yang", ""], ["Kong", "Adams Wai Kin", ""]]}, {"id": "1902.07071", "submitter": "Yusuke Ujitoko", "authors": "Yusuke Ujitoko, Yuki Ban, Koichi Hirota", "title": "Modulating Fine Roughness Perception of Vibrotactile Textured Surface\n  using Pseudo-haptic Effect", "comments": "Accepted by IEEE Transactions on Visualization and Computer Graphics", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2019", "doi": "10.1109/TVCG.2019.2898820", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Playing back vibrotactile signals through actuators is commonly used to\nsimulate tactile feelings of virtual textured surfaces. However, there is often\na small mismatch between the simulated tactile feelings and intended tactile\nfeelings by tactile designers. Thus, a method of modulating the vibrotactile\nperception is required. We focus on fine roughness perception and we propose a\nmethod using a pseudo-haptic effect to modulate fine roughness perception of\nvibrotactile texture. Specifically, we visually modify the pointer's position\non the screen slightly, which indicates the touch position on textured\nsurfaces. We hypothesized that if users receive vibrational feedback watching\nthe pointer visually oscillating back/forth and left/right, users would believe\nthe vibrotactile surfaces more uneven. We also hypothesized that as the size of\nvisual oscillation is getting larger, the amount of modification of roughness\nperception of vibrotactile surfaces would be larger. We conducted user studies\nto test the hypotheses. Results of first user study suggested that users felt\nvibrotactile texture with our method rougher than they did without our method\nat a high probability.Results of second user study suggested that users felt\ndifferent roughness for vibrational texture in response to the size of visual\noscillation. These results confirmed our hypotheses and they suggested that our\nmethod was effective. Also, the same effect could potentially be applied to the\nvisual movement of virtual hands or fingertips when users are interacting with\nvirtual surfaces using their hands.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 14:31:21 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 05:13:40 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Ujitoko", "Yusuke", ""], ["Ban", "Yuki", ""], ["Hirota", "Koichi", ""]]}, {"id": "1902.07112", "submitter": "Hang Hu", "authors": "Shuangyue Yu, Hadia Perez, James Barkas, Mohamed Mohamed, Mohamed\n  Eldaly, Tzu-Hao Huang, Xiaolong Yang, Hao Su, Maria del Mar Cortes, Dylan J.\n  Edwards", "title": "A Soft High Force Hand Exoskeleton for Rehabilitation and Assistance of\n  Spinal Cord Injury and Stroke Individuals", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals with spinal cord injury (SCI) and stroke who is lack of\nmanipulation capability have a particular need for robotic hand exoskeletons.\nAmong assistive and rehabilitative medical exoskeletons, there exists a sharp\ntrade-off between device power on the one hand and ergonomics and portability\non other, devices that provide stronger grasping assistance do so at the cost\nof patient comfort. This paper proposes using fin-ray inspired, cable-driven\nfinger orthoses to generate high fingertip forces without the painful\ncompressive and shear stresses commonly associated with conventional\ncable-drive exoskeletons. With combination cable-driven transmission and\nsegmented-finger orthoses, the exoskeleton transmitted larger forces and\napplied torques discretely to the fingers, leading to strong fingertip forces.\nA prototype of the finger orthoses and associated cable transmission was\nfabricated, and force transmission tests of the prototype in the finger flexion\nmode demonstrated a 2:1 input-output ratio between cable tension and fingertip\nforce, with a maximum fingertip force of 22 N. Moreover, the proposed design\nprovides a comfortable experience for wearers thanks to its lightweight and\nconformal properties to the hands.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 16:13:54 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Yu", "Shuangyue", ""], ["Perez", "Hadia", ""], ["Barkas", "James", ""], ["Mohamed", "Mohamed", ""], ["Eldaly", "Mohamed", ""], ["Huang", "Tzu-Hao", ""], ["Yang", "Xiaolong", ""], ["Su", "Hao", ""], ["Cortes", "Maria del Mar", ""], ["Edwards", "Dylan J.", ""]]}, {"id": "1902.07244", "submitter": "Christiane Gresse von Wangenheim", "authors": "Tha\\'isa C. Lacerda, Christiane G. von Wangenheim, Jean C.R. Hauck", "title": "UPCASE - A Method for Self-Assessing the Capability of the Usability\n  Process in Small Organizations", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing usable products is important to provide a competitive edge through\nuser satisfaction. A first step to establish or improve a usability process is\nto perform a process assessment. As this may be costly, an alternative for\norganizations seeking for lighter assessments, especially small organizations,\nmay be self-assessments. They can be carried out by an organization on its own\nto assess the capability of its process. Although there are specific assessment\nmethods to assess the usability process, none of them provides a\nself-assessment method considering the specific characteristics of small\norganizations. The objective of this research is to propose a method for\nself-assessing the capability of the usability process in small organizations.\nThe method consists of a usability process reference model, a measurement\nframework, an assessment model, and a self-assessment process supported by an\nonline tool. Based on systematic mapping studies on usability\ncapability/maturity models and software process self-assessment methods, we\nidentified the specific requirements of such a method. The UPCASE method was\nsystematically developed using a multi-method approach based on the ISO/IEC TR\n29110 and ISO/TR 18529 standard. The method has been applied and evaluated with\nrespect to its reliability, usability, comprehensibility and internal\nconsistency through a series of case studies. First results indicate that the\nmethod may be reliable. Feedback also indicates that the method is easy to use\nand understandable even for non-software process improvement experts. The\nUPCASE method is a first step to the self-assessment of the usability process\nin small organizations supporting the systematic establishment and improvement\nof the usability process contributing to the improvement of the usability of\ntheir software products.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 19:24:37 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Lacerda", "Tha\u00edsa C.", ""], ["von Wangenheim", "Christiane G.", ""], ["Hauck", "Jean C. R.", ""]]}, {"id": "1902.07403", "submitter": "Yusuke Ujitoko", "authors": "Yusuke Ujitoko, Koichi Hirota", "title": "Interpretation of Tactile Sensation using an Anthropomorphic Finger\n  Motion Interface to Operate a Virtual Avatar", "comments": "accepted for publication in ICAT-EGVE 2014 - International Conference\n  on Artificial Reality and Telexistence and Eurographics Symposium on Virtual\n  Environments", "journal-ref": null, "doi": "10.2312/ve.20141359", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the system presented in this paper is to give users tactile\nfeedback while walking in a virtual world through an anthropomorphic finger\nmotion interface. We determined that the synchrony between the first person\nperspective and proprioceptive information together with the motor activity of\nthe user's fingers are able to induce an illusionary feeling that is equivalent\nto the sense of ownership of the invisible avatar's legs. Under this condition,\nthe perception of the ground under the virtual avatar's foot is felt through\nthe user's fingertip. The experiments indicated that using our method the scale\nof the tactile perception of the texture roughness was extended and that the\nenlargement ratio was proportional to the avatar's body (foot) size. In order\nto display the target tactile perception to the users, we have to control only\nthe virtual avatar's body (foot) size and the roughness of the tactile texture.\nOur results suggest that in terms of tactile perception fingers can be a\nreplacement for legs in locomotion interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 04:54:01 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Ujitoko", "Yusuke", ""], ["Hirota", "Koichi", ""]]}, {"id": "1902.07480", "submitter": "Yusuke Ujitoko", "authors": "Yusuke Ujitoko, Yuki Ban", "title": "Vibrotactile Signal Generation from Texture Images or Attributes using\n  Generative Adversarial Network", "comments": "accepted for EuroHaptics 2018: Haptics: Science, Technology, and\n  Applications, pp.25-36", "journal-ref": "Haptics: Science, Technology, and Applications. EuroHaptics 2018.\n  Lecture Notes in Computer Science, vol 10894. Springer", "doi": "10.1007/978-3-319-93399-3_3", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing vibrotactile feedback that corresponds to the state of the virtual\ntexture surfaces allows users to sense haptic properties of them. However,\nhand-tuning such vibrotactile stimuli for every state of the texture takes much\ntime. Therefore, we propose a new approach to create models that realize the\nautomatic vibrotactile generation from texture images or attributes. In this\npaper, we make the first attempt to generate the vibrotactile stimuli\nleveraging the power of deep generative adversarial training. Specifically, we\nuse conditional generative adversarial networks (GANs) to achieve generation of\nvibration during moving a pen on the surface. The preliminary user study showed\nthat users could not discriminate generated signals and genuine ones and users\nfelt realism for generated signals. Thus our model could provide the\nappropriate vibration according to the texture images or the attributes of\nthem. Our approach is applicable to any case where the users touch the various\nsurfaces in a predefined way.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 10:01:38 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Ujitoko", "Yusuke", ""], ["Ban", "Yuki", ""]]}, {"id": "1902.07683", "submitter": "Mohamed Mostafa", "authors": "Mohamed Mostafa", "title": "Modelling and Analysing Behaviours and Emotions via Complex User\n  Interactions", "comments": "176 pages; PhD thesis accepted at Cardiff Metropolitan University, UK\n  (February 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past 15 years, the volume, richness and quality of data collected\nfrom the combined social networking platforms has increased beyond all\nexpectation, providing researchers from a variety of disciplines to use it in\ntheir research. Perhaps more impactfully, it has provided the foundation for a\nrange of new products and services, transforming industries such as advertising\nand marketing, as well as bringing the challenges of sharing personal data into\nthe public consciousness. But how to make sense of the ever-increasing volume\nof big social data so that we can better understand and improve the user\nexperience in increasingly complex, data-driven digital systems. This link with\nusability and the user experience of data-driven system bridges into the wider\nfield of HCI, attracting interdisciplinary researchers as we see the demand for\nconsumer technologies, software and systems, as well as the integration of\nsocial networks into our everyday lives. The fact that the data largely posted\non social networks tends to be textual, provides a further link to linguistics,\npsychology and psycholinguistics to better understand the relationship between\nhuman behaviours offline and online.\n  In this thesis, we present a novel conceptual framework based on a complex\ndigital system using collected longitudinal datasets to predict system status\nbased on the personality traits and emotions extracted from text posted by\nusers. The system framework was built using a dataset collected from an online\nscholarship system in which 2000 students had their digital behaviour and\nsocial network behaviour collected for this study. We contextualise this\nresearch project with a wider review and critical analysis of the current\npsycholinguistics, artificial intelligence and human-computer interaction\nliterature, which reveals a gap of mapping and understanding digital profiling\nagainst system status.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 18:04:28 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Mostafa", "Mohamed", ""]]}, {"id": "1902.07704", "submitter": "Jinghui Cheng", "authors": "Jinghui Cheng, Jin L.C. Guo", "title": "How Do the Open Source Communities Address Usability and UX Issues? An\n  Exploratory Study", "comments": "The 2018 CHI Conference on Human Factors in Computing Systems (CHI\n  2018) Late-Breaking Work", "journal-ref": null, "doi": "10.1145/3170427.3188467", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usability and user experience (UX) issues are often not well emphasized and\naddressed in open source software (OSS) development. There is an imperative\nneed for supporting OSS communities to collaboratively identify, understand,\nand fix UX design issues in a distributed environment. In this paper, we\nprovide an initial step towards this effort and report on an exploratory study\nthat investigated how the OSS communities currently reported, discussed,\nnegotiated, and eventually addressed usability and UX issues. We conducted\nin-depth qualitative analysis of selected issue tracking threads from three OSS\nprojects hosted on GitHub. Our findings indicated that discussions about\nusability and UX issues in OSS communities were largely influenced by the\npersonal opinions and experiences of the participants. Moreover, the\ncharacteristics of the community may have greatly affected the focus of such\ndiscussion.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 18:57:14 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Cheng", "Jinghui", ""], ["Guo", "Jin L. C.", ""]]}, {"id": "1902.07705", "submitter": "Jinghui Cheng", "authors": "Wenting Wang, Jinghui Cheng, Jin L.C. Guo", "title": "Usability of Virtual Reality Application Through the Lens of the User\n  Community: A Case Study", "comments": "The 2019 ACM CHI Conference on Human Factors in Computing Systems\n  (CHI2019) Late-Breaking Work", "journal-ref": null, "doi": "10.1145/3290607.3312816", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability and diversity of virtual reality (VR)\napplications highlighted the importance of their usability. Function-oriented\nVR applications posed new challenges that are not well studied in the\nliterature. Moreover, user feedback becomes readily available thanks to modern\nsoftware engineering tools, such as app stores and open source platforms. Using\nFirefox Reality as a case study, we explored the major types of VR usability\nissues raised in these platforms. We found that 77% of usability feedbacks can\nbe mapped to Nielsen's heuristics while few were mappable to VR-specific\nheuristics. This result indicates that Nielsen's heuristics could potentially\nhelp developers address the usability of this VR application in its early\ndevelopment stage. This work paves the road for exploring tools leveraging the\ncommunity effort to promote the usability of function-oriented VR applications.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 18:57:26 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Wang", "Wenting", ""], ["Cheng", "Jinghui", ""], ["Guo", "Jin L. C.", ""]]}, {"id": "1902.07769", "submitter": "Felix Hamza-Lup", "authors": "Jannick P. Rolland, Frank Biocca, Felix G. Hamza-Lup, Yanggang Ha,\n  Ricardo Martins", "title": "Development of Head-Mounted Projection Displays for Distributed,\n  Collaborative, Augmented Reality Applications", "comments": null, "journal-ref": "Immersive Projection Technology, 2005, PRESENCE, Vol. 14(5), pp.\n  528-549", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed systems technologies supporting 3D visualization and social\ncollaboration will be increasing in frequency and type over time. An emerging\ntype of head-mounted display referred to as the head-mounted projection display\n(HMPD) was recently developed that only requires ultralight optics (i.e., less\nthan 8 g per eye) that enables immersive multiuser, mobile augmented reality 3D\nvisualization, as well as remote 3D collaborations. In this paper a review of\nthe development of lightweight HMPD technology is provided, together with\ninsight into what makes this technology timely and so unique. Two novel\nemerging HMPD-based technologies are then described: a teleportal HMPD(T-HMPD)\nenabling face-to-face communication and visualization of shared 3D virtual\nobjects, and a mobile HMPD (M-HMPD) designed for outdoor wearable visualization\nand communication. Finally, the use of HMPD in medical visualization and\ntraining, as well as in infospaces, two applications developed in the ODA and\nMIND labs respectively, are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 20:32:51 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Rolland", "Jannick P.", ""], ["Biocca", "Frank", ""], ["Hamza-Lup", "Felix G.", ""], ["Ha", "Yanggang", ""], ["Martins", "Ricardo", ""]]}, {"id": "1902.07807", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Faith-Anne L. Kocadag", "title": "Simulating Forces - Learning Through Touch, Virtual Laboratories", "comments": null, "journal-ref": "IARIA, 2013, pp.55-58", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the expansion of e-learning course curricula and the affordability of\nhaptic devices, at-home virtual laboratories are emerging as an increasingly\nviable option for e-learners. We outline three novel haptic simulations for the\nintroductory physics concepts of friction, the Coriolis Effect, and Precession.\nThese simulations provide force feedback through one or more Novint Falcon\ndevices, allowing students to \"feel\" the forces at work in a controlled\nlearning environment. This multi-modal approach to education (beyond the\naudiovisual) may lead to increased interest and immersion for e-learners and\nappeal to the kinesthetic learners who may struggle in a traditional e-learning\ncourse setting.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 23:14:41 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Kocadag", "Faith-Anne L.", ""]]}, {"id": "1902.08068", "submitter": "Yu Guan", "authors": "Yan Gao, Yang Long, Yu Guan, Anna Basu, Jessica Baggaley, Thomas\n  Ploetz", "title": "Towards Reliable, Automated General Movement Assessment for Perinatal\n  Stroke Screening in Infants Using Wearable Accelerometers", "comments": "Gao and Long share equal contributions; This work has been accepted\n  for publication in ACM IMWUT (Ubicomp) 2019;", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perinatal stroke (PS) is a serious condition that, if undetected and thus\nuntreated, often leads to life-long disability, in particular Cerebral Palsy\n(CP). In clinical settings, Prechtl's General Movement Assessment (GMA) can be\nused to classify infant movements using a Gestalt approach, identifying infants\nat high risk of developing PS. Training and maintenance of assessment skills\nare essential and expensive for the correct use of GMA, yet many practitioners\nlack these skills, preventing larger-scale screening and leading to significant\nrisks of missing opportunities for early detection and intervention for\naffected infants. We present an automated approach to GMA, based on body-worn\naccelerometers and a novel sensor data analysis method-Discriminative Pattern\nDiscovery (DPD)-that is designed to cope with scenarios where only coarse\nannotations of data are available for model training. We demonstrate the\neffectiveness of our approach in a study with 34 newborns (21 typically\ndeveloping infants and 13 PS infants with abnormal movements). Our method is\nable to correctly recognise the trials with abnormal movements with at least\nthe accuracy that is required by newly trained human annotators (75%), which is\nencouraging towards our ultimate goal of an automated PS screening system that\ncan be used population-wide.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 14:30:59 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Gao", "Yan", ""], ["Long", "Yang", ""], ["Guan", "Yu", ""], ["Basu", "Anna", ""], ["Baggaley", "Jessica", ""], ["Ploetz", "Thomas", ""]]}, {"id": "1902.08327", "submitter": "Ting-Hao Huang", "authors": "Ting-Yao Hsu and Yen-Chia Hsu and Ting-Hao 'Kenneth' Huang", "title": "On How Users Edit Computer-Generated Visual Stories", "comments": "To appear in CHI'19 Late-Breaking Work on Human Factors in Computing\n  Systems (CHI LBW 2019), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant body of research in Artificial Intelligence (AI) has focused on\ngenerating stories automatically, either based on prior story plots or input\nimages. However, literature has little to say about how users would receive and\nuse these stories. Given the quality of stories generated by modern AI\nalgorithms, users will nearly inevitably have to edit these stories before\nputting them to real use. In this paper, we present the first analysis of how\nhuman users edit machine-generated stories. We obtained 962 short stories\ngenerated by one of the state-of-the-art visual storytelling models. For each\nstory, we recruited five crowd workers from Amazon Mechanical Turk to edit it.\nOur analysis of these edits shows that, on average, users (i) slightly\nshortened machine-generated stories, (ii) increased lexical diversity in these\nstories, and (iii) often replaced nouns and their determiners/articles with\npronouns. Our study provides a better understanding on how users receive and\nedit machine-generated stories,informing future researchers to create more\nusable and helpful story generation systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 01:26:05 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 20:28:11 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Hsu", "Ting-Yao", ""], ["Hsu", "Yen-Chia", ""], ["Huang", "Ting-Hao 'Kenneth'", ""]]}, {"id": "1902.08740", "submitter": "Julian Theis", "authors": "Julian Theis and Houshang Darabi", "title": "Behavioral Petri Net Mining and Automated Analysis for Human-Computer\n  Interaction Recommendations in Multi-Application Environments", "comments": "Fixed typos, updated event ordering in traces and PN marking\n  definition", "journal-ref": null, "doi": "10.1145/3331155", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process Mining is a famous technique which is frequently applied to Software\nDevelopment Processes, while being neglected in Human-Computer Interaction\n(HCI) recommendation applications. Organizations usually train employees to\ninteract with required IT systems. Often, employees, or users in general,\ndevelop their own strategies for solving repetitive tasks and processes.\nHowever, organizations find it hard to detect whether employees interact\nefficiently with IT systems or not. Hence, we have developed a method which\ndetects inefficient behavior assuming that at least one optimal HCI strategy is\nknown. This method provides recommendations to gradually adapt users' behavior\ntowards the optimal way of interaction considering satisfaction of users. Based\non users' behavior logs tracked by a Java application suitable for\nmulti-application and multi-instance environments, we demonstrate the\napplicability for a specific task in a common Windows environment utilizing\nrealistic simulated behaviors of users.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 05:55:51 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 15:37:48 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Theis", "Julian", ""], ["Darabi", "Houshang", ""]]}, {"id": "1902.08990", "submitter": "Chongyang Wang", "authors": "Chongyang Wang, Temitayo A. Olugbade, Akhil Mathur, Amanda C. De C.\n  Williams, Nicholas D. Lane, Nadia Bianchi-Berthouze", "title": "Chronic-Pain Protective Behavior Detection with Deep Learning", "comments": "24 pages, 12 figures, 7 tables. Accepted by ACM Transactions on\n  Computing for Healthcare", "journal-ref": null, "doi": "10.1145/3449068", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In chronic pain rehabilitation, physiotherapists adapt physical activity to\npatients' performance based on their expression of protective behavior,\ngradually exposing them to feared but harmless and essential everyday\nactivities. As rehabilitation moves outside the clinic, technology should\nautomatically detect such behavior to provide similar support. Previous works\nhave shown the feasibility of automatic protective behavior detection (PBD)\nwithin a specific activity. In this paper, we investigate the use of deep\nlearning for PBD across activity types, using wearable motion capture and\nsurface electromyography data collected from healthy participants and people\nwith chronic pain. We approach the problem by continuously detecting protective\nbehavior within an activity rather than estimating its overall presence. The\nbest performance reaches mean F1 score of 0.82 with leave-one-subject-out cross\nvalidation. When protective behavior is modelled per activity type, performance\nis mean F1 score of 0.77 for bend-down, 0.81 for one-leg-stand, 0.72 for\nsit-to-stand, 0.83 for stand-to-sit, and 0.67 for reach-forward. This\nperformance reaches excellent level of agreement with the average experts'\nrating performance suggesting potential for personalized chronic pain\nmanagement at home. We analyze various parameters characterizing our approach\nto understand how the results could generalize to other PBD datasets and\ndifferent levels of ground truth granularity.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 17:50:44 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 18:48:08 GMT"}, {"version": "v3", "created": "Thu, 26 Nov 2020 01:03:12 GMT"}, {"version": "v4", "created": "Thu, 1 Apr 2021 06:10:35 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Chongyang", ""], ["Olugbade", "Temitayo A.", ""], ["Mathur", "Akhil", ""], ["Williams", "Amanda C. De C.", ""], ["Lane", "Nicholas D.", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "1902.09022", "submitter": "Ahmed Fadhil Dr.", "authors": "Ahmed Fadhil, Gianluca Schiavo", "title": "Designing for Health Chatbots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building conversational agents have many technical, design and linguistic\nchallenges. Other more complex elements include using emotionally intelligent\nconversational agent to build trust with the individuals. In this chapter, we\nintroduce the nature of conversational user interfaces (CUIs) for health and\ndescribe UX design principles informed by a systematic literature review of\nrelevant research works. We analyze scientific literature in conversational\ninterfaces and chatterbots, providing a survey of major studies and describing\nUX design principles and interaction patterns.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 22:05:57 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Fadhil", "Ahmed", ""], ["Schiavo", "Gianluca", ""]]}, {"id": "1902.09159", "submitter": "Silas {\\O}rting", "authors": "Silas {\\O}rting, Andrew Doyle, Arno van Hilten, Matthias Hirth, Oana\n  Inel, Christopher R. Madan, Panagiotis Mavridis, Helen Spiers, Veronika\n  Cheplygina", "title": "A Survey of Crowdsourcing in Medical Image Analysis", "comments": "Submitted to Human Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in image processing capabilities have been seen across many\ndomains, fostered by the application of machine learning algorithms to\n\"big-data\". However, within the realm of medical image analysis, advances have\nbeen curtailed, in part, due to the limited availability of large-scale,\nwell-annotated datasets. One of the main reasons for this is the high cost\noften associated with producing large amounts of high-quality meta-data.\nRecently, there has been growing interest in the application of crowdsourcing\nfor this purpose; a technique that has proven effective for creating\nlarge-scale datasets across a range of disciplines, from computer vision to\nastrophysics. Despite the growing popularity of this approach, there has not\nyet been a comprehensive literature review to provide guidance to researchers\nconsidering using crowdsourcing methodologies in their own medical imaging\nanalysis. In this survey, we review studies applying crowdsourcing to the\nanalysis of medical images, published prior to July 2018. We identify common\napproaches, challenges and considerations, providing guidance of utility to\nresearchers adopting this approach. Finally, we discuss future opportunities\nfor development within this emerging domain.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 09:21:09 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 12:47:16 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["\u00d8rting", "Silas", ""], ["Doyle", "Andrew", ""], ["van Hilten", "Arno", ""], ["Hirth", "Matthias", ""], ["Inel", "Oana", ""], ["Madan", "Christopher R.", ""], ["Mavridis", "Panagiotis", ""], ["Spiers", "Helen", ""], ["Cheplygina", "Veronika", ""]]}, {"id": "1902.09289", "submitter": "Luca Benedetto", "authors": "Luca Benedetto, Paolo Cremonesi, Manuel Parenti", "title": "A Virtual Teaching Assistant for Personalized Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this extended abstract, we propose an intelligent system that can be used\nas a Personalized Virtual Teaching Assistant (PVTA) to improve the students\nlearning experience both for online and on-site courses. We show the\narchitecture of such system, which is composed of an instance of IBM Watson\nAssistant and a server, and present an initial implementation, consisting in a\nchatbot that can be questioned about the content and the organization of the\nRecSys course, an introductory course on recommender systems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 14:29:43 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Benedetto", "Luca", ""], ["Cremonesi", "Paolo", ""], ["Parenti", "Manuel", ""]]}, {"id": "1902.09749", "submitter": "Andr\\'es Monroy-Hern\\'andez", "authors": "Taryn Bipat, Maarten Willem Bos, Rajan Vaish, Andr\\'es\n  Monroy-Hern\\'andez", "title": "Analyzing the Use of Camera Glasses in the Wild", "comments": "In Proceedings of the 37th Annual ACM Conference on Human Factors in\n  Computing Systems (CHI 2019). ACM, New York, NY, USA", "journal-ref": null, "doi": "10.1145/3290605.3300651", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Camera glasses enable people to capture point-of-view videos using a common\naccessory, hands-free. In this paper, we investigate how, when, and why people\nused one such product: Spectacles. We conducted 39 semi-structured interviews\nand surveys with 191 owners of Spectacles. We found that the form factor\nelicits sustained usage behaviors, and opens opportunities for new use-cases\nand types of content captured. We provide a usage typology, and highlight\nsocietal and individual factors that influence the classification of behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 06:20:44 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Bipat", "Taryn", ""], ["Bos", "Maarten Willem", ""], ["Vaish", "Rajan", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""]]}, {"id": "1902.09834", "submitter": "Homayun Afrabandpey", "authors": "Homayun Afrabandpey, Tomi Peltola, Samuel Kaski", "title": "Human-in-the-loop Active Covariance Learning for Improving Prediction in\n  Small Data Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning predictive models from small high-dimensional data sets is a key\nproblem in high-dimensional statistics. Expert knowledge elicitation can help,\nand a strong line of work focuses on directly eliciting informative prior\ndistributions for parameters. This either requires considerable statistical\nexpertise or is laborious, as the emphasis has been on accuracy and not on\nefficiency of the process. Another line of work queries about importance of\nfeatures one at a time, assuming them to be independent and hence missing\ncovariance information. In contrast, we propose eliciting expert knowledge\nabout pairwise feature similarities, to borrow statistical strength in the\npredictions, and using sequential decision making techniques to minimize the\neffort of the expert. Empirical results demonstrate improvement in predictive\nperformance on both simulated and real data, in high-dimensional linear\nregression tasks, where we learn the covariance structure with a Gaussian\nprocess, based on sequential elicitation.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 10:03:18 GMT"}, {"version": "v2", "created": "Mon, 18 Mar 2019 14:57:35 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Afrabandpey", "Homayun", ""], ["Peltola", "Tomi", ""], ["Kaski", "Samuel", ""]]}, {"id": "1902.09944", "submitter": "Siddique Latif", "authors": "Rajib Rana, Siddique Latif, Raj Gururajan, Anthony Gray, Geraldine\n  Mackenzie, Gerald Humphris, and Jeff Dunn", "title": "Automated Screening for Distress: A Perspective for the Future", "comments": "Accepted in European Journal of Cancer Care", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distress is a complex condition which affects a significant percentage of\ncancer patients and may lead to depression, anxiety, sadness, suicide and other\nforms of psychological morbidity. Compelling evidence supports screening for\ndistress as a means of facilitating early intervention and subsequent\nimprovements in psychological well-being and overall quality of life.\nNevertheless, despite the existence of evidence based and easily administered\nscreening tools, for example, the Distress Thermometer, routine screening for\ndistress is yet to achieve widespread implementation. Efforts are intensifying\nto utilise innovative, cost effective methods now available through emerging\ntechnologies in the informatics and computational arenas.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 07:17:29 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 01:40:35 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Rana", "Rajib", ""], ["Latif", "Siddique", ""], ["Gururajan", "Raj", ""], ["Gray", "Anthony", ""], ["Mackenzie", "Geraldine", ""], ["Humphris", "Gerald", ""], ["Dunn", "Jeff", ""]]}, {"id": "1902.10223", "submitter": "Zhu Wang", "authors": "Zhu Wang, Anat Lubetzky, Marta Gospodarek, Makan TaghaviDilamani, Ken\n  Perlin", "title": "Virtual Environments for Rehabilitation of Postural Control Dysfunction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a novel virtual reality [VR] platform with 3-dimensional sounds\nto help improve sensory integration and visuomotor processing for postural\ncontrol and fall prevention in individuals with balance problems related to\nsensory deficits, such as vestibular dysfunction (disease of the inner ear).\nThe system has scenes that simulate scenario-based environments. We can adjust\nthe intensity of the visual and audio stimuli in the virtual scenes by\ncontrolling the user interface (UI) settings. A VR headset (HTC Vive or Oculus\nRift) delivers stereo display while providing real-time position and\norientation of the participants' head. The 3D game-like scenes make\nparticipants feel immersed and gradually exposes them to situations that may\ninduce dizziness, anxiety or imbalance in their daily-living.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 20:16:42 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Wang", "Zhu", ""], ["Lubetzky", "Anat", ""], ["Gospodarek", "Marta", ""], ["TaghaviDilamani", "Makan", ""], ["Perlin", "Ken", ""]]}, {"id": "1902.10699", "submitter": "Ahmadreza Mahmoudzadeh", "authors": "Sayna Firoozi Yeganeh, Ahmadreza Mahmoudzadeh, Mohammad Amin Azizpour,\n  Amir Golroo", "title": "Validation of smartphone based pavement roughness measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones are equipped with sensors such as accelerometers, gyroscope, and\nGPS in one cost-effective device with an acceptable level of accuracy. There\nhave been some research studies carried out in terms of using smartphones to\nmeasure the pavement roughness. However, a little attention has been paid to\ninvestigate the validity of the measured pavement roughness by smartphones via\nother subjective methods such as the user opinion. This paper aims at\ncalculating the pavement roughness data with a smartphone using its embedded\nsensors and investigating its correlation with a user opinion about the ride\nquality. In addition, the applicability of using smartphones to assess the\npavement surface distresses is examined. Furthermore, to validate the\nsmartphone sensor outputs objectively, the Road Surface Profiler is applied.\nFinally, a good roughness model is developed which demonstrates an acceptable\nlevel of correlation between the pavement roughness measured by smartphones and\nthe ride quality rated by users.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 22:35:57 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Yeganeh", "Sayna Firoozi", ""], ["Mahmoudzadeh", "Ahmadreza", ""], ["Azizpour", "Mohammad Amin", ""], ["Golroo", "Amir", ""]]}, {"id": "1902.11216", "submitter": "Bernd Huber", "authors": "Bernd Huber, Hijung Valentina Shin, Bryan Russell, Oliver Wang,\n  Gautham J. Mysore", "title": "B-Script: Transcript-based B-roll Video Editing with Recommendations", "comments": "11 pages, 10 figures, CHI 2019", "journal-ref": null, "doi": "10.1145/3290605.3300311", "report-no": null, "categories": "cs.HC cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video production, inserting B-roll is a widely used technique to enrich\nthe story and make a video more engaging. However, determining the right\ncontent and positions of B-roll and actually inserting it within the main\nfootage can be challenging, and novice producers often struggle to get both\ntiming and content right. We present B-Script, a system that supports B-roll\nvideo editing via interactive transcripts. B-Script has a built-in\nrecommendation system trained on expert-annotated data, recommending users\nB-roll position and content. To evaluate the system, we conducted a\nwithin-subject user study with 110 participants, and compared three interface\nvariations: a timeline-based editor, a transcript-based editor, and a\ntranscript-based editor with recommendations. Users found it easier and were\nfaster to insert B-roll using the transcript-based interface, and they created\nmore engaging videos when recommendations were provided.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 17:01:29 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Huber", "Bernd", ""], ["Shin", "Hijung Valentina", ""], ["Russell", "Bryan", ""], ["Wang", "Oliver", ""], ["Mysore", "Gautham J.", ""]]}, {"id": "1902.11247", "submitter": "Yang Li", "authors": "Amanda Swearngin, Yang Li", "title": "Modeling Mobile Interface Tappability Using Crowdsourcing and Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tapping is an immensely important gesture in mobile touchscreen interfaces,\nyet people still frequently are required to learn which elements are tappable\nthrough trial and error. Predicting human behavior for this everyday gesture\ncan help mobile app designers understand an important aspect of the usability\nof their apps without having to run a user study. In this paper, we present an\napproach for modeling tappability of mobile interfaces at scale. We conducted\nlarge-scale data collection of interface tappability over a rich set of mobile\napps using crowdsourcing and computationally investigated a variety of\nsignifiers that people use to distinguish tappable versus not-tappable\nelements. Based on the dataset, we developed and trained a deep neural network\nthat predicts how likely a user will perceive an interface element as tappable\nversus not tappable. Using the trained tappability model, we developed TapShoe,\na tool that automatically diagnoses mismatches between the tappability of each\nelement as perceived by a human user---predicted by our model, and the intended\nor actual tappable state of the element specified by the developer or designer.\nOur model achieved reasonable accuracy: mean precision 90.2\\% and recall\n87.0\\%, in matching human perception on identifying tappable UI elements. The\ntappability model and TapShoe were well received by designers via an informal\nevaluation with 7 professional interaction designers.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 17:53:23 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Swearngin", "Amanda", ""], ["Li", "Yang", ""]]}]