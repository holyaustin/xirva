[{"id": "1905.00231", "submitter": "Jennifer Sorinas", "authors": "Jennifer Sorinas, Jose Manuel Ferr\\'andez and Eduardo Fernandez", "title": "The Psychological and Physiological Part of Emotions: Multimodal\n  Approximation for Valence Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to develop more precise and functional affective applications, it is\nnecessary to achieve a balance between the psychology and the engineering\napplied to emotions. Signals from the central and peripheral nervous systems\nhave been used for emotion recognition purposes, however, their operation and\nthe relationship between them remains unknown. In this context, in the present\nwork we have tried to approach the study of the psychobiology of both systems\nin order to generate a computational model for the recognition of emotions in\nthe dimension of valence. To this end, the electroencephalography (EEG) signal,\nelectrocardiography (ECG) signal and skin temperature of 24 subjects have been\nstudied. Each methodology has been evaluated individually, finding\ncharacteristic patterns of positive and negative emotions in each of them.\nAfter feature selection of each methodology, the results of the classification\nshowed that, although the classification of emotions is possible at both\ncentral and peripheral levels, the multimodal approach did not improve the\nresults obtained through the EEG alone. In addition, differences have been\nobserved between cerebral and physiological responses in the processing\nemotions by separating the sample by sex; though, the differences between men\nand women were only notable at the physiological level.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 09:29:53 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Sorinas", "Jennifer", ""], ["Ferr\u00e1ndez", "Jose Manuel", ""], ["Fernandez", "Eduardo", ""]]}, {"id": "1905.00503", "submitter": "Siddharth Siddharth", "authors": "Siddharth and Mohan M. Trivedi", "title": "Attention Monitoring and Hazard Assessment with Bio-Sensing and Vision:\n  Empirical Analysis Utilizing CNNs on the KITTI Dataset", "comments": "Accepted for publication at IEEE Intelligent Vehicles Symposium 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the driver's attention and detecting various hazardous and\nnon-hazardous events during a drive are critical for driver's safety. Attention\nmonitoring in driving scenarios has mostly been carried out using vision\n(camera-based) modality by tracking the driver's gaze and facial expressions.\nIt is only recently that bio-sensing modalities such as Electroencephalogram\n(EEG) are being explored. But, there is another open problem which has not been\nexplored sufficiently yet in this paradigm. This is the detection of specific\nevents, hazardous and non-hazardous, during driving that affects the driver's\nmental and physiological states. The other challenge in evaluating multi-modal\nsensory applications is the absence of very large scale EEG data because of the\nvarious limitations of using EEG in the real world. In this paper, we use both\nof the above sensor modalities and compare them against the two tasks of\nassessing the driver's attention and detecting hazardous vs. non-hazardous\ndriving events. We collect user data on twelve subjects and show how in the\nabsence of very large-scale datasets, we can still use pre-trained deep\nlearning convolution networks to extract meaningful features from both of the\nabove modalities. We used the publicly available KITTI dataset for evaluating\nour platform and to compare it with previous studies. Finally, we show that the\nresults presented in this paper surpass the previous benchmark set up in the\nabove driver awareness-related applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 21:19:35 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 23:28:40 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Siddharth", "", ""], ["Trivedi", "Mohan M.", ""]]}, {"id": "1905.00646", "submitter": "Lisa Andreevna Chalaguine", "authors": "Lisa A. Chalaguine, Anthony Hunter, Fiona L. Hamilton, Henry W. W.\n  Potts", "title": "Impact of Argument Type and Concerns in Argumentation with a Chatbot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents, also known as chatbots, are versatile tools that have\nthe potential of being used in dialogical argumentation. They could possibly be\ndeployed in tasks such as persuasion for behaviour change (e.g. persuading\npeople to eat more fruit, to take regular exercise, etc.) However, to achieve\nthis, there is a need to develop methods for acquiring appropriate arguments\nand counterargument that reflect both sides of the discussion. For instance, to\npersuade someone to do regular exercise, the chatbot needs to know\ncounterarguments that the user might have for not doing exercise. To address\nthis need, we present methods for acquiring arguments and counterarguments, and\nimportantly, meta-level information that can be useful for deciding when\narguments can be used during an argumentation dialogue. We evaluate these\nmethods in studies with participants and show how harnessing these methods in a\nchatbot can make it more persuasive.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 09:53:24 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Chalaguine", "Lisa A.", ""], ["Hunter", "Anthony", ""], ["Hamilton", "Fiona L.", ""], ["Potts", "Henry W. W.", ""]]}, {"id": "1905.00653", "submitter": "Joshua Kim", "authors": "Joshua Y. Kim, Rafael A. Calvo, Kalina Yacef, N.J. Enfield", "title": "A Review on Dyadic Conversation Visualizations - Purposes, Data, Lens of\n  Analysis", "comments": "19 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many professional services are provided through text and voice systems, from\nvoice calls over the internet to messaging and emails. There is a growing need\nfor both individuals and organizations to understand these online conversations\nbetter and find actionable insights. One method that allows the user to explore\ninsights is to build intuitive and rich visualizations that illustrate the\ncontent of the conversation. In this paper, we present a systematic survey of\nthe various methods of visualizing a conversation and research papers involving\ninteractive visualizations and human participants. Findings from the survey\nshow that there have been attempts to visualize most, if not all, of the types\nof conversation that are taking place digitally, from speech to messages and\nemails. Through this survey, we make two contributions. One, we summarize the\ncurrent practices in the domain of visualizing dyadic conversations. Two, we\nprovide suggestions for future dialogue visualization research.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 10:24:27 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Kim", "Joshua Y.", ""], ["Calvo", "Rafael A.", ""], ["Yacef", "Kalina", ""], ["Enfield", "N. J.", ""]]}, {"id": "1905.01127", "submitter": "Jochen G\\\"ortler", "authors": "Jochen G\\\"ortler, Thilo Spinner, Dirk Streeb, Daniel Weiskopf, Oliver\n  Deussen", "title": "Uncertainty-Aware Principal Component Analysis", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2020", "doi": "10.1109/TVCG.2019.2934812", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique to perform dimensionality reduction on data that is\nsubject to uncertainty. Our method is a generalization of traditional principal\ncomponent analysis (PCA) to multivariate probability distributions. In\ncomparison to non-linear methods, linear dimensionality reduction techniques\nhave the advantage that the characteristics of such probability distributions\nremain intact after projection. We derive a representation of the PCA sample\ncovariance matrix that respects potential uncertainty in each of the inputs,\nbuilding the mathematical foundation of our new method: uncertainty-aware PCA.\nIn addition to the accuracy and performance gained by our approach over\nsampling-based strategies, our formulation allows us to perform sensitivity\nanalysis with regard to the uncertainty in the data. For this, we propose\nfactor traces as a novel visualization that enables to better understand the\ninfluence of uncertainty on the chosen principal components. We provide\nmultiple examples of our technique using real-world datasets. As a special\ncase, we show how to propagate multivariate normal distributions through PCA in\nclosed form. Furthermore, we discuss extensions and limitations of our\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 11:46:53 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 16:32:10 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 21:35:33 GMT"}, {"version": "v4", "created": "Thu, 1 Aug 2019 15:07:35 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["G\u00f6rtler", "Jochen", ""], ["Spinner", "Thilo", ""], ["Streeb", "Dirk", ""], ["Weiskopf", "Daniel", ""], ["Deussen", "Oliver", ""]]}, {"id": "1905.01302", "submitter": "Chengzheng Sun", "authors": "David Sun, Chengzheng Sun, Agustina Ng, Weiwei Cai", "title": "Real Differences between OT and CRDT in Correctness and Complexity for\n  Consistency Maintenance in Co-Editors", "comments": "30 pages. arXiv admin note: substantial text overlap with\n  arXiv:1810.02137", "journal-ref": "PACMHCI Vol. 4, CSCW1, Article 21, May 2020", "doi": "10.1145/3392825", "report-no": null, "categories": "cs.DC cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OT (Operational Transformation) was invented for supporting real-time\nco-editors in the late 1980s and has evolved to become core techniques widely\nused in today's working co-editors and adopted in industrial products. CRDT\n(Commutative Replicated Data Type) for co-editors was first proposed around\n2006, under the name of WOOT (WithOut Operational Transformation). Follow-up\nCRDT variations are commonly labeled as \"post-OT\" techniques capable of making\nconcurrent operations natively commutative in co-editors. On top of that, CRDT\nsolutions have made broad claims of superiority over OT solutions, and often\nportrayed OT as an incorrect and inefficient technique. Over one decade later,\nhowever, CRDT is rarely found in working co-editors; OT remains the choice for\nbuilding the vast majority of today's co-editors. Contradictions between the\nreality and CRDT's purported advantages have been the source of much confusion\nand debate among co-editing researcher sand developers. To seek truth from\nfacts, we set out to conduct a comprehensive and critical review on\nrepresentative OT and CRDT solutions and co-editors based on them. From this\nwork, we have made important discoveries about OT and CRDT, and revealed facts\nand evidences that refute CRDT claims over OT on all accounts. These\ndiscoveries help explain the underlying reasons for the choice between OT and\nCRDT in the real world. We report these results in a series of three articles.\n  In the second article of this series, we reveal the differences between OT\nand CRDT in their basic approaches to realizing the same general transformation\nand how such differences had resulted in different challenges and consequential\ncorrectness and complexity issues. Moreover, we reveal hidden complexity and\nalgorithmic flaws with representative CRDT solutions, and discuss common myths\nand facts related to correctness and complexity of OT and CRDT.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 07:45:28 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 14:41:17 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Sun", "David", ""], ["Sun", "Chengzheng", ""], ["Ng", "Agustina", ""], ["Cai", "Weiwei", ""]]}, {"id": "1905.01352", "submitter": "Mina Khan", "authors": "Mina Khan, Glenn Fernandes, Utkarsh Sarawgi, Prudhvi Rampey, Pattie\n  Maes", "title": "PAL: A Wearable Platform for Real-time, Personalized and Context-Aware\n  Health and Cognition Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized Active Learner (PAL) is a wearable system for real-time,\npersonalized, and context-aware health and cognition support. PAL's system\nconsists of a wearable device, mobile app, cloud database, data visualization\nweb app, and machine learning server. PAL's wearable device uses multi-modal\nsensors (camera, microphone, heart-rate) with on-device machine learning and\nopen-ear audio output to provide real-time and context-aware cognitive,\nbehavioral and psychological interventions. PAL also allows users to track the\nlong-term correlations between their activities and physiological states to\nmake well-informed lifestyle decisions. In this paper, we present and\nopen-source PAL's system so that people can use it for health and cognition\nsupport applications. We also open-source three fully-developed example\napplications using PAL for face-based memory augmentation, contextual language\nlearning, and heart-rate-based psychological support. PAL's flexible, modular\nand extensible platform combines trends in data-driven medicine, mobile\npsychology, and cognitive enhancement to support data-driven and empowering\nhealth and cognition applications.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 19:54:24 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Khan", "Mina", ""], ["Fernandes", "Glenn", ""], ["Sarawgi", "Utkarsh", ""], ["Rampey", "Prudhvi", ""], ["Maes", "Pattie", ""]]}, {"id": "1905.01517", "submitter": "Chengzheng Sun", "authors": "David Sun, Chengzheng Sun, Agustina Ng, Weiwei Cai", "title": "Real Differences between OT and CRDT in Building Co-Editing Systems and\n  Real World Applications", "comments": "16 pages. This article has been revised in accordance to the other\n  two companion articles, which have been recently published at PACMHCI\n  (https://doi.org/10.1145/3375186; and https://doi.org/10.1145/3392825,\n  respectively). arXiv admin note: substantial text overlap with\n  arXiv:1810.02137, arXiv:1905.01302", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OT (Operational Transformation) was invented for supporting real-time\nco-editors in the late 1980s and has evolved to become a core technique used in\ntoday's working co-editors and adopted in major industrial products. CRDT\n(Commutative Replicated Data Type) for co-editors was first proposed around\n2006, under the name of WOOT (WithOut Operational Transformation). Follow-up\nCRDT variations are commonly labeled as \"post-OT\" techniques and have made\nbroad claims of superiority over OT solutions, in terms of correctness, time\nand space complexity, simplicity, etc. Over one decade later, however, OT\nremains the choice for building the vast majority of co-editors, whereas CRDT\nis rarely found in working co-editors. Why? To seek truth from facts, we set\nout to conduct a comprehensive and critical review of representative OT and\nCRDT solutions and working co-editors based on them. From this work, we have\nmade important discoveries about OT and CRDT, and revealed facts and evidences\nthat refute CRDT claims over OT on all accounts. We present our discoveries in\nthree related and complementary articles.\n  In prior two articles, we have revealed the similarities of OT and CRDT in\nfollowing the same general transformation approach in co-editors, and their\nreal differences in correctness and complexity. In this article, we examine the\nrole of building working co-editors in shaping OT and CRDT research and\nsolutions, and consequential differences in the choice between OT and CRDT in\nreal world co-editors and industry products. In particular, we review the\nevolution of co-editors from research vehicles to real world applications, and\ndiscuss representative OT-based co-editors and alternative approaches in\nindustry products and open source projects. Moreover, we evaluate CRDT-based\nco-editors in relation to published CRDT solutions, and clarify some myths\nsurrounding \"peer-to-peer\" co-editing.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 10:10:28 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 15:15:23 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Sun", "David", ""], ["Sun", "Chengzheng", ""], ["Ng", "Agustina", ""], ["Cai", "Weiwei", ""]]}, {"id": "1905.01518", "submitter": "Chengzheng Sun", "authors": "Chengzheng Sun, David Sun, Agustina, Weiwei Cai", "title": "Real Differences between OT and CRDT under a General Transformation\n  Framework for Consistency Maintenance in Co-Editors", "comments": "25 pages. This is the author's version of the revised and published\n  article at PACMHCI, Vol.4. GROUP, Article 6, January 2020:\n  https://doi.org/10.1145/3375186. arXiv admin note: substantial text overlap\n  with arXiv:1810.02137, arXiv:1905.01302, arXiv:1905.01517", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  OT (Operational Transformation) was invented for supporting real-time\nco-editors in the late 1980s and has evolved to become a core technique used in\ntoday's working co-editors and adopted in major industrial products. CRDT\n(Commutative Replicated Data Type) for co-editors was first proposed around\n2006, under the name of WOOT (WithOut Operational Transformation). Follow-up\nCRDT variations are commonly labeled as \"post-OT\" techniques capable of making\nconcurrent operations natively commutative in co-editors. On top of that, CRDT\nsolutions have made broad claims of superiority over OT solutions, and\nroutinely portrayed OT as an incorrect, complex and inefficient technique. Over\none decade later, however, OT remains the choice for building the vast majority\nof co-editors, whereas CRDT is rarely found in working co-editors.\nContradictions between the reality and CRDT's purported advantages have been\nthe source of much confusion and debate in co-editing communities. Have the\nvast majority of co-editors been unfortunate in choosing the faulty and\ninferior OT, or those CRDT claims are false? What are the real differences\nbetween OT and CRDT for co-editors? What are the key factors and underlying\nreasons behind the choices between OT and CRDT in the real world? To seek truth\nfrom facts, we set out to conduct a comprehensive and critical review on\nrepresentative OT and CRDT solutions and working co-editors based on them. From\nthis work, we have made important discoveries about OT and CRDT, and revealed\nfacts and evidences that refute CRDT claims over OT on all accounts. We report\nour discoveries in a series of three articles and the current article is the\nfirst one in this series. We hope the discoveries from this work help clear up\ncommon misconceptions and confusions surrounding OT and CRDT, and accelerate\nprogress in co-editing technology for real world applications.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 07:22:33 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 15:03:41 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Sun", "Chengzheng", ""], ["Sun", "David", ""], ["Agustina", "", ""], ["Cai", "Weiwei", ""]]}, {"id": "1905.01734", "submitter": "Marcus Scheunemann", "authors": "Marcus M. Scheunemann and Christoph Salge and Kerstin Dautenhahn", "title": "Intrinsically Motivated Autonomy in Human-Robot Interaction: Human\n  Perception of Predictive Information in Robots", "comments": "12 pages, 1 figure, 1 table, Towards Autonomous Robotic Systems\n  (TAROS), 2019", "journal-ref": null, "doi": "10.1007/978-3-030-23807-0_27", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a fully autonomous and intrinsically motivated robot\nusable for HRI experiments. We argue that an intrinsically motivated approach\nbased on the Predictive Information formalism, like the one presented here,\ncould provide us with a pathway towards autonomous robot behaviour generation,\nthat is capable of producing behaviour interesting enough for sustaining the\ninteraction with humans and without the need for a human operator in the loop.\nWe present a possible reactive baseline behaviour for comparison for future\nresearch. Participants perceive the baseline and the adaptive, intrinsically\nmotivated behaviour differently. In our exploratory study we see evidence that\nparticipants perceive an intrinsically motivated robot as less intelligent than\nthe reactive baseline behaviour. We argue that is mostly due to the high\nadaptation rate chosen and the design of the environment. However, we also see\nthat the adaptive robot is perceived as more warm, a factor which carries more\nweight in interpersonal interaction than competence.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 19:01:24 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Scheunemann", "Marcus M.", ""], ["Salge", "Christoph", ""], ["Dautenhahn", "Kerstin", ""]]}, {"id": "1905.01817", "submitter": "Song Gao", "authors": "Yuhao Kang, Qingyuan Jia, Song Gao, Xiaohuan Zeng, Yueyao Wang,\n  Stephan Angsuesser, Yu Liu, Xinyue Ye, Teng Fei", "title": "Extracting human emotions at different places based on facial\n  expressions and spatial clustering analysis", "comments": "40 pages; 9 figures", "journal-ref": "Transactions in GIS, Year 2019, Volume 23, Issue 3", "doi": "10.1111/tgis.12552", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The emergence of big data enables us to evaluate the various human emotions\nat places from a statistic perspective by applying affective computing. In this\nstudy, a novel framework for extracting human emotions from large-scale\ngeoreferenced photos at different places is proposed. After the construction of\nplaces based on spatial clustering of user generated footprints collected in\nsocial media websites, online cognitive services are utilized to extract human\nemotions from facial expressions using the state-of-the-art computer vision\ntechniques. And two happiness metrics are defined for measuring the human\nemotions at different places. To validate the feasibility of the framework, we\ntake 80 tourist attractions around the world as an example and a happiness\nranking list of places is generated based on human emotions calculated over 2\nmillion faces detected out from over 6 million photos. Different kinds of\ngeographical contexts are taken into consideration to find out the relationship\nbetween human emotions and environmental factors. Results show that much of the\nemotional variation at different places can be explained by a few factors such\nas openness. The research may offer insights on integrating human emotions to\nenrich the understanding of sense of place in geography and in place-based GIS.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 04:10:37 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Kang", "Yuhao", ""], ["Jia", "Qingyuan", ""], ["Gao", "Song", ""], ["Zeng", "Xiaohuan", ""], ["Wang", "Yueyao", ""], ["Angsuesser", "Stephan", ""], ["Liu", "Yu", ""], ["Ye", "Xinyue", ""], ["Fei", "Teng", ""]]}, {"id": "1905.01825", "submitter": "Pawan Kumar Patel", "authors": "Pawan Kr Patel and Amey Karkare", "title": "Accessibility Evaluation of Computer Based Tests", "comments": "9 pages; 15 Figures; 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer-based tests (CBTs) play an important role in the professional career\nof any person. Universities use CBTs for admissions. Further, many large\ncourses use CBTs for evaluation and grading. Almost all software companies use\nCBTs to offer jobs. However, many of these CBTs do not take attention to the\naccessibility barriers for persons with disabilities, specifically visually\nimpaired persons. In this paper, we present a study of accessibility barriers\nin various CBTs as faced by visually impaired persons in India. These barriers\nhave been identified by a questionnaire survey approach. Our analysis of the\nresponses shows that most CBTs do not meet the expectations of visually\nimpaired persons. We conclude the paper with some recommendations to improve\naccessibility.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 05:12:54 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Patel", "Pawan Kr", ""], ["Karkare", "Amey", ""]]}, {"id": "1905.01911", "submitter": "Justin Cheng", "authors": "Justin Cheng, Moira Burke, Elena Goetz Davis", "title": "Understanding Perceptions of Problematic Facebook Use: When People\n  Experience Negative Life Impact and a Lack of Control", "comments": "CHI 2019", "journal-ref": null, "doi": "10.1145/3290605.3300429", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many people use social network sites to connect with friends and\nfamily, some feel that their use is problematic, seriously affecting their\nsleep, work, or life. Pairing a survey of 20,000 Facebook users measuring\nperceptions of problematic use with behavioral and demographic data, we\nexamined Facebook activities associated with problematic use as well as the\nkinds of people most likely to experience it. People who feel their use is\nproblematic are more likely to be younger, male, and going through a major life\nevent such as a breakup. They spend more time on the platform, particularly at\nnight, and spend proportionally more time looking at profiles and less time\nbrowsing their News Feeds. They also message their friends more frequently.\nWhile they are more likely to respond to notifications, they are also more\nlikely to deactivate their accounts, perhaps in an effort to better manage\ntheir time. Further, they are more likely to have seen content about social\nmedia or phone addiction. Notably, people reporting problematic use rate the\nsite as more valuable to them, highlighting the complex relationship between\ntechnology use and well-being. A better understanding of problematic Facebook\nuse can inform the design of context-appropriate and supportive tools to help\npeople become more in control.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 10:27:31 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Cheng", "Justin", ""], ["Burke", "Moira", ""], ["Davis", "Elena Goetz", ""]]}, {"id": "1905.01950", "submitter": "J{\\o}rgen Falck Erichsen M.Sc.", "authors": "Jorgen F. Erichsen, Heikki Sj\\\"oman, Martin Steinert, Torgeir Welo", "title": "Digitally Capturing Physical Prototypes During Early-Stage Engineering\n  Design Projects for Initial Analysis of Project Output and Progression", "comments": "27 pages, 2 tables, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to help researchers capture output from the early stages of\nengineering design projects, this article presents a new research tool for\ndigitally capturing physical prototypes. The motivation for this work is to\ncollect observations that can aid in understanding prototyping in the early\nstages of engineering design projects, and this article investigates if and how\ndigital capture of physical prototypes can be used for this purpose.\nEarly-stage prototypes are usually rough and of low-fidelity and are thus often\ndiscarded or substantially modified through the projects. Hence, retrospective\naccess to prototypes is a challenge when trying to gather accurate empirical\ndata. To capture the prototypes developed through the early stages of a\nproject, a new research tool has been developed for capturing prototypes\nthrough multi-view images, along with metadata describing by whom, why, when\nand where the prototypes were captured. Over the course of 17 months, this\nresearch tool has been used to capture more than 800 physical prototypes from\n76 individual users across many projects. In this article, one project is shown\nin detail to demonstrate how this capturing system can gather empirical data\nfor enriching engineering design project cases that focus on prototyping for\nconcept generation. The authors also analyse the metadata provided by the\nsystem to give understanding into prototyping patterns in the projects. Lastly,\nthrough enabling digital capture of large quantities of data, the research tool\npresents the foundations for training artificial intelligence-based predictors\nand classifiers that can be used for analysis in engineering design research.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2019 09:04:37 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 09:08:16 GMT"}], "update_date": "2020-03-20", "authors_parsed": [["Erichsen", "Jorgen F.", ""], ["Sj\u00f6man", "Heikki", ""], ["Steinert", "Martin", ""], ["Welo", "Torgeir", ""]]}, {"id": "1905.01984", "submitter": "Bin Guo", "authors": "Qiuyun Zhang, Bin Guo, Hao Wang, Yunji Liang, Shaoyang Hao, Zhiwen Yu", "title": "AI-Powered Text Generation for Harmonious Human-Machine Interaction:\n  Current State and Future Directions", "comments": "Accepted by IEEE UIC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades, the landscape of text generation has undergone\ntremendous changes and is being reshaped by the success of deep learning. New\ntechnologies for text generation ranging from template-based methods to neural\nnetwork-based methods emerged. Meanwhile, the research objectives have also\nchanged from generating smooth and coherent sentences to infusing personalized\ntraits to enrich the diversification of newly generated content. With the rapid\ndevelopment of text generation solutions, one comprehensive survey is urgent to\nsummarize the achievements and track the state of the arts. In this survey\npaper, we present the general systematical framework, illustrate the widely\nutilized models and summarize the classic applications of text generation.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 23:26:38 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Zhang", "Qiuyun", ""], ["Guo", "Bin", ""], ["Wang", "Hao", ""], ["Liang", "Yunji", ""], ["Hao", "Shaoyang", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1905.02058", "submitter": "Philipp M\\\"uller", "authors": "Philipp M\\\"uller and Andreas Bulling", "title": "Emergent Leadership Detection Across Datasets", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of emergent leaders in small groups from nonverbal\nbehaviour is a growing research topic in social signal processing but existing\nmethods were evaluated on single datasets -- an unrealistic assumption for\nreal-world applications in which systems are required to also work in settings\nunseen at training time. It therefore remains unclear whether current methods\nfor emergent leadership detection generalise to similar but new settings and to\nwhich extent. To overcome this limitation, we are the first to study a\ncross-dataset evaluation setting for the emergent leadership detection task. We\nprovide evaluations for within- and cross-dataset prediction using two current\ndatasets (PAVIS and MPIIGroupInteraction), as well as an investigation on the\nrobustness of commonly used feature channels (visual focus of attention, body\npose, facial action units, speaking activity) and online prediction in the\ncross-dataset setting. Our evaluations show that using pose and eye contact\nbased features, cross-dataset prediction is possible with an accuracy of 0.68,\nas such providing another important piece of the puzzle towards emergent\nleadership detection in the real world.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 14:20:39 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["M\u00fcller", "Philipp", ""], ["Bulling", "Andreas", ""]]}, {"id": "1905.02486", "submitter": "Anush Sankaran", "authors": "Srikanth Tamilselvam, Naveen Panwar, Shreya Khare, Rahul Aralikatte,\n  Anush Sankaran, Senthil Mani", "title": "A Visual Programming Paradigm for Abstract Deep Learning Model\n  Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is one of the fastest growing technologies in computer science\nwith a plethora of applications. But this unprecedented growth has so far been\nlimited to the consumption of deep learning experts. The primary challenge\nbeing a steep learning curve for learning the programming libraries and the\nlack of intuitive systems enabling non-experts to consume deep learning.\nTowards this goal, we study the effectiveness of a no-code paradigm for\ndesigning deep learning models. Particularly, a visual drag-and-drop interface\nis found more efficient when compared with the traditional programming and\nalternative visual programming paradigms. We conduct user studies of different\nexpertise levels to measure the entry level barrier and the developer load\nacross different programming paradigms. We obtain a System Usability Scale\n(SUS) of 90 and a NASA Task Load index (TLX) score of 21 for the proposed\nvisual programming compared to 68 and 52, respectively, for the traditional\nprogramming methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 11:54:20 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 17:24:30 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Tamilselvam", "Srikanth", ""], ["Panwar", "Naveen", ""], ["Khare", "Shreya", ""], ["Aralikatte", "Rahul", ""], ["Sankaran", "Anush", ""], ["Mani", "Senthil", ""]]}, {"id": "1905.02691", "submitter": "Patrick M. Pilarski", "authors": "Patrick M. Pilarski, Andrew Butcher, Michael Johanson, Matthew M.\n  Botvinick, Andrew Bolt, Adam S. R. Parker", "title": "Learned human-agent decision-making, communication and joint action in a\n  virtual reality environment", "comments": "5 pages, 3 figures. Accepted to The 4th Multidisciplinary Conference\n  on Reinforcement Learning and Decision Making, July 7-10, 2019, McGill\n  University, Montreal, Quebec, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans make decisions and act alongside other humans to pursue both\nshort-term and long-term goals. As a result of ongoing progress in areas such\nas computing science and automation, humans now also interact with non-human\nagents of varying complexity as part of their day-to-day activities;\nsubstantial work is being done to integrate increasingly intelligent machine\nagents into human work and play. With increases in the cognitive, sensory, and\nmotor capacity of these agents, intelligent machinery for human assistance can\nnow reasonably be considered to engage in joint action with humans---i.e., two\nor more agents adapting their behaviour and their understanding of each other\nso as to progress in shared objectives or goals. The mechanisms, conditions,\nand opportunities for skillful joint action in human-machine partnerships is of\ngreat interest to multiple communities. Despite this, human-machine joint\naction is as yet under-explored, especially in cases where a human and an\nintelligent machine interact in a persistent way during the course of\nreal-time, daily-life experience. In this work, we contribute a virtual reality\nenvironment wherein a human and an agent can adapt their predictions, their\nactions, and their communication so as to pursue a simple foraging task. In a\ncase study with a single participant, we provide an example of human-agent\ncoordination and decision-making involving prediction learning on the part of\nthe human and the machine agent, and control learning on the part of the\nmachine agent wherein audio communication signals are used to cue its human\npartner in service of acquiring shared reward. These comparisons suggest the\nutility of studying human-machine coordination in a virtual reality\nenvironment, and identify further research that will expand our understanding\nof persistent human-machine joint action.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:53:48 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Pilarski", "Patrick M.", ""], ["Butcher", "Andrew", ""], ["Johanson", "Michael", ""], ["Botvinick", "Matthew M.", ""], ["Bolt", "Andrew", ""], ["Parker", "Adam S. R.", ""]]}, {"id": "1905.02812", "submitter": "Christopher Mendez", "authors": "Christopher Mendez, Lara Letaw, Margaret Burnett, Simone Stumpf, Anita\n  Sarma, Claudia Hilderbrand", "title": "From GenderMag to InclusiveMag: An Inclusive Design Meta-Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can software practitioners assess whether their software supports diverse\nusers? Although there are empirical processes that can be used to find\n\"inclusivity bugs\" piecemeal, what is often needed is a systematic inspection\nmethod to assess soft-ware's support for diverse populations. To help fill this\ngap, this paper introduces InclusiveMag, a generalization of GenderMag that can\nbe used to generate systematic inclusiveness methods for a particular dimension\nof diversity. We then present a multi-case study covering eight diversity\ndimensions, of eight teams' experiences applying InclusiveMag to eight\nunder-served populations and their \"mainstream\" counterparts.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 21:07:56 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Mendez", "Christopher", ""], ["Letaw", "Lara", ""], ["Burnett", "Margaret", ""], ["Stumpf", "Simone", ""], ["Sarma", "Anita", ""], ["Hilderbrand", "Claudia", ""]]}, {"id": "1905.02813", "submitter": "Zoe Steine-Hanson", "authors": "Zoe Steine-Hanson, Claudia Hilderbrand, Lara Letaw, Jillian Emard,\n  Christopher Perdriau, Christopher Mendez, Margaret Burnett, Anita Sarma", "title": "Fixing Inclusivity Bugs for Information Processing Styles and Learning\n  Styles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most software systems today do not support cognitive diversity. Further,\nbecause of differences in problem-solving styles that cluster by gender,\nsoftware that poorly supports cognitive diversity can also embed gender biases.\nTo help software professionals fix gender bias \"bugs\" related to people's\nproblem-solving styles for information processing and learning of new software\nwe collected inclusivity fixes from three sources. The first two are empirical\nstudies we conducted: a heuristics-driven user study and a field research\nindustry study. The third is data that we obtained about a before/after user\nstudy of inclusivity bugs. The resulting seven potential inclusivity fixes show\nhow to debug software to be more inclusive for diverse problem-solving styles.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 21:13:10 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Steine-Hanson", "Zoe", ""], ["Hilderbrand", "Claudia", ""], ["Letaw", "Lara", ""], ["Emard", "Jillian", ""], ["Perdriau", "Christopher", ""], ["Mendez", "Christopher", ""], ["Burnett", "Margaret", ""], ["Sarma", "Anita", ""]]}, {"id": "1905.02823", "submitter": "Stephen Bottos", "authors": "Stephen Bottos, Balakumar Balasingam", "title": "Tracking the Progression of Reading Through Eye-gaze Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of tracking the progression of reading\nthrough eye-gaze measurements. Such an algorithm is novel and will ultimately\nhelp to develop a method of analyzing eye-gaze data which had been collected\nduring reading activity in order to uncover crucial information regarding the\nindividual's interest level and quality of experience while reading a passage\nof text or book. Additionally, such an approach will serve as a \"visual\nsignature\" - a method of verifying if an individual has indeed given adequate\nattention to critical text-based information. Further, an accurate\n\"reading-progression-tracker\" has potential applications in educational\ninstitutions, e-readers and parenting solutions. Tracking the progression of\nreading remains a challenging problem due to the fact that eye-gaze movements\nare highly noisy and the eye-gaze is easily distracted in a limited space, like\nan e-book. In a prior work, we proposed an approach to analyze eye-gaze\nfixation points collected while reading a page of text in order to classify\neach measurement to a line of text; this approach did not consider tracking the\nprogression of reading along the line of text. In this paper, we extend the\ncapabilities of the previous algorithm in order to accurately track the\nprogression of reading along each line. the proposed approach employs least\nsquares batch estimation in order to estimate three states of the horizontal\nsaccade: position, velocity and acceleration. First, the proposed approach is\nobjectively evaluated on a simulated eye-gaze dataset. Then, the proposed\nalgorithm is demonstrated on real data collected by a Gazepoint eye-tracker\nwhile the subject is reading several pages from an electronic book.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 22:17:33 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Bottos", "Stephen", ""], ["Balasingam", "Balakumar", ""]]}, {"id": "1905.02853", "submitter": "Nils Gehlenborg", "authors": "Sabrina Nusrat, Theresa Harbig, Nils Gehlenborg", "title": "Tasks, Techniques, and Tools for Genomic Data Visualization", "comments": "25 pages, 21 figures, 6 tables", "journal-ref": null, "doi": "10.1111/cgf.13727", "report-no": null, "categories": "q-bio.GN cs.HC q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Genomic data visualization is essential for interpretation and hypothesis\ngeneration as well as a valuable aid in communicating discoveries. Visual tools\nbridge the gap between algorithmic approaches and the cognitive skills of\ninvestigators. Addressing this need has become crucial in genomics, as\nbiomedical research is increasingly data-driven and many studies lack\nwell-defined hypotheses. A key challenge in data-driven research is to discover\nunexpected patterns and to formulate hypotheses in an unbiased manner in vast\namounts of genomic and other associated data. Over the past two decades, this\nhas driven the development of numerous data visualization techniques and tools\nfor visualizing genomic data. Based on a comprehensive literature survey, we\npropose taxonomies for data, visualization, and tasks involved in genomic data\nvisualization. Furthermore, we provide a comprehensive review of published\ngenomic visualization tools in the context of the proposed taxonomies.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 00:53:22 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Nusrat", "Sabrina", ""], ["Harbig", "Theresa", ""], ["Gehlenborg", "Nils", ""]]}, {"id": "1905.02994", "submitter": "Philipp Wei{\\ss}", "authors": "Martin Schuessler, Philipp Wei{\\ss}", "title": "Minimalistic Explanations: Capturing the Essence of Decisions", "comments": null, "journal-ref": "ACM CHI Extended Abstracts (2019) Paper No. LBW2810", "doi": "10.1145/3290607.3312823", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of complex machine learning models can make systems opaque to users.\nMachine learning research proposes the use of post-hoc explanations. However,\nit is unclear if they give users insights into otherwise uninterpretable\nmodels. One minimalistic way of explaining image classifications by a deep\nneural network is to show only the areas that were decisive for the assignment\nof a label. In a pilot study, 20 participants looked at 14 of such explanations\ngenerated either by a human or the LIME algorithm. For explanations of correct\ndecisions, they identified the explained object with significantly higher\naccuracy (75.64% vs. 18.52%). We argue that this shows that explanations can be\nvery minimalistic while retaining the essence of a decision, but the\ndecision-making contexts that can be conveyed in this manner is limited.\nFinally, we found that explanations are unique to the explainer and\nhuman-generated explanations were assigned 79% higher trust ratings. As a\nstarting point for further studies, this work shares our first insights into\nquality criteria of post-hoc explanations.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 10:15:20 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Schuessler", "Martin", ""], ["Wei\u00df", "Philipp", ""]]}, {"id": "1905.03302", "submitter": "Priyadarshini Kumari", "authors": "Priyadarshini Kumari, Siddhartha Chaudhuri, and Subhasis Chaudhuri", "title": "PerceptNet: Learning Perceptual Similarity of Haptic Textures in\n  Presence of Unorderable Triplets", "comments": "Published in IEEE World Haptics Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to design haptic icons or build a haptic vocabulary, we require a\nset of easily distinguishable haptic signals to avoid perceptual ambiguity,\nwhich in turn requires a way to accurately estimate the perceptual\n(dis)similarity of such signals. In this work, we present a novel method to\nlearn such a perceptual metric based on data from human studies. Our method is\nbased on a deep neural network that projects signals to an embedding space\nwhere the natural Euclidean distance accurately models the degree of\ndissimilarity between two signals. The network is trained only on non-numerical\ncomparisons of triplets of signals, using a novel triplet loss that considers\nboth types of triplets that are easy to order (inequality constraints), as well\nas those that are unorderable/ambiguous (equality constraints). Unlike prior\nMDS-based non-parametric approaches, our method can be trained on a partial set\nof comparisons and can embed new haptic signals without retraining the model\nfrom scratch. Extensive experimental evaluations show that our method is\nsignificantly more effective at modeling perceptual dissimilarity than\nalternatives.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 19:06:39 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 07:23:28 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kumari", "Priyadarshini", ""], ["Chaudhuri", "Siddhartha", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "1905.03570", "submitter": "Lamiaa Elrefaei", "authors": "Lamiaa A. Elrefaei, Bshaer Azan, Sameera Hakami, Safiah Melebari", "title": "Jcave: A 3D Interactive Game to Assist Home Physiotherapy Rehabilitation", "comments": null, "journal-ref": null, "doi": "10.5121/ijma.2019.11201", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to investigate the applicability of applying\ngamification approach on the physiotherapy rehabilitation. A new developing\ngame called JCave was designed and developed for the prove of concept. The\npropose game target the children from six to twelve years of age who need\nphysical therapy in their upper limbs. JCave is a finite and multilevel\nsingle-player 3D video game. The player's role is to collect jewels from a cave\nand increase his/her score by performing physical therapy exercises. The game\nuses Xbox360 Kinect as a motion capture camera to observe gestures and track\nthe child. Automatic gesture recognition algorithms are implemented for elbow\nflexion-extension exercises and shoulder flexion, which are the active range of\nmotion (AROM) exercises for both the right and left arms. The JCave game is\nimplemented using Unity3D and Blender to design 3D model objects.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 12:27:45 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Elrefaei", "Lamiaa A.", ""], ["Azan", "Bshaer", ""], ["Hakami", "Sameera", ""], ["Melebari", "Safiah", ""]]}, {"id": "1905.03638", "submitter": "Ruixue Liu", "authors": "Ruixue Liu, Baoyang Chen, Meng Chen, Youzheng Wu, Zhijie Qiu, Xiaodong\n  He", "title": "Mappa Mundi: An Interactive Artistic Mind Map Generator with Artificial\n  Imagination", "comments": "Paper accepted by IJCAI 2019 Demo track", "journal-ref": null, "doi": null, "report-no": "978-0-9992411-4-1", "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel real-time, collaborative, and interactive AI painting\nsystem, Mappa Mundi, for artistic Mind Map creation. The system consists of a\nvoice-based input interface, an automatic topic expansion module, and an image\nprojection module. The key innovation is to inject Artificial Imagination into\npainting creation by considering lexical and phonological similarities of\nlanguage, learning and inheriting artist's original painting style, and\napplying the principles of Dadaism and impossibility of improvisation. Our\nsystem indicates that AI and artist can collaborate seamlessly to create\nimaginative artistic painting and Mappa Mundi has been applied in art\nexhibition in UCCA, Beijing\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 13:51:46 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 11:22:59 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Liu", "Ruixue", ""], ["Chen", "Baoyang", ""], ["Chen", "Meng", ""], ["Wu", "Youzheng", ""], ["Qiu", "Zhijie", ""], ["He", "Xiaodong", ""]]}, {"id": "1905.03676", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Ruben Vera-Rodriguez, Richard Guest, Julian Fierrez\n  and Javier Ortega-Garcia", "title": "Exploiting Complexity in Pen- and Touch-based Signature Biometrics", "comments": null, "journal-ref": "International Journal on Document Analysis and Recognition, 2019", "doi": "10.1007/s10032-020-00351-3", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biometric signature verification has been traditionally performed in\npen-based office-like scenarios using devices specifically designed for\nacquiring handwriting. However, the high deployment of devices such as\nsmartphones and tablets has given rise to new and thriving scenarios for\nsignature biometrics where handwriting can be performed using not only a pen\nstylus but also the finger via touch interaction. Some preliminary studies have\nhighlighted the challenge of this new scenario and the necessity of further\nresearch on the topic. The main contribution of this work is to propose a new\non-line signature verification architecture adapted to the signature complexity\nin order to tackle this new and challenging scenario. Additionally, an\nexhaustive comparative analysis of both pen- and touch-based scenarios using\nour proposed methodology is carried out along with a review of the most\nrelevant and recent studies in the field. Significant improvements of biometric\nverification performance and practical insights are extracted for the\napplication of signature verification in real scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 15:03:23 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 08:46:05 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Guest", "Richard", ""], ["Fierrez", "Julian", ""], ["Ortega-Garcia", "Javier", ""]]}, {"id": "1905.03809", "submitter": "Kim Phuc Tran", "authors": "H.D. Nguyen, K.P. Tran, X. Zeng, L. Koehl, and G. Tartare", "title": "Wearable Sensor Data Based Human Activity Recognition using Machine\n  Learning: A new approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed the rapid development of human activity\nrecognition (HAR) based on wearable sensor data. One can find many practical\napplications in this area, especially in the field of health care. Many machine\nlearning algorithms such as Decision Trees, Support Vector Machine, Naive\nBayes, K-Nearest Neighbor, and Multilayer Perceptron are successfully used in\nHAR. Although these methods are fast and easy for implementation, they still\nhave some limitations due to poor performance in a number of situations. In\nthis paper, we propose a novel method based on the ensemble learning to boost\nthe performance of these machine learning methods for HAR.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 18:28:10 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Nguyen", "H. D.", ""], ["Tran", "K. P.", ""], ["Zeng", "X.", ""], ["Koehl", "L.", ""], ["Tartare", "G.", ""]]}, {"id": "1905.03911", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Oh-Hyun Kwon, Kwan-Liu Ma", "title": "Supporting Analysis of Dimensionality Reduction Results with Contrastive\n  Learning", "comments": "This is the author's version of the article that has been published\n  in IEEE Transactions on Visualization and Computer Graphics. The final\n  version of this record is available at: 10.1109/TVCG.2019.2934251", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934251", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) is frequently used for analyzing and\nvisualizing high-dimensional data as it provides a good first glance of the\ndata. However, to interpret the DR result for gaining useful insights from the\ndata, it would take additional analysis effort such as identifying clusters and\nunderstanding their characteristics. While there are many automatic methods\n(e.g., density-based clustering methods) to identify clusters, effective\nmethods for understanding a cluster's characteristics are still lacking. A\ncluster can be mostly characterized by its distribution of feature values.\nReviewing the original feature values is not a straightforward task when the\nnumber of features is large. To address this challenge, we present a visual\nanalytics method that effectively highlights the essential features of a\ncluster in a DR result. To extract the essential features, we introduce an\nenhanced usage of contrastive principal component analysis (cPCA). Our method,\ncalled ccPCA (contrasting clusters in PCA), can calculate each feature's\nrelative contribution to the contrast between one cluster and other clusters.\nWith ccPCA, we have created an interactive system including a scalable\nvisualization of clusters' feature contributions. We demonstrate the\neffectiveness of our method and system with case studies using several publicly\navailable datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 02:07:58 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 04:55:59 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 00:01:18 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Kwon", "Oh-Hyun", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1905.04000", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Jia-Kai Chou, Shilpika, Panpan Xu, Liu Ren,\n  Kwan-Liu Ma", "title": "An Incremental Dimensionality Reduction Method for Visualizing Streaming\n  Multidimensional Data", "comments": "This is the author's version of the article that has been published\n  in IEEE Transactions on Visualization and Computer Graphics. The final\n  version of this record is available at: 10.1109/TVCG.2019.2934433", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934433", "report-no": null, "categories": "cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction (DR) methods are commonly used for analyzing and\nvisualizing multidimensional data. However, when data is a live streaming feed,\nconventional DR methods cannot be directly used because of their computational\ncomplexity and inability to preserve the projected data positions at previous\ntime points. In addition, the problem becomes even more challenging when the\ndynamic data records have a varying number of dimensions as often found in\nreal-world applications. This paper presents an incremental DR solution. We\nenhance an existing incremental PCA method in several ways to ensure its\nusability for visualizing streaming multidimensional data. First, we use\ngeometric transformation and animation methods to help preserve a viewer's\nmental map when visualizing the incremental results. Second, to handle data\ndimension variants, we use an optimization method to estimate the projected\ndata positions, and also convey the resulting uncertainty in the visualization.\nWe demonstrate the effectiveness of our design with two case studies using\nreal-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 08:15:42 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 05:38:20 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 04:16:00 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Chou", "Jia-Kai", ""], ["Shilpika", "", ""], ["Xu", "Panpan", ""], ["Ren", "Liu", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "1905.04071", "submitter": "Jan Deriu", "authors": "Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie\n  Rosset, Eneko Agirre, Mark Cieliebak", "title": "Survey on Evaluation Methods for Dialogue Systems", "comments": null, "journal-ref": "Artificial Intelligence Review, June 2020", "doi": "10.1007/s10462-020-09866-x", "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we survey the methods and concepts developed for the evaluation\nof dialogue systems. Evaluation is a crucial part during the development\nprocess. Often, dialogue systems are evaluated by means of human evaluations\nand questionnaires. However, this tends to be very cost and time intensive.\nThus, much work has been put into finding methods, which allow to reduce the\ninvolvement of human labour. In this survey, we present the main concepts and\nmethods. For this, we differentiate between the various classes of dialogue\nsystems (task-oriented dialogue systems, conversational dialogue systems, and\nquestion-answering dialogue systems). We cover each class by introducing the\nmain technologies developed for the dialogue systems and then by presenting the\nevaluation methods regarding this class.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:14:12 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 08:07:53 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Deriu", "Jan", ""], ["Rodrigo", "Alvaro", ""], ["Otegi", "Arantxa", ""], ["Echegoyen", "Guillermo", ""], ["Rosset", "Sophie", ""], ["Agirre", "Eneko", ""], ["Cieliebak", "Mark", ""]]}, {"id": "1905.04149", "submitter": "Xiang Zhang", "authors": "Xiang Zhang, Lina Yao, Xianzhi Wang, Jessica Monaghan, David Mcalpine,\n  Yu Zhang", "title": "A Survey on Deep Learning-based Non-Invasive Brain Signals:Recent\n  Advances and New Frontiers", "comments": "Accepted by Journal of Neural Engineering. Summarized more than 200+\n  brain signal-related papers, systematically covering 8 Brain-Computer\n  Interface (BCI) categories and 10+ deep learning models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interface (BCI) bridges the human's neural world and the outer\nphysical world by decoding individuals' brain signals into commands\nrecognizable by computer devices. Deep learning has lifted the performance of\nbrain-computer interface systems significantly in recent years. In this\narticle, we systematically investigate brain signal types for BCI and related\ndeep learning concepts for brain signal analysis. We then present a\ncomprehensive survey of deep learning techniques used for BCI, by summarizing\nover 230 contributions most published in the past five years. Finally, we\ndiscuss the applied areas, opening challenges, and future directions for deep\nlearning-based BCI.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 13:04:00 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 02:26:12 GMT"}, {"version": "v3", "created": "Sat, 15 Jun 2019 13:32:22 GMT"}, {"version": "v4", "created": "Sat, 26 Oct 2019 06:29:35 GMT"}, {"version": "v5", "created": "Wed, 21 Oct 2020 23:44:10 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Zhang", "Xiang", ""], ["Yao", "Lina", ""], ["Wang", "Xianzhi", ""], ["Monaghan", "Jessica", ""], ["Mcalpine", "David", ""], ["Zhang", "Yu", ""]]}, {"id": "1905.04218", "submitter": "Aran Sena", "authors": "Aran Sena, Matthew J Howard", "title": "Quantifying Teaching Behaviour in Robot Learning from Demonstration", "comments": "Preprint for International Journal of Robotics Research (IJRR)\n  submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from demonstration allows for rapid deployment of robot manipulators\nto a great many tasks, by relying on a person showing the robot what to do\nrather than programming it. While this approach provides many opportunities,\nmeasuring, evaluating and improving the person's teaching ability has remained\nlargely unexplored in robot manipulation research. To this end, a model for\nlearning from demonstration is presented here which incorporates the teacher's\nunderstanding of, and influence on, the learner. The proposed model is used to\nclarify the teacher's objectives during learning from demonstration, providing\nnew views on how teaching failures and efficiency can be defined. The benefit\nof this approach is shown in two experiments (N=30 and N=36, respectively),\nwhich highlight the difficulty teachers have in providing effective\ndemonstrations, and show how ~169-180% improvement in teaching efficiency can\nbe achieved through evaluation and feedback shaped by the proposed framework,\nrelative to unguided teaching.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 15:30:25 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Sena", "Aran", ""], ["Howard", "Matthew J", ""]]}, {"id": "1905.04225", "submitter": "Okan K\\\"op\\\"ukl\\\"u", "authors": "Okan K\\\"op\\\"ukl\\\"u, Yao Rong, Gerhard Rigoll", "title": "Talking With Your Hands: Scaling Hand Gestures and Recognition With CNNs", "comments": "Accepted to ICCV 2019 workshop - Observing and Understanding Hands in\n  Action (HANDS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of hand gestures provides a natural alternative to cumbersome\ninterface devices for Human-Computer Interaction (HCI) systems. As the\ntechnology advances and communication between humans and machines becomes more\ncomplex, HCI systems should also be scaled accordingly in order to accommodate\nthe introduced complexities. In this paper, we propose a methodology to scale\nhand gestures by forming them with predefined gesture-phonemes, and a\nconvolutional neural network (CNN) based framework to recognize hand gestures\nby learning only their constituents of gesture-phonemes. The total number of\npossible hand gestures can be increased exponentially by increasing the number\nof used gesture-phonemes. For this objective, we introduce a new benchmark\ndataset named Scaled Hand Gestures Dataset (SHGD) with only gesture-phonemes in\nits training set and 3-tuples gestures in the test set. In our experimental\nanalysis, we achieve to recognize hand gestures containing one and three\ngesture-phonemes with an accuracy of 98.47% (in 15 classes) and 94.69% (in 810\nclasses), respectively. Our dataset, code and pretrained models are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 15:49:16 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 08:28:07 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["K\u00f6p\u00fckl\u00fc", "Okan", ""], ["Rong", "Yao", ""], ["Rigoll", "Gerhard", ""]]}, {"id": "1905.04577", "submitter": "Suzan Verberne", "authors": "Suzan Verberne, Jiyin He, Gineke Wiggers, Tony Russell-Rose, Udo\n  Kruschwitz, Arjen P. de Vries", "title": "Information search in a professional context - exploring a collection of\n  professional search tasks", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search conducted in a work context is an everyday activity that has been\naround since long before the Web was invented, yet we still seem to understand\nlittle about its general characteristics. With this paper we aim to contribute\nto a better understanding of this large but rather multi-faceted area of\n`professional search'. Unlike task-based studies that aim at measuring the\neffectiveness of search methods, we chose to take a step back by conducting a\nsurvey among professional searchers to understand their typical search tasks.\nBy doing so we offer complementary insights into the subject area. We asked our\nrespondents to provide actual search tasks they have worked on, information\nabout how these were conducted and details on how successful they eventually\nwere. We then manually coded the collection of 56 search tasks with task\ncharacteristics and relevance criteria, and used the coded dataset for\nexploration purposes. Despite the relatively small scale of this study, our\ndata provides enough evidence that professional search is indeed very different\nfrom Web search in many key respects and that this is a field that offers many\navenues for future research.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 19:05:20 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Verberne", "Suzan", ""], ["He", "Jiyin", ""], ["Wiggers", "Gineke", ""], ["Russell-Rose", "Tony", ""], ["Kruschwitz", "Udo", ""], ["de Vries", "Arjen P.", ""]]}, {"id": "1905.04615", "submitter": "Jason R.C. Nurse Dr", "authors": "Oliver Buckley and Jason R. C. Nurse", "title": "The Language of Biometrics: Analysing Public Perceptions", "comments": null, "journal-ref": "Journal of Information Security and Applications, Volume 47,\n  August 2019, Pages 112-119", "doi": "10.1016/j.jisa.2019.05.001", "report-no": null, "categories": "cs.CR cs.CY cs.ET cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing shift in technology towards biometric solutions, but\none of the biggest barriers to widespread use is the acceptance by the users.\nIn this paper we investigate the understanding, awareness and acceptance of\nbiometrics by the general public. The primary research method was a survey,\nwhich had 282 respondents, designed to gauge public opinion around biometrics.\nAdditionally, qualitative data was captured in the form of the participants'\ndefinition of the term \\textit{biometrics}. We applied thematic analysis as\nwell as an automated Word Vector analysis to this data to provide a deeper\ninsight into the perceptions and understanding of the term. Our results\ndemonstrate that while there is generally a reasonable level of understanding\nof what biometrics are, this is typically limited to the techniques that are\nmost familiar to participants (e.g., fingerprints or facial recognition). Most\nnotably individuals' awareness overlooks emerging areas such as behavioural\nbiometrics (e.g., gait). This was also apparent when we compared participants'\nviews to definitions provided by official, published sources (e.g., ISO, NIST,\nOED, DHS). Overall, this article provides unique insight into the perceptions\nand understanding of biometrics as well as areas where users may lack knowledge\non biometric applications.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 00:45:52 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Buckley", "Oliver", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "1905.04616", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Kevin Hu and Neil Gaikwad and Michiel Bakker and Madelon Hulsebos and\n  Emanuel Zgraggen and C\\'esar Hidalgo and Tim Kraska and Guoliang Li and\n  Arvind Satyanarayan and \\c{C}a\\u{g}atay Demiralp", "title": "VizNet: Towards A Large-Scale Visualization Learning and Benchmarking\n  Repository", "comments": "CHI'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers currently rely on ad hoc datasets to train automated\nvisualization tools and evaluate the effectiveness of visualization designs.\nThese exemplars often lack the characteristics of real-world datasets, and\ntheir one-off nature makes it difficult to compare different techniques. In\nthis paper, we present VizNet: a large-scale corpus of over 31 million datasets\ncompiled from open data repositories and online visualization galleries. On\naverage, these datasets comprise 17 records over 3 dimensions and across the\ncorpus, we find 51% of the dimensions record categorical data, 44%\nquantitative, and only 5% temporal. VizNet provides the necessary common\nbaseline for comparing visualization design techniques, and developing\nbenchmark models and algorithms for automating visual analysis. To demonstrate\nVizNet's utility as a platform for conducting online crowdsourced experiments\nat scale, we replicate a prior study assessing the influence of user task and\ndata distribution on visual encoding effectiveness, and extend it by\nconsidering an additional task: outlier detection. To contend with running such\nstudies at scale, we demonstrate how a metric of perceptual effectiveness can\nbe learned from experimental results, and show its predictive power across test\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 00:47:28 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Hu", "Kevin", ""], ["Gaikwad", "Neil", ""], ["Bakker", "Michiel", ""], ["Hulsebos", "Madelon", ""], ["Zgraggen", "Emanuel", ""], ["Hidalgo", "C\u00e9sar", ""], ["Kraska", "Tim", ""], ["Li", "Guoliang", ""], ["Satyanarayan", "Arvind", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1905.04638", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Wenbo Tao and Xiaoyu Liu and \\c{C}a\\u{g}atay Demiralp and Remco Chang\n  and Michael Stonebraker", "title": "Kyrix: Interactive Visual Data Exploration at Scale", "comments": "CIDR'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable interactive visual data exploration is crucial in many domains due\nto increasingly large datasets generated at rapid rates. Details-on-demand\nprovides a useful interaction paradigm for exploring large datasets, where\nusers start at an overview, find regions of interest, zoom in to see detailed\nviews, zoom out and then repeat. This paradigm is the primary user interaction\nmode of widely-used systems such as Google Maps, Aperture Tiles and ForeCache.\nThese earlier systems, however, are highly customized with hardcoded visual\nrepresentations and optimizations. A more general framework is needed to\nfacilitate the development of visual data exploration systems at scale. In this\npaper, we present Kyrix, an end-to-end system for developing scalable\ndetails-on-demand data exploration applications. Kyrix provides developers with\na declarative model for easy specification of general visualizations. Behind\nthe scenes, Kyrix utilizes a suite of performance optimization techniques to\nachieve a response time within 500ms for various user interactions. We also\nreport results from a performance study which shows that a novel dynamic\nfetching scheme adopted by Kyrix outperforms tile-based fetching used in\nearlier systems.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 03:06:40 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Tao", "Wenbo", ""], ["Liu", "Xiaoyu", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Chang", "Remco", ""], ["Stonebraker", "Michael", ""]]}, {"id": "1905.05144", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Nadia Bianchi-Berthouze, Manuel Oliveira, Catherine\n  Holloway, Simon Julier", "title": "Nose Heat: Exploring Stress-induced Nasal Thermal Variability through\n  Mobile Thermal Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically monitoring and quantifying stress-induced thermal dynamic\ninformation in real-world settings is an extremely important but challenging\nproblem. In this paper, we explore whether we can use mobile thermal imaging to\nmeasure the rich physiological cues of mental stress that can be deduced from a\nperson's nose temperature. To answer this question we build i) a framework for\nmonitoring nasal thermal variable patterns continuously and ii) a novel set of\nthermal variability metrics to capture a richness of the dynamic information.\nWe evaluated our approach in a series of studies including laboratory-based\npsychosocial stress-induction tasks and real-world factory settings. We\ndemonstrate our approach has the potential for assessing stress responses\nbeyond controlled laboratory settings.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 16:59:33 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Cho", "Youngjun", ""], ["Bianchi-Berthouze", "Nadia", ""], ["Oliveira", "Manuel", ""], ["Holloway", "Catherine", ""], ["Julier", "Simon", ""]]}, {"id": "1905.05182", "submitter": "Gregoire Cattan", "authors": "Gijsbrecht Van Veen (GIPSA-VIBS), Alexandre Barachant (CEA-LETI,\n  GIPSA-VIBS), Anton Andreev (GIPSA-Services), Gr\\'egoire Cattan\n  (GIPSA-Services), Pedro Coelho Rodrigues (GIPSA-VIBS), Marco Congedo\n  (GIPSA-VIBS)", "title": "Building Brain Invaders: EEG data of an experimental validation", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.09111", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the experimental procedures for a dataset that we have made\npublicly available at https://doi.org/10.5281/zenodo.2649006 in mat and csv\nformats. This dataset contains electroencephalographic (EEG) recordings of 25\nsubjects testing the Brain Invaders (Congedo, 2011), a visual P300\nBrain-Computer Interface inspired by the famous vintage video game Space\nInvaders (Taito, Tokyo, Japan). The visual P300 is an event-related potential\nelicited by a visual stimulation, peaking 240-600 ms after stimulus onset. EEG\ndata were recorded by 16 electrodes in an experiment that took place in the\nGIPSA-lab, Grenoble, France, in 2012 (Van Veen, 2013 and Congedo, 2013). Python\ncode for manipulating the data is available at\nhttps://github.com/plcrodrigues/py.BI.EEG.2012-GIPSA. The ID of this dataset is\nBI.EEG.2012-GIPSA.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 07:18:52 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Van Veen", "Gijsbrecht", "", "GIPSA-VIBS"], ["Barachant", "Alexandre", "", "CEA-LETI,\n  GIPSA-VIBS"], ["Andreev", "Anton", "", "GIPSA-Services"], ["Cattan", "Gr\u00e9goire", "", "GIPSA-Services"], ["Rodrigues", "Pedro Coelho", "", "GIPSA-VIBS"], ["Congedo", "Marco", "", "GIPSA-VIBS"]]}, {"id": "1905.05222", "submitter": "Jason R.C. Nurse Dr", "authors": "Meredydd Williams and Jason R. C. Nurse and Sadie Creese", "title": "Smartwatch games: Encouraging privacy-protective behaviour in a\n  longitudinal study", "comments": "21 pages, 2 figures", "journal-ref": "Computers in Human Behavior, 2019", "doi": "10.1016/j.chb.2019.04.026", "report-no": null, "categories": "cs.HC cs.CR cs.CY cs.ET cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the public claim concern for their privacy, they frequently appear to\noverlook it. This disparity between concern and behaviour is known as the\nPrivacy Paradox. Such issues are particularly prevalent on wearable devices.\nThese products can store personal data, such as text messages and contact\ndetails. However, owners rarely use protective features. Educational games can\nbe effective in encouraging changes in behaviour. Therefore, we developed the\nfirst privacy game for (Android) Wear OS watches. 10 participants used\nsmartwatches for two months, allowing their high-level settings to be\nmonitored. Five individuals were randomly assigned to our treatment group, and\nthey played a dynamically-customised privacy-themed game. To minimise\nconfounding variables, the other five received the same app but lacking the\nprivacy topic. The treatment group improved their protection, with their usage\nof screen locks significantly increasing (p = 0.043). In contrast, 80% of the\ncontrol group continued to never restrict their settings. After the posttest\nphase, we evaluated behavioural rationale through semi-structured interviews.\nPrivacy concerns became more nuanced in the treatment group, with opinions\naligning with behaviour. Actions appeared influenced primarily by three\nfactors: convenience, privacy salience and data sensitivity. This is the first\nsmartwatch game to encourage privacy-protective behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 18:14:40 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Williams", "Meredydd", ""], ["Nurse", "Jason R. C.", ""], ["Creese", "Sadie", ""]]}, {"id": "1905.05360", "submitter": "Jangho Kwon", "authors": "Jangho Kwon, Laehyun Kim", "title": "Emotion recognition using a glasses-type wearable device via\n  multi-channel facial responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a glasses type wearable device to detect emotions from a human\nface in an unobtrusive manner. The device is designed to gather multi channel\nresponses from the user face naturally and continuously while the user is\nwearing it. The multi channel responses include physiological responses of the\nfacial muscles and organs based on electrodermal activity (EDA) and\nphotoplethysmogram. We conducted experiments to determine the optimal positions\nof EDA sensors on the wearable device because EDA signal quality is very\nsensitive to the sensing position. In addition to the physiological data, the\ndevice can capture the image region representing local facial expressions\naround the left eye via a built in camera. In this study, we developed and\nvalidated an algorithm to recognize emotions using multi channel responses\nobtained from the device. The results show that the emotion recognition\nalgorithm using only local facial expressions has an accuracy of 78 percent at\nclassifying emotions. Using multi channel data, this accuracy was increased by\n10.1 percent. This unobtrusive wearable system with facial multi channel\nresponses is very useful for monitoring a user emotions in daily life, which\nhas a huge potential for use in the healthcare industry.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 02:29:46 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Kwon", "Jangho", ""], ["Kim", "Laehyun", ""]]}, {"id": "1905.05390", "submitter": "Changkun Ou", "authors": "Matthias Geiger, Changkun Ou, Cedric Quintes", "title": "WatchOut: A Road Safety Extension for Pedestrians on a Public Windshield\n  Display", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We conducted a field study to investigate whether public windshield displays\nare applicable as an additional interactive digital road safety warning sign.\nWe focused on investigating the acceptance and usability of our novel public\nwindshield display and its potential use for future applications. The study has\nshown that users are open-minded to the idea of an extraverted windshield\ndisplay regardless the use case, whether it is used for safety purposes or\ndifferent content. Contrary to our hypothesis most people assumed they would\nmistrust the system if it were as well established as traffic lights and\nprimarily rely on their own perception.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 04:42:21 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Geiger", "Matthias", ""], ["Ou", "Changkun", ""], ["Quintes", "Cedric", ""]]}, {"id": "1905.05587", "submitter": "Lea Pillette", "authors": "Aline Roc (Potioc, LaBRI), L\\'ea Pillette (Potioc, LaBRI), B. N'Kaoua\n  (HACS), Fabien Lotte (LaBRI, Potioc)", "title": "Would Motor-Imagery based BCI user training benefit from more women\n  experimenters?", "comments": null, "journal-ref": "8th International BCI Conference, Sep 2019, Graz, Austria", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental Imagery based Brain-Computer Interfaces (MI-BCI) are a mean to control\ndigital technologies by performing MI tasks alone. Throughout MI-BCI use, human\nsupervision (e.g., experimenter or caregiver) plays a central role. While\nproviding emotional and social feedback, people present BCIs to users and\nensure smooth users' progress with BCI use. Though, very little is known about\nthe influence experimenters might have on the results obtained. Such influence\nis to be expected as social and emotional feedback were shown to influence\nMI-BCI performances. Furthermore, literature from different fields showed an\nexperimenter effect, and specifically of their gender, on experimental outcome.\nWe assessed the impact of the interaction between experi-menter and participant\ngender on MI-BCI performances and progress throughout a session. Our results\nrevealed an interaction between participants gender, experimenter gender and\nprogress over runs. It seems to suggest that women experimenters may positively\ninfluence partici-pants' progress compared to men experimenters.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 13:24:29 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Roc", "Aline", "", "Potioc, LaBRI"], ["Pillette", "L\u00e9a", "", "Potioc, LaBRI"], ["N'Kaoua", "B.", "", "HACS"], ["Lotte", "Fabien", "", "LaBRI, Potioc"]]}, {"id": "1905.05601", "submitter": "HaiLong Liu", "authors": "Hailong Liu and Toshihiro Hiraoka and Takatsugu Hirayama and Dongmin\n  Kim", "title": "Saliency difference based objective evaluation method for a superimposed\n  screen of the HUD with various background", "comments": "10 pages, 5 fighres, 1 table, accepted by IFAC-HMS 2019", "journal-ref": null, "doi": "10.1016/j.ifacol.2019.12.073", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The head-up display (HUD) is an emerging device which can project information\non a transparent screen. The HUD has been used in airplanes and vehicles, and\nit is usually placed in front of the operator's view. In the case of the\nvehicle, the driver can see not only various information on the HUD but also\nthe backgrounds (driving environment) through the HUD. However, the projected\ninformation on the HUD may interfere with the colors in the background because\nthe HUD is transparent. For example, a red message on the HUD will be less\nnoticeable when there is an overlap between it and the red brake light from the\nfront vehicle. As the first step to solve this issue, how to evaluate the\nmutual interference between the information on the HUD and backgrounds is\nimportant. Therefore, this paper proposes a method to evaluate the mutual\ninterference based on saliency. It can be evaluated by comparing the HUD part\ncut from a saliency map of a measured image with the HUD image.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 06:45:38 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Liu", "Hailong", ""], ["Hiraoka", "Toshihiro", ""], ["Hirayama", "Takatsugu", ""], ["Kim", "Dongmin", ""]]}, {"id": "1905.05652", "submitter": "Xingqian Li Mr", "authors": "Xingqian Li, Chenwei Lou, Jian Zhao, HuaPeng Wei, Hongwei Zhao", "title": "\"Tom\" pet robot applied to urban autism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast development of network information technology, more and more\npeople are immersed in the virtual community environment brought by the\nnetwork, ignoring the social interaction in real life. The consequent urban\nautism problem has become more and more serious. Promoting offline\ncommunication between people \" and \"eliminating loneliness through emotional\ncommunication between pet robots and breeders\" to solve this problem, and has\ndeveloped a design called \"Tom\". \"Tom\" is a smart pet robot with a pet\nrobot-based social mechanism Called \"Tom-Talker\". The main contribution of this\npaper is to propose a social mechanism called \"Tom-Talker\" that encourages\nusers to socialize offline. And \"Tom-Talker\" also has a corresponding reward\nmechanism and a friend recommendation algorithm. It also proposes a pet robot\nnamed \"Tom\" with an emotional interaction algorithm to recognize users'\nemotions, simulate animal emotions and communicate emotionally with use s. This\npaper designs experiments and analyzes the results. The results show that our\npet robots have a good effect on solving urban autism problems.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:53:38 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Li", "Xingqian", ""], ["Lou", "Chenwei", ""], ["Zhao", "Jian", ""], ["Wei", "HuaPeng", ""], ["Zhao", "Hongwei", ""]]}, {"id": "1905.05673", "submitter": "Christian Mai", "authors": "Christian Mai, Heinrich Hu{\\ss}mann", "title": "A Qualitative Post-Experience Method for Evaluating Changes in VR\n  Presence Experience Over Time", "comments": "12 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A particular measure to evaluate a head-mounted display (HMD) based\nexperience is the state of feeling present in virtual reality. Interruptions of\na presence experience - break in presence (BIP) - appearing over time, need to\nbe detected to assess and improve an application. Existing methods either lack\nin taking these BIPs into account - questionnaires - or are complex in their\napplication and evaluation - physiological and behavioral measures -. To\nprovide a practical approach, we propose a post-experience method in which the\nusers reflect on their experience by drawing a line, indicating their\nexperienced state of presence, in a paper-based drawing template. The amplitude\nof the drawn line represents the variation of their presence experience over\ntime. We propose a descriptive model that describes temporal variations in the\ndrawings by the definition of relevant points over time - e.g., putting on the\nHMD -, phases of the experience - e.g., transition into VR - and parameters -\ne.g., the transition time -. The descriptive model enables us to objectively\nevaluate user drawings and represent the course of the drawings by a defined\nset of parameters. An exploratory user study (N=30) showed that the drawings\nare very consistent, the method can detect all BIPs and shows good indications\nfor representing the intensity of a BIP. With our method practitioners and\nresearchers can accelerate the evaluation and optimization of experiences by\nevaluating BIPs. The possibility to store objective parameters paves the way\nfor automated evaluation methods and big data approaches.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:36:17 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Mai", "Christian", ""], ["Hu\u00dfmann", "Heinrich", ""]]}, {"id": "1905.05810", "submitter": "Isabelle Pecci", "authors": "Ilyasse Belkacem, Isabelle Pecci, Beno\\^it Martin", "title": "Pointing task on smart glasses: Comparison of four interaction\n  techniques", "comments": "31 pages, 21 figures. The final version has been submitted to\n  International Journal of Human-Computer Studies. It has been received by\n  Elsevier on January 25, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices such as smartphones, smartwatches or smart glasses have\nrevolutionized how we interact. We are interested in smart glasses because they\nhave the advantage of providing a simultaneous view of both physical and\ndigital worlds. Despite this potential, pointing task on smart glasses is not\nreally widespread. In this paper, we compared four interaction techniques for\nselecting targets : (a) the Absolute Head Movement and (b) the Relative Head\nMovement, where head movement controls the cursor on smart glasses in absolute\nor relative way, (c) the Absolute Free Hand interaction, where the forefinger\ncontrol the cursor and (d) the Tactile Surface interaction, where the user\ncontrols the cursor via a small touchpad connected to smart glasses. We\nconducted an experiment with 18 participants. The Tactile Surface and Absolute\nHead Movement were the most efficient. The Relative Head Movement and Absolute\nFree Hand interactions were promising and require more exploration for other\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 19:39:17 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Belkacem", "Ilyasse", ""], ["Pecci", "Isabelle", ""], ["Martin", "Beno\u00eet", ""]]}, {"id": "1905.05851", "submitter": "F. Maxwell Harper", "authors": "Sarah McRoberts, Joshua Wissbroecker, Ruotong Wang, F. Maxwell Harper", "title": "Exploring Interactions with Voice-Controlled TV", "comments": "11 pages, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Intelligent agents such as Alexa, Siri, and Google Assistant are now built\ninto streaming TV systems, allowing people to use voice input to navigate the\nincreasingly complex set of apps available on a TV. However, these systems\ntypically support a narrow range of control- and search-oriented commands, and\ndo not support deeper recommendation or exploration queries. To learn about how\npeople interact with a recommendation-oriented voice-controlled TV, we use\nresearch through design methods to explore an early prototype movie\nrecommendation system where the only input modality is voice. We describe\nin-depth qualitative research sessions with 11 participants. We contribute\nimplications for designers of voice-controlled TV: mitigating the drawbacks of\nvoice-only interactions, navigating the tension between expressiveness and\nefficiency, and building voice-driven recommendation interfaces that facilitate\nexploration.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 21:31:47 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["McRoberts", "Sarah", ""], ["Wissbroecker", "Joshua", ""], ["Wang", "Ruotong", ""], ["Harper", "F. Maxwell", ""]]}, {"id": "1905.06102", "submitter": "Christian Mai", "authors": "Christian Mai, Alexander Knittel, Heinrich Hu{\\ss}mann", "title": "Frontal Screens on Head-Mounted Displays to Increase Awareness of the\n  HMD Users' State in Mixed Presence Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the everyday context, e.g., a household, HMD users remain a part of the\nsocial life for Non-HMD users being co-located with them. Due to the social\ncontext situations arise that demand interaction between the HMD and the\nNon-HMD user. We focus on the challenge that the Non-HMD user is not able to\ninterpret the HMD user's state -- e.g., attentiveness; the need for assistance\n--, as the HMD covers the wearer's face. We propose a front facing display\nattached to the HMD that supports collaboration by showing the state. We\nexplore the impact of abstract and realistic visualizations for such displays\non collaborative performance and social presence in a within-subject user study\n(N=25). We present to the Non-HMD user (1) a blank screen (baseline), (2)\ntextual representation of the user's state and (3) a representation that looks\nlike the HMD is see-through. The results show positive effects for textual\nrepresentation on collaborative performance and a positive effect of realistic\nrepresentation on social presence. We conclude that when developing HMDs we\nneed to take into account the social needs of everyday life to reduce the risk\nof social separation in a household context.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 12:02:32 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Mai", "Christian", ""], ["Knittel", "Alexander", ""], ["Hu\u00dfmann", "Heinrich", ""]]}, {"id": "1905.06229", "submitter": "Josef Spjut", "authors": "Josef Spjut and Ben Boudaoud and Jonghyun Kim and Trey Greer and\n  Rachel Albert and Michael Stengel and Kaan Aksit and David Luebke", "title": "Toward Standardized Classification of Foveated Displays", "comments": "9 pages, 8 figures, presented at IEEE VR 2020", "journal-ref": "in IEEE Transactions on Visualization and Computer Graphics, vol.\n  26, no. 5, pp. 2126-2134, May 2020", "doi": "10.1109/TVCG.2020.2973053", "report-no": null, "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergent in the field of head mounted display design is a desire to leverage\nthe limitations of the human visual system to reduce the computation,\ncommunication, and display workload in power and form-factor constrained\nsystems. Fundamental to this reduced workload is the ability to match display\nresolution to the acuity of the human visual system, along with a resulting\nneed to follow the gaze of the eye as it moves, a process referred to as\nfoveation. A display that moves its content along with the eye may be called a\nFoveated Display, though this term is also commonly used to describe displays\nwith non-uniform resolution that attempt to mimic human visual acuity. We\ntherefore recommend a definition for the term Foveated Display that accepts\nboth of these interpretations. Furthermore, we include a simplified model for\nhuman visual Acuity Distribution Functions (ADFs) at various levels of visual\nacuity, across wide fields of view and propose comparison of this ADF with the\nResolution Distribution Function of a foveated display for evaluation of its\nresolution at a particular gaze direction. We also provide a taxonomy to allow\nthe field to meaningfully compare and contrast various aspects of foveated\ndisplays in a display and optical technology-agnostic manner.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 20:45:05 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 16:14:09 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Spjut", "Josef", ""], ["Boudaoud", "Ben", ""], ["Kim", "Jonghyun", ""], ["Greer", "Trey", ""], ["Albert", "Rachel", ""], ["Stengel", "Michael", ""], ["Aksit", "Kaan", ""], ["Luebke", "David", ""]]}, {"id": "1905.06289", "submitter": "Kory W Mathewson", "authors": "Kory W. Mathewson", "title": "A Human-Centered Approach to Interactive Machine Learning", "comments": "4 pages, 4th Multidisciplinary Conference on Reinforcement Learning\n  and Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interactive machine learning (IML) community aims to augment humans'\nability to learn and make decisions over time through the development of\nautomated decision-making systems. This interaction represents a collaboration\nbetween multiple intelligent systems---humans and machines. A lack of\nappropriate consideration for the humans involved can lead to problematic\nsystem behaviour, and issues of fairness, accountability, and transparency.\nThis work presents a human-centred thinking approach to applying IML methods.\nThis guide is intended to be used by AI practitioners who incorporate human\nfactors in their work. These practitioners are responsible for the health,\nsafety, and well-being of interacting humans. An obligation of responsibility\nfor public interaction means acting with integrity, honesty, fairness, and\nabiding by applicable legal statutes. With these values and principles in mind,\nwe as a research community can better achieve the collective goal of augmenting\nhuman ability. This practical guide aims to support many of the responsible\ndecisions necessary throughout iterative design, development, and dissemination\nof IML systems.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 16:46:55 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Mathewson", "Kory W.", ""]]}, {"id": "1905.06384", "submitter": "Syed Anwar", "authors": "Aamir Arsalan, Muhammad Majid, Syed Muhammad Anwar, Ulas Bagci", "title": "Classification of Perceived Human Stress using Physiological Signals", "comments": "Accepted for publication in EMBC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an experimental study for the classification of\nperceived human stress using non-invasive physiological signals. These include\nelectroencephalography (EEG), galvanic skin response (GSR), and\nphotoplethysmography (PPG). We conducted experiments consisting of steps\nincluding data acquisition, feature extraction, and perceived human stress\nclassification. The physiological data of $28$ participants are acquired in an\nopen eye condition for a duration of three minutes. Four different features are\nextracted in time domain from EEG, GSR and PPG signals and classification is\nperformed using multiple classifiers including support vector machine, the\nNaive Bayes, and multi-layer perceptron (MLP). The best classification accuracy\nof 75% is achieved by using MLP classifier. Our experimental results have shown\nthat our proposed scheme outperforms existing perceived stress classification\nmethods, where no stress inducers are used.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 23:27:47 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Arsalan", "Aamir", ""], ["Majid", "Muhammad", ""], ["Anwar", "Syed Muhammad", ""], ["Bagci", "Ulas", ""]]}, {"id": "1905.06452", "submitter": "Andrew Stanton", "authors": "Andrew Stanton, Akhila Ananthram, Congzhe Su, Liangjie Hong", "title": "Revenue, Relevance, Arbitrage and More: Joint Optimization Framework for\n  Search Experiences in Two-Sided Marketplaces", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two-sided marketplaces such as eBay, Etsy and Taobao have two distinct groups\nof customers: buyers who use the platform to seek the most relevant and\ninteresting item to purchase and sellers who view the same platform as a tool\nto reach out to their audience and grow their business. Additionally, platforms\nhave their own objectives ranging from growing both buyer and seller user bases\nto revenue maximization. It is not difficult to see that it would be\nchallenging to obtain a globally favorable outcome for all parties. Taking the\nsearch experience as an example, any interventions are likely to impact either\nbuyers or sellers unfairly to course correct for a greater perceived need. In\nthis paper, we address how a company-aligned search experience can be provided\nwith competing business metrics that E-commerce companies typically tackle. As\nfar as we know, this is a pioneering work to consider multiple different\naspects of business indicators in two-sided marketplaces to optimize a search\nexperience. We demonstrate that many problems are difficult or impossible to\ndecompose down to credit assigned scores on individual documents, rendering\ntraditional methods inadequate. Instead, we express market-level metrics as\nconstraints and discuss to what degree multiple potentially conflicting metrics\ncan be tuned to business needs. We further explore the use of policy learners\nin the form of Evolutionary Strategies to jointly optimize both group-level and\nmarket-level metrics simultaneously, side-stepping traditional cascading\nmethods and manual interventions. We empirically evaluate the effectiveness of\nthe proposed method on Etsy data and demonstrate its potential with insights.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 22:02:44 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Stanton", "Andrew", ""], ["Ananthram", "Akhila", ""], ["Su", "Congzhe", ""], ["Hong", "Liangjie", ""]]}, {"id": "1905.06715", "submitter": "Abdulelah Abuabat", "authors": "Abdulelah Abuabat, Steven Johnston, Mohammed Aldosari, Taylor Neal", "title": "Homegrown Governments: Visualizing Regional Governance in the United\n  States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional Intergovernmental Organizations (RIGOs) are constituted by the local\ngovernments within their respective regions and are supported by the active\nengagement of the regions community and citizens. Metropolitan Statistical\nAreas (MSAs), on the other hand, are classified by the federal government based\non commuting and commerce patterns. They do not adhere to any local government.\nThe Graduate School of Policy and International Affairs Center for Metropolitan\nStudies (GSPIA) at the University of Pittsburgh have been researching the\nboundaries of RIGOs and the characteristics defining them. In this paper, we\npropose, design, and implement an approach to enhance the current visualization\nby visualizing two categorical data: RIGOs and MSAs and the overlapping between\nthem. We attempted to use a combination of visual attributes that leverage\nhuman perception system and do not impose cognitive and mental effort. The\noverall result of the evaluation shows that our work proved to be more\neffective than the current visualization.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 17:25:42 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Abuabat", "Abdulelah", ""], ["Johnston", "Steven", ""], ["Aldosari", "Mohammed", ""], ["Neal", "Taylor", ""]]}, {"id": "1905.06716", "submitter": "Julie Dugdale", "authors": "Antoine Flepp, Julie Dugdale, Fabrice Bourge, Tiphaine Marie-Cardot", "title": "A Method to Discover Digital Collaborative Conversations in Business\n  Collaborations", "comments": null, "journal-ref": "10th International Joint Conference on Knowledge Discovery,\n  Knowledge Engineering and Knowledge Management, May 2018, Seville, Spain", "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many companies have a suite of digital tools, such as Enterprise Social\nNetworks, conferencing and document sharing software, and email, to facilitate\ncollaboration among employees. During, or at the end of a collaboration,\ndocuments are often produced. People who were not involved in the initial\ncollaboration often have difficulties understanding parts of its content\nbecause they are lacking the overall context. We argue there is valuable\ncontextual and collaborative knowledge contained in these tools (content and\nuse) that can be used to understand the document. Our goal is to rebuild the\nconversations that took place over a messaging service and their links with a\ndigital conferencing tool during document production. The novelty in our\napproach is to combine several conversation-threading methods to identify\ninteresting links between distinct conversations. Specifically we combine\nheader-field information with social, temporal and semantic proximities. Our\nfindings suggest the messaging service and conferencing tool are used in a\ncomplementary way. The primary results confirm that combining different\nconversation threading approaches is efficient to detect and construct\nconversation threads from distinct digital conversations concerning the same\ndocument.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2019 12:21:47 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Flepp", "Antoine", ""], ["Dugdale", "Julie", ""], ["Bourge", "Fabrice", ""], ["Marie-Cardot", "Tiphaine", ""]]}, {"id": "1905.06717", "submitter": "Xavier Favory", "authors": "Xavier Favory and Xavier Serra", "title": "Multi Web Audio Sequencer: Collaborative Music Making", "comments": "4 pages, 4 figures, short paper of the Web Audio Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advancements in web-based audio systems have enabled sufficiently\naccurate timing control and real-time sound processing capabilities. Numerous\nspecialized music tools, as well as digital audio workstations, are now\naccessible from browsers. Features such as the large accessibility of data and\nreal-time communication between clients make the web attractive for\ncollaborative data manipulation. However, this innovative field has yet to\nproduce effective tools for multiple-user coordination on specialized music\ncreation tasks. The Multi Web Audio Sequencer is a prototype of an application\nfor segment-based sequencing of Freesound sound clips, with an emphasis on\nseamless remote collaboration. In this work we consider a fixed-grid step\nsequencer as a probe for understanding the necessary features of crowd-shared\nmusic creation sessions. This manuscript describes the sequencer and the\nfunctionalities and types of interactions required for effective and attractive\ncollaboration of remote people during creative music creation activities.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 13:10:26 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Favory", "Xavier", ""], ["Serra", "Xavier", ""]]}, {"id": "1905.06720", "submitter": "Yang Shi", "authors": "Yang Shi, Yuyin Liu, Hanghang Tong, Jingrui He, Gang Yan, and Nan Cao", "title": "Visual Analytics of Anomalous User Behaviors: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing accessibility of data provides substantial opportunities for\nunderstanding user behaviors. Unearthing anomalies in user behaviors is of\nparticular importance as it helps signal harmful incidents such as network\nintrusions, terrorist activities, and financial frauds. Many visual analytics\nmethods have been proposed to help understand user behavior-related data in\nvarious application domains. In this work, we survey the state of art in visual\nanalytics of anomalous user behaviors and classify them into four categories\nincluding social interaction, travel, network communication, and transaction.\nWe further examine the research works in each category in terms of data types,\nanomaly detection techniques, and visualization techniques, and interaction\nmethods. Finally, we discuss the findings and potential research directions.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 03:58:31 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 08:37:27 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Shi", "Yang", ""], ["Liu", "Yuyin", ""], ["Tong", "Hanghang", ""], ["He", "Jingrui", ""], ["Yan", "Gang", ""], ["Cao", "Nan", ""]]}, {"id": "1905.06762", "submitter": "Nataliya Shakhovska Prof", "authors": "Glib Shchur, Nataliya Shakhovska", "title": "Smartphone app with usage of AR technologies - SolAR System", "comments": "6 pages,9 figures", "journal-ref": "ECONTECHMOD. AN INTERNATIONAL QUARTERLY JOURNAL - 2018, Vol. 07,\n  No. 3, 63 - 68", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes the AR mobile system for Sun system simulation. The\nmain characteristics of AR systems architecture are given. The differences\nbetween tracking and without tracking technics are underlined. The architecture\nof the system of use of complemented reality for the study of astronomy is\ndescribed. The features of the system and the principles of its work are\ndetermined.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 02:49:38 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Shchur", "Glib", ""], ["Shakhovska", "Nataliya", ""]]}, {"id": "1905.06777", "submitter": "Igor Ivkic", "authors": "Igor Ivkic, Alexander W\\\"ohrer, Markus Tauber", "title": "Towards Comparing Programming Paradigms", "comments": null, "journal-ref": "2017 12th International Conference for Internet Technology and\n  Secured Transactions (ICITST), Cambridge, UK", "doi": "10.23919/ICITST.2017.8356440", "report-no": null, "categories": "cs.HC cs.CL cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid technological progress in computer sciences finds solutions and at the\nsame time creates ever more complex requirements. Due to an evolving complexity\ntodays programming languages provide powerful frameworks which offer standard\nsolutions for recurring tasks to assist the programmer and to avoid the\nre-invention of the wheel with so-called out-of-the-box-features. In this\npaper, we propose a way of comparing different programming paradigms on a\ntheoretical, technical and practical level. Furthermore, the paper presents the\nresults of an initial comparison of two representative programming approaches,\nboth in the closed SAP environment.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 09:55:10 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ivkic", "Igor", ""], ["W\u00f6hrer", "Alexander", ""], ["Tauber", "Markus", ""]]}, {"id": "1905.06991", "submitter": "Ahmad Abdellatif", "authors": "Ahmad Abdellatif, Khaled Badran, and Emad Shihab", "title": "MSRBot: Using Bots to Answer Questions from Software Repositories", "comments": null, "journal-ref": null, "doi": "10.1007/s10664-019-09788-5", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software repositories contain a plethora of useful information that can be\nused to enhance software projects. Prior work has leveraged repository data to\nimprove many aspects of the software development process, such as, help extract\nrequirement decisions, identify potentially defective code and improve\nmaintenance and evolution. However, in many cases, practitioners are not able\nto fully benefit from software repositories due to the fact that they need\nspecial expertise and dedicated effort to mine their repositories.\n  Therefore, in this paper, we use bots to automate and ease the process of\nextracting useful information from software repositories. Particularly, we lay\nout an approach of how bots, layered on top of software repositories, can be\nused to answer some of the most common software development/maintenance\nquestions facing developers. We perform a preliminary study with 12\nparticipants to validate the effectiveness of the bot. Our findings indicate\nthat using bots achieves very promising results in terms of answer accuracy,\nspeed and usefulness. Our work has the potential to transform the MSR field by\nsignificantly lowering the barrier to entry, making the extraction of useful\ninformation from software repositories as easy as chatting with a bot.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 18:44:48 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2020 14:09:49 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Abdellatif", "Ahmad", ""], ["Badran", "Khaled", ""], ["Shihab", "Emad", ""]]}, {"id": "1905.06995", "submitter": "Diane Hosfelt", "authors": "Diane Hosfelt", "title": "Making ethical decisions for the immersive web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed reality (MR) ethics occupies a space that intersects with web ethics,\nemerging tech ethics, healthcare ethics and product ethics (among others). This\npaper focuses on how we can build an immersive web that encourages ethical\ndevelopment and usage. The technology is beyond emerging (footnote: generally,\nthe ethics of emerging technologies are focused on ethical assessments of\nresearch and innovation), but not quite entrenched. We're still in a position\nto intervene in the development process, instead of attempting to retrofit\nethical decisions into an established design. While we have a wider range of\ndata to analyze than most emerging technologies, we're still in a much more\nspeculative state than entrenched technologies. This space is a challenge and\nan opportunity.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:57:20 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Hosfelt", "Diane", ""]]}, {"id": "1905.07039", "submitter": "Siddharth Siddharth", "authors": "Siddharth Siddharth, Tzyy-Ping Jung, and Terrence J. Sejnowski", "title": "Utilizing Deep Learning Towards Multi-modal Bio-sensing and Vision-based\n  Affective Computing", "comments": "Accepted for publication in IEEE Transactions on Affective Computing.\n  This version on the arXiv is the updated version of the same manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the use of bio-sensing signals such as electroencephalogram\n(EEG), electrocardiogram (ECG), etc. have garnered interest towards\napplications in affective computing. The parallel trend of deep-learning has\nled to a huge leap in performance towards solving various vision-based research\nproblems such as object detection. Yet, these advances in deep-learning have\nnot adequately translated into bio-sensing research. This work applies novel\ndeep-learning-based methods to various bio-sensing and video data of four\npublicly available multi-modal emotion datasets. For each dataset, we first\nindividually evaluate the emotion-classification performance obtained by each\nmodality. We then evaluate the performance obtained by fusing the features from\nthese modalities. We show that our algorithms outperform the results reported\nby other studies for emotion/valence/arousal/liking classification on DEAP and\nMAHNOB-HCI datasets and set up benchmarks for the newer AMIGOS and DREAMER\ndatasets. We also evaluate the performance of our algorithms by combining the\ndatasets and by using transfer learning to show that the proposed method\novercomes the inconsistencies between the datasets. Hence, we do a thorough\nanalysis of multi-modal affective data from more than 120 subjects and 2,800\ntrials. Finally, utilizing a convolution-deconvolution network, we propose a\nnew technique towards identifying salient brain regions corresponding to\nvarious affective states.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 21:19:52 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Siddharth", "Siddharth", ""], ["Jung", "Tzyy-Ping", ""], ["Sejnowski", "Terrence J.", ""]]}, {"id": "1905.07054", "submitter": "Lionel Robert", "authors": "Lionel Peter Robert Jr", "title": "Are Automated Vehicles Safer than Manually Driven Cars?", "comments": "3 pages. AI & Society (2019)", "journal-ref": null, "doi": "10.1007/s00146-019-00894-y", "report-no": null, "categories": "cs.HC cs.CY cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Are automated vehicles really safer than manually driven vehicles? If so, how\nwould we know? Answering this question has spurred a contentious debate.\nUnfortunately, several issues make answering this question difficult for the\nforeseeable future. First, how do we measure safety? Second, how can we keep\ntrack of automated vehicle (AV) safety? Finally, how do we determine what is or\nwhat is not an AV? Until these questions are addressed, it will continue to be\ndifficult to determine whether or when AVs might really be safer than manually\ndriven vehicles.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 22:59:14 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Robert", "Lionel Peter", "Jr"]]}, {"id": "1905.07076", "submitter": "Richard Marcus", "authors": "Richard Marcus, Michael Kohlhase and Florian Rabe", "title": "TGView3D System Description: 3-Dimensional Visualization of Theory\n  Graphs", "comments": "5 pages, 2 figures, fixed typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe TGView3D, an interactive 3D graph viewer optimized for exploring\ntheory graphs. To exploit the three spatial dimensions, it extends a\nforce-directed layout with a hierarchical component. Because of the limitations\nof regular displays, the system also supports the use of a head-mounted display\nand utilizes several virtual realty interaction concepts.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 01:17:03 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 16:49:12 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Marcus", "Richard", ""], ["Kohlhase", "Michael", ""], ["Rabe", "Florian", ""]]}, {"id": "1905.07379", "submitter": "David Robb", "authors": "Helen Hastie, David A. Robb, Jos\\'e Lopes, Muneeb Ahmad, Pierre Le\n  Bras, Xingkun Liu, Ronald P. A. Petrick, Katrin Lohan, Mike J. Chantler", "title": "Challenges in Collaborative HRI for Remote Robot Teams", "comments": "9 pages. Peer reviewed position paper accepted in the CHI 2019\n  Workshop: The Challenges of Working on Social Robots that Collaborate with\n  People (SIRCHI2019), ACM CHI Conference on Human Factors in Computing\n  Systems, May 2019, Glasgow, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaboration between human supervisors and remote teams of robots is highly\nchallenging, particularly in high-stakes, distant, hazardous locations, such as\noff-shore energy platforms. In order for these teams of robots to truly be\nbeneficial, they need to be trusted to operate autonomously, performing tasks\nsuch as inspection and emergency response, thus reducing the number of\npersonnel placed in harm's way. As remote robots are generally trusted less\nthan robots in close-proximity, we present a solution to instil trust in the\noperator through a `mediator robot' that can exhibit social skills, alongside\nsophisticated visualisation techniques. In this position paper, we present\ngeneral challenges and then take a closer look at one challenge in particular,\ndiscussing an initial study, which investigates the relationship between the\nlevel of control the supervisor hands over to the mediator robot and how this\naffects their trust. We show that the supervisor is more likely to have higher\ntrust overall if their initial experience involves handing over control of the\nemergency situation to the robotic assistant. We discuss this result, here, as\nwell as other challenges and interaction techniques for human-robot\ncollaboration.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 17:15:55 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Hastie", "Helen", ""], ["Robb", "David A.", ""], ["Lopes", "Jos\u00e9", ""], ["Ahmad", "Muneeb", ""], ["Bras", "Pierre Le", ""], ["Liu", "Xingkun", ""], ["Petrick", "Ronald P. A.", ""], ["Lohan", "Katrin", ""], ["Chantler", "Mike J.", ""]]}, {"id": "1905.07394", "submitter": "Ching-Yun Ko", "authors": "Ching-Yun Ko, Rui Lin, Shu Li, Ngai Wong", "title": "MiSC: Mixed Strategies Crowdsourcing", "comments": "8 pages, accepted to IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular crowdsourcing techniques mostly focus on evaluating workers' labeling\nquality before adjusting their weights during label aggregation. Recently,\nanother cohort of models regard crowdsourced annotations as incomplete tensors\nand recover unfilled labels by tensor completion. However, mixed strategies of\nthe two methodologies have never been comprehensively investigated, leaving\nthem as rather independent approaches. In this work, we propose $\\textit{MiSC}$\n($\\textbf{Mi}$xed $\\textbf{S}$trategies $\\textbf{C}$rowdsourcing), a versatile\nframework integrating arbitrary conventional crowdsourcing and tensor\ncompletion techniques. In particular, we propose a novel iterative Tucker label\naggregation algorithm that outperforms state-of-the-art methods in extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 17:42:56 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Ko", "Ching-Yun", ""], ["Lin", "Rui", ""], ["Li", "Shu", ""], ["Wong", "Ngai", ""]]}, {"id": "1905.07528", "submitter": "Yue Zhang", "authors": "Daniel Zhang, Nathan Vance, Dong Wang", "title": "When Social Sensing Meets Edge Computing: Vision and Challenges", "comments": "This manuscript has been accepted to ICCCN 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper overviews the state of the art, research challenges, and future\nopportunities in an emerging research direction: Social Sensing based Edge\nComputing (SSEC). Social sensing has emerged as a new sensing application\nparadigm where measurements about the physical world are collected from humans\nor from devices on their behalf. The advent of edge computing pushes the\nfrontier of computation, service, and data along the cloud-to-things continuum.\nThe merging of these two technical trends generates a set of new research\nchallenges that need to be addressed. In this paper, we first define the new\nSSEC paradigm that is motivated by a few underlying technology trends. We then\npresent a few representative real-world case studies of SSEC applications and\nseveral key research challenges that exist in those applications. Finally, we\nenvision a few exciting research directions in future SSEC. We hope this paper\nwill stimulate discussions of this emerging research direction in the\ncommunity.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 03:26:58 GMT"}, {"version": "v2", "created": "Sat, 25 May 2019 15:45:25 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Zhang", "Daniel", ""], ["Vance", "Nathan", ""], ["Wang", "Dong", ""]]}, {"id": "1905.07665", "submitter": "Shaoxiong Ji", "authors": "Shaoxiong Ji and Guodong Long and Shirui Pan and Tianqing Zhu and Jing\n  Jiang and Sen Wang and Xue Li", "title": "Knowledge Transferring via Model Aggregation for Online Social Care", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet and the Web are being increasingly used in proactive social care\nto provide people, especially the vulnerable, with a better life and services,\nand their derived social services generate enormous data. However, the strict\nprotection of privacy makes user's data become an isolated island and limits\nthe predictive performance of standalone clients. To enable effective proactive\nsocial care and knowledge sharing within intelligent agents, this paper\ndevelops a knowledge transferring framework via model aggregation. Under this\nframework, distributed clients perform on-device training, and a third-party\nserver integrates multiple clients' models and redistributes to clients for\nknowledge transferring among users. To improve the generalizability of the\nknowledge sharing, we further propose a novel model aggregation algorithm,\nnamely the average difference descent aggregation (AvgDiffAgg for short). In\nparticular, to evaluate the effectiveness of the learning algorithm, we use a\ncase study on the early detection and prevention of suicidal ideation, and the\nexperiment results on four datasets derived from social communities demonstrate\nthe effectiveness of the proposed learning method.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 00:06:02 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 09:02:47 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Ji", "Shaoxiong", ""], ["Long", "Guodong", ""], ["Pan", "Shirui", ""], ["Zhu", "Tianqing", ""], ["Jiang", "Jing", ""], ["Wang", "Sen", ""], ["Li", "Xue", ""]]}, {"id": "1905.07917", "submitter": "Luc Brun", "authors": "Xuan Nguyen, Luc Brun, Olivier Lezoray, S\\'ebastien Bougleux", "title": "Skeleton-Based Hand Gesture Recognition by Learning SPD Matrices with\n  Neural Networks", "comments": null, "journal-ref": "14th IEEE International Conference on Automatic Face and Gesture\n  Recognition, May 2019, Lille, France", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new hand gesture recognition method based on\nskeletal data by learning SPD matrices with neural networks. We model the hand\nskeleton as a graph and introduce a neural network for SPD matrix learning,\ntaking as input the 3D coordinates of hand joints. The proposed network is\nbased on two newly designed layers that transform a set of SPD matrices into a\nSPD matrix. For gesture recognition, we train a linear SVM classifier using\nfeatures extracted from our network. Experimental results on a challenging\ndataset (Dynamic Hand Gesture dataset from the SHREC 2017 3D Shape Retrieval\nContest) show that the proposed method outperforms state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 07:10:49 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Nguyen", "Xuan", ""], ["Brun", "Luc", ""], ["Lezoray", "Olivier", ""], ["Bougleux", "S\u00e9bastien", ""]]}, {"id": "1905.07984", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov, Marius Pedersen, Nam Wook Kim, Sumit Shekhar", "title": "Are all the frames equally important?", "comments": "CHI'20 Late Breaking Works", "journal-ref": null, "doi": "10.1145/3334480.3382980", "report-no": null, "categories": "cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we address the problem of measuring and predicting temporal\nvideo saliency - a metric which defines the importance of a video frame for\nhuman attention. Unlike the conventional spatial saliency which defines the\nlocation of the salient regions within a frame (as it is done for still\nimages), temporal saliency considers importance of a frame as a whole and may\nnot exist apart from context. The proposed interface is an interactive\ncursor-based algorithm for collecting experimental data about temporal\nsaliency. We collect the first human responses and perform their analysis. As a\nresult, we show that qualitatively, the produced scores have very explicit\nmeaning of the semantic changes in a frame, while quantitatively being highly\ncorrelated between all the observers. Apart from that, we show that the\nproposed tool can simultaneously collect fixations similar to the ones produced\nby eye-tracker in a more affordable way. Further, this approach may be used for\ncreation of first temporal saliency datasets which will allow training\ncomputational predictive algorithms. The proposed interface does not rely on\nany special equipment, which allows to run it remotely and cover a wide\naudience.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 10:48:38 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 18:30:57 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Sidorov", "Oleksii", ""], ["Pedersen", "Marius", ""], ["Kim", "Nam Wook", ""], ["Shekhar", "Sumit", ""]]}, {"id": "1905.08063", "submitter": "Mark Keane", "authors": "Molly S Quinn, Kathleen Campbell, Mark T Keane", "title": "The Unexpected Unexpected and the Expected Unexpected: How People's\n  Conception of the Unexpected is Not That Unexpected", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The answers people give when asked to 'think of the unexpected' for everyday\nevent scenarios appear to be more expected than unexpected. There are expected\nunexpected outcomes that closely adhere to the given information in a scenario,\nbased on familiar disruptions and common plan-failures. There are also\nunexpected unexpected outcomes that are more inventive, that depart from given\ninformation, adding new concepts/actions. However, people seem to tend to\nconceive of the unexpected as the former more than the latter. Study 1 tests\nthese proposals by analysing the object-concepts people mention in their\nreports of the unexpected and the agreement between their answers. Study 2\nshows that object-choices are weakly influenced by recency, the order of\nsentences in the scenario. The implications of these results for ideas in\nphilosophy, psychology and computing is discussed\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 10:14:07 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Quinn", "Molly S", ""], ["Campbell", "Kathleen", ""], ["Keane", "Mark T", ""]]}, {"id": "1905.08088", "submitter": "Yasushi Kawase", "authors": "Yasushi Kawase, Yuko Kuroki, Atsushi Miyauchi", "title": "Graph Mining Meets Crowdsourcing: Extracting Experts for Answer\n  Aggregation", "comments": "Accepted to IJCAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregating responses from crowd workers is a fundamental task in the process\nof crowdsourcing. In cases where a few experts are overwhelmed by a large\nnumber of non-experts, most answer aggregation algorithms such as the majority\nvoting fail to identify the correct answers. Therefore, it is crucial to\nextract reliable experts from the crowd workers. In this study, we introduce\nthe notion of \"expert core\", which is a set of workers that is very unlikely to\ncontain a non-expert. We design a graph-mining-based efficient algorithm that\nexactly computes the expert core. To answer the aggregation task, we propose\ntwo types of algorithms. The first one incorporates the expert core into\nexisting answer aggregation algorithms such as the majority voting, whereas the\nsecond one utilizes information provided by the expert core extraction\nalgorithm pertaining to the reliability of workers. We then give a theoretical\njustification for the first type of algorithm. Computational experiments using\nsynthetic and real-world datasets demonstrate that our proposed answer\naggregation algorithms outperform state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 12:49:50 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Kawase", "Yasushi", ""], ["Kuroki", "Yuko", ""], ["Miyauchi", "Atsushi", ""]]}, {"id": "1905.08331", "submitter": "Amarda Shehu", "authors": "Kevin Molloy, Erion Plaku, and Amarda Shehu", "title": "ROMEO: A Plug-and-play Software Platform of Robotics-inspired Algorithms\n  for Modeling Biomolecular Structures and Motions", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Due to the central role of protein structure in molecular\nrecognition, great computational efforts are devoted to modeling protein\nstructures and motions that mediate structural rearrangements. The size,\ndimensionality, and non-linearity of the protein structure space present\noutstanding challenges. Such challenges also arise in robot motion planning,\nand robotics-inspired treatments of protein structure and motion are\nincreasingly showing high exploration capability. Encouraged by such findings,\nwe debut here ROMEO, which stands for Robotics prOtein Motion ExplOration\nframework. ROMEO is an open-source, object-oriented platform that allows\nresearchers access to and reproducibility of published robotics-inspired\nalgorithms for modeling protein structures and motions, as well as facilitates\nnovel algorithmic design via its plug-and-play architecture.\n  Availability and implementation: ROMEO is written in C++ and is available in\nGitLab (https://github.com/). This software is freely available under the\nCreative Commons license (Attribution and Non-Commercial).\n  Contact: amarda@gmu.edu\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 20:23:47 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Molloy", "Kevin", ""], ["Plaku", "Erion", ""], ["Shehu", "Amarda", ""]]}, {"id": "1905.08367", "submitter": "Andrew Banman", "authors": "Andrew Banman", "title": "SmartNight: Turning Off the Lights on Android", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone users benefit from content with dark color schemes: increasingly\ncommon OLED displays are more power efficient the darker the display, and many\nusers prefer a dark display for night time use. Despite these benefits, many\napplications and the majority of web content are drawn with white backgrounds.\nThere are many partial solutions to darken the displayed content, but none work\nin all situations. Enter SmartNight, a content-aware solution to dynamically\ndarken content on Android. By trading off content fidelity, Android with\nSmartNight displays content with nearly 90% lower average picture level. It is\nimplemented in the Android framework, and requires no external support. It\nseamlessly incorporates existing solutions, making it a bridge between the\nstate-of-the-art and future solutions.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 22:28:11 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Banman", "Andrew", ""]]}, {"id": "1905.08418", "submitter": "Zhao Wang", "authors": "Zhao Wang, Anna Sapienza, Aron Culotta, Emilio Ferrara", "title": "Personality and Behavior in Role-based Online Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both offline and online human behaviors are affected by personality. Of\nspecial interests are online games, where players have to impersonate specific\nroles and their behaviors are extensively tracked by the game. In this paper,\nwe propose to study the relationship between players' personality and game\nbehavior in League of Legends (LoL), one of the most popular Multiplayer Online\nBattle Arena (MOBA) games. We use linear mixed effects (LME) models to describe\nrelationships between players' personality traits (measured by the Five Factor\nModel) and two major aspects of the game: the impersonated roles and in-game\nactions. On the one hand, we study relationships within the game environment by\nmodeling role attributes from match behaviors and vice versa. On the other\nhand, we analyze the relationship between a player's five personality traits\nand their game behavior by showing significant correlations between each\npersonality trait and the set of corresponding behaviors. Our findings suggest\nthat personality and behavior are highly entangled and provide a new\nperspective to understand how personality can affect behavior in role-based\nonline games.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 03:11:19 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Wang", "Zhao", ""], ["Sapienza", "Anna", ""], ["Culotta", "Aron", ""], ["Ferrara", "Emilio", ""]]}, {"id": "1905.08685", "submitter": "Jen-Yen Chang", "authors": "Jen-Yen Chang, Antonio Tejero-de-Pablos, Tatsuya Harada", "title": "Improved Optical Flow for Gesture-based Human-robot Interaction", "comments": "Accepted by ICRA 2019 on Jan 31 2019", "journal-ref": null, "doi": "10.1109/ICRA.2019.8793825", "report-no": null, "categories": "cs.HC cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gesture interaction is a natural way of communicating with a robot as an\nalternative to speech. Gesture recognition methods leverage optical flow in\norder to understand human motion. However, while accurate optical flow\nestimation (i.e., traditional) methods are costly in terms of runtime, fast\nestimation (i.e., deep learning) methods' accuracy can be improved. In this\npaper, we present a pipeline for gesture-based human-robot interaction that\nuses a novel optical flow estimation method in order to achieve an improved\nspeed-accuracy trade-off. Our optical flow estimation method introduces four\nimprovements to previous deep learning-based methods: strong feature\nextractors, attention to contours, midway features, and a combination of these\nthree. This results in a better understanding of motion, and a finer\nrepresentation of silhouettes. In order to evaluate our pipeline, we generated\nour own dataset, MIBURI, which contains gestures to command a house service\nrobot. In our experiments, we show how our method improves not only optical\nflow estimation, but also gesture recognition, offering a speed-accuracy\ntrade-off more realistic for practical robot applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 15:03:31 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chang", "Jen-Yen", ""], ["Tejero-de-Pablos", "Antonio", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1905.08773", "submitter": "Issam Damaj", "authors": "Marwa Kandil (1), Reem AlBaghdadi (1), Fatemah AlAttar (1), Issam\n  Damaj (2) ((1) American University of Kuwait, (2) Rafik Hariri University)", "title": "AmIE: An Ambient Intelligent Environment for Assisted Living", "comments": "6 pages, 8 figures, 1 table", "journal-ref": "The 2nd Eng Innovations in Healthcare International Conference,\n  IEEE, Dubai, UAE, March 26-27, (2019) 1-6", "doi": "10.1109/ICASET.2019.8714499", "report-no": null, "categories": "cs.CY cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the modern world of technology Internet-of-things (IoT) systems strives to\nprovide an extensive interconnected and automated solutions for almost every\nlife aspect. This paper proposes an IoT context-aware system to present an\nAmbient Intelligence (AmI) environment; such as an apartment, house, or a\nbuilding; to assist blind, visually-impaired, and elderly people. The proposed\nsystem aims at providing an easy-to-utilize voice-controlled system to locate,\nnavigate and assist users indoors. The main purpose of the system is to provide\nindoor positioning, assisted navigation, outside weather information, room\ntemperature, people availability, phone calls and emergency evacuation when\nneeded. The system enhances the user's awareness of the surrounding environment\nby feeding them with relevant information through a wearable device to assist\nthem. In addition, the system is voice-controlled in both English and Arabic\nlanguages and the information are displayed as audio messages in both\nlanguages. The system design, implementation, and evaluation consider the\nconstraints in common types of premises in Kuwait and in challenges, such as\nthe training needed by the users. This paper presents cost-effective\nimplementation options by the adoption of a Raspberry Pi microcomputer,\nBluetooth Low Energy devices and an Android smart watch.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 13:57:31 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Kandil", "Marwa", "", "American University of Kuwait"], ["AlBaghdadi", "Reem", "", "American University of Kuwait"], ["AlAttar", "Fatemah", "", "American University of Kuwait"], ["Damaj", "Issam", "", "Rafik Hariri University"]]}, {"id": "1905.08878", "submitter": "Lionel Robert", "authors": "Na Du, Jacob Haspiel, Qiaoning Zhang, Dawn Tilbury, Anuj K. Pradhan,\n  X. Jessie Yang, Lionel P. Robert Jr", "title": "Look Who's Talking Now: Implications of AV's Explanations on Driver's\n  Trust, AV Preference, Anxiety and Mental Workload", "comments": "42 pages, 5 figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explanations given by automation are often used to promote automation\nadoption. However, it remains unclear whether explanations promote acceptance\nof automated vehicles (AVs). In this study, we conducted a within-subject\nexperiment in a driving simulator with 32 participants, using four different\nconditions. The four conditions included: (1) no explanation, (2) explanation\ngiven before or (3) after the AV acted and (4) the option for the driver to\napprove or disapprove the AV's action after hearing the explanation. We\nexamined four AV outcomes: trust, preference for AV, anxiety and mental\nworkload. Results suggest that explanations provided before an AV acted were\nassociated with higher trust in and preference for the AV, but there was no\ndifference in anxiety and workload. These results have important implications\nfor the adoption of AVs.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 21:39:07 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Du", "Na", ""], ["Haspiel", "Jacob", ""], ["Zhang", "Qiaoning", ""], ["Tilbury", "Dawn", ""], ["Pradhan", "Anuj K.", ""], ["Yang", "X. Jessie", ""], ["Robert", "Lionel P.", "Jr"]]}, {"id": "1905.08937", "submitter": "Ranjan Satapathy", "authors": "Nidhi Mishra, Manoj Ramanathan, Ranjan Satapathy, Erik Cambria and\n  Nadia Magnenat-Thalmann", "title": "Can a Humanoid Robot be part of the Organizational Workforce? A User\n  Study Leveraging Sentiment Analysis", "comments": "Submitted to IEEE RO-MAN2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hiring robots for the workplaces is a challenging task as robots have to\ncater to customer demands, follow organizational protocols and behave with\nsocial etiquette. In this study, we propose to have a humanoid social robot,\nNadine, as a customer service agent in an open social work environment. The\nobjective of this study is to analyze the effects of humanoid robots on\ncustomers at work environment, and see if it can handle social scenarios. We\npropose to evaluate these objectives through two modes, namely, survey\nquestionnaire and customer feedback. We also propose a novel approach to\nanalyze customer feedback data (text) using sentic computing methods.\nSpecifically, we employ aspect extraction and sentiment analysis to analyze the\ndata. From our framework, we detect sentiment associated to the aspects that\nmainly concerned the customers during their interaction. This allows us to\nunderstand customers expectations and current limitations of robots as\nemployees.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 03:34:59 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 06:22:53 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Mishra", "Nidhi", ""], ["Ramanathan", "Manoj", ""], ["Satapathy", "Ranjan", ""], ["Cambria", "Erik", ""], ["Magnenat-Thalmann", "Nadia", ""]]}, {"id": "1905.08948", "submitter": "Kaixuan Chen", "authors": "Kaixuan Chen, Lina Yao, Dalin Zhang, Bin Guo, Zhiwen Yu", "title": "Multi-agent Attentional Activity Recognition", "comments": "Accepted by IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-modality is an important feature of sensor based activity recognition.\nIn this work, we consider two inherent characteristics of human activities, the\nspatially-temporally varying salience of features and the relations between\nactivities and corresponding body part motions. Based on these, we propose a\nmulti-agent spatial-temporal attention model. The spatial-temporal attention\nmechanism helps intelligently select informative modalities and their active\nperiods. And the multiple agents in the proposed model represent activities\nwith collective motions across body parts by independently selecting modalities\nassociated with single motions. With a joint recognition goal, the agents share\ngained information and coordinate their selection policies to learn the optimal\nrecognition model. The experimental results on four real-world datasets\ndemonstrate that the proposed model outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 04:17:55 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Chen", "Kaixuan", ""], ["Yao", "Lina", ""], ["Zhang", "Dalin", ""], ["Guo", "Bin", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1905.08988", "submitter": "Damien Vurpillot", "authors": "Damien Vurpillot (CESR), Perrine Pittet (CESR), Johann Forte (CESR),\n  Benoist Pierre (CESR)", "title": "From heterogeneous data to heterogeneous public: thoughts on transmedia\n  applications for digital heritage research and dissemination", "comments": "Computer Applications and quantitative methods in Archaeology 2019:\n  Check Object Integrity, Apr 2019, Cracovie, Poland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have seen a tenfold increase in volume and complexity of\ndigital data acquired for cultural heritage documentation. Meanwhile, open data\nand open science have become leading trends in digital humanities. The\nconvergence of those two parameters compels us to deliver, in an interoperable\nfashion, datasets that are vastly heterogeneous both in content and format and,\nmoreover, in such a way that they fit the expectation of a broad array of\nresearchers and an even broader public audience. Tackling those issues is one\nof the main goal of the \"HeritageS\" digital platform project supported by the\n\"Intelligence des Patrimoines\" research program. This platform is designed to\nallow research projects from many interdisciplinary fields to share, integrate\nand valorize cultural and natural heritage datasets related to the Loire\nValley. In this regard, one of our main project is the creation of the\n\"Renaissance Transmedia Lab\". Its core element is a website which acts as a hub\nto access various interactive experiences linked to project about the\nRenaissance period: augmented web-documentary, serious game, virtual reality,\n3D application. We expect to leverage those transmedia experiences to foster\nbetter communication between researchers and the public while keeping the\nquality of scientific discourse. By presenting the current and upcoming\nproductions, we intend to share our experience with other participants:\npreparatory work and how we cope with researchers to produce, in concertation,\ntailor-made experiences that convey the desired scientific discourse while\nremaining appealing to the general public.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 07:07:34 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Vurpillot", "Damien", "", "CESR"], ["Pittet", "Perrine", "", "CESR"], ["Forte", "Johann", "", "CESR"], ["Pierre", "Benoist", "", "CESR"]]}, {"id": "1905.09402", "submitter": "Hyungik Oh", "authors": "Hyungik Oh, Ramesh Jain", "title": "Detecting Events of Daily Living Using Multimodal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Events are fundamental for understanding how people experience their lives.\nIt is challenging, however, to automatically record all events in daily life.\nAn understanding of multimedia signals allows recognizing events of daily\nliving and getting their attributes as automatically as possible. In this\npaper, we consider the problem of recognizing a daily event by employing the\ncommonly used multimedia data obtained from a smartphone and wearable device.\nWe develop an unobtrusive approach to obtain latent semantic information from\nthe data, and therefore an approach for daily event recognition based on\nsemantic context enrichment. We represent the enrichment process through an\nevent knowledge graph that semantically enriches a daily event from a low-level\ndaily activity. To show a concrete example of this enrichment, we perform an\nexperiment with eating activity, which may be one of the most complex events,\nby using 14 months of data for three users. In this process, to unobtrusively\ncomplement the lack of semantic information, we suggest a new food\nrecognition/classification method that focuses only on a physical response to\nfood consumption. Experimental results indicate that our approach is able to\nshow automatic abstraction of life experience. These daily events can then be\nused to create a personal model that can capture how a person reacts to\ndifferent stimuli under specific conditions.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 23:16:41 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Oh", "Hyungik", ""], ["Jain", "Ramesh", ""]]}, {"id": "1905.09644", "submitter": "Sergey Andreyev", "authors": "Sergey Andreyev", "title": "Scientific Programs Imply Uncertainty. Results Expected and Unexpected", "comments": "6 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science and engineering have requests for a wide variety of programs, but I\nthink that all of them can be divided between two groups. Programs of the first\ngroup deal with the well known situations and, by using well known equations,\ngive results for any combination of input parameters. Such programs are\nspecialized very powerful calculators. Another group of programs is needed to\nanalyse the situations with different levels of uncertainty. Programs are\ndeveloped at the best level of their authors, but scientists need to look at\nthe situations beyond the area of current knowledge, and they need programs to\ndo analysis in the areas of uncertainty. Is it possible do design programs\nwhich allow to analyse the situations beyond the knowledge of developers?\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 09:37:11 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Andreyev", "Sergey", ""]]}, {"id": "1905.09658", "submitter": "Lea Pillette", "authors": "L\\'ea Pillette (Potioc, LaBRI, Bordeaux INP), Camille Jeunet (CNBI,\n  Hybrid), Roger N'Kambou (Laboratoire GDAC), Bernard N'Kaoua (PHOENIX-POST,\n  HACS, CNRS), Fabien Lotte (Potioc, LaBRI, Bordeaux INP)", "title": "Towards Artificial Learning Companions for Mental Imagery-based\n  Brain-Computer Interfaces", "comments": null, "journal-ref": "WACAI 2018 - Workshop sur les \"Affects, Compagnons Artificiels et\n  Interactions'', Jun 2018, Ile de Porquerolles, France. pp.1-8", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental Imagery based Brain-Computer Interfaces (MI-BCI) enable their users to\ncontrol an interface, e.g., a prosthesis, by performing mental imagery tasks\nonly, such as imagining a right arm movement while their brain activity is\nmeasured and processed by the system. Designing and using a BCI requires users\nto learn how to produce different and stable patterns of brain activity for\neach of the mental imagery tasks. However, current training protocols do not\nenable every user to acquire the skills required to use BCIs. These training\nprotocols are most likely one of the main reasons why BCIs remain not reliable\nenough for wider applications outside research laboratories. Learning\ncompanions have been shown to improve training in different disciplines, but\nthey have barely been explored for BCIs so far. This article aims at\ninvestigating the potential benefits learning companions could bring to BCI\ntraining by improving the feedback, i.e., the information provided to the user,\nwhich is primordial to the learning process and yet have proven both\ntheoretically and practically inadequate in BCI. This paper first presents the\npotentials of BCI and the limitations of current training approaches. Then, it\nreviews both the BCI and learning companion literature regarding three main\ncharacteristics of feedback: its appearance, its social and emotional\ncomponents and its cognitive component. From these considerations, this paper\ndraws some guidelines, identify open challenges and suggests potential\nsolutions to design and use learning companions for BCIs.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 13:51:14 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Pillette", "L\u00e9a", "", "Potioc, LaBRI, Bordeaux INP"], ["Jeunet", "Camille", "", "CNBI,\n  Hybrid"], ["N'Kambou", "Roger", "", "Laboratoire GDAC"], ["N'Kaoua", "Bernard", "", "PHOENIX-POST,\n  HACS, CNRS"], ["Lotte", "Fabien", "", "Potioc, LaBRI, Bordeaux INP"]]}, {"id": "1905.09864", "submitter": "Varun Kumar", "authors": "Varun Kumar, Alison Smith-Renner, Leah Findlater, Kevin Seppi and\n  Jordan Boyd-Graber", "title": "Why Didn't You Listen to Me? Comparing User Control of Human-in-the-Loop\n  Topic Models", "comments": "In proceedings of ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To address the lack of comparative evaluation of Human-in-the-Loop Topic\nModeling (HLTM) systems, we implement and evaluate three contrasting HLTM\nmodeling approaches using simulation experiments. These approaches extend\npreviously proposed frameworks, including constraints and informed prior-based\nmethods. Users should have a sense of control in HLTM systems, so we propose a\ncontrol metric to measure whether refinement operations' results match users'\nexpectations. Informed prior-based methods provide better control than\nconstraints, but constraints yield higher quality topics.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 18:40:57 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 02:24:03 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Kumar", "Varun", ""], ["Smith-Renner", "Alison", ""], ["Findlater", "Leah", ""], ["Seppi", "Kevin", ""], ["Boyd-Graber", "Jordan", ""]]}, {"id": "1905.10141", "submitter": "Enis Ulqinaku", "authors": "Enis Ulqinaku and Julinda Stefa and Alessandro Mei", "title": "Scan-and-Pay on Android is Dangerous", "comments": "Published in Infocom MobiSec Workshop 2019, Paris, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile payments have increased significantly in the recent years and\none-to-one money transfers are offered by a wide variety of smartphone\napplications. These applications usually support scan-and-pay -- a technique\nthat allows a payer to easily scan the destination address of the payment\ndirectly from the payee's smartphone screen. This technique is pervasive\nbecause it does not require any particular hardware, only the camera, which is\npresent on all modern smartphones. However, in this work we show that a\nmalicious application can exploit the overlay feature on Android to compromise\nthe integrity of transactions that make use of the scan-and-pay technique. We\nimplement Malview, a proof-of-concept malicious application that runs in the\nbackground on the payee's smartphone and show that it succeeds in redirecting\npayments to a malicious wallet. We analyze the weaknesses of the current\ndefense mechanisms and discuss possible countermeasures against the attack.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 10:49:49 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Ulqinaku", "Enis", ""], ["Stefa", "Julinda", ""], ["Mei", "Alessandro", ""]]}, {"id": "1905.10212", "submitter": "Marco Winckler", "authors": "Thiago Rocha, Jean-Luc Hak (UPS), Marco Winckler (UFRGS, Polytech'Lab,\n  IRIT)", "title": "A Behavior-Based Ontology for Supporting Automated Assessment of\n  Interactive Systems", "comments": null, "journal-ref": "11th International Conference on Semantic Computing IEEE ICSC\n  2017, 2017, San Diego, United States", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays many software development frameworks implement Behavior-Driven\nDevelopment (BDD) as a mean of automating the test of interactive systems under\nconstruction. Automated testing helps to simulate user's action on the User\nInterface and therefore check if the system behaves properly and in accordance\nto Scenarios that describe functional requirements. However, most of tools\nsupporting BDD requires that tests should be written using low-level events and\ncomponents that only exist when the system is already implemented. As a\nconsequence of such low-level of abstraction, BDD tests can hardly be reused\nwith diverse artifacts and with versions of the system. To address this\nproblem, this paper proposes to raise the abstraction level by the means of a\nbehavior-based ontology that is aimed at supporting test automation. The paper\npresents an ontology and an on-tology-based approach for automating the test of\nfunctional requirements of interactive systems. With the help of a case study\nfor the flight tickets e-commerce domain, we demonstrate how tests written\nusing our ontology can be used to assess functional requirements using\ndifferent artifacts, from low-fidelity to full-fledged UI Prototypes.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 13:06:05 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Rocha", "Thiago", "", "UPS"], ["Hak", "Jean-Luc", "", "UPS"], ["Winckler", "Marco", "", "UFRGS, Polytech'Lab,\n  IRIT"]]}, {"id": "1905.10215", "submitter": "Marco Winckler", "authors": "Gabriela Bosetti, Sergio Firmenich (UNLP), Alejandro Fernandez\n  (LIFIA), Marco Winckler (UFRGS, Polytech'Lab, IRIT), Gustavo Rossi (UNLP)", "title": "From Search Engines to Search Services: An End-User Driven Approach", "comments": null, "journal-ref": "17th International Conference, ICWE 2017, 2017, Rome, Italy", "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web is a vast and continuously changing source of information\nwhere searching is a frequent, and sometimes critical, user task. Searching is\nnot always the user's primary goal but an ancillary task that is performed to\nfind complementary information allowing to complete another task. In this\npaper, we explore primary and/or ancillary search tasks and propose an approach\nfor simplifying the user interaction during search tasks. Rather than fo-cusing\non dedicated search engines, our approach allows the user to abstract search\nengines already provided by Web applications into pervasive search services\nthat will be available for performing searches from any other Web site. We also\npropose to allow users to manage the way in which searching results are\ndisplayed and the interaction with them. In order to illustrate the feasibility\nof this approach, we have built a support tool based on a plug-in architecture\nthat allows users to integrate new search services (created by themselves by\nmeans of visual tools) and execute them in the context of both kinds of\nsearches. A case study illustrates the use of these tools. We also present the\nresults of two evaluations that demonstrate the feasibility of the approach and\nthe benefits in its use.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 13:10:10 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Bosetti", "Gabriela", "", "UNLP"], ["Firmenich", "Sergio", "", "UNLP"], ["Fernandez", "Alejandro", "", "LIFIA"], ["Winckler", "Marco", "", "UFRGS, Polytech'Lab, IRIT"], ["Rossi", "Gustavo", "", "UNLP"]]}, {"id": "1905.10254", "submitter": "Stefano Tortora", "authors": "Tortora Stefano, Beraldo Gloria, Tonin Luca, Menegatti Emanuele", "title": "Entropy-based Motion Intention Identification for Brain-Computer\n  Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of intentionally delivered commands is a challenge in\nBrain Computer Interfaces (BCIs) based on Sensory-Motor Rhythms (SMR). It is of\nfundamental importance that BCI systems controlling a robotic device (i.e.,\nupper limb prosthesis) are capable of detecting if the user is in the so called\nIntentional Non-Control (INC) state (i.e., holding the prosthesis in a given\nposition). In this work, we propose a novel approach based on the entropy of\nthe Electroencephalogram (EEG) signals to provide a continuous identification\nof motion intention. Results from ten healthy subjects suggest that the\nproposed system can be used for reliably predicting motion in real-time at a\nframerate of 8 Hz with $80\\% \\pm 5\\%$ of accuracy. Moreover, motion intention\ncan be detected more than 1 second before muscular activation with an average\naccuracy of $76\\% \\pm 11\\%$.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 14:27:31 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Stefano", "Tortora", ""], ["Gloria", "Beraldo", ""], ["Luca", "Tonin", ""], ["Emanuele", "Menegatti", ""]]}, {"id": "1905.10315", "submitter": "Yunlong Wang", "authors": "Yunlong Wang, Harald Reiterer", "title": "The Impact of Augmented-Reality Head-Mounted Displays on Users' Movement\n  Behavior: An Exploratory Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The augmented-reality head-mounted display (e.g., Microsoft HoloLens) is one\nof the most innovative technologies in multimedia and human-computer\ninteraction in recent years. Despite the emerging research of its applications\non engineering, education, medicines, to name a few, its impact on users'\nmovement behavior is still underexplored. The movement behavior, especially for\noffice workers with sedentary lifestyles, is related to many chronic\nconditions. Unlike the traditional screens, the augmented-reality head-mounted\ndisplay (AR-HMD) could enable mobile virtual screens, which might impact on\nusers' movement behavior. In this paper, we present our initial study to\nexplore the impact of AR-HMDs on users' movement behavior. We compared the\ndifferences of macro-movements (e.g., sit-stand transitions) and\nmicro-movements (e.g., moving the head) between two experimental modes (i.e.,\nspatial-mapping and tag-along) with a dedicated trivial quiz task using\nHoloLens. The study reveals interesting findings: strong evidence supports that\nparticipants had more head-movements in the tag-along mode where higher\nsimplicity and freedom of moving the virtual screen were given; body\nposition/direction changes show the same effect with moderate evidence, while\nsit-stand transitions show no difference between the two modes with weak\nevidence. Our results imply several design considerations and research\nopportunities for future work on the ergonomics of AR-HMDs in the perspective\nof health.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 16:20:08 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 12:33:54 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Wang", "Yunlong", ""], ["Reiterer", "Harald", ""]]}, {"id": "1905.10366", "submitter": "Paul Linton", "authors": "Paul Linton", "title": "Would Gaze-Contingent Rendering Improve Depth Perception in Virtual and\n  Augmented Reality?", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near distances are overestimated in virtual reality, and far distances are\nunderestimated, but an explanation for these distortions remains elusive. One\npotential concern is that whilst the eye rotates to look at the virtual scene,\nthe virtual cameras remain static. Could using eye-tracking to change the\nperspective of the virtual cameras as the eye rotates improve depth perception\nin virtual reality? This paper identifies 14 distinct perspective distortions\nthat could in theory occur from keeping the virtual cameras fixed whilst the\neye rotates in the context of near-eye displays. However, the impact of eye\nmovements on the displayed image depends on the optical, rather than physical,\ndistance of the display. Since the optical distance of most head-mounted\ndisplays is over 1m, most of these distortions will have only a negligible\neffect. The exception are 'gaze-contingent disparities', which will leave near\nvirtual objects looking displaced from physical objects that are meant to be at\nthe same distance in augmented reality.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 11:47:08 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Linton", "Paul", ""]]}, {"id": "1905.10423", "submitter": "Syed Anwar", "authors": "Aasim Raheel, Muhammad Majid, Syed Muhammad Anwar, Ulas Bagci", "title": "Emotion Classification in Response to Tactile Enhanced Multimedia using\n  Frequency Domain Features of Brain Signals", "comments": "Accepted in IEEE EMBC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile enhanced multimedia is generated by synchronizing traditional\nmultimedia clips, to generate hot and cold air effect, with an electric heater\nand a fan. This objective is to give viewers a more realistic and immersing\nfeel of the multimedia content. The response to this enhanced multimedia\ncontent (mulsemedia) is evaluated in terms of the appreciation/emotion by using\nhuman brain signals. We observe and record electroencephalography (EEG) data\nusing a commercially available four channel MUSE headband. A total of 21\nparticipants voluntarily participated in this study for EEG recordings. We\nextract frequency domain features from five different bands of each EEG\nchannel. Four emotions namely: happy, relaxed, sad, and angry are classified\nusing a support vector machine in response to the tactile enhanced multimedia.\nAn increased accuracy of 76:19% is achieved when compared to 63:41% by using\nthe time domain features. Our results show that the selected frequency domain\nfeatures could be better suited for emotion classification in mulsemedia\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 23:11:41 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Raheel", "Aasim", ""], ["Majid", "Muhammad", ""], ["Anwar", "Syed Muhammad", ""], ["Bagci", "Ulas", ""]]}, {"id": "1905.10444", "submitter": "Oleksii Sidorov", "authors": "Oleksii Sidorov and Joshua S. Harvey and Hannah E. Smithson and Jon Y.\n  Hardeberg", "title": "Overt visual attention on rendered 3D objects", "comments": "Draft submitted to a conference. To be updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work covers multiple aspects of overt visual attention on 3D renders:\nmeasurement, projection, visualization, and application to studying the\ninfluence of material appearance on looking behaviour. In the scope of this\nwork, we ran an eye-tracking experiment in which the observers are presented\nwith animations of rotating 3D objects. The objects were rendered to simulate\ndifferent metallic appearance, particularly smooth (glossy), rough (matte), and\ncoated gold. The eye-tracking results illustrate how material appearance itself\ninfluences the observer's attention, while all the other parameters remain\nunchanged. In order to make visualization of the attention maps more natural\nand also make the analysis more accurate, we develop a novel technique of\nprojection of gaze fixations on the 3D surface of the figure itself, instead of\nthe conventional 2D plane of the screen. The proposed methodology will be\nuseful for further studies of attention and saliency in the computer graphics\ndomain.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 21:09:11 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Sidorov", "Oleksii", ""], ["Harvey", "Joshua S.", ""], ["Smithson", "Hannah E.", ""], ["Hardeberg", "Jon Y.", ""]]}, {"id": "1905.10482", "submitter": "Subhasis Dasgupta", "authors": "Junan Guo, Subhasis Dasgupta and Amarnath Gupta", "title": "Multi-Model Investigative Exploration of Social Media Data with\n  boutique: A Case Study in Public Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC cs.IR cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present our experience with a data science problem in Public Health, where\nresearchers use social media (Twitter) to determine whether the public shows\nawareness of HIV prevention measures offered by Public Health campaigns. To\nhelp the researcher, we develop an investigative exploration system called\nboutique that allows a user to perform a multi-step visualization and\nexploration of data through a dashboard interface. Unique features of boutique\nincludes its ability to handle heterogeneous types of data provided by a\npolystore, and its ability to use computation as part of the investigative\nexploration process. In this paper, we present the design of the boutique\nmiddleware and walk through an investigation process for a real-life problem.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 23:34:48 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Guo", "Junan", ""], ["Dasgupta", "Subhasis", ""], ["Gupta", "Amarnath", ""]]}, {"id": "1905.10565", "submitter": "Pepa Atanasova", "authors": "Pepa Atanasova, Georgi Karadzhov, Yasen Kiprov, Preslav Nakov,\n  Fabrizio Sebastiani", "title": "Evaluating Variable-Length Multiple-Option Lists in Chatbots and Mobile\n  Search", "comments": "4 pages, in Proceeding of SIGIR 2019", "journal-ref": null, "doi": "10.1145/3331184.3331308", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the proliferation of smart mobile devices has lead to the\ngradual integration of search functionality within mobile platforms. This has\ncreated an incentive to move away from the \"ten blue links'' metaphor, as\nmobile users are less likely to click on them, expecting to get the answer\ndirectly from the snippets. In turn, this has revived the interest in Question\nAnswering. Then, along came chatbots, conversational systems, and messaging\nplatforms, where the user needs could be better served with the system asking\nfollow-up questions in order to better understand the user's intent. While\ntypically a user would expect a single response at any utterance, a system\ncould also return multiple options for the user to select from, based on\ndifferent system understandings of the user's intent. However, this possibility\nshould not be overused, as this practice could confuse and/or annoy the user.\nHow to produce good variable-length lists, given the conflicting objectives of\nstaying short while maximizing the likelihood of having a correct answer\nincluded in the list, is an underexplored problem. It is also unclear how to\nevaluate a system that tries to do that. Here we aim to bridge this gap. In\nparticular, we define some necessary and some optional properties that an\nevaluation measure fit for this purpose should have. We further show that\nexisting evaluation measures from the IR tradition are not entirely suitable\nfor this setup, and we propose novel evaluation measures that address it\nsatisfactorily.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 10:22:15 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Atanasova", "Pepa", ""], ["Karadzhov", "Georgi", ""], ["Kiprov", "Yasen", ""], ["Nakov", "Preslav", ""], ["Sebastiani", "Fabrizio", ""]]}, {"id": "1905.10576", "submitter": "Michael Gygli", "authors": "Michael Gygli and Vittorio Ferrari", "title": "Efficient Object Annotation via Speaking and Pointing", "comments": "this article is an extension of arXiv:1811.09461, which was published\n  at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks deliver state-of-the-art visual recognition, but they\nrely on large datasets, which are time-consuming to annotate. These datasets\nare typically annotated in two stages: (1) determining the presence of object\nclasses at the image level and (2) marking the spatial extent for all objects\nof these classes. In this work we use speech, together with mouse inputs, to\nspeed up this process. We first improve stage one, by letting annotators\nindicate object class presence via speech. We then combine the two stages:\nannotators draw an object bounding box via the mouse and simultaneously provide\nits class label via speech. Using speech has distinct advantages over relying\non mouse inputs alone. First, it is fast and allows for direct access to the\nclass name, by simply saying it. Second, annotators can simultaneously speak\nand mark an object location. Finally, speech-based interfaces can be kept\nextremely simple, hence using them requires less mouse movement compared to\nexisting approaches. Through extensive experiments on the COCO and ILSVRC\ndatasets we show that our approach yields high-quality annotations at\nsignificant speed gains. Stage one takes 2.3x - 14.9x less annotation time than\nexisting methods based on a hierarchical organization of the classes to be\nannotated. Moreover, when combining the two stages, we find that object class\nlabels come for free: annotating them at the same time as bounding boxes has\nzero additional cost. On COCO, this makes the overall process 1.9x faster than\nthe two-stage approach.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 11:36:16 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 12:44:38 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 16:50:25 GMT"}, {"version": "v4", "created": "Thu, 19 Dec 2019 12:57:30 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Gygli", "Michael", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1905.10700", "submitter": "Ziang Xiao", "authors": "Ziang Xiao, Michelle X. Zhou, Q. Vera Liao, Gloria Mark, Changyan Chi,\n  Wenxi Chen, and Huahai Yang", "title": "Tell Me About Yourself: Using an AI-Powered Chatbot to Conduct\n  Conversational Surveys with Open-ended Questions", "comments": "The paper is accepted by ACM Transactions on Computer-Human\n  Interaction (TOCHI)", "journal-ref": null, "doi": "10.1145/3381804", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of increasingly more powerful chatbots offers a new way to collect\ninformation through conversational surveys, where a chatbot asks open-ended\nquestions, interprets a user's free-text responses, and probes answers whenever\nneeded. To investigate the effectiveness and limitations of such a chatbot in\nconducting surveys, we conducted a field study involving about 600\nparticipants. In this study with mostly open-ended questions, half of the\nparticipants took a typical online survey on Qualtrics and the other half\ninteracted with an AI-powered chatbot to complete a conversational survey. Our\ndetailed analysis of over 5200 free-text responses revealed that the chatbot\ndrove a significantly higher level of participant engagement and elicited\nsignificantly better quality responses measured by Gricean Maxims in terms of\ntheir informativeness, relevance, specificity, and clarity. Based on our\nresults, we discuss design implications for creating AI-powered chatbots to\nconduct effective surveys and beyond.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 23:46:29 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 00:07:09 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Xiao", "Ziang", ""], ["Zhou", "Michelle X.", ""], ["Liao", "Q. Vera", ""], ["Mark", "Gloria", ""], ["Chi", "Changyan", ""], ["Chen", "Wenxi", ""], ["Yang", "Huahai", ""]]}, {"id": "1905.10893", "submitter": "Shuhan Wang", "authors": "Shuhan Wang, Hao Wu, Ji Hun Kim and Erik Andersen", "title": "Adaptive Learning Material Recommendation in Online Language Education", "comments": "The short version of this paper is published at AIED 2019", "journal-ref": "The 20th International Conference on Artificial Intelligence in\n  Education (AIED), 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending personalized learning materials for online language learning is\nchallenging because we typically lack data about the student's ability and the\nrelative difficulty of learning materials. This makes it hard to recommend\nappropriate content that matches the student's prior knowledge. In this paper,\nwe propose a refined hierarchical knowledge structure to model vocabulary\nknowledge, which enables us to automatically organize the authentic and\nup-to-date learning materials collected from the internet. Based on this\nknowledge structure, we then introduce a hybrid approach to recommend learning\nmaterials that adapts to a student's language level. We evaluate our work with\nan online Japanese learning tool and the results suggest adding adaptivity into\nmaterial recommendation significantly increases student engagement.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 21:59:49 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Wang", "Shuhan", ""], ["Wu", "Hao", ""], ["Kim", "Ji Hun", ""], ["Andersen", "Erik", ""]]}, {"id": "1905.10958", "submitter": "Prashan Madumal", "authors": "Prashan Madumal, Tim Miller, Liz Sonenberg, Frank Vetere", "title": "Explainable Reinforcement Learning Through a Causal Lens", "comments": "Accepted to AAAI 2020 - full paper (oral) - main track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prevalent theories in cognitive science propose that humans understand and\nrepresent the knowledge of the world through causal relationships. In making\nsense of the world, we build causal models in our mind to encode cause-effect\nrelations of events and use these to explain why new events happen. In this\npaper, we use causal models to derive causal explanations of behaviour of\nreinforcement learning agents. We present an approach that learns a structural\ncausal model during reinforcement learning and encodes causal relationships\nbetween variables of interest. This model is then used to generate explanations\nof behaviour based on counterfactual analysis of the causal model. We report on\na study with 120 participants who observe agents playing a real-time strategy\ngame (Starcraft II) and then receive explanations of the agents' behaviour. We\ninvestigated: 1) participants' understanding gained by explanations through\ntask prediction; 2) explanation satisfaction and 3) trust. Our results show\nthat causal model explanations perform better on these measures compared to two\nother baseline explanation models.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 03:39:17 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 07:54:08 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Madumal", "Prashan", ""], ["Miller", "Tim", ""], ["Sonenberg", "Liz", ""], ["Vetere", "Frank", ""]]}, {"id": "1905.11342", "submitter": "Shawn Jones", "authors": "Shawn M. Jones and Michele C. Weigle and Michael L. Nelson", "title": "Social Cards Probably Provide For Better Understanding Of Web Archive\n  Collections", "comments": "58 pages, 53 figures", "journal-ref": null, "doi": "10.1145/3357384.3358039", "report-no": null, "categories": "cs.DL cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Used by a variety of researchers, web archive collections have become\ninvaluable sources of evidence. If a researcher is presented with a web archive\ncollection that they did not create, how do they know what is inside so that\nthey can use it for their own research? Search engine results and social media\nlinks are represented as surrogates, small easily digestible summaries of the\nunderlying page. Search engines and social media have a different focus, and\nhence produce different surrogates than web archives. Search engine surrogates\nhelp a user answer the question \"Will this link meet my information need?\"\nSocial media surrogates help a user decide \"Should I click on this?\" Our use\ncase is subtly different. We hypothesize that groups of surrogates together are\nuseful for summarizing a collection. We want to help users answer the question\nof \"What does the underlying collection contain?\" But which surrogate should we\nuse? With Mechanical Turk participants, we evaluate six different surrogate\ntypes against each other. We find that the type of surrogate does not influence\nthe time to complete the task we presented the participants. Of particular\ninterest are social cards, surrogates typically found on social media, and\nbrowser thumbnails, screen captures of web pages rendered in a browser. At\n$p=0.0569$, and $p=0.0770$, respectively, we find that social cards and social\ncards paired side-by-side with browser thumbnails probably provide better\ncollection understanding than the surrogates currently used by the popular\nArchive-It web archiving platform. We measure user interactions with each\nsurrogate and find that users interact with social cards less than other types.\nThe results of this study have implications for our web archive summarization\nwork, live web curation platforms, social media, and more.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 17:00:54 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 16:55:15 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 02:06:42 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Jones", "Shawn M.", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "1905.11393", "submitter": "Jin Zeng", "authors": "Mengyang Chen, Jin Zeng, and Jie Lou", "title": "A Self-Attention Joint Model for Spoken Language Understanding in\n  Situational Dialog Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spoken language understanding (SLU) acts as a critical component in\ngoal-oriented dialog systems. It typically involves identifying the speakers\nintent and extracting semantic slots from user utterances, which are known as\nintent detection (ID) and slot filling (SF). SLU problem has been intensively\ninvestigated in recent years. However, these methods just constrain SF results\ngrammatically, solve ID and SF independently, or do not fully utilize the\nmutual impact of the two tasks. This paper proposes a multi-head self-attention\njoint model with a conditional random field (CRF) layer and a prior mask. The\nexperiments show the effectiveness of our model, as compared with\nstate-of-the-art models. Meanwhile, online education in China has made great\nprogress in the last few years. But there are few intelligent educational\ndialog applications for students to learn foreign languages. Hence, we design\nan intelligent dialog robot equipped with different scenario settings to help\nstudents learn communication skills.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 10:22:20 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Chen", "Mengyang", ""], ["Zeng", "Jin", ""], ["Lou", "Jie", ""]]}, {"id": "1905.11633", "submitter": "Davide Brunelli PhD", "authors": "M. Guermandi, S. Benatti, D. Brunelli, V. Kartsch, L. Benini", "title": "Towards a Wearable Interface for Food Quality Grading through ERP\n  Analysis", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": "10.1109/ISCAS.2019.8702725", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory evaluation is used to assess the consumer acceptance of foods or\nother consumer products, so as to improve industrial processes and marketing\nstrategies. The procedures currently involved are time-consuming because they\nrequire a statistical approach from measurements and feedback reports from a\nwide set of evaluators under a well-established measurement setup. In this\npaper, we propose to collect directly the signal of the perceived quality of\nthe food from Event-related potentials (ERPs) that are the outcome of the\nprocessing of visual stimuli. This permits to narrow the number of evaluators\nsince errors related to psychological factors are by-passed. We present the\ndesign of a wearable system for ERP measurement and we present preliminary\nresults on the use of ERP to give a quantitative measure to the appearance of a\nfood product. The system is developed to be wearable and our experiments\ndemonstrate that is possible to use it to identify and classify the grade of\nacceptance of the food.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 06:41:37 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Guermandi", "M.", ""], ["Benatti", "S.", ""], ["Brunelli", "D.", ""], ["Kartsch", "V.", ""], ["Benini", "L.", ""]]}, {"id": "1905.11652", "submitter": "Charith Perera", "authors": "Ahmed Hussein, Mahmoud Barhamgi, Massimo Vecchio, Charith Perera", "title": "Crowdsourced Peer Learning Activity for Internet of Things Education: A\n  Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing devices such as laptops, tablets and mobile phones have become part\nof our daily lives. End users increasingly know more and more information about\nthese devices. Further, more technically savvy end users know how such devices\nare being built and know how to choose one over the others. However, we cannot\nsay the same about the Internet of Things (IoT) products. Due to its infancy\nnature of the marketplace, end users have very little idea about IoT products.\nTo address this issue, we developed a method, a crowdsourced peer learning\nactivity, supported by an online platform (OLYMPUS) to enable a group of\nlearners to learn IoT products space better. We conducted two different user\nstudies to validate that our tool enables better IoT education. Our method\nguide learners to think more deeply about IoT products and their design\ndecisions. The learning platform we developed is open source and available for\nthe community.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 07:28:10 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Hussein", "Ahmed", ""], ["Barhamgi", "Mahmoud", ""], ["Vecchio", "Massimo", ""], ["Perera", "Charith", ""]]}, {"id": "1905.11695", "submitter": "Xavier Ouvrard", "authors": "Xavier Ouvrard and Jean-Marie Le Goff and St\\'ephane Marchand-Maillet", "title": "The HyperBagGraph DataEdron: An Enriched Browsing Experience of\n  Multimedia Datasets", "comments": "Extension of the hypergraph framework shortly presented in\n  arXiv:1809.00164 (possible small overlaps); use the theoretical framework of\n  hb-graphs presented in arXiv:1809.00190", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional verbatim browsers give back information in a linear way according\nto a ranking performed by a search engine that may not be optimal for the\nsurfer. The latter may need to assess the pertinence of the information\nretrieved, particularly when s$\\cdot$he wants to explore other facets of a\nmulti-facetted information space. For instance, in a multimedia dataset\ndifferent facets such as keywords, authors, publication category, organisations\nand figures can be of interest. The facet simultaneous visualisation can help\nto gain insights on the information retrieved and call for further searches.\nFacets are co-occurence networks, modeled by HyperBag-Graphs -- families of\nmultisets -- and are in fact linked not only to the publication itself, but to\nany chosen reference. These references allow to navigate inside the dataset and\nperform visual queries. We explore here the case of scientific publications\nbased on Arxiv searches.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 09:14:08 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Ouvrard", "Xavier", ""], ["Goff", "Jean-Marie Le", ""], ["Marchand-Maillet", "St\u00e9phane", ""]]}, {"id": "1905.11734", "submitter": "Stefano Tortora", "authors": "Stefano Tortora, Stefano Michieletto, Francesca Stival, Emanuele\n  Menegatti", "title": "Fast human motion prediction for human-robot collaboration with wearable\n  interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at improving human motion prediction during human-robot\ncollaboration in industrial facilities by exploiting contributions from both\nphysical and physiological signals. Improved human-machine collaboration could\nprove useful in several areas, while it is crucial for interacting robots to\nunderstand human movement as soon as possible to avoid accidents and injuries.\nIn this perspective, we propose a novel human-robot interface capable to\nanticipate the user intention while performing reaching movements on a working\nbench in order to plan the action of a collaborative robot. The proposed\ninterface can find many applications in the Industry 4.0 framework, where\nautonomous and collaborative robots will be an essential part of innovative\nfacilities. A motion intention prediction and a motion direction prediction\nlevels have been developed to improve detection speed and accuracy. A Gaussian\nMixture Model (GMM) has been trained with IMU and EMG data following an\nevidence accumulation approach to predict reaching direction. Novel dynamic\nstopping criteria have been proposed to flexibly adjust the trade-off between\nearly anticipation and accuracy according to the application. The output of the\ntwo predictors has been used as external inputs to a Finite State Machine (FSM)\nto control the behaviour of a physical robot according to user's action or\ninaction. Results show that our system outperforms previous methods, achieving\na real-time classification accuracy of $94.3\\pm2.9\\%$ after\n$160.0msec\\pm80.0msec$ from movement onset.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 10:48:48 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Tortora", "Stefano", ""], ["Michieletto", "Stefano", ""], ["Stival", "Francesca", ""], ["Menegatti", "Emanuele", ""]]}, {"id": "1905.11775", "submitter": "Pekka Siirtola", "authors": "Pekka Siirtola, Heli Koskim\\\"aki, Juha R\\\"oning", "title": "Importance of user inputs while using incremental learning to\n  personalize human activity recognition models", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN) 2019, pages 449-454", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, importance of user inputs is studied in the context of\npersonalizing human activity recognition models using incremental learning.\nInertial sensor data from three body positions are used, and the classification\nis based on Learn++ ensemble method. Three different approaches to update\nmodels are compared: non-supervised, semi-supervised and supervised.\nNon-supervised approach relies fully on predicted labels, supervised fully on\nuser labeled data, and the proposed method for semi-supervised learning, is a\ncombination of these two. In fact, our experiments show that by relying on\npredicted labels with high confidence, and asking the user to label only\nuncertain observations (from 12% to 26% of the observations depending on the\nused base classifier), almost as low error rates can be achieved as by using\nsupervised approach. In fact, the difference was less than 2%-units. Moreover,\nunlike non-supervised approach, semi-supervised approach does not suffer from\ndrastic concept drift, and thus, the error rate of the non-supervised approach\nis over 5%-units higher than using semi-supervised approach.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:41:02 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Siirtola", "Pekka", ""], ["Koskim\u00e4ki", "Heli", ""], ["R\u00f6ning", "Juha", ""]]}, {"id": "1905.11780", "submitter": "Pekka Siirtola", "authors": "Pekka Siirtola, Jukka Komulainen, Vili Kellokumpu", "title": "Effect of context in swipe gesture-based continuous authentication on\n  smartphones", "comments": "European Symposium on Artificial Neural Networks, Computational\n  Intelligence and Machine Learning (ESANN) 2018, pages 639-644", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates how context should be taken into account when\nperforming continuous authentication of a smartphone user based on touchscreen\nand accelerometer readings extracted from swipe gestures. The study is\nconducted on the publicly available HMOG dataset consisting of 100 study\nsubjects performing pre-defined reading and navigation tasks while sitting and\nwalking. It is shown that context-specific models are needed for different\nsmartphone usage and human activity scenarios to minimize authentication error.\nAlso, the experimental results suggests that utilization of phone movement\nimproves swipe gesture-based verification performance only when the user is\nmoving.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:49:55 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Siirtola", "Pekka", ""], ["Komulainen", "Jukka", ""], ["Kellokumpu", "Vili", ""]]}, {"id": "1905.11791", "submitter": "Mevlut Serkan Tok", "authors": "Mevlut Serkan Tok, Osman Tufan Tekin, Kemal Bicakci", "title": "Analyzing Turkish F and Turkish E keyboard layouts using learning curves", "comments": "10 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-layout was introduced in 1955 and eventually enforced as a national\nstandard as a replacement to the popular QWERTY keyboard layout in Turkey. In a\nmore recent work, another alternative (E-layout) was developed for Turkish\nlanguage and argued to be faster and more comfortable than the F-layout.\nHowever, there has not been any empirical evidence favouring any of these\nlayouts so far. To fill this research gap in the literature, we have employed a\nhybrid model and conducted both between-subjects and within-subjects user\nexperiments with twelve freshmen majoring in computer engineering. The\nexperimental results show that there is no significant difference between\nlearning percentage of these two layouts but the completion time of typing a\ntrial passage with the F-layout is significantly lower than the E-layout. The\nF-layout has also a significantly lower physical demand score, as revealed by\nthe subjective assessments of participants. Based on our user survey data, we\nalso discuss some possible reasons of F-keyboard limited prevalence among\nTurkish users.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 11:12:26 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Tok", "Mevlut Serkan", ""], ["Tekin", "Osman Tufan", ""], ["Bicakci", "Kemal", ""]]}, {"id": "1905.11984", "submitter": "Rohit Vaish", "authors": "Haoming Li, Sujoy Sikdar, Rohit Vaish, Junming Wang, Lirong Xia,\n  Chaonan Ye", "title": "Minimizing Time-to-Rank: A Learning and Recommendation Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following problem faced by an online voting platform: A user is\nprovided with a list of alternatives, and is asked to rank them in order of\npreference using only drag-and-drop operations. The platform's goal is to\nrecommend an initial ranking that minimizes the time spent by the user in\narriving at her desired ranking. We develop the first optimization framework to\naddress this problem, and make theoretical as well as practical contributions.\nOn the practical side, our experiments on Amazon Mechanical Turk provide two\ninteresting insights about user behavior: First, that users' ranking strategies\nclosely resemble selection or insertion sort, and second, that the time taken\nfor a drag-and-drop operation depends linearly on the number of positions\nmoved. These insights directly motivate our theoretical model of the\noptimization problem. We show that computing an optimal recommendation is\nNP-hard, and provide exact and approximation algorithms for a variety of\nspecial cases of the problem. Experimental evaluation on MTurk shows that,\ncompared to a random recommendation strategy, the proposed approach reduces the\n(average) time-to-rank by up to 50%.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 23:50:06 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Li", "Haoming", ""], ["Sikdar", "Sujoy", ""], ["Vaish", "Rohit", ""], ["Wang", "Junming", ""], ["Xia", "Lirong", ""], ["Ye", "Chaonan", ""]]}, {"id": "1905.12239", "submitter": "Simon D. Duque Anton", "authors": "Simon Duque Anton, Daniel Fraunholz, Christoph Lipps, Khurshid Alam,\n  and Hans Dieter Schotten", "title": "Putting Things in Context: Securing Industrial Authentication with\n  Context Information", "comments": "This is the preprint of a work published in the Intl. Journal on\n  Cyber Situational Awareness (IJCSA)", "journal-ref": null, "doi": "10.22619/IJCSA.2018.100122", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development in the area of wireless communication, mobile and embedded\ncomputing leads to significant changes in the application of devices. Over the\nlast years, embedded devices were brought into the consumer area creating the\nInternet of Things. Furthermore, industrial applications increasingly rely on\ncommunication through trust boundaries. Networking is cheap and easily\napplicable while providing the possibility to make everyday life more easy and\ncomfortable and industry more efficient and less time-consuming. One of the\ncrucial parts of this interconnected world is sound and secure authentication\nof entities. Only entities with valid authorisation should be enabled to act on\na resource according to an access control scheme. An overview of challenges and\npractices of authentication is provided in this work, with a special focus on\ncontext information as part of security solutions. It can be used for\nauthentication and security solutions in industrial applications. Additional\ninformation about events in networks can aid intrusion detection, especially in\ncombination with security information and event management systems. Finally, an\nauthentication and access control approach, based on context information and -\ndepending on the scenario - multiple factors is presented. The combination of\nmultiple factors with context information makes it secure and at the same time\ncase adaptive, so that the effort always matches, but never exceeds, the\nsecurity demand. This is a common issue of standard cyber security, entities\nhaving to obey strict, inflexible and unhandy policies. This approach has been\nimplemented exemplary based on RADIUS. Different scenarios were considered,\nshowing that this approach is capable of providing flexible and scalable\nsecurity for authentication processes.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 06:32:06 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Anton", "Simon Duque", ""], ["Fraunholz", "Daniel", ""], ["Lipps", "Christoph", ""], ["Alam", "Khurshid", ""], ["Schotten", "Hans Dieter", ""]]}, {"id": "1905.12240", "submitter": "Na Dong", "authors": "Na Dong, Wen-qi Zhang, Zhong-ke Gao", "title": "Research on fuzzy PID Shared control method of small brain-controlled\n  uav", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-controlled unmanned aerial vehicle (uav) is a uav that can analyze\nhuman brain electrical signals through BCI to obtain flight commands. The\nresearch of brain-controlled uav can promote the integration of brain-computer\nand has a broad application prospect. At present, BCI still has some problems,\nsuch as limited recognition accuracy, limited recognition time and small number\nof recognition commands in the acquisition of control commands by analyzing eeg\nsignals. Therefore, the control performance of the quadrotor which is\ncontrolled only by brain is not ideal. Based on the concept of Shared control,\nthis paper designs an assistant controller using fuzzy PID control, and\nrealizes the cooperative control between automatic control and brain control.\nBy evaluating the current flight status and setting the switching rate, the\nswitching mechanism of automatic control and brain control can be decided to\nimprove the system control performance. Finally, a rectangular trajectory\ntracking control experiment of the same height is designed for small quadrotor\nto verify the algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 06:35:04 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Dong", "Na", ""], ["Zhang", "Wen-qi", ""], ["Gao", "Zhong-ke", ""]]}, {"id": "1905.12285", "submitter": "Pekka Siirtola", "authors": "Pekka Siirtola, Heli Koskim\\\"aki, Juha R\\\"oning", "title": "From User-independent to Personal Human Activity Recognition Models\n  Exploiting the Sensors of a Smartphone", "comments": "From User-independent to Personal Human Activity Recognition Models\n  Exploiting the Sensors of a Smartphone, European Symposium on Artificial\n  Neural Networks, Compu-tational Intelligence and Machine Learning, ESANN\n  2016., Bruges, Belgium 27-29 April 2016, 471--476", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, a novel method to obtain user-dependent human activity\nrecognition models unobtrusively by exploiting the sensors of a smartphone is\npresented. The recognition consists of two models: sensor fusion-based\nuser-independent model for data labeling and single sensor-based user-dependent\nmodel for final recognition. The functioning of the presented method is tested\nwith human activity data set, including data from accelerometer and\nmagnetometer, and with two classifiers. Comparison of the detection accuracies\nof the proposed method to traditional user-independent model shows that the\npresented method has potential, in nine cases out of ten it is better than the\ntraditional method, but more experiments using different sensor combinations\nshould be made to show the full potential of the method.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 09:25:08 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Siirtola", "Pekka", ""], ["Koskim\u00e4ki", "Heli", ""], ["R\u00f6ning", "Juha", ""]]}, {"id": "1905.12487", "submitter": "Kaylen Pfisterer", "authors": "Kaylen J. Pfisterer, Jennifer Boger, Alexander Wong", "title": "Food for thought: Ethical considerations of user trust in computer\n  vision", "comments": "Accepted to CVPR2019: Fairness Accountability Transparency and Ethics\n  in Computer Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer vision research, especially when novel applications of tools are\ndeveloped, ethical implications around user perceptions of trust in the\nunderlying technology should be considered and supported. Here, we describe an\nexample of the incorporation of such considerations within the long-term care\nsector for tracking resident food and fluid intake. We highlight our recent\nuser study conducted to develop a Goldilocks quality horizontal prototype\ndesigned to support trust cues in which perceived trust in our horizontal\nprototype was higher than the existing system in place. We discuss the\nimportance and need for user engagement as part of ongoing computer\nvision-driven technology development and describe several important factors\nrelated to trust that are relevant to developing decision-making tools.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 14:25:43 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Pfisterer", "Kaylen J.", ""], ["Boger", "Jennifer", ""], ["Wong", "Alexander", ""]]}, {"id": "1905.12686", "submitter": "Sophie Hilgard", "authors": "Sophie Hilgard, Nir Rosenfeld, Mahzarin R. Banaji, Jack Cao, David C.\n  Parkes", "title": "Learning Representations by Humans, for Humans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of optimizing machines to support human decision-making is often\nconflated with that of optimizing machines for accuracy even though they are\nmaterially different. Whereas it is typical for learning systems to prescribe\nactions through prediction, here we propose an approach in which the role of\nmachines is to reframe problems in order to directly support human decisions.\nInspired by the success of representation learning in promoting machine\nperformance, we frame the problem as one of learning representations that are\nconducive to good human performance. This \"Man Composed with Machine\" framework\nincorporates a human decision-making model directly into the representation\nlearning paradigm with optimization achieved through a novel human-in-the-loop\ntraining procedure. We empirically demonstrate on various tasks and\nrepresentational forms that the framework is capable of learning\nrepresentations that better coincide with human decision-making processes and\ncan lead to good decisions.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 19:19:09 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 22:41:49 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2020 13:55:11 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Hilgard", "Sophie", ""], ["Rosenfeld", "Nir", ""], ["Banaji", "Mahzarin R.", ""], ["Cao", "Jack", ""], ["Parkes", "David C.", ""]]}, {"id": "1905.12694", "submitter": "Guenter Wallner", "authors": "G\\\"unter Wallner, Simone Kriglstein, Anders Drachen", "title": "Tweeting your Destiny: Profiling Users in the Twitter Landscape around\n  an Online Game", "comments": "Accepted at IEEE Conference on Games 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has become a major communication channel for communities\ncentered around video games. Consequently, social media offers a rich data\nsource to study online communities and the discussions evolving around games.\nTowards this end, we explore a large-scale dataset consisting of over 1 million\ntweets related to the online multiplayer shooter Destiny and spanning a time\nperiod of about 14 months using unsupervised clustering and topic modelling.\nFurthermore, we correlate Twitter activity of over 3,000 players with their\nplaytime. Our results contribute to the understanding of online player\ncommunities by identifying distinct player groups with respect to their Twitter\ncharacteristics, describing subgroups within the Destiny community, and\nuncovering broad topics of community interest.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 19:32:32 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Wallner", "G\u00fcnter", ""], ["Kriglstein", "Simone", ""], ["Drachen", "Anders", ""]]}, {"id": "1905.12884", "submitter": "Fabio Paolizzo", "authors": "Fabio Paolizzo", "title": "M-GWAP: An Online and Multimodal Game With A Purpose in WordPress for\n  Mental States Annotation", "comments": "2 figures, 4 tables. The research is supported by the EU through the\n  MUSICAL-MOODS project funded by the Marie Sklodowska-Curie Actions Individual\n  Fellowships Global Fellowships (MSCA-IF-GF) of the Horizon 2020 Programme\n  H2020/2014-2020, REA grant agreement n.659434", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  M-GWAP is a multimodal game with a purpose of that leverages on the wisdom of\ncrowds phenomenon for the annotation of multimedia data in terms of mental\nstates. This game with a purpose is developed in WordPress to allow users\nimplementing the game without programming skills. The game adopts motivational\nstrategies for the player to remain engaged, such as a score system, text\nmotivators while playing, a ranking system to foster competition and mechanics\nfor identify building. The current version of the game was deployed after alpha\nand beta testing helped refining the game accordingly.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 07:07:52 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Paolizzo", "Fabio", ""]]}, {"id": "1905.13044", "submitter": "Na Dong", "authors": "Na Dong, Wen-qi Zhang, Zhong-ke Gao", "title": "Shared control schematic for brain controlled vehicle based on fuzzy\n  logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain controlled vehicle refers to the vehicle that obtains control commands\nby analyzing the driver's EEG through Brain-Computer Interface (BCI). The\nresearch of brain controlled vehicles can not only promote the integration of\nbrain machines, but also expand the range of activities and living ability of\nthe disabled or some people with limited physical activity, so the research of\nbrain controlled vehicles is of great significance and has broad application\nprospects. At present, BCI has some problems such as limited recognition\naccuracy, long recognition time and limited number of recognition commands in\nthe process of analyzing EEG signals to obtain control commands. If only use\nthe driver's EEG signals to control the vehicle, the control performance is not\nideal. Based on the concept of Shared control, this paper uses the fuzzy\ncontrol (FC) to design an auxiliary controller to realize the cooperative\ncontrol of automatic control and brain control. Designing a Shared controller\nwhich evaluates the current vehicle status and decides the switching mechanism\nbetween automatic control and brain control to improve the system control\nperformance. Finally, based on the joint simulation platform of Carsim and\nMATLAB, with the simulated brain control signals, the designed experiment\nverifies that the control performance of the brain control vehicle can be\nimproved by adding the auxiliary controller.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 05:00:46 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Dong", "Na", ""], ["Zhang", "Wen-qi", ""], ["Gao", "Zhong-ke", ""]]}, {"id": "1905.13072", "submitter": "Advait Sarkar", "authors": "Judith Borghouts, Andrew D. Gordon, Advait Sarkar, Kenton P. O'Hara,\n  Neil Toronto", "title": "Somewhere Around That Number: An Interview Study of How Spreadsheet\n  Users Manage Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spreadsheet users regularly deal with uncertainty in their data, for example\ndue to errors and estimates. While an insight into data uncertainty can help in\nmaking better informed decisions, prior research suggests that people often use\ninformal heuristics to reason with probabilities, which leads to incorrect\nconclusions. Moreover, people often ignore or simplify uncertainty. To\nunderstand how people currently encounter and deal with uncertainty in\nspreadsheets, we conducted an interview study with 11 spreadsheet users from a\nrange of domains. We found that how people deal with uncertainty is influenced\nby the role the spreadsheet plays in people's work and the user's aims.\nSpreadsheets are used as a database, template, calculation tool, notepad and\nexploration tool. In doing so, participants' aims were to compute and compare\ndifferent scenarios, understand something about the nature of the uncertainty\nin their situation, and translate the complexity of data uncertainty into\nsimplified presentations to other people, usually decision-makers. Spreadsheets\ncurrently provide limited tools to support these aims, and participants had\nvarious workarounds.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 14:26:46 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Borghouts", "Judith", ""], ["Gordon", "Andrew D.", ""], ["Sarkar", "Advait", ""], ["O'Hara", "Kenton P.", ""], ["Toronto", "Neil", ""]]}, {"id": "1905.13135", "submitter": "Katy Williams", "authors": "Katy Williams, Alex Bigelow, Katherine E. Isaacs", "title": "Visualizing a Moving Target: A Design Study on Task Parallel Programs in\n  the Presence of Evolving Data and Concerns", "comments": "IEEE VIS InfoVis 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934285", "report-no": null, "categories": "cs.HC cs.DC cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common pitfalls in visualization projects include lack of data availability\nand the domain users' needs and focus changing too rapidly for the design\nprocess to complete. While it is often prudent to avoid such projects, we argue\nit can be beneficial to engage them in some cases as the visualization process\ncan help refine data collection, solving a \"chicken and egg\" problem of having\nthe data and tools to analyze it. We found this to be the case in the domain of\ntask parallel computing where such data and tooling is an open area of\nresearch. Despite these hurdles, we conducted a design study. Through a\ntightly-coupled iterative design process, we built Atria, a multi-view\nexecution graph visualization to support performance analysis. Atria simplifies\nthe initial representation of the execution graph by aggregating nodes as\nrelated to their line of code. We deployed Atria on multiple platforms, some\nrequiring design alteration. We describe how we adapted the design study\nmethodology to the \"moving target\" of both the data and the domain experts'\nconcerns and how this movement kept both the visualization and programming\nproject healthy. We reflect on our process and discuss what factors allow the\nproject to be successful in the presence of changing data and user needs.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:01:13 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 18:26:05 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 18:29:54 GMT"}, {"version": "v4", "created": "Tue, 15 Oct 2019 17:34:03 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Williams", "Katy", ""], ["Bigelow", "Alex", ""], ["Isaacs", "Katherine E.", ""]]}, {"id": "1905.13161", "submitter": "Ning Jiang", "authors": "Xin Zhang, Guanghua Xu, Aravind Ravi, Sarah Pearce, and Ning Jiang", "title": "Simultaneous induction of SSMVEP and SMR Using a Gaiting video stimulus:\n  a novel hybrid brain-computer interface", "comments": "22 pages, 7 figures and 2 tables", "journal-ref": "Journal of Neural Engineering, Mar. 2020", "doi": "10.1088/1741-2552/ab85b2", "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a novel visual stimulus for brain-computer interface. The\nstimulus is in the form gaiting sequence of a human. The hypothesis is that\nobserving such a visual stimulus would simultaneously induce 1) steady-state\nmotion visual evoked potential (SSMVEP) in the occipital area, similarly to an\nSSVEP stimulus; and 2) sensorimotor rhythm (SMR) in the primary sensorimotor\narea, because such action observation (AO) could activate the mirror neuron\nsystem. Canonical correlation analysis (CCA) was used to detect SSMVEP from\noccipital EEG, and event-related spectral perturbations (ERSP) were used to\nidentify SMR in the EEG from the sensorimotor area. The results showed that the\nproposed visual gaiting stimulus-induced SSMVEP, with classification accuracies\nof 88.9 $\\pm$ 12.0% in a four-class scenario. More importantly, it induced\nclear and sustained event-related desynchronization/synchronization (ERD/ERS)\nin the EEG from the sensorimotor area, while no ERD/ERS in the sensorimotor\narea could be observed when the other two SSVEP stimuli were used. Further, for\nparticipants with sufficiently clear SSMVEP pattern (classification accuracy >\n85%), the ERD index values in mu-beta band induced by the proposed gaiting\nstimulus were statistically different from that of the other two types of\nstimulus. Therefore, a novel BCI based on the proposed stimulus has potential\nin neurorehabilitation applications because it simultaneously has the high\naccuracy of an SSMVEP (~90% accuracy in a four-class setup) and the ability to\nactivate sensorimotor cortex. And such potential will be further explored in\nfuture studies.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:42:46 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Zhang", "Xin", ""], ["Xu", "Guanghua", ""], ["Ravi", "Aravind", ""], ["Pearce", "Sarah", ""], ["Jiang", "Ning", ""]]}, {"id": "1905.13172", "submitter": "Phillip Smith", "authors": "Phillip Smith, Anh Luong, Shamik Sarkar, Harsimran Singh, Neal\n  Patwari, Sneha Kasera, Kurt Derr and Samuel Ramirez", "title": "Sitara: Spectrum Measurement Goes Mobile Through Crowd-sourcing", "comments": "13 pages, 13 figures, 3 tables; For additional documentation and\n  source code refer to https://github.com/SPAN-UofU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software-defined radios (SDRs) are often used in the experimental evaluation\nof next-generation wireless technologies. While crowd-sourced spectrum\nmonitoring is an important component of future spectrum-agile technologies,\nthere is no clear way to test it in the real world, i.e., with hundreds of\nusers each with an SDR in their pocket participating in RF experiments\ncontrolled by, and data uploaded to, the cloud. Current fully functional SDRs\nare bulky, with components connected via wires, and last at most hours on a\nsingle battery charge. To address the needs of such experiments, we design and\ndevelop a compact, portable, untethered, and inexpensive SDR we call Sitara.\nOur SDR interfaces with a mobile device over Bluetooth 5 and can function\nstandalone or as a client to a central command and control server. The Sitara\noffers true portability: it operates up to one week on battery power, requires\nno external wired connections and occupies a footprint smaller than a credit\ncard. It transmits and receives common waveforms, uploads IQ samples or\nprocessed receiver data through a mobile device to a server for remote\nprocessing and performs spectrum sensing functions. Multiple Sitaras form a\ndistributed system capable of conducting experiments in wireless networking and\ncommunication in addition to RF monitoring and sensing activities. In this\npaper, we describe our design, evaluate our solution, present experimental\nresults from multi-sensor deployments and discuss the value of this system in\nfuture experimentation.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:56:29 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Smith", "Phillip", ""], ["Luong", "Anh", ""], ["Sarkar", "Shamik", ""], ["Singh", "Harsimran", ""], ["Patwari", "Neal", ""], ["Kasera", "Sneha", ""], ["Derr", "Kurt", ""], ["Ramirez", "Samuel", ""]]}, {"id": "1905.13313", "submitter": "Junwei Liang", "authors": "Junwei Liang and Jay D. Aronson and Alexander Hauptmann", "title": "Technical Report of the Video Event Reconstruction and Analysis (VERA)\n  System -- Shooter Localization, Models, Interface, and Beyond", "comments": "The code and models are available at\n  https://github.com/JunweiLiang/VERA_Shooter_Localization . Our system is live\n  at https://vera.cs.cmu.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every minute, hundreds of hours of video are uploaded to social media sites\nand the Internet from around the world. This material creates a visual record\nof the experiences of a significant percentage of humanity and can help\nilluminate how we live in the present moment. When properly analyzed, this\nvideo can also help analysts to reconstruct events of interest, including war\ncrimes, human rights violations, and terrorist acts. Machine learning and\ncomputer vision can play a crucial role in this process. In this technical\nreport, we describe the Video Event Reconstruction and Analysis (VERA) system.\nThis new tool brings together a variety of capabilities we have developed over\nthe past few years (including video synchronization and geolocation to order\nunstructured videos lacking metadata over time and space, and sound recognition\nalgorithms) to enable the reconstruction and analysis of events captured on\nvideo. Among other uses, VERA enables the localization of a shooter from just a\nfew videos that include the sound of gunshots. To demonstrate the efficacy of\nthis suite of tools, we present the results of estimating the shooter's\nlocation of the Las Vegas Shooting in 2017 and show that VERA accurately\npredicts the shooter's location using only the first few gunshots. We then\npoint out future directions that can help improve the system and further reduce\nunnecessary human labor in the process. All of the components of VERA run\nthrough a web interface that enables human-in-the-loop verification to ensure\naccurate estimations. All relevant source code, including the web interface and\nmachine learning models, is freely available on Github. We hope that\nresearchers and software developers will be inspired to improve and expand this\nsystem moving forward to better meet the needs of human rights and public\nsafety.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 17:55:50 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 21:12:12 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 16:04:11 GMT"}, {"version": "v4", "created": "Tue, 2 Jul 2019 06:15:49 GMT"}, {"version": "v5", "created": "Fri, 5 Jul 2019 05:23:13 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Liang", "Junwei", ""], ["Aronson", "Jay D.", ""], ["Hauptmann", "Alexander", ""]]}, {"id": "1905.13436", "submitter": "Yilun Xu", "authors": "Peng Cao, Yilun Xu, Yuqing Kong, Yizhou Wang", "title": "Max-MIG: an Information Theoretic Approach for Joint Learning from\n  Crowds", "comments": "Accepted by ICLR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eliciting labels from crowds is a potential way to obtain large labeled data.\nDespite a variety of methods developed for learning from crowds, a key\nchallenge remains unsolved: \\emph{learning from crowds without knowing the\ninformation structure among the crowds a priori, when some people of the crowds\nmake highly correlated mistakes and some of them label effortlessly (e.g.\nrandomly)}. We propose an information theoretic approach, Max-MIG, for joint\nlearning from crowds, with a common assumption: the crowdsourced labels and the\ndata are independent conditioning on the ground truth. Max-MIG simultaneously\naggregates the crowdsourced labels and learns an accurate data classifier.\nFurthermore, we devise an accurate data-crowds forecaster that employs both the\ndata and the crowdsourced labels to forecast the ground truth. To the best of\nour knowledge, this is the first algorithm that solves the aforementioned\nchallenge of learning from crowds. In addition to the theoretical validation,\nwe also empirically show that our algorithm achieves the new state-of-the-art\nresults in most settings, including the real-world data, and is the first\nalgorithm that is robust to various information structures. Codes are available\nat\n\\hyperlink{https://github.com/Newbeeer/Max-MIG}{https://github.com/Newbeeer/Max-MIG}\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 06:32:18 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Cao", "Peng", ""], ["Xu", "Yilun", ""], ["Kong", "Yuqing", ""], ["Wang", "Yizhou", ""]]}, {"id": "1905.13582", "submitter": "Artem Voronkov", "authors": "Artem Voronkov, Leonardo A. Martucci, Stefan Lindskog", "title": "System Administrators Prefer Command Line Interfaces, Don't They? An\n  Exploratory Study of Firewall Interfaces", "comments": "Preprint. To appear in the proceeding of the fifteenth Symposium on\n  Usable Privacy and Security (SOUPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graphical user interface (GUI) represents the most common option for\ninteracting with computer systems. However, according to the literature system\nadministrators often favor command line interfaces (CLIs). The goal of our work\nis to investigate which interfaces system administrators prefer, and which they\nactually utilize in their daily tasks. We collected experiences and opinions\nfrom 300 system administrators with the help of an online survey. All our\nrespondents are system administrators, who work or have worked with firewalls.\nOur results show that only 32% of the respondents prefer CLIs for managing\nfirewalls, while the corresponding figure is 60% for GUIs. We report the\nmentioned strengths and limitations of each interface and the tasks for which\nthey are utilized by the system administrators. Based on these results, we\nprovide design recommendations for firewall interfaces.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 12:44:51 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 13:32:19 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Voronkov", "Artem", ""], ["Martucci", "Leonardo A.", ""], ["Lindskog", "Stefan", ""]]}, {"id": "1905.13618", "submitter": "Roman Klinger", "authors": "Enrica Troiano and Sebastian Pad\\'o and Roman Klinger", "title": "Crowdsourcing and Validating Event-focused Emotion Corpora for German\n  and English", "comments": "14 pages, 1 figure, accepted for publication at ACL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis has a range of corpora available across multiple\nlanguages. For emotion analysis, the situation is more limited, which hinders\npotential research on cross-lingual modeling and the development of predictive\nmodels for other languages. In this paper, we fill this gap for German by\nconstructing deISEAR, a corpus designed in analogy to the well-established\nEnglish ISEAR emotion dataset. Motivated by Scherer's appraisal theory, we\nimplement a crowdsourcing experiment which consists of two steps. In step 1,\nparticipants create descriptions of emotional events for a given emotion. In\nstep 2, five annotators assess the emotion expressed by the texts. We show that\ntransferring an emotion classification model from the original English ISEAR to\nthe German crowdsourced deISEAR via machine translation does not, on average,\ncause a performance drop.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 13:54:54 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Troiano", "Enrica", ""], ["Pad\u00f3", "Sebastian", ""], ["Klinger", "Roman", ""]]}]