[{"id": "1712.00048", "submitter": "Navaneeth Kamballur Kottayil", "authors": "Navaneeth Kamballur Kottayil, Rositsa Bogdanova, Irene Cheng, Anup\n  Basu and Bin Zheng", "title": "Investigation of Gaze Patterns in Multi View Laparoscopic Surgery", "comments": null, "journal-ref": "38th Annual International Conference of the IEEE EMBC, Orlando,\n  FL, 2016, pp. 4031-4034", "doi": "10.1109/EMBC.2016.7591611", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Laparoscopic Surgery (LS) is a modern surgical technique whereby the surgery\nis performed through an incision with tools and camera as opposed to\nconventional open surgery. This promises minimal recovery times and less\nhemorrhaging. Multi view LS is the latest development in the field, where the\nsystem uses multiple cameras to give the surgeon more information about the\nsurgical site, potentially making the surgery easier. In this publication, we\nstudy the gaze patterns of a high performing subject in a multi-view LS\nenvironment and compare it with that of a novice to detect the differences\nbetween the gaze behavior. This was done by conducting a user study with 20\nuniversity students with varying levels of expertise in Multi-view LS. The\nsubjects performed an laparoscopic task in simulation with three cameras\n(front/top/side). The subjects were then separated as high and low performers\ndepending on the performance times and their data was analyzed. Our results\nshow statistically significant differences between the two behaviors. This\nopens up new areas from of training novices to Multi-view LS to making smart\ndisplays that guide your shows the optimum view depending on the situation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 19:45:56 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Kottayil", "Navaneeth Kamballur", ""], ["Bogdanova", "Rositsa", ""], ["Cheng", "Irene", ""], ["Basu", "Anup", ""], ["Zheng", "Bin", ""]]}, {"id": "1712.00088", "submitter": "J. Edward Swan II", "authors": "Gujot Singh, Stephen R. Ellis, and J. Edward Swan II", "title": "The Effect of Focal Distance, Age, and Brightness on Near-Field\n  Augmented Reality Depth Matching", "comments": "14 pages, 18 figures", "journal-ref": null, "doi": "10.1109/TVCG.2018.2869729", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many augmented reality (AR) applications operate within near-field reaching\ndistances, and require matching the depth of a virtual object with a real\nobject. The accuracy of this matching was measured in three experiments, which\nexamined the effect of focal distance, age, and brightness, within distances of\n33.3 to 50 cm, using a custom-built AR haploscope. Experiment I examined the\neffect of focal demand, at the levels of collimated (infinite focal distance),\nconsistent with other depth cues, and at the midpoint of reaching distance.\nObservers were too young to exhibit age-related reductions in accommodative\nability. The depth matches of collimated targets were increasingly\noverestimated with increasing distance, consistent targets were slightly\nunderestimated, and midpoint targets were accurately estimated. Experiment II\nreplicated Experiment I, with older observers. Results were similar to\nExperiment I. Experiment III replicated Experiment I with dimmer targets, using\nyoung observers. Results were again consistent with Experiment I, except that\nboth consistent and midpoint targets were accurately estimated. In all cases,\ncollimated results were explained by a model, where the collimation biases the\neyes' vergence angle outwards by a constant amount. Focal demand and brightness\naffect near-field AR depth matching, while age-related reductions in\naccommodative ability have no effect.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 21:24:22 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Singh", "Gujot", ""], ["Ellis", "Stephen R.", ""], ["Swan", "J. Edward", "II"]]}, {"id": "1712.00146", "submitter": "Zhu Wang", "authors": "Zhu Wang, Bin Guo, Zhiwen Yu, Xingshe Zhou", "title": "Wi-Fi CSI based Behavior Recognition: From Signals, Actions to\n  Activities", "comments": "10 pages, 3 figures, 3 tables, IEEE Communications Magazine, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human behavior recognition has been considered as a core technology that can\nfacilitate variety of applications. However, accurate detection and recognition\nof human behavior is still a big challenge that attracts a lot of research\nefforts. Recent advances in the wireless technology (e.g., Wi-Fi Channel State\nInformation, i.e., CSI) enable a new behavior recognition paradigm, which is\nable to recognize behaviors in a device-free and non-intrusive manner. In this\narticle, we first provide an overview of the basics of Wi-Fi CSI based behavior\nrecognition. Afterwards, we classify related applications into\nthree-granularity: signals, actions and activities, and then provide some\ninsights for designing new schemes. Finally, we conclude by discussing the\nchallenges, possible solutions to these challenges and some open issues\ninvolved in CSI based behavior recognition.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 01:29:28 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Wang", "Zhu", ""], ["Guo", "Bin", ""], ["Yu", "Zhiwen", ""], ["Zhou", "Xingshe", ""]]}, {"id": "1712.00216", "submitter": "Laixi Shi", "authors": "Yu Sang, Laixi Shi, Yimin Liu", "title": "Micro Hand Gesture Recognition System Using Ultrasonic Active Sensing", "comments": null, "journal-ref": "IEEE.Access. 6(2018)49339-49347", "doi": "10.1109/ACCESS.2018.2868268", "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a micro hand gesture recognition system and methods\nusing ultrasonic active sensing. This system uses micro dynamic hand gestures\nfor recognition to achieve human-computer interaction (HCI). The implemented\nsystem, called hand-ultrasonic gesture (HUG), consists of ultrasonic active\nsensing, pulsed radar signal processing, and time-sequence pattern recognition\nby machine learning. We adopt lower frequency (300 kHz) ultrasonic active\nsensing to obtain high resolution range-Doppler image features. Using high\nquality sequential range-Doppler features, we propose a state-transition-based\nhidden Markov model for gesture recognition. This method achieves a recognition\naccuracy of nearly 90\\% by using symbolized range-Doppler features and\nsignificantly reduces the computational complexity and power consumption.\nFurthermore, to achieve higher classification accuracy, we utilize an\nend-to-end neural network model and obtain a recognition accuracy of 96.32\\%.\nIn addition to offline analysis, a real-time prototype is released to verify\nour method's potential for application in the real world.\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 07:26:33 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 02:43:00 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Sang", "Yu", ""], ["Shi", "Laixi", ""], ["Liu", "Yimin", ""]]}, {"id": "1712.00334", "submitter": "Fabio Paolizzo", "authors": "Fabio Paolizzo", "title": "Enabling Embodied Analogies in Intelligent Music Systems", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present methodology is aimed at cross-modal machine learning and uses\nmultidisciplinary tools and methods drawn from a broad range of areas and\ndisciplines, including music, systematic musicology, dance, motion capture,\nhuman-computer interaction, computational linguistics and audio signal\nprocessing. Main tasks include: (1) adapting wisdom-of-the-crowd approaches to\nembodiment in music and dance performance to create a dataset of music and\nmusic lyrics that covers a variety of emotions, (2) applying\naudio/language-informed machine learning techniques to that dataset to identify\nautomatically the emotional content of the music and the lyrics, and (3)\nintegrating motion capture data from a Vicon system and dancers performing on\nthat music.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 08:27:08 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Paolizzo", "Fabio", ""]]}, {"id": "1712.00715", "submitter": "Austin Graham", "authors": "Austin Graham, Yan Liang, Le Gruenwald, Christan Grant", "title": "Formalizing Interruptible Algorithms for Human over-the-loop Analytics", "comments": "6 pages, 3 figures, Human-Machine Collaboration in Big Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional data mining algorithms are exceptional at seeing patterns in data\nthat humans cannot, but are often confused by details that are obvious to the\norganic eye. Algorithms that include humans \"in-the-loop\" have proved\nbeneficial for accuracy by allowing a user to provide direction in these\nsituations, but the slowness of human interactions causes execution times to\nincrease exponentially. Thus, we seek to formalize frameworks that include\nhumans \"over-the-loop\", giving the user an option to intervene when they deem\nit necessary while not having user feedback be an execution requirement. With\nthis strategy, we hope to increase the accuracy of solutions with minimal\nlosses in execution time. This paper describes our vision of this strategy and\nassociated problems.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 06:03:24 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Graham", "Austin", ""], ["Liang", "Yan", ""], ["Gruenwald", "Le", ""], ["Grant", "Christan", ""]]}, {"id": "1712.00750", "submitter": "Morad Behandish", "authors": "Morad Behandish and Horea T. Ilies", "title": "Haptic Assembly and Prototyping: An Expository Review", "comments": "Technical Report, University of Connecticut, 2016", "journal-ref": null, "doi": null, "report-no": "CDL-TR-16-04", "categories": "cs.HC cs.CG cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important application of haptic technology to digital product development\nis in virtual prototyping (VP), part of which deals with interactive planning,\nsimulation, and verification of assembly-related activities, collectively\ncalled virtual assembly (VA). In spite of numerous research and development\nefforts over the last two decades, the industrial adoption of haptic-assisted\nVP/VA has been slower than expected. Putting hardware limitations aside, the\nmain roadblocks faced in software development can be traced to the lack of\neffective and efficient computational models of haptic feedback. Such models\nmust 1) accommodate the inherent geometric complexities faced when assembling\nobjects of arbitrary shape; and 2) conform to the computation time limitation\nimposed by the notorious frame rate requirements---namely, 1 kHz for haptic\nfeedback compared to the more manageable 30-60 Hz for graphic rendering. The\nsimultaneous fulfillment of these competing objectives is far from trivial.\n  This survey presents some of the conceptual and computational challenges and\nopportunities as well as promising future directions in haptic-assisted VP/VA,\nwith a focus on haptic assembly from a geometric modeling and spatial reasoning\nperspective. The main focus is on revisiting definitions and classifications of\ndifferent methods used to handle the constrained multibody simulation in\nreal-time, ranging from physics-based and geometry-based to hybrid and unified\napproaches using a variety of auxiliary computational devices to specify,\nimpose, and solve assembly constraints. Particular attention is given to the\nnewly developed 'analytic methods' inherited from motion planning and protein\ndocking that have shown great promise as an alternative paradigm to the more\npopular combinatorial methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 10:57:05 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Behandish", "Morad", ""], ["Ilies", "Horea T.", ""]]}, {"id": "1712.01328", "submitter": "Rakshit Agrawal", "authors": "Rakshit Agrawal, Anwar Habeeb, Chih-Hsin Hsueh", "title": "Learning User Intent from Action Sequences on Interactive Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive systems have taken over the web and mobile space with increasing\nparticipation from users. Applications across every marketing domain can now be\naccessed through mobile or web where users can directly perform certain actions\nand reach a desired outcome. Actions of user on a system, though, can be\nrepresentative of a certain intent. Ability to learn this intent through user's\nactions can help draw certain insight into the behavior of users on a system.\n  In this paper, we present models to optimize interactive systems by learning\nand analyzing user intent through their actions on the system. We present a\nfour phased model that uses time-series of interaction actions sequentially\nusing a Long Short-Term Memory (LSTM) based sequence learning system that helps\nbuild a model for intent recognition. Our system then provides an objective\nspecific maximization followed by analysis and contrasting methods in order to\nidentify spaces of improvement in the interaction system. We discuss deployment\nscenarios for such a system and present results from evaluation on an online\nmarketplace using user clickstream data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 20:16:25 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Agrawal", "Rakshit", ""], ["Habeeb", "Anwar", ""], ["Hsueh", "Chih-Hsin", ""]]}, {"id": "1712.01856", "submitter": "Behzad Tabibian", "authors": "Behzad Tabibian, Utkarsh Upadhyay, Abir De, Ali Zarezade, Bernhard\n  Schoelkopf, Manuel Gomez-Rodriguez", "title": "Optimizing Human Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spaced repetition is a technique for efficient memorization which uses\nrepeated, spaced review of content to improve long-term retention. Can we find\nthe optimal reviewing schedule to maximize the benefits of spaced repetition?\nIn this paper, we introduce a novel, flexible representation of spaced\nrepetition using the framework of marked temporal point processes and then\naddress the above question as an optimal control problem for stochastic\ndifferential equations with jumps. For two well-known human memory models, we\nshow that the optimal reviewing schedule is given by the recall probability of\nthe content to be learned. As a result, we can then develop a simple, scalable\nonline algorithm, Memorize, to sample the optimal reviewing times. Experiments\non both synthetic and real data gathered from Duolingo, a popular\nlanguage-learning online platform, show that our algorithm may be able to help\nlearners memorize more effectively than alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 19:00:11 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 19:32:38 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Tabibian", "Behzad", ""], ["Upadhyay", "Utkarsh", ""], ["De", "Abir", ""], ["Zarezade", "Ali", ""], ["Schoelkopf", "Bernhard", ""], ["Gomez-Rodriguez", "Manuel", ""]]}, {"id": "1712.02007", "submitter": "Qiyu Zhi", "authors": "Ronald Metoyer, Qiyu Zhi, Bart Janczuk, Walter Scheirer", "title": "Coupling Story to Visualization: Using Textual Analysis as a Bridge\n  Between Data and Interpretation", "comments": "ACM IUI'18, 3 figures, 5 pages", "journal-ref": null, "doi": "10.1145/3172944.3173007", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online writers and journalism media are increasingly combining visualization\n(and other multimedia content) with narrative text to create narrative\nvisualizations. Often, however, the two elements are presented independently of\none another. We propose an approach to automatically integrate text and\nvisualization elements. We begin with a writer's narrative that presumably can\nbe supported with visual data evidence. We leverage natural language\nprocessing, quantitative narrative analysis, and information visualization to\n(1) automatically extract narrative components (who, what, when, where) from\ndata-rich stories, and (2) integrate the supporting data evidence with the text\nto develop a narrative visualization. We also employ bidirectional interaction\nfrom text to visualization and visualization to text to support reader\nexploration in both directions. We demonstrate the approach with a case study\nin the data-rich field of sports journalism.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 02:12:21 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 17:39:15 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Metoyer", "Ronald", ""], ["Zhi", "Qiyu", ""], ["Janczuk", "Bart", ""], ["Scheirer", "Walter", ""]]}, {"id": "1712.02033", "submitter": "Sushant Kafle", "authors": "Sushant Kafle and Matt Huenerfauth", "title": "Evaluating the Usability of Automatically Generated Captions for People\n  who are Deaf or Hard of Hearing", "comments": "10 pages, 8 figures, published in ACM SIGACCESS Conference on\n  Computers and Accessibility (ASSETS '17)", "journal-ref": "ASSETS'17 (2017) 165-174", "doi": "10.1145/3132525.3132542", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of Automated Speech Recognition (ASR) technology has improved,\nbut it is still imperfect in many settings. Researchers who evaluate ASR\nperformance often focus on improving the Word Error Rate (WER) metric, but WER\nhas been found to have little correlation with human-subject performance on\nmany applications. We propose a new captioning-focused evaluation metric that\nbetter predicts the impact of ASR recognition errors on the usability of\nautomatically generated captions for people who are Deaf or Hard of Hearing\n(DHH). Through a user study with 30 DHH users, we compared our new metric with\nthe traditional WER metric on a caption usability evaluation task. In a\nside-by-side comparison of pairs of ASR text output (with identical WER), the\ntexts preferred by our new metric were preferred by DHH participants. Further,\nour metric had significantly higher correlation with DHH participants'\nsubjective scores on the usability of a caption, as compared to the correlation\nbetween WER metric and participant subjective scores. This new metric could be\nused to select ASR systems for captioning applications, and it may be a better\nmetric for ASR researchers to consider when optimizing ASR systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 04:28:46 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Kafle", "Sushant", ""], ["Huenerfauth", "Matt", ""]]}, {"id": "1712.02421", "submitter": "S\\'everin Lemaignan", "authors": "S\\'everin Lemaignan, Charlotte Edmunds, Emmanuel Senft, Tony Belpaeme", "title": "The Free-play Sandbox: a Methodology for the Evaluation of Social\n  Robotics and a Dataset of Social Interactions", "comments": "conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating human-robot social interactions in a rigorous manner is\nnotoriously difficult: studies are either conducted in labs with constrained\nprotocols to allow for robust measurements and a degree of replicability, but\nat the cost of ecological validity; or in the wild, which leads to superior\nexperimental realism, but often with limited replicability and at the expense\nof rigorous interaction metrics.\n  We introduce a novel interaction paradigm, designed to elicit rich and varied\nsocial interactions while having desirable scientific properties\n(replicability, clear metrics, possibility of either autonomous or Wizard-of-Oz\nrobot behaviours). This paradigm focuses on child-robot interactions, and\nbuilds on a sandboxed free-play environment. We present the rationale and\ndesign of the interaction paradigm, its methodological and technical aspects\n(including the open-source implementation of the software platform), as well as\ntwo large open datasets acquired with this paradigm, and meant to act as\nexperimental baselines for future research.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 22:15:04 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Lemaignan", "S\u00e9verin", ""], ["Edmunds", "Charlotte", ""], ["Senft", "Emmanuel", ""], ["Belpaeme", "Tony", ""]]}, {"id": "1712.02548", "submitter": "Yunlong Wang", "authors": "Yunlong Wang, Ahmed Fadhil, Jan-Philipp Lange, Harald Reiterer", "title": "Towards a Holistic Approach to Designing Theory-based Mobile Health\n  Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing evidence has shown that theory-based health behavior change\ninterventions are more effective than non-theory-based ones. However, only a\nfew segments of relevant studies were theory-based, especially the studies\nconducted by non-psychology researchers. On the other hand, many mobile health\ninterventions, even those based on the behavioral theories, may still fail in\nthe absence of a user-centered design process. The gap between behavioral\ntheories and user-centered design increases the difficulty of designing and\nimplementing mobile health interventions. To bridge this gap, we propose a\nholistic approach to designing theory-based mobile health interventions built\non the existing theories and frameworks of three categories: (1) behavioral\ntheories (e.g., the Social Cognitive Theory, the Theory of Planned Behavior,\nand the Health Action Process Approach), (2) the technological models and\nframeworks (e.g., the Behavior Change Techniques, the Persuasive System Design\nand Behavior Change Support System, and the Just-in-Time Adaptive\nInterventions), and (3) the user-centered systematic approaches (e.g., the\nCeHRes Roadmap, the Wendel's Approach, and the IDEAS Model). This holistic\napproach provides researchers a lens to see the whole picture for developing\nmobile health interventions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 09:29:44 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Wang", "Yunlong", ""], ["Fadhil", "Ahmed", ""], ["Lange", "Jan-Philipp", ""], ["Reiterer", "Harald", ""]]}, {"id": "1712.02881", "submitter": "Hojjat Abdollahi", "authors": "Hojjat Abdollahi, Ali Mollahosseini, Josh T. Lane, Mohammad H. Mahoor", "title": "A Pilot Study on Using an Intelligent Life-like Robot as a Companion for\n  Elderly Individuals with Dementia and Depression", "comments": "Published in 2017 IEEE-RAS International Conference on Humanoid\n  Robots", "journal-ref": null, "doi": "10.1109/HUMANOIDS.2017.8246925", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design, development, methodology, and the results of\na pilot study on using an intelligent, emotive and perceptive social robot (aka\nCompanionbot) for improving the quality of life of elderly people with dementia\nand/or depression. Ryan Companionbot prototyped in this project, is a\nrear-projected life-like conversational robot. Ryan is equipped with features\nthat can (1) interpret and respond to users' emotions through facial\nexpressions and spoken language, (2) proactively engage in conversations with\nusers, and (3) remind them about their daily life schedules (e.g. taking their\nmedicine on time). Ryan engages users in cognitive games and reminiscence\nactivities. We conducted a pilot study with six elderly individuals with\nmoderate dementia and/or depression living in a senior living facility in\nDenver. Each individual had 24/7 access to a Ryan in his/her room for a period\nof 4-6 weeks. Our observations of these individuals, interviews with them and\ntheir caregivers, and analyses of their interactions during this period\nrevealed that they established rapport with the robot and greatly valued and\nenjoyed having a Companionbot in their room.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 22:51:11 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Abdollahi", "Hojjat", ""], ["Mollahosseini", "Ali", ""], ["Lane", "Josh T.", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "1712.02926", "submitter": "Yuan Yuan", "authors": "Yuan Yuan, Tracy Xiao Liu, Chenhao Tan, Jie Tang", "title": "Online Red Packets: A Large-scale Empirical Study of Gift Giving on\n  WeChat", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC cs.MM econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gift giving is a ubiquitous social phenomenon, and red packets have been used\nas monetary gifts in Asian countries for thousands of years. In recent years,\nonline red packets have become widespread in China through the WeChat platform.\nExploiting a unique dataset consisting of 61 million group red packets and\nseven million users, we conduct a large-scale, data-driven study to understand\nthe spread of red packets and the effect of red packets on group activity. We\nfind that the cash flows between provinces are largely consistent with\nprovincial GDP rankings, e.g., red packets are sent from users in the south to\nthose in the north. By distinguishing spontaneous from reciprocal red packets,\nwe reveal the behavioral patterns in sending red packets: males, seniors, and\npeople with more in-group friends are more inclined to spontaneously send red\npackets, while red packets from females, youths, and people with less in-group\nfriends are more reciprocal. Furthermore, we use propensity score matching to\nstudy the external effects of red packets on group dynamics. We show that red\npackets increase group participation and strengthen in-group relationships,\nwhich partly explain the benefits and motivations for sending red packets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 03:15:58 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Yuan", "Yuan", ""], ["Liu", "Tracy Xiao", ""], ["Tan", "Chenhao", ""], ["Tang", "Jie", ""]]}, {"id": "1712.03012", "submitter": "Claudio Pinhanez", "authors": "Claudio Pinhanez", "title": "Computer Interfaces to Organizations: Perspectives on Borg-Human\n  Interaction Design", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We use the term borg to refer to the complex organizations composed of\npeople, machines, and processes with which users frequently interact using\ncomputer interfaces and websites. Unlike interfaces to pure machines, we\ncontend that borg-human interaction (BHI) happens in a context combining the\nanthropomorphization of the interface, conflict with users, and dramatization\nof the interaction process. We believe this context requires designers to\nconstruct the human facet of the borg, a structure encompassing the borg's\npersonality, social behavior, and embodied actions; and the strategies to\nco-create dramatic narratives with the user. To design the human facet of a\nborg, different concepts and models are explored and discussed, borrowing ideas\nfrom psychology, sociology, and arts. Based on those foundations, we propose\nsix design methodologies to complement traditional computer-human interface\ndesign techniques, including play-and-freeze enactment of conflicts and the use\nof giant puppets as interface prototypes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 10:37:25 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Pinhanez", "Claudio", ""]]}, {"id": "1712.03121", "submitter": "Muhammad Jameel Malik", "authors": "Jameel Malik, Ahmed Elhayek, Didier Stricker", "title": "Simultaneous Hand Pose and Skeleton Bone-Lengths Estimation from a\n  Single Depth Image", "comments": "This paper has been accepted and presented in 3DV-2017 conference\n  held at Qingdao, China. http://irc.cs.sdu.edu.cn/3dv/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Articulated hand pose estimation is a challenging task for human-computer\ninteraction. The state-of-the-art hand pose estimation algorithms work only\nwith one or a few subjects for which they have been calibrated or trained.\nParticularly, the hybrid methods based on learning followed by model fitting or\nmodel based deep learning do not explicitly consider varying hand shapes and\nsizes. In this work, we introduce a novel hybrid algorithm for estimating the\n3D hand pose as well as bone-lengths of the hand skeleton at the same time,\nfrom a single depth image. The proposed CNN architecture learns hand pose\nparameters and scale parameters associated with the bone-lengths\nsimultaneously. Subsequently, a new hybrid forward kinematics layer employs\nboth parameters to estimate 3D joint positions of the hand. For end-to-end\ntraining, we combine three public datasets NYU, ICVL and MSRA-2015 in one\nunified format to achieve large variation in hand shapes and sizes. Among\nhybrid methods, our method shows improved accuracy over the state-of-the-art on\nthe combined dataset and the ICVL dataset that contain multiple subjects. Also,\nour algorithm is demonstrated to work well with unseen images.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 15:25:00 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Malik", "Jameel", ""], ["Elhayek", "Ahmed", ""], ["Stricker", "Didier", ""]]}, {"id": "1712.03163", "submitter": "Valdemar \\v{S}v\\'abensk\\'y", "authors": "Valdemar \\v{S}v\\'abensk\\'y and Jan Vykopal", "title": "Challenges Arising from Prerequisite Testing in Cybersecurity Games", "comments": "ACM SIGCSE 2018 conference, 6 pages, 4 figures, 4 tables", "journal-ref": null, "doi": "10.1145/3159450.3159454", "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybersecurity games are an attractive and popular method of active learning.\nHowever, the majority of current games are created for advanced players, which\noften leads to frustration in less experienced learners. Therefore, we decided\nto focus on a diagnostic assessment of participants entering the games. We\nassume that information about the players' knowledge, skills, and experience\nenables tutors or learning environments to suitably assist participants with\ngame challenges and maximize learning in their virtual adventure. In this\npaper, we present a pioneering experiment examining the predictive value of a\nshort quiz and self-assessment for identifying learners' readiness before\nplaying a cybersecurity game. We hypothesized that these predictors would model\nplayers' performance. A linear regression analysis showed that the game\nperformance can be accurately predicted by well-designed prerequisite testing,\nbut not by self-assessment. At the same time, we identified major challenges\nrelated to the design of pretests for cybersecurity games: calibrating test\nquestions with respect to the skills relevant for the game, minimizing the\nquiz's length while maximizing its informative value, and embedding the pretest\nin the game. Our results are relevant for educational researchers and\ncybersecurity instructors of students at all learning levels.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 16:36:02 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["\u0160v\u00e1bensk\u00fd", "Valdemar", ""], ["Vykopal", "Jan", ""]]}, {"id": "1712.03329", "submitter": "Abu Zohran Qaiser", "authors": "Abu Zohran Qaiser, Muhammad Taha Khan", "title": "Adaptive Interface for Accommodating Colour-Blind Users by Using\n  Ishihara Test", "comments": "3 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Imperative visual data frequently vanishes when color applications are seen\nby partially color blind users. A new method for adaptive interface for\naccommodating color blind users is presented. The method presented here has two\nsections: 1) test client perceivability by utilizing Ishihara plates. 2) change\nthe interface color scheme to accommodate color blind users if necessary. We\ndemonstrate how the method works via a simple interface and evaluate the\nefficiency of our method by experimenting it on 100 users.\n", "versions": [{"version": "v1", "created": "Sat, 9 Dec 2017 01:47:29 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Qaiser", "Abu Zohran", ""], ["Khan", "Muhammad Taha", ""]]}, {"id": "1712.03579", "submitter": "Mohi Reza", "authors": "Mohi Reza, Warida Rashid, Moin Mostakim", "title": "Prodorshok I: A Bengali Isolated Speech Dataset for Voice-Based\n  Assistive Technologies - A comparative analysis of the effects of data\n  augmentation on HMM-GMM and DNN classifiers", "comments": "4 pages, accepted for oral presentation at the 5th IEEE R10 HTC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prodorshok I is a Bengali isolated word dataset tailored to help create\nspeaker-independent, voice-command driven automated speech recognition (ASR)\nbased assistive technologies to help improve human-computer interaction (HCI).\nThis paper presents the results of an objective analysis that was undertaken\nusing a subset of words from Prodorshok I to assess its reliability in ASR\nsystems that utilize Hidden Markov Models (HMM) with Gaussian emissions and\nDeep Neural Networks (DNN). The results show that simple data augmentation\ninvolving a small pitch shift can make surprisingly tangible improvements to\naccuracy levels in speech recognition.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 19:52:51 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Reza", "Mohi", ""], ["Rashid", "Warida", ""], ["Mostakim", "Moin", ""]]}, {"id": "1712.03585", "submitter": "Daniyal Shahrokhian", "authors": "Daniyal Shahrokhian, Alejandro Vera de Juan", "title": "SneakPeek: Interest Mining of Images based on User Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, eye tracking is the most used technology to detect areas of\ninterest. This kind of technology requires specialized equipment recording\nuser's eyes. In this paper, we propose SneakPeek, a different approach to\ndetect areas of interest on images displayed in web pages based on the zooming\nand panning actions of the users through the image. We have validated our\nproposed solution with a group of test subjects that have performed a test in\nour on-line prototype. Being this the first iteration of the algorithm, we have\nfound both good and bad results, depending on the type of image. In specific,\nSneakPeek works best with medium/big objects in medium/big sized images. The\nreason behind it is the limitation on detection when smartphone screens keep\ngetting bigger and bigger. SneakPeek can be adapted to any website by simply\nadapting the controller interface for the specific case.\n", "versions": [{"version": "v1", "created": "Sun, 10 Dec 2017 20:37:19 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Shahrokhian", "Daniyal", ""], ["de Juan", "Alejandro Vera", ""]]}, {"id": "1712.03622", "submitter": "George Philipp", "authors": "George Philipp, Ryen W. White", "title": "Interactions between Health Searchers and Search Engines", "comments": "SIGIR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web is an important resource for understanding and diagnosing medical\nconditions. Based on exposure to online content, people may develop undue\nhealth concerns, believing that common and benign symptoms are explained by\nserious illnesses. In this paper, we investigate potential strategies to mine\nqueries and searcher histories for clues that could help search engines choose\nthe most appropriate information to present in response to exploratory medical\nqueries. To do this, we performed a longitudinal study of health search\nbehavior using the logs of a popular search engine. We found that query\nvariations which might appear innocuous (e.g. \"bad headache\" vs \"severe\nheadache\") may hold valuable information about the searcher which could be used\nby search engines to improve performance. Furthermore, we investigated how\nmedically concerned users respond differently to search engine result pages\n(SERPs) and find that their disposition for clicking on concerning pages is\npronounced, potentially leading to a self-reinforcement of concern. Finally, we\nstudied to which degree variations in the SERP impact future search and\nreal-world health-seeking behavior and obtained some surprising results (e.g.,\nviewing concerning pages may lead to a short-term reduction of real-world\nhealth seeking).\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 01:44:54 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Philipp", "George", ""], ["White", "Ryen W.", ""]]}, {"id": "1712.03650", "submitter": "Samira Samadi", "authors": "Samira Samadi, Santosh Vempala, Adam Tauman Kalai", "title": "Usability of Humanly Computable Passwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reusing passwords across multiple websites is a common practice that\ncompromises security. Recently, Blum and Vempala have proposed password\nstrategies to help people calculate, in their heads, passwords for different\nsites without dependence on third-party tools or external devices. Thus far,\nthe security and efficiency of these \"mental algorithms\" has been analyzed only\ntheoretically. But are such methods usable? We present the first usability\nstudy of humanly computable password strategies, involving a learning phase (to\nlearn a password strategy), then a rehearsal phase (to login to a few\nwebsites), and multiple follow-up tests. In our user study, with training,\nparticipants were able to calculate a deterministic eight-character password\nfor an arbitrary new website in under 20 seconds.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 05:52:47 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 21:59:00 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Samadi", "Samira", ""], ["Vempala", "Santosh", ""], ["Kalai", "Adam Tauman", ""]]}, {"id": "1712.03724", "submitter": "Sarthak Ahuja", "authors": "Rakesh R Pimplikar, Kushal Mukherjee, Gyana Parija, Harit Vishwakarma,\n  Ramasuri Narayanam, Sarthak Ahuja, Rohith D Vallam, Ritwik Chaudhuri, Joydeep\n  Mondal", "title": "Cogniculture: Towards a Better Human-Machine Co-evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in Artificial Intelligence is breaking technology barriers every\nday. New algorithms and high performance computing are making things possible\nwhich we could only have imagined earlier. Though the enhancements in AI are\nmaking life easier for human beings day by day, there is constant fear that AI\nbased systems will pose a threat to humanity. People in AI community have\ndiverse set of opinions regarding the pros and cons of AI mimicking human\nbehavior. Instead of worrying about AI advancements, we propose a novel idea of\ncognitive agents, including both human and machines, living together in a\ncomplex adaptive ecosystem, collaborating on human computation for producing\nessential social goods while promoting sustenance, survival and evolution of\nthe agents' life cycle. We highlight several research challenges and technology\nbarriers in achieving this goal. We propose a governance mechanism around this\necosystem to ensure ethical behaviors of all cognitive agents. Along with a\nnovel set of use-cases of Cogniculture, we discuss the road map ahead for this\njourney.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 11:31:28 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Pimplikar", "Rakesh R", ""], ["Mukherjee", "Kushal", ""], ["Parija", "Gyana", ""], ["Vishwakarma", "Harit", ""], ["Narayanam", "Ramasuri", ""], ["Ahuja", "Sarthak", ""], ["Vallam", "Rohith D", ""], ["Chaudhuri", "Ritwik", ""], ["Mondal", "Joydeep", ""]]}, {"id": "1712.04202", "submitter": "Adam Agocs", "authors": "Adam Agocs, Dimitrios Dardanis, Jean-Marie Le Goff and Dimitrios\n  Proios", "title": "Interactive graph query language for multidimensional data in\n  Collaboration Spotting visual analytics framework", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human reasoning in visual analytics of data networks relies mainly on the\nquality of visual perception and the capability of interactively exploring the\ndata from different facets. Visual quality strongly depends on networks' size\nand dimensional complexity while network exploration capability on the\nintuitiveness and expressiveness of user frontends. The approach taken in this\npaper aims at addressing the above by decomposing data networks into multiple\nnetworks of smaller dimensions and building an interactive graph query language\nthat supports full navigation across the sub-networks. Within sub-networks of\nreduced dimensionality, structural abstraction and semantic techniques can then\nbe used to enhance visual perception further.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 10:12:43 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Agocs", "Adam", ""], ["Dardanis", "Dimitrios", ""], ["Goff", "Jean-Marie Le", ""], ["Proios", "Dimitrios", ""]]}, {"id": "1712.04260", "submitter": "Krzysztof Czuszynski", "authors": "Krzysztof Czuszynski, Jacek Ruminski, Jerzy Wtorek", "title": "The passive operating mode of the linear optical gesture sensor", "comments": "10 pages, 14 figures", "journal-ref": "Adv. Electr. Comput. Eng.18 1 (2018) 145-156", "doi": "10.4316/AECE.2018.01018", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study evaluates the influence of natural light conditions on the\neffectiveness of the linear optical gesture sensor, working in the presence of\nambient light only (passive mode). The orientations of the device in reference\nto the light source were modified in order to verify the sensitivity of the\nsensor. A criterion for the differentiation between two states: \"possible\ngesture\" and \"no gesture\" was proposed. Additionally, different light\nconditions and possible features were investigated, relevant for the decision\nof switching between the passive and active modes of the device. The criterion\nwas evaluated based on the specificity and sensitivity analysis of the binary\nambient light condition classifier. The elaborated classifier predicts ambient\nlight conditions with the accuracy of 85.15%. Understanding the light\nconditions, the hand pose can be detected. The achieved accuracy of the hand\nposes classifier trained on the data obtained in the passive mode in favorable\nlight conditions was 98.76%. It was also shown that the passive operating mode\nof the linear gesture sensor reduces the total energy consumption by 93.34%,\nresulting in 0.132 mA. It was concluded that optical linear sensor could be\nefficiently used in various lighting conditions.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 12:23:12 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Czuszynski", "Krzysztof", ""], ["Ruminski", "Jacek", ""], ["Wtorek", "Jerzy", ""]]}, {"id": "1712.04314", "submitter": "Yuri G. Gordienko", "authors": "Serhii Hamotskyi, Sergii Stirenko, Yuri Gordienko, Anis Rojbi", "title": "Generating and Estimating Nonverbal Alphabets for Situated and\n  Multimodal Communications", "comments": "5 pages, 5 figures", "journal-ref": "International Journal of Systems Applications Engineering and\n  Development, 11, 232-236 (2017)", "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the formalized approach for generating and\nestimating symbols (and alphabets), which can be communicated by the wide range\nof non-verbal means based on specific user requirements (medium, priorities,\ntype of information that needs to be conveyed). The short characterization of\nbasic terms and parameters of such symbols (and alphabets) with approaches to\ngenerate them are given. Then the framework, experimental setup, and some\nmachine learning methods to estimate usefulness and effectiveness of the\nnonverbal alphabets and systems are presented. The previous results demonstrate\nthat usage of multimodal data sources (like wearable accelerometer, heart\nmonitor, muscle movements sensors, braincomputer interface) along with machine\nlearning approaches can provide the deeper understanding of the usefulness and\neffectiveness of such alphabets and systems for nonverbal and situated\ncommunication. The symbols (and alphabets) generated and estimated by such\nmethods may be useful in various applications: from synthetic languages and\nconstructed scripts to multimodal nonverbal and situated interaction between\npeople and artificial intelligence systems through Human-Computer Interfaces,\nsuch as mouse gestures, touchpads, body gestures, eyetracking cameras,\nwearables, and brain-computing interfaces, especially in applications for\nelderly care and people with disabilities.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 14:38:34 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Hamotskyi", "Serhii", ""], ["Stirenko", "Sergii", ""], ["Gordienko", "Yuri", ""], ["Rojbi", "Anis", ""]]}, {"id": "1712.04652", "submitter": "Maria Spichkova", "authors": "Alber J. Christianto, Peng Chen, Osheen Walawedura, Annie Vuong, Jun\n  Feng, Dong Wang, Maria Spichkova", "title": "Software Engineering Solutions To Support Vertical Transportation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the core results of the project on visualisation\nand analysis of data collected from the vertical transport facilities. The aim\nof the project was to provide better user experience as well as to help\nbuilding maintenance staff to increase productivity of their work. We\nelaborated a web-based system for vertical transportation, to cover the needs\nof (1) staff working on building maintenance, (2) people who are regularly\nusing the facilities in the corresponding buildings.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 08:19:08 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Christianto", "Alber J.", ""], ["Chen", "Peng", ""], ["Walawedura", "Osheen", ""], ["Vuong", "Annie", ""], ["Feng", "Jun", ""], ["Wang", "Dong", ""], ["Spichkova", "Maria", ""]]}, {"id": "1712.04753", "submitter": "Karttikeya Mangalam", "authors": "Karttikeya Mangalam, Tanaya Guha", "title": "Learning Spontaneity to Improve Emotion Recognition In Speech", "comments": "Accepted at Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CL cs.HC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the effect and usefulness of spontaneity (i.e. whether a given\nspeech is spontaneous or not) in speech in the context of emotion recognition.\nWe hypothesize that emotional content in speech is interrelated with its\nspontaneity, and use spontaneity classification as an auxiliary task to the\nproblem of emotion recognition. We propose two supervised learning settings\nthat utilize spontaneity to improve speech emotion recognition: a hierarchical\nmodel that performs spontaneity detection before performing emotion\nrecognition, and a multitask learning model that jointly learns to recognize\nboth spontaneity and emotion. Through various experiments on the well known\nIEMOCAP database, we show that by using spontaneity detection as an additional\ntask, significant improvement can be achieved over emotion recognition systems\nthat are unaware of spontaneity. We achieve state-of-the-art emotion\nrecognition accuracy (4-class, 69.1%) on the IEMOCAP database outperforming\nseveral relevant and competitive baselines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 14:30:27 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 01:15:38 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 18:58:29 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Mangalam", "Karttikeya", ""], ["Guha", "Tanaya", ""]]}, {"id": "1712.04787", "submitter": "Ingmar Steiner", "authors": "Ingmar Steiner, S\\'ebastien Le Maguer", "title": "Creating New Language and Voice Components for the Updated MaryTTS\n  Text-to-Speech Synthesis Platform", "comments": null, "journal-ref": "Proc. LREC 11 (2018) 3171-3175", "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a new workflow to create components for the MaryTTS text-to-speech\nsynthesis platform, which is popular with researchers and developers, extending\nit to support new languages and custom synthetic voices. This workflow replaces\nthe previous toolkit with an efficient, flexible process that leverages modern\nbuild automation and cloud-hosted infrastructure. Moreover, it is compatible\nwith the updated MaryTTS architecture, enabling new features and\nstate-of-the-art paradigms such as synthesis based on deep neural networks\n(DNNs). Like MaryTTS itself, the new tools are free, open source software\n(FOSS), and promote the use of open data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 14:29:09 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 09:23:11 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Steiner", "Ingmar", ""], ["Maguer", "S\u00e9bastien Le", ""]]}, {"id": "1712.04798", "submitter": "Ingmar Steiner", "authors": "Arif Khan, Ingmar Steiner, Yusuke Sugano, Andreas Bulling, Ross\n  Macdonald", "title": "A Multimodal Corpus of Expert Gaze and Behavior during Phonetic\n  Segmentation Tasks", "comments": null, "journal-ref": "Proc. LREC 11 (2018) 4277-4281", "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Phonetic segmentation is the process of splitting speech into distinct\nphonetic units. Human experts routinely perform this task manually by analyzing\nauditory and visual cues using analysis software, which is an extremely\ntime-consuming process. Methods exist for automatic segmentation, but these are\nnot always accurate enough. In order to improve automatic segmentation, we need\nto model it as close to the manual segmentation as possible. This corpus is an\neffort to capture the human segmentation behavior by recording experts\nperforming a segmentation task. We believe that this data will enable us to\nhighlight the important aspects of manual segmentation, which can be used in\nautomatic segmentation to improve its accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 14:45:29 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 14:57:09 GMT"}, {"version": "v3", "created": "Fri, 11 May 2018 07:22:56 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Khan", "Arif", ""], ["Steiner", "Ingmar", ""], ["Sugano", "Yusuke", ""], ["Bulling", "Andreas", ""], ["Macdonald", "Ross", ""]]}, {"id": "1712.05570", "submitter": "Hongying Meng Dr", "authors": "Yi Liu, Hongying Meng, Mohammad Rafiq Swash, Yona Falinie A. Gaus, Rui\n  Qin", "title": "Holoscopic 3D Micro-Gesture Database for Wearable Device Interaction", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of augmented reality (AR) and virtual reality (VR)\ntechnology, human-computer interaction (HCI) has been greatly improved for\ngaming interaction of AR and VR control. The finger micro-gesture is one of the\nimportant interactive methods for HCI applications such as in the Google Soli\nand Microsoft Kinect projects. However, the progress in this research is slow\ndue to the lack of high quality public available database. In this paper,\nholoscopic 3D camera is used to capture high quality micro-gesture images and a\nnew unique holoscopic 3D micro-gesture (HoMG) database is produced. The\nprinciple of the holoscopic 3D camera is based on the fly viewing system to see\nthe objects. HoMG database recorded the image sequence of 3 conventional\ngestures from 40 participants under different settings and conditions. For the\npurpose of micro-gesture recognition, HoMG has a video subset with 960 videos\nand a still image subset with 30635 images. Initial micro-gesture recognition\non both subsets has been conducted using traditional 2D image and video\nfeatures and popular classifiers and some encouraging performance has been\nachieved. The database will be available for the research communities and speed\nup the research in this area.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 07:49:04 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 16:15:44 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Liu", "Yi", ""], ["Meng", "Hongying", ""], ["Swash", "Mohammad Rafiq", ""], ["Gaus", "Yona Falinie A.", ""], ["Qin", "Rui", ""]]}, {"id": "1712.05796", "submitter": "Chris Callison-Burch", "authors": "Kotaro Hara, Abi Adams, Kristy Milland, Saiph Savage, Chris\n  Callison-Burch, Jeffrey Bigham", "title": "A Data-Driven Analysis of Workers' Earnings on Amazon Mechanical Turk", "comments": "Conditionally accepted for inclusion in the 2018 ACM Conference on\n  Human Factors in Computing Systems (CHI'18) Papers program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of people are working as part of on-line crowd work, which\nhas been characterized by its low wages; yet, we know little about wage\ndistribution and causes of low/high earnings. We recorded 2,676 workers\nperforming 3.8 million tasks on Amazon Mechanical Turk. Our task-level analysis\nrevealed that workers earned a median hourly wage of only ~\\$2/h, and only 4%\nearned more than \\$7.25/h. The average requester pays more than \\$11/h,\nalthough lower-paying requesters post much more work. Our wage calculations are\ninfluenced by how unpaid work is included in our wage calculations, e.g., time\nspent searching for tasks, working on tasks that are rejected, and working on\ntasks that are ultimately not submitted. We further explore the characteristics\nof tasks and working patterns that yield higher hourly wages. Our analysis\ninforms future platform design and worker tools to create a more positive\nfuture for crowd work.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 22:08:23 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 16:40:11 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Hara", "Kotaro", ""], ["Adams", "Abi", ""], ["Milland", "Kristy", ""], ["Savage", "Saiph", ""], ["Callison-Burch", "Chris", ""], ["Bigham", "Jeffrey", ""]]}, {"id": "1712.05944", "submitter": "Katar\\'ina Furmanov\\'a", "authors": "Katarina Furmanova, Samuel Gratzl, Holger Stitz, Thomas Zichner,\n  Miroslava Jaresova, Alexander Lex, Marc Streit", "title": "Taggle: Combining Overview and Details in Tabular Data Visualizations", "comments": null, "journal-ref": null, "doi": "10.1177/1473871619878085", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most tabular data visualization techniques focus on overviews, yet many\npractical analysis tasks are concerned with investigating individual items of\ninterest. At the same time, relating an item to the rest of a potentially large\ntable is important. In this work we present Taggle, a tabular visualization\ntechnique for exploring and presenting large and complex tables. Taggle takes\nan item-centric, spreadsheet-like approach, visualizing each row in the source\ndata individually using visual encodings for the cells. At the same time,\nTaggle introduces data-driven aggregation of data subsets. The aggregation\nstrategy is complemented by interaction methods tailored to answer specific\nanalysis questions, such as sorting based on multiple columns and rich data\nselection and filtering capabilities. We demonstrate Taggle using a case study\nconducted by a domain expert on complex genomics data analysis for the purpose\nof drug discovery.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 12:06:11 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 18:08:21 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 12:12:20 GMT"}, {"version": "v4", "created": "Tue, 24 Sep 2019 09:22:35 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Furmanova", "Katarina", ""], ["Gratzl", "Samuel", ""], ["Stitz", "Holger", ""], ["Zichner", "Thomas", ""], ["Jaresova", "Miroslava", ""], ["Lex", "Alexander", ""], ["Streit", "Marc", ""]]}, {"id": "1712.06179", "submitter": "Eduardo Graells-Garrido", "authors": "Ignacio Perez-Messina, Claudio Gutierrez, Eduardo Graells-Garrido", "title": "Organic Visualization of Document Evolution", "comments": "5 pages. Short paper accepted at the 23rd ACM Conference on\n  Intelligent User Interfaces", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent availability of data of writing processes at keystroke-granularity has\nenabled research on the evolution of document writing. A natural step is to\ndevelop systems that can actually show this data and make it understandable.\nHere we propose a data structure that captures a document's fine-grained\nhistory and an organic visualization that serves as an interface to it. We\nevaluate a proof-of-concept implementation of the system through a pilot study\nwith documents written by students at a public university. Our results are\npromising and reveal facets such as general strategies adopted, local edition\ndensity and hierarchical structure of the final text.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 21:20:13 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Perez-Messina", "Ignacio", ""], ["Gutierrez", "Claudio", ""], ["Graells-Garrido", "Eduardo", ""]]}, {"id": "1712.06933", "submitter": "Renato Fabbri", "authors": "Renato Fabbri", "title": "An anthropological account of the Vim text editor: features and tweaks\n  after 10 years of usage", "comments": "Scripts and other files are in this repository:\n  https://github.com/ttm/vim", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vim text editor is very rich in capabilities and thus complex. This\narticle is a description of Vim and a set of considerations about its usage and\ndesign. It results from more than ten years of experience in using Vim for\nwriting and editing various types of documents, e.g. Python, C++, JavaScript,\nChucK programs; \\LaTeX, Markdown, HTML, RDF, Make and other markup files; % TTM\nbinary files. It is commonplace, in the Vim users and developers communities,\nto say that it takes about ten years to master (or start mastering) this text\neditor, and I find that other experienced users have a different view of Vim\nand that they use a different set of features. Therefore, this document exposes\nmy understandings in order to confront my usage with that of other Vim users.\nAnother goal is to make available a reference document with which new users can\ngrasp a sound overview by reading it and the discussions that it might\ngenerate. Also, it should be useful for users of any degree of experience,\nincluding me, as a compendium of commands, namespaces and tweaks. Upon\nfeedback, and maturing of my Vim usage, this document might be enhanced and\nexpanded.\n", "versions": [{"version": "v1", "created": "Mon, 18 Dec 2017 14:14:21 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Fabbri", "Renato", ""]]}, {"id": "1712.07029", "submitter": "Paul Vickers", "authors": "Mohamed Debashi and Paul Vickers", "title": "Sonification of Network Traffic Flow for Monitoring and Situational\n  Awareness", "comments": "17 pages, 7 figures plus supplemental material in Github repository", "journal-ref": null, "doi": "10.1371/journal.pone.0195948", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining situational awareness of what is happening within a network is\nchallenging, not least because the behaviour happens within computers and\ncommunications networks, but also because data traffic speeds and volumes are\nbeyond human ability to process. Visualisation is widely used to present\ninformation about the dynamics of network traffic dynamics. Although it\nprovides operators with an overall view and specific information about\nparticular traffic or attacks on the network, it often fails to represent the\nevents in an understandable way. Visualisations require visual attention and so\nare not well suited to continuous monitoring scenarios in which network\nadministrators must carry out other tasks. Situational awareness is critical\nand essential for decision-making in the domain of computer network monitoring\nwhere it is vital to be able to identify and recognize network environment\nbehaviours.Here we present SoNSTAR (Sonification of Networks for SiTuational\nAwaReness), a real-time sonification system to be used in the monitoring of\ncomputer networks to support the situational awareness of network\nadministrators. SoNSTAR provides an auditory representation of all the TCP/IP\nprotocol traffic within a network based on the different traffic flows between\nbetween network hosts. SoNSTAR raises situational awareness levels for computer\nnetwork defence by allowing operators to achieve better understanding and\nperformance while imposing less workload compared to visual techniques. SoNSTAR\nidentifies the features of network traffic flows by inspecting the status flags\nof TCP/IP packet headers and mapping traffic events to recorded sounds to\ngenerate a soundscape representing the real-time status of the network traffic\nenvironment. Listening to the soundscape allows the administrator to recognise\nanomalous behaviour quickly and without having to continuously watch a computer\nscreen.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 16:32:02 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Debashi", "Mohamed", ""], ["Vickers", "Paul", ""]]}, {"id": "1712.07120", "submitter": "Kleomenis Katevas", "authors": "Kleomenis Katevas, Ilias Leontiadis, Martin Pielot, Joan Serr\\`a", "title": "Continual Prediction of Notification Attendance with Classical and Deep\n  Network Approaches", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate to what extent mobile use patterns can predict -- at the\nmoment it is posted -- whether a notification will be clicked within the next\n10 minutes. We use a data set containing the detailed mobile phone usage logs\nof 279 users, who over the course of 5 weeks received 446,268 notifications\nfrom a variety of apps. Besides using classical gradient-boosted trees, we\ndemonstrate how to make continual predictions using a recurrent neural network\n(RNN). The two approaches achieve a similar AUC of ca. 0.7 on unseen users,\nwith a possible operation point of 50% sensitivity and 80% specificity\nconsidering all notification types (an increase of 40% with respect to a\nprobabilistic baseline). These results enable automatic, intelligent handling\nof mobile phone notifications without the need for user feedback or\npersonalization. Furthermore, they showcase how forego feature-extraction by\nusing RNNs for continual predictions directly on mobile usage logs. To the best\nof our knowledge, this is the first work that leverages mobile sensor data for\ncontinual, context-aware predictions of interruptibility using deep neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 12:20:55 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Katevas", "Kleomenis", ""], ["Leontiadis", "Ilias", ""], ["Pielot", "Martin", ""], ["Serr\u00e0", "Joan", ""]]}, {"id": "1712.07610", "submitter": "Lorenzo Sabattini", "authors": "Valeria Villani and Lorenzo Sabattini and Julia N. Czerniak and\n  Alexander Mertens and Cesare Fantuzzi", "title": "MATE robots simplifying my work: benefits and socio-ethical implications", "comments": "IEEE Robotics and Automation Magazine, 2018", "journal-ref": null, "doi": "10.1109/MRA.2017.2781308", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing complexity of modern industrial automatic and robotic\nsystems, an increasing burden is put on the operators, who are requested to\nsupervise and interact with very complex systems, typically under challenging\nand stressful conditions. To overcome this issue, it is necessary to adopt a\nresponsible approach based on the anthropocentric design methodology, such that\nmachines adapt to the humans capabilities, and not vice versa. Moving along\nthese lines, in this paper we consider an integrated methodological design\napproach, which we call MATE, consisting in devising complex automatic or\nrobotic solutions that measure current operator's status, adapting the\ninteraction accordingly, and providing her/him with proper training to improve\nthe interaction and learn lacking skills and expertise. Accordingly, a MATE\nsystem is intended to be easily usable for all users, thus meeting the\nprinciples of inclusive design. Using such a MATE system gives rise to several\nethical and social implications, which are discussed in this paper.\nAdditionally, a discussion about which factors in the organization of companies\nare critical with respect to the introduction of a MATE system is presented.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 17:55:13 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 09:14:39 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Villani", "Valeria", ""], ["Sabattini", "Lorenzo", ""], ["Czerniak", "Julia N.", ""], ["Mertens", "Alexander", ""], ["Fantuzzi", "Cesare", ""]]}, {"id": "1712.08084", "submitter": "Viral Parekh", "authors": "Viral Parekh, Pin Sym Foong, Shendong Zhao and Ramanathan Subramanian", "title": "AVEID: Automatic Video System for Measuring Engagement In Dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engagement in dementia is typically measured using behavior observational\nscales (BOS) that are tedious and involve intensive manual labor to annotate,\nand are therefore not easily scalable. We propose AVEID, a low cost and\neasy-to-use video-based engagement measurement tool to determine the engagement\nlevel of a person with dementia (PwD) during digital interaction. We show that\nthe objective behavioral measures computed via AVEID correlate well with\nsubjective expert impressions for the popular MPES and OME BOS, confirming its\nviability and effectiveness. Moreover, AVEID measures can be obtained for a\nvariety of engagement designs, thereby facilitating large-scale studies with\nPwD populations.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 16:57:35 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Parekh", "Viral", ""], ["Foong", "Pin Sym", ""], ["Zhao", "Shendong", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1712.08195", "submitter": "Amy LaViers", "authors": "Amy LaViers, Catie Cuan, Madison Heimerdinger, Umer Huzaifa, Catherine\n  Maguire, Reika McNish, Alexandra Nilles, Ishaan Pakrasi, Karen Bradley, Kim\n  Brooks Mata, Novoneel Chakraborty, Ilya Vidrin, and Alexander Zurawski", "title": "Choreographic and Somatic Approaches for the Development of Expressive\n  Robotic Systems", "comments": "Under review at MDPI Arts Special Issue \"The Machine as Artist (for\n  the 21st Century)\"\n  http://www.mdpi.com/journal/arts/special_issues/Machine_Artist", "journal-ref": null, "doi": "10.3390/arts7020011", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robotic systems are moved out of factory work cells into human-facing\nenvironments questions of choreography become central to their design,\nplacement, and application. With a human viewer or counterpart present, a\nsystem will automatically be interpreted within context, style of movement, and\nform factor by human beings as animate elements of their environment. The\ninterpretation by this human counterpart is critical to the success of the\nsystem's integration: knobs on the system need to make sense to a human\ncounterpart; an artificial agent should have a way of notifying a human\ncounterpart of a change in system state, possibly through motion profiles; and\nthe motion of a human counterpart may have important contextual clues for task\ncompletion. Thus, professional choreographers, dance practitioners, and\nmovement analysts are critical to research in robotics. They have design\nmethods for movement that align with human audience perception, can identify\nsimplified features of movement for human-robot interaction goals, and have\ndetailed knowledge of the capacity of human movement. This article provides\napproaches employed by one research lab, specific impacts on technical and\nartistic projects within, and principles that may guide future such work. The\nbackground section reports on choreography, somatic perspectives,\nimprovisation, the Laban/Bartenieff Movement System, and robotics. From this\ncontext methods including embodied exercises, writing prompts, and community\nbuilding activities have been developed to facilitate interdisciplinary\nresearch. The results of this work is presented as an overview of a smattering\nof projects in areas like high-level motion planning, software development for\nrapid prototyping of movement, artistic output, and user studies that help\nunderstand how people interpret movement. Finally, guiding principles for other\ngroups to adopt are posited.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 20:15:26 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["LaViers", "Amy", ""], ["Cuan", "Catie", ""], ["Heimerdinger", "Madison", ""], ["Huzaifa", "Umer", ""], ["Maguire", "Catherine", ""], ["McNish", "Reika", ""], ["Nilles", "Alexandra", ""], ["Pakrasi", "Ishaan", ""], ["Bradley", "Karen", ""], ["Mata", "Kim Brooks", ""], ["Chakraborty", "Novoneel", ""], ["Vidrin", "Ilya", ""], ["Zurawski", "Alexander", ""]]}, {"id": "1712.08348", "submitter": "Maria Spichkova", "authors": "Chong Sun, Jiongyan Zhang, Cong Liu, Barry Chew Bao King, Yuwei Zhang,\n  Matthew Galle, Maria Spichkova", "title": "Towards Software Development For Social Robotics Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the core results of the project on software\ndevelopment for social robotics systems. The usability of maintenance and\ncontrol features is crucial for many kinds of systems, but in the case of\nsocial robotics we also have to take into account that (1) the humanoid robot\nphysically interacts with humans, (2) the conversation with children might have\ndifferent requirements in comparison to the conversation with adults. The\nresults of our work were implement for the humanoid PAL REEM robot, but their\ncore ideas can be applied for other types of humanoid robots. We developed a\nweb-based solution that supports the management of robot-guided tours, provides\nrecommendations for the users as well as allows for a visual analysis of the\ndata on previous tours.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 08:57:51 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Sun", "Chong", ""], ["Zhang", "Jiongyan", ""], ["Liu", "Cong", ""], ["King", "Barry Chew Bao", ""], ["Zhang", "Yuwei", ""], ["Galle", "Matthew", ""], ["Spichkova", "Maria", ""]]}, {"id": "1712.08900", "submitter": "Thiago Santini", "authors": "Thiago Santini, Wolfgang Fuhl, Enkelejda Kasneci", "title": "PuRe: Robust pupil detection for real-time pervasive eye tracking", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2018.02.002", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time, accurate, and robust pupil detection is an essential prerequisite\nto enable pervasive eye-tracking and its applications -- e.g., gaze-based human\ncomputer interaction, health monitoring, foveated rendering, and advanced\ndriver assistance. However, automated pupil detection has proved to be an\nintricate task in real-world scenarios due to a large mixture of challenges\nsuch as quickly changing illumination and occlusions. In this paper, we\nintroduce the Pupil Reconstructor PuRe, a method for pupil detection in\npervasive scenarios based on a novel edge segment selection and conditional\nsegment combination schemes; the method also includes a confidence measure for\nthe detected pupil. The proposed method was evaluated on over 316,000 images\nacquired with four distinct head-mounted eye tracking devices. Results show a\npupil detection rate improvement of over 10 percentage points w.r.t.\nstate-of-the-art algorithms in the two most challenging data sets (6.46 for all\ndata sets), further pushing the envelope for pupil detection. Moreover, we\nadvance the evaluation protocol of pupil detection algorithms by also\nconsidering eye images in which pupils are not present. In this aspect, PuRe\nimproved precision and specificity w.r.t. state-of-the-art algorithms by 25.05\nand 10.94 percentage points, respectively, demonstrating the meaningfulness of\nPuRe's confidence measure. PuRe operates in real-time for modern eye trackers\n(at 120 fps).\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 10:09:10 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Santini", "Thiago", ""], ["Fuhl", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1712.09424", "submitter": "Jan Vykopal", "authors": "Jan Vykopal, Radek O\\v{s}lej\\v{s}ek, Karol\\'ina Bursk\\'a, Krist\\'ina\n  Z\\'akop\\v{c}anov\\'a", "title": "Timely Feedback in Unstructured Cybersecurity Exercises", "comments": "6 pages; SIGCSE '18, Baltimore, MD, USA", "journal-ref": null, "doi": "10.1145/3159450.3159561", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber defence exercises are intensive, hands-on learning events for teams of\nprofessionals who gain or develop their skills to successfully prevent and\nrespond to cyber attacks. The exercises mimic the real-life, routine operation\nof an organization which is being attacked by an unknown offender. Teams of\nlearners receive very limited immediate feedback from the instructors during\nthe exercise; they can usually see only a scoreboard showing the aggregated\ngain or loss of points for particular tasks. An in-depth analysis of learners'\nactions requires considerable human effort, which results in days or weeks of\ndelay. The intensive experience is thus not followed by proper feedback\nfacilitating actual learning, and this diminishes the effect of the exercise.\n  In this initial work, we investigate how to provide valuable feedback to\nlearners right after the exercise without any unnecessary delay. Based on the\nscoring system of a cyber defence exercise, we have developed a new feedback\ntool that presents an interactive, personalized timeline of exercise events. We\ndeployed this tool during an international exercise, where we monitored\nparticipants' interactions and gathered their reflections. The results show\nthat learners did use the new tool and rated it positively. Since this new\nfeature is not bound to a particular defence exercise, it can be applied to all\nexercises that employ scoring based on the evaluation of individual exercise\nobjectives. As a result, it enables the learner to immediately reflect on the\nexperience gained.\n", "versions": [{"version": "v1", "created": "Tue, 26 Dec 2017 21:29:19 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Vykopal", "Jan", ""], ["O\u0161lej\u0161ek", "Radek", ""], ["Bursk\u00e1", "Karol\u00edna", ""], ["Z\u00e1kop\u010danov\u00e1", "Krist\u00edna", ""]]}, {"id": "1712.09929", "submitter": "Karan Grewal", "authors": "Karan Grewal, Khai N. Truong", "title": "On the Challenges of Detecting Rude Conversational Behaviour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we aim to identify moments of rudeness between two\nindividuals. In particular, we segment all occurrences of rudeness in\nconversations into three broad, distinct categories and try to identify each.\nWe show how machine learning algorithms can be used to identify rudeness based\non acoustic and semantic signals extracted from conversations. Furthermore, we\nmake note of our shortcomings in this task and highlight what makes this\nproblem inherently difficult. Finally, we provide next steps which are needed\nto ensure further success in identifying rudeness in conversations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 16:49:40 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Grewal", "Karan", ""], ["Truong", "Khai N.", ""]]}, {"id": "1712.09943", "submitter": "Sungjin Lee", "authors": "Sungjin Lee", "title": "Toward Continual Learning for Conversational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While end-to-end neural conversation models have led to promising advances in\nreducing hand-crafted features and errors induced by the traditional complex\nsystem architecture, they typically require an enormous amount of data due to\nthe lack of modularity. Previous studies adopted a hybrid approach with\nknowledge-based components either to abstract out domain-specific information\nor to augment data to cover more diverse patterns. On the contrary, we propose\nto directly address the problem using recent developments in the space of\ncontinual learning for neural models. Specifically, we adopt a\ndomain-independent neural conversational model and introduce a novel neural\ncontinual learning algorithm that allows a conversational agent to accumulate\nskills across different tasks in a data-efficient way. To the best of our\nknowledge, this is the first work that applies continual learning to\nconversation systems. We verified the efficacy of our method through a\nconversational skill transfer from either synthetic dialogs or human-human\ndialogs to human-computer conversations in a customer support domain.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 17:21:42 GMT"}, {"version": "v2", "created": "Mon, 1 Jan 2018 19:19:52 GMT"}, {"version": "v3", "created": "Tue, 9 Jan 2018 17:53:37 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Lee", "Sungjin", ""]]}, {"id": "1712.10073", "submitter": "Per Ola Kristensson", "authors": "Emli-Mari Nel, Per Ola Kristensson, David J.C. MacKay", "title": "Modelling Noise-Resilient Single-Switch Scanning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-switch scanning systems allow nonspeaking individuals with motor\ndisabilities to communicate by triggering a single switch (e.g., raising an eye\nbrow). A problem with current single-switch scanning systems is that while they\nresult in reasonable performance in noiseless conditions, for instance via\nsimulation or tests with able-bodied users, they fail to accurately model the\nnoise sources that are introduced when a non-speaking individual with motor\ndisabilities is triggering the switch in a realistic use context. To help\nassist the development of more noise-resilient single-switch scanning systems\nwe have developed a mathematical model of scanning systems which incorporates\nextensive noise modelling. Our model includes an improvement to the standard\nscanning method, which we call fast-scan, which we show via simulation can be\nmore suitable for certain users of scanning systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 22:35:49 GMT"}], "update_date": "2018-01-01", "authors_parsed": [["Nel", "Emli-Mari", ""], ["Kristensson", "Per Ola", ""], ["MacKay", "David J. C.", ""]]}]