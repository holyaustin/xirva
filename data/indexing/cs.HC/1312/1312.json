[{"id": "1312.0162", "submitter": "Dmitry Ignatov", "authors": "Anastasia Bezzubtseva and Dmitry I. Ignatov", "title": "A Typology of Collaboration Platform Users", "comments": null, "journal-ref": "R. Tagiew et al. (Eds.) Proc. of Int. Workshop on Experimental\n  Economics in Machine Learning 2012. Published by KU-Leuven, ISBN\n  978-9-08-140992-6, pp. 9-19", "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a review of the existing typologies of Internet\nservice users. We zoom in on social networking services including blogs and\ncrowdsourcing websites. Based on the results of the analysis of the considered\ntypologies obtained by means of FCA we developed a new user typology of a\ncertain class of Internet services, namely a collaboration innovation platform.\nCluster analysis of data extracted from the collaboration platform Witology was\nused to divide more than 500 participants into six groups based on three\nactivity indicators: idea generation, commenting, and evaluation (assigning\nmarks) The obtained groups and their percentages appear to follow the \"90 - 9 -\n1\" rule.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 23:58:32 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Bezzubtseva", "Anastasia", ""], ["Ignatov", "Dmitry I.", ""]]}, {"id": "1312.0750", "submitter": "Jean-Baptiste Lamy", "authors": "Jean-Baptiste Lamy (LIM\\&BIO), Rosy Tsopra (LIM\\&BIO), Alain Venot\n  (LIM\\&BIO), Catherine Duclos (LIM\\&BIO)", "title": "A semi-automatic semantic method for mapping SNOMED CT concepts to VCM\n  Icons", "comments": null, "journal-ref": "Studies in Health Technology and Informatics 192 (2013) 42-6", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VCM (Visualization of Concept in Medicine) is an iconic language for\nrepresenting key medical concepts by icons. However, the use of this language\nwith reference terminologies, such as SNOMED CT, will require the mapping of\nits icons to the terms of these terminologies. Here, we present and evaluate a\nsemi-automatic semantic method for the mapping of SNOMED CT concepts to VCM\nicons. Both SNOMED CT and VCM are compositional in nature; SNOMED CT is\nexpressed in description logic and VCM semantics are formalized in an OWL\nontology. The proposed method involves the manual mapping of a limited number\nof underlying concepts from the VCM ontology, followed by automatic generation\nof the rest of the mapping. We applied this method to the clinical findings of\nthe SNOMED CT CORE subset, and 100 randomly-selected mappings were evaluated by\nthree experts. The results obtained were promising, with 82 of the SNOMED CT\nconcepts correctly linked to VCM icons according to the experts. Most of the\nerrors were easy to fix.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 09:33:45 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Lamy", "Jean-Baptiste", "", "LIM\\&BIO"], ["Tsopra", "Rosy", "", "LIM\\&BIO"], ["Venot", "Alain", "", "LIM\\&BIO"], ["Duclos", "Catherine", "", "LIM\\&BIO"]]}, {"id": "1312.2877", "submitter": "Mohammad H. Alomari", "authors": "Mohammad H. Alomari, Aya Samaha, Khaled AlKamha", "title": "Automated Classification of L/R Hand Movement EEG Signals using Advanced\n  Feature Extraction and Machine Learning", "comments": "6 pages, 4 figures", "journal-ref": "International Journal of Advanced Computer Science and\n  Applications (ijacsa) 07/2013; 4(6):207-212", "doi": "10.14569/IJACSA.2013.040628", "report-no": null, "categories": "cs.NE cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an automated computer platform for the purpose of\nclassifying Electroencephalography (EEG) signals associated with left and right\nhand movements using a hybrid system that uses advanced feature extraction\ntechniques and machine learning algorithms. It is known that EEG represents the\nbrain activity by the electrical voltage fluctuations along the scalp, and\nBrain-Computer Interface (BCI) is a device that enables the use of the brain\nneural activity to communicate with others or to control machines, artificial\nlimbs, or robots without direct physical movements. In our research work, we\naspired to find the best feature extraction method that enables the\ndifferentiation between left and right executed fist movements through various\nclassification algorithms. The EEG dataset used in this research was created\nand contributed to PhysioNet by the developers of the BCI2000 instrumentation\nsystem. Data was preprocessed using the EEGLAB MATLAB toolbox and artifacts\nremoval was done using AAR. Data was epoched on the basis of Event-Related (De)\nSynchronization (ERD/ERS) and movement-related cortical potentials (MRCP)\nfeatures. Mu/beta rhythms were isolated for the ERD/ERS analysis and delta\nrhythms were isolated for the MRCP analysis. The Independent Component Analysis\n(ICA) spatial filter was applied on related channels for noise reduction and\nisolation of both artifactually and neutrally generated EEG sources. The final\nfeature vector included the ERD, ERS, and MRCP features in addition to the\nmean, power and energy of the activations of the resulting independent\ncomponents of the epoched feature datasets. The datasets were inputted into two\nmachine-learning algorithms: Neural Networks (NNs) and Support Vector Machines\n(SVMs). Intensive experiments were carried out and optimum classification\nperformances of 89.8 and 97.1 were obtained using NN and SVM, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 17:04:18 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Alomari", "Mohammad H.", ""], ["Samaha", "Aya", ""], ["AlKamha", "Khaled", ""]]}, {"id": "1312.3724", "submitter": "Pierluigi Gallo", "authors": "Pierluigi Gallo, Ilenia Tinnirello, Laura Giarr\\'e, Domenico Garlisi,\n  Daniele Croce and Adriano Fagiolini", "title": "ARIANNA: pAth Recognition for Indoor Assisted NavigatioN with Augmented\n  perception", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ARIANNA stands for pAth Recognition for Indoor Assisted Navigation with\nAugmented perception. It is a flexible and low cost navigation system for vi-\nsually impaired people. Arianna permits to navigate colored paths painted or\nsticked on the floor revealing their directions through vibrational feedback on\ncommercial smartphones.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 07:54:22 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Gallo", "Pierluigi", ""], ["Tinnirello", "Ilenia", ""], ["Giarr\u00e9", "Laura", ""], ["Garlisi", "Domenico", ""], ["Croce", "Daniele", ""], ["Fagiolini", "Adriano", ""]]}, {"id": "1312.4106", "submitter": "Tomasz Rutkowski", "authors": "Chisaki Nakaizumi, Koichi Mori, Toshie Matsui, Shoji Makino, and\n  Tomasz M. Rutkowski", "title": "Auditory Brain-Computer Interface Paradigm with Head Related Impulse\n  Response-based Spatial Cues", "comments": "The final publication is available at IEEE Xplore\n  http://ieeexplore.ieee.org and the copyright of the final version has been\n  transferred to IEEE (c)2013", "journal-ref": "Proceedings of the 9th International Conference on Signal Image\n  Technology and Internet Based Systems. Kyoto, Japan: IEEE Computer Society;\n  2013. p. 806-811", "doi": "10.1109/SITIS.2013.131", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The aim of this study is to provide a comprehensive test of head related\nimpulse response (HRIR) for an auditory spatial speller brain-computer\ninterface (BCI) paradigm. The study is conducted with six users in an\nexperimental set up based on five Japanese hiragana vowels. Auditory evoked\npotentials resulted with encouragingly good and stable \"aha-\" or P300-responses\nin real-world online BCI experiments. Our case study indicated that the\nauditory HRIR spatial sound reproduction paradigm could be a viable alternative\nto the established multi-loudspeaker surround sound BCI-speller applications,\nas far as healthy pilot study users are concerned.\n", "versions": [{"version": "v1", "created": "Sun, 15 Dec 2013 04:54:22 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Nakaizumi", "Chisaki", ""], ["Mori", "Koichi", ""], ["Matsui", "Toshie", ""], ["Makino", "Shoji", ""], ["Rutkowski", "Tomasz M.", ""]]}, {"id": "1312.4322", "submitter": "Stephen Bryson", "authors": "Steve Bryson", "title": "Virtual Reality: A Definition History - A Personal Essay", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This essay, written in 1998 by an active participant in both virtual reality\ndevelopment and the virtual reality definition debate, discusses the definition\nof the phrase \"Virtual Reality\" (VR). I start with history from a personal\nperspective, concentrating on the debate between the \"Virtual Reality\" and\n\"Virtual Environment\" labels in the late 1980's and early 1990's. Definitions\nof VR based on specific technologies are shown to be unsatisfactory. I propose\nthe following definition of VR, based on the striking effects of a good VR\nsystem: \"Virtual Reality is the use of computer technology to create the effect\nof an interactive three-dimensional world in which the objects have a sense of\nspatial presence.\" The justification for this definition is discussed in\ndetail, and is favorably compared with the dictionary definitions of \"virtual\"\nand \"reality\". The implications of this definition for virtual reality\ntechnology are briefly examined.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 11:33:06 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Bryson", "Steve", ""]]}, {"id": "1312.4640", "submitter": "Sarajane Marques Peres Dr.", "authors": "Renata Cristina Barros Madeo, Priscilla Koch Wagner, Sarajane Marques\n  Peres", "title": "A Review of Temporal Aspects of Hand Gesture Analysis Applied to\n  Discourse Analysis and Natural Conversation", "comments": "20 pages, International Journal of Computer Science & Information\n  Technology (IJCSIT) Vol 5, No 4, August 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lately, there has been an increasing interest in hand gesture analysis\nsystems. Recent works have employed pattern recognition techniques and have\nfocused on the development of systems with more natural user interfaces. These\nsystems may use gestures to control interfaces or recognize sign language\ngestures, which can provide systems with multimodal interaction; or consist in\nmultimodal tools to help psycholinguists to understand new aspects of discourse\nanalysis and to automate laborious tasks. Gestures are characterized by several\naspects, mainly by movements and sequence of postures. Since data referring to\nmovements or sequences carry temporal information, this paper presents a\nliterature review about temporal aspects of hand gesture analysis, focusing on\napplications related to natural conversation and psycholinguistic analysis,\nusing Systematic Literature Review methodology. In our results, we organized\nworks according to type of analysis, methods, highlighting the use of Machine\nLearning techniques, and applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 05:00:23 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Madeo", "Renata Cristina Barros", ""], ["Wagner", "Priscilla Koch", ""], ["Peres", "Sarajane Marques", ""]]}, {"id": "1312.4706", "submitter": "Donna Vakharia", "authors": "Donna Vakharia, Rachel Gibbs", "title": "Designing Spontaneous Speech Search Interface for Historical Archives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spontaneous speech in the form of conversations, meetings, voice-mail,\ninterviews, oral history, etc. is one of the most ubiquitous forms of human\ncommunication. Search engines providing access to such speech collections have\nthe potential to better inform intelligence and make relevant data over vast\naudio/video archives available to users. This project presents a search user\ninterface design supporting search tasks over a speech collection consisting of\nan historical archive with nearly 52,000 audiovisual testimonies of survivors\nand witnesses of the Holocaust and other genocides. The design incorporates\nfaceted search, along with other UI elements like highlighted search items,\ntags, snippets, etc., to promote discovery and exploratory search. Two\ndifferent designs have been created to support both manual and automated\ntranscripts. Evaluation was performed using human subjects to measure accuracy\nin retrieving results, understanding user-perspective on the design elements,\nand ease of parsing information.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 10:18:44 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Vakharia", "Donna", ""], ["Gibbs", "Rachel", ""]]}, {"id": "1312.5547", "submitter": "Lassi A Liikkanen", "authors": "Lassi A Liikkanen", "title": "Three Metrics for Measuring User Engagement with Online Media and a\n  YouTube Case Study", "comments": "4 pages, 1 figure, 3 tables, 2 appendixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical report discusses three metrics of user engagement with online\nmedia. They are Commenting frequency, Voting frequency, and Voting balance.\nThese relative figures can be derived from established, basic statistics\navailable for many services, prominently YouTube. The paper includes case a\nstudy of popular YouTube videos to illustrate the characteristics and\nusefulness of the measures. The study documents the range of observed values\nand their relationships. The empirical sample shows the three measures to be\nonly moderately correlated with the original statistics despite the common\nnumerators and denominators. The paper concludes by discussing future\napplications and the needs of the quantification of user interaction with new\nmedia services.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 13:45:11 GMT"}, {"version": "v2", "created": "Thu, 10 Apr 2014 12:18:57 GMT"}], "update_date": "2014-04-11", "authors_parsed": [["Liikkanen", "Lassi A", ""]]}, {"id": "1312.7076", "submitter": "Jinyun Yan", "authors": "Stratis Ioannidis, S. Muthukrishnan, Jinyun Yan", "title": "A Consensus-Focused Group Recommender System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases, recommendations are consumed by groups of users rather than\nindividuals. In this paper, we present a system which recommends social events\nto groups. The system helps groups to organize a joint activity and\ncollectively select which activity to perform among several possible options.\nWe also facilitate the consensus making, following the principle of group\nconsensus decision making. Our system allows users to asynchronously vote, add\nand comment on alternatives. We observe social influence within groups through\npost-recommendation feedback during the group decision making process. We\npropose a decision cascading model and estimate such social influence, which\ncan be used to improve the performance of group recommendation. We conduct\nexperiments to measure the prediction performance of our model. The result\nshows that the model achieves better results than that of independent decision\nmaking model.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2013 09:35:50 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 19:42:54 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Ioannidis", "Stratis", ""], ["Muthukrishnan", "S.", ""], ["Yan", "Jinyun", ""]]}, {"id": "1312.7444", "submitter": "Narayan Chakraborty", "authors": "Mohammad Jabed Morshed Chowdhury and Narayan Ranjan Chakraborty", "title": "CAPTCHA Based on Human Cognitive Factor", "comments": "International Journal of Advanced Computer Science and Applications,\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A CAPTCHA (Completely Automated Public Turing test to tell Computers and\nHumans Apart) is an automatic security mechanism used to determine whether the\nuser is a human or a malicious computer program. It is a program that generates\nand grades tests that are human solvable, but intends to be beyond the\ncapabilities of current computer programs. CAPTCHA should be designed to be\nvery easy for humans but very hard for machines. Unfortunately, the existing\nCAPTCHA systems while trying to maximize the difficulty for automated programs\nto pass tests by increasing distortion or noise have consequently, made it also\nvery difficult for potential users. To address the issue, this paper addresses\nan alternative form of CAPTCHA that provides a variety of questions from\nmathematical, logical and general problems which only human can understand and\nanswer correctly in a given time. The proposed framework supports diversity in\nchoosing the questions to be answered and a user-friendly framework to the\nusers. A user-study is also conducted to judge the performance of the developed\nsystem with different background. The study shows the efficacy of the\nimplemented system with a good level of user satisfaction over traditional\nCAPTCHA available today.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 15:28:23 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Chowdhury", "Mohammad Jabed Morshed", ""], ["Chakraborty", "Narayan Ranjan", ""]]}, {"id": "1312.7520", "submitter": "Muhammad Imran", "authors": "Muhammad Imran", "title": "An Effective End-User Development Approach Through Domain-Specific\n  Mashups for Research Impact Evaluation", "comments": "This PhD dissertation consists of 206 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, there has been growing interest in the assessment of\nthe performance of researchers, research groups, universities and even\ncountries. The assessment of productivity is an instrument to select and\npromote personnel, assign research grants and measure the results of research\nprojects. One particular assessment approach is bibliometrics i.e., the\nquantitative analysis of scientific publications through citation and content\nanalysis. However, there is little consensus today on how research evaluation\nshould be performed, and it is commonly acknowledged that the quantitative\nmetrics available today are largely unsatisfactory. A number of different\nscientific data sources available on the Web (e.g., DBLP, Google Scholar) that\nare used for such analysis purposes. Taking data from these diverse sources,\nperforming the analysis and visualizing results in different ways is not a\ntrivial and straight forward task. Moreover, people involved in such evaluation\nprocesses are not always IT experts and hence not capable to crawl data\nsources, merge them and compute the needed evaluation procedures. The recent\nemergence of mashup tools has refueled research on end-user development, i.e.,\non enabling end-users without programming skills to produce their own\napplications. We believe that the heart of the problem is that it is\nimpractical to design tools that are generic enough to cover a wide range of\napplication domains, powerful enough to enable the specification of non-trivial\nlogic, and simple enough to be actually accessible to non-programmers. This\nthesis presents a novel approach for an effective end-user development,\nspecifically for non-programmers. That is, we introduce a domain-specific\napproach to mashups that \"speaks the language of users\"., i.e., that is aware\nof the terminology, concepts, rules, and conventions (the domain) the user is\ncomfortable with.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 11:19:21 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2014 18:11:41 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2014 19:08:43 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Imran", "Muhammad", ""]]}, {"id": "1312.7560", "submitter": "Amiraj Dhawan", "authors": "Amiraj Dhawan, Vipul Honrao", "title": "Implementation of Hand Detection based Techniques for Human Computer\n  Interaction", "comments": null, "journal-ref": "International Journal of Computer Applications, Volume 72 No 17,\n  June 2013", "doi": "10.5120/12632-9151 10.5120/12632-9151 10.5120/12632-9151", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computer industry is developing at a fast pace. With this development\nalmost all of the fields under computers have advanced in the past couple of\ndecades. But the same technology is being used for human computer interaction\nthat was used in 1970s. Even today the same type of keyboard and mouse is used\nfor interacting with computer systems. With the recent boom in the mobile\nsegment touchscreens have become popular for interaction with cell phones. But\nthese touchscreens are rarely used on traditional systems. This paper tries to\nintroduce methods for human computer interaction using the users hand which can\nbe used both on traditional computer platforms as well as cell phones. The\nmethods explain how the users detected hand can be used as input for\napplications and also explain applications that can take advantage of this type\nof interaction mechanism.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 17:29:01 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Dhawan", "Amiraj", ""], ["Honrao", "Vipul", ""]]}]