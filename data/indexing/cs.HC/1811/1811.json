[{"id": "1811.00101", "submitter": "Mosab Khayat", "authors": "Mosab Khayat and Arif Ghafoor", "title": "A Process-driven View on Summative Evaluation of Visual Analytics\n  Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many evaluation methods have been applied to assess the usefulness of visual\nanalytics solutions. These methods are branching from a variety of origins with\ndifferent assumptions, and goals. We provide a high-level overview of the\nprocess employed in each method using the generic evaluation model \"GEM\" that\ngeneralizes the process of usefulness evaluation. The model treats evaluation\nmethods as processes that generate evidence of usefulness as output. Our model\nserves three purposes: It educate new VA practitioners about the heterogeneous\nevaluation practices in the field, it highlights potential risks in the process\nof evaluation which reduces their validity and It provide a guideline to elect\nsuitable evaluation method.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:26:00 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Khayat", "Mosab", ""], ["Ghafoor", "Arif", ""]]}, {"id": "1811.00746", "submitter": "Michelle Zhou", "authors": "Jingyi Li, Michelle X. Zhou, Huahai Yang, Gloria Mark", "title": "Confiding in and Listening to Virtual Agents: The Effect of Personality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an intelligent virtual interviewer that engages with a user in a\ntext-based conversation and automatically infers the user's psychological\ntraits, such as personality. We investigate how the personality of a virtual\ninterviewer influences a user's behavior from two perspectives: the user's\nwillingness to confide in, and listen to, a virtual interviewer. We have\ndeveloped two virtual interviewers with distinct personalities and deployed\nthem in a real-world recruiting event. We present findings from completed\ninterviews with 316 actual job applicants. Notably, users are more willing to\nconfide in and listen to a virtual interviewer with a serious, assertive\npersonality. Moreover, users' personality traits, inferred from their chat\ntext, influence their perception of a virtual interviewer, and their\nwillingness to confide in and listen to a virtual interviewer. Finally, we\ndiscuss the implications of our work on building hyper-personalized,\nintelligent agents based on user traits.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 05:50:39 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Li", "Jingyi", ""], ["Zhou", "Michelle X.", ""], ["Yang", "Huahai", ""], ["Mark", "Gloria", ""]]}, {"id": "1811.00876", "submitter": "Deniz Lefkeli", "authors": "Deniz Lefkeli, Baris Akgun, Sahibzada Omar, Aansa Malik, Zeynep Gurhan\n  Canli, Terry Eskenazi", "title": "Mind in the Machine: Perceived Minds Induce Decision Change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on human robot interaction explored whether people's tendency\nto conform to others extends to artificial agents (Hertz & Wiese, 2016).\nHowever, little is known about to what extent perception of a robot as having a\nmind affects people's decisions. Grounded on the theory of mind perception, the\ncurrent study proposes that artificial agents can induce decision change to the\nextent in which individuals perceive them as having minds. By varying the\ndegree to which robots expressed ability to act (agency) or feel (experience),\nwe specifically investigated the underlying mechanisms of mind attribution to\nrobots and social influence. Our results show an interactive effect of\nperceived experience and perceived agency on social influence induced by\nartificial agents. The findings provide preliminary insights regarding\nautonomous robots' influence on individuals' decisions and form a basis for\nunderstanding the underlying dynamics of decision making with robots.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:22:47 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Lefkeli", "Deniz", ""], ["Akgun", "Baris", ""], ["Omar", "Sahibzada", ""], ["Malik", "Aansa", ""], ["Canli", "Zeynep Gurhan", ""], ["Eskenazi", "Terry", ""]]}, {"id": "1811.00981", "submitter": "Erfan Pakdamanian", "authors": "Brian An, Inki Kim, Erfan Pakdamanian, Donald E. Brown", "title": "Exploring Gaze Behavior to Assess Performance in Digital Game-Based\n  Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent growth of sophisticated digital gaming technologies has spawned an\n\\$8.1B industry around using these games for pedagogical purposes. Though\nDigital Game-Based Learning Systems have been adopted by industries ranging\nfrom military to medical applications, these systems continue to rely on\ntraditional measures of explicit interactions to gauge player performance which\ncan be subject to guessing and other factors unrelated to actual performance.\nThis study presents a novel implicit eye-tracking based metric for digital\ngame-based learning environments. The proposed metric introduces a weighted\neye-tracking measure of traditional in-game scoring to consider the mental\nschema of a player's decision making. In order to validate the efficacy of this\nmetric, we conducted an experiment with 25 participants playing a game designed\nto evaluate Chinese cultural competency and communication. This experiment\nshowed strong correlation between the novel eye-tracking performance metric and\ntraditional measures of in-game performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 16:54:38 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["An", "Brian", ""], ["Kim", "Inki", ""], ["Pakdamanian", "Erfan", ""], ["Brown", "Donald E.", ""]]}, {"id": "1811.01033", "submitter": "Philipp Jordan", "authors": "Philipp Jordan, Paula Alexandra Silva", "title": "Building an Argument for the Use of Science Fiction in HCI Education", "comments": "6 pages, 1 table, IHSI 2019 accepted submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science fiction literature, comics, cartoons and, in particular, audio-visual\nmaterials, such as science fiction movies and shows, can be a valuable addition\nin Human-computer interaction (HCI) Education. In this paper, we present an\noverview of research relative to future directions in HCI Education, distinct\ncrossings of science fiction in HCI and Computer Science teaching and the\nFramework for 21st Century Learning. Next, we provide examples where science\nfiction can add to the future of HCI Education. In particular, we argue herein\nfirst that science fiction, as tangible and intangible cultural artifact, can\nserve as a trigger for creativity and innovation and thus, support us in\nexploring the design space. Second, science fiction, as a means to analyze\nyet-to-come HCI technologies, can assist us in developing an open-minded and\nreflective dialogue about technological futures, thus creating a singular base\nfor critical thinking and problem solving. Provided that one is cognizant of\nits potential and limitations, we reason that science fiction can be a\nmeaningful extension of selected aspects of HCI curricula and research.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 18:24:18 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jordan", "Philipp", ""], ["Silva", "Paula Alexandra", ""]]}, {"id": "1811.01106", "submitter": "Stefan Hell", "authors": "Stefan Hell, Vasileios Argyriou", "title": "Machine learning architectures to predict motion sickness using a\n  Virtual Reality rollercoaster simulation tool", "comments": "4 pages, accepted to AIVR conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) can cause an unprecedented immersion and feeling of\npresence yet a lot of users experience motion sickness when moving through a\nvirtual environment. Rollercoaster rides are popular in Virtual Reality but\nhave to be well designed to limit the amount of nausea the user may feel. This\npaper describes a novel framework to get automated ratings on motion sickness\nusing Neural Networks. An application that lets users create rollercoasters\ndirectly in VR, share them with other users and ride and rate them is used to\ngather real-time data related to the in-game behaviour of the player, the track\nitself and users' ratings based on a Simulator Sickness Questionnaire (SSQ)\nintegrated into the application. Machine learning architectures based on deep\nneural networks are trained using this data aiming to predict motion sickness\nlevels. While this paper focuses on rollercoasters this framework could help to\nrate any VR application on motion sickness and intensity that involves camera\nmovement. A new well defined dataset is provided in this paper and the\nperformance of the proposed architectures are evaluated in a comparative study.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 22:02:40 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Hell", "Stefan", ""], ["Argyriou", "Vasileios", ""]]}, {"id": "1811.01267", "submitter": "Gillian Hadfield", "authors": "Dylan Hadfield-Menell, McKane Andrus, and Gillian K. Hadfield", "title": "Legible Normativity for AI Alignment: The Value of Silly Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become commonplace to assert that autonomous agents will have to be\nbuilt to follow human rules of behavior--social norms and laws. But human laws\nand norms are complex and culturally varied systems, in many cases agents will\nhave to learn the rules. This requires autonomous agents to have models of how\nhuman rule systems work so that they can make reliable predictions about rules.\nIn this paper we contribute to the building of such models by analyzing an\noverlooked distinction between important rules and what we call silly\nrules--rules with no discernible direct impact on welfare. We show that silly\nrules render a normative system both more robust and more adaptable in response\nto shocks to perceived stability. They make normativity more legible for\nhumans, and can increase legibility for AI systems as well. For AI systems to\nintegrate into human normative systems, we suggest, it may be important for\nthem to have models that include representations of silly rules.\n", "versions": [{"version": "v1", "created": "Sat, 3 Nov 2018 19:09:18 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Hadfield-Menell", "Dylan", ""], ["Andrus", "McKane", ""], ["Hadfield", "Gillian K.", ""]]}, {"id": "1811.01627", "submitter": "Rateb Jabbar Mr.", "authors": "Rateb Jabbar, Khalifa Al-Khalifa, Mohamed Kharbeche, Wael Alhajyaseen,\n  Mohsen Jafari, Shan Jiang", "title": "Real-time Driver Drowsiness Detection for Android Application Using Deep\n  Neural Networks Techniques", "comments": "Conference, 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road crashes and related forms of accidents are a common cause of injury and\ndeath among the human population. According to 2015 data from the World Health\nOrganization, road traffic injuries resulted in approximately 1.25 million\ndeaths worldwide, i.e. approximately every 25 seconds an individual will\nexperience a fatal crash. While the cost of traffic accidents in Europe is\nestimated at around 160 billion Euros, driver drowsiness accounts for\napproximately 100,000 accidents per year in the United States alone as reported\nby The American National Highway Traffic Safety Administration (NHTSA). In this\npaper, a novel approach towards real-time drowsiness detection is proposed.\nThis approach is based on a deep learning method that can be implemented on\nAndroid applications with high accuracy. The main contribution of this work is\nthe compression of heavy baseline model to a lightweight model. Moreover,\nminimal network structure is designed based on facial landmark key point\ndetection to recognize whether the driver is drowsy. The proposed model is able\nto achieve an accuracy of more than 80%. Keywords: Driver Monitoring System;\nDrowsiness Detection; Deep Learning; Real-time Deep Neural Network; Android.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 11:49:26 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Jabbar", "Rateb", ""], ["Al-Khalifa", "Khalifa", ""], ["Kharbeche", "Mohamed", ""], ["Alhajyaseen", "Wael", ""], ["Jafari", "Mohsen", ""], ["Jiang", "Shan", ""]]}, {"id": "1811.01786", "submitter": "Michael Filhol", "authors": "Michael Filhol", "title": "A human-editable Sign Language representation for software editing---and\n  a writing system?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To equip SL with software properly, we need an input system to represent and\nmanipulate signed contents in the same way that every day software allows to\nprocess written text. Refuting the claim that video is good enough a medium to\nserve the purpose, we propose to build a representation that is: editable,\nqueryable, synthesisable and user-friendly---we define those terms upfront. The\nissue being functionally and conceptually linked to that of writing, we study\nexisting writing systems, namely those in use for vocal languages, those\ndesigned and proposed for SLs, and more spontaneous ways in which SL users put\ntheir language in writing. Observing each paradigm in turn, we move on to\npropose a new approach to satisfy our goals of integration in software. We\nfinally open the prospect of our proposition being used outside of this\nrestricted scope, as a writing system in itself, and compare its properties to\nthe other writing systems presented.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 15:21:40 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Filhol", "Michael", ""]]}, {"id": "1811.01938", "submitter": "Sophie Van Der Zee", "authors": "Sophie van der Zee, Ronald Poppe, Alice Havrileck, and Aurelien\n  Baillon", "title": "A personal model of trumpery: Deception detection in a real-world\n  high-stakes setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language use reveals information about who we are and how we feel1-3. One of\nthe pioneers in text analysis, Walter Weintraub, manually counted which types\nof words people used in medical interviews and showed that the frequency of\nfirst-person singular pronouns (i.e., I, me, my) was a reliable indicator of\ndepression, with depressed people using I more often than people who are not\ndepressed4. Several studies have demonstrated that language use also differs\nbetween truthful and deceptive statements5-7, but not all differences are\nconsistent across people and contexts, making prediction difficult8. Here we\nshow how well linguistic deception detection performs at the individual level\nby developing a model tailored to a single individual: the current US\npresident. Using tweets fact-checked by an independent third party (Washington\nPost), we found substantial linguistic differences between factually correct\nand incorrect tweets and developed a quantitative model based on these\ndifferences. Next, we predicted whether out-of-sample tweets were either\nfactually correct or incorrect and achieved a 73% overall accuracy. Our results\ndemonstrate the power of linguistic analysis in real-world deception research\nwhen applied at the individual level and provide evidence that factually\nincorrect tweets are not random mistakes of the sender.\n", "versions": [{"version": "v1", "created": "Mon, 5 Nov 2018 10:46:45 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["van der Zee", "Sophie", ""], ["Poppe", "Ronald", ""], ["Havrileck", "Alice", ""], ["Baillon", "Aurelien", ""]]}, {"id": "1811.02163", "submitter": "Aaron Springer", "authors": "Aaron Springer, Steve Whittaker", "title": "\"I had a solid theory before but it's falling apart\": Polarizing Effects\n  of Algorithmic Transparency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of machine learning has brought closer scrutiny to intelligent\nsystems, leading to calls for greater transparency and explainable algorithms.\nWe explore the effects of transparency on user perceptions of a working\nintelligent system for emotion detection. In exploratory Study 1, we observed\nparadoxical effects of transparency which improves perceptions of system\naccuracy for some participants while reducing accuracy perceptions for others.\nIn Study 2, we test this observation using mixed methods, showing that the\napparent transparency paradox can be explained by a mismatch between\nparticipant expectations and system predictions. We qualitatively examine this\nprocess, indicating that transparency can undermine user confidence by causing\nusers to fixate on flaws when they already have a model of system operation. In\ncontrast transparency helps if users lack such a model. Finally, we revisit the\nnotion of transparency and suggest design considerations for building safe and\nsuccessful machine learning systems based on our insights.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 05:01:21 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Springer", "Aaron", ""], ["Whittaker", "Steve", ""]]}, {"id": "1811.02164", "submitter": "Aaron Springer", "authors": "Aaron Springer, Steve Whittaker", "title": "Progressive Disclosure: Designing for Effective Transparency", "comments": "arXiv admin note: text overlap with arXiv:1811.02163", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we increasingly delegate important decisions to intelligent systems, it is\nessential that users understand how algorithmic decisions are made. Prior work\nhas often taken a technocentric approach to transparency. In contrast, we\nexplore empirical user-centric methods to better understand user reactions to\ntransparent systems. We assess user reactions to global and incremental\nfeedback in two studies. In Study 1, users anticipated that the more\ntransparent incremental system would perform better, but retracted this\nevaluation after experience with the system. Qualitative data suggest this may\narise because incremental feedback is distracting and undermines simple\nheuristics users form about system operation. Study 2 explored these effects in\ndepth, suggesting that users may benefit from initially simplified feedback\nthat hides potential system errors and assists users in building working\nheuristics about system operation. We use these findings to motivate new\nprogressive disclosure principles for transparency in intelligent systems.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 05:01:25 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Springer", "Aaron", ""], ["Whittaker", "Steve", ""]]}, {"id": "1811.02218", "submitter": "Zhuochen Jin", "authors": "Zhuochen Jin and Jingshun Yang and Shuyuan Cui and David Gotz and\n  Jimeng Sun and Nan Cao", "title": "CarePre: An Intelligent Clinical Decision Assistance System", "comments": null, "journal-ref": null, "doi": "10.1145/3344258", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical decision support systems (CDSS) are widely used to assist with\nmedical decision making. However, CDSS typically require manually curated rules\nand other data which are difficult to maintain and keep up-to-date. Recent\nsystems leverage advanced deep learning techniques and electronic health\nrecords (EHR) to provide more timely and precise results. Many of these\ntechniques have been developed with a common focus on predicting upcoming\nmedical events. However, while the prediction results from these approaches are\npromising, their value is limited by their lack of interpretability. To address\nthis challenge, we introduce CarePre, an intelligent clinical decision\nassistance system. The system extends a state-of-the-art deep learning model to\npredict upcoming diagnosis events for a focal patient based on his/her\nhistorical medical records. The system includes an interactive framework\ntogether with intuitive visualizations designed to support the diagnosis,\ntreatment outcome analysis, and the interpretation of the analysis results. We\ndemonstrate the effectiveness and usefulness of CarePre system by reporting\nresults from a quantities evaluation of the prediction algorithm and a case\nstudy and three interviews with senior physicians.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 08:36:04 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Jin", "Zhuochen", ""], ["Yang", "Jingshun", ""], ["Cui", "Shuyuan", ""], ["Gotz", "David", ""], ["Sun", "Jimeng", ""], ["Cao", "Nan", ""]]}, {"id": "1811.02322", "submitter": "Michael Kaufmann", "authors": "Michael Kaufmann, Thomas Parnell, Kornilios Kourtis", "title": "Elastic CoCoA: Scaling In to Improve Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we experimentally analyze the convergence behavior of CoCoA and\nshow, that the number of workers required to achieve the highest convergence\nrate at any point in time, changes over the course of the training. Based on\nthis observation, we build Chicle, an elastic framework that dynamically\nadjusts the number of workers based on feedback from the training algorithm, in\norder to select the number of workers that results in the highest convergence\nrate. In our evaluation of 6 datasets, we show that Chicle is able to\naccelerate the time-to-accuracy by a factor of up to 5.96x compared to the best\nstatic setting, while being robust enough to find an optimal or near-optimal\nsetting automatically in most cases.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 12:35:28 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Kaufmann", "Michael", ""], ["Parnell", "Thomas", ""], ["Kourtis", "Kornilios", ""]]}, {"id": "1811.02353", "submitter": "Mengying Lei", "authors": "Xian-Rui Zhang, Meng-Ying Lei, Yang Li", "title": "An amplitudes-perturbation data augmentation method in convolutional\n  neural networks for EEG decoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interface (BCI) system provides a pathway between humans and\nthe outside world by analyzing brain signals which contain potential neural\ninformation. Electroencephalography (EEG) is one of most commonly used brain\nsignals and EEG recognition is an important part of BCI system. Recently,\nconvolutional neural networks (ConvNet) in deep learning are becoming the new\ncutting edge tools to tackle the problem of EEG recognition. However, training\nan effective deep learning model requires a big number of data, which limits\nthe application of EEG datasets with a small number of samples. In order to\nsolve the issue of data insufficiency in deep learning for EEG decoding, we\npropose a novel data augmentation method that add perturbations to amplitudes\nof EEG signals after transform them to frequency domain. In experiments, we\nexplore the performance of signal recognition with the state-of-the-art models\nbefore and after data augmentation on BCI Competition IV dataset 2a and our\nlocal dataset. The results show that our data augmentation technique can\nimprove the accuracy of EEG recognition effectively.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:00:05 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Zhang", "Xian-Rui", ""], ["Lei", "Meng-Ying", ""], ["Li", "Yang", ""]]}, {"id": "1811.02367", "submitter": "Christian Sieber", "authors": "Christian Sieber, Susanna Schwarzmann, Andreas Blenk, Thomas Zinner,\n  Wolfgang Kellerer", "title": "Scalable Application- and User-aware Resource Allocation in Enterprise\n  Networks Using End-host Pacing", "comments": "Accepted for publication in ACM Transactions on Modeling and\n  Performance Evaluation of Computing Systems (TOMPECS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scalable user- and application-aware resource allocation for heterogeneous\napplications sharing an enterprise network is still an unresolved problem. The\nmain challenges are: (i) How to define user- and application-aware shares of\nresources? (ii) How to determine an allocation of shares of network resources\nto applications? (iii) How to allocate the shares per application in\nheterogeneous networks at scale? In this paper we propose solutions to the\nthree challenges and introduce a system design for enterprise deployment.\nDefining the necessary resource shares per application is hard, as the intended\nuse case and user's preferences influence the resource demand. Utility\nfunctions based on user experience enable a mapping of network resources in\nterms of throughput and latency budget to a common user-level utility scale. A\nmulti-objective MILP is formulated to solve the throughput- and delay-aware\nembedding of each utility function for a max-min fairness criteria. The\nallocation of resources in traditional networks with policing and scheduling\ncannot distinguish large numbers of classes. We propose a resource allocation\nsystem design for enterprise networks based on Software-Defined Networking\nprinciples to achieve delay-constrained routing in the network and application\npacing at the end-hosts. The system design is evaluated against best effort\nnetworks with applications competing for the throughput of a constrained link.\nThe competing applications belong to the five application classes web browsing,\nfile download, remote terminal work, video streaming, and Voice-over-IP. The\nresults show that the proposed methodology improves the minimum and total\nutility, minimizes packet loss and queuing delay at bottlenecks, establishes\nfairness in terms of utility between applications, and achieves predictable\napplication performance at high link utilization.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 14:29:21 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 19:52:13 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Sieber", "Christian", ""], ["Schwarzmann", "Susanna", ""], ["Blenk", "Andreas", ""], ["Zinner", "Thomas", ""], ["Kellerer", "Wolfgang", ""]]}, {"id": "1811.02391", "submitter": "Till Massing", "authors": "Nils Schwinning, Michael Striewe, Till Massing, Christoph Hanck,\n  Michael Goedicke", "title": "Towards digitalisation of summative and formative assessments in\n  academic teaching of statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web-based systems for assessment or homework are commonly used in many\ndifferent domains. Several studies show that these systems can have positive\neffects on learning outcomes. Many research efforts also have made these\nsystems quite flexible with respect to different item formats and exercise\nstyles. However, there is still a lack of support for complex exercises in\nseveral domains at university level. Although there are systems that allow for\nquite sophisticated operations for generating exercise contents, there is less\nsupport for using similar operations for evaluating students' input and for\nfeedback generation. This paper elaborates on filling this gap in the specific\ncase of statistics. We present both the conceptional requirements for this\nspecific domain as well as a fully implemented solution. Furthermore, we report\non using this solution for formative and summative assessments in lectures with\nlarge numbers of participants at a big university.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 15:01:22 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Schwinning", "Nils", ""], ["Striewe", "Michael", ""], ["Massing", "Till", ""], ["Hanck", "Christoph", ""], ["Goedicke", "Michael", ""]]}, {"id": "1811.02795", "submitter": "Tomoya Kitamura", "authors": "Tomoya Kitamura, Yuu Hasegawa, Sho Sakaino, and Toshiaki Tsuji", "title": "Estimation of Relationship between Stimulation Current and Force Exerted\n  during Isometric Contraction", "comments": "(C) 2018 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "The 44th International Conference on Industrial Electronics,\n  Control and Instrumentation, pp. 5080-5085 (2018)", "doi": "10.1109/IECON.2018.8591190", "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we developed a method to estimate the relationship between\nstimulation current and volatility during isometric contraction. In functional\nelectrical stimulation (FES), joints are driven by applying voltage to muscles.\nThis technology has been used for a long time in the field of rehabilitation,\nand recently application oriented research has been reported. However,\nestimation of the relationship between stimulus value and exercise capacity has\nnot been discussed to a great extent. Therefore, in this study, a human muscle\nmodel was estimated using the transfer function estimation method with fast\nFourier transform. It was found that the relationship between stimulation\ncurrent and force exerted could be expressed by a first-order lag system. In\nverification of the force estimate, the ability of the proposed model to\nestimate the exerted force under steady state response was found to be good.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 08:20:26 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 09:06:40 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kitamura", "Tomoya", ""], ["Hasegawa", "Yuu", ""], ["Sakaino", "Sho", ""], ["Tsuji", "Toshiaki", ""]]}, {"id": "1811.02848", "submitter": "Irene Celino", "authors": "Gloria Re Calegari, Andrea Fiano, Irene Celino", "title": "A Framework to build Games with a Purpose for Linked Data Refinement", "comments": "17 pages, 4 figures, 17th International Semantic Web Conference (ISWC\n  2018) - Resources Track", "journal-ref": null, "doi": "10.1007/978-3-030-00668-6_10", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of linked data and knowledge graphs, the need becomes\ncompelling to find suitable solutions to increase the coverage and correctness\nof datasets, to add missing knowledge and to identify and remove errors.\nSeveral approaches - mostly relying on machine learning and NLP techniques -\nhave been proposed to address this refinement goal; they usually need a partial\ngold standard, i.e. some \"ground truth\" to train automatic models. Gold\nstandards are manually constructed, either by involving domain experts or by\nadopting crowdsourcing and human computation solutions.\n  In this paper, we present an open source software framework to build Games\nwith a Purpose for linked data refinement, i.e. web applications to crowdsource\npartial ground truth, by motivating user participation through fun incentive.\nWe detail the impact of this new resource by explaining the specific data\nlinking \"purposes\" supported by the framework (creation, ranking and validation\nof links) and by defining the respective crowdsourcing tasks to achieve those\ngoals.\n  To show this resource's versatility, we describe a set of diverse\napplications that we built on top of it; to demonstrate its reusability and\nextensibility potential, we provide references to detailed documentation,\nincluding an entire tutorial which in a few hours guides new adopters to\ncustomize and adapt the framework to a new use case.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 11:44:24 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Calegari", "Gloria Re", ""], ["Fiano", "Andrea", ""], ["Celino", "Irene", ""]]}, {"id": "1811.03046", "submitter": "Mohammad Rafayet Ali", "authors": "Mohammad Rafayet Ali, Zahra Razavi, Abdullah Al Mamun, Raina Langevin,\n  Benjamin Kane, Reza Rawassizadeh, Lenhart Schubert, M Ehsan Hoque", "title": "A Virtual Conversational Agent for Teens with Autism: Experimental\n  Results and Design Lessons", "comments": null, "journal-ref": null, "doi": "10.1145/3383652.3423900", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the design of an online social skills development interface for\nteenagers with autism spectrum disorder (ASD). The interface is intended to\nenable private conversation practice anywhere, anytime using a web-browser.\nUsers converse informally with a virtual agent, receiving feedback on nonverbal\ncues in real-time, and summary feedback. The prototype was developed in\nconsultation with an expert UX designer, two psychologists, and a pediatrician.\nUsing the data from 47 individuals, feedback and dialogue generation were\nautomated using a hidden Markov model and a schema-driven dialogue manager\ncapable of handling multi-topic conversations. We conducted a study with nine\nhigh-functioning ASD teenagers. Through a thematic analysis of post-experiment\ninterviews, identified several key design considerations, notably: 1) Users\nshould be fully briefed at the outset about the purpose and limitations of the\nsystem, to avoid unrealistic expectations. 2) An interface should incorporate\npositive acknowledgment of behavior change. 3) Realistic appearance of a\nvirtual agent and responsiveness are important in engaging users. 4)\nConversation personalization, for instance in prompting laconic users for more\ninput and reciprocal questions, would help the teenagers engage for longer\nterms and increase the system's utility.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 18:02:53 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 12:58:31 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 04:09:05 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Ali", "Mohammad Rafayet", ""], ["Razavi", "Zahra", ""], ["Mamun", "Abdullah Al", ""], ["Langevin", "Raina", ""], ["Kane", "Benjamin", ""], ["Rawassizadeh", "Reza", ""], ["Schubert", "Lenhart", ""], ["Hoque", "M Ehsan", ""]]}, {"id": "1811.03180", "submitter": "Eugene Wu", "authors": "Gabriel Ryan, Abigail Mosca, Remco Chang, Eugene Wu", "title": "At a Glance: Pixel Approximate Entropy as a Measure of Line Chart\n  Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When inspecting information visualizations under time critical settings, such\nas emergency response or monitoring the heart rate in a surgery room, the user\nonly has a small amount of time to view the visualization \"at a glance\". In\nthese settings, it is important to provide a quantitative measure of the\nvisualization to understand whether or not the visualization is too \"complex\"\nto accurately judge at a glance. This paper proposes Pixel Approximate Entropy\n(PAE), which adapts the approximate entropy statistical measure commonly used\nto quantify regularity and unpredictability in time-series data, as a measure\nof visual complexity for line charts. We show that PAE is correlated with\nuser-perceived chart complexity, and that increased chart PAE correlates with\nreduced judgement accuracy. We also find that the correlation between PAE\nvalues and participants' judgment increases when the user has less time to\nexamine the line charts.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 23:19:16 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Ryan", "Gabriel", ""], ["Mosca", "Abigail", ""], ["Chang", "Remco", ""], ["Wu", "Eugene", ""]]}, {"id": "1811.03401", "submitter": "Zhenyue Qin", "authors": "Jiaxu Zuo and Tom Gedeon and Zhenyue Qin", "title": "Your Eyes Say You're Lying: An Eye Movement Pattern Analysis for Face\n  Familiarity and Deceptive Cognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movement patterns reflect human latent internal cognitive activities. We\naim to discover eye movement patterns during face recognition under different\ncognitions of information concealing. These cognitions include the degrees of\nface familiarity and deception or not, namely telling the truth when observing\nfamiliar and unfamiliar faces, and deceiving in front of familiar faces. We\napply Hidden Markov models with Gaussian emission to generalize regions and\ntrajectories of eye fixation points under the above three conditions. Our\nresults show that both eye movement patterns and eye gaze regions become\nsignificantly different during deception compared with truth-telling. We show\nthe feasibility of detecting deception and further cognitive activity\nclassification using eye movement patterns.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 13:32:53 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Zuo", "Jiaxu", ""], ["Gedeon", "Tom", ""], ["Qin", "Zhenyue", ""]]}, {"id": "1811.03423", "submitter": "Kory W Mathewson", "authors": "Markus Eger and Kory W. Mathewson", "title": "dAIrector: Automatic Story Beat Generation through Knowledge Synthesis", "comments": "10 pages with references, 1 figure. Accepted at Joint Workshop on\n  Intelligent Narrative Technologies and Intelligent Cinematography and Editing\n  at AAAI Conference on Artificial Intelligence and Interactive Digital\n  Entertainment (AIIDE'18). Edmonton, Alberta, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  dAIrector is an automated director which collaborates with humans\nstorytellers for live improvisational performances and writing assistance.\ndAIrector can be used to create short narrative arcs through contextual plot\ngeneration. In this work, we present the system architecture, a quantitative\nevaluation of design choices, and a case-study usage of the system which\nprovides qualitative feedback from a professional improvisational performer. We\npresent relevant metrics for the understudied domain of human-machine creative\ngeneration, specifically long-form narrative creation. We include, alongside\npublication, open-source code so that others may test, evaluate, and run the\ndAIrector.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 15:41:22 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Eger", "Markus", ""], ["Mathewson", "Kory W.", ""]]}, {"id": "1811.03541", "submitter": "Shuo Niu", "authors": "Shuo Niu, D. Scott McCrickard, Steve Harrison", "title": "Towards Connecting Experiences during Collocated Events through Data\n  Mining and Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Themed collocated events, such as conferences, workshops, and seminars,\ninvite people with related life experiences to connect with each other. In this\nera when people record lives through the Internet, individual experiences exist\nin different forms of digital contents. People share digital life records\nduring collocated events, such as sharing blogs they wrote, Twitter posts they\nforwarded, and books they have read. However, connecting experiences during\ncollocated events are challenging. Not only one is blind to the large contents\nof others, identifying related experiential items depends on how well\nexperiences are retrieved. The collection of personal contents from all\nparticipants forms a valuable group repository, from which connections between\nexperiences can be mined. Visualizing same or related experiences inspire\nconversations and support social exchange. Common topics in group content also\nhelp participants generate new perspectives about the collocated group.\nAdvances in machine learning and data visualization provide automated\napproaches to process large data and enable interactions with data\nrepositories. This position paper promotes the idea of event mining: how to\nutilize state-of-the-art data processing and visualization techniques to design\nevent mining systems for connecting experiences during collocated activities.\nWe discuss empirical and constructive problems in this design space, and our\npreliminary study of deploying a tabletop-based system, BlogCloud, which\nsupports experience re-visitation and exchange with machine-learning and data\nvisualization.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:42:30 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Niu", "Shuo", ""], ["McCrickard", "D. Scott", ""], ["Harrison", "Steve", ""]]}, {"id": "1811.03566", "submitter": "David Robb", "authors": "David A. Robb, Jonatan Scharff Willners, Nicolas Valeyrie, Francisco\n  J. Chiyah Garcia, Atanas Laskov, Xingkun Liu, Pedro Patron, Helen Hastie,\n  Yvan R. Petillot", "title": "A Natural Language Interface with Relayed Acoustic Communications for\n  Improved Command and Control of AUVs", "comments": "The definitive version of this preprint is to be Published in AUV2018\n  Keywords: Conversational agent, Natural Language Understanding, Chatbot, AUV,\n  USV, Communication Relay, Acoustic, Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous underwater vehicles (AUVs) are being tasked with increasingly\ncomplex missions. The acoustic communications required for AUVs are, by the\nnature of the medium, low bandwidth while adverse environmental conditions\nunderwater often mean they are also intermittent. This has motivated\ndevelopment of highly autonomous systems, which can operate independently of\ntheir operators for considerable periods of time. These missions often involve\nmultiple vehicles leading not only to challenges in communications but also in\ncommand and control (C2). Specifically operators face complexity in controlling\nmulti-objective, multi-vehicle missions, whilst simultaneously facing\nuncertainty over the current status and safety of several remote high value\nassets. Additionally, it may be required to perform command and control of\nthese complex missions in a remote control room. In this paper, we propose a\ncombination of an intuitive, natural language operator interface combined with\ncommunications that use platforms from multiple domains to relay data over\ndifferent mediums and transmission modes, improving command and control of\ncollaborative and fully autonomous missions. In trials, we have demonstrated an\nintegrated system combining working prototypes with established commercial C2\nsoftware that enables the use of a natural language interface to monitor an AUV\nsurvey mission in an on-shore command and control centre.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 17:41:17 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 12:13:47 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Robb", "David A.", ""], ["Willners", "Jonatan Scharff", ""], ["Valeyrie", "Nicolas", ""], ["Garcia", "Francisco J. Chiyah", ""], ["Laskov", "Atanas", ""], ["Liu", "Xingkun", ""], ["Patron", "Pedro", ""], ["Hastie", "Helen", ""], ["Petillot", "Yvan R.", ""]]}, {"id": "1811.03621", "submitter": "Hang Qiu", "authors": "Hang Qiu, Krishna Chintalapudi, Ramesh Govindan", "title": "Satyam: Democratizing Groundtruth for Machine Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The democratization of machine learning (ML) has led to ML-based machine\nvision systems for autonomous driving, traffic monitoring, and video\nsurveillance. However, true democratization cannot be achieved without greatly\nsimplifying the process of collecting groundtruth for training and testing\nthese systems. This groundtruth collection is necessary to ensure good\nperformance under varying conditions. In this paper, we present the design and\nevaluation of Satyam, a first-of-its-kind system that enables a layperson to\nlaunch groundtruth collection tasks for machine vision with minimal effort.\nSatyam leverages a crowdtasking platform, Amazon Mechanical Turk, and automates\nseveral challenging aspects of groundtruth collection: creating and launching\nof custom web-UI tasks for obtaining the desired groundtruth, controlling\nresult quality in the face of spammers and untrained workers, adapting prices\nto match task complexity, filtering spammers and workers with poor performance,\nand processing worker payments. We validate Satyam using several popular\nbenchmark vision datasets, and demonstrate that groundtruth obtained by Satyam\nis comparable to that obtained from trained experts and provides matching ML\nperformance when used for training.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:35:47 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Qiu", "Hang", ""], ["Chintalapudi", "Krishna", ""], ["Govindan", "Ramesh", ""]]}, {"id": "1811.03888", "submitter": "Feng Wang", "authors": "Feng Wang, Wanna Zhang and Wei Luo", "title": "An Empirical Evaluation On Vibrotactile Feedback For Wristband System", "comments": "10 pages,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid development of mobile computing, wearable wrist-worn is\nbecoming more and more popular. But the current vibrotactile feedback patterns\nof most wrist-worn devices are too simple to enable effective interaction in\nnonvisual scenarios. In this paper, we propose the wristband system with four\nvibrating motors placed in different positions in the wristband, providing\nmultiple vibration patterns to transmit multi-semantic information for users in\neyes-free scenarios. However, we just applied five vibrotactile patterns in\nexperiments (positional up and down, horizontal diagonal, clockwise circular,\nand total vibration) after contrastive analyzing nine patterns in a pilot\nexperiment. The two experiments with the same 12 participants perform the same\nexperimental process in lab and outdoors. According to the experimental\nresults, users can effectively distinguish the five patterns both in lab and\noutside, with approximately 90% accuracy (except clockwise circular vibration\nof outside experiment), proving these five vibration patterns can be used to\noutput multi-semantic information. The system can be applied to eyes-free\ninteraction scenarios for wrist-worn devices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 13:14:40 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Wang", "Feng", ""], ["Zhang", "Wanna", ""], ["Luo", "Wei", ""]]}, {"id": "1811.04172", "submitter": "Zhengwei Wang", "authors": "Zhengwei Wang, Graham Healy, Alan F. Smeaton, Tomas E. Ward", "title": "Use of Neural Signals to Evaluate the Quality of Generative Adversarial\n  Network Performance in Facial Image Generation", "comments": null, "journal-ref": "Cognitive Computation, August 2019", "doi": "10.1007/s12559-019-09670-y", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in using generative adversarial networks (GANs)\nto produce image content that is indistinguishable from real images as judged\nby a typical person. A number of GAN variants for this purpose have been\nproposed, however, evaluating GANs performance is inherently difficult because\ncurrent methods for measuring the quality of their output are not always\nconsistent with what a human perceives. We propose a novel approach that\ncombines a brain-computer interface (BCI) with GANs to generate a measure we\ncall Neuroscore, which closely mirrors the behavioral ground truth measured\nfrom participants tasked with discerning real from synthetic images. This\ntechnique we call a neuro-AI interface, as it provides an interface between a\nhuman's neural systems and an AI process. In this paper, we first compare the\nthree most widely used metrics in the literature for evaluating GANs in terms\nof visual quality and compare their outputs with human judgments. Secondly we\npropose and demonstrate a novel approach using neural signals and rapid serial\nvisual presentation (RSVP) that directly measures a human perceptual response\nto facial production quality, independent of a behavioral response measurement.\nThe correlation between our proposed Neuroscore and human perceptual judgments\nhas Pearson correlation statistics: $\\mathrm{r}(48) = -0.767, \\mathrm{p} =\n2.089e-10$. We also present the bootstrap result for the correlation i.e.,\n$\\mathrm{p}\\leq 0.0001$. Results show that our Neuroscore is more consistent\nwith human judgment compared to the conventional metrics we evaluated. We\nconclude that neural signals have potential applications for high quality,\nrapid evaluation of GANs in the context of visual image synthesis.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 01:37:56 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 16:22:03 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 15:28:39 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Wang", "Zhengwei", ""], ["Healy", "Graham", ""], ["Smeaton", "Alan F.", ""], ["Ward", "Tomas E.", ""]]}, {"id": "1811.04272", "submitter": "Chao Yu", "authors": "Chao Yu, Tianpei Yang, Wenxuan Zhu, Dongxu wang, Guangliang Li", "title": "Learning Shaping Strategies in Human-in-the-loop Interactive\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing reinforcement learning agents with informationally rich human\nknowledge can dramatically improve various aspects of learning. Prior work has\ndeveloped different kinds of shaping methods that enable agents to learn\nefficiently in complex environments. All these methods, however, tailor human\nguidance to agents in specialized shaping procedures, thus embodying various\ncharacteristics and advantages in different domains. In this paper, we\ninvestigate the interplay between different shaping methods for more robust\nlearning performance. We propose an adaptive shaping algorithm which is capable\nof learning the most suitable shaping method in an on-line manner. Results in\ntwo classic domains verify its effectiveness from both simulated and real human\nstudies, shedding some light on the role and impact of human factors in\nhuman-robot collaborative learning.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 15:26:31 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Yu", "Chao", ""], ["Yang", "Tianpei", ""], ["Zhu", "Wenxuan", ""], ["wang", "Dongxu", ""], ["Li", "Guangliang", ""]]}, {"id": "1811.04631", "submitter": "Christoph Anderson", "authors": "Judith S. Heinisch, Christoph Anderson, Klaus David", "title": "Angry or Climbing Stairs? Towards Physiological Emotion Recognition in\n  the Wild", "comments": "6 pages, 5 figures, submitted to the 2018 IEEE International\n  Conference on Pervasive Computing and Communications Workshops (PerCom\n  Workshops), EmotionAware", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring emotions from physiological signals has gained much traction in the\nlast years. Physiological responses to emotions, however, are commonly\ninterfered and overlapped by physical activities, posing a challenge towards\nemotion recognition in the wild. In this paper, we address this challenge by\ninvestigating new features and machine-learning models for emotion recognition,\nnon-sensitive to physical-based interferences. We recorded physiological\nsignals from 18 participants that were exposed to emotions before and while\nperforming physical activities to assess the performance of non-sensitive\nemotion recognition models. We trained models with the least exhaustive\nphysical activity (sitting) and tested with the remaining, more exhausting\nactivities. For three different emotion categories, we achieve classification\naccuracies ranging from 47.88% - 73.35% for selected feature sets and per\nparticipant. Furthermore, we investigate the performance across all\nparticipants and of each activity individually. In this regard, we achieve\nsimilar results, between 55.17% and 67.41%, indicating the viability of emotion\nrecognition models not being influenced by single physical activities.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 09:51:52 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Heinisch", "Judith S.", ""], ["Anderson", "Christoph", ""], ["David", "Klaus", ""]]}, {"id": "1811.04797", "submitter": "Nisha Vinayaga Sureshkanth", "authors": "Nisha Vinayaga-Sureshkanth, Anindya Maiti, Murtuza Jadliwala, Kirsten\n  Crager, Jibo He, Heena Rathore", "title": "A Practical Framework for Preventing Distracted Pedestrian-related\n  Incidents using Wrist Wearables", "comments": "arXiv admin note: substantial text overlap with arXiv:1710.03755", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Distracted pedestrians, like distracted drivers, are an increasingly\ndangerous threat and precursors to pedestrian accidents in urban communities,\noften resulting in grave injuries and fatalities. Mitigating such hazards to\npedestrian safety requires employment of pedestrian safety systems and\napplications that are effective in detecting them. Designing such frameworks is\npossible with the availability of sophisticated mobile and wearable devices\nequipped with high-precision on-board sensors capable of capturing fine-grained\nuser movements and context, especially distracted activities. However, the key\ntechnical challenge is accurate recognition of distractions with minimal\nresources in real-time given the computation and communication limitations of\nthese devices. Several recently published works improve distracted pedestrian\nsafety by leveraging on complex activity recognition frameworks using mobile\nand wearable sensors to detect pedestrian distractions. Their primary focus,\nhowever, was to achieve high detection accuracy, and therefore most designs are\neither resource intensive and unsuitable for implementation on mainstream\nmobile devices, or computationally slow and not useful for real-time pedestrian\nsafety applications, or require specialized hardware and less likely to be\nadopted by most users. In the quest for a pedestrian safety system, we design\nan efficient and real-time pedestrian distraction detection technique that\novercomes some of these shortcomings. We demonstrate its practicality by\nimplementing prototypes on commercially-available mobile and wearable devices\nand evaluating them using data collected from participants in realistic\npedestrian experiments. Using these evaluations, we show that our technique\nachieves a favorable balance between computational efficiency, detection\naccuracy and energy consumption compared to some other techniques in the\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 18:55:05 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Vinayaga-Sureshkanth", "Nisha", ""], ["Maiti", "Anindya", ""], ["Jadliwala", "Murtuza", ""], ["Crager", "Kirsten", ""], ["He", "Jibo", ""], ["Rathore", "Heena", ""]]}, {"id": "1811.04895", "submitter": "Po-Ming Law", "authors": "Po-Ming Law, Yanhong Wu, Rahul C. Basole", "title": "Segue: Overviewing Evolution Patterns of Egocentric Networks by\n  Interactive Construction of Spatial Layouts", "comments": "Published at IEEE Conference on Visual Analytics Science and\n  Technology (IEEE VAST 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Getting the overall picture of how a large number of ego-networks evolve is a\ncommon yet challenging task. Existing techniques often require analysts to\ninspect the evolution patterns of ego-networks one after another. In this\nstudy, we explore an approach that allows analysts to interactively create\nspatial layouts in which each dot is a dynamic ego-network. These spatial\nlayouts provide overviews of the evolution patterns of ego-networks, thereby\nrevealing different global patterns such as trends, clusters and outliers in\nevolution patterns. To let analysts interactively construct interpretable\nspatial layouts, we propose a data transformation pipeline, with which analysts\ncan adjust the spatial layouts and convert dynamic egonetworks into event\nsequences to aid interpretations of the spatial positions. Based on this\ntransformation pipeline, we developed Segue, a visual analysis system that\nsupports thorough exploration of the evolution patterns of ego-networks.\nThrough two usage scenarios, we demonstrate how analysts can gain insights into\nthe overall evolution patterns of a large collection of ego-networks by\ninteractively creating different spatial layouts.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:28:41 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Law", "Po-Ming", ""], ["Wu", "Yanhong", ""], ["Basole", "Rahul C.", ""]]}, {"id": "1811.05083", "submitter": "Abbas Ganji", "authors": "Abbas Ganji and Scott Miles", "title": "Toward Human-Centered Simulation Modeling for Critical Infrastructure\n  Disaster Recovery Planning", "comments": null, "journal-ref": "IEEE Global Humanitarian Technology Conference (GHTC) 2018", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical infrastructure is vulnerable to a broad range of hazards. Timely and\neffective recovery of critical infrastructure after extreme events is crucial.\nHowever, critical infrastructure disaster recovery planning is complicated and\ninvolves both domain- and user-centered characteristics and complexities.\nRecovery planning currently uses few quantitative computer-based tools and\ninstead largely relies on expert judgment. Simulation modeling can simplify\ndomain-centered complexities but not the human factors. Conversely,\nhuman-centered design places end-users at the center of design. We discuss the\nbenefits of combining simulation modeling with human-centered design and refer\nit as human-centered simulation modeling. Human-centered simulation modeling\nhas the capability to make recovery planning simpler and more understandable\nfor critical infrastructure and emergency management experts and other recovery\nplanning decision-makers.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 03:07:54 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Ganji", "Abbas", ""], ["Miles", "Scott", ""]]}, {"id": "1811.05299", "submitter": "Kaixuan Chen", "authors": "Kaixuan Chen, Lina Yao, Dalin Zhang, Xiaojun Chang, Guodong Long, Sen\n  Wang", "title": "Distributionally Robust Semi-Supervised Learning for People-Centric\n  Sensing", "comments": "8 pages, accepted by AAAI2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning is crucial for alleviating labelling burdens in\npeople-centric sensing. However, human-generated data inherently suffer from\ndistribution shift in semi-supervised learning due to the diverse biological\nconditions and behavior patterns of humans. To address this problem, we propose\na generic distributionally robust model for semi-supervised learning on\ndistributionally shifted data. Considering both the discrepancy and the\nconsistency between the labeled data and the unlabeled data, we learn the\nlatent features that reduce person-specific discrepancy and preserve\ntask-specific consistency. We evaluate our model in a variety of people-centric\nrecognition tasks on real-world datasets, including intention recognition,\nactivity recognition, muscular movement recognition and gesture recognition.\nThe experiment results demonstrate that the proposed model outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 10:53:33 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Chen", "Kaixuan", ""], ["Yao", "Lina", ""], ["Zhang", "Dalin", ""], ["Chang", "Xiaojun", ""], ["Long", "Guodong", ""], ["Wang", "Sen", ""]]}, {"id": "1811.05364", "submitter": "Chun-Wei Chiang", "authors": "Chun-Wei Chiang, Anna Kasunic, Saiph Savage", "title": "Crowd Coach: Peer Coaching for Crowd Workers' Skill Growth", "comments": "17 pages, 4 figures", "journal-ref": "Chun-Wei Chiang, Anna Kasunic, and Saiph Savage. 2018. Crowd\n  Coach: Peer Coaching for Crowd Workers' Skill Growth. Proc. ACM Hum.-Comput.\n  Interact. 2, CSCW, Article 37 (November 2018), 17 pages. DOI:\n  https://doi.org/10.1145/3274306", "doi": "10.1145/3274306", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional employment usually provides mechanisms for workers to improve\ntheir skills to access better opportunities. However, crowd work platforms like\nAmazon Mechanical Turk (AMT) generally do not support skill development (i.e.,\nbecoming faster and better at work). While researchers have started to tackle\nthis problem, most solutions are dependent on experts or requesters willing to\nhelp. However, requesters generally lack the necessary knowledge, and experts\nare rare and expensive. To further facilitate crowd workers' skill growth, we\npresent Crowd Coach, a system that enables workers to receive peer coaching\nwhile on the job. We conduct a field experiment and real world deployment to\nstudy Crowd Coach in the wild. Hundreds of workers used Crowd Coach in a\nvariety of tasks, including writing, doing surveys, and labeling images. We\nfind that Crowd Coach enhances workers' speed without sacrificing their work\nquality, especially in audio transcription tasks. We posit that peer coaching\nsystems hold potential for better supporting crowd workers' skill development\nwhile on the job. We finish with design implications from our research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:31:19 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Chiang", "Chun-Wei", ""], ["Kasunic", "Anna", ""], ["Savage", "Saiph", ""]]}, {"id": "1811.05536", "submitter": "Saverio Perugini", "authors": "Brandon M. Williams and Saverio Perugini", "title": "Staging Human-computer Dialogs: An Application of the Futamura\n  Projections", "comments": "13 pages, 6 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.CL cs.HC cs.SC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate an application of the Futamura Projections to human-computer\ninteraction, and particularly to staging human-computer dialogs. Specifically,\nby providing staging analogs to the classical Futamura Projections, we\ndemonstrate that the Futamura Projections can be applied to the staging of\nhuman-computer dialogs in addition to the execution of programs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 21:45:20 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Williams", "Brandon M.", ""], ["Perugini", "Saverio", ""]]}, {"id": "1811.05902", "submitter": "Gerard Llorach", "authors": "Gerard Llorach and Josep Blat", "title": "Say Hi to Eliza. An Embodied Conversational Agent on the Web", "comments": "4 Pages, 1 figure, 17th International Conference Intelligence Virtual\n  Agents (IVA)", "journal-ref": null, "doi": "10.1007/978-3-319-67401-8_34", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The creation and support of Embodied Conversational Agents (ECAs) has been\nquite challenging, as features required might not be straight-forward to\nimplement and to integrate in a single application. Furthermore, ECAs as\ndesktop applications present drawbacks for both developers and users; the\nformer have to develop for each device and operating system and the latter must\ninstall additional software, limiting their widespread use. In this paper we\ndemonstrate how recent advances in web technologies show promising steps\ntowards capable web-based ECAs, through some off-the-shelf technologies, in\nparticular, the Web Speech API, Web Audio API, WebGL and Web Workers. We\ndescribe their integration into a simple fully functional web-based 3D ECA\naccessible from any modern device, with special attention to our novel work in\nthe creation and support of the embodiment aspects.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 16:55:01 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Llorach", "Gerard", ""], ["Blat", "Josep", ""]]}, {"id": "1811.06040", "submitter": "Reza Abbasi-Asl", "authors": "Reza Abbasi-Asl, Mohammad Keshavarzi, and Dorian Yao Chan", "title": "Brain-Computer Interface in Virtual Reality", "comments": null, "journal-ref": null, "doi": "10.1109/NER.2019.8717158", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the performance of brain computer interface (BCI) system in a\nvirtual reality (VR) environment and compare it to 2D regular displays. First,\nwe design a headset that consists of three components: a wearable\nelectroencephalography (EEG) device, a VR headset and an interface. Recordings\nof brain and behavior from human subjects, performing a wide variety of tasks\nusing our device are collected. The tasks consist of object rotation or scaling\nin VR using either mental commands or facial expression (smile and eyebrow\nmovement). Subjects are asked to repeat similar tasks on regular 2D monitor\nscreens. The performance in 3-D virtual reality environment is considerably\nhigher compared to the to the 2D screen. Particularly, the median number of\nsuccess rate across trials for VR setting is double of that for the 2D setting\n(8 successful command in VR setting compared to 4 successful command in 2D\nscreen in 1 minute trials). Our results suggest that the design of future BCI\nsystems can remarkably benefit from the VR setting.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 15:52:21 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Abbasi-Asl", "Reza", ""], ["Keshavarzi", "Mohammad", ""], ["Chan", "Dorian Yao", ""]]}, {"id": "1811.06624", "submitter": "Jason R.C. Nurse Dr", "authors": "Jason R. C. Nurse", "title": "Cybercrime and You: How Criminals Attack and the Human Factors That They\n  Seek to Exploit", "comments": null, "journal-ref": "The Oxford Handbook of Cyberpsychology, 2018", "doi": "10.1093/oxfordhb/9780198812746.013.35", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cybercrime is a significant challenge to society, but it can be particularly\nharmful to the individuals who become victims. This chapter engages in a\ncomprehensive and topical analysis of the cybercrimes that target individuals.\nIt also examines the motivation of criminals that perpetrate such attacks and\nthe key human factors and psychological aspects that help to make\ncybercriminals successful. Key areas assessed include social engineering (e.g.,\nphishing, romance scams, catfishing), online harassment (e.g., cyberbullying,\ntrolling, revenge porn, hate crimes), identity-related crimes (e.g., identity\ntheft, doxing), hacking (e.g., malware, cryptojacking, account hacking), and\ndenial-of-service crimes. As a part of its contribution, the chapter introduces\na summary taxonomy of cybercrimes against individuals and a case for why they\nwill continue to occur if concerted interdisciplinary efforts are not pursued.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 23:16:37 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Nurse", "Jason R. C.", ""]]}, {"id": "1811.06722", "submitter": "Manuel Aiple", "authors": "Manuel Aiple, Jan Smisek, and Andre Schiele", "title": "Increasing Impact by Mechanical Resonance for Teleoperated Hammering", "comments": "Copyright 2018 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": "10.1109/TOH.2018.2882401", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Series elastic actuators (SEAs) are interesting for usage in harsh\nenvironments as they are more robust than rigid actuators. This paper shows how\nSEAs can be used in teleoperation to increase output velocity in dynamic tasks.\nA first experiment is presented that tested human ability to achieve higher\nhammerhead velocities with a flexible hammer than with a rigid hammer, and to\nevaluate the influence of the resonance frequency. In this experiment, 13\nparticipants executed a hammering task in direct manipulation using flexible\nhammers in four conditions with resonance frequencies of 3.0 Hz to 9.9 Hz and\none condition with a rigid hammer. Then, a second experiment is presented that\ntested the ability of 32 participants to reproduce the findings of the first\nexperiment in teleoperated manipulation with different feedback conditions:\nwith visual and force feedback, without visual feedback, without force\nfeedback, and with a communication delay of 40 ms. The results indicate that\nhumans can exploit the mechanical resonance of a flexible system to at least\ndouble the output velocity without combined force and vision feedback. This is\nan unexpected result, allowing the design of simpler and more robust\nteleoperators for dynamic tasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 09:34:05 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Aiple", "Manuel", ""], ["Smisek", "Jan", ""], ["Schiele", "Andre", ""]]}, {"id": "1811.06858", "submitter": "Vincent Goudard", "authors": "Vincent Goudard (STMS)", "title": "John, the semi-conductor : a tool for comprovisation", "comments": null, "journal-ref": "Sandeep Bhagwati; Jean Bresson. International Conference on\n  Technologies for Music Notation and Representation (TENOR'18), May 2018,\n  Montr{\\'e}al, Canada. 2018, Proceedings of the 4th International Conference\n  on Technologies for Music Notation and Representation.\n  http://tenor-conference.org/", "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents \"John\", an open-source software designed to help\ncollective free improvisation. It provides generated screen-scores running on\ndistributed, reactive web-browsers. The musicians can then concurrently edit\nthe scores in their own browser. John is used by ONE, a septet playing\nimprovised electro-acoustic music with digital musical instruments (DMI). One\nof the original features of John is that its design takes care of leaving the\nmusician's attention as free as possible. Firstly, a quick review of the\ncontext of screen-based scores will help situate this research in the history\nof contemporary music notation. Then I will trace back how improvisation\nsessions led to John's particular \"notational perspective\". A brief description\nof the software will precede a discussion about the various aspects guiding its\ndesign.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:31:12 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Goudard", "Vincent", "", "STMS"]]}, {"id": "1811.06874", "submitter": "Manuel Zierl", "authors": "Manuel Zierl", "title": "Wing Expansion Menu - An approach for faster and more precise navigation\n  with cascading pull-down menus", "comments": "4 Pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a new design suggestion for cascading pull-down menus to\nmake user interaction with it faster and therefore easier: The Wing Expansion\nMenu (WEM). The proposal is based on the Steering Law, which implies a wider\nsteering path for menu items. Our Approach combines this enlargement with a\nheuristic function that provides a probability with which the user will select\nan menu item. The menu can also be adapted to a wide variety of situations\nusing certain variables. A user study of a WEM against a standard pull-down\nmenu showed an average improvement of 18.63% in user interaction speed. A\nsecond user study, which evaluated one of the significant innovations of the\nWEM compared to a similar approach, showed an average improvement of 7.01% in\nuser interaction speed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 15:47:57 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Zierl", "Manuel", ""]]}, {"id": "1811.06962", "submitter": "Fernando De Mesentier Silva", "authors": "Fernando de Mesentier Silva, Igor Borovikov, John Kolen, Navid Aghdaie\n  and Kazi Zaman", "title": "Exploring Gameplay With AI Agents", "comments": "8 pages, 5 images. Published on The 14th AAAI Conference on\n  Artificial Intelligence and Interactive Digital Entertainment", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of playtesting a game is subjective, expensive and incomplete. In\nthis paper, we present a playtesting approach that explores the game space with\nautomated agents and collects data to answer questions posed by the designers.\nRather than have agents interacting with an actual game client, this approach\nrecreates the bare bone mechanics of the game as a separate system. Our agent\nis able to play in minutes what would take testers days of organic gameplay.\nThe analysis of thousands of game simulations exposed imbalances in game\nactions, identified inconsequential rewards and evaluated the effectiveness of\noptional strategic choices. Our test case game, The Sims Mobile, was recently\nreleased and the findings shown here influenced design changes that resulted in\nimproved player experience.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 18:37:25 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Silva", "Fernando de Mesentier", ""], ["Borovikov", "Igor", ""], ["Kolen", "John", ""], ["Aghdaie", "Navid", ""], ["Zaman", "Kazi", ""]]}, {"id": "1811.07162", "submitter": "Yang Xu", "authors": "Yang Xu, Min Chen, Wei Yang, Sheng Chen, Liusheng Huang", "title": "Attention-based Walking Gait and Direction Recognition in Wi-Fi Networks", "comments": "12 pages,12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of human gait recognition has been becoming an active research\nfield. In this paper, we propose to adopt the attention-based Recurrent Neural\nNetwork (RNN) encoder-decoder framework to implement a cycle-independent human\ngait and walking direction recognition system in Wi-Fi networks. For capturing\nmore human walking dynamics, two receivers together with one transmitter are\ndeployed in different spatial layouts. In the proposed system, the Channel\nState Information (CSI) measurements from different receivers are first\ngathered together and refined to form an integrated walking profile. Then, the\nRNN encoder reads and encodes the walking profile into primary feature vectors.\nGiven a specific recognition task, the decoder computes a corresponding\nattention vector which is a weighted sum of the primary features assigned with\ndifferent attentions, and is finally used to predict the target. The attention\nscheme motivates our system to learn to adaptively align with different\ncritical clips of CSI data sequence for human walking gait and direction\nrecognitions. We implement our system on commodity Wi-Fi devices in indoor\nenvironment, and the experimental results demonstrate that our system can\nachieve average F1 scores of 89.69% for gait recognition from a group of 8\nsubjects and 95.06% for direction recognition from 8 walking directions, in\naddition, the average accuracies of these two recognition tasks both exceed\n97%.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 14:06:15 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 05:46:33 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 04:04:34 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Xu", "Yang", ""], ["Chen", "Min", ""], ["Yang", "Wei", ""], ["Chen", "Sheng", ""], ["Huang", "Liusheng", ""]]}, {"id": "1811.07206", "submitter": "Neha Baranwal", "authors": "Neha Baranwal", "title": "On Human Robot Interaction using Multiple Modes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Humanoid robots have apparently similar body structure like human beings. Due\nto their technical design, they are sharing the same workspace with humans.\nThey are placed to clean things, to assist old age people, to entertain us and\nmost importantly to serve us. To be acceptable in the household, they must have\nhigher level of intelligence than industrial robots and they must be social and\ncapable of interacting people around it, who are not supposed to be robot\nspecialist. All these come under the field of human robot interaction (HRI).\nThere are various modes like speech, gesture, behavior etc. through which human\ncan interact with robots. To solve all these challenges, a multimodel technique\nhas been introduced where gesture as well as speech is used as a mode of\ninteraction.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 18:28:44 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Baranwal", "Neha", ""]]}, {"id": "1811.07243", "submitter": "Samuel Gomes", "authors": "Samuel Gomes, Carlos Martinho, Jo\\~ao Dias", "title": "Dynamic Social Interaction Mechanics CrossAnt", "comments": "5 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, big effort is being put to study gamification and how game elements\ncan be used to engage players. In this scope, we believe there is a growing\nneed to explore the impact game mechanics have on the players' interactions and\nperception. This work focuses on the application of game mechanics to lead\nplayers to achieve certain types of social interaction (we named this type of\nmechanics social interaction mechanics). A word matching game called CrossAnt\nwas modified so that it could dynamically generate different social interaction\nmechanics. These mechanics consisted in different key combinations needed to\nplay the game and were aimed to promote what we think are three important types\nof social interactions: cooperation, competition and individual exploration.\nOur evaluation consisted on the execution of several sessions where two players\ninteracted with the game for several levels and had to find for themselves how\nto perform the actions needed to succeed. While some of the levels required the\ninput from both players in order to be completed, others could be completed by\neach player independently. Our results show that cooperation was perceived when\nboth players had to intervene to perform the game actions. However, longer\ninteractions may still be needed so that the other types of interactions are\npromoted.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 23:13:12 GMT"}, {"version": "v2", "created": "Sun, 24 Mar 2019 18:02:00 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Gomes", "Samuel", ""], ["Martinho", "Carlos", ""], ["Dias", "Jo\u00e3o", ""]]}, {"id": "1811.07271", "submitter": "Michael Correll", "authors": "Michael Correll", "title": "Ethical Dimensions of Visualization Research", "comments": null, "journal-ref": null, "doi": "10.1145/3290605.3300418", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visualizations have a potentially enormous influence on how data are used to\nmake decisions across all areas of human endeavor. However, it is not clear how\nthis power connects to ethical duties: what obligations do we have when it\ncomes to visualizations and visual analytics systems, beyond our duties as\nscientists and engineers? Drawing on historical and contemporary examples, I\naddress the moral components of the design and use of visualizations, identify\nsome ongoing areas of visualization research with ethical dilemmas, and propose\na set of additional moral obligations that we have as designers, builders, and\nresearchers of visualizations.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 04:37:32 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 23:20:03 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Correll", "Michael", ""]]}, {"id": "1811.07273", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Stephen White", "title": "Design and Assessment for Hybrid Courses: Insights and Overviews", "comments": null, "journal-ref": "International Journal of Advances in Life Sciences (2015),\n  vol.7(3), pp.122-131", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology is influencing education, providing new delivery and assessment\nmodels. A combination between online and traditional course, the hybrid\n(blended) course, may present a solution with many benefits as it provides a\ngradual transition towards technology enabled education. This research work\nprovides a set of definitions for several course delivery approaches, and\nevaluates five years of data from a course that has been converted from\ntraditional face-to-face delivery, to hybrid delivery. The collected\nexperimental data proves that the revised course, in the hybrid delivery mode,\nis at least as good, if not better, than it previously was and it provides some\nbenefits in terms of student retention.\n", "versions": [{"version": "v1", "created": "Sun, 18 Nov 2018 04:59:38 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["White", "Stephen", ""]]}, {"id": "1811.07463", "submitter": "Felix Hamza-Lup", "authors": "Priya T. Goeser, Felix G. Hamza-Lup, Wayne M. Johnson, Dirk Scharfer", "title": "VIEW: A Virtual Interactive Web-based Learning Environment for\n  Engineering", "comments": null, "journal-ref": "IEEE Advances in Engineering Education Journal, Special Issue on\n  Research on e-Learning in Engineering Education (2011), Vol. 2(3), pp. 1-24", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of computer-aided and web-based educational technologies such as\nVirtual Learning Environments (VLE) has increased significantly in the recent\npast. One example of such a VLE is Virtual Interactive Engineering on the Web\n(VIEW). VIEW is a 3D virtual, interactive, student centered, framework of\nweb-based modules based on the Extensible 3D standard. These modules are\ndedicated to the improvement of student success and learning. In this paper, an\noverview of the recent developments in VIEW along with associated assessment\nresults is presented. An experimental study was also performed to compare the\nlearning experience and performance of students in a physical dissection\nactivity vs. that in a virtual dissection activity using a VIEW module. The\nresults of this study show that students can meet given learning objectives and\nthat there is limited difference in their learning and performance irrespective\nof a physical or virtual setting.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:22:10 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Goeser", "Priya T.", ""], ["Hamza-Lup", "Felix G.", ""], ["Johnson", "Wayne M.", ""], ["Scharfer", "Dirk", ""]]}, {"id": "1811.07473", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Dorin M. Popovici, Crenguta M. Bogdan", "title": "Haptic Feedback Systems in Medical Education", "comments": null, "journal-ref": "Journal of Advanced Distributed Learning Technology (2013), Vol.\n  1(2), pp. 7-16", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper brings into discussion some of the most relevant technological\nchallenges involving haptic systems in medical education. One of these\nchallenges is choosing the suitable haptic hardware, API or framework for\ndeveloping a visuo-haptic e-Learning system. The decision is based on several\ncriteria such as the multimodal resources needed by the software system,\ncompatibility with haptic devices and the dynamic configuration of the scene.\nAnother challenge is related to the software system reactivity in conjunction\nwith the user's actions. The immediate haptic feedback from the virtual models,\ntogether with the synchronization of the rendered haptic and visual cues seen\nby the users are essential for enhancing the user's learning ability.\nVisuo-haptic simulation facilitates accurate training scenarios of medical\nprotocols and surgical processes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 02:56:02 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Popovici", "Dorin M.", ""], ["Bogdan", "Crenguta M.", ""]]}, {"id": "1811.07547", "submitter": "Sumit Shekhar", "authors": "Sumit Shekhar, Aditya Siddhant, Anindya Shankar Bhandari, Nishant\n  Yadav", "title": "VoCoG: An Intelligent, Non-Intrusive Assistant for Voice-based\n  Collaborative Group-Viewing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There have been significant innovations in media technologies in the recent\nyears. While these developments have improved experiences for individual users,\ndesign of multi-user interfaces still remains a challenge. A relatively\nunexplored area in this context, is enabling multiple users to enjoy shared\nviewing (e.g. deciding on movies to watch together). In particular, the\nchallenge is to design an intelligent system which would enable viewers to\nexplore together shows or movies they like, seamlessly. This is a complex\ndesign problem, as it requires the system to (i) assess affinities of\nindividual users (movies or genres), (ii) combine individual preferences taking\ninto account user-user interactions, and (iii) be non-intrusive simultaneously.\nThe proposed system VoCoG, is an end-to-end intelligent system for\ncollaborative viewing. VoCoG incorporates an online recommendation algorithm,\nefficient methods for analyzing natural conversation and a graph-based method\nto fuse preferences of multiple users. It takes user conversation as input,\nmaking it non-intrusive. A usability survey of the system indicates that the\nsystem provides a good experience to the users as well as relevant\nrecommendations. Further analysis of the usage data reveals insights about the\nnature of conversation during the interaction sessions, final consensus among\nthe users as well as ratings of varied user groups.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:16:13 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Shekhar", "Sumit", ""], ["Siddhant", "Aditya", ""], ["Bhandari", "Anindya Shankar", ""], ["Yadav", "Nishant", ""]]}, {"id": "1811.07690", "submitter": "Xiaogang Cheng", "authors": "Bin Yang, Xiaogang Cheng, Dengxin Dai, Thomas Olofsson, Haibo Li, Alan\n  Meier", "title": "Macro pose based non-invasive thermal comfort perception for energy\n  efficiency", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual thermal comfort perception gives important feedback signals for\nenergy efficient control of smart buildings. However, there is no effective\nmethod to measure real-time thermal comfort status of individual occupant until\nnow. For overcoming this challenge, a novel macro posed based non-invasive\nperception method for thermal comfort (NIMAP) was presented. The occupant pose\nimages were captured by normal phone camera (computer or cell phone) and the\ncorresponding 2D coordinates can be obtained. Based on this, a novel pose\nrecognition algorithm for thermal comfort, including 12 sub-algorithms, was\npresented. The 12 thermal comfort related macro poses can be recognized.\nFurther, based on Fanger theory, 369 subjects were invited for subjective\nquestionnaire survey. 3 human occupants participated in the validation of the\nproposed method and massive data were collected. All the 12 thermal comfort\nrelated poses can be recognized effectively.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:40:45 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 16:01:44 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Yang", "Bin", ""], ["Cheng", "Xiaogang", ""], ["Dai", "Dengxin", ""], ["Olofsson", "Thomas", ""], ["Li", "Haibo", ""], ["Meier", "Alan", ""]]}, {"id": "1811.07691", "submitter": "Zheng Lian", "authors": "Zheng Lian, Ya Li, Jianhua Tao, Jian Huang", "title": "Improving speech emotion recognition via Transformer-based Predictive\n  Coding through transfer learning", "comments": "I have submitted a new version to arXiv:1910.13806. I forget to\n  choose to replace the old version, but submitted a new one. It's my mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I have submitted a new version to arXiv:1910.13806. I forget to choose to\nreplace the old version, but submitted a new one. It's my mistake.\n", "versions": [{"version": "v1", "created": "Sun, 11 Nov 2018 15:39:13 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 01:15:22 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Lian", "Zheng", ""], ["Li", "Ya", ""], ["Tao", "Jianhua", ""], ["Huang", "Jian", ""]]}, {"id": "1811.07882", "submitter": "John Co-Reyes", "authors": "John D. Co-Reyes, Abhishek Gupta, Suvansh Sanjeev, Nick Altieri, Jacob\n  Andreas, John DeNero, Pieter Abbeel, Sergey Levine", "title": "Guiding Policies with Language via Meta-Learning", "comments": "Accepted at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavioral skills or policies for autonomous agents are conventionally\nlearned from reward functions, via reinforcement learning, or from\ndemonstrations, via imitation learning. However, both modes of task\nspecification have their disadvantages: reward functions require manual\nengineering, while demonstrations require a human expert to be able to actually\nperform the task in order to generate the demonstration. Instruction following\nfrom natural language instructions provides an appealing alternative: in the\nsame way that we can specify goals to other humans simply by speaking or\nwriting, we would like to be able to specify tasks for our machines. However, a\nsingle instruction may be insufficient to fully communicate our intent or, even\nif it is, may be insufficient for an autonomous agent to actually understand\nhow to perform the desired task. In this work, we propose an interactive\nformulation of the task specification problem, where iterative language\ncorrections are provided to an autonomous agent, guiding it in acquiring the\ndesired skill. Our proposed language-guided policy learning algorithm can\nintegrate an instruction and a sequence of corrections to acquire new skills\nvery quickly. In our experiments, we show that this method can enable a policy\nto follow instructions and corrections for simulated navigation and\nmanipulation tasks, substantially outperforming direct, non-interactive\ninstruction following.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 18:58:42 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 18:54:15 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Co-Reyes", "John D.", ""], ["Gupta", "Abhishek", ""], ["Sanjeev", "Suvansh", ""], ["Altieri", "Nick", ""], ["Andreas", "Jacob", ""], ["DeNero", "John", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1811.08006", "submitter": "Xiaogang Cheng", "authors": "Xiaogang Cheng, Bin Yang, Anders Hedman, Thomas Olofsson, Haibo Li,\n  Luc Van Gool", "title": "Non-invasive thermal comfort perception based on subtleness\n  magnification and deep learning for energy efficiency", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human thermal comfort measurement plays a critical role in giving feedback\nsignals for building energy efficiency. A non-invasive measuring method based\non subtleness magnification and deep learning (NIDL) was designed to achieve a\ncomfortable, energy efficient built environment. The method relies on skin\nfeature data, e.g., subtle motion and texture variation, and a 315-layer deep\nneural network for constructing the relationship between skin features and skin\ntemperature. A physiological experiment was conducted for collecting feature\ndata (1.44 million) and algorithm validation. The non-invasive measurement\nalgorithm based on a partly-personalized saturation temperature model (NIPST)\nwas used for algorithm performance comparisons. The results show that the mean\nerror and median error of the NIDL are 0.4834 Celsius and 0.3464 Celsius which\nis equivalent to accuracy improvements of 16.28% and 4.28%, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 18:24:48 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Cheng", "Xiaogang", ""], ["Yang", "Bin", ""], ["Hedman", "Anders", ""], ["Olofsson", "Thomas", ""], ["Li", "Haibo", ""], ["Van Gool", "Luc", ""]]}, {"id": "1811.08127", "submitter": "Alireza Abedin Varamin", "authors": "Alireza Abedin Varamin, Ehsan Abbasnejad, Qinfeng Shi, Damith\n  Ranasinghe, Hamid Rezatofighi", "title": "Deep Auto-Set: A Deep Auto-Encoder-Set Network for Activity Recognition\n  Using Wearables", "comments": "Accepted at MobiQuitous 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic recognition of human activities from time-series sensor data\n(referred to as HAR) is a growing area of research in ubiquitous computing.\nMost recent research in the field adopts supervised deep learning paradigms to\nautomate extraction of intrinsic features from raw signal inputs and addresses\nHAR as a multi-class classification problem where detecting a single activity\nclass within the duration of a sensory data segment suffices. However, due to\nthe innate diversity of human activities and their corresponding duration, no\ndata segment is guaranteed to contain sensor recordings of a single activity\ntype. In this paper, we express HAR more naturally as a set prediction problem\nwhere the predictions are sets of ongoing activity elements with unfixed and\nunknown cardinality. For the first time, we address this problem by presenting\na novel HAR approach that learns to output activity sets using deep neural\nnetworks. Moreover, motivated by the limited availability of annotated HAR\ndatasets as well as the unfortunate immaturity of existing unsupervised\nsystems, we complement our supervised set learning scheme with a prior\nunsupervised feature learning process that adopts convolutional auto-encoders\nto exploit unlabeled data. The empirical experiments on two widely adopted HAR\ndatasets demonstrate the substantial improvement of our proposed methodology\nover the baseline models.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 08:54:31 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Varamin", "Alireza Abedin", ""], ["Abbasnejad", "Ehsan", ""], ["Shi", "Qinfeng", ""], ["Ranasinghe", "Damith", ""], ["Rezatofighi", "Hamid", ""]]}, {"id": "1811.08244", "submitter": "Irene Celino", "authors": "Gloria Re Calegari and Irene Celino", "title": "Interplay of Game Incentives, Player Profiles and Task Difficulty in\n  Games with a Purpose", "comments": "15 pages, 8 figures, 21st International Conference on Knowledge\n  Engineering and Knowledge Management (EKAW 2018)", "journal-ref": "LNAI Volume 11313 (2018), pp. 306-321", "doi": "10.1007/978-3-030-03667-6_20", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to take multiple factors into account when evaluating a Game with a\nPurpose? How is player behaviour or participation influenced by different\nincentives? How does player engagement impact their accuracy in solving tasks?\nIn this paper, we present a detailed investigation of multiple factors\naffecting the evaluation of a GWAP and we show how they impact on the achieved\nresults. We inform our study with the experimental assessment of a GWAP\ndesigned to solve a multinomial classification task.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 13:42:38 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Calegari", "Gloria Re", ""], ["Celino", "Irene", ""]]}, {"id": "1811.08374", "submitter": "Md Mofijul Islam", "authors": "Md Mofijul Islam, Amar Debnath, Tahsin Al Sayeed, Jyotirmay Nag Setu,\n  Md Mahmudur Rahman, Md Sadman Sakib, Md Abdur Razzaque, Md. Mosaddek Khan,\n  Swakkhar Shatabda", "title": "A Gray Box Interpretable Visual Debugging Approach for Deep Sequence\n  Learning Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning algorithms are often used as black box type learning and they\nare too complex to understand. The widespread usability of Deep Learning\nalgorithms to solve various machine learning problems demands deep and\ntransparent understanding of the internal representation as well as decision\nmaking. Moreover, the learning models, trained on sequential data, such as\naudio and video data, have intricate internal reasoning process due to their\ncomplex distribution of features. Thus, a visual simulator might be helpful to\ntrace the internal decision making mechanisms in response to adversarial input\ndata, and it would help to debug and design appropriate deep learning models.\nHowever, interpreting the internal reasoning of deep learning model is not well\nstudied in the literature. In this work, we have developed a visual interactive\nweb application, namely d-DeVIS, which helps to visualize the internal\nreasoning of the learning model which is trained on the audio data. The\nproposed system allows to perceive the behavior as well as to debug the model\nby interactively generating adversarial audio data point. The web application\nof d-DeVIS is available at ddevis.herokuapp.com.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 17:13:49 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Islam", "Md Mofijul", ""], ["Debnath", "Amar", ""], ["Sayeed", "Tahsin Al", ""], ["Setu", "Jyotirmay Nag", ""], ["Rahman", "Md Mahmudur", ""], ["Sakib", "Md Sadman", ""], ["Razzaque", "Md Abdur", ""], ["Khan", "Md. Mosaddek", ""], ["Shatabda", "Swakkhar", ""]]}, {"id": "1811.08460", "submitter": "Ruba Abu-Salma", "authors": "Ruba Abu-Salma, Benjamin Livshits", "title": "Evaluating the End-User Experience of Private Browsing Mode", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, all major web browsers have a private browsing mode. However, the\nmode's benefits and limitations are not particularly understood. Through the\nuse of survey studies, prior work has found that most users are either unaware\nof private browsing or do not use it. Further, those who do use private\nbrowsing generally have misconceptions about what protection it provides.\nHowever, prior work has not investigated \\emph{why} users misunderstand the\nbenefits and limitations of private browsing. In this work, we do so by\ndesigning and conducting a three-part study: (1) an analytical approach\ncombining cognitive walkthrough and heuristic evaluation to inspect the user\ninterface of private mode in different browsers; (2) a qualitative,\ninterview-based study to explore users' mental models of private browsing and\nits security goals; (3) a participatory design study to investigate why\nexisting browser disclosures, the in-browser explanations of private browsing\nmode, do not communicate the security goals of private browsing to users.\nParticipants critiqued the browser disclosures of three web browsers: Brave,\nFirefox, and Google Chrome, and then designed new ones. We find that the user\ninterface of private mode in different web browsers violates several\nwell-established design guidelines and heuristics. Further, most participants\nhad incorrect mental models of private browsing, influencing their\nunderstanding and usage of private mode. Additionally, we find that existing\nbrowser disclosures are not only vague, but also misleading. None of the three\nstudied browser disclosures communicates or explains the primary security goal\nof private browsing. Drawing from the results of our user study, we extract a\nset of design recommendations that we encourage browser designers to validate,\nin order to design more effective and informative browser disclosures related\nto private mode.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 19:44:00 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 14:52:19 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Abu-Salma", "Ruba", ""], ["Livshits", "Benjamin", ""]]}, {"id": "1811.08639", "submitter": "Yaohua Xie", "authors": "Yaohua Xie, Danli Wang, Fang Sun", "title": "How visual discomfort changes with horizontal viewing angle on\n  stereoscopic display", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When viewing stereoscopic displays, people may not always be able to stay\nexactly in front of the display. It is known that viewing stereoscopic display\nfrom different vertical angles lead to different visual discomfort. However,\nthe effects of horizontal viewing angle on stereoscopic visual discomfort have\nbeen rarely investigated, especially for household stereoscopic displays. In\nthis study, subjects were required to view a stereoscopic display from various\nhorizontal viewing angles, and assessed their visual discomfort during viewing.\nThe visual stimuli have various amount of disparities: positive disparity,\nnegative disparity or zero disparity. Results showed that the visual discomfort\nchanges with horizontal viewing angle, and greater angles generally lead to\nmore serious visual discomfort. Furthermore, the relationship between visual\ndiscomfort and horizontal viewing angle can be approximately expressed by a\nquadratic function.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 09:01:17 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Xie", "Yaohua", ""], ["Wang", "Danli", ""], ["Sun", "Fang", ""]]}, {"id": "1811.08762", "submitter": "Guy Andre Boy", "authors": "Guy Andre Boy, Wei Tan", "title": "Tablet-based Information System for Commercial Air-craft: Onboard\n  Context-Sensitive Information System (OCSIS)", "comments": null, "journal-ref": "Proceedings of HCI International 2018, Jul 2018, Orlando, United\n  States", "doi": null, "report-no": null, "categories": "cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pilots currently use paper-based documentation and electronic systems to help\nthem perform procedures to ensure safety, efficiency and comfort on commercial\naircrafts. Management of interconnections among paper-based operational\ndocuments can be a challenge for pilots, especially when time pressure is high\nin normal, abnormal, and emergency situations. This dissertation is a\ncontribution to the design of an Onboard Context-Sensitive Information System\n(OCSIS), which was developed on a tablet. The claim is that the use of\ncon-textual information facilitates access to appropriate operational content\nat the right time either automatically or on demand. OCSIS was tested using\nhuman-in-the-loop simulations that involved professional pilots in the Airbus\n320 cockpit simulator. First results are encouraging that show OCSIS can be\nusable and useful for operational information access. More specifically,\ncontext-sensitivity contributes to simplify this access (i.e., appropriate\noperational information is provided at the right time in the right format. In\naddition, OCSIS provides other features that paper-based documents do not have,\nsuch as procedure execution status after an interruption. Also, the fact that\nseveral calculations are automatically done by OCSIS tends to decrease the\npilot's task demand .\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 14:55:03 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Boy", "Guy Andre", ""], ["Tan", "Wei", ""]]}, {"id": "1811.08833", "submitter": "Felix Hamza-Lup", "authors": "Jannick Rolland, Ozan Cakmakci, Jeff Covelli, Cali Fidopiastis,\n  Florian Fournier, Ricardo Martins, Felix G. Hamza-Lup, Denise Nicholson", "title": "Beyond the Desktop: Emerging Technologies for Supporting 3D\n  Collaborative Teams", "comments": null, "journal-ref": "International Journal on Interactive Design and Manufacturing\n  (2007), Vol. 4(1), pp. 239-241", "doi": "10.1007/s12008-007-0027-z", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of several trends, including the increased availability of\nwireless networks, miniaturization of electronics and sensing technologies, and\nnovel input and output devices, is creating a demand for integrated, full-time\ndisplays for use across a wide range of applications, including collaborative\nenvironments. In this paper, we present and discuss emerging visualization\nmethods we are developing particularly as they relate to deployable displays\nand displays worn on the body to support mobile users.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 06:00:02 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Rolland", "Jannick", ""], ["Cakmakci", "Ozan", ""], ["Covelli", "Jeff", ""], ["Fidopiastis", "Cali", ""], ["Fournier", "Florian", ""], ["Martins", "Ricardo", ""], ["Hamza-Lup", "Felix G.", ""], ["Nicholson", "Denise", ""]]}, {"id": "1811.08854", "submitter": "Philip Schmidt", "authors": "Philip Schmidt, Attila Reiss, Robert Duerichen, Kristof Van Laerhoven", "title": "Wearable affect and stress recognition: A review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Affect recognition aims to detect a person's affective state based on\nobservables, with the goal to e.g. provide reasoning for decision making or\nsupport mental wellbeing. Recently, besides approaches based on audio, visual\nor text information, solutions relying on wearable sensors as observables\n(recording mainly physiological and inertial parameters) have received\nincreasing attention. Wearable systems offer an ideal platform for long-term\naffect recognition applications due to their rich functionality and form\nfactor. However, existing literature lacks a comprehensive overview of\nstate-of-the-art research in wearable-based affect recognition. Therefore, the\naim of this paper is to provide a broad overview and in-depth understanding of\nthe theoretical background, methods, and best practices of wearable affect and\nstress recognition. We summarise psychological models, and detail\naffect-related physiological changes and their measurement with wearables. We\noutline lab protocols eliciting affective states, and provide guidelines for\nground truth generation in field studies. We also describe the standard data\nprocessing chain, and review common approaches to preprocessing, feature\nextraction, and classification. By providing a comprehensive summary of the\nstate-of-the-art and guidelines to various aspects, we would like to enable\nother researchers in the field of affect recognition to conduct and evaluate\nuser studies and develop wearable systems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 18:00:08 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Schmidt", "Philip", ""], ["Reiss", "Attila", ""], ["Duerichen", "Robert", ""], ["Van Laerhoven", "Kristof", ""]]}, {"id": "1811.09381", "submitter": "Isidoros Rodomagoulakis", "authors": "Isidoros Rodomagoulakis and Petros Maragos", "title": "Improved Frequency Modulation Features for Multichannel Distant Speech\n  Recognition", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2019.2923372", "report-no": null, "categories": "cs.SD cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequency modulation features capture the fine structure of speech formants\nthat constitute beneficial and supplementary to the traditional energy-based\ncepstral features. Improvements have been demonstrated mainly in GMM-HMM\nsystems for small and large vocabulary tasks. Yet, they have limited\napplications in DNN-HMM systems and Distant Speech Recognition (DSR) tasks.\nHerein, we elaborate on their integration within state-of-the-art front-end\nschemes that include post-processing of MFCCs resulting in discriminant and\nspeaker adapted features of large temporal contexts. We explore 1) multichannel\ndemodulation schemes for multi-microphone setups, 2) richer descriptors of\nfrequency modulations, and 3) feature transformation and combination via\nhierarchical deep networks. We present results for tandem and hybrid\nrecognition with GMM and DNN acoustic models, respectively. The improved\nmodulation features are combined efficiently with MFCCs yielding modest and\nconsistent improvements in multichannel distant speech recognition tasks on\nreverberant and noisy environments, where recognition rates are far from human\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 07:54:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Rodomagoulakis", "Isidoros", ""], ["Maragos", "Petros", ""]]}, {"id": "1811.09461", "submitter": "Michael Gygli", "authors": "Michael Gygli, Vittorio Ferrari", "title": "Fast Object Class Labelling via Speech", "comments": "to be published at CVPR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object class labelling is the task of annotating images with labels on the\npresence or absence of objects from a given class vocabulary. Simply asking one\nyes/no question per class, however, has a cost that is linear in the vocabulary\nsize and is thus inefficient for large vocabularies. Modern approaches rely on\na hierarchical organization of the vocabulary to reduce annotation time, but\nremain expensive (several minutes per image for the 200 classes in ILSVRC).\nInstead, we propose a new interface where classes are annotated via speech.\nSpeaking is fast and allows for direct access to the class name, without\nsearching through a list or hierarchy. As additional advantages, annotators can\nsimultaneously speak and scan the image for objects, the interface can be kept\nextremely simple, and using it requires less mouse movement. As annotators\nusing our interface should only say words from a given class vocabulary, we\npropose a dedicated task that trains them to do so. Through experiments on COCO\nand ILSVRC, we show our method yields high-quality annotations at 2.3x - 14.9x\nless annotation time than existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 13:08:39 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 15:33:27 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Gygli", "Michael", ""], ["Ferrari", "Vittorio", ""]]}, {"id": "1811.09688", "submitter": "Haruna Isah", "authors": "Mandeep Singh Kandhari, Farhana Zulkernine, Haruna Isah", "title": "A Voice Controlled E-Commerce Web Application", "comments": "7 pages", "journal-ref": null, "doi": "10.1109/IEMCON.2018.8614771", "report-no": null, "categories": "cs.CY cs.CL cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic voice-controlled systems have changed the way humans interact with\na computer. Voice or speech recognition systems allow a user to make a\nhands-free request to the computer, which in turn processes the request and\nserves the user with appropriate responses. After years of research and\ndevelopments in machine learning and artificial intelligence, today\nvoice-controlled technologies have become more efficient and are widely applied\nin many domains to enable and improve human-to-human and human-to-computer\ninteractions. The state-of-the-art e-commerce applications with the help of web\ntechnologies offer interactive and user-friendly interfaces. However, there are\nsome instances where people, especially with visual disabilities, are not able\nto fully experience the serviceability of such applications. A voice-controlled\nsystem embedded in a web application can enhance user experience and can\nprovide voice as a means to control the functionality of e-commerce websites.\nIn this paper, we propose a taxonomy of speech recognition systems (SRS) and\npresent a voice-controlled commodity purchase e-commerce application using IBM\nWatson speech-to-text to demonstrate its usability. The prototype can be\nextended to other application scenarios such as government service kiosks and\nenable analytics of the converted text data for scenarios such as medical\ndiagnosis at the clinics.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 01:35:09 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Kandhari", "Mandeep Singh", ""], ["Zulkernine", "Farhana", ""], ["Isah", "Haruna", ""]]}, {"id": "1811.09776", "submitter": "Heinrich S\\\"obke", "authors": "Heinrich S\\\"obke and Maria Reichelt", "title": "Sewer Rats in Teaching Action: An explorative field study on students'\n  perception of a game-based learning app in graduate engineering education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game-based technologies and mobile learning aids open up many opportunities\nfor learners; however, evidence-based decisions on their appropriate use are\nnecessary. This explorative study (N = 100) examines the role of game elements\nin university education using a game-based learning app for mobile devices. The\neducational goal of the app is to support students in the field of engineering\nto memorize factual knowledge. The study investigates how the game-based app\naffects learners' motivation. It analyses the perceived impact and appeal as\nwell as the game elements as an incentive in learners' perception. To realize\nthis aim, the study combines structured methods like questionnaires with\nsemi-structured methods like thinking aloud, game diaries, and interviews. The\nresults indicate that flexible tem-poral and spatial use of the app was an\nimportant factor of learners' motivation. The app allowed more spontaneous\ninvolvement with the subject matter and the learners took advantage of an\nimproved attitude toward the subject matter. However, only a low impact on\nintrinsic motivation could be observed. We discuss reasons and present\npractical implications.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 06:40:06 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["S\u00f6bke", "Heinrich", ""], ["Reichelt", "Maria", ""]]}, {"id": "1811.10111", "submitter": "Abhay Koushik", "authors": "Abhay Koushik, Judith Amores and Pattie Maes", "title": "Real-Time Sleep Staging using Deep Learning on a Smartphone for a\n  Wearable EEG", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/114", "categories": "cs.HC cs.LG eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first real-time sleep staging system that uses deep learning\nwithout the need for servers in a smartphone application for a wearable EEG. We\nemploy real-time adaptation of a single channel Electroencephalography (EEG) to\ninfer from a Time-Distributed 1-D Deep Convolutional Neural Network.\nPolysomnography (PSG)-the gold standard for sleep staging, requires a human\nscorer and is both complex and resource-intensive. Our work demonstrates an\nend-to-end on-smartphone pipeline that can infer sleep stages in just single\n30-second epochs, with an overall accuracy of 83.5% on 20-fold cross validation\nfor five-class classification of sleep stages using the open Sleep-EDF dataset.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 22:25:31 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 02:30:23 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Koushik", "Abhay", ""], ["Amores", "Judith", ""], ["Maes", "Pattie", ""]]}, {"id": "1811.10120", "submitter": "Martin Weiss", "authors": "Martin Weiss, Margaux Luck, Roger Girgis, Chris Pal, Joseph Paul Cohen", "title": "A Survey of Mobile Computing for the Visually Impaired", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of visually impaired or blind (VIB) people in the world is\nestimated at several hundred million. Based on a series of interviews with the\nVIB and developers of assistive technology, this paper provides a survey of\nmachine-learning based mobile applications and identifies the most relevant\napplications. We discuss the functionality of these apps, how they align with\nthe needs and requirements of the VIB users, and how they can be improved with\ntechniques such as federated learning and model compression. As a result of\nthis study we identify promising future directions of research in mobile\nperception, micro-navigation, and content-summarization.\n", "versions": [{"version": "v1", "created": "Sun, 25 Nov 2018 23:58:42 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 19:02:35 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Weiss", "Martin", ""], ["Luck", "Margaux", ""], ["Girgis", "Roger", ""], ["Pal", "Chris", ""], ["Cohen", "Joseph Paul", ""]]}, {"id": "1811.10123", "submitter": "Ariel Noyman", "authors": "Ariel Noyman, Tobias Holtz, Johannes Kroger, Jorg Rainer Noennig, Kent\n  Larson", "title": "Finding Places: HCI Platform for Public Participation in Refugees\n  Accommodation Process", "comments": "Procedia Computer Science, 9 pages, 2 figures", "journal-ref": "Procedia Computer Science Volume 112, 2017, Pages 2463-2472", "doi": "10.1016/j.procs.2017.08.180", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the conception, development and deployment of a novel\nHCI system for public participation and decision making. This system was\napplied for the process of allocating refugee accommodation in the City of\nHamburg within the FindingPlaces project in 2016. The CityScope a rapid\nprototyping platform for urban planning and decision making offered a technical\nsolution which was complemented by a workshop process to facilitate effective\ninteraction of multiple participants and stakeholder groups. This paper\npresents the origins of CS and the evolution of the tangible user interface\napproach to urban planning and public participation. It further outlines\ntechnical features of the system, including custom hardware and software in\nuse, utilization in real time as well as technical constraints and limitations.\nSpecial focus is on the adaptation of the CS technology to the specific demands\nof Hamburg FP project, whose procedures, processes, and results are reflected.\nThe final section analyzes success factors as well as shortcomings of the\napproach, and indicates further R&D as well as application scenarios for the\nCS.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 00:11:14 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Noyman", "Ariel", ""], ["Holtz", "Tobias", ""], ["Kroger", "Johannes", ""], ["Noennig", "Jorg Rainer", ""], ["Larson", "Kent", ""]]}, {"id": "1811.10548", "submitter": "Adam Aviv", "authors": "Adam J. Aviv and Markus Duermuth", "title": "A Survey of Collection Methods and Cross-Data Set Comparison of Android\n  Unlock Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Android's graphical password unlock remains one of the most widely used\nschemes for phone unlock authentication, and it is has been studied extensively\nin the last decade since its launch. We have learned that users' choice of\npatterns mimics the poor password choices in other systems, such as PIN or\ntext-based passwords. A wide variety of analysis and data collections methods\nwas used to reach these conclusions, but what is missing from the literature is\na systemized comparison of the related work in this space that compares both\nthe methodology and the results. In this paper, we take a detailed accounting\nof the different methods applied to data collection and analysis for Android\nunlock patterns. We do so in two dimensions. First we systemize prior work into\na detailed taxonomy of collection methods, and in the second dimension, we\nperform a detailed analysis of 9 different data sets collected using different\nmethods. While this study focuses singularly on the collection methods and\ncomparisons of the Android pattern unlock scheme, we believe that many of the\nfindings generalize to other graphical password schemes, unlock authentication\ntechnology, and other knowledge-based authentication schemes.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 17:51:34 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Aviv", "Adam J.", ""], ["Duermuth", "Markus", ""]]}, {"id": "1811.10740", "submitter": "Subba Reddy Oota", "authors": "Subba Reddy Oota, Adithya Avvaru, Naresh Manwani, Raju S. Bapi", "title": "Mixture of Regression Experts in fMRI Encoding", "comments": "8 pages, 3 figures, Workshop on Visually Grounded Interaction and\n  Language @ 32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), Montr\\'eal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  fMRI semantic category understanding using linguistic encoding models attempt\nto learn a forward mapping that relates stimuli to the corresponding brain\nactivation. Classical encoding models use linear multi-variate methods to\npredict the brain activation (all voxels) given the stimulus. However, these\nmethods essentially assume multiple regions as one large uniform region or\nseveral independent regions, ignoring connections among them. In this paper, we\npresent a mixture of experts-based model where a group of experts captures\nbrain activity patterns related to particular regions of interest (ROI) and\nalso show the discrimination across different experts. The model is trained\nword stimuli encoded as 25-dimensional feature vectors as input and the\ncorresponding brain responses as output. Given a new word (25-dimensional\nfeature vector), it predicts the entire brain activation as the linear\ncombination of multiple experts brain activations. We argue that each expert\nlearns a certain region of brain activations corresponding to its category of\nwords, which solves the problem of identifying the regions with a simple\nencoding model. We showcase that proposed mixture of experts-based model indeed\nlearns region-based experts to predict the brain activations with high spatial\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 23:21:30 GMT"}, {"version": "v2", "created": "Sat, 1 Dec 2018 17:14:03 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Oota", "Subba Reddy", ""], ["Avvaru", "Adithya", ""], ["Manwani", "Naresh", ""], ["Bapi", "Raju S.", ""]]}, {"id": "1811.10815", "submitter": "EPTCS", "authors": "Jan Bessai (Technical University of Dortmund), Anna Vasileva\n  (Technical University of Dortmund)", "title": "User Support for the Combinator Logic Synthesizer Framework", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 16-25", "doi": "10.4204/EPTCS.284.2", "report-no": null, "categories": "cs.LO cs.FL cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usability is crucial for the adoption of software development technologies.\nThis is especially true in development stages, where build processes fail,\nbecause software is not yet complete or was incompletely modified. We present\nearly work that aims to improve usability of the Combinatory Logic Synthesizer\n(CL)S framework, especially in these stages. (CL)S is a publicly available\ntype-based development tool for the automatic composition of software\ncomponents from a user-specified repository. It provides an implementation of a\ntype inhabitation algorithm for Combinatory Logic with intersection types,\nwhich is fully integrated into the Scala programming language. Here, we\nspecifically focus on building a web-based IDE to make potentially incomplete\nor erroneous input specifications for and decisions of the algorithm\nunderstandable for non-experts. A main aspect of this is providing graphical\nrepresentations illustrating the step-wise search process of the algorithm. We\nalso provide a detailed discussion of possible future work to further improve\nthe understandability of these representations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:00:29 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Bessai", "Jan", "", "Technical University of Dortmund"], ["Vasileva", "Anna", "", "Technical University of Dortmund"]]}, {"id": "1811.10817", "submitter": "EPTCS", "authors": "Rui Couto (HASLab/INESC TEC & University of Minho), Jos\\'e C. Campos\n  (HASLab/INESC TEC & University of Minho), Nuno Macedo (HASLab/INESC TEC &\n  University of Minho), Alcino Cunha (HASLab/INESC TEC & University of Minho)", "title": "Improving the Visualization of Alloy Instances", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 37-52", "doi": "10.4204/EPTCS.284.4", "report-no": null, "categories": "cs.HC cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alloy is a lightweight formal specification language, supported by an IDE,\nwhich has proven well-suited for reasoning about software design in early\ndevelopment stages. The IDE provides a visualizer that produces graphical\nrepresentations of analysis results, which is essential for the proper\nvalidation of the model. Alloy is a rich language but inherently static, so\nbehavior needs to be explicitly encoded and reasoned about. Even though this is\na common scenario, the visualizer presents limitations when dealing with such\nmodels. The main contribution of this paper is a principled approach to\ngenerate instance visualizations, which improves the current Alloy Visualizer,\nfocusing on the representation of behavior.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:01:04 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Couto", "Rui", "", "HASLab/INESC TEC & University of Minho"], ["Campos", "Jos\u00e9 C.", "", "HASLab/INESC TEC & University of Minho"], ["Macedo", "Nuno", "", "HASLab/INESC TEC &\n  University of Minho"], ["Cunha", "Alcino", "", "HASLab/INESC TEC & University of Minho"]]}, {"id": "1811.10819", "submitter": "EPTCS", "authors": "Makarius Wenzel", "title": "Isabelle/jEdit as IDE for Domain-specific Formal Languages and Informal\n  Text Documents", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 71-84", "doi": "10.4204/EPTCS.284.6", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isabelle/jEdit is the main application of the Prover IDE (PIDE) framework and\nthe default user-interface of Isabelle, but it is not limited to theorem\nproving. This paper explores possibilities to use it as a general IDE for\nformal languages that are defined in user-space, and embedded into informal\ntext documents. It covers overall document structure with auxiliary files and\ndocument antiquotations, formal text delimiters and markers for interpretation\n(via control symbols). The ultimate question behind this: How far can we\nstretch a plain text editor like jEdit in order to support semantic text\nprocessing, with support by the underlying PIDE framework?\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:01:43 GMT"}], "update_date": "2018-12-02", "authors_parsed": [["Wenzel", "Makarius", ""]]}, {"id": "1811.10821", "submitter": "EPTCS", "authors": "Nathaniel Watson (Department of Computer Science, University of\n  Waikato, Hamilton, New Zealand), Steve Reeves (Department of Computer\n  Science, University of Waikato, Hamilton, New Zealand), Paolo Masci (High\n  Assurance Software Laboratory (HASLab), INESC TEC and Universidade do Minho,\n  Braga, Portugal)", "title": "Integrating User Design and Formal Models within PVSio-Web", "comments": "In Proceedings F-IDE 2018, arXiv:1811.09014", "journal-ref": "EPTCS 284, 2018, pp. 95-104", "doi": "10.4204/EPTCS.284.8", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating formal models of interactive systems has wide reaching benefits, not\nonly for verifying low-level correctness, but also as a tool for ensuring user\ninterfaces behave logically and consistently. Despite this, tools for designing\nuser experiences and tools for creating and working with formal models are\ntypically distinctly separate systems. This work aims to bridge this divide by\nallowing the generation of state machine diagrams and formal models via a\nsimple, interactive prototyping tool that mirrors the basic functionality of\nmany modern digital prototyping applications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 05:02:15 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Watson", "Nathaniel", "", "Department of Computer Science, University of\n  Waikato, Hamilton, New Zealand"], ["Reeves", "Steve", "", "Department of Computer\n  Science, University of Waikato, Hamilton, New Zealand"], ["Masci", "Paolo", "", "High\n  Assurance Software Laboratory"]]}, {"id": "1811.10921", "submitter": "Aritz Arrate Galan", "authors": "Aritz Arrate, Jos\\'e Gonz\\'alez Caba\\~nas, \\'Angel Cuevas, Mar\\'ia\n  Calder\\'on and Rub\\'en Cuevas", "title": "Large-scale analysis of user exposure to online advertising in Facebook", "comments": null, "journal-ref": "IEEE Access 7 (2021) 11959-11971", "doi": "10.1109/ACCESS.2019.2892237", "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online advertising is the major source of income for a large portion of\nInternet Services. There exists a body of literature aiming at optimizing ads\nengagement, understanding the privacy and ethical implications of online\nadvertising, etc. However, to the best of our knowledge, no previous work\nanalyses at large scale the exposure of real users to online advertising. This\npaper performs a comprehensive analysis of the exposure of users to ads and\nadvertisers using a dataset including more than 7M ads from 140K unique\nadvertisers delivered to more than 5K users that was collected between October\n2016 and May 2018. The study focuses on Facebook, which is the second largest\nadvertising platform only to Google in terms of revenue, and accounts for more\nthan 2.2B monthly active users. Our analysis reveals that Facebook users are\nexposed (in median) to 70 ads per week, which come from 12 advertisers. Ads\nrepresent between 10% and 15% of all the information received in users'\nnewsfeed. A small increment of 1% in the portion of ads in the newsfeed could\nroughly represent a revenue increase of 8.17M USD per week for Facebook.\nFinally, we also reveal that Facebook users are overprofiled since in the best\ncase only 22.76% of the interests Facebook assigns to users for advertising\npurpose are actually related to the ads those users receive.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 11:33:04 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 15:47:42 GMT"}, {"version": "v3", "created": "Wed, 26 Dec 2018 12:04:17 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Arrate", "Aritz", ""], ["Caba\u00f1as", "Jos\u00e9 Gonz\u00e1lez", ""], ["Cuevas", "\u00c1ngel", ""], ["Calder\u00f3n", "Mar\u00eda", ""], ["Cuevas", "Rub\u00e9n", ""]]}, {"id": "1811.10988", "submitter": "Xavier Favory", "authors": "Xavier Favory, Eduardo Fonseca, Frederic Font, Xavier Serra", "title": "Facilitating the Manual Annotation of Sounds When Using Large Taxonomies", "comments": "5 pages, 5 figures, IEEE FRUCT International Workshop on Semantic\n  Audio and the Internet of Things", "journal-ref": "Proceedings of the 23rd Conference of Open Innovations Association\n  FRUCT, Bologna, Italy. 2018. ISSN 2305-7254, ISBN 978-952-68653-6-2, FRUCT\n  Oy, e-ISSN 2343-0737 (license CC BY-ND)", "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Properly annotated multimedia content is crucial for supporting advances in\nmany Information Retrieval applications. It enables, for instance, the\ndevelopment of automatic tools for the annotation of large and diverse\nmultimedia collections. In the context of everyday sounds and online\ncollections, the content to describe is very diverse and involves many\ndifferent types of concepts, often organised in large hierarchical structures\ncalled taxonomies. This makes the task of manually annotating content arduous.\nIn this paper, we present our user-centered development of two tools for the\nmanual annotation of audio content from a wide range of types. We conducted a\npreliminary evaluation of functional prototypes involving real users. The goal\nis to evaluate them in a real context, engage in discussions with users, and\ninspire new ideas. A qualitative analysis was carried out including usability\nquestionnaires and semi-structured interviews. This revealed interesting\naspects to consider when developing tools for the manual annotation of audio\ncontent with labels drawn from large hierarchical taxonomies.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:43:11 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Favory", "Xavier", ""], ["Fonseca", "Eduardo", ""], ["Font", "Frederic", ""], ["Serra", "Xavier", ""]]}, {"id": "1811.11316", "submitter": "S\\'ergio Rebelo", "authors": "S\\'ergio Rebelo and Pedro Martins and Jo\\~ao Bicker and Penousal\n  Machado", "title": "Using Computer Vision Techniques for Moving Poster Design", "comments": "This paper will be published in the sixth International Conference\n  Ergotrip Design 29-30 November 2017, Aveiro, Portugal", "journal-ref": "REBELO, S\\'ergio et al. - Using Computer Vision Techniques for\n  Moving Poster Design. In Proceedings of sixth Ergotrip Design (ETD 17).\n  Aveiro, Portugal : Universidade de Aveiro, 2017", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphic Design encompasses a wide range of activities from the design of\ntraditional print media (e.g., books and posters) to site-specific (e.g.,\nsignage systems) and electronic media (e.g., interfaces). Its practice always\nexplores the new possibilities of information and communication technologies.\nTherefore, interactivity and participation have become key features in the\ndesign process. Even in traditional print media, graphic designers are trying\nto enhance user experience and exploring new interaction models. Moving posters\nare an example of this. This type of posters combine the specific features of\nmotion and print worlds in order to produce attractive forms of communication\nthat explore and exploit the potential of digital screens. In our opinion, the\nnext step towards the integration of moving posters with the surroundings,\nwhere they operate, is incorporating data from the environment, which also\nenables the seamless participation of the audience. As such, the adoption of\ncomputer vision techniques for moving poster design becomes a natural approach.\nFollowing this line of thought, we present a system wherein computer vision\ntechniques are used to shape a moving poster. Although it is still a work in\nprogress, the system is already able to sense the surrounding physical\nenvironment and translate the collected data into graphical information. The\ndata is gathered from the environment in two ways: (1) directly using motion\ntracking; and (2) indirectly via contextual ambient data. In this sense, each\nuser interaction with the system results in a different experience and in a\nunique poster design.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 23:59:46 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Rebelo", "S\u00e9rgio", ""], ["Martins", "Pedro", ""], ["Bicker", "Jo\u00e3o", ""], ["Machado", "Penousal", ""]]}, {"id": "1811.11839", "submitter": "Sina Mohseni", "authors": "Sina Mohseni and Niloofar Zarei and Eric D. Ragan", "title": "A Multidisciplinary Survey and Framework for Design and Evaluation of\n  Explainable AI Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for interpretable and accountable intelligent systems grows along\nwith the prevalence of artificial intelligence applications used in everyday\nlife. Explainable intelligent systems are designed to self-explain the\nreasoning behind system decisions and predictions, and researchers from\ndifferent disciplines work together to define, design, and evaluate\ninterpretable systems. However, scholars from different disciplines focus on\ndifferent objectives and fairly independent topics of interpretable machine\nlearning research, which poses challenges for identifying appropriate design\nand evaluation methodology and consolidating knowledge across efforts. To this\nend, this paper presents a survey and framework intended to share knowledge and\nexperiences of XAI design and evaluation methods across multiple disciplines.\nAiming to support diverse design goals and evaluation methods in XAI research,\nafter a thorough review of XAI related papers in the fields of machine\nlearning, visualization, and human-computer interaction, we present a\ncategorization of interpretable machine learning design goals and evaluation\nmethods to show a mapping between design goals for different XAI user groups\nand their evaluation methods. From our findings, we develop a framework with\nstep-by-step design guidelines paired with evaluation methods to close the\niterative design and evaluation cycles in multidisciplinary XAI teams. Further,\nwe provide summarized ready-to-use tables of evaluation methods and\nrecommendations for different goals in XAI research.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 21:20:11 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 16:09:11 GMT"}, {"version": "v3", "created": "Sat, 7 Dec 2019 07:49:04 GMT"}, {"version": "v4", "created": "Sun, 26 Apr 2020 20:55:35 GMT"}, {"version": "v5", "created": "Wed, 5 Aug 2020 07:31:35 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Mohseni", "Sina", ""], ["Zarei", "Niloofar", ""], ["Ragan", "Eric D.", ""]]}, {"id": "1811.11876", "submitter": "Rajesh Rao", "authors": "Rajesh P. N. Rao", "title": "Towards Neural Co-Processors for the Brain: Combining Decoding and\n  Encoding in Brain-Computer Interfaces", "comments": "Invited submission to the journal Current Opinion in Neurobiology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of brain-computer interfaces is poised to advance from the\ntraditional goal of controlling prosthetic devices using brain signals to\ncombining neural decoding and encoding within a single neuroprosthetic device.\nSuch a device acts as a \"co-processor\" for the brain, with applications ranging\nfrom inducing Hebbian plasticity for rehabilitation after brain injury to\nreanimating paralyzed limbs and enhancing memory. We review recent progress in\nsimultaneous decoding and encoding for closed-loop control and plasticity\ninduction. To address the challenge of multi-channel decoding and encoding, we\nintroduce a unifying framework for developing brain co-processors based on\nartificial neural networks and deep learning. These \"neural co-processors\" can\nbe used to jointly optimize cost functions with the nervous system to achieve\ndesired behaviors ranging from targeted neuro-rehabilitation to augmentation of\nbrain function.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 23:13:24 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 18:56:32 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Rao", "Rajesh P. N.", ""]]}, {"id": "1811.11943", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Ioana A. Stanescu", "title": "The haptic paradigm in education: Challenges and case studies", "comments": null, "journal-ref": "Internet and Higher Education Journal (2010), Vol. 13(1), pp.\n  78-81 (ISSN 1096-7516)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of learning involves interaction with the learning environment\nthrough our five senses (sight, hearing, touch, smell, and taste). Until\nrecently, distance education focused only on the first two of those senses,\nsight and sound. Internet-based learning environments are predominantly visual\nwith auditory components. With the advent of haptic technology we can now\nsimulate/generate forces and, as a result, the sense of touch. The gaming\nindustry has promoted the \"touch\" on the \"wire\", allowing complex multi-modal\ninteractions online. In this article we provide a brief overview of the\nevolution of haptic technology, its potential for education, and existing\nchallenges. We review recent data on 21st century students' behaviors, and\nshare our experiences in designing interactive haptic environments for\neducation. From the \"Community of Inquiry\" framework perspective, we discuss\nthe potential impact of haptic feedback on cognitive and social presence.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 03:31:28 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Stanescu", "Ioana A.", ""]]}, {"id": "1811.11953", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Anand P. Santhanam, Celina Imielinska, Sanford\n  Meeks and Jannick P. Rolland", "title": "Distributed Augmented Reality with 3D Lung Dynamics -- A Planning Tool\n  Concept", "comments": null, "journal-ref": "IEEE Transactions on Information Technology in Biomedicine (2007),\n  Vol. 11(1), pp. 40-46", "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) systems add visual information to the world by using\nadvanced display techniques. The advances in miniaturization and reduced costs\nmake some of these systems feasible for applications in a wide set of fields.\nWe present a potential component of the cyber infrastructure for the operating\nroom of the future; a distributed AR based software-hardware system that allows\nreal-time visualization of 3D lung dynamics superimposed directly on the\npatient's body. Several emergency events (e.g. closed and tension pneumothorax)\nand surgical procedures related to the lung (e.g. lung transplantation, lung\nvolume reduction surgery, surgical treatment of lung infections, lung cancer\nsurgery) could benefit from the proposed prototype.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 04:10:23 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Santhanam", "Anand P.", ""], ["Imielinska", "Celina", ""], ["Meeks", "Sanford", ""], ["Rolland", "Jannick P.", ""]]}, {"id": "1811.12155", "submitter": "Christian Jilek", "authors": "Christian Jilek, Yannick Runge, Claudia Nieder\\'ee, Heiko Maus, Tobias\n  Tempel, Andreas Dengel, Christian Frings", "title": "Managed Forgetting to Support Information Management and Knowledge Work", "comments": "10 pages, 2 figures, preprint, final version to appear in KI -\n  K\\\"unstliche Intelligenz, Special Issue: Intentional Forgetting", "journal-ref": "KI - K\\\"nstliche Intelligenz, German Journal on Artificial\n  Intelligence, 33.1 (Mar. 2019), pp. 45-55, Springer, 2019", "doi": "10.1007/s13218-018-00568-9", "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trends like digital transformation even intensify the already overwhelming\nmass of information knowledge workers face in their daily life. To counter\nthis, we have been investigating knowledge work and information management\nsupport measures inspired by human forgetting. In this paper, we give an\noverview of solutions we have found during the last five years as well as\nchallenges that still need to be tackled. Additionally, we share experiences\ngained with the prototype of a first forgetful information system used 24/7 in\nour daily work for the last three years. We also address the untapped potential\nof more explicated user context as well as features inspired by Memory\nInhibition, which is our current focus of research.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 09:33:35 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Jilek", "Christian", ""], ["Runge", "Yannick", ""], ["Nieder\u00e9e", "Claudia", ""], ["Maus", "Heiko", ""], ["Tempel", "Tobias", ""], ["Dengel", "Andreas", ""], ["Frings", "Christian", ""]]}, {"id": "1811.12194", "submitter": "Antonio H. Ribeiro", "authors": "Ant\\^onio H. Ribeiro, Manoel Horta Ribeiro, Gabriela Paix\\~ao, Derick\n  Oliveira, Paulo R. Gomes, J\\'essica A. Canazart, Milton Pifano, Wagner Meira\n  Jr., Thomas B. Sch\\\"on, Antonio Luiz Ribeiro", "title": "Automatic Diagnosis of Short-Duration 12-Lead ECG using a Deep\n  Convolutional Network", "comments": "Machine Learning for Health (ML4H) Workshop at NeurIPS 2018\n  arXiv:1811.07216", "journal-ref": null, "doi": null, "report-no": "ML4H/2018/82", "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for predicting electrocardiogram (ECG) abnormalities in\nshort-duration 12-lead ECG signals which outperformed medical doctors on the\n4th year of their cardiology residency. Such exams can provide a full\nevaluation of heart activity and have not been studied in previous end-to-end\nmachine learning papers. Using the database of a large telehealth network, we\nbuilt a novel dataset with more than 2 million ECG tracings, orders of\nmagnitude larger than those used in previous studies. Moreover, our dataset is\nmore realistic, as it consist of 12-lead ECGs recorded during standard\nin-clinics exams. Using this data, we trained a residual neural network with 9\nconvolutional layers to map 7 to 10 second ECG signals to 6 classes of ECG\nabnormalities. Future work should extend these results to cover a large range\nof ECG abnormalities, which could improve the accessibility of this diagnostic\ntool and avoid wrong diagnosis from medical doctors.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 07:39:10 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 07:18:36 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Ribeiro", "Ant\u00f4nio H.", ""], ["Ribeiro", "Manoel Horta", ""], ["Paix\u00e3o", "Gabriela", ""], ["Oliveira", "Derick", ""], ["Gomes", "Paulo R.", ""], ["Canazart", "J\u00e9ssica A.", ""], ["Pifano", "Milton", ""], ["Meira", "Wagner", "Jr."], ["Sch\u00f6n", "Thomas B.", ""], ["Ribeiro", "Antonio Luiz", ""]]}, {"id": "1811.12199", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Marco Cavallo and \\c{C}a\\u{g}atay Demiralp", "title": "A Visual Interaction Framework for Dimensionality Reduction Based Data\n  Exploration", "comments": "CHI'18. arXiv admin note: text overlap with arXiv:1707.04281", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is a common method for analyzing and visualizing\nhigh-dimensional data. However, reasoning dynamically about the results of a\ndimensionality reduction is difficult. Dimensionality-reduction algorithms use\ncomplex optimizations to reduce the number of dimensions of a dataset, but\nthese new dimensions often lack a clear relation to the initial data\ndimensions, thus making them difficult to interpret. Here we propose a visual\ninteraction framework to improve dimensionality-reduction based exploratory\ndata analysis. We introduce two interaction techniques, forward projection and\nbackward projection, for dynamically reasoning about dimensionally reduced\ndata. We also contribute two visualization techniques, prolines and feasibility\nmaps, to facilitate the effective use of the proposed interactions. We apply\nour framework to PCA and autoencoder-based dimensionality reductions. Through\ndata-exploration examples, we demonstrate how our visual interactions can\nimprove the use of dimensionality reduction in exploratory data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 01:47:49 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1811.12476", "submitter": "Khaza Anuarul Hoque", "authors": "Aniket Gulhane, Akhil Vyas, Reshmi Mitra, Roland Oruche, Gabriela\n  Hoefer, Samaikya Valluripally, Prasad Calyam, Khaza Anuarul Hoque", "title": "Security, Privacy and Safety Risk Assessment for Virtual Reality\n  Learning Environment Applications", "comments": "Tp appear in the CCNC 2019 Conference", "journal-ref": null, "doi": "10.1109/CCNC.2019.8651847", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Virtual Reality based Learning Environments (VRLEs) such as vSocial\nrender instructional content in a three-dimensional immersive computer\nexperience for training youth with learning impediments. There are limited\nprior works that explored attack vulnerability in VR technology, and hence\nthere is a need for systematic frameworks to quantify risks corresponding to\nsecurity, privacy, and safety (SPS) threats. The SPS threats can adversely\nimpact the educational user experience and hinder delivery of VRLE content. In\nthis paper, we propose a novel risk assessment framework that utilizes attack\ntrees to calculate a risk score for varied VRLE threats with rate and duration\nof threats as inputs. We compare the impact of a well-constructed attack tree\nwith an adhoc attack tree to study the trade-offs between overheads in managing\nattack trees, and the cost of risk mitigation when vulnerabilities are\nidentified. We use a vSocial VRLE testbed in a case study to showcase the\neffectiveness of our framework and demonstrate how a suitable attack tree\nformalism can result in a more safer, privacy-preserving and secure VRLE\nsystem.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:46:35 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Gulhane", "Aniket", ""], ["Vyas", "Akhil", ""], ["Mitra", "Reshmi", ""], ["Oruche", "Roland", ""], ["Hoefer", "Gabriela", ""], ["Valluripally", "Samaikya", ""], ["Calyam", "Prasad", ""], ["Hoque", "Khaza Anuarul", ""]]}, {"id": "1811.12815", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Jannick P. Rolland, Charles Hughes", "title": "A Distributed Augmented Reality System for Medical Training and\n  Simulation", "comments": "arXiv admin note: text overlap with arXiv:1111.2993 by other authors", "journal-ref": "Energy, Simulation-Training, Ocean Engineering and\n  Instrumentation: Research Papers of the Link Foundation Fellows (2004), Vol.\n  4, pp. 213-235", "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented Reality (AR) systems describe the class of systems that use\ncomputers to overlay virtual information on the real world. AR environments\nallow the development of promising tools in several application domains. In\nmedical training and simulation the learning potential of AR is significantly\namplified by the capability of the system to present 3D medical models in\nreal-time at remote locations. Furthermore the simulation applicability is\nbroadened by the use of real-time deformable medical models. This work presents\na distributed medical training prototype designed to train medical\npractitioners' hand-eye coordination when performing endotracheal intubations.\nThe system we present accomplishes this task with the help of AR paradigms. An\nextension of this prototype to medical simulations by employing deformable\nmedical models is possible. The shared state maintenance of the collaborative\nAR environment is assured through a novel adaptive synchronization algorithm\n(ASA) that increases the sense of presence among participants and facilitates\ntheir interactivity in spite of infrastructure delays. The system will allow\nparamedics, pre-hospital personnel, and students to practice their skills\nwithout touching a real patient and will provide them with the visual feedback\nthey could not otherwise obtain. Such a distributed AR training tool has the\npotential to: allow an instructor to simultaneously train local and remotely\nlocated students and, allow students to actually \"see\" the internal anatomy and\ntherefore better understand their actions on a human patient simulator (HPS).\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 04:35:44 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Rolland", "Jannick P.", ""], ["Hughes", "Charles", ""]]}]