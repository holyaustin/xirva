[{"id": "1708.00076", "submitter": "Kami Vaniea", "authors": "Kami Vaniea, Ella Tallyn and Chris Speed", "title": "Capturing the Connections: Unboxing Internet of Things Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based upon a study of how to capture data from Internet of Things (IoT)\ndevices, this paper explores the challenges for data centric design\nethnography. Often purchased to perform specific tasks, IoT devices exist in a\ncomplex ecosystem. This paper describes a study that used a variety of methods\nto capture the interactions an IoT device engaged in when it was first setup.\nThe complexity of the study that is explored through the annotated\ndocumentation across video and router activity, presents the ethnographic\nchallenges that designers face in an age of connected things.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 21:33:32 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Vaniea", "Kami", ""], ["Tallyn", "Ella", ""], ["Speed", "Chris", ""]]}, {"id": "1708.00495", "submitter": "Brett Israelsen", "authors": "Brett W Israelsen", "title": "\"I can assure you [$\\ldots$] that it's going to be all right\" -- A\n  definition, case for, and survey of algorithmic assurances in human-autonomy\n  trust relationships", "comments": "Copy submitted to area exam committee", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As technology become more advanced, those who design, use and are otherwise\naffected by it want to know that it will perform correctly, and understand why\nit does what it does, and how to use it appropriately. In essence they want to\nbe able to trust the systems that are being designed. In this survey we present\nassurances that are the method by which users can understand how to trust this\ntechnology. Trust between humans and autonomy is reviewed, and the implications\nfor the design of assurances are highlighted. A survey of research that has\nbeen performed with respect to assurances is presented, and several key ideas\nare extracted in order to refine the definition of assurances. Several\ndirections for future research are identified and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 20:13:18 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 18:39:22 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Israelsen", "Brett W", ""]]}, {"id": "1708.00885", "submitter": "Artur Lugmayr", "authors": "Artur Lugmayr, Richard Seale, Andrew Woods, Eunice Sari, Adi\n  Tedjasaputra", "title": "Proc. of the 9th Workshop on Semantic Ambient Media Experiences\n  (SAME'2016/2): Visualisation, Emerging Media, and User-Experience:\n  International Series on Information Systems and Management in Creative eMedia\n  (CreMedia)", "comments": null, "journal-ref": "Proc. of the 9th Workshop on Semantic Ambient Media Experiences,\n  Visualisation, Emerging Media, and User-Experience, International Series on\n  Information Systems and Management in Creative eMedia (CreMedia), No. 2016/2,\n  2016", "doi": null, "report-no": null, "categories": "cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 9th Semantic Ambient Media Experience (SAME) proceedings where based on\nthe academic contributions to a two day workshop that was held at Curtin\nUniversity, Perth, WA, Australia. The symposium was held to discuss\nvisualisation, emerging media, and user-experience from various angles. The\npapers of this workshop are freely available through\nhttp://www.ambientmediaassociation.org/Journal under open access as provided by\nthe International Ambient Media Association (iAMEA) Ry. iAMEA is hosting the\ninternational open access journal entitled \"International Journal on\nInformation Systems and Management in Creative eMedia\", and the series entitled\n\"International Series on Information Systems and Management in Creative\neMedia\". For any further information, please visit the website of the\nAssociation: http://www.ambientmediaassociation.org.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 14:46:51 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Lugmayr", "Artur", ""], ["Seale", "Richard", ""], ["Woods", "Andrew", ""], ["Sari", "Eunice", ""], ["Tedjasaputra", "Adi", ""]]}, {"id": "1708.00908", "submitter": "Eunji Chong", "authors": "Eunji Chong, Christian Nitschke, Atsushi Nakazawa, Agata Rozga, and\n  James M. Rehg", "title": "Noninvasive Corneal Image-Based Gaze Measurement System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze tracking is an important technology as the system can give information\nabout a person from what and where the person is seeing. There have been many\nattempts to make robust and accurate gaze trackers using either monitor or\nwearable devices. However, those contraptions often require fine individual\ncalibration per session and/or require a person wearing a device, which may not\nbe suitable for certain situations. In this paper, we propose a robust and a\ncompletely noninvasive gaze tracking system that involves neither complex\ncalibrations nor the use of wearable devices. We achieve this via direct eye\nreflection analysis by building a real-time system that effectively enables it.\nWe also show several interesting applications for our system including\nexperiments with young children.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 19:52:01 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Chong", "Eunji", ""], ["Nitschke", "Christian", ""], ["Nakazawa", "Atsushi", ""], ["Rozga", "Agata", ""], ["Rehg", "James M.", ""]]}, {"id": "1708.00931", "submitter": "Sanchit Alekh", "authors": "Abhinav Gupta, Agrim Khanna, Anmol Jagetia, Devansh Sharma, Sanchit\n  Alekh, Vaibhav Choudhary", "title": "Combining Keystroke Dynamics and Face Recognition for User Verification", "comments": null, "journal-ref": null, "doi": "10.1109/CSE.2015.37", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The massive explosion and ubiquity of computing devices and the outreach of\nthe web have been the most defining events of the century so far. As more and\nmore people gain access to the internet, traditional know-something and\nhave-something authentication methods such as PINs and passwords are proving to\nbe insufficient for prohibiting unauthorized access to increasingly personal\ndata on the web. Therefore, the need of the hour is a user-verification system\nthat is not only more reliable and secure, but also unobtrusive and\nminimalistic. Keystroke Dynamics is a novel Biometric Technique; it is not only\nunobtrusive, but also transparent and inexpensive. The fusion of keystroke\ndynamics and Face Recognition engenders the most desirable characteristics of a\nverification system. Our implementation uses Hidden Markov Models (HMM) for\nmodelling the Keystroke Dynamics, with the help of two widely used Feature\nVectors: Keypress Latency and Keypress Duration. On the other hand, Face\nRecognition makes use of the traditional Eigenfaces approach.The results show\nthat the system has a high precision, with a False Acceptance Rate of 5.4% and\na False Rejection Rate of 9.2%. Moreover, it is also future-proof, as the\nhardware requirements, i.e. camera and keyboard (physical or on-screen), have\nbecome an indispensable part of modern computing.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 21:13:59 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Gupta", "Abhinav", ""], ["Khanna", "Agrim", ""], ["Jagetia", "Anmol", ""], ["Sharma", "Devansh", ""], ["Alekh", "Sanchit", ""], ["Choudhary", "Vaibhav", ""]]}, {"id": "1708.01318", "submitter": "Khanh Nguyen", "authors": "Amr Sharaf, Shi Feng, Khanh Nguyen, Kiant\\'e Brantley, Hal Daum\\'e III", "title": "The UMD Neural Machine Translation Systems at WMT17 Bandit Learning Task", "comments": "7 pages, 1 figure, WMT 2017 Bandit Learning Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the University of Maryland machine translation systems submitted\nto the WMT17 German-English Bandit Learning Task. The task is to adapt a\ntranslation system to a new domain, using only bandit feedback: the system\nreceives a German sentence to translate, produces an English sentence, and only\ngets a scalar score as feedback. Targeting these two challenges (adaptation and\nbandit learning), we built a standard neural machine translation system and\nextended it in two ways: (1) robust reinforcement learning techniques to learn\neffectively from the bandit feedback, and (2) domain adaptation using data\nselection from a large corpus of parallel data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 21:42:46 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 20:45:50 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Sharaf", "Amr", ""], ["Feng", "Shi", ""], ["Nguyen", "Khanh", ""], ["Brantley", "Kiant\u00e9", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "1708.01377", "submitter": "Taeheon Kim", "authors": "Taeheon Kim, Bahador Saket, Alex Endert, Blair MacIntyre", "title": "VisAR: Bringing Interactivity to Static Data Visualizations through\n  Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static visualizations have analytic and expressive value. However, many\ninteractive tasks cannot be completed using static visualizations. As datasets\ngrow in size and complexity, static visualizations start losing their analytic\nand expressive power for interactive data exploration. Despite this limitation\nof static visualizations, there are still many cases where visualizations are\nlimited to being static (e.g., visualizations on presentation slides or\nposters). We believe in many of these cases, static visualizations will benefit\nfrom allowing users to perform interactive tasks on them. Inspired by the\nintroduction of numerous commercial personal augmented reality (AR) devices, we\npropose an AR solution that allows interactive data exploration of datasets on\nstatic visualizations. In particular, we present a prototype system named VisAR\nthat uses the Microsoft Hololens to enable users to complete interactive tasks\non static visualizations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 04:54:24 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Kim", "Taeheon", ""], ["Saket", "Bahador", ""], ["Endert", "Alex", ""], ["MacIntyre", "Blair", ""]]}, {"id": "1708.01465", "submitter": "Joos Behncke", "authors": "Dominik Welke, Joos Behncke, Marina Hader, Robin Tibor Schirrmeister,\n  Andreas Sch\\\"onau, Boris E{\\ss}mann, Oliver M\\\"uller, Wolfram Burgard, Tonio\n  Ball", "title": "Brain Responses During Robot-Error Observation", "comments": null, "journal-ref": null, "doi": "10.17185/duepublico/44533", "report-no": null, "categories": "cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-controlled robots are a promising new type of assistive device for\nseverely impaired persons. Little is however known about how to optimize the\ninteraction of humans and brain-controlled robots. Information about the\nhuman's perceived correctness of robot performance might provide a useful\nteaching signal for adaptive control algorithms and thus help enhancing robot\ncontrol. Here, we studied whether watching robots perform erroneous vs. correct\naction elicits differential brain responses that can be decoded from single\ntrials of electroencephalographic (EEG) recordings, and whether brain activity\nduring human-robot interaction is modulated by the robot's visual similarity to\na human. To address these topics, we designed two experiments. In experiment I,\nparticipants watched a robot arm pour liquid into a cup. The robot performed\nthe action either erroneously or correctly, i.e. it either spilled some liquid\nor not. In experiment II, participants observed two different types of robots,\nhumanoid and non-humanoid, grabbing a ball. The robots either managed to grab\nthe ball or not. We recorded high-resolution EEG during the observation tasks\nin both experiments to train a Filter Bank Common Spatial Pattern (FBCSP)\npipeline on the multivariate EEG signal and decode for the correctness of the\nobserved action, and for the type of the observed robot. Our findings show that\nit was possible to decode both correctness and robot type for the majority of\nparticipants significantly, although often just slightly, above chance level.\nOur findings suggest that non-invasive recordings of brain responses elicited\nwhen observing robots indeed contain decodable information about the\ncorrectness of the robot's action and the type of observed robot.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 11:58:52 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 08:21:25 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Welke", "Dominik", ""], ["Behncke", "Joos", ""], ["Hader", "Marina", ""], ["Schirrmeister", "Robin Tibor", ""], ["Sch\u00f6nau", "Andreas", ""], ["E\u00dfmann", "Boris", ""], ["M\u00fcller", "Oliver", ""], ["Burgard", "Wolfram", ""], ["Ball", "Tonio", ""]]}, {"id": "1708.01640", "submitter": "Najmeh Sadoughi", "authors": "Najmeh Sadoughi and Carlos Busso", "title": "Speech-driven Animation with Meaningful Behaviors", "comments": "13 pages, 12 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents (CAs) play an important role in human computer\ninteraction. Creating believable movements for CAs is challenging, since the\nmovements have to be meaningful and natural, reflecting the coupling between\ngestures and speech. Studies in the past have mainly relied on rule-based or\ndata-driven approaches. Rule-based methods focus on creating meaningful\nbehaviors conveying the underlying message, but the gestures cannot be easily\nsynchronized with speech. Data-driven approaches, especially speech-driven\nmodels, can capture the relationship between speech and gestures. However, they\ncreate behaviors disregarding the meaning of the message. This study proposes\nto bridge the gap between these two approaches overcoming their limitations.\nThe approach builds a dynamic Bayesian network (DBN), where a discrete variable\nis added to constrain the behaviors on the underlying constraint. The study\nimplements and evaluates the approach with two constraints: discourse functions\nand prototypical behaviors. By constraining on the discourse functions (e.g.,\nquestions), the model learns the characteristic behaviors associated with a\ngiven discourse class learning the rules from the data. By constraining on\nprototypical behaviors (e.g., head nods), the approach can be embedded in a\nrule-based system as a behavior realizer creating trajectories that are timely\nsynchronized with speech. The study proposes a DBN structure and a training\napproach that (1) models the cause-effect relationship between the constraint\nand the gestures, (2) initializes the state configuration models increasing the\nrange of the generated behaviors, and (3) captures the differences in the\nbehaviors across constraints by enforcing sparse transitions between shared and\nexclusive states per constraint. Objective and subjective evaluations\ndemonstrate the benefits of the proposed approach over an unconstrained model.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 18:53:44 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Sadoughi", "Najmeh", ""], ["Busso", "Carlos", ""]]}, {"id": "1708.01817", "submitter": "Anuradha Kar", "authors": "Anuradha Kar, Peter Corcoran", "title": "A Review and Analysis of Eye-Gaze Estimation Systems, Algorithms and\n  Performance Evaluation Methods in Consumer Platforms", "comments": "25 pages, 13 figures, Accepted for publication in IEEE Access in July\n  2017", "journal-ref": null, "doi": "10.1109/ACCESS.2017.2735633", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a review is presented of the research on eye gaze estimation\ntechniques and applications, that has progressed in diverse ways over the past\ntwo decades. Several generic eye gaze use-cases are identified: desktop, TV,\nhead-mounted, automotive and handheld devices. Analysis of the literature leads\nto the identification of several platform specific factors that influence gaze\ntracking accuracy. A key outcome from this review is the realization of a need\nto develop standardized methodologies for performance evaluation of gaze\ntracking systems and achieve consistency in their specification and comparative\nevaluation. To address this need, the concept of a methodological framework for\npractical evaluation of different gaze tracking systems is proposed.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 19:50:19 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Kar", "Anuradha", ""], ["Corcoran", "Peter", ""]]}, {"id": "1708.01931", "submitter": "Muaz Niazi", "authors": "Faisal Riaz, Muaz A. Niazi", "title": "Towards Social Autonomous Vehicles: Efficient Collision Avoidance Scheme\n  Using Richardson's Arms Race Model", "comments": "48 pages, 21 figures", "journal-ref": "PLoS ONE12(10): e0186103 (2017)", "doi": "10.1371/journal.pone.0186103", "report-no": null, "categories": "cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Road collisions and casualties pose a serious threat to commuters\naround the globe. Autonomous Vehicles (AVs) aim to make the use of technology\nto reduce the road accidents. However, the most of research work in the context\nof collision avoidance has been performed to address, separately, the rear end,\nfront end and lateral collisions in less congested and with high\ninter-vehicular distances. Purpose The goal of this paper is to introduce the\nconcept of a social agent, which interact with other AVs in social manners like\nhumans are social having the capability of predicting intentions, i.e.\nmentalizing and copying the actions of each other, i.e. mirroring. The proposed\nsocial agent is based on a human-brain inspired mentalizing and mirroring\ncapabilities and has been modelled for collision detection and avoidance under\ncongested urban road traffic.\n  Method We designed our social agent having the capabilities of mentalizing\nand mirroring and for this purpose we utilized Exploratory Agent Based Modeling\n(EABM) level of Cognitive Agent Based Computing (CABC) framework proposed by\nNiazi and Hussain.\n  Results Our simulation and practical experiments reveal that by embedding\nRichardson's arms race model within AVs, collisions can be avoided while\ntravelling on congested urban roads in a flock like topologies. The performance\nof the proposed social agent has been compared at two different levels.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 20:07:14 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Riaz", "Faisal", ""], ["Niazi", "Muaz A.", ""]]}, {"id": "1708.01944", "submitter": "Abram Handler", "authors": "Abram Handler, Brendan O'Connor", "title": "Rookie: A unique approach for exploring news archives", "comments": "Presented at KDD 2017: Data Science + Journalism workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  News archives are an invaluable primary source for placing current events in\nhistorical context. But current search engine tools do a poor job at uncovering\nbroad themes and narratives across documents. We present Rookie: a practical\nsoftware system which uses natural language processing (NLP) to help readers,\nreporters and editors uncover broad stories in news archives. Unlike prior\nwork, Rookie's design emerged from 18 months of iterative development in\nconsultation with editors and computational journalists. This process lead to a\ndramatically different approach from previous academic systems with similar\ngoals. Our efforts offer a generalizable case study for others building\nreal-world journalism software using NLP.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 22:20:02 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Handler", "Abram", ""], ["O'Connor", "Brendan", ""]]}, {"id": "1708.02167", "submitter": "Wen Shen", "authors": "Wen Shen, Alanoud Al Khemeiri, Abdulla Almehrezi, Wael Al Enezi, Iyad\n  Rahwan, Jacob W. Crandall", "title": "Regulating Highly Automated Robot Ecologies: Insights from Three User\n  Studies", "comments": "10 pages, 7 figures, to appear in the 5th International Conference on\n  Human Agent Interaction (HAI-2017), Bielefeld, Germany", "journal-ref": "In Proceedings of the 5th International Conference on Human Agent\n  Interaction (HAI 2017). ACM, New York, NY, USA, 111-120", "doi": "10.1145/3125739.3125758", "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Highly automated robot ecologies (HARE), or societies of independent\nautonomous robots or agents, are rapidly becoming an important part of much of\nthe world's critical infrastructure. As with human societies, regulation,\nwherein a governing body designs rules and processes for the society, plays an\nimportant role in ensuring that HARE meet societal objectives. However, to\ndate, a careful study of interactions between a regulator and HARE is lacking.\nIn this paper, we report on three user studies which give insights into how to\ndesign systems that allow people, acting as the regulatory authority, to\neffectively interact with HARE. As in the study of political systems in which\ngovernments regulate human societies, our studies analyze how interactions\nbetween HARE and regulators are impacted by regulatory power and individual\n(robot or agent) autonomy. Our results show that regulator power, decision\nsupport, and adaptive autonomy can each diminish the social welfare of HARE,\nand hint at how these seemingly desirable mechanisms can be designed so that\nthey become part of successful HARE.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:28:09 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Shen", "Wen", ""], ["Khemeiri", "Alanoud Al", ""], ["Almehrezi", "Abdulla", ""], ["Enezi", "Wael Al", ""], ["Rahwan", "Iyad", ""], ["Crandall", "Jacob W.", ""]]}, {"id": "1708.02174", "submitter": "Mehran Maghoumi", "authors": "Pooya Khaloo, Mehran Maghoumi, Eugene Taranta II, David Bettner,\n  Joseph Laviola Jr", "title": "Code Park: A New 3D Code Visualization Tool", "comments": "Accepted for publication in 2017 IEEE Working Conference on Software\n  Visualization (VISSOFT 2017); Supplementary video:\n  https://www.youtube.com/watch?v=LUiy1M9hUKU", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Code Park, a novel tool for visualizing codebases in a 3D\ngame-like environment. Code Park aims to improve a programmer's understanding\nof an existing codebase in a manner that is both engaging and intuitive,\nappealing to novice users such as students. It achieves these goals by laying\nout the codebase in a 3D park-like environment. Each class in the codebase is\nrepresented as a 3D room-like structure. Constituent parts of the class\n(variable, member functions, etc.) are laid out on the walls, resembling a\nsyntax-aware \"wallpaper\". The users can interact with the codebase using an\noverview, and a first-person viewer mode. We conducted two user studies to\nevaluate Code Park's usability and suitability for organizing an existing\nproject. Our results indicate that Code Park is easy to get familiar with and\nsignificantly helps in code understanding compared to a traditional IDE.\nFurther, the users unanimously believed that Code Park was a fun tool to work\nwith.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:53:10 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Khaloo", "Pooya", ""], ["Maghoumi", "Mehran", ""], ["Taranta", "Eugene", "II"], ["Bettner", "David", ""], ["Laviola", "Joseph", "Jr"]]}, {"id": "1708.02214", "submitter": "Qing Ping", "authors": "Qing Ping, Chaomei Chen", "title": "LitStoryTeller: An Interactive System for Visual Exploration of\n  Scientific Papers Leveraging Named entities and Comparative Sentences", "comments": "Accepted at the 16th International Conference On Scientometrics &\n  Informetrics (ISSI 2017). Please include ISSI 2017 in any citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study proposes LitStoryTeller, an interactive system for visually\nexploring the semantic structure of a scientific article. We demonstrate how\nLitStoryTeller could be used to answer some of the most fundamental research\nquestions, such as how a new method was built on top of existing methods, based\non what theoretical proof and experimental evidences. More importantly,\nLitStoryTeller can assist users to understand the full and interesting story a\nscientific paper, with a concise outline and important details. The proposed\nsystem borrows a metaphor from screen play, and visualizes the storyline of a\nscientific paper by arranging its characters (scientific concepts or\nterminologies) and scenes (paragraphs/sentences) into a progressive and\ninteractive storyline. Such storylines help to preserve the semantic structure\nand logical thinking process of a scientific paper. Semantic structures, such\nas scientific concepts and comparative sentences, are extracted using existing\nnamed entity recognition APIs and supervised classifiers, from a scientific\npaper automatically. Two supplementary views, ranked entity frequency view and\nentity co-occurrence network view, are provided to help users identify the\n\"main plot\" of such scientific storylines. When collective documents are ready,\nLitStoryTeller also provides a temporal entity evolution view and entity\ncommunity view for collection digestion.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 17:32:56 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 19:04:17 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 14:06:54 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Ping", "Qing", ""], ["Chen", "Chaomei", ""]]}, {"id": "1708.02274", "submitter": "Burak Pak", "authors": "Burak Pak, Alvin Chua, Andrew Vande Moere", "title": "FixMyStreet Brussels: Socio-Demographic Inequality in Crowdsourced Civic\n  Participation", "comments": null, "journal-ref": "Journal of Urban Technology 2017 Volume 24 No 2 65 to 87", "doi": "10.1080/10630732.2016.1270047", "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  FixMyStreet (FMS) is a web-based civic participation platform that allows\ninhabitants to report environmental defects like potholes and damaged pavements\nto the government. In this paper, we examine the use of FMS in Brussels, the\ncapital city of Belgium. Analyzing a total of 30,041 reports since its\ninception in 2013, we demonstrate how civic participation on FMS varies between\nthe ethnically diverse districts in Brussels. We compare FMS use to a range of\nsociodemographic indicators derived from official city statistics as well as\ngeotagged social media data from Twitter. Our statistical analysis revealed\nseveral significant differences between the districts that suggested that\ncrowdsourced civic participation platforms tend to marginalize low-income and\nethnically diverse communities. In this respect, our findings provide timely\nevidence to inform the design of more inclusive crowdsourced, civic\nparticipation platforms in the future.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 19:24:36 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Pak", "Burak", ""], ["Chua", "Alvin", ""], ["Moere", "Andrew Vande", ""]]}, {"id": "1708.02363", "submitter": "Ilias Flaounas", "authors": "Ilias Flaounas", "title": "Beyond the technical challenges for deploying Machine Learning solutions\n  in a software company", "comments": "Human in the Loop Machine Learning Workshop, International Conference\n  on Machine Learning, Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently software development companies started to embrace Machine Learning\n(ML) techniques for introducing a series of advanced functionality in their\nproducts such as personalisation of the user experience, improved search,\ncontent recommendation and automation. The technical challenges for tackling\nthese problems are heavily researched in literature. A less studied area is a\npragmatic approach to the role of humans in a complex modern industrial\nenvironment where ML based systems are developed. Key stakeholders affect the\nsystem from inception and up to operation and maintenance. Product managers\nwant to embed \"smart\" experiences for their users and drive the decisions on\nwhat should be built next; software engineers are challenged to build or\nutilise ML software tools that require skills that are well outside of their\ncomfort zone; legal and risk departments may influence design choices and data\naccess; operations teams are requested to maintain ML systems which are\nnon-stationary in their nature and change behaviour over time; and finally ML\npractitioners should communicate with all these stakeholders to successfully\nbuild a reliable system. This paper discusses some of the challenges we faced\nin Atlassian as we started investing more in the ML space.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 03:59:09 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Flaounas", "Ilias", ""]]}, {"id": "1708.02660", "submitter": "Zoya Bylinskii", "authors": "Zoya Bylinskii, Nam Wook Kim, Peter O'Donovan, Sami Alsheikh, Spandan\n  Madan, Hanspeter Pfister, Fredo Durand, Bryan Russell, Aaron Hertzmann", "title": "Learning Visual Importance for Graphic Designs and Data Visualizations", "comments": null, "journal-ref": "UIST 2017", "doi": "10.1145/3126594.3126653", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing where people look and click on visual designs can provide clues about\nhow the designs are perceived, and where the most important or relevant content\nlies. The most important content of a visual design can be used for effective\nsummarization or to facilitate retrieval from a database. We present automated\nmodels that predict the relative importance of different elements in data\nvisualizations and graphic designs. Our models are neural networks trained on\nhuman clicks and importance annotations on hundreds of designs. We collected a\nnew dataset of crowdsourced importance, and analyzed the predictions of our\nmodels with respect to ground truth importance and human eye movements. We\ndemonstrate how such predictions of importance can be used for automatic design\nretargeting and thumbnailing. User studies with hundreds of MTurk participants\nvalidate that, with limited post-processing, our importance-driven applications\nare on par with, or outperform, current state-of-the-art methods, including\nnatural image saliency. We also provide a demonstration of how our importance\npredictions can be built into interactive design tools to offer immediate\nfeedback during the design process.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 21:50:51 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Bylinskii", "Zoya", ""], ["Kim", "Nam Wook", ""], ["O'Donovan", "Peter", ""], ["Alsheikh", "Sami", ""], ["Madan", "Spandan", ""], ["Pfister", "Hanspeter", ""], ["Durand", "Fredo", ""], ["Russell", "Bryan", ""], ["Hertzmann", "Aaron", ""]]}, {"id": "1708.02664", "submitter": "Leonardo Angelini", "authors": "Leonardo Angelini, Nadine Couture, Omar Abou Khaled and Elena\n  Mugellini", "title": "Internet of Tangible Things (IoTT): Challenges and Opportunities for\n  Tangible Interaction with IoT", "comments": "Suibmitted to MDPI Informatics, Special Issue on Tangible and\n  Embodied Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Internet of Things era, an increasing number of household devices and\neveryday objects are able to send to and retrieve information from the\nInternet, offering innovative services to the user. However, most of these\ndevices provide only smartphone or web interfaces to control the IoT object\nproperties and functions. As a result, generally, the interaction is\ndisconnected from the physical world, decreasing the user experience and\nincreasing the risk of isolating the user in digital bubbles. We argue that\ntangible interaction can counteract this trend and this paper discusses the\npotential benefits and the still open challenges of tangible interaction\napplied to the Internet of Things. To underline this need, we introduce the\nterm Internet of Tangible Things. In the article, after an analysis of current\nopen challenges for Human-Computer Interaction in IoT, we summarize current\ntrends in tangible interaction and extrapolate eight tangible interaction\nproperties that could be exploited for designing novel interactions with IoT\nobjects. Through a systematic literature review of tangible interaction applied\nto IoT, we show what has been already explored in the systems that pioneered\nthe field and the future explorations that still have to be conducted.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 22:07:20 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Angelini", "Leonardo", ""], ["Couture", "Nadine", ""], ["Khaled", "Omar Abou", ""], ["Mugellini", "Elena", ""]]}, {"id": "1708.02749", "submitter": "Yan Shoshitaishvili", "authors": "Yan Shoshitaishvili, Michael Weissbacher, Lukas Dresel, Christopher\n  Salls, Ruoyu Wang, Christopher Kruegel, Giovanni Vigna", "title": "Rise of the HaCRS: Augmenting Autonomous Cyber Reasoning Systems with\n  Human Assistance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the size and complexity of software systems increase, the number and\nsophistication of software security flaws increase as well. The analysis of\nthese flaws began as a manual approach, but it soon became apparent that tools\nwere necessary to assist human experts in this task, resulting in a number of\ntechniques and approaches that automated aspects of the vulnerability analysis\nprocess.\n  Recently, DARPA carried out the Cyber Grand Challenge, a competition among\nautonomous vulnerability analysis systems designed to push the tool-assisted\nhuman-centered paradigm into the territory of complete automation. However,\nwhen the autonomous systems were pitted against human experts it became clear\nthat certain tasks, albeit simple, could not be carried out by an autonomous\nsystem, as they require an understanding of the logic of the application under\nanalysis.\n  Based on this observation, we propose a shift in the vulnerability analysis\nparadigm, from tool-assisted human-centered to human-assisted tool-centered. In\nthis paradigm, the automated system orchestrates the vulnerability analysis\nprocess, and leverages humans (with different levels of expertise) to perform\nwell-defined sub-tasks, whose results are integrated in the analysis. As a\nresult, it is possible to scale the analysis to a larger number of programs,\nand, at the same time, optimize the use of expensive human resources.\n  In this paper, we detail our design for a human-assisted automated\nvulnerability analysis system, describe its implementation atop an open-sourced\nautonomous vulnerability analysis system that participated in the Cyber Grand\nChallenge, and evaluate and discuss the significant improvements that\nnon-expert human assistance can offer to automated analysis approaches.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 08:04:50 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Shoshitaishvili", "Yan", ""], ["Weissbacher", "Michael", ""], ["Dresel", "Lukas", ""], ["Salls", "Christopher", ""], ["Wang", "Ruoyu", ""], ["Kruegel", "Christopher", ""], ["Vigna", "Giovanni", ""]]}, {"id": "1708.02895", "submitter": "Dingzeyu Li", "authors": "Dingzeyu Li", "title": "Interacting with Acoustic Simulation and Fabrication", "comments": "ACM UIST 2017 Doctoral Symposium", "journal-ref": null, "doi": "10.1145/3131785.3131842", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating accurate physics-based simulation into interactive design tools\nis challenging. However, adding the physics accurately becomes crucial to\nseveral emerging technologies. For example, in virtual/augmented reality\n(VR/AR) videos, the faithful reproduction of surrounding audios is required to\nbring the immersion to the next level. Similarly, as personal fabrication is\nmade possible with accessible 3D printers, more intuitive tools that respect\nthe physical constraints can help artists to prototype designs. One main hurdle\nis the sheer amount of computation complexity to accurately reproduce the\nreal-world phenomena through physics-based simulation. In my thesis research, I\ndevelop interactive tools that implement efficient physics-based simulation\nalgorithms for automatic optimization and intuitive user interaction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 16:20:12 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 17:14:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Li", "Dingzeyu", ""]]}, {"id": "1708.02924", "submitter": "Anna Iurchenko", "authors": "Anna Iurchenko", "title": "Medication non adherence: finding solutions through design thinking\n  approach", "comments": "4 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical non-adherence increasingly is recognized as a major medical health\nproblem. Approximately 50% of patients do not take their medications as\nprescribed and such poor adherence has been shown to result in complications,\ndeath, and increased health care costs. This problem becomes even more\nsignificant for patients with chronic illness and those who need to take\nmedications lifetime, like transplant patients. Studies show that one-half of\nrejection episodes and 15% of graft losses happen due to immunosuppression\nmedications non-adherence. This article explores factors that have an impact on\nnon-compliant behavior among transplant patients: patient factors, illness\nfactor, therapeutic regimen factors. Using user-centered design thinking\napproach a set of hypotheses are defined and discussed strategies to enhance\nadherence by using mobile technology and gamification techniques.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 17:51:36 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Iurchenko", "Anna", ""]]}, {"id": "1708.03044", "submitter": "Ting-Hao Huang", "authors": "Ting-Hao Kenneth Huang and Walter S. Lasecki and Amos Azaria and\n  Jeffrey P. Bigham", "title": "\"Is there anything else I can help you with?\": Challenges in Deploying\n  an On-Demand Crowd-Powered Conversational Agent", "comments": "10 pages. In Proceedings of Conference on Human Computation &\n  Crowdsourcing (HCOMP 2016), 2016, Austin, TX, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent conversational assistants, such as Apple's Siri, Microsoft's\nCortana, and Amazon's Echo, have quickly become a part of our digital life.\nHowever, these assistants have major limitations, which prevents users from\nconversing with them as they would with human dialog partners. This limits our\nability to observe how users really want to interact with the underlying\nsystem. To address this problem, we developed a crowd-powered conversational\nassistant, Chorus, and deployed it to see how users and workers would interact\ntogether when mediated by the system. Chorus sophisticatedly converses with end\nusers over time by recruiting workers on demand, which in turn decide what\nmight be the best response for each user sentence. Up to the first month of our\ndeployment, 59 users have held conversations with Chorus during 320\nconversational sessions. In this paper, we present an account of Chorus'\ndeployment, with a focus on four challenges: (i) identifying when conversations\nare over, (ii) malicious users and workers, (iii) on-demand recruiting, and\n(iv) settings in which consensus is not enough. Our observations could assist\nthe deployment of crowd-powered conversation systems and crowd-powered systems\nin general.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 01:40:49 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Huang", "Ting-Hao Kenneth", ""], ["Lasecki", "Walter S.", ""], ["Azaria", "Amos", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1708.03462", "submitter": "Yanhong Wu", "authors": "Xun Zhao, Yanhong Wu, Weiwei Cui, Xinnan Du, Yuan Chen, Yong Wang, Dik\n  Lun Lee, Huamin Qu", "title": "SkyLens: Visual Analysis of Skyline on Multi-dimensional Data", "comments": "10 pages. Accepted for publication at IEEE VIS 2017 (in proceedings\n  of VAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skyline queries have wide-ranging applications in fields that involve\nmulti-criteria decision making, including tourism, retail industry, and human\nresources. By automatically removing incompetent candidates, skyline queries\nallow users to focus on a subset of superior data items (i.e., the skyline),\nthus reducing the decision-making overhead. However, users are still required\nto interpret and compare these superior items manually before making a\nsuccessful choice. This task is challenging because of two issues. First,\npeople usually have fuzzy, unstable, and inconsistent preferences when\npresented with multiple candidates. Second, skyline queries do not reveal the\nreasons for the superiority of certain skyline points in a multi-dimensional\nspace. To address these issues, we propose SkyLens, a visual analytic system\naiming at revealing the superiority of skyline points from different\nperspectives and at different scales to aid users in their decision making. Two\nscenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of\nattributes. A qualitative study is also conducted to show that users can\nefficiently accomplish skyline understanding and comparison tasks with SkyLens.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 08:04:58 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 01:32:05 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Zhao", "Xun", ""], ["Wu", "Yanhong", ""], ["Cui", "Weiwei", ""], ["Du", "Xinnan", ""], ["Chen", "Yuan", ""], ["Wang", "Yong", ""], ["Lee", "Dik Lun", ""], ["Qu", "Huamin", ""]]}, {"id": "1708.03655", "submitter": "James Tompkin", "authors": "Eric Rosen and David Whitney and Elizabeth Phillips and Gary Chien and\n  James Tompkin and George Konidaris and Stefanie Tellex", "title": "Communicating Robot Arm Motion Intent Through Mixed Reality Head-mounted\n  Displays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient motion intent communication is necessary for safe and collaborative\nwork environments with collocated humans and robots. Humans efficiently\ncommunicate their motion intent to other humans through gestures, gaze, and\nsocial cues. However, robots often have difficulty efficiently communicating\ntheir motion intent to humans via these methods. Many existing methods for\nrobot motion intent communication rely on 2D displays, which require the human\nto continually pause their work and check a visualization. We propose a mixed\nreality head-mounted display visualization of the proposed robot motion over\nthe wearer's real-world view of the robot and its environment. To evaluate the\neffectiveness of this system against a 2D display visualization and against no\nvisualization, we asked 32 participants to labeled different robot arm motions\nas either colliding or non-colliding with blocks on a table. We found a 16%\nincrease in accuracy with a 62% decrease in the time it took to complete the\ntask compared to the next best system. This demonstrates that a mixed-reality\nHMD allows a human to more quickly and accurately tell where the robot is going\nto move than the compared baselines.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 18:28:02 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Rosen", "Eric", ""], ["Whitney", "David", ""], ["Phillips", "Elizabeth", ""], ["Chien", "Gary", ""], ["Tompkin", "James", ""], ["Konidaris", "George", ""], ["Tellex", "Stefanie", ""]]}, {"id": "1708.03783", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Abigale Stangl, Mark D. Gross, Tom Yeh", "title": "FluxMarker: Enhancing Tactile Graphics with Dynamic Tactile Markers", "comments": "ASSETS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For people with visual impairments, tactile graphics are an important means\nto learn and explore information. However, raised line tactile graphics created\nwith traditional materials such as embossing are static. While available\nrefreshable displays can dynamically change the content, they are still too\nexpensive for many users, and are limited in size. These factors limit\nwide-spread adoption and the representation of large graphics or data sets. In\nthis paper, we present FluxMaker, an inexpensive scalable system that renders\ndynamic information on top of static tactile graphics with movable tactile\nmarkers. These dynamic tactile markers can be easily reconfigured and used to\nannotate static raised line tactile graphics, including maps, graphs, and\ndiagrams. We developed a hardware prototype that actuates magnetic tactile\nmarkers driven by low-cost and scalable electromagnetic coil arrays, which can\nbe fabricated with standard printed circuit board manufacturing. We evaluate\nour prototype with six participants with visual impairments and found positive\nresults across four application areas: location finding or navigating on\ntactile maps, data analysis, and physicalization, feature identification for\ntactile graphics, and drawing support. The user study confirms advantages in\napplication domains such as education and data exploration.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 14:25:50 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Suzuki", "Ryo", ""], ["Stangl", "Abigale", ""], ["Gross", "Mark D.", ""], ["Yeh", "Tom", ""]]}, {"id": "1708.03786", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Gustavo Soares, Andrew Head, Elena Glassman, Ruan Reis,\n  Melina Mongiovi, Loris D'Antoni, Bjoern Hartmann", "title": "TraceDiff: Debugging Unexpected Code Behavior Using Trace Divergences", "comments": "VL/HCC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in program synthesis offer means to automatically debug\nstudent submissions and generate personalized feedback in massive programming\nclassrooms. When automatically generating feedback for programming assignments,\na key challenge is designing pedagogically useful hints that are as effective\nas the manual feedback given by teachers. Through an analysis of teachers'\nhint-giving practices in 132 online Q&A posts, we establish three design\nguidelines that an effective feedback design should follow. Based on these\nguidelines, we develop a feedback system that leverages both program synthesis\nand visualization techniques. Our system compares the dynamic code execution of\nboth incorrect and fixed code and highlights how the error leads to a\ndifference in behavior and where the incorrect code trace diverges from the\nexpected solution. Results from our study suggest that our system enables\nstudents to detect and fix bugs that are not caught by students using another\nexisting visual debugging tool.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 14:48:32 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Suzuki", "Ryo", ""], ["Soares", "Gustavo", ""], ["Head", "Andrew", ""], ["Glassman", "Elena", ""], ["Reis", "Ruan", ""], ["Mongiovi", "Melina", ""], ["D'Antoni", "Loris", ""], ["Hartmann", "Bjoern", ""]]}, {"id": "1708.03788", "submitter": "Daniel Smilkov", "authors": "Daniel Smilkov, Shan Carter, D. Sculley, Fernanda B. Vi\\'egas, Martin\n  Wattenberg", "title": "Direct-Manipulation Visualization of Deep Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent successes of deep learning have led to a wave of interest from\nnon-experts. Gaining an understanding of this technology, however, is\ndifficult. While the theory is important, it is also helpful for novices to\ndevelop an intuitive feel for the effect of different hyperparameters and\nstructural variations. We describe TensorFlow Playground, an interactive, open\nsourced visualization that allows users to experiment via direct manipulation\nrather than coding, enabling them to quickly build an intuition about neural\nnets.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 15:36:26 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Smilkov", "Daniel", ""], ["Carter", "Shan", ""], ["Sculley", "D.", ""], ["Vi\u00e9gas", "Fernanda B.", ""], ["Wattenberg", "Martin", ""]]}, {"id": "1708.03892", "submitter": "Fabio Calefato", "authors": "Fabio Calefato, Filippo Lanubile, Nicole Novielli", "title": "EmoTxt: A Toolkit for Emotion Recognition from Text", "comments": "In Proc. 7th Affective Computing and Intelligent Interaction\n  (ACII'17), San Antonio, TX, USA, Oct. 23-26, 2017, p. 79-80, ISBN:\n  978-1-5386-0563-9", "journal-ref": "In Proc. 7th Affective Computing and Intelligent Interaction\n  (ACII'17), San Antonio, TX, USA, Oct. 23-26, 2017, p. 79-80, ISBN:\n  978-1-5386-0563-9", "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present EmoTxt, a toolkit for emotion recognition from text, trained and\ntested on a gold standard of about 9K question, answers, and comments from\nonline interactions. We provide empirical evidence of the performance of\nEmoTxt. To the best of our knowledge, EmoTxt is the first open-source toolkit\nsupporting both emotion recognition from text and training of custom emotion\nclassification models.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 11:55:02 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 09:21:05 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Calefato", "Fabio", ""], ["Lanubile", "Filippo", ""], ["Novielli", "Nicole", ""]]}, {"id": "1708.04139", "submitter": "Zhenyi He", "authors": "Zhenyi He, Fengyuan Zhu and Ken Perlin", "title": "PhyShare: Sharing Physical Interaction in Virtual Reality", "comments": "7 pages. arXiv admin note: text overlap with arXiv:1701.08879", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PhyShare, a new haptic user interface based on actuated robots.\nVirtual reality has recently been gaining wide adoption, and an effective\nhaptic feedback in these scenarios can strongly support user's sensory in\nbridging virtual and physical world. Since participants do not directly observe\nthese robotic proxies, we investigate the multiple mappings between physical\nrobots and virtual proxies that can utilize the resources needed to provide a\nwell rounded VR experience. PhyShare bots can act either as directly touchable\nobjects or invisible carriers of physical objects, depending on different\nscenarios. They also support distributed collaboration, allowing remotely\nlocated VR collaborators to share the same physical feedback.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 21:03:21 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["He", "Zhenyi", ""], ["Zhu", "Fengyuan", ""], ["Perlin", "Ken", ""]]}, {"id": "1708.04164", "submitter": "Casper Hansen", "authors": "Christian Hansen, Casper Hansen, Niklas Hjuler, Stephen Alstrup,\n  Christina Lioma", "title": "Sequence Modelling For Analysing Student Interaction with Educational\n  Systems", "comments": "The 10th International Conference on Educational Data Mining 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of log data generated by online educational systems is an\nimportant task for improving the systems, and furthering our knowledge of how\nstudents learn. This paper uses previously unseen log data from Edulab, the\nlargest provider of digital learning for mathematics in Denmark, to analyse the\nsessions of its users, where 1.08 million student sessions are extracted from a\nsubset of their data. We propose to model students as a distribution of\ndifferent underlying student behaviours, where the sequence of actions from\neach session belongs to an underlying student behaviour. We model student\nbehaviour as Markov chains, such that a student is modelled as a distribution\nof Markov chains, which are estimated using a modified k-means clustering\nalgorithm. The resulting Markov chains are readily interpretable, and in a\nqualitative analysis around 125,000 student sessions are identified as\nexhibiting unproductive student behaviour. Based on our results this student\nrepresentation is promising, especially for educational systems offering many\ndifferent learning usages, and offers an alternative to common approaches like\nmodelling student behaviour as a single Markov chain often done in the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 15:02:34 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Hansen", "Christian", ""], ["Hansen", "Casper", ""], ["Hjuler", "Niklas", ""], ["Alstrup", "Stephen", ""], ["Lioma", "Christina", ""]]}, {"id": "1708.04399", "submitter": "Rajesh  Kumar", "authors": "Rajesh Kumar, Partha Pratim Kundu, Diksha Shukla, Vir V. Phoha", "title": "Continuous User Authentication via Unlabeled Phone Movement Patterns", "comments": "IEEE International Joint Conference on Biometrics (IJCB 2017),\n  Denver, Colorado", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel continuous authentication system for\nsmartphone users. The proposed system entirely relies on unlabeled phone\nmovement patterns collected through smartphone accelerometer. The data was\ncollected in a completely unconstrained environment over five to twelve days.\nThe contexts of phone usage were identified using k-means clustering. Multiple\nprofiles, one for each context, were created for every user. Five machine\nlearning algorithms were employed for classification of genuine and impostors.\nThe performance of the system was evaluated over a diverse population of 57\nusers. The mean equal error rates achieved by Logistic Regression, Neural\nNetwork, kNN, SVM, and Random Forest were 13.7%, 13.5%, 12.1%, 10.7%, and 5.6%\nrespectively. A series of statistical tests were conducted to compare the\nperformance of the classifiers. The suitability of the proposed system for\ndifferent types of users was also investigated using the failure to enroll\npolicy.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 05:35:44 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Kumar", "Rajesh", ""], ["Kundu", "Partha Pratim", ""], ["Shukla", "Diksha", ""], ["Phoha", "Vir V.", ""]]}, {"id": "1708.04418", "submitter": "Anna Iurchenko", "authors": "Anna Iurchenko", "title": "An Exploratory Study of Health Habit Formation Through Gamification", "comments": "5 pages, 2 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promotion of healthy habits help maintain and improve people health, reduce\ndisease risks, and manage chronic illness. Regular healthy activities like\nwalking, exercising, healthy eating, drinking water or taking medication on\ntime require forming the new habits. Gamification techniques are promising in\npromoting healthy behaviors and delivering health promotion information.\nHowever, using gaming elements such as badges, leader boards, health-related\nchallenges in mobile applications to motivate and engage people to change\nhealth behavior is quite new. In this exploratory study, we aimed to assess how\ngame mechanics and dynamics influence formation of a habit through the mobile\napplication. Results indicate the different level of user engagement depending\non the presence of gamification elements and suggest that there is value in\nadding game elements to the user experience.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 07:33:24 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 19:17:31 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Iurchenko", "Anna", ""]]}, {"id": "1708.04636", "submitter": "David Hallac", "authors": "David Hallac, Abhijit Sharang, Rainer Stahlmann, Andreas Lamprecht,\n  Markus Huber, Martin Roehder, Rok Sosic, Jure Leskovec", "title": "Driver Identification Using Automobile Sensor Data from a Single Turn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As automotive electronics continue to advance, cars are becoming more and\nmore reliant on sensors to perform everyday driving operations. These sensors\nare omnipresent and help the car navigate, reduce accidents, and provide\ncomfortable rides. However, they can also be used to learn about the drivers\nthemselves. In this paper, we propose a method to predict, from sensor data\ncollected at a single turn, the identity of a driver out of a given set of\nindividuals. We cast the problem in terms of time series classification, where\nour dataset contains sensor readings at one turn, repeated several times by\nmultiple drivers. We build a classifier to find unique patterns in each\nindividual's driving style, which are visible in the data even on such a short\nroad segment. To test our approach, we analyze a new dataset collected by AUDI\nAG and Audi Electronics Venture, where a fleet of test vehicles was equipped\nwith automotive data loggers storing all sensor readings on real roads. We show\nthat turns are particularly well-suited for detecting variations across\ndrivers, especially when compared to straightaways. We then focus on the 12\nmost frequently made turns in the dataset, which include rural, urban, highway\non-ramps, and more, obtaining accurate identification results and learning\nuseful insights about driver behavior in a variety of settings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 17:15:00 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Hallac", "David", ""], ["Sharang", "Abhijit", ""], ["Stahlmann", "Rainer", ""], ["Lamprecht", "Andreas", ""], ["Huber", "Markus", ""], ["Roehder", "Martin", ""], ["Sosic", "Rok", ""], ["Leskovec", "Jure", ""]]}, {"id": "1708.04653", "submitter": "Mahdi Miraz", "authors": "Monir Bhuiyan, Ambreen Zaman and Mahdi H. Miraz", "title": "Usability Evaluation of a Mobile Application in Extraordinary\n  Environment for Extraordinary People", "comments": null, "journal-ref": "Proceedings of the International Conference on eBusiness,\n  eCommerce, eManagement, eLearning and eGovernance (IC5E 2014), held at\n  University of Greenwich, London, UK, 30-31 July 2014, pp. 96-103", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a contemporary world, people become dependent on electronic devices.\nTechnologies help to clarification and structure life in many ways to meet the\nneed of the children oriented requirements. The children suffering from\ndisabilities (e.g. autism) has desperate needs for elucidation and structures\ntheir life. MumIES is a research based system facilitates to support and manage\ntheir living. This paper works on MumIES system to evaluate usability of the\nsystem in extraordinary environment for extraordinary people. The paper shows\nfrom the survey observation users need supporting tools to access the\nchildren's potential and challenges and to give the full support to overcome\ndisabilities. Usability evaluation has been considered one of the key\nchallenges to MumIES system. The paper represents analysis, design of usability\nstudies for the extraordinary user in environment.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 18:46:11 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Bhuiyan", "Monir", ""], ["Zaman", "Ambreen", ""], ["Miraz", "Mahdi H.", ""]]}, {"id": "1708.04655", "submitter": "Mahdi Miraz", "authors": "Mahdi H. Miraz, Sajid Khan, Monir Bhuiyan and Peter Excell", "title": "Mobile Academy: A Ubiquitous Mobile Learning (mLearning) Platform", "comments": null, "journal-ref": "Proceedings of the International Conference on eBusiness,\n  eCommerce, eManagement, eLearning and eGovernance (IC5E 2014), held at\n  University of Greenwich, London, UK, 30-31 July 2014, pp. 89-95", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper reports on an ongoing research project into the development of\n\"Mobile Academy\", an Android-based mobile learning (mLearning) application\n(app). The project comprises three major phases: requirement analysis,\napplication development and testing and evaluation. To satisfy the user\nrequirement analysis, a detailed ethnographic study was conducted to\ninvestigate how people from different background use mobile devices for\nlearning purposes. The initial analysis and evaluation of the first version of\nthe projected app demonstrates very promising results. Making use of the app\nseemed to have, in general, a positive dimension in facilitating educational\nuse of mobile devices.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 19:00:25 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Miraz", "Mahdi H.", ""], ["Khan", "Sajid", ""], ["Bhuiyan", "Monir", ""], ["Excell", "Peter", ""]]}, {"id": "1708.04656", "submitter": "Mahdi Miraz", "authors": "Sajid Khan, Md Al Shayokh, Mahdi H. Miraz and Monir Bhuiyan", "title": "A Framework for Android Based Shopping Mall Applications", "comments": null, "journal-ref": "Proceedings of the International Conference on eBusiness,\n  eCommerce, eManagement, eLearning and eGovernance (IC5E 2014), held at\n  University of Greenwich, London, UK, 30-31 July 2014, pp. 27-32", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Android is Google's latest open source software platform for mobile devices\nwhich has already attained enormous popularity. The purpose of this paper is to\ndescribe the development of mobile application for shopping mall using Android\nplatform. A prototype was developed for the shoppers of Bashundhara Shopping\nMall of Bangladesh. This prototype will serve as a framework for any such\napplications (apps). The paper presents a practical demonstration of how to\nintegrate shops' information, such as names, categories, locations,\ndescriptions, floor layout and so forth, with map module via an android\napplication. A summary of survey results of the related literature and projects\nhave also been included. Critical Evaluation of the prototype along with future\nresearch and development plan has been revealed. The paper will serve as a\nguideline for the researchers and developers to introduce and develop similar\napps.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 18:56:47 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Khan", "Sajid", ""], ["Shayokh", "Md Al", ""], ["Miraz", "Mahdi H.", ""], ["Bhuiyan", "Monir", ""]]}, {"id": "1708.05006", "submitter": "Gaurav Bhorkar", "authors": "Gaurav Bhorkar", "title": "A Survey of Augmented Reality Navigation", "comments": "Seminar on Software Systems, Technologies and Security, Spring 2016,\n  Aalto University", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigation has been a popular area of research in both academia and industry.\nCombined with maps, and different localization technologies, navigation systems\nhave become robust and more usable. By combining navigation with augmented\nreality, it can be improved further to become realistic and user friendly. This\npaper surveys existing researches carried out in this area, describes existing\ntechniques for building augmented reality navigation systems, and the problems\nfaced.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 09:40:53 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Bhorkar", "Gaurav", ""]]}, {"id": "1708.05073", "submitter": "Mahdi Miraz", "authors": "Mohammed Fakrudeen, Sufian Yousef, and Mahdi H Miraz", "title": "Finger Based Technique (FBT): An Innovative System for Improved\n  Usability for the Blind Users' Dynamic Interaction with Mobile Touch Screen\n  Devices", "comments": null, "journal-ref": "World Congress on Engineering (WCE 2014) held at Imperial College,\n  London, UK, 2-4 July 2014, ISBN-13: 978-988-19252-7-5, Print ISSN: 2078-0958,\n  Online ISSN: 2078-0966, Vol. 1, pp. 128-133", "doi": "10.13140/2.1.4190.7529", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents Finger Based Technique (FBT) prototypes, a novel\ninteraction system for blind users, which is especially designed and developed\nfor non-visual touch screen devices and their applications. The FBT prototypes\nwere developed with virtual keys to be identified based on finger holding\npositions. Two different models namely the single digit FBT and double digit\nFBT were propounded. FBT technique were applied using two different phone\ndialer applications: a single digit virtual key for the single digit FBT model\nand a double digit virtual key with audio feedback enabling touch as input\ngesture for the later one. An evaluation with 7 blind participants showed that\nsingle digit FBT was significantly faster and more accurate than double digit\nFBT. In addition to that, single digit FBT was found to be much faster than\niPhone VoiceOver entry speeds in performing similar tasks. Furthermore, our\nresearch also suggests 11 accessible regions for quick access or navigation in\nflat touch screen based smart phones for blind users. These accessible regions\nwill serve as a usability design framework and facilitate the developers to\nplace the widget for the blind user for dynamic interaction with the touch\nscreen devices. As far as is known to the authors, this is a novel suggestion.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:47:14 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Fakrudeen", "Mohammed", ""], ["Yousef", "Sufian", ""], ["Miraz", "Mahdi H", ""]]}, {"id": "1708.05085", "submitter": "Mahdi Miraz", "authors": "Mahdi H. Miraz, Maaruf Ali and Peter Excell", "title": "Multilingual Website Usability Analysis Based on an International User\n  Survey", "comments": null, "journal-ref": "the proceedings of the fifth international conference on Internet\n  Technologies and Applications (ITA 13) held at Glyndwr University in Wrexham,\n  UK, 10-13 September 2013, ISBN-10: 0-946881-81-2, ISBN-13: 978-0-946881-81-9,\n  pp. 236-244", "doi": "10.13140/2.1.1208.8648", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A study was undertaken to determine the important usability factors (UF) used\nin the English and the non-English version of a website. The important\nusability factors were determined, based on a detailed questionnaire used in an\ninternational survey. Analysis of the questionnaire found inequalities in the\nuser satisfaction and a general dissatisfaction with the non-English version of\nthe website. The study concluded that more care should be taken in creating the\ntext, taking into account the cultural and linguistic background of the users\nand the use of graphics in multilingual websites.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:34:26 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Miraz", "Mahdi H.", ""], ["Ali", "Maaruf", ""], ["Excell", "Peter", ""]]}, {"id": "1708.05122", "submitter": "Prithvijit Chattopadhyay Chattopadhyay", "authors": "Prithvijit Chattopadhyay, Deshraj Yadav, Viraj Prabhu, Arjun\n  Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh", "title": "Evaluating Visual Conversational Agents via Cooperative Human-AI Games", "comments": "HCOMP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As AI continues to advance, human-AI teams are inevitable. However, progress\nin AI is routinely measured in isolation, without a human in the loop. It is\ncrucial to benchmark progress in AI, not just in isolation, but also in terms\nof how it translates to helping humans perform certain tasks, i.e., the\nperformance of human-AI teams.\n  In this work, we design a cooperative game - GuessWhich - to measure human-AI\nteam performance in the specific context of the AI being a visual\nconversational agent. GuessWhich involves live interaction between the human\nand the AI. The AI, which we call ALICE, is provided an image which is unseen\nby the human. Following a brief description of the image, the human questions\nALICE about this secret image to identify it from a fixed pool of images.\n  We measure performance of the human-ALICE team by the number of guesses it\ntakes the human to correctly identify the secret image after a fixed number of\ndialog rounds with ALICE. We compare performance of the human-ALICE teams for\ntwo versions of ALICE. Our human studies suggest a counterintuitive trend -\nthat while AI literature shows that one version outperforms the other when\npaired with an AI questioner bot, we find that this improvement in AI-AI\nperformance does not translate to improved human-AI performance. This suggests\na mismatch between benchmarking of AI in isolation and in the context of\nhuman-AI teams.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 03:27:53 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Chattopadhyay", "Prithvijit", ""], ["Yadav", "Deshraj", ""], ["Prabhu", "Viraj", ""], ["Chandrasekaran", "Arjun", ""], ["Das", "Abhishek", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1708.05174", "submitter": "Sid Chi-Kin Chau", "authors": "Muhammad Aftab, Sid Chi-Kin Chau, and Majid Khonji", "title": "Enabling Self-aware Smart Buildings by Augmented Reality", "comments": "This paper appears in ACM International Conference on Future Energy\n  Systems (e-Energy), 2018", "journal-ref": null, "doi": "10.1145/3208903.3208943", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional HVAC control systems are usually incognizant of the physical\nstructures and materials of buildings. These systems merely follow pre-set HVAC\ncontrol logic based on abstract building thermal response models, which are\nrough approximations to true physical models, ignoring dynamic spatial\nvariations in built environments. To enable more accurate and responsive HVAC\ncontrol, this paper introduces the notion of \"self-aware\" smart buildings, such\nthat buildings are able to explicitly construct physical models of themselves\n(e.g., incorporating building structures and materials, and thermal flow\ndynamics). The question is how to enable self-aware buildings that\nautomatically acquire dynamic knowledge of themselves. This paper presents a\nnovel approach using \"augmented reality\". The extensive user-environment\ninteractions in augmented reality not only can provide intuitive user\ninterfaces for building systems, but also can capture the physical structures\nand possibly materials of buildings accurately to enable real-time building\nsimulation and control. This paper presents a building system prototype\nincorporating augmented reality, and discusses its applications.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 08:56:01 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 09:38:54 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Aftab", "Muhammad", ""], ["Chau", "Sid Chi-Kin", ""], ["Khonji", "Majid", ""]]}, {"id": "1708.05688", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg and Sergej Sizov", "title": "Human Uncertainty and Ranking Error -- The Secret of Successful\n  Evaluation in Predictive Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most crucial issues in data mining is to model human behaviour in\norder to provide personalisation, adaptation and recommendation. This usually\ninvolves implicit or explicit knowledge, either by observing user interactions,\nor by asking users directly. But these sources of information are always\nsubject to the volatility of human decisions, making utilised data uncertain to\na particular extent. In this contribution, we elaborate on the impact of this\nhuman uncertainty when it comes to comparative assessments of different data\nmining approaches. In particular, we reveal two problems: (1) biasing effects\non various metrics of model-based prediction and (2) the propagation of\nuncertainty and its thus induced error probabilities for algorithm rankings.\nFor this purpose, we introduce a probabilistic view and prove the existence of\nthose problems mathematically, as well as provide possible solution strategies.\nWe exemplify our theory mainly in the context of recommender systems along with\nthe metric RMSE as a prominent example of precision quality measures.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:44:08 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1708.05805", "submitter": "Poorna Talkad Sukumar", "authors": "Poorna Talkad Sukumar and Ronald Metoyer", "title": "Design Space of Programming Tools on Mobile Touchscreen Devices", "comments": "11 pages, includes one-page table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While mobile touchscreen devices are ubiquitous and present opportunities for\nnovel applications, they have seen little adoption as tools for computer\nprogramming. In this literature survey, we bring together the diverse research\nwork on programming-related tasks supported by mobile touchscreen devices to\nexplore the design space for applying them to programming situations. We used\nthe Grounded theory approach to identify themes and classify previous work. We\npresent these themes and how each paper contributes to the theme, and we\noutline the remaining challenges in and opportunities for using mobile\ntouchscreen devices in programming applications.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 05:04:05 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Sukumar", "Poorna Talkad", ""], ["Metoyer", "Ronald", ""]]}, {"id": "1708.05905", "submitter": "Lesandro Ponciano", "authors": "Lesandro Ponciano and Pedro Barbosa and Francisco Brasileiro and\n  Andrey Brito and Nazareno Andrade", "title": "Designing for Pragmatists and Fundamentalists: Privacy Concerns and\n  Attitudes on the Internet of Things", "comments": "XVI Brazilian Symposium on Human Factors in Computing Systems\n  (IHC'17), October 23-27, 2017, Joinville, SC, Brazil", "journal-ref": "In Proceedings of IHC 2017. ACM, US, Article 21, 10 pages (2017)", "doi": "10.1145/3160504.3160545", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) systems have aroused enthusiasm and concerns.\nEnthusiasm comes from their utilities in people daily life, and concerns may be\nassociated with privacy issues. By using two IoT systems as case-studies, we\nexamine users' privacy beliefs, concerns and attitudes. We focus on four major\ndimensions: the collection of personal data, the inference of new information,\nthe exchange of information to third parties, and the risk-utility trade-off\nposed by the features of the system. Altogether, 113 Brazilian individuals\nanswered a survey about such dimensions. Although their perceptions seem to be\ndependent on the context, there are recurrent patterns. Our results suggest\nthat IoT users can be classified into unconcerned, fundamentalists and\npragmatists. Most of them exhibit a pragmatist profile and believe in privacy\nas a right guaranteed by law. One of the most privacy concerning aspect is the\nexchange of personal information to third parties. Individuals' perceived risk\nis negatively correlated with their perceived utility in the features of the\nsystem. We discuss practical implications of these results and suggest\nheuristics to cope with privacy concerns when designing IoT systems.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 22:15:58 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 16:05:41 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 15:07:58 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Ponciano", "Lesandro", ""], ["Barbosa", "Pedro", ""], ["Brasileiro", "Francisco", ""], ["Brito", "Andrey", ""], ["Andrade", "Nazareno", ""]]}, {"id": "1708.06026", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Nadia Bianchi-Berthouze and Simon J. Julier", "title": "DeepBreath: Deep Learning of Breathing Patterns for Automatic Stress\n  Recognition using Low-Cost Thermal Imaging in Unconstrained Settings", "comments": "Submitted to \"2017 7th International Conference on Affective\n  Computing and Intelligent Interaction (ACII)\" - ACII 2017", "journal-ref": null, "doi": "10.1109/ACII.2017.8273639", "report-no": null, "categories": "cs.HC cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose DeepBreath, a deep learning model which automatically recognises\npeople's psychological stress level (mental overload) from their breathing\npatterns. Using a low cost thermal camera, we track a person's breathing\npatterns as temperature changes around his/her nostril. The paper's technical\ncontribution is threefold. First of all, instead of creating hand-crafted\nfeatures to capture aspects of the breathing patterns, we transform the\nuni-dimensional breathing signals into two dimensional respiration variability\nspectrogram (RVS) sequences. The spectrograms easily capture the complexity of\nthe breathing dynamics. Second, a spatial pattern analysis based on a deep\nConvolutional Neural Network (CNN) is directly applied to the spectrogram\nsequences without the need of hand-crafting features. Finally, a data\naugmentation technique, inspired from solutions for over-fitting problems in\ndeep learning, is applied to allow the CNN to learn with a small-scale dataset\nfrom short-term measurements (e.g., up to a few hours). The model is trained\nand tested with data collected from people exposed to two types of cognitive\ntasks (Stroop Colour Word Test, Mental Computation test) with sessions of\ndifferent difficulty levels. Using normalised self-report as ground truth, the\nCNN reaches 84.59% accuracy in discriminating between two levels of stress and\n56.52% in discriminating between three levels. In addition, the CNN\noutperformed powerful shallow learning methods based on a single layer neural\nnetwork. Finally, the dataset of labelled thermal images will be open to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 21:36:30 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cho", "Youngjun", ""], ["Bianchi-Berthouze", "Nadia", ""], ["Julier", "Simon J.", ""]]}, {"id": "1708.06207", "submitter": "Someshwar Roy", "authors": "Roy Someshwar and Yael Edan", "title": "Givers & Receivers perceive handover tasks differently: Implications for\n  Human-Robot collaborative system design", "comments": "16 pages, 8 figures, This manuscript is a prior version of the\n  article accepted in the IJSR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-human joint-action in short-cycle repetitive handover tasks was\ninvestigated for a bottle handover task using a three-fold approach:\nwork-methods field studies in multiple supermarkets, simulation analysis using\nan ergonomics software package and by conducting an in-house lab experiment on\nhuman-human collaboration by re-creating the environment and conditions of a\nsupermarket. Evaluation included both objective and subjective measures.\nSubjective evaluation was done taking a psychological perspective and showcases\namong other things, the differences in the way a common joint-action is being\nperceived by individual team partners depending upon their role (giver or\nreceiver). The proposed approach can provide a systematic method to analyze\nsimilar tasks. Combining the results of all the three analyses, this research\ngives insight into the science of joint-action for short-cycle repetitive tasks\nand its implications for human-robot collaborative system design.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:04:32 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 17:26:05 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Someshwar", "Roy", ""], ["Edan", "Yael", ""]]}, {"id": "1708.06276", "submitter": "Alessandro Saffiotti", "authors": "Barbara Bruno, Nak Young Chong, Hiroko Kamide, Sanjeev Kanoria,\n  Jaeryoung Lee, Yuto Lim, Amit Kumar Pandey, Chris Papadopoulos, Irena\n  Papadopoulos, Federico Pecora, Alessandro Saffiotti, Antonio Sgorbissa", "title": "The CARESSES EU-Japan project: making assistive robots culturally\n  competent", "comments": "Paper presented at: Ambient Assisted Living, Italian Forum. Genova,\n  Italy, June 12--15, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nursing literature shows that cultural competence is an important\nrequirement for effective healthcare. We claim that personal assistive robots\nshould likewise be culturally competent, that is, they should be aware of\ngeneral cultural characteristics and of the different forms they take in\ndifferent individuals, and take these into account while perceiving, reasoning,\nand acting. The CARESSES project is an Europe-Japan collaborative effort that\naims at designing, developing and evaluating culturally competent assistive\nrobots. These robots will be able to adapt the way they behave, speak and\ninteract to the cultural identity of the person they assist. This paper\ndescribes the approach taken in the CARESSES project, its initial steps, and\nits future plans.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 15:12:54 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Bruno", "Barbara", ""], ["Chong", "Nak Young", ""], ["Kamide", "Hiroko", ""], ["Kanoria", "Sanjeev", ""], ["Lee", "Jaeryoung", ""], ["Lim", "Yuto", ""], ["Pandey", "Amit Kumar", ""], ["Papadopoulos", "Chris", ""], ["Papadopoulos", "Irena", ""], ["Pecora", "Federico", ""], ["Saffiotti", "Alessandro", ""], ["Sgorbissa", "Antonio", ""]]}, {"id": "1708.06445", "submitter": "Vicky Charisi", "authors": "Vicky Charisi, Bram Ridder, Jaebok Kim, Vanessa Evers", "title": "Planning Based System for Child-Robot Interaction in Dynamic Play\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the initial steps towards the design of a robotic system\nthat intends to perform actions autonomously in a naturalistic play\nenvironment. At the same time it aims for social human-robot interaction~(HRI),\nfocusing on children. We draw on existing theories of child development and on\ndimensional models of emotions to explore the design of a dynamic interaction\nframework for natural child-robot interaction. In this dynamic setting, the\nsocial HRI is defined by the ability of the system to take into consideration\nthe socio-emotional state of the user and to plan appropriately by selecting\nappropriate strategies for execution. The robot needs a temporal planning\nsystem, which combines features of task-oriented actions and principles of\nsocial human robot interaction. We present initial results of an empirical\nstudy for the evaluation of the proposed framework in the context of a\ncollaborative sorting game.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 22:42:55 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Charisi", "Vicky", ""], ["Ridder", "Bram", ""], ["Kim", "Jaebok", ""], ["Evers", "Vanessa", ""]]}, {"id": "1708.06578", "submitter": "Dalin Zhang", "authors": "Dalin Zhang, Lina Yao, Xiang Zhang, Sen Wang, Weitong Chen, Robert\n  Boots", "title": "Cascade and Parallel Convolutional Recurrent Neural Networks on\n  EEG-based Intention Recognition for Brain Computer Interface", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interface (BCI) is a system empowering humans to communicate\nwith or control the outside world with exclusively brain intentions.\nElectroencephalography (EEG) based BCIs are promising solutions due to their\nconvenient and portable instruments. Motor imagery EEG (MI-EEG) is a kind of\nmost widely focused EEG signals, which reveals a subjects movement intentions\nwithout actual actions. Despite the extensive research of MI-EEG in recent\nyears, it is still challenging to interpret EEG signals effectively due to the\nmassive noises in EEG signals (e.g., low signal noise ratio and incomplete EEG\nsignals), and difficulties in capturing the inconspicuous relationships between\nEEG signals and certain brain activities. Most existing works either only\nconsider EEG as chain-like sequences neglecting complex dependencies between\nadjacent signals or performing simple temporal averaging over EEG sequences. In\nthis paper, we introduce both cascade and parallel convolutional recurrent\nneural network models for precisely identifying human intended movements by\neffectively learning compositional spatio-temporal representations of raw EEG\nstreams. The proposed models grasp the spatial correlations between physically\nneighboring EEG signals by converting the chain like EEG sequences into a 2D\nmesh like hierarchy. An LSTM based recurrent network is able to extract the\nsubtle temporal dependencies of EEG data streams. Extensive experiments on a\nlarge-scale MI-EEG dataset (108 subjects, 3,145,160 EEG records) have\ndemonstrated that both models achieve high accuracy near 98.3% and outperform a\nset of baseline methods and most recent deep learning based EEG recognition\nmodels, yielding a significant accuracy increase of 18% in the cross-subject\nvalidation scenario.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 12:21:21 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 14:42:54 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhang", "Dalin", ""], ["Yao", "Lina", ""], ["Zhang", "Xiang", ""], ["Wang", "Sen", ""], ["Chen", "Weitong", ""], ["Boots", "Robert", ""]]}, {"id": "1708.06664", "submitter": "Nicole Novielli", "authors": "Daniela Girardi, Filippo Lanubile, Nicole Novielli", "title": "Emotion Detection Using Noninvasive Low Cost Sensors", "comments": "To appear in Proceedings of ACII 2017, the Seventh International\n  Conference on Affective Computing and Intelligent Interaction, San Antonio,\n  TX, USA, Oct. 23-26, 2017", "journal-ref": "In: Proceedings of the 2017 Seventh International Conference on\n  Affective Computing and Intelligent Interaction (ACII). p. 125-130, IEEE,\n  ISBN: 978-1-5386-0563-9, San Antonio, Texas, October 23-26, 2017, p. 125-130", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition from biometrics is relevant to a wide range of\napplication domains, including healthcare. Existing approaches usually adopt\nmulti-electrodes sensors that could be expensive or uncomfortable to be used in\nreal-life situations. In this study, we investigate whether we can reliably\nrecognize high vs. low emotional valence and arousal by relying on noninvasive\nlow cost EEG, EMG, and GSR sensors. We report the results of an empirical study\ninvolving 19 subjects. We achieve state-of-the- art classification performance\nfor both valence and arousal even in a cross-subject classification setting,\nwhich eliminates the need for individual training and tuning of classification\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 15:03:00 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Girardi", "Daniela", ""], ["Lanubile", "Filippo", ""], ["Novielli", "Nicole", ""]]}, {"id": "1708.06794", "submitter": "Jonti Talukdar", "authors": "Jonti Talukdar and Bhavana Mehta", "title": "Human Action Recognition System using Good Features and Multilayer\n  Perceptron Network", "comments": "6 pages, 7 Figures, IEEE International Conference on Communication\n  and Signal Processing 2017 (ICCSP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition involves the characterization of human actions\nthrough the automated analysis of video data and is integral in the development\nof smart computer vision systems. However, several challenges like dynamic\nbackgrounds, camera stabilization, complex actions, occlusions etc. make action\nrecognition in a real time and robust fashion difficult. Several complex\napproaches exist but are computationally intensive. This paper presents a novel\napproach of using a combination of good features along with iterative optical\nflow algorithm to compute feature vectors which are classified using a\nmultilayer perceptron (MLP) network. The use of multiple features for motion\ndescriptors enhances the quality of tracking. Resilient backpropagation\nalgorithm is used for training the feedforward neural network reducing the\nlearning time. The overall system accuracy is improved by optimizing the\nvarious parameters of the multilayer perceptron network.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 19:39:45 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Talukdar", "Jonti", ""], ["Mehta", "Bhavana", ""]]}, {"id": "1708.06858", "submitter": "Yale Song", "authors": "Haojian Jin, Yale Song, Koji Yatani", "title": "ElasticPlay: Interactive Video Summarization with Dynamic Time Budgets", "comments": "ACM Multimedia 2017 preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video consumption is being shifted from sit-and-watch to selective skimming.\nExisting video player interfaces, however, only provide indirect manipulation\nto support this emerging behavior. Video summarization alleviates this issue to\nsome extent, shortening a video based on the desired length of a summary as an\ninput variable. But an optimal length of a summarized video is often not\navailable in advance. Moreover, the user cannot edit the summary once it is\nproduced, limiting its practical applications. We argue that video\nsummarization should be an interactive, mixed-initiative process in which users\nhave control over the summarization procedure while algorithms help users\nachieve their goal via video understanding. In this paper, we introduce\nElasticPlay, a mixed-initiative approach that combines an advanced video\nsummarization technique with direct interface manipulation to help users\ncontrol the video summarization process. Users can specify a time budget for\nthe remaining content while watching a video; our system then immediately\nupdates the playback plan using our proposed cut-and-forward algorithm,\ndetermining which parts to skip or to fast-forward. This interactive process\nallows users to fine-tune the summarization result with immediate feedback. We\nshow that our system outperforms existing video summarization techniques on the\nTVSum50 dataset. We also report two lab studies (22 participants) and a\nMechanical Turk deployment study (60 participants), and show that the\nparticipants responded favorably to ElasticPlay.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 00:25:13 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Jin", "Haojian", ""], ["Song", "Yale", ""], ["Yatani", "Koji", ""]]}, {"id": "1708.07123", "submitter": "Girish Chowdhary", "authors": "Milecia Matthews and Girish Chowdhary and Emily Kieson", "title": "Intent Communication between Autonomous Vehicles and Pedestrians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When pedestrians encounter vehicles, they typically stop and wait for a\nsignal from the driver to either cross or wait. What happens when the car is\nautonomous and there isn't a human driver to signal them? This paper seeks to\naddress this issue with an intent communication system (ICS) that acts in place\nof a human driver. This intent system has been developed to take into account\nthe psychology behind what pedestrians are familiar with and what they expect\nfrom machines. The system integrates those expectations into the design of\nphysical systems and mathematical algorithms. The goal of the system is to\nensure that communication is simple, yet effective without leaving pedestrians\nwith a sense of distrust in autonomous vehicles. To validate the ICS, two types\nof experiments have been run: field tests with an autonomous vehicle to\ndetermine how humans actually interact with the ICS and simulations to account\nfor multiple potential behaviors.The results from both experiments show that\nhumans react positively and more predictably when the intent of the vehicle is\ncommunicated compared to when the intent of the vehicle is unknown. In\nparticular, the results from the simulation specifically showed a 142 percent\ndifference between the pedestrian's trust in the vehicle's actions when the ICS\nis enabled and the pedestrian has prior knowledge of the vehicle than when the\nICS is not enabled and the pedestrian having no prior knowledge of the vehicle.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 13:31:01 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Matthews", "Milecia", ""], ["Chowdhary", "Girish", ""], ["Kieson", "Emily", ""]]}, {"id": "1708.07752", "submitter": "Bart{\\l}omiej Balcerzak", "authors": "Bart{\\l}omiej Balcerzak, Wies{\\l}aw Kope\\'c, Rados{\\l}aw Nielek, Kamil\n  Warpechowski, Agnieszka Czajka", "title": "From close the door to do not click and back. Security by design for\n  older adults", "comments": "arXiv admin note: text overlap with arXiv:1706.10223", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing number of older adults who adopt mobile technology in their\nlife, a new form of challenge faces both them, as well as the software\nengineering communities. This challenge is the issue of safety, not only in the\ncontext of risk older adults already face on-line, but also, due to the mobile\nnature of the used applications, real life safety issues raising from the use\nof on-line solutions. In this paper, we wish to use a case study they conducted\nin order to address this issue of interrelating on-line and real life threats.\nWe describe how the observation from the case study relate to the collected\nbody off knowledge in the relevant topic, as well as propose a set of\nsuggestion for improving the design of applications in regards to addressing\nthe issue of older adults safety.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 09:18:39 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Balcerzak", "Bart\u0142omiej", ""], ["Kope\u0107", "Wies\u0142aw", ""], ["Nielek", "Rados\u0142aw", ""], ["Warpechowski", "Kamil", ""], ["Czajka", "Agnieszka", ""]]}, {"id": "1708.07762", "submitter": "Cihan Kucukkececi", "authors": "Cihan Kucukkececi, Ugur Dogrusoz, Esat Belviranli, and Alptug Dilek", "title": "Chisio: A Compound Graph Editing and Layout Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new free, open-source compound graph editing and layout\nframework named Chisio, based on the Eclipse Graph Editing Framework (GEF) and\nwritten in Java. Chisio can be used as a finished graph editor with its\neasy-to-use graphical interface. The framework has an architecture suitable for\neasy customization of the tool for end-user's specific needs as well. Chisio\ncomes with a variety of graph layout algorithms, most supporting compound\nstructures and non-uniform node dimensions. Furthermore, new algorithms are\nstraightforward to add, making Chisio an ideal test environment for layout\nalgorithm developers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 14:36:07 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Kucukkececi", "Cihan", ""], ["Dogrusoz", "Ugur", ""], ["Belviranli", "Esat", ""], ["Dilek", "Alptug", ""]]}, {"id": "1708.08033", "submitter": "Deokgun Park", "authors": "Deokgun Park, Sung-Hee Kim, Niklas Elmqvist", "title": "Gatherplots: Generalized Scatterplots for Nominal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Overplotting of data points is a common problem when visualizing large\ndatasets in a scatterplot, particularly when mapping nominal dimensions to one\nof the scatterplot axes. Transparency, aggregation, and jittering have\npreviously been suggested to address this issue, but these solutions all have\ndrawbacks for assessing the data distribution in the plot. We propose\ngatherplots, an extension of scatterplots that eliminates overplotting,\nparticularly for nominal variables. In gatherplots, every data point that maps\nto the same position coalesces to form a stacked entity, thereby making it\neasier to compare the absolute and relative sizes of data groupings. The size\nand aspect ratio of data points can also be changed dynamically to make it\neasier to compare the composition of different groups. Furthermore, several\nembedded interaction techniques support slicing and dicing the gatherplot by\npivoting on particular dimensions, ranges, and values in the dataset. Our\nevaluation shows that gatherplots enable users from the general public to judge\nthe relative portion of subgroups more quickly and more correctly than when\nusing conventional scatterplots with jittering. Furthermore, a review conducted\nby a group of visualization experts evaluated and commented on the gatherplot\ndesign.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 01:45:29 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Park", "Deokgun", ""], ["Kim", "Sung-Hee", ""], ["Elmqvist", "Niklas", ""]]}, {"id": "1708.08698", "submitter": "Arnau Oliver", "authors": "Christian Mata, Alain Lalande, Paul Walker, Arnau Oliver, Joan Mart\\'i", "title": "Semi-automated labelling of medical images: benefits of a collaborative\n  work in the evaluation of prostate cancer in MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The goal of this study is to show the advantage of a collaborative\nwork in the annotation and evaluation of prostate cancer tissues from\nT2-weighted MRI compared to the commonly used double blind evaluation.\n  Methods: The variability of medical findings focused on the prostate gland\n(central gland, peripheral and tumoural zones) by two independent experts was\nfirstly evaluated, and secondly compared with a consensus of these two experts.\nUsing a prostate MRI database, experts drew regions of interest (ROIs)\ncorresponding to healthy prostate (peripheral and central zones) and cancer\nusing a semi-automated tool. One of the experts then drew the ROI with\nknowledge of the other expert's ROI.\n  Results: The surface area of each ROI as the Hausdorff distance and the Dice\ncoefficient for each contour were evaluated between the different experiments,\ntaking the drawing of the second expert as the reference. The results showed\nthat the significant differences between the two experts became non-significant\nwith a collaborative work.\n  Conclusions: This study shows that collaborative work with a dedicated tool\nallows a better consensus between expertise than using a double blind\nevaluation. Although we show this for prostate cancer evaluation in T2-weighted\nMRI, the results of this research can be extrapolated to other diseases and\nkind of medical images.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 11:16:38 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Mata", "Christian", ""], ["Lalande", "Alain", ""], ["Walker", "Paul", ""], ["Oliver", "Arnau", ""], ["Mart\u00ed", "Joan", ""]]}, {"id": "1708.08729", "submitter": "Maneesh Bilalpur", "authors": "Maneesh Bilalpur, Seyed Mostafa Kia, Tat-Seng Chua and Ramanathan\n  Subramanian", "title": "Discovering Gender Differences in Facial Emotion Recognition via\n  Implicit Behavioral Cues", "comments": "To be published in the Proceedings of Seventh International\n  Conference on Affective Computing and Intelligent Interaction.2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the utility of implicit behavioral cues in the form of EEG brain\nsignals and eye movements for gender recognition (GR) and emotion recognition\n(ER). Specifically, the examined cues are acquired via low-cost, off-the-shelf\nsensors. We asked 28 viewers (14 female) to recognize emotions from unoccluded\n(no mask) as well as partially occluded (eye and mouth masked) emotive faces.\nObtained experimental results reveal that (a) reliable GR and ER is achievable\nwith EEG and eye features, (b) differential cognitive processing especially for\nnegative emotions is observed for males and females and (c) some of these\ncognitive differences manifest under partial face occlusion, as typified by the\neye and mouth mask conditions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 12:53:46 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Bilalpur", "Maneesh", ""], ["Kia", "Seyed Mostafa", ""], ["Chua", "Tat-Seng", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1708.08735", "submitter": "Maneesh Bilalpur", "authors": "Maneesh Bilalpur, Seyed Mostafa Kia, Manisha Chawla, Tat-Seng Chua and\n  Ramanathan Subramanian", "title": "Gender and Emotion Recognition with Implicit User Signals", "comments": "To be published in the Proceedings of 19th International Conference\n  on Multimodal Interaction.2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the utility of implicit user behavioral signals captured using\nlow-cost, off-the-shelf devices for anonymous gender and emotion recognition. A\nuser study designed to examine male and female sensitivity to facial emotions\nconfirms that females recognize (especially negative) emotions quicker and more\naccurately than men, mirroring prior findings. Implicit viewer responses in the\nform of EEG brain signals and eye movements are then examined for existence of\n(a) emotion and gender-specific patterns from event-related potentials (ERPs)\nand fixation distributions and (b) emotion and gender discriminability.\nExperiments reveal that (i) Gender and emotion-specific differences are\nobservable from ERPs, (ii) multiple similarities exist between explicit\nresponses gathered from users and their implicit behavioral signals, and (iii)\nSignificantly above-chance ($\\approx$70%) gender recognition is achievable on\ncomparing emotion-specific EEG responses-- gender differences are encoded best\nfor anger and disgust. Also, fairly modest valence (positive vs negative\nemotion) recognition is achieved with EEG and eye-based features.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 13:16:46 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Bilalpur", "Maneesh", ""], ["Kia", "Seyed Mostafa", ""], ["Chawla", "Manisha", ""], ["Chua", "Tat-Seng", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1708.08758", "submitter": "Junaid Qadir", "authors": "Ali Harris, Saif ul Islam, Junaid Qadir, Ussama Ahmad Khan", "title": "Persuasive Technology For Human Development: Review and Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technology is an extremely potent tool that can be leveraged for human\ndevelopment and social good. Owing to the great importance of environment and\nhuman psychology in driving human behavior, and the ubiquity of technology in\nmodern life, there is a need to leverage the insights and capabilities of both\nfields together for nudging people towards a behavior that is optimal in some\nsense (personal or social). In this regard, the field of persuasive technology,\nwhich proposes to infuse technology with appropriate design and incentives\nusing insights from psychology, behavioral economics, and human-computer\ninteraction holds a lot of promise. Whilst persuasive technology is already\nbeing developed and is at play in many commercial applications, it can have the\ngreat social impact in the field of Information and Communication Technology\nfor Development (ICTD) which uses Information and Communication Technology\n(ICT) for human developmental ends such as education and health. In this paper\nwe will explore what persuasive technology is and how it can be used for the\nends of human development. To develop the ideas in a concrete setting, we\npresent a case study outlining how persuasive technology can be used for human\ndevelopment in Pakistan, a developing South Asian country, that suffers from\nmany of the problems that plague typical developing country.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 12:01:02 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Harris", "Ali", ""], ["Islam", "Saif ul", ""], ["Qadir", "Junaid", ""], ["Khan", "Ussama Ahmad", ""]]}, {"id": "1708.09404", "submitter": "Mahdi Miraz", "authors": "Mohammed Fakrudeen, Mahdi H. Miraz and Peter Excell", "title": "Success Criteria For Implementing Technology in Special Education: a\n  Case Study", "comments": null, "journal-ref": "the proceedings of the fifth international conference on Internet\n  Technologies and Applications (ITA 13) held at Glynd\\^wr University in\n  Wrexham, UK, 10-13 September 2013,ISBN-10: 0-946881-81-2, ISBN-13:\n  978-0-946881-81-9, pp. 226-235", "doi": "10.13140/2.1.2650.6567", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kingdom of Saudi Arabia (KSA) has made a large investment in deploying\ntechnology to develop the infrastructure and resources for special education.\nThe aims of the present research were to find out the rate of return of these\ninvestments in terms of success and, based on the findings, to propose a\nframework for success criteria. To achieve these aims, a mixed\nmethodology-based research was conducted. Our study found that the use of\ntechnology in special education could not reach the desired level of\nimplementation. We found that various success criteria such as professional\nexperience and technology skills of special educators, administrative support,\nassistive hardware issues and assistive software issues, pedagogical issues,\nand teaching style are the key influencing factors of the implementation\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:33:03 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Fakrudeen", "Mohammed", ""], ["Miraz", "Mahdi H.", ""], ["Excell", "Peter", ""]]}, {"id": "1708.09540", "submitter": "Sukhdev Singh", "authors": "Ayush Sharma, Piyush Bajpai, Sukhdev Singh and Kiran Khatter", "title": "Virtual Reality: Blessings and Risk Assessment", "comments": "22 page and 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: This paper presents an up-to-date overview of research performed\nin the Virtual Reality (VR) environment ranging from definitions, its presence\nin the various fields, and existing market players and their projects in the VR\ntechnology. Further an attempt is made to gain an insight on the psychological\nmechanism underlying experience in using VR device. Methods: Our literature\nsurvey is based on the research articles, analysis of the projects of various\ncompanies and their findings for different areas of interest. Findings: In our\nliterature survey we observed that the recent advances in virtual reality\nenabling technologies have led to variety of virtual devices that facilitate\npeople to interact with the digital world. In fact in the past two decades\nresearchers have tried to integrate reality and VR in the form of intuitive\ncomputer interface. Improvements: This has led to variety of potential benefits\nof VR in many applications such as News, Healthcare, Entertainment, Tourism,\nMilitary and Defence etc. However despite the extensive research efforts in\ncreating virtual system environments it is yet to become apparent in normal\ndaily life.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 02:34:22 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Sharma", "Ayush", ""], ["Bajpai", "Piyush", ""], ["Singh", "Sukhdev", ""], ["Khatter", "Kiran", ""]]}, {"id": "1708.09654", "submitter": "Malay Bhattacharyya", "authors": "Sankar Kumar Mridha, Braznev Sarkar, Sujoy Chatterjee and Malay\n  Bhattacharyya", "title": "Identifying Unsafe Videos on Online Public Media using Real-time\n  Crowdsourcing", "comments": "Works-in-Progress, Fifth AAAI Conference on Human Computation and\n  Crowdsourcing (HCOMP 2017), Quebec City, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the significant growth of social networking and human activities\nthrough the web in recent years, attention to analyzing big data using\nreal-time crowdsourcing has increased. This data may appear in the form of\nstreaming images, audio or videos. In this paper, we address the problem of\ndeciding the appropriateness of streaming videos in public media with the help\nof crowdsourcing in real-time.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 10:25:28 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Mridha", "Sankar Kumar", ""], ["Sarkar", "Braznev", ""], ["Chatterjee", "Sujoy", ""], ["Bhattacharyya", "Malay", ""]]}, {"id": "1708.09662", "submitter": "Malay Bhattacharyya", "authors": "Sujoy Chatterjee, Anirban Mukhopadhyay and Malay Bhattacharyya", "title": "Quality Enhancement by Weighted Rank Aggregation of Crowd Opinion", "comments": "Works-in-Progress, Fifth AAAI Conference on Human Computation and\n  Crowdsourcing (HCOMP 2017), Quebec City, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expertise of annotators has a major role in crowdsourcing based opinion\naggregation models. In such frameworks, accuracy and biasness of annotators are\noccasionally taken as important features and based on them priority of the\nannotators are assigned. But instead of relying on a single feature, multiple\nfeatures can be considered and separate rankings can be produced to judge the\nannotators properly. Finally, the aggregation of those rankings with perfect\nweightage can be done with an aim to produce better ground truth prediction.\nHere, we propose a novel weighted rank aggregation method and its efficacy with\nrespect to other existing approaches is shown on artificial dataset. The\neffectiveness of weighted rank aggregation to enhance quality prediction is\nalso shown by applying it on an Amazon Mechanical Turk (AMT) dataset.\n", "versions": [{"version": "v1", "created": "Thu, 31 Aug 2017 11:04:22 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Chatterjee", "Sujoy", ""], ["Mukhopadhyay", "Anirban", ""], ["Bhattacharyya", "Malay", ""]]}, {"id": "1708.09706", "submitter": "Andreas Maier", "authors": "Gerd H\\\"ausler, Aleksandra Milczarek, Markus Schreiter, Thomas\n  K\\\"astner, Florian Willomitzer, Andreas Maier, Florian Schiffers, Stefan\n  Steidl, Temitope Paul Onanuga, Mathias Unberath, Florian D\\\"otzer, Maike\n  St\\\"ove, Jonas Hajek, Christian Heidorn, Felix H\\\"au{\\ss}ler, Tobias Geimer,\n  Johannes Wendel", "title": "Seminar Innovation Management - Winter Term 2017", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document contains the results obtained by the Innovation Management\nSeminar in winter term 2017. In total 11 ideas have been developed by the team.\nIn the document all 11 ideas show improvements for future applications in\nophthalmology. The 11 ideas are AR/VR Glasses with Medical Applications,\nAugmented Reality Eye Surgery, Game Diagnosis, Intelligent Adapting Glasses, MD\nFacebook, Medical Crowd Segmentation, Personalized 3D Model of the Human Eye,\nPhotoacoustic Contact Lens, Power Supply Smart Contact Lens, VR-Cornea and Head\nMount for Fundus Imaging\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 06:27:18 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["H\u00e4usler", "Gerd", ""], ["Milczarek", "Aleksandra", ""], ["Schreiter", "Markus", ""], ["K\u00e4stner", "Thomas", ""], ["Willomitzer", "Florian", ""], ["Maier", "Andreas", ""], ["Schiffers", "Florian", ""], ["Steidl", "Stefan", ""], ["Onanuga", "Temitope Paul", ""], ["Unberath", "Mathias", ""], ["D\u00f6tzer", "Florian", ""], ["St\u00f6ve", "Maike", ""], ["Hajek", "Jonas", ""], ["Heidorn", "Christian", ""], ["H\u00e4u\u00dfler", "Felix", ""], ["Geimer", "Tobias", ""], ["Wendel", "Johannes", ""]]}]