[{"id": "1812.00048", "submitter": "Iza Marfisi-Schottman", "authors": "Aous Karoui (LIUM), Iza Marfisi-Schottman (LIUM), S\\'ebastien George\n  (LIUM)", "title": "Mobile Learning Game Authoring Tools: Assessment, Synthesis and\n  Proposals", "comments": "European Games and Learning Alliance conference, GALA, Dec 2016,\n  Utrech, Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Learning Games (MLGs) show great potential for increasing engagement,\ncreativity and authentic learning. Yet, despite their great potential for\neducation, the use of MLGs by teachers, remains limited. This is partly due to\nthe fact that MLGs are often designed to match a specific learning context, and\nthus cannot be reusable for other contexts. Therefore, researchers have\nrecently designed various types of MLG authoring tools. However, these\nauthoring tools are not always adapted to non-computer-scientists or\nnon-game-designers. Hence, we propose in this paper to focus on five existing\nMLG authoring tools, in order to assess their features and usability with the\nhelp of five teachers, who are used to organizing educational field trips. In\nthe second part of this paper, we present an approach for designing a MLG\nauthoring tool, based on the lacks identified through the analysis, and\ntailored to the teachers' different profiles and needs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 07:48:51 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Karoui", "Aous", "", "LIUM"], ["Marfisi-Schottman", "Iza", "", "LIUM"], ["George", "S\u00e9bastien", "", "LIUM"]]}, {"id": "1812.00148", "submitter": "Sooyeon Lee", "authors": "Sooyeon Lee, Madison Reddie, Krish Gurdasani, Xiying Wang, Jordan\n  Beck, Mary Beth Rosson, John M. Carroll", "title": "Conversations for Vision: Remote Sighted Assistants Helping People with\n  Visual Impairments", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People with visual impairment (PVI) must interact with a world they cannot\nsee. Remote sighted assistance has emerged as a conversational/social support\nsystem. We interviewed participants who either provide or receive assistance\nvia a conversational/social prosthetic called Aira (https://aira.io/). We\nidentified four types of support provided: scene description, performance,\nsocial interaction, and navigation. We found that conversational style is\ncontext-dependent. Sighted assistants make intentional efforts to elicit PVI's\npersonal knowledge and leverage it in the guidance they provide. PVI used\nnon-verbal behaviors (e.g. hand gestures) as a parallel communication channel\nto provide feedback or guidance to sighted assistants. We also discuss\nimplications for design.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 05:29:38 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Lee", "Sooyeon", ""], ["Reddie", "Madison", ""], ["Gurdasani", "Krish", ""], ["Wang", "Xiying", ""], ["Beck", "Jordan", ""], ["Rosson", "Mary Beth", ""], ["Carroll", "John M.", ""]]}, {"id": "1812.00263", "submitter": "Varun Chandrasekaran", "authors": "Varun Chandrasekaran, Suman Banerjee, Bilge Mutlu, Kassem Fawaz", "title": "PowerCut and Obfuscator: An Exploration of the Design Space for\n  Privacy-Preserving Interventions for Voice Assistants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pervasive use of smart speakers has raised numerous privacy concerns.\nWhile work to date provides an understanding of user perceptions of these\nthreats, limited research focuses on how we can mitigate these concerns, either\nthrough redesigning the smart speaker or through dedicated privacy-preserving\ninterventions. In this paper, we present the design and prototyping of two\nprivacy-preserving interventions: `Obfuscator' targeted at disabling recording\nat the microphones, and `PowerCut' targeted at disabling power to the smart\nspeaker. We present our findings from a technology probe study involving 24\nhouseholds that interacted with our prototypes; the primary objective was to\ngain a better understanding of the design space for technological interventions\nthat might address these concerns. Our data and findings reveal complex\ntrade-offs among utility, privacy, and usability and stresses the importance of\nmulti-functionality, aesthetics, ease-of-use, and form factor. We discuss the\nimplications of our findings for the development of subsequent interventions\nand the future design of smart speakers.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 21:04:04 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 12:02:43 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 18:58:02 GMT"}, {"version": "v4", "created": "Wed, 9 Jun 2021 18:24:53 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Chandrasekaran", "Varun", ""], ["Banerjee", "Suman", ""], ["Mutlu", "Bilge", ""], ["Fawaz", "Kassem", ""]]}, {"id": "1812.00474", "submitter": "Florian Lemmerich", "authors": "Florian Lemmerich, Diego S\\'aez-Trumper, Robert West, Leila Zia", "title": "Why the World Reads Wikipedia: Beyond English Speakers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As one of the Web's primary multilingual knowledge sources, Wikipedia is read\nby millions of people across the globe every day. Despite this global\nreadership, little is known about why users read Wikipedia's various language\neditions. To bridge this gap, we conduct a comparative study by combining a\nlarge-scale survey of Wikipedia readers across 14 language editions with a\nlog-based analysis of user activity. We proceed in three steps. First, we\nanalyze the survey results to compare the prevalence of Wikipedia use cases\nacross languages, discovering commonalities, but also substantial differences,\namong Wikipedia languages with respect to their usage. Second, we match survey\nresponses to the respondents' traces in Wikipedia's server logs to characterize\nbehavioral patterns associated with specific use cases, finding that\ndistinctive patterns consistently mark certain use cases across language\neditions. Third, we show that certain Wikipedia use cases are more common in\ncountries with certain socio-economic characteristics; e.g., in-depth reading\nof Wikipedia articles is substantially more common in countries with a low\nHuman Development Index. These findings advance our understanding of reader\nmotivations and behaviors across Wikipedia languages and have implications for\nWikipedia editors and developers of Wikipedia and other Web technologies.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 21:52:13 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Lemmerich", "Florian", ""], ["S\u00e1ez-Trumper", "Diego", ""], ["West", "Robert", ""], ["Zia", "Leila", ""]]}, {"id": "1812.00836", "submitter": "\\'Alvaro Fern\\'andez-Rodr\\'iguez", "authors": "\\'Alvaro Fern\\'andez-Rodr\\'iguez, Francisco Velasco-\\'Alvarez, Mar\\'ia\n  Teresa Medina-Juli\\'a and Ricardo Ron-Angevin", "title": "Evaluation of flashing stimuli shape and colour heterogeneity using a\n  P300 brain-computer interface speller", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Previous works using a visual P300-based speller have reported an\nimprovement modifying the shape or colour of the presented stimulus. However,\nthe effects of both blended factors have not been yet studied. Thus, the aim of\nthe present work was to study both factors and assess the interaction between\nthem. Method: Fifteen na\\\"ive participants tested four different spellers in a\ncalibration and online task. All spellers were similar except the employed\nillumination of the target stimulus: white letters, white blocks, coloured\nletters, and coloured blocks. Results: The block-shaped conditions offered an\nimprovement versus the letter-shaped conditions in the calibration (accuracy)\nand online (accuracy and correct commands per minute) tasks. Analysis of the\nP300 waveform showed a larger difference between target and no target stimulus\nwaveforms for the block-shaped conditions versus the letter-shaped. The\nhypothesis regarding the colour heterogeneity of the stimuli was not found at\nany level of the analysis. Conclusion: The use of block-shaped illumination\ndemonstrated a better performance than the standard letter-shaped flashing\nstimuli in classification performance, correct commands per minute, and P300\nwaveform.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 15:31:35 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Fern\u00e1ndez-Rodr\u00edguez", "\u00c1lvaro", ""], ["Velasco-\u00c1lvarez", "Francisco", ""], ["Medina-Juli\u00e1", "Mar\u00eda Teresa", ""], ["Ron-Angevin", "Ricardo", ""]]}, {"id": "1812.01499", "submitter": "Rui Portocarrero Sarmento MSc", "authors": "Rui Portocarrero Sarmento, Andr\\'e Tarrinho, Pedro C\\^amara, Vera\n  Costa", "title": "A System for Efficient Communication between Patients and Pharmacies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When studying human-technology interaction systems, researchers thrive to\nachieve intuitiveness and facilitate the people's life through a thoughtful and\nin-depth study of several components of the application system that supports\nsome particular business communication with customers. Particularly in the\nhealthcare field, some requirements such as clarity, transparency, efficiency,\nand speed in transmitting information to patients and or healthcare\nprofessionals might mean an important increase in the well-being of the patient\nand productivity of the healthcare professional. In this work, the authors\nstudy the difficulties patients frequently have when communicating with\npharmacists. In addition to a statistical study of a survey conducted with more\nthan two hundred frequent pharmacy customers, we propose an IT solution for\nbetter communication between patients and pharmacists.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 16:01:43 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Sarmento", "Rui Portocarrero", ""], ["Tarrinho", "Andr\u00e9", ""], ["C\u00e2mara", "Pedro", ""], ["Costa", "Vera", ""]]}, {"id": "1812.01638", "submitter": "Nitish Nag", "authors": "Nitish Nag, Ramesh Jain", "title": "A Navigational Approach to Health: Actionable Guidance for Improved\n  Quality of Life", "comments": "IEEE Computer - April 2019", "journal-ref": "Computer 52 (4), 12-20, 2019", "doi": "10.1109/MC.2018.2883280", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lifestyle and environment interacting with our biological machine are\nprimarily responsible for shaping our health and wellbeing. Continuous,\nmulti-modal, and quantitative approaches to understanding and controlling these\nfactors will allow each person to better reach their desired quality of life. A\nnavigational paradigm can help users towards a specified health goal by using\nconstantly captured measurements to feed estimations of how a user's health is\ncontinuously changing in order to provide actionable guidance. As various\nactions are taken by the user, measurements of the resulting effects loop back\ninto the estimation and the next step of guidance. This perpetual cycle of\nmeasuring, estimating, guiding, and acting articulates a Personal Health\nNavigation information and actuation framework. Personal Health Navigation\nfocuses on fulfilling a user's health goals by ensuring minimal deviation from\nhealthy states, rather than treating disease or symptoms after derailment from\nproper biological function.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 19:27:49 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 02:03:48 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Nag", "Nitish", ""], ["Jain", "Ramesh", ""]]}, {"id": "1812.01706", "submitter": "Zhenan Feng", "authors": "Zhenan Feng, Vicente A. Gonz\\'alez, Ling Ma, Mustafa M.A. Al-Adhami,\n  Claudio Mourgues", "title": "Rapid 3D Reconstruction of Indoor Environments to Generate Virtual\n  Reality Serious Games Scenarios", "comments": null, "journal-ref": "Proceedings of the 18th International Conference on Construction\n  Applications of Virtual Reality (2018) 185-195", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) for Serious Games (SGs) is attracting increasing\nattention for training applications due to its potential to provide\nsignificantly enhanced learning to users. Some examples of the application of\nVR for SGs are complex training evacuation problems such as indoor earthquake\nevacuation or fire evacuation. The indoor 3D geometry of existing buildings can\nlargely influence evacuees' behaviour, being instrumental in the design of VR\nSGs storylines and simulation scenarios. The VR scenarios of existing buildings\ncan be generated from drawings and models. However, these data may not reflect\nthe 'as-is' state of the indoor environment and may not be suitable to reflect\ndynamic changes of the system (e.g. Earthquakes), resulting in excessive\ndevelopment efforts to design credible and meaningful user experience. This\npaper explores several workflows for the rapid and effective reconstruction of\n3D indoor environments of existing buildings that are suitable for earthquake\nsimulations. These workflows start from Building Information Modelling (BIM),\nlaser scanning and 360-degree panoramas. We evaluated the feasibility and\nefficiency of different approaches by using an earthquake-based case study\ndeveloped for VR SGs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 21:58:49 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Feng", "Zhenan", ""], ["Gonz\u00e1lez", "Vicente A.", ""], ["Ma", "Ling", ""], ["Al-Adhami", "Mustafa M. A.", ""], ["Mourgues", "Claudio", ""]]}, {"id": "1812.01766", "submitter": "Dong Ma", "authors": "Dong Ma, Guohao Lan, Mahbub Hassan, Wen Hu, Mushfika B. Upama, Ashraf\n  Uddin, Moustafa Youssef", "title": "SolarGest: Ubiquitous and Battery-free Gesture Recognition using Solar\n  Cells", "comments": "15 pages, 20 figures, 4 tables, MobiCom 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a system, SolarGest, which can recognize hand gestures near a\nsolar-powered device by analyzing the patterns of the photocurrent. SolarGest\nis based on the observation that each gesture interferes with incident light\nrays on the solar panel in a unique way, leaving its distinguishable signature\nin harvested photocurrent. Using solar energy harvesting laws, we develop a\nmodel to optimize design and usage of SolarGest. To further improve the\nrobustness of SolarGest under non-deterministic operating conditions, we\ncombine dynamic time warping with Z-score transformation in a signal processing\npipeline to pre-process each gesture waveform before it is analyzed for\nclassification. We evaluate SolarGest with both conventional opaque solar cells\nas well as emerging see-through transparent cells. Our experiments with 6,960\ngesture samples for 6 different gestures reveal that even with transparent\ncells, SolarGest can detect 96% of the gestures while consuming 44% less power\ncompared to light sensor based systems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 00:59:06 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 08:18:03 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Ma", "Dong", ""], ["Lan", "Guohao", ""], ["Hassan", "Mahbub", ""], ["Hu", "Wen", ""], ["Upama", "Mushfika B.", ""], ["Uddin", "Ashraf", ""], ["Youssef", "Moustafa", ""]]}, {"id": "1812.01777", "submitter": "Huanle Zhang", "authors": "Huanle Zhang, Ahmed Elmokashfi, Zhicheng Yang, Prasant Mohapatra", "title": "Wireless Access to Ultimate Virtual Reality 360-Degree Video At Home", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality 360-degree videos will become the first prosperous online VR\napplication. VR 360 videos are data-hungry and latency-sensitive that pose\nunique challenges to the networking infrastructure. In this paper, we focus on\nthe ultimate VR 360 that satisfies human eye fidelity. The ultimate VR 360\nrequires downlink 1.5 Gbps for viewing and uplink 6.6 Gbps for live\nbroadcasting, with round-trip time of less than 8.3 ms. On the other hand,\nwireless access to VR 360 services is preferred over wire-line transmission\nbecause of the better user experience and the safety concern (e.g., tripping\nhazard). We explore in this paper whether the most advanced wireless\ntechnologies from both cellular communications and WiFi communications support\nthe ultimate VR 360. Specifically, we consider 5G in cellular communications,\nIEEE 802.11ac (operating in 5GHz) and IEEE 802.11ad (operating in 60GHz) in\nWiFi communications. According to their performance specified in their\nstandards and/or empirical measurements, we have the following findings: (1)\nOnly 5G has the potential to support both the the ultimate VR 360 viewing and\nlive broadcasting. However, it is difficult for 5G to support multiple users of\nthe ultimate VR live broadcasting at home; (2) IEEE 802.11ac supports the\nultimate VR 360 viewing but fails to support the ultimate VR 360 live\nbroadcasting because it does not meet the data rate requirement of the ultimate\nVR 360 live broadcasting; (3) IEEE 802.11ad fails to support the ultimate VR\n360, because its current implementation incurs very high latency. Our\npreliminary results indicate that more advanced wireless technologies are\nneeded to fully support multiple ultimate VR 360 users at home.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 01:54:53 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 21:44:59 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Zhang", "Huanle", ""], ["Elmokashfi", "Ahmed", ""], ["Yang", "Zhicheng", ""], ["Mohapatra", "Prasant", ""]]}, {"id": "1812.02088", "submitter": "Maartje Maria Elisabeth Hendrikse", "authors": "Maartje M. E. Hendrikse, Gerard Llorach, Giso Grimm and Volker Hohmann", "title": "Influence of visual cues on head and eye movements during listening\n  tasks in multi-talker audiovisual environments with animated characters", "comments": "Accepted manuscript, 29 pages, 7 figures", "journal-ref": "Speech Communication 101 (2018) 70-84", "doi": "10.1016/j.specom.2018.05.008", "report-no": null, "categories": "physics.med-ph cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent studies of hearing aid benefits indicate that head movement behavior\ninfluences performance. To systematically assess these effects, movement\nbehavior must be measured in realistic communication conditions. For this, the\nuse of virtual audiovisual environments with animated characters as visual\nstimuli has been proposed. It is unclear, however, how these animations\ninfluence the head- and eye-movement behavior of subjects. Here, two listening\ntasks were carried out with a group of 14 young normal hearing subjects to\ninvestigate the influence of visual cues on head- and eye-movement behavior; on\ncombined localization and speech intelligibility task performance; as well as\non perceived speech intelligibility, perceived listening effort and the general\nimpression of the audiovisual environments. Animated characters with different\nlip-syncing and gaze patterns were compared to an audio-only condition and to a\nvideo of real persons. Results show that movement behavior, task performance,\nand perception were all influenced by visual cues. The movement behavior of\nyoung normal hearing listeners in animation conditions with lip-syncing was\nsimilar to that in the video condition. These results in young normal hearing\nlisteners are a first step towards using the animated characters to assess the\ninfluence of head movement behavior on hearing aid performance.\n", "versions": [{"version": "v1", "created": "Fri, 16 Nov 2018 10:12:21 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Hendrikse", "Maartje M. E.", ""], ["Llorach", "Gerard", ""], ["Grimm", "Giso", ""], ["Hohmann", "Volker", ""]]}, {"id": "1812.02129", "submitter": "Michael Segundo Ortiz", "authors": "Michael Segundo Ortiz, Kazuhiro Seki, Javed Mostafa", "title": "Toward Exploratory Search in Biomedicine: Evaluating Document Clusters\n  by MeSH as a Semantic Anchor", "comments": "This work is currently under consideration for the 17th World\n  Congress of Medical and Health Informatics. Please follow the link for more\n  information - http://www.medinfo-lyon.org/en/about-us/medinfo2019/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current mode of biomedical literature search is severely limited in\neffectively finding information relevant to specialists. A potential approach\nto solving this problem is exploratory search, which allows users to\ninteractively navigate through a vast document collection. As the first step\ntoward exploratory search for specialists in biomedicine, this paper develops a\nmethodology to evaluate quality of document clusters. For this purpose, we\nincorporate human expertise into data set creation and evaluation framework by\nleveraging MeSH terms as semantic anchors. In addition, we investigate the\nbenefit of full-text data for improving cluster quality.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:31:47 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Ortiz", "Michael Segundo", ""], ["Seki", "Kazuhiro", ""], ["Mostafa", "Javed", ""]]}, {"id": "1812.02197", "submitter": "Jens Grubert", "authors": "Jens Grubert, Eyal Ofek, Michel Pahud, Per Ola Kristensson", "title": "The Office of the Future: Virtual, Portable and Global", "comments": null, "journal-ref": null, "doi": "10.1109/MCG.2018.2875609", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality has the potential to change the way we work. We envision the\nfuture office worker to be able to work productively everywhere solely using\nportable standard input devices and immersive head-mounted displays. Virtual\nReality has the potential to enable this, by allowing users to create working\nenvironments of their choice and by relieving them from physical world\nlimitations such as constrained space or noisy environments. In this article,\nwe investigate opportunities and challenges for realizing this vision and\ndiscuss implications from recent findings of text entry in virtual reality as a\ncore office task.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 19:35:40 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Grubert", "Jens", ""], ["Ofek", "Eyal", ""], ["Pahud", "Michel", ""], ["Kristensson", "Per Ola", ""]]}, {"id": "1812.02272", "submitter": "Emilio Ferrara", "authors": "Yilei Zeng, Anna Sapienza, Emilio Ferrara", "title": "The Influence of Social Ties on Performance in Team-based Online Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social ties are the invisible glue that keeps together human ecosystems.\nDespite the massive amount of research studying the role of social ties in\ncommunities (groups, teams, etc.) and society at large, little attention has\nbeen devoted to study their interplay with other human behavioral dynamics. Of\nparticular interest is the influence that social ties have on human performance\nin collaborative team-based settings. Our research aims to elucidate the\ninfluence of social ties on individual and team performance dynamics. We will\nfocus on a popular Multiplayer Online Battle Arena (MOBA) collaborative\nteam-based game, Defense of the Ancients 2 (Dota 2), a rich dataset with\nmillions of players and matches. Our research reveals that, when playing with\ntheir friends, individuals are systematically more active in the game as\nopposed to taking part in a team of strangers. However, we find that increased\nactivity does not homogeneously lead to an improvement in players' performance.\nDespite being beneficial to low skill players, playing with friends negatively\naffects performance of high skill players. Our findings shed light on the mixed\ninfluence of social ties on performance, and can inform new perspectives on\nvirtual team management and on behavioral incentives.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 23:55:21 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Zeng", "Yilei", ""], ["Sapienza", "Anna", ""], ["Ferrara", "Emilio", ""]]}, {"id": "1812.02357", "submitter": "Zeyad Al-Odat", "authors": "Zeyad A. Al-Odat, Sudarshan K. Srinivasan, Eman Al-qtiemat, Mohana\n  Asha Latha Dubasi, Sana Shuja", "title": "IoT-Based Secure Embedded Scheme for Insulin Pump Data Acquisition and\n  Monitoring", "comments": "4 pages, 4 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an Internet of Things (IoT)-based data acquisition and\nmonitoring scheme for insulin pumps. The proposed work employs embedded system\nhardware (Keil LPC1768-board) for data acquisition and monitoring. The hardware\nis used as an abstract layer between the insulin pump and the cloud. Diabetes\ndata are secured before they are sent to the cloud for storage. Each patient's\nrecord is digitally signed using a secure hash algorithm mechanism. The\nproposed work will protect the patient's records from being breached from\nunauthorized entities, and authenticates them from improper modifications. The\ndesign is tested and verified using $\\mu$Vision studio, the Keil board\nmentioned above, and an ALARIS 8100 infusion pump. Moreover, a test case for a\nreal cloud example is presented with the help of the Center of Computationally\nAssisted System and Technology. This center provided the infrastructure service\nto test our work.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 05:32:35 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Al-Odat", "Zeyad A.", ""], ["Srinivasan", "Sudarshan K.", ""], ["Al-qtiemat", "Eman", ""], ["Dubasi", "Mohana Asha Latha", ""], ["Shuja", "Sana", ""]]}, {"id": "1812.02522", "submitter": "Peter O. Fedichev", "authors": "Evgeny Getmantsev, Boris Zhurov, Timothy V. Pyrkov, and Peter O.\n  Fedichev", "title": "A novel health risk model based on intraday physical activity time\n  series collected by smartphones", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compiled a demo application and collected a motion database of more than\n10,000 smartphone users to produce a health risk model trained on physical\nactivity streams. We turned to adversarial domain adaptation and employed the\nUK Biobank dataset of motion data, augmented by a rich set of clinical\ninformation as the source domain to train the model using a deep residual\nconvolutional neuron network (ResNet). The model risk score is a biomarker of\nageing, since it was predictive of lifespan and healthspan (as defined by the\nonset of specified diseases), and was elevated in groups associated with\nlife-shortening lifestyles, such as smoking. We ascertained the target domain\nperformance in a smaller cohort of the mobile application that included users\nwho were willing to share answers to a short questionnaire related to their\ndisease and smoking status. We thus conclude that the proposed pipeline\ncombining deep convolutional and Domain Adversarial neuron networks (DANN) is a\npowerful tool for disease risk and lifestyle-associated hazard assessment from\nmobile motion sensors that are transferable across devices and populations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 13:46:15 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Getmantsev", "Evgeny", ""], ["Zhurov", "Boris", ""], ["Pyrkov", "Timothy V.", ""], ["Fedichev", "Peter O.", ""]]}, {"id": "1812.02630", "submitter": "S\\\"oren Merting", "authors": "Martin Bichler, S\\\"oren Merting, Aykut Uzunoglu", "title": "Assigning Course Schedules: About Preference Elicitation, Fairness, and\n  Truthfulness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.DM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Course assignment is a wide-spread problem in education and beyond. Often\nstudents have preferences for bundles of course seats or course schedules over\nthe week, which need to be considered. The problem is a challenging distributed\nscheduling task requiring decision support. First-Come First-Served (FCFS) is\nsimple and the most widely used assignment rule in practice, but it leads to\ninefficient outcomes and envy in the allocation. Recent theoretical results\nsuggest alternatives with attractive economic and computational properties.\nBundled Probabilistic Serial (BPS) is a randomized mechanism satisfying ordinal\nefficiency, envy-freeness, and weak strategy-proofness. This mechanism also\nruns in polynomial time, which is important for the large problem instances in\nthe field. We report empirical results from a first implementation of BPS at\nthe Technical University of Munich, which allows us to provide important\nempirical metrics such as the size of the resulting matching, the average rank,\nthe profile, and the popularity of the assignments. These metrics were central\nfor the adoption of BPS. In particular, we compare these metrics to Random\nSerial Dictatorship with bundle bids (BRSD). The BRSD mechanism is used to\nsimulate the wide-spread First-Come First-Served (FCFS) mechanism and it allows\nus to compare FCFS (BRSD) and BPS. While theoretically appealing, preference\nelicitation is a major challenge when considering preferences over\nexponentially many packages. We introduce tools to elicit preferences which\nreduce the number of parameters a student needs to a manageable set. The\napproach together with BPS yields a computationally effective tool to solve\ncourse assignment problems with thousands of students, and possibly provides an\napproach for other distributed scheduling tasks in organizations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 16:11:16 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 08:53:00 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Bichler", "Martin", ""], ["Merting", "S\u00f6ren", ""], ["Uzunoglu", "Aykut", ""]]}, {"id": "1812.02736", "submitter": "Yuan Jin", "authors": "Yuan Jin and Mark Carman and Ye Zhu and Yong Xiang", "title": "A Technical Survey on Statistical Modelling and Design Methods for\n  Crowdsourcing Quality Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online crowdsourcing provides a scalable and inexpensive means to collect\nknowledge (e.g. labels) about various types of data items (e.g. text, audio,\nvideo). However, it is also known to result in large variance in the quality of\nrecorded responses which often cannot be directly used for training machine\nlearning systems. To resolve this issue, a lot of work has been conducted to\ncontrol the response quality such that low-quality responses cannot adversely\naffect the performance of the machine learning systems. Such work is referred\nto as the quality control for crowdsourcing. Past quality control research can\nbe divided into two major branches: quality control mechanism design and\nstatistical models. The first branch focuses on designing measures, thresholds,\ninterfaces and workflows for payment, gamification, question assignment and\nother mechanisms that influence workers' behaviour. The second branch focuses\non developing statistical models to perform effective aggregation of responses\nto infer correct responses. The two branches are connected as statistical\nmodels (i) provide parameter estimates to support the measure and threshold\ncalculation, and (ii) encode modelling assumptions used to derive (theoretical)\nperformance guarantees for the mechanisms. There are surveys regarding each\nbranch but they lack technical details about the other branch. Our survey is\nthe first to bridge the two branches by providing technical details on how they\nwork together under frameworks that systematically unify crowdsourcing aspects\nmodelled by both of them to determine the response quality. We are also the\nfirst to provide taxonomies of quality control papers based on the proposed\nframeworks. Finally, we specify the current limitations and the corresponding\nfuture directions for the quality control research.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 22:29:42 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Jin", "Yuan", ""], ["Carman", "Mark", ""], ["Zhu", "Ye", ""], ["Xiang", "Yong", ""]]}, {"id": "1812.02782", "submitter": "Karan Sharma", "authors": "Karan Sharma, Claudio Castellini, Egon L. van den Broek, Alin\n  Albu-Schaeffer and Friedhelm Schwenker", "title": "A dataset of continuous affect annotations and physiological signals for\n  emotion analysis", "comments": "Dataset available at:\n  https://rmc.dlr.de/download/CASE_dataset/CASE_dataset.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a computational viewpoint, emotions continue to be intriguingly hard to\nunderstand. In research, direct, real-time inspection in realistic settings is\nnot possible. Discrete, indirect, post-hoc recordings are therefore the norm.\nAs a result, proper emotion assessment remains a problematic issue. The\nContinuously Annotated Signals of Emotion (CASE) dataset provides a solution as\nit focusses on real-time continuous annotation of emotions, as experienced by\nthe participants, while watching various videos. For this purpose, a novel,\nintuitive joystick-based annotation interface was developed, that allowed for\nsimultaneous reporting of valence and arousal, that are instead often annotated\nindependently. In parallel, eight high quality, synchronized physiological\nrecordings (1000 Hz, 16-bit ADC) were made of ECG, BVP, EMG (3x), GSR (or EDA),\nrespiration and skin temperature. The dataset consists of the physiological and\nannotation data from 30 participants, 15 male and 15 female, who watched\nseveral validated video-stimuli. The validity of the emotion induction, as\nexemplified by the annotation and physiological data, is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 20:09:24 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Sharma", "Karan", ""], ["Castellini", "Claudio", ""], ["Broek", "Egon L. van den", ""], ["Albu-Schaeffer", "Alin", ""], ["Schwenker", "Friedhelm", ""]]}, {"id": "1812.02867", "submitter": "Robert Bridges", "authors": "Robert A. Bridges and Michael D. Iannacone and John R. Goodall and\n  Justin M. Beaver", "title": "How do information security workers use host data? A summary of\n  interviews with security analysts", "comments": "interviews with 13 security analysts about host data, tools, desires", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern security operations centers (SOCs) employ a variety of tools for\nintrusion detection, prevention, and widespread log aggregation and analysis.\nWhile research efforts are quickly proposing novel algorithms and technologies\nfor cyber security, access to actual security personnel, their data, and their\nproblems are necessarily limited by security concerns and time constraints. To\nhelp bridge the gap between researchers and security centers, this paper\nreports results of semi-structured interviews of 13 professionals from five\ndifferent SOCs including at least one large academic, research, and government\norganization. The interviews focused on the current practices and future\ndesires of SOC operators about host-based data collection capabilities, what is\nlearned from the data, what tools are used, and how tools are evaluated.\nQuestions and the responses are organized and reported by topic. Then broader\nthemes are discussed. Forest-level takeaways from the interviews center on\nproblems stemming from size of data, correlation of heterogeneous but related\ndata sources, signal-to-noise ratio of data, and analysts' time.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 01:24:22 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Bridges", "Robert A.", ""], ["Iannacone", "Michael D.", ""], ["Goodall", "John R.", ""], ["Beaver", "Justin M.", ""]]}, {"id": "1812.02945", "submitter": "Richard Romano", "authors": "Richard Romano, Gustav Markkula, Erwin Boer, Hamish Jamson, Alex Bean,\n  Andrew Tomlinson, Anthony Horrobin, Ehsan Sadraei", "title": "An Objective Assessment of the Utility of a Driving Simulator for Low Mu\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driving simulators can be used to test vehicle designs earlier, prior to\nbuilding physical prototypes. One area of particular interest is winter testing\nsince testing is limited to specific times of year and specific regions in the\nworld. To ensure that the simulator is fit for purpose, an objective assessment\nis required. In this study a simulator and real world comparison was performed\nwith three simulator configurations (standard, no steering torque, no motion)\nto assess the ability of a utility triplet of analyses to be able to quantify\nthe differences between the real world and the different simulator\nconfigurations. The results suggest that the utility triplet is effective in\nmeasuring the differences in simulator configurations and that the developed\nVirtual Sweden environment achieved rather good behavioural fidelity in the\nsense of preserving absolute levels of many measures of behaviour. The main\nlimitation in the simulated environment seemed to be the poor match of the\ndynamic lateral friction limit on snow and ice when compared to the real world.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 08:42:56 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Romano", "Richard", ""], ["Markkula", "Gustav", ""], ["Boer", "Erwin", ""], ["Jamson", "Hamish", ""], ["Bean", "Alex", ""], ["Tomlinson", "Andrew", ""], ["Horrobin", "Anthony", ""], ["Sadraei", "Ehsan", ""]]}, {"id": "1812.03066", "submitter": "Gregoire Cattan", "authors": "Gr\\'egoire Cattan (GIPSA-Services, IHMTEK), Anton Andreev\n  (GIPSA-Services, CNRS), Bastien Maureille (IHMTEK), Marco Congedo\n  (GIPSA-Services)", "title": "Analysis of tagging latency when comparing event-related potentials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-related potentials (ERPs) are very small voltage produced by the brain\nin response to external stimulation. In order to detect and evaluate an ERP in\nan ongoing electroencephalogram (EEG), it is necessary to tag the EEG with the\nexact onset time of the stimulus. We define the latency as the delay between\nthe time the tagging command is sent and the detection of the stimulus on the\nscreen. Failing to control sequencing in the tagging pipeline causes problems\nwhen interpreting latency, in particular when comparing ERPs generated from\nstimuli displayed by different systems. In this work, we present number of\ntechnical aspects which can influence latency such as the refresh rate of the\nscreen or the display of a stimulus at different screen location. A few\npropositions are suggested to estimate and correct this latency.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 15:19:46 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Cattan", "Gr\u00e9goire", "", "GIPSA-Services, IHMTEK"], ["Andreev", "Anton", "", "GIPSA-Services, CNRS"], ["Maureille", "Bastien", "", "IHMTEK"], ["Congedo", "Marco", "", "GIPSA-Services"]]}, {"id": "1812.03125", "submitter": "Batu Aytemiz", "authors": "Zeping Zhan, Batu Aytemiz, Adam M. Smith", "title": "Taking the Scenic Route: Automatic Exploration for Videogames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine playtesting tools and game moment search engines require exposure to\nthe diversity of a game's state space if they are to report on or index the\nmost interesting moments of possible play. Meanwhile, mobile app distribution\nservices would like to quickly determine if a freshly-uploaded game is fit to\nbe published. Having access to a semantic map of reachable states in the game\nwould enable efficient inference in these applications. However, human gameplay\ndata is expensive to acquire relative to the coverage of a game that it\nprovides. We show that off-the-shelf automatic exploration strategies can\nexplore with an effectiveness comparable to human gameplay on the same\ntimescale. We contribute generic methods for quantifying exploration quality as\na function of time and demonstrate our metric on several elementary techniques\nand human players on a collection of commercial games sampled from multiple\ngame platforms (from Atari 2600 to Nintendo 64). Emphasizing the diversity of\nstates reached and the semantic map extracted, this work makes productive\ncontrast with the focus on finding a behavior policy or optimizing game score\nused in most automatic game playing research.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 17:38:46 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Zhan", "Zeping", ""], ["Aytemiz", "Batu", ""], ["Smith", "Adam M.", ""]]}, {"id": "1812.03200", "submitter": "Evgeny Burnaev", "authors": "Nikita Khromov and Alexander Korotin and Andrey Lange and Anton\n  Stepanov and Evgeny Burnaev and Andrey Somov", "title": "Esports Athletes and Players: a Comparative Study", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comparative study of the players' and professional players'\n(athletes') performance in Counter Strike: Global Offensive (CS:GO) discipline.\nOur study is based on ubiquitous sensing helping identify the biometric\nfeatures significantly contributing to the classification of particular skills\nof the players. The research provides better understanding why the athletes\ndemonstrate superior performance as compared to other players.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 20:09:57 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 07:10:04 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 06:58:19 GMT"}, {"version": "v4", "created": "Sun, 18 Aug 2019 08:16:54 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Khromov", "Nikita", ""], ["Korotin", "Alexander", ""], ["Lange", "Andrey", ""], ["Stepanov", "Anton", ""], ["Burnaev", "Evgeny", ""], ["Somov", "Andrey", ""]]}, {"id": "1812.03219", "submitter": "Aaron Springer", "authors": "Aaron Springer, Victoria Hollis, Steve Whittaker", "title": "Dice in the Black Box: User Experiences with an Inscrutable Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that users may be prone to place an inordinate amount of trust\nin black box algorithms that are framed as intelligent. We deploy an algorithm\nthat purportedly assesses the positivity and negativity of a users' writing\nemotional writing. In actuality, the algorithm responds in a random fashion. We\nqualitatively examine the paths to trust that users followed while testing the\nsystem. In light of the ease with which users may trust systems exhibiting\n\"intelligent behavior\" we recommend corrective approaches.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 21:37:49 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Springer", "Aaron", ""], ["Hollis", "Victoria", ""], ["Whittaker", "Steve", ""]]}, {"id": "1812.03220", "submitter": "Aaron Springer", "authors": "Aaron Springer, Steve Whittaker", "title": "What Are You Hiding? Algorithmic Transparency and User Perceptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensive recent media focus has been directed towards the dark side of\nintelligent systems, how algorithms can influence society negatively. Often,\ntransparency is proposed as a solution or step in the right direction.\nUnfortunately, research is mixed on the impact of transparency on the user\nexperience. We examine transparency in the context an interactive system that\npredicts positive/negative emotion from a users' written text. We unify\nseemingly this contradictory research under a single model. We show that\ntransparency can negatively affect accuracy perceptions for users whose\nexpectations were not violated by the system's prediction; however,\ntransparency also limits the damage done when users' expectations are violated\nby system predictions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 21:38:05 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Springer", "Aaron", ""], ["Whittaker", "Steve", ""]]}, {"id": "1812.03325", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Adrian Seitan, Dorin M. Popovici, Crenguta M.\n  Bogdan", "title": "Medical Simulation and Training: \"Haptic\" Liver", "comments": "Proceedings of the 7th International Conference on Virtual Learning\n  (2012), Brasov, Romania", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile perception plays an important role in medical simulation and\ntraining, specifically in surgery. The surgeon must feel organic tissue\nhardness, evaluate anatomical structures, measure tissue properties, and apply\nappropriate force control actions for safe tissue manipulation. Development of\nnovel cost effective haptic-based simulators and their introduction in the\nminimally invasive surgery learning cycle can absorb the learning curve for\nresidents. Receiving pre-training in a core set of surgical skills can reduce\nskill acquisition time and risks. We present the development of a\ncost-effective visuo-haptic simulator for the liver tissue, designed to improve\npractice-based education in minimally invasive surgery. Such systems can\npositively affect the next generations of learners by enhancing their knowledge\nin connection with real-life situations while they train in mandatory safe\nconditions.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 14:19:15 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Seitan", "Adrian", ""], ["Popovici", "Dorin M.", ""], ["Bogdan", "Crenguta M.", ""]]}, {"id": "1812.03364", "submitter": "Sangeetha Balasubramanian", "authors": "Sangeetha Balasubramanian, Shruti Shriya Gullapuram, Abhinav Shukla", "title": "Engagement Estimation in Advertisement Videos with EEG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Engagement is a vital metric in the advertising industry and its automatic\nestimation has huge commercial implications. This work presents a basic and\nsimple framework for engagement estimation using EEG (electroencephalography)\ndata specifically recorded while watching advertisement videos, and is meant to\nbe a first step in a promising line of research. The system combines recent\nadvances in low cost commercial Brain-Computer Interfaces with modeling user\nengagement in response to advertisement videos. We achieve an F1 score of\nnearly 0.7 for a binary classification of high and low values of self-reported\nengagement from multiple users. This study illustrates the possibility of\nseamless engagement measurement in the wild when interacting with media using a\nnon invasive and readily available commercial EEG device. Performing engagement\nmeasurement via implicit tagging in this manner with a direct feedback from\nphysiological signals, thus requiring no additional human effort, demonstrates\na novel and potentially commercially relevant application in the area of\nadvertisement video analysis.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 18:34:09 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Balasubramanian", "Sangeetha", ""], ["Gullapuram", "Shruti Shriya", ""], ["Shukla", "Abhinav", ""]]}, {"id": "1812.03441", "submitter": "Richard Skarbez", "authors": "Richard Skarbez and Doug A. Bowman and J. Todd Ogle and Thomas Tucker\n  and Joseph L. Gabbard", "title": "Virtual replicas of real places: Experimental investigations", "comments": "21 pages", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics (Early\n  Access) (2021)", "doi": "10.1109/TVCG.2021.3096494", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of social virtual reality (VR) experiences, such as Facebook\nSpaces, Oculus Rooms, and Oculus Venues, will generate increased interest from\nusers who want to share real places (both personal and public) with their\nfellow users in VR. At the same time, advances in scanning and reconstruction\ntechnology are making the realistic capture of real places more and more\nfeasible. These complementary pressures mean that the representation of real\nplaces in virtual reality will be an increasingly common use case for VR.\nDespite this, there has been very little research into how users perceive such\nreplicated spaces. This paper reports the results from a series of three user\nstudies investigating this topic. Taken together, these results show that\ngetting the scale of the space correct is the most important factor for\ngenerating a \"feeling of reality\", that it is important to avoid incoherent\nbehaviors (such as floating objects), and that lighting makes little difference\nto perceptual similarity.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 08:04:00 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Skarbez", "Richard", ""], ["Bowman", "Doug A.", ""], ["Ogle", "J. Todd", ""], ["Tucker", "Thomas", ""], ["Gabbard", "Joseph L.", ""]]}, {"id": "1812.03484", "submitter": "Bishal Ghosh", "authors": "Bishal Ghosh, Abhinav Dhall, Ekta Singla", "title": "Speech-Gesture Mapping and Engagement Evaluation in Human Robot\n  Interaction", "comments": "8 pages, 9 figures, Under review in IRC 2019", "journal-ref": null, "doi": "10.1109/RO-MAN46459.2019.8956462", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robot needs contextual awareness, effective speech production and\ncomplementing non-verbal gestures for successful communication in society. In\nthis paper, we present our end-to-end system that tries to enhance the\neffectiveness of non-verbal gestures. For achieving this, we identified\nprominently used gestures in performances by TED speakers and mapped them to\ntheir corresponding speech context and modulated speech based upon the\nattention of the listener. The proposed method utilized Convolutional Pose\nMachine [4] to detect the human gesture. Dominant gestures of TED speakers were\nused for learning the gesture-to-speech mapping. The speeches by them were used\nfor training the model. We also evaluated the engagement of the robot with\npeople by conducting a social survey. The effectiveness of the performance was\nmonitored by the robot and it self-improvised its speech pattern on the basis\nof the attention level of the audience, which was calculated using visual\nfeedback from the camera. The effectiveness of interaction as well as the\ndecisions made during improvisation was further evaluated based on the\nhead-pose detection and interaction survey.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 13:22:01 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Ghosh", "Bishal", ""], ["Dhall", "Abhinav", ""], ["Singla", "Ekta", ""]]}, {"id": "1812.03660", "submitter": "Yupeng Jiang", "authors": "Lakshmi Sirisha Revadi, Xi Zheng, Yupeng Jiang", "title": "Investigating Key User Experiencing Engineering Aspects in\n  Software-as-a-Service Service Delivery Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software as a Service (SaaS) is well established as an effective model for\nthe development, deployment and customization of software. As it continues to\ngain more momentum in the IT industry, many user experience challenges and\nissues are being reported by the experts and end users.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:49:57 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Revadi", "Lakshmi Sirisha", ""], ["Zheng", "Xi", ""], ["Jiang", "Yupeng", ""]]}, {"id": "1812.03880", "submitter": "Antonio Bevilacqua", "authors": "Antonio Bevilacqua, Bingquan Huang, Rob Argent, Brian Caulfield, Tahar\n  Kechadi", "title": "Automatic Classification of Knee Rehabilitation Exercises Using a Single\n  Inertial Sensor: a Case Study", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": "10.1109/BSN.2018.8329649", "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inertial measurement units have the ability to accurately record the\nacceleration and angular velocity of human limb segments during discrete joint\nmovements. These movements are commonly used in exercise rehabilitation\nprogrammes following orthopaedic surgery such as total knee replacement. This\nprovides the potential for a biofeedback system with data mining technique for\npatients undertaking exercises at home without physician supervision. We\npropose to use machine learning techniques to automatically analyse inertial\nmeasurement unit data collected during these exercises, and then assess whether\neach repetition of the exercise was executed correctly or not. Our approach\nconsists of two main phases: signal segmentation, and segment classification.\nAccurate pre-processing and feature extraction are paramount topics in order\nfor the technique to work. In this paper, we present a classification method\nfor unsupervised rehabilitation exercises, based on a segmentation process that\nextracts repetitions from a longer signal activity. The results obtained from\nexperimental datasets of both clinical and healthy subjects, for a set of 4\nknee exercises commonly used in rehabilitation, are very promising.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 15:36:41 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Bevilacqua", "Antonio", ""], ["Huang", "Bingquan", ""], ["Argent", "Rob", ""], ["Caulfield", "Brian", ""], ["Kechadi", "Tahar", ""]]}, {"id": "1812.04081", "submitter": "Song Feng", "authors": "Paola Cascante-Bonilla, Xuwang Yin, Vicente Ordonez, Song Feng", "title": "Chat-crowd: A Dialog-based Platform for Visual Layout Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce Chat-crowd, an interactive environment for visual\nlayout composition via conversational interactions. Chat-crowd supports\nmultiple agents with two conversational roles: agents who play the role of a\ndesigner are in charge of placing objects in an editable canvas according to\ninstructions or commands issued by agents with a director role. The system can\nbe integrated with crowdsourcing platforms for both synchronous and\nasynchronous data collection and is equipped with comprehensive quality\ncontrols on the performance of both types of agents. We expect that this system\nwill be useful to build multimodal goal-oriented dialog tasks that require\nspatial and geometric reasoning.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 20:52:41 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 12:29:36 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 14:19:06 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Cascante-Bonilla", "Paola", ""], ["Yin", "Xuwang", ""], ["Ordonez", "Vicente", ""], ["Feng", "Song", ""]]}, {"id": "1812.04406", "submitter": "Johan F. Hoorn", "authors": "Johan F. Hoorn", "title": "Theory of Robot Communication: II. Befriending a Robot over Time", "comments": "Hoorn, J. F. (2018). Theory of robot communication: II. Befriending a\n  robot over time. arXiv:cs, 2502572(v1), 1-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In building on theories of Computer-Mediated Communication (CMC), Human-Robot\nInteraction, and Media Psychology (i.e. Theory of Affective Bonding), the\ncurrent paper proposes an explanation of how over time, people experience the\nmediated or simulated aspects of the interaction with a social robot. In two\nsimultaneously running loops, a more reflective process is balanced with a more\naffective process. If human interference is detected behind the machine,\nRobot-Mediated Communication commences, which basically follows CMC\nassumptions; if human interference remains undetected, Human-Robot\nCommunication comes into play, holding the robot for an autonomous social\nactor. The more emotionally aroused a robot user is, the more likely they\ndevelop an affective relationship with what actually is a machine. The main\ncontribution of this paper is an integration of Computer-Mediated\nCommunication, Human-Robot Communication, and Media Psychology, outlining a\nfull-blown theory of robot communication connected to friendship formation,\naccounting for communicative features, modes of processing, as well as\npsychophysiology.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 05:20:29 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Hoorn", "Johan F.", ""]]}, {"id": "1812.04408", "submitter": "Johan F. Hoorn", "authors": "Johan F. Hoorn", "title": "Theory of Robot Communication: I. The Medium is the Communication\n  Partner", "comments": "Hoorn, J. F. (2018). Theory of robot communication: I. The medium is\n  the communication partner. arXiv:cs, 2502565(v1), 1-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  When people use electronic media for their communication, Computer-Mediated\nCommunication (CMC) theories describe the social and communicative aspects of\npeople's interpersonal transactions. When people interact via a\nremote-controlled robot, many of the CMC theses hold. Yet, what if people\ncommunicate with a conversation robot that is (partly) autonomous? Do the same\ntheories apply? This paper discusses CMC theories in confrontation with\nobservations and research data gained from human-robot communication. As a\nresult, I argue for an addition to CMC theorizing when the robot as a medium\nitself becomes the communication partner. In view of the rise of social robots\nin coming years, I define the theoretical precepts of a possible next step in\nCMC, which I elaborate in a second paper.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 05:18:14 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Hoorn", "Johan F.", ""]]}, {"id": "1812.04478", "submitter": "Steven Jeuris", "authors": "Steven Jeuris", "title": "Socratrees: Exploring the Design of Argument Technology for Layman Users", "comments": "Rejected several times, primarily on the basis of needing a larger\n  study. While trying to obtain funding for this (this project has received no\n  funding so far), leaving this out here for now", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Terms like 'misinformation', 'fake news', and 'echo chambers' permeate\ncurrent discussions on the state of the Internet. We believe a lack of\ntechnological support to evaluate, contest, and reason about information\nonline---as opposed to merely disseminating it---lies at the root of these\nproblems. Several argument technologies support such functionality, but have\nseen limited use outside of niche communities. Most research systems\noveremphasize argument analysis and structure, standing in stark contrast with\nthe informal dialectical nature of everyday argumentation. Conversely,\nnon-academic systems overlook important implications for design which can be\nderived from theory. In this paper, we present the design of a system aiming to\nstrike a balance between structured argumentation and ease of use. Socratrees\nis a website for collaborative argumentative discussion targeting layman users,\nbut includes sophisticated community guidelines and novel features inspired by\ninformal logic. During an exploratory study, we evaluate the usefulness of our\nimposed structure on argumentation and investigate how users perceive it.\nContributing to arguments remains a complex task, but most users learned to do\nso effectively with minimal guidance and all recognized that the structure of\nSocratrees may improve online discussion and results in a clearer overview of\narguments.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 15:45:10 GMT"}, {"version": "v2", "created": "Sun, 31 Mar 2019 09:48:41 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 18:53:25 GMT"}, {"version": "v4", "created": "Tue, 17 Mar 2020 13:02:14 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Jeuris", "Steven", ""]]}, {"id": "1812.04818", "submitter": "Matin Hashemi", "authors": "Saeed Saadatnejad, Mohammadhosein Oveisi, Matin Hashemi", "title": "LSTM-Based ECG Classification for Continuous Monitoring on Personal\n  Wearable Devices", "comments": "Accepted for publication in IEEE Journal of Biomedical and Health\n  Informatics (J-BHI)", "journal-ref": "IEEE Journal of Biomedical and Health Informatics (JBHI), Vol. 24,\n  No. 2, February 2020", "doi": "10.1109/JBHI.2019.2911367", "report-no": null, "categories": "eess.SP cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: A novel ECG classification algorithm is proposed for continuous\ncardiac monitoring on wearable devices with limited processing capacity.\nMethods: The proposed solution employs a novel architecture consisting of\nwavelet transform and multiple LSTM recurrent neural networks. Results:\nExperimental evaluations show superior ECG classification performance compared\nto previous works. Measurements on different hardware platforms show the\nproposed algorithm meets timing requirements for continuous and real-time\nexecution on wearable devices. Conclusion: In contrast to many\ncompute-intensive deep-learning based approaches, the proposed algorithm is\nlightweight, and therefore, brings continuous monitoring with accurate\nLSTM-based ECG classification to wearable devices. Significance: The proposed\nalgorithm is both accurate and lightweight. The source code is available online\n[1].\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 06:04:44 GMT"}, {"version": "v2", "created": "Sat, 13 Apr 2019 10:39:43 GMT"}, {"version": "v3", "created": "Sat, 11 May 2019 22:53:26 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Saadatnejad", "Saeed", ""], ["Oveisi", "Mohammadhosein", ""], ["Hashemi", "Matin", ""]]}, {"id": "1812.04865", "submitter": "Tony T. Luo", "authors": "Qing Liu, Tie Luo, Ruiming Tang, and Stephane Bressan", "title": "An Efficient and Truthful Pricing Mechanism for Team Formation in\n  Crowdsourcing Markets", "comments": "6 pages, 3 figures, IEEE ICC 2015", "journal-ref": "IEEE International Conference on Communications (ICC), June 2015,\n  pp. 567-572", "doi": "10.1109/ICC.2015.7248382", "report-no": null, "categories": "cs.GT cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a crowdsourcing market, a requester is looking to form a team of workers\nto perform a complex task that requires a variety of skills. Candidate workers\nadvertise their certified skills and bid prices for their participation. We\ndesign four incentive mechanisms for selecting workers to form a valid team\n(that can complete the task) and determining each individual worker's payment.\nWe examine profitability, individual rationality, computational efficiency, and\ntruthfulness for each of the four mechanisms. Our analysis shows that TruTeam,\none of the four mechanisms, is superior to the others, particularly due to its\ncomputational efficiency and truthfulness. Our extensive simulations confirm\nthe analysis and demonstrate that TruTeam is an efficient and truthful pricing\nmechanism for team formation in crowdsourcing markets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 09:43:18 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Liu", "Qing", ""], ["Luo", "Tie", ""], ["Tang", "Ruiming", ""], ["Bressan", "Stephane", ""]]}, {"id": "1812.04949", "submitter": "Miklas S. Kristoffersen", "authors": "Andrea Coifman, P\\'eter Rohoska, Miklas S. Kristoffersen, Sven E.\n  Shepstone, Zheng-Hua Tan", "title": "Subjective Annotations for Vision-Based Attention Level Estimation", "comments": "14th International Conference on Computer Vision Theory and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention level estimation systems have a high potential in many use cases,\nsuch as human-robot interaction, driver modeling and smart home systems, since\nbeing able to measure a person's attention level opens the possibility to\nnatural interaction between humans and computers. The topic of estimating a\nhuman's visual focus of attention has been actively addressed recently in the\nfield of HCI. However, most of these previous works do not consider attention\nas a subjective, cognitive attentive state. New research within the field also\nfaces the problem of the lack of annotated datasets regarding attention level\nin a certain context. The novelty of our work is two-fold: First, we introduce\na new annotation framework that tackles the subjective nature of attention\nlevel and use it to annotate more than 100,000 images with three attention\nlevels and second, we introduce a novel method to estimate attention levels,\nrelying purely on extracted geometric features from RGB and depth images, and\nevaluate it with a deep learning fusion framework. The system achieves an\noverall accuracy of 80.02%. Our framework and attention level annotations are\nmade publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 14:00:46 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 11:43:12 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Coifman", "Andrea", ""], ["Rohoska", "P\u00e9ter", ""], ["Kristoffersen", "Miklas S.", ""], ["Shepstone", "Sven E.", ""], ["Tan", "Zheng-Hua", ""]]}, {"id": "1812.05239", "submitter": "Kenneth Holstein", "authors": "Kenneth Holstein, Jennifer Wortman Vaughan, Hal Daum\\'e III, Miro\n  Dud\\'ik, Hanna Wallach", "title": "Improving fairness in machine learning systems: What do industry\n  practitioners need?", "comments": "To appear in the 2019 ACM CHI Conference on Human Factors in\n  Computing Systems (CHI 2019)", "journal-ref": null, "doi": "10.1145/3290605.3300830", "report-no": null, "categories": "cs.HC cs.CY cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The potential for machine learning (ML) systems to amplify social inequities\nand unfairness is receiving increasing popular and academic attention. A surge\nof recent work has focused on the development of algorithmic tools to assess\nand mitigate such unfairness. If these tools are to have a positive impact on\nindustry practice, however, it is crucial that their design be informed by an\nunderstanding of real-world needs. Through 35 semi-structured interviews and an\nanonymous survey of 267 ML practitioners, we conduct the first systematic\ninvestigation of commercial product teams' challenges and needs for support in\ndeveloping fairer ML systems. We identify areas of alignment and disconnect\nbetween the challenges faced by industry practitioners and solutions proposed\nin the fair ML research literature. Based on these findings, we highlight\ndirections for future ML and HCI research that will better address industry\npractitioners' needs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 02:54:24 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 19:30:11 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Holstein", "Kenneth", ""], ["Vaughan", "Jennifer Wortman", ""], ["Daum\u00e9", "Hal", "III"], ["Dud\u00edk", "Miro", ""], ["Wallach", "Hanna", ""]]}, {"id": "1812.05465", "submitter": "Alejandro Baldominos", "authors": "Alejandro Baldominos and David Quintana", "title": "Data-Driven Interaction Review of an Ed-Tech Application", "comments": "14 pages, 2 figures", "journal-ref": "Sensors 2019, 19(8), 1910", "doi": "10.3390/s19081910", "report-no": null, "categories": "cs.CY cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smile and Learn is an Ed-Tech company that runs a smart library with more\nthat 100 applications, games and interactive stories, aimed at children aged\ntwo to 10 and their families. The platform gathers thousands of data points\nfrom the interaction with the system to subsequently offer reports and\nrecommendations. Given the complexity of navigating all the content, the\nlibrary implements a recommender system. The purpose of this paper is to\nevaluate two aspects of such system focused on children: the influence of the\norder of recommendations on user exploratory behavior, and the impact of the\nchoice of the recommendation algorithm on engagement. The assessment, based on\ndata collected between 15 October 2018 and 1 December 2018, required the\nanalysis of the number of clicks performed on the recommendations depending on\ntheir ordering, and an A/B/C testing where two standard recommendation\nalgorithmswere comparedwith a randomrecommendation that served as baseline. The\nresults suggest a direct connection between the order of the recommendation and\nthe interest raised, and the superiority of recommendations based on popularity\nagainst other alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 14:55:57 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 13:43:41 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 15:14:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Baldominos", "Alejandro", ""], ["Quintana", "David", ""]]}, {"id": "1812.05659", "submitter": "Enes Karaaslan", "authors": "Enes Karaaslan, Ulas Bagci, F. Necati Catbas", "title": "Artificial Intelligence Assisted Infrastructure Assessment Using Mixed\n  Reality Systems", "comments": "5,240 word texts, 3 tables, 14 figures. Transportation Research\n  Record: Journal of the Transportation Research Board, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional methods for visual assessment of civil infrastructures have\ncertain limitations, such as subjectivity of the collected data, long\ninspection time, and high cost of labor. Although some new technologies i.e.\nrobotic techniques that are currently in practice can collect objective,\nquantified data, the inspectors own expertise is still critical in many\ninstances since these technologies are not designed to work interactively with\nhuman inspector. This study aims to create a smart, human centered method that\noffers significant contributions to infrastructure inspection, maintenance,\nmanagement practice, and safety for the bridge owners. By developing a smart\nMixed Reality framework, which can be integrated into a wearable holographic\nheadset device, a bridge inspector, for example, can automatically analyze a\ncertain defect such as a crack that he or she sees on an element, display its\ndimension information in real-time along with the condition state. Such systems\ncan potentially decrease the time and cost of infrastructure inspections by\naccelerating essential tasks of the inspector such as defect measurement,\ncondition assessment and data processing to management systems. The human\ncentered artificial intelligence will help the inspector collect more\nquantified and objective data while incorporating inspectors professional\njudgement. This study explains in detail the described system and related\nmethodologies of implementing attention guided semi supervised deep learning\ninto mixed reality technology, which interacts with the human inspector during\nassessment. Thereby, the inspector and the AI will collaborate or communicate\nfor improved visual inspection.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 19:46:00 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Karaaslan", "Enes", ""], ["Bagci", "Ulas", ""], ["Catbas", "F. Necati", ""]]}, {"id": "1812.05767", "submitter": "Yanbang Wang", "authors": "Yanbang Wang, Nancy Law, Erik Hemberg and Una-May O'Reilly", "title": "Using Detailed Access Trajectories for Learning Behavior Analysis", "comments": "10 pages, accepted by 2019 International Conference on Learning\n  Analytics and Knowledge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student learning activity in MOOCs can be viewed from multiple perspectives.\nWe present a new organization of MOOC learner activity data at a resolution\nthat is in between the fine granularity of the clickstream and coarse\norganizations that count activities, aggregate students or use long duration\ntime units. A detailed access trajectory (DAT) consists of binary values and is\ntwo dimensional with one axis that is a time series, e.g. days and the other\nthat is a chronologically ordered list of a MOOC component type's instances,\ne.g. videos in instructional order. Most popular MOOC platforms generate data\nthat can be organized as detailed access trajectories (DATs).We explore the\nvalue of DATs by conducting four empirical mini-studies. Our studies suggest\nDATs contain rich information about students' learning behaviors and facilitate\nMOOC learning analyses.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 03:16:54 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Wang", "Yanbang", ""], ["Law", "Nancy", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""]]}, {"id": "1812.05804", "submitter": "Andrew Simmons", "authors": "Andrew J. Simmons, Scott Barnett, Simon Vajda and Rajesh Vasa", "title": "Data Provenance for Sport", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analysts often discover irregularities in their underlying dataset,\nwhich need to be traced back to the original source and corrected. Standards\nfor representing data provenance (i.e. the origins of the data), such as the\nW3C PROV standard, can assist with this process, however require a mapping\nbetween abstract provenance concepts and the domain of use in order to apply\nthem effectively. We propose a custom notation for expressing provenance of\ninformation in the sport performance analysis domain, and map our notation to\nconcepts in the W3C PROV standard where possible. We evaluate the functionality\nof W3C PROV (without specialisations) and the VisTrails workflow manager\n(without extensions), and find that as is, neither are able to fully capture\nsport performance analysis workflows, notably due to limitations surrounding\ncapture of automated and manual activities respectively. Furthermore, their\nnotations suffer from ineffective use of visual design space, and present\npotential usability issues as their terminology is unlikely to match that of\nsport practitioners. Our findings suggest that one-size-fits-all provenance and\nworkflow systems are a poor fit in practice, and that their notation and\nfunctionality need to be optimised for the domain of use.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 07:41:25 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Simmons", "Andrew J.", ""], ["Barnett", "Scott", ""], ["Vajda", "Simon", ""], ["Vasa", "Rajesh", ""]]}, {"id": "1812.05945", "submitter": "Daniel Omeiza A", "authors": "Daniel Omeiza, Kayode Sakariyah Adewole, Daniel Nkemelu", "title": "EEG-based Communication with a Predictive Text Algorithm", "comments": "Conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several changes occur in the brain in response to voluntary and involuntary\nactivities performed by a person. The ability to retrieve data from the brain\nwithin a time space provides a basis for in-depth analyses that offer insight\non what changes occur in the brain during its decision-making processes. In\nthis work, we present the technical description and software implementation of\nan electroencephalographic (EEG) based communication system. We read EEG data\nin real-time with which we compute the likelihood that a voluntary eye blink\nhas been made by a person and use the decision to trigger buttons on a user\ninterface in order to produce text. Relevant texts are suggested using a\nmodification of the T9 algorithm. Our results indicate that EEG-based\ntechnology can be effectively applied in facilitating speech for people with\nsevere speech and muscular disabilities, providing a foundation for future work\nin the area.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 14:08:35 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 19:55:29 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 14:58:02 GMT"}, {"version": "v4", "created": "Sat, 27 Jun 2020 18:04:30 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Omeiza", "Daniel", ""], ["Adewole", "Kayode Sakariyah", ""], ["Nkemelu", "Daniel", ""]]}, {"id": "1812.06051", "submitter": "Min Chen", "authors": "Min Chen", "title": "The Value of Interaction in Data Intelligence", "comments": "This paper was first submitted to CHI 2019 on 21 September 2018.\n  Although it was not accepted, the author believes that it may still be a\n  useful contribution to the field of HCI. The file also includes CHI 2019\n  reviews and the author's feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human computer interaction (HCI), it is common to evaluate the value of\nHCI designs, techniques, devices, and systems in terms of their benefit to\nusers. It is less common to discuss the benefit of HCI to computers. Every HCI\ntask allows a computer to receive some data from the user. In many situations,\nthe data received by the computer embodies human knowledge and intelligence in\nhandling complex problems, and/or some critical information without which the\ncomputer cannot proceed. In this paper, we present an information-theoretic\nframework for quantifying the knowledge received by the computer from its users\nvia HCI. We apply information-theoretic measures to some common HCI tasks as\nwell as HCI tasks in complex data intelligence processes. We formalize the\nmethods for estimating such quantities analytically and measuring them\nempirically. Using theoretical reasoning, we can confirm the significant but\noften undervalued role of HCI in data intelligence workflows.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 17:56:55 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Chen", "Min", ""]]}, {"id": "1812.06090", "submitter": "Avishek Choudhury", "authors": "Avishek Choudhury, Huiyang Li, Christopher Greene, Sunanda Perumalla", "title": "Humanoid Robot-Application and Influence", "comments": null, "journal-ref": "Archives of Clinical and Biomedical Research 2 (2018): 198-227", "doi": "10.26502/acbr.50170059", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Application of humanoid robots has been common in the field of healthcare and\neducation. It has been recurrently used to improve social behavior and mollify\ndistress level among children with autism, cancer and cerebral palsy. This\narticle discusses the same from a human factors perspective. It shows how\npeople of different age and gender have a different opinion towards the\napplication and acceptance of humanoid robots. Additionally, this article\nhighlights the influence of cerebral condition and social interaction on a user\nbehavior and attitude towards humanoid robots. Our study performed a literature\nreview and found that (a) children and elderly individuals prefer humanoid\nrobots due to inactive social interaction, (b) The deterministic behavior of\nhumanoid robots can be acknowledged to improve social behavior of autistic\nchildren, (c) Trust on humanoid robots is highly driven by its application and\na user age, gender, and social life.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 17:37:23 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Choudhury", "Avishek", ""], ["Li", "Huiyang", ""], ["Greene", "Christopher", ""], ["Perumalla", "Sunanda", ""]]}, {"id": "1812.06128", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha, Danielle Griego, Saskia Kuliga, Martin Bielik, Peter\n  Bus, Charlotte Schaeben, Lukas Treyer, Matthias Standfest, Sven Schneider,\n  Reinhard Konig, Dirk Donath, and Gerhard Schmitt", "title": "Machine learning approaches to understand the influence of urban\n  environments on human's physiological response", "comments": null, "journal-ref": "Information Sciences 474, 154-169, 2019", "doi": "10.1016/j.ins.2018.09.061", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research proposes a framework for signal processing and information\nfusion of spatial-temporal multi-sensor data pertaining to understanding\npatterns of humans physiological changes in an urban environment. The framework\nincludes signal frequency unification, signal pairing, signal filtering, signal\nquantification, and data labeling. Furthermore, this paper contributes to\nhuman-environment interaction research, where a field study to understand the\ninfluence of environmental features such as varying sound level, illuminance,\nfield-of-view, or environmental conditions on humans' perception was proposed.\nIn the study, participants of various demographic backgrounds walked through an\nurban environment in Zurich, Switzerland while wearing physiological and\nenvironmental sensors. Apart from signal processing, four machine learning\ntechniques, classification, fuzzy rule-based inference, feature selection, and\nclustering, were applied to discover relevant patterns and relationship between\nthe participants' physiological responses and environmental conditions. The\npredictive models with high accuracies indicate that the change in the\nfield-of-view corresponds to increased participant arousal. Among all features,\nthe participants' physiological responses were primarily affected by the change\nin environmental conditions and field-of-view.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 13:45:20 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Griego", "Danielle", ""], ["Kuliga", "Saskia", ""], ["Bielik", "Martin", ""], ["Bus", "Peter", ""], ["Schaeben", "Charlotte", ""], ["Treyer", "Lukas", ""], ["Standfest", "Matthias", ""], ["Schneider", "Sven", ""], ["Konig", "Reinhard", ""], ["Donath", "Dirk", ""], ["Schmitt", "Gerhard", ""]]}, {"id": "1812.06145", "submitter": "Mahdi Abavisani", "authors": "Mahdi Abavisani, Hamid Reza Vaezi Joze, Vishal M. Patel", "title": "Improving the Performance of Unimodal Dynamic Hand-Gesture Recognition\n  with Multimodal Training", "comments": null, "journal-ref": "The IEEE Conference on Computer Vision and Pattern Recognition\n  (CVPR), 2019, pp. 1165-1174", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an efficient approach for leveraging the knowledge from multiple\nmodalities in training unimodal 3D convolutional neural networks (3D-CNNs) for\nthe task of dynamic hand gesture recognition. Instead of explicitly combining\nmultimodal information, which is commonplace in many state-of-the-art methods,\nwe propose a different framework in which we embed the knowledge of multiple\nmodalities in individual networks so that each unimodal network can achieve an\nimproved performance. In particular, we dedicate separate networks per\navailable modality and enforce them to collaborate and learn to develop\nnetworks with common semantics and better representations. We introduce a\n\"spatiotemporal semantic alignment\" loss (SSA) to align the content of the\nfeatures from different networks. In addition, we regularize this loss with our\nproposed \"focal regularization parameter\" to avoid negative knowledge transfer.\nExperimental results show that our framework improves the test time recognition\naccuracy of unimodal networks, and provides the state-of-the-art performance on\nvarious dynamic hand gesture recognition datasets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 20:08:24 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 09:51:14 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Abavisani", "Mahdi", ""], ["Joze", "Hamid Reza Vaezi", ""], ["Patel", "Vishal M.", ""]]}, {"id": "1812.06147", "submitter": "Ronald Gruber Md", "authors": "Ronald P. Gruber and Ryan P. Smith", "title": "An Experimental Information Gathering and Utilization Systems (IGUS)\n  Robot to Demonstrate the Physics of Now", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": "10.1119/1.5093293", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past, present and future are not fundamental properties of Minkowski\nspacetime. It has been suggested that they are properties of a class of\ninformation gathering and utilizing systems (IGUSs).The past, present and\nfuture are psychologically created phenomena not actually properties of\nspacetime. A human is a model IGUS robot. We develop a way to establish that\nthe past, present, and future do not follow from the laws of physics by\nconstructing robots that process information differently and therefore\nexperience different nows (presents). We construct a customized virtual reality\n(VR) system which allows an observer to switch between present and past. This\nrobot (human with VR system) can experience immersion in the immediate past ad\nlibitum. Being able to actually construct an IGUS that has the same present at\ntwo different coordinates along the worldline lends support to the IGUS\nhypothesis.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 17:20:55 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Gruber", "Ronald P.", ""], ["Smith", "Ryan P.", ""]]}, {"id": "1812.06237", "submitter": "Sara Price", "authors": "Kimberly K. Arcand, Elaine Jiang, Sara Price, Megan Watzke, Tom\n  Sgouros, Peter Edmonds", "title": "Walking Through an Exploded Star: Rendering Supernova Remnant Cassiopeia\n  A into Virtual Reality", "comments": "20 pages, 6 figures", "journal-ref": "Communicating Astronomy with the Public Journal Volume 1, Issue 24\n  (2018): 17", "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.HE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NASA and other astrophysical data of the Cassiopeia A supernova remnant have\nbeen rendered into a three-dimensional virtual reality (VR) and augmented\nreality (AR) program, the first of its kind. This data-driven experience of a\nsupernova remnant allows viewers to walk inside the leftovers from the\nexplosion of a massive star, select the parts of the supernova remnant to\nengage with, and access descriptive texts on what the materials are. The basis\nof this program is a unique 3D model of the 340-year old remains of a stellar\nexplosion, made by combining data from the NASA Chandra X-ray Observatory,\nSpitzer Space Telescope, and ground-based facilities. A collaboration between\nthe Smithsonian Astrophysical Observatory and Brown University allowed the 3D\nastronomical data collected on Cassiopeia A to be featured in the VR/AR\nprogram, which is an innovation in digital technologies with public, education,\nand research-based impacts.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 05:29:04 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Arcand", "Kimberly K.", ""], ["Jiang", "Elaine", ""], ["Price", "Sara", ""], ["Watzke", "Megan", ""], ["Sgouros", "Tom", ""], ["Edmonds", "Peter", ""]]}, {"id": "1812.06337", "submitter": "Alex Bigelow", "authors": "Alex Bigelow, Carolina Nobre, Miriah Meyer, Alexander Lex", "title": "Origraph: Interactive Network Wrangling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are a natural way of thinking about many datasets. The data on which\na network is based, however, is rarely collected in a form that suits the\nanalysis process, making it necessary to create and reshape networks. Data\nwrangling is widely acknowledged to be a critical part of the data analysis\npipeline, yet interactive network wrangling has received little attention in\nthe visualization research community. In this paper, we discuss a set of\noperations that are important for wrangling network datasets and introduce a\nvisual data wrangling tool, Origraph, that enables analysts to apply these\noperations to their datasets. Key operations include creating a network from\nsource data such as tables, reshaping a network by introducing new node or edge\nclasses, filtering nodes or edges, and deriving new node or edge attributes.\nOur tool, Origraph, enables analysts to execute these operations with little to\nno programming, and to immediately visualize the results. Origraph provides\nviews to investigate the network model, a sample of the network, and node and\nedge attributes. In addition, we introduce interfaces designed to aid analysts\nin specifying arguments for sensible network wrangling operations. We\ndemonstrate the usefulness of Origraph in two Use Cases: first, we investigate\ngender bias in the film industry, and then the influence of money on the\npolitical support for the war in Yemen.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 18:56:15 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 20:49:19 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 23:26:29 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bigelow", "Alex", ""], ["Nobre", "Carolina", ""], ["Meyer", "Miriah", ""], ["Lex", "Alexander", ""]]}, {"id": "1812.06525", "submitter": "Homanga Bharadhwaj", "authors": "Homanga Bharadhwaj, Nisheeth Srivastava", "title": "New tab page recommendations cause a strong suppression of exploratory\n  web browsing behaviors", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through a combination of experimental and simulation results, we illustrate\nthat passive recommendations encoded in typical computer user-interfaces (UIs)\ncan subdue users' natural proclivity to access diverse information sources.\nInspired by traditional demonstrations of a part-set cueing effect in the\ncognitive science literature, we performed an online experiment manipulating\nthe operation of the 'New Tab' page for consenting volunteers over a two month\nperiod. Examination of their browsing behavior reveals that typical frequency\nand recency-based methods for displaying websites in these displays subdues\nusers' propensity to access infrequently visited pages compared to a situation\nwherein no web page icons are displayed on the new tab page. Using a carefully\ndesigned simulation study, representing user behavior as a random walk on a\ngraph, we inferred quantitative predictions about the extent to which discovery\nof new sources of information may be hampered by personalized 'New Tab'\nrecommendations in typical computer UIs. We show that our results are\nsignificant at the individual level and explain the potential consequences of\nthe observed suppression in web-exploration.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 19:41:56 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Bharadhwaj", "Homanga", ""], ["Srivastava", "Nisheeth", ""]]}, {"id": "1812.06784", "submitter": "Trenton Schulz", "authors": "Trenton Schulz, Jim Torresen, Jo Herstad", "title": "Animation Techniques in Human-Robot Interaction User Studies: a\n  Systematic Literature Review", "comments": "23 pages, 6 tables", "journal-ref": "ACM Transactions on Human-Robot Interaction, Volume 8, Issue 2,\n  Article 12, 2019", "doi": "10.1145/3317325", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many different ways a robot can move in Human-Robot Interaction.\nOne way is to use techniques from film animation to instruct the robot to move.\nThis article is a systematic literature review of human-robot trials, pilots,\nand evaluations that have applied techniques from animation to move a robot.\nThrough 27 articles, we find that animation techniques improves individual's\ninteraction with robots, improving individual's perception of qualities of a\nrobot, understanding what a robot intends to do, and showing the robot's state,\nor possible emotion. Animation techniques also help people relate to robots\nthat do not resemble a human or robot. The studies in the articles show further\nareas for research, such as applying animation principles in other types of\nrobots and situations, combining animation techniques with other modalities,\nand testing robots moving with animation techniques over the long term.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 14:21:37 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 12:21:26 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 12:05:26 GMT"}, {"version": "v4", "created": "Wed, 24 Apr 2019 08:44:46 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Schulz", "Trenton", ""], ["Torresen", "Jim", ""], ["Herstad", "Jo", ""]]}, {"id": "1812.06857", "submitter": "Ozan Ozdenizci", "authors": "Ozan Ozdenizci, Ye Wang, Toshiaki Koike-Akino, Deniz Erdogmus", "title": "Transfer Learning in Brain-Computer Interfaces with Adversarial\n  Variational Autoencoders", "comments": "9th International IEEE EMBS Conference on Neural Engineering (NER'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce adversarial neural networks for representation learning as a\nnovel approach to transfer learning in brain-computer interfaces (BCIs). The\nproposed approach aims to learn subject-invariant representations by\nsimultaneously training a conditional variational autoencoder (cVAE) and an\nadversarial network. We use shallow convolutional architectures to realize the\ncVAE, and the learned encoder is transferred to extract subject-invariant\nfeatures from unseen BCI users' data for decoding. We demonstrate a\nproof-of-concept of our approach based on analyses of electroencephalographic\n(EEG) data recorded during a motor imagery BCI experiment.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 15:53:53 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Ozdenizci", "Ozan", ""], ["Wang", "Ye", ""], ["Koike-Akino", "Toshiaki", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1812.07081", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi, Morgan Harvey, Luca Costa, Matthew Pointon,\n  Fabio Crestani", "title": "Understanding Mobile Search Task Relevance and User Behaviour in Context", "comments": "To appear in CHIIR 2019 in Glasgow, UK", "journal-ref": null, "doi": "10.1145/3295750.3298923", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improvements in mobile technologies have led to a dramatic change in how and\nwhen people access and use information, and is having a profound impact on how\nusers address their daily information needs. Smart phones are rapidly becoming\nour main method of accessing information and are frequently used to perform\n`on-the-go' search tasks. As research into information retrieval continues to\nevolve, evaluating search behaviour in context is relatively new. Previous\nresearch has studied the effects of context through either self-reported diary\nstudies or quantitative log analysis; however, neither approach is able to\naccurately capture context of use at the time of searching. In this study, we\naim to gain a better understanding of task relevance and search behaviour via a\ntask-based user study (n=31) employing a bespoke Android app. The app allowed\nus to accurately capture the user's context when completing tasks at different\ntimes of the day over the period of a week. Through analysis of the collected\ndata, we gain a better understanding of how using smart phones on the go\nimpacts search behaviour, search performance and task relevance and whether or\nnot the actual context is an important factor.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 22:37:34 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 17:45:37 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Harvey", "Morgan", ""], ["Costa", "Luca", ""], ["Pointon", "Matthew", ""], ["Crestani", "Fabio", ""]]}, {"id": "1812.07143", "submitter": "Muratcan Cicek", "authors": "Muratcan Cicek, Jinrong Xie, Qiaosong Wang and Robinson Piramuthu", "title": "Mobile Head Tracking for eCommerce and Beyond", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shopping is difficult for people with motor impairments. This includes online\nshopping. Proprietary software can emulate mouse and keyboard via head\ntracking. However, such a solution is not common for smartphones. Unlike\ndesktop and laptop computers, they are also much easier to carry indoors and\noutdoors.To address this, we implement and open source button that is sensitive\nto head movements tracked from the front camera of iPhone X. This allows\ndevelopers to integrate in eCommerce applications easily without requiring\nspecialized knowledge. Other applications include gaming and use in hands-free\nsituations such as during cooking, auto-repair. We built a sample online\nshopping application that allows users to easily browse between items from\nvarious categories and take relevant action just by head movements. We present\nresults of user studies on this sample application and also include sensitivity\nstudies based on two independent tests performed at 3 different distances to\nthe screen.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 02:51:39 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Cicek", "Muratcan", ""], ["Xie", "Jinrong", ""], ["Wang", "Qiaosong", ""], ["Piramuthu", "Robinson", ""]]}, {"id": "1812.07328", "submitter": "Gestionnaire Hal-Su", "authors": "Claire Meyniel, Dalila Samri (IM2A), Farah Stefano (CH St Joseph),\n  Joel Crevoisier, Florence Bont\\'e (SAPPH), Raffaella Migliaccio (ICM, IM2A,\n  UPMC), Laure Delaby (IM2A), Anne Bertrand (ARAMIS, UPMC, ICM), Marie Odile\n  Habert (CATI), Bruno Dubois (UPMC, ICM, IM2A), Bahram Bodaghi, St\\'ephane\n  Epelbaum (IM2A, ARAMIS, UPMC, ICM)", "title": "COGEVIS: A New Scale to Evaluate Cognition in Patients with Visual\n  Deficiency", "comments": null, "journal-ref": "Behavioural Neurology, IOS Press, 2018, 2018, pp.1-7", "doi": "10.1155/2018/4295184", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluated the cognitive status of visually impaired patients referred to\nlow vision rehabilitation (LVR) based on a standard cognitive battery and a new\nevaluation tool, named the COGEVIS, which can be used to assess patients with\nsevere visual deficits. We studied patients aged 60 and above, referred to the\nLVR Hospital in Paris. Neurological and cognitive evaluations were performed in\nan expert memory center. Thirty-eight individuals, 17 women and 21 men with a\nmean age of 70.3 $\\pm$ 1.3 years and a mean visual acuity of 0.12 $\\pm$ 0.02,\nwere recruited over a one-year period. Sixty-three percent of participants had\nnormal cognitive status. Cognitive impairment was diagnosed in 37.5% of\nparticipants. The COGEVIS score cutoff point to screen for cognitive impairment\nwas 24 (maximum score of 30) with a sensitivity of 66.7% and a specificity of\n95%. Evaluation following 4 months of visual rehabilitation showed an\nimprovement of Instrumental Activities of Daily Living (p = 0 004), National\nEye Institute Visual Functioning Questionnaire (p = 0 035), and\nMontgomery-{\\AA}sberg Depression Rating Scale (p = 0 037). This study\nintroduces a new short test to screen for cognitive impairment in visually\nimpaired patients.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 12:28:48 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Meyniel", "Claire", "", "IM2A"], ["Samri", "Dalila", "", "IM2A"], ["Stefano", "Farah", "", "CH St Joseph"], ["Crevoisier", "Joel", "", "SAPPH"], ["Bont\u00e9", "Florence", "", "SAPPH"], ["Migliaccio", "Raffaella", "", "ICM, IM2A,\n  UPMC"], ["Delaby", "Laure", "", "IM2A"], ["Bertrand", "Anne", "", "ARAMIS, UPMC, ICM"], ["Habert", "Marie Odile", "", "CATI"], ["Dubois", "Bruno", "", "UPMC, ICM, IM2A"], ["Bodaghi", "Bahram", "", "IM2A, ARAMIS, UPMC, ICM"], ["Epelbaum", "St\u00e9phane", "", "IM2A, ARAMIS, UPMC, ICM"]]}, {"id": "1812.07339", "submitter": "Daniel Graziotin", "authors": "Falko Koetter, Matthias Blohm, Monika Kochanowski, Joscha Goetzer,\n  Daniel Graziotin, Stefan Wagner", "title": "Motivations, Classification and Model Trial of Conversational Agents for\n  Insurance Companies", "comments": "12 pages, 6 figure, accepted for presentation at The International\n  Conference on Agents and Artificial Intelligence 2019 (ICAART 2019)", "journal-ref": null, "doi": "10.5220/0007252100190030", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in artificial intelligence have renewed interest in conversational\nagents. So-called chatbots have reached maturity for industrial applications.\nGerman insurance companies are interested in improving their customer service\nand digitizing their business processes. In this work we investigate the\npotential use of conversational agents in insurance companies by determining\nwhich classes of agents are of interest to insurance companies, finding\nrelevant use cases and requirements, and developing a prototype for an\nexemplary insurance scenario. Based on this approach, we derive key findings\nfor conversational agent implementation in insurance companies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 12:56:51 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 10:01:57 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Koetter", "Falko", ""], ["Blohm", "Matthias", ""], ["Kochanowski", "Monika", ""], ["Goetzer", "Joscha", ""], ["Graziotin", "Daniel", ""], ["Wagner", "Stefan", ""]]}, {"id": "1812.07509", "submitter": "Brendon Lutnick", "authors": "Brendon Lutnick, Brandon Ginley, Darshana Govind, Sean D. McGarry,\n  Peter S. LaViolette, Rabi Yacoub, Sanjay Jain, John E. Tomaszewski, Kuang-Yu\n  Jen, and Pinaki Sarder", "title": "Iterative annotation to ease neural network training: Specialized\n  machine learning in medical image analysis", "comments": "15 pages, 7 figures, 2 supplemental figures (on the last page)", "journal-ref": "Nature Machine Intelligence 1.2 (2019): 112", "doi": "10.1038/s42256-019-0018-3", "report-no": null, "categories": "eess.IV cs.CV cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks promise to bring robust, quantitative analysis to medical\nfields, but adoption is limited by the technicalities of training these\nnetworks. To address this translation gap between medical researchers and\nneural networks in the field of pathology, we have created an intuitive\ninterface which utilizes the commonly used whole slide image (WSI) viewer,\nAperio ImageScope (Leica Biosystems Imaging, Inc.), for the annotation and\ndisplay of neural network predictions on WSIs. Leveraging this, we propose the\nuse of a human-in-the-loop strategy to reduce the burden of WSI annotation. We\ntrack network performance improvements as a function of iteration and quantify\nthe use of this pipeline for the segmentation of renal histologic findings on\nWSIs. More specifically, we present network performance when applied to\nsegmentation of renal micro compartments, and demonstrate multi-class\nsegmentation in human and mouse renal tissue slides. Finally, to show the\nadaptability of this technique to other medical imaging fields, we demonstrate\nits ability to iteratively segment human prostate glands from radiology imaging\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:29:23 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Lutnick", "Brendon", ""], ["Ginley", "Brandon", ""], ["Govind", "Darshana", ""], ["McGarry", "Sean D.", ""], ["LaViolette", "Peter S.", ""], ["Yacoub", "Rabi", ""], ["Jain", "Sanjay", ""], ["Tomaszewski", "John E.", ""], ["Jen", "Kuang-Yu", ""], ["Sarder", "Pinaki", ""]]}, {"id": "1812.07653", "submitter": "Katrin Lohan", "authors": "Georgios Minadakis and Katrin Lohan", "title": "Using Pupil Diameter to Measure Cognitive Load", "comments": "Presented at AI-HRI AAAI-FSS, 2018 (arXiv:1809.06606)", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2018/08", "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will present a method for measuring cognitive load and\nonline real-time feedback using the Tobii Pro 2 eye-tracking glasses. The\nsystem is envisaged to be capable of estimating high cognitive load states and\nsituations, and adjust human-machine interfaces to the user's needs. The system\nis using well-known metrics such as average pupillary size over time. Our\nsystem can provide cognitive load feedback at 17-18 Hz. We will elaborate on\nour results of a HRI study using this tool to show it's functionality.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:31:45 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Minadakis", "Georgios", ""], ["Lohan", "Katrin", ""]]}, {"id": "1812.07809", "submitter": "Paul Pu Liang", "authors": "Hai Pham, Paul Pu Liang, Thomas Manzini, Louis-Philippe Morency,\n  Barnabas Poczos", "title": "Found in Translation: Learning Robust Joint Representations by Cyclic\n  Translations Between Modalities", "comments": "AAAI 2019, code available at https://github.com/hainow/MCTN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal sentiment analysis is a core research area that studies speaker\nsentiment expressed from the language, visual, and acoustic modalities. The\ncentral challenge in multimodal learning involves inferring joint\nrepresentations that can process and relate information from these modalities.\nHowever, existing work learns joint representations by requiring all modalities\nas input and as a result, the learned representations may be sensitive to noisy\nor missing modalities at test time. With the recent success of sequence to\nsequence (Seq2Seq) models in machine translation, there is an opportunity to\nexplore new ways of learning joint representations that may not require all\ninput modalities at test time. In this paper, we propose a method to learn\nrobust joint representations by translating between modalities. Our method is\nbased on the key insight that translation from a source to a target modality\nprovides a method of learning joint representations using only the source\nmodality as input. We augment modality translations with a cycle consistency\nloss to ensure that our joint representations retain maximal information from\nall modalities. Once our translation model is trained with paired multimodal\ndata, we only need data from the source modality at test time for final\nsentiment prediction. This ensures that our model remains robust from\nperturbations or missing information in the other modalities. We train our\nmodel with a coupled translation-prediction objective and it achieves new\nstate-of-the-art results on multimodal sentiment analysis datasets: CMU-MOSI,\nICT-MMMO, and YouTube. Additional experiments show that our model learns\nincreasingly discriminative joint representations with more input modalities\nwhile maintaining robustness to missing or perturbed modalities.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:38:21 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 09:20:33 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Pham", "Hai", ""], ["Liang", "Paul Pu", ""], ["Manzini", "Thomas", ""], ["Morency", "Louis-Philippe", ""], ["Poczos", "Barnabas", ""]]}, {"id": "1812.07941", "submitter": "Temitayo Olugbade", "authors": "Temitayo A. Olugbade, Joseph Newbold, Rose Johnson, Erica Volta, Paolo\n  Alborno, Radoslaw Niewiadomski, Max Dillon, Gualtiero Volpe, and Nadia\n  Bianchi-Berthouze", "title": "Automatic Detection of Reflective Thinking in Mathematical Problem\n  Solving based on Unconstrained Bodily Exploration", "comments": null, "journal-ref": null, "doi": "10.1109/TAFFC.2020.2978069", "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For technology (like serious games) that aims to deliver interactive\nlearning, it is important to address relevant mental experiences such as\nreflective thinking during problem solving. To facilitate research in this\ndirection, we present the weDraw-1 Movement Dataset of body movement sensor\ndata and reflective thinking labels for 26 children solving mathematical\nproblems in unconstrained settings where the body (full or parts) was required\nto explore these problems. Further, we provide qualitative analysis of\nbehaviours that observers used in identifying reflective thinking moments in\nthese sessions. The body movement cues from our compilation informed features\nthat lead to average F1 score of 0.73 for automatic detection of reflective\nthinking based on Long Short-Term Memory neural networks. We further obtained\n0.79 average F1 score for end-to-end detection of reflective thinking periods,\ni.e. based on raw sensor data. Finally, the algorithms resulted in 0.64 average\nF1 score for period subsegments as short as 4 seconds. Overall, our results\nshow the possibility of detecting reflective thinking moments from body\nmovement behaviours of a child exploring mathematical concepts bodily, such as\nwithin serious game play.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 17:38:19 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 08:23:19 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Olugbade", "Temitayo A.", ""], ["Newbold", "Joseph", ""], ["Johnson", "Rose", ""], ["Volta", "Erica", ""], ["Alborno", "Paolo", ""], ["Niewiadomski", "Radoslaw", ""], ["Dillon", "Max", ""], ["Volpe", "Gualtiero", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "1812.08000", "submitter": "Julian Steil", "authors": "Julian Steil, Inken Hagestedt, Michael Xuelin Huang, Andreas Bulling", "title": "Privacy-Aware Eye Tracking Using Differential Privacy", "comments": "9 pages, 8 figures, supplementary material", "journal-ref": null, "doi": "10.1145/3314111.3319915", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With eye tracking being increasingly integrated into virtual and augmented\nreality (VR/AR) head-mounted displays, preserving users' privacy is an ever\nmore important, yet under-explored, topic in the eye tracking community. We\nreport a large-scale online survey (N=124) on privacy aspects of eye tracking\nthat provides the first comprehensive account of with whom, for which services,\nand to what extent users are willing to share their gaze data. Using these\ninsights, we design a privacy-aware VR interface that uses differential\nprivacy, which we evaluate on a new 20-participant dataset for two privacy\nsensitive tasks: We show that our method can prevent user re-identification and\nprotect gender information while maintaining high performance for gaze-based\ndocument type classification. Our results highlight the privacy challenges\nparticular to gaze data and demonstrate that differential privacy is a\npotential means to address them. Thus, this paper lays important foundations\nfor future research on privacy-aware gaze interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:10:05 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 18:05:00 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 21:19:15 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Steil", "Julian", ""], ["Hagestedt", "Inken", ""], ["Huang", "Michael Xuelin", ""], ["Bulling", "Andreas", ""]]}, {"id": "1812.08032", "submitter": "Cagatay Turkay", "authors": "Cagatay Turkay, Nicola Pezzotti, Carsten Binnig, Hendrik Strobelt,\n  Barbara Hammer, Daniel A. Keim, Jean-Daniel Fekete, Themis Palpanas, Yunhai\n  Wang, Florin Rusu", "title": "Progressive Data Science: Potential and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science requires time-consuming iterative manual activities. In\nparticular, activities such as data selection, preprocessing, transformation,\nand mining, highly depend on iterative trial-and-error processes that could be\nsped-up significantly by providing quick feedback on the impact of changes. The\nidea of progressive data science is to compute the results of changes in a\nprogressive manner, returning a first approximation of results quickly and\nallow iterative refinements until converging to a final result. Enabling the\nuser to interact with the intermediate results allows an early detection of\nerroneous or suboptimal choices, the guided definition of modifications to the\npipeline and their quick assessment. In this paper, we discuss the\nprogressiveness challenges arising in different steps of the data science\npipeline. We describe how changes in each step of the pipeline impact the\nsubsequent steps and outline why progressive data science will help to make the\nprocess more effective. Computing progressive approximations of outcomes\nresulting from changes creates numerous research challenges, especially if the\nchanges are made in the early steps of the pipeline. We discuss these\nchallenges and outline first steps towards progressiveness, which, we argue,\nwill ultimately help to significantly speed-up the overall data science\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:45:03 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 17:02:46 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Turkay", "Cagatay", ""], ["Pezzotti", "Nicola", ""], ["Binnig", "Carsten", ""], ["Strobelt", "Hendrik", ""], ["Hammer", "Barbara", ""], ["Keim", "Daniel A.", ""], ["Fekete", "Jean-Daniel", ""], ["Palpanas", "Themis", ""], ["Wang", "Yunhai", ""], ["Rusu", "Florin", ""]]}, {"id": "1812.08335", "submitter": "Bodong Chen", "authors": "Bodong Chen and Haiyi Zhu", "title": "Towards Value-Sensitive Learning Analytics Design", "comments": "The 9th International Learning Analytics & Knowledge Conference\n  (LAK19)", "journal-ref": null, "doi": "10.1145/3303772.3303798", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To support ethical considerations and system integrity in learning analytics,\nthis paper introduces two cases of applying the Value Sensitive Design\nmethodology to learning analytics design. The first study applied two methods\nof Value Sensitive Design, namely stakeholder analysis and value analysis, to a\nconceptual investigation of an existing learning analytics tool. This\ninvestigation uncovered a number of values and value tensions, leading to\ndesign trade-offs to be considered in future tool refinements. The second study\nholistically applied Value Sensitive Design to the design of a recommendation\nsystem for the Wikipedia WikiProjects. To proactively consider values among\nstakeholders, we derived a multi-stage design process that included literature\nanalysis, empirical investigations, prototype development, community\nengagement, iterative testing and refinement, and continuous evaluation. By\nreporting on these two cases, this paper responds to a need of practical means\nto support ethical considerations and human values in learning analytics\nsystems. These two cases demonstrate that Value Sensitive Design could be a\nviable approach for balancing a wide range of human values, which tend to\nencompass and surpass ethical issues, in learning analytics design.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 02:45:34 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 16:59:32 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Chen", "Bodong", ""], ["Zhu", "Haiyi", ""]]}, {"id": "1812.08660", "submitter": "Jibon Naher", "authors": "Jibon Naher and Matiur Rahman Minar", "title": "Impact of Social Media Posts in Real life Violence: A Case Study in\n  Bangladesh", "comments": "7 pages. arXiv admin note: substantial text overlap with\n  arXiv:1804.11241", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Networking Site (SNS) is a great innovation of modern times. Facebook,\nTwitter etc. have become an everyday part of peoples' life. Among all SNSs,\nFacebook is the most popular social network all over the world. Bangladesh is\nno exception. People of Bangladesh use Facebook for social communication,\nonline shopping, business, knowledge and experience sharing etc. As well as the\nvarious uses of SNSs, people sometimes find themselves involved in real life\nviolence, provoked by some social media posts or activities. In this paper, we\ndiscussed some case studies in which real life violence is originated based on\nFacebook activities in Bangladesh. Facebook was used in these incidents\nintentionally or unintentionally mostly as a tool to trigger hatred and\nviolence. We analyzed and discussed the real-world consequences of these\nvirtual activities in social media. Lastly, we recommended possible future\nmeasurements to prevent such violence.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:02:22 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Naher", "Jibon", ""], ["Minar", "Matiur Rahman", ""]]}, {"id": "1812.08842", "submitter": "arXiv Admin", "authors": "Zeliang Cheng, Vahab Vahdat, Yingzi Lin", "title": "A novel approach to study the effect of font and background color\n  combinations on the text recognition efficiency on LCDs", "comments": "arXiv admin note: This paper has been withdrawn due to an\n  irreconcilable author dispute", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the popularization of cell phones, laptops, and tablets, Liquid Crystal\nDisplays (LCDs) have become one of the main types of User Interface (UI) in the\nmodern world. While LCDs are widely used for retrieving text information, the\nimpact of text formatting on the legibility is often overlooked. With the goal\nof improving recognition efficiency (RE) on LCDs, this paper studies the impact\nof font-background colors on RE of texts being presented on LCD. For this\npurpose, difference between font/background color combinations, Primary Color\nDifference (PCD), is introduced that brings efficient RE assessment under wider\nspectrum. Accordingly, a testing platform is designed in C#. NET that captures\nparticipants response time to different font-background color combination\nstimuli. Based on the results, black background and green font color outperform\nother tested colors especially when the PCD is maximized. In correspond to\nresults, Implications for using research outcome in prototype LCDs are\nsuggested.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 21:13:07 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 21:54:54 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Cheng", "Zeliang", ""], ["Vahdat", "Vahab", ""], ["Lin", "Yingzi", ""]]}, {"id": "1812.08989", "submitter": "Jianfeng Gao", "authors": "Li Zhou, Jianfeng Gao, Di Li, Heung-Yeung Shum", "title": "The Design and Implementation of XiaoIce, an Empathetic Social Chatbot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the development of Microsoft XiaoIce, the most popular\nsocial chatbot in the world. XiaoIce is uniquely designed as an AI companion\nwith an emotional connection to satisfy the human need for communication,\naffection, and social belonging. We take into account both intelligent quotient\n(IQ) and emotional quotient (EQ) in system design, cast human-machine social\nchat as decision-making over Markov Decision Processes (MDPs), and optimize\nXiaoIce for long-term user engagement, measured in expected Conversation-turns\nPer Session (CPS). We detail the system architecture and key components\nincluding dialogue manager, core chat, skills, and an empathetic computing\nmodule. We show how XiaoIce dynamically recognizes human feelings and states,\nunderstands user intent, and responds to user needs throughout long\nconversations. Since her launch in 2014, XiaoIce has communicated with over 660\nmillion active users and succeeded in establishing long-term relationships with\nmany of them. Analysis of large scale online logs shows that XiaoIce has\nachieved an average CPS of 23, which is significantly higher than that of other\nchatbots and even human conversations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 08:01:31 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 21:23:51 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zhou", "Li", ""], ["Gao", "Jianfeng", ""], ["Li", "Di", ""], ["Shum", "Heung-Yeung", ""]]}, {"id": "1812.08996", "submitter": "HaiLong Liu", "authors": "Hailong Liu and Toshihiro Hiraoka", "title": "Driving behavior model considering driver's over-trust in driving\n  automation system", "comments": "10 pages, 1 table, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Levels one to three of driving automation systems~(DAS) are spreading fast.\nHowever, as the DAS functions become more and more sophisticated, not only the\ndriver's driving skills will reduce, but also the problem of over-trust will\nbecome serious. If a driver has over-trust in the DAS, he/she will become not\naware of hazards in time. To prevent the driver's over-trust in the DAS, this\npaper discusses the followings: 1) the definition of over-trust in the DAS, 2)\na hypothesis of occurrence condition and occurrence process of over-trust in\nthe DAS, and 3) a driving behavior model based on the trust in the DAS, the\nrisk homeostasis theory, and the over-trust prevention human-machine interface.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 08:38:32 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 02:32:36 GMT"}, {"version": "v3", "created": "Wed, 19 Jun 2019 05:45:30 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Liu", "Hailong", ""], ["Hiraoka", "Toshihiro", ""]]}, {"id": "1812.09049", "submitter": "Christiane Gresse Von Wangenheim", "authors": "Christiane G. von Wangenheim, Jo\\~ao V. Araujo Porto, Jean C.R. Hauck,\n  Adriano F. Borgatto", "title": "Do we agree on user interface aesthetics of Android apps?", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Context: Visual aesthetics is increasingly seen as an essential factor in\nperceived usability, interaction, and overall appraisal of user interfaces\nespecially with respect to mobile applications. Yet, a question that remains is\nhow to assess and to which extend users agree on visual aesthetics. Objective:\nThis paper analyzes the inter-rater agreement on visual aesthetics of user\ninterfaces of Android apps as a basis for guidelines and evaluation models.\nMethod: We systematically collected ratings on the visual aesthetics of 100\nuser interfaces of Android apps from 10 participants and analyzed the frequency\ndistribution, reliability and influencing design aspects. Results: In general,\nuser interfaces of Android apps are perceived more ugly than beautiful. Yet,\nraters only moderately agree on the visual aesthetics. Disagreements seem to be\nrelated to subtle differences with respect to layout, shapes, colors,\ntypography, and background images. Conclusion: Visual aesthetics is a key\nfactor for the success of apps. However, the considerable disagreement of\nraters on the perceived visual aesthetics indicates the need for a better\nunderstanding of this software quality with respect to mobile apps.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 11:08:23 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["von Wangenheim", "Christiane G.", ""], ["Porto", "Jo\u00e3o V. Araujo", ""], ["Hauck", "Jean C. R.", ""], ["Borgatto", "Adriano F.", ""]]}, {"id": "1812.09548", "submitter": "Malte Jung", "authors": "Malte F. Jung, Dominic DiFranzo, Brett Stoll, Solace Shen, Austin\n  Lawrence, Houston Claure", "title": "Robot Assisted Tower Construction - A Resource Distribution Task to\n  Study Human-Robot Collaboration and Interaction with Groups of People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on human-robot collaboration or human-robot teaming, has focused\npredominantly on understanding and enabling collaboration between a single\nrobot and a single human. Extending human-robot collaboration research beyond\nthe dyad, raises novel questions about how a robot should distribute resources\namong group members and about what the social and task related consequences of\nthe distribution are. Methodological advances are needed to allow researchers\nto collect data about human robot collaboration that involves multiple people.\nThis paper presents Tower Construction, a novel resource distribution task that\nallows researchers to examine collaboration between a robot and groups of\npeople. By focusing on the question of whether and how a robot's distribution\nof resources (wooden blocks required for a building task) affects collaboration\ndynamics and outcomes, we provide a case of how this task can be applied in a\nlaboratory study with 124 participants to collect data about human robot\ncollaboration that involves multiple humans. We highlight the kinds of insights\nthe task can yield. In particular we find that the distribution of resources\naffects perceptions of performance, and interpersonal dynamics between human\nteam-members.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 15:52:27 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Jung", "Malte F.", ""], ["DiFranzo", "Dominic", ""], ["Stoll", "Brett", ""], ["Shen", "Solace", ""], ["Lawrence", "Austin", ""], ["Claure", "Houston", ""]]}, {"id": "1812.09628", "submitter": "Jugal Kalita", "authors": "Ali Alkhatlan and Jugal Kalita", "title": "Intelligent Tutoring Systems: A Comprehensive Historical Survey with\n  Recent Developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides interested beginners with an updated and detailed\nintroduction to the field of Intelligent Tutoring Systems (ITS). ITSs are\ncomputer programs that use artificial intelligence techniques to enhance and\npersonalize automation in teaching. This paper is a literature review that\nprovides the following: First, a review of the history of ITS along with a\ndiscussion on the interface between human learning and computer tutors and how\neffective ITSs are in contemporary education. Second, the traditional\narchitectural components of an ITS and their functions are discussed along with\napproaches taken by various ITSs. Finally, recent innovative ideas in ITS\nsystems are presented. This paper concludes with some of the author's views\nregarding future work in the field of intelligent tutoring systems.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 00:40:44 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Alkhatlan", "Ali", ""], ["Kalita", "Jugal", ""]]}, {"id": "1812.09640", "submitter": "Vikram Krishnamurthy", "authors": "William Hoiles and Vikram Krishnamurthy", "title": "Estimating Rationally Inattentive Utility Functions with Deep Clustering\n  for Framing - Applications in YouTube Engagement Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a framework involving behavioral economics and machine learning.\nRationally inattentive Bayesian agents make decisions based on their posterior\ndistribution, utility function and information acquisition cost Renyi\ndivergence which generalizes Shannon mutual information). By observing these\ndecisions, how can an observer estimate the utility function and information\nacquisition cost? Using deep learning, we estimate framing information\n(essential extrinsic features) that determines the agent's attention strategy.\nThen we present a preference based inverse reinforcement learning algorithm to\ntest for rational inattention: is the agent an utility maximizer, attention\nmaximizer, and does an information cost function exist that rationalizes the\ndata? The test imposes a Renyi mutual information constraint which impacts how\nthe agent can select attention strategies to maximize their expected utility.\nThe test provides constructive estimates of the utility function and\ninformation acquisition cost of the agent. We illustrate these methods on a\nmassive YouTube dataset for characterizing the commenting behavior of users.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 02:31:11 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Hoiles", "William", ""], ["Krishnamurthy", "Vikram", ""]]}, {"id": "1812.09724", "submitter": "Xi Chen", "authors": "Xi Chen, Caylin Hickey", "title": "Parallelized Interactive Machine Learning on Autonomous Vehicles", "comments": "6 pages, NAECON 2018 - IEEE National Aerospace and Electronics\n  Conference", "journal-ref": null, "doi": "10.1109/NAECON.2018.8556776", "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (deep RL) has achieved superior performance in\ncomplex sequential tasks by learning directly from image input. A deep neural\nnetwork is used as a function approximator and requires no specific state\ninformation. However, one drawback of using only images as input is that this\napproach requires a prohibitively large amount of training time and data for\nthe model to learn the state feature representation and approach reasonable\nperformance. This is not feasible in real-world applications, especially when\nthe data are expansive and training phase could introduce disasters that affect\nhuman safety. In this work, we use a human demonstration approach to speed up\ntraining for learning features and use the resulting pre-trained model to\nreplace the neural network in the deep RL Deep Q-Network (DQN), followed by\nhuman interaction to further refine the model. We empirically evaluate our\napproach by using only a human demonstration model and modified DQN with human\ndemonstration model included in the Microsoft AirSim car simulator. Our results\nshow that (1) pre-training with human demonstration in a supervised learning\napproach is better and much faster at discovering features than DQN alone, (2)\ninitializing the DQN with a pre-trained model provides a significant\nimprovement in training time and performance even with limited human\ndemonstration, and (3) providing the ability for humans to supply suggestions\nduring DQN training can speed up the network's convergence on an optimal\npolicy, as well as allow it to learn more complex policies that are harder to\ndiscover by random exploration.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 14:57:28 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Chen", "Xi", ""], ["Hickey", "Caylin", ""]]}, {"id": "1812.09835", "submitter": "Tommy Hosman", "authors": "Tommy Hosman, Marco Vilela, Daniel Milstein, Jessica N. Kelemen, David\n  M. Brandman, Leigh R. Hochberg, John D. Simeral", "title": "BCI decoder performance comparison of an LSTM recurrent neural network\n  and a Kalman filter in retrospective simulation", "comments": "Accepted for the 9th International IEEE EMBS Conference on Neural\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intracortical brain computer interfaces (iBCIs) using linear Kalman decoders\nhave enabled individuals with paralysis to control a computer cursor for\ncontinuous point-and-click typing on a virtual keyboard, browsing the internet,\nand using familiar tablet apps. However, further advances are needed to deliver\niBCI-enabled cursor control approaching able-bodied performance. Motivated by\nrecent evidence that nonlinear recurrent neural networks (RNNs) can provide\nhigher performance iBCI cursor control in nonhuman primates (NHPs), we\nevaluated decoding of intended cursor velocity from human motor cortical\nsignals using a long-short term memory (LSTM) RNN trained across multiple days\nof multi-electrode recordings. Running simulations with previously recorded\nintracortical signals from three BrainGate iBCI trial participants, we\ndemonstrate an RNN that can substantially increase bits-per-second metric in a\nhigh-speed cursor-based target selection task as well as a challenging\nsmall-target high-accuracy task when compared to a Kalman decoder. These\nresults indicate that RNN decoding applied to human intracortical signals could\nachieve substantial performance advances in continuous 2-D cursor control and\nmotivate a real-time RNN implementation for online evaluation by individuals\nwith tetraplegia.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 05:22:15 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Hosman", "Tommy", ""], ["Vilela", "Marco", ""], ["Milstein", "Daniel", ""], ["Kelemen", "Jessica N.", ""], ["Brandman", "David M.", ""], ["Hochberg", "Leigh R.", ""], ["Simeral", "John D.", ""]]}, {"id": "1812.10394", "submitter": "Afsaneh Doryab", "authors": "Afsaneh Doryab, Prerna Chikarsel, Xinwen Liu, Anind K. Dey", "title": "Extraction of Behavioral Features from Smartphone and Wearable Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rich set of sensors in smartphones and wearable devices provides the\npossibility to passively collect streams of data in the wild. The raw data\nstreams, however, can rarely be directly used in the modeling pipeline. We\nprovide a generic framework that can process raw data streams and extract\nuseful features related to non-verbal human behavior. This framework can be\nused by researchers in the field who are interested in processing data from\nsmartphones and Wearable devices.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 18:45:33 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 01:46:42 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Doryab", "Afsaneh", ""], ["Chikarsel", "Prerna", ""], ["Liu", "Xinwen", ""], ["Dey", "Anind K.", ""]]}, {"id": "1812.11090", "submitter": "Roy Shilkrot", "authors": "Zhi Chai and Roy Shilkrot", "title": "Enhanced Touchable Projector-depth System with Deep Hand Pose Estimation", "comments": "9 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Touchable projection with structured light range cameras is a prolific medium\nfor large interaction surfaces, affording multiple simultaneous users and\nsimple, cheap setup. However robust touch detection in such projector-depth\nsystems is difficult to achieve due to measurement noise. We propose a novel\ncombination of surface touch detection and a deep network for hand pose\nestimation, which aids in detecting both on- and above-surface hand gestures,\ndisambiguating multiple touch fingers, as well as recovering fingertip\npositions in face of noisy input. We present the details of our GPU-accelerated\nsystem and an evaluation of its performance, as well as applications such as an\nenhanced virtual keyboard that utilizes the added features.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 16:21:40 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Chai", "Zhi", ""], ["Shilkrot", "Roy", ""]]}, {"id": "1812.11423", "submitter": "Asma Ghandeharioun", "authors": "Asma Ghandeharioun, Daniel McDuff, Mary Czerwinski, Kael Rowan", "title": "EMMA: An Emotion-Aware Wellbeing Chatbot", "comments": "Accepted for presentation at 2019 8th International Conference on\n  Affective Computing and Intelligent Interaction (ACII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The delivery of mental health interventions via ubiquitous devices has shown\nmuch promise. A conversational chatbot is a promising oracle for delivering\nappropriate just-in-time interventions. However, designing emotionally-aware\nagents, specially in this context, is under-explored. Furthermore, the\nfeasibility of automating the delivery of just-in-time mHealth interventions\nvia such an agent has not been fully studied. In this paper, we present the\ndesign and evaluation of EMMA (EMotion-Aware mHealth Agent) through a two-week\nlong human-subject experiment with N=39 participants. EMMA provides emotionally\nappropriate micro-activities in an empathetic manner. We show that the system\ncan be extended to detect a user's mood purely from smartphone sensor data. Our\nresults show that our personalized machine learning model was perceived as\nlikable via self-reports of emotion from users. Finally, we provide a set of\nguidelines for the design of emotion-aware bots for mHealth.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 18:50:00 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 01:46:01 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Ghandeharioun", "Asma", ""], ["McDuff", "Daniel", ""], ["Czerwinski", "Mary", ""], ["Rowan", "Kael", ""]]}, {"id": "1812.11622", "submitter": "Abbas Ganji", "authors": "Abbas Ganji, Mania Orand and David W. McDonald", "title": "Ease on Down the Code: Complex Collaborative Qualitative Coding\n  Simplified with 'Code Wizard'", "comments": null, "journal-ref": "Proceedings of the ACM on Human-Computer Interaction (CSCW),\n  Volume 2 Issue CSCW, Article No. 132, November 2018", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and development of a preliminary qualitative\ncoding tool as well as a method to improve the process of achieving inter-coder\nreliability (ICR) in small teams. Software applications that support\nqualitative coding do not sufficiently assist collaboration among coders and\noverlook some fundamental issues related to ICR. We propose a new dimension of\ncollaborative coding called \"coders' certainty\" and demonstrate its ability to\nillustrate valuable code disagreements that are missing from existing\napproaches. Through a case study, we describe the utility of our tool, Code\nWizard, and how it helped a group of researchers effectively collaborate to\ncode naturalistic observation data. We report the valuable lessons we learned\nfrom the development of our tool and method: (1) identifying coders' certainty\nconstitutes an important part of determining the quality of data analysis and\nfacilitates identifying overlapping and ambiguous codes, (2) making the details\nof coding process visible helps streamline the coding process and leads to a\nsense of ownership of the research results, and (3) there is valuable\ninformation hidden in coding disagreements that can be leveraged for improving\nthe process of data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 22:29:41 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Ganji", "Abbas", ""], ["Orand", "Mania", ""], ["McDonald", "David W.", ""]]}, {"id": "1812.11850", "submitter": "Diego Ulisse Pizzagalli", "authors": "Diego Ulisse Pizzagalli, Santiago Fernandez Gonzalez, Rolf Krause", "title": "A shortest-path based clustering algorithm for joint human-machine\n  analysis of complex datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a technique for the analysis of datasets obtained by empirical\nstudies in several disciplines with a major application for biomedical\nresearch. Essentially, clustering algorithms are executed by machines aiming at\nfinding groups of related points in a dataset. However, the result of grouping\ndepends on both metrics for point-to-point similarity and rules for\npoint-to-group association. Indeed, non-appropriate metrics and rules can lead\nto undesirable clustering artifacts. This is especially relevant for datasets,\nwhere groups with heterogeneous structures co-exist. In this work, we propose\nan algorithm that achieves clustering by exploring the paths between points.\nThis allows both, to evaluate the properties of the path (such as gaps, density\nvariations, etc.), and expressing the preference for certain paths. Moreover,\nour algorithm supports the integration of existing knowledge about admissible\nand non-admissible clusters by training a path classifier. We demonstrate the\naccuracy of the proposed method on challenging datasets including points from\nsynthetic shapes in publicly available benchmarks and microscopy data.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 15:50:53 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Pizzagalli", "Diego Ulisse", ""], ["Gonzalez", "Santiago Fernandez", ""], ["Krause", "Rolf", ""]]}]