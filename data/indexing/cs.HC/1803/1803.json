[{"id": "1803.00296", "submitter": "Jeremy Frey", "authors": "Jelena Mladenovic (IDC, Potioc), J\\'er\\'emy Frey (IDC), Jessica\n  Cauchard (IDC)", "title": "Di\\v{s}imo: Anchoring Our Breath", "comments": null, "journal-ref": "CHI '18 Interactivity - SIGCHI Conference on Human Factors in\n  Computing System, Apr 2018, Montreal, Canada", "doi": "10.1145/3170427.3186517", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system that raises awareness about users' inner state.\nDi\\v{s}imo is a multimodal ambient display that provides feedback about one's\nstress level, which is assessed through heart rate monitoring. Upon detecting a\nlow heart rate variability for a prolonged period of time, Di\\v{s}imo plays an\naudio track, setting the pace of a regular and deep breathing. Users can then\nchoose to take a moment to focus on their breath. By doing so, they will\nactivate the Di\\v{s}imo devices belonging to their close ones, who can then\njoin for a shared relaxation session.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 10:45:12 GMT"}], "update_date": "2018-06-24", "authors_parsed": [["Mladenovic", "Jelena", "", "IDC, Potioc"], ["Frey", "J\u00e9r\u00e9my", "", "IDC"], ["Cauchard", "Jessica", "", "IDC"]]}, {"id": "1803.00458", "submitter": "TonTon Huang", "authors": "TonTon Hsien-De Huang, and Hung-Yu Kao", "title": "C-3PO: Click-sequence-aware DeeP Neural Network (DNN)-based Pop-uPs\n  RecOmmendation", "comments": "2018/12/20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of mobile and wearable devices, push notification becomes\na powerful tool to connect and maintain the relationship with App users, but\nsending inappropriate or too many messages at the wrong time may result in the\nApp being removed by the users. In order to maintain the retention rate and the\ndelivery rate of advertisement, we adopt Deep Neural Network (DNN) to develop a\npop-up recommendation system \"Click sequence-aware deeP neural network\n(DNN)-based Pop-uPs recOmmendation (C-3PO)\" enabled by collaborative\nfiltering-based hybrid user behavioral analysis. We further verified the system\nwith real data collected from the product Security Master, Clean Master and CM\nBrowser, supported by Leopard Mobile Inc. (Cheetah Mobile Taiwan Agency). In\nthis way, we can know precisely about users' preference and frequency to click\non the push notification/pop-ups, decrease the troublesome to users\nefficiently, and meanwhile increase the click through rate of push\nnotifications/pop-ups.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 15:30:54 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 15:00:50 GMT"}, {"version": "v3", "created": "Fri, 9 Mar 2018 12:45:25 GMT"}, {"version": "v4", "created": "Thu, 20 Dec 2018 05:58:38 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Huang", "TonTon Hsien-De", ""], ["Kao", "Hung-Yu", ""]]}, {"id": "1803.00459", "submitter": "Gourab Mitra", "authors": "Gourab Mitra", "title": "Challenges and opportunities in visual interpretation of Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We live in a world where data generation is omnipresent. Innovations in\ncomputer hardware in the last few decades coupled with increasingly reliable\nconnectivity among them have fueled this phenomenon. We are constantly creating\nand consuming data across digital devices of varying form factors. Leveraging\nhuge quantities of data involves making interpretations from it. However,\ninterpreting data is still a difficult task. We need data analysts to help make\ndecisions. These experts apply their domain knowledge, understanding of the\nproblem space and numerical analysis to draw inferences from the data in order\nto support decision making. Existing tools and techniques for interference\nserve users making decisions with hard constraints. Consumer systems are often\nbuilt to support exploratory data analysis in mind rather than sense making.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 15:41:52 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Mitra", "Gourab", ""]]}, {"id": "1803.00757", "submitter": "Ting Sun", "authors": "Ting Sun, Shengyi Nie, Dit-Yan Yeung, Shaojie Shen", "title": "Gesture-based Piloting of an Aerial Robot using Monocular Vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial robots are becoming popular among general public, and with the\ndevelopment of artificial intelligence (AI), there is a trend to equip aerial\nrobots with a natural user interface (NUI). Hand/arm gestures are an intuitive\nway to communicate for humans, and various research works have focused on\ncontrolling an aerial robot with natural gestures. However, the techniques in\nthis area are still far from mature. Many issues in this area have been poorly\naddressed, such as the principles of choosing gestures from the design point of\nview, hardware requirements from an economic point of view, considerations of\ndata availability, and algorithm complexity from a practical perspective. Our\nwork focuses on building an economical monocular system particularly designed\nfor gesture-based piloting of an aerial robot. Natural arm gestures are mapped\nto rich target directions and convenient fine adjustment is achieved. Practical\npiloting scenarios, hardware cost and algorithm applicability are jointly\nconsidered in our system design. The entire system is successfully implemented\nin an aerial robot and various properties of the system are tested.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 08:41:09 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Sun", "Ting", ""], ["Nie", "Shengyi", ""], ["Yeung", "Dit-Yan", ""], ["Shen", "Shaojie", ""]]}, {"id": "1803.01145", "submitter": "Pedro Campos", "authors": "Pedro Campos, Nils Ehrenberg and Miguel Campos", "title": "Designing Interactions with Furniture: Towards Multi-Sensorial\n  Interaction Design Processes for Interactive Furniture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue for novel user experience design methods, in the\ncontext of reimagining ergonomics of interactive furniture.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 11:13:53 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Campos", "Pedro", ""], ["Ehrenberg", "Nils", ""], ["Campos", "Miguel", ""]]}, {"id": "1803.01166", "submitter": "Seonwook Park", "authors": "Seonwook Park and Christoph Gebhardt and Roman R\\\"adle and Anna Feit\n  and Hana Vrzakova and Niraj Dayama and Hui-Shyong Yeo and Clemens Klokmose\n  and Aaron Quigley and Antti Oulasvirta and Otmar Hilliges", "title": "AdaM: Adapting Multi-User Interfaces for Collaborative Environments in\n  Real-Time", "comments": "formatting tweaks", "journal-ref": null, "doi": "10.1145/3173574.3173758", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing cross-device multi-user interfaces (UIs) is a challenging problem.\nThere are numerous ways in which content and interactivity can be distributed.\nHowever, good solutions must consider multiple users, their roles, their\npreferences and access rights, as well as device capabilities. Manual and\nrule-based solutions are tedious to create and do not scale to larger problems\nnor do they adapt to dynamic changes, such as users leaving or joining an\nactivity. In this paper, we cast the problem of UI distribution as an\nassignment problem and propose to solve it using combinatorial optimization. We\npresent a mixed integer programming formulation which allows real-time\napplications in dynamically changing collaborative settings. It optimizes the\nallocation of UI elements based on device capabilities, user roles,\npreferences, and access rights. We present a proof-of-concept\ndesigner-in-the-loop tool, allowing for quick solution exploration. Finally, we\ncompare our approach to traditional paper prototyping in a lab study.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 14:05:07 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 11:22:23 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Park", "Seonwook", ""], ["Gebhardt", "Christoph", ""], ["R\u00e4dle", "Roman", ""], ["Feit", "Anna", ""], ["Vrzakova", "Hana", ""], ["Dayama", "Niraj", ""], ["Yeo", "Hui-Shyong", ""], ["Klokmose", "Clemens", ""], ["Quigley", "Aaron", ""], ["Oulasvirta", "Antti", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1803.01256", "submitter": "Yuan Lu", "authors": "Yuan Lu, Qiang Tang, Guiling Wang", "title": "ZebraLancer: Decentralized Crowdsourcing of Human Knowledge atop Open\n  Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.DC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and implement the first private and anonymous decentralized\ncrowdsourcing system ZebraLancer, and overcome two fundamental challenges of\ndecentralizing crowdsourcing, i.e., data leakage and identity breach.\n  First, our outsource-then-prove methodology resolves the tension between the\nblockchain transparency and the data confidentiality to guarantee the basic\nutilities/fairness requirements of data crowdsourcing, thus ensuring: (i) a\nrequester will not pay more than what data deserve, according to a policy\nannounced when her task is published via the blockchain; (ii) each worker\nindeed gets a payment based on the policy, if he submits data to the\nblockchain; (iii) the above properties are realized not only without a central\narbiter, but also without leaking the data to the open blockchain. Second, the\ntransparency of blockchain allows one to infer private information about\nworkers and requesters through their participation history. Simply enabling\nanonymity is seemingly attempting but will allow malicious workers to submit\nmultiple times to reap rewards. ZebraLancer also overcomes this problem by\nallowing anonymous requests/submissions without sacrificing accountability. The\nidea behind is a subtle linkability: if a worker submits twice to a task,\nanyone can link the submissions, or else he stays anonymous and unlinkable\nacross tasks. To realize this delicate linkability, we put forward a novel\ncryptographic concept, i.e., the common-prefix-linkable anonymous\nauthentication. We remark the new anonymous authentication scheme might be of\nindependent interest. Finally, we implement our protocol for a common image\nannotation task and deploy it in a test net of Ethereum. The experiment results\nshow the applicability of our protocol atop the existing real-world blockchain.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 22:42:14 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 20:23:27 GMT"}, {"version": "v3", "created": "Sun, 13 Jan 2019 05:17:25 GMT"}, {"version": "v4", "created": "Sun, 17 Feb 2019 23:55:44 GMT"}, {"version": "v5", "created": "Thu, 7 May 2020 17:53:10 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Lu", "Yuan", ""], ["Tang", "Qiang", ""], ["Wang", "Guiling", ""]]}, {"id": "1803.01316", "submitter": "Johannes F\\\"urnkranz", "authors": "Johannes F\\\"urnkranz, Tom\\'a\\v{s} Kliegr, Heiko Paulheim", "title": "On Cognitive Preferences and the Plausibility of Rule-based Models", "comments": "V4: Another rewrite of section on interpretability to clarify focus\n  on plausibility and relation to interpretability, comprehensibility, and\n  justifiability", "journal-ref": "Machine Learning 109(4):853-898, 2020", "doi": "10.1007/s10994-019-05856-5", "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is conventional wisdom in machine learning and data mining that logical\nmodels such as rule sets are more interpretable than other models, and that\namong such rule-based models, simpler models are more interpretable than more\ncomplex ones. In this position paper, we question this latter assumption by\nfocusing on one particular aspect of interpretability, namely the plausibility\nof models. Roughly speaking, we equate the plausibility of a model with the\nlikeliness that a user accepts it as an explanation for a prediction. In\nparticular, we argue that, all other things being equal, longer explanations\nmay be more convincing than shorter ones, and that the predominant bias for\nshorter models, which is typically necessary for learning powerful\ndiscriminative models, may not be suitable when it comes to user acceptance of\nthe learned models. To that end, we first recapitulate evidence for and against\nthis postulate, and then report the results of an evaluation in a\ncrowd-sourcing study based on about 3.000 judgments. The results do not reveal\na strong preference for simple rules, whereas we can observe a weak preference\nfor longer rules in some domains. We then relate these results to well-known\ncognitive biases such as the conjunction fallacy, the representative heuristic,\nor the recogition heuristic, and investigate their relation to rule length and\nplausibility.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 08:26:43 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 14:03:06 GMT"}, {"version": "v3", "created": "Sat, 18 Aug 2018 15:02:51 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2019 08:37:58 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["F\u00fcrnkranz", "Johannes", ""], ["Kliegr", "Tom\u00e1\u0161", ""], ["Paulheim", "Heiko", ""]]}, {"id": "1803.01325", "submitter": "Huilin Zhu", "authors": "Wei Cao, Wenxu Song, Xinge Li, Sixiao Zheng, Ge Zhang, Yanting Wu,\n  Sailing He, Huilin Zhu, Jiajia Chen", "title": "Could Interaction with Social Robots Facilitate Joint Attention of\n  Children with Autism Spectrum Disorder?", "comments": "First author: Wei Cao and Wenxu Song; Corresponding author: Huilin\n  Zhu(huilin.zhu@m.scnu.edu.cn)ans Jiajia Chen(jiajiac@kth.se)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This research addressed whether interactions with social robots could\nfacilitate joint attention of the autism spectrum disorder (ASD). Two\nconditions of initiators, namely 'Human' vs. 'Robot' were measured with 15\nchildren with ASD and 15 age-matched typically developing (TD) children. Apart\nfrom fixation and gaze transition, a new longest common subsequence (LCS)\napproach was proposed to analyze eye-movement traces. Results revealed that\nchildren with ASD showed deficits of joint attention. Compared to the human\nagent, robot facilitate less fixations towards the targets, but it attracted\nmore attention and allowed the children to show gaze transition and to follow\njoint attention logic. This results highlight both potential application of LCS\nanalysis on eye-tracking studies and of social robot to intervention.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 09:12:34 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Cao", "Wei", ""], ["Song", "Wenxu", ""], ["Li", "Xinge", ""], ["Zheng", "Sixiao", ""], ["Zhang", "Ge", ""], ["Wu", "Yanting", ""], ["He", "Sailing", ""], ["Zhu", "Huilin", ""], ["Chen", "Jiajia", ""]]}, {"id": "1803.01468", "submitter": "EPTCS", "authors": "Ludovic Font (\\'Ecole Polytechnique de Montr\\'eal), Philippe R.\n  Richard (Universit\\'e de Montr\\'eal), Michel Gagnon (\\'Ecole Polytechnique de\n  Montr\\'eal)", "title": "Improving QED-Tutrix by Automating the Generation of Proofs", "comments": "In Proceedings ThEdu'17, arXiv:1803.00722", "journal-ref": "EPTCS 267, 2018, pp. 38-58", "doi": "10.4204/EPTCS.267.3", "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The idea of assisting teachers with technological tools is not new.\nMathematics in general, and geometry in particular, provide interesting\nchallenges when developing educative softwares, both in the education and\ncomputer science aspects. QED-Tutrix is an intelligent tutor for geometry\noffering an interface to help high school students in the resolution of\ndemonstration problems. It focuses on specific goals: 1) to allow the student\nto freely explore the problem and its figure, 2) to accept proofs elements in\nany order, 3) to handle a variety of proofs, which can be customized by the\nteacher, and 4) to be able to help the student at any step of the resolution of\nthe problem, if the need arises. The software is also independent from the\nintervention of the teacher. QED-Tutrix offers an interesting approach to\ngeometry education, but is currently crippled by the lengthiness of the process\nof implementing new problems, a task that must still be done manually.\nTherefore, one of the main focuses of the QED-Tutrix' research team is to ease\nthe implementation of new problems, by automating the tedious step of finding\nall possible proofs for a given problem. This automation must follow\nfundamental constraints in order to create problems compatible with QED-Tutrix:\n1) readability of the proofs, 2) accessibility at a high school level, and 3)\npossibility for the teacher to modify the parameters defining the\n\"acceptability\" of a proof. We present in this paper the result of our\npreliminary exploration of possible avenues for this task. Automated theorem\nproving in geometry is a widely studied subject, and various provers exist.\nHowever, our constraints are quite specific and some adaptation would be\nrequired to use an existing prover. We have therefore implemented a prototype\nof automated prover to suit our needs. The future goal is to compare\nperformances and usability in our specific use-case between the existing\nprovers and our implementation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 02:46:13 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Font", "Ludovic", "", "\u00c9cole Polytechnique de Montr\u00e9al"], ["Richard", "Philippe R.", "", "Universit\u00e9 de Montr\u00e9al"], ["Gagnon", "Michel", "", "\u00c9cole Polytechnique de\n  Montr\u00e9al"]]}, {"id": "1803.01469", "submitter": "EPTCS", "authors": "Mario Frank (University of Potsdam, Institute for Computer Science,\n  Potsdam, Germany), Christoph Kreitz (University of Potsdam, Institute for\n  Computer Science, Potsdam, Germany)", "title": "A Theorem Prover for Scientific and Educational Purposes", "comments": "In Proceedings ThEdu'17, arXiv:1803.00722", "journal-ref": "EPTCS 267, 2018, pp. 59-69", "doi": "10.4204/EPTCS.267.4", "report-no": null, "categories": "cs.HC cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a prototype of an integrated reasoning environment for educational\npurposes. The presented tool is a fragment of a proof assistant and automated\ntheorem prover. We describe the existing and planned functionality of the\ntheorem prover and especially the functionality of the educational fragment.\nThis currently supports working with terms of the untyped lambda calculus and\naddresses both undergraduate students and researchers. We show how the tool can\nbe used to support the students' understanding of functional programming and\ndiscuss general problems related to the process of building theorem proving\nsoftware that aims at supporting both research and education.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 02:46:32 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Frank", "Mario", "", "University of Potsdam, Institute for Computer Science,\n  Potsdam, Germany"], ["Kreitz", "Christoph", "", "University of Potsdam, Institute for\n  Computer Science, Potsdam, Germany"]]}, {"id": "1803.01470", "submitter": "EPTCS", "authors": "Alan Krempler, Walther Neuper", "title": "Prototyping \"Systems that Explain Themselves\" for Education", "comments": "In Proceedings ThEdu'17, arXiv:1803.00722", "journal-ref": "EPTCS 267, 2018, pp. 89-107", "doi": "10.4204/EPTCS.267.6", "report-no": null, "categories": "cs.SE cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Systems that Explain Themselves\" appears a provocative wording, in\nparticular in the context of mathematics education -- it is as provocative as\nthe idea of building educational software upon technology from computer theorem\nproving. In spite of recent success stories like the proofs of the Four Colour\nTheorem or the Kepler Conjecture, mechanised proof is still considered somewhat\nesoteric by mainstream mathematics. This paper describes the process of\nprototyping in the ISAC project from a technical perspective. This perspective\ndepends on two moving targets: On the one side the rapidly increasing power and\ncoverage of computer theorem provers and their user interfaces, and on the\nother side potential users: What can students and teachers request from\neducational systems based on technology and concepts from computer theorem\nproving, now and then? By the way of describing the process of prototyping the\nfirst comprehensive survey on the state of the ISAC prototype is given as a\nside effect, made precise by pointers to the code and by citation of all\ncontributing theses.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 02:47:06 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Krempler", "Alan", ""], ["Neuper", "Walther", ""]]}, {"id": "1803.01477", "submitter": "Phillip Grice", "authors": "Phillip M. Grice and Charles C. Kemp", "title": "In-home and remote use of robotic body surrogates by people with\n  profound motor deficits", "comments": "43 Pages, 13 Figures", "journal-ref": "PLoS ONE 14(3): e0212904 (2019)", "doi": "10.1371/journal.pone.0212904", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By controlling robots comparable to the human body, people with profound\nmotor deficits could potentially perform a variety of physical tasks for\nthemselves, improving their quality of life. The extent to which this is\nachievable has been unclear due to the lack of suitable interfaces by which to\ncontrol robotic body surrogates and a dearth of studies involving substantial\nnumbers of people with profound motor deficits. We developed a novel, web-based\naugmented reality interface that enables people with profound motor deficits to\nremotely control a PR2 mobile manipulator from Willow Garage, which is a\nhuman-scale, wheeled robot with two arms. We then conducted two studies to\ninvestigate the use of robotic body surrogates. In the first study, 15 novice\nusers with profound motor deficits from across the United States controlled a\nPR2 in Atlanta, GA to perform a modified Action Research Arm Test (ARAT) and a\nsimulated self-care task. Participants achieved clinically meaningful\nimprovements on the ARAT and 12 of 15 participants (80%) successfully completed\nthe simulated self-care task. Participants agreed that the robotic system was\neasy to use, was useful, and would provide a meaningful improvement in their\nlives. In the second study, one expert user with profound motor deficits had\nfree use of a PR2 in his home for seven days. He performed a variety of\nself-care and household tasks, and also used the robot in novel ways. Taking\nboth studies together, our results suggest that people with profound motor\ndeficits can improve their quality of life using robotic body surrogates, and\nthat they can gain benefit with only low-level robot autonomy and without\ninvasive interfaces. However, methods to reduce the rate of errors and increase\noperational speed merit further investigation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 03:02:27 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 01:22:33 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Grice", "Phillip M.", ""], ["Kemp", "Charles C.", ""]]}, {"id": "1803.01537", "submitter": "Zhenyu Gu", "authors": "Zhenyu Gu, Chenhao Jin, Zhanxun Dong, Danni Chang", "title": "Predicting Webpage Aesthetics with Heatmap Entropy", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, eye trackers are extensively used in user interface evaluations.\nHowever, it's still hard to analyze and interpret eye tracking data from the\naesthetic point of view. To find quantitative links between eye movements and\naesthetic experience, we tracked 30 observers' initial landings for 40 web\npages (each displayed for 3 seconds). The web pages were also rated based on\nthe observers' subjective aesthetic judgments. Shannon entropy was introduced\nto analyze the eye-tracking data. The result shows that the heatmap entropy\n(visual attention entropy, VAE) is highly correlated with the observers'\naesthetic judgements of the web pages. Its improved version, relative VAE\n(rVAE), has a more significant correlation with the perceived aesthetics.\n(r=-0.65, F= 26.84, P$<$0.0001). This single metric alone can distinguish\nbetween good- and bad-looking pages with an approximate 85\\% accuracy. Further\ninvestigation reveals that the performance of both VAE and rVAE became stable\nafter 1 second. The curves indicate that their performances could be better, if\nthe tracking time was extended beyond 3 seconds.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 07:47:17 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Gu", "Zhenyu", ""], ["Jin", "Chenhao", ""], ["Dong", "Zhanxun", ""], ["Chang", "Danni", ""]]}, {"id": "1803.01645", "submitter": "Pedro Campos", "authors": "Pedro F. Campos", "title": "You Are Okay: Towards User Interfaces for Improving Well-being", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well-being is a relatively broad concept which can be succinctly described as\nthe state of being happy, healthy or successful. Interesting things happen when\nbridging user interface design with the psychology of human well-being. This\nposition paper aims at providing a short on reflection the challenges and\nopportunities in this context and presents concrete examples on how to tackle\nthese challenges and exploit the existing design opportunities.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 13:01:42 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Campos", "Pedro F.", ""]]}, {"id": "1803.01660", "submitter": "Jonny O'Dwyer", "authors": "Jonny O'Dwyer, Ronan Flynn, Niall Murray", "title": "Continuous Affect Prediction using Eye Gaze", "comments": "Accepted paper for 28th Irish Signals and Systems Conference (ISSC),\n  2017", "journal-ref": "J. O Dwyer, R. Flynn, and N. Murray, Continuous affect prediction\n  using eye gaze, in 2017 28th Irish Signals and Systems Conference (ISSC),\n  2017, pp. 1-6", "doi": "10.1109/ISSC.2017.7983611", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent times, there has been significant interest in the machine\nrecognition of human emotions, due to the suite of applications to which this\nknowledge can be applied. A number of different modalities, such as speech or\nfacial expression, individually and with eye gaze, have been investigated by\nthe affective computing research community to either classify the emotion (e.g.\nsad, happy, angry) or predict the continuous values of affective dimensions\n(e.g. valence, arousal, dominance) at each moment in time. Surprisingly after\nan extensive literature review, eye gaze as a unimodal input to a continuous\naffect prediction system has not been considered. In this context, this paper\nevaluates the use of eye gaze as a unimodal input to a continuous affect\nprediction system. The performance of continuous prediction of arousal and\nvalence using eye gaze is compared with the performance of a speech system\nusing the AVEC 2014 speech feature set. The experimental evaluation when using\neye gaze as the single modality in a continuous affect prediction system\nproduced a correlation result for valence prediction that is better than the\ncorrelation result obtained with the AVEC 2014 speech feature set. Furthermore,\nthe eye gaze feature set proposed in this paper contains 98% fewer features\ncompared to the number of features in the AVEC 2014 feature set.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 13:49:43 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["O'Dwyer", "Jonny", ""], ["Flynn", "Ronan", ""], ["Murray", "Niall", ""]]}, {"id": "1803.01662", "submitter": "Jonny O'Dwyer", "authors": "Jonny O'Dwyer, Ronan Flynn, Niall Murray", "title": "Continuous Affect Prediction Using Eye Gaze and Speech", "comments": "Accepted paper for the 2017 IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM)", "journal-ref": "J. O Dwyer, R. Flynn, and N. Murray, Continuous affect prediction\n  using eye gaze and speech, in 2017 IEEE International Conference on\n  Bioinformatics and Biomedicine (BIBM), 2017, pp. 2001-2007", "doi": "10.1109/BIBM.2017.8217968", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective computing research traditionally focused on labeling a person's\nemotion as one of a discrete number of classes e.g. happy or sad. In recent\ntimes, more attention has been given to continuous affect prediction across\ndimensions in the emotional space, e.g. arousal and valence. Continuous affect\nprediction is the task of predicting a numerical value for different emotion\ndimensions. The application of continuous affect prediction is powerful in\ndomains involving real-time audio-visual communications which could include\nremote or assistive technologies for psychological assessment of subjects.\nModalities used for continuous affect prediction may include speech, facial\nexpressions and physiological responses. As opposed to single modality\nanalysis, the research community have combined multiple modalities to improve\nthe accuracy of continuous affect prediction. In this context, this paper\ninvestigates a continuous affect prediction system using the novel combination\nof speech and eye gaze. A new eye gaze feature set is proposed. This novel\napproach uses open source software for real-time affect prediction in\naudio-visual communication environments. A unique advantage of the\nhuman-computer interface used here is that it does not require the subject to\nwear specialized and expensive eye-tracking headsets or intrusive devices. The\nresults indicate that the combination of speech and eye gaze improves arousal\nprediction by 3.5% and valence prediction by 19.5% compared to using speech\nalone.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 13:57:05 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["O'Dwyer", "Jonny", ""], ["Flynn", "Ronan", ""], ["Murray", "Niall", ""]]}, {"id": "1803.01842", "submitter": "Ahmed Fadhil", "authors": "Ahmed Fadhil", "title": "Towards Automatic & Personalised Mobile Health Interventions: An\n  Interactive Machine Learning Perspective", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Machine learning (ML) is the fastest growing field in computer science and\nhealthcare, providing future benefits in improved medical diagnoses, disease\nanalyses and prevention. In this paper, we introduce an application of\ninteractive machine learning (iML) in a telemedicine system, to enable\nautomatic and personalised interventions for lifestyle promotion. We first\npresent the high level architecture of the system and the components forming\nthe overall architecture. We then illustrate the interactive machine learning\nprocess design. Prediction models are expected to be trained through the\nparticipants' profiles, activity performance, and feedback from the caregiver.\nFinally, we show some preliminary results during the system implementation and\ndiscuss future directions. We envisage the proposed system to be digitally\nimplemented, and behaviourally designed to promote healthy lifestyle and\nactivities, and hence prevent users from the risk of chronic diseases.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 10:30:56 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Fadhil", "Ahmed", ""]]}, {"id": "1803.02015", "submitter": "Boris Ivanovic", "authors": "Boris Ivanovic, Edward Schmerling, Karen Leung, Marco Pavone", "title": "Generative Modeling of Multimodal Multi-Human Behavior", "comments": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS) 2018 -- 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a methodology for modeling and predicting human behavior\nin settings with N humans interacting in highly multimodal scenarios (i.e.\nwhere there are many possible highly-distinct futures). A motivating example\nincludes robots interacting with humans in crowded environments, such as\nself-driving cars operating alongside human-driven vehicles or human-robot\ncollaborative bin packing in a warehouse. Our approach to model human behavior\nin such uncertain environments is to model humans in the scene as nodes in a\ngraphical model, with edges encoding relationships between them. For each\nhuman, we learn a multimodal probability distribution over future actions from\na dataset of multi-human interactions. Learning such distributions is made\npossible by recent advances in the theory of conditional variational\nautoencoders and deep learning approximations of probabilistic graphical\nmodels. Specifically, we learn action distributions conditioned on interaction\nhistory, neighboring human behavior, and candidate future agent behavior in\norder to take into account response dynamics. We demonstrate the performance of\nsuch a modeling approach in modeling basketball player trajectories, a highly\nmultimodal, multi-human scenario which serves as a proxy for many robotic\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 04:49:58 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 06:01:26 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Ivanovic", "Boris", ""], ["Schmerling", "Edward", ""], ["Leung", "Karen", ""], ["Pavone", "Marco", ""]]}, {"id": "1803.02088", "submitter": "David Robb", "authors": "Francisco J. Chiyah Garcia, David A. Robb, Xingkun Liu, Atanas Laskov,\n  Pedro Patron and Helen Hastie", "title": "Explain Yourself: A Natural Language Interface for Scrutable Autonomous\n  Robots", "comments": "2 pages. Peer reviewed position paper accepted in the Explainable\n  Robotic Systems Workshop, ACM Human-Robot Interaction conference, March 2018,\n  Chicago, IL USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems in remote locations have a high degree of autonomy and\nthere is a need to explain what they are doing and why in order to increase\ntransparency and maintain trust. Here, we describe a natural language chat\ninterface that enables vehicle behaviour to be queried by the user. We obtain\nan interpretable model of autonomy through having an expert 'speak out-loud'\nand provide explanations during a mission. This approach is agnostic to the\ntype of autonomy model and as expert and operator are from the same user-group,\nwe predict that these explanations will align well with the operator's mental\nmodel, increase transparency and assist with operator training.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 10:13:29 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Garcia", "Francisco J. Chiyah", ""], ["Robb", "David A.", ""], ["Liu", "Xingkun", ""], ["Laskov", "Atanas", ""], ["Patron", "Pedro", ""], ["Hastie", "Helen", ""]]}, {"id": "1803.02097", "submitter": "Maarten Bieshaar", "authors": "Maarten Bieshaar", "title": "Where is my Device? - Detecting the Smart Device's Wearing Location in\n  the Context of Active Safety for Vulnerable Road Users", "comments": "10 pages, 3 figures, accepted for publication in Organic Computing:\n  Doctoral Dissertation Colloquium 2017. Volume 11 of Intelligent Embedded\n  Systems. Kassel University Press, Kassel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes an approach to detect the wearing location of smart\ndevices worn by pedestrians and cyclists. The detection, which is based solely\non the sensors of the smart devices, is important context-information which can\nbe used to parametrize subsequent algorithms, e.g. for dead reckoning or\nintention detection to improve the safety of vulnerable road users. The wearing\nlocation recognition can in terms of Organic Computing (OC) be seen as a step\ntowards self-awareness and self-adaptation. For the wearing location detection\na two-stage process is presented. It is subdivided into moving detection\nfollowed by the wearing location classification. Finally, the approach is\nevaluated on a real world dataset consisting of pedestrians and cyclists.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 10:34:47 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Bieshaar", "Maarten", ""]]}, {"id": "1803.02100", "submitter": "David Robb", "authors": "Helen Hastie, Katrin Lohan, Mike Chantler, David A. Robb, Subramanian\n  Ramamoorthy, Ron Petrick, Sethu Vijayakumar and David Lane", "title": "The ORCA Hub: Explainable Offshore Robotics through Intelligent\n  Interfaces", "comments": "2 pages. Peer reviewed position paper accepted in the Explainable\n  Robotic Systems Workshop, ACM Human-Robot Interaction conference, March 2018,\n  Chicago, IL USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the UK Robotics and Artificial Intelligence Hub for Offshore\nRobotics for Certification of Assets (ORCA Hub), a 3.5 year EPSRC funded,\nmulti-site project. The ORCA Hub vision is to use teams of robots and\nautonomous intelligent systems (AIS) to work on offshore energy platforms to\nenable cheaper, safer and more efficient working practices. The ORCA Hub will\nresearch, integrate, validate and deploy remote AIS solutions that can operate\nwith existing and future offshore energy assets and sensors, interacting safely\nin autonomous or semi-autonomous modes in complex and cluttered environments,\nco-operating with remote operators. The goal is that through the use of such\nrobotic systems offshore, the need for personnel will decrease. To enable this\nto happen, the remote operator will need a high level of situation awareness\nand key to this is the transparency of what the autonomous systems are doing\nand why. This increased transparency will facilitate a trusting relationship,\nwhich is particularly key in high-stakes, hazardous situations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 10:43:38 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Hastie", "Helen", ""], ["Lohan", "Katrin", ""], ["Chantler", "Mike", ""], ["Robb", "David A.", ""], ["Ramamoorthy", "Subramanian", ""], ["Petrick", "Ron", ""], ["Vijayakumar", "Sethu", ""], ["Lane", "David", ""]]}, {"id": "1803.02122", "submitter": "Jose Berengueres Ph.D", "authors": "Lojain Jibawi, Saoussen Said, Kenjiro Tadakuma and Jose Berengueres", "title": "Smartphone-based Home Robotics", "comments": "6 pages, 3 figures, IEEE IROS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanoid robotics is a promising field because the strong human preference to\ninteract with anthropomorphic interfaces. Despite this, humanoid robots are far\nfrom reaching main stream adoption and the features available in such robots\nseem to lag that of the latest smartphones. A fragmented robot ecosystem and\nlow incentives to developers do not help to foster the creation of Robot-Apps\neither. In contrast, smartphones enjoy high adoption rates and a vibrant app\necosystem (4M apps published). Given this, it seems logical to apply the mobile\nSW and HW development model to humanoid robots. One way is to use a smartphone\nto power the robot. Smartphones have been embedded in toys and drones before.\nHowever, they have never been used as the main compute unit in a humanoid\nembodiment. Here, we introduce a novel robot architecture based on smartphones\nthat demonstrates x3 cost reduction and that is compatible with iOS/Android.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 11:29:21 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Jibawi", "Lojain", ""], ["Said", "Saoussen", ""], ["Tadakuma", "Kenjiro", ""], ["Berengueres", "Jose", ""]]}, {"id": "1803.02124", "submitter": "David Robb", "authors": "Helen Hastie, Francisco J. Chiyah Garcia, David A. Robb, Pedro Patron\n  and Atanas Laskov", "title": "MIRIAM: A Multimodal Chat-Based Interface for Autonomous Systems", "comments": "2 pages, ICMI'17, 19th ACM International Conference on Multimodal\n  Interaction, November 13-17 2017, Glasgow, UK", "journal-ref": null, "doi": "10.1145/3136755.3143022", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MIRIAM (Multimodal Intelligent inteRactIon for Autonomous\nsysteMs), a multimodal interface to support situation awareness of autonomous\nvehicles through chat-based interaction. The user is able to chat about the\nvehicle's plan, objectives, previous activities and mission progress. The\nsystem is mixed initiative in that it pro-actively sends messages about key\nevents, such as fault warnings. We will demonstrate MIRIAM using SeeByte's\nSeeTrack command and control interface and Neptune autonomy simulator.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 11:33:04 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Hastie", "Helen", ""], ["Garcia", "Francisco J. Chiyah", ""], ["Robb", "David A.", ""], ["Patron", "Pedro", ""], ["Laskov", "Atanas", ""]]}, {"id": "1803.02179", "submitter": "Dominik Kowald", "authors": "Dominik Kowald, Paul Seitlinger, Tobias Ley, Elisabeth Lex", "title": "The Impact of Semantic Context Cues on the User Acceptance of Tag\n  Recommendations: An Online Study", "comments": "2 pages, poster", "journal-ref": "WWW'2018", "doi": "10.1145/3184558.3186899", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present the results of an online study with the aim to shed\nlight on the impact that semantic context cues have on the user acceptance of\ntag recommendations. Therefore, we conducted a work-integrated social\nbookmarking scenario with 17 university employees in order to compare the user\nacceptance of a context-aware tag recommendation algorithm called 3Layers with\nthe user acceptance of a simple popularity-based baseline. In this scenario, we\nvalidated and verified the hypothesis that semantic context cues have a higher\nimpact on the user acceptance of tag recommendations in a collaborative tagging\nsetting than in an individual tagging setting. With this paper, we contribute\nto the sparse line of research presenting online recommendation studies.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 14:00:37 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Kowald", "Dominik", ""], ["Seitlinger", "Paul", ""], ["Ley", "Tobias", ""], ["Lex", "Elisabeth", ""]]}, {"id": "1803.02307", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Andrea Bianchi, Nicolai Marquardt and Nadia\n  Bianchi-Berthouze", "title": "RealPen: Providing Realism in Handwriting Tasks on Touch Surfaces using\n  Auditory-Tactile Feedback", "comments": "Proceedings of the 29th Annual Symposium on User Interface Software\n  and Technology (UIST '16)", "journal-ref": null, "doi": "10.1145/2984511.2984550", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RealPen, an augmented stylus for capacitive tablet screens that\nrecreates the physical sensation of writing on paper with a pencil, ball-point\npen or marker pen. The aim is to create a more engaging experience when writing\non touch surfaces, such as screens of tablet computers. This is achieved by\nre-generating the friction-induced oscillation and sound of a real writing tool\nin contact with paper. To generate realistic tactile feedback, our algorithm\nanalyses the frequency spectrum of the friction oscillation generated when\nwriting with traditional tools, extracts principal frequencies, and uses the\nactuator's frequency response profile for an adjustment weighting function. We\nenhance the realism by providing the sound feedback aligned with the writing\npressure and speed. Furthermore, we investigated the effects of superposition\nand fluctuation of several frequencies on human tactile perception, evaluated\nthe performance of RealPen, and characterized users' perception and preference\nof each feedback type.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 17:17:19 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Cho", "Youngjun", ""], ["Bianchi", "Andrea", ""], ["Marquardt", "Nicolai", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "1803.02310", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Nadia Bianchi-Berthouze, Nicolai Marquardt and Simon J.\n  Julier", "title": "Deep Thermal Imaging: Proximate Material Type Recognition in the Wild\n  through Deep Learning of Spatial Surface Temperature Patterns", "comments": "Proceedings of the 2018 CHI Conference on Human Factors in Computing\n  Systems", "journal-ref": null, "doi": "10.1145/3173574.3173576", "report-no": null, "categories": "cs.CV cond-mat.mtrl-sci cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep Thermal Imaging, a new approach for close-range automatic\nrecognition of materials to enhance the understanding of people and ubiquitous\ntechnologies of their proximal environment. Our approach uses a low-cost mobile\nthermal camera integrated into a smartphone to capture thermal textures. A deep\nneural network classifies these textures into material types. This approach\nworks effectively without the need for ambient light sources or direct contact\nwith materials. Furthermore, the use of a deep learning network removes the\nneed to handcraft the set of features for different materials. We evaluated the\nperformance of the system by training it to recognise 32 material types in both\nindoor and outdoor environments. Our approach produced recognition accuracies\nabove 98% in 14,860 images of 15 indoor materials and above 89% in 26,584\nimages of 17 outdoor materials. We conclude by discussing its potentials for\nreal-time use in HCI applications and future directions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 17:29:08 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cho", "Youngjun", ""], ["Bianchi-Berthouze", "Nadia", ""], ["Marquardt", "Nicolai", ""], ["Julier", "Simon J.", ""]]}, {"id": "1803.02499", "submitter": "Tianlong Zu", "authors": "Tianlong Zu, John Hutson, Lester C. Loschky, and N. Sanjay Rebello", "title": "Use of Eye-Tracking Technology to Investigate Cognitive Load Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ed-ph cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive load theory (CLT) provides us guiding principles in the design of\nlearning materials. CLT differentiates three different kinds of cognitive load\n-- intrinsic, extraneous and germane load. Intrinsic load is related to the\nlearning goal, extraneous load costs cognitive resources but does not\ncontribute to learning. Germane load can foster learning. Objective methods,\nsuch as eye movement measures and EEG have been used measure the total\ncognitive load. Very few research studies, if any, have been completed to\nmeasure the three kinds of load separately with physiological methods in a\ncontinuous manner. In this current study, we will show how several eye-tracking\nbased parameters are related to the three kinds of load by having explicit\nmanipulation of the three loads independently. Participants having low prior\nknowledge regarding the learning material participated in the study. Working\nmemory capacity was also measured by an operation memory span task.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 02:03:34 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Zu", "Tianlong", ""], ["Hutson", "John", ""], ["Loschky", "Lester C.", ""], ["Rebello", "N. Sanjay", ""]]}, {"id": "1803.02543", "submitter": "Mingliang Xu", "authors": "Mingliang Xu, Yibo Guo, Bailin Yang, Wei Chen, Pei Lv, Liwei Fan, Bin\n  Zhou", "title": "Improving Aviation Safety using Synthetic Vision System integrated with\n  Eye-tracking Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By collecting the data of eyeball movement of pilots, it is possible to\nmonitor pilot's operation in the future flight in order to detect potential\naccidents. In this paper, we designed a novel SVS system that is integrated\nwith an eye tracking device, and is able to achieve the following functions:1)\nA novel method that is able to learn from the eyeball movements of pilots and\npreload or render the terrain data in various resolutions, in order to improve\nthe quality of terrain display by comprehending the interested regions of the\npilot. 2) A warning mechanism that may detect the risky operation via analyzing\nthe aviation information from the SVS and the eyeball movement from the eye\ntracking device, in order to prevent the maloperations or human factor\naccidents. The user study and experiments show that the proposed\nSVS-Eyetracking system works efficiently and is capable of avoiding potential\nrisked caused by fatigue in the flight simulation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 06:58:11 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Xu", "Mingliang", ""], ["Guo", "Yibo", ""], ["Yang", "Bailin", ""], ["Chen", "Wei", ""], ["Lv", "Pei", ""], ["Fan", "Liwei", ""], ["Zhou", "Bin", ""]]}, {"id": "1803.02887", "submitter": "Shayan Eskandari", "authors": "Shayan Eskandari, Andreas Leoutsarakos, Troy Mursch, Jeremy Clark", "title": "A first look at browser-based Cryptojacking", "comments": "9 pages, IEEE SECURITY & PRIVACY ON THE BLOCKCHAIN (IEEE S&B) 2018\n  University College London (UCL), London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the recent trend towards in-browser mining of\ncryptocurrencies; in particular, the mining of Monero through Coinhive and\nsimilar code- bases. In this model, a user visiting a website will download a\nJavaScript code that executes client-side in her browser, mines a\ncryptocurrency, typically without her consent or knowledge, and pays out the\nseigniorage to the website. Websites may consciously employ this as an\nalternative or to supplement advertisement revenue, may offer premium content\nin exchange for mining, or may be unwittingly serving the code as a result of a\nbreach (in which case the seigniorage is collected by the attacker). The\ncryptocurrency Monero is preferred seemingly for its unfriendliness to\nlarge-scale ASIC mining that would drive browser-based efforts out of the\nmarket, as well as for its purported privacy features. In this paper, we survey\nthis landscape, conduct some measurements to establish its prevalence and\nprofitability, outline an ethical framework for considering whether it should\nbe classified as an attack or business opportunity, and make suggestions for\nthe detection, mitigation and/or prevention of browser-based mining for non-\nconsenting users.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 21:50:37 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Eskandari", "Shayan", ""], ["Leoutsarakos", "Andreas", ""], ["Mursch", "Troy", ""], ["Clark", "Jeremy", ""]]}, {"id": "1803.02952", "submitter": "Tianran Hu", "authors": "Tianran Hu, Anbang Xu, Zhe Liu, Quanzeng You, Yufan Guo, Vibha Sinha,\n  Jiebo Luo, Rama Akkiraju", "title": "Touch Your Heart: A Tone-aware Chatbot for Customer Care on Social Media", "comments": "12 pages, CHI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Chatbot has become an important solution to rapidly increasing customer care\ndemands on social media in recent years. However, current work on chatbot for\ncustomer care ignores a key to impact user experience - tones. In this work, we\ncreate a novel tone-aware chatbot that generates toned responses to user\nrequests on social media. We first conduct a formative research, in which the\neffects of tones are studied. Significant and various influences of different\ntones on user experience are uncovered in the study. With the knowledge of\neffects of tones, we design a deep learning based chatbot that takes tone\ninformation into account. We train our system on over 1.5 million real customer\ncare conversations collected from Twitter. The evaluation reveals that our\ntone-aware chatbot generates as appropriate responses to user requests as human\nagents. More importantly, our chatbot is perceived to be even more empathetic\nthan human agents.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 03:18:41 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 01:00:25 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Hu", "Tianran", ""], ["Xu", "Anbang", ""], ["Liu", "Zhe", ""], ["You", "Quanzeng", ""], ["Guo", "Yufan", ""], ["Sinha", "Vibha", ""], ["Luo", "Jiebo", ""], ["Akkiraju", "Rama", ""]]}, {"id": "1803.03092", "submitter": "EPTCS", "authors": "Graham Leach-Krouse (Kansas State University)", "title": "Carnap: An Open Framework for Formal Reasoning in the Browser", "comments": "In Proceedings ThEdu'17, arXiv:1803.00722", "journal-ref": "EPTCS 267, 2018, pp. 70-88", "doi": "10.4204/EPTCS.267.5", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an overview of Carnap, a free and open framework for the\ndevelopment of formal reasoning applications. Carnap's design emphasizes\nflexibility, extensibility, and rapid prototyping. Carnap-based applications\nare written in Haskell, but can be compiled to JavaScript to run in standard\nweb browsers. This combination of features makes Carnap ideally suited for\neducational applications, where ease-of-use is crucial for students and\nadaptability to different teaching strategies and classroom needs is crucial\nfor instructors. The paper describes Carnap's implementation, along with its\ncurrent and projected pedagogical applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 02:46:48 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Leach-Krouse", "Graham", "", "Kansas State University"]]}, {"id": "1803.03093", "submitter": "Hussein Abbass A", "authors": "Aya Hussein and Leo Ghignone and Tung Nguyen and Nima Salimi and Hung\n  Nguyen and Min Wang and Hussein A. Abbass", "title": "Towards Bi-Directional Communication in Human-Swarm Teaming: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Swarm systems consist of large numbers of robots that collaborate\nautonomously. With an appropriate level of human control, swarm systems could\nbe applied in a variety of contexts ranging from search-and-rescue situations\nto Cyber defence. The two decision making cycles of swarms and humans operate\non two different time-scales, where the former is normally orders of magnitude\nfaster than the latter. Closing the loop at the intersection of these two\ncycles will create fast and adaptive human-swarm teaming networks. This paper\nbrings desperate pieces of the ground work in this research area together to\nreview this multidisciplinary literature. We conclude with a framework to\nsynthesize the findings and summarize the multi-modal indicators needed for\nclosed-loop human-swarm adaptive systems.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 07:04:44 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Hussein", "Aya", ""], ["Ghignone", "Leo", ""], ["Nguyen", "Tung", ""], ["Salimi", "Nima", ""], ["Nguyen", "Hung", ""], ["Wang", "Min", ""], ["Abbass", "Hussein A.", ""]]}, {"id": "1803.03428", "submitter": "Joy Bose", "authors": "Anish Anil Patankar, Joy Bose, Harshit Khanna", "title": "A Bias Aware News Recommendation System", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of fake news and political polarization, it is desirable to have\na system to enable users to access balanced news content. Current solutions\nfocus on top down, server based approaches to decide whether a news article is\nfake or biased, and display only trusted news to the end users. In this paper,\nwe follow a different approach to help the users make informed choices about\nwhich news they want to read, making users aware in real time of the bias in\nnews articles they were browsing and recommending news articles from other\nsources on the same topic with different levels of bias. We use a recent Pew\nresearch report to collect news sources that readers with varying political\ninclinations prefer to read. We then scrape news articles on a variety of\ntopics from these varied news sources. After this, we perform clustering to\nfind similar topics of the articles, as well as calculate a bias score for each\narticle. For a news article the user is currently reading, we display the bias\nscore and also display other articles on the same topic, out of the previously\ncollected articles, from different news sources. This we present to the user.\nThis approach, we hope, would make it possible for users to access more\nbalanced articles on given news topics. We present the implementation details\nof the system along with some preliminary results on news articles.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 09:19:16 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Patankar", "Anish Anil", ""], ["Bose", "Joy", ""], ["Khanna", "Harshit", ""]]}, {"id": "1803.03430", "submitter": "Joy Bose", "authors": "Ramanujam R Srinivasa, Joy Bose, Dipin KP", "title": "VR Content Capture using Aligned Smartphones", "comments": "6 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a number of dedicated 3D capture devices in the market, but\ngenerally they are unaffordable and do not make use of existing smartphone\ncameras, which are generally of decent quality. Due to this, while there are\nseveral means to consume 3D or VR content, there is currently lack of means to\ncapture 3D content, resulting in very few 3D videos being publicly available.\nSome mobile applications such as Camerada enable 3D or VR content capture by\ncombining the output of two existing smartphones, but users would have to hold\nthe cameras in their hand, making it difficult to align properly. In this paper\nwe present the design of a system to enable 3D content capture using one or\nmore smartphones, taking care of alignment issues so as to get optimal\nalignment of the smartphone cameras. We aim to keep the distance between the\ncameras constant and equal to the inter-pupillary distance of about 6.5 cm. Our\nsolution is applicable for one, two and three smartphones. We have a mobile app\nto generate a template given the dimensions of the smartphones, camera\npositions and other specifications. The template can be printed by the user and\ncut out on 2D cardboard, similar to Google cardboard. Alternatively, it can be\nprinted using a 3D printer. During video capture, with the smartphones aligned\nusing our printed template, we capture videos which are then combined to get\nthe optimal 3D content. We present the details of a small proof of concept\nimplementation. Our solution would make it easier for people to use existing\nsmartphones to generate 3D content.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 09:24:03 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Srinivasa", "Ramanujam R", ""], ["Bose", "Joy", ""], ["KP", "Dipin", ""]]}, {"id": "1803.03697", "submitter": "Srijan Kumar", "authors": "Srijan Kumar, William L. Hamilton, Jure Leskovec, Dan Jurafsky", "title": "Community Interaction and Conflict on the Web", "comments": "In WWW 2018: The Web Conference. Project website with data and code\n  is https://snap.stanford.edu/conflict/", "journal-ref": null, "doi": "10.1145/3178876.3186141", "report-no": null, "categories": "cs.SI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users organize themselves into communities on web platforms. These\ncommunities can interact with one another, often leading to conflicts and toxic\ninteractions. However, little is known about the mechanisms of interactions\nbetween communities and how they impact users.\n  Here we study intercommunity interactions across 36,000 communities on\nReddit, examining cases where users of one community are mobilized by negative\nsentiment to comment in another community. We show that such conflicts tend to\nbe initiated by a handful of communities---less than 1% of communities start\n74% of conflicts. While conflicts tend to be initiated by highly active\ncommunity members, they are carried out by significantly less active members.\nWe find that conflicts are marked by formation of echo chambers, where users\nprimarily talk to other users from their own community. In the long-term,\nconflicts have adverse effects and reduce the overall activity of users in the\ntargeted communities.\n  Our analysis of user interactions also suggests strategies for mitigating the\nnegative impact of conflicts---such as increasing direct engagement between\nattackers and defenders. Further, we accurately predict whether a conflict will\noccur by creating a novel LSTM model that combines graph embeddings, user,\ncommunity, and text features. This model can be used toreate early-warning\nsystems for community moderators to prevent conflicts. Altogether, this work\npresents a data-driven view of community interactions and conflict, and paves\nthe way towards healthier online communities.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 21:26:13 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Kumar", "Srijan", ""], ["Hamilton", "William L.", ""], ["Leskovec", "Jure", ""], ["Jurafsky", "Dan", ""]]}, {"id": "1803.04074", "submitter": "Susana Vidrio-Bar\\'on", "authors": "Susana B. Vidrio Bar\\'on, Andrew W. Luse, Anthony M. Townsend", "title": "Development of a culturally-oriented website usability evaluation", "comments": "15th Americas Conference on Information Systems 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the uni-cultural studies of website usability have matured, the paucity of\ncross-cultural studies of usability become increasingly apparent. Moving toward\nthese cross-cultural studies will require the development of a new tool to\nassess website usability in the context of cultural dimensions. This paper\nintroduces the preliminary results from the first phase of this project and\nthen presents the proposed method for the research in progress that\nspecifically is directed to the development and quantitative evaluation of a\nmeasurement scale of a culture sensitive measurement of website usability. The\nrecognition of the need to develop this scale resulted from the identification\nof culture-related shortcomings of previous measurement tools that have been\nused widely within the Management of Information Systems (MIS) literature.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 00:39:08 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Bar\u00f3n", "Susana B. Vidrio", ""], ["Luse", "Andrew W.", ""], ["Townsend", "Anthony M.", ""]]}, {"id": "1803.04099", "submitter": "Xiang Zhang", "authors": "Xiang Zhang", "title": "Context-aware Human Intent Inference for Improving Human Machine\n  Cooperation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of human beings to precisely recog- nize others intents is a\nsignificant mental activity in reasoning about actions, such as, what other\npeople are doing and what they will do next. Recent research has revealed that\nhuman intents could be inferred by measuring human cognitive activities through\nheterogeneous body and brain sensors (e.g., sensors for detecting physiological\nsignals like ECG, brain signals like EEG and IMU sensors like accelerometers\nand gyros etc.). In this proposal, we aim at developing a computa- tional\nframework for enabling reliable and precise real-time human intent recognition\nby measuring human cognitive and physiological activities through the\nheterogeneous body and brain sensors for improving human machine interactions,\nand serving intent-based human activity prediction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 03:12:42 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zhang", "Xiang", ""]]}, {"id": "1803.04548", "submitter": "Sylvie Delacroix", "authors": "Sylvie Delacroix", "title": "Taking Turing by Surprise? Designing Digital Computers for\n  morally-loaded contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much to learn from what Turing hastily dismissed as Lady Lovelace s\nobjection. Digital computers can indeed surprise us. Just like a piece of art,\nalgorithms can be designed in such a way as to lead us to question our\nunderstanding of the world, or our place within it. Some humans do lose the\ncapacity to be surprised in that way. It might be fear, or it might be the\ncomfort of ideological certainties. As lazy normative animals, we do need to be\nable to rely on authorities to simplify our reasoning: that is ok. Yet the\ngrowing sophistication of systems designed to free us from the constraints of\nnormative engagement may take us past a point of no-return. What if, through\nlack of normative exercise, our moral muscles became so atrophied as to leave\nus unable to question our social practices? This paper makes two distinct\nnormative claims:\n  1. Decision-support systems should be designed with a view to regularly\njolting us out of our moral torpor.\n  2. Without the depth of habit to somatically anchor model certainty, a\ncomputer s experience of something new is very different from that which in\nhumans gives rise to non-trivial surprises. This asymmetry has key\nrepercussions when it comes to the shape of ethical agency in artificial moral\nagents. The worry is not just that they would be likely to leap morally ahead\nof us, unencumbered by habits. The main reason to doubt that the moral\ntrajectories of humans v. autonomous systems might remain compatible stems from\nthe asymmetry in the mechanisms underlying moral change. Whereas in humans\nsurprises will continue to play an important role in waking us to the need for\nmoral change, cognitive processes will rule when it comes to machines. This\nasymmetry will translate into increasingly different moral outlooks, to the\npoint of likely unintelligibility. The latter prospect is enough to doubt the\ndesirability of autonomous moral agents.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 21:51:48 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Delacroix", "Sylvie", ""]]}, {"id": "1803.04713", "submitter": "Vijay Rajanna", "authors": "Vijay Rajanna, Tracy Hammond", "title": "A Gaze-Assisted Multimodal Approach to Rich and Accessible\n  Human-Computer Interaction", "comments": "4 pages, 5 figures, ACM Richard Tapia Conference, Atlanta, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in eye tracking technology are driving the adoption of\ngaze-assisted interaction as a rich and accessible human-computer interaction\nparadigm. Gaze-assisted interaction serves as a contextual, non-invasive, and\nexplicit control method for users without disabilities; for users with motor or\nspeech impairments, text entry by gaze serves as the primary means of\ncommunication. Despite significant advantages, gaze-assisted interaction is\nstill not widely accepted because of its inherent limitations: 1) Midas touch,\n2) low accuracy for mouse-like interactions, 3) need for repeated calibration,\n4) visual fatigue with prolonged usage, 5) lower gaze typing speed, and so on.\n  This dissertation research proposes a gaze-assisted, multimodal, interaction\nparadigm, and related frameworks and their applications that effectively enable\ngaze-assisted interactions while addressing many of the current limitations. In\nthis regard, we present four systems that leverage gaze-assisted interaction:\n1) a gaze- and foot-operated system for precise point-and-click interactions,\n2) a dwell-free, foot-operated gaze typing system. 3) a gaze gesture-based\nauthentication system, and 4) a gaze gesture-based interaction toolkit. In\naddition, we also present the goals to be achieved, technical approach, and\noverall contributions of this dissertation research.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 10:27:39 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Rajanna", "Vijay", ""], ["Hammond", "Tracy", ""]]}, {"id": "1803.04818", "submitter": "Michael Barz", "authors": "Jan Zacharias and Michael Barz and Daniel Sonntag", "title": "A Survey on Deep Learning Toolkits and Libraries for Intelligent User\n  Interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper provides an overview of prominent deep learning toolkits and, in\nparticular, reports on recent publications that contributed open source\nsoftware for implementing tasks that are common in intelligent user interfaces\n(IUI). We provide a scientific reference for researchers and software engineers\nwho plan to utilise deep learning techniques within their IUI research and\ndevelopment projects.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:07:40 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 08:32:02 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Zacharias", "Jan", ""], ["Barz", "Michael", ""], ["Sonntag", "Daniel", ""]]}, {"id": "1803.04968", "submitter": "Daniele Sportillo", "authors": "Daniele Sportillo, Alexis Paljic, Luciano Ojeda, Philippe Fuchs,\n  Vincent Roussarie", "title": "Light Virtual Reality systems for the training of conditionally\n  automated vehicle drivers", "comments": "Extended abstract for poster presentation at IEEE VR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conditionally automated vehicles, drivers can engage in secondary\nactivities while traveling to their destination. However, drivers are required\nto appropriately respond, in a limited amount of time, to a take-over request\nwhen the system reaches its functional boundaries. In this context, Virtual\nReality systems represent a promising training and learning tool to properly\nfamiliarize drivers with the automated vehicle and allow them to interact with\nthe novel equipment involved. In this study, the effectiveness of an\nHead-Mounted display (HMD)-based training program for acquiring interaction\nskills in automated cars was compared to a user manual and a fixed-base\nsimulator. Results show that the training system affects the take-over\nperformances evaluated in a test drive in a high-end driving simulator.\nMoreover, self-reported measures indicate that the HMD-based training is\npreferred with respect to the other systems.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 12:43:00 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Sportillo", "Daniele", ""], ["Paljic", "Alexis", ""], ["Ojeda", "Luciano", ""], ["Fuchs", "Philippe", ""], ["Roussarie", "Vincent", ""]]}, {"id": "1803.05032", "submitter": "Chun-Wei Chiang", "authors": "Chun-Wei Chiang, Caroline Anderson, Claudia Flores-Saviaga, Eduardo Jr\n  Arenas, Felipe Colin, Mario Romero, Cuauhtemoc Rivera-Loaiza, Norma Elva\n  Chavez, Saiph Savage", "title": "Understanding Interface Design and Mobile Money Perceptions in Latin\n  America", "comments": "8 pages, 5 figures, CLIHC'17", "journal-ref": "2017. In Proceedings of the 8th Latin American Conference on\n  Human-Computer Interaction (CLIHC '17). ACM, New York, NY, USA, Article 5, 8\n  pages", "doi": "10.1145/3151470.3151473", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile money can facilitate financial inclusion in developing countries,\nwhich usually have high mobile phone use and steady remittance activity. Many\ncountries in Latin America meet the minimum technological requirements to use\nmobile money, however, the adoption in this region is relatively low. This\npaper investigates the different factors that lead people in Latin America to\ndistrust and therefore not adopt mobile money. For this purpose, we analyzed 27\nmobile money applications on the market and investigated the perceptions that\npeople in Latin America have of such interfaces. From our study, we singled out\nthe interface features that have the greatest influence in user adoption in\ndeveloping countries. We identified that for the Latin America market it is\ncrucial to create mobile applications that allow the user to visualize and\nunderstand the workflow through which their money is traveling to recipients.\nWe examined the significance of these findings in the design of future mobile\nmoney applications that can effectively improve the use of electronic financial\ntransactions in Latin America.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 20:17:19 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Chiang", "Chun-Wei", ""], ["Anderson", "Caroline", ""], ["Flores-Saviaga", "Claudia", ""], ["Arenas", "Eduardo Jr", ""], ["Colin", "Felipe", ""], ["Romero", "Mario", ""], ["Rivera-Loaiza", "Cuauhtemoc", ""], ["Chavez", "Norma Elva", ""], ["Savage", "Saiph", ""]]}, {"id": "1803.05073", "submitter": "Yang Li", "authors": "Yang Li, Samy Bengio, Gilles Bailly", "title": "Predicting Human Performance in Vertical Menu Selection Using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting human performance in interaction tasks allows designers or\ndevelopers to understand the expected performance of a target interface without\nactually testing it with real users. In this work, we present a deep neural net\nto model and predict human performance in performing a sequence of UI tasks. In\nparticular, we focus on a dominant class of tasks, i.e., target selection from\na vertical list or menu. We experimented with our deep neural net using a\npublic dataset collected from a desktop laboratory environment and a dataset\ncollected from hundreds of touchscreen smartphone users via crowdsourcing. Our\nmodel significantly outperformed previous methods on these datasets.\nImportantly, our method, as a deep model, can easily incorporate additional UI\nattributes such as visual appearance and content semantics without changing\nmodel architectures. By understanding about how a deep learning model learns\nfrom human behaviors, our approach can be seen as a vehicle to discover new\npatterns about human behaviors to advance analytical modeling.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 23:30:35 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Li", "Yang", ""], ["Bengio", "Samy", ""], ["Bailly", "Gilles", ""]]}, {"id": "1803.05181", "submitter": "Rizwan Ahmed Khan", "authors": "Muhammad Shoaib Jaliawala, Rizwan Ahmed Khan", "title": "Can Autism be Catered with Artificial Intelligence-Assisted Intervention\n  Technology? A Literature Review", "comments": null, "journal-ref": "Artificial Intelligence Review 2019", "doi": "10.1007/s10462-019-09686-8", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an extensive literature review of technology based\nintervention methodologies for individuals facing Autism Spectrum Disorder\n(ASD). Reviewed methodologies include: contemporary Computer Aided Systems\n(CAS), Computer Vision Assisted Technologies (CVAT) and Virtual Reality (VR) or\nArtificial Intelligence (AI)-Assisted interventions. The research over the past\ndecade has provided enough demonstrations that individuals with ASD have a\nstrong interest in technology based interventions, which are useful in both,\nclinical settings as well as at home and classrooms. Despite showing great\npromise, research in developing an advanced technology based intervention that\nis clinically quantitative for ASD is minimal. Moreover, the clinicians are\ngenerally not convinced about the potential of the technology based\ninterventions due to non-empirical nature of published results. A major reason\nbehind this lack of acceptability is that a vast majority of studies on\ndistinct intervention methodologies do not follow any specific standard or\nresearch design. We conclude from our findings that there remains a gap between\nthe research community of computer science, psychology and neuroscience to\ndevelop an AI assisted intervention technology for individuals suffering from\nASD. Following the development of a standardized AI based intervention\ntechnology, a database needs to be developed, to devise effective AI\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 09:56:39 GMT"}, {"version": "v2", "created": "Fri, 16 Mar 2018 04:37:12 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 18:54:34 GMT"}, {"version": "v4", "created": "Fri, 23 Nov 2018 05:15:02 GMT"}, {"version": "v5", "created": "Sat, 19 Jan 2019 16:16:32 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Jaliawala", "Muhammad Shoaib", ""], ["Khan", "Rizwan Ahmed", ""]]}, {"id": "1803.05434", "submitter": "Pablo Barros", "authors": "Pablo Barros, Nikhil Churamani, Egor Lakomkin, Henrique Siqueira,\n  Alexander Sutherland and Stefan Wermter", "title": "The OMG-Emotion Behavior Dataset", "comments": "Submited to WCCI/IJCNN 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is the basis paper for the accepted IJCNN challenge One-Minute\nGradual-Emotion Recognition (OMG-Emotion) by which we hope to foster\nlong-emotion classification using neural models for the benefit of the IJCNN\ncommunity. The proposed corpus has as the novelty the data collection and\nannotation strategy based on emotion expressions which evolve over time into a\nspecific context. Different from other corpora, we propose a novel multimodal\ncorpus for emotion expression recognition, which uses gradual annotations with\na focus on contextual emotion expressions. Our dataset was collected from\nYoutube videos using a specific search strategy based on restricted keywords\nand filtering which guaranteed that the data follow a gradual emotion\nexpression transition, i.e. emotion expressions evolve over time in a natural\nand continuous fashion. We also provide an experimental protocol and a series\nof unimodal baseline experiments which can be used to evaluate deep and\nrecurrent neural models in a fair and standard manner.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 15:31:03 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 14:00:37 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Barros", "Pablo", ""], ["Churamani", "Nikhil", ""], ["Lakomkin", "Egor", ""], ["Siqueira", "Henrique", ""], ["Sutherland", "Alexander", ""], ["Wermter", "Stefan", ""]]}, {"id": "1803.05714", "submitter": "Boyi Hou", "authors": "Boyi Hou, Qun Chen, Zhaoqiang Chen, Youcef Nafa, Zhanhuai Li", "title": "r-HUMO: A Risk-Aware Human-Machine Cooperation Framework for Entity\n  Resolution with Quality Guarantees", "comments": "12 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1710.00204", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though many approaches have been proposed for entity resolution (ER), it\nremains very challenging to find one with quality guarantees. To this end, we\nproposea risk-aware HUman-Machine cOoperation framework for ER, denoted by\nr-HUMO. Built on the existing HUMO framework, r-HUMO similarly enforces both\nprecision and recall levels by partitioning an ER workload between the human\nand the machine. However, r-HUMO is the first solution to optimize the process\nof human workload selection from a risk perspective. It iteratively selects\nhuman workload based on real-time risk analysis on human-labeled results as\nwell as prespecified machine metrics. In this paper,we first introduce the\nr-HUMO framework and then present the risk analysis technique to prioritize the\ninstances for manual labeling. Finally,we empirically evaluate r-HUMO's\nperformance on real data. Our extensive experiments show that r-HUMO is\neffective in enforcing quality guarantees,and compared with the\nstate-of-the-art alternatives, it can achieve better quality control with\nreduced human cost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 12:45:46 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 12:35:05 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 02:04:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Hou", "Boyi", ""], ["Chen", "Qun", ""], ["Chen", "Zhaoqiang", ""], ["Nafa", "Youcef", ""], ["Li", "Zhanhuai", ""]]}, {"id": "1803.05805", "submitter": "Robert Arbon", "authors": "Robert E. Arbon, Alex J. Jones, Lars A. Bratholm, Tom Mitchell, David\n  R. Glowacki", "title": "Sonifying stochastic walks on biomolecular energy landscapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.bio-ph physics.comp-ph q-bio.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Translating the complex, multi-dimensional data from simulations of\nbiomolecules to intuitive knowledge is a major challenge in computational\nchemistry and biology. The so-called \"free energy landscape\" is amongst the\nmost fundamental concepts used by scientists to understand both static and\ndynamic properties of biomolecular systems. In this paper we use Markov models\nto design a strategy for mapping features of this landscape to sonic\nparameters, for use in conjunction with visual display techniques such as\nstructural animations and free energy diagrams.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 15:26:06 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Arbon", "Robert E.", ""], ["Jones", "Alex J.", ""], ["Bratholm", "Lars A.", ""], ["Mitchell", "Tom", ""], ["Glowacki", "David R.", ""]]}, {"id": "1803.05843", "submitter": "Kristina Yordanova", "authors": "Kristina Yordanova, Adeline Paiement, Max Schr\\\"oder, Emma Tonkin,\n  Przemyslaw Woznowski, Carl Magnus Olsson, Joseph Rafferty, Timo Sztyler", "title": "Challenges in Annotation of useR Data for UbiquitOUs Systems: Results\n  from the 1st ARDUOUS Workshop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Labelling user data is a central part of the design and evaluation of\npervasive systems that aim to support the user through situation-aware\nreasoning. It is essential both in designing and training the system to\nrecognise and reason about the situation, either through the definition of a\nsuitable situation model in knowledge-driven applications, or through the\npreparation of training data for learning tasks in data-driven models. Hence,\nthe quality of annotations can have a significant impact on the performance of\nthe derived systems. Labelling is also vital for validating and quantifying the\nperformance of applications. In particular, comparative evaluations require the\nproduction of benchmark datasets based on high-quality and consistent\nannotations. With pervasive systems relying increasingly on large datasets for\ndesigning and testing models of users' activities, the process of data\nlabelling is becoming a major concern for the community. In this work we\npresent a qualitative and quantitative analysis of the challenges associated\nwith annotation of user data and possible strategies towards addressing these\nchallenges. The analysis was based on the data gathered during the 1st\nInternational Workshop on Annotation of useR Data for UbiquitOUs Systems\n(ARDUOUS) and consisted of brainstorming as well as annotation and\nquestionnaire data gathered during the talks, poster session, live annotation\nsession, and discussion session.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 16:37:40 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Yordanova", "Kristina", ""], ["Paiement", "Adeline", ""], ["Schr\u00f6der", "Max", ""], ["Tonkin", "Emma", ""], ["Woznowski", "Przemyslaw", ""], ["Olsson", "Carl Magnus", ""], ["Rafferty", "Joseph", ""], ["Sztyler", "Timo", ""]]}, {"id": "1803.05986", "submitter": "Ahmed Fadhil", "authors": "Ahmed Fadhil", "title": "A Review of Empirical Applications on Food Waste Prevention & Management", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Food waste has a significant detrimental economic, environmental and social\nimpact. Recent efforts in HCI re-search have examined ways of influencing\nsurplus food waste management. In this paper, we conduct a research survey to\ninvestigate and compare the effectiveness of existing approaches in food waste\nmanagement throughout its lifecycle from agricultural production, post-harvest\nhandling and storage, processing, distribution and consumption. The objectives\nof the survey are 1) to identify methods in food waste management, 2) their\narea of focus, 3) the ICT techniques they apply, 4) and the food waste\nlifecycle they target. In addition, we analyse if 5) they provide an open\naccess API for food waste data analysis. Based on the literature analysis, we\nthen highlight their pros and cons with respect to applications in food waste\nmanagement. The implications of this research could present a new opportunity\nfor interested stack-holders and future technologies to play a key role in\nreducing domestic and national food waste.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 20:55:48 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Fadhil", "Ahmed", ""]]}, {"id": "1803.06032", "submitter": "Zhicong Lu", "authors": "Zhicong Lu, Haijun Xia, Seongkook Heo, Daniel Wigdor", "title": "You Watch, You Give, and You Engage: A Study of Live Streaming Practices\n  in China", "comments": "Published at ACM CHI Conference on Human Factors in Computing Systems\n  (CHI 2018). Please cite the CHI version", "journal-ref": "Zhicong Lu, Haijun Xia, Seongkook Heo, and Daniel Wigdor. 2018.\n  You Watch, You Give, and You Engage: A Study of Live Streaming Practices in\n  China. In Proceedings of the 2018 CHI Conference on Human Factors in\n  Computing Systems (CHI '18)", "doi": "10.1145/3173574.3174040", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite gaining traction in North America, live streaming has not reached the\npopularity it has in China, where livestreaming has a tremendous impact on the\nsocial behaviors of users. To better understand this socio-technological\nphenomenon, we conducted a mixed methods study of live streaming practices in\nChina. We present the results of an online survey of 527 live streaming users,\nfocusing on their broadcasting or viewing practices and the experiences they\nfind most engaging. We also interviewed 14 active users to explore their\nmotivations and experiences. Our data revealed the different categories of\ncontent that was broadcasted and how varying aspects of this content engaged\nviewers. We also gained insight into the role reward systems and fan group-chat\nplay in engaging users, while also finding evidence that both viewers and\nstreamers desire deeper channels and mechanisms for interaction in addition to\nthe commenting, gifting, and fan groups that are available today.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 23:32:52 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Lu", "Zhicong", ""], ["Xia", "Haijun", ""], ["Heo", "Seongkook", ""], ["Wigdor", "Daniel", ""]]}, {"id": "1803.06174", "submitter": "Michael Veale", "authors": "Michael Veale, Reuben Binns and Max Van Kleek", "title": "Some HCI Priorities for GDPR-Compliant Machine Learning", "comments": "8 pages, 0 figures, The General Data Protection Regulation: An\n  Opportunity for the CHI Community? (CHI-GDPR 2018), Workshop at ACM CHI'18,\n  22 April 2018, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this short paper, we consider the roles of HCI in enabling the better\ngovernance of consequential machine learning systems using the rights and\nobligations laid out in the recent 2016 EU General Data Protection Regulation\n(GDPR)---a law which involves heavy interaction with people and systems.\nFocussing on those areas that relate to algorithmic systems in society, we\npropose roles for HCI in legal contexts in relation to fairness, bias and\ndiscrimination; data protection by design; data protection impact assessments;\ntransparency and explanations; the mitigation and understanding of automation\nbias; and the communication of envisaged consequences of processing.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 11:40:33 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Veale", "Michael", ""], ["Binns", "Reuben", ""], ["Van Kleek", "Max", ""]]}, {"id": "1803.06213", "submitter": "Mehdi Ghatee Dr.", "authors": "Roya Lotfi and Mehdi Ghatee", "title": "Smartphone based Driving Style Classification Using Features Made by\n  Discrete Wavelet Transform", "comments": "9 Pages, 4 Tables, 1 Figure, Extracted from M.SC Project (2018),\n  Department of Computer Science, Amirkabir University of Technology, Tehran,\n  Iran", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones consist of different sensors, which provide a platform for data\nacquisition in many scientific researches such as driving style identification\nsystems. In the present paper, smartphone data are used to evaluate the driving\nstyles based on maneuvers analysis. The data obtained for each maneuver is the\nspeed of the vehicle steering and the vehicle's direct and lateral\nacceleration. To classify the drivers based on their driving style,\nmachine-learning algorithms can be used on these data. However, these data\nusually contains more information than it is needed and cause a bad effect on\nthe learning accuracy. In addition, they may transfer some wrong information to\nthe learning algorithm. Thus, we used Haar discrete wavelet transformation to\nremove noise effects. Then, we get the discrete wavelet transformation with\nfour levels from smartphone sensors data, which include low-to-high\nfrequencies, respectively. The obtained features vector for each maneuver\nincludes the raw signal variance as well as the variance of the wavelet\ntransform components. On these vectors, we use the k-nearest neighbors\nalgorithm for features selection. Then, we use SVM, RBF and MLP neural networks\non these features to separate braking and dangerous speed maneuvers from the\nsafe ones as well as dangerous turning, U-turn and lane-changing maneuvers. The\nresults are very interesting.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:56:47 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Lotfi", "Roya", ""], ["Ghatee", "Mehdi", ""]]}, {"id": "1803.06625", "submitter": "Gus Xia", "authors": "Gus Xia, Carter Jacobsen, Qianwen Chen, Xingdong Yang, Roger\n  Dannenberg", "title": "ShIFT: A Semi-haptic Interface for Flute Tutoring", "comments": "The paper has be accepted by NIME2018, The 18th International\n  Conference on New Interfaces for Musical Expression", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional instrument learning is time-consuming. It begins with learning\nmusic notation and necessitates layers of sophistication and abstraction.\nHaptic interfaces open another door to the music world for the vast majority of\nbeginners when traditional training methods are not effective. However,\nexisting haptic interfaces can only deal with specially designed pieces with\ngreat restrictions on performance duration and pitch range due to the fact that\nnot all performance motions could be guided haptically for most instruments.\nOur system breaks such restrictions using a semi-haptic interface. For the\nfirst time, the pitch range of the haptically learned pieces goes beyond an\noctave (with the fingering motion covers most of the possible choices) and the\nduration of learned pieces cover a whole phrase. This significant change leads\nto a more realistic instrument learning process. Experiments show that our\nsemi-haptic interface is effective as long as learners are not \"tone deaf.\"\nUsing our prototype device, the learning rate is about 30% faster compared to\nlearning from videos.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 08:24:32 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 10:07:33 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["Xia", "Gus", ""], ["Jacobsen", "Carter", ""], ["Chen", "Qianwen", ""], ["Yang", "Xingdong", ""], ["Dannenberg", "Roger", ""]]}, {"id": "1803.06720", "submitter": "Felix Beierle", "authors": "Felix Beierle, Vinh Thuy Tran, Mathias Allemand, Patrick Neff,\n  Winfried Schlee, Thomas Probst, R\\\"udiger Pryss, Johannes Zimmermann", "title": "TYDR - Track Your Daily Routine. Android App for Tracking Smartphone\n  Sensor and Usage Data", "comments": "Accepted for publication at the 5th IEEE/ACM International Conference\n  on Mobile Software Engineering and Systems (MOBILESoft '18)", "journal-ref": null, "doi": "10.1145/3197231.3197235", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Android app TYDR (Track Your Daily Routine) which tracks\nsmartphone sensor and usage data and utilizes standardized psychometric\npersonality questionnaires. With the app, we aim at collecting data for\nresearching correlations between the tracked smartphone data and the user's\npersonality in order to predict personality from smartphone data. In this\npaper, we highlight our approaches in addressing the challenges in developing\nsuch an app. We optimize the tracking of sensor data by assessing the trade-off\nof size of data and battery consumption and granularity of the stored\ninformation. Our user interface is designed to incentivize users to install the\napp and fill out questionnaires. TYDR processes and visualizes the tracked\nsensor and usage data as well as the results of the personality questionnaires.\nWhen developing an app that will be used in psychological studies, requirements\nposed by ethics commissions / institutional review boards and data protection\nofficials have to be met. We detail our approaches concerning those\nrequirements regarding the anonymized storing of user data, informing the users\nabout the data collection, and enabling an opt-out option. We present our\nprocess for anonymized data storing while still being able to identify\nindividual users who successfully completed a psychological study with the app.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 19:24:56 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Beierle", "Felix", ""], ["Tran", "Vinh Thuy", ""], ["Allemand", "Mathias", ""], ["Neff", "Patrick", ""], ["Schlee", "Winfried", ""], ["Probst", "Thomas", ""], ["Pryss", "R\u00fcdiger", ""], ["Zimmermann", "Johannes", ""]]}, {"id": "1803.07506", "submitter": "Alexandre Pitti", "authors": "Alexandre Pitti", "title": "Ideas from Developmental Robotics and Embodied AI on the Questions of\n  Ethics in Robots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Artificial Intelligence and robotics are currently questioning\ntheethical framework of their applications to deal with potential drifts, as\nwell as the way inwhich these algorithms learn because they will have a strong\nimpact on the behavior ofrobots and the type of robots. interactions with\npeople. We would like to highlight someprinciples and ideas from cognitive\nneuroscience and development sciences based on theimportance of the body for\nintelligence, contrary to the theory of the all-brain or all-algorithm, to\nrepresent the world and interacting with others, and their current\napplicationsin embodied AI and developmental robotics to propose models of\narchitectures andmechanisms for agency, representation of the body, recognition\nof the intention of others,predictive coding, active inference, the role of\nfeedback and error, imitation, artificialcuriosity and contextual learning. We\nwill explain how these are important for the design ofautonomous systems and\nbeyond what they can tell us for the ethics of systems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 16:15:24 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Pitti", "Alexandre", ""]]}, {"id": "1803.07540", "submitter": "Michael Veale", "authors": "Lilian Edwards and Michael Veale", "title": "Enslaving the Algorithm: From a \"Right to an Explanation\" to a \"Right to\n  Better Decisions\"?", "comments": "14 pages, 0 figures", "journal-ref": "IEEE Security & Privacy (2018) 16(3), 46--54", "doi": "10.1109/MSP.2018.2701152", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As concerns about unfairness and discrimination in \"black box\" machine\nlearning systems rise, a legal \"right to an explanation\" has emerged as a\ncompellingly attractive approach for challenge and redress. We outline recent\ndebates on the limited provisions in European data protection law, and\nintroduce and analyze newer explanation rights in French administrative law and\nthe draft modernized Council of Europe Convention 108. While individual rights\ncan be useful, in privacy law they have historically unreasonably burdened the\naverage data subject. \"Meaningful information\" about algorithmic logics is more\ntechnically possible than commonly thought, but this exacerbates a new\n\"transparency fallacy\"---an illusion of remedy rather than anything\nsubstantively helpful. While rights-based approaches deserve a firm place in\nthe toolbox, other forms of governance, such as impact assessments, \"soft law,\"\njudicial review, and model repositories deserve more attention, alongside\ncatalyzing agencies acting for users to control algorithmic system design.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 17:27:03 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 08:39:07 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Edwards", "Lilian", ""], ["Veale", "Michael", ""]]}, {"id": "1803.07738", "submitter": "Zhilei Liu", "authors": "Haotian Guan, Zhilei Liu, Longbiao Wang, Jianwu Dang, Ruiguo Yu", "title": "Speech Emotion Recognition Considering Local Dynamic Features", "comments": "10 pages, 3 figures, accepted by ISSP 2017", "journal-ref": null, "doi": "10.1007/978-3-030-00126-1_2", "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, increasing attention has been directed to the study of the speech\nemotion recognition, in which global acoustic features of an utterance are\nmostly used to eliminate the content differences. However, the expression of\nspeech emotion is a dynamic process, which is reflected through dynamic\ndurations, energies, and some other prosodic information when one speaks. In\nthis paper, a novel local dynamic pitch probability distribution feature, which\nis obtained by drawing the histogram, is proposed to improve the accuracy of\nspeech emotion recognition. Compared with most of the previous works using\nglobal features, the proposed method takes advantage of the local dynamic\ninformation conveyed by the emotional speech. Several experiments on Berlin\nDatabase of Emotional Speech are conducted to verify the effectiveness of the\nproposed method. The experimental results demonstrate that the local dynamic\ninformation obtained with the proposed method is more effective for speech\nemotion recognition than the traditional global features.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 03:52:26 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Guan", "Haotian", ""], ["Liu", "Zhilei", ""], ["Wang", "Longbiao", ""], ["Dang", "Jianwu", ""], ["Yu", "Ruiguo", ""]]}, {"id": "1803.07782", "submitter": "Vijay Rajanna", "authors": "Vijay Rajanna, Tracy Hammond", "title": "Gaze-Assisted User Authentication to Counter Shoulder-surfing Attacks", "comments": "5 pages, 7 figures, 2 tables, ACM Richard Tapia Conference, Austin,\n  2016", "journal-ref": "ACM Richard Tapia Conference, Austin, 2016", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A highly secure, foolproof, user authentication system is still a primary\nfocus of research in the field of User Privacy and Security. Shoulder-surfing\nis an act of spying when an authorized user is logging into a system, and is\npromoted by a malicious intent of gaining unauthorized access. We present a\ngaze-assisted user authentication system as a potential solution to counter\nshoulder-surfing attacks. The system comprises of an eye tracker and an\nauthentication interface with 12 pre-defined shapes (e.g., triangle, circle,\netc.) that move simultaneously on the screen. A user chooses a set of three\nshapes as a password. To authenticate, the user follows the paths of three\nshapes as they move, one on each frame, over three consecutive frames.\n  The system uses either the template matching or decision tree algorithms to\nmatch the scan-path of the user's gaze with the path traversed by the shape.\nThe system was evaluated with seven users to test the accuracy of both the\nalgorithms. We found that with the template matching algorithm the system\nachieves an accuracy of 95%, and with the decision tree algorithm an accuracy\nof 90.2%. We also present the advantages and disadvantages of using both the\nalgorithms. Our study suggests that gaze-based authentication is a highly\nsecure method against shoulder-surfing attacks as the unique pattern of eye\nmovements for each individual makes the system hard to break into.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 07:48:22 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Rajanna", "Vijay", ""], ["Hammond", "Tracy", ""]]}, {"id": "1803.07947", "submitter": "Evgeny Krivosheev", "authors": "Evgeny Krivosheev, Bahareh Harandizadeh, Fabio Casati and Boualem\n  Benatallah", "title": "Crowd-Machine Collaboration for Item Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe how crowd and machine classifier can be efficiently\ncombined to screen items that satisfy a set of predicates. We show that this is\na recurring problem in many domains, present machine-human (hybrid) algorithms\nthat screen items efficiently and estimate the gain over human-only or\nmachine-only screening in terms of performance and cost.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 14:40:05 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Krivosheev", "Evgeny", ""], ["Harandizadeh", "Bahareh", ""], ["Casati", "Fabio", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1803.08067", "submitter": "Thanh Thi Nguyen", "authors": "Thanh Nguyen, Chee Peng Lim, Ngoc Duy Nguyen, Lee Gordon-Brown, Saeid\n  Nahavandi", "title": "A Review of Situation Awareness Assessment Approaches in Aviation\n  Environments", "comments": "IEEE Systems Journal, https://ieeexplore.ieee.org/document/8732669", "journal-ref": null, "doi": "10.1109/JSYST.2019.2918283", "report-no": null, "categories": "cs.HC cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Situation awareness (SA) is an important constituent in human information\nprocessing and essential in pilots' decision-making processes. Acquiring and\nmaintaining appropriate levels of SA is critical in aviation environments as it\naffects all decisions and actions taking place in flights and air traffic\ncontrol. This paper provides an overview of recent measurement models and\napproaches to establishing and enhancing SA in aviation environments. Many\naspects of SA are examined including the classification of SA techniques into\nsix categories, and different theoretical SA models from individual, to shared\nor team, and to distributed or system levels. Quantitative and qualitative\nperspectives pertaining to SA methods and issues of SA for unmanned vehicles\nare also addressed. Furthermore, future research directions regarding SA\nassessment approaches are raised to deal with shortcomings of the existing\nstate-of-the-art methods in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 11:17:12 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 22:59:10 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 02:34:30 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Nguyen", "Thanh", ""], ["Lim", "Chee Peng", ""], ["Nguyen", "Ngoc Duy", ""], ["Gordon-Brown", "Lee", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "1803.08264", "submitter": "Micha Pfeiffer", "authors": "Micha Pfeiffer, Hannes Kenngott, Anas Preukschas, Matthias Huber, Lisa\n  Bettscheider, Beat M\\\"uller-Stich, Stefanie Speidel", "title": "IMHOTEP - Virtual Reality Framework for Surgical Applications", "comments": "This is a post-peer-review, pre-copyedit version of an article\n  published in the International Journal of Computer Assisted Radiology and\n  Surgery (IJCARS). The final authenticated version is available online at:\n  https://doi.org/10.1007/s11548-018-1730-x", "journal-ref": "Int J CARS (2018)", "doi": "10.1007/s11548-018-1730-x", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: The data which is available to surgeons before, during and after\nsurgery is steadily increasing in quantity as well as diversity. When planning\na patient's treatment, this large amount of information can be difficult to\ninterpret. To aid in processing the information, new methods need to be found\nto present multi-modal patient data, ideally combining textual, imagery,\ntemporal and 3D data in a holistic and context-aware system. Methods: We\npresent an open-source framework which allows handling of patient data in a\nvirtual reality (VR) environment. By using VR technology, the workspace\navailable to the surgeon is maximized and 3D patient data is rendered in\nstereo, which increases depth perception. The framework organizes the data into\nworkspaces and contains tools which allow users to control, manipulate and\nenhance the data. Due to the framework's modular design, it can easily be\nadapted and extended for various clinical applications. Results: The framework\nwas evaluated by clinical personnel (77 participants). The majority of the\ngroup stated that a complex surgical situation is easier to comprehend by using\nthe framework, and that it is very well suited for education. Furthermore, the\napplication to various clinical scenarios - including the simulation of\nexcitation-propagation in the human atrium - demonstrated the framework's\nadaptability. As a feasibility study, the framework was used during the\nplanning phase of the surgical removal of a large central carcinoma from a\npatient's liver. Conclusion: The clinical evaluation showed a large potential\nand high acceptance for the VR environment in a medical context. The various\napplications confirmed that the framework is easily extended and can be used in\nreal-time simulation as well as for the manipulation of complex anatomical\nstructures.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 08:38:44 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Pfeiffer", "Micha", ""], ["Kenngott", "Hannes", ""], ["Preukschas", "Anas", ""], ["Huber", "Matthias", ""], ["Bettscheider", "Lisa", ""], ["M\u00fcller-Stich", "Beat", ""], ["Speidel", "Stefanie", ""]]}, {"id": "1803.08383", "submitter": "Marcos Maroto", "authors": "Marcos Maroto, Enrique Ca\\~no, Pavel Gonz\\'alez, Diego Villegas", "title": "Head-up Displays (HUD) in driving", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head Up Displays (HUDs) were designed originally to present at the usual\nviewpoints of the pilot the main sensor data during aircraft missions, because\nof placing instrument information in the forward field of view enhances pilots\nability to utilize both instrument and environmental information\nsimultaneously. The first civilian motor vehicle had a monochrome HUD that was\nreleased in 1988 by General Motors as a technological improvement of HeadDown\nDisplay (HDD) interface, which is commonly used in automobile industry. The HUD\nreduces the number and duration of the drivers sight deviations from the road,\nby projecting the required information directly into the drivers line of\nvision. There are many studies about ways of presenting the information:\nstandard oneearpiece presentation, threedimensional audio presentation, visual\nonly or audiovisual presentation. Results have shown that using a 3D auditory\ndisplay the time of acquiring targets is approximately 2.2 seconds faster than\nusing a oneearpiece way. Nevertheless, a disadvantage is when the drivers\nattention unconsciously shifts away from the road and goes focused on\nprocessing the information presented by the HUD. By this reason, the time, the\nway and the channel are important to represent the information on a HUD. A\nsolution is a context aware multimodal proactive recommended system that\nfeatures personalized content combined with the use of car sensors to determine\nwhen the information has to be presented.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 14:46:45 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Maroto", "Marcos", ""], ["Ca\u00f1o", "Enrique", ""], ["Gonz\u00e1lez", "Pavel", ""], ["Villegas", "Diego", ""]]}, {"id": "1803.08395", "submitter": "Philipp Jordan", "authors": "Philipp Jordan, Omar Mubin, Mohammad Obaid, Paula Alexandra Silva", "title": "Exploring the Referral and Usage of Science Fiction in HCI Literature", "comments": "v1: 20 pages, 4 figures, 3 tables, HCI International 2018 accepted\n  submission v2: 20 pages, 4 figures, 3 tables, added link/doi for Springer\n  proceeding", "journal-ref": null, "doi": "10.1007/978-3-319-91803-7_2", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on science fiction (sci-fi) in scientific publications has indicated\nthe usage of sci-fi stories, movies or shows to inspire novel Human-Computer\nInteraction (HCI) research. Yet no studies have analysed sci-fi in a top-ranked\ncomputer science conference at present. For that reason, we examine the CHI\nmain track for the presence and nature of sci-fi referrals in relationship to\nHCI research. We search for six sci-fi terms in a dataset of 5812 CHI main\nproceedings and code the context of 175 sci-fi referrals in 83 papers indexed\nin the CHI main track. In our results, we categorize these papers into five\ncontemporary HCI research themes wherein sci-fi and HCI interconnect: 1)\nTheoretical Design Research; 2) New Interactions; 3) Human-Body Modification or\nExtension; 4) Human-Robot Interaction and Artificial Intelligence; and 5)\nVisions of Computing and HCI. In conclusion, we discuss results and\nimplications located in the promising arena of sci-fi and HCI research.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:08:09 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 08:58:06 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Jordan", "Philipp", ""], ["Mubin", "Omar", ""], ["Obaid", "Mohammad", ""], ["Silva", "Paula Alexandra", ""]]}, {"id": "1803.08420", "submitter": "Jose Cambronero Sanchez", "authors": "Jose Cambronero and Phillip Stanley-Marbell and Martin Rinard", "title": "Incremental Color Quantization for Color-Vision-Deficient Observers\n  Using Mobile Gaming Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The sizes of compressed images depend on their spatial resolution (number of\npixels) and on their color resolution (number of color quantization levels). We\nintroduce DaltonQuant, a new color quantization technique for image compression\nthat cloud services can apply to images destined for a specific user with known\ncolor vision deficiencies. DaltonQuant improves compression in a user-specific\nbut reversible manner thereby improving a user's network bandwidth and data\nstorage efficiency. DaltonQuant quantizes image data to account for\nuser-specific color perception anomalies, using a new method for incremental\ncolor quantization based on a large corpus of color vision acuity data obtained\nfrom a popular mobile game. Servers that host images can revert DaltonQuant's\nimage requantization and compression when those images must be transmitted to a\ndifferent user, making the technique practical to deploy on a large scale. We\nevaluate DaltonQuant's compression performance on the Kodak PC reference image\nset and show that it improves compression by an additional 22%-29% over the\nstate-of-the-art compressors TinyPNG and pngquant.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 15:54:43 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Cambronero", "Jose", ""], ["Stanley-Marbell", "Phillip", ""], ["Rinard", "Martin", ""]]}, {"id": "1803.08488", "submitter": "Arunesh Mathur", "authors": "Arunesh Mathur, Arvind Narayanan, Marshini Chetty", "title": "An Empirical Study of Affiliate Marketing Disclosures on YouTube and\n  Pinterest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While disclosures relating to various forms of Internet advertising are well\nestablished and follow specific formats, endorsement marketing disclosures are\noften open-ended in nature and written by individual publishers. Because such\nmarketing often appears as part of publishers' actual content, ensuring that it\nis adequately disclosed is critical so that end-users can identify it as such.\nIn this paper, we characterize disclosures relating to affiliate marketing---a\ntype of endorsement based marketing---on two popular social media platforms:\nYouTube & Pinterest. We find that only roughly one-tenth of affiliate content\non both platforms contains disclosures. Based on our findings, we make policy\nrecommendations geared towards various stakeholders in the affiliate marketing\nindustry, highlighting how both social media platforms and affiliate companies\ncan enable better disclosure practices.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 17:43:14 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 00:50:00 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Mathur", "Arunesh", ""], ["Narayanan", "Arvind", ""], ["Chetty", "Marshini", ""]]}, {"id": "1803.08986", "submitter": "Bokai Cao", "authors": "Bokai Cao, Lei Zheng, Chenwei Zhang, Philip S. Yu, Andrea Piscitello,\n  John Zulueta, Olu Ajilore, Kelly Ryan and Alex D. Leow", "title": "DeepMood: Modeling Mobile Phone Typing Dynamics for Mood Detection", "comments": "KDD 2017", "journal-ref": null, "doi": "10.1145/3097983.3098086", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of electronic forms of communication presents new\nopportunities in the study of mental health, including the ability to\ninvestigate the manifestations of psychiatric diseases unobtrusively and in the\nsetting of patients' daily lives. A pilot study to explore the possible\nconnections between bipolar affective disorder and mobile phone usage was\nconducted. In this study, participants were provided a mobile phone to use as\ntheir primary phone. This phone was loaded with a custom keyboard that\ncollected metadata consisting of keypress entry time and accelerometer\nmovement. Individual character data with the exceptions of the backspace key\nand space bar were not collected due to privacy concerns. We propose an\nend-to-end deep architecture based on late fusion, named DeepMood, to model the\nmulti-view metadata for the prediction of mood scores. Experimental results\nshow that 90.31% prediction accuracy on the depression score can be achieved\nbased on session-level mobile phone typing dynamics which is typically less\nthan one minute. It demonstrates the feasibility of using mobile phone metadata\nto infer mood disturbance and severity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 21:29:21 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cao", "Bokai", ""], ["Zheng", "Lei", ""], ["Zhang", "Chenwei", ""], ["Yu", "Philip S.", ""], ["Piscitello", "Andrea", ""], ["Zulueta", "John", ""], ["Ajilore", "Olu", ""], ["Ryan", "Kelly", ""], ["Leow", "Alex D.", ""]]}, {"id": "1803.09152", "submitter": "Benjamin Finley", "authors": "Benjamin Finley, Tapio Soikkeli", "title": "Mobile Device Type Substitution", "comments": null, "journal-ref": "Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2, 1,\n  Article 8 (March 2018), 20 pages", "doi": "10.1145/3191740", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile users today interact with a variety of mobile device types including\nsmartphones, tablets, smartwatches, and others. However research on mobile\ndevice type substitution has been limited in several respects including a lack\nof detailed and robust analyses. Therefore, in this work we study mobile device\ntype substitution through analysis of multidevice usage data from a large\nUS-based user panel. Specifically, we use regression analysis over paired user\ngroups to test five device type substitution hypotheses. We find that both\ntablets and PCs are partial substitutes for smartphones with tablet and PC\nownership decreasing smartphone usage by about 12.5 and 13 hours/month\nrespectively. Additionally, we find that tablets and PCs also prompt about 20\nand 57 hours/month respectively of additional (non-substituted) usage. We also\nillustrate significant inter-user diversity in substituted and additional\nusage. Overall, our results can help in understanding the relative positioning\nof different mobile device types and in parameterizing higher level mobile\necosystem models.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 19:16:09 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Finley", "Benjamin", ""], ["Soikkeli", "Tapio", ""]]}, {"id": "1803.09461", "submitter": "Nicolas Jullien", "authors": "Shubham Krishna, Romain Billot, Nicolas Jullien", "title": "A clustering approach to infer Wikipedia contributors' profile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In online communities, recent studies have strongly improved our knowledge\nabout the different types or profiles of contributors, from casual to very\ninvolved ones, through focused people. However they do so by using very complex\nmethodologies (qualitative-quantitative mix, with a high workload to manually\ncodify/characterize the edits), making their replication for the practitioners\nlimited. These studies are on the English Wikipedia only. The objective of this\npaper is to highlight different profiles of contributors with clustering\ntechniques. The originality is to show how using only the edits, and their\ndistribution over time, allows to build these contributors profiles with a good\naccuracy and stability amongst languages. The methodology is validated with\nboth Romanian and Danish wikis. The highlighted profiles are identifiable early\nin the history of involvement, suggesting that light monitoring of newcomers\nmay be sufficient to adapt the interaction with them and increase the retention\nrate.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 08:17:10 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Krishna", "Shubham", ""], ["Billot", "Romain", ""], ["Jullien", "Nicolas", ""]]}, {"id": "1803.09689", "submitter": "Cem Eteke", "authors": "Cem Eteke, Hayati Havlucu, Nisa \\.Irem K{\\i}rba\\c{c}, Mehmet Cengiz\n  Onba\\c{s}l{\\i}, Aykut Co\\c{s}kun, Terry Eskenazi, O\\u{g}uzhan \\\"Ozcan,\n  Bar{\\i}\\c{s} Akg\\\"un", "title": "Flow From Motion: A Deep Learning Approach", "comments": "7 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable devices have the potential to enhance sports performance, yet they\nare not fulfilling this promise. Our previous studies with 6 professional\ntennis coaches and 20 players indicate that this could be due the lack of\npsychological or mental state feedback, which the coaches claim to provide.\nTowards this end, we propose to detect the flow state, mental state of optimal\nperformance, using wearables data to be later used in training. We performed a\nstudy with a professional tennis coach and two players. The coach provided\nlabels about the players' flow state while each player had a wearable device on\ntheir racket holding wrist. We trained multiple models using the wearables data\nand the coach labels. Our deep neural network models achieved around 98%\ntesting accuracy for a variety of conditions. This suggests that the flow state\nor what coaches recognize as flow, can be detected using wearables data in\ntennis which is a novel result. The implication for the HCI community is that\nhaving access to such information would allow for design of novel hardware and\ninteraction paradigms that would be helpful in professional athlete training.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 16:12:48 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Eteke", "Cem", ""], ["Havlucu", "Hayati", ""], ["K\u0131rba\u00e7", "Nisa \u0130rem", ""], ["Onba\u015fl\u0131", "Mehmet Cengiz", ""], ["Co\u015fkun", "Aykut", ""], ["Eskenazi", "Terry", ""], ["\u00d6zcan", "O\u011fuzhan", ""], ["Akg\u00fcn", "Bar\u0131\u015f", ""]]}, {"id": "1803.09702", "submitter": "Olivier Deiss", "authors": "Olivier Deiss, Siddharth Biswal, Jing Jin, Haoqi Sun, M. Brandon\n  Westover, Jimeng Sun", "title": "HAMLET: Interpretable Human And Machine co-LEarning Technique", "comments": "Removed KDD template", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient label acquisition processes are key to obtaining robust\nclassifiers. However, data labeling is often challenging and subject to high\nlevels of label noise. This can arise even when classification targets are well\ndefined, if instances to be labeled are more difficult than the prototypes used\nto define the class, leading to disagreements among the expert community. Here,\nwe enable efficient training of deep neural networks. From low-confidence\nlabels, we iteratively improve their quality by simultaneous learning of\nmachines and experts. We call it Human And Machine co-LEarning Technique\n(HAMLET). Throughout the process, experts become more consistent, while the\nalgorithm provides them with explainable feedback for confirmation. HAMLET uses\na neural embedding function and a memory module filled with diverse reference\nembeddings from different classes. Its output includes classification labels\nand highly relevant reference embeddings as explanation. We took the study of\nbrain monitoring at intensive care unit (ICU) as an application of HAMLET on\ncontinuous electroencephalography (cEEG) data. Although cEEG monitoring yields\nlarge volumes of data, labeling costs and difficulty make it hard to build a\nclassifier. Additionally, while experts agree on the labels of clear-cut\nexamples of cEEG patterns, labeling many real-world cEEG data can be extremely\nchallenging. Thus, a large minority of sequences might be mislabeled. HAMLET\nhas shown significant performance gain against deep learning and other\nbaselines, increasing accuracy from 7.03% to 68.75% on challenging inputs.\nBesides improved performance, clinical experts confirmed the interpretability\nof those reference embeddings in helping explaining the classification results\nby HAMLET.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 16:29:03 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2018 13:28:50 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 05:41:09 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Deiss", "Olivier", ""], ["Biswal", "Siddharth", ""], ["Jin", "Jing", ""], ["Sun", "Haoqi", ""], ["Westover", "M. Brandon", ""], ["Sun", "Jimeng", ""]]}, {"id": "1803.09814", "submitter": "Evgeny Krivosheev", "authors": "Evgeny Krivosheev, Fabio Casati and Boualem Benatallah", "title": "Crowd-based Multi-Predicate Screening of Papers in Literature Reviews", "comments": "Please cite the www2018 version of this paper:\n  @inproceedings{krivosheev2018, title={Crowd-based Multi-Predicate Screening\n  of Papers in Literature Reviews}, author={Evgeny Krivosheev, Fabio Casati and\n  Boualem Benatallah}, year={2018}, organization={International World Wide Web\n  Conferences Steering Committee} }", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic literature reviews (SLRs) are one of the most common and useful\nform of scientific research and publication. Tens of thousands of SLRs are\npublished each year, and this rate is growing across all fields of science.\nPerforming an accurate, complete and unbiased SLR is however a difficult and\nexpensive endeavor. This is true in general for all phases of a literature\nreview, and in particular for the paper screening phase, where authors lter a\nset of potentially in-scope papers based on a number of exclusion criteria. To\naddress the problem, in recent years the research community has began to\nexplore the use of the crowd to allow for a faster, accurate, cheaper and\nunbiased screening of papers. Initial results show that crowdsourcing can be\neffective, even for relatively complex reviews. In this paper we derive and\nanalyze a set of strategies for crowd-based screening, and show that an\nadaptive strategy, that continuously re-assesses the statistical properties of\nthe problem to minimize the number of votes needed to take decisions for each\npaper, significantly outperforms a number of non-adaptive approaches in terms\nof cost and accuracy. We validate both applicability and results of the\napproach through a set of crowdsourcing experiments, and discuss properties of\nthe problem and algorithms that we believe to be generally of interest for\nclassification problems where items are classified via a series of successive\ntests (as it often happens in medicine).\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 14:38:07 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Krivosheev", "Evgeny", ""], ["Casati", "Fabio", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1803.09861", "submitter": "Kumar Akash", "authors": "Kumar Akash, Wan-Lin Hu, Neera Jain, Tahira Reid", "title": "A Classification Model for Sensing Human Trust in Machines Using EEG and\n  GSR", "comments": "20 pages", "journal-ref": null, "doi": "10.1145/3132743", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, intelligent machines \\emph{interact and collaborate} with humans in a\nway that demands a greater level of trust between human and machine. A first\nstep towards building intelligent machines that are capable of building and\nmaintaining trust with humans is the design of a sensor that will enable\nmachines to estimate human trust level in real-time. In this paper, two\napproaches for developing classifier-based empirical trust sensor models are\npresented that specifically use electroencephalography (EEG) and galvanic skin\nresponse (GSR) measurements. Human subject data collected from 45 participants\nis used for feature extraction, feature selection, classifier training, and\nmodel validation. The first approach considers a general set of\npsychophysiological features across all participants as the input variables and\ntrains a classifier-based model for each participant, resulting in a trust\nsensor model based on the general feature set (i.e., a \"general trust sensor\nmodel\"). The second approach considers a customized feature set for each\nindividual and trains a classifier-based model using that feature set,\nresulting in improved mean accuracy but at the expense of an increase in\ntraining time. This work represents the first use of real-time\npsychophysiological measurements for the development of a human trust sensor.\nImplications of the work, in the context of trust management algorithm design\nfor intelligent machines, are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 03:03:14 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Akash", "Kumar", ""], ["Hu", "Wan-Lin", ""], ["Jain", "Neera", ""], ["Reid", "Tahira", ""]]}, {"id": "1803.10177", "submitter": "Wieslaw Kopec", "authors": "Wies{\\l}aw Kope\\'c, Rados{\\l}aw Nielek, Adam Wierzbicki", "title": "Guidelines Towards Better Participation of Older Adults in Software\n  Development Processes using a new SPIRAL Method and Participatory Approach", "comments": null, "journal-ref": null, "doi": "10.1145/3195836.3195840", "report-no": null, "categories": "cs.SE cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new method of engaging older participants in the\nprocess of application and IT solutions development for older adults for\nemerging IT and tech startups. A new method called SPIRAL (Support for\nParticipant Involvement in Rapid and Agile software development Labs) is\nproposed which adds both sustainability and flexibility to the development\nprocess with older adults. This method is based on the participatory approach\nand user empowerment of older adults with the aid of a bootstrapped Living Lab\nconcept and it goes beyond well established user-centered and empathic design.\nSPIRAL provides strategies for direct involvement of older participants in the\nsoftware development processes from the very early stage to support the agile\napproach with rapid prototyping, in particular in new and emerging startup\nenvironments with limited capabilities, including time, team and resources.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 16:49:01 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Kope\u0107", "Wies\u0142aw", ""], ["Nielek", "Rados\u0142aw", ""], ["Wierzbicki", "Adam", ""]]}, {"id": "1803.10311", "submitter": "Doris Xin", "authors": "Doris Xin, Litian Ma, Shuchen Song, Aditya Parameswaran", "title": "How Developers Iterate on Machine Learning Workflows -- A Survey of the\n  Applied Machine Learning Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning workflow development is anecdotally regarded to be an\niterative process of trial-and-error with humans-in-the-loop. However, we are\nnot aware of quantitative evidence corroborating this popular belief. A\nquantitative characterization of iteration can serve as a benchmark for machine\nlearning workflow development in practice, and can aid the development of\nhuman-in-the-loop machine learning systems. To this end, we conduct a\nsmall-scale survey of the applied machine learning literature from five\ndistinct application domains. We collect and distill statistics on the role of\niteration within machine learning workflow development, and report preliminary\ntrends and insights from our investigation, as a starting point towards this\nbenchmark. Based on our findings, we finally describe desiderata for effective\nand versatile human-in-the-loop machine learning systems that can cater to\nusers in diverse domains.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 20:38:05 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 22:16:31 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Xin", "Doris", ""], ["Ma", "Litian", ""], ["Song", "Shuchen", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1803.10318", "submitter": "Gaurav Gupta", "authors": "Gaurav Gupta, Sergio Pequito, Paul Bogdan", "title": "Re-thinking EEG-based non-invasive brain interfaces: modeling and\n  analysis", "comments": "12 pages, 16 figures, ICCPS-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain interfaces are cyber-physical systems that aim to harvest information\nfrom the (physical) brain through sensing mechanisms, extract information about\nthe underlying processes, and decide/actuate accordingly. Nonetheless, the\nbrain interfaces are still in their infancy, but reaching to their maturity\nquickly as several initiatives are released to push forward their development\n(e.g., NeuraLink by Elon Musk and `typing-by-brain' by Facebook). This has\nmotivated us to revisit the design of EEG-based non-invasive brain interfaces.\nSpecifically, current methodologies entail a highly skilled neuro-functional\napproach and evidence-based \\emph{a priori} knowledge about specific signal\nfeatures and their interpretation from a neuro-physiological point of view.\nHereafter, we propose to demystify such approaches, as we propose to leverage\nnew time-varying complex network models that equip us with a fractal dynamical\ncharacterization of the underlying processes. Subsequently, the parameters of\nthe proposed complex network models can be explained from a system's\nperspective, and, consecutively, used for classification using machine learning\nalgorithms and/or actuation laws determined using control system's theory.\nBesides, the proposed system identification methods and techniques have\ncomputational complexities comparable with those currently used in EEG-based\nbrain interfaces, which enable comparable online performances. Furthermore, we\nforesee that the proposed models and approaches are also valid using other\ninvasive and non-invasive technologies. Finally, we illustrate and\nexperimentally evaluate this approach on real EEG-datasets to assess and\nvalidate the proposed methodology. The classification accuracies are high even\non having less number of training samples.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 20:54:29 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Gupta", "Gaurav", ""], ["Pequito", "Sergio", ""], ["Bogdan", "Paul", ""]]}, {"id": "1803.10530", "submitter": "Sara Heitlinger Dr", "authors": "Sara Heitlinger and Rob Comber", "title": "Design for the Right to the Smart City in More-than-Human Worlds", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental concerns have driven an interest in sustainable smart cities,\nthrough the monitoring and optimisation of networked infrastructure processes.\nAt the same time, there are concerns about who these interventions and services\nare for, and who benefits. HCI researchers and designers interested in civic\nlife have started to call for the democratisation of urban space through\nresistance and political action to challenge state and corporate claims. This\npaper aims to add to the growing body of critical and civic led smart city\nliterature in HCI by leveraging concepts from the environmental humanities\nabout more than human worlds, as a way to shift understandings within HCI of\nsmart cities away from the exceptional and human centered, towards a more\ninclusive understanding that incorporates and designs for other others and\nother species. We illustrate through a case study that involved codesigning\nInternet of Things with urban agricultural communities, possibilities for\ncreating more environmentally and socially just smart cities.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 11:15:09 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Heitlinger", "Sara", ""], ["Comber", "Rob", ""]]}, {"id": "1803.10769", "submitter": "Benajmin Radford J", "authors": "Benjamin J. Radford, Leonardo M. Apolonio, Antonio J. Trias, Jim A.\n  Simpson", "title": "Network Traffic Anomaly Detection Using Recurrent Neural Networks", "comments": "Prepared for the 2017 National Symposium on Sensor and Data Fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a recurrent neural network is able to learn a model to represent\nsequences of communications between computers on a network and can be used to\nidentify outlier network traffic. Defending computer networks is a challenging\nproblem and is typically addressed by manually identifying known malicious\nactor behavior and then specifying rules to recognize such behavior in network\ncommunications. However, these rule-based approaches often generalize poorly\nand identify only those patterns that are already known to researchers. An\nalternative approach that does not rely on known malicious behavior patterns\ncan potentially also detect previously unseen patterns. We tokenize and\ncompress netflow into sequences of \"words\" that form \"sentences\" representative\nof a conversation between computers. These sentences are then used to generate\na model that learns the semantic and syntactic grammar of the newly generated\nlanguage. We use Long-Short-Term Memory (LSTM) cell Recurrent Neural Networks\n(RNN) to capture the complex relationships and nuances of this language. The\nlanguage model is then used predict the communications between two IPs and the\nprediction error is used as a measurement of how typical or atyptical the\nobserved communication are. By learning a model that is specific to each\nnetwork, yet generalized to typical computer-to-computer traffic within and\noutside the network, a language model is able to identify sequences of network\nactivity that are outliers with respect to the model. We demonstrate positive\nunsupervised attack identification performance (AUC 0.84) on the ISCX IDS\ndataset which contains seven days of network activity with normal traffic and\nfour distinct attack patterns.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 14:49:25 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Radford", "Benjamin J.", ""], ["Apolonio", "Leonardo M.", ""], ["Trias", "Antonio J.", ""], ["Simpson", "Jim A.", ""]]}, {"id": "1803.10810", "submitter": "Danilo Alvares", "authors": "Helena Reis, Danilo Alvares, Patricia Jaques, Seiji Isotani", "title": "Analysis of permanence time in emotional states: A case study using\n  educational software", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the results of an experiment in which we investigated\nhow prior algebra knowledge and personality can influence the permanence time\nfrom the confusion state to frustration/boredom state in a computer learning\nenvironment. Our experimental results indicate that people with a neurotic\npersonality and a low level of algebra knowledge can deal with confusion for\nless time and can easily feel frustrated/bored when there is no intervention.\nOur analysis also suggest that people with an extroversion personality and a\nlow level of algebra knowledge are able to control confusion for longer,\nleading to later interventions. These findings support that it is possible to\ndetect emotions in a less invasive way and without the need of physiological\nsensors or complex algorithms. Furthermore, obtained median times can be\nincorporated into computational regulation models (e.g. adaptive interfaces) to\nregulate students' emotion during the teaching-learning process.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 19:05:49 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Reis", "Helena", ""], ["Alvares", "Danilo", ""], ["Jaques", "Patricia", ""], ["Isotani", "Seiji", ""]]}, {"id": "1803.11046", "submitter": "Swarup Chauhan", "authors": "Swarup Chauhan (1 and 2), Kathleen Sell (2 and 5), Frieder Enzmann\n  (2), Wolfram R\\\"uhaak (3), Thorsten Wille (4), Ingo Sass (1), Michael Kersten\n  (2) ((1) Institute of Applied Geosciences, University of Technology,\n  Darmstadt, Germany (2) Institute for Geosciences, Johannes\n  Gutenberg-University, Mainz, Germany (3) Federal Institute for Geosciences\n  and Natural Resources (BGR), Hannover, Germany (4) APS Antriebs-, Pr\\\"uf- und\n  Steuertechnik GmbH, G\\\"ottingen-Rosdorf, Germany (5) igem - Institute for\n  Geothermal Ressource Management, Bingen, Germany)", "title": "CobWeb - a toolbox for automatic tomographic image analysis based on\n  machine learning techniques: application and examples", "comments": "29 pages (article + appendix). 16 figures (8 Article/8 Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we introduce CobWeb 1.0 which is a graphical user interface\ntailored explicitly for accurate image segmentation and representative\nelementary volume analysis of digital rock images derived from high resolution\ntomography. The CobWeb code is a work package deployed as a series of windows\nexecutable binaries which use image processing and machine learning libraries\nof MATLAB. The user-friendly interface enables image segmentation and\ncross-validation employing K-means, Fuzzy C-means, least square support vector\nmachine, and ensemble classification (bragging and boosting) segmentation\ntechniques. A quick region of interest analysis including relative porosity\ntrends, pore size distribution, and volume fraction of different phases can be\nperformed on different geomaterials. Data can be exported to ParaView, DSI\nStudio (.fib), Microsoft Excel and MATLAB for further visualisation and\nstatistical analysis. The efficiency of the new tool was verified using gas\nhydrate-bearing sediment samples and Berea sandstone, both from synchrotron\ntomography datasets, as well as Grosmont carbonate rock X-ray micro-tomographic\ndataset. Despite its high sub-micrometer resolution, the gas hydrate dataset\nwas suffering from edge enhancement artefacts. These artefacts were primarily\nnormalized by the dual filtering approach using both non-local means and\nanisotropic diffusion filtering. The desired automatic segmentation of the\nphases (brine, sand, and gas hydrate) was thus successfully achieved using the\ndual clustering approach.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 13:13:57 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 13:50:42 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 00:28:36 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Chauhan", "Swarup", "", "1 and 2"], ["Sell", "Kathleen", "", "2 and 5"], ["Enzmann", "Frieder", ""], ["R\u00fchaak", "Wolfram", ""], ["Wille", "Thorsten", ""], ["Sass", "Ingo", ""], ["Kersten", "Michael", ""]]}, {"id": "1803.11088", "submitter": "Kalin Stefanov", "authors": "Kalin Stefanov", "title": "Webcam-based Eye Gaze Tracking under Natural Head Movement", "comments": "MSc Thesis in Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript investigates and proposes a visual gaze tracker that tackles\nthe problem using only an ordinary web camera and no prior knowledge in any\nsense (scene set-up, camera intrinsic and/or extrinsic parameters). The tracker\nwe propose is based on the observation that our desire to grant the freedom of\nnatural head movement to the user requires 3D modeling of the scene set-up.\nAlthough, using a single low resolution web camera bounds us in dimensions (no\ndepth can be recovered), we propose ways to cope with this drawback and model\nthe scene in front of the user. We tackle this three-dimensional problem by\nrealizing that it can be viewed as series of two-dimensional special cases.\nThen, we propose a procedure that treats each movement of the user's head as a\nspecial two-dimensional case, hence reducing the complexity of the problem back\nto two dimensions. Furthermore, the proposed tracker is calibration free and\ndiscards this tedious part of all previously mentioned trackers.\n  Experimental results show that the proposed tracker achieves good results,\ngiven the restrictions on it. We can report that the tracker commits a mean\nerror of (56.95, 70.82) pixels in x and y direction, respectively, when the\nuser's head is as static as possible (no chin-rests are used). Furthermore, we\ncan report that the proposed tracker commits a mean error of (87.18, 103.86)\npixels in x and y direction, respectively, under natural head movement.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 14:16:00 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Stefanov", "Kalin", ""]]}, {"id": "1803.11280", "submitter": "Birgitta Dresp-Langley", "authors": "AU Batmaz, M de Mathelin and Birgitta Dresp-Langley", "title": "Getting nowhere fast: trade-off between speed and precision in training\n  to execute image-guided hand-tool movements", "comments": null, "journal-ref": "2016, BMC Psychology, 4, 55", "doi": "10.1186/s40359-016-0161-0", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The speed and precision with which objects are moved by hand or\nhand-tool interaction under image guidance depend on a specific type of visual\nand spatial sensorimotor learning. Novices have to learn to optimally control\nwhat their hands are doing in a real-world environment while looking at an\nimage representation of the scene on a video monitor. Previous research has\nshown slower task execution times and lower performance scores under\nimage-guidance compared with situations of direct action viewing. The cognitive\nprocesses for overcoming this drawback by training are not yet understood.\nMethods: We investigated the effects of training on the time and precision of\ndirect view versus image guided object positioning on targets of a Real-world\nAction Field (RAF). Two men and two women had to learn to perform the task as\nswiftly and as precisely as possible with their dominant hand, using a tool or\nnot and wearing a glove or not. Individuals were trained in sessions of mixed\ntrial blocks with no feed-back. Results: As predicted, image-guidance produced\nsignificantly slower times and lesser precision in all trainees and\nsessionscompared with direct viewing. With training, all trainees get faster in\nall conditions, but only one of them gets reliably more precise in the\nimage-guided conditions. Speed-accuracy trade-offs in the individual\nperformance data show that the highest precision scores and steepest learning\ncurve, for time and precision, were produced by the slowest\nstarter.Conclusions: Performance evolution towards optimal precision is\ncompromised when novices start by going as fast as they can. The findings have\ndirect implications for individual skill monitoring in training programmes for\nimage-guided technology applications with human operators.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 23:07:19 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Batmaz", "AU", ""], ["de Mathelin", "M", ""], ["Dresp-Langley", "Birgitta", ""]]}, {"id": "1803.11283", "submitter": "Birgitta Dresp-Langley", "authors": "AU Batmaz, M de Mathelin and Birgitta Dresp-Langley", "title": "Effects of 2D and 3D image views on hand movement trajectories in the\n  surgeons peripersonal space in a computer controlled simulator environment", "comments": null, "journal-ref": "2018, Cogent Medecine, 5, 1426232", "doi": "10.1080/2331205X.2018.1426232", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In image-guided surgical tasks, the precision and timing of hand movements\ndepend on the effectiveness of visual cues relative to specific target areas in\nthe surgeons peri-personal space. Two-dimensional (2D) image views of\nreal-world movements are known to negatively affect both constrained (with\ntool) and unconstrained(no tool) hand movements compared with direct action\nviewing. Task conditions where virtual 3D would generate and advantage for\nsurgical eye-hand coordination are unclear. Here, we compared effects of 2D and\n3D image views on the precision and timing of surgical hand movement\ntrajectories in a simulator environment. Eight novices had to pick and place a\nsmall cube on target areas across different trajectory segments in the surgeons\nperi-personal space, with the dominant hand, with and without a tool, under\nconditions of: (1) direct (2) 2D fisheye camera and (3) virtual 3D viewing\n(headmounted). Significant effects of the location of trajectories in the\nsurgeons peri-personal space on movement times and precision were found.\nSubjects were faster and more precise across specific target locations,\ndepending on the viewing modality.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 23:20:11 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Batmaz", "AU", ""], ["de Mathelin", "M", ""], ["Dresp-Langley", "Birgitta", ""]]}, {"id": "1803.11300", "submitter": "Wei Zheng", "authors": "Wei Zheng, Bo Wu and Hai Lin", "title": "POMDP Model Learning for Human Robot Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen human robot collaboration (HRC) quickly emerged as a\nhot research area at the intersection of control, robotics, and psychology.\nWhile most of the existing work in HRC focused on either low-level human-aware\nmotion planning or HRC interface design, we are particularly interested in a\nformal design of HRC with respect to high-level complex missions, where it is\nof critical importance to obtain an accurate and meanwhile tractable human\nmodel. Instead of assuming the human model is given, we ask whether it is\nreasonable to learn human models from observed perception data, such as the\ngesture, eye movements, head motions of the human in concern. As our initial\nstep, we adopt a partially observable Markov decision process (POMDP) model in\nthis work as mounting evidences have suggested Markovian properties of human\nbehaviors from psychology studies. In addition, POMDP provides a general\nmodeling framework for sequential decision making where states are hidden and\nactions have stochastic outcomes. Distinct from the majority of POMDP model\nlearning literature, we do not assume that the state, the transition structure\nor the bound of the number of states in POMDP model is given. Instead, we use a\nBayesian non-parametric learning approach to decide the potential human states\nfrom data. Then we adopt an approach inspired by probably approximately correct\n(PAC) learning to obtain not only an estimation of the transition probability\nbut also a confidence interval associated to the estimation. Then, the\nperformance of applying the control policy derived from the estimated model is\nguaranteed to be sufficiently close to the true model. Finally, data collected\nfrom a driver-assistance test-bed are used to train the model, which\nillustrates the effectiveness of the proposed learning method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 01:09:30 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Zheng", "Wei", ""], ["Wu", "Bo", ""], ["Lin", "Hai", ""]]}]