[{"id": "2005.00093", "submitter": "Stanis{\\l}aw Saganowski", "authors": "Stanis{\\l}aw Saganowski, Przemys{\\l}aw Kazienko, Maciej Dzie\\.zyc,\n  Patrycja Jakim\\'ow, Joanna Komoszy\\'nska, Weronika Michalska, Anna Dutkowiak,\n  Adam Polak, Adam Dziadek, Micha{\\l} Ujma", "title": "Consumer Wearables and Affective Computing for Wellbeing Support", "comments": "Article submited to the International Workshop on e-Health and\n  m-Health Technologies for Ambient Assisted Living and Healthcare, EAI\n  MobiQuitous 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearables equipped with pervasive sensors enable us to monitor physiological\nand behavioral signals in our everyday life. We propose the WellAff system able\nto recognize affective states for wellbeing support. It also includes health\ncare scenarios, in particular patients with chronic kidney disease (CKD)\nsuffering from bipolar disorders. For the need of a large-scale field study, we\nrevised over 50 off-the-shelf devices in terms of usefulness for emotion,\nstress, meditation, sleep, and physical activity recognition and analysis.\nTheir usability directly comes from the types of sensors they possess as well\nas the quality and availability of raw signals. We found there is no versatile\ndevice suitable for all purposes. Using Empatica E4 and Samsung Galaxy Watch,\nwe have recorded physiological signals from 11 participants over many weeks.\nThe gathered data enabled us to train a classifier that accurately recognizes\nstrong affective states.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 20:38:54 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 09:53:57 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Saganowski", "Stanis\u0142aw", ""], ["Kazienko", "Przemys\u0142aw", ""], ["Dzie\u017cyc", "Maciej", ""], ["Jakim\u00f3w", "Patrycja", ""], ["Komoszy\u0144ska", "Joanna", ""], ["Michalska", "Weronika", ""], ["Dutkowiak", "Anna", ""], ["Polak", "Adam", ""], ["Dziadek", "Adam", ""], ["Ujma", "Micha\u0142", ""]]}, {"id": "2005.00127", "submitter": "Hans Dermot Doran", "authors": "Hans Dermot Doran, Monika Reif, Marco Oehler, Curdin Stoehr, Pierluigi\n  Capone", "title": "Conceptual Design of Human-Drone Communication in Collaborative\n  Environments", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots and drones will work collaboratively and cooperatively in\ntomorrow's industry and agriculture. Before this becomes a reality, some form\nof standardised communication between man and machine must be established that\nspecifically facilitates communication between autonomous machines and both\ntrained and untrained human actors in the working environment. We present\npreliminary results on a human-drone and a drone-human language situated in the\nagricultural industry where interactions with trained and untrained workers and\nvisitors can be expected. We present basic visual indicators enhanced with\nflight patterns for drone-human interaction and human signaling based on\naircraft marshaling for humane-drone interaction. We discuss preliminary\nresults on image recognition and future work.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 22:20:56 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Doran", "Hans Dermot", ""], ["Reif", "Monika", ""], ["Oehler", "Marco", ""], ["Stoehr", "Curdin", ""], ["Capone", "Pierluigi", ""]]}, {"id": "2005.00160", "submitter": "Jorge Piazentin Ono", "authors": "Jorge Piazentin Ono, Sonia Castelo, Roque Lopez, Enrico Bertini,\n  Juliana Freire, Claudio Silva", "title": "PipelineProfiler: A Visual Analytics Tool for the Exploration of AutoML\n  Pipelines", "comments": "To appear at IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a wide variety of automated machine learning (AutoML)\nmethods have been proposed to search and generate end-to-end learning\npipelines. While these techniques facilitate the creation of models for\nreal-world applications, given their black-box nature, the complexity of the\nunderlying algorithms, and the large number of pipelines they derive, it is\ndifficult for their developers to debug these systems. It is also challenging\nfor machine learning experts to select an AutoML system that is well suited for\na given problem or class of problems. In this paper, we present the\nPipelineProfiler, an interactive visualization tool that allows the exploration\nand comparison of the solution space of machine learning (ML) pipelines\nproduced by AutoML systems. PipelineProfiler is integrated with Jupyter\nNotebook and can be used together with common data science tools to enable a\nrich set of analyses of the ML pipelines and provide insights about the\nalgorithms that generated them. We demonstrate the utility of our tool through\nseveral use cases where PipelineProfiler is used to better understand and\nimprove a real-world AutoML system. Furthermore, we validate our approach by\npresenting a detailed analysis of a think-aloud experiment with six data\nscientists who develop and evaluate AutoML tools.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 00:54:14 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 01:20:37 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Ono", "Jorge Piazentin", ""], ["Castelo", "Sonia", ""], ["Lopez", "Roque", ""], ["Bertini", "Enrico", ""], ["Freire", "Juliana", ""], ["Silva", "Claudio", ""]]}, {"id": "2005.00284", "submitter": "William Seymour", "authors": "William Seymour, Reuben Binns, Petr Slovak, Max Van Kleek, Nigel\n  Shadbolt", "title": "Strangers in the Room: Unpacking Perceptions of 'Smartness' and Related\n  Ethical Concerns in the Home", "comments": "10 pages, 1 figure. To appear in the Proceedings of the 2020 ACM\n  Conference on Designing Interactive Systems (DIS '20)", "journal-ref": null, "doi": "10.1145/3357236.3395501", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasingly widespread use of 'smart' devices has raised multifarious\nethical concerns regarding their use in domestic spaces. Previous work\nexamining such ethical dimensions has typically either involved empirical\nstudies of concerns raised by specific devices and use contexts, or\nalternatively expounded on abstract concepts like autonomy, privacy or trust in\nrelation to 'smart homes' in general. This paper attempts to bridge these\napproaches by asking what features of smart devices users consider as rendering\nthem 'smart' and how these relate to ethical concerns. Through a multimethod\ninvestigation including surveys with smart device users (n=120) and\nsemi-structured interviews (n=15), we identify and describe eight types of\nsmartness and explore how they engender a variety of ethical concerns including\nprivacy, autonomy, and disruption of the social order. We argue that this\nmiddle ground, between concerns arising from particular devices and more\nabstract ethical concepts, can better anticipate potential ethical concerns\nregarding smart devices.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 09:33:42 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Seymour", "William", ""], ["Binns", "Reuben", ""], ["Slovak", "Petr", ""], ["Van Kleek", "Max", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "2005.00324", "submitter": "Lonni Besan\\c{c}on", "authors": "Lonni Besan\\c{c}on, Matthew Cooper, Anders Ynnerman, Fr\\'ed\\'eric\n  Vernier", "title": "An Evaluation of Visualization Methods for Population Statistics Based\n  on Choropleth Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We evaluate several augmentations to the choropleth map to convey additional\ninformation, including glyphs, 3D, cartograms, juxtaposed maps, and shading\nmethods. While choropleth maps are a common method used to represent societal\ndata, with multivariate data they can impede as much as improve understanding.\nIn particular large, low population density regions often dominate the map and\ncan mislead the viewer as to the message conveyed. Our results highlight the\npotential of 3D choropleth maps as well as the low accuracy of choropleth map\ntasks with multivariate data. We also introduce and evaluate popcharts, four\ntechniques designed to show the density of population at a very fine scale on\ntop of choropleth maps. All the data, results, and scripts are available from\nhttps://osf.io/8rxwg/.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 12:11:59 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Besan\u00e7on", "Lonni", ""], ["Cooper", "Matthew", ""], ["Ynnerman", "Anders", ""], ["Vernier", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2005.00387", "submitter": "Ulrik G\\\"unther", "authors": "Ulrik G\\\"unther, Kyle I.S. Harrington, Raimund Dachselt, Ivo F.\n  Sbalzarini", "title": "Bionic Tracking: Using Eye Tracking to Track Biological Cells in Virtual\n  Reality", "comments": "22 pages, 10 figures. Accepted at BioImageComputing workshop at ECCV\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present Bionic Tracking, a novel method for solving biological cell\ntracking problems with eye tracking in virtual reality using commodity\nhardware. Using gaze data, and especially smooth pursuit eye movements, we are\nable to track cells in time series of 3D volumetric datasets. The problem of\ntracking cells is ubiquitous in developmental biology, where large volumetric\nmicroscopy datasets are acquired on a daily basis, often comprising hundreds or\nthousands of time points that span hours or days. The image data, however, is\nonly a means to an end, and scientists are often interested in the\nreconstruction of cell trajectories and cell lineage trees. Reliably tracking\ncells in crowded three-dimensional space over many timepoints remains an open\nproblem, and many current approaches rely on tedious manual annotation and\ncuration. In our Bionic Tracking approach, we substitute the usual 2D\npoint-and-click annotation to track cells with eye tracking in a virtual\nreality headset, where users simply have to follow a cell with their eyes in 3D\nspace in order to track it. We detail the interaction design of our approach\nand explain the graph-based algorithm used to connect different time points,\nalso taking occlusion and user distraction into account. We demonstrate our\ncell tracking method using the example of two different biological datasets.\nFinally, we report on a user study with seven cell tracking experts,\ndemonstrating the benefits of our approach over manual point-and-click\ntracking.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:08:40 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 12:00:03 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["G\u00fcnther", "Ulrik", ""], ["Harrington", "Kyle I. S.", ""], ["Dachselt", "Raimund", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "2005.00400", "submitter": "Oliver Hohlfeld", "authors": "Dennis Guse and Oliver Hohlfeld and Anna Wunderlich and Benjamin Weiss\n  and Sebastian M\\\"oller", "title": "Multi-episodic Perceived Quality of an Audio-on-Demand Service", "comments": "To appear at IEEE QoMEX 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.NI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  QoE is traditionally evaluated by using short stimuli usually representing\nparts or single usage episodes. This opens the question on how the overall\nservice perception involving multiple} usage episodes can be evaluated---a\nquestion of high practical relevance to service operators. Despite initial\nresearch on this challenging aspect of multi-episodic perceived quality, the\nquestion of the underlying quality formation processes and its factors are\nstill to be discovered. We present a multi-episodic experiment of an Audio on\nDemand service over a usage period of 6~days with 93 participants. Our work\ndirectly extends prior work investigating the impact of time between usage\nepisodes. The results show similar effects---also the recency effect is not\nstatistically significant. In addition, we extend prediction of multi-episodic\njudgments by accounting for the observed saturation.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:28:27 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Guse", "Dennis", ""], ["Hohlfeld", "Oliver", ""], ["Wunderlich", "Anna", ""], ["Weiss", "Benjamin", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2005.00402", "submitter": "Darren Edge", "authors": "Darren Edge, Jonathan Larson, Nikolay Trandev, Neha Parikh Shah,\n  Carolyn Buractaon, Nicholas Caurvina, Nathan Evans, and Christopher M. White", "title": "Workgroup Mapping: Visual Analysis of Collaboration Culture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digital transformation of work presents new opportunities to understand\nhow informal workgroups organize around the dynamic needs of organizations,\npotentially in contrast to the formal, static, and idealized hierarchies\ndepicted by org charts. We present a design study that spans multiple enabling\ncapabilities for the visual mapping and analysis of organizational workgroups,\nincluding metrics for quantifying two dimensions of collaboration culture: the\nfluidity of collaborative relationships (measured using network machine\nlearning) and the freedom with which workgroups form across organizational\nboundaries. These capabilities come together to create a turnkey pipeline that\ncombines the analysis of a target organization, the generation of data graphics\nand statistics, and their integration in a template-based presentation that\nenables narrative visualization of results. Our metrics and visuals have\nsupported hundreds of presentations to executives of major US-based and\nmultinational organizations, while our engineering practices have created an\nensemble of standalone tools with broad relevance to visualization and visual\nanalytics. We present our work as an example of applied visual analytics\nresearch, describing the design iterations that allowed us to move from\nexperimentation to production, as well as the perspectives of the research team\nand the customer-facing team at each stage in this process.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 14:28:33 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 18:59:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Edge", "Darren", ""], ["Larson", "Jonathan", ""], ["Trandev", "Nikolay", ""], ["Shah", "Neha Parikh", ""], ["Buractaon", "Carolyn", ""], ["Caurvina", "Nicholas", ""], ["Evans", "Nathan", ""], ["White", "Christopher M.", ""]]}, {"id": "2005.00465", "submitter": "Oana Inel", "authors": "Oana Inel, Nava Tintarev and Lora Aroyo", "title": "Eliciting User Preferences for Personalized Explanations for Video\n  Summaries", "comments": "To appear in the Proceedings of the 28th Conference on User Modeling,\n  Adaptation and Personalization, 2020 (UMAP'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video summaries or highlights are a compelling alternative for exploring and\ncontextualizing unprecedented amounts of video material. However, the\nsummarization process is commonly automatic, non-transparent and potentially\nbiased towards particular aspects depicted in the original video. Therefore,\nour aim is to help users like archivists or collection managers to quickly\nunderstand which summaries are the most representative for an original video.\nIn this paper, we present empirical results on the utility of different types\nof visual explanations to achieve transparency for end users on how\nrepresentative video summaries are, with respect to the original video. We\nconsider four types of video summary explanations, which use in different ways\nthe concepts extracted from the original video subtitles and the video stream,\nand their prominence. The explanations are generated to meet target user\npreferences and express different dimensions of transparency: concept\nprominence, semantic coverage, distance and quantity of coverage. In two user\nstudies we evaluate the utility of the visual explanations for achieving\ntransparency for end users. Our results show that explanations representing all\nof the dimensions have the highest utility for transparency, and consequently,\nfor understanding the representativeness of video summaries.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 15:59:31 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Inel", "Oana", ""], ["Tintarev", "Nava", ""], ["Aroyo", "Lora", ""]]}, {"id": "2005.00497", "submitter": "Hubert Baniecki", "authors": "Hubert Baniecki, Przemyslaw Biecek", "title": "The Grammar of Interactive Explanatory Model Analysis", "comments": "17 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing need for in-depth analysis of predictive models leads to a series\nof new methods for explaining their local and global properties. Which of these\nmethods is the best? It turns out that this is an ill-posed question. One\ncannot sufficiently explain a black-box machine learning model using a single\nmethod that gives only one perspective. Isolated explanations are prone to\nmisunderstanding, which inevitably leads to wrong or simplistic reasoning. This\nproblem is known as the Rashomon effect and refers to diverse, even\ncontradictory interpretations of the same phenomenon. Surprisingly, the\nmajority of methods developed for explainable machine learning focus on a\nsingle aspect of the model behavior. In contrast, we showcase the problem of\nexplainability as an interactive and sequential analysis of a model. This paper\npresents how different Explanatory Model Analysis (EMA) methods complement each\nother and why it is essential to juxtapose them together. The introduced\nprocess of Interactive EMA (IEMA) derives from the algorithmic side of\nexplainable machine learning and aims to embrace ideas developed in cognitive\nsciences. We formalize the grammar of IEMA to describe potential human-model\ndialogues. IEMA is implemented in the human-centered framework that adopts\ninteractivity, customizability and automation as its main traits. Combined,\nthese methods enhance the responsible approach to predictive modeling.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:12:22 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 17:55:17 GMT"}, {"version": "v3", "created": "Sun, 28 Mar 2021 16:10:54 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Baniecki", "Hubert", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "2005.00595", "submitter": "Fritz Lekschas", "authors": "Fritz Lekschas, Xinyi Zhou, Wei Chen, Nils Gehlenborg, Benjamin Bach,\n  Hanspeter Pfister", "title": "A Generic Framework and Library for Exploration of Small Multiples\n  through Interactive Piling", "comments": "- Extended Section 4 to improve the clarity of our rationale -\n  Expanded Section 7 to elaborate on the intended target user, the lessons\n  learned from implementing the use cases, and the limitations of visual piling\n  interfaces - Added Figure S1 and S4 and Table S1 to the supplementary\n  material - Improved the clarity of our writing in several other sections, and\n  we corrected grammar and typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Small multiples are miniature representations of visual information used\ngenerically across many domains. Handling large numbers of small multiples\nimposes challenges on many analytic tasks like inspection, comparison,\nnavigation, or annotation. To address these challenges, we developed a\nframework and implemented a library called Piling.js for designing interactive\npiling interfaces. Based on the piling metaphor, such interfaces afford\nflexible organization, exploration, and comparison of large numbers of small\nmultiples by interactively aggregating visual objects into piles. Based on a\nsystematic analysis of previous work, we present a structured design space to\nguide the design of visual piling interfaces. To enable designers to\nefficiently build their own visual piling interfaces, Piling.js provides a\ndeclarative interface to avoid having to write low-level code and implements\ncommon aspects of the design space. An accompanying GUI additionally supports\nthe dynamic configuration of the piling interface. We demonstrate the\nexpressiveness of Piling.js with examples from machine learning,\nimmunofluorescence microscopy, genomics, and public health.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 20:29:32 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 20:39:09 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Lekschas", "Fritz", ""], ["Zhou", "Xinyi", ""], ["Chen", "Wei", ""], ["Gehlenborg", "Nils", ""], ["Bach", "Benjamin", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2005.00670", "submitter": "Morihiro Mizutani", "authors": "Morihiro Mizutani, Akifumi Okuno, Geewook Kim, Hidetoshi Shimodaira", "title": "Stochastic Neighbor Embedding of Multimodal Relational Data for\n  Image-Text Simultaneous Visualization", "comments": "20 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal relational data analysis has become of increasing importance in\nrecent years, for exploring across different domains of data, such as images\nand their text tags obtained from social networking services (e.g., Flickr). A\nvariety of data analysis methods have been developed for visualization; to give\nan example, t-Stochastic Neighbor Embedding (t-SNE) computes low-dimensional\nfeature vectors so that their similarities keep those of the observed data\nvectors. However, t-SNE is designed only for a single domain of data but not\nfor multimodal data; this paper aims at visualizing multimodal relational data\nconsisting of data vectors in multiple domains with relations across these\nvectors. By extending t-SNE, we herein propose Multimodal Relational Stochastic\nNeighbor Embedding (MR-SNE), that (1) first computes augmented relations, where\nwe observe the relations across domains and compute those within each of\ndomains via the observed data vectors, and (2) jointly embeds the augmented\nrelations to a low-dimensional space. Through visualization of Flickr and\nAnimal with Attributes 2 datasets, proposed MR-SNE is compared with other graph\nembedding-based approaches; MR-SNE demonstrates the promising performance.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 00:39:29 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mizutani", "Morihiro", ""], ["Okuno", "Akifumi", ""], ["Kim", "Geewook", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "2005.00698", "submitter": "Satya P. Singh Ph.D.", "authors": "Satya P. Singh, Aim\\'e Lay-Ekuakille, Deepak Gangwar, Madan Kumar\n  Sharma, Sukrit Gupta", "title": "Deep ConvLSTM with self-attention for human activity decoding using\n  wearables", "comments": "8 pages, 2 figures, 3 tables. IEEE Sensors Journal, 2020", "journal-ref": null, "doi": "10.1109/JSEN.2020.3045135", "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decoding human activity accurately from wearable sensors can aid in\napplications related to healthcare and context awareness. The present\napproaches in this domain use recurrent and/or convolutional models to capture\nthe spatio-temporal features from time-series data from multiple sensors. We\npropose a deep neural network architecture that not only captures the\nspatio-temporal features of multiple sensor time-series data but also selects,\nlearns important time points by utilizing a self-attention mechanism. We show\nthe validity of the proposed approach across different data sampling strategies\non six public datasets and demonstrate that the self-attention mechanism gave a\nsignificant improvement in performance over deep networks using a combination\nof recurrent and convolution networks. We also show that the proposed approach\ngave a statistically significant performance enhancement over previous\nstate-of-the-art methods for the tested datasets. The proposed methods open\navenues for better decoding of human activity from multiple body sensors over\nextended periods of time. The code implementation for the proposed model is\navailable at https://github.com/isukrit/encodingHumanActivity.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 04:30:31 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 03:08:37 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Singh", "Satya P.", ""], ["Lay-Ekuakille", "Aim\u00e9", ""], ["Gangwar", "Deepak", ""], ["Sharma", "Madan Kumar", ""], ["Gupta", "Sukrit", ""]]}, {"id": "2005.00749", "submitter": "Zheng Wang", "authors": "Jie Ren, Lu Yuan, Petteri Nurmi, Xiaoming Wang, Miao Ma, Ling Gao,\n  Zhanyong Tang, Jie Zheng, Zheng Wang", "title": "Smart, Adaptive Energy Optimization for Mobile Web Interactions", "comments": "Accepted to be published at INFOCOM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Web technology underpins many interactive mobile applications. However,\nenergy-efficient mobile web interactions is an outstanding challenge. Given the\nincreasing diversity and complexity of mobile hardware, any practical\noptimization scheme must work for a wide range of users, mobile platforms and\nweb workloads. This paper presents CAMEL , a novel energy optimization system\nfor mobile web interactions. CAMEL leverages machine learning techniques to\ndevelop a smart, adaptive scheme to judiciously trade performance for reduced\npower consumption. Unlike prior work, C AMEL directly models how a given web\ncontent affects the user expectation and uses this to guide energy\noptimization. It goes further by employing transfer learning and conformal\npredictions to tune a previously learned model in the end-user environment and\nimprove it over time. We apply CAMEL to Chromium and evaluate it on four\ndistinct mobile systems involving 1,000 testing webpages and 30 users. Compared\nto four state-of-the-art web-event optimizers, CAMEL delivers 22% more energy\nsavings, but with 49% fewer violations on the quality of user experience, and\nexhibits orders of magnitudes less overhead when targeting a new computing\nenvironment.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 08:51:07 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Ren", "Jie", ""], ["Yuan", "Lu", ""], ["Nurmi", "Petteri", ""], ["Wang", "Xiaoming", ""], ["Ma", "Miao", ""], ["Gao", "Ling", ""], ["Tang", "Zhanyong", ""], ["Zheng", "Jie", ""], ["Wang", "Zheng", ""]]}, {"id": "2005.00777", "submitter": "Shuyue Jia", "authors": "Yimin Hou, Shuyue Jia, Xiangmin Lun, Yan Shi, Yang Li", "title": "Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor\n  Imagery Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognition accuracy and response time are both critically essential ahead of\nbuilding practical electroencephalography (EEG) based brain-computer interface\n(BCI). Recent approaches, however, have either compromised in the\nclassification accuracy or responding time. This paper presents a novel deep\nlearning approach designed towards remarkably accurate and responsive motor\nimagery (MI) recognition based on scalp EEG. Bidirectional Long Short-term\nMemory (BiLSTM) with the Attention mechanism manages to derive relevant\nfeatures from raw EEG signals. The connected graph convolutional neural network\n(GCN) promotes the decoding performance by cooperating with the topological\nstructure of features, which are estimated from the overall data. The\n0.4-second detection framework has shown effective and efficient prediction\nbased on individual and group-wise training, with 98.81% and 94.64% accuracy,\nrespectively, which outperformed all the state-of-the-art studies. The\nintroduced deep feature mining approach can precisely recognize human motion\nintents from raw EEG signals, which paves the road to translate the EEG based\nMI recognition to practical BCI systems.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 10:03:40 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 03:29:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hou", "Yimin", ""], ["Jia", "Shuyue", ""], ["Lun", "Xiangmin", ""], ["Shi", "Yan", ""], ["Li", "Yang", ""]]}, {"id": "2005.00825", "submitter": "Tetsunari Inamura", "authors": "Tetsunari Inamura and Yoshiaki Mizuchi", "title": "SIGVerse: A cloud-based VR platform for research on social and embodied\n  human-robot interaction", "comments": "16 pages. Under review in Frontiers in Robotics and AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common sense and social interaction related to daily-life environments are\nconsiderably important for autonomous robots, which support human activities.\nOne of the practical approaches for acquiring such social interaction skills\nand semantic information as common sense in human activity is the application\nof recent machine learning techniques. Although recent machine learning\ntechniques have been successful in realizing automatic manipulation and driving\ntasks, it is difficult to use these techniques in applications that require\nhuman-robot interaction experience. Humans have to perform several times over a\nlong term to show embodied and social interaction behaviors to robots or\nlearning systems. To address this problem, we propose a cloud-based immersive\nvirtual reality (VR) platform which enables virtual human-robot interaction to\ncollect the social and embodied knowledge of human activities in a variety of\nsituations. To realize the flexible and reusable system, we develop a real-time\nbridging mechanism between ROS and Unity, which is one of the standard\nplatforms for developing VR applications. We apply the proposed system to a\nrobot competition field named RoboCup@Home to confirm the feasibility of the\nsystem in a realistic human-robot interaction scenario. Through demonstration\nexperiments at the competition, we show the usefulness and potential of the\nsystem for the development and evaluation of social intelligence through\nhuman-robot interaction. The proposed VR platform enables robot systems to\ncollect social experiences with several users in a short time. The platform\nalso contributes in providing a dataset of social behaviors, which would be a\nkey aspect for intelligent service robots to acquire social interaction skills\nbased on machine learning techniques.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 13:02:54 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Inamura", "Tetsunari", ""], ["Mizuchi", "Yoshiaki", ""]]}, {"id": "2005.01020", "submitter": "Mingfei Sun", "authors": "Mingfei Sun, Zhenhui Peng, Meng Xia, Xiaojuan Ma", "title": "Investigating the Effects of Robot Engagement Communication on Learning\n  from Demonstration", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot Learning from Demonstration (RLfD) is a technique for robots to derive\npolicies from instructors' examples. Although the reciprocal effects of student\nengagement on teacher behavior are widely recognized in the educational\ncommunity, it is unclear whether the same phenomenon holds true for RLfD. To\nfill this gap, we first design three types of robot engagement behavior\n(attention, imitation, and a hybrid of the two) based on the learning\nliterature. We then conduct, in a simulation environment, a within-subject user\nstudy to investigate the impact of different robot engagement cues on humans\ncompared to a \"without-engagement\" condition. Results suggest that engagement\ncommunication significantly changes the human's estimation of the robots'\ncapability and significantly raises their expectation towards the learning\noutcomes, even though we do not run actual learning algorithms in the\nexperiments. Moreover, imitation behavior affects humans more than attention\ndoes in all metrics, while their combination has the most profound influences\non humans. We also find that communicating engagement via imitation or the\ncombined behavior significantly improve humans' perception towards the quality\nof demonstrations, even if all demonstrations are of the same quality.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 08:43:38 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Sun", "Mingfei", ""], ["Peng", "Zhenhui", ""], ["Xia", "Meng", ""], ["Ma", "Xiaojuan", ""]]}, {"id": "2005.01121", "submitter": "Eric Monteiro", "authors": "Thomas {\\O}sterlie and Eric Monteiro", "title": "Digital Sand: The Becoming of Digital Representations", "comments": null, "journal-ref": "Information and Organization, 30(1), 2020, p.100275", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The versatility of digital technologies relies on a capacity to represent and\nsubsequently manipulate algorithmically selected physical processes, objects or\nqualities in a domain. Organizationally real digital representations are those\nthat, beyond the mere capacity to, actually get woven into everyday work\npractices. Empirically, we draw on a four-year case study of offshore oil and\ngas production. Our case provides a vivid illustration of Internet of Things\n(IoT) based visualizations and data driven predictions characteristic for\nefforts of digitally transforming industrial process and manufacturing\nenterprises. We contribute by identifying and discussing three mechanisms\nthrough which digital representations become organizationally real: (i) noise\nreduction (the strategies and heuristics to filter out signal from noise), (ii)\nmaterial tethering (grounding the digital representations to a corresponding\nphysical measurement) and (iii) triangulating (in the absence of a direct\ncorrespondence, corroborating digital representations relative to other\nrepresentations).\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 16:01:45 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["\u00d8sterlie", "Thomas", ""], ["Monteiro", "Eric", ""]]}, {"id": "2005.01180", "submitter": "Paul Zikas", "authors": "George Papagiannakis, Paul Zikas, Nick Lydatakis, Steve Kateros, Mike\n  Kentros, Efstratios Geronikolakis, Manos Kamarianakis, Ioanna Kartsonaki,\n  Giannis Evangelou", "title": "MAGES 3.0: Tying the knot of medical VR", "comments": null, "journal-ref": "SIGGRAPH '20: ACM SIGGRAPH 2020 Immersive Pavilion, August 2020,\n  Article No: 6, Pages 1-2", "doi": "10.1145/3388536.3407888", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present MAGES 3.0, a novel Virtual Reality (VR)-based\nauthoring SDK platform for accelerated surgical training and assessment. The\nMAGES Software Development Kit (SDK) allows code-free prototyping of any VR\npsychomotor simulation of medical operations by medical professionals, who\nurgently need a tool to solve the issue of outdated medical training. Our\nplatform encapsulates the following novel algorithmic techniques: a)\ncollaborative networking layer with Geometric Algebra (GA) interpolation engine\nb) supervised machine learning analytics module for real-time recommendations\nand user profiling c) GA deformable cutting and tearing algorithm d) on-the-go\nconfigurable soft body simulation for deformable surfaces.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 20:27:10 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 11:49:27 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Papagiannakis", "George", ""], ["Zikas", "Paul", ""], ["Lydatakis", "Nick", ""], ["Kateros", "Steve", ""], ["Kentros", "Mike", ""], ["Geronikolakis", "Efstratios", ""], ["Kamarianakis", "Manos", ""], ["Kartsonaki", "Ioanna", ""], ["Evangelou", "Giannis", ""]]}, {"id": "2005.01291", "submitter": "Pedram Daee", "authors": "Fabio Colella, Pedram Daee, Jussi Jokinen, Antti Oulasvirta, Samuel\n  Kaski", "title": "Human Strategic Steering Improves Performance of Interactive\n  Optimization", "comments": "10 pages, 5 figures, The paper is published in the proceedings of\n  UMAP 2020. Codes available at\n  https://github.com/fcole90/interactive_bayesian_optimisation", "journal-ref": null, "doi": "10.1145/3340631.3394883", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central concern in an interactive intelligent system is optimization of its\nactions, to be maximally helpful to its human user. In recommender systems for\ninstance, the action is to choose what to recommend, and the optimization task\nis to recommend items the user prefers. The optimization is done based on\nearlier user's feedback (e.g. \"likes\" and \"dislikes\"), and the algorithms\nassume the feedback to be faithful. That is, when the user clicks \"like,\" they\nactually prefer the item. We argue that this fundamental assumption can be\nextensively violated by human users, who are not passive feedback sources.\nInstead, they are in control, actively steering the system towards their goal.\nTo verify this hypothesis, that humans steer and are able to improve\nperformance by steering, we designed a function optimization task where a human\nand an optimization algorithm collaborate to find the maximum of a\n1-dimensional function. At each iteration, the optimization algorithm queries\nthe user for the value of a hidden function $f$ at a point $x$, and the user,\nwho sees the hidden function, provides an answer about $f(x)$. Our study on 21\nparticipants shows that users who understand how the optimization works,\nstrategically provide biased answers (answers not equal to $f(x)$), which\nresults in the algorithm finding the optimum significantly faster. Our work\nhighlights that next-generation intelligent systems will need user models\ncapable of helping users who steer systems to pursue their goals.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 06:56:52 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Colella", "Fabio", ""], ["Daee", "Pedram", ""], ["Jokinen", "Jussi", ""], ["Oulasvirta", "Antti", ""], ["Kaski", "Samuel", ""]]}, {"id": "2005.01292", "submitter": "Niraj Ramesh Dayama", "authors": "Niraj Ramesh Dayama and Morteza Shiripour and Antti Oulasvirta and\n  Evgeny Ivanko and Andreas Karrenbauer", "title": "Foraging-based Optimization of Menu Systems", "comments": "22 pages paper. 6 pages appendix. 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational design of menu systems has been solved in limited cases such as\nthe linear menu (list) as an assignment task, where commands are assigned to\nmenu positions while optimizing for for users selection performance and\ndistance of associated items. We show that this approach falls short with\nlarger, hierarchically organized menu systems, where one must also take into\naccount how users navigate hierarchical structures. This paper presents a novel\ninteger programming formulation that models hierarchical menus as a combination\nof the exact set covering problem and the assignment problem. It organizes\ncommands into ordered groups of ordered groups via a novel objective function\nbased on information foraging theory. It minimizes, on the one hand, the time\nrequired to select a command whose location is known from previous usage and,\non the other, the time wasted in irrelevant parts of the menu while searching\nfor commands whose location is not known. The convergence of these two factors\nyields usable, well-ordered command hierarchies from a single model. In\ngenerated menus, the lead (first) elements of a group or tab are good\nindicators of the remaining contents, thereby facilitating the search process.\nIn a controlled usability evaluation, the performance of computationally\ndesigned menus was 25 faster than existing commercial designs with respect to\nselection time. The algorithm is efficient for large, representative instances\nof the problem. We further show applications in personalization and adaptation\nof menu systems.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 06:59:11 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 06:12:36 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Dayama", "Niraj Ramesh", ""], ["Shiripour", "Morteza", ""], ["Oulasvirta", "Antti", ""], ["Ivanko", "Evgeny", ""], ["Karrenbauer", "Andreas", ""]]}, {"id": "2005.01322", "submitter": "Pawel Swietojanski", "authors": "O. Miksik, I. Munasinghe, J. Asensio-Cubero, S. Reddy Bethi, S-T.\n  Huang, S. Zylfo, X. Liu, T. Nica, A. Mitrocsak, S. Mezza, R. Beard, R. Shi,\n  R. Ng, P. Mediano, Z. Fountas, S-H. Lee, J. Medvesek, H. Zhuang, Y. Rogers,\n  P. Swietojanski", "title": "Building Proactive Voice Assistants: When and How (not) to Interact", "comments": "17 pages, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice assistants have recently achieved remarkable commercial success.\nHowever, the current generation of these devices is typically capable of only\nreactive interactions. In other words, interactions have to be initiated by the\nuser, which somewhat limits their usability and user experience. We propose,\nthat the next generation of such devices should be able to proactively provide\nthe right information in the right way at the right time, without being\nprompted by the user. However, achieving this is not straightforward, since\nthere is the danger it could interrupt what the user is doing too much,\nresulting in it being distracting or even annoying. Furthermore, it could\nunwittingly, reveal sensitive/private information to third parties. In this\nreport, we discuss the challenges of developing proactively initiated\ninteractions, and suggest a framework for when it is appropriate for the device\nto intervene. To validate our design assumptions, we describe firstly, how we\nbuilt a functioning prototype and secondly, a user study that was conducted to\nassess users' reactions and reflections when in the presence of a proactive\nvoice assistant. This pre-print summarises the state, ideas and progress\ntowards a proactive device as of autumn 2018.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 08:42:50 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Miksik", "O.", ""], ["Munasinghe", "I.", ""], ["Asensio-Cubero", "J.", ""], ["Bethi", "S. Reddy", ""], ["Huang", "S-T.", ""], ["Zylfo", "S.", ""], ["Liu", "X.", ""], ["Nica", "T.", ""], ["Mitrocsak", "A.", ""], ["Mezza", "S.", ""], ["Beard", "R.", ""], ["Shi", "R.", ""], ["Ng", "R.", ""], ["Mediano", "P.", ""], ["Fountas", "Z.", ""], ["Lee", "S-H.", ""], ["Medvesek", "J.", ""], ["Zhuang", "H.", ""], ["Rogers", "Y.", ""], ["Swietojanski", "P.", ""]]}, {"id": "2005.01325", "submitter": "Gi-Hwan Shin", "authors": "Gi-Hwan Shin, Minji Lee, Hyeong-Jin Kim, and Seong-Whan Lee", "title": "Prediction of Event Related Potential Speller Performance Using\n  Resting-State EEG", "comments": "Accepted to IEEE EMBC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event-related potential (ERP) speller can be utilized in device control and\ncommunication for locked-in or severely injured patients. However, problems\nsuch as inter-subject performance instability and ERP-illiteracy are still\nunresolved. Therefore, it is necessary to predict classification performance\nbefore performing an ERP speller in order to use it efficiently. In this study,\nwe investigated the correlations with ERP speller performance using a\nresting-state before an ERP speller. In specific, we used spectral power and\nfunctional connectivity according to four brain regions and five frequency\nbands. As a result, the delta power in the frontal region and functional\nconnectivity in the delta, alpha, gamma bands are significantly correlated with\nthe ERP speller performance. Also, we predicted the ERP speller performance\nusing EEG features in the resting-state. These findings may contribute to\ninvestigating the ERP-illiteracy and considering the appropriate alternatives\nfor each user.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 08:50:18 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 08:19:38 GMT"}, {"version": "v3", "created": "Thu, 7 May 2020 04:25:08 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Shin", "Gi-Hwan", ""], ["Lee", "Minji", ""], ["Kim", "Hyeong-Jin", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2005.01351", "submitter": "Purnendu Mishra", "authors": "Purnendu Mishra and Kishor Sarawadekar", "title": "Anchors Based Method for Fingertips Position Estimation from a Monocular\n  RGB Image using Deep Neural Network", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Virtual, augmented, and mixed reality, the use of hand gestures is\nincreasingly becoming popular to reduce the difference between the virtual and\nreal world. The precise location of the fingertip is essential/crucial for a\nseamless experience. Much of the research work is based on using depth\ninformation for the estimation of the fingertips position. However, most of the\nwork using RGB images for fingertips detection is limited to a single finger.\nThe detection of multiple fingertips from a single RGB image is very\nchallenging due to various factors. In this paper, we propose a deep neural\nnetwork (DNN) based methodology to estimate the fingertips position. We\nchristened this methodology as an Anchor based Fingertips Position Estimation\n(ABFPE), and it is a two-step process. The fingertips location is estimated\nusing regression by computing the difference in the location of a fingertip\nfrom the nearest anchor point. The proposed framework performs the best with\nlimited dependence on hand detection results. In our experiments on the\nSCUT-Ego-Gesture dataset, we achieved the fingertips detection error of 2.3552\npixels on a video frame with a resolution of $640 \\times 480$ and about\n$92.98\\%$ of test images have average pixel errors of five pixels.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 09:45:56 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 06:57:58 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Mishra", "Purnendu", ""], ["Sarawadekar", "Kishor", ""]]}, {"id": "2005.01459", "submitter": "Aakash Gautam", "authors": "Aakash Gautam, Deborah Tatar, Steve Harrison", "title": "Crafting, Communality, and Computing: Building on Existing Strengths To\n  Support a Vulnerable Population", "comments": "14 pages, 1 figure. In Proceedings of the 2020 CHI Conference on\n  Human Factors in Computing Systems (CHI'20)", "journal-ref": null, "doi": "10.1145/3313831.3376647", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In Nepal, sex-trafficking survivors and the organizations that support them\nhave limited resources to assist the survivors in their on-going journey\ntowards reintegration. We take an asset-based approach wherein we identify and\nbuild on the strengths possessed by such groups. In this work, we present\nreflections from introducing a voice-annotated web application to a group of\nsurvivors. The web application tapped into and built upon two elements of\npre-existing strengths possessed by the survivors -- the social bond between\nthem and knowledge of crafting as taught to them by the organization. Our\nfindings provide insight into the array of factors influencing how the\nsurvivors act in relation to one another as they created novel use practices\nand adapted the technology. Experience with the application seemed to open\nknowledge of computing as a potential source of strength. Finally, we\narticulate three design desiderata that could help promote communal spaces:\nmake activity perceptible to the group, create appropriable steps, and build in\nfun choices.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 13:17:06 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Gautam", "Aakash", ""], ["Tatar", "Deborah", ""], ["Harrison", "Steve", ""]]}, {"id": "2005.01527", "submitter": "Aaron Steinfeld", "authors": "Aaron Steinfeld and Michael Goodrich", "title": "Proceedings of the 2020 Workshop on Assessing, Explaining, and Conveying\n  Robot Proficiency for Human-Robot Teaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This record contains the proceedings of the 2020 Workshop on Assessing,\nExplaining, and Conveying Robot Proficiency for Human-Robot Teaming, which was\nheld in conjunction with the 2020 ACM/IEEE International Conference on\nHuman-Robot Interaction (HRI). This workshop was originally scheduled to occur\nin Cambridge, UK on March 23, but was moved to a set of online talks due to the\nCOVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:38:35 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 19:41:23 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Steinfeld", "Aaron", ""], ["Goodrich", "Michael", ""]]}, {"id": "2005.01544", "submitter": "Tyler Frasca", "authors": "Tyler Frasca, Evan Krause, Ravenna Thielstrom, Matthias Scheutz", "title": "\"Can you do this?\" Self-Assessment Dialogues with Autonomous Robots\n  Before, During, and After a Mission", "comments": "Presented at the 2020 Workshop on Assessing, Explaining, and\n  Conveying Robot Proficiency for Human-Robot Teaming", "journal-ref": null, "doi": null, "report-no": "RobotProficiency/2020/02", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots with sophisticated capabilities can make it difficult for\nhuman instructors to assess its capabilities and proficiencies. Therefore, it\nis important future robots have the ability to: introspect on their\ncapabilities and assess their task performance. Introspection allows the robot\nto determine what it can accomplish and self-assessment allows the robot\nestimate the likelihood it will accomplish at given task. We introduce a\ngeneral framework for introspection and self-assessment that enables robots to\nhave task and performance-based dialogues before, during, and after a mission.\nWe then realize aspects of the framework in the cognitive robotic DIARC\narchitecture, and finally show a proof-of-concept demonstration on a Nao robot\nshowing its self-assessment capabilities before, during, and after an\ninstructed task.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:56:50 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 19:26:35 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Frasca", "Tyler", ""], ["Krause", "Evan", ""], ["Thielstrom", "Ravenna", ""], ["Scheutz", "Matthias", ""]]}, {"id": "2005.01546", "submitter": "Gertjan J. Burghouts", "authors": "Gertjan J. Burghouts, Albert Huizing, Mark A. Neerincx", "title": "Robotic Self-Assessment of Competence", "comments": "Presented at the 2020 Workshop on Assessing, Explaining, and\n  Conveying Robot Proficiency for Human-Robot Teaming", "journal-ref": null, "doi": null, "report-no": "RobotProficiency/2020/01", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In robotics, one of the main challenges is that the on-board Artificial\nIntelligence (AI) must deal with different or unexpected environments. Such AI\nagents may be incompetent there, while the underlying model itself may not be\naware of this (e.g., deep learning models are often overly confident). This\npaper proposes two methods for the online assessment of the competence of the\nAI model, respectively for situations when nothing is known about competence\nbeforehand, and when there is prior knowledge about competence (in semantic\nform). The proposed method assesses whether the current environment is known.\nIf not, it asks a human for feedback about its competence. If it knows the\nenvironment, it assesses its competence by generalizing from earlier\nexperience. Results on real data show the merit of competence assessment for a\nrobot moving through various environments in which it sometimes is competent\nand at other times it is not competent. We discuss the role of the human in\nrobot's self-assessment of its competence, and the challenges to acquire\ncomplementary information from the human that reinforces the assessments.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 14:57:42 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Burghouts", "Gertjan J.", ""], ["Huizing", "Albert", ""], ["Neerincx", "Mark A.", ""]]}, {"id": "2005.01575", "submitter": "Angelos Chatzimparmpas", "authors": "Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas\n  Kerren", "title": "StackGenVis: Alignment of Data, Algorithms, and Models for Stacking\n  Ensemble Learning Using Performance Metrics", "comments": "This manuscript is accepted for publication in a special issue of\n  IEEE Transactions on Visualization and Computer Graphics Journal (IEEE TVCG)", "journal-ref": "IEEE TVCG 2021, 27(2), 1547-1557", "doi": "10.1109/TVCG.2020.3030352", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning (ML), ensemble methods such as bagging, boosting, and\nstacking are widely-established approaches that regularly achieve top-notch\npredictive performance. Stacking (also called \"stacked generalization\") is an\nensemble method that combines heterogeneous base models, arranged in at least\none layer, and then employs another metamodel to summarize the predictions of\nthose models. Although it may be a highly-effective approach for increasing the\npredictive performance of ML, generating a stack of models from scratch can be\na cumbersome trial-and-error process. This challenge stems from the enormous\nspace of available solutions, with different sets of data instances and\nfeatures that could be used for training, several algorithms to choose from,\nand instantiations of these algorithms using diverse parameters (i.e., models)\nthat perform differently according to various metrics. In this work, we present\na knowledge generation model, which supports ensemble learning with the use of\nvisualization, and a visual analytics system for stacked generalization. Our\nsystem, StackGenVis, assists users in dynamically adapting performance metrics,\nmanaging data instances, selecting the most important features for a given data\nset, choosing a set of top-performant and diverse algorithms, and measuring the\npredictive performance. In consequence, our proposed tool helps users to decide\nbetween distinct models and to reduce the complexity of the resulting stack by\nremoving overpromising and underperforming models. The applicability and\neffectiveness of StackGenVis are demonstrated with two use cases: a real-world\nhealthcare data set and a collection of data related to sentiment/stance\ndetection in texts. Finally, the tool has been evaluated through interviews\nwith three ML experts.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:43:55 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 11:24:33 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 23:11:02 GMT"}, {"version": "v4", "created": "Thu, 20 Aug 2020 13:31:01 GMT"}, {"version": "v5", "created": "Mon, 24 Aug 2020 20:12:29 GMT"}, {"version": "v6", "created": "Fri, 28 Aug 2020 13:25:32 GMT"}, {"version": "v7", "created": "Thu, 17 Sep 2020 05:09:01 GMT"}, {"version": "v8", "created": "Tue, 1 Dec 2020 20:44:22 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Chatzimparmpas", "Angelos", ""], ["Martins", "Rafael M.", ""], ["Kucher", "Kostiantyn", ""], ["Kerren", "Andreas", ""]]}, {"id": "2005.01653", "submitter": "John Kastner", "authors": "Anis Abboud, John Kastner, Hanan Samet", "title": "Equal Area Breaks: A Classification Scheme for Data to Obtain an\n  Evenly-colored Choropleth Map", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient algorithm for computing the choropleth map classification scheme\nknown as equal area breaks or geographical quantiles is introduced. An equal\narea breaks classification aims to obtain a coloring for the map such that the\narea associated with each of the colors is approximately equal. This is meant\nto be an alternative to an approach that assigns an equal number of regions\nwith a particular range of property values to each color, called quantiles,\nwhich could result in the mapped area being dominated by one or a few colors.\nMoreover, it is possible that the other colors are barely discernible. This is\nthe case when some regions are much larger than others (e.g., compare\nSwitzerland with Russia). A number of algorithms of varying computational\ncomplexity are presented to achieve an equal area assignment to regions. They\ninclude a pair of greedy algorithms, as well as an optimal algorithm that is\nbased on dynamic programming. The classification obtained from the optimal\nequal area algorithm is compared with the quantiles and Jenks natural breaks\nalgorithms and found to be superior from a visual standpoint by a user study.\nFinally, a modified approach is presented which enables users to vary the\nextent to which the coloring algorithm satisfies the conflicting goals of equal\narea for each color with that of assigning an equal number of regions to each\ncolor.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 17:05:50 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Abboud", "Anis", ""], ["Kastner", "John", ""], ["Samet", "Hanan", ""]]}, {"id": "2005.01716", "submitter": "Bahareh Sarrafzadeh", "authors": "Bahareh Sarrafzadeh, Adam Roegiest, and Edward Lank", "title": "Hierarchical Knowledge Graphs: A Novel Information Representation for\n  Exploratory Search Tasks", "comments": "35 Pages of main content, Extension of previous work published at\n  SigIR 2017, Currently under review at TOIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exploratory search tasks, alongside information retrieval, information\nrepresentation is an important factor in sensemaking. In this paper, we explore\na multi-layer extension to knowledge graphs, hierarchical knowledge graphs\n(HKGs), that combines hierarchical and network visualizations into a unified\ndata representation asa tool to support exploratory search. We describe our\nalgorithm to construct these visualizations, analyze interaction logs to\nquantitatively demonstrate performance parity with networks and performance\nadvantages over hierarchies, and synthesize data from interaction logs,\ninterviews, and thinkalouds on a testbed data set to demonstrate the utility of\nthe unified hierarchy+network structure in our HKGs. Alongside the above study,\nwe perform an additional mixed methods analysis of the effect of precision and\nrecall on the performance of hierarchical knowledge graphs for two different\nexploratory search tasks. While the quantitative data shows a limited effect of\nprecision and recall on user performance and user effort, qualitative data\ncombined with post-hoc statistical analysis provides evidence that the type of\nexploratory search task (e.g., learning versus investigating) can be impacted\nby precision and recall. Furthermore, our qualitative analyses find that users\nare unable to perceive differences in the quality of extracted information. We\ndiscuss the implications of our results and analyze other factors that more\nsignificantly impact exploratory search performance in our experimental tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 16:04:09 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Sarrafzadeh", "Bahareh", ""], ["Roegiest", "Adam", ""], ["Lank", "Edward", ""]]}, {"id": "2005.01834", "submitter": "Seyed Amir Hossein Aqajari", "authors": "Seyed Amir Hossein Aqajari (1), Emad Kasaeyan Naeini (1), Milad Asgari\n  Mehrabadi (1), Sina Labbaf (1), Amir M. Rahmani (1 and 2), Nikil Dutt (1)\n  ((1) Department of Computer Science, University of California, Irvine, (2)\n  School of Nursing, University of California, Irvine)", "title": "GSR Analysis for Stress: Development and Validation of an Open Source\n  Tool for Noisy Naturalistic GSR Data", "comments": "6 pages and 5 figures. Link to the github of the tool:\n  https://github.com/HealthSciTech/pyEDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stress detection problem is receiving great attention in related research\ncommunities. This is due to its essential part in behavioral studies for many\nserious health problems and physical illnesses. There are different methods and\nalgorithms for stress detection using different physiological signals. Previous\nstudies have already shown that Galvanic Skin Response (GSR), also known as\nElectrodermal Activity (EDA), is one of the leading indicators for stress.\nHowever, the GSR signal itself is not trivial to analyze. Different features\nare extracted from GSR signals to detect stress in people like the number of\npeaks, max peak amplitude, etc. In this paper, we are proposing an open-source\ntool for GSR analysis, which uses deep learning algorithms alongside\nstatistical algorithms to extract GSR features for stress detection. Then we\nuse different machine learning algorithms and Wearable Stress and Affect\nDetection (WESAD) dataset to evaluate our results. The results show that we are\ncapable of detecting stress with the accuracy of 92 percent using 10-fold\ncross-validation and using the features extracted from our tool.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 20:40:39 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 00:47:23 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 19:06:12 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Aqajari", "Seyed Amir Hossein", "", "1 and 2"], ["Naeini", "Emad Kasaeyan", "", "1 and 2"], ["Mehrabadi", "Milad Asgari", "", "1 and 2"], ["Labbaf", "Sina", "", "1 and 2"], ["Rahmani", "Amir M.", "", "1 and 2"], ["Dutt", "Nikil", ""]]}, {"id": "2005.01986", "submitter": "Yukiko Osawa", "authors": "Yukiko Osawa (IDH), Abderrahmane Kheddar (IDH)", "title": "A Soft Robotic Cover with Dual Thermal Display and Sensing Capabilities", "comments": null, "journal-ref": "17th International Symposium on Experimental Robotics, Mar 2020,\n  Floriana, Malta", "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new robotic cover prototype that achieves thermal display while\nalso being soft. We focus on the thermal cue because previous human studies\nhave identified it as part of the touch pleasantness. The robotic cover surface\ncan be regulated to the desired temperature by circulating water through a\nthermally conductive pipe embedded in the cover, of which temperature is\ncontrolled. Besides, an observer for estimating heat from human contact is\nimplemented; it can detect human interaction while displaying the desired\ntemperature without temperature sensing on the surface directly. We assessed\nthe validity of the prototype in experiments of temperature control and contact\ndetection by human hand.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 07:55:18 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 16:07:43 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 09:56:56 GMT"}, {"version": "v4", "created": "Sat, 8 May 2021 00:49:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Osawa", "Yukiko", "", "IDH"], ["Kheddar", "Abderrahmane", "", "IDH"]]}, {"id": "2005.02022", "submitter": "Jessica Cauchard", "authors": "Anna Wojciechowska, Foad Hamidi, Andr\\'es Lucero, Jessica R. Cauchard", "title": "Chasing Lions: Co-Designing Human-Drone Interaction in Sub-Saharan\n  Africa", "comments": "To be published in the ACM conference on Designing Interactive\n  Systems (DIS '20)", "journal-ref": null, "doi": "10.1145/3357236.3395481", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones are an exciting technology that is quickly being adopted in the global\nconsumer market. Africa has become a center of deployment with the first drone\nairport established in Rwanda and drones currently being used for applications\nsuch as medical deliveries, agriculture, and wildlife monitoring. Despite this\nincreasing presence of drones, there is a lack of research on stakeholders'\nperspectives from this region. We ran a human-drone interaction user study\n(N=15) with experts from several sub-Saharan countries using a co-design\nmethodology. Participants described novel applications and identified important\ndesign aspects for the integration of drones in this context. Our results\nhighlight the potential of drones to address real world problems, the need for\nthem to be culturally situated, and the importance of considering the social\naspects of their interaction with humans. This research highlights the need for\ndiverse perspectives in the human-drone interaction design process.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 09:33:41 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 13:53:55 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Wojciechowska", "Anna", ""], ["Hamidi", "Foad", ""], ["Lucero", "Andr\u00e9s", ""], ["Cauchard", "Jessica R.", ""]]}, {"id": "2005.02118", "submitter": "Muhammad E. H. Chowdhury", "authors": "Mahmoud Dahmani, Muhammad E. H. Chowdhury, Amith Khandakar, Tawsifur\n  Rahman, Khaled Al-Jayyousi, Abdalla Hefny, and Serkan Kiranyaz", "title": "An Intelligent and Low-cost Eye-tracking System for Motorized Wheelchair\n  Control", "comments": "Accepted for publication in Sensor, 19 Figure, 3 Tables", "journal-ref": "Sensors 2020, 20(14), 3936", "doi": "10.3390/s20143936", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 34 developed and 156 developing countries, there are about 132 million\ndisabled people who need a wheelchair constituting 1.86% of the world\npopulation. Moreover, there are millions of people suffering from diseases\nrelated to motor disabilities, which cause inability to produce controlled\nmovement in any of the limbs or even head.The paper proposes a system to aid\npeople with motor disabilities by restoring their ability to move effectively\nand effortlessly without having to rely on others utilizing an eye-controlled\nelectric wheelchair. The system input was images of the users eye that were\nprocessed to estimate the gaze direction and the wheelchair was moved\naccordingly. To accomplish such a feat, four user-specific methods were\ndeveloped, implemented and tested; all of which were based on a benchmark\ndatabase created by the authors.The first three techniques were automatic,\nemploy correlation and were variants of template matching, while the last one\nuses convolutional neural networks (CNNs). Different metrics to quantitatively\nevaluate the performance of each algorithm in terms of accuracy and latency\nwere computed and overall comparison is presented. CNN exhibited the best\nperformance (i.e. 99.3% classification accuracy), and thus it was the model of\nchoice for the gaze estimator, which commands the wheelchair motion. The system\nwas evaluated carefully on 8 subjects achieving 99% accuracy in changing\nillumination conditions outdoor and indoor. This required modifying a motorized\nwheelchair to adapt it to the predictions output by the gaze estimation\nalgorithm. The wheelchair control can bypass any decision made by the gaze\nestimator and immediately halt its motion with the help of an array of\nproximity sensors, if the measured distance goes below a well-defined safety\nmargin.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 23:08:33 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Dahmani", "Mahmoud", ""], ["Chowdhury", "Muhammad E. H.", ""], ["Khandakar", "Amith", ""], ["Rahman", "Tawsifur", ""], ["Al-Jayyousi", "Khaled", ""], ["Hefny", "Abdalla", ""], ["Kiranyaz", "Serkan", ""]]}, {"id": "2005.02189", "submitter": "Saad Alqithami", "authors": "Saad Alqithami, Musaad Alzahrani, Abdulkareem Alzahrani and Ahmed\n  Mustafa", "title": "AR-Therapist: Design and Simulation of an AR-Game Environment as a CBT\n  for Patients with ADHD", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.01003", "journal-ref": "Healthcare 2019, 7, 146", "doi": "10.3390/healthcare7040146", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention Deficit Hyperactivity Disorder is one of the most common\nneurodevelopmental disorders in which patients have difficulties related to\ninattention, hyperactivity, and impulsivity. Those patients are in need of a\npsychological therapy use Cognitive Behavioral Therapy (CBT) to enhance the way\nthey think and behave. This type of therapy is mostly common in treating\npatients with anxiety and depression but also is useful in treating autism,\nobsessive compulsive disorder and post-traumatic stress disorder. A major\nlimitation of traditional CBT is that therapists may face difficulty in\noptimizing patients' neuropsychological stimulus following a specified\ntreatment plan. Other limitations include availability, accessibility and\nlevel-of-experience of the therapists. Hence, this paper aims to design and\nsimulate a generic cognitive model that can be used as an appropriate\nalternative treatment to traditional CBT, we term as \"AR-Therapist.\" This model\ntakes advantage of the current developments of augmented reality to engage\npatients in both real and virtual game-based environments.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 22:51:25 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Alqithami", "Saad", ""], ["Alzahrani", "Musaad", ""], ["Alzahrani", "Abdulkareem", ""], ["Mustafa", "Ahmed", ""]]}, {"id": "2005.02251", "submitter": "Vladislav Goncharenko", "authors": "V. Goncharenko, R. Grigoryan, A. Samokhina", "title": "Raccoons vs Demons: multiclass labeled P300 dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We publish dataset of visual P300 BCI performed in Virtual Reality (VR) game\nRaccoons versus Demons (RvD). Data contains reach labels incorporating\ninformation about stimulus chosen enabling us to estimate model's confidence at\neach stimulus prediction stage. Data and experiments code are available at\nhttps://gitlab.com/impulse-neiry_public/raccoons-vs-demons\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 20:10:31 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 15:47:38 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Goncharenko", "V.", ""], ["Grigoryan", "R.", ""], ["Samokhina", "A.", ""]]}, {"id": "2005.02304", "submitter": "Ilhan Aslan", "authors": "Ilhan Aslan, Andreas Seiderer, Chi Tai Dang, Simon R\\\"adler, Elisabeth\n  Andr\\'e", "title": "Resonating Experiences of Self and Others enabled by a Tangible\n  Somaesthetic Design", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitalization is penetrating every aspect of everyday life including a\nhuman's heart beating, which can easily be sensed by wearable sensors and\ndisplayed for others to see, feel, and potentially \"bodily resonate\" with.\nPrevious work in studying human interactions and interaction designs with\nphysiological data, such as a heart's pulse rate, have argued that feeding it\nback to the users may, for example support users' mindfulness and\nself-awareness during various everyday activities and ultimately support their\nwellbeing. Inspired by Somaesthetics as a discipline, which focuses on an\nappreciation of the living body's role in all our experiences, we designed and\nexplored mobile tangible heart beat displays, which enable rich forms of bodily\nexperiencing oneself and others in social proximity. In this paper, we first\nreport on the design process of tangible heart displays and then present\nresults of a field study with 30 pairs of participants. Participants were asked\nto use the tangible heart displays during watching movies together and report\ntheir experience in three different heart display conditions (i.e., displaying\ntheir own heart beat, their partner's heart beat, and watching a movie without\na heart display). We found, for example that participants reported significant\neffects in experiencing sensory immersion when they felt their own heart beats\ncompared to the condition without any heart beat display, and that feeling\ntheir partner's heart beats resulted in significant effects on social\nexperience. We refer to resonance theory to discuss the results, highlighting\nthe potential of how ubiquitous technology could utilize physiological data to\nprovide resonance in a modern society facing social acceleration.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 15:59:23 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Aslan", "Ilhan", ""], ["Seiderer", "Andreas", ""], ["Dang", "Chi Tai", ""], ["R\u00e4dler", "Simon", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "2005.02335", "submitter": "Mahsan Nourani", "authors": "Mahsan Nourani, Chiradeep Roy, Tahrima Rahman, Eric D. Ragan, Nicholas\n  Ruozzi, Vibhav Gogate", "title": "Don't Explain without Verifying Veracity: An Evaluation of Explainable\n  AI with Video Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable machine learning and artificial intelligence models have been\nused to justify a model's decision-making process. This added transparency aims\nto help improve user performance and understanding of the underlying model.\nHowever, in practice, explainable systems face many open questions and\nchallenges. Specifically, designers might reduce the complexity of deep\nlearning models in order to provide interpretability. The explanations\ngenerated by these simplified models, however, might not accurately justify and\nbe truthful to the model. This can further add confusion to the users as they\nmight not find the explanations meaningful with respect to the model\npredictions. Understanding how these explanations affect user behavior is an\nongoing challenge. In this paper, we explore how explanation veracity affects\nuser performance and agreement in intelligent systems. Through a controlled\nuser study with an explainable activity recognition system, we compare\nvariations in explanation veracity for a video review and querying task. The\nresults suggest that low veracity explanations significantly decrease user\nperformance and agreement compared to both accurate explanations and a system\nwithout explanations. These findings demonstrate the importance of accurate and\nunderstandable explanations and caution that poor explanations can sometimes be\nworse than no explanations with respect to their effect on user performance and\nreliance on an AI system.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:06:46 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Nourani", "Mahsan", ""], ["Roy", "Chiradeep", ""], ["Rahman", "Tahrima", ""], ["Ragan", "Eric D.", ""], ["Ruozzi", "Nicholas", ""], ["Gogate", "Vibhav", ""]]}, {"id": "2005.02367", "submitter": "Ting-Hao Huang", "authors": "Ting-Hao 'Kenneth' Huang, Chieh-Yang Huang, Chien-Kuang Cornelia Ding,\n  Yen-Chia Hsu, and C. Lee Giles", "title": "CODA-19: Using a Non-Expert Crowd to Annotate Research Aspects on\n  10,000+ Abstracts in the COVID-19 Open Research Dataset", "comments": "Accepted by the NLP COVID-19 Workshop at ACL 2020. (The data, code,\n  and model are available at: https://github.com/windx0303/CODA-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces CODA-19, a human-annotated dataset that codes the\nBackground, Purpose, Method, Finding/Contribution, and Other sections of 10,966\nEnglish abstracts in the COVID-19 Open Research Dataset. CODA-19 was created by\n248 crowd workers from Amazon Mechanical Turk within 10 days, and achieved\nlabeling quality comparable to that of experts. Each abstract was annotated by\nnine different workers, and the final labels were acquired by majority vote.\nThe inter-annotator agreement (Cohen's kappa) between the crowd and the\nbiomedical expert (0.741) is comparable to inter-expert agreement (0.788).\nCODA-19's labels have an accuracy of 82.2% when compared to the biomedical\nexpert's labels, while the accuracy between experts was 85.0%. Reliable human\nannotations help scientists access and integrate the rapidly accelerating\ncoronavirus literature, and also serve as the battery of AI/NLP research, but\nobtaining expert annotations can be slow. We demonstrated that a non-expert\ncrowd can be rapidly employed at scale to join the fight against COVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 17:51:42 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 14:28:51 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 01:53:51 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 06:22:02 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 22:00:29 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Huang", "Ting-Hao 'Kenneth'", ""], ["Huang", "Chieh-Yang", ""], ["Ding", "Chien-Kuang Cornelia", ""], ["Hsu", "Yen-Chia", ""], ["Giles", "C. Lee", ""]]}, {"id": "2005.02623", "submitter": "Hao Fang", "authors": "Hao Fang", "title": "Building A User-Centric and Content-Driven Socialbot", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build Sounding Board, we develop a system architecture that is capable of\naccommodating dialog strategies that we designed for socialbot conversations.\nThe architecture consists of a multi-dimensional language understanding module\nfor analyzing user utterances, a hierarchical dialog management framework for\ndialog context tracking and complex dialog control, and a language generation\nprocess that realizes the response plan and makes adjustments for speech\nsynthesis. Additionally, we construct a new knowledge base to power the\nsocialbot by collecting social chat content from a variety of sources. An\nimportant contribution of the system is the synergy between the knowledge base\nand the dialog management, i.e., the use of a graph structure to organize the\nknowledge base that makes dialog control very efficient in bringing related\ncontent to the discussion. Using the data collected from Sounding Board during\nthe competition, we carry out in-depth analyses of socialbot conversations and\nuser ratings which provide valuable insights in evaluation methods for\nsocialbots. We additionally investigate a new approach for system evaluation\nand diagnosis that allows scoring individual dialog segments in the\nconversation. Finally, observing that socialbots suffer from the issue of\nshallow conversations about topics associated with unstructured data, we study\nthe problem of enabling extended socialbot conversations grounded on a\ndocument. To bring together machine reading and dialog control techniques, a\ngraph-based document representation is proposed, together with methods for\nautomatically constructing the graph. Using the graph-based representation,\ndialog control can be carried out by retrieving nodes or moving along edges in\nthe graph. To illustrate the usage, a mixed-initiative dialog strategy is\ndesigned for socialbot conversations on news articles.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 07:11:57 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Fang", "Hao", ""]]}, {"id": "2005.02824", "submitter": "Maryam Alimardani", "authors": "Andrea Kuijt and Maryam Alimardani", "title": "Prediction of Human Empathy based on EEG Cortical Asymmetry", "comments": "Accepted in the 1st IEEE International Conference on Human-Machine\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans constantly interact with digital devices that disregard their\nfeelings. However, the synergy between human and technology can be strengthened\nif the technology is able to distinguish and react to human emotions. Models\nthat rely on unconscious indications of human emotions, such as\n(neuro)physiological signals, hold promise in personalization of feedback and\nadaptation of the interaction. The current study elaborated on adopting a\npredictive approach in studying human emotional processing based on brain\nactivity. More specifically, we investigated the proposition of predicting\nself-reported human empathy based on EEG cortical asymmetry in different areas\nof the brain. Different types of predictive models i.e. multiple linear\nregression analyses as well as binary and multiclass classifications were\nevaluated. Results showed that lateralization of brain oscillations at specific\nfrequency bands is an important predictor of self-reported empathy scores.\nAdditionally, prominent classification performance was found during\nresting-state which suggests that emotional stimulation is not required for\naccurate prediction of empathy -- as a personality trait -- based on EEG data.\nOur findings not only contribute to the general understanding of the mechanisms\nof empathy, but also facilitate a better grasp on the advantages of applying a\npredictive approach compared to hypothesis-driven studies in neuropsychological\nresearch. More importantly, our results could be employed in the development of\nbrain-computer interfaces that assist people with difficulties in expressing or\nrecognizing emotions.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 13:49:56 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Kuijt", "Andrea", ""], ["Alimardani", "Maryam", ""]]}, {"id": "2005.02972", "submitter": "Jean-Daniel Fekete", "authors": "Alexis Pister, Paolo Buono, Jean-Daniel Fekete, Catherine Plaisant,\n  Paola Valdivia", "title": "Integrating Prior Knowledge in Mixed Initiative Social Network\n  Clustering", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2021", "doi": "10.1109/TVCG.2020.3030347", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new approach -- called PK-clustering -- to help social\nscientists create meaningful clusters in social networks. Many clustering\nalgorithms exist but most social scientists find them difficult to understand,\nand tools do not provide any guidance to choose algorithms, or to evaluate\nresults taking into account the prior knowledge of the scientists. Our work\nintroduces a new clustering approach and a visual analytics user interface that\naddress this issue. It is based on a process that 1) captures the prior\nknowledge of the scientists as a set of incomplete clusters, 2) runs multiple\nclustering algorithms (similarly to clustering ensemble methods), 3) visualizes\nthe results of all the algorithms ranked and summarized by how well each\nalgorithm matches the prior knowledge, 4) evaluates the consensus between\nuser-selected algorithms, and 5) allows users to review details and iteratively\nupdate the acquired knowledge. We describe our approach using an initial\nfunctional prototype, then provide two examples of use and early feedback from\nsocial scientists. We believe our clustering approach offers a novel\nconstructive method to iteratively build knowledge while avoiding being overly\ninfluenced by the results of often randomly selected black-box clustering\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 17:26:07 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 15:19:40 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Pister", "Alexis", ""], ["Buono", "Paolo", ""], ["Fekete", "Jean-Daniel", ""], ["Plaisant", "Catherine", ""], ["Valdivia", "Paola", ""]]}, {"id": "2005.03011", "submitter": "Mohamed ElHelw", "authors": "Mohamed A. ElHelw", "title": "Overview of Surgical Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the current demand of clinical governance, surgical simulation\nis now a well-established modality for basic skills training and assessment.\nThe practical deployment of the technique is a multi-disciplinary venture\nencompassing areas in engineering, medicine and psychology. This paper provides\nan overview of the key topics involved in surgical simulation and associated\ntechnical challenges. The paper discusses the clinical motivation for surgical\nsimulation, the use of virtual environments for surgical training, model\nacquisition and simplification, deformable models, collision detection, tissue\nproperty measurement, haptic rendering and image synthesis. Additional topics\ninclude surgical skill training and assessment metrics as well as challenges\nfacing the incorporation of surgical simulation into medical education\ncurricula.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 11:51:31 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["ElHelw", "Mohamed A.", ""]]}, {"id": "2005.03068", "submitter": "Akash Deep Singh", "authors": "Akash Deep Singh, Luis Garcia, Joseph Noor, Mani Srivastava", "title": "I Always Feel Like Somebody's Sensing Me! A Framework to Detect,\n  Identify, and Localize Clandestine Wireless Sensors", "comments": "Accepted at USENIX Security 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing ubiquity of low-cost wireless sensors has enabled users to\neasily deploy systems to remotely monitor and control their environments.\nHowever, this raises privacy concerns for third-party occupants, such as a\nhotel room guest who may be unaware of deployed clandestine sensors. Previous\nmethods focused on specific modalities such as detecting cameras but do not\nprovide a generalized and comprehensive method to capture arbitrary sensors\nwhich may be \"spying\" on a user. In this work, we propose SnoopDog, a framework\nto not only detect common Wi-Fi-based wireless sensors that are actively\nmonitoring a user, but also classify and localize each device. SnoopDog works\nby establishing causality between patterns in observable wireless traffic and a\ntrusted sensor in the same space, e.g., an inertial measurement unit (IMU) that\ncaptures a user's movement. Once causality is established, SnoopDog performs\npacket inspection to inform the user about the monitoring device. Finally,\nSnoopDog localizes the clandestine device in a 2D plane using a novel\ntrial-based localization technique. We evaluated SnoopDog across several\ndevices and various modalities and were able to detect causality for snooping\ndevices 95.2% of the time and localize devices to a sufficiently reduced\nsub-space.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 18:41:05 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 08:22:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Singh", "Akash Deep", ""], ["Garcia", "Luis", ""], ["Noor", "Joseph", ""], ["Srivastava", "Mani", ""]]}, {"id": "2005.03174", "submitter": "Ryota Tanaka", "authors": "Ryota Tanaka, Akinobu Lee", "title": "Fact-based Dialogue Generation with Convergent and Divergent Decoding", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fact-based dialogue generation is a task of generating a human-like response\nbased on both dialogue context and factual texts. Various methods were proposed\nto focus on generating informative words that contain facts effectively.\nHowever, previous works implicitly assume a topic to be kept on a dialogue and\nusually converse passively, therefore the systems have a difficulty to generate\ndiverse responses that provide meaningful information proactively. This paper\nproposes an end-to-end fact-based dialogue system augmented with the ability of\nconvergent and divergent thinking over both context and facts, which can\nconverse about the current topic or introduce a new topic. Specifically, our\nmodel incorporates a novel convergent and divergent decoding that can generate\ninformative and diverse responses considering not only given inputs (context\nand facts) but also inputs-related topics. Both automatic and human evaluation\nresults on DSTC7 dataset show that our model significantly outperforms\nstate-of-the-art baselines, indicating that our model can generate more\nappropriate, informative, and diverse responses.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 23:49:35 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 00:43:36 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Tanaka", "Ryota", ""], ["Lee", "Akinobu", ""]]}, {"id": "2005.03244", "submitter": "Dong Sun", "authors": "Dong Sun, Zezheng Feng, Yuanzhe Chen, Yong Wang, Jia Zeng, Mingxuan\n  Yuan, Ting-Chuen Pong and Huamin Qu", "title": "DFSeer: A Visual Analytics Approach to Facilitate Model Selection for\n  Demand Forecasting", "comments": "10 pages, 5 figures, ACM CHI 2020", "journal-ref": null, "doi": "10.1145/3313831.3376866", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting an appropriate model to forecast product demand is critical to the\nmanufacturing industry. However, due to the data complexity, market uncertainty\nand users' demanding requirements for the model, it is challenging for demand\nanalysts to select a proper model. Although existing model selection methods\ncan reduce the manual burden to some extent, they often fail to present model\nperformance details on individual products and reveal the potential risk of the\nselected model. This paper presents DFSeer, an interactive visualization system\nto conduct reliable model selection for demand forecasting based on the\nproducts with similar historical demand. It supports model comparison and\nselection with different levels of details. Besides, it shows the difference in\nmodel performance on similar products to reveal the risk of model selection and\nincrease users' confidence in choosing a forecasting model. Two case studies\nand interviews with domain experts demonstrate the effectiveness and usability\nof DFSeer.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 04:34:18 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Sun", "Dong", ""], ["Feng", "Zezheng", ""], ["Chen", "Yuanzhe", ""], ["Wang", "Yong", ""], ["Zeng", "Jia", ""], ["Yuan", "Mingxuan", ""], ["Pong", "Ting-Chuen", ""], ["Qu", "Huamin", ""]]}, {"id": "2005.03257", "submitter": "Xiaodong Ge", "authors": "Siwei Fu, Kai Xiong, Xiaodong Ge, Siliang Tang, Wei Chen, Yingcai Wu", "title": "Quda: Natural Language Queries for Visual Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of analytic tasks from free text is critical for\nvisualization-oriented natural language interfaces (V-NLIs) to suggest\neffective visualizations. However, it is challenging due to the ambiguity and\ncomplexity nature of human language. To address this challenge, we present a\nnew dataset, called Quda, that aims to help V-NLIs recognize analytic tasks\nfrom free-form natural language by training and evaluating cutting-edge\nmulti-label classification models. Our dataset contains $14,035$ diverse user\nqueries, and each is annotated with one or multiple analytic tasks. We achieve\nthis goal by first gathering seed queries with data analysts and then employing\nextensive crowd force for paraphrase generation and validation. We demonstrate\nthe usefulness of Quda through three applications. This work is the first\nattempt to construct a large-scale corpus for recognizing analytic tasks. With\nthe release of Quda, we hope it will boost the research and development of\nV-NLIs in data analysis and visualization.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 05:35:16 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 16:00:51 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 12:45:39 GMT"}, {"version": "v4", "created": "Sun, 23 Aug 2020 07:34:50 GMT"}, {"version": "v5", "created": "Thu, 3 Dec 2020 06:58:56 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fu", "Siwei", ""], ["Xiong", "Kai", ""], ["Ge", "Xiaodong", ""], ["Tang", "Siliang", ""], ["Chen", "Wei", ""], ["Wu", "Yingcai", ""]]}, {"id": "2005.03503", "submitter": "David Glowacki", "authors": "Rhoslyn Roebuck Williams, Xan Varcoe, Becca R. Glowacki, Ella M. Gale,\n  Alexander Jamieson-Binnie, David R. Glowacki", "title": "Subtle Sensing: Detecting Differences in the Flexibility of Virtually\n  Simulated Molecular Objects", "comments": null, "journal-ref": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors\n  in Computing Systems", "doi": "10.1145/3334480.3383026", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  During VR demos we have performed over last few years, many participants (in\nthe absence of any haptic feedback) have commented on their perceived ability\nto 'feel' differences between simulated molecular objects. The mechanisms for\nsuch 'feeling' are not entirely clear: observing from outside VR, one can see\nthat there is nothing physical for participants to 'feel'. Here we outline\nexploratory user studies designed to evaluate the extent to which participants\ncan distinguish quantitative differences in the flexibility of VR-simulated\nmolecular objects. The results suggest that an individual's capacity to detect\ndifferences in molecular flexibility is enhanced when they can interact with\nand manipulate the molecules, as opposed to merely observing the same\ninteraction. Building on these results, we intend to carry out further studies\ninvestigating humans' ability to sense quantitative properties of VR\nsimulations without haptic technology.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:05:57 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Williams", "Rhoslyn Roebuck", ""], ["Varcoe", "Xan", ""], ["Glowacki", "Becca R.", ""], ["Gale", "Ella M.", ""], ["Jamieson-Binnie", "Alexander", ""], ["Glowacki", "David R.", ""]]}, {"id": "2005.03504", "submitter": "Maxime Ambard", "authors": "Maxime Ambard", "title": "Sunny Pointer: Designing a mouse pointer for people with peripheral\n  vision loss", "comments": "15 pages, 10 figures, 3 experiments", "journal-ref": "Assistive Technology; Taylor & Francis; 2021", "doi": "10.1080/10400435.2021.1872735", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new mouse cursor designed to facilitate the use of the mouse by\npeople with peripheral vision loss. The pointer consists of a collection of\nconverging straight lines covering the whole screen and following the position\nof the mouse cursor. We measured its positive effects with a group of\nparticipants with peripheral vision loss of different kinds and we found that\nit can reduce by a factor of 7 the time required to complete a targeting task\nusing the mouse. Using eye tracking, we show that this system makes it possible\nto initiate the movement towards the target without having to precisely locate\nthe mouse pointer. Using Fitts' Law, we compare these performances with those\nof full visual field users in order to understand the relation between the\naccuracy of the estimated mouse cursor position and the index of performance\nobtained with our tool.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:07:01 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ambard", "Maxime", ""]]}, {"id": "2005.03531", "submitter": "Noemi Mauro", "authors": "Noemi Mauro, Liliana Ardissono and Maurizio Lucenteforte", "title": "Faceted Search of Heterogeneous Geographic Information for Dynamic Map\n  Projection", "comments": null, "journal-ref": "Information Processing & Management, Volume 57, Issue 4, 2020", "doi": "10.1016/j.ipm.2020.102257", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper proposes a faceted information exploration model that supports\ncoarse-grained and fine-grained focusing of geographic maps by offering a\ngraphical representation of data attributes within interactive widgets. The\nproposed approach enables (i) a multi-category projection of long-lasting\ngeographic maps, based on the proposal of efficient facets for data exploration\nin sparse and noisy datasets, and (ii) an interactive representation of the\nsearch context based on widgets that support data visualization, faceted\nexploration, category-based information hiding and transparency of results at\nthe same time. The integration of our model with a semantic representation of\ngeographical knowledge supports the exploration of information retrieved from\nheterogeneous data sources, such as Public Open Data and OpenStreetMap. We\nevaluated our model with users in the OnToMap collaborative Web GIS. The\nexperimental results show that, when working on geographic maps populated with\nmultiple data categories, it outperforms simple category-based map projection\nand traditional faceted search tools, such as checkboxes, in both user\nperformance and experience.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:55:39 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Mauro", "Noemi", ""], ["Ardissono", "Liliana", ""], ["Lucenteforte", "Maurizio", ""]]}, {"id": "2005.03534", "submitter": "Aakash Gautam", "authors": "Aakash Gautam, Deborah Tatar", "title": "p for political: Participation Without Agency Is Not Enough", "comments": "5 pages, 1 figure. Accepted in the 16th Participatory Design\n  Conference (PDC'20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Participatory Design's vision of democratic participation assumes\nparticipants' feelings of agency in envisioning a collective future. But this\nassumption may be leaky when dealing with vulnerable populations. We reflect on\nthe results of a series of activities aimed at supporting\nagentic-future-envisionment with a group of sex-trafficking survivors in Nepal.\nWe observed a growing sense among the survivors that they could play a role in\nbringing about change in their families. They also became aware of how they\ncould interact with available institutional resources. Reflecting on the\nobservations, we argue that building participant agency on the small and\npersonal interactions is necessary before demanding larger Political\nparticipation. In particular, a value of PD, especially for vulnerable\npopulations, can lie in the process itself if it helps participants position\nthemselves as actors in the larger world.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 14:59:59 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Gautam", "Aakash", ""], ["Tatar", "Deborah", ""]]}, {"id": "2005.03652", "submitter": "Deepak Edakkattil Gopinath", "authors": "Deepak E. Gopinath and Brenna D. Argall", "title": "Active Intent Disambiguation for Shared Control Robots", "comments": null, "journal-ref": null, "doi": "10.1109/TNSRE.2020.2987878", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assistive shared-control robots have the potential to transform the lives of\nmillions of people afflicted with severe motor impairments. The usefulness of\nshared-control robots typically relies on the underlying autonomy's ability to\ninfer the user's needs and intentions, and the ability to do so unambiguously\nis often a limiting factor for providing appropriate assistance confidently and\naccurately. The contributions of this paper are four-fold. First, we introduce\nthe idea of intent disambiguation via control mode selection, and present a\nmathematical formalism for the same. Second, we develop a control mode\nselection algorithm which selects the control mode in which the user-initiated\nmotion helps the autonomy to maximally disambiguate user intent. Third, we\npresent a pilot study with eight subjects to evaluate the efficacy of the\ndisambiguation algorithm. Our results suggest that the disambiguation system\n(a) helps to significantly reduce task effort, as measured by number of button\npresses, and (b) is of greater utility for more limited control interfaces and\nmore complex tasks. We also observe that (c) subjects demonstrated a wide range\nof disambiguation request behaviors, with the common thread of concentrating\nrequests early in the execution. As our last contribution, we introduce a novel\nfield-theoretic approach to intent inference inspired by dynamic field theory\nthat works in tandem with the disambiguation scheme.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:58:12 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Gopinath", "Deepak E.", ""], ["Argall", "Brenna D.", ""]]}, {"id": "2005.03739", "submitter": "Pengcheng An", "authors": "Nine Sellier, Pengcheng An", "title": "How Peripheral Interactive Systems Can Support Teachers with\n  Differentiated Instruction: Using FireFlies as a Probe", "comments": "To be published in Proceedings of the 2020 on Designing Interactive\n  Systems Conference - DIS 2020, 13 pages, 3 figures", "journal-ref": null, "doi": "10.1145/3357236.3395497", "report-no": null, "categories": "physics.ed-ph cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teachers' response to the real-time needs of diverse learners in the\nclassroom is important for each learner's success. Teachers who give\ndifferentiated instruction (DI) provide pertinent support to each student and\nacknowledge their differences in learning style and pace. However, due to the\nalready complex and intensive routines in classrooms, it is demanding and\ntime-consuming for teachers to implement DI on-the-spot. This study aims to\nexplore how to ease teachers' classroom differentiation by enabling effortless,\nlow-threshold student-teacher communications through a peripheral interactive\nsystem. Namely, we present a six-week study, in which we iteratively\nco-designed and field-tested interaction solutions with eight school teachers,\nusing a set of distributed, interactive LED-objects (the 'FireFlies' platform).\nBy connecting our findings to the theories of DI, we contribute empirical\nknowledge about the advantages and limitations of a peripheral interactive\nsystem in supporting DI. Taken together, we summarize concrete opportunities\nand recommendations for future design.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 20:38:40 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Sellier", "Nine", ""], ["An", "Pengcheng", ""]]}, {"id": "2005.03747", "submitter": "Mine Sarac Stroppa", "authors": "Mine Sarac, Massimiliano Solazzi, Edoardo Sotgiu, Massimo Bergamasco,\n  Antonio Frisoli", "title": "Design and Kinematic Optimization of a Novel Underactuated Robotic Hand\n  Exoskeleton", "comments": "12 pages", "journal-ref": "Meccanica, 2015", "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents the design and the kinematic optimization of a novel,\nunderactuated, linkage-based robotic hand exoskeleton to assist users in\nperforming grasping tasks. The device has been designed to apply only normal\nforces to the finger phalanges during flexion/extension of the fingers, while\nproviding automatic adaptability for different finger sizes. Thus, the easiness\nof the attachment to the user's fingers and better comfort have been ensured.\nThe analyses of the device kinematic pose, statics, and stability of grasp have\nbeen performed. These analyses have been used to optimize the link lengths of\nthe mechanism, ensuring that a reasonable range of motion is satisfied while\nmaximizing the force transmission on the finger joints. Finally, the usability\nof a prototype with multiple fingers has been tested during grasping tasks with\ndifferent objects.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 20:48:10 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Sarac", "Mine", ""], ["Solazzi", "Massimiliano", ""], ["Sotgiu", "Edoardo", ""], ["Bergamasco", "Massimo", ""], ["Frisoli", "Antonio", ""]]}, {"id": "2005.03784", "submitter": "Yang Li", "authors": "Arianna Yuan, Yang Li", "title": "Modeling Human Visual Search Performance on Realistic Webpages Using\n  Analytical and Deep Learning Methods", "comments": "the 2020 CHI Conference on Human Factors in Computing Systems", "journal-ref": "CHI 2020", "doi": "10.1145/3313831.3376870", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling visual search not only offers an opportunity to predict the\nusability of an interface before actually testing it on real users, but also\nadvances scientific understanding about human behavior. In this work, we first\nconduct a set of analyses on a large-scale dataset of visual search tasks on\nrealistic webpages. We then present a deep neural network that learns to\npredict the scannability of webpage content, i.e., how easy it is for a user to\nfind a specific target. Our model leverages both heuristic-based features such\nas target size and unstructured features such as raw image pixels. This\napproach allows us to model complex interactions that might be involved in a\nrealistic visual search task, which can not be easily achieved by traditional\nanalytical models. We analyze the model behavior to offer our insights into how\nthe salience map learned by the model aligns with human intuition and how the\nlearned semantic representation of each target type relates to its visual\nsearch performance.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 22:21:03 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Yuan", "Arianna", ""], ["Li", "Yang", ""]]}, {"id": "2005.03795", "submitter": "Anuradha Kar", "authors": "Anuradha Kar", "title": "MLGaze: Machine Learning-Based Analysis of Gaze Error Patterns in\n  Consumer Eye Tracking Systems", "comments": "https://github.com/anuradhakar49/MLGaze", "journal-ref": null, "doi": "10.3390/vision4020025", "report-no": null, "categories": "eess.SP cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analyzing the gaze accuracy characteristics of an eye tracker is a critical\ntask as its gaze data is frequently affected by non-ideal operating conditions\nin various consumer eye tracking applications. In this study, gaze error\npatterns produced by a commercial eye tracking device were studied with the\nhelp of machine learning algorithms, such as classifiers and regression models.\nGaze data were collected from a group of participants under multiple conditions\nthat commonly affect eye trackers operating on desktop and handheld platforms.\nThese conditions (referred here as error sources) include user distance, head\npose, and eye-tracker pose variations, and the collected gaze data were used to\ntrain the classifier and regression models. It was seen that while the impact\nof the different error sources on gaze data characteristics were nearly\nimpossible to distinguish by visual inspection or from data statistics, machine\nlearning models were successful in identifying the impact of the different\nerror sources and predicting the variability in gaze error levels due to these\nconditions. The objective of this study was to investigate the efficacy of\nmachine learning methods towards the detection and prediction of gaze error\npatterns, which would enable an in-depth understanding of the data quality and\nreliability of eye trackers under unconstrained operating conditions. Coding\nresources for all the machine learning methods adopted in this study were\nincluded in an open repository named MLGaze to allow researchers to replicate\nthe principles presented here using data from their own eye trackers.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 23:07:02 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Kar", "Anuradha", ""]]}, {"id": "2005.03818", "submitter": "Byungsoo Kim", "authors": "Youngduck Choi, Yoonho Na, Youngjik Yoon, Jonghun Shin, Chan Bae,\n  Hongseok Suh, Byungsoo Kim, Jaewe Heo", "title": "Choose Your Own Question: Encouraging Self-Personalization in Learning\n  Path Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Path Recommendation is the heart of adaptive learning, the\neducational paradigm of an Interactive Educational System (IES) providing a\npersonalized learning experience based on the student's history of learning\nactivities. In typical existing IESs, the student must fully consume a\nrecommended learning item to be provided a new recommendation. This workflow\ncomes with several limitations. For example, there is no opportunity for the\nstudent to give feedback on the choice of learning items made by the IES.\nFurthermore, the mechanism by which the choice is made is opaque to the\nstudent, limiting the student's ability to track their learning. To this end,\nwe introduce Rocket, a Tinder-like User Interface for a general class of IESs.\nRocket provides a visual representation of Artificial Intelligence\n(AI)-extracted features of learning materials, allowing the student to quickly\ndecide whether the material meets their needs. The student can choose between\nengaging with the material and receiving a new recommendation by swiping or\ntapping. Rocket offers the following potential improvements for IES User\nInterfaces: First, Rocket enhances the explainability of IES recommendations by\nshowing students a visual summary of the meaningful AI-extracted features used\nin the decision-making process. Second, Rocket enables self-personalization of\nthe learning experience by leveraging the students' knowledge of their own\nabilities and needs. Finally, Rocket provides students with fine-grained\ninformation on their learning path, giving them an avenue to assess their own\nskills and track their learning progress. We present the source code of Rocket,\nin which we emphasize the independence and extensibility of each component, and\nmake it publicly available for all purposes.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 01:53:04 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Choi", "Youngduck", ""], ["Na", "Yoonho", ""], ["Yoon", "Youngjik", ""], ["Shin", "Jonghun", ""], ["Bae", "Chan", ""], ["Suh", "Hongseok", ""], ["Kim", "Byungsoo", ""], ["Heo", "Jaewe", ""]]}, {"id": "2005.04058", "submitter": "Alex Bigelow", "authors": "Alex Bigelow, Katy Williams, Katherine E. Isaacs", "title": "Guidelines For Pursuing and Revealing Data Abstractions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data abstraction types, such as networks or set relationships, remain\nunfamiliar to data workers beyond the visualization research community. We\nconduct a survey and series of interviews about how people describe their data,\neither directly or indirectly. We refer to the latter as latent data\nabstractions. We conduct a Grounded Theory analysis that (1) interprets the\nextent to which latent data abstractions exist, (2) reveals the far-reaching\neffects that the interventionist pursuit of such abstractions can have on data\nworkers, (3) describes why and when data workers may resist such explorations,\nand (4) suggests how to take advantage of opportunities and mitigate risks\nthrough transparency about visualization research perspectives and agendas. We\nthen use the themes and codes discovered in the Grounded Theory analysis to\ndevelop guidelines for data abstraction in visualization projects. To continue\nthe discussion, we make our dataset open along with a visual interface for\nfurther exploration.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:19:16 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 16:55:36 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Bigelow", "Alex", ""], ["Williams", "Katy", ""], ["Isaacs", "Katherine E.", ""]]}, {"id": "2005.04107", "submitter": "Yuki Koyama", "authors": "Yuki Koyama, Issei Sato, Masataka Goto", "title": "Sequential Gallery for Interactive Visual Design Optimization", "comments": "To be published at ACM Trans. Graph. (Proc. SIGGRAPH 2020); Project\n  page available at https://koyama.xyz/project/sequential_gallery/", "journal-ref": "ACM Trans. Graph. 39, 4 (July 2020), pp.88:1-88:12", "doi": "10.1145/3386569.3392444", "report-no": null, "categories": "cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual design tasks often involve tuning many design parameters. For example,\ncolor grading of a photograph involves many parameters, some of which\nnon-expert users might be unfamiliar with. We propose a novel user-in-the-loop\noptimization method that allows users to efficiently find an appropriate\nparameter set by exploring such a high-dimensional design space through much\neasier two-dimensional search subtasks. This method, called sequential plane\nsearch, is based on Bayesian optimization to keep necessary queries to users as\nfew as possible. To help users respond to plane-search queries, we also propose\nusing a gallery-based interface that provides options in the two-dimensional\nsubspace arranged in an adaptive grid view. We call this interactive framework\nSequential Gallery since users sequentially select the best option from the\noptions provided by the interface. Our experiment with synthetic functions\nshows that our sequential plane search can find satisfactory solutions in fewer\niterations than baselines. We also conducted a preliminary user study, results\nof which suggest that novices can effectively complete search tasks with\nSequential Gallery in a photo-enhancement scenario.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:24:35 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Koyama", "Yuki", ""], ["Sato", "Issei", ""], ["Goto", "Masataka", ""]]}, {"id": "2005.04120", "submitter": "Cheul Young Park", "authors": "Cheul Young Park, Narae Cha, Soowon Kang, Auk Kim, Ahsan Habib\n  Khandoker, Leontios Hadjileontiadis, Alice Oh, Yong Jeong, Uichin Lee", "title": "K-EmoCon, a multimodal sensor dataset for continuous emotion recognition\n  in naturalistic conversations", "comments": "20 pages, 4 figures, for associated dataset, see\n  https://doi.org/10.5281/zenodo.3814370", "journal-ref": "Sci Data 7, (2020) 293", "doi": "10.1038/s41597-020-00630-y", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing emotions during social interactions has many potential\napplications with the popularization of low-cost mobile sensors, but a\nchallenge remains with the lack of naturalistic affective interaction data.\nMost existing emotion datasets do not support studying idiosyncratic emotions\narising in the wild as they were collected in constrained environments.\nTherefore, studying emotions in the context of social interactions requires a\nnovel dataset, and K-EmoCon is such a multimodal dataset with comprehensive\nannotations of continuous emotions during naturalistic conversations. The\ndataset contains multimodal measurements, including audiovisual recordings,\nEEG, and peripheral physiological signals, acquired with off-the-shelf devices\nfrom 16 sessions of approximately 10-minute long paired debates on a social\nissue. Distinct from previous datasets, it includes emotion annotations from\nall three available perspectives: self, debate partner, and external observers.\nRaters annotated emotional displays at intervals of every 5 seconds while\nviewing the debate footage, in terms of arousal-valence and 18 additional\ncategorical emotions. The resulting K-EmoCon is the first publicly available\nemotion dataset accommodating the multiperspective assessment of emotions\nduring social interactions.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:51:12 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 08:25:29 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Park", "Cheul Young", ""], ["Cha", "Narae", ""], ["Kang", "Soowon", ""], ["Kim", "Auk", ""], ["Khandoker", "Ahsan Habib", ""], ["Hadjileontiadis", "Leontios", ""], ["Oh", "Alice", ""], ["Jeong", "Yong", ""], ["Lee", "Uichin", ""]]}, {"id": "2005.04209", "submitter": "Ramy Mounir", "authors": "Ramy Mounir, Redwan Alqasemi, Rajiv Dubey", "title": "BCI-Controlled Hands-Free Wheelchair Navigation with Obstacle Avoidance", "comments": "Accepted by IROS 2018 workshop on \"Haptic-enabled shared control of\n  robotic systems: a compromise between teleoperation and autonomy\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer interfaces (BCI) are widely used in reading brain signals and\nconverting them into real-world motion. However, the signals produced from the\nBCI are noisy and hard to analyze. This paper looks specifically towards\ncombining the BCI's latest technology with ultrasonic sensors to provide a\nhands-free wheelchair that can efficiently navigate through crowded\nenvironments. This combination provides safety and obstacle avoidance features\nnecessary for the BCI Navigation system to gain more confidence and operate the\nwheelchair at a relatively higher velocity. A population of six human subjects\ntested the BCI-controller and obstacle avoidance features. Subjects were able\nto mentally control the destination of the wheelchair, by moving the target\nfrom the starting position to a predefined position, in an average of 287.12\nseconds and a standard deviation of 48.63 seconds after 10 minutes of training.\nThe wheelchair successfully avoided all obstacles placed by the subjects during\nthe test.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 17:56:11 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Mounir", "Ramy", ""], ["Alqasemi", "Redwan", ""], ["Dubey", "Rajiv", ""]]}, {"id": "2005.04292", "submitter": "Siddarth Sairaj S", "authors": "Siddarth S, Sainath G, Vignesh S", "title": "Deep Residual Network based food recognition for enhanced Augmented\n  Reality application", "comments": "Total Pages:7 Total Figures:10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural network based learning approaches is widely utilized for image\nclassification or object detection based problems with remarkable outcomes.\nRealtime Object state estimation of objects can be used to track and estimate\nthe features that the object of the current frame possesses without causing any\nsignificant delay and misclassification. A system that can detect the features\nof such objects in the present state from camera images can be used to enhance\nthe application of Augmented Reality for improving user experience and\ndelivering information in a much perceptual way. The focus behind this paper is\nto determine the most suitable model to create a low-latency assistance AR to\naid users by providing them nutritional information about the food that they\nconsume in order to promote healthier life choices. Hence the dataset has been\ncollected and acquired in such a manner, and we conduct various tests in order\nto identify the most suitable DNN in terms of performance and complexity and\nestablish a system that renders such information realtime to the user.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 21:08:58 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 17:47:47 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["S", "Siddarth", ""], ["G", "Sainath", ""], ["S", "Vignesh", ""]]}, {"id": "2005.04319", "submitter": "Benjamin C.K. Tee", "authors": "Hian Hian See, Brian Lim, Si Li, Haicheng Yao, Wen Cheng, Harold Soh,\n  and Benjamin C.K. Tee", "title": "ST-MNIST -- The Spiking Tactile MNIST Neuromorphic Dataset", "comments": "Corresponding authors: Benjamin C.K. Tee and Harold Soh For dataset,\n  see http://www.benjamintee.com/stmnist 10 Pages, 4 Figures and 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile sensing is an essential modality for smart robots as it enables them\nto interact flexibly with physical objects in their environment. Recent\nadvancements in electronic skins have led to the development of data-driven\nmachine learning methods that exploit this important sensory modality. However,\ncurrent datasets used to train such algorithms are limited to standard\nsynchronous tactile sensors. There is a dearth of neuromorphic event-based\ntactile datasets, principally due to the scarcity of large-scale event-based\ntactile sensors. Having such datasets is crucial for the development and\nevaluation of new algorithms that process spatio-temporal event-based data. For\nexample, evaluating spiking neural networks on conventional frame-based\ndatasets is considered sub-optimal. Here, we debut a novel neuromorphic Spiking\nTactile MNIST (ST-MNIST) dataset, which comprises handwritten digits obtained\nby human participants writing on a neuromorphic tactile sensor array. We also\ndescribe an initial effort to evaluate our ST-MNIST dataset using existing\nartificial and spiking neural network models. The classification accuracies\nprovided herein can serve as performance benchmarks for future work. We\nanticipate that our ST-MNIST dataset will be of interest and useful to the\nneuromorphic and robotics research communities.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 23:44:14 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["See", "Hian Hian", ""], ["Lim", "Brian", ""], ["Li", "Si", ""], ["Yao", "Haicheng", ""], ["Cheng", "Wen", ""], ["Soh", "Harold", ""], ["Tee", "Benjamin C. K.", ""]]}, {"id": "2005.04343", "submitter": "Elissa M. Redmiles", "authors": "Gabriel Kaptchuk, Daniel G. Goldstein, Eszter Hargittai, Jake Hofman,\n  Elissa M. Redmiles", "title": "How good is good enough for COVID19 apps? The influence of benefits,\n  accuracy, and privacy on willingness to adopt", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of contact tracing apps are being developed to complement\nmanual contact tracing. A key question is whether users will be willing to\nadopt these contact tracing apps. In this work, we survey over 4,500 Americans\nto evaluate (1) the effect of both accuracy and privacy concerns on reported\nwillingness to install COVID19 contact tracing apps and (2) how different\ngroups of users weight accuracy vs. privacy. Drawing on our findings from these\nfirst two research questions, we (3) quantitatively model how the amount of\npublic health benefit (reduction in infection rate), amount of individual\nbenefit (true-positive detection of exposures to COVID), and degree of privacy\nrisk in a hypothetical contact tracing app may influence American's willingness\nto install. Our work takes a descriptive ethics approach toward offering\nimplications for the development of policy and app designs related to COVID19.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 01:53:52 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 17:32:30 GMT"}, {"version": "v3", "created": "Wed, 13 May 2020 22:56:16 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 23:13:05 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kaptchuk", "Gabriel", ""], ["Goldstein", "Daniel G.", ""], ["Hargittai", "Eszter", ""], ["Hofman", "Jake", ""], ["Redmiles", "Elissa M.", ""]]}, {"id": "2005.04411", "submitter": "Yiqing Hua", "authors": "Yiqing Hua, Thomas Ristenpart, Mor Naaman", "title": "Towards Measuring Adversarial Twitter Interactions against Candidates in\n  the US Midterm Elections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial interactions against politicians on social media such as Twitter\nhave significant impact on society. In particular they disrupt substantive\npolitical discussions online, and may discourage people from seeking public\noffice. In this study, we measure the adversarial interactions against\ncandidates for the US House of Representatives during the run-up to the 2018 US\ngeneral election. We gather a new dataset consisting of 1.7 million tweets\ninvolving candidates, one of the largest corpora focusing on political\ndiscourse. We then develop a new technique for detecting tweets with toxic\ncontent that are directed at any specific candidate.Such technique allows us to\nmore accurately quantify adversarial interactions towards political candidates.\nFurther, we introduce an algorithm to induce candidate-specific adversarial\nterms to capture more nuanced adversarial interactions that previous techniques\nmay not consider toxic. Finally, we use these techniques to outline the breadth\nof adversarial interactions seen in the election, including offensive\nname-calling, threats of violence, posting discrediting information, attacks on\nidentity, and adversarial message repetition.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 10:00:41 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hua", "Yiqing", ""], ["Ristenpart", "Thomas", ""], ["Naaman", "Mor", ""]]}, {"id": "2005.04412", "submitter": "Yiqing Hua", "authors": "Yiqing Hua, Mor Naaman, Thomas Ristenpart", "title": "Characterizing Twitter Users Who Engage in Adversarial Interactions\n  against Political Candidates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media provides a critical communication platform for political\nfigures, but also makes them easy targets for harassment. In this paper, we\ncharacterize users who adversarially interact with political figures on Twitter\nusing mixed-method techniques. The analysis is based on a dataset of\n400~thousand users' 1.2~million replies to 756 candidates for the U.S. House of\nRepresentatives in the two months leading up to the 2018 midterm elections. We\nshow that among moderately active users, adversarial activity is associated\nwith decreased centrality in the social graph and increased attention to\ncandidates from the opposing party. When compared to users who are similarly\nactive, highly adversarial users tend to engage in fewer supportive\ninteractions with their own party's candidates and express negativity in their\nuser profiles. Our results can inform the design of platform moderation\nmechanisms to support political figures countering online harassment.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 10:05:29 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hua", "Yiqing", ""], ["Naaman", "Mor", ""], ["Ristenpart", "Thomas", ""]]}, {"id": "2005.04439", "submitter": "Aaquib Tabrez", "authors": "Aaquib Tabrez, Matthew B. Luebbers, Bradley Hayes", "title": "Automated Failure-Mode Clustering and Labeling for Informed\n  Car-To-Driver Handover in Autonomous Vehicles", "comments": "Presented at the 2020 Workshop on Assessing, Explaining, and\n  Conveying Robot Proficiency for Human-Robot Teaming", "journal-ref": null, "doi": null, "report-no": "RobotProficiency/2020/03", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The car-to-driver handover is a critically important component of safe\nautonomous vehicle operation when the vehicle is unable to safely proceed on\nits own. Current implementations of this handover in automobiles take the form\nof a generic alarm indicating an imminent transfer of control back to the human\ndriver. However, certain levels of vehicle autonomy may allow the driver to\nengage in other, non-driving related tasks prior to a handover, leading to\nsubstantial difficulty in quickly regaining situational awareness. This delay\nin re-orientation could potentially lead to life-threatening failures unless\nmitigating steps are taken. Explainable AI has been shown to improve fluency\nand teamwork in human-robot collaboration scenarios. Therefore, we hypothesize\nthat by utilizing autonomous explanation, these car-to-driver handovers can be\nperformed more safely and reliably. The rationale is, by providing the driver\nwith additional situational knowledge, they will more rapidly focus on the\nrelevant parts of the driving environment. Towards this end, we propose an\nalgorithmic failure-mode identification and explanation approach to enable\ninformed handovers from vehicle to driver. Furthermore, we propose a set of\nhuman-subjects driving-simulator studies to determine the appropriate form of\nexplanation during handovers, as well as validate our framework.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 13:16:29 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Tabrez", "Aaquib", ""], ["Luebbers", "Matthew B.", ""], ["Hayes", "Bradley", ""]]}, {"id": "2005.04789", "submitter": "Jichen Zhu", "authors": "Jichen Zhu, Katelyn Alderfer, Brian Smith, Bruce Char, Santiago\n  Onta\\~n\\'on", "title": "Understanding Learners' Problem-Solving Strategies in Concurrent and\n  Parallel Programming: A Game-Based Approach", "comments": "Submitted to CHI Play '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concurrent and parallel programming (CPP) is an increasingly important\nsubject in Computer Science Education. However, the conceptual shift from\nsequential programming is notoriously difficult to make. Currently, relatively\nlittle research exists on how people learn CPP core concepts. This paper\npresents our results of using Parallel, an educational game about CPP, focusing\non the learners' self-efficacy and how they learn CPP concepts. Based on a\nstudy of 44 undergraduate students, our research shows that (a) self-efficacy\nincreased significantly after playing the game; (b) the problem-solving\nstrategies employed by students playing the game can be classified in three\nmain types: trial and error, single-thread, and multi-threaded strategies, and\n(c) that self-efficacy is correlated with the percentage of time students spend\nin multithreaded problem-solving.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 21:29:10 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Zhu", "Jichen", ""], ["Alderfer", "Katelyn", ""], ["Smith", "Brian", ""], ["Char", "Bruce", ""], ["Onta\u00f1\u00f3n", "Santiago", ""]]}, {"id": "2005.04881", "submitter": "Jeong-Hyun Cho", "authors": "Jeong-Hyun Cho, Ji-Hoon Jeong, and Seong-Whan Lee", "title": "Decoding of Grasp Motions from EEG Signals Based on a Novel Data\n  Augmentation Strategy", "comments": "Accepted to IEEE EMBC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalogram (EEG) based brain-computer interface (BCI) systems are\nuseful tools for clinical purposes like neural prostheses. In this study, we\ncollected EEG signals related to grasp motions. Five healthy subjects\nparticipated in this experiment. They executed and imagined five\nsustained-grasp actions. We proposed a novel data augmentation method that\nincreases the amount of training data using labels obtained from electromyogram\n(EMG) signals analysis. For implementation, we recorded EEG and EMG\nsimultaneously. The data augmentation over the original EEG data concluded\nhigher classification accuracy than other competitors. As a result, we obtained\nthe average classification accuracy of 52.49% for motor execution (ME) and\n40.36% for motor imagery (MI). These are 9.30% and 6.19% higher, respectively\nthan the result of the comparable methods. Moreover, the proposed method could\nminimize the need for the calibration session, which reduces the practicality\nof most BCIs. This result is encouraging, and the proposed method could\npotentially be used in future applications such as a BCI-driven robot control\nfor handling various daily use objects.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 06:39:45 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Cho", "Jeong-Hyun", ""], ["Jeong", "Ji-Hoon", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2005.05019", "submitter": "Jelle Van Dijk", "authors": "Jelle van Dijk", "title": "Post-human interaction design, yes, but cautiously", "comments": "A \"provocation\" contribution to the acm Designing Interactive Systems\n  2020 Conference, Eindhoven, July 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-human design runs the risk of obscuring the fact that AI technology\nactually imports a Cartesian humanist logic, which subsequently influences how\nwe design and conceive of so-called smart or intelligent objects. This leads to\nunwanted metaphorical attributions of human qualities to smart objects.\nInstead, starting from an embodied sensemaking perspective, designers should\ndemand of engineers to radically transform the very structure of AI technology,\nin order to truly support critical posthuman values of collectivity,\nrelationality and community building.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 09:17:19 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["van Dijk", "Jelle", ""]]}, {"id": "2005.05021", "submitter": "Youngnam Lee", "authors": "Youngnam Lee, Byungsoo Kim, Dongmin Shin, JungHoon Kim, Jineon Baek,\n  Jinhwan Lee, Youngduck Choi", "title": "Prescribing Deep Attentive Score Prediction Attracts Improved Student\n  Engagement", "comments": "EDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent Tutoring Systems (ITSs) have been developed to provide students\nwith personalized learning experiences by adaptively generating learning paths\noptimized for each individual. Within the vast scope of ITS, score prediction\nstands out as an area of study that enables students to construct individually\nrealistic goals based on their current position. Via the expected score\nprovided by the ITS, a student can instantaneously compare one's expected score\nto one's actual score, which directly corresponds to the reliability that the\nITS can instill. In other words, refining the precision of predicted scores\nstrictly correlates to the level of confidence that a student may have with an\nITS, which will evidently ensue improved student engagement. However, previous\nstudies have solely concentrated on improving the performance of a prediction\nmodel, largely lacking focus on the benefits generated by its practical\napplication. In this paper, we demonstrate that the accuracy of the score\nprediction model deployed in a real-world setting significantly impacts user\nengagement by providing empirical evidence. To that end, we apply a\nstate-of-the-art deep attentive neural network-based score prediction model to\nSanta, a multi-platform English ITS with approximately 780K users in South\nKorea that exclusively focuses on the TOEIC (Test of English for International\nCommunications) standardized examinations. We run a controlled A/B test on the\nITS with two models, respectively based on collaborative filtering and deep\nattentive neural networks, to verify whether the more accurate model engenders\nany student engagement. The results conclude that the attentive model not only\ninduces high student morale (e.g. higher diagnostic test completion ratio,\nnumber of questions answered, etc.) but also encourages active engagement (e.g.\nhigher purchase rate, improved total profit, etc.) on Santa.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 02:05:40 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 01:06:03 GMT"}, {"version": "v3", "created": "Tue, 19 May 2020 06:44:37 GMT"}, {"version": "v4", "created": "Thu, 25 Jun 2020 05:02:29 GMT"}, {"version": "v5", "created": "Wed, 1 Jul 2020 06:51:20 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Lee", "Youngnam", ""], ["Kim", "Byungsoo", ""], ["Shin", "Dongmin", ""], ["Kim", "JungHoon", ""], ["Baek", "Jineon", ""], ["Lee", "Jinhwan", ""], ["Choi", "Youngduck", ""]]}, {"id": "2005.05023", "submitter": "Hatice Gunes Dr", "authors": "Lorcan Reidy, Dennis Chan, Charles Nduka and Hatice Gunes", "title": "Facial Electromyography-based Adaptive Virtual Reality Gaming for\n  Cognitive Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive training has shown promising results for delivering improvements in\nhuman cognition related to attention, problem solving, reading comprehension\nand information retrieval. However, two frequently cited problems in cognitive\ntraining literature are a lack of user engagement with the training programme,\nand a failure of developed skills to generalise to daily life. This paper\nintroduces a new cognitive training (CT) paradigm designed to address these two\nlimitations by combining the benefits of gamification, virtual reality (VR),\nand affective adaptation in the development of an engaging, ecologically valid,\nCT task. Additionally, it incorporates facial electromyography (EMG) as a means\nof determining user affect while engaged in the CT task. This information is\nthen utilised to dynamically adjust the game's difficulty in real-time as users\nplay, with the aim of leading them into a state of flow. Affect recognition\nrates of 64.1% and 76.2%, for valence and arousal respectively, were achieved\nby classifying a DWT-Haar approximation of the input signal using kNN. The\naffect-aware VR cognitive training intervention was then evaluated with a\ncontrol group of older adults. The results obtained substantiate the notion\nthat adaptation techniques can lead to greater feelings of competence and a\nmore appropriate challenge of the user's skills.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 10:01:52 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 12:36:54 GMT"}, {"version": "v3", "created": "Sun, 30 Aug 2020 13:42:12 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Reidy", "Lorcan", ""], ["Chan", "Dennis", ""], ["Nduka", "Charles", ""], ["Gunes", "Hatice", ""]]}, {"id": "2005.05024", "submitter": "Felix Hamza-Lup", "authors": "Ioana R. Goldbach, Felix G. Hamza-Lup", "title": "Intelligent Tutoring Systems for Generation Z's Addiction", "comments": "4 pages", "journal-ref": "IARIA 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As generation Z's big data is flooding the Internet through social nets,\nneural network based data processing is turning an important cornerstone,\nshowing significant potential for fast extraction of data patterns. Online\ncourse delivery and associated tutoring are transforming into customizable,\non-demand services driven by the learner. Besides automated grading, strong\npotential exists for the development and deployment of next generation\nintelligent tutoring software agents. Self-adaptive, online tutoring agents\nexhibiting \"intelligent-like\" behavior, being capable \"to learn\" from the\nlearner, will become the next educational superstars. Over the past decade,\ncomputer-based tutoring agents were deployed in a variety of extended reality\nenvironments, from patient rehabilitation to psychological trauma healing. Most\nof these agents are driven by a set of conditional control statements and a\nlarge answers/questions pairs dataset. This article provides a brief\nintroduction on Generation Z's addiction to digital information, highlights\nimportant efforts for the development of intelligent dialogue systems, and\nexplains the main components and important design decisions for Intelligent\nTutoring System.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 19:10:12 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Goldbach", "Ioana R.", ""], ["Hamza-Lup", "Felix G.", ""]]}, {"id": "2005.05025", "submitter": "Pradipta Biswas", "authors": "LRD Murthy, Somnath Arjun, Kamalpreet Singh Saluja, Pradipta Biswas", "title": "Interactive Sensor Dashboard for Smart Manufacturing", "comments": "Was presented at 7th International Conference on PLMSS (Product Life\n  Cycle Modelling, Simulation and Synthesis), Bangalore from 18th-20th Dec 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents development of a smart sensor dashboard for Industry 4.0\nencompassing both 2D and 3D visualization modules. In 2D module, we described\nphysical connections among sensors and visualization modules and rendering data\non 2D screen. A user study was presented where participants answered a few\nquestions using four types of graphs. We analyzed eye gaze patterns in screen,\nnumber of correct answers and response time for all the four graphs. For 3D\nmodule, we developed a VR digital twin for sensor data visualization. A user\nstudy was presented evaluating the effect of different feedback scenarios on\nquantitative and qualitative metrics of interaction in the virtual environment.\nWe compared visual and haptic feedback and a multimodal combination of both\nvisual and haptic feedback for VR environment. We found that haptic feedback\nsignificantly improved quantitative metrics of interaction than a no feedback\ncase whereas a multimodal feedback is significantly improved qualitative\nmetrics of the interaction.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 14:38:00 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Murthy", "LRD", ""], ["Arjun", "Somnath", ""], ["Saluja", "Kamalpreet Singh", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2005.05026", "submitter": "Omar Sosa Tzec", "authors": "Omar Sosa-Tzec, Gowri Balasubramaniam, Sylvia Sinsabaugh, Evan\n  Sobetski, Rogerio Pinto, Shervin Assari", "title": "Delightful Companions: Supporting Well-Being Through Design Delight", "comments": "5 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents three design products referred to as delightful\ncompanions that are intended to help people engage in well-being practices. It\nalso introduces the approach utilized to guide the design decisions during\ntheir creation. Design delight is the name of this approach, which comprises\nsix experiential qualities that are regarded as antecedents of delight. The\nobjective of this paper is to introduce the approach and the companions and\nstate the two paths that have defined the future steps of this research.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:48:26 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Sosa-Tzec", "Omar", ""], ["Balasubramaniam", "Gowri", ""], ["Sinsabaugh", "Sylvia", ""], ["Sobetski", "Evan", ""], ["Pinto", "Rogerio", ""], ["Assari", "Shervin", ""]]}, {"id": "2005.05084", "submitter": "Martin Cooney", "authors": "Martin Cooney", "title": "Robot art, in the eye of the beholder?: Personalization through\n  self-disclosure facilitates visual communication of emotions in\n  representational art", "comments": "Please note: This is a preprint version of a journal article before\n  reviews, submitted on 2020-5-7 to The Art of Human-Robot Interaction:\n  Creative Perspectives from Design and the Arts, hosted by Dr(s) Damith C\n  Herath, Elizabeth Ann Jochum, Christian Kroos, and David St-Onge in Frontiers\n  in Robotics and AI - section Human-Robot Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Socially assistive robots could help to support people's well-being in\ncontexts such as art therapy where human therapists are scarce, by making art\nsuch as paintings together with people in a way that is emotionally contingent\nand creative. However, current art-making robots are typically either\ncontingent, controlled as a tool by a human artist, or creative, programmed to\npaint independently, possibly because some complex and idiosyncratic concepts\nrelated to art, such as emotion and creativity, are not yet well understood.\nFor example, the role of personalized taste in forming beauty evaluations has\nbeen studied in empirical aesthetics, but how to generate art that appears to\nan individual to express certain emotions such as happiness or sadness is less\nclear. In the current article, a collaborative prototyping/Wizard of Oz\napproach was used to explore generic robot art-making strategies and\npersonalization of art via an open-ended emotion profile intended to identify\ntricky concepts. As a result of conducting an exploratory user study,\nparticipants indicated some preference for a robot art-making strategy\ninvolving \"visual metaphors\" to balance exogenous and endogenous components,\nand personalized representational sketches were reported to convey emotion more\nclearly than generic sketches. The article closes by discussing personalized\nabstract art as well as suggestions for richer art-making strategies and user\nmodels. Thus, the main conceptual advance of the current work lies in\nsuggesting how an interactive robot can visually express emotions through\npersonalized art; the general aim is that this could help to inform next steps\nin this promising area and facilitate technological acceptance of robots in\neveryday human environments.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 12:19:10 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Cooney", "Martin", ""]]}, {"id": "2005.05438", "submitter": "Jennifer Mankoff", "authors": "Han Zhang and Paula Nurius and Yasaman Sefidgar and Margaret Morris\n  and Sreenithi Balasubramanian and Jennifer Brown and Anind K. Dey and Kevin\n  Kuehn and Eve Riskin and Xuhai Xu and Jen Mankoff", "title": "How Does COVID-19 impact Students with Disabilities/Health Concerns?", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of COVID-19 on students has been enormous, with an increase in\nworries about fiscal and physical health, a rapid shift to online learning, and\nincreased isolation. In addition to these changes, students with\ndisabilities/health concerns may face accessibility problems with online\nlearning or communication tools, and their stress may be compounded by\nadditional risks such as financial stress or pre-existing conditions. To our\nknowledge, no one has looked specifically at the impact of COVID-19 on students\nwith disabilities/health concerns. In this paper, we present data from a survey\nof 147 students with and without disabilities collected in late March to early\nApril of 2020 to assess the impact of COVID-19 on these students' education and\nmental health. Our findings show that students with disabilities/health\nconcerns were more concerned about classes going online than their peers\nwithout disabilities. In addition, students with disabilities/health concerns\nalso reported that they have experienced more COVID-19 related adversities\ncompared to their peers without disabilities/health concerns. We argue that\nstudents with disabilities/health concerns in higher education need confidence\nin the accessibility of the online learning tools that are becoming\nincreasingly prevalent in higher education not only because of COVID-19 but\nalso more generally. In addition, educational technologies will be more\naccessible if they consider the learning context, and are designed to provide a\nsupportive, calm, and connecting learning environment.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 21:09:02 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 18:32:19 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Zhang", "Han", ""], ["Nurius", "Paula", ""], ["Sefidgar", "Yasaman", ""], ["Morris", "Margaret", ""], ["Balasubramanian", "Sreenithi", ""], ["Brown", "Jennifer", ""], ["Dey", "Anind K.", ""], ["Kuehn", "Kevin", ""], ["Riskin", "Eve", ""], ["Xu", "Xuhai", ""], ["Mankoff", "Jen", ""]]}, {"id": "2005.05445", "submitter": "Ramy Mounir", "authors": "Ramy Mounir, Kyle Reed", "title": "Polyrhythmic Bimanual Coordination Training using Haptic Force Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to develop two thoughts at the same time or perform two\nuncorrelated motions simultaneously. This work looks specifically towards\ntraining humans to perform a 2:3 polyrhythmic bimanual ratio using haptic force\nfeedback devices (SensAble Phantom OMNI). We implemented an interactive\ntraining session to help participants learn to decouple their hand motions\nquickly. Three subjects (2 Females, 1 Male) were tested and have successfully\nincreased their scores after adaptive training durations of under five minutes.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 21:29:55 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Mounir", "Ramy", ""], ["Reed", "Kyle", ""]]}, {"id": "2005.05688", "submitter": "Darren Edge", "authors": "Darren Edge, Weiwei Yang, Kate Lytvynets, Harry Cook, Claire\n  Galez-Davis, Hannah Darnton, and Christopher M. White", "title": "Design of a Privacy-Preserving Data Platform for Collaboration Against\n  Human Trafficking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case records on victims of human trafficking are highly sensitive, yet the\nability to share such data is critical to evidence-based practice and policy\ndevelopment across government, business, and civil society. We present new\nmethods to anonymize, publish, and explore such data, implemented as a pipeline\ngenerating three artifacts: (1) synthetic data mitigating the privacy risk that\npublished attribute combinations might be linked to known individuals or\ngroups; (2) aggregate data mitigating the utility risk that synthetic data\nmight misrepresent statistics needed for official reporting; and (3) visual\nanalytics interfaces to both datasets mitigating the accessibility risk that\nprivacy mechanisms or analysis tools might not be understandable and usable by\nall stakeholders. We present our work as a design study motivated by the goal\nof transforming how the world's largest database of identified victims is made\navailable for global collaboration against human trafficking.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 11:08:15 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 12:56:34 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Edge", "Darren", ""], ["Yang", "Weiwei", ""], ["Lytvynets", "Kate", ""], ["Cook", "Harry", ""], ["Galez-Davis", "Claire", ""], ["Darnton", "Hannah", ""], ["White", "Christopher M.", ""]]}, {"id": "2005.05880", "submitter": "Susan Murphy A", "authors": "Ashley E. Walton, Linda M. Collins, Predrag Klasnja, Inbal\n  Nahum-Shani, Mashfiqui Rabbi, Maureen A. Walton, Susan A. Murphy", "title": "The Micro-Randomized Trial for Developing Digital Interventions:\n  Experimental Design Considerations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just-in-time adaptive interventions (JITAIs) are time-varying adaptive\ninterventions that use frequent opportunities for the intervention to be\nadapted such as weekly, daily, or even many times a day. This high intensity of\nadaptation is facilitated by the ability of digital technology to continuously\ncollect information about an individual's current context and deliver\ntreatments adapted to this information. The micro-randomized trial (MRT) has\nemerged for use in informing the construction of JITAIs. MRTs operate in, and\ntake advantage of, the rapidly time-varying digital intervention environment.\nMRTs can be used to address research questions about whether and under what\ncircumstances particular components of a JITAI are effective, with the ultimate\nobjective of developing effective and efficient components. The purpose of this\narticle is to clarify why, when, and how to use MRTs; to highlight elements\nthat must be considered when designing and implementing an MRT; and to discuss\nthe possibilities this emerging optimization trial design offers for future\nresearch in the behavioral sciences, education, and other fields. We briefly\nreview key elements of JITAIs, and then describe three case studies of MRTs,\neach of which highlights research questions that can be addressed using the MRT\nand experimental design considerations that might arise. We also discuss a\nvariety of considerations that go into planning and designing an MRT, using the\ncase studies as examples.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:41:00 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Walton", "Ashley E.", ""], ["Collins", "Linda M.", ""], ["Klasnja", "Predrag", ""], ["Nahum-Shani", "Inbal", ""], ["Rabbi", "Mashfiqui", ""], ["Walton", "Maureen A.", ""], ["Murphy", "Susan A.", ""]]}, {"id": "2005.05926", "submitter": "Marco Matarese", "authors": "Marco Matarese, Silvia Rossi, Alessandra Sciutti, Francesco Rea", "title": "Towards Transparency of TD-RL Robotic Systems with a Human Teacher", "comments": "Presented at the 2020 Workshop on Assessing, Explaining, and\n  Conveying Robot Proficiency for Human-Robot Teaming", "journal-ref": null, "doi": null, "report-no": "RobotProficiency/2020/04", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high request for autonomous and flexible HRI implies the necessity of\ndeploying Machine Learning (ML) mechanisms in the robot control. Indeed, the\nuse of ML techniques, such as Reinforcement Learning (RL), makes the robot\nbehaviour, during the learning process, not transparent to the observing user.\nIn this work, we proposed an emotional model to improve the transparency in RL\ntasks for human-robot collaborative scenarios. The architecture we propose\nsupports the RL algorithm with an emotional model able to both receive human\nfeedback and exhibit emotional responses based on the learning process. The\nmodel is entirely based on the Temporal Difference (TD) error. The architecture\nwas tested in an isolated laboratory with a simple setup. The results highlight\nthat showing its internal state through an emotional response is enough to make\na robot transparent to its human teacher. People also prefer to interact with a\nresponsive robot because they are used to understand their intentions via\nemotions and social signals.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:09:42 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Matarese", "Marco", ""], ["Rossi", "Silvia", ""], ["Sciutti", "Alessandra", ""], ["Rea", "Francesco", ""]]}, {"id": "2005.05940", "submitter": "Lindsay Sanneman", "authors": "Lindsay Sanneman, Julie A. Shah", "title": "Trust Considerations for Explainable Robots: A Human Factors Perspective", "comments": "Presented at the 2020 Workshop on Assessing, Explaining, and\n  Conveying Robot Proficiency for Human-Robot Teaming", "journal-ref": null, "doi": null, "report-no": "RobotProficiency/2020/05", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in artificial intelligence (AI) and robotics have drawn\nattention to the need for AI systems and robots to be understandable to human\nusers. The explainable AI (XAI) and explainable robots literature aims to\nenhance human understanding and human-robot team performance by providing users\nwith necessary information about AI and robot behavior. Simultaneously, the\nhuman factors literature has long addressed important considerations that\ncontribute to human performance, including human trust in autonomous systems.\nIn this paper, drawing from the human factors literature, we discuss three\nimportant trust-related considerations for the design of explainable robot\nsystems: the bases of trust, trust calibration, and trust specificity. We\nfurther detail existing and potential metrics for assessing trust in robotic\nsystems based on explanations provided by explainable robots.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 17:39:18 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Sanneman", "Lindsay", ""], ["Shah", "Julie A.", ""]]}, {"id": "2005.06011", "submitter": "David Saffo", "authors": "David Saffo, Aristotelis Leventidis, Twinkle Jain, Michelle A. Borkin,\n  Cody Dunne", "title": "Data Comets: Designing a Visualization Tool for Analyzing Autonomous\n  Aerial Vehicle Logs with Grounded Evaluation", "comments": "EuroVis 2020 Full Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous unmanned aerial vehicles are complex systems of hardware,\nsoftware, and human input. Understanding this complexity is key to their\ndevelopment and operation. Information visualizations already exist for\nexploring flight logs but comprehensive analyses currently require several\ndisparate and custom tools. This design study helps address the pain points\nfaced by autonomous unmanned aerial vehicle developers and operators. We\ncontribute: a spiral development process model for grounded evaluation\nvisualization development focused on progressively broadening target user\ninvolvement and refining user goals; a demonstration of the model as part of\ndeveloping a deployed and adopted visualization system; a data and task\nabstraction for developers and operators performing post-flight analysis of\nautonomous unmanned aerial vehicle logs; the design and implementation of DATA\nCOMETS, an open-source and web-based interactive visualization tool for\npost-flight log analysis incorporating temporal, geospatial, and multivariate\ndata; and the results of a summative evaluation of the visualization system and\nour abstractions based on in-the-wild usage. A free copy of this paper and\nsource code are available at osf.io/h4p7g\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 19:02:04 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Saffo", "David", ""], ["Leventidis", "Aristotelis", ""], ["Jain", "Twinkle", ""], ["Borkin", "Michelle A.", ""], ["Dunne", "Cody", ""]]}, {"id": "2005.06013", "submitter": "David Saffo", "authors": "David Saffo, Sara Di Bartolomeo, Caglar Yildirim, Cody Dunne", "title": "Two Dimensions for Organizing Immersive Analytics: Toward a Taxonomy for\n  Facet and Position", "comments": "Immersive Analytics Workshop CHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As immersive analytics continues to grow as a discipline, so too should its\nunderlying methodological support. Taxonomies play an important role for\ninformation visualization and human computer interaction. They provide an\norganization of the techniques used in a particular domain that better enable\nresearchers to describe their work, discover existing methods, and identify\ngaps in the literature. Existing taxonomies in related fields do not capture or\ndescribe the unique paradigms employed in immersive analytics. We conceptualize\na taxonomy that organizes immersive analytics according to two dimensions:\nspatial and visual presentation. Each intersection of this taxonomy represents\na unique design paradigm which, when thoroughly explored, can aid in the design\nand research of new immersive analytic applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 19:07:01 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Saffo", "David", ""], ["Di Bartolomeo", "Sara", ""], ["Yildirim", "Caglar", ""], ["Dunne", "Cody", ""]]}, {"id": "2005.06021", "submitter": "Carlos Toxtli", "authors": "Saiph Savage, Chun-Wei Chiang, Susumu Saito, Carlos Toxtli, Jeffrey\n  Bigham", "title": "Becoming the Super Turker: Increasing Wages via a Strategy from High\n  Earning Workers", "comments": "12.pages, 8 figures, The Web Conference 202, ACM WWW 2020", "journal-ref": null, "doi": "10.1145/3366423.3380199", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Crowd markets have traditionally limited workers by not providing\ntransparency information concerning which tasks pay fairly or which requesters\nare unreliable. Researchers believe that a key reason why crowd workers earn\nlow wages is due to this lack of transparency. As a result, tools have been\ndeveloped to provide more transparency within crowd markets to help workers.\nHowever, while most workers use these tools, they still earn less than minimum\nwage. We argue that the missing element is guidance on how to use transparency\ninformation. In this paper, we explore how novice workers can improve their\nearnings by following the transparency criteria of Super Turkers, i.e., crowd\nworkers who earn higher salaries on Amazon Mechanical Turk (MTurk). We believe\nthat Super Turkers have developed effective processes for using transparency\ninformation. Therefore, by having novices follow a Super Turker criteria (one\nthat is simple and popular among Super Turkers), we can help novices increase\ntheir wages. For this purpose, we: (i) conducted a survey and data analysis to\ncomputationally identify a simple yet common criteria that Super Turkers use\nfor handling transparency tools; (ii) deployed a two-week field experiment with\nnovices who followed this Super Turker criteria to find better work on MTurk.\nNovices in our study viewed over 25,000 tasks by 1,394 requesters. We found\nthat novices who utilized this Super Turkers' criteria earned better wages than\nother novices. Our results highlight that tool development to support crowd\nworkers should be paired with educational opportunities that teach workers how\nto effectively use the tools and their related metrics (e.g., transparency\nvalues). We finish with design recommendations for empowering crowd workers to\nearn higher salaries.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 00:04:46 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Savage", "Saiph", ""], ["Chiang", "Chun-Wei", ""], ["Saito", "Susumu", ""], ["Toxtli", "Carlos", ""], ["Bigham", "Jeffrey", ""]]}, {"id": "2005.06022", "submitter": "Carlos Toxtli", "authors": "Carlos Toxtli, Angela Richmond-Fuller, Saiph Savage", "title": "Reputation Agent: Prompting Fair Reviews in Gig Markets", "comments": "12 pages, 5 figures, The Web Conference 2020, ACM WWW 2020", "journal-ref": null, "doi": "10.1145/3366423.3380199", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Our study presents a new tool, Reputation Agent, to promote fairer reviews\nfrom requesters (employers or customers) on gig markets. Unfair reviews,\ncreated when requesters consider factors outside of a worker's control, are\nknown to plague gig workers and can result in lost job opportunities and even\ntermination from the marketplace. Our tool leverages machine learning to\nimplement an intelligent interface that: (1) uses deep learning to\nautomatically detect when an individual has included unfair factors into her\nreview (factors outside the worker's control per the policies of the market);\nand (2) prompts the individual to reconsider her review if she has incorporated\nunfair factors. To study the effectiveness of Reputation Agent, we conducted a\ncontrolled experiment over different gig markets. Our experiment illustrates\nthat across markets, Reputation Agent, in contrast with traditional approaches,\nmotivates requesters to review gig workers' performance more fairly. We discuss\nhow tools that bring more transparency to employers about the policies of a gig\nmarket can help build empathy thus resulting in reasoned discussions around\npotential injustices towards workers generated by these interfaces. Our vision\nis that with tools that promote truth and transparency we can bring fairer\ntreatment to gig workers.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 01:56:10 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Toxtli", "Carlos", ""], ["Richmond-Fuller", "Angela", ""], ["Savage", "Saiph", ""]]}, {"id": "2005.06039", "submitter": "Sara Di Bartolomeo", "authors": "Sara Di Bartolomeo, Aditeya Pandey, Aristotelis Leventidis, David\n  Saffo, Uzma Haque Syeda, Elin Carstensdottir, Magy Seif El-Nasr, Michelle A.\n  Borkin, Cody Dunne", "title": "Evaluating the Effect of Timeline Shape on Visualization Task\n  Performance", "comments": "12 pages, 5 figures", "journal-ref": "Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems, April 2020, Pages 1-12", "doi": "10.1145/3313831.3376237", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Timelines are commonly represented on a horizontal line, which is not\nnecessarily the most effective way to visualize temporal event sequences.\nHowever, few experiments have evaluated how timeline shape influences task\nperformance. We present the design and results of a controlled experiment run\non Amazon Mechanical Turk (n=192) in which we evaluate how timeline shape\naffects task completion time, correctness, and user preference. We tested 12\ncombinations of 4 shapes -- horizontal line, vertical line, circle, and spiral\n-- and 3 data types -- recurrent, non-recurrent, and mixed event sequences. We\nfound good evidence that timeline shape meaningfully affects user task\ncompletion time but not correctness and that users have a strong shape\npreference. Building on our results, we present design guidelines for creating\neffective timeline visualizations based on user task and data types. A free\ncopy of this paper, the evaluation stimuli and data, and code are available at\nhttps://osf.io/qr5yu/\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 20:41:42 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Di Bartolomeo", "Sara", ""], ["Pandey", "Aditeya", ""], ["Leventidis", "Aristotelis", ""], ["Saffo", "David", ""], ["Syeda", "Uzma Haque", ""], ["Carstensdottir", "Elin", ""], ["El-Nasr", "Magy Seif", ""], ["Borkin", "Michelle A.", ""], ["Dunne", "Cody", ""]]}, {"id": "2005.06057", "submitter": "Joachim Meyer", "authors": "Salomon Eisler, Joachim Meyer", "title": "Visual Analytics and Human Involvement in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapidly developing AI systems and applications still require human\ninvolvement in practically all parts of the analytics process. Human decisions\nare largely based on visualizations, providing data scientists details of data\nproperties and the results of analytical procedures. Different visualizations\nare used in the different steps of the Machine Learning (ML) process. The\ndecision which visualization to use depends on factors, such as the data\ndomain, the data model and the step in the ML process. In this chapter, we\ndescribe the seven steps in the ML process and review different visualization\ntechniques that are relevant for the different steps for different types of\ndata, models and purposes.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 21:22:47 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Eisler", "Salomon", ""], ["Meyer", "Joachim", ""]]}, {"id": "2005.06291", "submitter": "Viktorija Paneva", "authors": "Viktorija Paneva, Myroslav Bachynskyi and J\\\"org M\\\"uller", "title": "Levitation Simulator: Prototyping Ultrasonic Levitation Interfaces in\n  Virtual Reality", "comments": "12 pages, 14 figures, CHI'20", "journal-ref": "Proceedings of the 2020 CHI Conference on Human Factors in\n  Computing Systems (2020)", "doi": "10.1145/3313831.3376409", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the Levitation Simulator, a system that enables researchers and\ndesigners to iteratively develop and prototype levitation interface ideas in\nVirtual Reality. This includes user tests and formal experiments. We derive a\nmodel of the movement of a levitating particle in such an interface. Based on\nthis, we develop an interactive simulation of the levitation interface in VR,\nwhich exhibits the dynamical properties of the real interface. The results of a\nFitts' Law pointing study show that the Levitation Simulator enables\nperformance, comparable to the real prototype. We developed the first two\ninteractive games, dedicated for levitation interfaces: LeviShooter and\nBeadBounce, in the Levitation Simulator, and then implemented them on the real\ninterface. Our results indicate that participants experienced similar levels of\nuser engagement when playing the games, in the two environments. We share our\nLevitation Simulator as Open Source, thereby democratizing levitation research,\nwithout the need for a levitation apparatus.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 12:51:19 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Paneva", "Viktorija", ""], ["Bachynskyi", "Myroslav", ""], ["M\u00fcller", "J\u00f6rg", ""]]}, {"id": "2005.06292", "submitter": "Viktorija Paneva", "authors": "Viktorija Paneva, Sofia Seinfeld, Michael Kraiczi and J\\\"org M\\\"uller", "title": "HaptiRead: Reading Braille as Mid-Air Haptic Information", "comments": "8 pages, 8 figures, 2 tables, DIS'20", "journal-ref": "Proceedings of the 2020 on Designing Interactive Systems\n  Conference (2020)", "doi": "10.1145/3357236.3395515", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mid-air haptic interfaces have several advantages - the haptic information is\ndelivered directly to the user, in a manner that is unobtrusive to the\nimmediate environment. They operate at a distance, thus easier to discover;\nthey are more hygienic and allow interaction in 3D. We validate, for the first\ntime, in a preliminary study with sighted and a user study with blind\nparticipants, the use of mid-air haptics for conveying Braille. We tested three\nhaptic stimulation methods, where the haptic feedback was either: a) aligned\ntemporally, with haptic stimulation points presented simultaneously (Constant);\nb) not aligned temporally, presenting each point independently\n(Point-By-Point); or c) a combination of the previous methodologies, where\nfeedback was presented Row-by-Row. The results show that mid-air haptics is a\nviable technology for presenting Braille characters, and the highest average\naccuracy (94% in the preliminary and 88% in the user study) was achieved with\nthe Point-by-Point method.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 12:52:56 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Paneva", "Viktorija", ""], ["Seinfeld", "Sofia", ""], ["Kraiczi", "Michael", ""], ["M\u00fcller", "J\u00f6rg", ""]]}, {"id": "2005.06301", "submitter": "Viktorija Paneva", "authors": "Myroslav Bachynskyi, Viktorija Paneva and J\\\"org M\\\"uller", "title": "LeviCursor: Dexterous Interaction with a Levitating Object", "comments": "10 pages, 6 figures, ISS'18", "journal-ref": "Proceedings of the 2018 ACM International Conference on\n  Interactive Surfaces and Spaces (2018)", "doi": "10.1145/3279778.3279802", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LeviCursor, a method for interactively moving a physical,\nlevitating particle in 3D with high agility. The levitating object can move\ncontinuously and smoothly in any direction. We optimize the transducer phases\nfor each possible levitation point independently. Using precomputation, our\nsystem can determine the optimal transducer phases within a few microseconds\nand achieves round-trip latencies of 15 ms. Due to our interpolation scheme,\nthe levitated object can be controlled almost instantaneously with\nsub-millimeter accuracy. We present a particle stabilization mechanism which\nensures the levitating particle is always in the main levitation trap. Lastly,\nwe conduct the first Fitts' law-type pointing study with a real 3D cursor,\nwhere participants control the movement of the levitated cursor between two\nphysical targets. The results of the user study demonstrate that using\nLeviCursor, users reach performance comparable to that of a mouse pointer.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 13:15:26 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Bachynskyi", "Myroslav", ""], ["Paneva", "Viktorija", ""], ["M\u00fcller", "J\u00f6rg", ""]]}, {"id": "2005.06380", "submitter": "Pierre Le Bras", "authors": "Pierre Le Bras, Azimeh Gharavi, David A. Robb, Ana F. Vidal, Stefano\n  Padilla, Mike J. Chantler", "title": "Visualising COVID-19 Research", "comments": "11 pages. 10 figures. Preprint paper made available here prior to\n  submission. Update: special characters corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The world has seen in 2020 an unprecedented global outbreak of SARS-CoV-2, a\nnew strain of coronavirus, causing the COVID-19 pandemic, and radically\nchanging our lives and work conditions. Many scientists are working tirelessly\nto find a treatment and a possible vaccine. Furthermore, governments,\nscientific institutions and companies are acting quickly to make resources\navailable, including funds and the opening of large-volume data repositories,\nto accelerate innovation and discovery aimed at solving this pandemic. In this\npaper, we develop a novel automated theme-based visualisation method, combining\nadvanced data modelling of large corpora, information mapping and trend\nanalysis, to provide a top-down and bottom-up browsing and search interface for\nquick discovery of topics and research resources. We apply this method on two\nrecently released publications datasets (Dimensions' COVID-19 dataset and the\nAllen Institute for AI's CORD-19). The results reveal intriguing information\nincluding increased efforts in topics such as social distancing; cross-domain\ninitiatives (e.g. mental health and education); evolving research in medical\ntopics; and the unfolding trajectory of the virus in different territories\nthrough publications. The results also demonstrate the need to quickly and\nautomatically enable search and browsing of large corpora. We believe our\nmethodology will improve future large volume visualisation and discovery\nsystems but also hope our visualisation interfaces will currently aid\nscientists, researchers, and the general public to tackle the numerous issues\nin the fight against the COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:45:14 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 10:06:39 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Bras", "Pierre Le", ""], ["Gharavi", "Azimeh", ""], ["Robb", "David A.", ""], ["Vidal", "Ana F.", ""], ["Padilla", "Stefano", ""], ["Chantler", "Mike J.", ""]]}, {"id": "2005.06517", "submitter": "Nicola Melluso", "authors": "Elena Coli, Nicola Melluso, Gualtiero Fantoni and Daniele Mazzei", "title": "Towards Automatic building of Human-Machine Conversational System to\n  support Maintenance Processes", "comments": null, "journal-ref": "R&D Management Conference 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Companies are dealing with many cognitive changes with the introduction of\nthe Industry 4.0 paradigm. In this constantly changing environment, knowledge\nmanagement is a key factor. Dialog systems, being able to hold a conversation\nwith humans, could support the knowledge management in business environment.\nAlthough, these systems are currently hand-coded and need the intervention of a\nhuman being in writing all the possible questions and answers, and then\nplanning the interactions. This process, besides being time-consuming, is not\nscalable. Conversely, a dialog system, also referred to as chatbot, can be\nbuilt from scratch by simply extracting rules from technical documentation. So,\nthe goal of this research is designing a methodology for automatic building of\nhuman-machine conversational system, able to interact in an industrial\nenvironment. An initial taxonomy, containing entities expected to be found in\nmaintenance manuals, is used to identify the relevant sentences of a manual\nprovided by the company BOBST SA and applying text mining techniques, it is\nautomatically expanded. The final result is a taxonomy network representing the\nentities and their relation, that will be used in future works for managing the\ninteractions of a maintenance chatbot.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 18:39:00 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Coli", "Elena", ""], ["Melluso", "Nicola", ""], ["Fantoni", "Gualtiero", ""], ["Mazzei", "Daniele", ""]]}, {"id": "2005.06616", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Varun Gupta, Ekaterina Kochmar, Dung D. Vu, Robert\n  Belfer, Joelle Pineau, Aaron Courville, Laurent Charlin, Yoshua Bengio", "title": "A Large-Scale, Open-Domain, Mixed-Interface Dialogue-Based ITS for STEM", "comments": "6 pages, 1 figure, 1 table, accepted for publication in the 21st\n  International Conference on Artificial Intelligence in Education (AIED 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Korbit, a large-scale, open-domain, mixed-interface,\ndialogue-based intelligent tutoring system (ITS). Korbit uses machine learning,\nnatural language processing and reinforcement learning to provide interactive,\npersonalized learning online. Korbit has been designed to easily scale to\nthousands of subjects, by automating, standardizing and simplifying the content\ncreation process. Unlike other ITS, a teacher can develop new learning modules\nfor Korbit in a matter of hours. To facilitate learning across a widerange of\nSTEM subjects, Korbit uses a mixed-interface, which includes videos,\ninteractive dialogue-based exercises, question-answering, conceptual diagrams,\nmathematical exercises and gamification elements. Korbit has been built to\nscale to millions of students, by utilizing a state-of-the-art cloud-based\nmicro-service architecture. Korbit launched its first course in 2019 on machine\nlearning, and since then over 7,000 students have enrolled. Although Korbit was\ndesigned to be open-domain and highly scalable, A/B testing experiments with\nreal-world students demonstrate that both student learning outcomes and student\nmotivation are substantially improved compared to typical online courses.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 02:45:43 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Gupta", "Varun", ""], ["Kochmar", "Ekaterina", ""], ["Vu", "Dung D.", ""], ["Belfer", "Robert", ""], ["Pineau", "Joelle", ""], ["Courville", "Aaron", ""], ["Charlin", "Laurent", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2005.06842", "submitter": "Diego Garaialde", "authors": "Diego Garaialde, Christopher P. Bowers, Charlie Pinder, Priyal Shah,\n  Shashwat Parashar, Leigh Clark, Benjamin R. Cowan", "title": "Quantifying the Impact of Making and Breaking Interface Habits", "comments": "32 pages, 8 figures, to be published in the International Journal of\n  Human Computer Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The frequency with which people interact with technology means that users may\ndevelop interface habits, i.e. fast, automatic responses to stable interface\ncues. Design guidelines often assume that interface habits are beneficial.\nHowever, we lack quantitative evidence of how the development of habits\nactually affect user performance and an understanding of how changes in the\ninterface design may affect habit development. Our work quantifies the effect\nof habit formation and disruption on user performance in interaction. Through a\nforced choice lab study task (n=19) and in the wild deployment (n=18) of a\nnotificationdialog experiment on smartphones, we show that people become more\naccurate and faster at option selection as they develop an interface habit.\nCrucially this performance gain is entirely eliminated once the habit is\ndisrupted. We discuss reasons for this performance shift and analyse some\ndisadvantages of interface habits, outlining general design patterns on how to\nboth support and disrupt them.Keywords: Interface habits, user behaviour,\nbreaking habit, interaction science, quantitative research.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 09:48:42 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Garaialde", "Diego", ""], ["Bowers", "Christopher P.", ""], ["Pinder", "Charlie", ""], ["Shah", "Priyal", ""], ["Parashar", "Shashwat", ""], ["Clark", "Leigh", ""], ["Cowan", "Benjamin R.", ""]]}, {"id": "2005.07328", "submitter": "Devi Parikh", "authors": "Devi Parikh and C. Lawrence Zitnick", "title": "Exploring Crowd Co-creation Scenarios for Sketches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a first step towards studying the ability of human crowds and machines to\neffectively co-create, we explore several human-only collaborative co-creation\nscenarios. The goal in each scenario is to create a digital sketch using a\nsimple web interface. We find that settings in which multiple humans\niteratively add strokes and vote on the best additions result in the sketches\nwith highest perceived creativity (value + novelty). Lack of collaboration\nleads to a higher variance in quality and lower novelty or surprise.\nCollaboration without voting leads to high novelty but low quality.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 02:28:35 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 02:12:21 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Parikh", "Devi", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "2005.07478", "submitter": "Sean Walton", "authors": "Sean P. Walton and Alma A. M. Rahat and James Stovold", "title": "Evaluating Mixed-Initiative Procedural Level Design Tools using a\n  Triple-Blind Mixed-Method User Study", "comments": "Accepted to be Published in: IEEE Transactions on Games", "journal-ref": null, "doi": "10.1109/TG.2021.3086215", "report-no": null, "categories": "cs.NE cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Results from a triple-blind mixed-method user study into the effectiveness of\nmixed-initiative tools for the procedural generation of game levels are\npresented. A tool which generates levels using interactive evolutionary\noptimisation was designed for this study which (a) is focused on supporting the\ndesigner to explore the design space and (b) only requires the designer to\ninteract with it by designing levels. The tool identifies level design patterns\nin an initial hand-designed map and uses that information to drive an\ninteractive optimisation algorithm. A rigorous user study was designed which\ncompared the experiences of designers using the mixed-initiative tool to\ndesigners who were given a tool which provided completely random level\nsuggestions. The designers using the mixed-initiative tool showed an increased\nengagement in the level design task, reporting that it was effective in\ninspiring new ideas and design directions. This provides significant evidence\nthat procedural content generation can be used as a powerful tool to support\nthe human design process.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 11:40:53 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 08:46:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Walton", "Sean P.", ""], ["Rahat", "Alma A. M.", ""], ["Stovold", "James", ""]]}, {"id": "2005.07781", "submitter": "Forrest Huang", "authors": "Forrest Huang, Eldon Schoop, David Ha, John Canny", "title": "Scones: Towards Conversational Authoring of Sketches", "comments": "Long Paper, IUI '20: Proceedings of the 25th International Conference\n  on Intelligent User Interfaces", "journal-ref": null, "doi": "10.1145/3377325.3377485", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iteratively refining and critiquing sketches are crucial steps to developing\neffective designs. We introduce Scones, a mixed-initiative,\nmachine-learning-driven system that enables users to iteratively author\nsketches from text instructions. Scones is a novel deep-learning-based system\nthat iteratively generates scenes of sketched objects composed with semantic\nspecifications from natural language. Scones exceeds state-of-the-art\nperformance on a text-based scene modification task, and introduces a\nmask-conditioned sketching model that can generate sketches with poses\nspecified by high-level scene information. In an exploratory user evaluation of\nScones, participants reported enjoying an iterative drawing task with Scones,\nand suggested additional features for further applications. We believe Scones\nis an early step towards automated, intelligent systems that support\nhuman-in-the-loop applications for communicating ideas through sketching in art\nand design.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 00:02:25 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Huang", "Forrest", ""], ["Schoop", "Eldon", ""], ["Ha", "David", ""], ["Canny", "John", ""]]}, {"id": "2005.07792", "submitter": "Youyang Hou", "authors": "Youyang Hou", "title": "Technologies and Workflow of Creative Coding Projects: Examples from the\n  Google DevArt Competition", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many artists and creative technologists created computer\nprogramming as a goal to create something expressive instead of something\nfunctional. In this paper, I analyzed 18 creative coding projects from the\nGoogle DevArt competition and summarized the critical technologies, types of\ncontent, and key workflows of these creative coding projects. The paper also\ndiscusses the potential research opportunities for creative coding and art\ntechnologies.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 21:32:19 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Hou", "Youyang", ""]]}, {"id": "2005.07872", "submitter": "Yoon Kyung Lee", "authors": "Yoon Kyung Lee, Yong-Eun Rhee, Jeh-Kwang Ryu, Sowon Hahn", "title": "Gentlemen on the Road: Understanding How Pedestrians Interpret Yielding\n  Behavior of Autonomous Vehicles using Machine Learning", "comments": "14 pages, 8 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous vehicles (AVs) can prevent collisions by understanding pedestrian\nintention. We conducted a virtual reality experiment with 39 participants and\nmeasured crossing times (seconds) and head orientation (yaw degrees). We\nmanipulated AV yielding behavior (no-yield, slow-yield, and fast-yield) and the\nAV size (small, medium, and large). Using machine learning approach, we\nclassified head orientation change of pedestrians by time into 6 clusters of\npatterns. Results indicate that pedestrian head orientation change was\ninfluenced by AV yielding behavior as well as the size of the AV. Participants\nfixated on the front most of the time even when the car approached near.\nParticipants changed head orientation most frequently when a large size AV did\nnot yield (no-yield). In post-experiment interviews, participants reported that\nyielding behavior and size affected their decision to cross and perceived\nsafety. For autonomous vehicles to be perceived more safe and trustful,\nvehicle-specific factors such as size and yielding behavior should be\nconsidered in the designing process.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 04:54:37 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2020 08:07:49 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Lee", "Yoon Kyung", ""], ["Rhee", "Yong-Eun", ""], ["Ryu", "Jeh-Kwang", ""], ["Hahn", "Sowon", ""]]}, {"id": "2005.07898", "submitter": "Fatemah Husain", "authors": "Fatemah Husain, Vivian Motti", "title": "Social Media Usage in Kuwait: A Comparison of Perspectives Between\n  Healthcare Practitioners and Patients", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Media has been transforming numerous activities of everyday life,\nimpacting also healthcare. However, few studies investigate the medical use of\nsocial media by patients and medical practitioners, especially in the Arabian\nGulf region and Kuwait. To understand the behavior of patients and medical\npractitioners in social media toward healthcare and medical purposes, we\nconducted user studies. Through an online survey, we identified a decrease in\npatients and medical practitioners use of social media for medical purposes.\nPatients reported to be more aware than practitioners concerning: health\neducation, health-related network support, and communication activities. While\npractitioners use social media mostly as a source of medical information, for\nclinician marketing and for professional development. The findings highlighted\nthe need to design a social media platform that support healthcare online\ncampaign, professional career identity, medical repository, and social privacy\nsetting to increase users engagements toward medical purposes.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 08:12:03 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Husain", "Fatemah", ""], ["Motti", "Vivian", ""]]}, {"id": "2005.08076", "submitter": "Richard Jiang", "authors": "Fraser Young, L Zhang, Richard Jiang, Han Liu and Conor Wall", "title": "A Deep Learning based Wearable Healthcare IoT Device for AI-enabled\n  Hearing Assistance Automation", "comments": null, "journal-ref": "The 2020 International Conference on Machine Learning and\n  Cybernetics", "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the recent booming of artificial intelligence (AI), particularly deep\nlearning techniques, digital healthcare is one of the prevalent areas that\ncould gain benefits from AI-enabled functionality. This research presents a\nnovel AI-enabled Internet of Things (IoT) device operating from the ESP-8266\nplatform capable of assisting those who suffer from impairment of hearing or\ndeafness to communicate with others in conversations. In the proposed solution,\na server application is created that leverages Google's online speech\nrecognition service to convert the received conversations into texts, then\ndeployed to a micro-display attached to the glasses to display the conversation\ncontents to deaf people, to enable and assist conversation as normal with the\ngeneral population. Furthermore, in order to raise alert of traffic or\ndangerous scenarios, an 'urban-emergency' classifier is developed using a deep\nlearning model, Inception-v4, with transfer learning to detect/recognize\nalerting/alarming sounds, such as a horn sound or a fire alarm, with texts\ngenerated to alert the prospective user. The training of Inception-v4 was\ncarried out on a consumer desktop PC and then implemented into the AI based IoT\napplication. The empirical results indicate that the developed prototype system\nachieves an accuracy rate of 92% for sound recognition and classification with\nreal-time performance.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 19:42:16 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Young", "Fraser", ""], ["Zhang", "L", ""], ["Jiang", "Richard", ""], ["Liu", "Han", ""], ["Wall", "Conor", ""]]}, {"id": "2005.08090", "submitter": "Daniel Karl I. Weidele", "authors": "Loraine Franke, Daniel Karl I. Weidele, Fan Zhang, Suheyla\n  Cetin-Karayumak, Steve Pieper, Lauren J. O'Donnell, Yogesh Rathi, Daniel\n  Haehn", "title": "FiberStars: Visual Comparison of Diffusion Tractography Data between\n  Multiple Subjects", "comments": "10 pages, 9 figures", "journal-ref": "2021 IEEE 14th Pacific Visualization Symposium (PacificVis)", "doi": "10.1109/PacificVis52677.2021.00023", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tractography from high-dimensional diffusion magnetic resonance imaging\n(dMRI) data allows brain's structural connectivity analysis. Recent dMRI\nstudies aim to compare connectivity patterns across subject groups and disease\npopulations to understand subtle abnormalities in the brain's white matter\nconnectivity and distributions of biologically sensitive dMRI derived metrics.\nExisting software products focus solely on the anatomy, are not intuitive or\nrestrict the comparison of multiple subjects. In this paper, we present the\ndesign and implementation of FiberStars, a visual analysis tool for\ntractography data that allows the interactive visualization of brain fiber\nclusters combining existing 3D anatomy with compact 2D visualizations. With\nFiberStars, researchers can analyze and compare multiple subjects in large\ncollections of brain fibers using different views. To evaluate the usability of\nour software, we performed a quantitative user study. We asked domain experts\nand non-experts to find patterns in a tractography dataset with either\nFiberStars or an existing dMRI exploration tool. Our results show that\nparticipants using FiberStars can navigate extensive collections of\ntractography faster and more accurately. All our research, software, and\nresults are available openly.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 20:23:46 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:57:13 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Franke", "Loraine", ""], ["Weidele", "Daniel Karl I.", ""], ["Zhang", "Fan", ""], ["Cetin-Karayumak", "Suheyla", ""], ["Pieper", "Steve", ""], ["O'Donnell", "Lauren J.", ""], ["Rathi", "Yogesh", ""], ["Haehn", "Daniel", ""]]}, {"id": "2005.08101", "submitter": "Marie Destandau", "authors": "Marie Destandau and Jean-Daniel Fekete", "title": "The Missing Path: Analysing Incompleteness in Knowledge Graphs", "comments": "10 pages, 11 figures", "journal-ref": null, "doi": "10.1177/1473871621991539", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Knowledge Graphs (KG) allow to merge and connect heterogeneous data despite\ntheir differences; they are incomplete by design. Yet, KG data producers need\nto ensure the best level of completeness, as far as possible. The difficulty is\nthat they have no means to distinguish cases where incomplete entities could\nand should be fixed. We present a new visualisation tool: The Missing Path, to\nsupport them in identifying coherent subsets of entities that can be repaired.\nIt relies on a map, grouping entities according to their incomplete profile.\nThe map is coordinated with histograms and stacked charts to support\ninteractive exploration and analysis; the summary of a subset can be compared\nwith the one of the full collection to reveal its distinctive features. We\nconduct an iterative design process and evaluation with 9 Wikidata\ncontributors. Participants gain insights and find various strategies to\nidentify coherent subsets to be fixed.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 21:04:56 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 19:46:23 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 15:28:59 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Destandau", "Marie", ""], ["Fekete", "Jean-Daniel", ""]]}, {"id": "2005.08231", "submitter": "Johanna Johansen Ms", "authors": "Johanna Johansen, Tore Pedersen, Christian Johansen", "title": "Studying the Transfer of Biases from Programmers to Programs", "comments": "40 pages of which 7 pages of Appendix, 26 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is generally agreed that one origin of machine bias is resulting from\ncharacteristics within the dataset on which the algorithms are trained, i.e.,\nthe data does not warrant a generalized inference. We, however, hypothesize\nthat a different `mechanism', hitherto not articulated in the literature, may\nalso be responsible for machine's bias, namely that biases may originate from\n(i) the programmers' cultural background, such as education or line of work, or\n(ii) the contextual programming environment, such as software requirements or\ndeveloper tools. Combining an experimental and comparative design, we studied\nthe effects of cultural metaphors and contextual metaphors, and tested whether\neach of these would `transfer' from the programmer to program, thus\nconstituting a machine bias. The results show (i) that cultural metaphors\ninfluence the programmer's choices and (ii) that `induced' contextual metaphors\ncan be used to moderate or exacerbate the effects of the cultural metaphors.\nThis supports our hypothesis that biases in automated systems do not always\noriginate from within the machine's training data. Instead, machines may also\n`replicate' and `reproduce' biases from the programmers' cultural background by\nthe transfer of cultural metaphors into the programming process. Implications\nfor academia and professional practice range from the micro programming-level\nto the macro national-regulations or educational level, and span across all\nsocietal domains where software-based systems are operating such as the popular\nAI-based automated decision support systems.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 11:51:06 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 17:06:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Johansen", "Johanna", ""], ["Pedersen", "Tore", ""], ["Johansen", "Christian", ""]]}, {"id": "2005.08430", "submitter": "Young-Eun Lee", "authors": "Young-Eun Lee and Minji Lee and Seong-Whan Lee", "title": "Reconstructing ERP Signals Using Generative Adversarial Networks for\n  Mobile Brain-Machine Interface", "comments": "Submitted to IEEE International Conference on System, Man, and\n  Cybernetics (SMC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Practical brain-machine interfaces have been widely studied to accurately\ndetect human intentions using brain signals in the real world. However, the\nelectroencephalography (EEG) signals are distorted owing to the artifacts such\nas walking and head movement, so brain signals may be large in amplitude rather\nthan desired EEG signals. Due to these artifacts, detecting accurately human\nintention in the mobile environment is challenging. In this paper, we proposed\nthe reconstruction framework based on generative adversarial networks using the\nevent-related potentials (ERP) during walking. We used a pre-trained\nconvolutional encoder to represent latent variables and reconstructed ERP\nthrough the generative model which shape similar to the opposite of encoder.\nFinally, the ERP was classified using the discriminative model to demonstrate\nthe validity of our proposed framework. As a result, the reconstructed signals\nhad important components such as N200 and P300 similar to ERP during standing.\nThe accuracy of reconstructed EEG was similar to raw noisy EEG signals during\nwalking. The signal-to-noise ratio of reconstructed EEG was significantly\nincreased as 1.3. The loss of the generative model was 0.6301, which is\ncomparatively low, which means training generative model had high performance.\nThe reconstructed ERP consequentially showed an improvement in classification\nperformance during walking through the effects of noise reduction. The proposed\nframework could help recognize human intention based on the brain-machine\ninterface even in the mobile environment.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 02:39:16 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Lee", "Young-Eun", ""], ["Lee", "Minji", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2005.08535", "submitter": "Orestis Georgiou", "authors": "Gareth Young, Hamish Milne, Daniel Griffiths, Elliot Padfield, Robert\n  Blenkinsopp, Orestis Georgiou", "title": "Designing Mid-Air Haptic Gesture Controlled User Interfaces for Cars", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": "10.1145/3397869", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present advancements in the design and development of in-vehicle\ninfotainment systems that utilize gesture input and ultrasonic mid-air haptic\nfeedback. Such systems employ state-of-the-art hand tracking technology and\nnovel haptic feedback technology and promise to reduce driver distraction while\nperforming a secondary task therefore cutting the risk of road accidents. In\nthis paper, we document design process considerations during the development of\na mid-air haptic gesture-enabled user interface for human-vehicle-interactions.\nThis includes an online survey, business development insights, background\nresearch, and an agile framework component with three prototype iterations and\nuser-testing on a simplified driving simulator. We report on the reasoning that\nled to the convergence of the chosen gesture-input and haptic-feedback sets\nused in the final prototype, discuss the lessons learned, and give hints and\ntips that act as design guidelines for future research and development of this\ntechnology in cars.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 08:48:14 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Young", "Gareth", ""], ["Milne", "Hamish", ""], ["Griffiths", "Daniel", ""], ["Padfield", "Elliot", ""], ["Blenkinsopp", "Robert", ""], ["Georgiou", "Orestis", ""]]}, {"id": "2005.08637", "submitter": "Zhentao Huang", "authors": "Xiangjun Peng, Zhentao Huang, Xu Sun", "title": "Building BROOK: A Multi-modal and Facial Video Database for\n  Human-Vehicle Interaction Research", "comments": "Conference: ACM CHI Conference on Human Factors in Computing Systems\n  Workshops (CHI'20 Workshops)At: Honolulu, Hawaii, USA\n  URL:https://emergentdatatrails.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity of Autonomous Vehicles, more opportunities have\nbloomed in the context of Human-Vehicle Interactions. However, the lack of\ncomprehensive and concrete database support for such specific use case limits\nrelevant studies in the whole design spaces. In this paper, we present our\nwork-in-progress BROOK, a public multi-modal database with facial video\nrecords, which could be used to characterize drivers' affective states and\ndriving styles. We first explain how we over-engineer such database in details,\nand what we have gained through a ten-month study. Then we showcase a Neural\nNetwork-based predictor, leveraging BROOK, which supports multi-modal\nprediction (including physiological data of heart rate and skin conductance and\ndriving status data of speed)through facial videos. Finally, we discuss related\nissues when building such a database and our future directions in the context\nof BROOK. We believe BROOK is an essential building block for future\nHuman-Vehicle Interaction Research.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 12:20:17 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 14:42:30 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Peng", "Xiangjun", ""], ["Huang", "Zhentao", ""], ["Sun", "Xu", ""]]}, {"id": "2005.08658", "submitter": "Avishek Anand", "authors": "Avishek Anand, Lawrence Cavedon, Matthias Hagen, Hideo Joho, Mark\n  Sanderson, and Benno Stein", "title": "Conversational Search -- A Report from Dagstuhl Seminar 19461", "comments": "contains arXiv:2001.06910, arXiv:2001.02912", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dagstuhl Seminar 19461 \"Conversational Search\" was held on 10-15 November\n2019. 44~researchers in Information Retrieval and Web Search, Natural Language\nProcessing, Human Computer Interaction, and Dialogue Systems were invited to\nshare the latest development in the area of Conversational Search and discuss\nits research agenda and future directions. A 5-day program of the seminar\nconsisted of six introductory and background sessions, three visionary talk\nsessions, one industry talk session, and seven working groups and reporting\nsessions. The seminar also had three social events during the program. This\nreport provides the executive summary, overview of invited talks, and findings\nfrom the seven working groups which cover the definition, evaluation,\nmodelling, explanation, scenarios, applications, and prototype of\nConversational Search. The ideas and findings presented in this report should\nserve as one of the main sources for diverse research programs on\nConversational Search.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 12:48:33 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Anand", "Avishek", ""], ["Cavedon", "Lawrence", ""], ["Hagen", "Matthias", ""], ["Joho", "Hideo", ""], ["Sanderson", "Mark", ""], ["Stein", "Benno", ""]]}, {"id": "2005.08834", "submitter": "Shubham Jain", "authors": "Slobodan Milanko, Alexander Launi, and Shubham Jain", "title": "Designing Just-in-Time Detection for Gamified Fitness Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our findings from a multi-year effort to detect motion\nevents early using inertial sensors in real-world settings. We believe early\nevent detection is the next step in advancing motion tracking, and can enable\njust-in-time interventions, particularly for mHealth applications. Our system\ntargets strength training workouts in the fitness domain, where users perform\nwell-defined movements for each exercise, while wearing an inertial sensor. We\ncollect data for 20 exercises across 12 users over 26 months. We propose an\nalgorithm to detect repetitions before they end, to allow a user to visualize\nmovement derived metrics in real-time. We further develop a gamified approach\nto display this information to the user and encourage them to perform\nconsistent movements. Participants in a feasibility study find the gamified\nfeedback useful in improving their form. Our system can detect repetition\nevents as early as 500 ms before it ends, which is 2x faster and more accurate\nthan state-of-the-art trackers. We believe our approach will open exciting\navenues for tracking, detection, and gamification for fitness frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:04:15 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Milanko", "Slobodan", ""], ["Launi", "Alexander", ""], ["Jain", "Shubham", ""]]}, {"id": "2005.08874", "submitter": "Tobias Huber", "authors": "Tobias Huber, Katharina Weitz, Elisabeth Andr\\'e, Ofra Amir", "title": "Local and Global Explanations of Agent Behavior: Integrating Strategy\n  Summaries with Saliency Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.NE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With advances in reinforcement learning (RL), agents are now being developed\nin high-stakes application domains such as healthcare and transportation.\nExplaining the behavior of these agents is challenging, as the environments in\nwhich they act have large state spaces, and their decision-making can be\naffected by delayed rewards, making it difficult to analyze their behavior. To\naddress this problem, several approaches have been developed. Some approaches\nattempt to convey the $\\textit{global}$ behavior of the agent, describing the\nactions it takes in different states. Other approaches devised $\\textit{local}$\nexplanations which provide information regarding the agent's decision-making in\na particular state. In this paper, we combine global and local explanation\nmethods, and evaluate their joint and separate contributions, providing (to the\nbest of our knowledge) the first user study of combined local and global\nexplanations for RL agents. Specifically, we augment strategy summaries that\nextract important trajectories of states from simulations of the agent with\nsaliency maps which show what information the agent attends to. Our results\nshow that the choice of what states to include in the summary (global\ninformation) strongly affects people's understanding of agents: participants\nshown summaries that included important states significantly outperformed\nparticipants who were presented with agent behavior in a randomly set of chosen\nworld-states. We find mixed results with respect to augmenting demonstrations\nwith saliency maps (local information), as the addition of saliency maps did\nnot significantly improve performance in most cases. However, we do find some\nevidence that saliency maps can help users better understand what information\nthe agent relies on in its decision making, suggesting avenues for future work\nthat can further improve explanations of RL agents.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 16:44:55 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 17:34:10 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 17:54:59 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Huber", "Tobias", ""], ["Weitz", "Katharina", ""], ["Andr\u00e9", "Elisabeth", ""], ["Amir", "Ofra", ""]]}, {"id": "2005.08916", "submitter": "Sergey Alyaev", "authors": "Sergey Alyaev, Reidar Brumer Bratvold, Sofija Ivanova, Andrew\n  Holsaeter, Morten Bendiksen", "title": "Man vs machine: an experimental study of geosteering decision skills", "comments": "Additional analyses of the results leads to different conclusion\n  rending the current paper irrelevant and, possibly, confusing. The data\n  presented in this study is reused in arXiv:2011.00733", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the steady growth of the amount of real-time data while drilling,\noperational decision-making is becoming both better informed and more complex.\nTherefore, as no human brain has the capacity to interpret and integrate all\ndecision-relevant information from the data, the adoption of advanced\nalgorithms is required not only for data interpretation but also for decision\noptimization itself. However, the advantages of the automatic decision-making\nare hard to quantify.\n  The main contribution of this paper is an experiment in which we compare the\ndecision skills of geosteering experts with those of an automatic decision\nsupport system in a fully controlled synthetic environment. The implementation\nof the system, hereafter called DSS-1, is presented in our earlier work [Alyaev\net al. \"A decision support system for multi-target geosteering.\" Journal of\nPetroleum Science and Engineering 183 (2019)]. For the current study we have\ndeveloped an easy-to-use web-based platform which can visualize and update\nuncertainties in a 2D geological model. The platform has both user and\napplication interfaces (GUI and API) allowing us to put human participants and\nDSS-1 into a similar environment and conditions.\n  The results of comparing 29 geoscientists with DSS-1 over three experimental\nrounds showed that the automatic algorithm outperformed 28 participants. What's\nmore, no expert has beaten DSS-1 more than once over the three rounds, giving\nit the best comparative rating among the participants.\n  By design DSS-1 performs consistently, that is, identical problem setup is\nguaranteed to yield identical decisions. The study showed that only two experts\nmanaged to demonstrate partial consistency within a tolerance but ended up with\nmuch lower scores.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:42:46 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 10:58:26 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Alyaev", "Sergey", ""], ["Bratvold", "Reidar Brumer", ""], ["Ivanova", "Sofija", ""], ["Holsaeter", "Andrew", ""], ["Bendiksen", "Morten", ""]]}, {"id": "2005.08952", "submitter": "Tommy Nilsson", "authors": "Tommy Nilsson, Joel E. Fischer, Andy Crabtree, Murray Goulden, Jocelyn\n  Spence, Enrico Costanza", "title": "Visions, Values, and Videos: Revisiting Envisionings in Service of\n  UbiComp Design for the Home", "comments": "DIS'20, July 6-10, 2020, Eindhoven, Netherlands", "journal-ref": null, "doi": "10.1145/3357236.3395476", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UbiComp has been envisioned to bring about a future dominated by calm\ncomputing technologies making our everyday lives ever more convenient. Yet the\nsame vision has also attracted criticism for encouraging a solitary and passive\nlifestyle. The aim of this paper is to explore and elaborate these tensions\nfurther by examining the human values surrounding future domestic UbiComp\nsolutions. Drawing on envisioning and contravisioning, we probe members of the\npublic (N=28) through the presentation and focus group discussion of two\ncontrasting animated video scenarios, where one is inspired by \"calm\" and the\nother by \"engaging\" visions of future UbiComp technology. By analysing the\nreasoning of our participants, we identify and elaborate a number of relevant\nvalues involved in balancing the two perspectives. In conclusion, we articulate\npractically applicable takeaways in the form of a set of key design questions\nand challenges.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 21:56:26 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 05:51:02 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Nilsson", "Tommy", ""], ["Fischer", "Joel E.", ""], ["Crabtree", "Andy", ""], ["Goulden", "Murray", ""], ["Spence", "Jocelyn", ""], ["Costanza", "Enrico", ""]]}, {"id": "2005.09100", "submitter": "Kostadin Kushlev", "authors": "Kostadin Kushlev and Matthew R Leitao", "title": "The Effects of Smartphones on Well-Being: Theoretical Integration and\n  Research Agenda", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.IT cs.MM econ.GN math.IT q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As smartphones become ever more integrated in peoples lives, a burgeoning new\narea of research has emerged on their well-being effects. We propose that\ndisparate strands of research and apparently contradictory findings can be\nintegrated under three basic hypotheses, positing that smartphones influence\nwell-being by (1) replacing other activities (displacement hypothesis), (2)\ninterfering with concurrent activities (interference hypothesis), and (3)\naffording access to information and activities that would otherwise be\nunavailable (complementarity hypothesis). Using this framework, we highlight\nmethodological issues and go beyond net effects to examine how and when phones\nboost versus hurt well-being. We examine both psychological and contextual\nmediators and moderators of the effects, thus outlining an agenda for future\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 21:25:51 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Kushlev", "Kostadin", ""], ["Leitao", "Matthew R", ""]]}, {"id": "2005.09146", "submitter": "Brian Park", "authors": "Brian J. Park, Stephen J. Hunt, Gregory J. Nadolski, Terence P. Gade", "title": "3D Augmented Reality-Assisted CT-Guided Interventions: System Design and\n  Preclinical Trial on an Abdominal Phantom using HoloLens 2", "comments": "16 pages, 6 figures, 2 tables", "journal-ref": null, "doi": "10.1038/s41598-020-75676-4", "report-no": null, "categories": "physics.med-ph cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Out-of-plane lesions pose challenges for CT-guided interventions.\nAugmented reality (AR) headset devices have evolved and are readily capable to\nprovide virtual 3D guidance to improve CT-guided targeting.\n  Purpose: To describe the design of a three-dimensional (3D) AR-assisted\nnavigation system using HoloLens 2 and evaluate its performance through\nCT-guided simulations.\n  Materials and Methods: A prospective trial was performed assessing CT-guided\nneedle targeting on an abdominal phantom with and without AR guidance. A total\nof 8 operators with varying clinical experience were enrolled and performed a\ntotal of 86 needle passes. Procedure efficiency, radiation dose, and\ncomplication rates were compared with and without AR guidance. Vector analysis\nof the first needle pass was also performed.\n  Results: Average total number of needle passes to reach the target reduced\nfrom 7.4 passes without AR to 3.4 passes with AR (54.2% decrease, p=0.011).\nAverage dose-length product (DLP) decreased from 538 mGy-cm without AR to 318\nmGy-cm with AR (41.0% decrease, p=0.009). Complication rate of hitting a\nnon-targeted lesion decreased from 11.9% without AR (7/59 needle passes) to 0%\nwith AR (0/27 needle passes). First needle passes were more nearly aligned with\nthe ideal target trajectory with AR versus without AR (4.6{\\deg} vs 8.0{\\deg}\noffset, respectively, p=0.018). Medical students, residents, and attendings all\nperformed at the same level with AR guidance.\n  Conclusions: 3D AR guidance can provide significant improvements in\nprocedural efficiency and radiation dose savings for targeting challenging,\nout-of-plane lesions. AR guidance elevated the performance of all operators to\nthe same level irrespective of prior clinical experience.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 00:22:24 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Park", "Brian J.", ""], ["Hunt", "Stephen J.", ""], ["Nadolski", "Gregory J.", ""], ["Gade", "Terence P.", ""]]}, {"id": "2005.09324", "submitter": "Gorm Lai", "authors": "Gorm Lai, William Latham, Frederic Fol Leymarie", "title": "Towards Friendly Mixed Initiative Procedural Content Generation: Three\n  Pillars of Industry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the games industry is moving towards procedural content generation\n(PCG) with tools available under popular platforms such as Unreal, Unity or\nHoudini, and video game titles like No Man's Sky and Horizon Zero Dawn taking\nadvantage of PCG, the gap between academia and industry is as wide as it has\never been, in terms of communication and sharing methods. One of the authors,\nhas worked on both sides of this gap and in an effort to shorten it and\nincrease the synergy between the two sectors, has identified three design\npillars for PCG using mixed-initiative interfaces. The three pillars are\nRespect Designer Control, Respect the Creative Process and Respect Existing\nWork Processes. Respecting designer control is about creating a tool that gives\nenough control to bring out the designer's vision. Respecting the creative\nprocess concerns itself with having a feedback loop that is short enough, that\nthe creative process is not disturbed. Respecting existing work processes means\nthat a PCG tool should plug in easily to existing asset pipelines. As academics\nand communicators, it is surprising that publications often do not describe\nways for developers to use our work or lack considerations for how a piece of\nwork might fit into existing content pipelines.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 09:42:40 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Lai", "Gorm", ""], ["Latham", "William", ""], ["Leymarie", "Frederic Fol", ""]]}, {"id": "2005.09834", "submitter": "Vikram Ramanarayanan", "authors": "Vikram Ramanarayanan and Matthew Mulholland and Debanjan Ghosh", "title": "Exploring Recurrent, Memory and Attention Based Architectures for\n  Scoring Interactional Aspects of Human-Machine Text Dialog", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important step towards enabling English language learners to improve their\nconversational speaking proficiency involves automated scoring of multiple\naspects of interactional competence and subsequent targeted feedback. This\npaper builds on previous work in this direction to investigate multiple neural\narchitectures -- recurrent, attention and memory based -- along with\nfeature-engineered models for the automated scoring of interactional and topic\ndevelopment aspects of text dialog data. We conducted experiments on a\nconversational database of text dialogs from human learners interacting with a\ncloud-based dialog system, which were triple-scored along multiple dimensions\nof conversational proficiency. We find that fusion of multiple architectures\nperforms competently on our automated scoring task relative to expert\ninter-rater agreements, with (i) hand-engineered features passed to a support\nvector learner and (ii) transformer-based architectures contributing most\nprominently to the fusion.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 03:23:00 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Ramanarayanan", "Vikram", ""], ["Mulholland", "Matthew", ""], ["Ghosh", "Debanjan", ""]]}, {"id": "2005.09993", "submitter": "Fatemah Husain", "authors": "Fatemah Husain", "title": "Investigating Current State-of-The-Art Applications of Supportive\n  Technologies for Individuals with ADHD", "comments": "43 pages (systematic literature review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention Deficit Hyperactivity Disorder (ADHD) is a chronic mental and\nbehavioral disorder that interferes with everyday activities and has three core\nsymptoms: inattention, hyperactivity, and impulsivity. To help in reducing the\neffects of ADHD symptoms, there are multiple treatments, but none of them help\nin curing ADHD. Assistive technologies offer great opportunities in delivering\ntreatments, especially those related to behavioral interventions, monitoring,\nand changing in a more flexible, acceptable and accessible way. Focusing on\nassistive technology for children with ADHD is very important as early support\nduring childhood prevents the manifestation of its symptoms before entering\nadulthood. This systematic literature review paper investigates the available\nstudies covering assistive technologies for children with ADHD. The\ncontribution of this paper can help Human-Computer Interaction researchers to\nidentify the procedures and research methods used throughout requirements,\ndesign, and evaluation phases in developing assistive technology for children\nwith ADHD. Moreover, it provides researchers with information regarding\nframeworks and protocols of conducting studies on ADHD, current available\nsolutions, and their limitations.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 09:09:30 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Husain", "Fatemah", ""]]}, {"id": "2005.10044", "submitter": "Leonhard Hermansdorfer", "authors": "Leonhard Hermansdorfer, Johannes Betz, Markus Lienkamp", "title": "Benchmarking of a software stack for autonomous racing against a\n  professional human race driver", "comments": "Accepted at 2020 Fifteenth International Conference on Ecological\n  Vehicles and Renewable Energies (EVER)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The way to full autonomy of public road vehicles requires the step-by-step\nreplacement of the human driver, with the ultimate goal of replacing the driver\ncompletely. Eventually, the driving software has to be able to handle all\nsituations that occur on its own, even emergency situations. These particular\nsituations require extreme combined braking and steering actions at the limits\nof handling to avoid an accident or to diminish its consequences. An average\nhuman driver is not trained to handle such extreme and rarely occurring\nsituations and therefore often fails to do so. However, professional race\ndrivers are trained to drive a vehicle utilizing the maximum amount of possible\ntire forces. These abilities are of high interest for the development of\nautonomous driving software. Here, we compare a professional race driver and\nour software stack developed for autonomous racing with data analysis\ntechniques established in motorsports. The goal of this research is to derive\nindications for further improvement of the performance of our software and to\nidentify areas where it still fails to meet the performance level of the human\nrace driver. Our results are used to extend our software's capabilities and\nalso to incorporate our findings into the research and development of public\nroad autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:40:27 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Hermansdorfer", "Leonhard", ""], ["Betz", "Johannes", ""], ["Lienkamp", "Markus", ""]]}, {"id": "2005.10067", "submitter": "Biplav Srivastava", "authors": "Biplav Srivastava and Francesca Rossi and Sheema Usmani and and\n  Mariana Bernagozzi", "title": "Personalized Chatbot Trustworthiness Ratings", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversation agents, commonly referred to as chatbots, are increasingly\ndeployed in many domains to allow people to have a natural interaction while\ntrying to solve a specific problem. Given their widespread use, it is important\nto provide their users with methods and tools to increase users awareness of\nvarious properties of the chatbots, including non-functional properties that\nusers may consider important in order to trust a specific chatbot. For example,\nusers may want to use chatbots that are not biased, that do not use abusive\nlanguage, that do not leak information to other users, and that respond in a\nstyle which is appropriate for the user's cognitive level.\n  In this paper, we address the setting where a chatbot cannot be modified, its\ntraining data cannot be accessed, and yet a neutral party wants to assess and\ncommunicate its trustworthiness to a user, tailored to the user's priorities\nover the various trust issues. Such a rating can help users choose among\nalternative chatbots, developers test their systems, business leaders price\ntheir offering, and regulators set policies. We envision a personalized rating\nmethodology for chatbots that relies on separate rating modules for each issue,\nand users' detected priority orderings among the relevant trust issues, to\ngenerate an aggregate personalized rating for the trustworthiness of a chatbot.\nThe method is independent of the specific trust issues and is parametric to the\naggregation procedure, thereby allowing for seamless generalization. We\nillustrate its general use, integrate it with a live chatbot, and evaluate it\non four dialog datasets and representative user profiles, validated with user\nsurveys.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 22:42:45 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 01:13:36 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Srivastava", "Biplav", ""], ["Rossi", "Francesca", ""], ["Usmani", "Sheema", ""], ["Bernagozzi", "and Mariana", ""]]}, {"id": "2005.10161", "submitter": "Mu Mu", "authors": "Mu Mu, Murtada Dohan, Alison Goodyear, Gary Hill, Cleyon Johns, and\n  Andreas Mauthe", "title": "User Attention and Behaviour in Virtual Reality Art Encounter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of consumer virtual reality (VR) headsets and creative\ntools, content creators have started to experiment with new forms of\ninteractive audience experience using immersive media. Understanding user\nattention and behaviours in virtual environment can greatly inform creative\nprocesses in VR. We developed an abstract VR painting and an experimentation\nsystem to study audience encounters through eye gaze and movement tracking. The\ndata from a user experiment with 35 participants reveal a range of user\nactivity patterns in art exploration. Deep learning models are used to study\nthe connections between behavioural data and audience background. New\nintegrated methods to visualise user attention as part of the artwork are also\ndeveloped as a feedback loop to the content creator.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 16:09:57 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Mu", "Mu", ""], ["Dohan", "Murtada", ""], ["Goodyear", "Alison", ""], ["Hill", "Gary", ""], ["Johns", "Cleyon", ""], ["Mauthe", "Andreas", ""]]}, {"id": "2005.10612", "submitter": "Olivier Chapuis", "authors": "Rapha\\\"el James (LRI, ILDA), Anastasia Bezerianos (LRI, ILDA), Olivier\n  Chapuis (LRI, ILDA), Maxime Cordeil, Tim Dwyer, Arnaud Prouzeau", "title": "Personal+Context navigation: combining AR and shared displays in network\n  path-following", "comments": null, "journal-ref": "Proceedings of Graphics Interface, May 2020, Toronto, Canada", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shared displays are well suited to public viewing and collaboration, however\nthey lack personal space to view private information and act without disturbing\nothers. Combining them with Augmented Reality (AR) headsets allows interaction\nwithout altering the context on the shared display. We study a set of such\ninteraction techniques in the context of network navigation, in particular path\nfollowing, an important network analysis task. Applications abound, for example\nplanning private trips on a network map shown on a public display.The proposed\ntechniques allow for hands-free interaction, rendering visual aids inside the\nheadset, in order to help the viewer maintain a connection between the AR\ncursor and the network that is only shown on the shared display. In two\nexperiments on path following, we found that adding persistent connections\nbetween the AR cursor and the network on the shared display works well for high\nprecision tasks, but more transient connections work best for lower precision\ntasks. More broadly, we show that combining personal AR interaction with shared\ndisplays is feasible for network navigation.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 14:10:20 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["James", "Rapha\u00ebl", "", "LRI, ILDA"], ["Bezerianos", "Anastasia", "", "LRI, ILDA"], ["Chapuis", "Olivier", "", "LRI, ILDA"], ["Cordeil", "Maxime", ""], ["Dwyer", "Tim", ""], ["Prouzeau", "Arnaud", ""]]}, {"id": "2005.10865", "submitter": "Benjamin Nye", "authors": "Benjamin E. Nye, Ani Nenkova, Iain J. Marshall, Byron C. Wallace", "title": "Trialstreamer: Mapping and Browsing Medical Evidence in Real-Time", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Trialstreamer, a living database of clinical trial reports. Here\nwe mainly describe the evidence extraction component; this extracts from\nbiomedical abstracts key pieces of information that clinicians need when\nappraising the literature, and also the relations between these. Specifically,\nthe system extracts descriptions of trial participants, the treatments compared\nin each arm (the interventions), and which outcomes were measured. The system\nthen attempts to infer which interventions were reported to work best by\ndetermining their relationship with identified trial outcome measures. In\naddition to summarizing individual trials, these extracted data elements allow\nautomatic synthesis of results across many trials on the same topic. We apply\nthe system at scale to all reports of randomized controlled trials indexed in\nMEDLINE, powering the automatic generation of evidence maps, which provide a\nglobal view of the efficacy of different interventions combining data from all\nrelevant clinical trials on a topic. We make all code and models freely\navailable alongside a demonstration of the web interface.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 19:32:04 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Nye", "Benjamin E.", ""], ["Nenkova", "Ani", ""], ["Marshall", "Iain J.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2005.10960", "submitter": "Harini Suresh", "authors": "Harini Suresh, Natalie Lao, Ilaria Liccardi", "title": "Misplaced Trust: Measuring the Interference of Machine Learning in Human\n  Decision-Making", "comments": "10 pages", "journal-ref": "12th ACM Conference on Web Science, July 6-10, 2020, Southampton,\n  United Kingdom", "doi": "10.1145/3394231.3397922", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ML decision-aid systems are increasingly common on the web, but their\nsuccessful integration relies on people trusting them appropriately: they\nshould use the system to fill in gaps in their ability, but recognize signals\nthat the system might be incorrect. We measured how people's trust in ML\nrecommendations differs by expertise and with more system information through a\ntask-based study of 175 adults. We used two tasks that are difficult for\nhumans: comparing large crowd sizes and identifying similar-looking animals.\nOur results provide three key insights: (1) People trust incorrect ML\nrecommendations for tasks that they perform correctly the majority of the time,\neven if they have high prior knowledge about ML or are given information\nindicating the system is not confident in its prediction; (2) Four different\ntypes of system information all increased people's trust in recommendations;\nand (3) Math and logic skills may be as important as ML for decision-makers\nworking with ML recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 01:22:58 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Suresh", "Harini", ""], ["Lao", "Natalie", ""], ["Liccardi", "Ilaria", ""]]}, {"id": "2005.11151", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Adytia Suri, Ionut E. Iacob, Ioana R. Goldbach,\n  Lateef Rasheed and Paul N. Borza", "title": "Attention Patterns Detection using Brain Computer Interfaces", "comments": null, "journal-ref": "ACM SE 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human brain provides a range of functions such as expressing emotions,\ncontrolling the rate of breathing, etc., and its study has attracted the\ninterest of scientists for many years. As machine learning models become more\nsophisticated, and bio-metric data becomes more readily available through new\nnon-invasive technologies, it becomes increasingly possible to gain access to\ninteresting biometric data that could revolutionize Human-Computer Interaction.\nIn this research, we propose a method to assess and quantify human attention\nlevels and their effects on learning. In our study, we employ a brain computer\ninterface (BCI) capable of detecting brain wave activity and displaying the\ncorresponding electroencephalograms (EEG). We train recurrent neural networks\n(RNNS) to identify the type of activity an individual is performing.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 11:55:37 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Suri", "Adytia", ""], ["Iacob", "Ionut E.", ""], ["Goldbach", "Ioana R.", ""], ["Rasheed", "Lateef", ""], ["Borza", "Paul N.", ""]]}, {"id": "2005.11228", "submitter": "Vedant Das Swain", "authors": "V. Das Swain, H. Kwon, S. Sargolzaei, B. Saket, M. Bin Morshed, K.\n  Tran, D. Patel, Y. Tian, J. Philipose, Y. Cui, T. Pl\\\"otz, M. De Choudhury,\n  G. D. Abowd", "title": "Leveraging WiFi Network Logs to Infer Student Collocation and its\n  Relationship with Academic Performance", "comments": "25 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive understanding of collocation can help understand performance\noutcomes. For university cohorts, this needs data that describes large groups\nover a long period. Harnessing user devices to infer this, while tempting, is\nchallenged by privacy concerns, power consumption, and maintenance issues.\nAlternatively, embedding new sensors in the environment is limited by the\nexpense of covering the entire campus. We investigate the feasibility of\nleveraging WiFi association logs for this purpose. While these provide coarse\napproximations of location, these are easily obtainable and depict multiple\nusers on campus over a semester. We explore how these coarse collocations are\nrelated to individual performance. Specifically, we inspect the association\nbetween individual performance and the collocation behaviors of project group\nmembers. We study 163 students (in 54 project groups) over 14 weeks. After\ndescribing how we determine collocation with the WiFi logs, we present a study\nto analyze how collocation within groups relates to a student's final score. We\nfind collocation behaviors show a significant correlation (Pearson's r = 0.24)\nwith performance -- better than both peer feedback or individual behaviors like\nattendance. Finally, we discuss how repurposing WiFi logs can facilitate\napplications for domains like mental wellbeing and physical health.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 15:06:54 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 21:46:55 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Swain", "V. Das", ""], ["Kwon", "H.", ""], ["Sargolzaei", "S.", ""], ["Saket", "B.", ""], ["Morshed", "M. Bin", ""], ["Tran", "K.", ""], ["Patel", "D.", ""], ["Tian", "Y.", ""], ["Philipose", "J.", ""], ["Cui", "Y.", ""], ["Pl\u00f6tz", "T.", ""], ["De Choudhury", "M.", ""], ["Abowd", "G. D.", ""]]}, {"id": "2005.11445", "submitter": "Zonghe Chua", "authors": "Zonghe Chua, Allison M. Okamura, Darrel R. Deo", "title": "Evaluation of Non-Collocated Force Feedback Driven by Signal-Independent\n  Noise", "comments": "Accepted to ICRA 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals living with paralysis or amputation can operate robotic\nprostheses using input signals based on their intent or attempt to move.\nBecause sensory function is lost or diminished in these individuals, haptic\nfeedback must be non-collocated. The intracortical brain computer interface\n(iBCI) has enabled a variety of neural prostheses for people with paralysis. An\nimportant attribute of the iBCI is that its input signal contains\nsignal-independent noise. To understand the effects of signal-independent noise\non a system with non-collocated haptic feedback and inform iBCI-based\nprostheses control strategies, we conducted an experiment with a conventional\nhaptic interface as a proxy for the iBCI. Able-bodied users were tasked with\nlocating an indentation within a virtual environment using input from their\nright hand. Non-collocated haptic feedback of the interaction forces in the\nvirtual environment was augmented with noise of three different magnitudes and\nsimultaneously rendered on users' left hands. We found increases in distance\nerror of the guess of the indentation location, mean time per trial, mean peak\nabsolute displacement and speed of tool movements during localization for the\nhighest noise level compared to the other two levels. The findings suggest that\nusers have a threshold of disturbance rejection and that they attempt to\nincrease their signal-to-noise ratio through their exploratory actions.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 01:55:42 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Chua", "Zonghe", ""], ["Okamura", "Allison M.", ""], ["Deo", "Darrel R.", ""]]}, {"id": "2005.11474", "submitter": "Emad Aghayi", "authors": "Emad Aghayi, Aaron Massey, Thomas LaToza", "title": "Find Unique Usages: Helping Developers Understand Common Usages", "comments": null, "journal-ref": "VL/HCC 2020", "doi": "10.1109/VL/HCC50065.2020.9127285", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working in large and complex codebases, developers face challenges using\n\\textit{Find Usages} to understand how to reuse classes and methods. To better\nunderstand these challenges, we conducted a small exploratory study with 4\nparticipants. We found that developers often wasted time reading long lists of\nsimilar usages or prematurely focused on a single usage. Based on these\nfindings, we hypothesized that clustering usages by the similarity of their\nsurrounding context might enable developers to more rapidly understand how to\nuse a function. To explore this idea, we designed and implemented \\textit{Find\nUnique Usages}, which extracts usages, computes a diff between pairs of usages,\ngenerates similarity scores, and uses these scores to form usage clusters. To\nevaluate this approach, we conducted a controlled experiment with 12\nparticipants. We found that developers with Find Unique Usages were\nsignificantly faster, completing their task in 35% less time.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 05:20:41 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Aghayi", "Emad", ""], ["Massey", "Aaron", ""], ["LaToza", "Thomas", ""]]}, {"id": "2005.11507", "submitter": "Sonali Agarwal", "authors": "Sonali Agarwal, Narinder Singh Punn, Sanjay Kumar Sonbhadra, M.\n  Tanveer, P. Nagabhushan, K K Soundra Pandian and Praveer Saxena", "title": "Unleashing the power of disruptive and emerging technologies amid\n  COVID-19: A detailed review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The unprecedented outbreak of the novel coronavirus (COVID-19), during early\nDecember 2019 in Wuhan, China, has quickly evolved into a global pandemic,\nbecame a matter of grave concern, and placed government agencies worldwide in a\nprecarious position. The scarcity of resources and lack of experiences to\nendure the COVID-19 pandemic, combined with the fear of future consequences has\nestablished the need for adoption of emerging and future technologies to\naddress the upcoming challenges. Since the last five months, the amount of\npandemic impact has reached its pinnacle that is altering everyone's life; and\nhumans are now bound to adopt safe ways to survive under the risk of being\naffected. Technological advances are now accelerating faster than ever before\nto stay ahead of the consequences and acquire new capabilities to build a safer\nworld. Thus, there is a rising need to unfold the power of emerging, future and\ndisruptive technologies to explore all possible ways to fight against COVID-19.\nIn this review article, we attempt to study all emerging, future, and\ndisruptive technologies that can be utilized to mitigate the impact of\nCOVID-19. Building on background insights, detailed technological specific use\ncases to fight against COVID-19 have been discussed in terms of their\nstrengths, weaknesses, opportunities, and threats (SWOT). As concluding\nremarks, we highlight prioritized research areas and upcoming opportunities to\nblur the lines between the physical, digital, and biological domain-specific\nchallenges and also illuminate collaborative research directions for moving\ntowards a post-COVID-19 world.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 10:09:37 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 13:08:23 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 05:16:27 GMT"}, {"version": "v4", "created": "Mon, 17 Aug 2020 10:31:44 GMT"}, {"version": "v5", "created": "Tue, 29 Dec 2020 09:43:36 GMT"}, {"version": "v6", "created": "Mon, 19 Apr 2021 06:07:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Agarwal", "Sonali", ""], ["Punn", "Narinder Singh", ""], ["Sonbhadra", "Sanjay Kumar", ""], ["Tanveer", "M.", ""], ["Nagabhushan", "P.", ""], ["Pandian", "K K Soundra", ""], ["Saxena", "Praveer", ""]]}, {"id": "2005.11577", "submitter": "Nikesh Bajaj", "authors": "Nikesh Bajaj, Jes\\'us Requena Carri\\'on, Francesco Bellotti", "title": "PhyAAt: Physiology of Auditory Attention to Speech Dataset", "comments": "11 pages, 7 figures, For dataset and supporting resources, please see\n  https://phyaat.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Auditory attention to natural speech is a complex brain process. Its\nquantification from physiological signals can be valuable to improving and\nwidening the range of applications of current brain-computer-interface systems,\nhowever it remains a challenging task. In this article, we present a dataset of\nphysiological signals collected from an experiment on auditory attention to\nnatural speech. In this experiment, auditory stimuli consisting of\nreproductions of English sentences in different auditory conditions were\npresented to 25 non-native participants, who were asked to transcribe the\nsentences. During the experiment, 14 channel electroencephalogram, galvanic\nskin response, and photoplethysmogram signals were collected from each\nparticipant. Based on the number of correctly transcribed words, an attention\nscore was obtained for each auditory stimulus presented to subjects. A strong\ncorrelation ($p<<0.0001$) between the attention score and the auditory\nconditions was found. We also formulate four different predictive tasks\ninvolving the collected dataset and develop a feature extraction framework. The\nresults for each predictive task are obtained using a Support Vector Machine\nwith spectral features, and are better than chance level. The dataset has been\nmade publicly available for further research, along with a python library -\nphyaat to facilitate the preprocessing, modeling, and reproduction of the\nresults presented in this paper. The dataset and other resources are shared on\nwebpage - https://phyaat.github.io.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 17:55:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bajaj", "Nikesh", ""], ["Carri\u00f3n", "Jes\u00fas Requena", ""], ["Bellotti", "Francesco", ""]]}, {"id": "2005.11730", "submitter": "Julian Skirzynski", "authors": "Julian Skirzy\\'nski, Frederic Becker and Falk Lieder", "title": "Automatic Discovery of Interpretable Planning Strategies", "comments": "Submitted to the Special Issue on Reinforcement Learning for Real\n  Life in Machine Learning Journal (2021). Code available at\n  https://github.com/RationalityEnhancement/InterpretableStrategyDiscovery", "journal-ref": null, "doi": "10.1007/s10994-021-05963-2", "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When making decisions, people often overlook critical information or are\noverly swayed by irrelevant information. A common approach to mitigate these\nbiases is to provide decision-makers, especially professionals such as medical\ndoctors, with decision aids, such as decision trees and flowcharts. Designing\neffective decision aids is a difficult problem. We propose that recently\ndeveloped reinforcement learning methods for discovering clever heuristics for\ngood decision-making can be partially leveraged to assist human experts in this\ndesign process. One of the biggest remaining obstacles to leveraging the\naforementioned methods is that the policies they learn are opaque to people. To\nsolve this problem, we introduce AI-Interpret: a general method for\ntransforming idiosyncratic policies into simple and interpretable descriptions.\nOur algorithm combines recent advances in imitation learning and program\ninduction with a new clustering method for identifying a large subset of\ndemonstrations that can be accurately described by a simple, high-performing\ndecision rule. We evaluate our new algorithm and employ it to translate\ninformation-acquisition policies discovered through metalevel reinforcement\nlearning. The results of large behavioral experiments showed that prividing the\ndecision rules generated by AI-Interpret as flowcharts significantly improved\npeople's planning strategies and decisions across three diferent classes of\nsequential decision problems. Moreover, another experiment revealed that this\napproach is significantly more effective than training people by giving them\nperformance feedback. Finally, a series of ablation studies confirmed that\nAI-Interpret is critical to the discovery of interpretable decision rules. We\nconclude that the methods and findings presented herein are an important step\ntowards leveraging automatic strategy discovery to improve human\ndecision-making.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 12:24:52 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 12:55:54 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 05:28:59 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Skirzy\u0144ski", "Julian", ""], ["Becker", "Frederic", ""], ["Lieder", "Falk", ""]]}, {"id": "2005.11799", "submitter": "Priyadarshini Kumari", "authors": "Priyadarshini Kumari and Subhasis Chaudhuri", "title": "Haptic Rendering of Thin, Deformable Objects with Spatially Varying\n  Stiffness", "comments": "Accepted in Eurohaptics 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, we often come across soft objects having spatially varying\nstiffness, such as human palm or a wart on the skin. In this paper, we propose\na novel approach to render thin, deformable objects having spatially varying\nstiffness (inhomogeneous material). We use the classical Kirchhoff thin plate\ntheory to compute the deformation. In general, the physics-based rendering of\nan arbitrary 3D surface is complex and time-consuming. Therefore, we\napproximate the 3D surface locally by a 2D plane using an area-preserving\nmapping technique - Gall-Peters mapping. Once the deformation is computed by\nsolving a fourth-order partial differential equation, we project the points\nback onto the original object for proper haptic rendering. The method was\nvalidated through user experiments and was found to be realistic.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 16:43:24 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 07:30:59 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Kumari", "Priyadarshini", ""], ["Chaudhuri", "Subhasis", ""]]}, {"id": "2005.11884", "submitter": "C. Estelle Smith", "authors": "C. Estelle Smith, Zachary Levonian, Haiwei Ma, Robert Giaquinto, Gemma\n  Lein-Mcdonough, Zixuan Li, Susan O'Conner-Von, Svetlana Yarosh", "title": "\"I Cannot Do All of This Alone\": Exploring Instrumental and Prayer\n  Support in Online Health Communities", "comments": "Pre-print of accepted journal paper to ACM Transactions on\n  Computer-Human Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Health Communities (OHCs) are known to provide substantial emotional\nand informational support to patients and family caregivers facing\nlife-threatening diagnoses like cancer and other illnesses, injuries, or\nchronic conditions. Yet little work explores how OHCs facilitate other vital\nforms of social support, especially instrumental support. We partner with\nCaringBridge.org---a prominent OHC for journaling about health crises---to\ncomplete a two-phase study focused on instrumental support. Phase one involves\na content analysis of 641 CaringBridge updates. Phase two is a survey of 991\nCaringBridge users. Results show that patients and family caregivers diverge\nfrom their support networks in their preferences for specific instrumental\nsupport types. Furthermore, ``prayer support'' emerged as the most prominent\nsupport category across both phases. We discuss design implications to\naccommodate divergent preferences and to expand the instrumental support\nnetwork. We also discuss the need for future work to empower family caregivers\nand to support spirituality.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 01:42:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Smith", "C. Estelle", ""], ["Levonian", "Zachary", ""], ["Ma", "Haiwei", ""], ["Giaquinto", "Robert", ""], ["Lein-Mcdonough", "Gemma", ""], ["Li", "Zixuan", ""], ["O'Conner-Von", "Susan", ""], ["Yarosh", "Svetlana", ""]]}, {"id": "2005.11932", "submitter": "Tuan-Duy Nguyen", "authors": "Tuan-Duy H. Nguyen and Huu-Nghia H. Nguyen", "title": "Towards a Robust WiFi-based Fall Detection with Adversarial Data\n  Augmentation", "comments": "Will appear in Proceedings of the 54th Annual Conference on\n  Information Sciences and Systems (CISS2020)", "journal-ref": "2020 54th Annual Conference on Information Sciences and Systems\n  (CISS), Princeton, NJ, USA, 2020, pp. 1-6", "doi": "10.1109/CISS48834.2020.1570617398", "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent WiFi-based fall detection systems have drawn much attention due to\ntheir advantages over other sensory systems. Various implementations have\nachieved impressive progress in performance, thanks to machine learning and\ndeep learning techniques. However, many of such high accuracy systems have low\nreliability as they fail to achieve robustness in unseen environments. To\naddress that, this paper investigates a method of generalization through\nadversarial data augmentation. Our results show a slight improvement in deep\nlearning-systems in unseen domains, though the performance is not significant.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 05:46:27 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Nguyen", "Tuan-Duy H.", ""], ["Nguyen", "Huu-Nghia H.", ""]]}, {"id": "2005.11935", "submitter": "Min-Liang Wang", "authors": "Anurag Lal, Ming-Hsien Hu, Pei-Yuan Lee, Min Liang Wang", "title": "A Novel Approach of using AR and Smart Surgical Glasses Supported Trauma\n  Care", "comments": "10 pages, 9 Figures, Conference. arXiv admin note: text overlap with\n  arXiv:1801.01560 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BACKGROUND: Augmented reality (AR) is gaining popularity in varying field\nsuch as computer gaming and medical education fields. However, still few of\napplications in real surgeries. Orthopedic surgical applications are currently\nlimited and underdeveloped. - METHODS: The clinic validation was prepared with\nthe currently available AR equipment and software. A total of 1 Vertebroplasty,\n2 ORIF Pelvis fracture, 1 ORIF with PFN for Proximal Femoral Fracture, 1 CRIF\nfor distal radius fracture and 2 ORIF for Tibia Fracture cases were performed\nwith fluoroscopy combined with AR smart surgical glasses system. - RESULTS: A\ntotal of 1 Vertebroplasty, 2 ORIF Pelvis fracture, 1 ORIF with PFN for Proximal\nFemoral Fracture, 1 CRIF for distal radius fracture and 2 ORIF for Tibia\nFracture cases are performed to evaluate the benefits of AR surgery. Among the\nAR surgeries, surgeons wear the smart surgical are lot reduce of eyes of turns\nto focus on the monitors. This paper shows the potential ability of augmented\nreality technology for trauma surgery.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 06:03:30 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Lal", "Anurag", ""], ["Hu", "Ming-Hsien", ""], ["Lee", "Pei-Yuan", ""], ["Wang", "Min Liang", ""]]}, {"id": "2005.11957", "submitter": "Tianshi Li", "authors": "Tianshi Li, Jackie (Junrui) Yang, Cori Faklaris, Jennifer King, Yuvraj\n  Agarwal, Laura Dabbish, Jason I. Hong", "title": "Decentralized is not risk-free: Understanding public perceptions of\n  privacy-utility trade-offs in COVID-19 contact-tracing apps", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contact-tracing apps have potential benefits in helping health authorities to\nact swiftly to halt the spread of COVID-19. However, their effectiveness is\nheavily dependent on their installation rate, which may be influenced by\npeople's perceptions of the utility of these apps and any potential privacy\nrisks due to the collection and releasing of sensitive user data (e.g., user\nidentity and location). In this paper, we present a survey study that examined\npeople's willingness to install six different contact-tracing apps after\ninforming them of the risks and benefits of each design option (with a\nU.S.-only sample on Amazon Mechanical Turk, $N=208$). The six app designs\ncovered two major design dimensions (centralized vs decentralized, basic\ncontact tracing vs. also providing hotspot information), grounded in our\nanalysis of existing contact-tracing app proposals.\n  Contrary to assumptions of some prior work, we found that the majority of\npeople in our sample preferred to install apps that use a centralized server\nfor contact tracing, as they are more willing to allow a centralized authority\nto access the identity of app users rather than allowing tech-savvy users to\ninfer the identity of diagnosed users. We also found that the majority of our\nsample preferred to install apps that share diagnosed users' recent locations\nin public places to show hotspots of infection. Our results suggest that apps\nusing a centralized architecture with strong security protection to do basic\ncontact tracing and providing users with other useful information such as\nhotspots of infection in public places may achieve a high adoption rate in the\nU.S.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 07:50:51 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Li", "Tianshi", "", "Junrui"], ["Jackie", "", "", "Junrui"], ["Yang", "", ""], ["Faklaris", "Cori", ""], ["King", "Jennifer", ""], ["Agarwal", "Yuvraj", ""], ["Dabbish", "Laura", ""], ["Hong", "Jason I.", ""]]}, {"id": "2005.11994", "submitter": "Pradipta Biswas", "authors": "Vinay Krishna Sharma, L.R.D. Murthy, KamalPreet Singh Saluja, Vimal\n  Mollyn, Gourav Sharma and Pradipta Biswas", "title": "Eye Gaze Controlled Robotic Arm for Persons with SSMI", "comments": "Citation: VK Sharma, KPS Saluja, LRD Murthy, G Sharma and P Biswas,\n  Webcam Controlled Robotic Arm for Persons with SSMI, Technology and\n  Disability 32 (3), IOS Press 2020 [Official journal of EU AAATE association]", "journal-ref": null, "doi": "10.3233/TAD-200264", "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: People with severe speech and motor impairment (SSMI) often uses\na technique called eye pointing to communicate with outside world. One of their\nparents, caretakers or teachers hold a printed board in front of them and by\nanalyzing their eye gaze manually, their intentions are interpreted. This\ntechnique is often error prone and time consuming and depends on a single\ncaretaker.\n  Objective: We aimed to automate the eye tracking process electronically by\nusing commercially available tablet, computer or laptop and without requiring\nany dedicated hardware for eye gaze tracking. The eye gaze tracker is used to\ndevelop a video see through based AR (augmented reality) display that controls\na robotic device with eye gaze and deployed for a fabric printing task.\n  Methodology: We undertook a user centred design process and separately\nevaluated the web cam based gaze tracker and the video see through based human\nrobot interaction involving users with SSMI. We also reported a user study on\nmanipulating a robotic arm with webcam based eye gaze tracker.\n  Results: Using our bespoke eye gaze controlled interface, able bodied users\ncan select one of nine regions of screen at a median of less than 2 secs and\nusers with SSMI can do so at a median of 4 secs. Using the eye gaze controlled\nhuman-robot AR display, users with SSMI could undertake representative pick and\ndrop task at an average duration less than 15 secs and reach a randomly\ndesignated target within 60 secs using a COTS eye tracker and at an average\ntime of 2 mins using the webcam based eye gaze tracker.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 09:23:20 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Sharma", "Vinay Krishna", ""], ["Murthy", "L. R. D.", ""], ["Saluja", "KamalPreet Singh", ""], ["Mollyn", "Vimal", ""], ["Sharma", "Gourav", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2005.12196", "submitter": "Lionel Robert", "authors": "Rasha Alahmad, Lionel Robert", "title": "Artificial Intelligence (AI) and IT identity: Antecedents Identifying\n  with AI Applications", "comments": "10 pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the age of Artificial Intelligence and automation, machines have taken\nover many key managerial tasks. Replacing managers with AI systems may have a\nnegative impact on workers outcomes. It is unclear if workers receive the same\nbenefits from their relationships with AI systems, raising the question: What\ndegree does the relationship between AI systems and workers impact worker\noutcomes? We draw on IT identity to understand the influence of identification\nwith AI systems on job performance. From this theoretical perspective, we\npropose a research model and conduct a survey of 97 MTurk workers to test the\nmodel. The findings reveal that work role identity and organizational identity\nare key determinants of identification with AI systems. Furthermore, the\nfindings show that identification with AI systems does increase job\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 10:59:43 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Alahmad", "Rasha", ""], ["Robert", "Lionel", ""]]}, {"id": "2005.12501", "submitter": "Benjamin Kane", "authors": "Benjamin Kane, Georgiy Platonov, and Lenhart K. Schubert", "title": "History-Aware Question Answering in a Blocks World Dialogue System", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is essential for dialogue-based spatial reasoning systems to maintain\nmemory of historical states of the world. In addition to conveying that the\ndialogue agent is mentally present and engaged with the task, referring to\nhistorical states may be crucial for enabling collaborative planning (e.g., for\nplanning to return to a previous state, or diagnosing a past misstep). In this\npaper, we approach the problem of spatial memory in a multi-modal spoken\ndialogue system capable of answering questions about interaction history in a\nphysical blocks world setting. This work builds upon a full spatial\nquestion-answering pipeline consisting of a vision system, speech input and\noutput mediated by an animated avatar, a dialogue system that robustly\ninterprets spatial queries, and a constraint solver that derives answers based\non 3-D spatial modelling. The contributions of this work include a symbolic\ndialogue context registering knowledge about discourse history and changes in\nthe world, as well as a natural language understanding module capable of\ninterpreting free-form historical questions and querying the dialogue context\nto form an answer.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 03:16:11 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Kane", "Benjamin", ""], ["Platonov", "Georgiy", ""], ["Schubert", "Lenhart K.", ""]]}, {"id": "2005.12644", "submitter": "Jason R.C. Nurse Dr", "authors": "Rahime Belen Saglam and Jason R. C. Nurse", "title": "Is your chatbot GDPR compliant? Open issues in agent design", "comments": null, "journal-ref": "CUI 2020: International Conference on Conversational User\n  Interfaces, July, 2020", "doi": "10.1145/3405755.3406131", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents open the world to new opportunities for human\ninteraction and ubiquitous engagement. As their conversational abilities and\nknowledge has improved, these agents have begun to have access to an increasing\nvariety of personally identifiable information and intimate details on their\nuser base. This access raises crucial questions in light of regulations as\nrobust as the General Data Protection Regulation (GDPR). This paper explores\nsome of these questions, with the aim of defining relevant open issues in\nconversational agent design. We hope that this work can provoke further\nresearch into building agents that are effective at user interaction, but also\nrespectful of regulations and user privacy.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 11:54:44 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Saglam", "Rahime Belen", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "2005.12668", "submitter": "Tom Hope", "authors": "Tom Hope, Jason Portenoy, Kishore Vasan, Jonathan Borchardt, Eric\n  Horvitz, Daniel S. Weld, Marti A. Hearst, Jevin West", "title": "SciSight: Combining faceted navigation and research group detection for\n  COVID-19 exploratory scientific search", "comments": "Accepted to EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has sparked unprecedented mobilization of scientists,\ngenerating a deluge of papers that makes it hard for researchers to keep track\nand explore new directions. Search engines are designed for targeted queries,\nnot for discovery of connections across a corpus. In this paper, we present\nSciSight, a system for exploratory search of COVID-19 research integrating two\nkey capabilities: first, exploring associations between biomedical facets\nautomatically extracted from papers (e.g., genes, drugs, diseases, patient\noutcomes); second, combining textual and network information to search and\nvisualize groups of researchers and their ties. SciSight has so far served over\n$15K$ users with over $42K$ page views and $13\\%$ returns.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 08:56:21 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 03:05:32 GMT"}, {"version": "v3", "created": "Sun, 20 Sep 2020 15:43:20 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Hope", "Tom", ""], ["Portenoy", "Jason", ""], ["Vasan", "Kishore", ""], ["Borchardt", "Jonathan", ""], ["Horvitz", "Eric", ""], ["Weld", "Daniel S.", ""], ["Hearst", "Marti A.", ""], ["West", "Jevin", ""]]}, {"id": "2005.12951", "submitter": "Karan Ahuja", "authors": "Karan Ahuja, Abhishek Bose, Mohit Jain, Kuntal Dey, Anil Joshi,\n  Krishnaveni Achary, Blessin Varkey, Chris Harrison and Mayank Goel", "title": "Gaze-based Autism Detection for Adolescents and Young Adults using\n  Prosaic Videos", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism often remains undiagnosed in adolescents and adults. Prior research\nhas indicated that an autistic individual often shows atypical fixation and\ngaze patterns. In this short paper, we demonstrate that by monitoring a user's\ngaze as they watch commonplace (i.e., not specialized, structured or coded)\nvideo, we can identify individuals with autism spectrum disorder. We recruited\n35 autistic and 25 non-autistic individuals, and captured their gaze using an\noff-the-shelf eye tracker connected to a laptop. Within 15 seconds, our\napproach was 92.5% accurate at identifying individuals with an autism\ndiagnosis. We envision such automatic detection being applied during e.g., the\nconsumption of web media, which could allow for passive screening and\nadaptation of user interfaces.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 18:14:31 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Ahuja", "Karan", ""], ["Bose", "Abhishek", ""], ["Jain", "Mohit", ""], ["Dey", "Kuntal", ""], ["Joshi", "Anil", ""], ["Achary", "Krishnaveni", ""], ["Varkey", "Blessin", ""], ["Harrison", "Chris", ""], ["Goel", "Mayank", ""]]}, {"id": "2005.13275", "submitter": "Irene Celino", "authors": "Irene Celino", "title": "Who is this Explanation for? Human Intelligence and Knowledge Graphs for\n  eXplainable AI", "comments": "10 pages, 1 figure, book chapter", "journal-ref": "Ilaria Tiddi, Freddy Lecue, Pascal Hitzler (eds.), Knowledge\n  Graphs for eXplainable AI - Foundations, Applications and Challenges. Studies\n  on the Semantic Web, Volume 47, IOS Press, Amsterdam, 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  eXplainable AI focuses on generating explanations for the output of an AI\nalgorithm to a user, usually a decision-maker. Such user needs to interpret the\nAI system in order to decide whether to trust the machine outcome. When\naddressing this challenge, therefore, proper attention should be given to\nproduce explanations that are interpretable by the target community of users.\nIn this chapter, we claim for the need to better investigate what constitutes a\nhuman explanation, i.e. a justification of the machine behaviour that is\ninterpretable and actionable by the human decision makers. In particular, we\nfocus on the contributions that Human Intelligence can bring to eXplainable AI,\nespecially in conjunction with the exploitation of Knowledge Graphs. Indeed, we\ncall for a better interplay between Knowledge Representation and Reasoning,\nSocial Sciences, Human Computation and Human-Machine Cooperation research -- as\nalready explored in other AI branches -- in order to support the goal of\neXplainable AI with the adoption of a Human-in-the-Loop approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 10:47:15 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Celino", "Irene", ""]]}, {"id": "2005.13477", "submitter": "Luis Leiva", "authors": "Luis A. Leiva and Radu-Daniel Vatavu and Daniel Mart\\'in-Albo and\n  R\\'ejean Plamondon", "title": "Omnis Pr{\\ae}dictio: Estimating the Full Spectrum of Human Performance\n  with Stroke Gestures", "comments": null, "journal-ref": "Int. J. Hum. Comput. Stud. 142 (2020)", "doi": "10.1016/j.ijhcs.2020.102466", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing effective, usable, and widely adoptable stroke gesture commands for\ngraphical user interfaces is a challenging task that traditionally involves\nmultiple iterative rounds of prototyping, implementation, and follow-up user\nstudies and controlled experiments for evaluation, verification, and\nvalidation. An alternative approach is to employ theoretical models of human\nperformance, which can deliver practitioners with insightful information right\nfrom the earliest stages of user interface design. However, very few aspects of\nthe large spectrum of human performance with stroke gesture input have been\ninvestigated and modeled so far, leaving researchers and practitioners of\ngesture-based user interface design with a very narrow range of predictable\nmeasures of human performance, mostly focused on estimating production time, of\nwhich extremely few cases delivered accompanying software tools to assist\nmodeling. We address this problem by introducing \"Omnis Praedictio\" (Omnis for\nshort), a generic technique and companion web tool that provides accurate\nuser-independent estimations of any numerical stroke gesture feature, including\ncustom features specified in code. Our experimental results on three public\ndatasets show that our model estimations correlate on average r > .9 with\ngroundtruth data. Omnis also enables researchers and practitioners to\nunderstand human performance with stroke gestures on many levels and,\nconsequently, raises the bar for human performance models and estimation\ntechniques for stroke gesture input.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 16:35:47 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Leiva", "Luis A.", ""], ["Vatavu", "Radu-Daniel", ""], ["Mart\u00edn-Albo", "Daniel", ""], ["Plamondon", "R\u00e9jean", ""]]}, {"id": "2005.13523", "submitter": "Abdul Moeed", "authors": "Abdul Moeed", "title": "Emotion-robust EEG Classification for Motor Imagery", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in Brain Computer Interfaces (BCIs) are empowering those with\nsevere physical afflictions through their use in assistive systems. Common\nmethods of achieving this is via Motor Imagery (MI), which maps brain signals\nto code for certain commands. Electroencephalogram (EEG) is preferred for\nrecording brain signal data on account of it being non-invasive. Despite their\npotential utility, MI-BCI systems are yet confined to research labs. A major\ncause for this is lack of robustness of such systems. As hypothesized by two\nteams during Cybathlon 2016, a particular source of the system's vulnerability\nis the sharp change in the subject's state of emotional arousal. This work aims\ntowards making MI-BCI systems resilient to such emotional perturbations. To do\nso, subjects are exposed to high and low arousal-inducing virtual reality (VR)\nenvironments before recording EEG data. The advent of COVID-19 compelled us to\nmodify our methodology. Instead of training machine learning algorithms to\nclassify emotional arousal, we opt for classifying subjects that serve as proxy\nfor each state. Additionally, MI models are trained for each subject instead of\neach arousal state. As training subjects to use MI-BCI can be an arduous and\ntime-consuming process, reducing this variability and increasing robustness can\nconsiderably accelerate the acceptance and adoption of assistive technologies\npowered by BCI.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 17:31:07 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Moeed", "Abdul", ""]]}, {"id": "2005.13600", "submitter": "Pradipta Biswas", "authors": "LRD Murthy, Abhishek Mukhopadhyay, Varshit Yellheti, Somnath Arjun,\n  Peter Thomas, M Dilli Babu, Kamal Preet Singh Saluja, JeevithaShree DV and\n  Pradipta Biswas", "title": "Eye Gaze Controlled Interfaces for Head Mounted and Multi-Functional\n  Displays in Military Aviation Environment", "comments": "Presented at IEEE Aerospace 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye gaze controlled interfaces allow us to directly manipulate a graphical\nuser interface just by looking at it. This technology has great potential in\nmilitary aviation, in particular, operating different displays in situations\nwhere pilots hands are occupied with flying the aircraft. This paper reports\nstudies on analyzing accuracy of eye gaze controlled interface inside aircraft\nundertaking representative flying missions. We reported that pilots can\nundertake representative pointing and selection tasks at less than 2 secs on\naverage. Further, we evaluated the accuracy of eye gaze tracking glass under\nvarious G-conditions and analyzed its failure modes. We observed that the\naccuracy of an eye tracker is less than 5 degree of visual angle up to +3G,\nalthough it is less accurate at minus 1G and plus 5G. We observed that eye\ntracker may fail to track under higher external illumination. We also infer\nthat an eye tracker to be used in military aviation need to have larger\nvertical field of view than the present available systems. We used this\nanalysis to develop eye gaze trackers for Multi-Functional displays and Head\nMounted Display System. We obtained significant reduction in pointing and\nselection times using our proposed HMDS system compared to traditional TDS.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 19:19:35 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Murthy", "LRD", ""], ["Mukhopadhyay", "Abhishek", ""], ["Yellheti", "Varshit", ""], ["Arjun", "Somnath", ""], ["Thomas", "Peter", ""], ["Babu", "M Dilli", ""], ["Saluja", "Kamal Preet Singh", ""], ["DV", "JeevithaShree", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2005.13655", "submitter": "Aythami Morales", "authors": "Alejandro Acien and Aythami Morales and Julian Fierrez and Ruben\n  Vera-Rodriguez and Oscar Delgado-Mohatar", "title": "BeCAPTCHA: Behavioral Bot Detection using Touchscreen and Mobile Sensors\n  benchmarked on HuMIdb", "comments": "arXiv admin note: substantial text overlap with arXiv:2002.00918", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the suitability of a new generation of CAPTCHA methods\nbased on smartphone interactions. The heterogeneous flow of data generated\nduring the interaction with the smartphones can be used to model human behavior\nwhen interacting with the technology and improve bot detection algorithms. For\nthis, we propose BeCAPTCHA, a CAPTCHA method based on the analysis of the\ntouchscreen information obtained during a single drag and drop task in\ncombination with the accelerometer data. The goal of BeCAPTCHA is to determine\nwhether the drag and drop task was realized by a human or a bot. We evaluate\nthe method by generating fake samples synthesized with Generative Adversarial\nNeural Networks and handcrafted methods. Our results suggest the potential of\nmobile sensors to characterize the human behavior and develop a new generation\nof CAPTCHAs. The experiments are evaluated with HuMIdb (Human Mobile\nInteraction database), a novel multimodal mobile database that comprises 14\nmobile sensors acquired from 600 users. HuMIdb is freely available to the\nresearch community.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 21:09:08 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 11:11:14 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Acien", "Alejandro", ""], ["Morales", "Aythami", ""], ["Fierrez", "Julian", ""], ["Vera-Rodriguez", "Ruben", ""], ["Delgado-Mohatar", "Oscar", ""]]}, {"id": "2005.13709", "submitter": "Andreas Koenzen", "authors": "Andreas Koenzen, Neil Ernst, Margaret-Anne Storey", "title": "Code Duplication and Reuse in Jupyter Notebooks", "comments": "Accepted as a full paper at the IEEE Symposium on Visual Languages\n  and Human-Centric Computing (VL/HCC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Duplicating one's own code makes it faster to write software. This expediency\nis particularly valuable for users of computational notebooks. Duplication\nallows notebook users to quickly test hypotheses and iterate over data. In this\npaper, we explore how much, how and from where code duplication occurs in\ncomputational notebooks, and identify potential barriers to code reuse.\nPrevious work in the area of computational notebooks describes developers'\nmotivations for reuse and duplication but does not show how much reuse occurs\nor which barriers they face when reusing code. To address this gap, we first\nanalyzed GitHub repositories for code duplicates contained in a repository's\nJupyter notebooks, and then conducted an observational user study of code\nreuse, where participants solved specific tasks using notebooks. Our findings\nreveal that repositories in our sample have a mean self-duplication rate of\n7.6%. However, in our user study, few participants duplicated their own code,\npreferring to reuse code from online sources.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 23:37:57 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Koenzen", "Andreas", ""], ["Ernst", "Neil", ""], ["Storey", "Margaret-Anne", ""]]}, {"id": "2005.13714", "submitter": "Jingwen Qian", "authors": "Yiwei Chen, Jingwen Qian, Junming Wang, Lirong Xia and Gavriel Zahavi", "title": "OPRA: An Open-Source Online Preference Reporting and Aggregation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Online Preference Reporting and Aggregation (OPRA) system,\nan open-source online system that aims at providing support for group\ndecision-making. We illustrate OPRA's distinctive features: UI for reporting\nrankings with ties, comprehensive analytics of preferences, and group\ndecision-making in combinatorial domains. We also discuss our work in an\nautomatic mentor matching system. We hope that the open-source nature of OPRA\nwill foster the development of computerized group decision support systems.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 00:16:54 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Chen", "Yiwei", ""], ["Qian", "Jingwen", ""], ["Wang", "Junming", ""], ["Xia", "Lirong", ""], ["Zahavi", "Gavriel", ""]]}, {"id": "2005.13745", "submitter": "Hassan Habibi Gharakheili", "authors": "Ke Hu and Ashfaqur Rahman and Hassan Habibi Gharakheili and Vijay\n  Sivaraman", "title": "HazeDose: Design and Analysis of a Personal Air Pollution Inhaled Dose\n  Estimation System using Wearable Sensors", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays air pollution becomes one of the biggest world issues in both\ndeveloping and developed countries. Helping individuals understand their air\npollution exposure and health risks, the traditional way is to utilize data\nfrom static monitoring stations and estimate air pollution qualities in a large\narea by government agencies. Data from such sensing system is very sparse and\ncannot reflect real personal exposure. In recent years, several research groups\nhave developed participatory air pollution sensing systems which use wearable\nor portable units coupled with smartphones to crowd-source urban air pollution\ndata. These systems have shown remarkable improvement in spatial granularity\nover government-operated fixed monitoring systems. In this paper, we extend the\nparadigm to HazeDose system, which can personalize the individuals' air\npollution exposure. Specifically, we combine the pollution concentrations\nobtained from an air pollution estimation system with the activity data from\nthe individual's on-body activity monitors to estimate the personal inhalation\ndosage of air pollution. Users can visualize their personalized air pollution\nexposure information via a mobile application. We show that different\nactivities, such as walking, cycling, or driving, impact their dosage, and\ncommuting patterns contribute to a significant proportion of an individual's\ndaily air pollution dosage. Moreover, we propose a dosage minimization\nalgorithm, with the trial results showing that up to 14.1% of a biker's daily\nexposure can be reduced while using alternative routes the driver can inhale\n25.9% less than usual. One heuristic algorithm is also introduced to balance\nthe execution time and dosage reduction for alternative routes scenarios. The\nresults show that up to 20.3% dosage reduction can be achieved when the\nexecution time is almost one seventieth of the original one.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 02:35:13 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Hu", "Ke", ""], ["Rahman", "Ashfaqur", ""], ["Gharakheili", "Hassan Habibi", ""], ["Sivaraman", "Vijay", ""]]}, {"id": "2005.13754", "submitter": "Petros Spachos", "authors": "Pai Chet Ng, Petros Spachos, Konstantinos Plataniotis", "title": "COVID-19 and Your Smartphone: BLE-based Smart Contact Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contact tracing is of paramount importance when it comes to preventing the\nspreading of infectious diseases. Contact tracing is usually performed manually\nby authorized personnel. Manual contact tracing is an inefficient, error-prone,\ntime-consuming process of limited utility to the population at large as those\nin close contact with infected individuals are informed hours, if not days,\nlater. This paper introduces an alternative way to manual contact tracing. The\nproposed Smart Contact Tracing (SCT) system utilizes the smartphone's Bluetooth\nLow Energy (BLE) signals and machine learning classifier to accurately and\nquickly determined the contact profile. SCT's contribution is two-fold: a)\nclassification of the user's contact as high/low-risk using precise proximity\nsensing, and b) user anonymity using a privacy-preserving communications\nprotocol. SCT leverages BLE's non-connectable advertising feature to broadcast\na signature packet when the user is in the public space. Both broadcasted and\nobserved signatures are stored in the user's smartphone and they are only\nuploaded to a secure signature database when a user is confirmed by public\nhealth authorities to be infected. Using received signal strength (RSS) each\nsmartphone estimates its distance from other user's phones and issues real-time\nalerts when social distancing rules are violated. The paper includes extensive\nexperimentation utilizing real-life smartphone positions and a comparative\nevaluation of five machine learning classifiers. Reported results indicate that\na decision tree classifier outperforms other states of the art classification\nmethods in terms of accuracy. Lastly, to facilitate research in this area, and\nto contribute to the timely development of advanced solutions the entire data\nset of six experiments with about 123,000 data points is made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 02:56:17 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Ng", "Pai Chet", ""], ["Spachos", "Petros", ""], ["Plataniotis", "Konstantinos", ""]]}, {"id": "2005.13950", "submitter": "Simon Perrault", "authors": "Katherine Fennedy, Sylvain Malacria, Hyowon Lee, Simon Perrault", "title": "Investigating Performance and Usage of Input Methods for Soft Keyboard\n  Hotkeys", "comments": "17+2 pages, published at Mobile HCI 2020", "journal-ref": null, "doi": "10.1145/3379503.3403552", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Touch-based devices, despite their mainstream availability, do not support a\nunified and efficient command selection mechanism, available on every platform\nand application. We advocate that hotkeys, conventionally used as a shortcut\nmechanism on desktop computers, could be generalized as a command selection\nmechanism for touch-based devices, even for keyboard-less applications. In this\npaper, we investigate the performance and usage of soft keyboard shortcuts or\nhotkeys (abbreviated SoftCuts) through two studies comparing different input\nmethods across sitting, standing and walking conditions. Our results suggest\nthat SoftCuts not only are appreciated by participants but also support rapid\ncommand selection with different devices and hand configurations. We also did\nnot find evidence that walking deters their performance when using the Once\ninput method.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 12:36:55 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Fennedy", "Katherine", ""], ["Malacria", "Sylvain", ""], ["Lee", "Hyowon", ""], ["Perrault", "Simon", ""]]}, {"id": "2005.13982", "submitter": "AKMMahbubur Rahman", "authors": "AKMMahbubur Rahman, ASM Iftekhar Anam, and Mohammed Yeasin", "title": "Robust Modeling of Epistemic Mental States", "comments": "Accepted for Publication in Multimedia Tools and Application, Special\n  Issue: Socio-Affective Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work identifies and advances some research challenges in the analysis of\nfacial features and their temporal dynamics with epistemic mental states in\ndyadic conversations. Epistemic states are: Agreement, Concentration,\nThoughtful, Certain, and Interest. In this paper, we perform a number of\nstatistical analyses and simulations to identify the relationship between\nfacial features and epistemic states. Non-linear relations are found to be more\nprevalent, while temporal features derived from original facial features have\ndemonstrated a strong correlation with intensity changes. Then, we propose a\nnovel prediction framework that takes facial features and their nonlinear\nrelation scores as input and predict different epistemic states in videos. The\nprediction of epistemic states is boosted when the classification of emotion\nchanging regions such as rising, falling, or steady-state are incorporated with\nthe temporal features. The proposed predictive models can predict the epistemic\nstates with significantly improved accuracy: correlation coefficient (CoERR)\nfor Agreement is 0.827, for Concentration 0.901, for Thoughtful 0.794, for\nCertain 0.854, and for Interest 0.913.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 13:34:45 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Rahman", "AKMMahbubur", ""], ["Anam", "ASM Iftekhar", ""], ["Yeasin", "Mohammed", ""]]}, {"id": "2005.14136", "submitter": "Antonyo Musabini", "authors": "Antonyo Musabini, Mounsif Chetitah", "title": "Heatmap-Based Method for Estimating Drivers' Cognitive Distraction", "comments": "Accepted at IEEE ICCI*CC 2020 (matching camera-ready version)", "journal-ref": null, "doi": "10.1109/ICCICC50026.2020.9450216", "report-no": null, "categories": "cs.HC cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to increase road safety, among the visual and manual distractions,\nmodern intelligent vehicles need also to detect cognitive distracted driving\n(i.e., the drivers mind wandering). In this study, the influence of cognitive\nprocesses on the drivers gaze behavior is explored. A novel image-based\nrepresentation of the driver's eye-gaze dispersion is proposed to estimate\ncognitive distraction. Data are collected on open highway roads, with a\ntailored protocol to create cognitive distraction. The visual difference of\ncreated shapes shows that a driver explores a wider area in neutral driving\ncompared to distracted driving. Thus, support vector machine (SVM)-based\nclassifiers are trained, and 85.2% of accuracy is achieved for a two-class\nproblem, even with a small dataset. Thus, the proposed method has the\ndiscriminative power to recognize cognitive distraction using gaze information.\nFinally, this work details how this image-based representation could be useful\nfor other cases of distracted driving detection.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 16:37:30 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 16:47:18 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Musabini", "Antonyo", ""], ["Chetitah", "Mounsif", ""]]}, {"id": "2005.14223", "submitter": "\\\"Ozge Yal\\c{c}{\\i}n Nilay", "authors": "\u007fOzge Nilay Yalcin, Nouf Abukhodair and Steve DiPaola", "title": "Empathic AI Painter: A Computational Creativity System with Embodied\n  Conversational Interaction", "comments": "In press. NeurIPS 2019 Competition and Demonstration Track,\n  Proceedings of Machine Learning Research Vol. 123, 2020", "journal-ref": "Proceedings of the NeurIPS 2019 Competition and Demonstration\n  Track, PMLR 123:131-141, 2020", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing recognition that artists use valuable ways to understand\nand work with cognitive and perceptual mechanisms to convey desired experiences\nand narrative in their created artworks (DiPaola et al., 2010; Zeki, 2001).\nThis paper documents our attempt to computationally model the creative process\nof a portrait painter, who relies on understanding human traits (i.e.,\npersonality and emotions) to inform their art. Our system includes an empathic\nconversational interaction component to capture the dominant personality\ncategory of the user and a generative AI Portraiture system that uses this\ncategorization to create a personalized stylization of the user's portrait.\nThis paper includes the description of our systems and the real-time\ninteraction results obtained during the demonstration session of the NeurIPS\n2019 Conference.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 18:35:42 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Yalcin", "\u007fOzge Nilay", ""], ["Abukhodair", "Nouf", ""], ["DiPaola", "Steve", ""]]}, {"id": "2005.14517", "submitter": "Zahid Iqbal", "authors": "Affan Idrees, Zahid Iqbal, Maria Ishfaq", "title": "An Efficient Indoor Navigation Technique To Find Optimal Route For\n  Blinds Using QR Codes", "comments": "IEEE 10th Conference on Industrial Electronics and Applications\n  (ICIEA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind navigation is an accessibility application that enables blind to use an\nandroid Smartphone in an easy way for indoor navigation with instructions in\naudio form. We have proposed a prototype which is an indoor navigation\napplication for blinds that uses QR codes. It is developed for android Smart\nphones and does not require any additional hardware for navigation. It provides\nautomatic navigational assistance on pre-defined paths for blind. QR codes are\nplaced on the floor sections after specific distance that acts as an input for\ncurrent location detection and navigation. Whenever a QR code is scanned it\nprovides the user with the information of the current location and asks the\nuser to select the destination and then offers optimal and shortest path using\npath finding algorithms. During navigation whenever the deviation from the\nproposed path is detected it prompts the user and guides back to the right path\nby comparing the current path with the generated path. All of the instructions\nthroughout the application are provided in audio form to the user. The\ninterface of the application is well built for blinds which makes the smart\nphones user-friendly and useable for blind people. The user interacts with the\napplication through a specific set of user-friendly gestures for specific\ninputs and operations. At the end, we have performed comparison between\ndifferent state of art approaches and concluded that our approach is more user\nfriendly, cost effective and produced more accurate results.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 12:19:25 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Idrees", "Affan", ""], ["Iqbal", "Zahid", ""], ["Ishfaq", "Maria", ""]]}]