[{"id": "1707.00018", "submitter": "Duin Back", "authors": "Duin Back, Bong Jun Choi, Jing Chen", "title": "Small Profits and Quick Returns: A Practical SocialWelfare Maximizing\n  Incentive Mechanism for Deadline-Sensitive Tasks in Crowdsourcing", "comments": "2 pages, HCOMP 2017 Work-in-Progress paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the driving force of crowdsourcing is the interaction among participants,\nvarious incentive mechanisms have been proposed to attract sufficient\nparticipants. However, the existing works assume that all the providers always\nmeet the deadline and the task value accordingly remains constant. To bridge\nthe gap of such impractical assumption, we model the heterogeneous punctuality\nbehavior of providers and the task value depreciation of requesters. Based on\nthose models, we propose an Expected Social Welfare Maximizing (ESWM) mechanism\nthat aims to maximize the expected social welfare in polynomial time.\nSimulation results show that our heuristic-based mechanism achieves higher\nexpected social welfare and platform utility via attracting more participants.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 18:30:38 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Back", "Duin", ""], ["Choi", "Bong Jun", ""], ["Chen", "Jing", ""]]}, {"id": "1707.00086", "submitter": "Emilio Ferrara", "authors": "Emilio Ferrara", "title": "Disinformation and Social Bot Operations in the Run Up to the 2017\n  French Presidential Election", "comments": "33 pages, 6 figures, 9 tables; submitted to First Monday", "journal-ref": "First Monday, 22(8), 2017", "doi": "10.5210/fm.v22i8.8005", "report-no": null, "categories": "cs.SI cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent accounts from researchers, journalists, as well as federal\ninvestigators, reached a unanimous conclusion: social media are systematically\nexploited to manipulate and alter public opinion. Some disinformation campaigns\nhave been coordinated by means of bots, social media accounts controlled by\ncomputer scripts that try to disguise themselves as legitimate human users. In\nthis study, we describe one such operation occurred in the run up to the 2017\nFrench presidential election. We collected a massive Twitter dataset of nearly\n17 million posts occurred between April 27 and May 7, 2017 (Election Day). We\nthen set to study the MacronLeaks disinformation campaign: By leveraging a mix\nof machine learning and cognitive behavioral modeling techniques, we separated\nhumans from bots, and then studied the activities of the two groups taken\nindependently, as well as their interplay. We provide a characterization of\nboth the bots and the users who engaged with them and oppose it to those users\nwho didn't. Prior interests of disinformation adopters pinpoint to the reasons\nof the scarce success of this campaign: the users who engaged with MacronLeaks\nare mostly foreigners with a preexisting interest in alt-right topics and\nalternative news media, rather than French users with diverse political views.\nConcluding, anomalous account usage patterns suggest the possible existence of\na black-market for reusable political disinformation bots.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 02:37:13 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Ferrara", "Emilio", ""]]}, {"id": "1707.00195", "submitter": "Tim Weninger PhD", "authors": "Maria Glenski, Tim Weninger", "title": "Predicting User-Interactions on Reddit", "comments": "Presented at ASONAM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to keep up with the demand of curating the deluge of crowd-sourced\ncontent, social media platforms leverage user interaction feedback to make\ndecisions about which content to display, highlight, and hide. User\ninteractions such as likes, votes, clicks, and views are assumed to be a proxy\nof a content's quality, popularity, or news-worthiness. In this paper we ask:\nhow predictable are the interactions of a user on social media? To answer this\nquestion we recorded the clicking, browsing, and voting behavior of 186 Reddit\nusers over a year. We present interesting descriptive statistics about their\ncombined 339,270 interactions, and we find that relatively simple models are\nable to predict users' individual browse- or vote-interactions with reasonable\naccuracy.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 19:25:58 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Glenski", "Maria", ""], ["Weninger", "Tim", ""]]}, {"id": "1707.00375", "submitter": "Dmitry Kalika", "authors": "Dmitry Kalika, Leslie M. Collins, Chandra S. Throckmorton, Boyla O.\n  Mainsah", "title": "Adaptive Stimulus Selection in ERP-Based Brain-Computer Interfaces by\n  Maximizing Expected Discrimination Gain", "comments": "This paper has been accepted for the 2017 IEEE International\n  Conference on Systems, Man and Cybernetics (SMC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfaces (BCIs) can provide an alternative means of\ncommunication for individuals with severe neuromuscular limitations. The\nP300-based BCI speller relies on eliciting and detecting transient\nevent-related potentials (ERPs) in electroencephalography (EEG) data, in\nresponse to a user attending to rarely occurring target stimuli amongst a\nseries of non-target stimuli. However, in most P300 speller implementations,\nthe stimuli to be presented are randomly selected from a limited set of options\nand stimulus selection and presentation are not optimized based on previous\nuser data. In this work, we propose a data-driven method for stimulus selection\nbased on the expected discrimination gain metric. The data-driven approach\nselects stimuli based on previously observed stimulus responses, with the aim\nof choosing a set of stimuli that will provide the most information about the\nuser's intended target character. Our approach incorporates knowledge of\nphysiological and system constraints imposed due to real-time BCI\nimplementation. Simulations were performed to compare our stimulus selection\napproach to the row-column paradigm, the conventional stimulus selection method\nfor P300 spellers. Results from the simulations demonstrated that our adaptive\nstimulus selection approach has the potential to significantly improve\nperformance from the conventional method: up to 34% improvement in accuracy and\n43% reduction in the mean number of stimulus presentations required to spell a\ncharacter in a 72-character grid. In addition, our greedy approach to stimulus\nselection provides the flexibility to accommodate design constraints.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 01:08:08 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Kalika", "Dmitry", ""], ["Collins", "Leslie M.", ""], ["Throckmorton", "Chandra S.", ""], ["Mainsah", "Boyla O.", ""]]}, {"id": "1707.01031", "submitter": "Mohammad S. Jalali", "authors": "M. S. Jalali", "title": "Decision-Making and Biases in Cybersecurity Capability Development:\n  Evidence from a Simulation Game Experiment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC math.DS stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a simulation game to study the effectiveness of decision-makers\nin overcoming two complexities in building cybersecurity capabilities:\npotential delays in capability development; and uncertainties in predicting\ncyber incidents. Analyzing 1,479 simulation runs, we compared the performances\nof a group of experienced professionals with those of an inexperienced control\ngroup. Experienced subjects did not understand the mechanisms of delays any\nbetter than inexperienced subjects; however, experienced subjects were better\nable to learn the need for proactive decision-making through an iterative\nprocess. Both groups exhibited similar errors when dealing with the uncertainty\nof cyber incidents. Our findings highlight the importance of training for\ndecision-makers with a focus on systems thinking skills, and lay the groundwork\nfor future research on uncovering mental biases about the complexities of\ncybersecurity.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 15:17:38 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 19:23:07 GMT"}, {"version": "v3", "created": "Mon, 8 Jan 2018 22:14:17 GMT"}, {"version": "v4", "created": "Tue, 3 Jul 2018 00:06:29 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Jalali", "M. S.", ""]]}, {"id": "1707.01152", "submitter": "Jonathan Kelly", "authors": "Brandon Wagstaff, Valentin Peretroukhin, and Jonathan Kelly", "title": "Improving Foot-Mounted Inertial Navigation Through Real-Time Motion\n  Classification", "comments": "In Proceedings of the International Conference on Indoor Positioning\n  and Indoor Navigation (IPIN'17), Sapporo, Japan, Sep. 18-21, 2017", "journal-ref": null, "doi": "10.1109/IPIN.2017.8115947", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method to improve the accuracy of a foot-mounted,\nzero-velocity-aided inertial navigation system (INS) by varying estimator\nparameters based on a real-time classification of motion type. We train a\nsupport vector machine (SVM) classifier using inertial data recorded by a\nsingle foot-mounted sensor to differentiate between six motion types (walking,\njogging, running, sprinting, crouch-walking, and ladder-climbing) and report\nmean test classification accuracy of over 90% on a dataset with five different\nsubjects. From these motion types, we select two of the most common (walking\nand running), and describe a method to compute optimal zero-velocity detection\nparameters tailored to both a specific user and motion type by maximizing the\ndetector F-score. By combining the motion classifier with a set of optimal\ndetection parameters, we show how we can reduce INS position error during mixed\nwalking and running motion. We evaluate our adaptive system on a total of 5.9\nkm of indoor pedestrian navigation performed by five different subjects moving\nalong a 130 m path with surveyed ground truth markers.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 20:56:01 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 14:39:22 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 20:26:05 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Wagstaff", "Brandon", ""], ["Peretroukhin", "Valentin", ""], ["Kelly", "Jonathan", ""]]}, {"id": "1707.01204", "submitter": "Santosh Vempala", "authors": "Manuel Blum and Santosh Vempala", "title": "The Complexity of Human Computation: A Concrete Model with an\n  Application to Passwords", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What can humans compute in their heads? We are thinking of a variety of\nCrypto Protocols, games like Sudoku, Crossword Puzzles, Speed Chess, and so on.\nThe intent of this paper is to apply the ideas and methods of theoretical\ncomputer science to better understand what humans can compute in their heads.\nFor example, can a person compute a function in their head so that an\neavesdropper with a powerful computer --- who sees the responses to random\ninput --- still cannot infer responses to new inputs? To address such\nquestions, we propose a rigorous model of human computation and associated\nmeasures of complexity. We apply the model and measures first and foremost to\nthe problem of (1) humanly computable password generation, and then consider\nrelated problems of (2) humanly computable \"one-way functions\" and (3) humanly\ncomputable \"pseudorandom generators\".\n  The theory of Human Computability developed here plays by different rules\nthan standard computability, and this takes some getting used to. For reasons\nto be made clear, the polynomial versus exponential time divide of modern\ncomputability theory is irrelevant to human computation. In human\ncomputability, the step-counts for both humans and computers must be more\nconcrete. Specifically, we restrict the adversary to at most 10^24 (Avogadro\nnumber of) steps. An alternate view of this work is that it deals with the\nanalysis of algorithms and counting steps for the case that inputs are small as\nopposed to the usual case of inputs large-in-the-limit.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 03:25:52 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Blum", "Manuel", ""], ["Vempala", "Santosh", ""]]}, {"id": "1707.01375", "submitter": "Luo-Luo Jiang", "authors": "Jia Quan Shen, Luo-Luo Jiang", "title": "Loss impresses human beings more than gain in the decision-making game", "comments": "11pages, 5 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What happen in the brain when human beings play games with computers? Here a\nsimple zero-sum game was conducted to investigate how people make decision via\ntheir brain even they know that their opponent is a computer. There are two\nchoices (a low or high number) for people and also two strategies for the\ncomputer (red color or green color). When the number selected by the human\nsubject meet the red color, the person loses the score which is equal to the\nnumber. On the contrary, the person gains the number of score if the computer\nchooses a green color for the number selected by the human being. Both the\nhuman subject and the computer give their choice at the same time, and subjects\nhave been told that the computer make its decision randomly on the red color or\ngreen color. During the experiments, the signal of electroencephalograph (EEG)\nobtained from brain of subjects was recorded. From the analysis of EEG, we find\nthat people mind the loss more than the gain, and the phenomenon becoming\nobvious when the gap between loss and gain grows. In addition, the signal of\nEEG is clearly distinguishable before making different decisions. It is\nobserved that significant negative waves in the entire brain region when the\nparticipant has a greater expectation for the outcome, and these negative waves\nare mainly concentrated in the forebrain region in the brain of human beings.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:57:52 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Shen", "Jia Quan", ""], ["Jiang", "Luo-Luo", ""]]}, {"id": "1707.01389", "submitter": "Ladislav Peska", "authors": "Ladislav Peska and Hana Trojanova", "title": "Towards Recommender Systems for Police Photo Lineup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photo lineups play a significant role in the eyewitness identification\nprocess. This method is used to provide evidence in the prosecution and\nsubsequent conviction of suspects. Unfortunately, there are many cases where\nlineups have led to the conviction of an innocent suspect. One of the key\nfactors affecting the incorrect identification of a suspect is the lack of\nlineup fairness, i.e. that the suspect differs significantly from all other\ncandidates. Although the process of assembling fair lineup is both highly\nimportant and time-consuming, only a handful of tools are available to simplify\nthe task. In this paper, we describe our work towards using recommender systems\nfor the photo lineup assembling task. We propose and evaluate two complementary\nmethods for item-based recommendation: one based on the visual descriptors of\nthe deep neural network, the other based on the content-based attributes of\npersons. The initial evaluation made by forensic technicians shows that\nalthough results favored visual descriptors over attribute-based similarity,\nboth approaches are functional and highly diverse in terms of recommended\nobjects. Thus, future work should involve incorporating both approaches in a\nsingle prediction method, preference learning based on the feedback from\nforensic technicians and recommendation of assembled lineups instead of single\ncandidates.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 13:38:07 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Peska", "Ladislav", ""], ["Trojanova", "Hana", ""]]}, {"id": "1707.01627", "submitter": "Dawei Chen", "authors": "Dawei Chen, Dongwoo Kim, Lexing Xie, Minjeong Shin, Aditya Krishna\n  Menon, Cheng Soon Ong, Iman Avazpour, John Grundy", "title": "PathRec: Visual Analysis of Travel Route Recommendations", "comments": "3 pages with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive visualisation tool for recommending travel\ntrajectories. This system is based on new machine learning formulations and\nalgorithms for the sequence recommendation problem. The system starts from a\nmap-based overview, taking an interactive query as starting point. It then\nbreaks down contributions from different geographical and user behavior\nfeatures, and those from individual points-of-interest versus pairs of\nconsecutive points on a route. The system also supports detailed quantitative\ninterrogation by comparing a large number of features for multiple points.\nEffective trajectory visualisations can potentially benefit a large cohort of\nonline map users and assist their decision-making. More broadly, the design of\nthis system can inform visualisations of other structured prediction tasks,\nsuch as for sequences or trees.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 03:58:15 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 01:07:02 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Chen", "Dawei", ""], ["Kim", "Dongwoo", ""], ["Xie", "Lexing", ""], ["Shin", "Minjeong", ""], ["Menon", "Aditya Krishna", ""], ["Ong", "Cheng Soon", ""], ["Avazpour", "Iman", ""], ["Grundy", "John", ""]]}, {"id": "1707.01848", "submitter": "Wieslaw Kopec", "authors": "Wies{\\l}aw Kope\\'c, Katarzyna Abramczuk, Bart{\\l}omiej Balcerzak,\n  Marta Ju\\'zwin Katarzyna Gniadzik, Grzegorz Kowalik, Rados{\\l}aw Nielek", "title": "A Location-Based Game for Two Generations: Teaching Mobile Technology to\n  the Elderly with the Support of Young Volunteers", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/978-3-319-49655-9_12", "journal-ref": null, "doi": "10.1007/978-3-319-49655-9_12", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a cooperative location-based game for the elderly with\nthe use of tablets equipped with mobile application. The game was designed to\ntackle at once several crucial topics related to the issue of aging, namely the\nsocial inclusion, education in the field of modern technology, motivation for\nlearning as well as physical activity. Mixed-aged teams consisting of two\nplayers: a junior and a senior took part in the game. The preliminary results\nsuggest that the game can successfully address a number of issues including\nimproving the elderly technical skills, increasing the elderly physical\nactivity as well as positive intergenerational interaction. The paper describes\nthe game setup in details and presents some initial data gathered during the\ngameplay.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 16:07:00 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Kope\u0107", "Wies\u0142aw", ""], ["Abramczuk", "Katarzyna", ""], ["Balcerzak", "Bart\u0142omiej", ""], ["Gniadzik", "Marta Ju\u017awin Katarzyna", ""], ["Kowalik", "Grzegorz", ""], ["Nielek", "Rados\u0142aw", ""]]}, {"id": "1707.01865", "submitter": "Claudia Schulz", "authors": "Elias Marcopoulos, Christian Reotutar and Yuanlin Zhang", "title": "An Online Development Environment for Answer Set Programming", "comments": "Proceedings of the 2nd International Workshop on User-Oriented Logic\n  Paradigms(IULP 2017), Editors: Claudia Schulz and Stefan Ellmauthaler", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in logic programming (e.g., the development of the Answer Set\nProgramming paradigm) has made it possible to teach it to general undergraduate\nand even high school students. Given the limited exposure of these students to\ncomputer science, the complexity of downloading, installing and using tools for\nwriting logic programs could be a major barrier for logic programming to reach\na much wider audience. We developed an online answer set programming\nenvironment with a self contained file system and a simple interface, allowing\nusers to write logic programs and perform several tasks over the programs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 10:01:24 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Marcopoulos", "Elias", ""], ["Reotutar", "Christian", ""], ["Zhang", "Yuanlin", ""]]}, {"id": "1707.01886", "submitter": "Raiyan Abdul Baten", "authors": "Rasoul Shafipour, Raiyan Abdul Baten, Md Kamrul Hasan, Gourab Ghoshal,\n  Gonzalo Mateos, and Mohammed Ehsan Hoque", "title": "Buildup of Speaking Skills in an Online Learning Community: A\n  Network-Analytic Exploration", "comments": null, "journal-ref": "Palgrave Communications, vol. 4, June 2018", "doi": "10.1057/s41599-018-0116-6", "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we explore peer-interaction effects in online networks on\nspeaking skill development. In particular, we present an evidence for gradual\nbuildup of skills in a small-group setting that has not been reported in the\nliterature. We introduce a novel dataset of six online communities consisting\nof 158 participants focusing on improving their speaking skills. They\nvideo-record speeches for 5 prompts in 10 days and exchange comments and\nperformance-ratings with their peers. We ask (i) whether the participants'\nratings are affected by their interaction patterns with peers, and (ii) whether\nthere is any gradual buildup of speaking skills in the communities towards\nhomogeneity. To analyze the data, we employ tools from the emerging field of\nGraph Signal Processing (GSP). GSP enjoys a distinction from Social Network\nAnalysis in that the latter is concerned primarily with the connection\nstructures of graphs, while the former studies signals on top of graphs. We\nstudy the performance ratings of the participants as graph signals atop\nunderlying interaction topologies. Total variation analysis of the graph\nsignals show that the participants' rating differences decrease with time\n(slope=-0.04, p<0.01), while average ratings increase (slope=0.07,\np<0.05)--thereby gradually building up the ratings towards community-wide\nhomogeneity. We provide evidence for peer-influence through a prediction\nformulation. Our consensus-based prediction model outperforms baseline\nnetwork-agnostic regression models by about 23% in predicting performance\nratings. This, in turn, shows that participants' ratings are affected by their\npeers' ratings and the associated interaction patterns, corroborating previous\nfindings. Then, we formulate a consensus-based diffusion model that captures\nthese observations of peer-influence from our analyses.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 17:41:21 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 22:52:56 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 16:49:11 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Shafipour", "Rasoul", ""], ["Baten", "Raiyan Abdul", ""], ["Hasan", "Md Kamrul", ""], ["Ghoshal", "Gourab", ""], ["Mateos", "Gonzalo", ""], ["Hoque", "Mohammed Ehsan", ""]]}, {"id": "1707.01890", "submitter": "Gaurav Trivedi", "authors": "Gaurav Trivedi, Phuong Pham, Wendy Chapman, Rebecca Hwa, Janyce Wiebe,\n  Harry Hochheiser", "title": "An Interactive Tool for Natural Language Processing on Clinical Text", "comments": "8 pages, 2 figures, 2 tables, Presented at IUI TextVis 2015 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Processing (NLP) systems often make use of machine learning\ntechniques that are unfamiliar to end-users who are interested in analyzing\nclinical records. Although NLP has been widely used in extracting information\nfrom clinical text, current systems generally do not support model revision\nbased on feedback from domain experts.\n  We present a prototype tool that allows end users to visualize and review the\noutputs of an NLP system that extracts binary variables from clinical text. Our\ntool combines multiple visualizations to help the users understand these\nresults and make any necessary corrections, thus forming a feedback loop and\nhelping improve the accuracy of the NLP models. We have tested our prototype in\na formative think-aloud user study with clinicians and researchers involved in\ncolonoscopy research. Results from semi-structured interviews and a System\nUsability Scale (SUS) analysis show that the users are able to quickly start\nrefining NLP models, despite having very little or no experience with machine\nlearning. Observations from these sessions suggest revisions to the interface\nto better support review workflow and interpretation of results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 17:44:15 GMT"}, {"version": "v2", "created": "Fri, 7 Jul 2017 14:04:13 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Trivedi", "Gaurav", ""], ["Pham", "Phuong", ""], ["Chapman", "Wendy", ""], ["Hwa", "Rebecca", ""], ["Wiebe", "Janyce", ""], ["Hochheiser", "Harry", ""]]}, {"id": "1707.01895", "submitter": "Nikolaos K Tselios", "authors": "Adrian Stoica, Nikolaos Tselios, Christos Fidas", "title": "Adaptive user support in educational environments: A Bayesian Network\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the design and implementation of an innovative\nuser support system in the frame of an open educational environment. The\nenvironment adapted is ModelsCreator (MC), an educational system supporting\nlearning through modelling activities. The pupils typical interaction with the\nsystem was modelled us-ing Bayesian Belief Networks (BBN). This model has been\nused in ModelsCreator to build an adaptive help system providing the most\nuseful guidelines according to the current state of interaction. A brief\ndescription of the system and an overview of application of Bayesian techniques\nto educational systems is presented together with discussion about the process\nof building of the Bayesian Network derived from actual student interaction\ndata. A preliminary evaluation of the developed prototype indicates that the\nproposed approach produces systems with promising performance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 17:58:11 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Stoica", "Adrian", ""], ["Tselios", "Nikolaos", ""], ["Fidas", "Christos", ""]]}, {"id": "1707.02654", "submitter": "Yuan Yao", "authors": "Yuan Yao, Svetlana Yarosh", "title": "Vision-Based Classification of Social Gestures in Videochat Sessions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the design and evaluation of the vision-based\nclassification of social gestures, such as handshake, hug, high-five, etc. This\nis a component of the mediated social touch systems, which can be incorporated\ninto ShareTable and SqueezeBands system to achieve automated gestures\nrecognition and transmission of the touch between the users in real time. The\nresults from our pilot study show the recognition accuracy of each gestures,\nand they indicate that significant future work is necessary to improve its\npractical feasibility in the mediated social touch applications.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 22:45:43 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Yao", "Yuan", ""], ["Yarosh", "Svetlana", ""]]}, {"id": "1707.02698", "submitter": "Lex Fridman", "authors": "Lex Fridman, Bruce Mehler, Lei Xia, Yangyang Yang, Laura Yvonne\n  Facusse, Bryan Reimer", "title": "To Walk or Not to Walk: Crowdsourced Assessment of External\n  Vehicle-to-Pedestrian Displays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers, technology reviewers, and governmental agencies have expressed\nconcern that automation may necessitate the introduction of added displays to\nindicate vehicle intent in vehicle-to-pedestrian interactions. An automated\nonline methodology for obtaining communication intent perceptions for 30\nexternal vehicle-to-pedestrian display concepts was implemented and tested\nusing Amazon Mechanic Turk. Data from 200 qualified participants was quickly\nobtained and processed. In addition to producing a useful early-stage\nevaluation of these specific design concepts, the test demonstrated that the\nmethodology is scalable so that a large number of design elements or minor\nvariations can be assessed through a series of runs even on much larger samples\nin a matter of hours. Using this approach, designers should be able to refine\nconcepts both more quickly and in more depth than available development\nresources typically allow. Some concerns and questions about common assumptions\nrelated to the implementation of vehicle-to-pedestrian displays are posed.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 05:25:17 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Fridman", "Lex", ""], ["Mehler", "Bruce", ""], ["Xia", "Lei", ""], ["Yang", "Yangyang", ""], ["Facusse", "Laura Yvonne", ""], ["Reimer", "Bryan", ""]]}, {"id": "1707.02932", "submitter": "Farhad Shahbazi", "authors": "Mohammad Sharifi, Hamed Farahani, Farhad Shahbazi, Masood Sharifi,\n  Christofer T. Kello, and Marzieh Zare", "title": "Complexity of eye fixation duration time series in reading of Persian\n  texts: A multifractal detrended fluctuation analysis", "comments": "7 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing evidence that cognitive processes may have fractal\nstructures as a signature of complexity. It is an an ongoing topic of research\nto study the class of complexity and how it may differ as a function of\ncognitive variables. Here, we explore the eye movement trajectories generated\nduring reading different Persian texts. Features of eye movement trajectories\nwere recorded during reading Persian texts using an eye tracker. We show that\nfixation durations, as the main components of eye movements reflecting\ncognitive processing, exhibits multifractal behavior. This indicates that\nmultiple exponents are needed to capture the neural and cognitive processes\ninvolved in decoding symbols to derive meaning. We test whether multifractal\nbehavior varies as a function of two different fonts, familiarity of the text\nfor readers, and reading silently or aloud, and goal-oriented versus\nnon-goal-oriented reading. We find that, while mean fixation duration is\naffected by some of these factors, the multifractal pattern in time series of\neye fixation durations did not change significantly. Our results suggest that\nmultifractal dynamics may be intrinsic to the reading process.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 16:34:00 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Sharifi", "Mohammad", ""], ["Farahani", "Hamed", ""], ["Shahbazi", "Farhad", ""], ["Sharifi", "Masood", ""], ["Kello", "Christofer T.", ""], ["Zare", "Marzieh", ""]]}, {"id": "1707.03540", "submitter": "Moritz Schubotz", "authors": "Moritz Schubotz, Norman Meuschke, Thomas Hepp, Howard S. Cohl and Bela\n  Gipp", "title": "VMEXT: A Visualization Tool for Mathematical Expression Trees", "comments": "15 pages, 4 figures, Intelligent Computer Mathematics - 10th\n  International Conference CICM 2017, Edinburgh, UK, July 17-21, 2017,\n  Proceedings", "journal-ref": "Lecture Notes in Computer Science, Springer 2017", "doi": "10.1007/978-3-319-62075-6_24", "report-no": null, "categories": "cs.HC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical expressions can be represented as a tree consisting of terminal\nsymbols, such as identifiers or numbers (leaf nodes), and functions or\noperators (non-leaf nodes). Expression trees are an important mechanism for\nstoring and processing mathematical expressions as well as the most frequently\nused visualization of the structure of mathematical expressions. Typically,\nresearchers and practitioners manually visualize expression trees using\ngeneral-purpose tools. This approach is laborious, redundant, and error-prone.\nManual visualizations represent a user's notion of what the markup of an\nexpression should be, but not necessarily what the actual markup is. This paper\npresents VMEXT - a free and open source tool to directly visualize expression\ntrees from parallel MathML. VMEXT simultaneously visualizes the presentation\nelements and the semantic structure of mathematical expressions to enable users\nto quickly spot deficiencies in the Content MathML markup that does not affect\nthe presentation of the expression. Identifying such discrepancies previously\nrequired reading the verbose and complex MathML markup. VMEXT also allows one\nto visualize similar and identical elements of two expressions. Visualizing\nexpression similarity can support support developers in designing retrieval\napproaches and enable improved interaction concepts for users of mathematical\ninformation retrieval systems. We demonstrate VMEXT's visualizations in two\nweb-based applications. The first application presents the visualizations\nalone. The second application shows a possible integration of the\nvisualizations in systems for mathematical knowledge management and\nmathematical information retrieval. The application converts LaTeX input to\nparallel MathML, computes basic similarity measures for mathematical\nexpressions, and visualizes the results using VMEXT.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 04:57:45 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Schubotz", "Moritz", ""], ["Meuschke", "Norman", ""], ["Hepp", "Thomas", ""], ["Cohl", "Howard S.", ""], ["Gipp", "Bela", ""]]}, {"id": "1707.03742", "submitter": "Francisco Gomez-Donoso", "authors": "Francisco Gomez-Donoso, Sergio Orts-Escolano and Miguel Cazorla", "title": "Large-scale Multiview 3D Hand Pose Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate hand pose estimation at joint level has several uses on human-robot\ninteraction, user interfacing and virtual reality applications. Yet, it\ncurrently is not a solved problem. The novel deep learning techniques could\nmake a great improvement on this matter but they need a huge amount of\nannotated data. The hand pose datasets released so far present some issues that\nmake them impossible to use on deep learning methods such as the few number of\nsamples, high-level abstraction annotations or samples consisting in depth\nmaps. In this work, we introduce a multiview hand pose dataset in which we\nprovide color images of hands and different kind of annotations for each, i.e\nthe bounding box and the 2D and 3D location on the joints in the hand. Besides,\nwe introduce a simple yet accurate deep learning architecture for real-time\nrobust 2D hand pose estimation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 14:39:49 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 08:05:53 GMT"}, {"version": "v3", "created": "Tue, 18 Jul 2017 19:02:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Gomez-Donoso", "Francisco", ""], ["Orts-Escolano", "Sergio", ""], ["Cazorla", "Miguel", ""]]}, {"id": "1707.03751", "submitter": "Valdis Vitolins", "authors": "MacKenzie Cumings, Valdis V\\=itoli\\c{n}\\v{s}", "title": "New Symbols for Base-16 and Base-256 Numerals", "comments": null, "journal-ref": "International Journal of Computer Science & Engineering Technology\n  (2017) pp. 205-212. ISSN 2229-3345", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new system of hexadecimal and base-256 numerals is proposed whose digit\nshapes are based on binary numerals. The proposed numerals are implemented in\nopen source fonts and integrated into popular editors (Notepad++ and Eclipse)\nto prove the concept.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 13:50:20 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Cumings", "MacKenzie", ""], ["V\u012btoli\u0146\u0161", "Valdis", ""]]}, {"id": "1707.03753", "submitter": "Valdis Vitolins", "authors": "Valdis Vitolins", "title": "Modernized Latvian Ergonomic Keyboard", "comments": "7 pages, 7 figures, presented in conference of Latvian Ergonomic\n  Society on 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increasingly more people use computers and create content using keyboards\n(even with leading edge touch-screen technology). As in the most part of the\nworld, in Latvia also conventional \"Qwerty\" keyboard is used. Though for\nLatvian it is much worse than for English, especially due to enormous load to\nlittle fingers. It causes repetitive strain injuries and affects productivity\nof workers with extensive keyboard usage, especially for data input operators,\ncall centers, inquiry office workers, etc. Improving computer keyboard layout\ndecrease stress to hands and fingers thus minimizing exhaustion and injuries.\nWith analysis of English and Latvian public domain novels and modern texts,\nletter appearance an sequence distribution for Latvian language was found.\nQualities of alternative layouts for English (Dvorak, Colemak, Hallinstad) were\ninvestigated and open source carpalx simulation tool was adjusted according to\nthe findings. Then carpalx was used to check more than 25 million keyboard\nlayouts, measuring finger/hand effort, stroke typing convenience etc., to find\nthe best one. It was proved that existing \"\\v{S}usildatec\" (classic Latvian\nErgonomic standard) keyboard is only slightly better than \"Qwerty\" for Latvian,\nthough it is much worse for English. After computer simulation, several best\nlayouts were tried practically for more than 6 months and most convenient one\nwas promoted as a new \"Latvian Modern\" keyboard. Its typing effort is less than\nfor \"\\v{S}usildatec\", load is distributed according to finger strength, and\ntyping strokes are alternating better between hands and fingers. Comparing to\n\"Qwerty\" keyboard new layout is better not only for Latvian but for English\nalso. Keyboard drivers are developed for Microsoft Windows and Linux operating\nsystems and are freely available in the web under permissive license.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 13:51:33 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Vitolins", "Valdis", ""]]}, {"id": "1707.03988", "submitter": "Fulvio Mastrogiovanni", "authors": "Luca Buoncompagni, Barbara Bruno, Antonella Giuni, Fulvio\n  Mastrogiovanni, Renato Zaccaria", "title": "Arianna: towards a new paradigm for assistive technology at home", "comments": "Paper accepted at the Eight Italian Forum on Ambient Assisted Living\n  (ForItAAL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing elderly and people with special needs to retain their independence\nas long as possible is one of the biggest challenges of the society of\ntomorrow. Teseo, a startup company spinoff from the University of Genoa, aims\nat accelerating the transition towards a sustainable healthcare system. Teseo's\nfirst concept and product, Arianna, allows for the automated recognition of\nactivities of daily living at home and acts as a wellbeing and healthcare\npersonalized assistant. This abstract outlines the main concepts underlying its\nfeatures and capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 06:28:37 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Buoncompagni", "Luca", ""], ["Bruno", "Barbara", ""], ["Giuni", "Antonella", ""], ["Mastrogiovanni", "Fulvio", ""], ["Zaccaria", "Renato", ""]]}, {"id": "1707.04281", "submitter": "Cagatay Demiralp", "authors": "Marco Cavallo and \\c{C}a\\u{g}atay Demiralp", "title": "Exploring Dimensionality Reductions with Forward and Backward\n  Projections", "comments": "KDD IDEA'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimensionality reduction is a common method for analyzing and visualizing\nhigh-dimensional data across domains. Dimensionality-reduction algorithms\ninvolve complex optimizations and the reduced dimensions computed by these\nalgorithms generally lack clear relation to the initial data dimensions.\nTherefore, interpreting and reasoning about dimensionality reductions can be\ndifficult. In this work, we introduce two interaction techniques,\n\\textit{forward projection} and \\textit{backward projection}, for reasoning\ndynamically about scatter plots of dimensionally reduced data. We also\ncontribute two related visualization techniques, \\textit{prolines} and\n\\textit{feasibility map} to facilitate and enrich the effective use of the\nproposed interactions, which we integrate in a new tool called \\textit{Praxis}.\nTo evaluate our techniques, we first analyze their time and accuracy\nperformance across varying sample and dimension sizes. We then conduct a user\nstudy in which twelve data scientists use \\textit{Praxis} so as to assess the\nusefulness of the techniques in performing exploratory data analysis tasks.\nResults suggest that our visual interactions are intuitive and effective for\nexploring dimensionality reductions and generating hypotheses about the\nunderlying data.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 18:50:00 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 19:58:47 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1707.04291", "submitter": "An Yan", "authors": "An Yan, Michael J. Lee, Andrew J. Ko", "title": "Predicting Abandonment in Online Coding Tutorials", "comments": "Accepted to IEEE Symposium on Visual Languages and Human-Centric\n  Computing (VL/HCC), 2017", "journal-ref": null, "doi": "10.1109/VLHCC.2017.8103467", "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learners regularly abandon online coding tutorials when they get bored or\nfrustrated, but there are few techniques for anticipating this abandonment to\nintervene. In this paper, we examine the feasibility of predicting abandonment\nwith machine-learned classifiers. Using interaction logs from an online\nprogramming game, we extracted a collection of features that are potentially\nrelated to learner abandonment and engagement, then developed classifiers for\neach level. Across the first five levels of the game, our classifiers\nsuccessfully predicted 61% to 76% of learners who did not complete the next\nlevel, achieving an average AUC of 0.68. In these classifiers, features\nnegatively associated with abandonment included account activation and\nhelp-seeking behaviors, whereas features positively associated with abandonment\nincluded features indicating difficulty and disengagement. These findings\nhighlight the feasibility of providing timely intervention to learners likely\nto quit.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 19:55:00 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Yan", "An", ""], ["Lee", "Michael J.", ""], ["Ko", "Andrew J.", ""]]}, {"id": "1707.04790", "submitter": "Md. Iftekhar Tanveer", "authors": "Md Iftekhar Tanveer, RuJie Zhao, Mohammed Hoque", "title": "Automatic Identification of Non-Meaningful Body-Movements and What It\n  Reveals About Humans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to identify whether a public speaker's body movements\nare meaningful or non-meaningful (\"Mannerisms\") in the context of their\nspeeches. In a dataset of 84 public speaking videos from 28 individuals, we\nextract 314 unique body movement patterns (e.g. pacing, gesturing, shifting\nbody weights, etc.). Online workers and the speakers themselves annotated the\nmeaningfulness of the patterns. We extracted five types of features from the\naudio-video recordings: disfluency, prosody, body movements, facial, and\nlexical. We use linear classifiers to predict the annotations with AUC up to\n0.82. Analysis of the classifier weights reveals that it puts larger weights on\nthe lexical features while predicting self-annotations. Contrastingly, it puts\na larger weight on prosody features while predicting audience annotations. This\nanalysis might provide subtle hint that public speakers tend to focus more on\nthe verbal features while evaluating self-performances. The audience, on the\nother hand, tends to focus more on the non-verbal aspects of the speech. The\ndataset and code associated with this work has been released for peer review\nand further analysis.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 21:33:52 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Tanveer", "Md Iftekhar", ""], ["Zhao", "RuJie", ""], ["Hoque", "Mohammed", ""]]}, {"id": "1707.04792", "submitter": "Ding Zhao", "authors": "Ding Zhao, Huei Peng", "title": "From the Lab to the Street: Solving the Challenge of Accelerating\n  Automated Vehicle Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As automated vehicles and their technology become more advanced and\ntechnically sophisticated, evaluation procedures that can measure the safety\nand reliability of these new driverless cars must develop far beyond existing\nsafety tests. To get an accurate assessment in field tests, such cars would\nhave to be driven millions or even billions of miles to arrive at an acceptable\nlevel of certainty - a time-consuming process that would cost tens of millions\nof dollars.\n  Instead, researchers affiliated with the University of Michigan's Mcity\nconnected and automated vehicle center have developed an accelerated evaluation\nprocess that eliminates the many miles of uneventful driving activity to filter\nout only the potentially dangerous driving situations where an automated\nvehicle needs to respond, creating a faster, less expensive testing program.\nThis approach can reduce the amount of testing needed by a factor of 300 to\n100,000 so that an automated vehicle driven for 1,000 test miles can yield the\nequivalent of 300,000 to 100 million miles of real-world driving.\n  While more research and development needs to be done to perfect this\ntechnique, the accelerated evaluation procedure offers a ground-breaking\nsolution for safe and efficient testing that is crucial to deploying automated\nvehicles.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 21:56:16 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Zhao", "Ding", ""], ["Peng", "Huei", ""]]}, {"id": "1707.04935", "submitter": "Yuri G. Gordienko", "authors": "Serhii Hamotskyi, Anis Rojbi, Sergii Stirenko, and Yuri Gordienko", "title": "Automatized Generation of Alphabets of Symbols", "comments": "4 pages, 3 figures; Federated Conference on Computer Science and\n  Information Systems, Prague (FedCSIS-2017) (Prague, Czech Republic)", "journal-ref": "Proceedings of the 2017 Federated Conference on Computer Science\n  and Information Systems (FedCSIS-2017), p.639-642, Prague, Czech Republic,\n  September 3-6, 2017", "doi": "10.15439/2017F413", "report-no": null, "categories": "cs.HC cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the generation of symbols (and alphabets) based on\nspecific user requirements (medium, priorities, type of information that needs\nto be conveyed). A framework for the generation of alphabets is proposed, and\nits use for the generation of a shorthand writing system is explored. We\ndiscuss the possible use of machine learning and genetic algorithms to gather\ninputs for generation of such alphabets and for optimization of already\ngenerated ones. The alphabets generated using such methods may be used in very\ndifferent fields, from the creation of synthetic languages and constructed\nscripts to the creation of sensible commands for multimodal interaction through\nHuman-Computer Interfaces, such as mouse gestures, touchpads, body gestures,\neye-tracking cameras, and brain-computing Interfaces, especially in\napplications for elderly care and people with disabilities.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 19:40:26 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Hamotskyi", "Serhii", ""], ["Rojbi", "Anis", ""], ["Stirenko", "Sergii", ""], ["Gordienko", "Yuri", ""]]}, {"id": "1707.05015", "submitter": "Ethan Fast", "authors": "Ethan Fast, Binbin Chen, Julia Mendelsohn, Jonathan Bassen, Michael\n  Bernstein", "title": "Iris: A Conversational Agent for Complex Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's conversational agents are restricted to simple standalone commands.\nIn this paper, we present Iris, an agent that draws on human conversational\nstrategies to combine commands, allowing it to perform more complex tasks that\nit has not been explicitly designed to support: for example, composing one\ncommand to \"plot a histogram\" with another to first \"log-transform the data\".\nTo enable this complexity, we introduce a domain specific language that\ntransforms commands into automata that Iris can compose, sequence, and execute\ndynamically by interacting with a user through natural language, as well as a\nconversational type system that manages what kinds of commands can be combined.\nWe have designed Iris to help users with data science tasks, a domain that\nrequires support for command combination. In evaluation, we find that data\nscientists complete a predictive modeling task significantly faster (2.6 times\nspeedup) with Iris than a modern non-conversational programming environment.\nIris supports the same kinds of commands as today's agents, but empowers users\nto weave together these commands to accomplish complex goals.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 06:55:43 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Fast", "Ethan", ""], ["Chen", "Binbin", ""], ["Mendelsohn", "Julia", ""], ["Bassen", "Jonathan", ""], ["Bernstein", "Michael", ""]]}, {"id": "1707.05411", "submitter": "Ioannis Rigas", "authors": "Ioannis Rigas, Hayes Raffle, and Oleg V. Komogortsev", "title": "Hybrid PS-V Technique: A Novel Sensor Fusion Approach for Fast Mobile\n  Eye-Tracking with Sensor-Shift Aware Correction", "comments": "11 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces and evaluates a hybrid technique that fuses efficiently\nthe eye-tracking principles of photosensor oculography (PSOG) and video\noculography (VOG). The main concept of this novel approach is to use a few fast\nand power-economic photosensors as the core mechanism for performing high speed\neye-tracking, whereas in parallel, use a video sensor operating at low\nsampling-rate (snapshot mode) to perform dead-reckoning error correction when\nsensor movements occur. In order to evaluate the proposed method, we simulate\nthe functional components of the technique and present our results in\nexperimental scenarios involving various combinations of horizontal and\nvertical eye and sensor movements. Our evaluation shows that the developed\ntechnique can be used to provide robustness to sensor shifts that otherwise\ncould induce error larger than 5 deg. Our analysis suggests that the technique\ncan potentially enable high speed eye-tracking at low power profiles, making it\nsuitable to be used in emerging head-mounted devices, e.g. AR/VR headsets.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 23:26:09 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 04:28:11 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Rigas", "Ioannis", ""], ["Raffle", "Hayes", ""], ["Komogortsev", "Oleg V.", ""]]}, {"id": "1707.05413", "submitter": "Ioannis Rigas", "authors": "Ioannis Rigas, Hayes Raffle, and Oleg V. Komogortsev", "title": "Photosensor Oculography: Survey and Parametric Analysis of Designs using\n  Model-Based Simulation", "comments": "12 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a renewed overview of photosensor oculography (PSOG), an\neye-tracking technique based on the principle of using simple photosensors to\nmeasure the amount of reflected (usually infrared) light when the eye rotates.\nPhotosensor oculography can provide measurements with high precision, low\nlatency and reduced power consumption, and thus it appears as an attractive\noption for performing eye-tracking in the emerging head-mounted interaction\ndevices, e.g. augmented and virtual reality (AR/VR) headsets. In our current\nwork we employ an adjustable simulation framework as a common basis for\nperforming an exploratory study of the eye-tracking behavior of different\nphotosensor oculography designs. With the performed experiments we explore the\neffects from the variation of some basic parameters of the designs on the\nresulting accuracy and cross-talk, which are crucial characteristics for the\nseamless operation of human-computer interaction applications based on\neye-tracking. Our experimental results reveal the design trade-offs that need\nto be adopted to tackle the competing conditions that lead to optimum\nperformance of different eye-tracking characteristics. We also present the\ntransformations that arise in the eye-tracking output when sensor shifts occur,\nand assess the resulting degradation in accuracy for different combinations of\neye movements and sensor shifts.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 23:31:57 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 04:26:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Rigas", "Ioannis", ""], ["Raffle", "Hayes", ""], ["Komogortsev", "Oleg V.", ""]]}, {"id": "1707.05645", "submitter": "Snehalkumar `Neil' S. Gaikwad", "authors": "Snehalkumar \"Neil\" S. Gaikwad, Nalin Chhibber, Vibhor Sehgal, Alipta\n  Ballav, Catherine Mullings, Ahmed Nasser, Angela Richmond-Fuller, Aaron\n  Gilbee, Dilrukshi Gamage, Mark Whiting, Sharon Zhou, Sekandar Matin,\n  Senadhipathige Niranga, Shirish Goyal, Dinesh Majeti, Preethi Srinivas, Adam\n  Ginzberg, Kamila Mananova, Karolina Ziulkoski, Jeff Regino, Tejas Sarma,\n  Akshansh Sinha, Abhratanu Paul, Christopher Diemert, Mahesh Murag, William\n  Dai, Manoj Pandey, Rajan Vaish and Michael Bernstein", "title": "Prototype Tasks: Improving Crowdsourcing Results through Rapid,\n  Iterative Task Design", "comments": "2 pages (with 2 pages references, 2 pages Appx), HCOMP 2017,\n  Association for the Advancement of Artificial Intelligence (www.aaai.org)", "journal-ref": null, "doi": null, "report-no": "1952894A", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-quality results have been a long-standing problem on microtask\ncrowdsourcing platforms, driving away requesters and justifying low wages for\nworkers. To date, workers have been blamed for low-quality results: they are\nsaid to make as little effort as possible, do not pay attention to detail, and\nlack expertise. In this paper, we hypothesize that requesters may also be\nresponsible for low-quality work: they launch unclear task designs that confuse\neven earnest workers, under-specify edge cases, and neglect to include\nexamples. We introduce prototype tasks, a crowdsourcing strategy requiring all\nnew task designs to launch a small number of sample tasks. Workers attempt\nthese tasks and leave feedback, enabling the re- quester to iterate on the\ndesign before publishing it. We report a field experiment in which tasks that\nunderwent prototype task iteration produced higher-quality work results than\nthe original task designs. With this research, we suggest that a simple and\nrapid iteration cycle can improve crowd work, and we provide empirical evidence\nthat requester \"quality\" directly impacts result quality.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 14:35:20 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Gaikwad", "Snehalkumar \"Neil\" S.", ""], ["Chhibber", "Nalin", ""], ["Sehgal", "Vibhor", ""], ["Ballav", "Alipta", ""], ["Mullings", "Catherine", ""], ["Nasser", "Ahmed", ""], ["Richmond-Fuller", "Angela", ""], ["Gilbee", "Aaron", ""], ["Gamage", "Dilrukshi", ""], ["Whiting", "Mark", ""], ["Zhou", "Sharon", ""], ["Matin", "Sekandar", ""], ["Niranga", "Senadhipathige", ""], ["Goyal", "Shirish", ""], ["Majeti", "Dinesh", ""], ["Srinivas", "Preethi", ""], ["Ginzberg", "Adam", ""], ["Mananova", "Kamila", ""], ["Ziulkoski", "Karolina", ""], ["Regino", "Jeff", ""], ["Sarma", "Tejas", ""], ["Sinha", "Akshansh", ""], ["Paul", "Abhratanu", ""], ["Diemert", "Christopher", ""], ["Murag", "Mahesh", ""], ["Dai", "William", ""], ["Pandey", "Manoj", ""], ["Vaish", "Rajan", ""], ["Bernstein", "Michael", ""]]}, {"id": "1707.05754", "submitter": "Dingzeyu Li", "authors": "Dingzeyu Li, Avinash S. Nair, Shree K. Nayar, Changxi Zheng", "title": "AirCode: Unobtrusive Physical Tags for Digital Fabrication", "comments": "ACM UIST 2017 Technical Papers", "journal-ref": null, "doi": "10.1145/3126594.3126635", "report-no": null, "categories": "cs.HC cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AirCode, a technique that allows the user to tag physically\nfabricated objects with given information. An AirCode tag consists of a group\nof carefully designed air pockets placed beneath the object surface. These air\npockets are easily produced during the fabrication process of the object,\nwithout any additional material or postprocessing. Meanwhile, the air pockets\naffect only the scattering light transport under the surface, and thus are hard\nto notice to our naked eyes. But, by using a computational imaging method, the\ntags become detectable. We present a tool that automates the design of air\npockets for the user to encode information. AirCode system also allows the user\nto retrieve the information from captured images via a robust decoding\nalgorithm. We demonstrate our tagging technique with applications for metadata\nembedding, robotic grasping, as well as conveying object affordances.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:27:16 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 22:34:53 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Li", "Dingzeyu", ""], ["Nair", "Avinash S.", ""], ["Nayar", "Shree K.", ""], ["Zheng", "Changxi", ""]]}, {"id": "1707.05768", "submitter": "Richard Seymour", "authors": "Bobby Filar, Richard J. Seymour, Matthew Park", "title": "Ask Me Anything: A Conversational Interface to Augment Information\n  Security Workers", "comments": "8 pages, 3 figures, SOUPS 3rd Workshop on Security Information\n  Workers (WSIW 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security products often create more problems than they solve, drowning users\nin alerts without providing the context required to remediate threats. This\nchallenge is compounded by a lack of experienced personnel and security tools\nwith complex interfaces. These interfaces require users to become domain\nexperts or rely on repetitive, time consuming tasks to turn this data deluge\ninto actionable intelligence. In this paper we present Artemis, a\nconversational interface to endpoint detection and response (EDR) event data.\nArtemis leverages dialog to drive the automation of complex tasks and reduce\nthe need to learn a structured query language. Designed to empower\ninexperienced and junior security workers to better understand their security\nenvironment, Artemis provides an intuitive platform to ask questions of alert\ndata as users are guided through triage and hunt workflows. In this paper, we\nwill discuss our user-centric design methodology, feedback from user\ninterviews, and the design requirements generated upon completion of our study.\nWe will also present core functionality, findings from scenario-based testing,\nand future research for the Artemis platform.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:49:46 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Filar", "Bobby", ""], ["Seymour", "Richard J.", ""], ["Park", "Matthew", ""]]}, {"id": "1707.05859", "submitter": "Adam Starr", "authors": "Chiara Zizza, Adam Starr, Devin Hudson, Sai Shreya Nuguri, Prasad\n  Calyam, Zhihai He", "title": "Towards a Social Virtual Reality Learning Environment in High Fidelity", "comments": "4 pages, work-in-progress paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Learning Environments (VLEs) are spaces designed to educate students\nremotely via online platforms. Although traditional VLEs such as iSocial have\nshown promise in educating students, they offer limited immersion that\ndiminishes learning effectiveness. This paper outlines a virtual reality\nlearning environment (VRLE) over a high-speed network, which promotes\neducational effectiveness and efficiency via our creation of flexible content\nand infrastructure which meet established VLE standards with improved\nimmersion. This paper further describes our implementation of multiple learning\nmodules developed in High Fidelity, a \"social VR\" platform. Our experiment\nresults show that the VR mode of content delivery better stimulates the\ngeneralization of lessons to the real world than non-VR lessons and provides\nimproved immersion when compared to an equivalent desktop version.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 21:12:37 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 21:10:04 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zizza", "Chiara", ""], ["Starr", "Adam", ""], ["Hudson", "Devin", ""], ["Nuguri", "Sai Shreya", ""], ["Calyam", "Prasad", ""], ["He", "Zhihai", ""]]}, {"id": "1707.05900", "submitter": "Jeremy Kepner", "authors": "Andrew Prout, William Arcand, David Bestor, Bill Bergeron, Chansup\n  Byun, Vijay Gadepally, Matthew Hubbell, Michael Houle, Michael Jones, Peter\n  Michaleas, Lauren Milechin, Julie Mullen, Antonio Rosa, Siddharth Samsi,\n  Albert Reuther, Jeremy Kepner", "title": "MIT SuperCloud Portal Workspace: Enabling HPC Web Application Deployment", "comments": "6 pages, 3 figures, to appear in IEEE HPEC 2017", "journal-ref": null, "doi": "10.1109/HPEC.2017.8091097", "report-no": null, "categories": "cs.DC cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MIT SuperCloud Portal Workspace enables the secure exposure of web\nservices running on high performance computing (HPC) systems. The portal allows\nusers to run any web application as an HPC job and access it from their\nworkstation while providing authentication, encryption, and access control at\nthe system level to prevent unintended access. This capability permits users to\nseamlessly utilize existing and emerging tools that present their user\ninterface as a website on an HPC system creating a portal workspace.\nPerformance measurements indicate that the MIT SuperCloud Portal Workspace\nincurs marginal overhead when compared to a direct connection of the same\nservice.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 00:04:21 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Prout", "Andrew", ""], ["Arcand", "William", ""], ["Bestor", "David", ""], ["Bergeron", "Bill", ""], ["Byun", "Chansup", ""], ["Gadepally", "Vijay", ""], ["Hubbell", "Matthew", ""], ["Houle", "Michael", ""], ["Jones", "Michael", ""], ["Michaleas", "Peter", ""], ["Milechin", "Lauren", ""], ["Mullen", "Julie", ""], ["Rosa", "Antonio", ""], ["Samsi", "Siddharth", ""], ["Reuther", "Albert", ""], ["Kepner", "Jeremy", ""]]}, {"id": "1707.06105", "submitter": "Alexander Rind", "authors": "Markus Wagner (1 and 2), Djordje Slijepcevic (1 and 2), Brian Horsak\n  (1), Alexander Rind (1 and 2), Matthias Zeppelzauer (1 and 2), Wolfgang\n  Aigner (1 and 2) ((1) St. Poelten University of Applied Sciences, Austria,\n  (2) TU Wien, Austria)", "title": "KAVAGait: Knowledge-Assisted Visual Analytics for Clinical Gait Analysis", "comments": "16 pages, 8 figures, minor revisions during the peer review, to\n  appear in IEEE Transactions on Visualization and Computer Graphics", "journal-ref": "IEEE Trans. Visualization and Computer Graphics 25.3 (2018), pp.\n  1528-1542", "doi": "10.1109/TVCG.2017.2785271", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 2014, more than 10 million people in the US were affected by an ambulatory\ndisability. Thus, gait rehabilitation is a crucial part of health care systems.\nThe quantification of human locomotion enables clinicians to describe and\nanalyze a patient's gait performance in detail and allows them to base clinical\ndecisions on objective data. These assessments generate a vast amount of\ncomplex data which need to be interpreted in a short time period. We conducted\na design study in cooperation with gait analysis experts to develop a novel\nKnowledge-Assisted Visual Analytics solution for clinical Gait analysis\n(KAVAGait). KAVAGait allows the clinician to store and inspect complex data\nderived during clinical gait analysis. The system incorporates innovative and\ninteractive visual interface concepts, which were developed based on the needs\nof clinicians. Additionally, an explicit knowledge store (EKS) allows\nexternalization and storage of implicit knowledge from clinicians. It makes\nthis information available for others, supporting the process of data\ninspection and clinical decision making. We validated our system by conducting\nexpert reviews, a user study, and a case study. Results suggest that KAVAGait\nis able to support a clinician during clinical practice by visualizing complex\ngait data and providing knowledge of other clinicians.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 14:04:32 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 12:39:50 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Wagner", "Markus", "", "1 and 2"], ["Slijepcevic", "Djordje", "", "1 and 2"], ["Horsak", "Brian", "", "1 and 2"], ["Rind", "Alexander", "", "1 and 2"], ["Zeppelzauer", "Matthias", "", "1 and 2"], ["Aigner", "Wolfgang", "", "1 and 2"]]}, {"id": "1707.06209", "submitter": "Johannes Welbl", "authors": "Johannes Welbl, Nelson F. Liu, Matt Gardner", "title": "Crowdsourcing Multiple Choice Science Questions", "comments": "accepted for the Workshop on Noisy User-generated Text (W-NUT) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for obtaining high-quality, domain-targeted\nmultiple choice questions from crowd workers. Generating these questions can be\ndifficult without trading away originality, relevance or diversity in the\nanswer options. Our method addresses these problems by leveraging a large\ncorpus of domain-specific text and a small set of existing questions. It\nproduces model suggestions for document selection and answer distractor choice\nwhich aid the human question generation process. With this method we have\nassembled SciQ, a dataset of 13.7K multiple choice science exam questions\n(Dataset available at http://allenai.org/data.html). We demonstrate that the\nmethod produces in-domain questions by providing an analysis of this new\ndataset and by showing that humans cannot distinguish the crowdsourced\nquestions from original questions. When using SciQ as additional training data\nto existing questions, we observe accuracy improvements on real science exams.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 17:28:46 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Welbl", "Johannes", ""], ["Liu", "Nelson F.", ""], ["Gardner", "Matt", ""]]}, {"id": "1707.06354", "submitter": "Jaime Fisac", "authors": "Jaime F. Fisac, Monica A. Gates, Jessica B. Hamrick, Chang Liu, Dylan\n  Hadfield-Menell, Malayandi Palaniappan, Dhruv Malik, S. Shankar Sastry,\n  Thomas L. Griffiths, and Anca D. Dragan", "title": "Pragmatic-Pedagogic Value Alignment", "comments": "Published at the International Symposium on Robotics Research (ISRR\n  2017)", "journal-ref": "International Symposium on Robotics Research, 2017", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As intelligent systems gain autonomy and capability, it becomes vital to\nensure that their objectives match those of their human users; this is known as\nthe value-alignment problem. In robotics, value alignment is key to the design\nof collaborative robots that can integrate into human workflows, successfully\ninferring and adapting to their users' objectives as they go. We argue that a\nmeaningful solution to value alignment must combine multi-agent decision theory\nwith rich mathematical models of human cognition, enabling robots to tap into\npeople's natural collaborative capabilities. We present a solution to the\ncooperative inverse reinforcement learning (CIRL) dynamic game based on\nwell-established cognitive models of decision making and theory of mind. The\nsolution captures a key reciprocity relation: the human will not plan her\nactions in isolation, but rather reason pedagogically about how the robot might\nlearn from them; the robot, in turn, can anticipate this and interpret the\nhuman's actions pragmatically. To our knowledge, this work constitutes the\nfirst formal analysis of value alignment grounded in empirically validated\ncognitive models.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 03:07:19 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 20:44:09 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Fisac", "Jaime F.", ""], ["Gates", "Monica A.", ""], ["Hamrick", "Jessica B.", ""], ["Liu", "Chang", ""], ["Hadfield-Menell", "Dylan", ""], ["Palaniappan", "Malayandi", ""], ["Malik", "Dhruv", ""], ["Sastry", "S. Shankar", ""], ["Griffiths", "Thomas L.", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1707.06633", "submitter": "Martin V\\\"olker", "authors": "Felix Burget, Lukas Dominique Josef Fiederer, Daniel Kuhner, Martin\n  V\\\"olker, Johannes Aldinger, Robin Tibor Schirrmeister, Chau Do, Joschka\n  Boedecker, Bernhard Nebel, Tonio Ball, Wolfram Burgard", "title": "Acting Thoughts: Towards a Mobile Robotic Service Assistant for Users\n  with Limited Communication Skills", "comments": "* FB, LDJF, DK, MV and JA contributed equally to the work. Accepted\n  as a conference paper at the European Conference on Mobile Robotics 2017\n  (ECMR 2017), 6 pages, 3 figures", "journal-ref": "2017 European Conference on Mobile Robots (ECMR)", "doi": "10.1109/ECMR.2017.8098658", "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As autonomous service robots become more affordable and thus available also\nfor the general public, there is a growing need for user friendly interfaces to\ncontrol the robotic system. Currently available control modalities typically\nexpect users to be able to express their desire through either touch, speech or\ngesture commands. While this requirement is fulfilled for the majority of\nusers, paralyzed users may not be able to use such systems. In this paper, we\npresent a novel framework, that allows these users to interact with a robotic\nservice assistant in a closed-loop fashion, using only thoughts. The\nbrain-computer interface (BCI) system is composed of several interacting\ncomponents, i.e., non-invasive neuronal signal recording and decoding,\nhigh-level task planning, motion and manipulation planning as well as\nenvironment perception. In various experiments, we demonstrate its\napplicability and robustness in real world scenarios, considering\nfetch-and-carry tasks and tasks involving human-robot interaction. As our\nresults demonstrate, our system is capable of adapting to frequent changes in\nthe environment and reliably completing given tasks within a reasonable amount\nof time. Combined with high-level planning and autonomous robotic systems,\ninteresting new perspectives open up for non-invasive BCI-based human-robot\ninteractions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 17:51:12 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 06:30:43 GMT"}, {"version": "v3", "created": "Thu, 30 Nov 2017 08:25:20 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 14:52:41 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Burget", "Felix", ""], ["Fiederer", "Lukas Dominique Josef", ""], ["Kuhner", "Daniel", ""], ["V\u00f6lker", "Martin", ""], ["Aldinger", "Johannes", ""], ["Schirrmeister", "Robin Tibor", ""], ["Do", "Chau", ""], ["Boedecker", "Joschka", ""], ["Nebel", "Bernhard", ""], ["Ball", "Tonio", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1707.06691", "submitter": "Jingbo Zhao", "authors": "Jingbo Zhao, Robert S. Allison", "title": "Real-Time Head Gesture Recognition on Head-Mounted Displays using\n  Cascaded Hidden Markov Models", "comments": null, "journal-ref": "J. Zhao and R. S. Allison, \"Real-time head gesture recognition on\n  head-mounted displays using cascaded hidden Markov models,\" 2017 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC), Banff, AB,\n  2017, pp. 2361-2366", "doi": "10.1109/SMC.2017.8122975", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head gesture is a natural means of face-to-face communication between people\nbut the recognition of head gestures in the context of virtual reality and use\nof head gesture as an interface for interacting with virtual avatars and\nvirtual environments have been rarely investigated. In the current study, we\npresent an approach for real-time head gesture recognition on head-mounted\ndisplays using Cascaded Hidden Markov Models. We conducted two experiments to\nevaluate our proposed approach. In experiment 1, we trained the Cascaded Hidden\nMarkov Models and assessed the offline classification performance using\ncollected head motion data. In experiment 2, we characterized the real-time\nperformance of the approach by estimating the latency to recognize a head\ngesture with recorded real-time classification data. Our results show that the\nproposed approach is effective in recognizing head gestures. The method can be\nintegrated into a virtual reality system as a head gesture interface for\ninteracting with virtual worlds.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 19:46:13 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 21:44:31 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Zhao", "Jingbo", ""], ["Allison", "Robert S.", ""]]}, {"id": "1707.06742", "submitter": "Patrice Simard", "authors": "Patrice Y. Simard, Saleema Amershi, David M. Chickering, Alicia\n  Edelman Pelton, Soroush Ghorashi, Christopher Meek, Gonzalo Ramos, Jina Suh,\n  Johan Verwey, Mo Wang, and John Wernsing", "title": "Machine Teaching: A New Paradigm for Building Machine Learning Systems", "comments": "Also available at: http://aka.ms/machineteachingpaper", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2017-26", "categories": "cs.LG cs.AI cs.HC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current processes for building machine learning systems require\npractitioners with deep knowledge of machine learning. This significantly\nlimits the number of machine learning systems that can be created and has led\nto a mismatch between the demand for machine learning systems and the ability\nfor organizations to build them. We believe that in order to meet this growing\ndemand for machine learning systems we must significantly increase the number\nof individuals that can teach machines. We postulate that we can achieve this\ngoal by making the process of teaching machines easy, fast and above all,\nuniversally accessible.\n  While machine learning focuses on creating new algorithms and improving the\naccuracy of \"learners\", the machine teaching discipline focuses on the efficacy\nof the \"teachers\". Machine teaching as a discipline is a paradigm shift that\nfollows and extends principles of software engineering and programming\nlanguages. We put a strong emphasis on the teacher and the teacher's\ninteraction with data, as well as crucial components such as techniques and\ndesign principles of interaction and visualization.\n  In this paper, we present our position regarding the discipline of machine\nteaching and articulate fundamental machine teaching principles. We also\ndescribe how, by decoupling knowledge about machine learning algorithms from\nthe process of teaching, we can accelerate innovation and empower millions of\nnew uses for machine learning models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 02:37:04 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 05:45:05 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 00:16:49 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Simard", "Patrice Y.", ""], ["Amershi", "Saleema", ""], ["Chickering", "David M.", ""], ["Pelton", "Alicia Edelman", ""], ["Ghorashi", "Soroush", ""], ["Meek", "Christopher", ""], ["Ramos", "Gonzalo", ""], ["Suh", "Jina", ""], ["Verwey", "Johan", ""], ["Wang", "Mo", ""], ["Wernsing", "John", ""]]}, {"id": "1707.06939", "submitter": "James Bagrow", "authors": "Xipei Liu and James P. Bagrow", "title": "Autocompletion interfaces make crowd workers slower, but their use\n  promotes response diversity", "comments": "12 pages, 6 figures", "journal-ref": "Human Computation 6:1:42-55 (2019)", "doi": "10.15346/hc.v6i1.3", "report-no": null, "categories": "cs.HC cs.CL cs.CY cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creative tasks such as ideation or question proposal are powerful\napplications of crowdsourcing, yet the quantity of workers available for\naddressing practical problems is often insufficient. To enable scalable\ncrowdsourcing thus requires gaining all possible efficiency and information\nfrom available workers. One option for text-focused tasks is to allow assistive\ntechnology, such as an autocompletion user interface (AUI), to help workers\ninput text responses. But support for the efficacy of AUIs is mixed. Here we\ndesigned and conducted a randomized experiment where workers were asked to\nprovide short text responses to given questions. Our experimental goal was to\ndetermine if an AUI helps workers respond more quickly and with improved\nconsistency by mitigating typos and misspellings. Surprisingly, we found that\nneither occurred: workers assigned to the AUI treatment were slower than those\nassigned to the non-AUI control and their responses were more diverse, not\nless, than those of the control. Both the lexical and semantic diversities of\nresponses were higher, with the latter measured using word2vec. A crowdsourcer\ninterested in worker speed may want to avoid using an AUI, but using an AUI to\nboost response diversity may be valuable to crowdsourcers interested in\nreceiving as much novel information from workers as possible.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 15:41:38 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Liu", "Xipei", ""], ["Bagrow", "James P.", ""]]}, {"id": "1707.07191", "submitter": "Ting-Hao Huang", "authors": "Chieh-Yang Huang, Tristan Labetoulle, Ting-Hao Kenneth Huang, Yi-Pei\n  Chen, Hung-Chen Chen, Vallari Srivastava, Lun-Wei Ku", "title": "MoodSwipe: A Soft Keyboard that Suggests Messages Based on\n  User-Specified Emotions", "comments": "6 pages (including references), EMNLP 2017 Demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MoodSwipe, a soft keyboard that suggests text messages given the\nuser-specified emotions utilizing the real dialog data. The aim of MoodSwipe is\nto create a convenient user interface to enjoy the technology of emotion\nclassification and text suggestion, and at the same time to collect labeled\ndata automatically for developing more advanced technologies. While users\nselect the MoodSwipe keyboard, they can type as usual but sense the emotion\nconveyed by their text and receive suggestions for their message as a benefit.\nIn MoodSwipe, the detected emotions serve as the medium for suggested texts,\nwhere viewing the latter is the incentive to correcting the former. We conduct\nseveral experiments to show the superiority of the emotion classification\nmodels trained on the dialog data, and further to verify good emotion cues are\nimportant context for text suggestion.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 16:32:16 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Huang", "Chieh-Yang", ""], ["Labetoulle", "Tristan", ""], ["Huang", "Ting-Hao Kenneth", ""], ["Chen", "Yi-Pei", ""], ["Chen", "Hung-Chen", ""], ["Srivastava", "Vallari", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "1707.07233", "submitter": "Reza Abiri", "authors": "Reza Abiri, Griffin Heise, Xiaopeng Zhao, Yang Jiang, Fateme Abiri", "title": "Brain Computer Interface for Gesture Control of a Social Robot: an\n  Offline Study", "comments": "Presented in: 25th Iranian Conference on Electrical Engineering\n  (ICEE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain computer interface (BCI) provides promising applications in\nneuroprosthesis and neurorehabilitation by controlling computers and robotic\ndevices based on the patient's intentions. Here, we have developed a novel BCI\nplatform that controls a personalized social robot using noninvasively acquired\nbrain signals. Scalp electroencephalogram (EEG) signals are collected from a\nuser in real-time during tasks of imaginary movements. The imagined body\nkinematics are decoded using a regression model to calculate the user-intended\nvelocity. Then, the decoded kinematic information is mapped to control the\ngestures of a social robot. The platform here may be utilized as a\nhuman-robot-interaction framework by combining with neurofeedback mechanisms to\nenhance the cognitive capability of persons with dementia.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 00:13:03 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Abiri", "Reza", ""], ["Heise", "Griffin", ""], ["Zhao", "Xiaopeng", ""], ["Jiang", "Yang", ""], ["Abiri", "Fateme", ""]]}, {"id": "1707.07402", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen, Hal Daum\\'e III and Jordan Boyd-Graber", "title": "Reinforcement Learning for Bandit Neural Machine Translation with\n  Simulated Human Feedback", "comments": "11 pages, 5 figures, In Proceedings of Empirical Methods in Natural\n  Language Processing (EMNLP) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine translation is a natural candidate problem for reinforcement learning\nfrom human feedback: users provide quick, dirty ratings on candidate\ntranslations to guide a system to improve. Yet, current neural machine\ntranslation training focuses on expensive human-generated reference\ntranslations. We describe a reinforcement learning algorithm that improves\nneural machine translation systems from simulated human feedback. Our algorithm\ncombines the advantage actor-critic algorithm (Mnih et al., 2016) with the\nattention-based neural encoder-decoder architecture (Luong et al., 2015). This\nalgorithm (a) is well-designed for problems with a large action space and\ndelayed rewards, (b) effectively optimizes traditional corpus-level machine\ntranslation metrics, and (c) is robust to skewed, high-variance, granular\nfeedback modeled after actual human behaviors.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 04:35:19 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 17:19:01 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 06:10:55 GMT"}, {"version": "v4", "created": "Sat, 11 Nov 2017 05:01:23 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Nguyen", "Khanh", ""], ["Daum\u00e9", "Hal", "III"], ["Boyd-Graber", "Jordan", ""]]}, {"id": "1707.07672", "submitter": "Ankit Chaudhary", "authors": "J L Raheja, G A Rajsekhar, Ankit Chaudhary", "title": "Controlling a remotely located Robot using Hand Gestures in real time: A\n  DSP implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Telepresence is a necessity for present time as we can't reach everywhere and\nalso it is useful in saving human life at dangerous places. A robot, which\ncould be controlled from a distant location, can solve these problems. This\ncould be via communication waves or networking methods. Also controlling should\nbe in real time and smooth so that it can actuate on every minor signal in an\neffective way. This paper discusses a method to control a robot over the\nnetwork from a distant location. The robot was controlled by hand gestures\nwhich were captured by the live camera. A DSP board TMS320DM642EVM was used to\nimplement image pre-processing and fastening the whole system. PCA was used for\ngesture classification and robot actuation was done according to predefined\nprocedures. Classification information was sent over the network in the\nexperiment. This method is robust and could be used to control any kind of\nrobot over distance.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 22:15:58 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Raheja", "J L", ""], ["Rajsekhar", "G A", ""], ["Chaudhary", "Ankit", ""]]}, {"id": "1707.07887", "submitter": "Mauro Cherubini", "authors": "Gabriela Villalobos Zu\\~niga, Mauro Cherubini", "title": "Not a Technology Person: Motivating Older Adults Toward the Use of\n  Mobile Technology", "comments": "Presented at the International Workshop Mobile Interface Design with\n  Older Adults, part of CHI 2017, Denver, Colorado\n  https://olderadultsmobileinterfaces.wordpress.com/chi-2017/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Older users population is rapidly increasing all over the World. Presently,\nwe observe efforts in the human-computer interaction domain aiming to improve\nlife quality of age 65 and over through the use of mobile apps. Nonetheless,\nthese efforts focus primary on interface and interaction de- sign. Little work\nhas focused on the study of motivation to use and adherence to, of elderly to\ntechnology. Developing specific design guidelines for this population is\nrelevant, however it should be parallel to the study of desire of elderly to\nembrace specific technology in their life. Designers should not be limited to\ntechnology design but consider as well how to fully convey the value that\ntechnology can bring to the lives of the users and motivate adoption. This\nposition paper discusses techniques that might nudge elderly towards the use of\nnew technology.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 09:53:03 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Zu\u00f1iga", "Gabriela Villalobos", ""], ["Cherubini", "Mauro", ""]]}, {"id": "1707.07935", "submitter": "Jelena Mladenovic", "authors": "Jelena Mladenovi\\'c (1), J\\'er\\'emie Mattout, Fabien Lotte (1) ((1)\n  LaBRI, Potioc)", "title": "A generic framework for adaptive EEG-based BCI training and operation", "comments": null, "journal-ref": "Chapter 33: ''A generic framework for adaptive EEG-based BCI\n  training and operation'' , 1, CRC Press: Taylor \\& Francis Group, 2017,\n  Brain-Computer Interfaces Handbook: Technological and Theoretical Advances", "doi": null, "report-no": null, "categories": "cs.HC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are numerous possibilities and motivations for an adaptive BCI, which\nmay not be easy to clarify and organize for a newcomer to the field. To our\nknowledge, there has not been any work done in classifying the literature on\nadaptive BCI in a comprehensive and structured way. We propose a conceptual\nframework, a taxonomy of adaptive BCI methods which encompasses most important\napproaches to fit them in such a way that a reader can clearly visualize which\nelements are being adapted and for what reason. In the interest of having a\nclear review of existing adaptive BCIs, this framework considers adaptation\napproaches for both the user and the machine, i.e., using instructional design\nobservations as well as the usual machine learning techniques. This framework\nnot only provides a coherent review of such extensive literature but also\nenables the reader to perceive gaps and flaws in the current BCI systems, which\nwould hopefully bring novel solutions for an overall improvement.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 12:03:51 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Mladenovi\u0107", "Jelena", ""], ["Mattout", "J\u00e9r\u00e9mie", ""], ["Lotte", "Fabien", ""]]}, {"id": "1707.08011", "submitter": "Michael Lyons", "authors": "Michael J. Lyons", "title": "Machine Intelligence, New Interfaces, and the Art of the Soluble", "comments": "CHI 2015 Workshop on Collaborating with Intelligent Machines:\n  Interfaces for Creative Sound, April 18, 2015, Seoul, Republic of Korea", "journal-ref": null, "doi": "10.6084/m9.figshare.5242045.v1", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Position: (1) Partial solutions to machine intelligence can lead to systems\nwhich may be useful creating interesting and expressive musical works. (2) An\nappropriate general goal for this field is augmenting human expression. (3) The\nstudy of the aesthetics of human augmentation in musical performance is in its\ninfancy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 14:19:23 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Lyons", "Michael J.", ""]]}, {"id": "1707.08019", "submitter": "Michael Lyons", "authors": "Chamin Morikawa, Michael J. Lyons", "title": "Design and Evaluation of Vision-based Head and Face Tracking Interfaces\n  for Assistive Input", "comments": "Current manuscript is not satisfactory with all authors", "journal-ref": "Chapter 7 of Assistive Technologies and Computer Access for Motor\n  Disabilities (ed. G. Kouroupetroglou), IGI Global, 2013 pp. 180 - 205", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction methods based on computer-vision hold the potential to become the\nnext powerful technology to support breakthroughs in the field of\nhuman-computer interaction. Non-invasive vision-based techniques permit\nunconventional interaction methods to be considered, including use of movements\nof the face and head for intentional gestural control of computer systems.\nFacial gesture interfaces open new possibilities for assistive input\ntechnologies. This chapter gives an overview of research aimed at developing\nvision-based head and face-tracking interfaces. This work has important\nimplications for future assistive input devices. To illustrate this concretely\nwe describe work from our own research in which we developed two vision-based\nfacial feature tracking algorithms for human computer interaction and assistive\ninput. Evaluation forms a critical component of this research and we provide\nexamples of new quantitative evaluation tasks as well as the use of model\nreal-world applications for the qualitative evaluation of new interaction\nstyles.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 14:46:46 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 02:13:44 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Morikawa", "Chamin", ""], ["Lyons", "Michael J.", ""]]}, {"id": "1707.08073", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Nicholas Micallef, Nalin Asanka Gamagedara Arachchilage", "title": "A Gamified Approach to Improve Users' Memorability of Fall-back\n  Authentication", "comments": "6", "journal-ref": "Symposium on Usable Privacy and Security SOUPS 2017, July, 2017,\n  Santa Clara, California", "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security questions are one of the techniques used in fall-back authentication\nto retrieve forgotten passwords. This paper proposes a game design which aims\nto improve usability of system-generated security questions. In our game\ndesign, we adapted the popular picture-based \"4 Pics 1 word\" mobile game. This\ngame asks users to pick the word that relates the given pictures. We selected\nthis game because of its use of pictures and cues, in which, psychology\nresearch has found to be important to help with memorability. The proposed game\ndesign focuses on encoding information to users' long- term memory and to aide\nmemorability by using the follow- ing memory retrieval skills: (a) graphical\ncues - by using images in each challenge; (b) verbal cues - by using verbal\ndescriptions as hints; (c) spatial cues - by keeping same or- der of pictures;\n(d) interactivity - engaging nature of the game through the use of persuasive\ntechnology principles.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 16:31:19 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Micallef", "Nicholas", ""], ["Arachchilage", "Nalin Asanka Gamagedara", ""]]}, {"id": "1707.08287", "submitter": "Kevin Xu", "authors": "Yuning Zhang, Maysam Haghdan, and Kevin S. Xu", "title": "Unsupervised Motion Artifact Detection in Wrist-Measured Electrodermal\n  Activity Data", "comments": "To appear at International Symposium on Wearable Computers (ISWC)\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main benefits of a wrist-worn computer is its ability to collect a\nvariety of physiological data in a minimally intrusive manner. Among these\ndata, electrodermal activity (EDA) is readily collected and provides a window\ninto a person's emotional and sympathetic responses. EDA data collected using a\nwearable wristband are easily influenced by motion artifacts (MAs) that may\nsignificantly distort the data and degrade the quality of analyses performed on\nthe data if not identified and removed. Prior work has demonstrated that MAs\ncan be successfully detected using supervised machine learning algorithms on a\nsmall data set collected in a lab setting. In this paper, we demonstrate that\nunsupervised learning algorithms perform competitively with supervised\nalgorithms for detecting MAs on EDA data collected in both a lab-based setting\nand a real-world setting comprising about 23 hours of data. We also find,\nsomewhat surprisingly, that incorporating accelerometer data as well as EDA\nimproves detection accuracy only slightly for supervised algorithms and\nsignificantly degrades the accuracy of unsupervised algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 05:02:45 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Zhang", "Yuning", ""], ["Haghdan", "Maysam", ""], ["Xu", "Kevin S.", ""]]}, {"id": "1707.08569", "submitter": "Ramviyas Parasuraman", "authors": "Mohamed Abudulaziz Ali Haseeb and Ramviyas Parasuraman", "title": "Wisture: RNN-based Learning of Wireless Signals for Gesture Recognition\n  in Unmodified Smartphones", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Wisture, a new online machine learning solution for\nrecognizing touch-less dynamic hand gestures on a smartphone. Wisture relies on\nthe standard Wi-Fi Received Signal Strength (RSS) using a Long Short-Term\nMemory (LSTM) Recurrent Neural Network (RNN), thresholding filters and traffic\ninduction. Unlike other Wi-Fi based gesture recognition methods, the proposed\nmethod does not require a modification of the smartphone hardware or the\noperating system, and performs the gesture recognition without interfering with\nthe normal operation of other smartphone applications.\n  We discuss the characteristics of Wisture, and conduct extensive experiments\nto compare its performance against state-of-the-art machine learning solutions\nin terms of both accuracy and time efficiency. The experiments include a set of\ndifferent scenarios in terms of both spatial setup and traffic between the\nsmartphone and Wi-Fi access points (AP). The results show that Wisture achieves\nan online recognition accuracy of up to 94% (average 78%) in detecting and\nclassifying three hand gestures.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 17:15:15 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 18:55:44 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Haseeb", "Mohamed Abudulaziz Ali", ""], ["Parasuraman", "Ramviyas", ""]]}, {"id": "1707.08949", "submitter": "Artur Lugmayr", "authors": "Eunice Sari, Adi Tedjasaputra, Do Yi Luen Ellen, Henry Duh, Artur\n  Lugmayr", "title": "Proceedings of the 8th Workshop on Semantic Ambient Media Experiences\n  (SAME 2016): Smart Cities for Better Living with HCI and UX (SEACHI),\n  International Series on Information Systems and Management in Creative eMedia\n  (CreMedia)", "comments": null, "journal-ref": "Eunice Sari, et. al., Proc. of the 8th Workshop on Semantic\n  Ambient Media Experiences: Smart Cities for Better Living with HCI and UX,\n  Int. SERIES on Information Systems and Management in Creative eMedia\n  (CreMedia), n. 2016/1, 2017", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital and interactive technologies are becoming increasingly embedded in\neveryday lives of people around the world. Application of technologies such as\nreal-time, context-aware, and interactive technologies; augmented and immersive\nrealities; social media; and location-based services has been particularly\nevident in urban environments where technological and sociocultural\ninfrastructures enable easier deployment and adoption as compared to non-urban\nareas. There has been growing consumer demand for new forms of experiences and\nservices enabled through these emerging technologies. We call this ambient\nmedia, as the media is embedded in the natural human living environment.\n  The 8th Semantic Ambient Media Workshop Experience (SAME) Proceedings where\nbased on a collaboration between the SEACHI Workshop Smart Cities for Better\nLiving with HCI and UX, which has been organized by UX Indonesia and was held\nin conjunction with Computers and Human-Computer Interaction (CHI) 2016 in San\nJose, CA USA.\n  The extended versions of the workshop papers are freely available through\nwww.ambientmediaassociation.org/Journal under open access by the International\nAmbient Media Association (iAMEA). iAMEA is hosting the international open\naccess journal entitled \"International Journal on Information Systems and\nManagement in Creative eMedia\", and the international open access series\n\"International Series on Information Systems and Management in Creative eMedia\"\n(see http://www.ambientmediaassociation.org).\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 17:40:01 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 13:53:57 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Sari", "Eunice", ""], ["Tedjasaputra", "Adi", ""], ["Ellen", "Do Yi Luen", ""], ["Duh", "Henry", ""], ["Lugmayr", "Artur", ""]]}, {"id": "1707.09487", "submitter": "Nikolaos K Tselios", "authors": "Nikolaos Tselios, Manolis Maragoudakis", "title": "Method and apparatus for automatic text input insertion in digital\n  devices with a restricted number of keys", "comments": "European patent office", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A device which contains number of symbol input keys, where the number of\navailable keys is less than the number of symbols of an alphabet of any given\nlanguage, screen, and dynamic reordering table of the symbols which are mapped\nonto those keys, according to a disambiguation method based on the previously\nentered symbols. The device incorporates a previously entered keystrokes\ntracking mechanism, and the key selected by the user detector, as well as a\nmechanism to select the dynamic symbol reordering mapped onto this key\naccording to the information contained to the reordering table. The reordering\ntable occurs from a disambiguation method which reorders the symbol appearance.\nThe reordering information occurs from Bayesian Belief network construction and\ntraining from text corpora of the specific language.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 09:39:17 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Tselios", "Nikolaos", ""], ["Maragoudakis", "Manolis", ""]]}, {"id": "1707.09599", "submitter": "Michael Lyons", "authors": "Michael J. Lyons", "title": "Dimensional Affect and Expression in Natural and Mediated Interaction", "comments": "Invited article presented at the 23rd Annual Meeting of the\n  International Society for Psychophysics, Tokyo, Japan, 20-23 October, 2007,\n  Proceedings of Fechner Day vol. 23 (2007)", "journal-ref": null, "doi": "10.6084/m9.figshare.5258983", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a perceived controversy as to whether the cognitive representation\nof affect is better modelled using a dimensional or categorical theory. This\npaper first suggests that these views are, in fact, compatible. The paper then\ndiscusses this theme and related issues in reference to a commonly stated\napplication domain of research on human affect and expression: human computer\ninteraction (HCI). The novel suggestion here is that a more realistic framing\nof studies of human affect in expression with reference to HCI and,\nparticularly HCHI (Human-Computer-Human Interaction) entails some\nre-formulation of the approach to the basic phenomena themselves. This theme is\nillustrated with several examples from several recent research projects.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 09:28:09 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Lyons", "Michael J.", ""]]}, {"id": "1707.09728", "submitter": "Lik Hang Lee", "authors": "Lik-Hang Lee and Pan Hui", "title": "Interaction Methods for Smart Glasses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the launch of Google Glass in 2014, smart glasses have mainly been\ndesigned to support micro-interactions. The ultimate goal for them to become an\naugmented reality interface has not yet been attained due to an encumbrance of\ncontrols. Augmented reality involves superimposing interactive computer\ngraphics images onto physical objects in the real world. This survey reviews\ncurrent research issues in the area of human computer interaction for smart\nglasses. The survey first studies the smart glasses available in the market and\nafterwards investigates the interaction methods proposed in the wide body of\nliterature. The interaction methods can be classified into hand-held, touch,\nand touchless input. This paper mainly focuses on the touch and touchless\ninput. Touch input can be further divided into on-device and on-body, while\ntouchless input can be classified into hands-free and freehand. Next, we\nsummarize the existing research efforts and trends, in which touch and\ntouchless input are evaluated by a total of eight interaction goals. Finally,\nwe discuss several key design challenges and the possibility of multi-modal\ninput for smart glasses.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 05:49:03 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Lee", "Lik-Hang", ""], ["Hui", "Pan", ""]]}, {"id": "1707.09790", "submitter": "Carsten Eickhoff", "authors": "Zsolt Mezei, Carsten Eickhoff", "title": "Evaluating Music Recommender Systems for Groups", "comments": "Presented at the 2017 Workshop on Value-Aware and Multistakeholder\n  Recommendation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation to groups of users is a challenging and currently only\npassingly studied task. Especially the evaluation aspect often appears ad-hoc\nand instead of truly evaluating on groups of users, synthesizes groups by\nmerging individual preferences.\n  In this paper, we present a user study, recording the individual and shared\npreferences of actual groups of participants, resulting in a robust,\nstandardized evaluation benchmark. Using this benchmarking dataset, that we\nshare with the research community, we compare the respective performance of a\nwide range of music group recommendation techniques proposed in the\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 10:00:55 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Mezei", "Zsolt", ""], ["Eickhoff", "Carsten", ""]]}]