[{"id": "1801.00061", "submitter": "Jose Eduardo Novoa Ilic", "authors": "Jos\\'e Novoa, Juan Pablo Escudero, Josu\\'e Fredes, Jorge Wuth, Rodrigo\n  Mahu and N\\'estor Becerra Yoma", "title": "Multichannel Robot Speech Recognition Database: MChRSR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In real human robot interaction (HRI) scenarios, speech recognition\nrepresents a major challenge due to robot noise, background noise and\ntime-varying acoustic channel. This document describes the procedure used to\nobtain the Multichannel Robot Speech Recognition Database (MChRSR). It is\ncomposed of 12 hours of multichannel evaluation data recorded in a real mobile\nHRI scenario. This database was recorded with a PR2 robot performing different\ntranslational and azimuthal movements. Accordingly, 16 evaluation sets were\nobtained re-recording the clean set of the Aurora 4 database in different\nmovement conditions.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 00:01:08 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Novoa", "Jos\u00e9", ""], ["Escudero", "Juan Pablo", ""], ["Fredes", "Josu\u00e9", ""], ["Wuth", "Jorge", ""], ["Mahu", "Rodrigo", ""], ["Yoma", "N\u00e9stor Becerra", ""]]}, {"id": "1801.00184", "submitter": "Raeid Saqur", "authors": "Raeid Saqur", "title": "H4-Writer: A Text Entry Method Designed For Gaze Controlled Environment\n  with Low KSPC and Spatial Footprint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new text entry technique, namely H4-Writer, designed\nfor gaze controlled environments and aimed at reducing average KSPC . and\nspatial footprint. It also presents an empirical evaluation of this proposed\nsystem by using three different input devices: mouse, gamepad, and eye tracker.\nThe experiment was conducted using 9 participants and the obtained data were\nused to compare the entry speeds, efficiency and KSPC of H4-Writer for all the\ndevices. Over three blocks, the average entry speed was 3.54 wpm for the mouse,\n3.33 wpm for the gamepad and only 2.11 wpm for the eye tracker. While the eye\ntracker fared poorly compared to the mouse and the gamepad on entry speed, it\nshowed significant improvement in entry speed over progressing blocks\nindicating increase in entry speed with practice. A full longitudinal study was\nconducted to indicate this.\n  The average KSPC of all the three devices over all the text phrases entered\nwas 2.62, which is significantly lower compared to other hand writing\nrecognizing text entry techniques like EdgeWrite. An analysis of the blocks\nrevealed improvement in error rate, efficiency and KSPC values with progressing\nblock numbers as the participants got more acclimatized with the key codes for\ncorresponding characters.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 19:43:04 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Saqur", "Raeid", ""]]}, {"id": "1801.01237", "submitter": "Zheng Lian", "authors": "Zheng Lian, Ya Li, Jianhua Tao and Jian Huang", "title": "A pairwise discriminative task for speech emotion recognition", "comments": "I have submitted a new version to arXiv:1910.11174. I forget to\n  choose to replace the old version, but submitted a new one. It's my mistake", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I have submitted a new version to arXiv:1910.11174. I forget to choose to\nreplace the old version, but submitted a new one. It's my mistake.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 03:30:47 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 01:13:51 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Lian", "Zheng", ""], ["Li", "Ya", ""], ["Tao", "Jianhua", ""], ["Huang", "Jian", ""]]}, {"id": "1801.01531", "submitter": "Kevin Bowden", "authors": "Kevin K. Bowden, Jiaqi Wu, Shereen Oraby, Amita Misra and Marilyn\n  Walker", "title": "Slugbot: An Application of a Novel and Scalable Open Domain Socialbot\n  Framework", "comments": null, "journal-ref": "Alexa Prize Proceedings 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel, open domain socialbot for the Amazon\nAlexa Prize competition, aimed at carrying on friendly conversations with users\non a variety of topics. We present our modular system, highlighting our\ndifferent data sources and how we use the human mind as a model for data\nmanagement. Additionally we build and employ natural language understanding and\ninformation retrieval tools and APIs to expand our knowledge bases. We describe\nour semistructured, scalable framework for crafting topic-specific dialogue\nflows, and give details on our dialogue management schemes and scoring\nmechanisms. Finally we briefly evaluate the performance of our system and\nobserve the challenges that an open domain socialbot faces.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 19:58:46 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Bowden", "Kevin K.", ""], ["Wu", "Jiaqi", ""], ["Oraby", "Shereen", ""], ["Misra", "Amita", ""], ["Walker", "Marilyn", ""]]}, {"id": "1801.01565", "submitter": "Pedro Santana", "authors": "Jo\\~ao Antunes and Pedro Santana", "title": "A Study on the Use of Eye Tracking to Adapt Gameplay and Procedural\n  Content Generation in First-Person Shooter Games", "comments": null, "journal-ref": "Multimodal Technologies Interact. 2018, 2, 23", "doi": "10.3390/mti2020023", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the use of eye tracking in a First-Person Shooter (FPS)\ngame as a~mechanism to: (1) control the attention of the player's avatar\naccording to the attention deployed by the player, and (2) guide the gameplay\nand game's procedural content generation, accordingly. This results in a more\nnatural use of eye tracking in comparison to a use in which the eye tracker\ndirectly substitutes control input devices, such as gamepads. The study was\nconducted on a custom endless runner FPS, Zombie Runner, using an affordable\neye tracker. Evaluation sessions showed that the proposed use of eye tracking\nprovides a more challenging and immersive experience to the player, when\ncompared to its absence. However, a strong correlation between eye tracker\ncalibration problems and player's overall experience was found. This means that\neye tracking technology still needs to evolve but also means that once\ntechnology gets mature enough players are expected to benefit greatly from the\ninclusion of eye tracking in their gaming experience.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 22:28:15 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 13:41:38 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Antunes", "Jo\u00e3o", ""], ["Santana", "Pedro", ""]]}, {"id": "1801.01957", "submitter": "Xiaodong He", "authors": "Heung-Yeung Shum, Xiaodong He, Di Li", "title": "From Eliza to XiaoIce: Challenges and Opportunities with Social Chatbots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational systems have come a long way since their inception in the\n1960s. After decades of research and development, we've seen progress from\nEliza and Parry in the 60's and 70's, to task-completion systems as in the\nDARPA Communicator program in the 2000s, to intelligent personal assistants\nsuch as Siri in the 2010s, to today's social chatbots like XiaoIce. Social\nchatbots' appeal lies not only in their ability to respond to users' diverse\nrequests, but also in being able to establish an emotional connection with\nusers. The latter is done by satisfying users' need for communication,\naffection, as well as social belonging. To further the advancement and adoption\nof social chatbots, their design must focus on user engagement and take both\nintellectual quotient (IQ) and emotional quotient (EQ) into account. Users\nshould want to engage with a social chatbot; as such, we define the success\nmetric for social chatbots as conversation-turns per session (CPS). Using\nXiaoIce as an illustrative example, we discuss key technologies in building\nsocial chatbots from core chat to visual awareness to skills. We also show how\nXiaoIce can dynamically recognize emotion and engage the user throughout long\nconversations with appropriate interpersonal responses. As we become the first\ngeneration of humans ever living with AI, we have a responsibility to design\nsocial chatbots to be both useful and empathetic, so they will become\nubiquitous and help society as a whole.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 03:14:22 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 23:30:46 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Shum", "Heung-Yeung", ""], ["He", "Xiaodong", ""], ["Li", "Di", ""]]}, {"id": "1801.02546", "submitter": "Florian Daniel", "authors": "Florian Daniel, Pavel Kucherbaev, Cinzia Cappiello, Boualem\n  Benatallah, Mohammad Allahbakhsh", "title": "Quality Control in Crowdsourcing: A Survey of Quality Attributes,\n  Assessment Techniques and Assurance Actions", "comments": "40 pages main paper, 5 pages appendix", "journal-ref": "ACM Comput. Surv. 51, 1, Article 7 (January 2018)", "doi": "10.1145/3148148", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing enables one to leverage on the intelligence and wisdom of\npotentially large groups of individuals toward solving problems. Common\nproblems approached with crowdsourcing are labeling images, translating or\ntranscribing text, providing opinions or ideas, and similar - all tasks that\ncomputers are not good at or where they may even fail altogether. The\nintroduction of humans into computations and/or everyday work, however, also\nposes critical, novel challenges in terms of quality control, as the crowd is\ntypically composed of people with unknown and very diverse abilities, skills,\ninterests, personal objectives and technological resources. This survey studies\nquality in the context of crowdsourcing along several dimensions, so as to\ndefine and characterize it and to understand the current state of the art.\nSpecifically, this survey derives a quality model for crowdsourcing tasks,\nidentifies the methods and techniques that can be used to assess the attributes\nof the model, and the actions and strategies that help prevent and mitigate\nquality problems. An analysis of how these features are supported by the state\nof the art further identifies open issues and informs an outlook on hot future\nresearch directions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 16:42:17 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Daniel", "Florian", ""], ["Kucherbaev", "Pavel", ""], ["Cappiello", "Cinzia", ""], ["Benatallah", "Boualem", ""], ["Allahbakhsh", "Mohammad", ""]]}, {"id": "1801.02668", "submitter": "Ting-Hao Huang", "authors": "Ting-Hao 'Kenneth' Huang and Joseph Chee Chang and Jeffrey P. Bigham", "title": "Evorus: A Crowd-powered Conversational Assistant Built to Automate\n  Itself Over Time", "comments": "10 pages. To appear in the Proceedings of the Conference on Human\n  Factors in Computing Systems 2018 (CHI'18)", "journal-ref": null, "doi": "10.1145/3173574.3173869", "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd-powered conversational assistants have been shown to be more robust\nthan automated systems, but do so at the cost of higher response latency and\nmonetary costs. A promising direction is to combine the two approaches for high\nquality, low latency, and low cost solutions. In this paper, we introduce\nEvorus, a crowd-powered conversational assistant built to automate itself over\ntime by (i) allowing new chatbots to be easily integrated to automate more\nscenarios, (ii) reusing prior crowd answers, and (iii) learning to\nautomatically approve response candidates. Our 5-month-long deployment with 80\nparticipants and 281 conversations shows that Evorus can automate itself\nwithout compromising conversation quality. Crowd-AI architectures have long\nbeen proposed as a way to reduce cost and latency for crowd-powered systems;\nEvorus demonstrates how automation can be introduced successfully in a deployed\nsystem. Its architecture allows future researchers to make further innovation\non the underlying automated components in the context of a deployed open domain\ndialog system.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 20:07:35 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 03:49:24 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Huang", "Ting-Hao 'Kenneth'", ""], ["Chang", "Joseph Chee", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1801.02691", "submitter": "Fengjiao Peng", "authors": "Fengjiao Peng, Veronica LaBelle, Emily Yue and Rosalind Picard", "title": "A Trip to the Moon: Personalized Animated Movies for Self-reflection", "comments": null, "journal-ref": "Proceedings of the 2018 CHI Conference on Human Factors in\n  Computing Systems. ACM, 2018", "doi": "10.1145/3173574.3173827", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-tracking physiological and psychological data poses the challenge of\npresentation and interpretation. Insightful narratives for self-tracking data\ncan motivate the user towards constructive self-reflection. One powerful form\nof narrative that engages audience across various culture and age groups is\nanimated movies. We collected a week of self-reported mood and behavior data\nfrom each user and created in Unity a personalized animation based on their\ndata. We evaluated the impact of their video in a randomized control trial with\na non-personalized animated video as control. We found that personalized videos\ntend to be more emotionally engaging, encouraging greater and lengthier writing\nthat indicated self-reflection about moods and behaviors, compared to\nnon-personalized control videos.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 21:16:19 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Peng", "Fengjiao", ""], ["LaBelle", "Veronica", ""], ["Yue", "Emily", ""], ["Picard", "Rosalind", ""]]}, {"id": "1801.02788", "submitter": "Ian Dewancker", "authors": "Ian Dewancker, Jakob Bauer, Michael McCourt", "title": "Sequential Preference-Based Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CE cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world engineering problems rely on human preferences to guide their\ndesign and optimization. We present PrefOpt, an open source package to simplify\nsequential optimization tasks that incorporate human preference feedback. Our\napproach extends an existing latent variable model for binary preferences to\nallow for observations of equivalent preference from users.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 04:13:11 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Dewancker", "Ian", ""], ["Bauer", "Jakob", ""], ["McCourt", "Michael", ""]]}, {"id": "1801.02862", "submitter": "Ilya Musabirov", "authors": "Denis Bulygin, Ilya Musabirov, Alena Suvorova, Ksenia Konstantinova,\n  Pavel Okopnyi", "title": "Between an Arena and a Sports Bar: Online Chats of eSports Spectators", "comments": "v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hundreds of thousands of spectators use Twitch.tv to watch The International,\na Dota 2 eSports tournament and communicate in massive chats. In this paper, we\nanalyse crowd behavior in these chats, disentangle features of social\ncommunication, such as contextual meanings of emojis and short messages. We\napply structural topic modelling and cross-correlation analysis to investigate\ntopical and temporal patterns of chat messages and their relation to in-game\nevents. We show that in-game events drive the communication in the massive chat\nand define its emergent topical structure to a various extent. Following the\ndiscussion in communication and social computing literature, we discuss these\nfindings in the framework of analysis of communication of physical sports\ncrowds and outline some limitations of the 'stadium' metaphor, suggesting a\ncomplementary metaphor of 'sports bar' as a useful analytical and design\ndevice.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 10:12:06 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 21:54:02 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2020 00:42:42 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Bulygin", "Denis", ""], ["Musabirov", "Ilya", ""], ["Suvorova", "Alena", ""], ["Konstantinova", "Ksenia", ""], ["Okopnyi", "Pavel", ""]]}, {"id": "1801.02884", "submitter": "David Glowacki", "authors": "Michael O Connor, Helen M. Deeks, Edward Dawn, Oussama Metatla, Anne\n  Roudaut, Matthew Sutton, Becca Rose Glowacki, Rebecca Sage, Philip Tew, Mark\n  Wonnacott, Phil Bates, Adrian J. Mulholland, and David R. Glowacki", "title": "Sampling molecular conformations and dynamics in a multi-user virtual\n  reality framework", "comments": "5 pages, 3 figures, 19 pages Supporting Info", "journal-ref": "Science Advances, Jun 2018: Vol. 4, no. 6, eaat2731", "doi": "10.1126/sciadv.aat2731", "report-no": null, "categories": "physics.chem-ph cs.HC physics.bio-ph physics.ed-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe a framework for interactive molecular dynamics in a multiuser\nvirtual reality environment, combining rigorous cloud-mounted physical\natomistic simulation with commodity virtual reality hardware, which we have\nmade accessible to readers (see isci.itch.io/nsb-imd). It allows users to\nvisualize and sample, with atomic-level precision, the structures and dynamics\nof complex molecular structures 'on the fly', and to interact with other users\nin the same virtual environment. A series of controlled studies, wherein\nparticipants were tasked with a range of molecular manipulation goals\n(threading methane through a nanotube, changing helical screw-sense, and tying\na protein knot), quantitatively demonstrate that users within the interactive\nVR environment can complete sophisticated molecular modelling tasks more\nquickly than they can using conventional interfaces, especially for molecular\npathways and structural transitions whose conformational choreographies are\nintrinsically 3d. This framework should accelerate progress in nanoscale\nmolecular engineering areas such as drug development, synthetic biology, and\ncatalyst design. More broadly, our findings highlight VR's potential in\nscientific domains where 3d dynamics matter, spanning research and education.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 11:08:49 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Connor", "Michael O", ""], ["Deeks", "Helen M.", ""], ["Dawn", "Edward", ""], ["Metatla", "Oussama", ""], ["Roudaut", "Anne", ""], ["Sutton", "Matthew", ""], ["Glowacki", "Becca Rose", ""], ["Sage", "Rebecca", ""], ["Tew", "Philip", ""], ["Wonnacott", "Mark", ""], ["Bates", "Phil", ""], ["Mulholland", "Adrian J.", ""], ["Glowacki", "David R.", ""]]}, {"id": "1801.03261", "submitter": "Zeyuan Hu", "authors": "Zeyuan Hu, Julia Strout", "title": "Exploring Stereotypes and Biased Data with the Crowd", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of our research is to contribute information about how useful the\ncrowd is at anticipating stereotypes that may be biasing a data set without a\nresearcher's knowledge. The results of the crowd's prediction can potentially\nbe used during data collection to help prevent the suspected stereotypes from\nintroducing bias to the dataset. We conduct our research by asking the crowd on\nAmazon's Mechanical Turk (AMT) to complete two similar Human Intelligence Tasks\n(HITs) by suggesting stereotypes relating to their personal experience. Our\nanalysis of these responses focuses on determining the level of diversity in\nthe workers' suggestions and their demographics. Through this process we begin\na discussion on how useful the crowd can be in tackling this difficult problem\nwithin machine learning data collection.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 08:05:40 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Hu", "Zeyuan", ""], ["Strout", "Julia", ""]]}, {"id": "1801.03604", "submitter": "Chandra Khatri", "authors": "Ashwin Ram, Rohit Prasad, Chandra Khatri, Anu Venkatesh, Raefer\n  Gabriel, Qing Liu, Jeff Nunn, Behnam Hedayatnia, Ming Cheng, Ashish Nagar,\n  Eric King, Kate Bland, Amanda Wartick, Yi Pan, Han Song, Sk Jayadevan, Gene\n  Hwang, Art Pettigrue", "title": "Conversational AI: The Science Behind the Alexa Prize", "comments": "18 pages, 5 figures, Alexa Prize Proceedings Paper\n  (https://developer.amazon.com/alexaprize/proceedings), Alexa Prize University\n  Competition to advance Conversational AI", "journal-ref": "Alexa.Prize.Proceedings\n  https://developer.amazon.com/alexaprize/proceedings accessed (2018)-01-01", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CY cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents are exploding in popularity. However, much work remains\nin the area of social conversation as well as free-form conversation over a\nbroad range of domains and topics. To advance the state of the art in\nconversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar\nuniversity competition where sixteen selected university teams were challenged\nto build conversational agents, known as socialbots, to converse coherently and\nengagingly with humans on popular topics such as Sports, Politics,\nEntertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers\nthe academic community a unique opportunity to perform research with a live\nsystem used by millions of users. The competition provided university teams\nwith real user conversational data at scale, along with the user-provided\nratings and feedback augmented with annotations by the Alexa team. This enabled\nteams to effectively iterate and make improvements throughout the competition\nwhile being evaluated in real-time through live user interactions. To build\ntheir socialbots, university teams combined state-of-the-art techniques with\nnovel strategies in the areas of Natural Language Understanding, Context\nModeling, Dialog Management, Response Generation, and Knowledge Acquisition. To\nsupport the efforts of participating teams, the Alexa Prize team made\nsignificant scientific and engineering investments to build and improve\nConversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice\nUser Experience, and tools for traffic management and scalability. This paper\noutlines the advances created by the university teams as well as the Alexa\nPrize team to achieve the common goal of solving the problem of Conversational\nAI.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 01:23:50 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Ram", "Ashwin", ""], ["Prasad", "Rohit", ""], ["Khatri", "Chandra", ""], ["Venkatesh", "Anu", ""], ["Gabriel", "Raefer", ""], ["Liu", "Qing", ""], ["Nunn", "Jeff", ""], ["Hedayatnia", "Behnam", ""], ["Cheng", "Ming", ""], ["Nagar", "Ashish", ""], ["King", "Eric", ""], ["Bland", "Kate", ""], ["Wartick", "Amanda", ""], ["Pan", "Yi", ""], ["Song", "Han", ""], ["Jayadevan", "Sk", ""], ["Hwang", "Gene", ""], ["Pettigrue", "Art", ""]]}, {"id": "1801.03622", "submitter": "Chandra Khatri", "authors": "Fenfei Guo, Angeliki Metallinou, Chandra Khatri, Anirudh Raju, Anu\n  Venkatesh, Ashwin Ram", "title": "Topic-based Evaluation for Conversational Bots", "comments": "10 Pages, 2 figures, 9 tables. NIPS 2017 Conversational AI workshop\n  paper.\n  http://alborz-geramifard.com/workshops/nips17-Conversational-AI/Main.html", "journal-ref": "Nips.Workshop.ConversationalAI 2017-12-08", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog evaluation is a challenging problem, especially for non task-oriented\ndialogs where conversational success is not well-defined. We propose to\nevaluate dialog quality using topic-based metrics that describe the ability of\na conversational bot to sustain coherent and engaging conversations on a topic,\nand the diversity of topics that a bot can handle. To detect conversation\ntopics per utterance, we adopt Deep Average Networks (DAN) and train a topic\nclassifier on a variety of question and query data categorized into multiple\ntopics. We propose a novel extension to DAN by adding a topic-word attention\ntable that allows the system to jointly capture topic keywords in an utterance\nand perform topic classification. We compare our proposed topic based metrics\nwith the ratings provided by users and show that our metrics both correlate\nwith and complement human judgment. Our analysis is performed on tens of\nthousands of real human-bot dialogs from the Alexa Prize competition and\nhighlights user expectations for conversational bots.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 03:20:02 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Guo", "Fenfei", ""], ["Metallinou", "Angeliki", ""], ["Khatri", "Chandra", ""], ["Raju", "Anirudh", ""], ["Venkatesh", "Anu", ""], ["Ram", "Ashwin", ""]]}, {"id": "1801.03625", "submitter": "Chandra Khatri", "authors": "Anu Venkatesh, Chandra Khatri, Ashwin Ram, Fenfei Guo, Raefer Gabriel,\n  Ashish Nagar, Rohit Prasad, Ming Cheng, Behnam Hedayatnia, Angeliki\n  Metallinou, Rahul Goel, Shaohua Yang, Anirudh Raju", "title": "On Evaluating and Comparing Open Domain Dialog Systems", "comments": "10 pages, 5 tables. NIPS 2017 Conversational AI workshop.\n  http://alborz-geramifard.com/workshops/nips17-Conversational-AI/Main.html", "journal-ref": "NIPS.Workshop.ConversationalAI 2017-12-08\n  http://alborz-geramifard.com/workshops/nips17-Conversational-AI/Main.html\n  accessed 2018-01-01", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents are exploding in popularity. However, much work remains\nin the area of non goal-oriented conversations, despite significant growth in\nresearch interest over recent years. To advance the state of the art in\nconversational AI, Amazon launched the Alexa Prize, a 2.5-million dollar\nuniversity competition where sixteen selected university teams built\nconversational agents to deliver the best social conversational experience.\nAlexa Prize provided the academic community with the unique opportunity to\nperform research with a live system used by millions of users. The subjectivity\nassociated with evaluating conversations is key element underlying the\nchallenge of building non-goal oriented dialogue systems. In this paper, we\npropose a comprehensive evaluation strategy with multiple metrics designed to\nreduce subjectivity by selecting metrics which correlate well with human\njudgement. The proposed metrics provide granular analysis of the conversational\nagents, which is not captured in human ratings. We show that these metrics can\nbe used as a reasonable proxy for human judgment. We provide a mechanism to\nunify the metrics for selecting the top performing agents, which has also been\napplied throughout the Alexa Prize competition. To our knowledge, to date it is\nthe largest setting for evaluating agents with millions of conversations and\nhundreds of thousands of ratings from users. We believe that this work is a\nstep towards an automatic evaluation process for conversational AIs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 03:30:00 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 20:15:08 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Venkatesh", "Anu", ""], ["Khatri", "Chandra", ""], ["Ram", "Ashwin", ""], ["Guo", "Fenfei", ""], ["Gabriel", "Raefer", ""], ["Nagar", "Ashish", ""], ["Prasad", "Rohit", ""], ["Cheng", "Ming", ""], ["Hedayatnia", "Behnam", ""], ["Metallinou", "Angeliki", ""], ["Goel", "Rahul", ""], ["Yang", "Shaohua", ""], ["Raju", "Anirudh", ""]]}, {"id": "1801.03650", "submitter": "Azat Khusnutdinov", "authors": "Denis Usachev, Azat Khusnutdinov, Manuel Mazzara, Adil Khan, Ivan\n  Panchenko", "title": "Open source platform Digital Personal Assistant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays Digital Personal Assistants (DPA) become more and more popular. DPAs\nhelp to increase quality of life especially for elderly or disabled people. In\nthis paper we develop an open source DPA and smart home system as a 3-rd party\nextension to show the functionality of the assistant. The system is designed to\nuse the DPA as a learning platform for engineers to provide them with the\nopportunity to create and test their own hypothesis. The DPA is able to\nrecognize users' commands in natural language and transform it to the set of\nmachine commands that can be used to control different 3rd-party application.\nWe use smart home system as an example of such 3rd-party. We demonstrate that\nthe system is able to control home appliances, like lights, or to display\ninformation about the current state of the home, like temperature, through a\ndialogue between a user and the Digital Personal Assistant.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 07:43:41 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 18:33:06 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 18:01:02 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Usachev", "Denis", ""], ["Khusnutdinov", "Azat", ""], ["Mazzara", "Manuel", ""], ["Khan", "Adil", ""], ["Panchenko", "Ivan", ""]]}, {"id": "1801.03829", "submitter": "Kelly Mack", "authors": "Kelly Mack, John Lee, Kevin Chang, Karrie Karahalios, Aditya\n  Parameswaran", "title": "Characterizing Scalability Issues in Spreadsheet Software using Online\n  Forums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional usability studies, researchers talk to users of tools to\nunderstand their needs and challenges. Insights gained via such interviews\noffer context, detail, and background. Due to costs in time and money, we are\nbeginning to see a new form of tool interrogation that prioritizes scale, cost,\nand breadth by utilizing existing data from online forums. In this case study,\nwe set out to apply this method of using online forum data to a specific\nissue---challenges that users face with Excel spreadsheets. Spreadsheets are a\nversatile and powerful processing tool if used properly. However, with\nversatility and power come errors, from both users and the software, which make\nusing spreadsheets less effective. By scraping posts from the website Reddit,\nwe collected a dataset of questions and complaints about Excel. Specifically,\nwe explored and characterized the issues users were facing with spreadsheet\nsoftware in general, and in particular, as resulting from a large amount of\ndata in their spreadsheets. We discuss the implications of our findings on the\ndesign of next-generation spreadsheet software.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 15:47:35 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 04:47:25 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Mack", "Kelly", ""], ["Lee", "John", ""], ["Chang", "Kevin", ""], ["Karahalios", "Karrie", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1801.04069", "submitter": "Huoran Li", "authors": "Huoran Li, Xuanzhe Liu, Qiaozhu Mei", "title": "Predicting Smartphone Battery Life based on Comprehensive and Real-time\n  Usage Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones and smartphone apps have undergone an explosive growth in the\npast decade. However, smartphone battery technology hasn't been able to keep\npace with the rapid growth of the capacity and the functionality of smartphones\nand apps. As a result, battery has always been a bottleneck of a user's daily\nexperience of smartphones. An accurate estimation of the remaining battery life\ncould tremendously help the user to schedule their activities and use their\nsmartphones more efficiently. Existing studies on battery life prediction have\nbeen primitive due to the lack of real-world smartphone usage data at scale.\nThis paper presents a novel method that uses the state-of-the-art machine\nlearning models for battery life prediction, based on comprehensive and\nreal-time usage traces collected from smartphones. The proposed method is the\nfirst that identifies and addresses the severe data missing problem in this\ncontext, using a principled statistical metric called the concordance index.\nThe method is evaluated using a dataset collected from 51 users for 21 months,\nwhich covers comprehensive and fine-grained smartphone usage traces including\nsystem status, sensor indicators, system events, and app status. We find that\nthe remaining battery life of a smartphone can be accurately predicted based on\nhow the user uses the device at the real-time, in the current session, and in\nhistory. The machine learning models successfully identify predictive features\nfor battery life and their applicable scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 06:35:00 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Li", "Huoran", ""], ["Liu", "Xuanzhe", ""], ["Mei", "Qiaozhu", ""]]}, {"id": "1801.04457", "submitter": "Julian Steil", "authors": "Julian Steil, Marion Koelle, Wilko Heuten, Susanne Boll, Andreas\n  Bulling", "title": "PrivacEye: Privacy-Preserving Head-Mounted Eye Tracking Using Egocentric\n  Scene Image and Eye Movement Features", "comments": "10 pages, 6 figures, supplementary material", "journal-ref": null, "doi": "10.1145/3314111.3319913", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eyewear devices, such as augmented reality displays, increasingly integrate\neye tracking but the first-person camera required to map a user's gaze to the\nvisual scene can pose a significant threat to user and bystander privacy. We\npresent PrivacEye, a method to detect privacy-sensitive everyday situations and\nautomatically enable and disable the eye tracker's first-person camera using a\nmechanical shutter. To close the shutter in privacy-sensitive situations, the\nmethod uses a deep representation of the first-person video combined with rich\nfeatures that encode users' eye movements. To open the shutter without visual\ninput, PrivacEye detects changes in users' eye movements alone to gauge changes\nin the \"privacy level\" of the current situation. We evaluate our method on a\nfirst-person video dataset recorded in daily life situations of 17\nparticipants, annotated by themselves for privacy sensitivity, and show that\nour method is effective in preserving privacy in this challenging setting.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 15:46:22 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 17:54:46 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 21:08:23 GMT"}, {"version": "v4", "created": "Fri, 24 May 2019 12:42:07 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Steil", "Julian", ""], ["Koelle", "Marion", ""], ["Heuten", "Wilko", ""], ["Boll", "Susanne", ""], ["Bulling", "Andreas", ""]]}, {"id": "1801.04829", "submitter": "Xiaosong Li", "authors": "Xiaosong Li, Ye Liu, Zizhou Fan and Will Li", "title": "A Quantitative Approach in Heuristic Evaluation of E-commerce Websites", "comments": "Extended paper for AIAA-2017, Published on International Journal of\n  Artificial Intelligence and Applications (IJAIA), Vol.9, No.1, January 2018,\n  13 pages", "journal-ref": "International Journal of Artificial Intelligence and Applications\n  (IJAIA), Vol.9, No.1, January 2018", "doi": "10.5121/ijaia.2018.9101", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a pilot study on developing an instrument to predict the\nquality of e-commerce websites. The 8C model was adopted as the reference model\nof the heuristic evaluation. Each dimension of the 8C was mapped into a set of\nquantitative website elements, selected websites were scraped to get the\nquantitative website elements, and the score of each dimension was calculated.\nA software was developed in PHP for the experiments. In the training process,\n10 experiments were conducted and quantitative analyses were regressively\nconducted between the experiments. The conversion rate was used to verify the\nheuristic evaluation of an e-commerce website after each experiment. The\nresults showed that the mapping revisions between the experiments improved the\nperformance of the evaluation instrument, therefore the experiment process and\nthe quantitative mapping revision guideline proposed was on the right track.\nThe software resulted from the experiment 10 can serve as the aimed e-commerce\nwebsite evaluation instrument. The experiment results and the future work have\nbeen discussed.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 11:34:54 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 03:50:01 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Li", "Xiaosong", ""], ["Liu", "Ye", ""], ["Fan", "Zizhou", ""], ["Li", "Will", ""]]}, {"id": "1801.05075", "submitter": "Sina Mohseni", "authors": "Sina Mohseni and Jeremy E. Block and Eric D. Ragan", "title": "A Human-Grounded Evaluation Benchmark for Local Explanations of Machine\n  Learning", "comments": "Benchmark Available online at\n  https://github.com/SinaMohseni/ML-Interpretability-Evaluation-Benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in interpretable machine learning proposes different computational\nand human subject approaches to evaluate model saliency explanations. These\napproaches measure different qualities of explanations to achieve diverse goals\nin designing interpretable machine learning systems. In this paper, we propose\na human attention benchmark for image and text domains using multi-layer human\nattention masks aggregated from multiple human annotators. We then present an\nevaluation study to evaluate model saliency explanations obtained using\nGrad-cam and LIME techniques. We demonstrate our benchmark's utility for\nquantitative evaluation of model explanations by comparing it with human\nsubjective ratings and ground-truth single-layer segmentation masks\nevaluations. Our study results show that our threshold agnostic evaluation\nmethod with the human attention baseline is more effective than single-layer\nobject segmentation masks to ground truth. Our experiments also reveal user\nbiases in the subjective rating of model saliency explanations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 00:14:43 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 21:05:07 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Mohseni", "Sina", ""], ["Block", "Jeremy E.", ""], ["Ragan", "Eric D.", ""]]}, {"id": "1801.05076", "submitter": "Sina Mohseni", "authors": "Sina Mohseni, Andrew Pachuilo, Ehsanul Haque Nirjhar, Rhema Linder,\n  Alyssa Pena, Eric D. Ragan", "title": "Analytic Provenance Datasets: A Data Repository of Human Analysis\n  Activity and Interaction Logs", "comments": "Datasets are available online at\n  https://research.arch.tamu.edu/analytic-provenance/datasets/ for research\n  purposes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analytic provenance data repository that can be used to study\nhuman analysis activity, thought processes, and software interaction with\nvisual analysis tools during exploratory data analysis. We conducted a series\nof user studies involving exploratory data analysis scenario with textual and\ncyber security data. Interactions logs, think-alouds, videos and all coded data\nin this study are available online for research purposes. Analysis sessions are\nsegmented in multiple sub-task steps based on user think-alouds, video and\naudios captured during the studies. These analytic provenance datasets can be\nused for research involving tools and techniques for analyzing interaction logs\nand analysis history. By providing high-quality coded data along with\ninteraction logs, it is possible to compare algorithmic data processing\ntechniques to the ground-truth records of analysis history.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 00:19:47 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Mohseni", "Sina", ""], ["Pachuilo", "Andrew", ""], ["Nirjhar", "Ehsanul Haque", ""], ["Linder", "Rhema", ""], ["Pena", "Alyssa", ""], ["Ragan", "Eric D.", ""]]}, {"id": "1801.05085", "submitter": "Nicholas Katzakis", "authors": "Nicholas Katzakis and Frank Steinicke", "title": "Excuse me! Perception of Abrupt Direction Changes Using Body Cues and\n  Paths on Mixed Reality Avatars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We evaluate two methods of signalling abrupt direction changes of a robotic\nplatform using a Mixed Reality avatar. The \"Body\" method uses gaze, gesture and\ntorso direction to point to upcoming waypoints. The \"Path\" method visualises\nthe change in direction using an angled path on the ground. We compare these\ntwo methods using a controlled user study and show that each method has its\nstrengths depending on the situation. Overall the \"Path\" method was slightly\nmore accurate in communicating the direction change of the robot but\nparticipants overall preferred the \"Body\" method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 01:10:54 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Katzakis", "Nicholas", ""], ["Steinicke", "Frank", ""]]}, {"id": "1801.05096", "submitter": "Komei Sugiura", "authors": "Komei Sugiura, Hisashi Kawai", "title": "Grounded Language Understanding for Manipulation Instructions Using\n  GAN-Based Classification", "comments": "6 pages, 3 figures, published at IEEE ASRU 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The target task of this study is grounded language understanding for domestic\nservice robots (DSRs). In particular, we focus on instruction understanding for\nshort sentences where verbs are missing. This task is of critical importance to\nbuild communicative DSRs because manipulation is essential for DSRs. Existing\ninstruction understanding methods usually estimate missing information only\nfrom non-grounded knowledge; therefore, whether the predicted action is\nphysically executable or not was unclear.\n  In this paper, we present a grounded instruction understanding method to\nestimate appropriate objects given an instruction and situation. We extend the\nGenerative Adversarial Nets (GAN) and build a GAN-based classifier using latent\nrepresentations. To quantitatively evaluate the proposed method, we have\ndeveloped a data set based on the standard data set used for Visual QA.\nExperimental results have shown that the proposed method gives the better\nresult than baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 02:05:50 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Sugiura", "Komei", ""], ["Kawai", "Hisashi", ""]]}, {"id": "1801.05100", "submitter": "Nicholas Katzakis", "authors": "Nicholas Katzakis, Kiyoshi Kiyokawa, Masahiro Hori, Haruo Takemura", "title": "Plane-Casting: 3D Cursor Control with a SmartPhone", "comments": "Proceedings of \"Touching the Third Dimension\" (3DCHI), Workshop at\n  ACM SIGCHI Conference on Human Factors in Computing Systems", "journal-ref": "Proceedings of \"The 3rd dimension of CHI (3DCHI): Touching and\n  designing 3D user interfaces\" workshop at ACM SIGCHI Conference on Human\n  Factors in Computing Systems 2012. Austin, TX. pp 13-21", "doi": "10.1145/2212776.2212698", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Plane-Casting, a novel technique for 3D object manipulation from a\ndistance that is especially suitable for smartphones. We describe two\nvariations of Plane-Casting, Pivot and Free Plane-Casting, and present results\nfrom a pilot study. Results suggest that Pivot Plane-Casting is more suitable\nfor quick, coarse movements whereas Free Plane-Casting is more suited to\nslower, precise motion. In a 3D movement task, Pivot Plane-Casting performed\nbetter quantitatively, but subjects preferred Free Plane-Casting overall.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 02:38:00 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Katzakis", "Nicholas", ""], ["Kiyokawa", "Kiyoshi", ""], ["Hori", "Masahiro", ""], ["Takemura", "Haruo", ""]]}, {"id": "1801.05469", "submitter": "Sina Mohseni", "authors": "Sina Mohseni, Alyssa Pena, Eric D. Ragan", "title": "ProvThreads: Analytic Provenance Visualization and Segmentation", "comments": "Presented at IEEE VIS 2017 Poster Session", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work aims to generate visualizations to enable meta-analysis of analytic\nprovenance and aid better understanding of analysts' strategies during\nexploratory text analysis. We introduce ProvThreads, a visual analytics\napproach that incorporates interactive topic modeling outcomes to illustrate\nrelationships between user interactions and the data topics under\ninvestigation. ProvThreads uses a series of continuous analysis paths called\ntopic threads to demonstrate both topic coverage and the progression of an\ninvestigation over time. As an analyst interacts with different pieces of data\nduring the analysis, interactions are logged and used to track user interests\nin topics over time. A line chart shows different amounts of interest in\nmultiple topics over the duration of the analysis. We discuss how different\nconfigurations of ProvThreads can be used to reveal changes in focus throughout\nan analysis.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 20:06:03 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Mohseni", "Sina", ""], ["Pena", "Alyssa", ""], ["Ragan", "Eric D.", ""]]}, {"id": "1801.05800", "submitter": "R\\'emi Cura", "authors": "Remi Cura, Julien Perret, Nicolas Paparoditis", "title": "Interactive in-base street model edit: how common GIS software and a\n  database can serve as a custom Graphical User Interface", "comments": "this article is an extract from PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our modern world produces an increasing quantity of data, and especially\ngeospatial data, with advance of sensing technologies, and growing complexity\nand organisation of vector data. Tools are needed to efficiently create and\nedit those vector geospatial data. Procedural generation has been a tool of\nchoice to generate strongly organised data, yet it may be hard to control.\nBecause those data may be involved to take consequence-full real life\ndecisions, user interactions are required to check data and edit it. The\nclassical process to do so would be to build an adhoc Graphical User Interface\n(GUI) tool adapted for the model and method being used. This task is difficult,\ntakes a large amount of resources, and is very specific to one model, making it\nhard to share and re-use.\n  Besides, many common generic GUI already exists to edit vector data, each\nhaving its specialities. We propose a change of paradigm; instead of building a\nspecific tool for one task, we use common GIS software as GUIs, and deport the\nspecific interactions from the software to within the database. In this\nparadigm, GIS software simply modify geometry and attributes of database\nlayers, and those changes are used by the database to perform automated task.\n  This new paradigm has many advantages. The first one is genericity. With\nin-base interaction, any GIS software can be used to perform edition, whatever\nthe software is a Desktop sofware or a web application. The second is\nconcurrency and coherency. Because interaction is in-base, use of database\nfeatures allows seamless multi-user work, and can guarantee that the data is in\na coherent state. Last we propose tools to facilitate multi-user edits, both\nduring the edit phase (each user knows what areas are edited by other users),\nand before and after edit (planning of edit, analyse of edited areas).\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 18:55:43 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Cura", "Remi", ""], ["Perret", "Julien", ""], ["Paparoditis", "Nicolas", ""]]}, {"id": "1801.05831", "submitter": "Peter Krafft", "authors": "Peter M Krafft, Nicol\\'as Della Penna, Alex Pentland", "title": "An Experimental Study of Cryptocurrency Market Dynamics", "comments": "CHI 2018", "journal-ref": "Peter Krafft, Nicol\\'as Della Penna, Alex Pentland. (2018). An\n  Experimental Study of Cryptocurrency Market Dynamics. ACM CHI Conference on\n  Human Factors in Computing Systems (CHI)", "doi": "10.1145/3173574.3174179", "report-no": null, "categories": "cs.CY cs.HC cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As cryptocurrencies gain popularity and credibility, marketplaces for\ncryptocurrencies are growing in importance. Understanding the dynamics of these\nmarkets can help to assess how viable the cryptocurrnency ecosystem is and how\ndesign choices affect market behavior. One existential threat to\ncryptocurrencies is dramatic fluctuations in traders' willingness to buy or\nsell. Using a novel experimental methodology, we conducted an online experiment\nto study how susceptible traders in these markets are to peer influence from\ntrading behavior. We created bots that executed over one hundred thousand\ntrades costing less than a penny each in 217 cryptocurrencies over the course\nof six months. We find that individual \"buy\" actions led to short-term\nincreases in subsequent buy-side activity hundreds of times the size of our\ninterventions. From a design perspective, we note that the design choices of\nthe exchange we study may have promoted this and other peer influence effects,\nwhich highlights the potential social and economic impact of HCI in the design\nof digital institutions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 19:17:20 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 16:08:31 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Krafft", "Peter M", ""], ["Della Penna", "Nicol\u00e1s", ""], ["Pentland", "Alex", ""]]}, {"id": "1801.05972", "submitter": "Christoph Gebhardt", "authors": "Christoph Gebhardt, Otmar Hilliges", "title": "WYFIWYG: Investigating Effective User Support in Aerial Videography", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tools for quadrotor trajectory design have enabled single videographers to\ncreate complex aerial video shots that previously required dedicated hardware\nand several operators. We build on this prior work by studying film-maker's\nworking practices which informed a system design that brings expert workflows\ncloser to end-users. For this purpose, we propose WYFIWYG, a new quadrotor\ncamera tool which (i) allows to design a video solely via specifying its\nframes, (ii) encourages the exploration of the scene prior to filming and (iii)\nallows to continuously frame a camera target according to compositional\nintentions. Furthermore, we propose extensions to an existing algorithm,\ngenerating more intuitive angular camera motions and producing spatially and\ntemporally smooth trajectories. Finally, we conduct a user study where we\nevaluate how end-users work with current videography tools. We conclude by\nsummarizing the findings of work as implications for the design of UIs and\nalgorithms of quadrotor camera tools.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 08:53:55 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 08:32:50 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Gebhardt", "Christoph", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1801.06011", "submitter": "Julian Steil", "authors": "Julian Steil, Philipp M\\\"uller, Yusuke Sugano, Andreas Bulling", "title": "Forecasting User Attention During Everyday Mobile Interactions Using\n  Device-Integrated and Wearable Sensors", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": "10.1145/3229434.3229439", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual attention is highly fragmented during mobile interactions, but the\nerratic nature of attention shifts currently limits attentive user interfaces\nto adapting after the fact, i.e. after shifts have already happened. We instead\nstudy attention forecasting -- the challenging task of predicting users' gaze\nbehaviour (overt visual attention) in the near future. We present a novel\nlong-term dataset of everyday mobile phone interactions, continuously recorded\nfrom 20 participants engaged in common activities on a university campus over\n4.5 hours each (more than 90 hours in total). We propose a proof-of-concept\nmethod that uses device-integrated sensors and body-worn cameras to encode rich\ninformation on device usage and users' visual scene. We demonstrate that our\nmethod can forecast bidirectional attention shifts and predict whether the\nprimary attentional focus is on the handheld mobile device. We study the impact\nof different feature sets on performance and discuss the significant potential\nbut also remaining challenges of forecasting user attention during mobile\ninteractions.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 13:47:11 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 17:03:25 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 07:24:28 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Steil", "Julian", ""], ["M\u00fcller", "Philipp", ""], ["Sugano", "Yusuke", ""], ["Bulling", "Andreas", ""]]}, {"id": "1801.06048", "submitter": "Yuri G. Gordienko", "authors": "Yuri Gordienko, Sergii Stirenko, Yuriy Kochura, Oleg Alienin, Michail\n  Novotarskiy, Nikita Gordienko", "title": "Deep Learning for Fatigue Estimation on the Basis of Multimodal\n  Human-Machine Interactions", "comments": "12 pages, 10 figures, 1 table; presented at XXIX IUPAP Conference in\n  Computational Physics (CCP2017) July 9-13, 2017, Paris, University Pierre et\n  Marie Curie - Sorbonne (https://ccp2017.sciencesconf.org/program)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new method is proposed to monitor the level of current physical load and\naccumulated fatigue by several objective and subjective characteristics. It was\napplied to the dataset targeted to estimate the physical load and fatigue by\nseveral statistical and machine learning methods. The data from peripheral\nsensors (accelerometer, GPS, gyroscope, magnetometer) and brain-computing\ninterface (electroencephalography) were collected, integrated, and analyzed by\nseveral statistical and machine learning methods (moment analysis, cluster\nanalysis, principal component analysis, etc.). The hypothesis 1 was presented\nand proved that physical activity can be classified not only by objective\nparameters, but by subjective parameters also. The hypothesis 2 (experienced\nphysical load and subsequent restoration as fatigue level can be estimated\nquantitatively and distinctive patterns can be recognized) was presented and\nsome ways to prove it were demonstrated. Several \"physical load\" and \"fatigue\"\nmetrics were proposed. The results presented allow to extend application of the\nmachine learning methods for characterization of complex human activity\npatterns (for example, to estimate their actual physical load and fatigue, and\ngive cautions and advice).\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 17:49:03 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Gordienko", "Yuri", ""], ["Stirenko", "Sergii", ""], ["Kochura", "Yuriy", ""], ["Alienin", "Oleg", ""], ["Novotarskiy", "Michail", ""], ["Gordienko", "Nikita", ""]]}, {"id": "1801.06326", "submitter": "Stephan Sigg", "authors": "Stephan Sigg", "title": "Some aspects of physical prototyping in Pervasive Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document summarises the results of several research campaigns over the\npast seven years. The main connecting theme is the physical layer of widely\ndeployed sensors in Pervasive Computing domains. In particular, we have focused\non the RF-channel or on ambient audio.\n  The initial problem from which we started this work was that of distributed\nadaptive transmit beamforming. We have been looking for a simple method to\nalign the phases of jointly transmitting nodes (e.g. sensor or IoT nodes). The\nalgorithmic solution to this problem was to implement a distributed random\noptimisation method on the participating nodes in which the transmitters and\nthe receiver follow an iterative question-and-answer scheme. We have been able\nto derive sharp asymptotic bounds on the expected optimisation time of an\nevolutionary random optimiser and presented an asymptotically optimal approach.\n  One thing that we have learned from the work on these physical layer\nalgorithms was that the signals we work on are fragile and perceptive to\nphysical environmental changes. These could be obstacles such as furniture,\nopened or closed windows or doors as well as movement of individuals. This\nobservation motivated us to view the wireless interface as a sensor for\nenvironmental changes in Pervasive Computing environments.\n  Another use of physical layer RF-signals is for security applications.\n  We are currently working to further push these mentioned directions and novel\nfields of physical prototyping. In particular, the calculation of mathematical\noperations on the wireless channel at the time of transmission appears to\ncontain good potential for gains in efficiency for communication and\ncomputation in Pervasive Computing domains.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 07:43:51 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Sigg", "Stephan", ""]]}, {"id": "1801.06349", "submitter": "Matei Mancas", "authors": "Matei Mancas, Christian Frisson, Jo\\\"elle Tilmanne, Nicolas\n  d'Alessandro, Petr Barborka, Furkan Bayansar, Francisco Bernard, Rebecca\n  Fiebrink, Alexis Heloir, Edgar Hemery, Sohaib Laraba, Alexis Moinet, Fabrizio\n  Nunnari, Thierry Ravet, Lo\\\"ic Reboursi\\`ere, Alvaro Sarasua, Micka\\\"el Tits,\n  No\\'e Tits, Fran\\c{c}ois Zaj\\'ega, Paolo Alborno, Ksenia Kolykhalova, Emma\n  Frid, Damiano Malafronte, Lisanne Huis in't Veld, H\\\"useyin Cakmak, Kevin El\n  Haddad, Nicolas Riche, Julien Leroy, Pierre Marighetto, Bekir Berker\n  T\\\"urker, Hossein Khaki, Roberto Pulisci, Emer Gilmartin, Fasih Haider,\n  K\\\"ubra Cengiz, Martin Sulir, Ilaria Torre, Shabbir Marzban, Ramazan\n  Yaz{\\i}c{\\i}, Furkan Burak B\\^agc{\\i}, Vedat Gazi K{\\i}l{\\i}, Hilal Sezer,\n  Sena B\\\"usra Yenge, Charles-Alexandre Delestage, Sylvie Leleu-Merviel, Muriel\n  Meyer-Chemenska, Daniel Schmitt, Willy Yvart, St\\'ephane Dupont, Ozan Can\n  Altiok, Ayseg\\\"ul Bumin, Ceren Dikmen, Ivan Giangreco, Silvan Heller, Emre\n  K\\\"ulah, Gueorgui Pironkov, Luca Rossetto, Yusuf Sahillioglu, Heiko Schuldt,\n  Omar Seddati, Yusuf Setinkaya, Metin Sezgin, Claudiu Tanase, Emre Toyan, Sean\n  Wood, Doguhan Yeke, Fran\\c{c}cois Rocca, Pierre-Henri De Deken, Alessandra\n  Bandrabur, Fabien Grisard, Axel Jean-Caurant, Vincent Courboulay, Radhwan Ben\n  Madhkour, Ambroise Moreau", "title": "Proceedings of eNTERFACE 2015 Workshop on Intelligent Interfaces", "comments": "159 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 11th Summer Workshop on Multimodal Interfaces eNTERFACE 2015 was hosted\nby the Numediart Institute of Creative Technologies of the University of Mons\nfrom August 10th to September 2015. During the four weeks, students and\nresearchers from all over the world came together in the Numediart Institute of\nthe University of Mons to work on eight selected projects structured around\nintelligent interfaces. Eight projects were selected and their reports are\nshown here.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 10:03:35 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Mancas", "Matei", ""], ["Frisson", "Christian", ""], ["Tilmanne", "Jo\u00eblle", ""], ["d'Alessandro", "Nicolas", ""], ["Barborka", "Petr", ""], ["Bayansar", "Furkan", ""], ["Bernard", "Francisco", ""], ["Fiebrink", "Rebecca", ""], ["Heloir", "Alexis", ""], ["Hemery", "Edgar", ""], ["Laraba", "Sohaib", ""], ["Moinet", "Alexis", ""], ["Nunnari", "Fabrizio", ""], ["Ravet", "Thierry", ""], ["Reboursi\u00e8re", "Lo\u00efc", ""], ["Sarasua", "Alvaro", ""], ["Tits", "Micka\u00ebl", ""], ["Tits", "No\u00e9", ""], ["Zaj\u00e9ga", "Fran\u00e7ois", ""], ["Alborno", "Paolo", ""], ["Kolykhalova", "Ksenia", ""], ["Frid", "Emma", ""], ["Malafronte", "Damiano", ""], ["Veld", "Lisanne Huis in't", ""], ["Cakmak", "H\u00fcseyin", ""], ["Haddad", "Kevin El", ""], ["Riche", "Nicolas", ""], ["Leroy", "Julien", ""], ["Marighetto", "Pierre", ""], ["T\u00fcrker", "Bekir Berker", ""], ["Khaki", "Hossein", ""], ["Pulisci", "Roberto", ""], ["Gilmartin", "Emer", ""], ["Haider", "Fasih", ""], ["Cengiz", "K\u00fcbra", ""], ["Sulir", "Martin", ""], ["Torre", "Ilaria", ""], ["Marzban", "Shabbir", ""], ["Yaz\u0131c\u0131", "Ramazan", ""], ["B\u00e2gc\u0131", "Furkan Burak", ""], ["K\u0131l\u0131", "Vedat Gazi", ""], ["Sezer", "Hilal", ""], ["Yenge", "Sena B\u00fcsra", ""], ["Delestage", "Charles-Alexandre", ""], ["Leleu-Merviel", "Sylvie", ""], ["Meyer-Chemenska", "Muriel", ""], ["Schmitt", "Daniel", ""], ["Yvart", "Willy", ""], ["Dupont", "St\u00e9phane", ""], ["Altiok", "Ozan Can", ""], ["Bumin", "Ayseg\u00fcl", ""], ["Dikmen", "Ceren", ""], ["Giangreco", "Ivan", ""], ["Heller", "Silvan", ""], ["K\u00fclah", "Emre", ""], ["Pironkov", "Gueorgui", ""], ["Rossetto", "Luca", ""], ["Sahillioglu", "Yusuf", ""], ["Schuldt", "Heiko", ""], ["Seddati", "Omar", ""], ["Setinkaya", "Yusuf", ""], ["Sezgin", "Metin", ""], ["Tanase", "Claudiu", ""], ["Toyan", "Emre", ""], ["Wood", "Sean", ""], ["Yeke", "Doguhan", ""], ["Rocca", "Fran\u00e7cois", ""], ["De Deken", "Pierre-Henri", ""], ["Bandrabur", "Alessandra", ""], ["Grisard", "Fabien", ""], ["Jean-Caurant", "Axel", ""], ["Courboulay", "Vincent", ""], ["Madhkour", "Radhwan Ben", ""], ["Moreau", "Ambroise", ""]]}, {"id": "1801.06352", "submitter": "Jakob Eg Larsen", "authors": "Jakob Eg Larsen, Thomas Blomseth Christiansen, Kasper Eskelund", "title": "Fostering Bilateral Patient-Clinician Engagement in Active Self-Tracking\n  of Subjective Experience", "comments": "In Proc. of the Pervasive Health 2017 conference workshop on\n  Leveraging Patient-Generated Data for Collaborative Decision Making in\n  Healthcare", "journal-ref": null, "doi": "10.1145/3154862.3154918", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper we describe select aspects of our experience with\nhealth-related self-tracking, the data generated, and processes surrounding\nthose. In particular we focus on how bilateral patient-clinician engagement may\nbe fostered by the combination of technology and method. We exemplify with a\ncase study where a PTSD-suffering veteran has been self-tracking a specific\nsymptom precursor. The availability of high-resolution self-tracking data on\nthe occurrences of even a single symptom created new opportunities in the\ntherapeutic process for identifying underlying triggers of symptoms. The\npatient was highly engaged in self-tracking and sharing the collected data. We\nsuggest a key reason was the collaborative effort in defining the data\ncollection protocol and discussion of the data. The therapist also engaged\nhighly in the self-tracking data, as it supported the existing therapeutic\nprocess in reaching insights otherwise unobtainable.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 10:12:15 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Larsen", "Jakob Eg", ""], ["Christiansen", "Thomas Blomseth", ""], ["Eskelund", "Kasper", ""]]}, {"id": "1801.06889", "submitter": "Fred Hohman", "authors": "Fred Hohman, Minsuk Kahng, Robert Pienta, Duen Horng Chau", "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next\n  Frontiers", "comments": "Under review for IEEE Transactions on Visualization and Computer\n  Graphics (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 20:13:07 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 01:09:33 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 04:59:24 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Hohman", "Fred", ""], ["Kahng", "Minsuk", ""], ["Pienta", "Robert", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1801.07168", "submitter": "Lachlan Urquhart Ph.D", "authors": "Lachlan Urquhart, Tom Lodge, Andy Crabtree", "title": "Demonstrably Doing Accountability in the Internet of Things", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the importance of accountability to data protection, and\nhow it can be built into the Internet of Things (IoT). The need to build\naccountability into the IoT is motivated by the opaque nature of distributed\ndata flows, inadequate consent mechanisms, and lack of interfaces enabling\nend-user control over the behaviours of internet-enabled devices. The lack of\naccountability precludes meaningful engagement by end-users with their personal\ndata and poses a key challenge to creating user trust in the IoT and the\nreciprocal development of the digital economy. The EU General Data Protection\nRegulation 2016 (GDPR) seeks to remedy this particular problem by mandating\nthat a rapidly developing technological ecosystem be made accountable. In doing\nso it foregrounds new responsibilities for data controllers, including data\nprotection by design and default, and new data subject rights such as the right\nto data portability. While GDPR is technologically neutral, it is nevertheless\nanticipated that realising the vision will turn upon effective technological\ndevelopment. Accordingly, this paper examines the notion of accountability, how\nit has been translated into systems design recommendations for the IoT, and how\nthe IoT Databox puts key data protection principles into practice.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 16:12:19 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Urquhart", "Lachlan", ""], ["Lodge", "Tom", ""], ["Crabtree", "Andy", ""]]}, {"id": "1801.07185", "submitter": "Lachlan Urquhart Ph.D", "authors": "Lachlan Urquhart", "title": "White Noise from the White Goods? Conceptual and Empirical Perspectives\n  on Ambient Domestic Computing", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within this chapter we consider the emergence of ambient domestic computing\nsystems, both conceptually and empirically. We critically assess visions of\npost-desktop computing, paying particular attention to one contemporary trend:\nthe internet of things (IoT). We examine the contested nature of this term,\nlooking at the historical trajectory of similar technologies, and the\nregulatory issues they can pose, particularly in the home. We also look to the\nemerging regulatory solution of privacy by design, unpacking practical\nchallenges it faces. The novelty of our contribution stems from a turn to\npractice through a set of empirical perspectives. We present findings that\ndocument the practical experiences and viewpoints of leading experts in\ntechnology law and design.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 16:30:51 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Urquhart", "Lachlan", ""]]}, {"id": "1801.07189", "submitter": "Lachlan Urquhart Ph.D", "authors": "Lachlan Urquhart, Neelima Sailaja, Derek McAuley", "title": "Realising the Right to Data Portability for the Domestic Internet of\n  Things", "comments": null, "journal-ref": "Personal and Ubiquitous Computing, Springer, 2017", "doi": "10.1007/s00779-017-1069-2", "report-no": null, "categories": "cs.HC cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing role for the IT design community to play in regulation\nof emerging IT. Article 25 of the EU General Data Protection Regulation (GDPR)\n2016 puts this on a strict legal basis by establishing the need for information\nprivacy by design and default (PbD) for personal data-driven technologies.\nAgainst this backdrop, we examine legal, commercial and technical perspectives\naround the newly created legal right to data portability (RTDP) in GDPR. We are\nmotivated by a pressing need to address regulatory challenges stemming from the\nInternet of Things (IoT). We need to find channels to support the protection of\nthese new legal rights for users in practice. In Part I we introduce the\ninternet of things and information PbD in more detail. We briefly consider\nregulatory challenges posed by the IoT and the nature and practical challenges\nsurrounding the regulatory response of information privacy by design. In Part\nII, we look in depth at the legal nature of the RTDP, determining what it\nrequires from IT designers in practice but also limitations on the right and\nhow it relates to IoT. In Part III we focus on technical approaches that can\nsupport the realisation of the right. We consider the state of the art in data\nmanagement architectures, tools and platforms that can provide portability,\nincreased transparency and user control over the data flows. In Part IV, we\nbring our perspectives together to reflect on the technical, legal and business\nbarriers and opportunities that will shape the implementation of the RTDP in\npractice, and how the relationships may shape emerging IoT innovation and\nbusiness models. We finish with brief conclusions about the future for the RTDP\nand PbD in the IoT.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 16:49:18 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Urquhart", "Lachlan", ""], ["Sailaja", "Neelima", ""], ["McAuley", "Derek", ""]]}, {"id": "1801.07207", "submitter": "Lachlan Urquhart Ph.D", "authors": "Lachlan Urquhart, Derek McAuley", "title": "Avoiding the Internet of Insecure Industrial Things", "comments": null, "journal-ref": "Computer Law and Security Review, 2018", "doi": "10.1016/j.clsr.2017.12.004", "report-no": null, "categories": "cs.HC cs.CY cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security incidents such as targeted distributed denial of service (DDoS)\nattacks on power grids and hacking of factory industrial control systems (ICS)\nare on the increase. This paper unpacks where emerging security risks lie for\nthe industrial internet of things, drawing on both technical and regulatory\nperspectives. Legal changes are being ushered by the European Union (EU)\nNetwork and Information Security (NIS) Directive 2016 and the General Data\nProtection Regulation 2016 (GDPR) (both to be enforced from May 2018). We use\nthe case study of the emergent smart energy supply chain to frame, scope out\nand consolidate the breadth of security concerns at play, and the regulatory\nresponses. We argue the industrial IoT brings four security concerns to the\nfore, namely: appreciating the shift from offline to online infrastructure;\nmanaging temporal dimensions of security; addressing the implementation gap for\nbest practice; and engaging with infrastructural complexity. Our goal is to\nsurface risks and foster dialogue to avoid the emergence of an Internet of\nInsecure Industrial Things\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 17:19:18 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Urquhart", "Lachlan", ""], ["McAuley", "Derek", ""]]}, {"id": "1801.07518", "submitter": "Adam Aviv", "authors": "Adam J. Aviv and Ravi Kuber", "title": "Towards Understanding Connections between Security/Privacy Attitudes and\n  Unlock Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this study, we examine the ways in which user attitudes towards privacy\nand security relating to mobile devices and the data stored thereon may impact\nthe strength of unlock authentication, focusing on Android's graphical unlock\npatterns. We conducted an online study with Amazon Mechanical Turk ($N=750$)\nusing self-reported unlock authentication choices, as well as Likert scale\nagreement/disagreement responses to a set of seven privacy/security prompts. We\nthen analyzed the responses in multiple dimensions, including a straight\naverage of the Likert responses as well as using Principle Component Analysis\nto expose latent factors. We found that responses to two of the seven questions\nproved relevant and significant. These two questions considered attitudes\ntowards general concern for data stored on mobile devices, and attitudes\ntowards concerns for unauthorized access by known actors. Unfortunately, larger\nconclusions cannot be drawn on the efficacy of the broader set of questions for\nexposing connections between unlock authentication strength (Pearson Rank\n$r=-0.08$, $p<0.1$). However, both of our factor solutions exposed differences\nin responses for demographics groups, including age, gender, and residence\ntype. The findings of this study suggests that there is likely a link between\nperceptions of privacy/security on mobile devices and the perceived threats\ntherein, but more research is needed, particularly on developing better survey\nand measurement techniques of privacy/security attitudes that relate to mobile\ndevices specifically.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 12:54:26 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 13:34:17 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Aviv", "Adam J.", ""], ["Kuber", "Ravi", ""]]}, {"id": "1801.07618", "submitter": "Ilia Rushkin", "authors": "Ilia Rushkin, Isaac Chuang, Dustin Tingley", "title": "Modelling and Using Response Times in Online Courses", "comments": null, "journal-ref": "Journal of Learning Analytics, 2019, Volume 6(3), 76-89", "doi": "10.18608/jla.2019.63.10", "report-no": null, "categories": "cs.HC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each time a learner in a self-paced online course seeks to answer an\nassessment question, it takes some time for the student to read the question\nand arrive at an answer to submit. If multiple attempts are allowed, and the\nfirst answer is incorrect, it takes some time to provide a second answer. Here\nwe study the distribution of such \"response times.\" We find that the log-normal\nstatistical model for such times, previously suggested in the literature, holds\nfor online courses. Users who, according to this model, tend to take longer on\nsubmits are more likely to complete the course, have a higher level of\nengagement, and achieve a higher grade. This finding can be the basis for\ndesigning interventions in online courses, such as MOOCs, which would encourage\n\"fast\" users to slow down.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 15:34:32 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 21:05:36 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 15:37:21 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Rushkin", "Ilia", ""], ["Chuang", "Isaac", ""], ["Tingley", "Dustin", ""]]}, {"id": "1801.07633", "submitter": "Iyiola E. Olatunji", "authors": "Iyiola E. Olatunji", "title": "Human Activity Recognition for Mobile Robot", "comments": null, "journal-ref": null, "doi": "10.1088/1742-6596/1069/1/012148", "report-no": null, "categories": "cs.HC cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the increasing number of mobile robots including domestic robots for\ncleaning and maintenance in developed countries, human activity recognition is\ninevitable for congruent human-robot interaction. Needless to say that this is\nindeed a challenging task for robots, it is expedient to learn human activities\nfor autonomous mobile robots (AMR) for navigating in an uncontrolled\nenvironment without any guidance. Building a correct classifier for complex\nhuman action is non-trivial since simple actions can be combined to recognize a\ncomplex human activity. In this paper, we trained a model for human activity\nrecognition using convolutional neural network. We trained and validated the\nmodel using the Vicon physical action dataset and also tested the model on our\ngenerated dataset (VMCUHK). Our experiment shows that our method performs with\nhigh accuracy, human activity recognition task both on the Vicon physical\naction dataset and VMCUHK dataset.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:14:43 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Olatunji", "Iyiola E.", ""]]}, {"id": "1801.07964", "submitter": "Nadia Boukhelifa", "authors": "Nadia Boukhelifa, Anastasia Bezerianos and Evelyne Lutton", "title": "Evaluation of Interactive Machine Learning Systems", "comments": "20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of interactive machine learning systems remains a difficult\ntask. These systems learn from and adapt to the human, but at the same time,\nthe human receives feedback and adapts to the system. Getting a clear\nunderstanding of these subtle mechanisms of co-operation and co-adaptation is\nchallenging. In this chapter, we report on our experience in designing and\nevaluating various interactive machine learning applications from different\ndomains. We argue for coupling two types of validation: algorithm-centered\nanalysis, to study the computational behaviour of the system; and\nhuman-centered evaluation, to observe the utility and effectiveness of the\napplication for end-users. We use a visual analytics application for guided\nsearch, built using an interactive evolutionary approach, as an exemplar of our\nwork. Our observation is that human-centered design and evaluation complement\nalgorithmic analysis, and can play an important role in addressing the\n\"black-box\" effect of machine learning. Finally, we discuss research\nopportunities that require human-computer interaction methodologies, in order\nto support both the visible and hidden roles that humans play in interactive\nmachine learning.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 12:47:26 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Boukhelifa", "Nadia", ""], ["Bezerianos", "Anastasia", ""], ["Lutton", "Evelyne", ""]]}, {"id": "1801.08024", "submitter": "Grigori Fursin", "authors": "Grigori Fursin, Anton Lokhmotov, Dmitry Savenko and Eben Upton", "title": "A Collective Knowledge workflow for collaborative research into\n  multi-objective autotuning and machine learning techniques", "comments": "Interactive CK report: http://cKnowledge.org/rpi-crowd-tuning ; CK\n  repository with artifacts:\n  https://github.com/ctuning/ck-rpi-optimization-results ; FigShare data\n  archive: https://doi.org/10.6084/m9.figshare.5789007.v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient software and hardware has never been harder whether it\nis for a tiny IoT device or an Exascale supercomputer. Apart from the ever\ngrowing design and optimization complexity, there exist even more fundamental\nproblems such as lack of interdisciplinary knowledge required for effective\nsoftware/hardware co-design, and a growing technology transfer gap between\nacademia and industry.\n  We introduce our new educational initiative to tackle these problems by\ndeveloping Collective Knowledge (CK), a unified experimental framework for\ncomputer systems research and development. We use CK to teach the community how\nto make their research artifacts and experimental workflows portable,\nreproducible, customizable and reusable while enabling sustainable R&D and\nfacilitating technology transfer. We also demonstrate how to redesign\nmulti-objective autotuning and machine learning as a portable and extensible CK\nworkflow. Such workflows enable researchers to experiment with different\napplications, data sets and tools; crowdsource experimentation across diverse\nplatforms; share experimental results, models, visualizations; gradually expose\nmore design and optimization choices using a simple JSON API; and ultimately\nbuild upon each other's findings.\n  As the first practical step, we have implemented customizable compiler\nautotuning, crowdsourced optimization of diverse workloads across Raspberry Pi\n3 devices, reduced the execution time and code size by up to 40%, and applied\nmachine learning to predict optimizations. We hope such approach will help\nteach students how to build upon each others' work to enable efficient and\nself-optimizing software/hardware/model stack for emerging workloads.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 15:30:39 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Fursin", "Grigori", ""], ["Lokhmotov", "Anton", ""], ["Savenko", "Dmitry", ""], ["Upton", "Eben", ""]]}, {"id": "1801.08336", "submitter": "Nikolaos Bikakis", "authors": "Nikos Bikakis", "title": "Big Data Visualization Tools", "comments": "This article appears in Encyclopedia of Big Data Technologies,\n  Springer, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization is the presentation of data in a pictorial or graphical\nformat, and a data visualization tool is the software that generates this\npresentation. Data visualization provides users with intuitive means to\ninteractively explore and analyze data, enabling them to effectively identify\ninteresting patterns, infer correlations and causalities, and supports\nsense-making activities.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 10:16:48 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 22:03:28 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Bikakis", "Nikos", ""]]}, {"id": "1801.08379", "submitter": "Emre Aksan", "authors": "Emre Aksan, Fabrizio Pece, Otmar Hilliges", "title": "DeepWriting: Making Digital Ink Editable via Deep Generative Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital ink promises to combine the flexibility and aesthetics of handwriting\nand the ability to process, search and edit digital text. Character recognition\nconverts handwritten text into a digital representation, albeit at the cost of\nlosing personalized appearance due to the technical difficulties of separating\nthe interwoven components of content and style. In this paper, we propose a\nnovel generative neural network architecture that is capable of disentangling\nstyle from content and thus making digital ink editable. Our model can\nsynthesize arbitrary text, while giving users control over the visual\nappearance (style). For example, allowing for style transfer without changing\nthe content, editing of digital ink at the word level and other application\nscenarios such as spell-checking and correction of handwritten text. We\nfurthermore contribute a new dataset of handwritten text with fine-grained\nannotations at the character level and report results from an initial user\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 12:39:42 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Aksan", "Emre", ""], ["Pece", "Fabrizio", ""], ["Hilliges", "Otmar", ""]]}, {"id": "1801.08607", "submitter": "Glen Berseth", "authors": "Glen Berseth, Mahyar Khayatkhoei, Brandon Haworth, Muhammad Usman,\n  Mubbasir Kapadia, Petros Faloutsos", "title": "Interactive Diversity Optimization of Environments", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of a building requires an architect to balance a wide range of\nconstraints: aesthetic, geometric, usability, lighting, safety, etc. At the\nsame time, there are often a multiplicity of diverse designs that can meet\nthese constraints equally well. Architects must use their skills and artistic\nvision to explore these rich but highly constrained design spaces. A number of\ncomputer-aided design tools use automation to provide useful analytical data\nand optimal designs with respect to certain fitness criteria. However, this\nautomation can come at the expense of a designer's creative control.\n  We propose uDOME, a user-in-the-loop system for computer-aided design\nexploration that balances automation and control by efficiently exploring,\nanalyzing, and filtering the space of environment layouts to better inform an\narchitect's decision-making. At each design iteration, uDOME provides a set of\ndiverse designs which satisfy user-defined constraints and optimality criteria\nwithin a user defined parameterization of the design space. The user then\nselects a design and performs a similar optimization with the same or different\nparameters and objectives. This exploration process can be repeated as many\ntimes as the designer wishes. Our user studies indicates that \\DOME, with its\ndiversity-based approach, improves the efficiency and effectiveness of even\nnovice users with minimal training, without compromising the quality of their\ndesigns.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 00:35:06 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Berseth", "Glen", ""], ["Khayatkhoei", "Mahyar", ""], ["Haworth", "Brandon", ""], ["Usman", "Muhammad", ""], ["Kapadia", "Mubbasir", ""], ["Faloutsos", "Petros", ""]]}, {"id": "1801.08760", "submitter": "Elena Sibirtseva", "authors": "Elena Sibirtseva, Dimosthenis Kontogiorgos, Olov Nykvist, Hakan\n  Karaoguz, Iolanda Leite, Joakim Gustafson, Danica Kragic", "title": "A Comparison of Visualisation Methods for Disambiguating Verbal Requests\n  in Human-Robot Interaction", "comments": null, "journal-ref": null, "doi": "10.1109/ROMAN.2018.8525554", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Picking up objects requested by a human user is a common task in human-robot\ninteraction. When multiple objects match the user's verbal description, the\nrobot needs to clarify which object the user is referring to before executing\nthe action. Previous research has focused on perceiving user's multimodal\nbehaviour to complement verbal commands or minimising the number of follow up\nquestions to reduce task time. In this paper, we propose a system for reference\ndisambiguation based on visualisation and compare three methods to disambiguate\nnatural language instructions. In a controlled experiment with a YuMi robot, we\ninvestigated real-time augmentations of the workspace in three conditions --\nmixed reality, augmented reality, and a monitor as the baseline -- using\nobjective measures such as time and accuracy, and subjective measures like\nengagement, immersion, and display interference. Significant differences were\nfound in accuracy and engagement between the conditions, but no differences\nwere found in task time. Despite the higher error rates in the mixed reality\ncondition, participants found that modality more engaging than the other two,\nbut overall showed preference for the augmented reality condition over the\nmonitor and mixed reality conditions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 11:24:47 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Sibirtseva", "Elena", ""], ["Kontogiorgos", "Dimosthenis", ""], ["Nykvist", "Olov", ""], ["Karaoguz", "Hakan", ""], ["Leite", "Iolanda", ""], ["Gustafson", "Joakim", ""], ["Kragic", "Danica", ""]]}, {"id": "1801.08925", "submitter": "Mikhail Startsev", "authors": "Mikhail Startsev, Michael Dorr", "title": "Supersaliency: A Novel Pipeline for Predicting Smooth Pursuit-Based\n  Attention Improves Generalizability of Video Saliency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting attention is a popular topic at the intersection of human and\ncomputer vision. However, even though most of the available video saliency data\nsets and models claim to target human observers' fixations, they fail to\ndifferentiate them from smooth pursuit (SP), a major eye movement type that is\nunique to perception of dynamic scenes. In this work, we highlight the\nimportance of SP and its prediction (which we call supersaliency, due to\ngreater selectivity compared to fixations), and aim to make its distinction\nfrom fixations explicit for computational models. To this end, we (i) use\nalgorithmic and manual annotations of SP and fixations for two well-established\nvideo saliency data sets, (ii) train Slicing Convolutional Neural Networks for\nsaliency prediction on either fixation- or SP-salient locations, and (iii)\nevaluate our and 26 publicly available dynamic saliency models on three data\nsets against traditional saliency and supersaliency ground truth. Overall, our\nmodels outperform the state of the art in both the new supersaliency and the\ntraditional saliency problem settings, for which literature models are\noptimized. Importantly, on two independent data sets, our supersaliency model\nshows greater generalization ability and outperforms all other models, even for\nfixation prediction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 18:24:45 GMT"}, {"version": "v2", "created": "Mon, 29 Jan 2018 09:15:53 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 10:40:07 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Startsev", "Mikhail", ""], ["Dorr", "Michael", ""]]}, {"id": "1801.08997", "submitter": "Matthew Cooper", "authors": "Scott Carter, Pernilla Qvarfordt, Matthew Cooper, Aki Komori, Ville\n  Makela", "title": "Tools for online tutorials: comparing capture devices, tutorial\n  representations, and access devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tutorials are one of the most fundamental means of conveying knowledge.\nIdeally when the task involves physical or digital objects, tutorials not only\ndescribe each step with text or via audio narration but show it as well using\nphotos or animation. In most cases, online tutorial authors capture media from\nhandheld mobile devices to compose these documents, but increasingly they use\nwearable devices as well. In this work, we explore the full life-cycle of\nonline tutorial creation and viewing using head-mounted capture and displays.\nWe developed a media-capture tool for Google Glass that requires minimal\nattention to the capture device and instead allows the author to focus on\ncreating the tutorial's content rather than its capture. The capture tool is\ncoupled with web-based authoring tools for creating annotatable videos and\nmultimedia documents. In a study comparing standalone (camera on tripod) versus\nwearable capture (Google Glass) as well as two types of multimedia\nrepresentation for authoring tutorials, we show that tutorial authors have a\npreference for wearable capture devices, especially when recording activities\ninvolving larger objects in non-desktop environments. Authors preferred\ndocument-based multimedia tutorials because they are more straightforward to\ncompose and the step-based structure translates more directly to explaining a\nprocedure. In addition, we explored using head-mounted displays for accessing\ntutorials in comparison to lightweight computing devices such as tablets. Our\nstudy included tutorials recorded with the same capture methods as in our\naccess study. We found that although authors preferred head-mounted capture,\ntutorial consumers preferred video recorded by a camera on tripod that provides\na more stable image of the workspace.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 22:38:16 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Carter", "Scott", ""], ["Qvarfordt", "Pernilla", ""], ["Cooper", "Matthew", ""], ["Komori", "Aki", ""], ["Makela", "Ville", ""]]}, {"id": "1801.09626", "submitter": "Bhavya Kailkhura", "authors": "Aditya Vempaty, Bhavya Kailkhura, Pramod K. Varshney", "title": "Human-Machine Inference Networks For Smart Decision Making:\n  Opportunities and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combines\ncomplementary cognitive strengths of humans and machines in an intelligent\nmanner to tackle various inference tasks and achieves higher performance than\neither humans or machines by themselves. While inference performance\noptimization techniques for human-only or sensor-only networks are quite\nmature, HuMaINs require novel signal processing and machine learning solutions.\nIn this paper, we present an overview of the HuMaINs architecture with a focus\non three main issues that include architecture design, inference algorithms\nincluding security/privacy challenges, and application areas/use cases.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 17:03:34 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Vempaty", "Aditya", ""], ["Kailkhura", "Bhavya", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1801.09783", "submitter": "Emilio Ferrara", "authors": "Anna Sapienza, Hao Peng, Emilio Ferrara", "title": "Performance Dynamics and Success in Online Games", "comments": null, "journal-ref": "2017 IEEE International Conference on Data Mining Workshops\n  (ICDMW), pp:902-909, 2017", "doi": "10.1109/ICDMW.2017.124", "report-no": null, "categories": "cs.SI cs.CY cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online data provide a way to monitor how users behave in social systems like\nsocial networks and online games, and understand which features turn an\nordinary individual into a successful one. Here, we propose to study individual\nperformance and success in Multiplayer Online Battle Arena (MOBA) games. Our\npurpose is to identify those behaviors and playing styles that are\ncharacteristic of players with high skill level and that distinguish them from\nother players. To this aim, we study Defense of the ancient 2 (Dota 2), a\npopular MOBA game. Our findings highlight three main aspects to be successful\nin the game: (i) players need to have a warm-up period to enhance their\nperformance in the game; (ii) having a long in-game experience does not\nnecessarily translate in achieving better skills; but rather, (iii) players\nthat reach high skill levels differentiate from others because of their\naggressive playing strategy, which implies to kill opponents more often than\ncooperating with teammates, and trying to give an early end to the match.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 22:15:36 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Sapienza", "Anna", ""], ["Peng", "Hao", ""], ["Ferrara", "Emilio", ""]]}, {"id": "1801.09804", "submitter": "Kyongsik Yun", "authors": "Kyongsik Yun, Jessi Bustos, Thomas Lu", "title": "Predicting Rapid Fire Growth (Flashover) Using Conditional Generative\n  Adversarial Networks", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A flashover occurs when a fire spreads very rapidly through crevices due to\nintense heat. Flashovers present one of the most frightening and challenging\nfire phenomena to those who regularly encounter them: firefighters.\nFirefighters' safety and lives often depend on their ability to predict\nflashovers before they occur. Typical pre-flashover fire characteristics\ninclude dark smoke, high heat, and rollover (\"angel fingers\") and can be\nquantified by color, size, and shape. Using a color video stream from a\nfirefighter's body camera, we applied generative adversarial neural networks\nfor image enhancement. The neural networks were trained to enhance very dark\nfire and smoke patterns in videos and monitor dynamic changes in smoke and fire\nareas. Preliminary tests with limited flashover training videos showed that we\npredicted a flashover as early as 55 seconds before it occurred.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 00:09:48 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Yun", "Kyongsik", ""], ["Bustos", "Jessi", ""], ["Lu", "Thomas", ""]]}, {"id": "1801.10249", "submitter": "Grenville Croll", "authors": "Grenville J. Croll", "title": "The Reification of an Incorrect and Inappropriate Spreadsheet Model", "comments": "14 Pages, 4 Colour Figures, 2 Tables", "journal-ref": "Proceedings of the EuSpRIG 2017 Conference \"Spreadsheet Risk\n  Management\", Imperial College, London, pp63-76 ISBN: 978-1-905404-54-4", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Once information is loaded into a spreadsheet, it acquires properties that it\nmay not deserve. These properties include believability, correctness,\nappropriateness, concreteness, integrity, tangibility, objectivity and\nauthority. The information becomes reified. We describe a case study through\nwhich we were able to observe at close hand the reification of a demonstrably\nincorrect and inappropriate spreadsheet model within a small non profit\norganisation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 22:54:06 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Croll", "Grenville J.", ""]]}, {"id": "1801.10408", "submitter": "Michael Veale", "authors": "Reuben Binns, Max Van Kleek, Michael Veale, Ulrik Lyngs, Jun Zhao and\n  Nigel Shadbolt", "title": "'It's Reducing a Human Being to a Percentage'; Perceptions of Justice in\n  Algorithmic Decisions", "comments": "14 pages, 3 figures, ACM Conference on Human Factors in Computing\n  Systems (CHI'18), April 21--26, Montreal, Canada", "journal-ref": null, "doi": "10.1145/3173574.3173951", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven decision-making consequential to individuals raises important\nquestions of accountability and justice. Indeed, European law provides\nindividuals limited rights to 'meaningful information about the logic' behind\nsignificant, autonomous decisions such as loan approvals, insurance quotes, and\nCV filtering. We undertake three experimental studies examining people's\nperceptions of justice in algorithmic decision-making under different scenarios\nand explanation styles. Dimensions of justice previously observed in response\nto human decision-making appear similarly engaged in response to algorithmic\ndecisions. Qualitative analysis identified several concerns and heuristics\ninvolved in justice perceptions including arbitrariness, generalisation, and\n(in)dignity. Quantitative analysis indicates that explanation styles primarily\nmatter to justice perceptions only when subjects are exposed to multiple\ndifferent styles---under repeated exposure of one style, scenario effects\nobscure any explanation effects. Our results suggests there may be no 'best'\napproach to explaining algorithmic decisions, and that reflection on their\nautomated nature both implicates and mitigates justice dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 11:31:46 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Binns", "Reuben", ""], ["Van Kleek", "Max", ""], ["Veale", "Michael", ""], ["Lyngs", "Ulrik", ""], ["Zhao", "Jun", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "1801.10492", "submitter": "Charles Martin", "authors": "Charles P. Martin and Kai Olav Ellefsen and Jim Torresen", "title": "Deep Predictive Models in Interactive Music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.HC cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Musical performance requires prediction to operate instruments, to perform in\ngroups and to improvise. In this paper, we investigate how a number of digital\nmusical instruments (DMIs), including two of our own, have applied predictive\nmachine learning models that assist users by predicting unknown states of\nmusical processes. We characterise these predictions as focussed within a\nmusical instrument, at the level of individual performers, and between members\nof an ensemble. These models can connect to existing frameworks for DMI design\nand have parallels in the cognitive predictions of human musicians.\n  We discuss how recent advances in deep learning highlight the role of\nprediction in DMIs, by allowing data-driven predictive models with a long\nmemory of past states. The systems we review are used to motivate musical\nuse-cases where prediction is a necessary component, and to highlight a number\nof challenges for DMI designers seeking to apply deep predictive models in\ninteractive music systems of the future.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 15:30:32 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 08:48:26 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 22:16:26 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Martin", "Charles P.", ""], ["Ellefsen", "Kai Olav", ""], ["Torresen", "Jim", ""]]}]