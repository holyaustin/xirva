[{"id": "2007.00003", "submitter": "Chris Roast", "authors": "Chris Roast", "title": "EQUS -- helping to see formulae", "comments": "12 Pages, 7 Colour Figures", "journal-ref": "Proceedings of the EuSpRIG 2019 Conference \"Spreadsheet Risk\n  Management\", Browns, Covent Garden, London, pp115-126, ISBN:\n  978-1-905404-56-8", "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualisation is often presented as a means of simplifying information and\nhelping people understand complex data. In this paper we describe the design,\ndevelopment and evaluation of an interactive visualisation for spreadsheet\nformulae (EQUS). The work is justified on the grounds that these are widely\nused tools for significant numerical processing and modeling, yet the formula\ndeveloped can be easily misunderstood. The development process was one of\niterative refinement engaging an initial target audience of mid-teen learners,\ninvolving re-design and formative evaluation. The resulting visualisation\ntechniques have been found to be broadly relevant to spreadsheet users beyond\nthe initial target audience. EQUS has since been developed as fully integrated\nplug-in for MS Excel.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 22:07:01 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Roast", "Chris", ""]]}, {"id": "2007.00490", "submitter": "Chatura Samarakoon", "authors": "Chatura Samarakoon, Gehan Amaratunga, Phillip Stanley-Marbell", "title": "Inferring Human Observer Spectral Sensitivities from Video Game Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the use of primaries which have increasingly narrow bandwidths in modern\ndisplays, observer metameric breakdown is becoming a significant factor. This\ncan lead to discrepancies in the perceived color between different observers.\nIf the spectral sensitivity of a user's eyes could be easily measured, next\ngeneration displays would be able to adjust the display content to ensure that\nthe colors are perceived as intended by a given observer. We present a\nmathematical framework for calculating spectral sensitivities of a given human\nobserver using a color matching experiment that could be done on a mobile phone\ndisplay. This forgoes the need for expensive in-person experiments and allows\nsystem designers to easily calibrate displays to match the user's vision,\nin-the-wild. We show how to use sRGB pixel values along with a simple display\nmodel to calculate plausible color matching functions (CMFs) for the users of a\ngiven display device (e.g., a mobile phone). We evaluate the effect of\ndifferent regularization functions on the shape of the calculated CMFs and the\nresults show that a sum of squares regularizer is able to predict smooth and\nqualitatively realistic CMFs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:49:53 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 10:29:56 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Samarakoon", "Chatura", ""], ["Amaratunga", "Gehan", ""], ["Stanley-Marbell", "Phillip", ""]]}, {"id": "2007.00494", "submitter": "Chatura Samarakoon", "authors": "Chatura Samarakoon, Gehan Amaratunga, Phillip Stanley-Marbell", "title": "Content-Aware Automated Parameter Tuning for Approximate Color\n  Transforms", "comments": null, "journal-ref": null, "doi": "10.1145/3406324.3410713", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are numerous approximate color transforms reported in the literature\nthat aim to reduce display power consumption by imperceptibly changing the\ncolor content of displayed images. To be practical, these techniques need to be\ncontent-aware in picking transformation parameters to preserve perceptual\nquality. This work presents a computationally-efficient method for calculating\na parameter lower bound for approximate color transform parameters based on the\ncontent to be transformed. We conduct a user study with 62 participants and\n6,400 image pair comparisons to derive the proposed solution. We use the user\nstudy results to predict this lower bound reliably with a 1.6% mean squared\nerror by using simple image-color-based heuristics. We show that these\nheuristics have Pearson and Spearman rank correlation coefficients greater than\n0.7 (p<0.01) and that our model generalizes beyond the data from the user\nstudy. The user study results also show that the color transform is able to\nachieve up to 50% power saving with most users reporting negligible visual\nimpairment.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:52:49 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 12:37:48 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Samarakoon", "Chatura", ""], ["Amaratunga", "Gehan", ""], ["Stanley-Marbell", "Phillip", ""]]}, {"id": "2007.00613", "submitter": "Anis Zaman", "authors": "Anis Zaman, Boyu Zhang, Vincent Silenzio, Ehsan Hoque and Henry Kautz", "title": "Individual-level Anxiety Detection and Prediction from Longitudinal\n  YouTube and Google Search Engagement Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anxiety disorder is one of the world's most prevalent mental health\nconditions, arising from complex interactions of biological and environmental\nfactors and severely interfering one's ability to lead normal life activities.\nCurrent methods for detecting anxiety heavily rely on in-person interviews,\nwhich can be expensive, time-consuming, and blocked by social stigmas. In this\nwork, we propose an alternative method to identify individuals with anxiety and\nfurther estimate their levels of anxiety using personal online activity\nhistories from YouTube and the Google Search engine, platforms that are used by\nmillions of people daily. We ran a longitudinal study and collected multiple\nrounds of anonymized YouTube and Google Search logs from volunteering\nparticipants, along with their clinically validated ground-truth anxiety\nassessment scores. We then developed explainable features that capture both the\ntemporal and contextual aspects of online behaviors. Using those, we were able\nto train models that (i) identify individuals having anxiety disorder with an\naverage F1 score of 0.83 and (ii) assess the level of anxiety by predicting the\ngold standard Generalized Anxiety Disorder 7-item scores (ranges from 0 to 21)\nwith a mean square error of 1.87 based on the ubiquitous individual-level\nonline engagement data. Our proposed anxiety assessment framework is\ncost-effective, time-saving, scalable, and opens the door for it to be deployed\nin real-world clinical settings, empowering care providers and therapists to\nlearn about anxiety disorders of patients non-invasively at any moment in time.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:57:19 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 21:28:08 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Zaman", "Anis", ""], ["Zhang", "Boyu", ""], ["Silenzio", "Vincent", ""], ["Hoque", "Ehsan", ""], ["Kautz", "Henry", ""]]}, {"id": "2007.00747", "submitter": "Yusuf Sermet", "authors": "Yusuf Sermet and Ibrahim Demir", "title": "A Semantic Web Framework for Automated Smart Assistants: COVID-19 Case\n  Study", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  COVID-19 pandemic elucidated that knowledge systems will be instrumental in\ncases where accurate information needs to be communicated to a substantial\ngroup of people with different backgrounds and technological resources.\nHowever, several challenges and obstacles hold back the wide adoption of\nvirtual assistants by public health departments and organizations. This paper\npresents the Instant Expert, an open-source semantic web framework to build and\nintegrate voice-enabled smart assistants (i.e. chatbots) for any web platform\nregardless of the underlying domain and technology. The component allows\nnon-technical domain experts to effortlessly incorporate an operational\nassistant with voice recognition capability into their websites. Instant Expert\nis capable of automatically parsing, processing, and modeling Frequently Asked\nQuestions pages as an information resource as well as communicating with an\nexternal knowledge engine for ontology-powered inference and dynamic data\nutilization. The presented framework utilizes advanced web technologies to\nensure reusability and reliability, and an inference engine for natural\nlanguage understanding powered by deep learning and heuristic algorithms. A use\ncase for creating an informatory assistant for COVID-19 based on the Centers\nfor Disease Control and Prevention (CDC) data is presented to demonstrate the\nframework's usage and benefits.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:47:44 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2020 17:28:08 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Sermet", "Yusuf", ""], ["Demir", "Ibrahim", ""]]}, {"id": "2007.00762", "submitter": "Rakshit Naidu", "authors": "Jithin Sunny, Joel Jogy, Rohan Rout and Rakshit Naidu", "title": "TeleVital: Enhancing the quality of contactless health assessment", "comments": "4+1 pages and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the midst of rising positive cases of COVID-19, the hospitals face a\nnewfound difficulty to prioritize on their patients and accommodate them.\nMoreover, crowding of patients at hospitals pose a threat to the healthcare\nworkers and other patients at the hospital. With that in mind, a non-contact\nmethod of measuring the necessary vitals such as heart rate, respiratory rate\nand SPO$_2$ will prove highly beneficial for the hospitals to tackle this\nissue. This paper discusses our approach in achieving the non-contact\nmeasurement of vitals with the sole help of a webcam and further our design of\nan e-hospital platform for doctors and patients to attend appointments\nvirtually. The platform also provides the doctor with an option to provide with\nvoice-based prescriptions or digital prescriptions, to simplify the daily,\nexhausting routine of a doctor.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 17:44:56 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Sunny", "Jithin", ""], ["Jogy", "Joel", ""], ["Rout", "Rohan", ""], ["Naidu", "Rakshit", ""]]}, {"id": "2007.00900", "submitter": "Kamran Alipour", "authors": "Kamran Alipour, Arijit Ray, Xiao Lin, Jurgen P. Schulze, Yi Yao,\n  Giedrius T. Burachas", "title": "The Impact of Explanations on AI Competency Prediction in VQA", "comments": "Submitted to HCCAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainability is one of the key elements for building trust in AI systems.\nAmong numerous attempts to make AI explainable, quantifying the effect of\nexplanations remains a challenge in conducting human-AI collaborative tasks.\nAside from the ability to predict the overall behavior of AI, in many\napplications, users need to understand an AI agent's competency in different\naspects of the task domain. In this paper, we evaluate the impact of\nexplanations on the user's mental model of AI agent competency within the task\nof visual question answering (VQA). We quantify users' understanding of\ncompetency, based on the correlation between the actual system performance and\nuser rankings. We introduce an explainable VQA system that uses spatial and\nobject features and is powered by the BERT language model. Each group of users\nsees only one kind of explanation to rank the competencies of the VQA model.\nThe proposed model is evaluated through between-subject experiments to probe\nexplanations' impact on the user's perception of competency. The comparison\nbetween two VQA models shows BERT based explanations and the use of object\nfeatures improve the user's prediction of the model's competencies.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 06:11:28 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Alipour", "Kamran", ""], ["Ray", "Arijit", ""], ["Lin", "Xiao", ""], ["Schulze", "Jurgen P.", ""], ["Yao", "Yi", ""], ["Burachas", "Giedrius T.", ""]]}, {"id": "2007.01413", "submitter": "Ridwan Alam", "authors": "Ridwan Alam, David B. Peden, and John C. Lach", "title": "Wearable Respiration Monitoring: Interpretable Inference with Context\n  and Sensor Biomarkers", "comments": "10 pages, 10 figures", "journal-ref": null, "doi": "10.1109/JBHI.2020.3035776", "report-no": null, "categories": "eess.SP cs.AI cs.CY cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Breathing rate (BR), minute ventilation (VE), and other respiratory\nparameters are essential for real-time patient monitoring in many acute health\nconditions, such as asthma. The clinical standard for measuring respiration,\nnamely Spirometry, is hardly suitable for continuous use. Wearables can track\nmany physiological signals, like ECG and motion, yet not respiration. Deriving\nrespiration from other modalities has become an area of active research. In\nthis work, we infer respiratory parameters from wearable ECG and wrist motion\nsignals. We propose a modular and generalizable classification-regression\npipeline to utilize available context information, such as physical activity,\nin learning context-conditioned inference models. Morphological and power\ndomain novel features from the wearable ECG are extracted to use with these\nmodels. Exploratory feature selection methods are incorporated in this pipeline\nto discover application-specific interpretable biomarkers. Using data from 15\nsubjects, we evaluate two implementations of the proposed pipeline: for\ninferring BR and VE. Each implementation compares generalized linear model,\nrandom forest, support vector machine, Gaussian process regression, and\nneighborhood component analysis as contextual regression models. Permutation,\nregularization, and relevance determination methods are used to rank the ECG\nfeatures to identify robust ECG biomarkers across models and activities. This\nwork demonstrates the potential of wearable sensors not only in continuous\nmonitoring, but also in designing biomarker-driven preventive measures.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 22:12:49 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Alam", "Ridwan", ""], ["Peden", "David B.", ""], ["Lach", "John C.", ""]]}, {"id": "2007.01721", "submitter": "Soteris Demetriou", "authors": "Hsiao-Ying Huang, Soteris Demetriou, Rini Banerjee, G\\\"uliz Seray\n  Tuncay, Carl A. Gunter, Masooda Bashir", "title": "Smartphone Security Behavioral Scale: A New Psychometric Measurement for\n  Smartphone Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite widespread use of smartphones, there is no measurement standard\ntargeted at smartphone security behaviors. In this paper we translate a\nwell-known cybersecurity behavioral scale into the smartphone domain and show\nthat we can improve on this translation by following an established\npsychometrics approach surveying 1011 participants. We design a new 14-item\nSmartphone Security Behavioral Scale (SSBS) exhibiting high reliability and\ngood fit to a two-component behavioural model based on technical versus social\nprotection strategies. We then demonstrate how SSBS can be applied to measure\nthe influence of mental health issues on smartphone security behavior\nintentions. We found significant correlations that predict SSBS profiles from\nthree types of MHIs. Conversely, we are able to predict presence of MHIs using\nSSBS profiles.We obtain prediction AUCs of 72.1% for Internet addiction,75.8%\nfor depression and 66.2% for insomnia.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 14:43:50 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 17:54:15 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Huang", "Hsiao-Ying", ""], ["Demetriou", "Soteris", ""], ["Banerjee", "Rini", ""], ["Tuncay", "G\u00fcliz Seray", ""], ["Gunter", "Carl A.", ""], ["Bashir", "Masooda", ""]]}, {"id": "2007.01800", "submitter": "Jingxuan Tu", "authors": "Jingxuan Tu, Marc Verhagen, Brent Cochran, James Pustejovsky", "title": "Exploration and Discovery of the COVID-19 Literature through Semantic\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are developing semantic visualization techniques in order to enhance\nexploration and enable discovery over large datasets of complex networks of\nrelations. Semantic visualization is a method of enabling exploration and\ndiscovery over large datasets of complex networks by exploiting the semantics\nof the relations in them. This involves (i) NLP to extract named entities,\nrelations and knowledge graphs from the original data; (ii) indexing the output\nand creating representations for all relevant entities and relations that can\nbe visualized in many different ways, e.g., as tag clouds, heat maps, graphs,\netc.; (iii) applying parameter reduction operations to the extracted relations,\ncreating \"relation containers\", or functional entities that can also be\nvisualized using the same methods, allowing the visualization of multiple\nrelations, partial pathways, and exploration across multiple dimensions. Our\nhope is that this will enable the discovery of novel inferences over relations\nin complex data that otherwise would go unnoticed. We have applied this to\nanalysis of the recently released CORD-19 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 16:40:37 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Tu", "Jingxuan", ""], ["Verhagen", "Marc", ""], ["Cochran", "Brent", ""], ["Pustejovsky", "James", ""]]}, {"id": "2007.01862", "submitter": "HMN Dilum Bandara", "authors": "M. Shazmin Marikar and H.M.N. Dilum Bandara", "title": "An Analysis of Data Driven, Decision-Making Capabilities of Managers in\n  Banks", "comments": "19 pages, 8 figues, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organizations are adopting data analytics and Business Intelligence (BI)\ntools to gain insights from the past data, forecast future events, and to get\ntimely and reliable information for decision making. While the tools are\nbecoming mature, affordable, and more comfortable to use, it is also essential\nto understand whether the contemporary managers and leaders are ready for\nData-Driven Decision Making (DDDM). We explore the extent the Decision Makers\n(DMs) utilize data and tools, as well as their ability to interpret various\nforms of outputs from tools and to apply those insights to gain competitive\nadvantage. Our methodology was based on a qualitative survey, where we\ninterviewed 12 DMs of six commercial banks in Sri Lanka at the branch,\nregional, and CTO, CIO, and Head of IT levels. We identified that on many\noccasions, DMs' intuition overrules the DDDM due to uncertainty, lack of trust,\nknowledge, and risk-taking. Moreover, it was identified that the quality of\nvisualizations has a significant impact on the use of intuition by overruling\nDDDM. We further provide a set of recommendations on the adoption of BI tools\nand how to overcome the struggles faced while performing DDDM.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 08:14:56 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Marikar", "M. Shazmin", ""], ["Bandara", "H. M. N. Dilum", ""]]}, {"id": "2007.01921", "submitter": "Matthew Gombolay", "authors": "Ruisen Liu, Manisha Natarajan, and Matthew Gombolay", "title": "Human-Robot Team Coordination with Dynamic and Latent Human Task\n  Proficiencies: Scheduling with Learning Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots become ubiquitous in the workforce, it is essential that\nhuman-robot collaboration be both intuitive and adaptive. A robot's quality\nimproves based on its ability to explicitly reason about the time-varying (i.e.\nlearning curves) and stochastic capabilities of its human counterparts, and\nadjust the joint workload to improve efficiency while factoring human\npreferences. We introduce a novel resource coordination algorithm that enables\nrobots to explore the relative strengths and learning abilities of their human\nteammates, by constructing schedules that are robust to stochastic and\ntime-varying human task performance. We first validate our algorithmic approach\nusing data we collected from a user study (n = 20), showing we can quickly\ngenerate and evaluate a robust schedule while discovering the latest individual\nworker proficiency. Second, we conduct a between-subjects experiment (n = 90)\nto validate the efficacy of our coordinating algorithm. Results from the\nhuman-subjects experiment indicate that scheduling strategies favoring\nexploration tend to be beneficial for human-robot collaboration as it improves\nteam fluency (p = 0.0438), while also maximizing team efficiency (p < 0.001).\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 19:44:22 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 02:40:57 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Liu", "Ruisen", ""], ["Natarajan", "Manisha", ""], ["Gombolay", "Matthew", ""]]}, {"id": "2007.02014", "submitter": "Clayton Miller", "authors": "Prageeth Jayathissa, Matias Quintana, Mahmoud Abdelrahman, and Clayton\n  Miller", "title": "Humans-as-a-sensor for buildings: Intensive longitudinal indoor comfort\n  models", "comments": null, "journal-ref": "Buildings 2020, 10(10), 174", "doi": "10.3390/buildings10100174", "report-no": null, "categories": "cs.HC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating and optimising human comfort within the built environment is\nchallenging due to the large number of physiological, psychological and\nenvironmental variables that affect occupant comfort preference. Human\nperception could be helpful to capture these disparate phenomena and\ninterpreting their impact; the challenge is collecting spatially and temporally\ndiverse subjective feedback in a scalable way. This paper presents a\nmethodology to collect intensive longitudinal subjective feedback of\ncomfort-based preference using micro ecological momentary assessments on a\nsmartwatch platform. An experiment with 30 occupants over two weeks produced\n4,378 field-based surveys for thermal, noise, and acoustic preference. The\noccupants and the spaces in which they left feedback were then clustered\naccording to these preference tendencies. These groups were used to create\ndifferent feature sets with combinations of environmental and physiological\nvariables, for use in a multi-class classification task. These classification\nmodels were trained on a feature set that was developed from time-series\nattributes, environmental and near-body sensors, heart rate, and the historical\npreferences of both the individual and the comfort group assigned. The most\naccurate model had multi-class classification F1 micro scores of 64%, 80% and\n86% for thermal, light, and noise preference, respectively. The discussion\noutlines how these models can enhance comfort preference prediction when\nsupplementing data from installed sensors. The approach presented prompts\nreflection on how the building analysis community evaluates, controls, and\ndesigns indoor environments through balancing the measurement of variables with\nstrategically asking for occupant preferences in an intensive longitudinal way.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 05:52:56 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 06:16:27 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Jayathissa", "Prageeth", ""], ["Quintana", "Matias", ""], ["Abdelrahman", "Mahmoud", ""], ["Miller", "Clayton", ""]]}, {"id": "2007.02064", "submitter": "Oliver Carr", "authors": "Oliver Carr, Fernando Andreotti, Kate E. A. Saunders, Niclas Palmius,\n  Guy M. Goodwin, and Maarten De Vos", "title": "Monitoring Depression in Bipolar Disorder using Circadian Measures from\n  Smartphone Accelerometers", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current management of bipolar disorder relies on self-reported questionnaires\nand interviews with clinicians. The development of objective measures of\ndeteriorating mood may also allow for early interventions to take place to\navoid transitions into depressive states. The objective of this study was to\nuse acceleration data recorded from smartphones to predict levels of depression\nin a population of participants diagnosed with bipolar disorder. Data were\ncollected from 52 participants, with a mean of 37 weeks of acceleration data\nwith a corresponding depression score recorded per participant. Time varying\nhidden Markov models were used to extract weekly features of activity, sleep\nand circadian rhythms. Personalised regression achieved mean absolute errors of\n1.00(0.57) from a possible scale of 0 to 27 and was able to classify depression\nwith an accuracy of 0.84(0.16). The results demonstrate features derived from\nsmartphone accelerometers are able to provide objective markers of depression.\nLow barriers for uptake exist due to the widespread use of smartphones, with\npersonalised models able to account for differences in the behaviour of\nindividuals and provide accurate predictions of depression.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 10:22:25 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Carr", "Oliver", ""], ["Andreotti", "Fernando", ""], ["Saunders", "Kate E. A.", ""], ["Palmius", "Niclas", ""], ["Goodwin", "Guy M.", ""], ["De Vos", "Maarten", ""]]}, {"id": "2007.02161", "submitter": "Ellis Solaiman", "authors": "Bakri Awaji, Ellis Solaiman, Lindsay Marshall", "title": "Blockchain-Based Trusted Achievement Record System Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary purpose of this paper is to provide a design of a\nblockchain-based system, which produces a verifiable record of achievements.\nSuch a system has a wide range of potential benefits for students, employers\nand higher education institutions. A verifiable record of achievements enables\nstudents to present academic accomplishments to employers, within a trusted\nframework. Furthermore, the availability of such a record system would enable\nstudents to review their learning throughout their career, giving them a\nplatform on which to plan for their future accomplishments, both individually\nand with support from other parties (for example, academic advisors,\nsupervisors, or potential employers). The proposed system will help students in\nuniversities to increase their extra-curricular activities and improve\nnon-academic skills. Moreover, the system will facilitate communication between\nindustry, students, and universities for employment purposes and simplify the\nsearch for the most appropriate potential employees for the job.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 18:43:47 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Awaji", "Bakri", ""], ["Solaiman", "Ellis", ""], ["Marshall", "Lindsay", ""]]}, {"id": "2007.02162", "submitter": "Ellis Solaiman", "authors": "Bakri Awaji, Ellis Solaiman, Lindsay Marshall", "title": "Investigating the Requirements for Building a Blockchain- Based\n  Achievement Record System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trusted achievement record is a secure system that aims to record and\nauthenticate certificates as well as key learning activities and achievements.\nThis paper intends to gather important information on the thoughts and outlooks\nof stakeholders on an achievement record system that uses blockchain and smart\ncontract technology. The system would allow stakeholders (for example\nemployers) to validate learning records. Two main aims are investigated. The\nfirst is to evaluate the suitability of the idea of building a trusted\nachievement record for learners in higher education, and to evaluate potential\nuser knowledge of blockchain technology. This is to ensure that a designed\nsystem is usable. The second aim includes an interview conducted with a small\ngroup of participants to gather information about the challenges individuals\nhave when creating, and reviewing CVs. Overall, 90% of participants agreed that\nthere was a strong need for a trusted achievement record. In addition, 93.64%\nof respondents stated that they felt it was invaluable to have a system that is\nusable by all stakeholders. When tackling the second aim it was found that a\nprimary challenge is lack of knowledge of blockchain and its complexity. From\nthe employers' perspective, there is a lack of trust due to inaccuracies when\nstudents describe skills and qualifications in their resumes.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 18:44:14 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Awaji", "Bakri", ""], ["Solaiman", "Ellis", ""], ["Marshall", "Lindsay", ""]]}, {"id": "2007.02199", "submitter": "Kaylea Champion", "authors": "Kaylea Champion", "title": "Characterizing Online Vandalism: A Rational Choice Perspective", "comments": null, "journal-ref": null, "doi": "10.1145/3400806.3400813", "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  What factors influence the decision to vandalize? Although the harm is clear,\nthe benefit to the vandal is less clear. In many cases, the thing being damaged\nmay itself be something the vandal uses or enjoys. Vandalism holds\ncommunicative value: perhaps to the vandal themselves, to some audience at whom\nthe vandalism is aimed, and to the general public. Viewing vandals as rational\ncommunity participants despite their antinormative behavior offers the\npossibility of engaging with or countering their choices in novel ways.\nRational choice theory (RCT) as applied in value expectancy theory (VET) offers\na strategy for characterizing behaviors in a framework of rational choices, and\nbegins with the supposition that subject to some weighting of personal\npreferences and constraints, individuals maximize their own utility by\ncommitting acts of vandalism. This study applies the framework of RCT and VET\nto gain insight into vandals' preferences and constraints. Using a\nmixed-methods analysis of Wikipedia, I combine social computing and\ncriminological perspectives on vandalism to propose an ontology of vandalism\nfor online content communities. I use this ontology to categorize 141 instances\nof vandalism and find that the character of vandalistic acts varies by vandals'\nrelative identifiability, policy history with Wikipedia, and the effort\nrequired to vandalize.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 22:29:22 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Champion", "Kaylea", ""]]}, {"id": "2007.02642", "submitter": "Sang-Woo Lee", "authors": "Sang-Woo Lee, Hyunhoon Jung, SukHyun Ko, Sunyoung Kim, Hyewon Kim,\n  Kyoungtae Doh, Hyunjung Park, Joseph Yeo, Sang-Houn Ok, Joonhaeng Lee,\n  Sungsoon Lim, Minyoung Jeong, Seongjae Choi, SeungTae Hwang, Eun-Young Park,\n  Gwang-Ja Ma, Seok-Joo Han, Kwang-Seung Cha, Nako Sung, Jung-Woo Ha", "title": "CareCall: a Call-Based Active Monitoring Dialog Agent for Managing\n  COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking suspected cases of COVID-19 is crucial to suppressing the spread of\nCOVID-19 pandemic. Active monitoring and proactive inspection are indispensable\nto mitigate COVID-19 spread, though these require considerable social and\neconomic expense. To address this issue, we introduce CareCall, a call-based\ndialog agent which is deployed for active monitoring in Korea and Japan. We\ndescribe our system with a case study with statistics to show how the system\nworks. Finally, we discuss a simple idea which uses CareCall to support\nproactive inspection.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 11:05:22 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 09:21:08 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Lee", "Sang-Woo", ""], ["Jung", "Hyunhoon", ""], ["Ko", "SukHyun", ""], ["Kim", "Sunyoung", ""], ["Kim", "Hyewon", ""], ["Doh", "Kyoungtae", ""], ["Park", "Hyunjung", ""], ["Yeo", "Joseph", ""], ["Ok", "Sang-Houn", ""], ["Lee", "Joonhaeng", ""], ["Lim", "Sungsoon", ""], ["Jeong", "Minyoung", ""], ["Choi", "Seongjae", ""], ["Hwang", "SeungTae", ""], ["Park", "Eun-Young", ""], ["Ma", "Gwang-Ja", ""], ["Han", "Seok-Joo", ""], ["Cha", "Kwang-Seung", ""], ["Sung", "Nako", ""], ["Ha", "Jung-Woo", ""]]}, {"id": "2007.02674", "submitter": "Thomas Johnson", "authors": "Thomas Johnson, Eiman Kanjo, Kieran Woodward", "title": "Sensor Data and the City: Urban Visualisation and Aggregation of\n  Well-Being Data", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growth of mobile sensor technologies have made it possible for city\ncouncils to understand peoples' behaviour in urban spaces which could help to\nreduce stress around the city. We present a quantitative approach to convey a\ncollective sense of urban places. The data was collected at a high level of\ngranularity, navigating the space around a highly popular urban environment. We\ncapture people's behaviour by leveraging continuous multi-model sensor data\nfrom environmental and physiological sensors. The data is also tagged with\nself-report, location coordinates as well as the duration in different\nenvironments. The approach leverages an exploratory data visualisation along\nwith geometrical and spatial data analysis algorithms, allowing spatial and\ntemporal comparisons of data clusters in relation to people's behaviour.\nDeriving and quantifying such meaning allows us to observe how mobile sensing\nunveils the emotional characteristics of places from such crowd-contributed\ncontent.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 15:44:52 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Johnson", "Thomas", ""], ["Kanjo", "Eiman", ""], ["Woodward", "Kieran", ""]]}, {"id": "2007.02691", "submitter": "Anna Bleakley", "authors": "Anna Bleakley, Vincent Wade, Benjamin R. Cowan", "title": "Finally a Case for Collaborative VR?: The Need to Design for Remote\n  Multi-Party Conversations", "comments": null, "journal-ref": null, "doi": "10.1145/3405755.3406144", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Amid current social distancing measures requiring people to work from home,\nthere has been renewed interest on how to effectively converse and collaborate\nremotely utilizing currently available technologies. On the surface, VR\nprovides a perfect platform for effective remote communication. It can transfer\ncontextual and environmental cues and facilitate a shared perspective while\nalso allowing people to be virtually co-located. Yet we argue that currently VR\nis not adequately designed for such a communicative purpose. In this paper, we\noutline three key barriers to using VR for conversational activity : (1)\nvariability of social immersion, (2) unclear user roles, and (3) the need for\neffective shared visual reference. Based on this outline, key design topics are\ndiscussed through a user experience design perspective for considerations in a\nfuture collaborative design framework.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 12:36:29 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Bleakley", "Anna", ""], ["Wade", "Vincent", ""], ["Cowan", "Benjamin R.", ""]]}, {"id": "2007.02728", "submitter": "HMN Dilum Bandara", "authors": "Sandareka Wickramanayake, H.M.N Dilum Bandara, Nishal A. Samarasekara", "title": "Real-Time Monitoring and Driver Feedback to Promote Fuel Efficient\n  Driving", "comments": "17 pages, 9 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Improving the fuel efficiency of vehicles is imperative to reduce costs and\nprotect the environment. While the efficient engine and vehicle designs, as\nwell as intelligent route planning, are well-known solutions to enhance the\nfuel efficiency, research has also demonstrated that the adoption of\nfuel-efficient driving behaviors could lead to further savings. In this work,\nwe propose a novel framework to promote fuel-efficient driving behaviors\nthrough real-time automatic monitoring and driver feedback. In this framework,\na random-forest based classification model developed using historical data to\nidentifies fuel-inefficient driving behaviors. The classifier considers\ndriver-dependent parameters such as speed and acceleration/deceleration\npattern, as well as environmental parameters such as traffic, road topography,\nand weather to evaluate the fuel efficiency of one-minute driving events. When\nan inefficient driving action is detected, a fuzzy logic inference system is\nused to determine what the driver should do to maintain fuel-efficient driving\nbehavior. The decided action is then conveyed to the driver via a smartphone in\na non-intrusive manner. Using a dataset from a long-distance bus, we\ndemonstrate that the proposed classification model yields an accuracy of 85.2%\nwhile increasing the fuel efficiency up to 16.4%.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 09:23:53 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Wickramanayake", "Sandareka", ""], ["Bandara", "H. M. N Dilum", ""], ["Samarasekara", "Nishal A.", ""]]}, {"id": "2007.02884", "submitter": "Mathias Unberath", "authors": "Mathias Unberath, Kevin Yu, Roghayeh Barmaki, Alex Johnson, Nassir\n  Navab", "title": "Augment Yourself: Mixed Reality Self-Augmentation Using Optical\n  See-through Head-mounted Displays and Physical Mirrors", "comments": "This manuscript was initially submitted to IEEE VR TVCG 2018 on\n  November 22, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical see-though head-mounted displays (OST HMDs) are one of the key\ntechnologies for merging virtual objects and physical scenes to provide an\nimmersive mixed reality (MR) environment to its user. A fundamental limitation\nof HMDs is, that the user itself cannot be augmented conveniently as, in casual\nposture, only the distal upper extremities are within the field of view of the\nHMD. Consequently, most MR applications that are centered around the user, such\nas virtual dressing rooms or learning of body movements, cannot be realized\nwith HMDs. In this paper, we propose a novel concept and prototype system that\ncombines OST HMDs and physical mirrors to enable self-augmentation and provide\nan immersive MR environment centered around the user. Our system, to the best\nof our knowledge the first of its kind, estimates the user's pose in the\nvirtual image generated by the mirror using an RGBD camera attached to the HMD\nand anchors virtual objects to the reflection rather than the user directly. We\nevaluate our system quantitatively with respect to calibration accuracy and\ninfrared signal degradation effects due to the mirror, and show its potential\nin applications where large mirrors are already an integral part of the\nfacility. Particularly, we demonstrate its use for virtual fitting rooms,\ngaming applications, anatomy learning, and personal fitness. In contrast to\ncompeting devices such as LCD-equipped smart mirrors, the proposed system\nconsists of only an HMD with RGBD camera and, thus, does not require a prepared\nenvironment making it very flexible and generic. In future work, we will aim to\ninvestigate how the system can be optimally used for physical rehabilitation\nand personal training as a promising application.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 16:53:47 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Unberath", "Mathias", ""], ["Yu", "Kevin", ""], ["Barmaki", "Roghayeh", ""], ["Johnson", "Alex", ""], ["Navab", "Nassir", ""]]}, {"id": "2007.03177", "submitter": "Rahul Pandey", "authors": "Rahul Pandey, Hemant Purohit, Carlos Castillo, Valerie L. Shalin", "title": "Modeling and Mitigating Human Annotation Errors to Design Efficient\n  Stream Processing Systems with Human-in-the-loop Machine Learning", "comments": "Preprint submitted to International Journal of Human-Computer Studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-quality human annotations are necessary for creating effective machine\nlearning-driven stream processing systems. We study hybrid stream processing\nsystems based on a Human-In-The-Loop Machine Learning (HITL-ML) paradigm, in\nwhich one or many human annotators and an automatic classifier (trained at\nleast partially by the human annotators) label an incoming stream of instances.\nThis is typical of many near-real time social media analytics and web\napplications, including the annotation of social media posts during emergencies\nby digital volunteer groups. From a practical perspective, low-quality human\nannotations result in wrong labels for retraining automated classifiers and\nindirectly contribute to the creation of inaccurate classifiers.\n  Considering human annotation as a psychological process allows us to address\nthese limitations. We show that human annotation quality is dependent on the\nordering of instances shown to annotators, and can be improved by local changes\nin the instance sequence/ordering provided to the annotators, yielding a more\naccurate annotation of the stream. We design a theoretically-motivated human\nerror framework for the human annotation task to study the effect of ordering\ninstances (i.e., an \"annotation schedule\"). Further, we propose an\nerror-avoidance approach to the active learning (HITL-ML) paradigm for stream\nprocessing applications that is robust to these likely human errors when\ndeciding a human annotation schedule. We validate the human error framework\nusing crowdsourcing experiments and evaluate the proposed algorithm against\nstandard baselines for active learning via extensive experimentation on\nclassification tasks of filtering relevant social media posts during natural\ndisasters.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 02:48:34 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Pandey", "Rahul", ""], ["Purohit", "Hemant", ""], ["Castillo", "Carlos", ""], ["Shalin", "Valerie L.", ""]]}, {"id": "2007.03180", "submitter": "Chuang Yang", "authors": "Chuang Yang (1), Zhiwen Zhang (1), Zipei Fan (1 and 2), Renhe Jiang (1\n  and 2), Quanjun Chen (1 and 2), Xuan Song (1 and 2), Ryosuke Shibasaki (1 and\n  2) ((1) Center for Spatial Information Science, The University of Tokyo, (2)\n  SUSTech-UTokyo Joint Research Center on Super Smart City, Southern University\n  of Science and Technology)", "title": "EpiMob: Interactive Visual Analytics of Citywide Human Mobility\n  Restrictions for Epidemic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of coronavirus disease (COVID-19) has swept across more than 180\ncountries and territories since late January 2020. As a worldwide emergency\nresponse, governments have taken various measures and implemented policies,\nsuch as self-quarantine, travel restrictions, work from home, and regional\nlockdown, to control the rapid spread of this epidemic. The common intention of\nthese countermeasures is to restrict human mobility because COVID-19 is a\nhighly contagious disease that is spread by human-to-human transmission.\nMedical experts and policy makers have expressed the urgency of being able to\neffectively evaluate the effects of human restriction policies with the aid of\nbig data and information technology. Thus, in this study, based on big human\nmobility data and city POI data, we designed an interactive visual analytics\nsystem named EpiMob (Epidemic Mobility). The system interactively simulates the\nchanges in human mobility and the number of infected people in response to the\nimplementation of a certain restriction policy or combination of policies\n(e.g., regional lockdown, telecommuting, screening). Users can conveniently\ndesignate the spatial and temporal ranges for different mobility restriction\npolicies, and the result reflecting the infection situation under different\npolicies is dynamically displayed and can be flexibly compared. We completed\nmultiple case studies of the largest metropolitan area in Japan (i.e., Greater\nTokyo Area) and conducted interviews with domain experts to demonstrate that\nour system can provide illustrative insight by measuring and comparing the\neffects of different human mobility restriction policies for epidemic control.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 03:01:59 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 08:02:21 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Yang", "Chuang", "", "1 and 2"], ["Zhang", "Zhiwen", "", "1 and 2"], ["Fan", "Zipei", "", "1 and 2"], ["Jiang", "Renhe", "", "1\n  and 2"], ["Chen", "Quanjun", "", "1 and 2"], ["Song", "Xuan", "", "1 and 2"], ["Shibasaki", "Ryosuke", "", "1 and\n  2"]]}, {"id": "2007.03573", "submitter": "Zhibin Niu", "authors": "Zhibin Niu, Runlin Li, Junqi Wu, Yaqi Xue, Jiawan Zhang", "title": "regvis.net -- A Visual Bibliography of Regulatory Visualization", "comments": "2 pages. Refer to http://regvis.net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information visualization and visual analytics technology has attracted\nsignificant attention from the financial regulation community. In this\nresearch, we present regvis.net, a visual survey of regulatory visualization\nthat allows researchers from both the computing and financial communities to\nreview their literature of interest. We have collected and manually tagged more\nthan 80 regulation visualization related publications. To the best of our\nknowledge, this is the first publication set tailored for regulatory\nvisualization. We have provided a webpage (http://regvis.net) for interactive\nsearches and filtering. Each publication is represented by a thumbnail of the\nrepresentative system interface or key visualization chart, and users can\nconduct multi-condition screening explorations and fixed text searches.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 15:49:35 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Niu", "Zhibin", ""], ["Li", "Runlin", ""], ["Wu", "Junqi", ""], ["Xue", "Yaqi", ""], ["Zhang", "Jiawan", ""]]}, {"id": "2007.03600", "submitter": "Kamran Ali", "authors": "Kamran Ali, Alex X. Liu, Eugene Chai, Karthik Sundaresan", "title": "Monitoring Browsing Behavior of Customers in Retail Stores via RFID\n  Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose to use commercial off-the-shelf (COTS) monostatic\nRFID devices (i.e. which use a single antenna at a time for both transmitting\nand receiving RFID signals to and from the tags) to monitor browsing activity\nof customers in front of display items in places such as retail stores. To this\nend, we propose TagSee, a multi-person imaging system based on monostatic RFID\nimaging. TagSee is based on the insight that when customers are browsing the\nitems on a shelf, they stand between the tags deployed along the boundaries of\nthe shelf and the reader, which changes the multi-paths that the RFID signals\ntravel along, and both the RSS and phase values of the RFID signals that the\nreader receives change. Based on these variations observed by the reader,\nTagSee constructs a coarse grained image of the customers. Afterwards, TagSee\nidentifies the items that are being browsed by the customers by analyzing the\nconstructed images. The key novelty of this paper is on achieving browsing\nbehavior monitoring of multiple customers in front of display items by\nconstructing coarse grained images via robust, analytical model-driven deep\nlearning based, RFID imaging. To achieve this, we first mathematically\nformulate the problem of imaging humans using monostatic RFID devices and\nderive an approximate analytical imaging model that correlates the variations\ncaused by human obstructions in the RFID signals. Based on this model, we then\ndevelop a deep learning framework to robustly image customers with high\naccuracy. We implement TagSee scheme using a Impinj Speedway R420 reader and\nSMARTRAC DogBone RFID tags. TagSee can achieve a TPR of more than ~90% and a\nFPR of less than ~10% in multi-person scenarios using training data from just\n3-4 users.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 16:36:24 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Ali", "Kamran", ""], ["Liu", "Alex X.", ""], ["Chai", "Eugene", ""], ["Sundaresan", "Karthik", ""]]}, {"id": "2007.03604", "submitter": "Stefano Cresci", "authors": "Stefano Cresci", "title": "A Decade of Social Bot Detection", "comments": "Forthcoming in Communications of the ACM", "journal-ref": "Communications of the ACM 63.10 (2020):72-83", "doi": "10.1145/3409116", "report-no": null, "categories": "cs.CY cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On the morning of November 9th 2016, the world woke up to the shocking\noutcome of the US Presidential elections: Donald Trump was the 45th President\nof the United States of America. An unexpected event that still has tremendous\nconsequences all over the world. Today, we know that a minority of social bots,\nautomated social media accounts mimicking humans, played a central role in\nspreading divisive messages and disinformation, possibly contributing to\nTrump's victory. In the aftermath of the 2016 US elections, the world started\nto realize the gravity of widespread deception in social media. Following\nTrump's exploit, we witnessed to the emergence of a strident dissonance between\nthe multitude of efforts for detecting and removing bots, and the increasing\neffects that these malicious actors seem to have on our societies. This paradox\nopens a burning question: What strategies should we enforce in order to stop\nthis social bot pandemic? In these times, during the run-up to the 2020 US\nelections, the question appears as more crucial than ever. What stroke social,\npolitical and economic analysts after 2016, deception and automation, has been\nhowever a matter of study for computer scientists since at least 2010. In this\nwork, we briefly survey the first decade of research in social bot detection.\nVia a longitudinal analysis, we discuss the main trends of research in the\nfight against bots, the major results that were achieved, and the factors that\nmake this never-ending battle so challenging. Capitalizing on lessons learned\nfrom our extensive analysis, we suggest possible innovations that could give us\nthe upper hand against deception and manipulation. Studying a decade of\nendeavours at social bot detection can also inform strategies for detecting and\nmitigating the effects of other, more recent, forms of online deception, such\nas strategic information operations and political trolls.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 13:46:38 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 07:55:22 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Cresci", "Stefano", ""]]}, {"id": "2007.03617", "submitter": "Petros Spachos", "authors": "Katherine McLeod, Petros Spachos, Konstantinos Plataniotis", "title": "Smartphone-based Wellness Assessment Using Mobile Environmental Sensor", "comments": null, "journal-ref": null, "doi": "10.1109/JSYST.2020.3004558", "report-no": null, "categories": "cs.CY cs.HC cs.MM cs.SI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health and general wellness are becoming a growing concern in our\nsociety. Environmental factors contribute to mental illness and have the power\nto affect a person's wellness. This work presents a smartphone-based wellness\nassessment system and examines if there is any correlation with one's\nenvironment and their wellness. The introduced system was initiated in response\nto a growing need for individualized and independent mental health care and\nevaluated through experimentation. The participants were given an Android\nsmartphone and a mobile sensor board and they were asked to complete a brief\npsychological survey three times per day. During the survey completion, the\nboard in their possession is reading environmental data. The five environmental\nvariables collected are temperature, humidity, air pressure, luminosity, and\nnoise level. Upon submission of the survey, the results of the survey and the\nenvironmental data are sent to a server for further processing. Three\nexperiments with 62 participants in total have been completed. The correlation\nmost regularly deemed statistically significant was that of light and audio and\nstress.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 20:23:32 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["McLeod", "Katherine", ""], ["Spachos", "Petros", ""], ["Plataniotis", "Konstantinos", ""]]}, {"id": "2007.03647", "submitter": "Ardavan Bidgoli", "authors": "Ardavan Bidgoli, Manuel Ladron De Guevara, Cinnie Hsiung, Jean Oh,\n  Eunsu Kang", "title": "Artistic Style in Robotic Painting; a Machine Learning Approach to\n  Learning Brushstroke from Human Artists", "comments": "The 29th IEEE International Conference on Robot & Human Interactive\n  Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic painting has been a subject of interest among both artists and\nroboticists since the 1970s. Researchers and interdisciplinary artists have\nemployed various painting techniques and human-robot collaboration models to\ncreate visual mediums on canvas. One of the challenges of robotic painting is\nto apply a desired artistic style to the painting. Style transfer techniques\nwith machine learning models have helped us address this challenge with the\nvisual style of a specific painting. However, other manual elements of style,\ni.e., painting techniques and brushstrokes of an artist, have not been fully\naddressed. We propose a method to integrate an artistic style to the\nbrushstrokes and the painting process through collaboration with a human\nartist. In this paper, we describe our approach to 1) collect brushstrokes and\nhand-brush motion samples from an artist, and 2) train a generative model to\ngenerate brushstrokes that pertains to the artist's style, and 3) fine tune a\nstroke-based rendering model to work with our robotic painting setup. We will\nreport on the integration of these three steps in a separate publication. In a\npreliminary study, 71% of human evaluators find our reconstructed brushstrokes\nare pertaining to the characteristics of the artist's style. Moreover, 58% of\nparticipants could not distinguish a painting made by our method from a\nvisually similar painting created by a human artist.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:35:38 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 04:05:51 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bidgoli", "Ardavan", ""], ["De Guevara", "Manuel Ladron", ""], ["Hsiung", "Cinnie", ""], ["Oh", "Jean", ""], ["Kang", "Eunsu", ""]]}, {"id": "2007.03659", "submitter": "Shaun Kane", "authors": "Shaun Kane, Richard Ladner, and Clayton Lewis", "title": "Promoting Strategic Research on Inclusive Access to Rich Online Content\n  and Services", "comments": "A Computing Community Consortium (CCC) workshop report, 16 pages", "journal-ref": null, "doi": null, "report-no": "ccc2014report_5", "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Access to content and services online is increasingly important for everyone,\nincluding people with disabilities. National commitments, including the\nAmericans with Disabilities Act, and international resolutions, including the\nUnited Nations Declaration of the Rights of Persons with Disabilities, call for\nwork to ensure that people with disabilities can participate fully in the\nonline world. Gains in education, employment and health, as well as in civic\nengagement, social participation, and personal independence will follow from\nenhanced inclusion online. Research in many areas of computer science,\nincluding recognition technology, natural language processing, personalization,\nsoftware architecture, and others, is needed to secure these benefits.\nOrganizing this research calls for partnerships among academic researchers,\nfederal agencies, and commercial organizations, as well as effective division\nof labor and cooperation between computer scientists, behavioral scientists,\nadvocacy groups, and consumers.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:50:03 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kane", "Shaun", ""], ["Ladner", "Richard", ""], ["Lewis", "Clayton", ""]]}, {"id": "2007.03661", "submitter": "Yiling Chen", "authors": "Yiling Chen, Arpita Ghosh, Michael Kearns, Tim Roughgarden, and\n  Jennifer Wortman Vaughan", "title": "Mathematical Foundations for Social Computing", "comments": "A Computing Community Consortium (CCC) workshop report, 15 pages", "journal-ref": null, "doi": null, "report-no": "ccc2014report_5", "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social computing encompasses the mechanisms through which people interact\nwith computational systems: crowdsourcing systems, ranking and recommendation\nsystems, online prediction markets, citizen science projects, and\ncollaboratively edited wikis, to name a few. These systems share the common\nfeature that humans are active participants, making choices that determine the\ninput to, and therefore the output of, the system. The output of these systems\ncan be viewed as a joint computation between machine and human, and can be\nricher than what either could produce alone. The term social computing is often\nused as a synonym for several related areas, such as \"human computation\" and\nsubsets of \"collective intelligence\"; we use it in its broadest sense to\nencompass all of these things.\n  Social computing is blossoming into a rich research area of its own, with\ncontributions from diverse disciplines including computer science, economics,\nand other social sciences. Yet a broad mathematical foundation for social\ncomputing is yet to be established, with a plethora of under-explored\nopportunities for mathematical research to impact social computing.\n  As in other fields, there is great potential for mathematical work to\ninfluence and shape the future of social computing. However, we are far from\nhaving the systematic and principled understanding of the advantages,\nlimitations, and potentials of social computing required to match the impact on\napplications that has occurred in other fields. In June 2015, we brought\ntogether roughly 25 experts in related fields to discuss the promise and\nchallenges of establishing mathematical foundations for social computing. This\ndocument captures several of the key ideas discussed.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 17:50:27 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Chen", "Yiling", ""], ["Ghosh", "Arpita", ""], ["Kearns", "Michael", ""], ["Roughgarden", "Tim", ""], ["Vaughan", "Jennifer Wortman", ""]]}, {"id": "2007.03704", "submitter": "Rajeev Alur", "authors": "Rajeev Alur, Richard Baraniuk, Rastislav Bodik, Ann Drobnis, Sumit\n  Gulwani, Bjoern Hartmann, Yasmin Kafai, Jeff Karpicke, Ran Libeskind-Hadas,\n  Debra Richardson, Armando Solar-Lezama, Candace Thille, and Moshe Vardi", "title": "Computer-Aided Personalized Education", "comments": "A Computing Community Consortium (CCC) workshop report, 12 pages", "journal-ref": null, "doi": null, "report-no": "ccc2016report_6", "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shortage of people trained in STEM fields is becoming acute, and\nuniversities and colleges are straining to satisfy this demand. In the case of\ncomputer science, for instance, the number of US students taking introductory\ncourses has grown three-fold in the past decade. Recently, massive open online\ncourses (MOOCs) have been promoted as a way to ease this strain. This at best\nprovides access to education. The bigger challenge though is coping with\nheterogeneous backgrounds of different students, retention, providing feedback,\nand assessment. Personalized education relying on computational tools can\naddress this challenge.\n  While automated tutoring has been studied at different times in different\ncommunities, recent advances in computing and education technology offer\nexciting opportunities to transform the manner in which students learn. In\nparticular, at least three trends are significant. First, progress in logical\nreasoning, data analytics, and natural language processing has led to tutoring\ntools for automatic assessment, personalized instruction including targeted\nfeedback, and adaptive content generation for a variety of subjects. Second,\nresearch in the science of learning and human-computer interaction is leading\nto a better understanding of how different students learn, when and what types\nof interventions are effective for different instructional goals, and how to\nmeasure the success of educational tools. Finally, the recent emergence of\nonline education platforms, both in academia and industry, is leading to new\nopportunities for the development of a shared infrastructure. This CCC workshop\nbrought together researchers developing educational tools based on technologies\nsuch as logical reasoning and machine learning with researchers in education,\nhuman-computer interaction, and cognitive psychology.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:00:04 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Alur", "Rajeev", ""], ["Baraniuk", "Richard", ""], ["Bodik", "Rastislav", ""], ["Drobnis", "Ann", ""], ["Gulwani", "Sumit", ""], ["Hartmann", "Bjoern", ""], ["Kafai", "Yasmin", ""], ["Karpicke", "Jeff", ""], ["Libeskind-Hadas", "Ran", ""], ["Richardson", "Debra", ""], ["Solar-Lezama", "Armando", ""], ["Thille", "Candace", ""], ["Vardi", "Moshe", ""]]}, {"id": "2007.03746", "submitter": "Dongrui Wu", "authors": "Dongrui Wu and Xue Jiang and Ruimin Peng and Wanzeng Kong and Jian\n  Huang and Zhigang Zeng", "title": "Transfer Learning for Motor Imagery Based Brain-Computer Interfaces: A\n  Complete Pipeline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning (TL) has been widely used in motor imagery (MI) based\nbrain-computer interfaces (BCIs) to reduce the calibration effort for a new\nsubject, and demonstrated promising performance. While a closed-loop MI-based\nBCI system, after electroencephalogram (EEG) signal acquisition and temporal\nfiltering, includes spatial filtering, feature engineering, and classification\nblocks before sending out the control signal to an external device, previous\napproaches only considered TL in one or two such components. This paper\nproposes that TL could be considered in all three components (spatial\nfiltering, feature engineering, and classification) of MI-based BCIs.\nFurthermore, it is also very important to specifically add a data alignment\ncomponent before spatial filtering to make the data from different subjects\nmore consistent, and hence to facilitate subsequential TL. Offline calibration\nexperiments on two MI datasets verified our proposal. Especially, integrating\ndata alignment and sophisticated TL approaches can significantly improve the\nclassification performance, and hence greatly reduces the calibration effort.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 23:44:21 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 20:56:41 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 20:37:14 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wu", "Dongrui", ""], ["Jiang", "Xue", ""], ["Peng", "Ruimin", ""], ["Kong", "Wanzeng", ""], ["Huang", "Jian", ""], ["Zeng", "Zhigang", ""]]}, {"id": "2007.03819", "submitter": "Charlie Welch", "authors": "Charles Welch, Allison Lahnala, Ver\\'onica P\\'erez-Rosas, Siqi Shen,\n  Sarah Seraj, Larry An, Kenneth Resnicow, James Pennebaker, Rada Mihalcea", "title": "Expressive Interviewing: A Conversational System for Coping with\n  COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing COVID-19 pandemic has raised concerns for many regarding personal\nand public health implications, financial security and economic stability.\nAlongside many other unprecedented challenges, there are increasing concerns\nover social isolation and mental health. We introduce \\textit{Expressive\nInterviewing}--an interview-style conversational system that draws on ideas\nfrom motivational interviewing and expressive writing. Expressive Interviewing\nseeks to encourage users to express their thoughts and feelings through writing\nby asking them questions about how COVID-19 has impacted their lives. We\npresent relevant aspects of the system's design and implementation as well as\nquantitative and qualitative analyses of user interactions with the system. In\naddition, we conduct a comparative evaluation with a general purpose dialogue\nsystem for mental health that shows our system potential in helping users to\ncope with COVID-19 issues.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 22:52:14 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Welch", "Charles", ""], ["Lahnala", "Allison", ""], ["P\u00e9rez-Rosas", "Ver\u00f3nica", ""], ["Shen", "Siqi", ""], ["Seraj", "Sarah", ""], ["An", "Larry", ""], ["Resnicow", "Kenneth", ""], ["Pennebaker", "James", ""], ["Mihalcea", "Rada", ""]]}, {"id": "2007.03874", "submitter": "Kamran Ali", "authors": "Kamran Ali, Alex X. Liu", "title": "Fine-grained Vibration Based Sensing Using a Smartphone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recognizing surfaces based on their vibration signatures is useful as it can\nenable tagging of different locations without requiring any additional hardware\nsuch as Near Field Communication (NFC) tags. However, previous vibration based\nsurface recognition schemes either use custom hardware for creating and sensing\nvibration, which makes them difficult to adopt, or use inertial (IMU) sensors\nin commercial off-the-shelf (COTS) smartphones to sense movements produced due\nto vibrations, which makes them coarse-grained because of the low sampling\nrates of IMU sensors. The mainstream COTS smartphones based schemes are also\nsusceptible to inherent hardware based irregularities in vibration mechanism of\nthe smartphones. Moreover, the existing schemes that use microphones to sense\nvibration are prone to short-term and constant background noises (e.g.\nintermittent talking, exhaust fan, etc.) because microphones not only capture\nthe sounds created by vibration but also other interfering sounds present in\nthe environment. In this paper, we propose VibroTag, a robust and practical\nvibration based sensing scheme that works with smartphones with different\nhardware, can extract fine-grained vibration signatures of different surfaces,\nand is robust to environmental noise and hardware based irregularities. We\nimplemented VibroTag on two different Android phones and evaluated in multiple\ndifferent environments where we collected data from 4 individuals for 5 to 20\nconsecutive days. Our results show that VibroTag achieves an average accuracy\nof 86.55% while recognizing 24 different locations/surfaces, even when some of\nthose surfaces were made of similar material. VibroTag's accuracy is 37% higher\nthan the average accuracy of 49.25% achieved by one of the state-of-the-art\nIMUs based schemes, which we implemented for comparison with VibroTag.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 03:18:23 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 22:34:46 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ali", "Kamran", ""], ["Liu", "Alex X.", ""]]}, {"id": "2007.04299", "submitter": "Wilson Marc\\'ilio-Jr", "authors": "Wilson E. Marc\\'ilio-Jr, Danilo M. Eler, Rog\\'erio E. Garcia, Ronaldo\n  C. M. Correia, Rafael M. B. Rodrigues", "title": "Visual analytics of COVID-19 dissemination in S\\~ao Paulo state, Brazil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analytics techniques are useful tools to support decision-making and\ncope with increasing data, which is particularly important when monitoring\nnatural or artificial phenomena. When monitoring disease progression, visual\nanalytics approaches help decision-makers choose to understand or even prevent\ndissemination paths. In this paper, we propose a new visual analytics tool for\nmonitoring COVID-19 dissemination. We use k-nearest neighbors of cities to\nmimic neighboring cities and analyze COVID-19 dissemination based on the\ncomparison of a city under consideration and its neighborhood. Moreover, such\nanalysis is performed based on periods, which facilitates the assessment of\nisolation policies. We validate our tool by analyzing the progression of\nCOVID-19 in neighboring cities of S\\~ao Paulo state, Brazil.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 00:43:34 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 14:11:37 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 02:25:21 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Marc\u00edlio-Jr", "Wilson E.", ""], ["Eler", "Danilo M.", ""], ["Garcia", "Rog\u00e9rio E.", ""], ["Correia", "Ronaldo C. M.", ""], ["Rodrigues", "Rafael M. B.", ""]]}, {"id": "2007.04350", "submitter": "Ziran Wang", "authors": "Yongkang Liu, Ziran Wang, Kyungtae Han, Zhenyu Shou, Prashant Tiwari,\n  and John H. L. Hansen", "title": "Sensor Fusion of Camera and Cloud Digital Twin Information for\n  Intelligent Vehicles", "comments": "Accepted by the 31st IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of intelligent vehicles and Advanced Driving\nAssistance Systems (ADAS), a mixed level of human driver engagements is\ninvolved in the transportation system. Visual guidance for drivers is essential\nunder this situation to prevent potential risks. To advance the development of\nvisual guidance systems, we introduce a novel sensor fusion methodology,\nintegrating camera image and Digital Twin knowledge from the cloud. Target\nvehicle bounding box is drawn and matched by combining results of object\ndetector running on ego vehicle and position information from the cloud. The\nbest matching result, with a 79.2% accuracy under 0.7 Intersection over Union\n(IoU) threshold, is obtained with depth image served as an additional feature\nsource. Game engine-based simulation results also reveal that the visual\nguidance system could improve driving safety significantly cooperate with the\ncloud Digital Twin system.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:09:54 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Liu", "Yongkang", ""], ["Wang", "Ziran", ""], ["Han", "Kyungtae", ""], ["Shou", "Zhenyu", ""], ["Tiwari", "Prashant", ""], ["Hansen", "John H. L.", ""]]}, {"id": "2007.04361", "submitter": "Daniel Sullivan", "authors": "Daniel Sullivan, Carlos Caminha, Victor Dantas, Elizabeth Furtado,\n  Vasco Furtado, Virg\\'ilio Almeida", "title": "Understanding the impact of the alphabetical ordering of names in user\n  interfaces: a gender bias analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Listing people alphabetically on an electronic output device is a traditional\ntechnique, since alphabetical order is easily perceived by users and\nfacilitates access to information. However, this apparently harmless technique,\nespecially when the list is ordered by first name, needs to be used with\ncaution by designers and programmers. We show, via empirical data analysis,\nthat when an interface displays people's first name in alphabetical order in\nseveral pages/screens, each page/screen may have imbalances in respect to\ngender of its Top-k individuals.k represents the size of the list of names\nvisualized first, which may be the number of names that fits in a screen page\nof a certain device.The research work was carried out with the analysis of\nactual datasets of names of five different countries. Each dataset has a person\nname and the frequency of adoption of the name in the country.Our analysis\nshows that, even though all countries have exhibit imbalance problems, the\nsamples of individuals with Brazilian and Spanish first names are more prone to\ngender imbalance among their Top-k individuals. These results can be useful for\ndesigners and engineers to construct information systems that avoid gender bias\ninduction.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:40:21 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Sullivan", "Daniel", ""], ["Caminha", "Carlos", ""], ["Dantas", "Victor", ""], ["Furtado", "Elizabeth", ""], ["Furtado", "Vasco", ""], ["Almeida", "Virg\u00edlio", ""]]}, {"id": "2007.04380", "submitter": "Simon Flandin", "authors": "Simon Flandin (UNIGE, RIFT, FPSE), Deli Salini (IUFFP Lugano, CRAFT),\n  Art\\'emis Drakos (UNIGE, CRAFT, EDF R&D, FPSE), Germain Poizat (UNIGE, CRAFT,\n  RIFT, FPSE)", "title": "Training design fostering the emergence of new meanings toward\n  unprecedented and critical events", "comments": "@ctivit{\\'e}s, Association Recherches et Pratiques sur les\n  ACTivit{\\'e}s, A para{\\^i}tre, in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research is part of a technological research program in adult education\nconducted in reference to the \"course of action\" program. Using the\nactivity-sign hypothesis of this programme, training situations are thought as\nopportunities to perturb/relaunch the participants'dynamics of meaning. Our\ncontribution aims to (i) improve the conceptualization of training situations\nthat are thought as aids to the understanding and transformation of the\nparticipants situations, and (ii) derive cross-cutting design principles that\nallow the enactment of these training situations in different contexts. We rely\non the analysis of training programs aiming either at the management or at the\novercoming of events experienced as unprecedented and critical by the\nindividuals concerned. Depending on the case, these training programs have a\n\"restorative\" aim (resolving of impasse situations), or have a \"preparatory\"\naim (prefiguration of crisis situations). The design principles of these\ntraining programs and their effects are analysed with the conceptual tools\ndeveloped within the course-of-action, integrating two additional dimensions:\nfictional and event-driven. The conditions for the emergence of new meanings\nare described in terms of abduction, either for reparative purposes (resolution\nof deadlock situations) or for preparatory purposes (prefiguration of crisis\nsituations). The contribution to course-of-action program and to research on\nadult education is discussed through the prism of the hypothesis of\nactivity-sign.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:00:51 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Flandin", "Simon", "", "UNIGE, RIFT, FPSE"], ["Salini", "Deli", "", "IUFFP Lugano, CRAFT"], ["Drakos", "Art\u00e9mis", "", "UNIGE, CRAFT, EDF R&D, FPSE"], ["Poizat", "Germain", "", "UNIGE, CRAFT,\n  RIFT, FPSE"]]}, {"id": "2007.04399", "submitter": "Petros Spachos", "authors": "Pai Chet Ng, Petros Spachos, Stefano Gregori, Konstantinos Plataniotis", "title": "Epidemic Exposure Notification with Smartwatch: A Proximity-Based\n  Privacy-Preserving Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.LG cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Businesses planning for the post-pandemic world are looking for innovative\nways to protect the health and welfare of their employees and customers.\nWireless technologies can play a key role in assisting contact tracing to\nquickly halt a local infection outbreak and prevent further spread. In this\nwork, we present a wearable proximity and exposure notification solution based\non a smartwatch that also promotes safe physical distancing in business,\nhospitality, or recreational facilities. Our proximity-based privacy-preserving\ncontact tracing (P$^3$CT) leverages the Bluetooth Low Energy (BLE) technology\nfor reliable proximity sensing, and an ambient signature protocol for\npreserving identity. Proximity sensing exploits the received signal strength\n(RSS) to detect the user's interaction and thus classifying them into low- or\nhigh-risk with respect to a patient diagnosed with an infectious disease. More\nprecisely, a user is notified of their exposure based on their interactions, in\nterms of distance and time, with a patient. Our privacy-preserving protocol\nuses the ambient signatures to ensure that users' identities be anonymized. We\ndemonstrate the feasibility of our proposed solution through extensive\nexperimentation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 19:55:33 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Ng", "Pai Chet", ""], ["Spachos", "Petros", ""], ["Gregori", "Stefano", ""], ["Plataniotis", "Konstantinos", ""]]}, {"id": "2007.04444", "submitter": "Tanusree Sharma", "authors": "Tanusree Sharma and Masooda Bashir", "title": "Are PETs (Privacy Enhancing Technologies) Giving Protection for\n  Smartphones? -- A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With smartphone technologies enhanced way of interacting with the world\naround us, it has also been paving the way for easier access to our private and\npersonal information. This has been amplified by the existence of numerous\nembedded sensors utilized by millions of apps to users. While mobile apps have\npositively transformed many aspects of our lives with new functionalities, many\nof these applications are taking advantage of vast amounts of data, privacy\napps, a form of Privacy Enhancing Technology can be an effective privacy\nmanagement tool for smartphones. To protect against vulnerabilities related to\nthe collection, storage, and sharing of sensitive data, developers are building\nnumerous privacy apps. However, there has been a lack of discretion in this\nparticular area which calls for a proper assessment to understand the\nfar-reaching utilization of these apps among users. During this process we have\nconducted an evaluation of the most popular privacy apps from our total\ncollection of five hundred and twelve to demonstrate their functionality\nspecific data protections they are claiming to offer, both technologically and\nconventionally, measuring up to standards. Taking their offered security\nfunctionalities as a scale, we conducted forensic experiments to indicate where\nthey are failing to be consistent in maintaining protection. For legitimate\nvalidation of security gaps in assessed privacy apps, we have also utilized\nNIST and OWASP guidelines. We believe this study will be efficacious for\ncontinuous improvement and can be considered as a foundation towards a common\nstandard for privacy and security measures for an app's development stage.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:33:00 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Sharma", "Tanusree", ""], ["Bashir", "Masooda", ""]]}, {"id": "2007.04495", "submitter": "Dominic Kao", "authors": "Dominic Kao, Christos Mousas, Alejandra J. Magana, D. Fox Harrell,\n  Rabindra Ratan, Edward F. Melcer, Brett Sherrick, Paul Parsons, Dmitri A.\n  Gusev", "title": "Hack.VR: A Programming Game in Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we describe Hack.VR, an object-oriented programming game in\nvirtual reality. Hack.VR uses a VR programming language in which nodes\nrepresent functions and node connections represent data flow. Using this\nprogramming framework, players reprogram VR objects such as elevators, robots,\nand switches. Hack.VR has been designed to be highly interactable both\nphysically and semantically.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 01:14:25 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 21:47:25 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Kao", "Dominic", ""], ["Mousas", "Christos", ""], ["Magana", "Alejandra J.", ""], ["Harrell", "D. Fox", ""], ["Ratan", "Rabindra", ""], ["Melcer", "Edward F.", ""], ["Sherrick", "Brett", ""], ["Parsons", "Paul", ""], ["Gusev", "Dmitri A.", ""]]}, {"id": "2007.04604", "submitter": "Sao Mai Nguyen", "authors": "Linda Nanan Vall\\'ee (ESATIC), Christophe Lohr, Sao Mai Nguyen (IMT\n  Atlantique), Ioannis Kanellos (IMT Atlantique - INFO), O. Asseu (ESATIC)", "title": "Building an Automated Gesture Imitation Game for Teenagers with ASD", "comments": null, "journal-ref": "Far East Journal of Electronics and Communications, 2019, 22,\n  pp.19 - 28", "doi": "10.17654/EC023010001", "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autism spectrum disorder is a neurodevelopmental condition that includes\nissues with communication and social interactions. People with ASD also often\nhave restricted interests and repetitive behaviors. In this paper we build\npreliminary bricks of an automated gesture imitation game that will aim at\nimproving social interactions with teenagers with ASD. The structure of the\ngame is presented, as well as support tools and methods for skeleton detection\nand imitation learning. The game shall later be implemented using an\ninteractive robot.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 07:27:43 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Vall\u00e9e", "Linda Nanan", "", "ESATIC"], ["Lohr", "Christophe", "", "IMT\n  Atlantique"], ["Nguyen", "Sao Mai", "", "IMT\n  Atlantique"], ["Kanellos", "Ioannis", "", "IMT Atlantique - INFO"], ["Asseu", "O.", "", "ESATIC"]]}, {"id": "2007.04760", "submitter": "Lynsay Shepherd", "authors": "Lynsay A. Shepherd, Stefano De Paoli, Jim Conacher", "title": "Human-Computer Interaction Considerations When Developing Cyber Ranges", "comments": "5 pages, short discussion paper", "journal-ref": "2020 International Journal of Information Security and Cybercrime\n  9(2), pp.28-32", "doi": "10.19107/IJISC.2020.02.04", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of cyber-attacks are continuing to rise globally. It is therefore\nvital for organisations to develop the necessary skills to secure their assets\nand to protect critical national infrastructure. In this short paper, we\noutline upon human-computer interaction elements which should be considered\nwhen developing a cybersecurity training platform, in an effort to maintain\nlevels of user engagement. We provide an overview of existing training\nplatforms before covering specialist cyber ranges. Aspects of human-computer\ninteraction are noted with regards to their relevance in the context of cyber\nranges. We conclude with design suggestions when developing a cyber range\nplatform.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:08:05 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Shepherd", "Lynsay A.", ""], ["De Paoli", "Stefano", ""], ["Conacher", "Jim", ""]]}, {"id": "2007.04831", "submitter": "Nan Gao", "authors": "Nan Gao, Wei Shao, Mohammad Saiedur Rahaman, Flora D. Salim", "title": "n-Gage: Predicting in-class Emotional, Behavioural and Cognitive\n  Engagement in the Wild", "comments": "This paper has been accepted by the Proceedings of the ACM on\n  Interactive, Mobile, Wearable and Ubiquitous Technologies (IMWUT) volume 4\n  issue 3, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of student engagement has attracted growing interests to address\nproblems such as low academic performance, disaffection, and high dropout\nrates. Existing approaches to measuring student engagement typically rely on\nsurvey-based instruments. While effective, those approaches are time-consuming\nand labour-intensive. Meanwhile, both the response rate and quality of the\nsurvey are usually poor. As an alternative, in this paper, we investigate\nwhether we can infer and predict engagement at multiple dimensions, just using\nsensors. We hypothesize that multidimensional student engagement can be\ntranslated into physiological responses and activity changes during the class,\nand also be affected by the environmental changes. Therefore, we aim to explore\nthe following questions: Can we measure the multiple dimensions of high school\nstudent's learning engagement including emotional, behavioural and cognitive\nengagement with sensing data in the wild? Can we derive the activity,\nphysiological, and environmental factors contributing to the different\ndimensions of student engagement? If yes, which sensors are the most useful in\ndifferentiating each dimension of the engagement? Then, we conduct an in-situ\nstudy in a high school from 23 students and 6 teachers in 144 classes over 11\ncourses for 4 weeks. We present the n-Gage, a student engagement sensing system\nusing a combination of sensors from wearables and environments to automatically\ndetect student in-class multidimensional learning engagement. Experiment\nresults show that n-Gage can accurately predict multidimensional student\nengagement in real-world scenarios with an average MAE of 0.788 and RMSE of\n0.975 using all the sensors. We also show a set of interesting findings of how\ndifferent factors (e.g., combinations of sensors, school subjects, CO2 level)\naffect each dimension of the student learning engagement.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:27:21 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 12:25:58 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 11:08:57 GMT"}, {"version": "v4", "created": "Thu, 23 Jul 2020 03:13:37 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Gao", "Nan", ""], ["Shao", "Wei", ""], ["Rahaman", "Mohammad Saiedur", ""], ["Salim", "Flora D.", ""]]}, {"id": "2007.04848", "submitter": "Angus Main Mr", "authors": "Angus Main, Mick Grierson", "title": "Guru, Partner, or Pencil Sharpener? Understanding Designers' Attitudes\n  Towards Intelligent Creativity Support Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creativity Support Tools (CST) aim to enhance human creativity, but the\ndeeply personal and subjective nature of creativity makes the design of\nuniversal support tools challenging. Individuals develop personal approaches to\ncreativity, particularly in the context of commercial design where signature\nstyles and techniques are valuable commodities. Artificial Intelligence (AI)\nand Machine Learning (ML) techniques could provide a means of creating\n'intelligent' CST which learn and adapt to personal styles of creativity.\nIdentifying what kind of role such tools could play in the design process\nrequires a better understanding of designers' attitudes towards working with\nAI, and their willingness to include it in their personal creative process.\nThis paper details the results of a survey of professional designers which\nindicates a positive and pragmatic attitude towards collaborating with AI\ntools, and a particular opportunity for incorporating them in the research\nstages of a design project.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:52:52 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Main", "Angus", ""], ["Grierson", "Mick", ""]]}, {"id": "2007.04882", "submitter": "Jonas Hasbach", "authors": "Jonas D. Hasbach, Maren Bennewitz", "title": "A Neuro-inspired Theory of Joint Human-Swarm Interaction", "comments": "ICRA Workshop on Human-Swarm Interaction 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MA cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-swarm interaction (HSI) is an active research challenge in the realms\nof swarm robotics and human-factors engineering. Here we apply a cognitive\nsystems engineering perspective and introduce a neuro-inspired joint systems\ntheory of HSI. The mindset defines predictions for adaptive, robust and\nscalable HSI dynamics and therefore has the potential to inform human-swarm\nloop design.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 15:34:22 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Hasbach", "Jonas D.", ""], ["Bennewitz", "Maren", ""]]}, {"id": "2007.04912", "submitter": "Mokhtar Ben Henda", "authors": "Mokhtar Ben Henda", "title": "GUIDE for a blended learning system", "comments": "104 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This guide is proposed as an operational instrument for CONFRASIE member\nuniversities (Regional Rectors' Conference of AUF member institutions in\nPacific-Asia) in their projects to set up a blended learning system for\nbachelor's, Master's and Doctorate degrees. It is structured in sections\ncorresponding to a complete process of operationalizing a blended learning\nsystem, from the definition of an implementation strategy to the assessment of\nresults. This guide covers also conceptual and theoretical fundamentals of\ndistance learning as well as methodological and procedural tips and\nrecommendations on how to implement blended learning in an existing\nface-to-face curriculum. It can serve for leaders of educational ICT-based\nprojects as a guidance document to take pedagogical, technological and\nmethodological decisions for the development, monitoring and assessment of a\nblended learning curricula. This guide can be augmented by other standards,\ntool and software manuals offering further training materials and guidelines on\neducational skills ans=d services.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 16:21:14 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Henda", "Mokhtar Ben", ""]]}, {"id": "2007.04916", "submitter": "Salom\\'on Wollenstein-Betech", "authors": "Salom\\'on Wollenstein-Betech, Christian Muise, Christos G. Cassandras,\n  Ioannis Ch. Paschalidis, Yasaman Khazaeni", "title": "Explainability of Intelligent Transportation Systems using Knowledge\n  Compilation: a Traffic Light Controller Case", "comments": "Proc. IEEE Int. Conf. on Intelligent Transportation Systems, Rhodes,\n  Greece, 2020. (In Press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LO cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usage of automated controllers which make decisions on an environment are\nwidespread and are often based on black-box models. We use Knowledge\nCompilation theory to bring explainability to the controller's decision given\nthe state of the system. For this, we use simulated historical state-action\ndata as input and build a compact and structured representation which relates\nstates with actions. We implement this method in a Traffic Light Control\nscenario where the controller selects the light cycle by observing the presence\n(or absence) of vehicles in different regions of the incoming roads.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 16:27:47 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Wollenstein-Betech", "Salom\u00f3n", ""], ["Muise", "Christian", ""], ["Cassandras", "Christos G.", ""], ["Paschalidis", "Ioannis Ch.", ""], ["Khazaeni", "Yasaman", ""]]}, {"id": "2007.04950", "submitter": "Nitish Bhardwaj", "authors": "Alpana Dubey, Nitish Bhardwaj, Kumar Abhinav, Suma Mani Kuriakose,\n  Sakshi Jain and Veenu Arora", "title": "AI Assisted Apparel Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fashion is a fast-changing industry where designs are refreshed at large\nscale every season. Moreover, it faces huge challenge of unsold inventory as\nnot all designs appeal to customers. This puts designers under significant\npressure. Firstly, they need to create innumerous fresh designs. Secondly, they\nneed to create designs that appeal to customers. Although we see advancements\nin approaches to help designers analyzing consumers, often such insights are\ntoo many. Creating all possible designs with those insights is time consuming.\nIn this paper, we propose a system of AI assistants that assists designers in\ntheir design journey. The proposed system assists designers in analyzing\ndifferent selling/trending attributes of apparels. We propose two design\ngeneration assistants namely Apparel-Style-Merge and Apparel-Style-Transfer.\nApparel-Style-Merge generates new designs by combining high level components of\napparels whereas Apparel-Style-Transfer generates multiple customization of\napparels by applying different styles, colors and patterns. We compose a new\ndataset, named DeepAttributeStyle, with fine-grained annotation of landmarks of\ndifferent apparel components such as neck, sleeve etc. The proposed system is\nevaluated on a user group consisting of people with and without design\nbackground. Our evaluation result demonstrates that our approach generates high\nquality designs that can be easily used in fabrication. Moreover, the suggested\ndesigns aid to the designers creativity.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:24:40 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 17:14:17 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Dubey", "Alpana", ""], ["Bhardwaj", "Nitish", ""], ["Abhinav", "Kumar", ""], ["Kuriakose", "Suma Mani", ""], ["Jain", "Sakshi", ""], ["Arora", "Veenu", ""]]}, {"id": "2007.04959", "submitter": "Zackory Erickson", "authors": "Zackory Erickson, Yijun Gu, Charles C. Kemp", "title": "Assistive VR Gym: Interactions with Real People to Improve Virtual\n  Assistive Robots", "comments": "IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN 2020), 8 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Versatile robotic caregivers could benefit millions of people worldwide,\nincluding older adults and people with disabilities. Recent work has explored\nhow robotic caregivers can learn to interact with people through physics\nsimulations, yet transferring what has been learned to real robots remains\nchallenging. Virtual reality (VR) has the potential to help bridge the gap\nbetween simulations and the real world. We present Assistive VR Gym (AVR Gym),\nwhich enables real people to interact with virtual assistive robots. We also\nprovide evidence that AVR Gym can help researchers improve the performance of\nsimulation-trained assistive robots with real people. Prior to AVR Gym, we\ntrained robot control policies (Original Policies) solely in simulation for\nfour robotic caregiving tasks (robot-assisted feeding, drinking, itch\nscratching, and bed bathing) with two simulated robots (PR2 from Willow Garage\nand Jaco from Kinova). With AVR Gym, we developed Revised Policies based on\ninsights gained from testing the Original policies with real people. Through a\nformal study with eight participants in AVR Gym, we found that the Original\npolicies performed poorly, the Revised policies performed significantly better,\nand that improvements to the biomechanical models used to train the Revised\npolicies resulted in simulated people that better match real participants.\nNotably, participants significantly disagreed that the Original policies were\nsuccessful at assistance, but significantly agreed that the Revised policies\nwere successful at assistance. Overall, our results suggest that VR can be used\nto improve the performance of simulation-trained control policies with real\npeople without putting people at risk, thereby serving as a valuable stepping\nstone to real robotic assistance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:42:06 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 13:11:42 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Erickson", "Zackory", ""], ["Gu", "Yijun", ""], ["Kemp", "Charles C.", ""]]}, {"id": "2007.04983", "submitter": "Rolf Huesmann", "authors": "Rolf Huesmann, Alexander Zeier, Andreas Heinemann, Alexander Wiesmaier", "title": "Zur Benutzbarkeit und Verwendung von API-Dokumentationen", "comments": "13 pages, in German", "journal-ref": null, "doi": "10.18420/muc2020-ws119-002", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A good documentation is essential for a good usability of (security) APIs,\ni.e. especially for the correct use of the APIs. Requirements for good\ndocumentation of APIs have been described in several papers, but there is no\ntechnical implementation (hereinafter referred to as a documentation system)\nthat implements these requirements. The requirements can be divided into\nrequirements for the documentation system and requirements for the\ndocumentation content. Out of 13 identified requirements for a documentation\nsystem itself, 9 were implemented in a prototype and evaluated in a user study\nwith 22 test persons using a cryptographic API. It turned out that the\nimplementation of the requirement 'Enable quick use of the API' depends on the\none hand on the quality of the content entered, but on the other hand also\nincludes 5 other requirements or their implementation. The two other\nimplemented requirements ('classic reference' and 'question and answer\nfunction') were hardly or not at all used by the test persons. Their usefulness\nand relevance should be investigated in a long-term study.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 11:57:44 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Huesmann", "Rolf", ""], ["Zeier", "Alexander", ""], ["Heinemann", "Andreas", ""], ["Wiesmaier", "Alexander", ""]]}, {"id": "2007.05286", "submitter": "Kieran Woodward Mr", "authors": "Kieran Woodward, Eiman Kanjo, David J Brown and Becky Inkster", "title": "TangToys: Smart Toys that can Communicate and Improve Children's\n  Wellbeing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Children can find it challenging to communicate their emotions especially\nwhen experiencing mental health challenges. Technological solutions may help\nchildren communicate digitally and receive support from one another as advances\nin networking and sensors enable the real-time transmission of physical\ninteractions. In this work, we pursue the design of multiple tangible user\ninterfaces designed for children containing multiple sensors and feedback\nactuators. Bluetooth is used to provide communication between Tangible Toys\n(TangToys) enabling peer to peer support groups to be developed and allowing\nfeedback to be issued whenever other children are nearby. TangToys can provide\na non-intrusive means for children to communicate their wellbeing through play.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 10:13:34 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Woodward", "Kieran", ""], ["Kanjo", "Eiman", ""], ["Brown", "David J", ""], ["Inkster", "Becky", ""]]}, {"id": "2007.05373", "submitter": "Joris Dugueperoux", "authors": "Joris Dugu\\'ep\\'eroux (DRUID), Tristan Allard (DRUID)", "title": "From Task Tuning to Task Assignment in Privacy-Preserving Crowdsourcing\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.DB cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specialized worker profiles of crowdsourcing platforms may contain a large\namount of identifying and possibly sensitive personal information (e.g.,\npersonal preferences, skills, available slots, available devices) raising\nstrong privacy concerns. This led to the design of privacy-preserving\ncrowdsourcing platforms, that aim at enabling efficient crowd-sourcing\nprocesses while providing strong privacy guarantees even when the platform is\nnot fully trusted. In this paper, we propose two contributions. First, we\npropose the PKD algorithm with the goal of supporting a large variety of\naggregate usages of worker profiles within a privacy-preserving crowdsourcing\nplatform. The PKD algorithm combines together homomorphic encryption and\ndifferential privacy for computing (perturbed) partitions of the\nmulti-dimensional space of skills of the actual population of workers and a\n(perturbed) COUNT of workers per partition. Second, we propose to benefit from\nrecent progresses in Private Information Retrieval techniques in order to\ndesign a solution to task assignment that is both private and affordable. We\nperform an in-depth study of the problem of using PIR techniques for proposing\ntasks to workers, show that it is NP-Hard, and come up with the PKD PIR Packing\nheuristic that groups tasks together according to the partitioning output by\nthe PKD algorithm. In a nutshell, we design the PKD algorithm and the PKD PIR\nPacking heuristic, we prove formally their security against honest-but-curious\nworkers and/or platform, we analyze their complexities, and we demonstrate\ntheir quality and affordability in real-life scenarios through an extensive\nexperimental evaluation performed over both synthetic and realistic datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 13:21:18 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Dugu\u00e9p\u00e9roux", "Joris", "", "DRUID"], ["Allard", "Tristan", "", "DRUID"]]}, {"id": "2007.05394", "submitter": "Sao Mai Nguyen", "authors": "Linda Nanan Vall\\'ee (ESATIC), Sao Mai Nguyen (IMT Atlantique, IMT\n  Atlantique - INFO, Lab-STICC, Flowers), Christophe Lohr (Lab-STICC, IMT\n  Atlantique - INFO, IMT Atlantique), Ioannis Kanellos (Lab-STICC, IMT\n  Atlantique - INFO, IMT Atlantique), Olivier Asseu (ESATIC)", "title": "How An Automated Gesture Imitation Game Can Improve Social Interactions\n  With Teenagers With ASD", "comments": null, "journal-ref": "IEEE ICRA Workshop on Social Robotics for Neurodevelopmental\n  Disorders, Jun 2020, Paris, France", "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the outlook of improving communication and social abilities of people\nwith ASD, we propose to extend the paradigm of robot-based imitation games to\nASD teenagers. In this paper, we present an interaction scenario adapted to ASD\nteenagers, propose a computational architecture using the latest machine\nlearning algorithm Openpose for human pose detection, and present the results\nof our basic testing of the scenario with human caregivers. These results are\npreliminary due to the number of session (1) and participants (4). They include\na technical assessment of the performance of Openpose, as well as a preliminary\nuser study to confirm our game scenario could elicit the expected response from\nsubjects.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 14:01:24 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Vall\u00e9e", "Linda Nanan", "", "ESATIC"], ["Nguyen", "Sao Mai", "", "IMT Atlantique, IMT\n  Atlantique - INFO, Lab-STICC, Flowers"], ["Lohr", "Christophe", "", "Lab-STICC, IMT\n  Atlantique - INFO, IMT Atlantique"], ["Kanellos", "Ioannis", "", "Lab-STICC, IMT\n  Atlantique - INFO, IMT Atlantique"], ["Asseu", "Olivier", "", "ESATIC"]]}, {"id": "2007.05551", "submitter": "Yang Liu", "authors": "Yang Liu, Alex Kale, Tim Althoff, and Jeffrey Heer", "title": "Boba: Authoring and Visualizing Multiverse Analyses", "comments": "submitted to IEEE Transactions on Visualization and Computer Graphics\n  (Proc. VAST)", "journal-ref": null, "doi": "10.1109/TVCG.2020.3028985", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiverse analysis is an approach to data analysis in which all \"reasonable\"\nanalytic decisions are evaluated in parallel and interpreted collectively, in\norder to foster robustness and transparency. However, specifying a multiverse\nis demanding because analysts must manage myriad variants from a cross-product\nof analytic decisions, and the results require nuanced interpretation. We\ncontribute Boba: an integrated domain-specific language (DSL) and visual\nanalysis system for authoring and reviewing multiverse analyses. With the Boba\nDSL, analysts write the shared portion of analysis code only once, alongside\nlocal variations defining alternative decisions, from which the compiler\ngenerates a multiplex of scripts representing all possible analysis paths. The\nBoba Visualizer provides linked views of model results and the multiverse\ndecision space to enable rapid, systematic assessment of consequential\ndecisions and robustness, including sampling uncertainty and model fit. We\ndemonstrate Boba's utility through two data analysis case studies, and reflect\non challenges and design opportunities for multiverse analysis software.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 18:13:06 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 23:40:05 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Liu", "Yang", ""], ["Kale", "Alex", ""], ["Althoff", "Tim", ""], ["Heer", "Jeffrey", ""]]}, {"id": "2007.05751", "submitter": "Cheng Gong", "authors": "Zirui Li, Chao Lu, Cheng Gong, Cheng Gong, Jinghang Li, Lianzhen Wei", "title": "Driver Behavior Modelling at the Urban Intersection via Canonical\n  Correlation Analysis", "comments": "2020 3rd IEEE International Conference on Unmanned Systems (ICUS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The urban intersection is a typically dynamic and complex scenario for\nintelligent vehicles, which exists a variety of driving behaviors and traffic\nparticipants. Accurately modelling the driver behavior at the intersection is\nessential for intelligent transportation systems (ITS). Previous researches\nmainly focus on using attention mechanism to model the degree of correlation.\nIn this research, a canonical correlation analysis (CCA)-based framework is\nproposed. The value of canonical correlation is used for feature selection.\nGaussian mixture model and Gaussian process regression are applied for driver\nbehavior modelling. Two experiments using simulated and naturalistic driving\ndata are designed for verification. Experimental results are consistent with\nthe driver's judgment. Comparative studies show that the proposed framework can\nobtain a better performance.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 11:34:22 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Li", "Zirui", ""], ["Lu", "Chao", ""], ["Gong", "Cheng", ""], ["Gong", "Cheng", ""], ["Li", "Jinghang", ""], ["Wei", "Lianzhen", ""]]}, {"id": "2007.05801", "submitter": "Ravi Tejwani", "authors": "Ravi Tejwani, Felipe Moreno, Sooyeon Jeong, Hae Won Park, Cynthia\n  Breazeal", "title": "Migratable AI: Effect of identity and information migration on users\n  perception of conversational AI agents", "comments": "Accepted to RO-MAN 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational AI agents are proliferating, embodying a range of devices such\nas smart speakers, smart displays, robots, cars, and more. We can envision a\nfuture where a personal conversational agent could migrate across different\nform factors and environments to always accompany and assist its user to\nsupport a far more continuous, personalized, and collaborative experience. This\nopens the question of what properties of a conversational AI agent migrates\nacross forms, and how it would impact user perception. To explore this, we\ndeveloped a Migratable AI system where a user's information and/or the agent's\nidentity can be preserved as it migrates across form factors to help its user\nwith a task. We designed a 2x2 between-subjects study to explore the effects of\ninformation migration and identity migration on user perceptions of trust,\ncompetence, likeability, and social presence. Our results suggest that identity\nmigration had a positive effect on trust, competence, and social presence,\nwhile information migration had a positive effect on trust, competence, and\nlikeability. Overall, users report the highest trust, competence, likeability,\nand social presence towards the conversational agent when both identity and\ninformation were migrated across embodiments.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 15:46:37 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 22:36:36 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Tejwani", "Ravi", ""], ["Moreno", "Felipe", ""], ["Jeong", "Sooyeon", ""], ["Park", "Hae Won", ""], ["Breazeal", "Cynthia", ""]]}, {"id": "2007.05848", "submitter": "Claudia Flores-Saviaga", "authors": "Claudia Flores-Saviaga, Saiph Savage", "title": "Fighting Disaster Misinformation in Latin America: The #19S Mexican\n  Earthquake Case Study", "comments": null, "journal-ref": "Springer - Personal and Ubiquitous Computing 2020", "doi": "10.1007/s00779-020-01411-5", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms have been extensively used during natural disasters.\nHowever, most prior work has lacked focus on studying their usage during\ndisasters in the Global South, where Internet access and social media\nutilization differs from developing countries. In this paper, we study how\nsocial media was used in the aftermath of the 7.1-magnitude earthquake that hit\nMexico on September 19 of 2017 (known as the #19S earthquake). We conduct an\nanalysis of how participants utilized social media platforms in the #19S\naftermath. Our research extends investigations of crisis informatics by: 1)\nexamining how participants used different social media platforms in the\naftermath of a natural disaster in a Global South country; 2) uncovering how\nindividuals developed their own processes to verify news reports using an\non-the-ground citizen approach; 3) revealing how people developed their own\nmechanisms to deal with outdated information. For this, we surveyed 356 people.\nAdditionally, we analyze one month of activity from: Facebook (12,606 posts),\nTwitter (2,909,109 tweets), Slack (28,782 messages), and GitHub (2,602\ncommits). This work offers a multi-platform view on user behavior to coordinate\nrelief efforts, reduce the spread of misinformation and deal with obsolete\ninformation which seems to have been essential to help in the coordination and\nefficiency of relief efforts. Finally, based on our findings, we make\nrecommendations for technology design to improve the effectiveness of social\nmedia use during crisis response efforts and mitigate the spread of\nmisinformation across social media platforms.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 20:37:14 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Flores-Saviaga", "Claudia", ""], ["Savage", "Saiph", ""]]}, {"id": "2007.05902", "submitter": "Kartik Chugh", "authors": "Kartik Chugh, Andrea Y. Solis, Thomas D. LaToza", "title": "Editable AI: Mixed Human-AI Authoring of Code Patterns", "comments": null, "journal-ref": "2019 IEEE Symposium on Visual Languages and Human-Centric\n  Computing (VL/HCC), Memphis, TN, USA, 2019, pp. 35-43", "doi": "10.1109/VLHCC.2019.8818871", "report-no": null, "categories": "cs.HC cs.AI cs.LG cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developers authoring HTML documents define elements following patterns which\nestablish and reflect the visual structure of a document, such as making all\nimages in a footer the same height by applying a class to each. To surface\nthese patterns to developers and support developers in authoring consistent\nwith these patterns, we propose a mixed human-AI technique for creating code\npatterns. Patterns are first learned from individual HTML documents through a\ndecision tree, generating a representation which developers may view and edit.\nCode patterns are used to offer developers autocomplete suggestions, list\nexamples, and flag violations. To evaluate our technique, we conducted a user\nstudy in which 24 participants wrote, edited, and corrected HTML documents. We\nfound that our technique enabled developers to edit and correct documents more\nquickly and create, edit, and correct documents more successfully.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 03:49:42 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Chugh", "Kartik", ""], ["Solis", "Andrea Y.", ""], ["LaToza", "Thomas D.", ""]]}, {"id": "2007.05927", "submitter": "Huang Yanpei", "authors": "Yanpei Huang, Wenjie Lai, Lin Cao, Jiajun Liu, Xiaoguo Li, Etienne\n  Burdet and Soo Jay Phee", "title": "A Three-limb Teleoperated Robotic System with Foot Control for Flexible\n  Endoscopic Surgery", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible endoscopy requires high skills to manipulate both the endoscope and\nassociated instruments. In most robotic flexible endoscopic systems, the\nendoscope and instruments are controlled separately by two operators, which may\nresult in communication errors and inefficient operation. We present a novel\nteleoperation robotic endoscopic system that can be commanded by a surgeon\nalone. This 13 degrees-of-freedom (DoF) system integrates a foot-controlled\nrobotic flexible endoscope and two hand-controlled robotic endoscopic\ninstruments (a robotic grasper and a robotic cauterizing hook). A\nfoot-controlled human-machine interface maps the natural foot gestures to the\n4-DoF movements of the endoscope, and two hand-controlled interfaces map the\nmovements of the two hands to the two instruments individually. The proposed\nrobotic system was validated in an ex-vivo experiment carried out by six\nsubjects, where foot control was also compared with a sequential clutch-based\nhand control scheme. The participants could successfully teleoperate the\nendoscope and the two instruments to cut the tissues at scattered target areas\nin a porcine stomach. Foot control yielded 43.7% faster task completion and\nrequired less mental effort as compared to the clutch-based hand control\nscheme. The system introduced in this paper is intuitive for three-limb\nmanipulation even for operators without experience of handling the endoscope\nand robotic instruments. This three-limb teleoperated robotic system enables\none surgeon to intuitively control three endoscopic tools which normally\nrequire two operators, leading to reduced manpower, less communication errors,\nand improved efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 07:05:40 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Huang", "Yanpei", ""], ["Lai", "Wenjie", ""], ["Cao", "Lin", ""], ["Liu", "Jiajun", ""], ["Li", "Xiaoguo", ""], ["Burdet", "Etienne", ""], ["Phee", "Soo Jay", ""]]}, {"id": "2007.06062", "submitter": "Yuchao Ma", "authors": "Yuchao Ma, Andrew T. Campbell, Diane J. Cook, John Lach, Shwetak N.\n  Patel, Thomas Ploetz, Majid Sarrafzadeh, Donna Spruijt-Metz, Hassan\n  Ghasemzadeh", "title": "Transfer Learning for Activity Recognition in Mobile Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While activity recognition from inertial sensors holds potential for mobile\nhealth, differences in sensing platforms and user movement patterns cause\nperformance degradation. Aiming to address these challenges, we propose a\ntransfer learning framework, TransFall, for sensor-based activity recognition.\nTransFall's design contains a two-tier data transformation, a label estimation\nlayer, and a model generation layer to recognize activities for the new\nscenario. We validate TransFall analytically and empirically.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 18:32:46 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ma", "Yuchao", ""], ["Campbell", "Andrew T.", ""], ["Cook", "Diane J.", ""], ["Lach", "John", ""], ["Patel", "Shwetak N.", ""], ["Ploetz", "Thomas", ""], ["Sarrafzadeh", "Majid", ""], ["Spruijt-Metz", "Donna", ""], ["Ghasemzadeh", "Hassan", ""]]}, {"id": "2007.06218", "submitter": "Eric Rosen", "authors": "Eric Rosen, Stefanie Tellex, Geroge Konidaris", "title": "Steps Towards Best Practices For Robot Videos", "comments": "4 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are unwritten guidelines for how to make robot videos that researchers\nlearn from their advisors and pass onto their students. We believe that it is\nimportant for the community to collaboratively discuss and develop a standard\nset of best practices when making robot. We suggest a starting set of maxims\nfor best robot video practices, and highlight positive examples from the\ncommunity and negative examples only from videos made by the authors of this\narticle. In addition, we offer a checklist that we hope can act as an document\nthat can be given to robotic researchers to inform them of how to make robot\nvideos that truthfully characterize what a robot can and can not do. We\nconsider this a first draft, and are looking for feedback from the community as\nwe refine and grow our maxims and checklist.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:39:06 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Rosen", "Eric", ""], ["Tellex", "Stefanie", ""], ["Konidaris", "Geroge", ""]]}, {"id": "2007.06237", "submitter": "Madison Elliott", "authors": "Rebecca Vandenberg, Madison Elliott, Nicholas Harvey, and Tamara\n  Munzner", "title": "LSQT: Low-Stretch Quasi-Trees for Bundling and Layout", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce low-stretch trees to the visualization community with LSQT, our\nnovel technique that uses quasi-trees for both layout and edge bundling. Our\nmethod offers strong computational speed and complexity guarantees by\nleveraging the convenient properties of low-stretch trees, which accurately\nreflect the topological structure of arbitrary graphs with superior fidelity\ncompared to arbitrary spanning trees. Low-stretch quasi-trees also have\nprovable sparseness guarantees, providing algorithmic support for aggressive\nde-cluttering of hairball graphs. LSQT does not rely on previously computed\nvertex positions and computes bundles based on topological structure before any\ngeometric layout occurs. Edge bundles are computed efficiently and stored in an\nexplicit data structure that supports sophisticated visual encoding and\ninteraction techniques, including dynamic layout adjustment and interactive\nbundle querying. Our unoptimized implementation handles graphs of over 100,000\nedges in eight seconds, providing substantially higher performance than\nprevious approaches.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 08:39:09 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Vandenberg", "Rebecca", ""], ["Elliott", "Madison", ""], ["Harvey", "Nicholas", ""], ["Munzner", "Tamara", ""]]}, {"id": "2007.06272", "submitter": "Alberto Gomez", "authors": "Simona Treivase, Alberto Gomez, Jacqueline Matthew, Emily Skelton,\n  Julia A. Schnabel, Nicolas Toussaint", "title": "Screen Tracking for Clinical Translation of Live Ultrasound Image\n  Analysis Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultrasound (US) imaging is one of the most commonly used non-invasive imaging\ntechniques. However, US image acquisition requires simultaneous guidance of the\ntransducer and interpretation of images, which is a highly challenging task\nthat requires years of training. Despite many recent developments in\nintra-examination US image analysis, the results are not easy to translate to a\nclinical setting. We propose a generic framework to extract the US images and\nsuperimpose the results of an analysis task, without any need for physical\nconnection or alteration to the US system. The proposed method captures the US\nimage by tracking the screen with a camera fixed at the sonographer's view\npoint and reformats the captured image to the right aspect ratio, in 87.66 +-\n3.73ms on average.\n  It is hypothesized that this would enable to input such retrieved image into\nan image processing pipeline to extract information that can help improve the\nexamination. This information could eventually be projected back to the\nsonographer's field of view in real time using, for example, an augmented\nreality (AR) headset.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 09:53:20 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Treivase", "Simona", ""], ["Gomez", "Alberto", ""], ["Matthew", "Jacqueline", ""], ["Skelton", "Emily", ""], ["Schnabel", "Julia A.", ""], ["Toussaint", "Nicolas", ""]]}, {"id": "2007.06473", "submitter": "Min Hun Lee", "authors": "Min Hun Lee, Daniel P. Siewiorek, Asim Smailagic, Alexandre\n  Bernardino, and Sergi Berm\\'udez i Badia", "title": "Designing Personalized Interaction of a Socially Assistive Robot for\n  Stroke Rehabilitation Therapy", "comments": "IEEE International Conference on Robotics and Automation (ICRA)\n  Workshop on Social Robotics for Neurodevelopmental Disorders 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research of a socially assistive robot has a potential to augment and\nassist physical therapy sessions for patients with neurological and\nmusculoskeletal problems (e.g. stroke). During a physical therapy session,\ngenerating personalized feedback is critical to improve patient's engagement.\nHowever, prior work on socially assistive robotics for physical therapy has\nmainly utilized pre-defined corrective feedback even if patients have various\nphysical and functional abilities. This paper presents an interactive approach\nof a socially assistive robot that can dynamically select kinematic features of\nassessment on individual patient's exercises to predict the quality of motion\nand provide patient-specific corrective feedback for personalized interaction\nof a robot exercise coach.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 16:12:05 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Lee", "Min Hun", ""], ["Siewiorek", "Daniel P.", ""], ["Smailagic", "Asim", ""], ["Bernardino", "Alexandre", ""], ["Badia", "Sergi Berm\u00fadez i", ""]]}, {"id": "2007.06531", "submitter": "Mohammed Moshiul Hoque Dr.", "authors": "Mohammed Moshiul Hoque (Chittagong University of Engineering and\n  Technology)", "title": "A Robotic Framework for Making Eye Contact with Humans", "comments": "10 pages, 11 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meeting eye contact is the essential prerequisite skill of a human to\ninitiate any conversation with others. However, it is not an easy task for a\nrobot to meet eye contact with a human if they are not facing each other\ninitially or the human is intensely engaged his or her task. If the robot would\nlike to start communication with a particular person, it should turn its gaze\nto that person first. However, only such a turning action alone is not always\nenough to set up eye contact. Sometimes, the robot should perform some strong\nactions so that it can capture the human's attention toward it. In this paper,\nwe proposed a computational model for robots that can proactively capture human\nattention and makes eye contact with him or her. Evaluation experiments by\nusing a robotic head reveal the effectiveness of the proposed model in\ndifferent viewing situations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 17:41:53 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Hoque", "Mohammed Moshiul", "", "Chittagong University of Engineering and\n  Technology"]]}, {"id": "2007.06654", "submitter": "Jinghui Cheng", "authors": "Wenting Wang, Jinghui Cheng, Jin L.C. Guo", "title": "How Do Open Source Software Contributors Perceive and Address Usability?\n  Valued Factors, Practices, and Challenges", "comments": "6 pages, 2 figures, IEEE Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usability is an increasing concern in open source software (OSS). Given the\nrecent changes in the OSS landscape, it is imperative to examine the OSS\ncontributors' current valued factors, practices, and challenges concerning\nusability. We accumulated this knowledge through a survey with a wide range of\ncontributors to OSS applications. Through analyzing 84 survey responses, we\nfound that many participants recognized the importance of usability. While most\nrelied on issue tracking systems to collect user feedback, a few participants\nalso adopted typical user-centered design methods. However, most participants\ndemonstrated a system-centric rather than a user-centric view. Understanding\nthe diverse needs and consolidating various feedback of end-users posed unique\nchallenges for the OSS contributors when addressing usability in the most\nrecent development context. Our work provided important insights for OSS\npractitioners and tool designers in exploring ways for promoting a user-centric\nmindset and improving usability practice in the current OSS communities.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:45:15 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wang", "Wenting", ""], ["Cheng", "Jinghui", ""], ["Guo", "Jin L. C.", ""]]}, {"id": "2007.06718", "submitter": "Samantha Robertson", "authors": "Samantha Robertson and Niloufar Salehi", "title": "What If I Don't Like Any Of The Choices? The Limits of Preference\n  Elicitation for Participatory Algorithm Design", "comments": "Presented at the workshop on Participatory Approaches to Machine\n  Learning at ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging methods for participatory algorithm design have proposed collecting\nand aggregating individual stakeholder preferences to create algorithmic\nsystems that account for those stakeholders' values. Using algorithmic student\nassignment as a case study, we argue that optimizing for individual preference\nsatisfaction in the distribution of limited resources may actually inhibit\nprogress towards social and distributive justice. Individual preferences can be\na useful signal but should be expanded to support more expressive and inclusive\nforms of democratic participation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 21:58:30 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Robertson", "Samantha", ""], ["Salehi", "Niloufar", ""]]}, {"id": "2007.06720", "submitter": "Kourosh Darvish", "authors": "Prajval Kumar Murali, Kourosh Darvish, Fulvio Mastrogiovanni", "title": "Deployment and Evaluation of a Flexible Human-Robot Collaboration Model\n  Based on AND/OR Graphs in a Manufacturing Environment", "comments": null, "journal-ref": null, "doi": "10.1007/s11370-020-00332-9", "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Industry 4.0 paradigm promises shorter development times, increased\nergonomy, higher flexibility, and resource efficiency in manufacturing\nenvironments. Collaborative robots are an important tangible technology for\nimplementing such a paradigm. A major bottleneck to effectively deploy\ncollaborative robots to manufacturing industries is developing task planning\nalgorithms that enable them to recognize and naturally adapt to varying and\neven unpredictable human actions while simultaneously ensuring an overall\nefficiency in terms of production cycle time. In this context, an architecture\nencompassing task representation, task planning, sensing, and robot control has\nbeen designed, developed and evaluated in a real industrial environment. A\npick-and-place palletization task, which requires the collaboration between\nhumans and robots, is investigated. The architecture uses AND/OR graphs for\nrepresenting and reasoning upon human-robot collaboration models online.\nFurthermore, objective measures of the overall computational performance and\nsubjective measures of naturalness in human-robot collaboration have been\nevaluated by performing experiments with production-line operators. The results\nof this user study demonstrate how human-robot collaboration models like the\none we propose can leverage the flexibility and the comfort of operators in the\nworkplace. In this regard, an extensive comparison study among recent models\nhas been carried out.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 22:05:34 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Murali", "Prajval Kumar", ""], ["Darvish", "Kourosh", ""], ["Mastrogiovanni", "Fulvio", ""]]}, {"id": "2007.07172", "submitter": "Alireza Abedin", "authors": "Alireza Abedin, Mahsa Ehsanpour, Qinfeng Shi, Hamid Rezatofighi,\n  Damith C. Ranasinghe", "title": "Attend And Discriminate: Beyond the State-of-the-Art for Human Activity\n  Recognition using Wearable Sensors", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearables are fundamental to improving our understanding of human activities,\nespecially for an increasing number of healthcare applications from\nrehabilitation to fine-grained gait analysis. Although our collective know-how\nto solve Human Activity Recognition (HAR) problems with wearables has\nprogressed immensely with end-to-end deep learning paradigms, several\nfundamental opportunities remain overlooked. We rigorously explore these new\nopportunities to learn enriched and highly discriminating activity\nrepresentations. We propose: i) learning to exploit the latent relationships\nbetween multi-channel sensor modalities and specific activities; ii)\ninvestigating the effectiveness of data-agnostic augmentation for multi-modal\nsensor data streams to regularize deep HAR models; and iii) incorporating a\nclassification loss criterion to encourage minimal intra-class representation\ndifferences whilst maximising inter-class differences to achieve more\ndiscriminative features. Our contributions achieves new state-of-the-art\nperformance on four diverse activity recognition problem benchmarks with large\nmargins -- with up to 6% relative margin improvement. We extensively validate\nthe contributions from our design concepts through extensive experiments,\nincluding activity misalignment measures, ablation studies and insights shared\nthrough both quantitative and qualitative studies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:44:16 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Abedin", "Alireza", ""], ["Ehsanpour", "Mahsa", ""], ["Shi", "Qinfeng", ""], ["Rezatofighi", "Hamid", ""], ["Ranasinghe", "Damith C.", ""]]}, {"id": "2007.07407", "submitter": "Xiang 'Anthony' Chen", "authors": "Juan Rebanal, Yuqi Tang, Jordan Combitsis, Xiang 'Anthony' Chen", "title": "XAlgo: a Design Probe of Explaining Algorithms' Internal States via\n  Question-Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms often appear as 'black boxes' to non-expert users. While prior\nwork focuses on explainable representations and expert-oriented exploration, we\npropose and study an interactive approach using question answering to explain\ndeterministic algorithms to non-expert users who need to understand the\nalgorithms' internal states (e.g., students learning algorithms, operators\nmonitoring robots, admins troubleshooting network routing). We construct XAlgo\n-- a formal model that first classifies the type of question based on a\ntaxonomy and generates an answer based on a set of rules that extract\ninformation from representations of an algorithm's internal states, e.g., the\npseudocode. A design probe in an algorithm learning scenario with 18\nparticipants (9 for a Wizard-of-Oz XAlgo and 9 as a control group) reports\nfindings and design implications based on what kinds of questions people ask,\nhow well XAlgo responds, and what remain as challenges to bridge users' gulf of\nunderstanding algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 23:54:36 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 22:39:25 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Rebanal", "Juan", ""], ["Tang", "Yuqi", ""], ["Combitsis", "Jordan", ""], ["Chen", "Xiang 'Anthony'", ""]]}, {"id": "2007.07486", "submitter": "Stefan Langer", "authors": "Stefan Langer, Liza Obermeier, Andr\\'e Ebert, Markus Friedrich, Emma\n  Munisamy, Claudia Linnhoff-Popien", "title": "Content-based Recommendations for Radio Stations with Deep Learned Audio\n  Fingerprints", "comments": "Informatik 2020, LectureNotes in Informatics(LNI),Gesellschaft f\\\"ur\n  Informatik, Bonn2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world of linear radio broadcasting is characterized by a wide variety of\nstations and played content. That is why finding stations playing the preferred\ncontent is a tough task for a potential listener, especially due to the\noverwhelming number of offered choices. Here, recommender systems usually step\nin but existing content-based approaches rely on metadata and thus are\nconstrained by the available data quality. Other approaches leverage user\nbehavior data and thus do not exploit any domain-specific knowledge and are\nfurthermore disadvantageous regarding privacy concerns. Therefore, we propose a\nnew pipeline for the generation of audio-based radio station fingerprints\nrelying on audio stream crawling and a Deep Autoencoder. We show that the\nproposed fingerprints are especially useful for characterizing radio stations\nby their audio content and thus are an excellent representation for meaningful\nand reliable radio station recommendations. Furthermore, the proposed modules\nare part of the HRADIO Communication Platform, which enables hybrid radio\nfeatures to radio stations. It is released with a flexible open source license\nand enables especially small- and medium-sized businesses, to provide\ncustomized and high quality radio services to potential listeners.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 05:15:30 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Langer", "Stefan", ""], ["Obermeier", "Liza", ""], ["Ebert", "Andr\u00e9", ""], ["Friedrich", "Markus", ""], ["Munisamy", "Emma", ""], ["Linnhoff-Popien", "Claudia", ""]]}, {"id": "2007.07514", "submitter": "Lachlan Urquhart Ph.D", "authors": "Lachlan Urquhart and Peter Craigon", "title": "The Moral-IT Deck: A Tool for Ethics by Design", "comments": "Governance and Regulation; Design Tools; Responsible Research and\n  Innovation; Ethics by Design; Games; Human Computer Interaction, Card Based\n  Tools", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design process and empirical evaluation of a new tool\nfor enabling ethics by design: The Moral-IT Cards. Better tools are needed to\nsupport the role of technologists in addressing ethical issues during system\ndesign. These physical cards support reflection by technologists on normative\naspects of technology development, specifically on emerging risks, appropriate\nsafeguards and challenges of implementing these in the system. We discuss how\nthe cards were developed and tested within 5 workshops with 20 participants\nfrom both research and commercial settings. We consider the role of\ntechnologists in ethics from different EU/UK policymaking initiatives and\ndisciplinary perspectives (i.e. Science and Technology Studies (STS), IT Law,\nHuman Computer Interaction (HCI), Computer/Engineering Ethics). We then examine\nexisting ethics by design tools, and other cards based tools before arguing why\ncards can be a useful medium for addressing complex ethical issues. We present\nthe development process for the Moral-IT cards, document key features of our\ncard design, background on the content, the impact assessment board process for\nusing them and how this was formulated. We discuss our study design and\nmethodology before examining key findings which are clustered around three\noverarching themes. These are: the value of our cards as a tool, their impact\non the technology design process and how they structure ethical reflection\npractices. We conclude with key lessons and concepts such as how they level the\nplaying field for debate; enable ethical clustering, sorting and comparison;\nprovide appropriate anchors for discussion and highlighted the intertwined\nnature of ethics.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 07:26:45 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Urquhart", "Lachlan", ""], ["Craigon", "Peter", ""]]}, {"id": "2007.08476", "submitter": "Mahender Mandala", "authors": "Meenakshi Das, Daniela Marghitu, Fatemeh Jamshidi, Mahender Mandala\n  and Ayanna Howard", "title": "Accessible Computer Science for K-12 Students with Hearing Impairments", "comments": "12 Pages, 5 figures, 22nd International Conference on Human Computer\n  Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An inclusive science, technology, engineering and mathematics (STEM)\nworkforce is needed to maintain America's leadership in the scientific\nenterprise. Increasing the participation of underrepresented groups in STEM,\nincluding persons with disabilities, requires national attention to fully\nengage the nation's citizens in transforming its STEM enterprise. To address\nthis need, a number of initiatives, such as AccessCSforALL, Bootstrap, and\nCSforAll, are making efforts to make Computer Science inclusive to the 7.4\nmillion K-12 students with disabilities in the U.S. Of special interest to our\nproject are those K-12 students with hearing impairments. American Sign\nLanguage (ASL) is the primary means of communication for an estimated 500,000\npeople in the United States, yet there are limited online resources providing\nComputer Science instruction in ASL. This paper introduces a new project\ndesigned to support Deaf and Hard of Hearing (DHH) K-12 students and sign\ninterpreters in acquiring knowledge of complex Computer Science concepts. We\ndiscuss the motivation for the project and an early design of the accessible\nblock-based Computer Science curriculum to engage DHH students in hands-on\ncomputing education.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 17:21:52 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Das", "Meenakshi", ""], ["Marghitu", "Daniela", ""], ["Jamshidi", "Fatemeh", ""], ["Mandala", "Mahender", ""], ["Howard", "Ayanna", ""]]}, {"id": "2007.08604", "submitter": "Thomas Gross", "authors": "Uchechi Phyllis Nwadike and Thomas Gro{\\ss}", "title": "Investigation of the Effect of Incidental Fear Privacy Behavioral\n  Intention (Technical Report)", "comments": "Technical report, main paper to appear in proceedings of the 9th\n  International Workshop on Socio-Technical Aspects in Security (STAST'2019),\n  LNCS 11739, Springer Verlag, 2019, pp. 181-204. Supported by the ERC Starting\n  Grant CASCAde (GA no 716980)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background. Incidental emotions users feel during their online activities may\nalter their privacy behavioral intentions.\n  Aim. We investigate the effect of incidental affect (fear and happiness) on\nprivacy behavioral intention.\n  Method. We recruited $330$ participants for a within-subjects experiment in\nthree random-controlled user studies. The participants were exposed to three\nconditions \\textsf{neutral}, \\textsf{fear}, \\textsf{happiness} with\nstandardised stimuli videos for incidental affect induction. Fear and happiness\nwere assigned in random order. The participants' privacy behavioural intentions\n(PBI) were measured followed by a Positive and Negative Affect Schedule\n(PANAS-X) manipulation check on self-reported affect. The PBI and PANAS-X were\ncompared across treatment conditions.\n  Results. We observed a statistically significant difference in PBI and\nProtection Intention in neutral-fear and neutral-happy comparisons. However\nacross fear and happy conditions, we did not observe any statistically\nsignificant change in PBI scores.\n  Conclusions. We offer the first systematic analysis of the impact of\nincidental affects on Privacy Behavioral Intention (PBI) and its\nsub-constructs. We are the first to offer a fine-grained analysis of\nneutral-affect comparisons and interactions offering insights in hitherto\nunexplained phenomena reported in the field.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:04:51 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Nwadike", "Uchechi Phyllis", ""], ["Gro\u00df", "Thomas", ""]]}, {"id": "2007.08705", "submitter": "Naoki Wake", "authors": "Naoki Wake, Iori Yanokura, Kazuhiro Sasabuchi, Katsushi Ikeuchi", "title": "Verbal Focus-of-Attention System for Learning-from-Observation", "comments": "8 pages, 7 figures. Submitted to and accepted by IEEE ICRA 2021. Last\n  updated March 3rd, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The learning-from-observation (LfO) framework aims to map human\ndemonstrations to a robot to reduce programming effort. To this end, an LfO\nsystem encodes a human demonstration into a series of execution units for a\nrobot, which are referred to as task models. Although previous research has\nproposed successful task-model encoders, there has been little discussion on\nhow to guide a task-model encoder in a scene with spatio-temporal noises, such\nas cluttered objects or unrelated human body movements. Inspired by the\nfunction of verbal instructions guiding an observer's visual attention, we\npropose a verbal focus-of-attention (FoA) system (i.e., spatio-temporal\nfilters) to guide a task-model encoder. For object manipulation, the system\nfirst recognizes the name of a target object and its attributes from verbal\ninstructions. The information serves as a where-to-look FoA filter to confine\nthe areas in which the target object existed in the demonstration. The system\nthen detects the timings of grasp and release that occurred in the filtered\nareas. The timings serve as a when-to-look FoA filter to confine the period of\nobject manipulation. Finally, a task-model encoder recognizes the task models\nby employing FoA filters. We demonstrate the robustness of the verbal FoA in\nattenuating spatio-temporal noises by comparing it with an existing action\nlocalization network. The contributions of this study are as follows: (1) to\npropose a verbal FoA for LfO, (2) to design an algorithm to calculate FoA\nfilters from verbal input, and (3) to demonstrate the effectiveness of a verbal\nFoA in localizing an action by comparing it with a state-of-the-art vision\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 00:57:49 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 05:49:44 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 11:16:13 GMT"}, {"version": "v4", "created": "Wed, 24 Mar 2021 15:15:50 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wake", "Naoki", ""], ["Yanokura", "Iori", ""], ["Sasabuchi", "Kazuhiro", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "2007.08875", "submitter": "Christian Tiefenau", "authors": "Christian Tiefenau, Maximilian H\\\"aring, Katharina Krombholz, Emanuel\n  von Zezschwitz", "title": "Security, Availability, and Multiple Information Sources: Exploring\n  Update Behavior of System Administrators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Experts agree that keeping systems up to date is a powerful security measure.\nPrevious work found that users sometimes explicitly refrain from performing\ntimely updates, e.g., due to bad experiences which has a negative impact on\nend-user security. Another important user group has been investigated less\nextensively: system administrators, who are responsible for keeping complex and\nheterogeneous system landscapes available and secure.\n  In this paper, we sought to understand administrators' behavior, experiences,\nand attitudes regarding updates in a corporate environment. Based on the\nresults of an interview study, we developed an online survey and quantified\ncommon practices and obstacles (e.g., downtime or lack of information about\nupdates). The findings indicate that even experienced administrators struggle\nwith update processes as the consequences of an update are sometimes hard to\nassess. Therefore, we argue that more usable monitoring and update processes\nare essential to guarantee IT security at scale.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 10:09:10 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Tiefenau", "Christian", ""], ["H\u00e4ring", "Maximilian", ""], ["Krombholz", "Katharina", ""], ["von Zezschwitz", "Emanuel", ""]]}, {"id": "2007.08948", "submitter": "Emin Zerman", "authors": "Mair\\'ead Grogan, Emin Zerman, Gareth W. Young, Aljosa Smolic", "title": "A Case Study on Video Color Transfer: Exploring User Motivations,\n  Expectations, and Satisfaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia and creativity software products are being used to edit and\ncontrol various elements of creative media practices. These days, the technical\naffordances of mobile multimedia devices and the advent of high-speed 5G\ninternet access mean that these abilities are simpler and more readily\navailable to be harnessed by mobile applications. In this paper, using a\nprototype application, we discuss how potential users of such technology are\nmotivated to use a video recoloring application and explore the role that user\nexpectation and satisfaction play in this process. By exploring this topic and\nfocusing on the human-computer interaction, we found that color transfer\ninteractions are driven by several intrinsic motivations and that user\nexpectations and satisfaction ratings can be maintained via clear\nvisualizations of the processes to be undertaken. Furthermore, we reveal the\nspecific language that users use to communicate video recoloring when regarding\nuser motivations, expectations, and satisfaction. This research provides\nimportant information for developers of state-of-art recoloring processes and\ncontributes to dialogues surrounding the users of mobile multimedia technology\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 12:50:24 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Grogan", "Mair\u00e9ad", ""], ["Zerman", "Emin", ""], ["Young", "Gareth W.", ""], ["Smolic", "Aljosa", ""]]}, {"id": "2007.09028", "submitter": "Arnold Yeung", "authors": "Arnold YS Yeung, Shalmali Joshi, Joseph Jay Williams, Frank Rudzicz", "title": "Sequential Explanations with Mental Model-Based Policies", "comments": "Accepted into ICML 2020 Workshop on Human Interpretability in Machine\n  Learning (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The act of explaining across two parties is a feedback loop, where one\nprovides information on what needs to be explained and the other provides an\nexplanation relevant to this information. We apply a reinforcement learning\nframework which emulates this format by providing explanations based on the\nexplainee's current mental model. We conduct novel online human experiments\nwhere explanations generated by various explanation methods are selected and\npresented to participants, using policies which observe participants' mental\nmodels, in order to optimize an interpretability proxy. Our results suggest\nthat mental model-based policies (anchored in our proposed state\nrepresentation) may increase interpretability over multiple sequential\nexplanations, when compared to a random selection baseline. This work provides\ninsight into how to select explanations which increase relevant information for\nusers, and into conducting human-grounded experimentation to understand\ninterpretability.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 14:43:46 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Yeung", "Arnold YS", ""], ["Joshi", "Shalmali", ""], ["Williams", "Joseph Jay", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2007.09170", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Dai Hasegawa, Naoshi Kaneko, Gustav Eje Henter,\n  Hedvig Kjellstr\\\"om", "title": "Moving fast and slow: Analysis of representations and post-processing in\n  speech-driven automatic gesture generation", "comments": "Extension of our IVA'19 paper. Accepted at the International Journal\n  of Human-Computer Interaction. See more at\n  https://svito-zar.github.io/audio2gestures/. arXiv admin note: substantial\n  text overlap with arXiv:1903.03369", "journal-ref": "Int. J. Hum. Comput.Interact.(2021)", "doi": "10.1080/10447318.2021.1883883", "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for speech-driven gesture production,\napplicable to virtual agents to enhance human-computer interaction.\nSpecifically, we extend recent deep-learning-based, data-driven methods for\nspeech-driven gesture generation by incorporating representation learning. Our\nmodel takes speech as input and produces gestures as output, in the form of a\nsequence of 3D coordinates. We provide an analysis of different representations\nfor the input (speech) and the output (motion) of the network by both objective\nand subjective evaluations. We also analyse the importance of smoothing of the\nproduced motion. Our results indicated that the proposed method improved on our\nbaseline in terms of objective measures. For example, it better captured the\nmotion dynamics and better matched the motion-speed distribution. Moreover, we\nperformed user studies on two different datasets. The studies confirmed that\nour proposed method is perceived as more natural than the baseline, although\nthe difference in the studies was eliminated by appropriate post-processing:\nhip-centering and smoothing. We conclude that it is important to take both\nmotion representation and post-processing into account when designing an\nautomatic gesture-production method.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 07:32:00 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 17:30:30 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 12:49:17 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kucherenko", "Taras", ""], ["Hasegawa", "Dai", ""], ["Kaneko", "Naoshi", ""], ["Henter", "Gustav Eje", ""], ["Kjellstr\u00f6m", "Hedvig", ""]]}, {"id": "2007.09177", "submitter": "Jichen Zhu", "authors": "Jennifer Villareale, Ana Acosta-Ruiz, Samuel Arcaro, Thomas Fox, Evan\n  Freed, Robert Gray, Mathias L\\\"owe, Panote Nuchprayoon, Aleksanteri Sladek,\n  Rush Weigelt, Yifu Li, Sebastian Risi, Jichen Zhu", "title": "iNNk: A Multi-Player Game to Deceive a Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents iNNK, a multiplayer drawing game where human players team\nup against an NN. The players need to successfully communicate a secret code\nword to each other through drawings, without being deciphered by the NN. With\nthis game, we aim to foster a playful environment where players can, in a small\nway, go from passive consumers of NN applications to creative thinkers and\ncritical challengers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 18:25:10 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 17:31:00 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Villareale", "Jennifer", ""], ["Acosta-Ruiz", "Ana", ""], ["Arcaro", "Samuel", ""], ["Fox", "Thomas", ""], ["Freed", "Evan", ""], ["Gray", "Robert", ""], ["L\u00f6we", "Mathias", ""], ["Nuchprayoon", "Panote", ""], ["Sladek", "Aleksanteri", ""], ["Weigelt", "Rush", ""], ["Li", "Yifu", ""], ["Risi", "Sebastian", ""], ["Zhu", "Jichen", ""]]}, {"id": "2007.09207", "submitter": "Lik Hang Lee Dr.", "authors": "Lik Hang Lee, Tristan Braud, Simo Hosio, Pan Hui", "title": "Towards Augmented Reality-driven Human-City Interaction: Current\n  Research on Mobile Headsets and Future Challenges", "comments": "39 pages, 12 figures, ACM Computing Survey (Accepted, May 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interaction design for Augmented Reality (AR) is gaining increasing attention\nfrom both academia and industry. This survey discusses 260 articles (68.8% of\narticles published between 2015 - 2019) to review the field of human\ninteraction in connected cities with emphasis on augmented reality-driven\ninteraction. We provide an overview of Human-City Interaction and related\ntechnological approaches, followed by a review of the latest trends of\ninformation visualization, constrained interfaces, and embodied interaction for\nAR headsets. We highlight under-explored issues in interface design and input\ntechniques that warrant further research, and conjecture that AR with\ncomplementary Conversational User Interfaces (CUIs) is a key enabler for\nubiquitous interaction with immersive systems in smart cities. Our work helps\nresearchers understand the current potential and future needs of AR in\nHuman-City Interaction.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 19:47:02 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 06:16:20 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lee", "Lik Hang", ""], ["Braud", "Tristan", ""], ["Hosio", "Simo", ""], ["Hui", "Pan", ""]]}, {"id": "2007.09262", "submitter": "Caitlyn Seim", "authors": "Caitlyn E. Seim, Steven L. Wolf, and Thad E. Starner", "title": "Wearable vibrotactile stimulation for upper extremity rehabilitation in\n  chronic stroke: clinical feasibility trial using the VTS Glove", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Evaluate the feasibility and potential impacts on hand function\nusing a wearable stimulation device (the VTS Glove) which provides mechanical,\nvibratory input to the affected limb of chronic stroke survivors.\n  Methods: A double-blind, randomized, controlled feasibility study including\nsixteen chronic stroke survivors (mean age: 54; 1-13 years post-stroke) with\ndiminished movement and tactile perception in their affected hand. Participants\nwere given a wearable device to take home and asked to wear it for three hours\ndaily over eight weeks. The device intervention was either (1) the VTS Glove,\nwhich provided vibrotactile stimulation to the hand, or (2) an identical glove\nwith vibration disabled. Participants were equally randomly assigned to each\ncondition. Hand and arm function were measured weekly at home and in local\nphysical therapy clinics.\n  Results: Participants using the VTS Glove showed significantly improved\nSemmes-Weinstein monofilament exam, reduction in Modified Ashworth measures in\nthe fingers, and some increased voluntary finger flexion, elbow and shoulder\nrange of motion.\n  Conclusions: Vibrotactile stimulation applied to the disabled limb may impact\ntactile perception, tone and spasticity, and voluntary range of motion.\nWearable devices allow extended application and study of stimulation methods\noutside of a clinical setting.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 22:17:30 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Seim", "Caitlyn E.", ""], ["Wolf", "Steven L.", ""], ["Starner", "Thad E.", ""]]}, {"id": "2007.09341", "submitter": "Milad Haghani", "authors": "Milad Haghani, Michiel C. J. Bliemer, Bilal Farooq, Inhi Kim, Zhibin\n  Li, Cheol Oh, Zahra Shahhoseini, Hamish MacDougall", "title": "Applications of brain imaging methods in driving behaviour research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of neuroimaging methods have substantially contributed to the\nscientific understanding of human factors during driving by providing a deeper\ninsight into the neuro-cognitive aspects of driver brain. This has been\nachieved by conducting simulated (and occasionally, field) driving experiments\nwhile collecting driver brain signals of certain types. Here, this sector of\nstudies is comprehensively reviewed at both macro and micro scales. Different\nthemes of neuroimaging driving behaviour research are identified and the\nfindings within each theme are synthesised. The surveyed literature has\nreported on applications of four major brain imaging methods. These include\nFunctional Magnetic Resonance Imaging (fMRI), Electroencephalography (EEG),\nFunctional Near-Infrared Spectroscopy (fNIRS) and Magnetoencephalography (MEG),\nwith the first two being the most common methods in this domain. While\ncollecting driver fMRI signal has been particularly instrumental in studying\nneural correlates of intoxicated driving (e.g. alcohol or cannabis) or\ndistracted driving, the EEG method has been predominantly utilised in relation\nto the efforts aiming at development of automatic fatigue/drowsiness detection\nsystems, a topic to which the literature on neuro-ergonomics of driving\nparticularly has shown a spike of interest within the last few years. The\nsurvey also reveals that topics such as driver brain activity in semi-automated\nsettings or the brain activity of drivers with brain injuries or chronic\nneurological conditions have by contrast been investigated to a very limited\nextent. Further, potential topics in relation to driving behaviour are\nidentified that could benefit from the adoption of neuroimaging methods in\nfuture studies.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 06:31:16 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Haghani", "Milad", ""], ["Bliemer", "Michiel C. J.", ""], ["Farooq", "Bilal", ""], ["Kim", "Inhi", ""], ["Li", "Zhibin", ""], ["Oh", "Cheol", ""], ["Shahhoseini", "Zahra", ""], ["MacDougall", "Hamish", ""]]}, {"id": "2007.09515", "submitter": "Bharathan Balaji", "authors": "Bo-Jhang Ho, Bharathan Balaji, Mehmet Koseoglu, Sandeep Sandha, Siyou\n  Pei, Mani Srivastava", "title": "Quick Question: Interrupting Users for Microtasks with Reinforcement\n  Learning", "comments": "Presented at the 2nd Workshop on Human in the Loop Learning in ICML\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human attention is a scarce resource in modern computing. A multitude of\nmicrotasks vie for user attention to crowdsource information, perform momentary\nassessments, personalize services, and execute actions with a single touch. A\nlot gets done when these tasks take up the invisible free moments of the day.\nHowever, an interruption at an inappropriate time degrades productivity and\ncauses annoyance. Prior works have exploited contextual cues and behavioral\ndata to identify interruptibility for microtasks with much success. With Quick\nQuestion, we explore use of reinforcement learning (RL) to schedule microtasks\nwhile minimizing user annoyance and compare its performance with supervised\nlearning. We model the problem as a Markov decision process and use Advantage\nActor Critic algorithm to identify interruptible moments based on context and\nhistory of user interactions. In our 5-week, 30-participant study, we compare\nthe proposed RL algorithm against supervised learning methods. While the mean\nnumber of responses between both methods is commensurate, RL is more effective\nat avoiding dismissal of notifications and improves user experience over time.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jul 2020 20:26:53 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Ho", "Bo-Jhang", ""], ["Balaji", "Bharathan", ""], ["Koseoglu", "Mehmet", ""], ["Sandha", "Sandeep", ""], ["Pei", "Siyou", ""], ["Srivastava", "Mani", ""]]}, {"id": "2007.09809", "submitter": "Xiang 'Anthony' Chen", "authors": "Ritam Jyoti Sarmah, Yunpeng Ding, Di Wang, Cheuk Yin Phipson Lee, Toby\n  Jia-Jun Li, Xiang 'Anthony' Chen", "title": "Geno: A Developer Tool for Authoring Multimodal Interaction on Existing\n  Web Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supporting voice commands in applications presents significant benefits to\nusers. However, adding such support to existing GUI-based web apps is\neffort-consuming with a high learning barrier, as shown in our formative study,\ndue to the lack of unified support for creating multimodal interfaces. We\npresent Geno---a developer tool for adding the voice input modality to existing\nweb apps without requiring significant NLP expertise. Geno provides a\nhigh-level workflow for developers to specify functionalities to be supported\nby voice (intents), create language models for detecting intents and the\nrelevant information (parameters) from user utterances, and fulfill the intents\nby either programmatically invoking the corresponding functions or replaying\nGUI actions on the web app. Geno further supports multimodal references to GUI\ncontext in voice commands (e.g. \"move this [event] to next week\" while pointing\nat an event with the cursor). In a study, developers with little NLP expertise\nwere able to add multimodal voice command support for two existing web apps\nusing Geno.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 22:59:19 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Sarmah", "Ritam Jyoti", ""], ["Ding", "Yunpeng", ""], ["Wang", "Di", ""], ["Lee", "Cheuk Yin Phipson", ""], ["Li", "Toby Jia-Jun", ""], ["Chen", "Xiang 'Anthony'", ""]]}, {"id": "2007.09884", "submitter": "Dillon Lohr", "authors": "Alex Karpov, Jacob Liberman, Dillon Lohr, Oleg Komogortsev", "title": "Parallel Oculomotor Plant Mathematical Model for Large Scale Eye\n  Movement Simulation", "comments": "7 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of eye tracking sensors is expected to grow in virtual (VR) and\naugmented reality (AR) platforms. Provided that users of these platforms\nconsent to employing captured eye movement signals for authentication and\nhealth assessment, it becomes important to estimate oculomotor plant and brain\nfunction characteristics in real time. This paper shows a path toward that goal\nby presenting a parallel processing architecture capable of estimating\noculomotor plant characteristics and comparing its performance to a\nsingle-threaded implementation. Results show that the parallel implementation\nimproves the speed, accuracy, and throughput of oculomotor plant characteristic\nestimation versus the original serial version for both large-scale and\nreal-time simulation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 04:39:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Karpov", "Alex", ""], ["Liberman", "Jacob", ""], ["Lohr", "Dillon", ""], ["Komogortsev", "Oleg", ""]]}, {"id": "2007.09970", "submitter": "Simone Balloccu Mr", "authors": "Simone Balloccu, Ehud Reiter, Alexandra Johnstone, Claire Fyfe", "title": "How are you? Introducing stress-based text tailoring", "comments": null, "journal-ref": "IntelLang 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can stress affect not only your life but also how you read and interpret a\ntext? Healthcare has shown evidence of such dynamics and in this short paper we\ndiscuss customising texts based on user stress level, as it could represent a\ncritical factor when it comes to user engagement and behavioural change. We\nfirst show a real-world example in which user behaviour is influenced by\nstress, then, after discussing which tools can be employed to assess and\nmeasure it, we propose an initial method for tailoring the document by\nexploiting complexity reduction and affect enforcement. The result is a short\nand encouraging text which requires less commitment to be read and understood.\nWe believe this work in progress can raise some interesting questions on a\ntopic that is often overlooked in NLG.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 09:43:11 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Balloccu", "Simone", ""], ["Reiter", "Ehud", ""], ["Johnstone", "Alexandra", ""], ["Fyfe", "Claire", ""]]}, {"id": "2007.09989", "submitter": "Pedro F Da Costa", "authors": "Pedro F. da Costa, Romy Lorenz, Ricardo Pio Monti, Emily Jones, Robert\n  Leech", "title": "Bayesian optimization for automatic design of face stimuli", "comments": "Accepted at ICML2020 workshop track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigating the cognitive and neural mechanisms involved with face\nprocessing is a fundamental task in modern neuroscience and psychology. To\ndate, the majority of such studies have focused on the use of pre-selected\nstimuli. The absence of personalized stimuli presents a serious limitation as\nit fails to account for how each individual face processing system is tuned to\ncultural embeddings or how it is disrupted in disease. In this work, we propose\na novel framework which combines generative adversarial networks (GANs) with\nBayesian optimization to identify individual response patterns to many\ndifferent faces. Formally, we employ Bayesian optimization to efficiently\nsearch the latent space of state-of-the-art GAN models, with the aim to\nautomatically generate novel faces, to maximize an individual subject's\nresponse. We present results from a web-based proof-of-principle study, where\nparticipants rated images of themselves generated via performing Bayesian\noptimization over the latent space of a GAN. We show how the algorithm can\nefficiently locate an individual's optimal face while mapping out their\nresponse across different semantic transformations of a face; inter-individual\nanalyses suggest how the approach can provide rich information about individual\ndifferences in face processing.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 10:27:18 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["da Costa", "Pedro F.", ""], ["Lorenz", "Romy", ""], ["Monti", "Ricardo Pio", ""], ["Jones", "Emily", ""], ["Leech", "Robert", ""]]}, {"id": "2007.10089", "submitter": "Anirban Lahiri", "authors": "Anirban Lahiri, Utanko Mitra, Sunreeta Sen, Mrinal Chakraborty, Max\n  Kleiman-Weiner, Rajlakshmi Guha, Pabitra Mitra, Anupam Basu, Partha Pratim\n  Chakraborty", "title": "Antarjami: Exploring psychometric evaluation through a computer-based\n  game", "comments": "Submitted to CogSci 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of questionnaire based psychometric testing frameworks are globally\nfor example OCEAN (Five factor) indicator, MBTI (Myers Brigg Type Indicator)\netc. However, questionnaire based psychometric tests have some known\nshortcomings. This work explores whether these shortcomings can be mitigated\nthrough computer-based gaming platforms for evaluating psychometric parameters.\nA computer based psychometric game framework called Antarjami has been\ndeveloped for evaluating OCEAN (Five factor) indicators. It investigates the\nfeasibility of extracting psychometric parameters through computer-based games,\nutilizing underlying improvements in the area of modern artificial\nintelligence. The candidates for the test are subjected to a number scenarios\nas part of the computer based game and their reactions/responses are used to\nevaluate their psychometric parameters. As part of the study, the parameters\nobtained from the game were compared with those evaluated using paper based\ntests and scores given by a panel of psychologists. The achieved results were\nvery promising.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:37:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Lahiri", "Anirban", ""], ["Mitra", "Utanko", ""], ["Sen", "Sunreeta", ""], ["Chakraborty", "Mrinal", ""], ["Kleiman-Weiner", "Max", ""], ["Guha", "Rajlakshmi", ""], ["Mitra", "Pabitra", ""], ["Basu", "Anupam", ""], ["Chakraborty", "Partha Pratim", ""]]}, {"id": "2007.10235", "submitter": "Diane Hosfelt", "authors": "Diane Hosfelt and Nicole Shadowen", "title": "Privacy Implications of Eye Tracking in Mixed Reality", "comments": "Presented at CHI Workshop on Exploring Potentially Abusive Ethical\n  Social and Political Implications of Mixed Reality Research in HCI\n  (https://chi2020.acm.org/accepted-workshops/#W37)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed Reality (MR) devices require a world with always-on sensors and\nreal-time processing applied to their outputs. We have grappled with some of\nthe ethical concerns presented by this scenario, such as bystander privacy\nissues with smartphones and cameras. However, MR technologies demand that we\ndefine and defend privacy in this new paradigm. This paper focuses on the\nchallenges presented by eye tracking and gaze tracking, techniques that have\ncommonly been deployed in the HCI community for years but are now being\nintegrated into MR devices by default.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:25:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Hosfelt", "Diane", ""], ["Shadowen", "Nicole", ""]]}, {"id": "2007.10246", "submitter": "Diane Hosfelt", "authors": "Nicole Shadowen and Diane Hosfelt", "title": "Addressing the Privacy Implications of Mixed Reality: A Regulatory\n  Approach", "comments": "Presented at the CHI 2020 Workshop on Exploring Potentially Abusive\n  Ethical, Social and Political Implications of Mixed Reality Research in HCI\n  (https://chi2020.acm.org/accepted-workshops/#W37)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed reality (MR) technologies are emerging into the mainstream with\naffordable devices like the Oculus Quest. These devices blend the physical and\nvirtual in novel ways that blur the lines that exist in legal precedent, like\nthose between speech and conduct. In this paper, we discuss the challenges of\nregulating immersive technologies, focusing on the potential for extensive data\ncollection, and examine the trade-offs of three potential approaches to\nprotecting data privacy in the context of mixed reality environments.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 16:35:17 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Shadowen", "Nicole", ""], ["Hosfelt", "Diane", ""]]}, {"id": "2007.10333", "submitter": "Karan Yang", "authors": "Karan Yang, Chengxi Zang, Fei Wang", "title": "Visualizing Deep Graph Generative Models for Drug Discovery", "comments": "4 pages, 2020 KDD Workshop on Applied Data Science for Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug discovery aims at designing novel molecules with specific desired\nproperties for clinical trials. Over past decades, drug discovery and\ndevelopment have been a costly and time consuming process. Driven by big\nchemical data and AI, deep generative models show great potential to accelerate\nthe drug discovery process. Existing works investigate different deep\ngenerative frameworks for molecular generation, however, less attention has\nbeen paid to the visualization tools to quickly demo and evaluate model's\nresults. Here, we propose a visualization framework which provides interactive\nvisualization tools to visualize molecules generated during the encoding and\ndecoding process of deep graph generative models, and provide real time\nmolecular optimization functionalities. Our work tries to empower black box AI\ndriven drug discovery models with some visual interpretabilities.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 18:49:10 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Yang", "Karan", ""], ["Zang", "Chengxi", ""], ["Wang", "Fei", ""]]}, {"id": "2007.10497", "submitter": "Shayan Hassantabar", "authors": "Shayan Hassantabar, Novati Stefano, Vishweshwar Ghanakota, Alessandra\n  Ferrari, Gregory N. Nicola, Raffaele Bruno, Ignazio R. Marino, Kenza\n  Hamidouche, and Niraj K. Jha", "title": "CovidDeep: SARS-CoV-2/COVID-19 Test Based on Wearable Medical Sensors\n  and Efficient Neural Networks", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The novel coronavirus (SARS-CoV-2) has led to a pandemic. The current testing\nregime based on Reverse Transcription-Polymerase Chain Reaction for SARS-CoV-2\nhas been unable to keep up with testing demands, and also suffers from a\nrelatively low positive detection rate in the early stages of the resultant\nCOVID-19 disease. Hence, there is a need for an alternative approach for\nrepeated large-scale testing of SARS-CoV-2/COVID-19. We propose a framework\ncalled CovidDeep that combines efficient DNNs with commercially available WMSs\nfor pervasive testing of the virus. We collected data from 87 individuals,\nspanning three cohorts including healthy, asymptomatic, and symptomatic\npatients. We trained DNNs on various subsets of the features automatically\nextracted from six WMS and questionnaire categories to perform ablation studies\nto determine which subsets are most efficacious in terms of test accuracy for a\nthree-way classification. The highest test accuracy obtained was 98.1%. We also\naugmented the real training dataset with a synthetic training dataset drawn\nfrom the same probability distribution to impose a prior on DNN weights and\nleveraged a grow-and-prune synthesis paradigm to learn both DNN architecture\nand weights. This boosted the accuracy of the various DNNs further and\nsimultaneously reduced their size and floating-point operations.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 21:47:28 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 14:23:38 GMT"}, {"version": "v3", "created": "Wed, 28 Oct 2020 23:12:41 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hassantabar", "Shayan", ""], ["Stefano", "Novati", ""], ["Ghanakota", "Vishweshwar", ""], ["Ferrari", "Alessandra", ""], ["Nicola", "Gregory N.", ""], ["Bruno", "Raffaele", ""], ["Marino", "Ignazio R.", ""], ["Hamidouche", "Kenza", ""], ["Jha", "Niraj K.", ""]]}, {"id": "2007.10609", "submitter": "Gromit Yeuk-Yin Chan", "authors": "Gromit Yeuk-Yin Chan and Jun Yuan and Kyle Overton and Brian Barr and\n  Kim Rees and Luis Gustavo Nonato and Enrico Bertini and Claudio T. Silva", "title": "SUBPLEX: Towards a Better Understanding of Black Box Model Explanations\n  at the Subpopulation Level", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the interpretation of machine learning (ML) models has been of\nparamount importance when making decisions with societal impacts such as\ntransport control, financial activities, and medical diagnosis. While current\nmodel interpretation methodologies focus on using locally linear functions to\napproximate the models or creating self-explanatory models that give\nexplanations to each input instance, they do not focus on model interpretation\nat the subpopulation level, which is the understanding of model interpretations\nacross different subset aggregations in a dataset. To address the challenges of\nproviding explanations of an ML model across the whole dataset, we propose\nSUBPLEX, a visual analytics system to help users understand black-box model\nexplanations with subpopulation visual analysis. SUBPLEX is designed through an\niterative design process with machine learning researchers to address three\nusage scenarios of real-life machine learning tasks: model debugging, feature\nselection, and bias detection. The system applies novel subpopulation analysis\non ML model explanations and interactive visualization to explore the\nexplanations on a dataset with different levels of granularity. Based on the\nsystem, we conduct user evaluation to assess how understanding the\ninterpretation at a subpopulation level influences the sense-making process of\ninterpreting ML models from a user's perspective. Our results suggest that by\nproviding model explanations for different groups of data, SUBPLEX encourages\nusers to generate more ingenious ideas to enrich the interpretations. It also\nhelps users to acquire a tight integration between programming workflow and\nvisual analytics workflow. Last but not least, we summarize the considerations\nobserved in applying visualization to machine learning interpretations.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 05:35:14 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chan", "Gromit Yeuk-Yin", ""], ["Yuan", "Jun", ""], ["Overton", "Kyle", ""], ["Barr", "Brian", ""], ["Rees", "Kim", ""], ["Nonato", "Luis Gustavo", ""], ["Bertini", "Enrico", ""], ["Silva", "Claudio T.", ""]]}, {"id": "2007.10614", "submitter": "Gromit Yeuk-Yin Chan", "authors": "Gromit Yeuk-Yin Chan and Enrico Bertini and Luis Gustavo Nonato and\n  Brian Barr and Claudio T. Silva", "title": "Melody: Generating and Visualizing Machine Learning Model Summary to\n  Understand Data and Classifiers Together", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing sophistication of machine learning models, there are\ngrowing trends of developing model explanation techniques that focus on only\none instance (local explanation) to ensure faithfulness to the original model.\nWhile these techniques provide accurate model interpretability on various data\nprimitive (e.g., tabular, image, or text), a holistic Explainable Artificial\nIntelligence (XAI) experience also requires a global explanation of the model\nand dataset to enable sensemaking in different granularity. Thus, there is a\nvast potential in synergizing the model explanation and visual analytics\napproaches. In this paper, we present MELODY, an interactive algorithm to\nconstruct an optimal global overview of the model and data behavior by\nsummarizing the local explanations using information theory. The result (i.e.,\nan explanation summary) does not require additional learning models,\nrestrictions of data primitives, or the knowledge of machine learning from the\nusers. We also design MELODY UI, an interactive visual analytics system to\ndemonstrate how the explanation summary connects the dots in various XAI tasks\nfrom a global overview to local inspections. We present three usage scenarios\nregarding tabular, image, and text classifications to illustrate how to\ngeneralize model interpretability of different data. Our experiments show that\nour approaches: (1) provides a better explanation summary compared to a\nstraightforward information-theoretic summarization and (2) achieves a\nsignificant speedup in the end-to-end data modeling pipeline.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 06:19:03 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Chan", "Gromit Yeuk-Yin", ""], ["Bertini", "Enrico", ""], ["Nonato", "Luis Gustavo", ""], ["Barr", "Brian", ""], ["Silva", "Claudio T.", ""]]}, {"id": "2007.10884", "submitter": "Patrick Ebel", "authors": "Patrick Ebel, Florian Brokhausen, Andreas Vogelsang", "title": "The Role and Potentials of Field User Interaction Data in the Automotive\n  UX Development Lifecycle: An Industry Perspective", "comments": null, "journal-ref": null, "doi": "10.1145/3409120.3410638", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in the role of field user interaction data in the\ndevelopment of IVIS, the potentials practitioners see in analyzing this data,\nthe concerns they share, and how this compares to companies with digital\nproducts. We conducted interviews with 14 UX professionals, 8 from automotive\nand 6 from digital companies, and analyzed the results by emergent thematic\ncoding. Our key findings indicate that implicit feedback through field user\ninteraction data is currently not evident in the automotive UX development\nprocess. Most decisions regarding the design of IVIS are made based on personal\npreferences and the intuitions of stakeholders. However, the interviewees also\nindicated that user interaction data has the potential to lower the influence\nof guesswork and assumptions in the UX design process and can help to make the\nUX development lifecycle more evidence-based and user-centered.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:13:34 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Ebel", "Patrick", ""], ["Brokhausen", "Florian", ""], ["Vogelsang", "Andreas", ""]]}, {"id": "2007.10897", "submitter": "Jonathan Tirado MSc.", "authors": "Jonathan Tirado, Vladislav Panov, Vibol Yem, Dzmitry Tsetserukou, and\n  Hiroyuki Kajimoto", "title": "ElectroAR: Distributed Electro-tactile Stimulation for Tactile Transfer", "comments": "Acepted at EuroHaptics Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ElectroAR, a visual and tactile sharing system for hand skills\ntraining. This system comprises a head-mounted display (HMD), two cameras,\ntactile sensing glove, and electro-stimulation glove. The trainee wears a\ntactile sensing glove that gets pressure data from touching different objects.\nHis movements are recorded by two cameras, which are located in front and top\nside of the workspace. In the remote site, the trainer wears the\nelectro-tactile stimulation glove. This glove transforms the remotely collected\npressure data to electro-tactile stimuli. Additionally, the trainer wears an\nHMD to see and guide the movements of the trainee. The key part of this project\nis to combine distributed tactile sensor and electro-tactile display to let the\ntrainer understand what the trainee is doing. Results show our system supports\na higher user recognition performance.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 15:37:58 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Tirado", "Jonathan", ""], ["Panov", "Vladislav", ""], ["Yem", "Vibol", ""], ["Tsetserukou", "Dzmitry", ""], ["Kajimoto", "Hiroyuki", ""]]}, {"id": "2007.11117", "submitter": "Matteo Terzi", "authors": "Mattia Carletti, Matteo Terzi, Gian Antonio Susto", "title": "Interpretable Anomaly Detection with DIFFI: Depth-based Isolation Forest\n  Feature Importance", "comments": "Added pseudocode and overview diagram", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly Detection is an unsupervised learning task aimed at detecting\nanomalous behaviours with respect to historical data. In particular,\nmultivariate Anomaly Detection has an important role in many applications\nthanks to the capability of summarizing the status of a complex system or\nobserved phenomenon with a single indicator (typically called `Anomaly Score')\nand thanks to the unsupervised nature of the task that does not require human\ntagging. The Isolation Forest is one of the most commonly adopted algorithms in\nthe field of Anomaly Detection, due to its proven effectiveness and low\ncomputational complexity. A major problem affecting Isolation Forest is\nrepresented by the lack of interpretability, an effect of the inherent\nrandomness governing the splits performed by the Isolation Trees, the building\nblocks of the Isolation Forest. In this paper we propose effective, yet\ncomputationally inexpensive, methods to define feature importance scores at\nboth global and local level for the Isolation Forest. Moreover, we define a\nprocedure to perform unsupervised feature selection for Anomaly Detection\nproblems based on our interpretability method; such procedure also serves the\npurpose of tackling the challenging task of feature importance evaluation in\nunsupervised anomaly detection. We assess the performance on several synthetic\nand real-world datasets, including comparisons against state-of-the-art\ninterpretability techniques, and make the code publicly available to enhance\nreproducibility and foster research in the field.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:19:21 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 13:15:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Carletti", "Mattia", ""], ["Terzi", "Matteo", ""], ["Susto", "Gian Antonio", ""]]}, {"id": "2007.11119", "submitter": "Ziv Epstein", "authors": "Ziv Epstein, Oc\\'eane Boulais, Skylar Gordon, and Matt Groh", "title": "Interpolating GANs to Scaffold Autotelic Creativity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent space modeled by generative adversarial networks (GANs) represents\na large possibility space. By interpolating categories generated by GANs, it is\npossible to create novel hybrid images. We present \"Meet the Ganimals,\" a\ncasual creator built on interpolations of BigGAN that can generate novel,\nhybrid animals called ganimals by efficiently searching this possibility space.\nLike traditional casual creators, the system supports a simple creative flow\nthat encourages rapid exploration of the possibility space. Users can discover\nnew ganimals, create their own, and share their reactions to aesthetic,\nemotional, and morphological characteristics of the ganimals. As users provide\ninput to the system, the system adapts and changes the distribution of\ncategories upon which ganimals are generated. As one of the first GAN-based\ncasual creators, Meet the Ganimals is an example how casual creators can\nleverage human curation and citizen science to discover novel artifacts within\na large possibility space.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 22:29:07 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Epstein", "Ziv", ""], ["Boulais", "Oc\u00e9ane", ""], ["Gordon", "Skylar", ""], ["Groh", "Matt", ""]]}, {"id": "2007.11151", "submitter": "Luke Hallum", "authors": "Luke E Hallum, Shaun L Cloherty", "title": "Liquid-crystal display (LCD) of achromatic, mean-modulated flicker in\n  clinical assessment and experimental studies of visual systems", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0248180", "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achromatic, mean-modulated flicker (wherein luminance increments and\ndecrements of equal magnitude are applied, over time, to a test field) is\ncommonly used in both clinical assessment of vision and experimental studies of\nvisual systems. However, presenting flicker on computer-controlled displays is\nproblematic; displays typically introduce luminance artifacts at high flicker\nfrequency or contrast, potentially interfering with the validity of findings.\nHere, we present a battery of tests used to weigh the relative merits of two\ndisplays for presenting achromatic, mean-modulated flicker. These tests\nrevealed marked differences between a new high-performance liquid-crystal\ndisplay (LCD; EIZO ColorEdge CG247X) and a new consumer-grade LCD (Dell\nU2415b), despite displays' vendor-supplied specifications being almost\nidentical. We measured displayed luminance using a spot meter and a linearized\nphotodiode. We derived several measures, including spatial uniformity, the\neffect of viewing angle, response times, Fourier amplitude spectra, and\ncycle-averaged luminance. We presented paired luminance pulses to quantify the\ndisplays' nonlinear dynamics. The CG247X showed relatively good spatial\nuniformity (e.g., at moderate luminance, standard deviation 2.8% versus\nU2415b's 5.3%). Fourier transformation of nominally static test patches\nrevealed spectra free of artifacts, with the exception of a frame response. The\nCG247X's rise and fall times depended on both the luminance from which, and to\nwhich, it responded, as is to be generally expected from LCDs. Despite this\nnonlinear behaviour, we were able to define a contrast and frequency range\nwherein the CG247X appeared largely artifact-free; the relationship between\nnominal luminance and displayed luminance was accurately modelled using a\ncausal, linear time-invariant system. This range included contrasts up to 80%,\nand flicker frequencies up to 30 Hz.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 01:10:16 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 04:12:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Hallum", "Luke E", ""], ["Cloherty", "Shaun L", ""]]}, {"id": "2007.11199", "submitter": "Jiahao Li", "authors": "Jiahao Li, Meilin Cui, Jeeeun Kim, Xiang 'Anthony' Chen", "title": "Romeo: A Design Tool for Embedding Transformable Parts in 3D Models to\n  Robotically Augment Default Functionalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconfiguring shapes of objects enables transforming existing passive objects\nwith robotic functionalities, e.g., a transformable coffee cup holder can be\nattached to a chair's armrest, a piggy bank can reach out an arm to 'steal'\ncoins. Despite the advance in end-user 3D design and fabrication, it remains\nchallenging for non-experts to create such 'transformables' using existing\ntools due to the requirement of specific engineering knowledge such as\nmechanisms and robotic design.\n  We present Romeo -- a design tool for creating transformables to robotically\naugment objects' default functionalities. Romeo allows users to transform an\nobject into a robotic arm by expressing at a high level what type of task is\nexpected. Users can select which part of the object to be transformed, specify\nmotion points in space for the transformed part to follow and the corresponding\naction to be taken. Romeo then automatically generates a robotic arm embedded\nin the transformable part ready for fabrication. A design session validated\nthis tool where participants used Romeo to accomplish controlled design tasks\nand to open-endedly create coin-stealing piggy banks by transforming 3D objects\nof their own choice.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 04:54:43 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Li", "Jiahao", ""], ["Cui", "Meilin", ""], ["Kim", "Jeeeun", ""], ["Chen", "Xiang 'Anthony'", ""]]}, {"id": "2007.11210", "submitter": "Benjamin Zi Hao Zhao", "authors": "Benjamin Zi Hao Zhao, Hassan Jameel Asghar, Mohamed Ali Kaafar,\n  Francesca Trevisan and Haiyue Yuan", "title": "Exploiting Behavioral Side-Channels in Observation Resilient Cognitive\n  Authentication Schemes", "comments": "Accepted into ACM Transactions on Privacy and Security. 32 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observation Resilient Authentication Schemes (ORAS) are a class of shared\nsecret challenge-response identification schemes where a user mentally computes\nthe response via a cognitive function to authenticate herself such that\neavesdroppers cannot readily extract the secret. Security evaluation of ORAS\ngenerally involves quantifying information leaked via observed\nchallenge-response pairs. However, little work has evaluated information leaked\nvia human behavior while interacting with these schemes. A common way to\nachieve observation resilience is by including a modulus operation in the\ncognitive function. This minimizes the information leaked about the secret due\nto the many-to-one map from the set of possible secrets to a given response. In\nthis work, we show that user behavior can be used as a side-channel to obtain\nthe secret in such ORAS. Specifically, the user's eye-movement patterns and\nassociated timing information can deduce whether a modulus operation was\nperformed (a fundamental design element), to leak information about the secret.\nWe further show that the secret can still be retrieved if the deduction is\nerroneous, a more likely case in practice. We treat the vulnerability\nanalytically, and propose a generic attack algorithm that iteratively obtains\nthe secret despite the \"faulty\" modulus information. We demonstrate the attack\non five ORAS, and show that the secret can be retrieved with considerably less\nchallenge-response pairs than non-side-channel attacks (e.g.,\nalgebraic/statistical attacks). In particular, our attack is applicable on\nMod10, a one-time-pad based scheme, for which no non-side-channel attack\nexists. We field test our attack with a small-scale eye-tracking user study.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 05:45:43 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Zhao", "Benjamin Zi Hao", ""], ["Asghar", "Hassan Jameel", ""], ["Kaafar", "Mohamed Ali", ""], ["Trevisan", "Francesca", ""], ["Yuan", "Haiyue", ""]]}, {"id": "2007.11218", "submitter": "Araz Taeihagh", "authors": "Mikolaj firlej, Araz Taeihagh", "title": "Regulating human control over autonomous systems", "comments": null, "journal-ref": "Regulation and Governance (2020)", "doi": "10.1111/rego.12344", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many sectors have experienced significant progress in\nautomation, associated with the growing advances in artificial intelligence and\nmachine learning. There are already automated robotic weapons, which are able\nto evaluate and engage with targets on their own, and there are already\nautonomous vehicles that do not need a human driver. It is argued that the use\nof increasingly autonomous systems (AS) should be guided by the policy of human\ncontrol, according to which humans should execute a certain significant level\nof judgment over AS. While in the military sector there is a fear that AS could\nmean that humans lose control over life and death decisions, in the\ntransportation domain, on the contrary, there is a strongly held view that\nautonomy could bring significant operational benefits by removing the need for\na human driver. This article explores the notion of human control in the United\nStates in the two domains of defense and transportation. The operationalization\nof emerging policies of human control results in the typology of direct and\nindirect human controls exercised over the use of AS. The typology helps to\nsteer the debate away from the linguistic complexities of the term autonomy. It\nidentifies instead where human factors are undergoing important changes and\nultimately informs about more detailed rules and standards formulation, which\ndiffer across domains, applications, and sectors.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 06:05:41 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["firlej", "Mikolaj", ""], ["Taeihagh", "Araz", ""]]}, {"id": "2007.11346", "submitter": "Mononito Goswami Mr.", "authors": "Mononito Goswami, Minkush Manuja and Maitree Leekha", "title": "Towards Social & Engaging Peer Learning: Predicting Backchanneling and\n  Disengagement in Children", "comments": "14 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social robots and interactive computer applications have the potential to\nfoster early language development in young children by acting as peer learning\ncompanions. However, studies have found that children only trust robots which\nbehave in a natural and interpersonal manner. To help robots come across as\nengaging and attentive peer learning companions, we develop models to predict\nwhether the listener will lose attention (Listener Disengagement Prediction,\nLDP) and the extent to which a robot should generate backchanneling responses\n(Backchanneling Extent Prediction, BEP) in the next few seconds. We pose LDP\nand BEP as time series classification problems and conduct several experiments\nto assess the impact of different time series characteristics and feature sets\non the predictive performance of our model. Using statistics & machine\nlearning, we also examine which socio-demographic factors influence the amount\nof time children spend backchanneling and listening to their peers. To lend\ninterpretability to our models, we also analyzed critical features responsible\nfor their predictive performance. Our experiments revealed the utility of\nmultimodal features such as pupil dilation, blink rate, head movements, facial\naction units which have never been used before. We also found that the dynamics\nof time series features are rich predictors of listener disengagement and\nbackchanneling.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 11:16:42 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Goswami", "Mononito", ""], ["Manuja", "Minkush", ""], ["Leekha", "Maitree", ""]]}, {"id": "2007.11512", "submitter": "Thomas Ortner MMSc", "authors": "Thomas Ortner, Andreas Walch, Rebecca Nowak, Robert Barnes, Thomas\n  H\\\"ollt, Eduard Gr\\\"oller", "title": "InCorr: Interactive Data-Driven Correlation Panels for Digital Outcrop\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geological analysis of 3D Digital Outcrop Models (DOMs) for reconstruction of\nancient habitable environments is a key aspect of the upcoming ESA ExoMars 2022\nRosalind Franklin Rover and the NASA 2020 Rover Perseverance missions in\nseeking signs of past life on Mars. Geologists measure and interpret 3D DOMs,\ncreate sedimentary logs and combine them in `correlation panels' to map the\nextents of key geological horizons, and build a stratigraphic model to\nunderstand their position in the ancient landscape. Currently, the creation of\ncorrelation panels is completely manual and therefore time-consuming, and\ninflexible. With InCorr we present a visualization solution that encompasses a\n3D logging tool and an interactive data-driven correlation panel that evolves\nwith the stratigraphic analysis. For the creation of InCorr we closely\ncooperated with leading planetary geologists in the form of a design study. We\nverify our results by recreating an existing correlation analysis with InCorr\nand validate our correlation panel against a manually created illustration.\nFurther, we conducted a user-study with a wider circle of geologists. Our\nevaluation shows that InCorr efficiently supports the domain experts in\ntackling their research questions and that it has the potential to\nsignificantly impact how geologists work with digital outcrop representations\nin general.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 16:08:31 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 11:10:09 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Ortner", "Thomas", ""], ["Walch", "Andreas", ""], ["Nowak", "Rebecca", ""], ["Barnes", "Robert", ""], ["H\u00f6llt", "Thomas", ""], ["Gr\u00f6ller", "Eduard", ""]]}, {"id": "2007.11686", "submitter": "Alastair Donaldson", "authors": "Alastair F. Donaldson", "title": "A report on the first virtual PLDI conference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a report on the PLDI 2020 conference, for which I was General Chair,\nwhich was held virtually for the first time as a result of the COVID-19\npandemic. The report contains: my personal reflections on the positive and\nnegative aspects of the event; a description of the format of the event and\nassociated logistical details; and data (with some analysis) on attendees'\nviews on the conference format, the extent to which attendees engaged with the\nconference, attendees' views on virtual vs. physical conferences (with a focus\non PLDI specifically) and the diversity of conference registrants. I hope that\nthe report will be a useful resource for organizers of upcoming virtual\nconferences, and generally interesting to the Programming Languages community\nand beyond.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:19:58 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Donaldson", "Alastair F.", ""]]}, {"id": "2007.11740", "submitter": "EPTCS", "authors": "Connor Basich (University of Massachusetts Amherst), Justin Svegliato\n  (University of Massachusetts Amherst), Kyle Hollins Wray (Alliance Innovation\n  Lab Silicon Valley), Stefan J. Witwicki (Alliance Innovation Lab Silicon\n  Valley), Shlomo Zilberstein (University of Massachuetts Amherst)", "title": "Improving Competence for Reliable Autonomy", "comments": "In Proceedings AREA 2020, arXiv:2007.11260", "journal-ref": "EPTCS 319, 2020, pp. 37-53", "doi": "10.4204/EPTCS.319.4", "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the complexity of real-world, unstructured domains, it is often\nimpossible or impractical to design models that include every feature needed to\nhandle all possible scenarios that an autonomous system may encounter. For an\nautonomous system to be reliable in such domains, it should have the ability to\nimprove its competence online. In this paper, we propose a method for improving\nthe competence of a system over the course of its deployment. We specifically\nfocus on a class of semi-autonomous systems known as competence-aware systems\nthat model their own competence -- the optimal extent of autonomy to use in any\ngiven situation -- and learn this competence over time from feedback received\nthrough interactions with a human authority. Our method exploits such feedback\nto identify important state features missing from the system's initial model,\nand incorporates them into its state representation. The result is an agent\nthat better predicts human involvement, leading to improvements in its\ncompetence and reliability, and as a result, its overall performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 01:31:28 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Basich", "Connor", "", "University of Massachusetts Amherst"], ["Svegliato", "Justin", "", "University of Massachusetts Amherst"], ["Wray", "Kyle Hollins", "", "Alliance Innovation\n  Lab Silicon Valley"], ["Witwicki", "Stefan J.", "", "Alliance Innovation Lab Silicon\n  Valley"], ["Zilberstein", "Shlomo", "", "University of Massachuetts Amherst"]]}, {"id": "2007.11742", "submitter": "EPTCS", "authors": "Davide Ancona (University of Genova, DIBRIS), Chiara Bassano\n  (University of Genova, DIBRIS), Manuela Chessa (University of Genova,\n  DIBRIS), Viviana Mascardi (University of Genova, DIBRIS), Fabio Solari\n  (University of Genova, DIBRIS)", "title": "Engineering Reliable Interactions in the Reality-Artificiality Continuum", "comments": "In Proceedings AREA 2020, arXiv:2007.11260", "journal-ref": "EPTCS 319, 2020, pp. 69-80", "doi": "10.4204/EPTCS.319.6", "report-no": null, "categories": "cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Milgram's reality-virtuality continuum applies to interaction in the physical\nspace dimension, going from real to virtual. However, interaction has a social\ndimension as well, that can go from real to artificial depending on the\ncompanion with whom the user interacts. In this paper we present our vision of\nthe Reality-Artificiality bidimensional Continuum (RAC), we identify some\nchallenges in its design and development and we discuss how reliable\ninteractions might be supported inside RAC.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 01:32:00 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Ancona", "Davide", "", "University of Genova, DIBRIS"], ["Bassano", "Chiara", "", "University of Genova, DIBRIS"], ["Chessa", "Manuela", "", "University of Genova,\n  DIBRIS"], ["Mascardi", "Viviana", "", "University of Genova, DIBRIS"], ["Solari", "Fabio", "", "University of Genova, DIBRIS"]]}, {"id": "2007.12061", "submitter": "Angus Lamb", "authors": "Zichao Wang, Angus Lamb, Evgeny Saveliev, Pashmina Cameron, Yordan\n  Zaykov, Jos\\'e Miguel Hern\\'andez-Lobato, Richard E. Turner, Richard G.\n  Baraniuk, Craig Barton, Simon Peyton Jones, Simon Woodhead, Cheng Zhang", "title": "Instructions and Guide for Diagnostic Questions: The NeurIPS 2020\n  Education Challenge", "comments": "28 pages, 6 figures, NeurIPS 2020 Competition Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital technologies are becoming increasingly prevalent in education,\nenabling personalized, high quality education resources to be accessible by\nstudents across the world. Importantly, among these resources are diagnostic\nquestions: the answers that the students give to these questions reveal key\ninformation about the specific nature of misconceptions that the students may\nhold. Analyzing the massive quantities of data stemming from students'\ninteractions with these diagnostic questions can help us more accurately\nunderstand the students' learning status and thus allow us to automate learning\ncurriculum recommendations. In this competition, participants will focus on the\nstudents' answer records to these multiple-choice diagnostic questions, with\nthe aim of 1) accurately predicting which answers the students provide; 2)\naccurately predicting which questions have high quality; and 3) determining a\npersonalized sequence of questions for each student that best predicts the\nstudent's answers. These tasks closely mimic the goals of a real-world\neducational platform and are highly representative of the educational\nchallenges faced today. We provide over 20 million examples of students'\nanswers to mathematics questions from Eedi, a leading educational platform\nwhich thousands of students interact with daily around the globe. Participants\nto this competition have a chance to make a lasting, real-world impact on the\nquality of personalized education for millions of students across the world.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 15:17:36 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 07:22:32 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 22:45:06 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wang", "Zichao", ""], ["Lamb", "Angus", ""], ["Saveliev", "Evgeny", ""], ["Cameron", "Pashmina", ""], ["Zaykov", "Yordan", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Turner", "Richard E.", ""], ["Baraniuk", "Richard G.", ""], ["Barton", "Craig", ""], ["Jones", "Simon Peyton", ""], ["Woodhead", "Simon", ""], ["Zhang", "Cheng", ""]]}, {"id": "2007.12236", "submitter": "Jamy Li", "authors": "Jamy Li, Daniel Davison, Bob Schadenberg, Pauline Chevalier, Alyssa\n  Alcorn, Alria Williams, Suncica Petrovic, Snezana Babovic Dimitrijevic, Jie\n  Shen, Liz Pellicano, Vanessa Evers", "title": "Usability of a Robot's Realistic Facial Expressions and Peripherals in\n  Autistic Children's Therapy", "comments": "4 pages, 5 figures, 2nd Workshop on Social Robots in Therapy and\n  Care. 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI\n  2019)", "journal-ref": null, "doi": null, "report-no": "SREC/2019/02", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot-assisted therapy is an emerging form of therapy for autistic children,\nalthough designing effective robot behaviors is a challenge for effective\nimplementation of such therapy. A series of usability tests assessed trends in\nthe effectiveness of modelling a robot's facial expressions on realistic facial\nexpressions and of adding peripherals enabling child-led control of emotion\nlearning activities with autistic children. Nineteen autistic children\ninteracted with a small humanoid robot and an adult therapist in several\nemotion-learning activities that featured realistic facial expressions modelled\non either a pre-existing database or live facial mirroring, and that used\nperipherals (tablets or tangible 'squishies') to enable child-led activities.\nBoth types of realistic facial expressions by the robot were less effective\nthan exaggerated expressions, with the mirroring being unintuitive for\nchildren. The tablet was usable but required more feedback and lower latency,\nwhile the tactile tangibles were engaging aids.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 20:13:11 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Li", "Jamy", ""], ["Davison", "Daniel", ""], ["Schadenberg", "Bob", ""], ["Chevalier", "Pauline", ""], ["Alcorn", "Alyssa", ""], ["Williams", "Alria", ""], ["Petrovic", "Suncica", ""], ["Dimitrijevic", "Snezana Babovic", ""], ["Shen", "Jie", ""], ["Pellicano", "Liz", ""], ["Evers", "Vanessa", ""]]}, {"id": "2007.12238", "submitter": "Hendrik Strobelt", "authors": "Alexander M. Rush, Hendrik Strobelt", "title": "MiniConf -- A Virtual Conference Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MiniConf is a framework for hosting virtual academic conferences motivated by\nthe sudden inability for these events to be hosted globally. The framework is\ndesigned to be global and asynchronous, interactive, and to promote browsing\nand discovery. We developed the system to be sustainable and maintainable, in\nparticular ensuring that it is open-source, easy to setup, and scalable on\nminimal hardware. In this technical report, we discuss design decisions,\nprovide technical detail, and show examples of a case study deployment.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 17:58:22 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Rush", "Alexander M.", ""], ["Strobelt", "Hendrik", ""]]}, {"id": "2007.12248", "submitter": "Eric Chu", "authors": "Eric Chu, Deb Roy, Jacob Andreas", "title": "Are Visual Explanations Useful? A Case Study in Model-in-the-Loop\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized controlled trial for a model-in-the-loop regression\ntask, with the goal of measuring the extent to which (1) good explanations of\nmodel predictions increase human accuracy, and (2) faulty explanations decrease\nhuman trust in the model. We study explanations based on visual saliency in an\nimage-based age prediction task for which humans and learned models are\nindividually capable but not highly proficient and frequently disagree. Our\nexperimental design separates model quality from explanation quality, and makes\nit possible to compare treatments involving a variety of explanations of\nvarying levels of quality. We find that presenting model predictions improves\nhuman accuracy. However, visual explanations of various kinds fail to\nsignificantly alter human accuracy or trust in the model - regardless of\nwhether explanations characterize an accurate model, an inaccurate one, or are\ngenerated randomly and independently of the input image. These findings suggest\nthe need for greater evaluation of explanations in downstream decision making\ntasks, better design-based tools for presenting explanations to users, and\nbetter approaches for generating explanations.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 20:39:40 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Chu", "Eric", ""], ["Roy", "Deb", ""], ["Andreas", "Jacob", ""]]}, {"id": "2007.12312", "submitter": "Ashlesha Nesarikar", "authors": "Ashlesha Nesarikar (University of Texas at Dallas and Plano\n  Intelligence), Waqas Haque (UT Southwestern), Suchith Vuppala (UT\n  Southwestern), Abhijit Nesarikar (Plano Intelligence)", "title": "COVID-19 Remote Patient Monitoring: Social Impact of AI", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A primary indicator of success in the fight against COVID-19 is avoiding\nstress on critical care infrastructure and services (CCIS). However, CCIS will\nlikely remain stressed until sustained herd immunity is built. There are also\nsecondary considerations for success: mitigating economic damage; curbing the\nspread of misinformation, improving morale, and preserving a sense of control;\nbuilding global trust for diplomacy, trade and travel; and restoring\nreliability and normalcy to day-to-day life, among others. We envision\ntechnology plays a pivotal role. Here, we focus on the effective use of readily\navailable technology to improve the primary and secondary success criteria for\nthe fight against SARS-CoV-2. In a multifaceted technology approach, we start\nwith effective technology use for remote patient monitoring (RPM) of COVID-19\nwith the following objectives:\n  1. Deploy readily available technology for continuous real-time remote\nmonitoring of patient vitals with the help of biosensors on a large scale.\n  2. Effective and safe remote large-scale communitywide care of low-severity\ncases as a buffer against surges in COVID-19 hospitalizations to reduce strain\non critical care services and emergency hospitals.\n  3. Improve the patient, their family, and their community's sense of control\nand morale.\n  4. Propose a clear technology and medical definition of remote patient\nmonitoring for COVID-19 to address an urgent technology need; address\nobfuscated, narrow, and erroneous information and provide examples; and urge\npublishers to be clear and complete in their disclosures.\n  5. Leverage the cloud-based distributed cognitive RPM platform for community\nleaders and decision makers to enable planning and resource management,\npandemic research, damage prevention and containment, and receiving feedback on\nstrategies and executions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 01:09:56 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Nesarikar", "Ashlesha", "", "University of Texas at Dallas and Plano\n  Intelligence"], ["Haque", "Waqas", "", "UT Southwestern"], ["Vuppala", "Suchith", "", "UT\n  Southwestern"], ["Nesarikar", "Abhijit", "", "Plano Intelligence"]]}, {"id": "2007.12328", "submitter": "Zheng Wang", "authors": "Edric John Cruz Nacpil, Zheng Wang, Zhanhong Yan, Tsutomu Kaizuka, and\n  Kimihiko Nakano", "title": "Surface Electromyography-controlled Pedestrian Collision Avoidance: A\n  Driving Simulator Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drivers with disabilities such as hemiplegia or unilateral upper limb\namputation restricting steering wheel operation to one arm could encounter the\nchallenge of stabilizing vehicles during pedestrian collision avoidance. An\nsEMG-controlled steering assistance system was developed for these drivers to\nenable rapid steering wheel rotation with only one healthy arm. Test drivers\nwere recruited to use the Myo armband as a sEMG-based interface to perform\npedestrian collision avoidance in a driving simulator. It was hypothesized that\nthe sEMG-based interface would be comparable or superior in vehicle stability\nto manual takeover from automated driving and conventional steering wheel\noperation. The Myo armband interface was significantly superior to manual\ntakeover from automated driving and comparable to manual steering wheel\noperation. The results of the driving simulator trials confirm the feasibility\nof the sEMG-controlled system as a safe alternative that could benefit drivers\nwith the aforesaid disabilities.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 03:27:50 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Nacpil", "Edric John Cruz", ""], ["Wang", "Zheng", ""], ["Yan", "Zhanhong", ""], ["Kaizuka", "Tsutomu", ""], ["Nakano", "Kimihiko", ""]]}, {"id": "2007.12332", "submitter": "Shahryar Rahnamayan", "authors": "Kyle Robert Harrison, Azam Asilian Bidgoli, Shahryar Rahnamayan,\n  Kalyanmoy Deb", "title": "Image-Based Benchmarking and Visualization for Large-Scale Global\n  Optimization", "comments": "Preprint submitted to Applied Intelligence. 43 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of optimization, visualization techniques can be useful for\nunderstanding the behaviour of optimization algorithms and can even provide a\nmeans to facilitate human interaction with an optimizer. Towards this goal, an\nimage-based visualization framework, without dimension reduction, that\nvisualizes the solutions to large-scale global optimization problems as images\nis proposed. In the proposed framework, the pixels visualize decision variables\nwhile the entire image represents the overall solution quality. This framework\naffords a number of benefits over existing visualization techniques including\nenhanced scalability (in terms of the number of decision variables),\nfacilitation of standard image processing techniques, providing nearly infinite\nbenchmark cases, and explicit alignment with human perception. Furthermore,\nimage-based visualization can be used to visualize the optimization process in\nreal-time, thereby allowing the user to ascertain characteristics of the search\nprocess as it is progressing. To the best of the authors' knowledge, this is\nthe first realization of a dimension-preserving, scalable visualization\nframework that embeds the inherent relationship between decision space and\nobjective space. The proposed framework is utilized with 10 different mapping\nschemes on an image-reconstruction problem that encompass continuous, discrete,\nbinary, combinatorial, constrained, dynamic, and multi-objective optimization.\nThe proposed framework is then demonstrated on arbitrary benchmark problems\nwith known optima. Experimental results elucidate the flexibility and\ndemonstrate how valuable information about the search process can be gathered\nvia the proposed visualization framework.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 03:39:23 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Harrison", "Kyle Robert", ""], ["Bidgoli", "Azam Asilian", ""], ["Rahnamayan", "Shahryar", ""], ["Deb", "Kalyanmoy", ""]]}, {"id": "2007.12346", "submitter": "Bum Chul Kwon", "authors": "Bum Chul Kwon", "title": "User-driven Analysis of Longitudinal Health Data with Hidden Markov\n  Models for Clinical Insights", "comments": "Demo presented in KDD 2020 Workshop on Applied Data Science for\n  Healthcare", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A goal of clinical researchers is to understand the progression of a disease\nthrough a set of biomarkers. Researchers often conduct observational studies,\nwhere they collect numerous samples from selected subjects throughout multiple\nyears. Hidden Markov Models (HMMs) can be applied to discover latent states and\ntheir transition probabilities over time. However, it is challenging for\nclinical researchers to interpret the outcomes and to gain insights about the\ndisease. Thus, this demo introduces an interactive visualization system called\nDPVis, which was designed to help researchers to interactively explore HMM\noutcomes. The demo provides guidelines of how to implement the\nclinician-in-the-loop approach for analyzing longitudinal, observational health\ndata with visual analytics.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 04:36:08 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Kwon", "Bum Chul", ""]]}, {"id": "2007.12353", "submitter": "Flora D. Salim", "authors": "Sam Nolan, Shakila Khan Rumi, Christoph Anderson, Klaus David, Flora\n  D. Salim", "title": "Exploring the Impact of COVID-19 Lockdown on Social Roles and Emotions\n  while Working from Home", "comments": "9 pages, Accepted at The New Future of Work Symposium, Microsoft,\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the opening months of 2020, COVID-19 changed the way for which people\nwork, forcing more people to work from home. This research investigates the\nimpact of COVID-19 on five researchers' work and private roles, happiness, and\nmobile and desktop activity patterns. Desktop and smartphone application usage\nwere gathered before and during COVID-19. Individuals' roles and happiness were\ncaptured through experience sampling. Our analysis show that researchers tend\nto work more during COVID-19 resulting an imbalance of work and private roles.\nWe also found that as working styles and patterns as well as individual\nbehaviour changed, reported valence distribution was less varied in the later\nweeks of the pandemic when compared to the start. This shows a resilient\nadaptation to the disruption caused by the pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 05:17:01 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Nolan", "Sam", ""], ["Rumi", "Shakila Khan", ""], ["Anderson", "Christoph", ""], ["David", "Klaus", ""], ["Salim", "Flora D.", ""]]}, {"id": "2007.12506", "submitter": "Sinan Kalkan", "authors": "Jonas Tjomsland, Sinan Kalkan, Hatice Gunes", "title": "Mind Your Manners! A Dataset and A Continual Learning Approach for\n  Assessing Social Appropriateness of Robot Actions", "comments": "Human-Robot Interaction; Social Robotics; Social Appropriateness;\n  Continual Learning. Submitted to the RO-MAN 2020 Workshop on Lifelong\n  Learning for Long-term Human-Robot Interaction (LL4LHRI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, endowing robots with an ability to assess social appropriateness of\ntheir actions has not been possible. This has been mainly due to (i) the lack\nof relevant and labelled data, and (ii) the lack of formulations of this as a\nlifelong learning problem. In this paper, we address these two issues. We first\nintroduce the Socially Appropriate Domestic Robot Actions dataset (MANNERS-DB),\nwhich contains appropriateness labels of robot actions annotated by humans. To\nbe able to control but vary the configurations of the scenes and the social\nsettings, MANNERS-DB has been created utilising a simulation environment by\nuniformly sampling relevant contextual attributes. Secondly, we train and\nevaluate a baseline Bayesian Neural Network (BNN) that estimates social\nappropriateness of actions in the MANNERS-DB. Finally, we formulate learning\nsocial appropriateness of actions as a continual learning problem using the\nuncertainty of the BNN parameters. The experimental results show that the\nsocial appropriateness of robot actions can be predicted with a satisfactory\nlevel of precision. Our work takes robots one step closer to a human-like\nunderstanding of (social) appropriateness of actions, with respect to the\nsocial context they operate in. To facilitate reproducibility and further\nprogress in this area, the MANNERS-DB, the trained models and the relevant code\nwill be made publicly available.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 12:56:33 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Tjomsland", "Jonas", ""], ["Kalkan", "Sinan", ""], ["Gunes", "Hatice", ""]]}, {"id": "2007.12656", "submitter": "Hangxin Liu", "authors": "Shuwen Qiu, Hangxin Liu, Zeyu Zhang, Yixin Zhu, Song-Chun Zhu", "title": "Human-Robot Interaction in a Shared Augmented Reality Workspace", "comments": "Accepted to IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and develop a new shared Augmented Reality (AR) workspace for\nHuman-Robot Interaction (HRI), which establishes a bi-directional communication\nbetween human agents and robots. In a prototype system, the shared AR workspace\nenables a shared perception, so that a physical robot not only perceives the\nvirtual elements in its own view but also infers the utility of the human\nagent--the cost needed to perceive and interact in AR--by sensing the human\nagent's gaze and pose. Such a new HRI design also affords a shared\nmanipulation, wherein the physical robot can control and alter virtual objects\nin AR as an active agent; crucially, a robot can proactively interact with\nhuman agents, instead of purely passively executing received commands. In\nexperiments, we design a resource collection game that qualitatively\ndemonstrates how a robot perceives, processes, and manipulates in AR and\nquantitatively evaluates the efficacy of HRI using the shared AR workspace. We\nfurther discuss how the system can potentially benefit future HRI studies that\nare otherwise challenging.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:18:30 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 04:27:26 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Qiu", "Shuwen", ""], ["Liu", "Hangxin", ""], ["Zhang", "Zeyu", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2007.12665", "submitter": "Michael Segundo Ortiz", "authors": "Michael Segundo Ortiz", "title": "Considerations for Eye Tracking Experiments in Information Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this survey I discuss ophthalmic neurophysiology and the experimental\nconsiderations that must be made to reduce possible noise in an eye-tracking\ndata stream. I also review the history, experiments, technological benefits and\nlimitations of eye-tracking within the information retrieval field. The\nconcepts of aware and adaptive user interfaces are also explored that humbly\nmake an attempt to synthesize work from the fields of industrial engineering\nand psychophysiology with information retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 17:31:54 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Ortiz", "Michael Segundo", ""]]}, {"id": "2007.12761", "submitter": "Temiloluwa Prioleau", "authors": "Temiloluwa Prioleau, Ashutosh Sabharwal, Madhuri M. Vasudevan", "title": "Understanding Reflection Needs for Personal Health Data in Diabetes", "comments": "11 pages, 6 figures, paper to appear in Pervasive Health 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To empower users of wearable medical devices, it is important to enable\nmethods that facilitate reflection on previous care to improve future outcomes.\nIn this work, we conducted a two-phase user-study involving patients,\ncaregivers, and clinicians to understand gaps in current approaches that\nsupport reflection and user needs for new solutions. Our results show that\nusers desire to have specific summarization metrics, solutions that minimize\ncognitive effort, and solutions that enable data integration to support\nmeaningful reflection on diabetes management. In addition, we developed and\nevaluated a visualization called PixelGrid that presents key metrics in a\nmatrix-based plot. Majority of users (84%) found the matrix-based approach to\nbe useful for identifying salient patterns related to certain times and days in\nblood glucose data. Through our evaluation we identified that users desire data\nvisualization solutions with complementary textual descriptors, concise and\nflexible presentation, contextually-fitting content, and informative and\nactionable insights. Directions for future research on tools that automate\npattern discovery, detect abnormalities, and provide recommendations to improve\ncare were also identified.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 20:29:33 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Prioleau", "Temiloluwa", ""], ["Sabharwal", "Ashutosh", ""], ["Vasudevan", "Madhuri M.", ""]]}, {"id": "2007.12802", "submitter": "Lida Zhang", "authors": "Lida Zhang, Nathan C. Hurley, Bassem Ibrahim, Erica Spatz, Harlan M.\n  Krumholz, Roozbeh Jafari, Bobak J. Mortazavi", "title": "Developing Personalized Models of Blood Pressure Estimation from\n  Wearable Sensors Data Using Minimally-trained Domain Adversarial Neural\n  Networks", "comments": "Published at MLHC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blood pressure monitoring is an essential component of hypertension\nmanagement and in the prediction of associated comorbidities. Blood pressure is\na dynamic vital sign with frequent changes throughout a given day. Capturing\nblood pressure remotely and frequently (also known as ambulatory blood pressure\nmonitoring) has traditionally been achieved by measuring blood pressure at\ndiscrete intervals using an inflatable cuff. However, there is growing interest\nin developing a cuffless ambulatory blood pressure monitoring system to measure\nblood pressure continuously. One such approach is by utilizing bioimpedance\nsensors to build regression models. A practical problem with this approach is\nthat the amount of data required to confidently train such a regression model\ncan be prohibitive. In this paper, we propose the application of the\ndomain-adversarial training neural network (DANN) method on our multitask\nlearning (MTL) blood pressure estimation model, allowing for knowledge transfer\nbetween subjects. Our proposed model obtains average root mean square error\n(RMSE) of $4.80 \\pm 0.74$ mmHg for diastolic blood pressure and $7.34 \\pm 1.88$\nmmHg for systolic blood pressure when using three minutes of training data,\n$4.64 \\pm 0.60$ mmHg and $7.10 \\pm 1.79$ respectively when using four minutes\nof training data, and $4.48 \\pm 0.57$ mmHg and $6.79 \\pm 1.70$ respectively\nwhen using five minutes of training data. DANN improves training with minimal\ndata in comparison to both directly training and to training with a pretrained\nmodel from another subject, decreasing RMSE by $0.19$ to $0.26$ mmHg\n(diastolic) and by $0.46$ to $0.67$ mmHg (systolic) in comparison to the best\nbaseline models. We observe that four minutes of training data is the minimum\nrequirement for our framework to exceed ISO standards within this cohort of\npatients.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 23:26:28 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zhang", "Lida", ""], ["Hurley", "Nathan C.", ""], ["Ibrahim", "Bassem", ""], ["Spatz", "Erica", ""], ["Krumholz", "Harlan M.", ""], ["Jafari", "Roozbeh", ""], ["Mortazavi", "Bobak J.", ""]]}, {"id": "2007.12803", "submitter": "Xiaofeng Gao", "authors": "Xiaofeng Gao, Ran Gong, Yizhou Zhao, Shu Wang, Tianmin Shu, Song-Chun\n  Zhu", "title": "Joint Mind Modeling for Explanation Generation in Complex Human-Robot\n  Collaborative Tasks", "comments": "IEEE International Conference on Robot and Human Interactive\n  Communication (RO-MAN 2020), 8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human collaborators can effectively communicate with their partners to finish\na common task by inferring each other's mental states (e.g., goals, beliefs,\nand desires). Such mind-aware communication minimizes the discrepancy among\ncollaborators' mental states, and is crucial to the success in human ad-hoc\nteaming. We believe that robots collaborating with human users should\ndemonstrate similar pedagogic behavior. Thus, in this paper, we propose a novel\nexplainable AI (XAI) framework for achieving human-like communication in\nhuman-robot collaborations, where the robot builds a hierarchical mind model of\nthe human user and generates explanations of its own mind as a form of\ncommunications based on its online Bayesian inference of the user's mental\nstate. To evaluate our framework, we conduct a user study on a real-time\nhuman-robot cooking task. Experimental results show that the generated\nexplanations of our approach significantly improves the collaboration\nperformance and user perception of the robot. Code and video demos are\navailable on our project website: https://xfgao.github.io/xCookingWeb/.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 23:35:03 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gao", "Xiaofeng", ""], ["Gong", "Ran", ""], ["Zhao", "Yizhou", ""], ["Wang", "Shu", ""], ["Shu", "Tianmin", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2007.12841", "submitter": "Naeemul Hassan", "authors": "Md Mahfuzul Haque, Mohammad Yousuf, Ahmed Shatil Alam, Pratyasha Saha,\n  Syed Ishtiaque Ahmed, Naeemul Hassan", "title": "Combating Misinformation in Bangladesh: Roles and Responsibilities as\n  Perceived by Journalists, Fact-checkers, and Users", "comments": null, "journal-ref": null, "doi": "10.1145/3415201", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a growing interest within CSCW community in understanding the\ncharacteristics of misinformation propagated through computational media, and\nthe devising techniques to address the associated challenges. However, most\nwork in this area has been concentrated on the cases in the western world\nleaving a major portion of this problem unaddressed that is situated in the\nGlobal South. This paper aims to broaden the scope of this discourse by\nfocusing on this problem in the context of Bangladesh, a country in the Global\nSouth. The spread of misinformation on Facebook in Bangladesh, a country with a\npopulation over 163 million, has resulted in chaos, hate attacks, and killings.\nBy interviewing journalists, fact-checkers, in addition to surveying the\ngeneral public, we analyzed the current state of verifying misinformation in\nBangladesh. Our findings show that most people in the `news audience' want the\nnews media to verify the authenticity of online information that they see\nonline. However, the newspaper journalists say that fact-checking online\ninformation is not a part of their job, and it is also beyond their capacity\ngiven the amount of information being published online everyday. We further\nfind that the voluntary fact-checkers in Bangladesh are not equipped with\nsufficient infrastructural support to fill in this gap. We show how our\nfindings are connected to some of the core concerns of CSCW community around\nsocial media, collaboration, infrastructural politics, and information\ninequality. From our analysis, we also suggest several pathways to increase the\nimpact of fact-checking efforts through collaboration, technology design, and\ninfrastructure development.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 03:03:20 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 13:12:05 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 04:01:13 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Haque", "Md Mahfuzul", ""], ["Yousuf", "Mohammad", ""], ["Alam", "Ahmed Shatil", ""], ["Saha", "Pratyasha", ""], ["Ahmed", "Syed Ishtiaque", ""], ["Hassan", "Naeemul", ""]]}, {"id": "2007.13048", "submitter": "Emre Ugur", "authors": "Tugce Akkoc, Emre Ugur and Inci Ayhan", "title": "Trick the Body Trick the Mind: Avatar representation affects the\n  perception of available action possibilities in Virtual Reality", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In immersive Virtual Reality (VR), your brain can trick you into believing\nthat your virtual hands are your real hands. Manipulating the representation of\nthe body, namely the avatar, is a potentially powerful tool for the design of\ninnovative interactive systems in VR. In this study, we investigated\ninteractive behavior in VR by using the methods of experimental psychology.\nObjects with handles are known to potentiate the afforded action. Participants\ntend to respond faster when the handle is on the same side as the responding\nhand in bi-manual speed response tasks. In the first experiment, we\nsuccessfully replicated this affordance effect in a Virtual Reality (VR)\nsetting. In the second experiment, we showed that the affordance effect was\ninfluenced by the avatar, which was manipulated by two different hand types: 1)\nhand models with full finger tracking that are able to grasp objects, and 2)\ncapsule-shaped -- fingerless -- hand models that are not able to grasp objects.\nWe found that less than 5 minutes of adaptation to an avatar, significantly\naltered the affordance perception. Counter intuitively, action planning was\nsignificantly shorter with the hand model that is not able to grasp. Possibly,\nfewer action possibilities provided an advantage in processing time. The\npresence of a handle speeded up the initiation of the hand movement but slowed\ndown the action completion because of ongoing action planning. The results were\nexamined from a multidisciplinary perspective and the design implications for\nVR applications were discussed.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 03:35:08 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Akkoc", "Tugce", ""], ["Ugur", "Emre", ""], ["Ayhan", "Inci", ""]]}, {"id": "2007.13069", "submitter": "Yunqi Qiu", "authors": "Bin Fu, Yunqi Qiu, Chengguang Tang, Yang Li, Haiyang Yu, Jian Sun", "title": "A Survey on Complex Question Answering over Knowledge Base: Recent\n  Advances and Challenges", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question Answering (QA) over Knowledge Base (KB) aims to automatically answer\nnatural language questions via well-structured relation information between\nentities stored in knowledge bases. In order to make KBQA more applicable in\nactual scenarios, researchers have shifted their attention from simple\nquestions to complex questions, which require more KB triples and constraint\ninference. In this paper, we introduce the recent advances in complex QA.\nBesides traditional methods relying on templates and rules, the research is\ncategorized into a taxonomy that contains two main branches, namely Information\nRetrieval-based and Neural Semantic Parsing-based. After describing the methods\nof these branches, we analyze directions for future research and introduce the\nmodels proposed by the Alime team.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 07:13:32 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Fu", "Bin", ""], ["Qiu", "Yunqi", ""], ["Tang", "Chengguang", ""], ["Li", "Yang", ""], ["Yu", "Haiyang", ""], ["Sun", "Jian", ""]]}, {"id": "2007.13090", "submitter": "Seonwook Park", "authors": "Anna Maria Feit and Lukas Vordemann and Seonwook Park and Caterina\n  B\\'erub\\'e and Otmar Hilliges", "title": "Detecting Relevance during Decision-Making from Eye Movements for UI\n  Adaptation", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": "10.1145/3379155.3391321", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an approach to detect information relevance during\ndecision-making from eye movements in order to enable user interface\nadaptation. This is a challenging task because gaze behavior varies greatly\nacross individual users and tasks and groundtruth data is difficult to obtain.\nThus, prior work has mostly focused on simpler target-search tasks or on\nestablishing general interest, where gaze behavior is less complex. From the\nliterature, we identify six metrics that capture different aspects of the gaze\nbehavior during decision-making and combine them in a voting scheme. We\nempirically show, that this accounts for the large variations in gaze behavior\nand out-performs standalone metrics. Importantly, it offers an intuitive way to\ncontrol the amount of detected information, which is crucial for different UI\nadaptation schemes to succeed. We show the applicability of our approach by\ndeveloping a room-search application that changes the visual saliency of\ncontent detected as relevant. In an empirical study, we show that it detects up\nto 97% of relevant elements with respect to user self-reporting, which allows\nus to meaningfully adapt the interface, as confirmed by participants. Our\napproach is fast, does not need any explicit user input and can be applied\nindependent of task and user.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 10:06:37 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Feit", "Anna Maria", ""], ["Vordemann", "Lukas", ""], ["Park", "Seonwook", ""], ["B\u00e9rub\u00e9", "Caterina", ""], ["Hilliges", "Otmar", ""]]}, {"id": "2007.13117", "submitter": "Simon Perrault", "authors": "Pavithren V S Pakianathan, Simon Perrault", "title": "Towards Inclusive Design for Privacy and Security Perspectives from an\n  Aging Society", "comments": "4 + 2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, older adults in Singapore have been massively\nconnecting to the Internet using Smartphone. However due to the ever-changing\nnature of Technology and Cybersecurity landscape, an older adult's limited\ntechnical and Privacy and Security (P & S) knowledge, experience and declining\ncognitive and physical abilities puts them at higher risks. Furthermore\nmainstream smartphone applications, which are generally not designed with older\nadults in mind, could result in mismatched mental models thereby creating\nusability issues. We interviewed 10 older adults above 65 and 10 adults\nassisting them based in Singapore to investigate how smartphone P & S can be\nredesigned inclusively by addressing the needs of older adults and people who\nsupport them. Our results show that socio-cultural factors affected the process\nof getting or providing P & S help, culture and attitude affected learning\nbehaviours and older adults expressed heterogeneous P & S preferences based on\ncontextual factors and level of convenience, however there are opportunities\nfor the mechanisms to be senior-friendly. Due to the complex relationship\nbetween an older adult's milieu and technology, we aim to utilize a technology\nprobe to investigate further and contribute towards an inclusive P & S model.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 12:31:58 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Pakianathan", "Pavithren V S", ""], ["Perrault", "Simon", ""]]}, {"id": "2007.13151", "submitter": "Yaohui Guo", "authors": "Yaohui Guo, X. Jessie Yang", "title": "Modeling and Predicting Trust Dynamics in Human-Robot Teaming: A\n  Bayesian Inference Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trust in automation, or more recently trust in autonomy, has received\nextensive research attention in the past two decades. The majority of prior\nliterature adopted a \"snapshot\" view of trust and typically evaluated trust\nthrough questionnaires administered at the end of an experiment. This\n\"snapshot\" view, however, does not acknowledge that trust is a time-variant\nvariable that can strengthen or decay over time. To fill the research gap, the\npresent study aims to model trust dynamics when a human interacts with a\nrobotic agent over time. The underlying premise of the study is that by\ninteracting with a robotic agent and observing its performance over time, a\nrational human agent will update his/her trust in the robotic agent\naccordingly. Based on this premise, we develop a personalized trust prediction\nmodel based on Beta distribution and learn its parameters using Bayesian\ninference. Our proposed model adheres to three major properties of trust\ndynamics reported in prior empirical studies. We tested the proposed method\nusing an existing dataset involving 39 human participants interacting with four\ndrones in a simulated surveillance mission. The proposed method obtained a Root\nMean Square Error (RMSE) of 0.072, significantly outperforming existing\nprediction methods. Moreover, we identified three distinctive types of trust\ndynamics, the Bayesian decision maker, the oscillator, and the disbeliever,\nrespectively. This prediction model can be used for the design of\nindividualized and adaptive technologies.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 15:41:55 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 15:09:54 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2020 01:09:13 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Guo", "Yaohui", ""], ["Yang", "X. Jessie", ""]]}, {"id": "2007.13188", "submitter": "Sanchari Das", "authors": "Swapna Joshi, Kostas Stavrianakis, Sanchari Das", "title": "Substituting Restorative Benefits of Being Outdoors through Interactive\n  Augmented Spatial Soundscapes", "comments": null, "journal-ref": "ASSET '20: The 22nd International ACM SIGACCESS Conference on\n  Computers and Accessibility, October 26--28, 2020, Athens, Greece (Virtual\n  Conference: Poster)", "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geriatric depression is a common mental health condition affecting majority\nof older adults in the US. As per Attention Restoration Theory (ART),\nparticipation in outdoor activities is known to reduce depression and provide\nrestorative benefits. However, many older adults, who suffer from depression,\nespecially those who receive care in organizational settings, have less access\nto sensory experiences of the outdoor natural environment. This is often due to\ntheir physical or cognitive limitations and from lack of organizational\nresources to support outdoor activities. To address this, we plan to study how\ntechnology can bring the restorative benefits of outdoors to the indoor\nenvironments through augmented spatial natural soundscapes. Thus, we propose an\ninterview and observation-based study at an assisted living facility to\nevaluate how augmented soundscapes substitute for outdoor restorative, social,\nand experiential benefits. We aim to integrate these findings into a minimally\nintrusive and intuitive design of an interactive augmented soundscape, for\nindoor organizational care settings.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 18:00:04 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 20:28:10 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Joshi", "Swapna", ""], ["Stavrianakis", "Kostas", ""], ["Das", "Sanchari", ""]]}, {"id": "2007.13256", "submitter": "Yara Rizk", "authors": "Yara Rizk, Vatche Isahagian, Scott Boag, Yasaman Khazaeni, Merve\n  Unuvar, Vinod Muthusamy, Rania Khalaf", "title": "A Conversational Digital Assistant for Intelligent Process Automation", "comments": "International Conference on Business Process Management 2020 RPA\n  Forum", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic process automation (RPA) has emerged as the leading approach to\nautomate tasks in business processes. Moving away from back-end automation, RPA\nautomated the mouse-click on user interfaces; this outside-in approach reduced\nthe overhead of updating legacy software. However, its many shortcomings,\nnamely its lack of accessibility to business users, have prevented its\nwidespread adoption in highly regulated industries. In this work, we explore\ninteractive automation in the form of a conversational digital assistant. It\nallows business users to interact with and customize their automation solutions\nthrough natural language. The framework, which creates such assistants, relies\non a multi-agent orchestration model and conversational wrappers for autonomous\nagents including RPAs. We demonstrate the effectiveness of our proposed\napproach on a loan approval business process and a travel preapproval business\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 00:38:13 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Rizk", "Yara", ""], ["Isahagian", "Vatche", ""], ["Boag", "Scott", ""], ["Khazaeni", "Yasaman", ""], ["Unuvar", "Merve", ""], ["Muthusamy", "Vinod", ""], ["Khalaf", "Rania", ""]]}, {"id": "2007.13281", "submitter": "Ke Wang", "authors": "Ke Wang, Gang Li, Junlan Chen, Yan Long, Tao Chen, Long Chen and Qin\n  Xia", "title": "The Adaptability and Challenges of Autonomous Vehicles to Pedestrians in\n  Urban China", "comments": "24 pages, 9 figures", "journal-ref": "Accident Analysis and Prevention. 145(2020), 105692", "doi": "10.1016/j.aap.2020.105692", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  China is the world's largest automotive market and is ambitious for\nautonomous vehicles (AVs) development. As one of the key goals of AVs,\npedestrian safety is an important issue in China. Despite the rapid development\nof driverless technologies in recent years, there is a lack of researches on\nthe adaptability of AVs to pedestrians. To fill the gap, this study would\ndiscuss the adaptability of current driverless technologies to China urban\npedestrians by reviewing the latest researches. The paper firstly analyzed\ntypical Chinese pedestrian behaviors and summarized the safety demands of\npedestrians for AVs through articles and open database data, which are worked\nas the evaluation criteria. Then, corresponding driverless technologies are\ncarefully reviewed. Finally, the adaptability would be given combining the\nabove analyses. Our review found that autonomous vehicles have trouble in the\noccluded pedestrian environment and Chinese pedestrians do not accept AVs well.\nAnd more explorations should be conducted on standard human-machine\ninteraction, interaction information overload avoidance, occluded pedestrians\ndetection and nation-based receptivity research. The conclusions are very\nuseful for motor corporations and driverless car researchers to place more\nattention on the complexity of the Chinese pedestrian environment, for\ntransportation experts to protect pedestrian safety in the context of AVs, and\nfor governors to think about making new pedestrians policies to welcome the\nupcoming driverless cars.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 02:40:18 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Ke", ""], ["Li", "Gang", ""], ["Chen", "Junlan", ""], ["Long", "Yan", ""], ["Chen", "Tao", ""], ["Chen", "Long", ""], ["Xia", "Qin", ""]]}, {"id": "2007.13371", "submitter": "Lia Morra", "authors": "Lia Morra, Fabrizio Lamberti, F. Gabriele Prattic\\'o, Salvatore La\n  Rosa, Paolo Montuschi", "title": "Building Trust in Autonomous Vehicles: Role of Virtual Reality Driving\n  Simulators in HMI Design", "comments": null, "journal-ref": "IEEE Transactions on Vehicular Technology, 68(10), pp.9438-9450,\n  2019", "doi": "10.1109/TVT.2019.2933601", "report-no": null, "categories": "cs.HC cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The investigation of factors contributing at making humans trust Autonomous\nVehicles (AVs) will play a fundamental role in the adoption of such technology.\nThe user's ability to form a mental model of the AV, which is crucial to\nestablish trust, depends on effective user-vehicle communication; thus, the\nimportance of Human-Machine Interaction (HMI) is poised to increase. In this\nwork, we propose a methodology to validate the user experience in AVs based on\ncontinuous, objective information gathered from physiological signals, while\nthe user is immersed in a Virtual Reality-based driving simulation. We applied\nthis methodology to the design of a head-up display interface delivering visual\ncues about the vehicle' sensory and planning systems. Through this approach, we\nobtained qualitative and quantitative evidence that a complete picture of the\nvehicle's surrounding, despite the higher cognitive load, is conducive to a\nless stressful experience. Moreover, after having been exposed to a more\ninformative interface, users involved in the study were also more willing to\ntest a real AV. The proposed methodology could be extended by adjusting the\nsimulation environment, the HMI and/or the vehicle's Artificial Intelligence\nmodules to dig into other aspects of the user experience.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:42:07 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Morra", "Lia", ""], ["Lamberti", "Fabrizio", ""], ["Prattic\u00f3", "F. Gabriele", ""], ["La Rosa", "Salvatore", ""], ["Montuschi", "Paolo", ""]]}, {"id": "2007.13372", "submitter": "Dinislam Abdulgalimov", "authors": "Dinislam Abdulgalimov, Timur Osadchiy", "title": "Our House is Our Glassy Castle: Challenges of Pervasive Computing in\n  Private Spaces", "comments": "5 pages, CHI 2017 conference, \"Making Home: Asserting Agency in the\n  Age of IoT\" workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Modern society is going through the transformation under the influence of\nInformation Technologies. Internet of Things as one of the latest facet of it\nbecoming more visible and widely spread. We wish to reflect and discuss the\ncurrent concerns regarding its expansion. Our particular interests lie in the\nincreasing of usability and comfortability through the unification of the IoT\nprotocols and security measures. As well as addressing the privacy concerns and\ndiscussing the possible changings in the perception of privacy and personal\nspace concepts.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 08:42:48 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Abdulgalimov", "Dinislam", ""], ["Osadchiy", "Timur", ""]]}, {"id": "2007.13737", "submitter": "Teena Sharma Ms.", "authors": "Nishchal K. Verma, T. Sharma, S. Dixit, P. Agrawal, S. Sengupta, and\n  V. Singh", "title": "BIDEAL: A Toolbox for Bicluster Analysis -- Generation, Visualization\n  and Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.HC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel toolbox named BIDEAL for the generation of\nbiclusters, their analysis, visualization, and validation. The objective is to\nfacilitate researchers to use forefront biclustering algorithms embedded on a\nsingle platform. A single toolbox comprising various biclustering algorithms\nplay a vital role to extract meaningful patterns from the data for detecting\ndiseases, biomarkers, gene-drug association, etc. BIDEAL consists of seventeen\nbiclustering algorithms, three biclusters visualization techniques, and six\nvalidation indices. The toolbox can analyze several types of data, including\nbiological data through a graphical user interface. It also facilitates data\npreprocessing techniques i.e., binarization, discretization, normalization,\nelimination of null and missing values. The effectiveness of the developed\ntoolbox has been presented through testing and validations on Saccharomyces\ncerevisiae cell cycle, Leukemia cancer, Mammary tissue profile, and Ligand\nscreen in B-cells datasets. The biclusters of these datasets have been\ngenerated using BIDEAL and evaluated in terms of coherency, differential\nco-expression ranking, and similarity measure. The visualization of generated\nbiclusters has also been provided through a heat map and gene plot.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 15:24:53 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Verma", "Nishchal K.", ""], ["Sharma", "T.", ""], ["Dixit", "S.", ""], ["Agrawal", "P.", ""], ["Sengupta", "S.", ""], ["Singh", "V.", ""]]}, {"id": "2007.13872", "submitter": "Paul Rosen", "authors": "Ghulam Jilani Quadri and Paul Rosen", "title": "Modeling the Influence of Visual Density on Cluster Perception in\n  Scatterplots Using Topology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scatterplots are used for a variety of visual analytics tasks, including\ncluster identification, and the visual encodings used on a scatterplot play a\ndeciding role on the level of visual separation of clusters. For visualization\ndesigners, optimizing the visual encodings is crucial to maximizing the clarity\nof data. This requires accurately modeling human perception of cluster\nseparation, which remains challenging. We present a multi-stage user study\nfocusing on four factors---distribution size of clusters, number of points,\nsize of points, and opacity of points---that influence cluster identification\nin scatterplots. From these parameters, we have constructed two models, a\ndistance-based model, and a density-based model, using the merge tree data\nstructure from Topological Data Analysis. Our analysis demonstrates that these\nfactors play an important role in the number of clusters perceived, and it\nverifies that the distance-based and density-based models can reasonably\nestimate the number of clusters a user observes. Finally, we demonstrate how\nthese models can be used to optimize visual encodings on real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:22:22 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 01:44:56 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Quadri", "Ghulam Jilani", ""], ["Rosen", "Paul", ""]]}, {"id": "2007.13882", "submitter": "Paul Rosen", "authors": "Paul Rosen and Ghulam Jilani Quadri", "title": "LineSmooth: An Analytical Framework for Evaluating the Effectiveness of\n  Smoothing Techniques on Line Charts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive framework for evaluating line chart smoothing\nmethods under a variety of visual analytics tasks. Line charts are commonly\nused to visualize a series of data samples. When the number of samples is\nlarge, or the data are noisy, smoothing can be applied to make the signal more\napparent. However, there are a wide variety of smoothing techniques available,\nand the effectiveness of each depends upon both nature of the data and the\nvisual analytics task at hand. To date, the visualization community lacks a\nsummary work for analyzing and classifying the various smoothing methods\navailable. In this paper, we establish a framework, based on 8 measures of the\nline smoothing effectiveness tied to 8 low-level visual analytics tasks. We\nthen analyze 12 methods coming from 4 commonly used classes of line chart\nsmoothing---rank filters, convolutional filters, frequency domain filters, and\nsubsampling. The results show that while no method is ideal for all situations,\ncertain methods, such as Gaussian filters and Topology-based subsampling,\nperform well in general. Other methods, such as low-pass cutoff filters and\nDouglas-Peucker subsampling, perform well for specific visual analytics tasks.\nAlmost as importantly, our framework demonstrates that several methods,\nincluding the commonly used uniform subsampling, produce low-quality results,\nand should, therefore, be avoided, if possible.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:45:19 GMT"}, {"version": "v2", "created": "Sat, 19 Sep 2020 01:32:57 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Rosen", "Paul", ""], ["Quadri", "Ghulam Jilani", ""]]}, {"id": "2007.14038", "submitter": "Matthias Stierle", "authors": "Daniel Viner, Matthias Stierle, Martin Matzner", "title": "A Process Mining Software Comparison", "comments": null, "journal-ref": "Proceedings of the ICPM Doctoral Consortium and Tool Demonstration\n  Track 2020 co-located with the 2nd International Conference on Process Mining\n  (ICPM2020), volume 2703 of CEUR Workshop Proceedings, pages 19-22, 2020", "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  www.processmining-software.com is a dedicated website for process mining\nsoftware comparison and was developed to give practitioners and researchers an\noverview of commercial tools available on the market. Based on literature\nreview and experimental tool testing, a set of criteria was developed in order\nto assess the tools' functional capabilities in an objective manner. With our\npublicly accessible website, we intend to increase the transparency of tool\nfunctionality. Being an academic endeavour, the non-commercial nature of the\nstudy ensures a less biased assessment as compared with reports from analyst\nfirms.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 07:36:21 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 18:42:55 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 12:16:42 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Viner", "Daniel", ""], ["Stierle", "Matthias", ""], ["Matzner", "Martin", ""]]}, {"id": "2007.14273", "submitter": "Liz Dowthwaite", "authors": "Liz Dowthwaite, Elvira Perez Vallejos, Helen Creswick, Virginia\n  Portillo, Menisha Patel, Jun Zhao", "title": "Developing a measure of online wellbeing and user trust", "comments": "Full paper, 8 pages", "journal-ref": "In Societal Challenges in the Smart Society pp21-33. Ethicomp Book\n  Series, Universidad de La Rioja (2020)", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the first stage of the ongoing development of two scales\nto measure online wellbeing and trust, based on the results of a series of\nworkshops with younger and older adults. The first, the Online Wellbeing Scale\nincludes subscales covering both psychological, or eudaimonic, wellbeing and\nsubjective, or hedonic, wellbeing, as well as digital literacy and online\nactivity; the overall aim is to understand how a user's online experiences\naffect their wellbeing. The second scale, the Trust Index includes three\nsubscales covering the importance of trust to the user, trusting beliefs, and\ncontextual factors; the aim for this scale is to examine trust in online\nalgorithm-driven systems. The scales will be used together to aid researchers\nin understanding how trust (or lack of trust) relates to overall wellbeing\nonline. They will also contribute to the development of a suite of tools for\nempowering users to negotiate issues of trust online, as well as in designing\nguidelines for the inclusion of trust considerations in the development of\nonline algorithm-driven systems. The next step is to release the prototype\nscales developed as a result of this pilot in a large online study in to\nvalidate the measures.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 14:40:13 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Dowthwaite", "Liz", ""], ["Vallejos", "Elvira Perez", ""], ["Creswick", "Helen", ""], ["Portillo", "Virginia", ""], ["Patel", "Menisha", ""], ["Zhao", "Jun", ""]]}, {"id": "2007.14372", "submitter": "Weikai Yang", "authors": "Weikai Yang, Zhen Li, Mengchen Liu, Yafeng Lu, Kelei Cao, Ross\n  Maciejewski, Shixia Liu", "title": "Diagnosing Concept Drift with Visual Analytics", "comments": "Accepted for IEEE Conference on Visual Analytics Science and\n  Technology (VAST) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept drift is a phenomenon in which the distribution of a data stream\nchanges over time in unforeseen ways, causing prediction models built on\nhistorical data to become inaccurate. While a variety of automated methods have\nbeen developed to identify when concept drift occurs, there is limited support\nfor analysts who need to understand and correct their models when drift is\ndetected. In this paper, we present a visual analytics method, DriftVis, to\nsupport model builders and analysts in the identification and correction of\nconcept drift in streaming data. DriftVis combines a distribution-based drift\ndetection method with a streaming scatterplot to support the analysis of drift\ncaused by the distribution changes of data streams and to explore the impact of\nthese changes on the model's accuracy. A quantitative experiment and two case\nstudies on weather prediction and text classification have been conducted to\ndemonstrate our proposed tool and illustrate how visual analytics can be used\nto support the detection, examination, and correction of concept drift.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 17:29:43 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 04:42:25 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 04:12:44 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Yang", "Weikai", ""], ["Li", "Zhen", ""], ["Liu", "Mengchen", ""], ["Lu", "Yafeng", ""], ["Cao", "Kelei", ""], ["Maciejewski", "Ross", ""], ["Liu", "Shixia", ""]]}, {"id": "2007.14461", "submitter": "Julian Frommel", "authors": "Julian Frommel, Regan L Mandryk", "title": "Modeling Behaviour to Predict User State: Self-Reports as Ground Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods that detect user states such as emotions are useful for interactive\nsystems. In this position paper, we argue for model-based approaches that are\ntrained on user behaviour and self-reported user state as ground truths. In an\napplication context, they record behaviour, extract relevant features, and use\nthe models to predict user states. We describe how this approach can be\nimplemented and discuss its benefits in comparison to solely self-reports in an\napplication and to models of behaviour without the selfreport ground truths.\nFinally, we discuss shortcomings of this approach by considering its drawbacks\nand limitations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 20:09:09 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Frommel", "Julian", ""], ["Mandryk", "Regan L", ""]]}, {"id": "2007.14516", "submitter": "Alex Kale", "authors": "Alex Kale, Matthew Kay, Jessica Hullman", "title": "Visual Reasoning Strategies for Effect Size Judgments and Decisions", "comments": "Accepted for publication at IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty visualizations often emphasize point estimates to support\nmagnitude estimates or decisions through visual comparison. However, when\ndesign choices emphasize means, users may overlook uncertainty information and\nmisinterpret visual distance as a proxy for effect size. We present findings\nfrom a mixed design experiment on Mechanical Turk which tests eight uncertainty\nvisualization designs: 95% containment intervals, hypothetical outcome plots,\ndensities, and quantile dotplots, each with and without means added. We find\nthat adding means to uncertainty visualizations has small biasing effects on\nboth magnitude estimation and decision-making, consistent with discounting\nuncertainty. We also see that visualization designs that support the least\nbiased effect size estimation do not support the best decision-making,\nsuggesting that a chart user's sense of effect size may not necessarily be\nidentical when they use the same information for different tasks. In a\nqualitative analysis of users' strategy descriptions, we find that many users\nswitch strategies and do not employ an optimal strategy when one exists.\nUncertainty visualizations which are optimally designed in theory may not be\nthe most effective in practice because of the ways that users satisfice with\nheuristics, suggesting opportunities to better understand visualization\neffectiveness by modeling sets of potential strategies.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 22:56:32 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 18:06:03 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2020 20:21:47 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Kale", "Alex", ""], ["Kay", "Matthew", ""], ["Hullman", "Jessica", ""]]}, {"id": "2007.14666", "submitter": "Jun Yuan", "authors": "Jun Yuan, Shouxing Xiang, Jiazhi Xia, Lingyun Yu, Shixia Liu", "title": "Evaluation of Sampling Methods for Scatterplots", "comments": "9 pages + 2 pages reference, 15 figures, IEEE VIS(VAST) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a scatterplot with tens of thousands of points or even more, a natural\nquestion is which sampling method should be used to create a small but \"good\"\nscatterplot for a better abstraction. We present the results of a user study\nthat investigates the influence of different sampling strategies on multi-class\nscatterplots. The main goal of this study is to understand the capability of\nsampling methods in preserving the density, outliers, and overall shape of a\nscatterplot. To this end, we comprehensively review the literature and select\nseven typical sampling strategies as well as eight representative datasets. We\nthen design four experiments to understand the performance of different\nstrategies in maintaining: 1) region density; 2) class density; 3) outliers;\nand 4) overall shape in the sampling results. The results show that: 1) random\nsampling is preferred for preserving region density; 2) blue noise sampling and\nrandom sampling have comparable performance with the three multi-class sampling\nstrategies in preserving class density; 3) outlier biased density based\nsampling, recursive subdivision based sampling, and blue noise sampling perform\nthe best in keeping outliers; and 4) blue noise sampling outperforms the others\nin maintaining the overall shape of a scatterplot.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 08:25:34 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 17:18:36 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 04:50:29 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Yuan", "Jun", ""], ["Xiang", "Shouxing", ""], ["Xia", "Jiazhi", ""], ["Yu", "Lingyun", ""], ["Liu", "Shixia", ""]]}, {"id": "2007.14734", "submitter": "Yushan Pan", "authors": "Yushan Pan and Hans Petter Hildre", "title": "Maritime Design: A CSCW Territory?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on remote-control and autonomous vessels from a\nsociological perspective. We report that if CSCW research aims to shed light on\nother disciplines, researchers should be reflexive insider that first position\nthemselves in such disciplines. Through reflexive practice, CSCW researchers\ncould connect communities of practice, thus narrowing the distance between\ndesign and engineering.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 10:44:36 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Pan", "Yushan", ""], ["Hildre", "Hans Petter", ""]]}, {"id": "2007.14767", "submitter": "Jingbo Zhou", "authors": "Jingbo Zhou, Zhenwei Tang, Min Zhao, Xiang Ge, Fuzhen Zhuang, Meng\n  Zhou, Liming Zou, Chenglei Yang, Hui Xiong", "title": "Intelligent Exploration for User Interface Modules of Mobile App with\n  Collective Learning", "comments": "10 pages, accepted as a full paper in KDD 2020", "journal-ref": null, "doi": "10.1145/3394486.3403387", "report-no": null, "categories": "cs.SE cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mobile app interface usually consists of a set of user interface modules.\nHow to properly design these user interface modules is vital to achieving user\nsatisfaction for a mobile app. However, there are few methods to determine\ndesign variables for user interface modules except for relying on the judgment\nof designers. Usually, a laborious post-processing step is necessary to verify\nthe key change of each design variable. Therefore, there is a only very limited\namount of design solutions that can be tested. It is timeconsuming and almost\nimpossible to figure out the best design solutions as there are many modules.\nTo this end, we introduce FEELER, a framework to fast and intelligently explore\ndesign solutions of user interface modules with a collective machine learning\napproach. FEELER can help designers quantitatively measure the preference score\nof different design solutions, aiming to facilitate the designers to\nconveniently and quickly adjust user interface module. We conducted extensive\nexperimental evaluations on two real-life datasets to demonstrate its\napplicability in real-life cases of user interface module design in the Baidu\nApp, which is one of the most popular mobile apps in China.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 19:00:54 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 17:28:02 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Zhou", "Jingbo", ""], ["Tang", "Zhenwei", ""], ["Zhao", "Min", ""], ["Ge", "Xiang", ""], ["Zhuang", "Fuzhen", ""], ["Zhou", "Meng", ""], ["Zou", "Liming", ""], ["Yang", "Chenglei", ""], ["Xiong", "Hui", ""]]}, {"id": "2007.14886", "submitter": "Martin Schuessler", "authors": "Milagros Miceli and Martin Schuessler and Tianling Yang", "title": "Between Subjectivity and Imposition: Power Dynamics in Data Annotation\n  for Computer Vision", "comments": "accepted for CSCW 2020, will be published in October 2020 issue of\n  PACM HCI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of data is fundamental to machine learning. This paper\ninvestigates practices of image data annotation as performed in industrial\ncontexts. We define data annotation as a sense-making practice, where\nannotators assign meaning to data through the use of labels. Previous\nhuman-centered investigations have largely focused on annotators subjectivity\nas a major cause for biased labels. We propose a wider view on this issue:\nguided by constructivist grounded theory, we conducted several weeks of\nfieldwork at two annotation companies. We analyzed which structures, power\nrelations, and naturalized impositions shape the interpretation of data. Our\nresults show that the work of annotators is profoundly informed by the\ninterests, values, and priorities of other actors above their station.\nArbitrary classifications are vertically imposed on annotators, and through\nthem, on data. This imposition is largely naturalized. Assigning meaning to\ndata is often presented as a technical matter. This paper shows it is, in fact,\nan exercise of power with multiple implications for individuals and society.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:02:56 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 11:03:00 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Miceli", "Milagros", ""], ["Schuessler", "Martin", ""], ["Yang", "Tianling", ""]]}, {"id": "2007.14918", "submitter": "Bin Yang", "authors": "Thomas Buhl Andersen, R\\'ogvi Eliasen, Mikkel Jarlund, Bin Yang", "title": "Force myography benchmark data for hand gesture recognition and transfer\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Force myography has recently gained increasing attention for hand gesture\nrecognition tasks. However, there is a lack of publicly available benchmark\ndata, with most existing studies collecting their own data often with custom\nhardware and for varying sets of gestures. This limits the ability to compare\nvarious algorithms, as well as the possibility for research to be done without\nfirst needing to collect data oneself. We contribute to the advancement of this\nfield by making accessible a benchmark dataset collected using a commercially\navailable sensor setup from 20 persons covering 18 unique gestures, in the hope\nof allowing further comparison of results as well as easier entry into this\nfield of research. We illustrate one use-case for such data, showing how we can\nimprove gesture recognition accuracy by utilising transfer learning to\nincorporate data from multiple other persons. This also illustrates that the\ndataset can serve as a benchmark dataset to facilitate research on transfer\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:43:59 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Andersen", "Thomas Buhl", ""], ["Eliasen", "R\u00f3gvi", ""], ["Jarlund", "Mikkel", ""], ["Yang", "Bin", ""]]}, {"id": "2007.14958", "submitter": "Dewi Yokelson", "authors": "Dewi Yokelson", "title": "Advancing Visual Specification of Code Requirements for Graphs", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in the humanities are among the many who are now exploring the\nworld of big data. They have begun to use programming languages like Python or\nR and their corresponding libraries to manipulate large data sets and discover\nbrand new insights. One of the major hurdles that still exists is incorporating\nvisualizations of this data into their projects. Visualization libraries can be\ndifficult to learn how to use, even for those with formal training. Yet these\nvisualizations are crucial for recognizing themes and communicating results to\nnot only other researchers, but also the general public. This paper focuses on\nproducing meaningful visualizations of data using machine learning. We allow\nthe user to visually specify their code requirements in order to lower the\nbarrier for humanities researchers to learn how to program visualizations. We\nuse a hybrid model, combining a neural network and optical character\nrecognition to generate the code to create the visualization.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:01:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Yokelson", "Dewi", ""]]}, {"id": "2007.14964", "submitter": "David Gotz", "authors": "David Borland, Jonathan Zhang, Smiti Kaul, David Gotz", "title": "Selection-Bias-Corrected Visualization via Dynamic Reweighting", "comments": "This article will be published in IEEE Transactions on Visualization\n  and Computer Graphics (TVCG) in January 2021. The work will also be presented\n  at IEEE VIS 2020. Video figure available here: https://vimeo.com/442775090", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030455", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collection and visual analysis of large-scale data from complex systems,\nsuch as electronic health records or clickstream data, has become increasingly\ncommon across a wide range of industries. This type of retrospective visual\nanalysis, however, is prone to a variety of selection bias effects, especially\nfor high-dimensional data where only a subset of dimensions is visualized at\nany given time. The risk of selection bias is even higher when analysts\ndynamically apply filters or perform grouping operations during ad hoc\nanalyses. These bias effects threatens the validity and generalizability of\ninsights discovered during visual analysis as the basis for decision making.\nPast work has focused on bias transparency, helping users understand when\nselection bias may have occurred. However, countering the effects of selection\nbias via bias mitigation is typically left for the user to accomplish as a\nseparate process. Dynamic reweighting (DR) is a novel computational approach to\nselection bias mitigation that helps users craft bias-corrected visualizations.\nThis paper describes the DR workflow, introduces key DR visualization designs,\nand presents statistical methods that support the DR process. Use cases from\nthe medical domain, as well as findings from domain expert user interviews, are\nalso reported.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:15:36 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 21:02:54 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Borland", "David", ""], ["Zhang", "Jonathan", ""], ["Kaul", "Smiti", ""], ["Gotz", "David", ""]]}, {"id": "2007.14987", "submitter": "Patrick Jenkins", "authors": "Patrick Jenkins, Rishabh Sachdeva, Gaoussou Youssouf Kebe, Padraig\n  Higgins, Kasra Darvish, Edward Raff, Don Engel, John Winder, Francis Ferraro,\n  Cynthia Matuszek", "title": "Presentation and Analysis of a Multimodal Dataset for Grounded Language\n  Learning", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grounded language acquisition -- learning how language-based interactions\nrefer to the world around them -- is amajor area of research in robotics, NLP,\nand HCI. In practice the data used for learning consists almost entirely of\ntextual descriptions, which tend to be cleaner, clearer, and more grammatical\nthan actual human interactions. In this work, we present the Grounded Language\nDataset (GoLD), a multimodal dataset of common household objects described by\npeople using either spoken or written language. We analyze the differences and\npresent an experiment showing how the different modalities affect language\nlearning from human in-put. This will enable researchers studying the\nintersection of robotics, NLP, and HCI to better investigate how the multiple\nmodalities of image, text, and speech interact, as well as show differences in\nthe vernacular of these modalities impact results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 17:58:04 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 15:37:58 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 15:25:34 GMT"}, {"version": "v4", "created": "Mon, 28 Sep 2020 16:47:50 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Jenkins", "Patrick", ""], ["Sachdeva", "Rishabh", ""], ["Kebe", "Gaoussou Youssouf", ""], ["Higgins", "Padraig", ""], ["Darvish", "Kasra", ""], ["Raff", "Edward", ""], ["Engel", "Don", ""], ["Winder", "John", ""], ["Ferraro", "Francis", ""], ["Matuszek", "Cynthia", ""]]}, {"id": "2007.15032", "submitter": "Ulysse C\\^ot\\'e-Allard", "authors": "Cheikh Latyr Fall, Ulysse C\\^ot\\'e-Allard, Quentin Mascret, Alexandre\n  Campeau-Lecours, Mounir Boukadoum, Cl\\'ement Gosselin, Benoit Gosselin", "title": "A Flexible and Modular Body-Machine Interface for Individuals Living\n  with Severe Disabilities", "comments": "10 pages, submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a control interface to translate the residual body\nmotions of individuals living with severe disabilities, into control commands\nfor body-machine interaction. A custom, wireless, wearable multi-sensor network\nis used to collect motion data from multiple points on the body in real-time.\nThe solution proposed successfully leverage electromyography gesture\nrecognition techniques for the recognition of inertial measurement units-based\ncommands (IMU), without the need for cumbersome and noisy surface electrodes.\nMotion pattern recognition is performed using a computationally inexpensive\nclassifier (Linear Discriminant Analysis) so that the solution can be deployed\nonto lightweight embedded platforms. Five participants (three able-bodied and\ntwo living with upper-body disabilities) presenting different motion\nlimitations (e.g. spasms, reduced motion range) were recruited. They were asked\nto perform up to 9 different motion classes, including head, shoulder, finger,\nand foot motions, with respect to their residual functional capacities. The\nmeasured prediction performances show an average accuracy of 99.96% for\nable-bodied individuals and 91.66% for participants with upper-body\ndisabilities. The recorded dataset has also been made available online to the\nresearch community. Proof of concept for the real-time use of the system is\ngiven through an assembly task replicating activities of daily living using the\nJACO arm from Kinova Robotics.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:05:46 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Fall", "Cheikh Latyr", ""], ["C\u00f4t\u00e9-Allard", "Ulysse", ""], ["Mascret", "Quentin", ""], ["Campeau-Lecours", "Alexandre", ""], ["Boukadoum", "Mounir", ""], ["Gosselin", "Cl\u00e9ment", ""], ["Gosselin", "Benoit", ""]]}, {"id": "2007.15048", "submitter": "Krzysztof Kutt", "authors": "Krzysztof Kutt (1), Dominika Dr\\k{a}\\.zyk (1), Maciej Szel\\k{a}\\.zek\n  (2), Szymon Bobek (1), Grzegorz J. Nalepa (1) ((1) Jagiellonian University,\n  Poland, (2) AGH University of Science and Technology, Poland)", "title": "The BIRAFFE2 Experiment. Study in Bio-Reactions and Faces for\n  Emotion-based Personalization for AI Systems", "comments": "Presented during the Human-AI Interaction Workshop at ECAI 2020;\n  Acknowledgements added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes BIRAFFE2 data set, which is a result of an affective\ncomputing experiment conducted between 2019 and 2020, that aimed to develop\ncomputer models for classification and recognition of emotion. Such work is\nimportant to develop new methods of natural Human-AI interaction. As we believe\nthat models of emotion should be personalized by design, we present an unified\nparadigm allowing to capture emotional responses of different persons, taking\nindividual personality differences into account. We combine classical\npsychological paradigms of emotional response collection with the newer\napproach, based on the observation of the computer game player. By capturing\nones psycho-physiological reactions (ECG, EDA signal recording), mimic\nexpressions (facial emotion recognition), subjective valence-arousal balance\nratings (widget ratings) and gameplay progression (accelerometer and screencast\nrecording), we provide a framework that can be easily used and developed for\nthe purpose of the machine learning methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 18:35:34 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 20:11:03 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Kutt", "Krzysztof", ""], ["Dr\u0105\u017cyk", "Dominika", ""], ["Szel\u0105\u017cek", "Maciej", ""], ["Bobek", "Szymon", ""], ["Nalepa", "Grzegorz J.", ""]]}, {"id": "2007.15065", "submitter": "Haolin Liu", "authors": "Humphrey Yang, Kuanren Qian, Haolin Liu, Yuxuan Yu, Jianzhe Gu,\n  Matthew McGehee, Yongjie Jessica Zhang, Lining Yao", "title": "SimuLearn: Fast and Accurate Simulator to Support Morphing Materials\n  Design and Workflows", "comments": "Conditionally accepted to UIST 2020. 14 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Morphing materials allow us to create new modalities of interaction and\nfabrication by leveraging dynamic behaviors of materials. Yet, despite the\nongoing rapid growth of computational tools within this realm, current\ndevelopments are bottlenecked by the lack of an effective simulation method. As\na result, existing design tools must trade-off between speed and accuracy to\nsupport a real-time interactive design scenario. In response, we introduce\nSimuLearn, a data-driven method that combines finite element analysis and\nmachine learning to create real-time (0.61 seconds) and truthful (97% accuracy)\nmorphing material simulators. We use mesh-like 4D printed structures to\ncontextualize this method and prototype design tools to exemplify the design\nworkflows and spaces enabled by a fast and accurate simulation method.\nSituating this work among existing literature, we believe SimuLearn is a timely\naddition to the HCI CAD toolbox that can enable the proliferation of morphing\nmaterials.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 19:19:27 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Yang", "Humphrey", ""], ["Qian", "Kuanren", ""], ["Liu", "Haolin", ""], ["Yu", "Yuxuan", ""], ["Gu", "Jianzhe", ""], ["McGehee", "Matthew", ""], ["Zhang", "Yongjie Jessica", ""], ["Yao", "Lining", ""]]}, {"id": "2007.15182", "submitter": "Qianwen Wang", "authors": "Qianwen Wang, Zhenhua Xu, Zhutian Chen, Yong Wang, Shixia Liu, and\n  Huamin Qu", "title": "Visual Analysis of Discrimination in Machine Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3030471", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing use of automated decision-making in critical applications, such\nas crime prediction and college admission, has raised questions about fairness\nin machine learning. How can we decide whether different treatments are\nreasonable or discriminatory? In this paper, we investigate discrimination in\nmachine learning from a visual analytics perspective and propose an interactive\nvisualization tool, DiscriLens, to support a more comprehensive analysis. To\nreveal detailed information on algorithmic discrimination, DiscriLens\nidentifies a collection of potentially discriminatory itemsets based on causal\nmodeling and classification rules mining. By combining an extended Euler\ndiagram with a matrix-based visualization, we develop a novel set visualization\nto facilitate the exploration and interpretation of discriminatory itemsets. A\nuser study shows that users can interpret the visually encoded information in\nDiscriLens quickly and accurately. Use cases demonstrate that DiscriLens\nprovides informative guidance in understanding and reducing algorithmic\ndiscrimination.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 02:07:03 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Wang", "Qianwen", ""], ["Xu", "Zhenhua", ""], ["Chen", "Zhutian", ""], ["Wang", "Yong", ""], ["Liu", "Shixia", ""], ["Qu", "Huamin", ""]]}, {"id": "2007.15211", "submitter": "Victor Dibia", "authors": "Victor Dibia", "title": "NeuralQA: A Usable Library for Question Answering (Contextual Query\n  Expansion + BERT) on Large Datasets", "comments": "Published at Proceedings of the 2020 Conference on Empirical Methods\n  in Natural Language Processing: System Demonstrations. (EMNLP 2020), Demo\n  track. 8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing tools for Question Answering (QA) have challenges that limit their\nuse in practice. They can be complex to set up or integrate with existing\ninfrastructure, do not offer configurable interactive interfaces, and do not\ncover the full set of subtasks that frequently comprise the QA pipeline (query\nexpansion, retrieval, reading, and explanation/sensemaking). To help address\nthese issues, we introduce NeuralQA - a usable library for QA on large\ndatasets. NeuralQA integrates well with existing infrastructure (e.g.,\nElasticSearch instances and reader models trained with the HuggingFace\nTransformers API) and offers helpful defaults for QA subtasks. It introduces\nand implements contextual query expansion (CQE) using a masked language model\n(MLM) as well as relevant snippets (RelSnip) - a method for condensing large\ndocuments into smaller passages that can be speedily processed by a document\nreader model. Finally, it offers a flexible user interface to support workflows\nfor research explorations (e.g., visualization of gradient-based explanations\nto support qualitative inspection of model behaviour) and large scale search\ndeployment. Code and documentation for NeuralQA is available as open source on\nGithub (https://github.com/victordibia/neuralqa}{Github).\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 03:38:30 GMT"}, {"version": "v2", "created": "Sat, 28 Nov 2020 03:06:09 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Dibia", "Victor", ""]]}, {"id": "2007.15227", "submitter": "Yating Wei", "authors": "Wei Chen, Yating Wei, Zhiyong Wang, Shuyue Zhou, Bingru Lin, Zhiguang\n  Zhou", "title": "Federated Visualization: A Privacy-preserving Strategy for Decentralized\n  Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel privacy preservation strategy for decentralized\nvisualization. The key idea is to imitate the flowchart of the federated\nlearning framework, and reformulate the visualization process within a\nfederated infrastructure. The federation of visualization is fulfilled by\nleveraging a shared global module that composes the encrypted externalizations\nof transformed visual features of data pieces in local modules. We design two\nimplementations of federated visualization: a prediction-based scheme, and a\nquery-based scheme. We demonstrate the effectiveness of our approach with a set\nof visual forms, and verify its robustness with evaluations. We report the\nvalue of federated visualization in real scenarios with an expert review.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 04:57:26 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Chen", "Wei", ""], ["Wei", "Yating", ""], ["Wang", "Zhiyong", ""], ["Zhou", "Shuyue", ""], ["Lin", "Bingru", ""], ["Zhou", "Zhiguang", ""]]}, {"id": "2007.15272", "submitter": "Xumeng Wang", "authors": "Xumeng Wang, Wei Chen, Jiazhi Xia, Zexian Chen, Dongshi Xu, Xiangyang\n  Wu, Mingliang Xu and Tobias Schreck", "title": "ConceptExplorer: Visual Analysis of Concept Driftsin Multi-source\n  Time-series Data", "comments": "12 pages, 14 figures. Accepted by the IEEE VAST 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-series data is widely studied in various scenarios, like weather\nforecast, stock market, customer behavior analysis. To comprehensively learn\nabout the dynamic environments, it is necessary to comprehend features from\nmultiple data sources. This paper proposes a novel visual analysis approach for\ndetecting and analyzing concept drifts from multi-sourced time-series. We\npropose a visual detection scheme for discovering concept drifts from multiple\nsourced time-series based on prediction models. We design a drift level index\nto depict the dynamics, and a consistency judgment model to justify whether the\nconcept drifts from various sources are consistent. Our integrated visual\ninterface, ConceptExplorer, facilitates visual exploration, extraction,\nunderstanding, and comparison of concepts and concept drifts from multi-source\ntime-series data. We conduct three case studies and expert interviews to verify\nthe effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 07:21:52 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 08:48:25 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Wang", "Xumeng", ""], ["Chen", "Wei", ""], ["Xia", "Jiazhi", ""], ["Chen", "Zexian", ""], ["Xu", "Dongshi", ""], ["Wu", "Xiangyang", ""], ["Xu", "Mingliang", ""], ["Schreck", "Tobias", ""]]}, {"id": "2007.15407", "submitter": "Wei Zeng", "authors": "Xi Chen, Wei Zeng, Yanna Lin, Hayder Mahdi Al-maneea, Jonathan\n  Roberts, Remco Chang", "title": "Composition and Configuration Patterns in Multiple-View Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-view visualization (MV) is a layout design technique often employed\nto help users see a large number of data attributes and values in a single\ncohesive representation. Because of its generalizability, the MV design has\nbeen widely adopted by the visualization community to help users examine and\ninteract with large, complex, and high-dimensional data. However, although\nubiquitous, there has been little work to categorize and analyze MVs in order\nto better understand its design space. As a result, there has been little to no\nguideline in how to use the MV design effectively. In this paper, we present an\nin-depth study of how MVs are designed in practice. We focus on two fundamental\nmeasures of multiple-view patterns: composition, which quantifies what view\ntypes and how many are there; and configuration, which characterizes spatial\narrangement of view layouts in the display space. We build a new dataset\ncontaining 360 images of MVs collected from IEEE VIS, EuroVis, and PacificVis\npublications 2011 to 2019, and make fine-grained annotations of view types and\nlayouts for these visualization images. From this data we conduct composition\nand configuration analyses using quantitative metrics of term frequency and\nlayout topology. We identify common practices around MVs, including\nrelationship of view types, popular view layouts, and correlation between view\ntypes and layouts. We combine the findings into a MV recommendation system,\nproviding interactive tools to explore the design space, and support\nexample-based design.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 12:01:46 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 09:36:56 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Chen", "Xi", ""], ["Zeng", "Wei", ""], ["Lin", "Yanna", ""], ["Al-maneea", "Hayder Mahdi", ""], ["Roberts", "Jonathan", ""], ["Chang", "Remco", ""]]}, {"id": "2007.15446", "submitter": "Alexander Kumpf", "authors": "Alexander Kumpf, Josef Stumpfegger, Patrick Fabian H\\\"artl, R\\\"udiger\n  Westermann", "title": "Visual Analysis of Multi-Parameter Distributions across Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an ensemble of data points in a multi-parameter space, we present a\nvisual analytics technique to select a representative distribution of parameter\nvalues, and analyse how representative this distribution is in all ensemble\nmembers. A multi-parameter cluster in a representative ensemble member is\nvisualized via a parallel coordinates plot, to provide initial distributions\nand let domain experts interactively select relevant parameters and value\nranges. Since unions of value ranges select hyper-cubes in parameter space,\ndata points in these unions are not necessarily contained in the cluster. By\nusing a multi-parameter kD-tree to further refine the selected parameter\nranges, in combination with a covariance analysis of refined sets of data\npoints, a tight partition in multi-parameter space with reduced number of\nfalsely selected points is obtained. To assess the representativeness of the\nselected multi-parameter distribution across the ensemble, a linked\nside-by-side view of per-member violin plots is provided. We propose\nmodifications of violin plots to show multi-parameter distributions\nsimultaneously, and investigate the visual design that effectively conveys\n(dis-)similarities in multi-parameter distributions. In a linked spatial view,\nusers can analyse and compare the spatial distribution of selected points in\ndifferent ensemble members via interval-based isosurface raycasting. In two\nreal-world application cases we show how our approach is used to analyse the\nmulti-parameter distributions across an ensemble of 3D fields.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 13:21:33 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Kumpf", "Alexander", ""], ["Stumpfegger", "Josef", ""], ["H\u00e4rtl", "Patrick Fabian", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "2007.15486", "submitter": "Wei Zeng", "authors": "Wei Zeng, Chengqiao Lin, Juncong Lin, Jincheng Jiang, Jiazhi Xia,\n  Cagatay Turkay, Wei Chen", "title": "Revisiting the Modifiable Areal Unit Problem in Deep Traffic Prediction\n  with Visual Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods are being increasingly used for urban traffic\nprediction where spatiotemporal traffic data is aggregated into sequentially\norganized matrices that are then fed into convolution-based residual neural\nnetworks. However, the widely known modifiable areal unit problem within such\naggregation processes can lead to perturbations in the network inputs. This\nissue can significantly destabilize the feature embeddings and the predictions,\nrendering deep networks much less useful for the experts. This paper approaches\nthis challenge by leveraging unit visualization techniques that enable the\ninvestigation of many-to-many relationships between dynamically varied\nmulti-scalar aggregations of urban traffic data and neural network predictions.\nThrough regular exchanges with a domain expert, we design and develop a visual\nanalytics solution that integrates 1) a Bivariate Map equipped with an advanced\nbivariate colormap to simultaneously depict input traffic and prediction errors\nacross space, 2) a Morans I Scatterplot that provides local indicators of\nspatial association analysis, and 3) a Multi-scale Attribution View that\narranges non-linear dot plots in a tree layout to promote model analysis and\ncomparison across scales. We evaluate our approach through a series of case\nstudies involving a real-world dataset of Shenzhen taxi trips, and through\ninterviews with domain experts. We observe that geographical scale variations\nhave important impact on prediction performances, and interactive visual\nexploration of dynamically varying inputs and outputs benefit experts in the\ndevelopment of deep traffic prediction models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 14:32:17 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 08:18:04 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 14:20:33 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Zeng", "Wei", ""], ["Lin", "Chengqiao", ""], ["Lin", "Juncong", ""], ["Jiang", "Jincheng", ""], ["Xia", "Jiazhi", ""], ["Turkay", "Cagatay", ""], ["Chen", "Wei", ""]]}, {"id": "2007.15538", "submitter": "Filippo Gabriele Prattic\\`o", "authors": "F. Gabriele Prattic\\`o, Fabrizio Lamberti (Politecnico di Torino)", "title": "Mixed-Reality Robotic Games: Design Guidelines for Effective\n  Entertainment with Consumer Robots", "comments": "This paper is accepted for inclusion in future issue of IEEE Consumer\n  Electronic Magazine. Copyright IEEE 2020", "journal-ref": "IEEE Consumer Electronics Magazine, vol. 10, no. 1, pp. 6-16, Jan.\n  2021", "doi": "10.1109/MCE.2020.2988578", "report-no": null, "categories": "cs.HC cs.GR cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increasing interest in the use of robotic\ntechnology at home. A number of service robots appeared on the market,\nsupporting customers in the execution of everyday tasks. Roughly at the same\ntime, consumer level robots started to be used also as toys or gaming\ncompanions. However, gaming possibilities provided by current off-the-shelf\nrobotic products are generally quite limited, and this fact makes them quickly\nloose their attractiveness. A way that has been proven capable to boost robotic\ngaming and related devices consists in creating playful experiences in which\nphysical and digital elements are combined together using Mixed Reality\ntechnologies. However, these games differ significantly from digital- or\nphysical only experiences, and new design principles are required to support\ndevelopers in their creative work. This papers addresses such need, by drafting\na set of guidelines which summarize developments carried out by the research\ncommunity and their findings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 15:47:17 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Prattic\u00f2", "F. Gabriele", "", "Politecnico di Torino"], ["Lamberti", "Fabrizio", "", "Politecnico di Torino"]]}, {"id": "2007.15584", "submitter": "Longqi Yang", "authors": "Longqi Yang, Sonia Jaffe, David Holtz, Siddharth Suri, Shilpi Sinha,\n  Jeffrey Weston, Connor Joyce, Neha Shah, Kevin Sherman, CJ Lee, Brent Hecht,\n  Jaime Teevan", "title": "How Work From Home Affects Collaboration: A Large-Scale Study of\n  Information Workers in a Natural Experiment During COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has had a wide-ranging impact on information workers\nsuch as higher stress levels, increased workloads, new workstreams, and more\ncaregiving responsibilities during lockdown. COVID-19 also caused the\noverwhelming majority of information workers to rapidly shift to working from\nhome (WFH). The central question this work addresses is: can we isolate the\neffects of WFH on information workers' collaboration activities from all other\nfactors, especially the other effects of COVID-19? This is important because in\nthe future, WFH will likely to be more common than it was prior to the\npandemic.\n  We use difference-in-differences (DiD), a causal identification strategy\ncommonly used in the social sciences, to control for unobserved confounding\nfactors and estimate the causal effect of WFH. Our analysis relies on measuring\nthe difference in changes between those who WFH prior to COVID-19 and those who\ndid not. Our preliminary results suggest that on average, people spent more\ntime on collaboration in April (Post WFH mandate) than in February (Pre WFH\nmandate), but this is primarily due to factors other than WFH, such as\nlockdowns during the pandemic. The change attributable to WFH specifically is\nin the opposite direction: less time on collaboration and more focus time. This\nreversal shows the importance of using causal inference: a simple analysis\nwould have resulted in the wrong conclusion. We further find that the effect of\nWFH is moderated by individual remote collaboration experience prior to WFH.\nMeanwhile, the medium for collaboration has also shifted due to WFH: instant\nmessages were used more, whereas scheduled meetings were used less. We discuss\ndesign implications -- how future WFH may affect focused work, collaborative\nwork, and creative work.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:43:26 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Yang", "Longqi", ""], ["Jaffe", "Sonia", ""], ["Holtz", "David", ""], ["Suri", "Siddharth", ""], ["Sinha", "Shilpi", ""], ["Weston", "Jeffrey", ""], ["Joyce", "Connor", ""], ["Shah", "Neha", ""], ["Sherman", "Kevin", ""], ["Lee", "CJ", ""], ["Hecht", "Brent", ""], ["Teevan", "Jaime", ""]]}, {"id": "2007.15591", "submitter": "Jiazhi Xia", "authors": "Jiazhi Xia, Tianxiang Chen, Lei Zhang, Wei Chen, Yang Chen, Xiaolong\n  Zhang, Cong Xie, Tobias Schreck", "title": "SMAP: A Joint Dimensionality Reduction Scheme for Secure Multi-Party\n  Visualization", "comments": "12 pages, 10 figures. Conditionally accepted by VAST 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, as data becomes increasingly complex and distributed, data analyses\noften involve several related datasets that are stored on different servers and\nprobably owned by different stakeholders. While there is an emerging need to\nprovide these stakeholders with a full picture of their data under a global\ncontext, conventional visual analytical methods, such as dimensionality\nreduction, could expose data privacy when multi-party datasets are fused into a\nsingle site to build point-level relationships. In this paper, we reformulate\nthe conventional t-SNE method from the single-site mode into a secure\ndistributed infrastructure. We present a secure multi-party scheme for joint\nt-SNE computation, which can minimize the risk of data leakage. Aggregated\nvisualization can be optionally employed to hide disclosure of point-level\nrelationships. We build a prototype system based on our method, SMAP, to\nsupport the organization, computation, and exploration of secure joint\nembedding. We demonstrate the effectiveness of our approach with three case\nstudies, one of which is based on the deployment of our system in real-world\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 16:54:57 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Xia", "Jiazhi", ""], ["Chen", "Tianxiang", ""], ["Zhang", "Lei", ""], ["Chen", "Wei", ""], ["Chen", "Yang", ""], ["Zhang", "Xiaolong", ""], ["Xie", "Cong", ""], ["Schreck", "Tobias", ""]]}, {"id": "2007.15741", "submitter": "Ebenezer Narh Odonkor", "authors": "Ebenezer Narh Odonkor, Willie K. Ofosu, and Kingsley Nunoo", "title": "Using GSM SMS controller alarm Configurator to develop cost effective,\n  intelligent fire safety system in a developing country", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Electricity supply to facilities is essential, but can cause fires when care\nis not taken, and can destroy the properties within some few minutes. The need\nfor fire protection is therefore essential. This paper address-es this\npertinent issue facing Ghana as a developing country. At the fire outbreak, the\ndesigned system re-sponds to the smoke and cuts off the electricity supply.\nWhen a fire is detected, an alarm is turned on and Short Messaging Service\n(SMS) alert is sent to the owner. After ten to fifteen seconds, the system,\nresends SMS to the owner and the National Fire Service (NFS) safety officer\ngiving the exact location. The call alert is sent to the owner of a facility\nand fire safety personnel respectively, when the system is not reset. A\nmicrocontroller serves as the command center of the cost effective, intelligent\nfire safety system. The circuit is designed and simulated using Proteus\nsoftware, and programing was done using Global System for Mo-bile\nCommunications (GSM) SMS Controller Alarm Configurator software. The prototype\nwas constructed and tested in real-time. The proposed system is cost effective\nand will help policy makers, and save lives and property when implemented.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 13:00:04 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Odonkor", "Ebenezer Narh", ""], ["Ofosu", "Willie K.", ""], ["Nunoo", "Kingsley", ""]]}, {"id": "2007.15800", "submitter": "Yali Bian", "authors": "Yali Bian, John Wenskovitch, Chris North", "title": "DeepVA: Bridging Cognition and Computation through Semantic Interaction\n  and Deep Learning", "comments": null, "journal-ref": "Proceedings of the IEEE VIS Workshop MLUI 2019: Machine Learning\n  from User Interactions for Visualization and Analytics", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines how deep learning (DL) representations, in contrast to\ntraditional engineered features, can support semantic interaction (SI) in\nvisual analytics. SI attempts to model user's cognitive reasoning via their\ninteraction with data items, based on the data features. We hypothesize that DL\nrepresentations contain meaningful high-level abstractions that can better\ncapture users' high-level cognitive intent. To bridge the gap between cognition\nand computation in visual analytics, we propose DeepVA (Deep Visual Analytics),\nwhich uses high-level deep learning representations for semantic interaction\ninstead of low-level hand-crafted data features. To evaluate DeepVA and compare\nto SI models with lower-level features, we design and implement a system that\nextends a traditional SI pipeline with features at three different levels of\nabstraction. To test the relationship between task abstraction and feature\nabstraction in SI, we perform visual concept learning tasks at three different\ntask abstraction levels, using semantic interaction with three different\nfeature abstraction levels. DeepVA effectively hastened interactive convergence\nbetween cognitive understanding and computational modeling of the data,\nespecially in high abstraction tasks.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 01:53:45 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Bian", "Yali", ""], ["Wenskovitch", "John", ""], ["North", "Chris", ""]]}, {"id": "2007.15815", "submitter": "Weizhe Lin", "authors": "Weizhe Lin, Indigo Orton, Qingbiao Li, Gabriela Pavarini, Marwa\n  Mahmoud", "title": "Looking At The Body: Automatic Analysis of Body Gestures and\n  Self-Adaptors in Psychological Distress", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychological distress is a significant and growing issue in society.\nAutomatic detection, assessment, and analysis of such distress is an active\narea of research. Compared to modalities such as face, head, and vocal,\nresearch investigating the use of the body modality for these tasks is\nrelatively sparse. This is, in part, due to the limited available datasets and\ndifficulty in automatically extracting useful body features. Recent advances in\npose estimation and deep learning have enabled new approaches to this modality\nand domain. To enable this research, we have collected and analyzed a new\ndataset containing full body videos for short interviews and self-reported\ndistress labels. We propose a novel method to automatically detect\nself-adaptors and fidgeting, a subset of self-adaptors that has been shown to\nbe correlated with psychological distress. We perform analysis on statistical\nbody gestures and fidgeting features to explore how distress levels affect\nparticipants' behaviors. We then propose a multi-modal approach that combines\ndifferent feature representations using Multi-modal Deep Denoising\nAuto-Encoders and Improved Fisher Vector Encoding. We demonstrate that our\nproposed model, combining audio-visual features with automatically detected\nfidgeting behavioral cues, can successfully predict distress levels in a\ndataset labeled with self-reported anxiety and depression levels.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 02:45:00 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Lin", "Weizhe", ""], ["Orton", "Indigo", ""], ["Li", "Qingbiao", ""], ["Pavarini", "Gabriela", ""], ["Mahmoud", "Marwa", ""]]}, {"id": "2007.15824", "submitter": "Yali Bian", "authors": "Yali Bian, Michelle Dowling, Chris North", "title": "Evaluating Semantic Interaction on Word Embeddings via Simulation", "comments": null, "journal-ref": "EValuation of Interactive VisuAl Machine Learning systems, an IEEE\n  VIS 2019 Workshop", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic interaction (SI) attempts to learn the user's cognitive intents as\nthey directly manipulate data projections during sensemaking activity. For text\nanalysis, prior implementations of SI have used common data features, such as\nbag-of-words representations, for machine learning from user interactions.\nInstead, we hypothesize that features derived from deep learning word\nembeddings will enable SI to better capture the user's subtle intents. However,\nevaluating these effects is difficult. SI systems are usually evaluated by a\nhuman-centred qualitative approach, by observing the utility and effectiveness\nof the application for end-users. This approach has drawbacks in terms of\nreplicability, scalability, and objectiveness, which makes it hard to perform\nconvincing contrast experiments between different SI models. To tackle this\nproblem, we explore a quantitative algorithm-centered analysis as a\ncomplementary evaluation approach, by simulating users' interactions and\ncalculating the accuracy of the learned model. We use these methods to compare\nword-embeddings to bag-of-words features for SI.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:34:05 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Bian", "Yali", ""], ["Dowling", "Michelle", ""], ["North", "Chris", ""]]}, {"id": "2007.15828", "submitter": "Zezheng Feng", "authors": "Zezheng Feng, Haotian Li, Wei Zeng, Shuang-Hua Yang, and Huamin Qu", "title": "Topology Density Map for Urban Data Visualization and Analysis", "comments": "11 pages, 10 figures", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics ( Volume:\n  27, Issue: 2, Feb. 2021)", "doi": "10.1109/TVCG.2020.3030469", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density map is an effective visualization technique for depicting the scalar\nfield distribution in 2D space. Conventional methods for constructing density\nmaps are mainly based on Euclidean distance, limiting their applicability in\nurban analysis that shall consider road network and urban traffic. In this\nwork, we propose a new method named Topology Density Map, targeting for\naccurate and intuitive density maps in the context of urban environment. Based\non the various constraints of road connections and traffic conditions, the\nmethod first constructs a directed acyclic graph (DAG) that propagates\nnonlinear scalar fields along 1D road networks. Next, the method extends the\nscalar fields to a 2D space by identifying key intersecting points in the DAG,\ndividing the underlying territory into planar regions using a weighted Voronoi\ndiagram, and calculating the scalar fields for every point. Two case studies\ndemonstrate that the Topology Density Map supplies accurate information to\nusers and provides an intuitive visualization for decision making. An interview\nwith domain experts demonstrates the feasibility, usability, and effectiveness\nof our method.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:48:28 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 09:24:17 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 05:52:58 GMT"}, {"version": "v4", "created": "Fri, 5 Mar 2021 13:38:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Feng", "Zezheng", ""], ["Li", "Haotian", ""], ["Zeng", "Wei", ""], ["Yang", "Shuang-Hua", ""], ["Qu", "Huamin", ""]]}, {"id": "2007.15831", "submitter": "Philipp V. Rouast", "authors": "Philipp V. Rouast and Hamid Heydarian and Marc T. P. Adam and Megan E.\n  Rollo", "title": "OREBA: A Dataset for Objectively Recognizing Eating Behaviour and\n  Associated Intake", "comments": "To be published in IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3026965", "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of intake gestures is a key element of automatic dietary\nmonitoring. Several types of sensors, including inertial measurement units\n(IMU) and video cameras, have been used for this purpose. The common machine\nlearning approaches make use of the labeled sensor data to automatically learn\nhow to make detections. One characteristic, especially for deep learning\nmodels, is the need for large datasets. To meet this need, we collected the\nObjectively Recognizing Eating Behavior and Associated Intake (OREBA) dataset.\nThe OREBA dataset aims to provide comprehensive multi-sensor data recorded\nduring the course of communal meals for researchers interested in intake\ngesture detection. Two scenarios are included, with 100 participants for a\ndiscrete dish and 102 participants for a shared dish, totalling 9069 intake\ngestures. Available sensor data consists of synchronized frontal video and IMU\nwith accelerometer and gyroscope for both hands. We report the details of data\ncollection and annotation, as well as details of sensor processing. The results\nof studies on IMU and video data involving deep learning models are reported to\nprovide a baseline for future research. Specifically, the best baseline models\nachieve performances of $F_1$ = 0.853 for the discrete dish using video and\n$F_1$ = 0.852 for the shared dish using inertial data.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:54:05 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 06:06:24 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 23:55:23 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Rouast", "Philipp V.", ""], ["Heydarian", "Hamid", ""], ["Adam", "Marc T. P.", ""], ["Rollo", "Megan E.", ""]]}, {"id": "2007.15832", "submitter": "Arpit Narechania", "authors": "Arpit Narechania, Ahsan Qamar, Alex Endert", "title": "SafetyLens: Visual Data Analysis of Functional Safety of Vehicles", "comments": "11 pages, 11 figures. Proceedings of IEEE VIS'2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030382", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern automobiles have evolved from just being mechanical machines to having\nfull-fledged electronics systems that enhance vehicle dynamics and driver\nexperience. However, these complex hardware and software systems, if not\nproperly designed, can experience failures that can compromise the safety of\nthe vehicle, its occupants, and the surrounding environment. For example, a\nsystem to activate the brakes to avoid a collision saves lives when it\nfunctions properly, but could lead to tragic outcomes if the brakes were\napplied in a way that's inconsistent with the design. Broadly speaking, the\nanalysis performed to minimize such risks falls into a systems engineering\ndomain called Functional Safety. In this paper, we present SafetyLens, a visual\ndata analysis tool to assist engineers and analysts in analyzing automotive\nFunctional Safety datasets. SafetyLens combines techniques including network\nexploration and visual comparison to help analysts perform domain-specific\ntasks. This paper presents the design study with domain experts that resulted\nin the design guidelines, the tool, and user feedback.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 03:56:34 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 14:26:58 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 20:55:07 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Narechania", "Arpit", ""], ["Qamar", "Ahsan", ""], ["Endert", "Alex", ""]]}, {"id": "2007.15843", "submitter": "Baptiste Caramiaux", "authors": "Baptiste Caramiaux and Marco Donnarumma", "title": "Artificial Intelligence in Music and Performance: A Subjective\n  Art-Research Inquiry", "comments": "Pre-print version of a chapter for Handbook of Artificial\n  Intelligence for Music: Foundations, Advanced Approaches, and Developments\n  for Creativity, edited by Eduardo R. Miranda, for Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article presents a five-year collaboration situated at the intersection\nof Art practice and Scientific research in Human-Computer Interaction (HCI). At\nthe core of our collaborative work is a hybrid, Art and Science methodology\nthat combines computational learning technology -- Machine Learning (ML) and\nArtificial Intelligence (AI) -- with interactive music performance and\nchoreography. This article first exposes our thoughts on combining art,\nscience, movement and sound research. We then describe two of our artistic\nworks \\textit{Corpus Nil} and \\textit{Humane Methods} -- created five years\napart from each other -- that crystallize our collaborative research process.\nWe present the scientific and artistic motivations, framed through our research\ninterests and cultural environment of the time. We conclude by reflecting on\nthe methodology we developed during the collaboration and on the conceptual\nshift of computational learning technologies, from ML to AI, and its impact on\nMusic performance.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 04:35:51 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Caramiaux", "Baptiste", ""], ["Donnarumma", "Marco", ""]]}, {"id": "2007.16172", "submitter": "Zachary Levonian", "authors": "Zachary Levonian, Marco Dow, Drew Erikson, Sourojit Ghosh, Hannah\n  Miller Hillberg, Saumik Narayanan, Loren Terveen, Svetlana Yarosh", "title": "Patterns of Patient and Caregiver Mutual Support Connections in an\n  Online Health Community", "comments": "Pre-print of paper conditionally accepted to CSCW 2020. 31\n  main-article pages; 46 pages with appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online health communities offer the promise of support benefits to users, in\nparticular because these communities enable users to find peers with similar\nexperiences. Building mutually supportive connections between peers is a key\nmotivation for using online health communities. However, a user's role in a\ncommunity may influence the formation of peer connections. In this work, we\nstudy patterns of peer connections between two structural health roles: patient\nand non-professional caregiver. We examine user behavior in an online health\ncommunity where finding peers is not explicitly supported. This context lets us\nuse social network analysis methods to explore the growth of such connections\nin the wild and identify users' peer communication preferences. We investigated\nhow connections between peers were initiated, finding that initiations are more\nlikely between two authors who have the same role and who are close within the\nbroader communication network. Relationships are also more likely to form and\nbe more interactive when authors have the same role. Our results have\nimplications for the design of systems supporting peer communication, e.g.\npeer-to-peer recommendation systems.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 16:51:41 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 15:31:57 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2020 19:24:03 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Levonian", "Zachary", ""], ["Dow", "Marco", ""], ["Erikson", "Drew", ""], ["Ghosh", "Sourojit", ""], ["Hillberg", "Hannah Miller", ""], ["Narayanan", "Saumik", ""], ["Terveen", "Loren", ""], ["Yarosh", "Svetlana", ""]]}]