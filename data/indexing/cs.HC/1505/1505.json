[{"id": "1505.00092", "submitter": "Elad Yom-Tov", "authors": "Dan Pelleg, Elad Yom-Tov and Evgeniy Gabrilovich", "title": "On the Effect of Human-Computer Interfaces on Language Expression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language expression is known to be dependent on attributes intrinsic to the\nauthor. To date, however, little attention has been devoted to the effect of\ninterfaces used to articulate language on its expression. Here we study a large\ncorpus of text written using different input devices and show that writers\nunconsciously prefer different letters depending on the interplay between their\nindividual traits (e.g., hand laterality and injuries) and the layout of\nkeyboards. Our results show, for the first time, how the interplay between\ntechnology and its users modifies language expression.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 05:18:57 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Pelleg", "Dan", ""], ["Yom-Tov", "Elad", ""], ["Gabrilovich", "Evgeniy", ""]]}, {"id": "1505.00111", "submitter": "Bin Guo", "authors": "Bin Guo, Chao Chen, Daqing Zhang, Zhiwen Yu, Alvin Chin", "title": "Mobile Crowd Sensing and Computing: When Participatory Sensing Meets\n  Participatory Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of mobile sensing and mobile social networking\ntechniques, Mobile Crowd Sensing and Computing (MCSC), which leverages\nheterogeneous crowdsourced data for large-scale sensing, has become a leading\nparadigm. Built on top of the participatory sensing vision, MCSC has two\ncharacterizing features: (1) it leverages heterogeneous crowdsourced data from\ntwo data sources: participatory sensing and participatory social media; and (2)\nit presents the fusion of human and machine intelligence (HMI) in both the\nsensing and computing process. This paper characterizes the unique features and\nchallenges of MCSC. We further present early efforts on MCSC to demonstrate the\nbenefits of aggregating heterogeneous crowdsourced data.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 07:46:53 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Guo", "Bin", ""], ["Chen", "Chao", ""], ["Zhang", "Daqing", ""], ["Yu", "Zhiwen", ""], ["Chin", "Alvin", ""]]}, {"id": "1505.00145", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Ferran Cabezas, Axel Carlier, Amaia Salvador, Xavier Gir\\'o-i-Nieto\n  and Vincent Charvillat", "title": "Quality Control in Crowdsourced Object Segmentation", "comments": "Paper accepted at the IEEE International Conference on Image\n  Processing (ICIP) 2015. Quebec City, 27-30 September 2015", "journal-ref": null, "doi": "10.1109/ICIP.2015.7351606", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores processing techniques to deal with noisy data in\ncrowdsourced object segmentation tasks. We use the data collected with\n\"Click'n'Cut\", an online interactive segmentation tool, and we perform several\nexperiments towards improving the segmentation results. First, we introduce\ndifferent superpixel-based techniques to filter users' traces, and assess their\nimpact on the segmentation result. Second, we present different criteria to\ndetect and discard the traces from potential bad users, resulting in a\nremarkable increase in performance. Finally, we show a novel superpixel-based\nsegmentation algorithm which does not require any prior filtering and is based\non weighting each user's contribution according to his/her level of expertise.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 10:33:49 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Cabezas", "Ferran", ""], ["Carlier", "Axel", ""], ["Salvador", "Amaia", ""], ["Gir\u00f3-i-Nieto", "Xavier", ""], ["Charvillat", "Vincent", ""]]}, {"id": "1505.00489", "submitter": "Yulia  Silina", "authors": "Yulia Silina, Hamed Haddadi", "title": "The Distant Heart: Mediating Long-Distance Relationships through\n  Connected Computational Jewelry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the world where increasingly mobility and long-distance relationships with\nfamily, friends and loved-ones became commonplace, there exists a gap in\nintimate interpersonal communication mediated by technology. Considering the\nadvances in the field of mediation of relationships through technology, as well\nas prevalence of use of jewelry as love-tokens for expressing a wish to be\nremembered and to evoke the presence of the loved-one, developments in the new\nfield of computational jewelry offer some truly exciting possibilities. In this\npaper we investigate the role that the jewelry-like form factor of prototypes\ncan play in the context of studying effects of computational jewelry in\nmediating long-distance relationships.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 22:45:19 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Silina", "Yulia", ""], ["Haddadi", "Hamed", ""]]}, {"id": "1505.00922", "submitter": "Daniel Graziotin", "authors": "Daniel Graziotin, Xiaofeng Wang, Pekka Abrahamsson (Free University of\n  Bozen-Bolzano)", "title": "Happy software developers solve problems better: psychological\n  measurements in empirical software engineering", "comments": "33 pages, 11 figures, published at PeerJ", "journal-ref": "PeerJ, vol. 2, pp. e289, 2014", "doi": "10.7717/peerj.289", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  For more than 30 years, it has been claimed that a way to improve software\ndevelopers' productivity and software quality is to focus on people and to\nprovide incentives to make developers satisfied and happy. This claim has\nrarely been verified in software engineering research, which faces an\nadditional challenge in comparison to more traditional engineering fields:\nsoftware development is an intellectual activity and is dominated by\noften-neglected human aspects. Among the skills required for software\ndevelopment, developers must possess high analytical problem-solving skills and\ncreativity for the software construction process. According to psychology\nresearch, affects-emotions and moods-deeply influence the cognitive processing\nabilities and performance of workers, including creativity and analytical\nproblem solving. Nonetheless, little research has investigated the correlation\nbetween the affective states, creativity, and analytical problem-solving\nperformance of programmers. This article echoes the call to employ\npsychological measurements in software engineering research. We report a study\nwith 42 participants to investigate the relationship between the affective\nstates, creativity, and analytical problem-solving skills of software\ndevelopers. The results offer support for the claim that happy developers are\nindeed better problem solvers in terms of their analytical abilities. The\nfollowing contributions are made by this study: (1) providing a better\nunderstanding of the impact of affective states on the creativity and\nanalytical problem-solving capacities of developers, (2) introducing and\nvalidating psychological measurements, theories, and concepts of affective\nstates, creativity, and analytical-problem-solving skills in empirical software\nengineering, and (3) raising the need for studying the human factors of\nsoftware engineering by employing a multidisciplinary viewpoint.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 08:54:44 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Graziotin", "Daniel", "", "Free University of\n  Bozen-Bolzano"], ["Wang", "Xiaofeng", "", "Free University of\n  Bozen-Bolzano"], ["Abrahamsson", "Pekka", "", "Free University of\n  Bozen-Bolzano"]]}, {"id": "1505.01071", "submitter": "Darine Ameyed", "authors": "Darine Ameyed, Moeiz Miraoui, Chakib Tadj", "title": "A Spatiotemporal Context Definition for Service Adaptation Prediction in\n  a Pervasive Computing Environment", "comments": "Context-aware; Pervasive Computing; Context Definition; 2015.\n  International Journal of Advanced Studies in Computer Science and Engineering\n  (IJASCSE) http://www.ijascse.org/publications ;2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.ET cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pervasive systems refers to context-aware systems that can sense their\ncontext, and adapt their behavior accordingly to provide adaptable services.\nProactive adaptation of such systems allows changing the service and the\ncontext based on prediction. However, the definition of the context is still\nvague and not suitable to prediction. In this paper we discuss and classify\nprevious definitions of context. Then, we present a new definition which allows\npervasive systems to understand and predict their contexts. We analyze the\nessential lines that fall within the context definition, and propose some\nscenarios to make it clear our approach.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 16:34:23 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Ameyed", "Darine", ""], ["Miraoui", "Moeiz", ""], ["Tadj", "Chakib", ""]]}, {"id": "1505.01214", "submitter": "Babak Saleh", "authors": "Babak Saleh and Mira Dontcheva and Aaron Hertzmann and Zhicheng Liu", "title": "Learning Style Similarity for Searching Infographics", "comments": "6 pages, to appear in the 41st annual conference on Graphics\n  Interface (GI) 2015,", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.CV cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infographics are complex graphic designs integrating text, images, charts and\nsketches. Despite the increasing popularity of infographics and the rapid\ngrowth of online design portfolios, little research investigates how we can\ntake advantage of these design resources. In this paper we present a method for\nmeasuring the style similarity between infographics. Based on human perception\ndata collected from crowdsourced experiments, we use computer vision and\nmachine learning algorithms to learn a style similarity metric for infographic\ndesigns. We evaluate different visual features and learning algorithms and find\nthat a combination of color histograms and Histograms-of-Gradients (HoG)\nfeatures is most effective in characterizing the style of infographics. We\ndemonstrate our similarity metric on a preliminary image retrieval test.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 22:59:32 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Saleh", "Babak", ""], ["Dontcheva", "Mira", ""], ["Hertzmann", "Aaron", ""], ["Liu", "Zhicheng", ""]]}, {"id": "1505.01311", "submitter": "Andrea Monacchi", "authors": "Andrea Monacchi, Fabio Versolatto, Manuel Herold, Dominik Egarter,\n  Andrea M. Tonello, Wilfried Elmenreich", "title": "An Open Solution to Provide Personalized Feedback for Building Energy\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of renewable energy sources increases the complexity in\nmantaining the power grid. In particular, the highly dynamic nature of\ngeneration and consumption demands for a better utilization of energy\nresources, which seen the cost of storage infrastructure, can only be achieved\nthrough demand-response. Accordingly, the availability of energy and potential\noverload situations can be reflected using a price signal. The effectiveness of\nthis mechanism arises from the flexibility of device operation, which is\nnevertheless heavily reliant on the exchange of information between the grid\nand its consumers. In this paper, we investigate the capability of an\ninteractive energy management system to timely inform users on energy usage, in\norder to promote an optimal use of local resources. In particular, we analyze\ndata being collected in several households in Italy and Austria to gain\ninsights into usage behavior and drive the design of more effective systems.\nThe outcome is the formulation of energy efficiency policies for residential\nbuildings, as well as the design of an energy management system, consisting of\nhardware measurement units and a management software. The Mj\\\"olnir framework,\nwhich we release for open use, provides a platform where various feedback\nconcepts can be implemented and assessed. This includes widgets displaying\ndisaggregated and aggregated consumption information, as well as daily\nproduction and tailored advices. The formulated policies were implemented as an\nadvisor widget able to autonomously analyze usage and provide tailored energy\nfeedback.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 10:35:55 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Monacchi", "Andrea", ""], ["Versolatto", "Fabio", ""], ["Herold", "Manuel", ""], ["Egarter", "Dominik", ""], ["Tonello", "Andrea M.", ""], ["Elmenreich", "Wilfried", ""]]}, {"id": "1505.01319", "submitter": "Clemens Arth", "authors": "Clemens Arth and Raphael Grasset and Lukas Gruber and Tobias Langlotz\n  and Alessandro Mulloni and Daniel Wagner", "title": "The History of Mobile Augmented Reality", "comments": "43 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document summarizes the major milestones in mobile Augmented Reality\nbetween 1968 and 2014. Major parts of the list were compiled by the member of\nthe Christian Doppler Laboratory for Handheld Augmented Reality in 2010 (author\nlist in alphabetical order) for the ISMAR society. Later in 2013 it was\nupdated, and more recent work was added during preparation of this report.\nPermission is granted to copy and modify.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 10:56:12 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2015 12:12:41 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2015 10:02:28 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Arth", "Clemens", ""], ["Grasset", "Raphael", ""], ["Gruber", "Lukas", ""], ["Langlotz", "Tobias", ""], ["Mulloni", "Alessandro", ""], ["Wagner", "Daniel", ""]]}, {"id": "1505.04563", "submitter": "Daniel Graziotin", "authors": "Daniel Graziotin, Xiaofeng Wang, Pekka Abrahamsson", "title": "The Affect of Software Developers: Common Misconceptions and\n  Measurements", "comments": "2 pages. Research note to be presented at the 2015 IEEE/ACM 8th\n  International Workshop on Cooperative and Human Aspects of Software\n  Engineering (CHASE 2015)", "journal-ref": "8th Intl. Workshop on Cooperative and Human Aspects of Software\n  Engineering (CHASE '15), pp. 123-124, 2015", "doi": "10.1109/CHASE.2015.23", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of affects (i.e., emotions, moods) in the workplace has received a\nlot of attention in the last 15 years. Despite the fact that software\ndevelopment has been shown to be intellectual, creative, and driven by\ncognitive activities, and that affects have a deep influence on cognitive\nactivities, software engineering research lacks an understanding of the affects\nof software developers. This note provides (1) common misconceptions of affects\nwhen dealing with job satisfaction, motivation, commitment, well-being, and\nhappiness; (2) validated measurement instruments for affect measurement; and\n(3) our recommendations when measuring the affects of software developers.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 09:13:49 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Graziotin", "Daniel", ""], ["Wang", "Xiaofeng", ""], ["Abrahamsson", "Pekka", ""]]}, {"id": "1505.05753", "submitter": "Mario Fritz", "authors": "Iaroslav Shcherbatyi, Andreas Bulling, Mario Fritz", "title": "GazeDPM: Early Integration of Gaze Information in Deformable Part Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing number of works explore collaborative human-computer systems in\nwhich human gaze is used to enhance computer vision systems. For object\ndetection these efforts were so far restricted to late integration approaches\nthat have inherent limitations, such as increased precision without increase in\nrecall. We propose an early integration approach in a deformable part model,\nwhich constitutes a joint formulation over gaze and visual data. We show that\nour GazeDPM method improves over the state-of-the-art DPM baseline by 4% and a\nrecent method for gaze-supported object detection by 3% on the public POET\ndataset. Our approach additionally provides introspection of the learnt models,\ncan reveal salient image structures, and allows us to investigate the interplay\nbetween gaze attracting and repelling areas, the importance of view-specific\nmodels, as well as viewers' personal biases in gaze patterns. We finally study\nimportant practical aspects of our approach, such as the impact of using\nsaliency maps instead of real fixations, the impact of the number of fixations,\nas well as robustness to gaze estimation error.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 14:39:51 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Shcherbatyi", "Iaroslav", ""], ["Bulling", "Andreas", ""], ["Fritz", "Mario", ""]]}, {"id": "1505.05788", "submitter": "Weinan Zhang", "authors": "Weinan Zhang, Ye Pan, Tianxiong Zhou, Jun Wang", "title": "An Empirical Study on Display Ad Impression Viewability Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Display advertising normally charges advertisers for every single ad\nimpression. Specifically, if an ad in a webpage has been loaded in the browser,\nan ad impression is counted. However, due to the position and size of the ad\nslot, lots of ads are actually not viewed but still measured as impressions and\ncharged. These fraud ad impressions indeed undermine the efficacy of display\nadvertising. A perfect ad impression viewability measurement should match what\nthe user has really viewed with a short memory. In this paper, we conduct\nextensive investigations on display ad impression viewability measurements on\ndimensions of ad creative displayed pixel percentage and exposure time to find\nwhich measurement provides the most accurate ad impression counting. The\nempirical results show that the most accurate measurement counts one ad\nimpression if more than 75% of the ad creative pixels have been exposed for at\nleast 2 continuous seconds.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 17:00:23 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Zhang", "Weinan", ""], ["Pan", "Ye", ""], ["Zhou", "Tianxiong", ""], ["Wang", "Jun", ""]]}, {"id": "1505.06532", "submitter": "Ali Jahanian", "authors": "Ali Jahanian, S. V. N. Vishwanathan, Jan P. Allebach", "title": "Colors $-$Messengers of Concepts: Visual Design Mining for Learning\n  Color Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the concept of color semantics by modeling a dataset of\nmagazine cover designs, evaluating the model via crowdsourcing, and\ndemonstrating several prototypes that facilitate color-related design tasks. We\ninvestigate a probabilistic generative modeling framework that expresses\nsemantic concepts as a combination of color and word distributions\n$-$color-word topics. We adopt an extension to Latent Dirichlet Allocation\n(LDA) topic modeling called LDA-dual to infer a set of color-word topics over a\ncorpus of 2,654 magazine covers spanning 71 distinct titles and 12 genres.\nWhile LDA models text documents as distributions over word topics, we model\nmagazine covers as distributions over color-word topics. The results of our\ncrowdsourced experiments confirm that the model is able to successfully\ndiscover the associations between colors and linguistic concepts. Finally, we\ndemonstrate several simple prototypes that apply the learned model to color\npalette recommendation, design example retrieval, image retrieval, image color\nselection, and image recoloring.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 03:34:46 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Jahanian", "Ali", ""], ["Vishwanathan", "S. V. N.", ""], ["Allebach", "Jan P.", ""]]}, {"id": "1505.07096", "submitter": "Pietro Michelucci", "authors": "Pietro Michelucci, Lea Shanley, Janis Dickinson, Haym Hirsh", "title": "A U.S. Research Roadmap for Human Computation", "comments": "32 pages, 25 figures, Workshop report from the CRA-sponsored Human\n  Computation Roadmap Summit: P. Michelucci, L. Shanley, J. Dickinson, and H.\n  Hirsh, A U.S. Research Roadmap for Human Computation, Computing Community\n  Consortium Technical Report, 2015", "journal-ref": null, "doi": "10.13140/RG.2.1.4517.2648", "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Web has made it possible to harness human cognition en masse to achieve\nnew capabilities. Some of these successes are well known; for example Wikipedia\nhas become the go-to place for basic information on all things; Duolingo\nengages millions of people in real-life translation of text, while\nsimultaneously teaching them to speak foreign languages; and fold.it has\nenabled public-driven scientific discoveries by recasting complex biomedical\nchallenges into popular online puzzle games. These and other early successes\nhint at the tremendous potential for future crowd-powered capabilities for the\nbenefit of health, education, science, and society. In the process, a new field\ncalled Human Computation has emerged to better understand, replicate, and\nimprove upon these successes through scientific research. Human Computation\nrefers to the science that underlies online crowd-powered systems and was the\ntopic of a recent visioning activity in which a representative cross-section of\nresearchers, industry practitioners, visionaries, funding agency\nrepresentatives, and policy makers came together to understand what makes\ncrowd-powered systems successful. Teams of experts considered past, present,\nand future human computation systems to explore which kinds of crowd-powered\nsystems have the greatest potential for societal impact and which kinds of\nresearch will best enable the efficient development of new crowd-powered\nsystems to achieve this impact. This report summarize the products and findings\nof those activities as well as the unconventional process and activities\nemployed by the workshop, which were informed by human computation research.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 19:49:24 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Michelucci", "Pietro", ""], ["Shanley", "Lea", ""], ["Dickinson", "Janis", ""], ["Hirsh", "Haym", ""]]}, {"id": "1505.07240", "submitter": "Daniel Graziotin", "authors": "Daniel Graziotin, Xiaofeng Wang and Pekka Abrahamsson", "title": "How Do You Feel, Developer? An Explanatory Theory of the Impact of\n  Affects on Programming Performance", "comments": "24 pages, 2 figures. Postprint", "journal-ref": "PeerJ Computer Science 1:e18", "doi": "10.7717/peerj-cs.18", "report-no": null, "categories": "cs.SE cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affects---emotions and moods---have an impact on cognitive activities and the\nworking performance of individuals. Development tasks are undertaken through\ncognitive processes, yet software engineering research lacks theory on affects\nand their impact on software development activities. In this paper, we report\non an interpretive study aimed at broadening our understanding of the\npsychology of programming in terms of the experience of affects while\nprogramming, and the impact of affects on programming performance. We conducted\na qualitative interpretive study based on: face-to-face open-ended interviews,\nin-field observations, and e-mail exchanges. This enabled us to construct a\nnovel explanatory theory of the impact of affects on development performance.\nThe theory is explicated using an established taxonomy framework. The proposed\ntheory builds upon the concepts of events, affects, attractors, focus, goals,\nand performance. Theoretical and practical implications are given.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 09:42:12 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2015 13:57:52 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2015 14:07:34 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Graziotin", "Daniel", ""], ["Wang", "Xiaofeng", ""], ["Abrahamsson", "Pekka", ""]]}, {"id": "1505.07267", "submitter": "Gilles Falquet", "authors": "Claudine M\\'etral and Gilles Falquet", "title": "Prototyping Information Visualization in 3D City Models: a Model-based\n  Approach", "comments": "Proc. of 3DGeoInfo 2014 Conference, Dubai, November 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When creating 3D city models, selecting relevant visualization techniques is\na particularly difficult user interface design task. A first obstacle is that\ncurrent geodata-oriented tools, e.g. ArcGIS, have limited 3D capabilities and\nlimited sets of visualization techniques. Another important obstacle is the\nlack of unified description of information visualization techniques for 3D city\nmodels. If many techniques have been devised for different types of data or\ninformation (wind flows, air quality fields, historic or legal texts, etc.)\nthey are generally described in articles, and not really formalized. In this\npaper we address the problem of visualizing information in (rich) 3D city\nmodels by presenting a model-based approach for the rapid prototyping of\nvisualization techniques. We propose to represent visualization techniques as\nthe composition of graph transformations. We show that these transformations\ncan be specified with SPARQL construction operations over RDF graphs. These\nspecifications can then be used in a prototype generator to produce 3D scenes\nthat contain the 3D city model augmented with data represented using the\ndesired technique.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 11:25:46 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["M\u00e9tral", "Claudine", ""], ["Falquet", "Gilles", ""]]}, {"id": "1505.07310", "submitter": "Md. Iftekhar Tanveer", "authors": "M. Iftekhar Tanveer", "title": "Use of Laplacian Projection Technique for Summarizing Likert Scale\n  Annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summarizing Likert scale ratings from human annotators is an important step\nfor collecting human judgments. In this project we study a novel, graph\ntheoretic method for this purpose. We also analyze a few interesting properties\nfor this approach using real annotation datasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 15:45:00 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Tanveer", "M. Iftekhar", ""]]}, {"id": "1505.07395", "submitter": "Marko Horvat", "authors": "Marko Horvat, Dujo Duvnjak, Davor Jug", "title": "GWAT: The Geneva Affective Picture Database WordNet Annotation Tool", "comments": "5 pages, 3 figures. In the Proceedings of 38th International\n  Convention on Information and Communication Technology, Electronics and\n  Microelectronics MIPRO 2015 (pp. 1403-1407)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Geneva Affective Picture Database WordNet Annotation Tool (GWAT) is a\nuser-friendly web application for manual annotation of pictures in Geneva\nAffective Picture Database (GAPED) with WordNet. The annotation tool has an\nintuitive interface which can be efficiently used with very little technical\ntraining. A single picture may be labeled with many synsets allowing experts to\ndescribe semantics with different levels of detail. Noun, verb, adjective and\nadverb synsets can be keyword-searched and attached to a specific GAPED picture\nwith their unique identification numbers. Changes are saved automatically in\nthe tool's relational database. The attached synsets can be reviewed, changed\nor deleted later. Additionally, GAPED pictures may be browsed in the tool's\nuser interface using simple commands where previously attached WordNet synsets\nare displayed alongside the pictures. Stored annotations can be exported from\nthe tool's database to different data formats and used in 3rd party\napplications if needed. Since GAPED does not define keywords of individual\npictures but only a general category of picture groups, GWAT represents a\nsignificant improvement towards development of comprehensive picture semantics.\nThe tool was developed with open technologies WordNet API, Apache, PHP5 and\nMySQL. It is freely available for scientific and non-commercial use.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 16:27:23 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 14:07:05 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Horvat", "Marko", ""], ["Duvnjak", "Dujo", ""], ["Jug", "Davor", ""]]}, {"id": "1505.07396", "submitter": "Marko Horvat", "authors": "Marko Horvat, Davor Kukolja, Dragutin Ivanec", "title": "Retrieval of multimedia stimuli with semantic and emotional cues:\n  Suggestions from a controlled study", "comments": "4 pages, 3 figures, 1 table. In the Proceedings of 38th International\n  Convention on Information and Communication Technology, Electronics and\n  Microelectronics MIPRO 2015 (pp. 1399-1402)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to efficiently search pictures with annotated semantics and\nemotion is an important problem for Human-Computer Interaction with\nconsiderable interdisciplinary significance. Accuracy and speed of the\nmultimedia retrieval process depends on the chosen metadata annotation model.\nThe quality of such multifaceted retrieval is opposed to the potential\ncomplexity of data setup procedures and development of multimedia annotations.\nAdditionally, a recent study has shown that databases of emotionally annotated\nmultimedia are still being predominately searched manually which highlights the\nneed to study this retrieval modality. To this regard we present a study with N\n= 75 participants aimed to evaluate the influence of keywords and dimensional\nemotions in manual retrieval of pictures. The study showed that if the\nmultimedia database is comparatively small emotional annotations are sufficient\nto achieve a fast retrieval despite comparatively lesser overall accuracy. In a\nlarger dataset semantic annotations became necessary for efficient retrieval\nalthough they contributed to a slower beginning of the search process. The\nexperiment was performed in a controlled environment with a team of psychology\nexperts. The results were statistically consistent with validates measures of\nthe participants' perceptual speed.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 16:34:55 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 14:09:38 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Horvat", "Marko", ""], ["Kukolja", "Davor", ""], ["Ivanec", "Dragutin", ""]]}, {"id": "1505.07398", "submitter": "Marko Horvat", "authors": "Marko Horvat, Davor Kukolja, Dragutin Ivanec", "title": "Comparing affective responses to standardized pictures and videos: A\n  study report", "comments": "5 pages, 4 figures. In the Proceedings of 38th International\n  Convention on Information and Communication Technology, Electronics and\n  Microelectronics MIPRO 2015 (pp. 1394-1398)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimedia documents such as text, images, sounds or videos elicit emotional\nresponses of different polarity and intensity in exposed human subjects. These\nstimuli are stored in affective multimedia databases. The problem of emotion\nprocessing is an important issue in Human-Computer Interaction and different\ninterdisciplinary studies particularly those related to psychology and\nneuroscience. Accurate prediction of users' attention and emotion has many\npractical applications such as the development of affective computer\ninterfaces, multifaceted search engines, video-on-demand, Internet\ncommunication and video games. To this regard we present results of a study\nwith N=10 participants to investigate the capability of standardized affective\nmultimedia databases in stimulation of emotion. Each participant was exposed to\npicture and video stimuli with previously determined semantics and emotion.\nDuring exposure participants' physiological signals were recorded and estimated\nfor emotion in an off-line analysis. Participants reported their emotion states\nafter each exposure session. The a posteriori and a priori emotion values were\ncompared. The experiment showed, among other reported results, that carefully\ndesigned video sequences induce a stronger and more accurate emotional reaction\nthan pictures. Individual participants' differences greatly influence the\nintensity and polarity of experienced emotion.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 16:39:05 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 14:11:16 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Horvat", "Marko", ""], ["Kukolja", "Davor", ""], ["Ivanec", "Dragutin", ""]]}, {"id": "1505.07522", "submitter": "Daniele Quercia", "authors": "Miriam Redi, Daniele Quercia, Lindsay T. Graham, Samuel D. Gosling", "title": "Like Partying? Your Face Says It All. Predicting the Ambiance of Places\n  with Profile Pictures", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To choose restaurants and coffee shops, people are increasingly relying on\nsocial-networking sites. In a popular site such as Foursquare or Yelp, a place\ncomes with descriptions and reviews, and with profile pictures of people who\nfrequent them. Descriptions and reviews have been widely explored in the\nresearch area of data mining. By contrast, profile pictures have received\nlittle attention. Previous work showed that people are able to partly guess a\nplace's ambiance, clientele, and activities not only by observing the place\nitself but also by observing the profile pictures of its visitors. Here we\nfurther that work by determining which visual cues people may have relied upon\nto make their guesses; showing that a state-of-the-art algorithm could make\npredictions more accurately than humans at times; and demonstrating that the\nvisual cues people relied upon partly differ from those of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 01:23:05 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Redi", "Miriam", ""], ["Quercia", "Daniele", ""], ["Graham", "Lindsay T.", ""], ["Gosling", "Samuel D.", ""]]}, {"id": "1505.07783", "submitter": "Jeremy Frey", "authors": "J\\'er\\'emy Frey (UB, LaBRI, INRIA Bordeaux - Sud-Ouest), Aur\\'elien\n  Appriou (INRIA Bordeaux - Sud-Ouest, UB), Fabien Lotte (INRIA Bordeaux -\n  Sud-Ouest, LaBRI), Martin Hachet (INRIA Bordeaux - Sud-Ouest, LaBRI)", "title": "Estimating Visual Comfort in Stereoscopic Displays Using\n  Electroencephalography: A Proof-of-Concept", "comments": "INTERACT, Sep 2015, Bamberg, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With stereoscopic displays, a depth sensation that is too strong could impede\nvisual comfort and result in fatigue or pain. Electroencephalography (EEG) is a\ntechnology which records brain activity. We used it to develop a novel\nbrain-computer interface that monitors users' states in order to reduce visual\nstrain. We present the first proof-of-concept system that discriminates\ncomfortable conditions from uncomfortable ones during stereoscopic vision using\nEEG. It reacts within 1s to depth variations, achieving 63% accuracy on average\nand 74% when 7 consecutive variations are measured. This study could lead to\nadaptive systems that automatically suit stereoscopic displays to users and\nviewing conditions.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 18:11:26 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "UB, LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Appriou", "Aur\u00e9lien", "", "INRIA Bordeaux - Sud-Ouest, UB"], ["Lotte", "Fabien", "", "INRIA Bordeaux -\n  Sud-Ouest, LaBRI"], ["Hachet", "Martin", "", "INRIA Bordeaux - Sud-Ouest, LaBRI"]]}, {"id": "1505.07940", "submitter": "Jeremy Frey", "authors": "Dennis Wobrock (INRIA Bordeaux - Sud-Ouest), J\\'er\\'emy Frey (UB,\n  LaBRI, INRIA Bordeaux - Sud-Ouest), Delphine Graeff, Jean-Baptiste De La\n  Rivi\\`ere, Julien Castet, Fabien Lotte (LaBRI, INRIA Bordeaux - Sud-Ouest)", "title": "Continuous Mental Effort Evaluation during 3D Object Manipulation Tasks\n  based on Brain and Physiological Signals", "comments": "Published in INTERACT, Sep 2015, Bamberg, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing 3D User Interfaces (UI) requires adequate evaluation tools to\nensure good usability and user experience. While many evaluation tools are\nalready available and widely used, existing approaches generally cannot provide\ncontinuous and objective measures of usa-bility qualities during interaction\nwithout interrupting the user. In this paper, we propose to use brain (with\nElectroEncephaloGraphy) and physiological (ElectroCardioGraphy, Galvanic Skin\nResponse) signals to continuously assess the mental effort made by the user to\nperform 3D object manipulation tasks. We first show how this mental effort\n(a.k.a., mental workload) can be estimated from such signals, and then measure\nit on 8 participants during an actual 3D object manipulation task with an input\ndevice known as the CubTile. Our results suggest that monitoring workload\nenables us to continuously assess the 3DUI and/or interaction technique\nease-of-use. Overall, this suggests that this new measure could become a useful\naddition to the repertoire of available evaluation tools, enabling a finer\ngrain assessment of the ergonomic qualities of a given 3D user interface.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 06:53:08 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Wobrock", "Dennis", "", "INRIA Bordeaux - Sud-Ouest"], ["Frey", "J\u00e9r\u00e9my", "", "UB,\n  LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Graeff", "Delphine", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"], ["De La Rivi\u00e8re", "Jean-Baptiste", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Castet", "Julien", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"], ["Lotte", "Fabien", "", "LaBRI, INRIA Bordeaux - Sud-Ouest"]]}]