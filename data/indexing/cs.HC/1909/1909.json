[{"id": "1909.00031", "submitter": "Toby Jia-Jun Li", "authors": "Toby Jia-Jun Li, Marissa Radensky, Justin Jia, Kirielle Singarajah,\n  Tom M. Mitchell, Brad A. Myers", "title": "Interactive Task and Concept Learning from Natural Language Instructions\n  and GUI Demonstrations", "comments": "The AAAI-20 Workshop on Intelligent Process Automation (IPA-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language programming is a promising approach to enable end users to\ninstruct new tasks for intelligent agents. However, our formative study found\nthat end users would often use unclear, ambiguous or vague concepts when\nnaturally instructing tasks in natural language, especially when specifying\nconditionals. Existing systems have limited support for letting the user teach\nagents new concepts or explaining unclear concepts. In this paper, we describe\na new multi-modal domain-independent approach that combines natural language\nprogramming and programming-by-demonstration to allow users to first naturally\ndescribe tasks and associated conditions at a high level, and then collaborate\nwith the agent to recursively resolve any ambiguities or vagueness through\nconversations and demonstrations. Users can also define new procedures and\nconcepts by demonstrating and referring to contents within GUIs of existing\nmobile apps. We demonstrate this approach in PUMICE, an end-user programmable\nagent that implements this approach. A lab study with 10 users showed its\nusability.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 18:35:01 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 00:57:53 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Li", "Toby Jia-Jun", ""], ["Radensky", "Marissa", ""], ["Jia", "Justin", ""], ["Singarajah", "Kirielle", ""], ["Mitchell", "Tom M.", ""], ["Myers", "Brad A.", ""]]}, {"id": "1909.00045", "submitter": "Chuanmin Mi", "authors": "Chunmin Mi, Runjie Xu, Ching-Torng Lin, Run Yu Meng", "title": "An active smartphone authentication method based on daily cyclical\n  activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones have become an important tool for people's daily lives, which\nbrings higher security requirements in high-risk application areas, for\nexample, mobile payment. Although the combination of physical password,\nfingerprint and facial recognition have improved the security to a certain\nextent, there still exists a high risk of being decrepted. This paper attempts\nan algorithm which is more suitable for studying human partial periodic\nactivity, namely Prophet algorithm. This algorithm has strong robustness for\nmissing data and trend change, and can deal with outliers well. The\nexperimental results on the UniMiB SHAR DATA show that the user simply needs to\ndo 5 cycles of specified actions to realize the prediction of the next time\nseries. The Error analysis of cross validation was applied to 4 different\nindicators, and the Mean Squared Error of the optimal result \"Jumping\" behavior\nwas only 8.20%. With these appealing features, The main contribution of this\npaper is to propose a smart phone user identification system based on\nbehavioral activity cycle, which can be replicated in other behavioral studies.\nAnother outstanding feature of such a system is the capability of fitting\nmodels using small data set by exploiting behavioral characteristics derived\nfrom periodicity and thus reducing dependence on sensor scanning frequency,\ntherefore the system balances among energy consumption, data quantity and\nfitting accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 19:33:18 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 08:11:30 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Mi", "Chunmin", ""], ["Xu", "Runjie", ""], ["Lin", "Ching-Torng", ""], ["Meng", "Run Yu", ""]]}, {"id": "1909.00193", "submitter": "Dimitri Ognibene", "authors": "Francesca Bianco and Dimitri Ognibene", "title": "Functional advantages of an adaptive Theory of Mind for robotics: a\n  review of current architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Great advancements have been achieved in the field of robotics, however, main\nchallenges remain, including building robots with an adaptive Theory of Mind\n(ToM). In the present paper, seven current robotic architectures for\nhuman-robot interactions were described as well as four main functional\nadvantages of equipping robots with an adaptive ToM. The aim of the present\npaper was to determine in which way and how often ToM features are integrated\nin the architectures analyzed, and if they provide robots with the associated\nfunctional advantages. Our assessment shows that different methods are used to\nimplement ToM features in robotic architectures. Furthermore, while a ToM for\nfalse-belief understanding and tracking is often built in social robotic\narchitectures, a ToM for proactivity, active perception and learning is less\ncommon. Nonetheless, progresses towards better adaptive ToM features in robots\nare warranted to provide them with full access to the advantages of having a\nToM resembling that of humans.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 10:59:58 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bianco", "Francesca", ""], ["Ognibene", "Dimitri", ""]]}, {"id": "1909.00197", "submitter": "Dimitri Ognibene", "authors": "Francesca Bianco and Dimitri Ognibene", "title": "Transferring Adaptive Theory of Mind to social robots: insights from\n  developmental psychology to robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent advancement in the social robotic field, important\nlimitations restrain its progress and delay the application of robots in\neveryday scenarios. In the present paper, we propose to develop computational\nmodels inspired by our knowledge of human infants' social adaptive abilities.\nWe believe this may provide solutions at an architectural level to overcome the\nlimits of current systems. Specifically, we present the functional advantages\nthat adaptive Theory of Mind (ToM) systems would support in robotics (i.e.,\nmentalizing for belief understanding, proactivity and preparation, active\nperception and learning) and contextualize them in practical applications. We\nreview current computational models mainly based on the simulation and\nteleological theories, and robotic implementations to identify the limitations\nof ToM functions in current robotic architectures and suggest a possible future\ndevelopmental pathway. Finally, we propose future studies to create innovative\ncomputational models integrating the properties of the simulation and\nteleological approaches for an improved adaptive ToM ability in robots with the\naim of enhancing human-robot interactions and permitting the application of\nrobots in unexplored environments, such as disasters and construction sites. To\nachieve this goal, we suggest directing future research towards the modern\ncross-talk between the fields of robotics and developmental psychology.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 11:19:56 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Bianco", "Francesca", ""], ["Ognibene", "Dimitri", ""]]}, {"id": "1909.00360", "submitter": "Nicholas Cummins Dr", "authors": "Vidhyasaharan Sethu, Emily Mower Provost, Julien Epps, Carlos Busso,\n  Nicholas Cummins, Shrikanth Narayanan", "title": "The Ambiguous World of Emotion Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence and machine learning systems have demonstrated huge\nimprovements and human-level parity in a range of activities, including speech\nrecognition, face recognition and speaker verification. However, these diverse\ntasks share a key commonality that is not true in affective computing: the\nground truth information that is inferred can be unambiguously represented.\nThis observation provides some hints as to why affective computing, despite\nhaving attracted the attention of researchers for years, may not still be\nconsidered a mature field of research. A key reason for this is the lack of a\ncommon mathematical framework to describe all the relevant elements of emotion\nrepresentations. This paper proposes the AMBiguous Emotion Representation\n(AMBER) framework to address this deficiency. AMBER is a unified framework that\nexplicitly describes categorical, numerical and ordinal representations of\nemotions, including time varying representations. In addition to explaining the\ncore elements of AMBER, the paper also discusses how some of the commonly\nemployed emotion representation schemes can be viewed through the AMBER\nframework, and concludes with a discussion of how the proposed framework can be\nused to reason about current and future affective computing systems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 09:05:58 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Sethu", "Vidhyasaharan", ""], ["Provost", "Emily Mower", ""], ["Epps", "Julien", ""], ["Busso", "Carlos", ""], ["Cummins", "Nicholas", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "1909.00482", "submitter": "Mario Amrehn", "authors": "Mario Amrehn, Stefan Steidl, Reinier Kortekaas, Maddalena Strumia,\n  Markus Weingarten, Markus Kowarschik, Andreas Maier", "title": "A Semi-Automated Usability Evaluation Framework for Interactive Image\n  Segmentation Systems", "comments": "Accepted as research article at the International Journal of\n  Biomedical Imaging, Hindawi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For complex segmentation tasks, the achievable accuracy of fully automated\nsystems is inherently limited. Specifically, when a precise segmentation result\nis desired for a small amount of given data sets, semi-automatic methods\nexhibit a clear benefit for the user. The optimization of human computer\ninteraction (HCI) is an essential part of interactive image segmentation.\nNevertheless, publications introducing novel interactive segmentation systems\n(ISS) often lack an objective comparison of HCI aspects. It is demonstrated,\nthat even when the underlying segmentation algorithm is the same throughout\ninteractive prototypes, their user experience may vary substantially. As a\nresult, users prefer simple interfaces as well as a considerable degree of\nfreedom to control each iterative step of the segmentation. In this article, an\nobjective method for the comparison of ISS is proposed, based on extensive user\nstudies. A summative qualitative content analysis is conducted via abstraction\nof visual and verbal feedback given by the participants. A direct assessment of\nthe segmentation system is executed by the users via the system usability scale\n(SUS) and AttrakDiff-2 questionnaires. Furthermore, an approximation of the\nfindings regarding usability aspects in those studies is introduced, conducted\nsolely from the system-measurable user actions during their usage of\ninteractive segmentation prototypes. The prediction of all questionnaire\nresults has an average relative error of 8.9%, which is close to the expected\nprecision of the questionnaire results themselves. This automated evaluation\nscheme may significantly reduce the resources necessary to investigate each\nvariation of a prototype's user interface (UI) features and segmentation\nmethodologies.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 22:33:06 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Amrehn", "Mario", ""], ["Steidl", "Stefan", ""], ["Kortekaas", "Reinier", ""], ["Strumia", "Maddalena", ""], ["Weingarten", "Markus", ""], ["Kowarschik", "Markus", ""], ["Maier", "Andreas", ""]]}, {"id": "1909.00608", "submitter": "Manuela Waldner", "authors": "Sebastian Sippl, Michael Sedlmair, Manuela Waldner", "title": "Collecting and Structuring Information in the Information Collage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge workers, such as scientists, journalists, or consultants,\nadaptively seek, gather, and consume information. These processes are often\ninefficient as existing user interfaces provide limited possibilities to\ncombine information from various sources and different formats into a common\nknowledge representation. In this paper, we present the concept of an\ninformation collage (IC) -- a web browser extension combining manual spatial\norganization of gathered information fragments and automatic text analysis for\ninteractive content exploration and expressive visual summaries. We used IC for\ncase studies with knowledge workers from different domains and longer-term\nfield studies over a period of one month. We identified three different ways\nhow users collect and structure information and provide design recommendations\nhow to support these observed usage strategies.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 09:01:49 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Sippl", "Sebastian", ""], ["Sedlmair", "Michael", ""], ["Waldner", "Manuela", ""]]}, {"id": "1909.00699", "submitter": "Federica Paganelli", "authors": "Federica Paganelli, Georgios Mylonas, Giovanni Cuffaro, and Ilaria\n  Nesi", "title": "Experiences from Using Gamification and IoT-based Educational Tools in\n  High Schools towards Energy Savings", "comments": "to be presented at 2019 European Conference on Ambient Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raising awareness among young people, and especially students, on the\nrelevance of behavior change for achieving energy savings is increasingly being\nconsidered as a key enabler towards long-term and cost-effective energy\nefficiency policies. However, the way to successfully apply educational\ninterventions focused on such targets inside schools is still an open question.\nIn this paper, we present our approach for enabling IoT-based energy savings\nand sustainability awareness lectures and promoting data-driven energy-saving\nbehaviors focused on a high school audience. We present our experiences toward\nthe successful application of sets of educational tools and software over a\nreal-world Internet of Things (IoT) deployment. We discuss the use of\ngamification and competition as a very effective end-user engagement mechanism\nfor school audiences. We also present the design of an IoT-based hands-on lab\nactivity, integrated within a high school computer science curricula utilizing\nIoT devices and data produced inside the school building, along with the\nNode-RED platform. We describe the tools used, the organization of the\neducational activities and related goals. We report on the experience carried\nout in both directions in a high school in Italy and conclude by discussing the\nresults in terms of achieved energy savings within an observation period.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 12:55:18 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Paganelli", "Federica", ""], ["Mylonas", "Georgios", ""], ["Cuffaro", "Giovanni", ""], ["Nesi", "Ilaria", ""]]}, {"id": "1909.00855", "submitter": "Roger Turner", "authors": "Roger Turner", "title": "Defining and Adopting an End User Computing Policy: A Case Study", "comments": "25 Pages, 12 Colour Figures. 1 Table. First presented at the EuSpRIG\n  2018 Conference at Imperial College, London. Revised and updated following a\n  further presentation at the EuSpRIG 2019 Conference also at Imperial College,\n  London", "journal-ref": "Proceedings of the EuSpRIG 2019 Conference \"Spreadsheet Risk\n  Management\", Browns, Covent Garden, London, pp27-38, ISBN: 978-1-905404-56-8", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End User Computing carries significant risks if not well controlled. This\npaper is a case study of the introduction of an updated End User Computing\npolicy at the Wesleyan Assurance Society. The paper outlines the plan and\nidentifies various challenges. The paper explains how these challenges were\novercome. We wrote an End User Computing Risk Assessment Application which\ncalculates a risk rating band based on the Complexity, Materiality and Control\n(or lack of it) pertaining to any given application and the basis of assessment\nis given in this paper. The policy uses a risk based approach for assessing and\nmitigating against the highest risks first and obtaining the quickest benefit.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 20:36:10 GMT"}, {"version": "v2", "created": "Sun, 22 Nov 2020 23:39:04 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Turner", "Roger", ""]]}, {"id": "1909.00865", "submitter": "Maria Csernoch", "authors": "Maria Csernoch, Piroska Bir\\'o", "title": "Are digital natives spreadsheet natives?", "comments": "13 Pages, 6 Colour Figures, 9 Tables. First Presented at the EuSpRIG\n  2018 conference, Imperial College, London", "journal-ref": "Proceedings of the EuSpRIG 2019 Conference \"Spreadsheet Risk\n  Management\", Browns, Covent Garden, London, pp1-12 ISBN: 978-1-905404-56-8", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper reports the results of testing first year students of\nInformatics on their algorithmic skills and knowledge transfer abilities in\nspreadsheet environments. The selection of students plays a crucial role in the\nproject. On the one hand, they have officially finished their spreadsheet\ntraining - they know everything - while on the other hand, they do not need any\ntraining, since they are digital natives, to whom digital skills are assigned\nby birth. However, we found that the students had serious difficulties in\nsolving the spreadsheet problems presented: so low were their results that it\nallowed us to form broad tendencies. Considering computational thinking,\nalgorithmic skills, and knowledge transfer abilities, it is clear that those\nstudents performed better who used algorithm-based, multilevel array formulas\ninstead of problem specific, unconnected built-in functions. Furthermore, we\ncan conclude that students, regardless of their birth date and digital\ngeneration assigned to them, are in great need of official, high-mathability,\nalgorithm-based training with expert teachers.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 20:59:22 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Csernoch", "Maria", ""], ["Bir\u00f3", "Piroska", ""]]}, {"id": "1909.01039", "submitter": "Henrich Kolkhorst", "authors": "Henrich Kolkhorst and Wolfram Burgard and Michael Tangermann", "title": "Learning User Preferences for Trajectories from Brain Signals", "comments": "The International Symposium on Robotics Research (ISRR), Hanoi,\n  Vietnam, October 2019; reformatted to two-column layout", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robot motions in the presence of humans should not only be feasible and safe,\nbut also conform to human preferences. This, however, requires user feedback on\nthe robot's behavior. In this work, we propose a novel approach to leverage the\nuser's brain signals as a feedback modality in order to decode the judgment of\nrobot trajectories and rank them according to the user's preferences. We show\nthat brain signals measured using electroencephalography during observation of\na robotic arm's trajectory as well as in response to preference statements are\ninformative regarding the user's preference. Furthermore, we demonstrate that\nuser feedback from brain signals can be used to reliably infer pairwise\ntrajectory preferences as well as to retrieve the preferred observed\ntrajectories of the user with a performance comparable to explicit behavioral\nfeedback.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:20:50 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2019 17:51:42 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Kolkhorst", "Henrich", ""], ["Burgard", "Wolfram", ""], ["Tangermann", "Michael", ""]]}, {"id": "1909.01167", "submitter": "Abraham Glasser", "authors": "Abraham Glasser, Kesavan Kushalnagar, Raja Kushalnagar", "title": "Feasibility of Using Automatic Speech Recognition with Voices of Deaf\n  and Hard-of-Hearing Individuals", "comments": "2 pages, 3 figures", "journal-ref": null, "doi": "10.1145/3132525.3134819", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many personal devices have transitioned from visual-controlled interfaces to\nspeech-controlled interfaces to reduce device costs and interactive friction.\nThis transition has been hastened by the increasing capabilities of\nspeech-controlled interfaces, e.g., Amazon Echo or Apple's Siri. A consequence\nis that people who are deaf or hard of hearing (DHH) may be unable to use these\nspeech-controlled devices. We show that deaf speech has a high error rate\ncompared to hearing speech, in commercial speech-controlled interfaces. Deaf\nspeech had approximately a 78% word error rate (WER) compared to a hearing\nspeech 18% WER. Our findings show that current speech-controlled interfaces are\nnot usable by deaf and hard of hearing people. Therefore, it might be wise to\npursue other methods for deaf persons to deliver natural commands to computers.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:33:25 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Glasser", "Abraham", ""], ["Kushalnagar", "Kesavan", ""], ["Kushalnagar", "Raja", ""]]}, {"id": "1909.01176", "submitter": "Abraham Glasser", "authors": "Abraham Glasser, Kesavan Kushalnagar, Raja Kushalnagar", "title": "Deaf, Hard of Hearing, and Hearing Perspectives on using Automatic\n  Speech Recognition in Conversation", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": "10.1145/3132525.3134781", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many personal devices have transitioned from visual-controlled interfaces to\nspeech-controlled interfaces to reduce costs and interactive friction,\nsupported by the rapid growth in capabilities of speech-controlled interfaces,\ne.g., Amazon Echo or Apple's Siri. A consequence is that people who are deaf or\nhard of hearing (DHH) may be unable to use these speech-controlled devices. We\nshow that deaf speech has a high error rate compared to hearing speech, in\ncommercial speech-controlled interfaces. Deaf speech had approximately a 78%\nword error rate (WER) compared to a hearing speech 18% WER. Our findings show\nthat current speech-controlled interfaces are not usable by DHH people. Based\non our findings, significant advances in speech recognition software or\nalternative approaches will be needed for deaf use of speech-controlled\ninterfaces. We show that current speech-controlled interfaces are not usable by\nDHH people.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 13:45:29 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Glasser", "Abraham", ""], ["Kushalnagar", "Kesavan", ""], ["Kushalnagar", "Raja", ""]]}, {"id": "1909.01206", "submitter": "Amogh Gudi", "authors": "Amogh Gudi, Marian Bittner, Roelof Lochmans, Jan van Gemert", "title": "Efficient Real-Time Camera Based Estimation of Heart Rate and Its\n  Variability", "comments": "International Conference on Computer Vision (ICCV) Workshop on\n  Computer Vision for Physiological Measurement (CVPM) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote photo-plethysmography (rPPG) uses a remotely placed camera to\nestimating a person's heart rate (HR). Similar to how heart rate can provide\nuseful information about a person's vital signs, insights about the underlying\nphysio/psychological conditions can be obtained from heart rate variability\n(HRV). HRV is a measure of the fine fluctuations in the intervals between heart\nbeats. However, this measure requires temporally locating heart beats with a\nhigh degree of precision. We introduce a refined and efficient real-time rPPG\npipeline with novel filtering and motion suppression that not only estimates\nheart rate more accurately, but also extracts the pulse waveform to time heart\nbeats and measure heart rate variability. This method requires no rPPG specific\ntraining and is able to operate in real-time. We validate our method on a\nself-recorded dataset under an idealized lab setting, and show state-of-the-art\nresults on two public dataset with realistic conditions (VicarPPG and PURE).\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:21:11 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Gudi", "Amogh", ""], ["Bittner", "Marian", ""], ["Lochmans", "Roelof", ""], ["van Gemert", "Jan", ""]]}, {"id": "1909.01218", "submitter": "Maximilian M\\\"uller-Eberstein", "authors": "Maximilian M\\\"uller-Eberstein and Nanne van Noord", "title": "Translating Visual Art into Music", "comments": "Accepted for ICCV 2019 Workshop on Fashion, Art and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Synesthetic Variational Autoencoder (SynVAE) introduced in this research\nis able to learn a consistent mapping between visual and auditive sensory\nmodalities in the absence of paired datasets. A quantitative evaluation on\nMNIST as well as the Behance Artistic Media dataset (BAM) shows that SynVAE is\ncapable of retaining sufficient information content during the translation\nwhile maintaining cross-modal latent space consistency. In a qualitative\nevaluation trial, human evaluators were furthermore able to match musical\nsamples with the images which generated them with accuracies of up to 73%.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 14:36:19 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["M\u00fcller-Eberstein", "Maximilian", ""], ["van Noord", "Nanne", ""]]}, {"id": "1909.01304", "submitter": "Brendon Boldt", "authors": "Brendon Boldt, Zack While, Eric Breimer", "title": "Detecting Compromised Implicit Association Test Results Using Supervised\n  Learning", "comments": "6 pages, 1 figure", "journal-ref": "2018 17th IEEE International Conference on Machine Learning and\n  Applications (ICMLA), Orlando, FL, 2018, pp. 449-453", "doi": "10.1109/ICMLA.2018.00073", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An implicit association test is a human psychological test used to measure\nsubconscious associations. While widely recognized by psychologists as an\neffective tool in measuring attitudes and biases, the validity of the results\ncan be compromised if a subject does not follow the instructions or attempts to\nmanipulate the outcome. Compared to previous work, we collect training data\nusing a more generalized methodology. We train a variety of different\nclassifiers to identify a participant's first attempt versus a second possibly\ncompromised attempt. To compromise the second attempt, participants are shown\ntheir score and are instructed to change it using one of five randomly selected\ndeception methods. Compared to previous work, our methodology demonstrates a\nmore robust and practical framework for accurately identifying a wide variety\nof deception techniques applicable to the IAT.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:52:10 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Boldt", "Brendon", ""], ["While", "Zack", ""], ["Breimer", "Eric", ""]]}, {"id": "1909.01312", "submitter": "Cara Nunez", "authors": "Cara M. Nunez, Sophia R. Williams, Allison M. Okamura, Heather\n  Culbertson", "title": "Understanding Continuous and Pleasant Linear Sensations on the Forearm\n  from a Sequential Discrete Lateral Skin-Slip Haptic Device", "comments": null, "journal-ref": null, "doi": "10.1109/TOH.2019.2941190", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A continuous stroking sensation on the skin can convey messages or emotion\ncues. We seek to induce this sensation using a combination of illusory motion\nand lateral stroking via a haptic device. Our system provides discrete lateral\nskin-slip on the forearm with rotating tactors, which independently provide\nlateral skin-slip in a timed sequence. We vary the sensation by changing the\nangular velocity and delay between adjacent tactors, such that the apparent\nspeed of the perceived stroke ranges from 2.5 to 48.2 cm/s. We investigated\nwhich actuation parameters create the most pleasant and continuous sensations\nthrough a user study with 16 participants. On average, the sensations were\nrated by participants as both continuous and pleasant. The most continuous and\npleasant sensations were created by apparent speeds of 7.7 and 5.1 cm/s,\nrespectively. We also investigated the effect of spacing between contact points\non the pleasantness and continuity of the stroking sensation, and found that\nthe users experience a pleasant and continuous linear sensation even when the\nspace between contact points is relatively large (40 mm). Understanding how\nsequential discrete lateral skin-slip creates continuous linear sensations can\ninfluence the design and control of future wearable haptic devices.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 17:07:37 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Nunez", "Cara M.", ""], ["Williams", "Sophia R.", ""], ["Okamura", "Allison M.", ""], ["Culbertson", "Heather", ""]]}, {"id": "1909.01322", "submitter": "Shikib Mehri", "authors": "Shikib Mehri, Alan W Black and Maxine Eskenazi", "title": "CMU GetGoing: An Understandable and Memorable Dialog System for Seniors", "comments": "Accepted to the Dialog for Good (DiGo) workshop\n  (http://dialogforgood.org) at SIGDial 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice-based technologies are typically developed for the average user, and\nthus generally not tailored to the specific needs of any subgroup of the\npopulation, like seniors. This paper presents CMU GetGoing, an accessible trip\nplanning dialog system designed for senior users. The GetGoing system design is\ndescribed in detail, with particular attention to the senior-tailored features.\nA user study is presented, demonstrating that the senior-tailored features\nsignificantly improve comprehension and retention of information.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 17:35:27 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mehri", "Shikib", ""], ["Black", "Alan W", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1909.01362", "submitter": "Jonathan Chang", "authors": "Jonathan P. Chang and Cristian Danescu-Niculescu-Mizil", "title": "Trouble on the Horizon: Forecasting the Derailment of Online\n  Conversations as they Develop", "comments": "To appear in Proceedings of EMNLP 2019. Data and code to be released\n  as part of the Cornell Conversational Analysis Toolkit (convokit.cornell.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online discussions often derail into toxic exchanges between participants.\nRecent efforts mostly focused on detecting antisocial behavior after the fact,\nby analyzing single comments in isolation. To provide more timely notice to\nhuman moderators, a system needs to preemptively detect that a conversation is\nheading towards derailment before it actually turns toxic. This means modeling\nderailment as an emerging property of a conversation rather than as an isolated\nutterance-level event.\n  Forecasting emerging conversational properties, however, poses several\ninherent modeling challenges. First, since conversations are dynamic, a\nforecasting model needs to capture the flow of the discussion, rather than\nproperties of individual comments. Second, real conversations have an unknown\nhorizon: they can end or derail at any time; thus a practical forecasting model\nneeds to assess the risk in an online fashion, as the conversation develops. In\nthis work we introduce a conversational forecasting model that learns an\nunsupervised representation of conversational dynamics and exploits it to\npredict future derailment as the conversation develops. By applying this model\nto two new diverse datasets of online conversations with labels for antisocial\nevents, we show that it outperforms state-of-the-art systems at forecasting\nderailment.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:00:05 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Chang", "Jonathan P.", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1909.01379", "submitter": "S\\'ebastien Lall\\'e", "authors": "S\\'ebastien Lall\\'e, Dereck Toker, Cristina Conati", "title": "Gaze-Driven Adaptive Interventions for Magazine-Style Narrative\n  Visualizations", "comments": "\\c{opyright} 2019 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promo-tional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 2019", "doi": "10.1109/TVCG.2019.2958540", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the value of gaze-driven adaptive interventions\nto support processing of textual documents with embedded visualizations, i.e.,\nMagazine Style Narrative Visualizations (MSNVs). These interventions are\nprovided dynamically by highlighting relevant data points in the visualization\nwhen the user reads related sentences in the MNSV text, as detected by an\neye-tracker. We conducted a user study during which participants read a set of\nMSNVs with our interventions, and compared their performance and experience\nwith participants who received no interventions. Our work extends previous\nfindings by showing that dynamic, gaze-driven interventions can be delivered\nbased on reading behaviors in MSNVs, a widespread form of documents that have\nnever been considered for gaze-driven adaptation so far. Next, we found that\nthe interventions significantly improved the performance of users with low\nlevels of visualization literacy, i.e., those users who need help the most due\nto their lower ability to process and understand data visualizations. However,\nhigh literacy users were not impacted by the interventions, providing initial\nevidence that gaze-driven interventions can be further improved by\npersonalizing them to the levels of visualization literacy of their users.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 18:03:11 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Lall\u00e9", "S\u00e9bastien", ""], ["Toker", "Dereck", ""], ["Conati", "Cristina", ""]]}, {"id": "1909.01428", "submitter": "Auriol Degbelo", "authors": "Christian Kray and Auriol Degbelo", "title": "Map plasticity", "comments": "Poster Presentation at the 14th International Conference on Spatial\n  Information Theory: COSIT'19. Regensburg, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the arrival of digital maps, the ubiquity of maps has increased sharply\nand new map functionalities have become available such as changing the scale on\nthe fly or displaying/hiding layers. Users can now interact with maps on\nmultiple devices (e.g. smartphones, desktop computers, large-scale displays,\nhead-mounted displays) using different means of interaction such as touch,\nvoice or gestures. However, ensuring map functionalities and good user\nexperience across these devices and modalities frequently entails dedicated\ndevelopment efforts for each combination. In this paper, we argue that\nintroducing an abstract representation of what a map contains and affords can\nunlock new opportunities. For this purpose, we propose the concept of map\nplasticity, the capability of a map-based system to support different contexts\nof use while preserving usability and functionality. Based on this definition,\nwe discuss core components and an example. We also propose a research agenda\nfor realising map plasticity and its benefits.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 20:10:30 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Kray", "Christian", ""], ["Degbelo", "Auriol", ""]]}, {"id": "1909.01463", "submitter": "Qunwei Li", "authors": "Baocheng Geng, Qunwei Li, Pramod K. Varshney", "title": "Prospect Theory Based Crowdsourcing for Classification in the Presence\n  of Spammers", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": "10.1109/TSP.2020.3006754", "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the $M$-ary classification problem via crowdsourcing, where crowd\nworkers respond to simple binary questions and the answers are aggregated via\ndecision fusion. The workers have a reject option to skip answering a question\nwhen they do not have the expertise, or when the confidence of answering that\nquestion correctly is low. We further consider that there are spammers in the\ncrowd who respond to the questions with random guesses. Under the payment\nmechanism that encourages the reject option, we study the behavior of honest\nworkers and spammers, whose objectives are to maximize their monetary rewards.\nTo accurately characterize human behavioral aspects, we employ prospect theory\nto model the rationality of the crowd workers, whose perception of costs and\nprobabilities are distorted based on some value and weight functions,\nrespectively. Moreover, we estimate the number of spammers and employ a\nweighted majority voting decision rule, where we assign an optimal weight for\nevery worker to maximize the system performance. The probability of correct\nclassification and asymptotic system performance are derived. We also provide\nsimulation results to demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 21:25:19 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 13:29:29 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Geng", "Baocheng", ""], ["Li", "Qunwei", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1909.01625", "submitter": "Georgios Mylonas", "authors": "Georgios Mylonas, Dimitrios Amaxilatis, Lidia Pocero, Iraklis\n  Markelis, Joerg Hofstaetter, Pavlos Koulouris", "title": "Using an Educational IoT Lab Kit and Gamification for Energy Awareness\n  in European Schools", "comments": "This is the submitted preprint version of a paper published in the\n  FabLearn Europe'18 conference", "journal-ref": null, "doi": "10.1145/3213818.3213823", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of maker community tools and IoT technologies inside classrooms is\nspreading in an increasing number of education and science fields. GAIA is a\nEuropean research project focused on achieving behavior change for\nsustainability and energy awareness in schools. In this work, we report on how\na large IoT deployment in a number of educational buildings and real-world data\nfrom this infrastructure, are utilized to support a \"maker\" lab kit activity\ninside the classroom, together with a serious game. We also provide some\ninsights to the integration of these activities in the school curriculum, along\nwith a discussion on our feedback so far from a series of workshop activities\nin a number of schools. Our initial results show strong acceptance by the\nschool community.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 08:53:51 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Mylonas", "Georgios", ""], ["Amaxilatis", "Dimitrios", ""], ["Pocero", "Lidia", ""], ["Markelis", "Iraklis", ""], ["Hofstaetter", "Joerg", ""], ["Koulouris", "Pavlos", ""]]}, {"id": "1909.01678", "submitter": "Solomia Fedushko", "authors": "Olha Anisimova, Valeriia Vasylenko, Solomia Fedushko", "title": "Social Networks as a Tool for a Higher Education Institution Image\n  Creation", "comments": "11 pages, 3 figures", "journal-ref": "CEUR Workshop Proceedings. Vol 2392: Proceedings of the 1st\n  International Workshop on Control, Optimisation and Analytical Processing of\n  Social Networks, COAPSN-2019, 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents the dynamics of social networks users increase,\ndepending on the total world population from 2010 to 2018. It also identifies\nthe most popular social networks in Ukraine. The systematic risk indicator of\nusing social networks relative to the total number of Internet resources users\nis determined. Types of social intercourse in the process of the higher\neducation institution image creation are presented. The peculiarities of using\nsocial networks in the formation of a positive image of an educational\ninstitution are highlighted. The statistical indicators of user actions in the\nofficial group of the Faculty of Mathematics and Information Technologies of\nVasyl Stus Donetsk National University in January, February and March 2019 are\npresented, as well as the average attraction coefficient of users depending on\nthe subject of publications. The main technologies of astroturfing in the\ncreation process of the higher education institution negative image are\nconsidered.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:25:21 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Anisimova", "Olha", ""], ["Vasylenko", "Valeriia", ""], ["Fedushko", "Solomia", ""]]}, {"id": "1909.01871", "submitter": "Khanh Nguyen", "authors": "Khanh Nguyen and Hal Daum\\'e III", "title": "Help, Anna! Visual Navigation with Natural Multimodal Assistance via\n  Retrospective Curiosity-Encouraging Imitation Learning", "comments": "In EMNLP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile agents that can leverage help from humans can potentially accomplish\nmore complex tasks than they could entirely on their own. We develop \"Help,\nAnna!\" (HANNA), an interactive photo-realistic simulator in which an agent\nfulfills object-finding tasks by requesting and interpreting natural\nlanguage-and-vision assistance. An agent solving tasks in a HANNA environment\ncan leverage simulated human assistants, called ANNA (Automatic Natural\nNavigation Assistants), which, upon request, provide natural language and\nvisual instructions to direct the agent towards the goals. To address the HANNA\nproblem, we develop a memory-augmented neural agent that hierarchically models\nmultiple levels of decision-making, and an imitation learning algorithm that\nteaches the agent to avoid repeating past mistakes while simultaneously\npredicting its own chances of making future progress. Empirically, our approach\nis able to ask for help more effectively than competitive baselines and, thus,\nattains higher task success rate on both previously seen and previously unseen\nenvironments. We publicly release code and data at\nhttps://github.com/khanhptnk/hanna . A video demo is available at\nhttps://youtu.be/18P94aaaLKg .\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 15:20:01 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 16:16:31 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 03:30:13 GMT"}, {"version": "v4", "created": "Tue, 8 Oct 2019 16:08:40 GMT"}, {"version": "v5", "created": "Mon, 21 Oct 2019 07:12:17 GMT"}, {"version": "v6", "created": "Fri, 22 Nov 2019 16:11:17 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Nguyen", "Khanh", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "1909.02043", "submitter": "Chirag Tailor", "authors": "Noah Bilgrien, Roy Finkelberg, Chirag Tailor, India Irish, Girish\n  Murali, Abhishek Mangal, Niklas Gustafsson, Sumedha Raman, Thad Starner, Rosa\n  Arriaga", "title": "PARQR: Augmenting the Piazza Online Forum to Better Support Degree\n  Seeking Online Masters Students", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce PARQR, a tool for online education forums that reduces duplicate\nposts by 40\\% in a degree seeking online masters program at a top university.\nInstead of performing a standard keyword search, PARQR monitors questions as\nstudents compose them and continuously suggests relevant posts. In testing,\nPARQR correctly recommends a relevant post, if one exists, 73.5\\% of the time.\nWe discuss PARQR's design, initial experimental results comparing different\nsemesters with and without PARQR, and interviews we conducted with teaching\ninstructors regarding their experience with PARQR.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 18:37:58 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Bilgrien", "Noah", ""], ["Finkelberg", "Roy", ""], ["Tailor", "Chirag", ""], ["Irish", "India", ""], ["Murali", "Girish", ""], ["Mangal", "Abhishek", ""], ["Gustafsson", "Niklas", ""], ["Raman", "Sumedha", ""], ["Starner", "Thad", ""], ["Arriaga", "Rosa", ""]]}, {"id": "1909.02309", "submitter": "Dakuo Wang", "authors": "Dakuo Wang, Justin D. Weisz, Michael Muller, Parikshit Ram, Werner\n  Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, Alexander Gray", "title": "Human-AI Collaboration in Data Science: Exploring Data Scientists'\n  Perceptions of Automated AI", "comments": null, "journal-ref": null, "doi": "10.1145/3359313", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid advancement of artificial intelligence (AI) is changing our lives\nin many ways. One application domain is data science. New techniques in\nautomating the creation of AI, known as AutoAI or AutoML, aim to automate the\nwork practices of data scientists. AutoAI systems are capable of autonomously\ningesting and pre-processing data, engineering new features, and creating and\nscoring models based on a target objectives (e.g. accuracy or run-time\nefficiency). Though not yet widely adopted, we are interested in understanding\nhow AutoAI will impact the practice of data science. We conducted interviews\nwith 20 data scientists who work at a large, multinational technology company\nand practice data science in various business settings. Our goal is to\nunderstand their current work practices and how these practices might change\nwith AutoAI. Reactions were mixed: while informants expressed concerns about\nthe trend of automating their jobs, they also strongly felt it was inevitable.\nDespite these concerns, they remained optimistic about their future job\nsecurity due to a view that the future of data science work will be a\ncollaboration between humans and AI systems, in which both automation and human\nexpertise are indispensable.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 10:39:37 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Wang", "Dakuo", ""], ["Weisz", "Justin D.", ""], ["Muller", "Michael", ""], ["Ram", "Parikshit", ""], ["Geyer", "Werner", ""], ["Dugan", "Casey", ""], ["Tausczik", "Yla", ""], ["Samulowitz", "Horst", ""], ["Gray", "Alexander", ""]]}, {"id": "1909.02507", "submitter": "Yusuf Sermet", "authors": "Yusuf Sermet and Ibrahim Demir", "title": "A Generalized Web Component for Domain-Independent Smart Assistants", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces an open-source web component, Instant Expert, which\nallows robust and efficient integration of a natural language question\nanswering system to web-based platforms in any domain. Web Components are a set\nof web technologies to allow the creation of reusable, customizable, and\nencapsulated HTML elements. The Instant Expert web component consists of the\nuser input (i.e. text, voice, multi-selection), question processing, and user\ninterface modules. Two use cases are developed to demonstrate the component's\nfeatures, benefits, and usage. The goal of this project is to pave the way for\nnext-generation information systems by mitigating the challenges of developing\nvoice-enabled and domain-informed smart assistants for communicating knowledge\nin any domain.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 16:21:37 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Sermet", "Yusuf", ""], ["Demir", "Ibrahim", ""]]}, {"id": "1909.02538", "submitter": "Abraham Glasser", "authors": "Raja Kushalnagar, Matthew Seita, Abraham Glasser", "title": "Closed ASL Interpreting for Online Videos", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3058555.3058578", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deaf individuals face great challenges in today's society. It can be very\ndifficult to be able to understand different forms of media without a sense of\nhearing. Many videos and movies found online today are not captioned, and even\nfewer have a supporting video with an interpreter. Also, even with a supporting\ninterpreter video provided, information is still lost due to the inability to\nlook at both the video and the interpreter simultaneously. To alleviate this\nissue, we came up with a tool called closed interpreting. Similar to closed\ncaptioning, it will be displayed with an online video and can be toggled on and\noff. However, the closed interpreter is also user-adjustable. Settings, such as\ninterpreter size, transparency, and location, can be adjusted. Our goal with\nthis study is to find out what deaf and hard of hearing viewers like about\nvideos that come with interpreters, and whether the adjustability is\nbeneficial.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:23:14 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Kushalnagar", "Raja", ""], ["Seita", "Matthew", ""], ["Glasser", "Abraham", ""]]}, {"id": "1909.02638", "submitter": "Christine Utz", "authors": "Christine Utz and Martin Degeling and Sascha Fahl and Florian Schaub\n  and Thorsten Holz", "title": "(Un)informed Consent: Studying GDPR Consent Notices in the Field", "comments": "18 pages, 6 figures, 2019 ACM SIGSAC Conference on Computer and\n  Communications Security (CCS '19), November 11-15, 2019, London, United\n  Kingdom", "journal-ref": null, "doi": "10.1145/3319535.3354212", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the adoption of the General Data Protection Regulation (GDPR) in May\n2018 more than 60 % of popular websites in Europe display cookie consent\nnotices to their visitors. This has quickly led to users becoming fatigued with\nprivacy notifications and contributed to the rise of both browser extensions\nthat block these banners and demands for a solution that bundles consent across\nmultiple websites or in the browser.\n  In this work, we identify common properties of the graphical user interface\nof consent notices and conduct three experiments with more than 80,000 unique\nusers on a German website to investigate the influence of notice position, type\nof choice, and content framing on consent. We find that users are more likely\nto interact with a notice shown in the lower (left) part of the screen. Given a\nbinary choice, more users are willing to accept tracking compared to mechanisms\nthat require them to allow cookie use for each category or company\nindividually. We also show that the wide-spread practice of nudging has a large\neffect on the choices users make. Our experiments show that seemingly small\nimplementation decisions can substantially impact whether and how people\ninteract with consent notices. Our findings demonstrate the importance for\nregulation to not just require consent, but also provide clear requirements or\nguidance for how this consent has to be obtained in order to ensure that users\ncan make free and informed choices.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 21:17:55 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 14:34:16 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Utz", "Christine", ""], ["Degeling", "Martin", ""], ["Fahl", "Sascha", ""], ["Schaub", "Florian", ""], ["Holz", "Thorsten", ""]]}, {"id": "1909.02674", "submitter": "Lindah Kotut", "authors": "Lindah Kotut, Timothy L. Stelter, Michael Horning, D. Scott McCrickard", "title": "Willing Buyer, Willing Seller: Personal Data Trade as a Service", "comments": "10 pages, 6 figures. A design fiction paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increased sensitivity by people about how companies collect\ninformation about them, and how this information is packaged, used and sold.\nThis perceived lack of control is highlighted by the helplessness of users of\nvarious platforms in managing or halting what data is collected from/about\nthem. In a future where users have wrested control of their data and have the\nautonomy to decide what information is collected, how it is used and most\nimportantly, how much it is worth, a new market emerges. This design fiction\nconsiders possible steps prescient companies would take to meet these demands,\nsuch as providing third-party subscription platforms offering personal data\ntrade as a service. These services would provide a means for transparent\ntransactions that preserve an owner's control over their data; allowing them to\nindividually make decisions about what data they avail for sale, and the amount\nof compensation they would accept in trade.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 23:50:23 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kotut", "Lindah", ""], ["Stelter", "Timothy L.", ""], ["Horning", "Michael", ""], ["McCrickard", "D. Scott", ""]]}, {"id": "1909.02764", "submitter": "Roman Klinger", "authors": "Deniz Cevher and Sebastian Zepf and Roman Klinger", "title": "Towards Multimodal Emotion Recognition in German Speech Events in Cars\n  using Transfer Learning", "comments": "12 pages, 2 figures, accepted at KONVENS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recognition of emotions by humans is a complex process which considers\nmultiple interacting signals such as facial expressions and both prosody and\nsemantic content of utterances. Commonly, research on automatic recognition of\nemotions is, with few exceptions, limited to one modality. We describe an\nin-car experiment for emotion recognition from speech interactions for three\nmodalities: the audio signal of a spoken interaction, the visual signal of the\ndriver's face, and the manually transcribed content of utterances of the\ndriver. We use off-the-shelf tools for emotion detection in audio and face and\ncompare that to a neural transfer learning approach for emotion recognition\nfrom text which utilizes existing resources from other domains. We see that\ntransfer learning enables models based on out-of-domain corpora to perform\nwell. This method contributes up to 10 percentage points in F1, with up to 76\nmicro-average F1 across the emotions joy, annoyance and insecurity. Our\nfindings also indicate that off-the-shelf-tools analyzing face and audio are\nnot ready yet for emotion detection in in-car speech interactions without\nfurther adjustments.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 08:33:00 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 11:03:10 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Cevher", "Deniz", ""], ["Zepf", "Sebastian", ""], ["Klinger", "Roman", ""]]}, {"id": "1909.02780", "submitter": "Jorge Ramirez", "authors": "Jorge Ram\\'irez, Marcos Baez, Fabio Casati, Boualem Benatallah", "title": "Understanding the Impact of Text Highlighting in Crowdsourcing Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text classification is one of the most common goals of machine learning (ML)\nprojects, and also one of the most frequent human intelligence tasks in\ncrowdsourcing platforms. ML has mixed success in such tasks depending on the\nnature of the problem, while crowd-based classification has proven to be\nsurprisingly effective, but can be expensive. Recently, hybrid text\nclassification algorithms, combining human computation and machine learning,\nhave been proposed to improve accuracy and reduce costs. One way to do so is to\nhave ML highlight or emphasize portions of text that it believes to be more\nrelevant to the decision. Humans can then rely only on this text or read the\nentire text if the highlighted information is insufficient. In this paper, we\ninvestigate if and under what conditions highlighting selected parts of the\ntext can (or cannot) improve classification cost and/or accuracy, and in\ngeneral how it affects the process and outcome of the human intelligence tasks.\nWe study this through a series of crowdsourcing experiments running over\ndifferent datasets and with task designs imposing different cognitive demands.\nOur findings suggest that highlighting is effective in reducing classification\neffort but does not improve accuracy - and in fact, low-quality highlighting\ncan decrease it.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 09:14:26 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Ram\u00edrez", "Jorge", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1909.02800", "submitter": "Jorge Ramirez", "authors": "Jorge Ram\\'irez, Simone Degiacomi, Davide Zanella, Marcos Baez, Fabio\n  Casati, Boualem Benatallah", "title": "CrowdHub: Extending crowdsourcing platforms for the controlled\n  evaluation of tasks designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CrowdHub, a tool for running systematic evaluations of task\ndesigns on top of crowdsourcing platforms. The goal is to support the\nevaluation process, avoiding potential experimental biases that, according to\nour empirical studies, can amount to 38% loss in the utility of the collected\ndataset in uncontrolled settings. Using CrowdHub, researchers can map their\nexperimental design and automate the complex process of managing task execution\nover time while controlling for returning workers and crowd demographics, thus\nreducing bias, increasing utility of collected data, and making more efficient\nuse of a limited pool of subjects.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 10:07:20 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 10:40:14 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Ram\u00edrez", "Jorge", ""], ["Degiacomi", "Simone", ""], ["Zanella", "Davide", ""], ["Baez", "Marcos", ""], ["Casati", "Fabio", ""], ["Benatallah", "Boualem", ""]]}, {"id": "1909.02853", "submitter": "Abraham Glasser", "authors": "Abraham Glasser", "title": "Automatic Speech Recognition Services: Deaf and Hard-of-Hearing\n  Usability", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3290607.3308461", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, speech is becoming a more common, if not standard, interface to\ntechnology. This can be seen in the trend of technology changes over the years.\nIncreasingly, voice is used to control programs, appliances and personal\ndevices within homes, cars, workplaces, and public spaces through smartphones\nand home assistant devices using Amazon's Alexa, Google's Assistant and Apple's\nSiri, and other proliferating technologies. However, most speech interfaces are\nnot accessible for Deaf and Hard-of-Hearing (DHH) people. In this paper,\nperformances of current Automatic Speech Recognition (ASR) with voices of DHH\nspeakers are evaluated. ASR has improved over the years, and is able to reach\nWord Error Rates (WER) as low as 5-6% [1][2][3], with the help of\ncloud-computing and machine learning algorithms that take in custom vocabulary\nmodels. In this paper, a custom vocabulary model is used, and the significance\nof the improvement is evaluated when using DHH speech.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:52:21 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Glasser", "Abraham", ""]]}, {"id": "1909.02924", "submitter": "Carlos Valderrama", "authors": "C. Yvanoff-Frenchin, V. Ramos, T. Belabed, C. Valderrama", "title": "An Edge Computing Robot Experience for Automatic Elderly Mental Health\n  Care Based on Voice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We need open platforms driven by specialists, in which queries can be created\nand collected for long periods and the diagnosis made, based on a rigorous\nclinical follow-up. In this work, we developed a multi-language robot interface\nhelping to evaluate the mental health of seniors by interacting through\nquestions. The specialist can propose questions, as well as to receive users'\nanswers, in text form. The robot can automatically interact with the user using\nthe appropriate language. It can process the answers and under the guidance of\na specialist, questions and answers can be oriented towards the desired therapy\ndirection. The prototype, was implemented on an embedded device meant for edge\ncomputing, thus it is able to filter environmental noise and can be placed\nanywhere at home. The experience is now available for specialists to create\nqueries and answers through a Web-based interface.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 14:16:00 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Yvanoff-Frenchin", "C.", ""], ["Ramos", "V.", ""], ["Belabed", "T.", ""], ["Valderrama", "C.", ""]]}, {"id": "1909.02982", "submitter": "Theo Jaunet", "authors": "Theo Jaunet, Romain Vuillemot and Christian Wolf", "title": "DRLViz: Understanding Decisions and Memory in Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present DRLViz, a visual analytics interface to interpret the internal\nmemory of an agent (e.g. a robot) trained using deep reinforcement learning.\nThis memory is composed of large temporal vectors updated when the agent moves\nin an environment and is not trivial to understand due to the number of\ndimensions, dependencies to past vectors, spatial/temporal correlations, and\nco-correlation between dimensions. It is often referred to as a black box as\nonly inputs (images) and outputs (actions) are intelligible for humans. Using\nDRLViz, experts are assisted to interpret decisions using memory reduction\ninteractions, and to investigate the role of parts of the memory when errors\nhave been made (e.g. wrong direction). We report on DRLViz applied in the\ncontext of video games simulators (ViZDoom) for a navigation scenario with item\ngathering tasks. We also report on experts evaluation using DRLViz, and\napplicability of DRLViz to other scenarios and navigation problems beyond\nsimulation games, as well as its contribution to black box models\ninterpretability and explainability in the field of visual analytics.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 15:56:39 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 16:07:35 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Jaunet", "Theo", ""], ["Vuillemot", "Romain", ""], ["Wolf", "Christian", ""]]}, {"id": "1909.02988", "submitter": "Jens Grubert", "authors": "Anna Eiberger and Per Ola Kristensson and Susanne Mayr and Matthias\n  Kranz and Jens Grubert", "title": "Effects of Depth Layer Switching between an Optical See-Through\n  Head-Mounted Display and a Body-Proximate Display", "comments": null, "journal-ref": "Symposium on Spatial User Interaction (SUI '19), October 19--20,\n  2019, New Orleans, LA, USA", "doi": "10.1145/3357251.3357588", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optical see-through head-mounted displays (OST HMDs) typically display\nvirtual content at a fixed focal distance while users need to integrate this\ninformation with real-world information at different depth layers. This problem\nis pronounced in body-proximate multi-display systems, such as when an OST HMD\nis combined with a smartphone or smartwatch. While such joint systems open up a\nnew design space, they also reduce users' ability to integrate visual\ninformation. We quantify this cost by presenting the results of an experiment\n(n=24) that evaluates human performance in a visual search task across an OST\nHMD and a body-proximate display at 30 cm. The results reveal that task\ncompletion time increases significantly by approximately 50 % and the error\nrate increases significantly by approximately 100 % compared to visual search\non a single depth layer. These results highlight a design trade-off when\ndesigning joint OST HMD-body proximate display systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:04:52 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Eiberger", "Anna", ""], ["Kristensson", "Per Ola", ""], ["Mayr", "Susanne", ""], ["Kranz", "Matthias", ""], ["Grubert", "Jens", ""]]}, {"id": "1909.02993", "submitter": "Luis J. Manso", "authors": "Luis J. Manso, Pedro Nunez, Luis V. Calderita, Diego R. Faria and\n  Pilar Bachiller", "title": "SocNav1: A Dataset to Benchmark and Learn Social Navigation Conventions", "comments": null, "journal-ref": "Data, Vol. 5, Num. 1, pp. 1-10, MDPI (2020)", "doi": "10.3390/data5010007", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adapting to social conventions is an unavoidable requirement for the\nacceptance of assistive and social robots. While the scientific community\nbroadly accepts that assistive robots and social robot companions are unlikely\nto have widespread use in the near future, their presence in health-care and\nother medium-sized institutions is becoming a reality. These robots will have a\nbeneficial impact in industry and other fields such as health care. The growing\nnumber of research contributions to social navigation is also indicative of the\nimportance of the topic. To foster the future prevalence of these robots, they\nmust be useful, but also socially accepted. The first step to be able to\nactively ask for collaboration or permission is to estimate whether the robot\nwould make people feel uncomfortable otherwise, and that is precisely the goal\nof algorithms evaluating social navigation compliance. Some approaches provide\nanalytic models, whereas others use machine learning techniques such as neural\nnetworks. This data report presents and describes SocNav1, a dataset for social\nnavigation conventions. The aims of SocNav1 are two-fold: a) enabling\ncomparison of the algorithms that robots use to assess the convenience of their\npresence in a particular position when navigating; b) providing a sufficient\namount of data so that modern machine learning algorithms such as deep neural\nnetworks can be used. Because of the structured nature of the data, SocNav1 is\nparticularly well-suited to be used to benchmark non-Euclidean machine learning\nalgorithms such as Graph Neural Networks (see [1]). The dataset has been made\navailable in a public repository.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:19:15 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2020 13:41:59 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Manso", "Luis J.", ""], ["Nunez", "Pedro", ""], ["Calderita", "Luis V.", ""], ["Faria", "Diego R.", ""], ["Bachiller", "Pilar", ""]]}, {"id": "1909.03012", "submitter": "Amit Dhurandhar", "authors": "Vijay Arya, Rachel K. E. Bellamy, Pin-Yu Chen, Amit Dhurandhar,\n  Michael Hind, Samuel C. Hoffman, Stephanie Houde, Q. Vera Liao, Ronny Luss,\n  Aleksandra Mojsilovi\\'c, Sami Mourad, Pablo Pedemonte, Ramya Raghavendra,\n  John Richards, Prasanna Sattigeri, Karthikeyan Shanmugam, Moninder Singh,\n  Kush R. Varshney, Dennis Wei and Yunfeng Zhang", "title": "One Explanation Does Not Fit All: A Toolkit and Taxonomy of AI\n  Explainability Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As artificial intelligence and machine learning algorithms make further\ninroads into society, calls are increasing from multiple stakeholders for these\nalgorithms to explain their outputs. At the same time, these stakeholders,\nwhether they be affected citizens, government regulators, domain experts, or\nsystem developers, present different requirements for explanations. Toward\naddressing these needs, we introduce AI Explainability 360\n(http://aix360.mybluemix.net/), an open-source software toolkit featuring eight\ndiverse and state-of-the-art explainability methods and two evaluation metrics.\nEqually important, we provide a taxonomy to help entities requiring\nexplanations to navigate the space of explanation methods, not only those in\nthe toolkit but also in the broader literature on explainability. For data\nscientists and other users of the toolkit, we have implemented an extensible\nsoftware architecture that organizes methods according to their place in the AI\nmodeling pipeline. We also discuss enhancements to bring research innovations\ncloser to consumers of explanations, ranging from simplified, more accessible\nversions of algorithms, to tutorials and an interactive web demo to introduce\nAI explainability to different audiences and application domains. Together, our\ntoolkit and taxonomy can help identify gaps where more explainability methods\nare needed and provide a platform to incorporate them as they are developed.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:53:01 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 15:08:57 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Arya", "Vijay", ""], ["Bellamy", "Rachel K. E.", ""], ["Chen", "Pin-Yu", ""], ["Dhurandhar", "Amit", ""], ["Hind", "Michael", ""], ["Hoffman", "Samuel C.", ""], ["Houde", "Stephanie", ""], ["Liao", "Q. Vera", ""], ["Luss", "Ronny", ""], ["Mojsilovi\u0107", "Aleksandra", ""], ["Mourad", "Sami", ""], ["Pedemonte", "Pablo", ""], ["Raghavendra", "Ramya", ""], ["Richards", "John", ""], ["Sattigeri", "Prasanna", ""], ["Shanmugam", "Karthikeyan", ""], ["Singh", "Moninder", ""], ["Varshney", "Kush R.", ""], ["Wei", "Dennis", ""], ["Zhang", "Yunfeng", ""]]}, {"id": "1909.03054", "submitter": "Giuseppe Vizzari", "authors": "Luca Crociani, Giuseppe Vizzari, and Stefania Bandini", "title": "Calibrating Wayfinding Decisions in Pedestrian Simulation Models: The\n  Entropy Map", "comments": "pre-print of paper presented at the The 16th International Conference\n  on Modeling Decisions for Artificial Intelligence, Milan, Italy September 4 -\n  6, 2019. arXiv admin note: substantial text overlap with arXiv:1610.07901", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents entropy maps, an approach to describing and visualising\nuncertainty among alternative potential movement intentions in pedestrian\nsimulation models. In particular, entropy maps show the instantaneous level of\nrandomness in decisions of a pedestrian agent situated in a specific point of\nthe simulated environment with an heatmap approach. Experimental results\nhighlighting the relevance of this tool supporting modelers are provided and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 11:10:34 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Crociani", "Luca", ""], ["Vizzari", "Giuseppe", ""], ["Bandini", "Stefania", ""]]}, {"id": "1909.03372", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Clement Zheng, Yasuaki Kakehi, Tom Yeh, Ellen Yi-Luen Do,\n  Mark D. Gross, Daniel Leithinger", "title": "ShapeBots: Shape-changing Swarm Robots", "comments": "UIST 2019", "journal-ref": null, "doi": "10.1145/3332165.3347911", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce shape-changing swarm robots. A swarm of self-transformable\nrobots can both individually and collectively change their configuration to\ndisplay information, actuate objects, act as tangible controllers, visualize\ndata, and provide physical affordances. ShapeBots is a concept prototype of\nshape-changing swarm robots. Each robot can change its shape by leveraging\nsmall linear actuators that are thin (2.5 cm) and highly extendable (up to\n20cm) in both horizontal and vertical directions. The modular design of each\nactuator enables various shapes and geometries of self-transformation. We\nillustrate potential application scenarios and discuss how this type of\ninterface opens up possibilities for the future of ubiquitous and distributed\nshape-changing interfaces.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 02:34:59 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Suzuki", "Ryo", ""], ["Zheng", "Clement", ""], ["Kakehi", "Yasuaki", ""], ["Yeh", "Tom", ""], ["Do", "Ellen Yi-Luen", ""], ["Gross", "Mark D.", ""], ["Leithinger", "Daniel", ""]]}, {"id": "1909.03409", "submitter": "Bin Guo", "authors": "Bin Guo, Hao Wang, Yasan Ding, Wei Wu, Shaoyang Hao, Yueqi Sun, Zhiwen\n  Yu", "title": "Conditional Text Generation for Harmonious Human-Machine Interaction", "comments": "Accepted by the ACM Transactions on Intelligent Systems and\n  Technology (TIST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the development of deep learning, text generation\ntechnology has undergone great changes and provided many kinds of services for\nhuman beings, such as restaurant reservation and daily communication. The\nautomatically generated text is becoming more and more fluent so researchers\nbegin to consider more anthropomorphic text generation technology, that is the\nconditional text generation, including emotional text generation, personalized\ntext generation, and so on. Conditional Text Generation (CTG) has thus become a\nresearch hotspot. As a promising research field, we find that many efforts have\nbeen paid to exploring it. Therefore, we aim to give a comprehensive review of\nthe new research trends of CTG. We first summary several key techniques and\nillustrate the technical evolution route in the field of neural text\ngeneration, based on the concept model of CTG. We further make an investigation\nof existing CTG fields and propose several general learning models for CTG.\nFinally, we discuss the open issues and promising research directions of CTG.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 09:31:20 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 15:38:26 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Guo", "Bin", ""], ["Wang", "Hao", ""], ["Ding", "Yasan", ""], ["Wu", "Wei", ""], ["Hao", "Shaoyang", ""], ["Sun", "Yueqi", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1909.03466", "submitter": "Muhammad Usman Khalid", "authors": "Muhammad Usman Khalid and Jie Yu", "title": "Multi-Modal Three-Stream Network for Action Recognition", "comments": "Presented in IEEE ICPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition in video is an active yet challenging research topic\ndue to high variation and complexity of data. In this paper, a novel video\nbased action recognition framework utilizing complementary cues is proposed to\nhandle this complex problem. Inspired by the successful two stream networks for\naction classification, additional pose features are studied and fused to\nenhance understanding of human action in a more abstract and semantic way.\nTowards practices, not only ground truth poses but also noisy estimated poses\nare incorporated in the framework with our proposed pre-processing module. The\nwhole framework and each cue are evaluated on varied benchmarking datasets as\nJHMDB, sub-JHMDB and Penn Action. Our results outperform state-of-the-art\nperformance on these datasets and show the strength of complementary cues.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:40:16 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Khalid", "Muhammad Usman", ""], ["Yu", "Jie", ""]]}, {"id": "1909.03486", "submitter": "Dakuo Wang", "authors": "Yaoli Mao, Dakuo Wang, Michael Muller, Kush R. Varshney, Ioana\n  Baldini, Casey Dugan, AleksandraMojsilovi\\'c", "title": "How Data Scientists Work Together With Domain Experts in Scientific\n  Collaborations: To Find The Right Answer Or To Ask The Right Question?", "comments": null, "journal-ref": null, "doi": "10.1145/3361118", "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been an increasing trend in which data scientists\nand domain experts work together to tackle complex scientific questions.\nHowever, such collaborations often face challenges. In this paper, we aim to\ndecipher this collaboration complexity through a semi-structured interview\nstudy with 22 interviewees from teams of bio-medical scientists collaborating\nwith data scientists. In the analysis, we adopt the Olsons' four-dimensions\nframework proposed in Distance Matters to code interview transcripts. Our\nfindings suggest that besides the glitches in the collaboration readiness,\ntechnology readiness, and coupling of work dimensions, the tensions that exist\nin the common ground building process influence the collaboration outcomes, and\nthen persist in the actual collaboration process. In contrast to prior works'\ngeneral account of building a high level of common ground, the breakdowns of\ncontent common ground together with the strengthen of process common ground in\nthis process is more beneficial for scientific discovery. We discuss why that\nis and what the design suggestions are, and conclude the paper with future\ndirections and limitations.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 15:48:35 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Mao", "Yaoli", ""], ["Wang", "Dakuo", ""], ["Muller", "Michael", ""], ["Varshney", "Kush R.", ""], ["Baldini", "Ioana", ""], ["Dugan", "Casey", ""], ["AleksandraMojsilovi\u0107", "", ""]]}, {"id": "1909.03491", "submitter": "Evgeny Tsykunov", "authors": "Evgeny Tsykunov, Luiza Labazanova, Akerke Tleugazy, Dzmitry\n  Tsetserukou", "title": "SwarmTouch: Tactile Interaction of Human with Impedance Controlled Swarm\n  of Nano-Quadrotors", "comments": "\\c{opyright} 2018 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works. arXiv admin note: substantial text overlap with\n  arXiv:1909.02298", "journal-ref": "2018 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "doi": "10.1109/IROS.2018.8594424", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel interaction strategy for a human-swarm communication when\na human operator guides a formation of quadrotors with impedance control and\nreceives vibrotactile feedback. The presented approach takes into account the\nhuman hand velocity and changes the formation shape and dynamics accordingly\nusing impedance interlinks simulated between quadrotors, which helps to achieve\na life-like swarm behavior. Experimental results with Crazyflie 2.0 quadrotor\nplatform validate the proposed control algorithm. The tactile patterns\nrepresenting dynamics of the swarm (extension or contraction) are proposed. The\nuser feels the state of the swarm at his fingertips and receives valuable\ninformation to improve the controllability of the complex life-like formation.\nThe user study revealed the patterns with high recognition rates. Subjects\nstated that tactile sensation improves the ability to guide the drone formation\nand makes the human-swarm communication much more interactive. The proposed\ntechnology can potentially have a strong impact on the human-swarm interaction,\nproviding a new level of intuitiveness and immersion into the swarm navigation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 09:00:01 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Tsykunov", "Evgeny", ""], ["Labazanova", "Luiza", ""], ["Tleugazy", "Akerke", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "1909.03567", "submitter": "Andi Peng", "authors": "Andi Peng, Besmira Nushi, Emre Kiciman, Kori Inkpen, Siddharth Suri,\n  Ece Kamar", "title": "What You See Is What You Get? The Impact of Representation Criteria on\n  Human Bias in Hiring", "comments": "This paper has been accepted for publication at HCOMP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although systematic biases in decision-making are widely documented, the ways\nin which they emerge from different sources is less understood. We present a\ncontrolled experimental platform to study gender bias in hiring by decoupling\nthe effect of world distribution (the gender breakdown of candidates in a\nspecific profession) from bias in human decision-making. We explore the\neffectiveness of \\textit{representation criteria}, fixed proportional display\nof candidates, as an intervention strategy for mitigation of gender bias by\nconducting experiments measuring human decision-makers' rankings for who they\nwould recommend as potential hires. Experiments across professions with varying\ngender proportions show that balancing gender representation in candidate\nslates can correct biases for some professions where the world distribution is\nskewed, although doing so has no impact on other professions where human\npersistent preferences are at play. We show that the gender of the\ndecision-maker, complexity of the decision-making task and over- and\nunder-representation of genders in the candidate slate can all impact the final\ndecision. By decoupling sources of bias, we can better isolate strategies for\nbias mitigation in human-in-the-loop systems.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 23:52:23 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Peng", "Andi", ""], ["Nushi", "Besmira", ""], ["Kiciman", "Emre", ""], ["Inkpen", "Kori", ""], ["Suri", "Siddharth", ""], ["Kamar", "Ece", ""]]}, {"id": "1909.03582", "submitter": "Peng Xu", "authors": "Peng Xu, Chien-Sheng Wu, Andrea Madotto and Pascale Fung", "title": "Clickbait? Sensational Headline Generation with Auto-tuned Reinforcement\n  Learning", "comments": "Accepted by EMNLP2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensational headlines are headlines that capture people's attention and\ngenerate reader interest. Conventional abstractive headline generation methods,\nunlike human writers, do not optimize for maximal reader attention. In this\npaper, we propose a model that generates sensational headlines without labeled\ndata. We first train a sensationalism scorer by classifying online headlines\nwith many comments (\"clickbait\") against a baseline of headlines generated from\na summarization model. The score from the sensationalism scorer is used as the\nreward for a reinforcement learner. However, maximizing the noisy\nsensationalism reward will generate unnatural phrases instead of sensational\nheadlines. To effectively leverage this noisy reward, we propose a novel loss\nfunction, Auto-tuned Reinforcement Learning (ARL), to dynamically balance\nreinforcement learning (RL) with maximum likelihood estimation (MLE). Human\nevaluation shows that 60.8% of samples generated by our model are sensational,\nwhich is significantly better than the Pointer-Gen baseline and other RL\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 01:33:01 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Xu", "Peng", ""], ["Wu", "Chien-Sheng", ""], ["Madotto", "Andrea", ""], ["Fung", "Pascale", ""]]}, {"id": "1909.03596", "submitter": "Irwyn Sadien", "authors": "Irwyn Sadien, Konstantinos Papangelis, Charles Fleming, Hai-Ning Liang", "title": "Lessons Learned from Developing a Microservice Based Mobile\n  Location-Based Crowdsourcing Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in Mobile Location-Based Crowdsourcing is hindered by a marked lack\nof real-world data. The development of a standardized, lightweight, easily\ndeployable, modular, composable, and most of all, scalable experimentation\nframework would go a long way in facilitating such research. Conveniently,\nthese are all salient characteristics of systems developed using a\nmicroservices approach. We propose QRowdsource - a MLBC experimentation\nframework built using a distributed services architecture. In this paper, we\ndiscuss the design and development of QRowdsource, from the decomposition of\nfunctional components to the orchestration of services within the framework. We\nalso take a look at how the advantages and disadvantages of using a\nmicroservices approach translate to our specific use case and deliberate over a\nnumber of lessons learned while developing the experimentation framework.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 02:31:18 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Sadien", "Irwyn", ""], ["Papangelis", "Konstantinos", ""], ["Fleming", "Charles", ""], ["Liang", "Hai-Ning", ""]]}, {"id": "1909.03650", "submitter": "Hideki Kawahara", "authors": "Hideki Kawahara, Ken-Ichi Sakakibara, Eri Haneishi, Kaori Hagiwara", "title": "Real-time and interactive tools for vocal training based on an analytic\n  signal with a cosine series envelope", "comments": "4 pages, 6 figures, APSIPA ASC 2019", "journal-ref": "2019 Asia-Pacific Signal and Information Processing Association\n  Annual Summit and Conference (APSIPA ASC), Lanzhou, China, 2019, pp. 907-910", "doi": "10.1109/APSIPAASC47483.2019.9023094", "report-no": null, "categories": "cs.SD cs.HC eess.AS eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce real-time and interactive tools for assisting vocal training. In\nthis presentation, we demonstrate mainly a tool based on real-time visualizer\nof fundamental frequency candidates to provide information-rich feedback to\nlearners. The visualizer uses an efficient algorithm using analytic signals for\nderiving phase-based attributes. We start using these tools in vocal training\nfor assisting learners to acquire the awareness of appropriate vocalization.\nThe first author made the MATLAB implementation of the tools open-source. The\ncode and associated video materials are accessible in the first author's GitHub\nrepository.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 06:30:16 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Kawahara", "Hideki", ""], ["Sakakibara", "Ken-Ichi", ""], ["Haneishi", "Eri", ""], ["Hagiwara", "Kaori", ""]]}, {"id": "1909.03786", "submitter": "Alfie Abdul-Rahman", "authors": "Alfie Abdul-Rahman, Rita Borgo, Min Chen, Darren J. Edwards, and Brian\n  Fisher", "title": "Juxtaposing Controlled Empirical Studies in Visualization with Topic\n  Developments in Psychology", "comments": "Empirical studies in visualization, 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical studies form an integral part of visualization research. Not only\ncan they facilitate the evaluation of various designs, techniques, systems, and\npractices in visualization, but they can also enable the discovery of the\ncausalities explaining why and how visualization works. This state-of-the-art\nreport focuses on controlled and semi-controlled empirical studies conducted in\nlaboratories and crowd-sourcing environments. In particular, the survey\nprovides a taxonomic analysis of over 129 empirical studies in the\nvisualization literature. It juxtaposes these studies with topic developments\nbetween 1978 and 2017 in psychology, where controlled empirical studies have\nplayed a predominant role in research. To help appreciate this broad context,\nthe paper provides two case studies in detail, where specific\nvisualization-related topics were examined in the discipline of psychology as\nwell as the field of visualization. Following a brief discussion on some latest\ndevelopments in psychology, it outlines challenges and opportunities in making\nnew discoveries about visualization through empirical studies.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 12:50:34 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Abdul-Rahman", "Alfie", ""], ["Borgo", "Rita", ""], ["Chen", "Min", ""], ["Edwards", "Darren J.", ""], ["Fisher", "Brian", ""]]}, {"id": "1909.03847", "submitter": "Mohammed Khwaja", "authors": "Mohammed Khwaja, Miquel Ferrer, Jesus Omana Iglesias, A. Aldo Faisal,\n  Aleksandar Matic", "title": "Aligning Daily Activities with Personality: Towards A Recommender System\n  for Improving Wellbeing", "comments": "Presented at ACM Conference on Recommender Systems (RecSys) 2019,\n  Copenhagen, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems have not been explored to a great extent for improving\nhealth and subjective wellbeing. Recent advances in mobile technologies and\nuser modelling present the opportunity for delivering such systems, however the\nkey issue is understanding the drivers of subjective wellbeing at an individual\nlevel. In this paper we propose a novel approach for deriving personalized\nactivity recommendations to improve subjective wellbeing by maximizing the\ncongruence between activities and personality traits. To evaluate the model, we\nleveraged a rich dataset collected in a smartphone study, which contains three\nweeks of daily activity probes, the Big-Five personality questionnaire and\nsubjective wellbeing surveys. We show that the model correctly infers a range\nof activities that are 'good' or 'bad' (i.e. that are positively or negatively\nrelated to subjective wellbeing) for a given user and that the derived\nrecommendations greatly match outcomes in the real-world.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 13:29:21 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Khwaja", "Mohammed", ""], ["Ferrer", "Miquel", ""], ["Iglesias", "Jesus Omana", ""], ["Faisal", "A. Aldo", ""], ["Matic", "Aleksandar", ""]]}, {"id": "1909.04230", "submitter": "Travis Scheponik", "authors": "Travis Scheponik, Enis Golaszewski, Geoffrey Herman, Spencer\n  Offenberger, Linda Oliva, Peter A. H. Peterson, Alan T. Sherman", "title": "Investigating Crowdsourcing to Generate Distractors for Multiple-Choice\n  Assessments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyze results from a pilot study that explores how\ncrowdsourcing can be used in the process of generating distractors (incorrect\nanswer choices) in multiple-choice concept inventories (conceptual tests of\nunderstanding). To our knowledge, we are the first to propose and study this\napproach. Using Amazon Mechanical Turk, we collected approximately 180\nopen-ended responses to several question stems from the Cybersecurity Concept\nInventory of the Cybersecurity Assessment Tools Project and from the Digital\nLogic Concept Inventory. We generated preliminary distractors by filtering\nresponses, grouping similar responses, selecting the four most frequent groups,\nand refining a representative distractor for each of these groups. We analyzed\nour data in two ways. First, we compared the responses and resulting\ndistractors with those from the aforementioned inventories. Second, we obtained\nfeedback from Amazon Mechanical Turk on the resulting new draft test items\n(including distractors) from additional subjects. Challenges in using\ncrowdsourcing include controlling the selection of subjects and filtering out\nresponses that do not reflect genuine effort. Despite these challenges, our\nresults suggest that crowdsourcing can be a very useful tool in generating\neffective distractors (attractive to subjects who do not understand the\ntargeted concept). Our results also suggest that this method is faster, easier,\nand cheaper than is the traditional method of having one or more experts draft\ndistractors, and building on talk-aloud interviews with subjects to uncover\ntheir misconceptions. Our results are significant because generating effective\ndistractors is one of the most difficult steps in creating multiple-choice\nassessments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 01:37:53 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Scheponik", "Travis", ""], ["Golaszewski", "Enis", ""], ["Herman", "Geoffrey", ""], ["Offenberger", "Spencer", ""], ["Oliva", "Linda", ""], ["Peterson", "Peter A. H.", ""], ["Sherman", "Alan T.", ""]]}, {"id": "1909.04387", "submitter": "Amanda Cercas Curry", "authors": "Amanda Cercas Curry, Verena Rieser", "title": "A Crowd-based Evaluation of Abuse Response Strategies in Conversational\n  Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should conversational agents respond to verbal abuse through the user? To\nanswer this question, we conduct a large-scale crowd-sourced evaluation of\nabuse response strategies employed by current state-of-the-art systems. Our\nresults show that some strategies, such as \"polite refusal\" score highly across\nthe board, while for other strategies demographic factors, such as age, as well\nas the severity of the preceding abuse influence the user's perception of which\nresponse is appropriate. In addition, we find that most data-driven models lag\nbehind rule-based or commercial systems in terms of their perceived\nappropriateness.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 10:12:59 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Curry", "Amanda Cercas", ""], ["Rieser", "Verena", ""]]}, {"id": "1909.04492", "submitter": "Jurriaan van Diggelen", "authors": "J. van Diggelen, J.S. Barnhoorn, M.M.M. Peeters, W. van Staal, M.L.\n  Stolk, B. van der Vecht, J. van der Waa, J.M. Schraagen", "title": "Pluggable Social Artificial Intelligence for Enabling Human-Agent\n  Teaming", "comments": "presented at NATO HFM symposium on Human Autonomy Teaming,\n  Portsmouth, October 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As intelligent systems are increasingly capable of performing their tasks\nwithout the need for continuous human input, direction, or supervision, new\nhuman-machine interaction concepts are needed. A promising approach to this end\nis human-agent teaming, which envisions a novel interaction form where humans\nand machines behave as equal team partners. This paper presents an overview of\nthe current state of the art in human-agent teaming, including the analysis of\nhuman-agent teams on five dimensions; a framework describing important teaming\nfunctionalities; a technical architecture, called SAIL, supporting social\nhuman-agent teaming through the modular implementation of the human-agent\nteaming functionalities; a technical implementation of the architecture; and a\nproof-of-concept prototype created with the framework and architecture. We\nconclude this paper with a reflection on where we stand and a glance into the\nfuture showing the way forward.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 14:03:41 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 09:21:17 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["van Diggelen", "J.", ""], ["Barnhoorn", "J. S.", ""], ["Peeters", "M. M. M.", ""], ["van Staal", "W.", ""], ["Stolk", "M. L.", ""], ["van der Vecht", "B.", ""], ["van der Waa", "J.", ""], ["Schraagen", "J. M.", ""]]}, {"id": "1909.04747", "submitter": "Madeleine Bartlett Miss", "authors": "Madeleine Bartlett, Daniel Hernandez Garcia, Serge Thill and Tony\n  Belpaeme", "title": "Recognizing Human Internal States: A Conceptor-Based Approach", "comments": "4 pages, 1 figure, HRI conference workshop", "journal-ref": null, "doi": null, "report-no": "SREC/2019/04", "categories": "cs.HC cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The past few decades has seen increased interest in the application of social\nrobots to interventions for Autism Spectrum Disorder as behavioural coaches\n[4]. We consider that robots embedded in therapies could also provide\nquantitative diagnostic information by observing patient behaviours. The social\nnature of ASD symptoms means that, to achieve this, robots need to be able to\nrecognize the internal states their human interaction partners are\nexperiencing, e.g. states of confusion, engagement etc. Approaching this\nproblem can be broken down into two questions: (1) what information, accessible\nto robots, can be used to recognize internal states, and (2) how can a system\nclassify internal states such that it allows for sufficiently detailed\ndiagnostic information? In this paper we discuss these two questions in depth\nand propose a novel, conceptor-based classifier. We report the initial results\nof this system in a proof-of-concept study and outline plans for future work.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 13:10:52 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Bartlett", "Madeleine", ""], ["Garcia", "Daniel Hernandez", ""], ["Thill", "Serge", ""], ["Belpaeme", "Tony", ""]]}, {"id": "1909.04749", "submitter": "Meng Xia", "authors": "Meng Xia, Huan Wei, Min Xu, Leo Yu Ho Lo, Yong Wang, Rong Zhang,\n  Huamin Qu", "title": "Visual Analytics of Student Learning Behaviors on K-12 Mathematics\n  E-learning Platforms", "comments": "2 pages, 6 figures, 2019 VAST conference, Best Poster, Learning\n  Analytics, Visual Analytic System for Education, Online Learning, Learning\n  Data Analysis, Learning Trajectories Analysis, Mouse Movement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing popularity in online learning, a surge of E-learning\nplatforms have emerged to facilitate education opportunities for k-12 (from\nkindergarten to 12th grade) students and with this, a wealth of information on\ntheir learning logs are getting recorded. However, it remains unclear how to\nmake use of these detailed learning behavior data to improve the design of\nlearning materials and gain deeper insight into students' thinking and learning\nstyles. In this work, we propose a visual analytics system to analyze student\nlearning behaviors on a K-12 mathematics E-learning platform. It supports both\ncorrelation analysis between different attributes and a detailed visualization\nof user mouse-movement logs. Our case studies on a real dataset show that our\nsystem can better guide the design of learning resources (e.g., math questions)\nand facilitate quick interpretation of students' problem-solving and learning\nstyles.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 06:34:57 GMT"}, {"version": "v2", "created": "Sat, 21 Sep 2019 11:47:32 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Xia", "Meng", ""], ["Wei", "Huan", ""], ["Xu", "Min", ""], ["Lo", "Leo Yu Ho", ""], ["Wang", "Yong", ""], ["Zhang", "Rong", ""], ["Qu", "Huamin", ""]]}, {"id": "1909.04773", "submitter": "Rishita Kalyani", "authors": "Rishita Kalyani", "title": "Understanding user search processes across varying cognitive levels", "comments": "Master Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web is often used for finding information and with a learning intention. In\nthis thesis, we propose a study to investigate the process of learning online\nacross varying cognitive learning levels using crowd-sourced participants. Our\naim was to study the impact of cognitive learning levels on search as well as\nincrease in knowledge. We present 150 participants with 6 search tasks for\nvarying cognitive levels and collect user interactions and submitted answers as\nuser data. We present quantitative analysis of user data which shows that the\noutcome for all cognitive levels is learning by quantifying it as calculated\nknowledge gain. Further, we also investigate the impact of cognitive learning\nlevel on user interaction and knowledge gain with the help of user data. We\ndemonstrate that the cognitive learning level of search session has a\nsignificant impact on user's search behavior as well as on knowledge that is\ngained. Further, we establish a pattern in which the search behavior changes\nacross cognitive learning levels where the least complex search task has\nminimum number of user interactions and most complex search task has maximum\nuser interactions. With this observation, we were able to demonstrate a\nrelation between a learner's search behavior and Krathwohl's revised Bloom's\ntaxonomic structure of cognitive processes. The findings of this thesis intend\nto provide a significant work to bridge the relation between search, learning,\nand user.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 21:56:34 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Kalyani", "Rishita", ""]]}, {"id": "1909.04835", "submitter": "Bei Wang", "authors": "Ingrid Hotz, Roxana Bujack, Christoph Garth, Bei Wang", "title": "Mathematical Foundations in Visualization", "comments": "This is a preprint of a chapter for a planned book that was initiated\n  by participants of the Dagstuhl Seminar 18041 (\"Foundations of Data\n  Visualization\"). The book is expected to be published by Springer. The final\n  book chapter will differ from this preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical concepts and tools have shaped the field of visualization in\nfundamental ways and played a key role in the development of a large variety of\nvisualization techniques. In this chapter, we sample the visualization\nliterature to provide a taxonomy of the usage of mathematics in visualization,\nand to identify a fundamental set of mathematics that should be taught to\nstudents as part of an introduction to contemporary visualization research.\nWithin the scope of this chapter, we are unable to provide a full review of all\nmathematical foundations of visualization; rather, we identify a number of\nconcepts that are useful in visualization, explain their significance, and\nprovide references for further reading.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 03:10:40 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Hotz", "Ingrid", ""], ["Bujack", "Roxana", ""], ["Garth", "Christoph", ""], ["Wang", "Bei", ""]]}, {"id": "1909.04847", "submitter": "Eugene Ie", "authors": "Eugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar,\n  Jing Wang, Rui Wu, Craig Boutilier", "title": "RecSim: A Configurable Simulation Platform for Recommender Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.IR stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose RecSim, a configurable platform for authoring simulation\nenvironments for recommender systems (RSs) that naturally supports sequential\ninteraction with users. RecSim allows the creation of new environments that\nreflect particular aspects of user behavior and item structure at a level of\nabstraction well-suited to pushing the limits of current reinforcement learning\n(RL) and RS techniques in sequential interactive recommendation problems.\nEnvironments can be easily configured that vary assumptions about: user\npreferences and item familiarity; user latent state and its dynamics; and\nchoice models and other user response behavior. We outline how RecSim offers\nvalue to RL and RS researchers and practitioners, and how it can serve as a\nvehicle for academic-industrial collaboration.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 04:43:45 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 13:30:53 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Ie", "Eugene", ""], ["Hsu", "Chih-wei", ""], ["Mladenov", "Martin", ""], ["Jain", "Vihan", ""], ["Narvekar", "Sanmit", ""], ["Wang", "Jing", ""], ["Wu", "Rui", ""], ["Boutilier", "Craig", ""]]}, {"id": "1909.04961", "submitter": "Silvia Rossi", "authors": "Martina Ruocco, Marwa Larafa, Silvia Rossi", "title": "Emotional Distraction for Children Anxiety Reduction During Vaccination", "comments": null, "journal-ref": null, "doi": null, "report-no": "SREC/2019/08", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social assistive robots are starting to be widely used in pediatric\nhealth-care environments with the aim of distracting and entertaining children,\nand so of reducing a possible state of anxiety. In this paper, we present some\ninitial results of a study (N=69) conducted in a Health-Vaccines Center, where\nthe distraction role of a social robot, which interacts with a child showing an\nemotional behavior, is compared with the same not showing any emotional social\ncue. Outcome criteria for the evaluation of the intervention included the\nparents reported level of anxiety before, during and after the procedure.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 10:29:58 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 13:42:22 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Ruocco", "Martina", ""], ["Larafa", "Marwa", ""], ["Rossi", "Silvia", ""]]}, {"id": "1909.05152", "submitter": "Alireza Rahimpour", "authors": "Alireza Rahimpour, Sujitha Martin, Ashish Tawari, Hairong Qi", "title": "Context Aware Road-user Importance Estimation (iCARE)", "comments": "Published in: IEEE Intelligent Vehicles (IV), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road-users are a critical part of decision-making for both self-driving cars\nand driver assistance systems. Some road-users, however, are more important for\ndecision-making than others because of their respective intentions, ego\nvehicle's intention and their effects on each other. In this paper, we propose\na novel architecture for road-user importance estimation which takes advantage\nof the local and global context of the scene. For local context, the model\nexploits the appearance of the road users (which captures orientation,\nintention, etc.) and their location relative to ego-vehicle. The global context\nin our model is defined based on the feature map of the convolutional layer of\nthe module which predicts the future path of the ego-vehicle and contains rich\nglobal information of the scene (e.g., infrastructure, road lanes, etc.), as\nwell as the ego vehicle's intention information. Moreover, this paper\nintroduces a new data set of real-world driving, concentrated around\ninter-sections and includes annotations of important road users. Systematic\nevaluations of our proposed method against several baselines show promising\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 05:54:44 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Rahimpour", "Alireza", ""], ["Martin", "Sujitha", ""], ["Tawari", "Ashish", ""], ["Qi", "Hairong", ""]]}, {"id": "1909.05160", "submitter": "Muneeb Imtiaz Ahmad", "authors": "Muneeb Imtiaz Ahmad, Jasmin Bernotat, Katrin Lohan, Friederike Eyssel", "title": "Trust and Cognitive Load During Human-Robot Interaction", "comments": "10 Pages, 5 figures, AAAI Symposium on Artificial Intelligence for\n  Human-Robot Interaction, 7th-9th November, 2019", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/06", "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an exploratory study to understand the relationship\nbetween a humans' cognitive load, trust, and anthropomorphism during\nhuman-robot interaction. To understand the relationship, we created a\n\\say{Matching the Pair} game that participants could play collaboratively with\none of two robot types, Husky or Pepper. The goal was to understand if humans\nwould trust the robot as a teammate while being in the game-playing situation\nthat demanded a high level of cognitive load. Using a humanoid vs. a technical\nrobot, we also investigated the impact of physical anthropomorphism and we\nfurthermore tested the impact of robot error rate on subsequent judgments and\nbehavior. Our results showed that there was an inversely proportional\nrelationship between trust and cognitive load, suggesting that as the amount of\ncognitive load increased in the participants, their ratings of trust decreased.\nWe also found a triple interaction impact between robot-type, error-rate and\nparticipant's ratings of trust. We found that participants perceived Pepper to\nbe more trustworthy in comparison with the Husky robot after playing the game\nwith both robots under high error-rate condition. On the contrary, Husky was\nperceived as more trustworthy than Pepper when it was depicted as featuring a\nlow error-rate. Our results are interesting and call further investigation of\nthe impact of physical anthropomorphism in combination with variable\nerror-rates of the robot.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 15:56:22 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Ahmad", "Muneeb Imtiaz", ""], ["Bernotat", "Jasmin", ""], ["Lohan", "Katrin", ""], ["Eyssel", "Friederike", ""]]}, {"id": "1909.05189", "submitter": "Aaron Halfaker", "authors": "Aaron Halfaker and R. Stuart Geiger", "title": "ORES: Lowering Barriers with Participatory Machine Learning in Wikipedia", "comments": "29 pages + 3 pages appendix. Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Algorithmic systems---from rule-based bots to machine learning\nclassifiers---have a long history of supporting the essential work of content\nmoderation and other curation work in peer production projects. From\ncounter-vandalism to task routing, basic machine prediction has allowed open\nknowledge projects like Wikipedia to scale to the largest encyclopedia in the\nworld, while maintaining quality and consistency. However, conversations about\nhow quality control should work and what role algorithms should play have\ngenerally been led by the expert engineers who have the skills and resources to\ndevelop and modify these complex algorithmic systems. In this paper, we\ndescribe ORES: an algorithmic scoring service that supports real-time scoring\nof wiki edits using multiple independent classifiers trained on different\ndatasets. ORES decouples several activities that have typically all been\nperformed by engineers: choosing or curating training data, building models to\nserve predictions, auditing predictions, and developing interfaces or automated\nagents that act on those predictions. This meta-algorithmic system was designed\nto open up socio-technical conversations about algorithms in Wikipedia to a\nbroader set of participants. In this paper, we discuss the theoretical\nmechanisms of social change ORES enables and detail case studies in\nparticipatory machine learning around ORES from the 5 years since its\ndeployment.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 16:40:03 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 14:19:58 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 14:35:49 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Halfaker", "Aaron", ""], ["Geiger", "R. Stuart", ""]]}, {"id": "1909.05327", "submitter": "Aryabrata Basu", "authors": "Aryabrata Basu", "title": "Tracking the untracked", "comments": "11 pages, preprint to any suitable theoretical CS journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of seamless identification of users previously tracked using\nexisting real-time optical position tracking system such as the OptiTrack\nsystem and maintaining continuous tracking state (history) of each of those\nusers is a hard problem. In this article, we present a theoretical framework to\nintegrate existing tracking systems with features such as user identification\nand history of up to `n' person activity. In our approach, we assume no direct\ncommunication with the tracking system, but access to all data it collects.\nAlso, there are no guarantees that 1) the order of each tracked\nretro-reflective sphere reported is the same, and 2) that there will be any\nparticular number of spheres in the room at any given time. We describe how the\ndata is fused with existing tracking data to provide a seamless transition\nbetween other forms of position tracking.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 19:41:50 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 04:53:11 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Basu", "Aryabrata", ""]]}, {"id": "1909.05329", "submitter": "Yaqi Xie", "authors": "Yaqi Xie, Indu P Bodala, Desmond C. Ong, David Hsu, Harold Soh", "title": "Robot Capability and Intention in Trust-based Decisions across Tasks", "comments": null, "journal-ref": "ACM/IEEE Conference on Human Robot Interaction (HRI), 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present results from a human-subject study designed to\nexplore two facets of human mental models of robots---inferred capability and\nintention---and their relationship to overall trust and eventual decisions. In\nparticular, we examine delegation situations characterized by uncertainty, and\nexplore how inferred capability and intention are applied across different\ntasks. We develop an online survey where human participants decide whether to\ndelegate control to a simulated UAV agent. Our study shows that human\nestimations of robot capability and intent correlate strongly with overall\nself-reported trust. However, overall trust is not independently sufficient to\ndetermine whether a human will decide to trust (delegate) a given task to a\nrobot. Instead, our study reveals that estimations of robot intention,\ncapability, and overall trust are integrated when deciding to delegate. From a\nbroader perspective, these results suggest that calibrating overall trust alone\nis insufficient; to make correct decisions, humans need (and use) multi-faceted\nmental models when collaborating with robots across multiple contexts.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:42:24 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Xie", "Yaqi", ""], ["Bodala", "Indu P", ""], ["Ong", "Desmond C.", ""], ["Hsu", "David", ""], ["Soh", "Harold", ""]]}, {"id": "1909.05644", "submitter": "Richard Tomsett", "authors": "David Mott, Richard Tomsett", "title": "Illuminated Decision Trees with Lucid", "comments": "Presented at BMVC 2019: Workshop on Interpretable and Explainable\n  Machine Vision, Cardiff, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lucid methods described by Olah et al. (2018) provide a way to inspect\nthe inner workings of neural networks trained on image classification tasks\nusing feature visualization. Such methods have generally been applied to\nnetworks trained on visually rich, large-scale image datasets like ImageNet,\nwhich enables them to produce enticing feature visualizations. To investigate\nthese methods further, we applied them to classifiers trained to perform the\nmuch simpler (in terms of dataset size and visual richness), yet challenging\ntask of distinguishing between different kinds of white blood cell from\nmicroscope images. Such a task makes generating useful feature visualizations\ndifficult, as the discriminative features are inherently hard to identify and\ninterpret. We address this by presenting the \"Illuminated Decision Tree\"\napproach, in which we use a neural network trained on the task as a feature\nextractor, then learn a decision tree based on these features, and provide\nLucid visualizations for each node in the tree. We demonstrate our approach\nwith several examples, showing how this approach could be useful both in model\ndevelopment and debugging, and when explaining model outputs to non-experts.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 12:32:44 GMT"}], "update_date": "2019-09-15", "authors_parsed": [["Mott", "David", ""], ["Tomsett", "Richard", ""]]}, {"id": "1909.05655", "submitter": "Henry Griffith", "authors": "Henry K. Griffith, Dmytro Katrychuk, Oleg V. Komogortsev", "title": "Assessment of Shift-Invariant CNN Gaze Mappings for PS-OG Eye Movement\n  Sensors", "comments": "Accepted to be published in the 2019 OpenEDS Workshop: Eye Tracking\n  for VR and AR at the International Conference on Computer Vision (ICCV),\n  October 27- November 3, 2019, Seoul, Korea", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photosensor oculography (PS-OG) eye movement sensors offer desirable\nperformance characteristics for integration within wireless head mounted\ndevices (HMDs), including low power consumption and high sampling rates. To\naddress the known performance degradation of these sensors due to HMD shifts,\nvarious machine learning techniques have been proposed for mapping sensor\noutputs to gaze location. This paper advances the understanding of a recently\nintroduced convolutional neural network designed to provide shift invariant\ngaze mapping within a specified range of sensor translations. Performance is\nassessed for shift training examples which better reflect the distribution of\nvalues that would be generated through manual repositioning of the HMD during a\ndedicated collection of training data. The network is shown to exhibit\ncomparable accuracy for this realistic shift distribution versus a previously\nconsidered rectangular grid, thereby enhancing the feasibility of in-field\nset-up. In addition, this work further demonstrates the practical viability of\nthe proposed initialization process by demonstrating robust mapping performance\nversus training data scale. The ability to maintain reasonable accuracy for\nshifts extending beyond those introduced during training is also demonstrated.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 17:57:45 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Griffith", "Henry K.", ""], ["Katrychuk", "Dmytro", ""], ["Komogortsev", "Oleg V.", ""]]}, {"id": "1909.05667", "submitter": "Liam Hiley BSc", "authors": "Liam Hiley, Alun Preece, Yulia Hicks", "title": "Explainable Deep Learning for Video Recognition Tasks: A Framework &\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of Deep Learning for real-world applications is ever-growing.\nWith the introduction of high performance hardware, applications are no longer\nlimited to image recognition. With the introduction of more complex problems\ncomes more and more complex solutions, and the increasing need for explainable\nAI. Deep Neural Networks for Video tasks are amongst the most complex models,\nwith at least twice the parameters of their Image counterparts. However,\nexplanations for these models are often ill-adapted to the video domain. The\ncurrent work in explainability for video models is still overshadowed by Image\ntechniques, while Video Deep Learning itself is quickly gaining on methods for\nstill images. This paper seeks to highlight the need for explainability methods\ndesigned with video deep learning models, and by association spatio-temporal\ninput in mind, by first illustrating the cutting edge for video deep learning,\nand then noting the scarcity of research into explanations for these methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 19:34:48 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Hiley", "Liam", ""], ["Preece", "Alun", ""], ["Hicks", "Yulia", ""]]}, {"id": "1909.05719", "submitter": "Paul Zikas", "authors": "Paul Zikas, George Papagiannakis, Nick Lydatakis, Steve Kateros,\n  Stavroula Ntoa, Ilia Adami, Constantine Stephanidis", "title": "Scenior: An Immersive Visual Scripting system based on VR Software\n  Design Patterns for Experiential Training", "comments": null, "journal-ref": "The Visual Computer volume 36, pages 1965-1977 (2020)", "doi": "10.1007/s00371-020-01919-0", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) has re-emerged as a low-cost, highly accessible consumer\nproduct, and training on simulators is rapidly becoming standard in many\nindustrial sectors. However, the available systems are either focusing on\ngaming context, featuring limited capabilities or they support only content\ncreation of virtual environments without any rapid prototyping and\nmodification. In this project, we propose a code-free, visual scripting\nplatform to replicate gamified training scenarios through rapid prototyping and\nVR software design patterns. We implemented and compared two authoring tools:\na) visual scripting and b) VR editor for the rapid reconstruction of VR\ntraining scenarios. Our visual scripting module is capable to generate training\napplications utilizing a node-based scripting system whereas the VR editor\ngives user/developer the ability to customize and populate new VR training\nscenarios directly from the virtual environment. We also introduce action\nprototypes, a new software design pattern suitable to replicate behavioral\ntasks for VR experiences. In addition, we present the training scenegraph\narchitecture as the main model to represent training scenarios on a modular,\ndynamic and highly adaptive acyclic graph based on a structured educational\ncurriculum. Finally, a user-based evaluation of the proposed solution indicated\nthat users - regardless of their programming expertise - can effectively use\nthe tools to create and modify training scenarios in VR.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 14:37:13 GMT"}, {"version": "v2", "created": "Sun, 3 May 2020 23:07:27 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Zikas", "Paul", ""], ["Papagiannakis", "George", ""], ["Lydatakis", "Nick", ""], ["Kateros", "Steve", ""], ["Ntoa", "Stavroula", ""], ["Adami", "Ilia", ""], ["Stephanidis", "Constantine", ""]]}, {"id": "1909.05725", "submitter": "Ting-Hao Huang", "authors": "Ting-Hao 'Kenneth' Huang, Amos Azaria, Oscar J. Romero, Jeffrey P.\n  Bigham", "title": "InstructableCrowd: Creating IF-THEN Rules for Smartphones via\n  Conversations with the Crowd", "comments": "Published at Human Computation (2019) 6:1:113-146", "journal-ref": "Human Computation (2019) 6:1:113-146", "doi": "10.15346/hc.v6i1.7", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language interfaces have become a common part of modern digital life.\nChatbots utilize text-based conversations to communicate with users; personal\nassistants on smartphones such as Google Assistant take direct speech commands\nfrom their users; and speech-controlled devices such as Amazon Echo use voice\nas their only input mode. In this paper, we introduce InstructableCrowd, a\ncrowd-powered system that allows users to program their devices via\nconversation. The user verbally expresses a problem to the system, in which a\ngroup of crowd workers collectively respond and program relevant multi-part\nIF-THEN rules to help the user. The IF-THEN rules generated by\nInstructableCrowd connect relevant sensor combinations (e.g., location,\nweather, device acceleration, etc.) to useful effectors (e.g., text messages,\ndevice alarms, etc.). Our study showed that non-programmers can use the\nconversational interface of InstructableCrowd to create IF-THEN rules that have\nsimilar quality compared with the rules created manually. InstructableCrowd\ngenerally illustrates how users may converse with their devices, not only to\ntrigger simple voice commands, but also to personalize their increasingly\npowerful and complicated devices.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 14:43:38 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Huang", "Ting-Hao 'Kenneth'", ""], ["Azaria", "Amos", ""], ["Romero", "Oscar J.", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1909.05833", "submitter": "Jungsu Pak", "authors": "Jungsu Pak, Uri Maoz", "title": "Compact 3 DOF Driving Simulator using Immersive Virtual Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A driving simulator was created using commercially available 3 degree of\nfreedom motion platform (DOFreality H3) and a virtual reality head-mounted\ndisplay (Oculus CV1). Using virtual reality headset as the visual simulation\nsystem with low-cost moving base platform allowed us to create a high-fidelity\ndriving simulator with minimal cost and space. A custom motion cueing algorithm\nwas used to minimize visuo-vestibular conflict, and simulator sickness\nquestionnaire (SSQ) was used to measure progression of simulator sickness over\ntime while driving on a highway environment.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:39:50 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Pak", "Jungsu", ""], ["Maoz", "Uri", ""]]}, {"id": "1909.05897", "submitter": "Prajwal Chidananda", "authors": "Prajwal Chidananda, Ayan Sinha, Adithya Rao, Douglas Lee, Andrew\n  Rabinovich (Magic Leap, Inc)", "title": "Efficient 2.5D Hand Pose Estimation via Auxiliary Multi-Task Training\n  for Embedded Devices", "comments": "CVPR Workshop on Computer Vision for Augmented and Virtual Reality,\n  Long Beach, CA, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  2D Key-point estimation is an important precursor to 3D pose estimation\nproblems for human body and hands. In this work, we discuss the data,\narchitecture, and training procedure necessary to deploy extremely efficient\n2.5D hand pose estimation on embedded devices with highly constrained memory\nand compute envelope, such as AR/VR wearables. Our 2.5D hand pose estimation\nconsists of 2D key-point estimation of joint positions on an egocentric image,\ncaptured by a depth sensor, and lifted to 2.5D using the corresponding depth\nvalues. Our contributions are two fold: (a) We discuss data labeling and\naugmentation strategies, the modules in the network architecture that\ncollectively lead to $3\\%$ the flop count and $2\\%$ the number of parameters\nwhen compared to the state of the art MobileNetV2 architecture. (b) We propose\nan auxiliary multi-task training strategy needed to compensate for the small\ncapacity of the network while achieving comparable performance to MobileNetV2.\nOur 32-bit trained model has a memory footprint of less than 300 Kilobytes,\noperates at more than 50 Hz with less than 35 MFLOPs.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:33:05 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Chidananda", "Prajwal", "", "Magic Leap, Inc"], ["Sinha", "Ayan", "", "Magic Leap, Inc"], ["Rao", "Adithya", "", "Magic Leap, Inc"], ["Lee", "Douglas", "", "Magic Leap, Inc"], ["Rabinovich", "Andrew", "", "Magic Leap, Inc"]]}, {"id": "1909.06077", "submitter": "Boris Bogaerts", "authors": "Boris Bogaerts, Seppe Sels, Steve Vanlanduit and Rudi Penne", "title": "Enabling Humans to Plan Inspection Paths Using a Virtual Reality\n  Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate whether humans can manually generate\nhigh-quality robot paths for optical inspections. Typically, automated\nalgorithms are used to solve the inspection planning problem. The use of\nautomated algorithms implies that specialized knowledge from users is needed to\nset up the algorithm. We aim to replace this need for specialized experience,\nby entrusting a non-expert human user with the planning task. We augment this\nuser with intuitive visualizations and interactions in virtual reality. To\ninvestigate if humans can generate high-quality inspection paths, we perform a\nuser study in which users from different experience categories, generate\ninspection paths with the proposed virtual reality interface. From our study,\nit can be concluded that users without experience can generate high-quality\ninspection paths: The median inspection quality of user generated paths ranged\nbetween 66-81\\% of the quality of a state-of-the-art automated algorithm on\nvarious inspection planning scenarios. We noticed however, a sizable variation\nin the performance of users, which is a result of some typical user behaviors.\nThese behaviors are discussed, and possible solutions are provided.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 08:15:36 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Bogaerts", "Boris", ""], ["Sels", "Seppe", ""], ["Vanlanduit", "Steve", ""], ["Penne", "Rudi", ""]]}, {"id": "1909.06174", "submitter": "Christian Dondrup", "authors": "Christian Dondrup, Ioannis Papaioannou, Oliver Lemon", "title": "Petri Net Machines for Human-Agent Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/02", "categories": "cs.AI cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Smart speakers and robots become ever more prevalent in our daily lives.\nThese agents are able to execute a wide range of tasks and actions and,\ntherefore, need systems to control their execution. Current state-of-the-art\nsuch as (deep) reinforcement learning, however, requires vast amounts of data\nfor training which is often hard to come by when interacting with humans. To\novercome this issue, most systems still rely on Finite State Machines. We\nintroduce Petri Net Machines which present a formal definition for state\nmachines based on Petri Nets that are able to execute concurrent actions\nreliably, execute and interleave several plans at the same time, and provide an\neasy to use modelling language. We show their workings based on the example of\nHuman-Robot Interaction in a shopping mall.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 12:31:39 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Dondrup", "Christian", ""], ["Papaioannou", "Ioannis", ""], ["Lemon", "Oliver", ""]]}, {"id": "1909.06331", "submitter": "Emrah Sisbot", "authors": "E. Akin Sisbot, Jonathan H. Connell", "title": "Where is My Stuff? An Interactive System for Spatial Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/13", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a system that detects and tracks objects and agents,\ncomputes spatial relations, and communicates those relations to the user using\nspeech. Our system is able to detect multiple objects and agents at 30 frames\nper second using a RGBD camera. It is able to extract the spatial relations in,\non, next to, near, and belongs to, and communicate these relations using\nnatural language. The notion of belonging is particularly important for\nHuman-Robot Interaction since it allows the robot ground the language and\nreason about the right objects. Although our system is currently static and\ntargeted to a fixed location in a room, we are planning to port it to a mobile\nrobot thus allowing it explore the environment and create a spatial knowledge\nbase.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:18:17 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Sisbot", "E. Akin", ""], ["Connell", "Jonathan H.", ""]]}, {"id": "1909.06342", "submitter": "Umang Bhatt", "authors": "Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly,\n  Yunhan Jia, Joydeep Ghosh, Ruchir Puri, Jos\\'e M. F. Moura, Peter Eckersley", "title": "Explainable Machine Learning in Deployment", "comments": "ACM Conference on Fairness, Accountability, and Transparency 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable machine learning offers the potential to provide stakeholders\nwith insights into model behavior by using various methods such as feature\nimportance scores, counterfactual explanations, or influential training data.\nYet there is little understanding of how organizations use these methods in\npractice. This study explores how organizations view and use explainability for\nstakeholder consumption. We find that, currently, the majority of deployments\nare not for end users affected by the model but rather for machine learning\nengineers, who use explainability to debug the model itself. There is thus a\ngap between explainability in practice and the goal of transparency, since\nexplanations primarily serve internal stakeholders rather than external ones.\nOur study synthesizes the limitations of current explainability techniques that\nhamper their use for end users. To facilitate end user interaction, we develop\na framework for establishing clear goals for explainability. We end by\ndiscussing concerns raised regarding explainability.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 17:35:53 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 18:30:09 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 17:31:01 GMT"}, {"version": "v4", "created": "Fri, 10 Jul 2020 13:53:00 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Bhatt", "Umang", ""], ["Xiang", "Alice", ""], ["Sharma", "Shubham", ""], ["Weller", "Adrian", ""], ["Taly", "Ankur", ""], ["Jia", "Yunhan", ""], ["Ghosh", "Joydeep", ""], ["Puri", "Ruchir", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Eckersley", "Peter", ""]]}, {"id": "1909.06415", "submitter": "Jason Gregory", "authors": "Jason M. Gregory, Christopher Reardon, Kevin Lee, Geoffrey White, Ki\n  Ng, Caitlyn Sims", "title": "Enabling Intuitive Human-Robot Teaming Using Augmented Reality and\n  Gesture Control", "comments": "Proceedings of the Artificial Intelligence for Human-Robot\n  Interaction AAAI Symposium Series (AI-HRI 2019)", "journal-ref": "Proceedings of the Artificial Intelligence for Human-Robot\n  Interaction AAAI Symposium Series (AI-HRI 2019), Arlington, Virginia USA,\n  November 2019", "doi": null, "report-no": "AI-HRI/2019/31", "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot teaming offers great potential because of the opportunities to\ncombine strengths of heterogeneous agents. However, one of the critical\nchallenges in realizing an effective human-robot team is efficient information\nexchange - both from the human to the robot as well as from the robot to the\nhuman. In this work, we present and analyze an augmented reality-enabled,\ngesture-based system that supports intuitive human-robot teaming through\nimproved information exchange. Our proposed system requires no external\ninstrumentation aside from human-wearable devices and shows promise of\nreal-world applicability for service-oriented missions. Additionally, we\npresent preliminary results from a pilot study with human participants, and\nhighlight lessons learned and open research questions that may help direct\nfuture development, fielding, and experimentation of autonomous HRI systems.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 19:18:52 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gregory", "Jason M.", ""], ["Reardon", "Christopher", ""], ["Lee", "Kevin", ""], ["White", "Geoffrey", ""], ["Ng", "Ki", ""], ["Sims", "Caitlyn", ""]]}, {"id": "1909.06508", "submitter": "Connor Brooks", "authors": "Connor Brooks and Daniel Szafir", "title": "Building Second-Order Mental Models for Human-Robot Interaction", "comments": null, "journal-ref": "2019 AAAI Fall Symposium Series", "doi": null, "report-no": "AI-HRI/2019/18", "categories": "cs.RO cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mental models that humans form of other agents---encapsulating human\nbeliefs about agent goals, intentions, capabilities, and more---create an\nunderlying basis for interaction. These mental models have the potential to\naffect both the human's decision making during the interaction and the human's\nsubjective assessment of the interaction. In this paper, we surveyed existing\nmethods for modeling how humans view robots, then identified a potential method\nfor improving these estimates through inferring a human's model of a robot\nagent directly from their actions. Then, we conducted an online study to\ncollect data in a grid-world environment involving humans moving an avatar past\na virtual agent. Through our analysis, we demonstrated that participants'\naction choices leaked information about their mental models of a virtual agent.\nWe conclude by discussing the implications of these findings and the potential\nfor such a method to improve human-robot interactions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 02:08:06 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Brooks", "Connor", ""], ["Szafir", "Daniel", ""]]}, {"id": "1909.06510", "submitter": "Jason Wilson", "authors": "Jason R. Wilson, Seongsik Kim, Ulyana Kurylo, Joseph Cummings, Eshan\n  Tarneja", "title": "Developing Computational Models of Social Assistance to Guide Socially\n  Assistive Robots", "comments": "5 pages, AI-HRI, work in progress", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/23", "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there are many examples in which robots provide social assistance, a\nlack of theory on how the robots should decide how to assist impedes progress\nin realizing these technologies. To address this deficiency, we propose a pair\nof computational models to guide a robot as it provides social assistance. The\nmodel of social autonomy helps a robot select an appropriate assistance that\nwill help with the task at hand while also maintaining the autonomy of the\nperson being assisted. The model of social alliance describes how a to\ndetermine whether the robot and the person being assisted are cooperatively\nworking towards the same goal. Each of these models are rooted in social\nreasoning between people, and we describe here our ongoing work to adapt this\nsocial reasoning to human-robot interactions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 02:37:40 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Wilson", "Jason R.", ""], ["Kim", "Seongsik", ""], ["Kurylo", "Ulyana", ""], ["Cummings", "Joseph", ""], ["Tarneja", "Eshan", ""]]}, {"id": "1909.06527", "submitter": "Christoforos Mavrogiannis", "authors": "Gilwoo Lee, Christoforos Mavrogiannis, Siddhartha S. Srinivasa", "title": "Towards Effective Human-AI Teams: The Case of Collaborative Packing", "comments": "Added two graphs and made some minor edits to the text", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/20", "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of designing an artificial agent (AI), capable of\nassisting a human user to complete a task. Our goal is to guide human users\ntowards optimal task performance while keeping their cognitive load as low as\npossible. Our insight is that doing so requires an understanding of human\ndecision making for the task domain at hand. In this work, we consider the\ndomain of collaborative packing, in which an AI agent provides placement\nrecommendations to a human user. As a first step, we explore the mechanisms\nunderlying human packing strategies. We conducted a user study in which 100\nhuman participants completed a series of packing tasks in a virtual\nenvironment. We analyzed their packing strategies and discovered spatial and\ntemporal patterns, such as that humans tend to place larger items at corners\nfirst. We expect that imbuing an artificial agent with an understanding of this\nspatiotemporal structure will enable improved assistance, which will be\nreflected in the task performance and the human perception of the AI. Ongoing\nwork involves the development of a framework that incorporates the extracted\ninsights to predict and manipulate human decision making towards an efficient\ntrajectory of low cognitive load and high efficiency. A follow-up study will\nevaluate our framework against a set of baselines featuring alternative\nstrategies of assistance. Our eventual goal is the deployment and evaluation of\nour framework on an autonomous robotic manipulator, actively assisting users on\na packing task.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 04:13:35 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 18:53:11 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 04:18:15 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Lee", "Gilwoo", ""], ["Mavrogiannis", "Christoforos", ""], ["Srinivasa", "Siddhartha S.", ""]]}, {"id": "1909.06560", "submitter": "Justin Hart", "authors": "Justin Hart, Reuth Mirsky, Stone Tejeda, Bonny Mahajan, Jamin Goo,\n  Kathryn Baldauf, Sydney Owen, Peter Stone", "title": "Unclogging Our Arteries: Using Human-Inspired Signals to Disambiguate\n  Navigational Intentions", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/27", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are proficient at communicating their intentions in order to avoid\nconflicts when navigating in narrow, crowded environments. In many situations\nmobile robots lack both the ability to interpret human intentions and the\nability to clearly communicate their own intentions to people sharing their\nspace. This work addresses the second of these points, leveraging insights\nabout how people implicitly communicate with each other through observations of\nbehaviors such as gaze to provide mobile robots with better social navigation\nskills. In a preliminary human study, the importance of gaze as a signal used\nby people to interpret each-other's intentions during navigation of a shared\nspace is observed. This study is followed by the development of a virtual agent\nhead which is mounted to the top of the chassis of the BWIBot mobile robot\nplatform. Contrasting the performance of the virtual agent head against an LED\nturn signal demonstrates that the naturalistic, implicit gaze cue is more\neasily interpreted than the LED turn signal.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 08:45:51 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 09:00:23 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Hart", "Justin", ""], ["Mirsky", "Reuth", ""], ["Tejeda", "Stone", ""], ["Mahajan", "Bonny", ""], ["Goo", "Jamin", ""], ["Baldauf", "Kathryn", ""], ["Owen", "Sydney", ""], ["Stone", "Peter", ""]]}, {"id": "1909.06561", "submitter": "Victor Fernandez Castro", "authors": "Victor Fernandez Castro, Aurelie Clodic, Rachid Alami and Elisabeth\n  Pacherie", "title": "Commitments in Human-Robot Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/30", "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important tradition in philosophy holds that in order to successfully\nperform a joint action, the participants must be capable of establishing\ncommitments on joint goals and shared plans. This suggests that social robotics\nshould endow robots with similar competences for commitment management in order\nto achieve the objective of performing joint tasks in human-robot interactions.\nIn this paper, we examine two philosophical approaches to commitments. These\napproaches, we argue, emphasize different behavioral and cognitive aspects of\ncommitments that give roboticists a way to give meaning to monitoring and\npro-active signaling in joint action with human partners. To show that, we\npresent an example of use-case with guiding robots and we sketch a framework\nthat can be used to explore the type of capacities and behaviors that a robot\nmay need to manage commitments.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 08:49:09 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 12:14:33 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Castro", "Victor Fernandez", ""], ["Clodic", "Aurelie", ""], ["Alami", "Rachid", ""], ["Pacherie", "Elisabeth", ""]]}, {"id": "1909.06675", "submitter": "James Boerkoel Jr.", "authors": "Seth Isaacson and Gretchen Rice and James C. Boerkoel Jr", "title": "MAD-TN: A Tool for Measuring Fluency in Human-Robot Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/03", "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fluency is an important metric in Human-Robot Interaction (HRI) that\ndescribes the coordination with which humans and robots collaborate on a task.\nFluency is inherently linked to the timing of the task, making temporal\nconstraint networks a promising way to model and measure fluency. We show that\nthe Multi-Agent Daisy Temporal Network (MAD-TN) formulation, which expands on\nan existing concept of daisy-structured networks, is both an effective model of\nhuman-robot collaboration and a natural way to measure a number of existing\nfluency metrics. The MAD-TN model highlights new metrics that we hypothesize\nwill strongly correlate with human teammates' perception of fluency.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 21:00:48 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 16:41:40 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 00:43:09 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Isaacson", "Seth", ""], ["Rice", "Gretchen", ""], ["Boerkoel", "James C.", "Jr"]]}, {"id": "1909.06799", "submitter": "Tom Blount", "authors": "Tom Blount and Callum Spawforth", "title": "Pathos in Play: How Game Designers Evoke Negative Emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much in the same way that people enjoy, from time to time, the pathos of\nconsuming a tragic film or piece of literature, designers of digital games are\nincreasingly including elements within their games that evoke uncomfortable or\nnegative emotions in their audience, allowing players to introspect and explore\n\"adult\" themes and topics, including loss, regret, powerlessness,\nmental-health, and mortality. In this paper we examine a number of recent games\nas case studies and explore the way in which they use their mechanics to evoke\nfeelings of discomfort in their players, and the way in which pathos serves\nplay. Through this, we highlight a number of different techniques used by game\ndesigners and conclude by proposing further work in this space to determine\nexactly why players are drawn to these types of games, and to explore the ways\nin which research in this field could be used to drive yet more emotive and\nempathetic games.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 13:34:12 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Blount", "Tom", ""], ["Spawforth", "Callum", ""]]}, {"id": "1909.06857", "submitter": "Lesandro Ponciano", "authors": "Lesandro Ponciano", "title": "HCI Support Card: Creating and Using a Support Card for Education in\n  Human-Computer Interaction", "comments": "Workshop on HCI Education (WEIHC '19)", "journal-ref": "Extended Proceedings of the 18th Brazilian Symposium on Human\n  Factors in Computing Systems, October 21--25, 2019, Vit\\'oria - ES, Brazil", "doi": "10.5753/ihc.2019.8409", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Support cards summarise a set of core information about a subject. The\nperiodic table of chemical elements and the mathematical tables are well-known\nexamples of support cards for didactic purposes. Technology professionals also\nuse support cards for recalling information such as syntactic details of\nprogramming languages or harmonic colour palettes for designing user\ninterfaces. While support cards have proved useful in many contexts, little is\nknown about its didactic use in the Human-Computer Interaction (HCI) field. To\nfill this gap, this study proposes and evaluates a process for creating and\nusing an HCI support card. The process considers the interdisciplinary nature\nof the field, covering the syllabus, curriculum, textbooks, and students'\nperception about HCI topics. The evaluation is based on case studies of\ncreating and using a card during a semester in two undergraduate courses:\nSoftware Engineering and Information Systems. Results show that a support card\ncan help students in following the lessons, remembering and integrating the\ndifferent topics studied in the classroom. The card guides the students in\nbuilding their cognitive maps, mind maps, and concept maps to study\nhuman-computer interaction. It fosters students' curiosity and permanent\nengagement with the HCI topics. The card usefulness goes beyond the HCI\nclassroom, being also used by students in their professional activities and\nother academic disciplines, fostering an interdisciplinary application of HCI\ntopics.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 18:47:16 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Ponciano", "Lesandro", ""]]}, {"id": "1909.06907", "submitter": "Arjun Akula", "authors": "Arjun R. Akula, Changsong Liu, Sari Saba-Sadiya, Hongjing Lu, Sinisa\n  Todorovic, Joyce Y. Chai, Song-Chun Zhu", "title": "X-ToM: Explaining with Theory-of-Mind for Gaining Justified Human Trust", "comments": "A short version of this was presented at CVPR 2019 Workshop on\n  Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new explainable AI (XAI) framework aimed at increasing justified\nhuman trust and reliance in the AI machine through explanations. We pose\nexplanation as an iterative communication process, i.e. dialog, between the\nmachine and human user. More concretely, the machine generates sequence of\nexplanations in a dialog which takes into account three important aspects at\neach dialog turn: (a) human's intention (or curiosity); (b) human's\nunderstanding of the machine; and (c) machine's understanding of the human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. In other words, these explicit mental\nrepresentations in ToM are incorporated to learn an optimal explanation policy\nthat takes into account human's perception and beliefs. Furthermore, we also\nshow that ToM facilitates in quantitatively measuring justified human trust in\nthe machine by comparing all the three mental representations.\n  We applied our framework to three visual recognition tasks, namely, image\nclassification, action recognition, and human body pose estimation. We argue\nthat our ToM based explanations are practical and more natural for both expert\nand non-expert users to understand the internal workings of complex machine\nlearning models. To the best of our knowledge, this is the first work to derive\nexplanations using ToM. Extensive human study experiments verify our\nhypotheses, showing that the proposed explanations significantly outperform the\nstate-of-the-art XAI methods in terms of all the standard quantitative and\nqualitative XAI evaluation metrics including human trust, reliance, and\nexplanation satisfaction.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 23:24:32 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Akula", "Arjun R.", ""], ["Liu", "Changsong", ""], ["Saba-Sadiya", "Sari", ""], ["Lu", "Hongjing", ""], ["Todorovic", "Sinisa", ""], ["Chai", "Joyce Y.", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1909.06959", "submitter": "Dritjon Gruda", "authors": "Dritjon Gruda and Souleiman Hasan", "title": "Feeling Anxious? Perceiving Anxiety in Tweets using Machine Learning", "comments": "36 pages, 6 figures", "journal-ref": "Computers in Human Behavior, 98, 2019, 245-255", "doi": "10.1016/j.chb.2019.04.020", "report-no": null, "categories": "cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study provides a predictive measurement tool to examine perceived\nanxiety from a longitudinal perspective, using a non-intrusive machine learning\napproach to scale human rating of anxiety in microblogs. Results suggest that\nour chosen machine learning approach depicts perceived user state-anxiety\nfluctuations over time, as well as mean trait anxiety. We further find a\nreverse relationship between perceived anxiety and outcomes such as social\nengagement and popularity. Implications on the individual, organizational, and\nsocietal levels are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 10:38:55 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gruda", "Dritjon", ""], ["Hasan", "Souleiman", ""]]}, {"id": "1909.06976", "submitter": "Zijia Zhong", "authors": "Zijia Zhong and Joyoung Lee", "title": "Virtual Guide Dog: Next Generation Pedestrian Signal for the Visually\n  Impaired", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessible pedestrian signal (APS) was proposed as a mean to achieve the same\nlevel of service that is set forth by the American with Disability Act (ADA)\nfor the visually impaired. One of the major issues of existing APSs is the\nfailure to deliver adequate crossing information for the visually impaired.\nThis paper presents a mobile-based APS application, namely Virtual Guide Dog\n(VGD). Integrating intersection information and onboard sensors (e.g., GPS,\ncompass, accelerometer, and gyroscope sensor) of modern smartphones, the VGD\napplication can notify the visually impaired: 1) the close proximity of an\nintersection and 2) the street information for crossing. By employing a screen\ntapping interface, VGD can remotely place a pedestrian crossing call to the\ncontroller, without the need of using a push button. In addition, VGD informs\nVIs the start of a crossing phase by using text-to-speech technology. The\nproof-of-concept test shows that VGD keeps the users informed about the\nremaining distance as their approaching the intersection. It was also found\nthat the GPS-only mode is accompanied by greater distance deviation compared to\nthe mode jointly operating with both GPS and cellular positioning.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 04:04:10 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Zhong", "Zijia", ""], ["Lee", "Joyoung", ""]]}, {"id": "1909.07013", "submitter": "Csaba D. Toth", "authors": "Daniel Archambault, Csaba D. T\\'oth", "title": "Proceedings of the 27th International Symposium on Graph Drawing and\n  Network Visualization (GD 2019)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DM cs.DS cs.HC cs.SI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the arXiv index for the electronic proceedings of GD 2019, which\ncontains the peer-reviewed and revised accepted papers with an optional\nappendix. Proceedings (without appendices) are also to be published by Springer\nin the Lecture Notes in Computer Science series.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 06:29:50 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Archambault", "Daniel", ""], ["T\u00f3th", "Csaba D.", ""]]}, {"id": "1909.07065", "submitter": "Maximilian Mackeprang", "authors": "Maximilian Mackeprang, Claudia M\\\"uller-Birn, Maximilian Timo Stauss", "title": "Discovering the Sweet Spot of Human-Computer Configurations: A Case\n  Study in Information Extraction", "comments": "Accepted Version of the Paper published at CSCW 2019", "journal-ref": null, "doi": "10.1145/3359297", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive intelligent systems, i.e., interactive systems that employ AI\ntechnologies, are currently present in many parts of our social, public and\npolitical life. An issue reoccurring often in the development of these systems\nis the question regarding the level of appropriate human and computer\ncontributions. Engineers and designers lack a way of systematically defining\nand delimiting possible options for designing such systems in terms of levels\nof automation. In this paper, we propose, apply and reflect on a method for\nhuman-computer configuration design. It supports the systematic investigation\nof the design space for developing an interactive intelligent system. We\nillustrate our method with a use case in the context of collaborative ideation.\nHere, we developed a tool for information extraction from idea content. A\nchallenge was to find the right level of algorithmic support, whereby the\nquality of the information extraction should be as high as possible, but, at\nthe same time, the human effort should be low. Such contradicting goals are\noften an issue in system development; thus, our method proposed helped us to\nconceptualize and explore the design space. Based on a critical reflection on\nour method application, we want to offer a complementary perspective to the\nvalue-centered design of interactive intelligent systems. Our overarching goal\nis to contribute to the design of so-called hybrid systems where humans and\ncomputers are partners.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:53:14 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mackeprang", "Maximilian", ""], ["M\u00fcller-Birn", "Claudia", ""], ["Stauss", "Maximilian Timo", ""]]}, {"id": "1909.07088", "submitter": "Chieh Yu Chen", "authors": "Hsin-Ying Hsieh, Chieh-Yu Chen, Yu-Shuen Wang, Jung-Hong Chuang", "title": "BasketballGAN: Generating Basketball Play Simulation Through Sketching", "comments": "9 pages, Accepted paper at ACMMM 2019, code is available at\n  https://github.com/chychen/BasketballGAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a data-driven basketball set play simulation. Given an offensive\nset play sketch, our method simulates potential scenarios that may occur in the\ngame. The simulation provides coaches and players with insights on how a given\nset play can be executed. To achieve the goal, we train a conditional\nadversarial network on NBA movement data to imitate the behaviors of how\nplayers move around the court through two major components: a generator that\nlearns to generate natural player movements based on a latent noise and a user\nsketched set play; and a discriminator that is used to evaluate the realism of\nthe basketball play. To improve the quality of simulation, we minimize 1.) a\ndribbler loss to prevent the ball from drifting away from the dribbler; 2.) a\ndefender loss to prevent the dribbler from not being defended; 3.) a ball\npassing loss to ensure the straightness of passing trajectories; and 4) an\nacceleration loss to minimize unnecessary players' movements. To evaluate our\nsystem, we objectively compared real and simulated basketball set plays.\nBesides, a subjective test was conducted to judge whether a set play was real\nor generated by our network. On average, the mean correct rates to the binary\ntests were 56.17 \\%. Experiment results and the evaluations demonstrated the\neffectiveness of our system.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 09:45:49 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 02:33:51 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Hsieh", "Hsin-Ying", ""], ["Chen", "Chieh-Yu", ""], ["Wang", "Yu-Shuen", ""], ["Chuang", "Jung-Hong", ""]]}, {"id": "1909.07208", "submitter": "Emna Rejaibi", "authors": "Emna Rejaibi, Ali Komaty, Fabrice Meriaudeau, Said Agrebi, and Alice\n  Othmani", "title": "MFCC-based Recurrent Neural Network for Automatic Clinical Depression\n  Recognition and Assessment from Speech", "comments": "14 pages, 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Clinical depression or Major Depressive Disorder (MDD) is a common and\nserious medical illness. In this paper, a deep recurrent neural network-based\nframework is presented to detect depression and to predict its severity level\nfrom speech. Low-level and high-level audio features are extracted from audio\nrecordings to predict the 24 scores of the Patient Health Questionnaire and the\nbinary class of depression diagnosis. To overcome the problem of the small size\nof Speech Depression Recognition (SDR) datasets, expanding training labels and\ntransferred features are considered. The proposed approach outperforms the\nstate-of-art approaches on the DAIC-WOZ database with an overall accuracy of\n76.27% and a root mean square error of 0.4 in assessing depression, while a\nroot mean square error of 0.168 is achieved in predicting the depression\nseverity levels. The proposed framework has several advantages (fastness,\nnon-invasiveness, and non-intrusion), which makes it convenient for real-time\napplications. The performances of the proposed approach are evaluated under a\nmulti-modal and a multi-features experiments. MFCC based high-level features\nhold relevant information related to depression. Yet, adding visual action\nunits and different other acoustic features further boosts the classification\nresults by 20% and 10% to reach an accuracy of 95.6% and 86%, respectively.\nConsidering visual-facial modality needs to be carefully studied as it sparks\npatient privacy concerns while adding more acoustic features increases the\ncomputation time.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:03:01 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 13:09:24 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Rejaibi", "Emna", ""], ["Komaty", "Ali", ""], ["Meriaudeau", "Fabrice", ""], ["Agrebi", "Said", ""], ["Othmani", "Alice", ""]]}, {"id": "1909.07268", "submitter": "Jessica Rivera Villicana", "authors": "Jessica Rivera-Villicana, Fabio Zambetta, James Harland, Marsha Berry", "title": "Exploring Apprenticeship Learning for Player Modelling in Interactive\n  Narratives", "comments": "Extended Abstracts of the 2019 Annual Symposium on Computer-Human\n  Interaction in Play (CHI Play)", "journal-ref": null, "doi": "10.1145/3341215.3356314", "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an early Apprenticeship Learning approach to mimic\nthe behaviour of different players in a short adaption of the interactive\nfiction Anchorhead. Our motivation is the need to understand and simulate\nplayer behaviour to create systems to aid the design and personalisation of\nInteractive Narratives (INs). INs are partially observable for the players and\ntheir goals are dynamic as a result. We used Receding Horizon IRL (RHIRL) to\nlearn players' goals in the form of reward functions, and derive policies to\nimitate their behaviour. Our preliminary results suggest that RHIRL is able to\nlearn action sequences to complete a game, and provided insights towards\ngenerating behaviour more similar to specific players.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 15:15:57 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Rivera-Villicana", "Jessica", ""], ["Zambetta", "Fabio", ""], ["Harland", "James", ""], ["Berry", "Marsha", ""]]}, {"id": "1909.07471", "submitter": "Kunal Bagewadi", "authors": "Kunal Bagewadi, Joseph Campbell, Heni Ben Amor", "title": "Multimodal Dataset of Human-Robot Hugging Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/09", "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hug is a tight embrace and an expression of warmth, sympathy and\ncamaraderie. Despite the fact that a hug often only takes a few seconds, it is\nfilled with details and nuances and is a highly complex process of coordination\nbetween two agents. For human-robot collaborative tasks, it is necessary for\nhumans to develop trust and see the robot as a partner to perform a given task\ntogether. Datasets representing agent-agent interaction are scarce and, if\navailable, of limited quality. To study the underlying phenomena and variations\nin a hug between a person and a robot, we deployed Baxter humanoid robot and\nwearable sensors on persons to record 353 episodes of hugging activity. 33\npeople were given minimal instructions to hug the humanoid robot for as natural\nhugging interaction as possible. In the paper, we present our methodology and\nanalysis of the collected dataset. The use of this dataset is to implement\nmachine learning methods for the humanoid robot to learn to anticipate and\nreact to the movements of a person approaching for a hug. In this regard, we\nshow the significance of the dataset by highlighting certain features in our\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 20:40:40 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Bagewadi", "Kunal", ""], ["Campbell", "Joseph", ""], ["Amor", "Heni Ben", ""]]}, {"id": "1909.07502", "submitter": "Benjamin Shickel", "authors": "Benjamin Shickel, Scott Siegel, Martin Heesacker, Sherry Benton, and\n  Parisa Rashidi", "title": "Automatic Detection and Classification of Cognitive Distortions in\n  Mental Health Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cognitive psychology, automatic and self-reinforcing irrational thought\npatterns are known as cognitive distortions. Left unchecked, patients\nexhibiting these types of thoughts can become stuck in negative feedback loops\nof unhealthy thinking, leading to inaccurate perceptions of reality commonly\nassociated with anxiety and depression. In this paper, we present a machine\nlearning framework for the automatic detection and classification of 15 common\ncognitive distortions in two novel mental health free text datasets collected\nfrom both crowdsourcing and a real-world online therapy program. When\ndifferentiating between distorted and non-distorted passages, our model\nachieved a weighted F1 score of 0.88. For classifying distorted passages into\none of 15 distortion categories, our model yielded weighted F1 scores of 0.68\nin the larger crowdsourced dataset and 0.45 in the smaller online counseling\ndataset, both of which outperformed random baseline metrics by a large margin.\nFor both tasks, we also identified the most discriminative words and phrases\nbetween classes to highlight common thematic elements for improving targeted\nand therapist-guided mental health treatment. Furthermore, we performed an\nexploratory analysis using unsupervised content-based clustering and topic\nmodeling algorithms as first efforts towards a data-driven perspective on the\nthematic relationship between similar cognitive distortions traditionally\ndeemed unique. Finally, we highlight the difficulties in applying mental\nhealth-based machine learning in a real-world setting and comment on the\nimplications and benefits of our framework for improving automated delivery of\ntherapeutic treatment in conjunction with traditional cognitive-behavioral\ntherapy.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 22:21:27 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 00:50:07 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Shickel", "Benjamin", ""], ["Siegel", "Scott", ""], ["Heesacker", "Martin", ""], ["Benton", "Sherry", ""], ["Rashidi", "Parisa", ""]]}, {"id": "1909.08052", "submitter": "Martin Ross", "authors": "Martin K. Ross, Frank Broz and Lynne Baillie", "title": "Towards an Adaptive Robot for Sports and Rehabilitation Coaching", "comments": "AI-HRI 2019", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/08", "categories": "cs.HC cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The work presented in this paper aims to explore how, and to what extent, an\nadaptive robotic coach has the potential to provide extra motivation to adhere\nto long-term rehabilitation and help fill the coaching gap which occurs during\nrepetitive solo practice in high performance sport. Adapting the behavior of a\nsocial robot to a specific user, using reinforcement learning (RL), could be a\nway of increasing adherence to an exercise routine in both domains. The\nrequirements gathering phase is underway and is presented in this paper along\nwith the rationale of using RL in this context.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:51:05 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ross", "Martin K.", ""], ["Broz", "Frank", ""], ["Baillie", "Lynne", ""]]}, {"id": "1909.08161", "submitter": "Nikhil Krishnaswamy", "authors": "Nikhil Krishnaswamy and James Pustejovsky", "title": "Multimodal Continuation-style Architectures for Human-Robot Interaction", "comments": "Advances in Cognitive Systems Cognitive Vision Workshop (2019), 8\n  pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an architecture for integrating real-time, multimodal input into a\ncomputational agent's contextual model. Using a human-avatar interaction in a\nvirtual world, we treat aligned gesture and speech as an ensemble where content\nmay be communicated by either modality. With a modified nondeterministic\npushdown automaton architecture, the computer system: (1) consumes input\nincrementally using continuation-passing style until it achieves sufficient\nunderstanding the user's aim; (2) constructs and asks questions where necessary\nusing established contextual information; and (3) maintains track of prior\ndiscourse items using multimodal cues. This type of architecture supports\nspecial cases of pushdown and finite state automata as well as integrating\noutputs from machine learning models. We present examples of this\narchitecture's use in multimodal one-shot learning interactions of novel\ngestures and live action composition.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 01:42:03 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Krishnaswamy", "Nikhil", ""], ["Pustejovsky", "James", ""]]}, {"id": "1909.08172", "submitter": "Abraham Glasser", "authors": "Raja Kushalnagar, Gary Behm, Kevin Wolfe, Peter Yeung, Becca Dingman,\n  Shareef Ali, Abraham Glasser, Claire Ryan", "title": "RTTD-ID: Tracked Captions with Multiple Speakers for Deaf Students", "comments": "ASEE 2018 conference, 8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Students who are deaf and hard of hearing cannot hear in class and do not\nhave full access to spoken information. They can use accommodations such as\ncaptions that display speech as text. However, compared with their hearing\npeers, the caption accommodations do not provide equal access, because they are\nfocused on reading captions on their tablet and cannot see who is talking. This\nviewing isolation contributes to student frustration and risk of doing poorly\nor withdrawing from introductory engineering courses with lab components. It\nalso contributes to their lack of inclusion and sense of belonging. We report\non the evaluation of a Real-Time Text Display with Speaker-Identification,\nwhich displays the location of a speaker in a group (RTTD-ID). RTTD-ID aims to\nreduce frustration in identifying and following an active speaker when there\nare multiple speakers, e.g., in a lab. It has three different display schemes\nto identify the location of the active speaker, which helps deaf students in\nviewing both the speaker's words and the speaker's expression and actions. We\nevaluated three RTTD speaker identification methods: 1) traditional: captions\nstay in one place and viewers search for the speaker, 2) pointer: captions stay\nin one place, and a pointer to the speaker is displayed, and 3) pop-up:\ncaptions \"pop-up\" next to the speaker. We gathered both quantitative and\nqualitative information through evaluations with deaf and hard of hearing\nusers. The users preferred the pointer identification method over the\ntraditional and pop-up methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 02:21:00 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Kushalnagar", "Raja", ""], ["Behm", "Gary", ""], ["Wolfe", "Kevin", ""], ["Yeung", "Peter", ""], ["Dingman", "Becca", ""], ["Ali", "Shareef", ""], ["Glasser", "Abraham", ""], ["Ryan", "Claire", ""]]}, {"id": "1909.08258", "submitter": "EPTCS", "authors": "Kinjal Basu", "title": "Conversational AI : Open Domain Question Answering and Commonsense\n  Reasoning", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646", "journal-ref": "EPTCS 306, 2019, pp. 396-402", "doi": "10.4204/EPTCS.306.53", "report-no": null, "categories": "cs.AI cs.HC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research is focused on making a human-like question answering system\nwhich can answer rationally. The distinguishing characteristic of our approach\nis that it will use automated common sense reasoning to truly \"understand\"\ndialogues, allowing it to converse like a human. Humans often make many\nassumptions during conversations. We infer facts not told explicitly by using\nour common sense. Incorporating commonsense knowledge in a question answering\nsystem will simply make it more robust.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:13:46 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Basu", "Kinjal", ""]]}, {"id": "1909.08398", "submitter": "Valeria Pannunzio", "authors": "Valeria Pannunzio, Maaike Kleinsmann, Dirk Snelders", "title": "Design research, eHealth, and the convergence revolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Quadruple Aim is a framework which prioritizes four aims, or dimensions\nof performance, for innovating in the healthcare domain, respectively: 1)\nenhancing the individual experience of care; 2) improving the work life of\nhealth care clinicians and staff; 3) improving the health of populations; and\n4) reducing the per capita cost of care. In this contribution, recent\nliterature providing examples of design research in the eHealth domain is\nreviewed to answer the research question: in which measure has design research\ncontributed to each of the four aims of eHealth innovation in the past five\nyears?. The results of the review are presented and employed to draw three main\nobservations: 1) design researchers in eHealth seem to be largely focused on\nimproving experiences of care, either patients' or health professionals; 2)\ndesign researchers' contribution on reducing per capita costs of care appears\nto be less pronounced, which is outlined as a point for improvement; and 3) in\na considerable amount of reviewed contributions, design researchers appear to\nbe contributing to multiple aims at once. In this sub-group of reviewed\ncontributions, several disciplinary areas and types of stakeholders interact\nand integrate through design research activities. The latter observation leads\nto a reflection on the strategic role of design research in the contexts of the\nconvergence revolution and of the non-communicable disease crisis. Implications\nof this reflection for design researchers are recognized in the opportunity and\ntimeliness to develop eHealth-specific ways to orchestrate design integration.\nA direction for further research in this sense is identified in the use of\nsensory and self-monitored data as a boundary object for eHealth innovation.\nThe prospective value of this direction is finally exemplified through the case\nof blood pressure.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 13:49:19 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Pannunzio", "Valeria", ""], ["Kleinsmann", "Maaike", ""], ["Snelders", "Dirk", ""]]}, {"id": "1909.08500", "submitter": "Ranya Aloufi", "authors": "Ranya Aloufi, Hamed Haddadi, David Boyle", "title": "Emotion Filtering at the Edge", "comments": "6 pages, 6 figures, Sensys-ML19 workshop in conjunction with the 17th\n  ACM Conference on Embedded Networked Sensor Systems (SenSys 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.CR cs.HC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice controlled devices and services have become very popular in the\nconsumer IoT. Cloud-based speech analysis services extract information from\nvoice inputs using speech recognition techniques. Services providers can thus\nbuild very accurate profiles of users' demographic categories, personal\npreferences, emotional states, etc., and may therefore significantly compromise\ntheir privacy. To address this problem, we have developed a privacy-preserving\nintermediate layer between users and cloud services to sanitize voice input\ndirectly at edge devices. We use CycleGAN-based speech conversion to remove\nsensitive information from raw voice input signals before regenerating\nneutralized signals for forwarding. We implement and evaluate our emotion\nfiltering approach using a relatively cheap Raspberry Pi 4, and show that\nperformance accuracy is not compromised at the edge. In fact, signals generated\nat the edge differ only slightly (~0.16%) from cloud-based approaches for\nspeech recognition. Experimental evaluation of generated signals show that\nidentification of the emotional state of a speaker can be reduced by ~91%.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 15:28:12 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Aloufi", "Ranya", ""], ["Haddadi", "Hamed", ""], ["Boyle", "David", ""]]}, {"id": "1909.08758", "submitter": "Edward Sheffield", "authors": "Edward Y. Sheffield", "title": "Extracting Super-resolution Structures inside a Single Molecule or\n  Overlapped Molecules from One Blurred Image", "comments": "Edward Y. Sheffield is a pen name of Yaohua Xie\n  (Yaohua.Xie@hotmail.com, fjpnxyh2000@163.com,\n  http://orcid.org/0000-0001-6780-3156)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some super-resolution techniques, adjacent points are illuminated at\ndifferent times. Thereby, their locations and light intensities can be detected\neven if the images are very blurred due to diffraction. According to\nconventional theories, the points' inner details cannot be recovered because\nthe images' high frequency components are removed due to the diffraction-limit.\nBut this study finds an exception, and full information can be extracted from a\ndiffraction-blurred image. In such a \"resolvable condition\", neither profile\nnor detail information is damaged by diffraction. Thereby, it can be recovered\nreversibly by solving equation systems in spatial domain or frequency domain.\nThis condition is tightly relevant to the imaging condition of existing\nsuper-resolution techniques. Based on the condition, a method is proposed which\ncan achieve unlimited high resolutions in principle, and its effectiveness is\ndemonstrated by both theoretical analysis and simulation experiments. It can\nalso work without any observed image outside the region of interest. Simulation\nexperiments also show its tolerance to certain level of noise.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 01:09:59 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 01:39:31 GMT"}, {"version": "v3", "created": "Sat, 7 Dec 2019 09:21:34 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Sheffield", "Edward Y.", ""]]}, {"id": "1909.08766", "submitter": "Deepali Aneja", "authors": "Deepali Aneja, Daniel McDuff, Shital Shah", "title": "A High-Fidelity Open Embodied Avatar with Lip Syncing and Expression\n  Capabilities", "comments": "International Conference on Multimodal Interaction (ICMI 2019)", "journal-ref": null, "doi": "10.1145/3340555.3353744", "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied avatars as virtual agents have many applications and provide\nbenefits over disembodied agents, allowing non-verbal social and interactional\ncues to be leveraged, in a similar manner to how humans interact with each\nother. We present an open embodied avatar built upon the Unreal Engine that can\nbe controlled via a simple python programming interface. The avatar has lip\nsyncing (phoneme control), head gesture and facial expression (using either\nfacial action units or cardinal emotion categories) capabilities. We release\ncode and models to illustrate how the avatar can be controlled like a puppet or\nused to create a simple conversational agent using public application\nprogramming interfaces (APIs). GITHUB link:\nhttps://github.com/danmcduff/AvatarSim\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 01:39:39 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 05:10:21 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Aneja", "Deepali", ""], ["McDuff", "Daniel", ""], ["Shah", "Shital", ""]]}, {"id": "1909.08839", "submitter": "Irawan Nurhas", "authors": "Irawan Nurhas, Jan Pawlowski, Stefan Geisler", "title": "Towards humane digitization: a wellbeing-driven process of personas\n  creation", "comments": "8 Pages, CHIuXiD '19: Proceedings of the 5th International ACM\n  In-Cooperation HCI and UX Conference", "journal-ref": null, "doi": "10.1145/3328243.3328247", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital transformation is a process of digitizing the working and living\nenvironment in which people are at the center of digitization. In this paper,\nwe present a personas-based guideline for system developers on how the\nhumanization of digital transformation integrates into the design process. The\nproposed guideline uses the positive personas from the beginning as a basis for\nthe transformation of the working environment into the digital form. We used\nthe literature research as a preliminary study for the process of\nwellbeing-driven digital transformation design, consisting of questions for\nstructuring the required information in the positive personas as well as a\npotential method that could be integrated into the wellbeing-based design\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 07:55:17 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Nurhas", "Irawan", ""], ["Pawlowski", "Jan", ""], ["Geisler", "Stefan", ""]]}, {"id": "1909.08845", "submitter": "L\\'eo Hemamou", "authors": "L\\'eo Hemamou, Ghazi Felhi, Jean-Claude Martin and Chlo\\'e Clavel", "title": "Slices of Attention in Asynchronous Video Job Interviews", "comments": "Accepted at 2019 8th International Conference on Affective Computing\n  and Intelligent Interaction (ACII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The impact of non verbal behaviour in a hiring decision remains an open\nquestion. Investigating this question is important, as it could provide a\nbetter understanding on how to train candidates for job interviews and make\nrecruiters be aware of influential non verbal behaviour. This research has\nrecently been accelerated due to the development of tools for the automatic\nanalysis of social signals, and the emergence of machine learning methods.\nHowever, these studies are still mainly based on hand engineered features,\nwhich imposes a limit to the discovery of influential social signals. On the\nother side, deep learning methods are a promising tool to discover complex\npatterns without the necessity of feature engineering. In this paper, we focus\non studying influential non verbal social signals in asynchronous job video\ninterviews that are discovered by deep learning methods. We use a previously\npublished deep learning system that aims at inferring the hirability of a\ncandidate with regard to a sequence of interview questions. One particularity\nof this system is the use of attention mechanisms, which aim at identifying the\nrelevant parts of an answer. Thus, information at a fine-grained temporal level\ncould be extracted using global (at the interview level) annotations on\nhirability. While most of the deep learning systems use attention mechanisms to\noffer a quick visualization of slices when a rise of attention occurs, we\nperform an in-depth analysis to understand what happens during these moments.\nFirst, we propose a methodology to automatically extract slices where there is\na rise of attention (attention slices). Second, we study the content of\nattention slices by comparing them with randomly sampled slices. Finally, we\nshow that they bear significantly more information for hirability than randomly\nsampled slices.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 08:15:46 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Hemamou", "L\u00e9o", ""], ["Felhi", "Ghazi", ""], ["Martin", "Jean-Claude", ""], ["Clavel", "Chlo\u00e9", ""]]}, {"id": "1909.08997", "submitter": "Chuanmin Mi", "authors": "Chunmin Mi, Runjie Xu, Ching-Torng Lin", "title": "Real-time Recognition of Smartphone User Behavior Based on Prophet\n  Algorithms", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.00045", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the traditional physical password, fingerprint unlocking and facial\nfeatures have improved the security to a certain extent, they have the\ncharacteristics of passive authentication and easiness to be stolen. The\nexisting behavioral data collected based on mobile phone sensors is mainly used\nfor human activity recognition and fall detection and health management.\nProphet is a procedure for forecasting time series data based on an additive\nmodel where non-linear trends are fit with yearly, weekly, and daily\nseasonality, plus holiday effects. It works best with time series that have\nstrong seasonal effects and several seasons of historical data. Prophet is\nrobust to missing data and shifts in the trend, and typically handles outliers\nwell. Based on the time series behavior data of mobile terminal users, this\npaper uses Prophet algorithm to decompose the time series of six kinds of daily\nbehavior and strip off the singular value, to get the inherent cycle and trend\nof each behavior, and to verify the legitimacy of the behavior user at the next\nmoment. The experimental results on the UniMiB SHAR public dataset show that\nthe user only needs to do 2 cycles of specified actions to realize the\nprediction of the next time series. The main contribution of this paper is that\nwe propose a new idea for smartphone user authentication. It is based on\nreal-time data of smartphone user behavior, through Phophet algorithm for\nfeature decomposition and time series prediction, and to find the inherent\ncycle and other characteristics, so as to perform user behavior recognition.\nThis data-driven auxiliary authentication method can effectively solve the\nproblem of easy forgery of static feature recognition such as password,\nfingerprint and face recognition.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:17:00 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Mi", "Chunmin", ""], ["Xu", "Runjie", ""], ["Lin", "Ching-Torng", ""]]}, {"id": "1909.09064", "submitter": "Xudong Liu", "authors": "Joseph Allen, Ahmed Moussa, Xudong Liu", "title": "Human-In-The-Loop Learning of Qualitative Preference Models", "comments": "Published in the Proceedings of the 32nd International Florida\n  Artificial Intelligence Research Society Conference, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel human-in-the-loop framework to help the\nhuman user understand the decision making process that involves choosing\npreferred options. We focus on qualitative preference models over alternatives\nfrom combinatorial domains. This framework is interactive: the user provides\nher behavioral data to the framework, and the framework explains the learned\nmodel to the user. It is iterative: the framework collects feedback on the\nlearned model from the user and tries to improve it accordingly till the user\nterminates the iteration. In order to communicate the learned preference model\nto the user, we develop visualization of intuitive and explainable graphic\nmodels, such as lexicographic preference trees and forests, and conditional\npreference networks. To this end, we discuss key aspects of our framework for\nlexicographic preference models.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 16:02:40 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Allen", "Joseph", ""], ["Moussa", "Ahmed", ""], ["Liu", "Xudong", ""]]}, {"id": "1909.09078", "submitter": "Andre Rodrigues", "authors": "Andr\\'e Rodrigues, Hugo Nicolau, Kyle Montague, Jo\\~ao Guerreiro and\n  Tiago Guerreiro", "title": "Open Challenges of Blind People using Smartphones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind people face significant challenges when using smartphones. The focus on\nimproving non-visual mobile accessibility has been at the level of touchscreen\naccess. Our research investigates the challenges faced by blind people in their\neveryday usage of mobile phones. In this paper, we present a set of studies\nperformed with the target population, novices and experts, using a variety of\nmethods, targeted at identifying and verifying challenges; and coping\nmechanisms. Through a multiple methods approach we identify and validate\nchallenges locally with a diverse set of user expertise and devices, and at\nscale through the analyses of the largest Android and iOS dedicate forums for\nblind people. We contribute with a prioritized corpus of smartphone challenges\nfor blind people, and a discussion on a set of directions for future research\nthat tackle the open and often overlooked challenges.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 16:19:05 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 10:35:27 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Rodrigues", "Andr\u00e9", ""], ["Nicolau", "Hugo", ""], ["Montague", "Kyle", ""], ["Guerreiro", "Jo\u00e3o", ""], ["Guerreiro", "Tiago", ""]]}, {"id": "1909.09156", "submitter": "Amos Azaria", "authors": "Moshe Hanukoglu, Nissan Goldberg, Aviv Rovshitz, Amos Azaria", "title": "Learning to Conceal: A Deep Learning Based Method for Preserving Privacy\n  and Avoiding Prejudice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a learning model able to conceals personal\ninformation (e.g. gender, age, ethnicity, etc.) from an image, while\nmaintaining any additional information present in the image (e.g. smile,\nhair-style, brightness). Our trained model is not provided the information that\nit is concealing, and does not try learning it either. Namely, we created a\nvariational autoencoder (VAE) model that is trained on a dataset including\nlabels of the information one would like to conceal (e.g. gender, ethnicity,\nage). These labels are directly added to the VAE's sampled latent vector. Due\nto the limited number of neurons in the latent vector and its appended noise,\nthe VAE avoids learning any relation between the given images and the given\nlabels, as those are given directly. Therefore, the encoded image lacks any of\nthe information one wishes to conceal. The encoding may be decoded back into an\nimage according to any provided properties (e.g. a 40 year old woman).\n  The proposed architecture can be used as a mean for privacy preserving and\ncan serve as an input to systems, which will become unbiased and not suffer\nfrom prejudice. We believe that privacy and discrimination are two of the most\nimportant aspects in which the community should try and develop methods to\nprevent misuse of technological advances.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 13:54:18 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Hanukoglu", "Moshe", ""], ["Goldberg", "Nissan", ""], ["Rovshitz", "Aviv", ""], ["Azaria", "Amos", ""]]}, {"id": "1909.09209", "submitter": "EPTCS", "authors": "Daoming Lyu (Auburn University), Fangkai Yang (NVIDIA Corporation), Bo\n  Liu (Auburn University), Steven Gustafson (Maana Inc.)", "title": "A Human-Centered Data-Driven Planner-Actor-Critic Architecture via Logic\n  Programming", "comments": "In Proceedings ICLP 2019, arXiv:1909.07646. arXiv admin note:\n  significant text overlap with arXiv:1906.07268", "journal-ref": "EPTCS 306, 2019, pp. 182-195", "doi": "10.4204/EPTCS.306.23", "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes of Reinforcement Learning (RL) allow an agent to learn\npolicies that surpass human experts but suffers from being time-hungry and\ndata-hungry. By contrast, human learning is significantly faster because prior\nand general knowledge and multiple information resources are utilized. In this\npaper, we propose a Planner-Actor-Critic architecture for huMAN-centered\nplanning and learning (PACMAN), where an agent uses its prior, high-level,\ndeterministic symbolic knowledge to plan for goal-directed actions, and also\nintegrates the Actor-Critic algorithm of RL to fine-tune its behavior towards\nboth environmental rewards and human feedback. This work is the first unified\nframework where knowledge-based planning, RL, and human teaching jointly\ncontribute to the policy learning of an agent. Our experiments demonstrate that\nPACMAN leads to a significant jump-start at the early stage of learning,\nconverges rapidly and with small variance, and is robust to inconsistent,\ninfrequent, and misleading feedback.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 07:06:06 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Lyu", "Daoming", "", "Auburn University"], ["Yang", "Fangkai", "", "NVIDIA Corporation"], ["Liu", "Bo", "", "Auburn University"], ["Gustafson", "Steven", "", "Maana Inc."]]}, {"id": "1909.09351", "submitter": "Patrick Cheong-Iao Pang", "authors": "Patrick Cheong-Iao Pang, Robert P. Biuk-Aghai, Simon Fong, Yain-Whar\n  Si", "title": "An Experimental Comparison of Map-like Visualisations and Treemaps", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treemaps have been used in information visualisation for over two decades.\nThey make use of nested filled areas to represent information hierarchies such\nas file systems, library catalogues, etc. Recent years have witnessed the\nemergence of visualisations that resemble geographic maps. In this paper we\npresent a study that compares the performance of one such map-like\nvisualisation with the original two forms of the treemap, namely nested and\nnon-nested treemaps. Our study employed a mixed-method evaluation of accuracy,\nspeed and usability (such as the ease-of-use and helpfulness of understanding\nthe information). We found that accuracy was highest for the map-like\nvisualisations, followed by nested treemaps and lastly non-nested treemaps.\nTask performance was fastest for nested treemaps, followed by non-nested\ntreemaps, and then map-like visualisations. For usability, nested treemaps was\nconsidered slightly more helpful than map-like visualisations while non-nested\nperformed poorly. We conclude that the results regarding accuracy are promising\nfor the use of map-like visualisations in tasks involving the visualisation of\nhierarchical information, while non-nested treemap are favoured in tasks\nrequiring speed.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 07:26:11 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Pang", "Patrick Cheong-Iao", ""], ["Biuk-Aghai", "Robert P.", ""], ["Fong", "Simon", ""], ["Si", "Yain-Whar", ""]]}, {"id": "1909.09429", "submitter": "Paul Zikas", "authors": "Efstratios Geronikolakis, Paul Zikas, Steve Kateros, Nick Lydatakis,\n  Stelios Georgiou, Mike Kentros, George Papagiannakis", "title": "A True AR Authoring Tool for Interactive Virtual Museums", "comments": "This is a preprint of a chapter for a planned book that was initiated\n  by \"Visual Computing in Cultural Heritage\" and that is expected to be\n  published by Springer. The final book chapter will differ from this preprint", "journal-ref": null, "doi": "10.1007/978-3-030-37191-3_12", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a new and innovative way of spatial computing that appeared\nrecently in the bibliography called True Augmented Reality (AR), is employed in\ncultural heritage preservation. This innovation could be adapted by the Virtual\nMuseums of the future to enhance the quality of experience. It emphasises, the\nfact that a visitor will not be able to tell, at a first glance, if the\nartefact that he/she is looking at is real or not and it is expected to draw\nthe visitors' interest. True AR is not limited to artefacts but extends even to\nbuildings or life-sized character simulations of statues. It provides the best\nvisual quality possible so that the users will not be able to tell the real\nobjects from the augmented ones. Such applications can be beneficial for future\nmuseums, as with True AR, 3D models of various exhibits, monuments, statues,\ncharacters and buildings can be reconstructed and presented to the visitors in\na realistic and innovative way. We also propose our Virtual Reality Sample\napplication, a True AR playground featuring basic components and tools for\ngenerating interactive Virtual Museum applications, alongside a 3D\nreconstructed character (the priest of Asinou church) facilitating the\nstoryteller of the augmented experience.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 11:10:23 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 12:39:51 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 05:29:31 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 11:09:20 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Geronikolakis", "Efstratios", ""], ["Zikas", "Paul", ""], ["Kateros", "Steve", ""], ["Lydatakis", "Nick", ""], ["Georgiou", "Stelios", ""], ["Kentros", "Mike", ""], ["Papagiannakis", "George", ""]]}, {"id": "1909.09633", "submitter": "Keith Burghardt", "authors": "Keith Burghardt, Tad Hogg, Kristina Lerman", "title": "Quantifying the Impact of Cognitive Biases in Question-Answering Systems", "comments": "9 pages, 5 figures", "journal-ref": "a short version is in the Proceeding of the Twelfth International\n  Conference on Web and Social Media (ICWSM-18), pp. 568-571 (2018)", "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing can identify high-quality solutions to problems; however,\nindividual decisions are constrained by cognitive biases. We investigate some\nof these biases in an experimental model of a question-answering system. In\nboth natural and controlled experiments, we observe a strong position bias in\nfavor of answers appearing earlier in a list of choices. This effect is\nenhanced by three cognitive factors: the attention an answer receives, its\nperceived popularity, and cognitive load, measured by the number of choices a\nuser has to process. While separately weak, these effects synergistically\namplify position bias and decouple user choices of best answers from their\nintrinsic quality. We end our paper by discussing the novel ways we can apply\nthese findings to substantially improve how high-quality answers are found in\nquestion-answering systems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 17:54:49 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Burghardt", "Keith", ""], ["Hogg", "Tad", ""], ["Lerman", "Kristina", ""]]}, {"id": "1909.09738", "submitter": "Philipp Moll", "authors": "Philipp Moll, Veit Frick, Natascha Rauscher, Mathias Lux", "title": "How Players Play Games: Observing the Influences of Game Mechanics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of computer games is remarkably high and is still growing\nevery year. Despite this popularity and the economical importance of gaming,\nresearch in game design, or to be more precise, of game mechanics that can be\nused to improve the enjoyment of a game, is still scarce. In this paper, we\nanalyze Fortnite, one of the currently most successful games, and observe how\nplayers play the game. We investigate what makes playing the game enjoyable by\nanalyzing video streams of experienced players from game streaming platforms\nand by conducting a user study with players who are new to the game. We\nformulate four hypotheses about how game mechanics influence the way players\ninteract with the game and how it influences player enjoyment. We present\ndifferences in player behavior between experienced players and beginners and\ndiscuss how game mechanics could be used to improve the enjoyment for\nbeginners. In addition, we describe our approach to analyze games without\naccess to game-internal data by using a toolchain which automatically extracts\ngame information from video streams.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 23:18:30 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Moll", "Philipp", ""], ["Frick", "Veit", ""], ["Rauscher", "Natascha", ""], ["Lux", "Mathias", ""]]}, {"id": "1909.09926", "submitter": "Clara Andrade Pimentel", "authors": "Clara Andrade Pimentel, Philipe de Freitas Melo, Mar\\'ilia Lyra\n  Bergamo", "title": "Levantamento de Requisitos para Jogos Educativos Infantis", "comments": "Paper is written in PT-BR (Brazilian Portuguese)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of games within the classroom is in constant growing, however, there\nare still demands for better solutions to problems such as reconciling leisure\nwith class or the lack of attention of children. This work demonstrates\nmethodologies used to survey and analyze requirements for the development of\nchildren's educational games, with the target audience of children from 5 to 9\nyears old. A list of basic requirements that can improve the development of\neducational games and provide a basis for further development of a game is\nraised. It is possible to continue the work by addressing theories of\neducation, the process of gamification of education and other requirements\ngathering methods.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 01:40:35 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Pimentel", "Clara Andrade", ""], ["Melo", "Philipe de Freitas", ""], ["Bergamo", "Mar\u00edlia Lyra", ""]]}, {"id": "1909.09942", "submitter": "Shujun Li", "authors": "Yang Lu, Shujun Li, Athina Ioannou and Iis Tussyadiah", "title": "From Data Disclosure to Privacy Nudges: A Privacy-aware and User-centric\n  Personal Data Management Framework", "comments": "18 pages, 6 figures, accepted to DependSys 2019 (5th International\n  Conference on Dependability in Sensor, Cloud, and Big Data Systems and\n  Applications), to be held from November 12-15, 2019 in Guangzhou, China, to\n  be published in a volume of Communications in Computer and Information\n  Science by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although there are privacy-enhancing tools designed to protect users' online\nprivacy, it is surprising to see a lack of user-centric solutions allowing\nprivacy control based on the joint assessment of privacy risks and benefits,\ndue to data disclosure to multiple platforms. In this paper, we propose a\nconceptual framework to fill the gap: aiming at the user-centric privacy\nprotection, we show the framework can not only assess privacy risks in using\nonline services but also the added values earned from data disclosure. Through\nfollowing a human-in-the-loop approach, it is expected the framework provides a\npersonalized solution via preference learning, continuous privacy assessment,\nbehavior monitoring and nudging. Finally, we describe a case study towards\n\"leisure travelers\" and several future areas to be studied in the ongoing\nproject.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 05:55:56 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Lu", "Yang", ""], ["Li", "Shujun", ""], ["Ioannou", "Athina", ""], ["Tussyadiah", "Iis", ""]]}, {"id": "1909.10173", "submitter": "Jieqiong Zhao", "authors": "Jieqiong Zhao, Morteza Karimzadeh, Hanye Xu, Abish Malik, Shehzad\n  Afzal, Guizhen Wang, Niklas Elmqvist, David S. Ebert", "title": "Route Packing: Geospatially-Accurate Visualization of Route Networks", "comments": "10 pages, 11 figures, 2 tables, The 53rd Hawaii International\n  Conference on System Sciences (HICSS-53)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present route packing, a novel (geo)visualization technique for displaying\nseveral routes simultaneously on a geographic map while preserving the\ngeospatial layout, identity, directionality, and volume of individual routes.\nThe technique collects variable-width route lines side by side while minimizing\ncrossings, encodes them with categorical colors, and decorates them with glyphs\nto show their directions. Furthermore, nodes representing sources and sinks use\nglyphs to indicate whether routes stop at the node or merely pass through it.\nWe conducted a crowd-sourced user study investigating route tracing performance\nwith road networks visualized using our route packing technique. Our findings\nhighlight the visual parameters under which the technique yields optimal\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 06:03:33 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Zhao", "Jieqiong", ""], ["Karimzadeh", "Morteza", ""], ["Xu", "Hanye", ""], ["Malik", "Abish", ""], ["Afzal", "Shehzad", ""], ["Wang", "Guizhen", ""], ["Elmqvist", "Niklas", ""], ["Ebert", "David S.", ""]]}, {"id": "1909.10305", "submitter": "Alice Othmani", "authors": "Amine Djerghri, Ahmed Rachid Hazourli, Alice Othmani", "title": "Deep Multi-Facial patches Aggregation Network for Expression\n  Classification from Face Images", "comments": "we have a new version of the paper arXiv:2002.09298", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Emotional Intelligence in Human-Computer Interaction has attracted increasing\nattention from researchers in multidisciplinary research fields including\npsychology, computer vision, neuroscience, artificial intelligence, and related\ndisciplines. Human prone to naturally interact with computers face-to-face.\nHuman Expressions is an important key to better link human and computers. Thus,\ndesigning interfaces able to understand human expressions and emotions can\nimprove Human-Computer Interaction (HCI) for better communication. In this\npaper, we investigate HCI via a deep multi-facial patches aggregation network\nfor Face Expression Recognition (FER). Deep features are extracted from facial\nparts and aggregated for expression classification. Several problems may affect\nthe performance of the proposed framework like the small size of FER datasets\nand the high number of parameters to learn. For That, two data augmentation\ntechniques are proposed for facial expression generation to expand the labeled\ntraining. The proposed framework is evaluated on the extended Cohn-Konade\ndataset (CK+) and promising results are achieved.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:52:27 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 11:52:19 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Djerghri", "Amine", ""], ["Hazourli", "Ahmed Rachid", ""], ["Othmani", "Alice", ""]]}, {"id": "1909.10371", "submitter": "Jessica Rivera Villicana", "authors": "Nic Velissaris and Jessica Rivera-Villicana", "title": "Towards Intelligent Interactive Theatre: Drama Management as a way of\n  Handling Performance", "comments": "International Conference on Interactive Digital Storytelling (ICIDS)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new modality for intelligent interactive\nnarratives within the theatre domain. We discuss the possibilities of using an\nintelligent agent that serves as a drama manager and as an actor that plays a\ncharacter within the live theatre experience. We pose a set of research\nchallenges that arise from our analysis towards the implementation of such an\nagent, as well as potential methodologies as a starting point to bridge the\ngaps between current literature and the proposed modality.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 13:59:35 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Velissaris", "Nic", ""], ["Rivera-Villicana", "Jessica", ""]]}, {"id": "1909.10414", "submitter": "Jessica Rivera Villicana", "authors": "Jessica Rivera-Villicana, Fabio Zambetta, James Harland, Marsha Berry", "title": "Informing a BDI Player Model for an Interactive Narrative", "comments": "CHI Play 2018", "journal-ref": "Proceedings of the 2018 Annual Symposium on Computer-Human\n  Interaction in Play (CHI PLAY '18). ACM, New York, NY, USA, 417-428. DOI:\n  https://doi.org/10.1145/3242671.3242700", "doi": "10.1145/3242671.3242700", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on studying players behaviour in interactive narratives\nwith the aim to simulate their choices. Besides sub-optimal player behaviour\ndue to limited knowledge about the environment, the difference in each player's\nstyle and preferences represents a challenge when trying to make an intelligent\nsystem mimic their actions. Based on observations from players interactions\nwith an extract from the interactive fiction Anchorhead, we created a player\nprofile to guide the behaviour of a generic player model based on the BDI\n(Belief-Desire-Intention) model of agency. We evaluated our approach using\nqualitative and quantitative methods and found that the player profile can\nimprove the performance of the BDI player model. However, we found that players\nself-assessment did not yield accurate data to populate their player profile\nunder our current approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 15:12:45 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Rivera-Villicana", "Jessica", ""], ["Zambetta", "Fabio", ""], ["Harland", "James", ""], ["Berry", "Marsha", ""]]}, {"id": "1909.10513", "submitter": "Prayitno Prayitno", "authors": "Eko Prasetyo, Prayitno, Jing-Doo Wang, Karisma Trinanda Putra", "title": "Visualization and Travel Time Extraction System for the Statistics of\n  TDCS Travel using MapReduce Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, extracting some information as a knowledge from big data is very\nchallenging activity. The size of data is very huge and it requires some\nspecial techniques and adequate processing hardware. It is also applied in\nvehicles transportation data at Taiwan National Freeway from the Traffic Data\nCollection System (TDCS). The results of this extraction will be very useful if\nit can be used by the community. So that the delivery of information extracted\nfrom large data that is easily understood becomes a necessary thing.\nPresentation of results using images / visuals will make it easier for people\nto interpret the information provided. In this project, an interactive\nvisualization of the results of extracting statistical information is attempted\nto be provided. The results can be used by users to support the decision making\nof road users in determining the appropriate time when going through the road\npieces around the Taichung City. This visualization of the statistics will help\npeople who want to predict the travel time around Taichung City.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 08:35:49 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Prasetyo", "Eko", ""], ["Prayitno", "", ""], ["Wang", "Jing-Doo", ""], ["Putra", "Karisma Trinanda", ""]]}, {"id": "1909.10567", "submitter": "Jiachen Xu", "authors": "Jiachen Xu, Moritz Grosse-Wentrup, Vinay Jayaram", "title": "Tangent space spatial filters for interpretable and efficient Riemannian\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods based on Riemannian geometry have proven themselves to be good models\nfor decoding in brain-computer interfacing (BCI). However, one major drawback\nof these methods is that it is not possible to determine what aspect of the\nsignal the classifier is built on, leaving open the possibility that artifacts\ndrive classification performance. In areas where artifactual control is\nproblematic, specifically neurofeedback and BCIs in patient populations, this\nhas led people to continue to rely on spatial filters as a way of generating\nfeatures that are provably brain-related. Furthermore, these methods also\nsuffer from the curse of dimensionality and are almost infeasible in\nhigh-density online BCI systems. To tackle these drawbacks, we introduce here a\nmethod for computing spatial filters from any linear function in the Riemannian\ntangent space, which allows for more efficient classification as well as the\nremoval of artifact sources from classifiers built on Riemannian methods. We\nfirst prove a fundamental relationship between certain tangent spaces and\nspatial filtering methods, including an explanation of common spatial patterns\nwithin this framework, and then validate our proposed approach using an\nopen-access BCI analysis framework.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 18:36:53 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Xu", "Jiachen", ""], ["Grosse-Wentrup", "Moritz", ""], ["Jayaram", "Vinay", ""]]}, {"id": "1909.10614", "submitter": "Shiwali Mohan", "authors": "Shiwali Mohan, Hesham Rakha, Matthew Klenk", "title": "Acceptable Planning: Influencing Individual Behavior to Reduce\n  Transportation Energy Expenditure of a City", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research 66 (2019) 555-587", "doi": "10.1613/jair.1.11352", "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our research aims at developing intelligent systems to reduce the\ntransportation-related energy expenditure of a large city by influencing\nindividual behavior. We introduce COPTER - an intelligent travel assistant that\nevaluates multi-modal travel alternatives to find a plan that is acceptable to\na person given their context and preferences. We propose a formulation for\nacceptable planning that brings together ideas from AI, machine learning, and\neconomics. This formulation has been incorporated in COPTER that produces\nacceptable plans in real-time. We adopt a novel empirical evaluation framework\nthat combines human decision data with a high fidelity multi-modal\ntransportation simulation to demonstrate a 4\\% energy reduction and 20\\% delay\nreduction in a realistic deployment scenario in Los Angeles, California, USA.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 20:55:18 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Mohan", "Shiwali", ""], ["Rakha", "Hesham", ""], ["Klenk", "Matthew", ""]]}, {"id": "1909.10823", "submitter": "Patr\\'icia Alves-Oliveira", "authors": "Patr\\'icia Alves-Oliveira, Samuel Gomes, Ankita Chandak, Patr\\'icia\n  Arriaga, Guy Hoffman, Ana Paiva", "title": "Software architecture for YOLO, a creativity-stimulating robot", "comments": "17 pages, 7 figurs, 2 tables, open-source code, submitted to\n  SoftwareX journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  YOLO is a social robot designed and developed to stimulate creativity in\nchildren through storytelling activities. Children use it as a character in\ntheir stories. This article details the artificial intelligence software\ndeveloped for YOLO. The implemented software schedules through several\nCreativity Behaviors to find the ones that stimulate creativity more\neffectively. YOLO can choose between convergent and divergent thinking\ntechniques, two important processes of creative thought. These techniques were\ndeveloped based on the psychological theories of creativity development and on\nresearch from creativity experts who work with children. Additionally, this\nsoftware allows the creation of Social Behaviors that enable the robot to\nbehave as a believable character. On top of our framework, we built 3 main\nsocial behavior parameters: Exuberant, Aloof, and Harmonious. These behaviors\nare meant to ease immersive play and the process of character creation. The 3\nsocial behaviors were based on psychological theories of personality and\ndeveloped using children's input during co-design studies. Overall, this work\npresents an attempt to design, develop, and deploy social robots that nurture\nintrinsic human abilities, such as the ability to be creative.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 11:40:05 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Alves-Oliveira", "Patr\u00edcia", ""], ["Gomes", "Samuel", ""], ["Chandak", "Ankita", ""], ["Arriaga", "Patr\u00edcia", ""], ["Hoffman", "Guy", ""], ["Paiva", "Ana", ""]]}, {"id": "1909.10851", "submitter": "Huy Kang Kim", "authors": "Kyoung Ho Kim and Huy Kang Kim", "title": "Oldie is Goodie: Effective User Retention by In-game Promotion Event\n  Analysis", "comments": "6 pages, 7 figures, 5 tables; This paper is an author version of the\n  same title paper appeared in the proceedings on ACM CHIPLAY 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sustainable growth and profitability, online game companies are\nconstantly carrying out various events to attract new game users, to maximize\nreturn users, and to minimize churn users in online games. Because minimizing\nchurn users is the most cost-effective method, many pieces of research are\nbeing conducted on ways to predict and to prevent churns in advance. However,\nthere is still little research on the validity of event effects. In this study,\nwe investigate whether game events influence the user churn rate and confirm\nthe difference in how game users respond to events by character level, item\npurchasing frequency and game-playing time band.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:53:03 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Kim", "Kyoung Ho", ""], ["Kim", "Huy Kang", ""]]}, {"id": "1909.10966", "submitter": "Binny Mathew", "authors": "Binny Mathew, Anurag Illendula, Punyajoy Saha, Soumya Sarkar, Pawan\n  Goyal, Animesh Mukherjee", "title": "Hate begets Hate: A Temporal Study of Hate Speech", "comments": "24 pages, 14 figures, 1 table. Accepted at CSCW 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the ongoing debate on 'freedom of speech' vs. 'hate speech' there is an\nurgent need to carefully understand the consequences of the inevitable\nculmination of the two, i.e., 'freedom of hate speech' over time. An ideal\nscenario to understand this would be to observe the effects of hate speech in\nan (almost) unrestricted environment. Hence, we perform the first temporal\nanalysis of hate speech on Gab.com, a social media site with very loose\nmoderation policy. We first generate temporal snapshots of Gab from millions of\nposts and users. Using these temporal snapshots, we compute an activity vector\nbased on DeGroot model to identify hateful users. The amount of hate speech in\nGab is steadily increasing and the new users are becoming hateful at an\nincreased and faster rate. Further, our analysis analysis reveals that the hate\nusers are occupying the prominent positions in the Gab network. Also, the\nlanguage used by the community as a whole seem to correlate more with that of\nthe hateful users as compared to the non-hateful ones. We discuss how, many\ncrucial design questions in CSCW open up from our work.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 14:48:01 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 17:32:44 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Mathew", "Binny", ""], ["Illendula", "Anurag", ""], ["Saha", "Punyajoy", ""], ["Sarkar", "Soumya", ""], ["Goyal", "Pawan", ""], ["Mukherjee", "Animesh", ""]]}, {"id": "1909.11004", "submitter": "Mehdi Ghayoumi", "authors": "Mehdi Ghayoumi, Maryam Pourebadi", "title": "Fuzzy Knowledge-Based Architecture for Learning and Interaction in\n  Social Robots", "comments": "7 pages, AI-HRI 2019", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/21", "categories": "cs.RO cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an extension of our presented cognitive-based\nemotion model [27][28]and [30], where we enhance our knowledge-based emotion\nunit of the architecture by embedding a fuzzy rule-based system to it. The\nmodel utilizes the cognitive parameters dependency and their corresponding\nweights to regulate the robot's behavior and fuse their behavior data to\nachieve the final decision in their interaction with the environment. Using\nthis fuzzy system, our previous model can simulate linguistic parameters for\nbetter controlling and generating understandable and flexible behaviors in the\nrobots. We implement our model on an assistive healthcare robot, named Robot\nNurse Assistant (RNA) and test it with human subjects. Our model records all\nthe emotion states and essential information based on its predefined rules and\nlearning system. Our results show that our robot interacts with patients in a\nreasonable, faithful way in special conditions which are defined by rules. This\nwork has the potential to provide better on-demand service for clinical experts\nto monitor the patients' emotion states and help them make better decisions\naccordingly.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 07:15:37 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Ghayoumi", "Mehdi", ""], ["Pourebadi", "Maryam", ""]]}, {"id": "1909.11202", "submitter": "Christopher Collins", "authors": "Brandon Laughlin, Christopher Collins, Karthik Sankaranarayanan,\n  Khalil El-Khatib", "title": "A Visual Analytics Framework for Adversarial Text Generation", "comments": null, "journal-ref": "2019 IEEE Symposium on Visualization for Cyber Security (VizSec)", "doi": "10.1109/VizSec48167.2019.9161563", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework which enables a user to more easily make\ncorrections to adversarial texts. While attack algorithms have been\ndemonstrated to automatically build adversaries, changes made by the algorithms\ncan often have poor semantics or syntax. Our framework is designed to\nfacilitate human intervention by aiding users in making corrections. The\nframework extends existing attack algorithms to work within an evolutionary\nattack process paired with a visual analytics loop. Using an interactive\ndashboard a user is able to review the generation process in real time and\nreceive suggestions from the system for edits to be made. The adversaries can\nbe used to both diagnose robustness issues within a single classifier or to\ncompare various classifier options. With the weaknesses identified, the\nframework can also be used as a first step in mitigating adversarial threats.\nThe framework can be used as part of further research into defense methods in\nwhich the adversarial examples are used to evaluate new countermeasures. We\ndemonstrate the framework with a word swapping attack for the task of sentiment\nclassification.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 21:56:53 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Laughlin", "Brandon", ""], ["Collins", "Christopher", ""], ["Sankaranarayanan", "Karthik", ""], ["El-Khatib", "Khalil", ""]]}, {"id": "1909.11233", "submitter": "Jalal Mahmud", "authors": "Yash Bhalgat, Zhe Liu, Pritam Gundecha, Jalal Mahmud, Amita Misra", "title": "Teacher-Student Learning Paradigm for Tri-training: An Efficient Method\n  for Unlabeled Data Exploitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given that labeled data is expensive to obtain in real-world scenarios, many\nsemi-supervised algorithms have explored the task of exploitation of unlabeled\ndata. Traditional tri-training algorithm and tri-training with disagreement\nhave shown promise in tasks where labeled data is limited. In this work, we\nintroduce a new paradigm for tri-training, mimicking the real world\nteacher-student learning process. We show that the adaptive teacher-student\nthresholds used in the proposed method provide more control over the learning\nprocess with higher label quality. We perform evaluation on SemEval sentiment\nanalysis task and provide comprehensive comparisons over experimental settings\ncontaining varied labeled versus unlabeled data rates. Experimental results\nshow that our method outperforms other strong semi-supervised baselines, while\nrequiring less number of labeled training samples.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 00:09:56 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Bhalgat", "Yash", ""], ["Liu", "Zhe", ""], ["Gundecha", "Pritam", ""], ["Mahmud", "Jalal", ""], ["Misra", "Amita", ""]]}, {"id": "1909.11248", "submitter": "John Gideon", "authors": "John Gideon, Katie Matton, Steve Anderau, Melvin G McInnis, Emily\n  Mower Provost", "title": "When to Intervene: Detecting Abnormal Mood using Everyday Smartphone\n  Conversations", "comments": "Submitted to IEEE Transactions on Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipolar disorder (BPD) is a chronic mental illness characterized by extreme\nmood and energy changes from mania to depression. These changes drive behaviors\nthat often lead to devastating personal or social consequences. BPD is managed\nclinically with regular interactions with care providers, who assess mood,\nenergy levels, and the form and content of speech. Recent work has proposed\nsmartphones for monitoring mood using speech. However, these works do not\npredict when to intervene. Predicting when to intervene is challenging because\nthere is not a single measure that is relevant for every person: different\nindividuals may have different levels of symptom severity considered typical.\nAdditionally, this typical mood, or baseline, may change over time, making a\nsingle symptom threshold insufficient. This work presents an innovative\napproach that expands clinical mood monitoring to predict when interventions\nare necessary using an anomaly detection framework, which we call Temporal\nNormalization. We first validate the model using a dataset annotated for\nclinical interventions and then incorporate this method in a deep learning\nframework to predict mood anomalies from natural, unstructured, telephone\nspeech data. The combination of these approaches provides a framework to enable\nreal-world speech-focused mood monitoring.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 01:14:05 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 01:11:49 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Gideon", "John", ""], ["Matton", "Katie", ""], ["Anderau", "Steve", ""], ["McInnis", "Melvin G", ""], ["Provost", "Emily Mower", ""]]}, {"id": "1909.11456", "submitter": "Dongrui Wu", "authors": "Yuqi Cuui and Yifan Xu and Dongrui Wu", "title": "EEG-Based Driver Drowsiness Estimation Using Feature Weighted Episodic\n  Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drowsy driving is pervasive, and also a major cause of traffic accidents.\nEstimating a driver's drowsiness level by monitoring the electroencephalogram\n(EEG) signal and taking preventative actions accordingly may improve driving\nsafety. However, individual differences among different drivers make this task\nvery challenging. A calibration session is usually required to collect some\nsubject-specific data and tune the model parameters before applying it to a new\nsubject, which is very inconvenient and not user-friendly. Many approaches have\nbeen proposed to reduce the calibration effort, but few can completely\neliminate it. This paper proposes a novel approach, feature weighted episodic\ntraining (FWET), to completely eliminate the calibration requirement. It\nintegrates two techniques: feature weighting to learn the importance of\ndifferent features, and episodic training for domain generalization.\nExperiments on EEG-based driver drowsiness estimation demonstrated that both\nfeature weighting and episodic training are effective, and their integration\ncan further improve the generalization performance. FWET does not need any\nlabelled or unlabelled calibration data from the new subject, and hence could\nbe very useful in plug-and-play brain-computer interfaces.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 12:52:02 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Cuui", "Yuqi", ""], ["Xu", "Yifan", ""], ["Wu", "Dongrui", ""]]}, {"id": "1909.11752", "submitter": "Kieran Woodward Mr", "authors": "Kieran Woodward, Eiman Kanjo, David Brown", "title": "Challenges of Designing and Developing Tangible Interfaces for Mental\n  Well-being", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/17", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental well-being technologies possess many qualities that give them the\npotential to help people receive assessment and treatment who may otherwise not\nreceive help due to fear of stigma or lack of resources. The combination of\nadvances in sensors, microcontrollers and machine learning is leading to the\nemergence of dedicated tangible interfaces to monitor and promote positive\nmental well-being. However, there are key technical, ergonomic and aesthetic\nchallenges to be overcome in order to make these interfaces effective and\nrespond to users' needs. In this paper, the barriers to develop mental\nwell-being tangible interfaces are discussed by identifying and examining the\nrecent technological challenges machine learning, sensors, microcontrollers and\nbatteries create.User-oriented challenges that face the development of mental\nwell-being technologies are then considered ranging from user engagement during\nco-design and trials to ethical and privacy concerns.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 09:03:58 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Woodward", "Kieran", ""], ["Kanjo", "Eiman", ""], ["Brown", "David", ""]]}, {"id": "1909.11754", "submitter": "Joel Fischer", "authors": "Gustavo Berumen, Joel E. Fischer, Anthony Brown, Martin Baumers", "title": "Finding Design Opportunities for Smartness in Consumer Packaged Goods", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:1906.06089)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/15", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This study attempts to understand the use of Consumer Packaged Goods (CPG) in\npractice to obtain insights to develop design interventions that bring the CPGs\ninto the Internet of Things. Our ultimate aim is to equip CPGs with a layer of\nsmartness so that CPGs could collect information about their use and provide\nextra services and functionalities. With a practice perspective we developed an\nassemblage of methods to analyze and represent how people use CPGs. We chose\ncooking as our practice case and use an auto-ethnographic data sample to\ndemonstrate the application of our methods. Despite the early stage of our\nstudy, our methods provide ways to get an understanding of how CPGs are used in\npractice and an opening to establish opportunities for design interventions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 09:47:30 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Berumen", "Gustavo", ""], ["Fischer", "Joel E.", ""], ["Brown", "Anthony", ""], ["Baumers", "Martin", ""]]}, {"id": "1909.11765", "submitter": "Zitao Liu", "authors": "Jiahao Chen, Hang Li, Wenxin Wang, Wenbiao Ding, Gale Yan Huang, Zitao\n  Liu", "title": "A Multimodal Alerting System for Online Class Quality Assurance", "comments": "The 20th International Conference on Artificial Intelligence in\n  Education(AIED), 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online 1 on 1 class is created for more personalized learning experience. It\ndemands a large number of teaching resources, which are scarce in China. To\nalleviate this problem, we build a platform (marketplace), i.e., \\emph{Dahai}\nto allow college students from top Chinese universities to register as\npart-time instructors for the online 1 on 1 classes. To warn the unqualified\ninstructors and ensure the overall education quality, we build a monitoring and\nalerting system by utilizing multimodal information from the online\nenvironment. Our system mainly consists of two key components: banned word\ndetector and class quality predictor. The system performance is demonstrated\nboth offline and online. By conducting experimental evaluation of real-world\nonline courses, we are able to achieve 74.3\\% alerting accuracy in our\nproduction environment.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 10:23:44 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Chen", "Jiahao", ""], ["Li", "Hang", ""], ["Wang", "Wenxin", ""], ["Ding", "Wenbiao", ""], ["Huang", "Gale Yan", ""], ["Liu", "Zitao", ""]]}, {"id": "1909.11766", "submitter": "Lionel Robert", "authors": "Qiaoning Zhang, Connor Esterwood, X. Jessie Yang, Lionel P. Robert Jr", "title": "An Automated Vehicle (AV) like Me? The Impact of Personality\n  Similarities and Differences between Humans and AVs", "comments": "4 pages, 2 figures, 2019 AAAI Fall Symposium on Artificial\n  Intelligence for Human-Robot Interaction", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2019/01", "categories": "cs.HC cs.AI cs.CY cs.RO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  To better understand the impacts of similarities and dissimilarities in human\nand AV personalities we conducted an experimental study with 443 individuals.\nGenerally, similarities in human and AV personalities led to a higher\nperception of AV safety only when both were high in specific personality\ntraits. Dissimilarities in human and AV personalities also yielded a higher\nperception of AV safety, but only when the AV was higher than the human in a\nparticular personality trait.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:51:28 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zhang", "Qiaoning", ""], ["Esterwood", "Connor", ""], ["Yang", "X. Jessie", ""], ["Robert", "Lionel P.", "Jr"]]}, {"id": "1909.11767", "submitter": "Awais Jumani Khan", "authors": "Awais Khan Jumani, Anware Ali Sanjrani, Fida Hussain Khoso, Mashooque\n  Ahmed Memon, Mumtaz Hussain Mahar, Vishal Kumar", "title": "Generic Framework of Knowledge-Based Learning: Designing and Deploying\n  of Web Application", "comments": null, "journal-ref": "ICST Institute for Computer Sciences, Social Informatics and\n  Telecommunications Engineering 2019 Published by Springer Nature Switzerland", "doi": "10.1007/978-3-030-23943-5_20", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning technology was used as standalone software to install in a\nparticular system, which needs to buy learning software of a particular\nsubject. It was costly and difficult to search CD/DVD of the particular program\nin the market. Nowadays the trend of learning is changed and people are\nlearning via the internet and it is known as Electronic Learning (E-learning).\nSeveral e-learning web applications are available which are providing more\nstuff about students and it fulfills requirements. The aim of this paper is to\npresent a well structured, user-friendly framework with the web application for\ne-learning, which does not need any subscription. The experiment was conducted\nwith 691 students and teachers, the result shows 91.98% of participants were\nsatisfied with the proposed E-learning system.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 12:39:36 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Jumani", "Awais Khan", ""], ["Sanjrani", "Anware Ali", ""], ["Khoso", "Fida Hussain", ""], ["Memon", "Mashooque Ahmed", ""], ["Mahar", "Mumtaz Hussain", ""], ["Kumar", "Vishal", ""]]}, {"id": "1909.11869", "submitter": "Joshua Kroll", "authors": "Deirdre K. Mulligan, Joshua A. Kroll, Nitin Kohli, Richmond Y. Wong", "title": "This Thing Called Fairness: Disciplinary Confusion Realizing a Value in\n  Technology", "comments": "36 pages", "journal-ref": "Proc. ACM Hum.-Comput. Interact. 3, CSCW, Article 119 (November\n  2019)", "doi": "10.1145/3359221", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion in the use of software in important sociotechnical systems has\nrenewed focus on the study of the way technical constructs reflect policies,\nnorms, and human values. This effort requires the engagement of scholars and\npractitioners from many disciplines. And yet, these disciplines often\nconceptualize the operative values very differently while referring to them\nusing the same vocabulary. The resulting conflation of ideas confuses\ndiscussions about values in technology at disciplinary boundaries. In the\nservice of improving this situation, this paper examines the value of shared\nvocabularies, analytics, and other tools that facilitate conversations about\nvalues in light of these disciplinary specific conceptualizations, the role\nsuch tools play in furthering research and practice, outlines different\nconceptions of \"fairness\" deployed in discussions about computer systems, and\nprovides an analytic tool for interdisciplinary discussions and collaborations\naround the concept of fairness. We use a case study of risk assessments in\ncriminal justice applications to both motivate our effort--describing how\nconflation of different concepts under the banner of \"fairness\" led to\nunproductive confusion--and illustrate the value of the fairness analytic by\ndemonstrating how the rigorous analysis it enables can assist in identifying\nkey areas of theoretical, political, and practical misunderstanding or\ndisagreement, and where desired support alignment or collaboration in the\nabsence of consensus.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 03:55:20 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Mulligan", "Deirdre K.", ""], ["Kroll", "Joshua A.", ""], ["Kohli", "Nitin", ""], ["Wong", "Richmond Y.", ""]]}, {"id": "1909.11980", "submitter": "Lina Rojas-Barahona", "authors": "Lina M. Rojas-Barahona, Pascal Bellec, Benoit Besset, Martinho\n  Dos-Santos, Johannes Heinecke, Munshi Asadullah, Olivier Le-Blouch, Jean Y.\n  Lancien, G\\'eraldine Damnati, Emmanuel Mory and Fr\\'ed\\'eric Herledan", "title": "Spoken Conversational Search for General Knowledge", "comments": "SIGDial2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a spoken conversational question answering proof of concept that\nis able to answer questions about general knowledge from Wikidata. The dialogue\ncomponent does not only orchestrate various components but also solve\ncoreferences and ellipsis.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 08:46:02 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Rojas-Barahona", "Lina M.", ""], ["Bellec", "Pascal", ""], ["Besset", "Benoit", ""], ["Dos-Santos", "Martinho", ""], ["Heinecke", "Johannes", ""], ["Asadullah", "Munshi", ""], ["Le-Blouch", "Olivier", ""], ["Lancien", "Jean Y.", ""], ["Damnati", "G\u00e9raldine", ""], ["Mory", "Emmanuel", ""], ["Herledan", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1909.12281", "submitter": "Will Crichton", "authors": "Will Crichton", "title": "Human-Centric Program Synthesis", "comments": "To appear at PLATEAU'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis techniques offer significant new capabilities in searching\nfor programs that satisfy high-level specifications. While synthesis has been\nthoroughly explored for input/output pair specifications\n(programming-by-example), this paper asks: what does program synthesis look\nlike beyond examples? What actual issues in day-to-day development would stand\nto benefit the most from synthesis? How can a human-centric perspective inform\nthe exploration of alternative specification languages for synthesis? I sketch\na human-centric vision for program synthesis where programmers explore and\nlearn languages and APIs aided by a synthesis tool.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 17:44:06 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Crichton", "Will", ""]]}, {"id": "1909.12622", "submitter": "Jan Ka{\\ss}el", "authors": "Maryam Foradi, Jan Ka{\\ss}el, Johannes Pein, Gregory R. Crane", "title": "Multi-Modal Citizen Science: From Disambiguation to Transcription of\n  Classical Literature", "comments": null, "journal-ref": "Proceedings of the 30th ACM Conference on Hypertext and Social\n  Media - HT 2019, 49-53. Hof, Germany: ACM Press", "doi": "10.1145/3342220.3343667", "report-no": null, "categories": "cs.CL cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The engagement of citizens in the research projects, including Digital\nHumanities projects, has risen in prominence in recent years. This type of\nengagement not only leads to incidental learning of participants but also\nindicates the added value of corpus enrichment via different types of\nannotations undertaken by users generating so-called smart texts. Our work\nfocuses on the continuous task of adding new layers of annotation to Classical\nLiterature. We aim to provide more extensive tools for readers of smart texts,\nenhancing their reading comprehension and at the same time empowering the\nlanguage learning by introducing intellectual tasks, i.e., linking, tagging,\nand disambiguation. The current study adds a new mode of annotation-audio\nannotations-to the extensively annotated corpus of poetry by the Persian poet\nHafiz. By proposing tasks with three different difficulty levels, we estimate\nthe users' ability of providing correct annotations in order to rate their\nanswers in further stages of the project, where no ground truth data is\navailable. While proficiency in Persian is beneficial, annotators with no\nknowledge of Persian are also able to add annotations to the corpus.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 11:19:21 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Foradi", "Maryam", ""], ["Ka\u00dfel", "Jan", ""], ["Pein", "Johannes", ""], ["Crane", "Gregory R.", ""]]}, {"id": "1909.12932", "submitter": "Benjamin Renoust", "authors": "Benjamin Renoust, Matheus Oliveira Franca, Jacob Chan, Van Le, Ayaka\n  Uesaka, Yuta Nakashima, Hajime Nagahara, Jueren Wang, Yutaka Fujioka", "title": "BUDA.ART: A Multimodal Content-Based Analysis and Retrieval System for\n  Buddha Statues", "comments": "Demo video at: https://www.youtube.com/watch?v=3XJvLjSWieY", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce BUDA.ART, a system designed to assist researchers in Art\nHistory, to explore and analyze an archive of pictures of Buddha statues. The\nsystem combines different CBIR and classical retrieval techniques to assemble\n2D pictures, 3D statue scans and meta-data, that is focused on the Buddha\nfacial characteristics. We build the system from an archive of 50,000 Buddhism\npictures, identify unique Buddha statues, extract contextual information, and\nprovide specific facial embedding to first index the archive. The system allows\nfor mobile, on-site search, and to explore similarities of statues in the\narchive. In addition, we provide search visualization and 3D analysis of the\nstatues\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 06:35:24 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Renoust", "Benjamin", ""], ["Franca", "Matheus Oliveira", ""], ["Chan", "Jacob", ""], ["Le", "Van", ""], ["Uesaka", "Ayaka", ""], ["Nakashima", "Yuta", ""], ["Nagahara", "Hajime", ""], ["Wang", "Jueren", ""], ["Fujioka", "Yutaka", ""]]}, {"id": "1909.12969", "submitter": "Matthew Olson", "authors": "Matthew L. Olson, Lawrence Neal, Fuxin Li, Weng-Keen Wong", "title": "Counterfactual States for Atari Agents via Generative Deep Learning", "comments": "IJCAI XAI Workshop 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep reinforcement learning agents have produced impressive results\nin many domains, their decision making is difficult to explain to humans. To\naddress this problem, past work has mainly focused on explaining why an action\nwas chosen in a given state. A different type of explanation that is useful is\na counterfactual, which deals with \"what if?\" scenarios. In this work, we\nintroduce the concept of a counterfactual state to help humans gain a better\nunderstanding of what would need to change (minimally) in an Atari game image\nfor the agent to choose a different action. We introduce a novel method to\ncreate counterfactual states from a generative deep learning architecture. In\naddition, we evaluate the effectiveness of counterfactual states on human\nparticipants who are not machine learning experts. Our user study results\nsuggest that our generated counterfactual states are useful in helping\nnon-expert participants gain a better understanding of an agent's decision\nmaking process.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 21:55:01 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Olson", "Matthew L.", ""], ["Neal", "Lawrence", ""], ["Li", "Fuxin", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "1909.13256", "submitter": "Jason R.C. Nurse Dr", "authors": "Maria Bada and Jason R. C. Nurse", "title": "The Social and Psychological Impact of Cyber-Attacks", "comments": "21 pages", "journal-ref": "Benson, Vladlena and McAlaney, John, eds. (2019). Emerging Cyber\n  Threats and Cognitive Vulnerabilities. pp. 73-92", "doi": "10.1016/B978-0-12-816203-3.00004-6", "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-attacks have become as commonplace as the Internet itself. Each year,\nindustry reports, media outlets and academic articles highlight this increased\nprevalence, spanning both the amount and variety of attacks and cybercrimes. In\nthis article, we seek to further advance discussions on cyber threats,\ncognitive vulnerabilities and cyberpsychology through a critical reflection on\nthe social and psychological aspects related to cyber-attacks. In particular,\nwe are interested in understanding how members of the public perceive and\nengage with risk and how they are impacted during and after a cyber-attack has\noccurred. This research focuses on key cognitive issues relevant to\ncomprehending public reactions to malicious cyber events including risk\nperception, protection motivation, culture, and attacker characteristics (e.g.,\nattacker identity, target identity and scale of attack). To consider the\napplicability of our findings, we investigate two significant cyber-attacks\nover the last few years, namely the WannaCry attack of 2017 and the Lloyds\nBanking Group attack in the same year.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 11:11:57 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bada", "Maria", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "1909.13443", "submitter": "Nalin Chhibber", "authors": "Nalin Chhibber, Edith Law", "title": "Using Conversational Agents To Support Learning By Teaching", "comments": "7 pages, 2 figures, Presented at Conversational Agents Workshop, CHI\n  2019 (Glasgow, UK)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents are becoming increasingly popular for supporting and\nfacilitating learning. Conventional pedagogical agents are designed to play the\nrole of human teachers by giving instructions to the students. In this paper,\nwe investigate the use of conversational agents to support the\n'learning-by-teaching' paradigm where the agent receives instructions from\nstudents. In particular, we introduce Curiosity Notebook: an educational\napplication that harvests conversational interventions to facilitate students'\nlearning. Recognizing such interventions can not only help in engaging students\nwithin learning interactions, but also provide a deeper insight into the\nintricacies involved in designing conversational agents for educational\npurposes.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 03:40:59 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chhibber", "Nalin", ""], ["Law", "Edith", ""]]}, {"id": "1909.13775", "submitter": "Vincent Goudard", "authors": "Vincent Goudard (SU)", "title": "Ephemeral instruments", "comments": "New Interfaces for Musical Expression, Jun 2019, Porto-Alegre, Brazil", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article questions the notion of ephemerality of digital musical\ninstruments (DMI). Longevity is generally regarded as a valuable quality that\ngood design criteria should help to achieve. However, the nature of the tools,\nof the performance conditions and of the music itself may lead to think of\nephemerality as an intrinsic modality of the existence of DMIs. In particular,\nthe conditions of contemporary musical production suggest that contextual\nadaptations of instrumental devices beyond the monolithic unity of classical\ninstruments should be considered. The first two parts of this article analyse\nvarious reasons to reassess the issue of longevity and ephemerality. The last\ntwo sections attempt to propose an articulation of these two aspects to inform\nboth the design of the DMI and their learning.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 09:02:34 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Goudard", "Vincent", "", "SU"]]}, {"id": "1909.13875", "submitter": "Tiago Ribeiro", "authors": "Tiago Ribeiro and Ana Paiva", "title": "Expressive Inverse Kinematics Solving in Real-time for Virtual and\n  Robotic Interactive Characters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With new advancements in interaction techniques, character animation also\nrequires new methods, to support fields such as robotics, and VR/AR.\nInteractive characters in such fields are becoming driven by AI which opens up\nthe possibility of non-linear and open-ended narratives that may even include\ninteraction with the real, physical world. This paper presents and describes\nERIK, an expressive inverse kinematics technique aimed at such applications.\nOur technique allows an arbitrary kinematic chain, such as an arm, snake, or\nrobotic manipulator, to exhibit an expressive posture while aiming its\nend-point towards a given target orientation. The technique runs in\ninteractive-time and does not require any pre-processing step such as e.g.\ntraining in machine learning techniques, in order to support new embodiments or\nnew postures. That allows it to be integrated in an artist-friendly workflow,\nbringing artists closer to the development of such AI-driven expressive\ncharacters, by allowing them to use their typical animation tools of choice,\nand to properly pre-visualize the animation during design-time, even on a real\nrobot. The full algorithmic specification is presented and described so that it\ncan be implemented and used throughout the communities of the various fields we\naddress. We demonstrate ERIK on different virtual kinematic structures, and\nalso on a low-fidelity robot that was crafted using wood and hobby-grade\nservos, to show how well the technique performs even on a low-grade robot. Our\nevaluation shows how well the technique performs, i.e., how well the character\nis able to point at the target orientation, while minimally disrupting its\ntarget expressive posture, and respecting its mechanical rotation limits.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 17:56:24 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 12:58:34 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Ribeiro", "Tiago", ""], ["Paiva", "Ana", ""]]}]