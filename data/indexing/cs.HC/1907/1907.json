[{"id": "1907.00042", "submitter": "Wei Cai", "authors": "Tengfei Wang and Shuyi Zhang and Xiao Wu and Wei Cai", "title": "Rhythm Dungeon: A Blockchain-based Music Roguelike Game", "comments": null, "journal-ref": "2019 Foundation of Digital Games Demos (FDG 2019 DEMO), San Luis\n  Obispo, California, USA, August 26-30, 2019", "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rhythm Dungeon is a rhythm game which leverages the blockchain as a shared\nopen database. During the gaming session, the player explores a roguelike\ndungeon by inputting specific sequences in time to music rhythm. By integrating\nsmart contract to the game program, the enemies through the venture are\ngenerated from other games which share the identical blockchain. On the other\nhand, the player may upload their characters at the end of their journey, so\nthat their own character may appear in other games and make an influence.\nRhythm Dungeon is designed and implemented to show the potential of\ndecentralized gaming experience, which utilizes the blockchain to provide\nasynchronous interactions among massive players.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 19:05:53 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wang", "Tengfei", ""], ["Zhang", "Shuyi", ""], ["Wu", "Xiao", ""], ["Cai", "Wei", ""]]}, {"id": "1907.00062", "submitter": "Yifan Wu", "authors": "Yifan Wu, Remco Chang, Eugene Wu, Joseph M. Hellerstein", "title": "DIEL: Transparent Scaling for Interactive Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We live in an era of big data and rich data visualization. As data sets\nincrease in size, browser-based interactive visualizations eventually hit\nlimits in storage and processing capacity. In order to provide interactivity\nover large datasets, visualization applications typically need to be\nextensively rewritten to make use of powerful back-end services. It would be\nfar preferable if front-end developers could write visualizations once in a\nnatural way, and have a framework take responsibility for transparently scaling\nup the visualization to use back-end services as needed. Achieving this goal\nrequires rethinking how communication and state are managed by the framework:\nthe mapping of interaction logic to server APIs or database queries, handling\nof results arriving asynchronously over the network, as well as basic\ncross-layer performance optimizations like caching.\n  In this paper, we present DIEL, a framework that achieves this cross-layer\nautoscaling transparently under a simple, declarative interface. DIEL treats UI\nevents as a stream of data that is captured in an event history for reuse.\nDevelopers declare what the state of the interface should be after the arrival\nof events. DIEL compiles these declarative specifications into relational\nqueries over both event history and the data to be visualized. In doing so,\nDIEL makes it easier to develop visualizations that are robust against changes\nto the size and location of data. To evaluate the DIEL framework, we developed\na prototype implementation and confirmed that DIEL supports a range of\nvisualization and interaction designs. Visualizations written using DIEL can\ntransparently and seamlessly scale to use back-end services with little\nintervention from the developer.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 20:24:40 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wu", "Yifan", ""], ["Chang", "Remco", ""], ["Wu", "Eugene", ""], ["Hellerstein", "Joseph M.", ""]]}, {"id": "1907.00075", "submitter": "Yifan Wu", "authors": "Yifan Wu, Remco Chang, Eugene Wu, Joe Hellerstein", "title": "Programming with Timespans in Interactive Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern interactive visualizations are akin to distributed systems, where user\ninteractions, background data processing, remote requests, and streaming data\nread and modify the interface at the same time. This concurrency is crucial to\nprovide an interactive user experience---forbidding it can cripple\nresponsiveness. However, it is notoriously challenging to program distributed\nsystems, and concurrency can easily lead to ambiguous or confusing interface\nbehaviors. In this paper, we present DIEL, a declarative programming model to\nhelp developers reason about and reconcile concurrency-related issues. Using\nDIEL, developers no longer need to procedurally describe how the interface\nshould update based on different input events, but rather declaratively specify\nwhat the state of the interface should be as queries over event history. We\nshow that resolving conflicts from concurrent processes in real-world\ninteractive visualizations can be done in a few lines of DIEL code.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 20:53:51 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wu", "Yifan", ""], ["Chang", "Remco", ""], ["Wu", "Eugene", ""], ["Hellerstein", "Joe", ""]]}, {"id": "1907.00146", "submitter": "Christan Grant", "authors": "Elena Montes, Monique Shotande, Daniel Helm, Christan Grant", "title": "DataPop: Knowledge Base Population using Distributed Voice Enabled\n  Devices", "comments": "7 pages, 2 references, unsubmitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data scientists are constantly creating methods to efficiently and accurately\npopulate big data sets for use in large-scale applications. Many recent efforts\nutilize crowd-sourcing and textual interfaces. In this paper, we propose a new\nmethod of curating data; namely, creating a multi-device Amazon Alexa Skill in\nthe form of a research trivia game. Users experience a synchronized gaming\nexperience with other Amazon Echo users, competing against one another while\nfilling in gaps of a connected knowledge base. This allows for full\nexploitation of the speed improvement offered by voice interface technology in\na game-based format.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 04:45:37 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Montes", "Elena", ""], ["Shotande", "Monique", ""], ["Helm", "Daniel", ""], ["Grant", "Christan", ""]]}, {"id": "1907.00193", "submitter": "Debin Meng", "authors": "Debin Meng, Xiaojiang Peng, Kai Wang, Yu Qiao", "title": "Frame attention networks for facial expression recognition in videos", "comments": "Accepted by ICIP 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The video-based facial expression recognition aims to classify a given video\ninto several basic emotions. How to integrate facial features of individual\nframes is crucial for this task. In this paper, we propose the Frame Attention\nNetworks (FAN), to automatically highlight some discriminative frames in an\nend-to-end framework. The network takes a video with a variable number of face\nimages as its input and produces a fixed-dimension representation. The whole\nnetwork is composed of two modules. The feature embedding module is a deep\nConvolutional Neural Network (CNN) which embeds face images into feature\nvectors. The frame attention module learns multiple attention weights which are\nused to adaptively aggregate the feature vectors to form a single\ndiscriminative video representation. We conduct extensive experiments on CK+\nand AFEW8.0 datasets. Our proposed FAN shows superior performance compared to\nother CNN based methods and achieves state-of-the-art performance on CK+.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 12:11:44 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 07:21:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Meng", "Debin", ""], ["Peng", "Xiaojiang", ""], ["Wang", "Kai", ""], ["Qiao", "Yu", ""]]}, {"id": "1907.00332", "submitter": "Vidyasagar Sadhu", "authors": "Gabriel Salles-Loustau, Vidyasagar Sadhu, Dario Pompili, Saman Zonouz,\n  Vincent Sritapan", "title": "Secure Mobile Technologies for Proactive Critical Infrastructure\n  Situational Awareness", "comments": "6 pages, IEEE HST 2016", "journal-ref": "2016 IEEE Symposium on Technologies for Homeland Security (HST),\n  Waltham, pp. 1-6", "doi": "10.1109/THS.2016.7568966", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trustworthy operation of our national critical infrastructures, such as the\nelectricity grid, against adversarial parties and accidental failures requires\nconstant and secure monitoring capabilities. In this paper, Eyephone is\npresented to leverage secure smartphone sensing and data acquisition\ncapabilities and enable pervasive sensing of the national critical\ninfrastructures. The reported information by the smartphone users will notify\nthe control center operators about particular accidental or malicious remote\ncritical infrastructure incidents. The reporting will be proactive regarding\npotentially upcoming failures given the system's current risky situation, e.g.,\na tree close to fall on a power grid transmission line. The information will\ninclude various modalities such as images, video, audio, time and location.\nEyephone will use system-wide information flow analysis and policy enforcement\nto prevent user privacy violations during the incident reportings. A working\nproof-of-concept prototype of Eyephone is implemented. Our results show that\nEyephone allows secure and effective use of smartphones for real-time\nsituational awareness of our national critical infrastructures.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 07:03:05 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Salles-Loustau", "Gabriel", ""], ["Sadhu", "Vidyasagar", ""], ["Pompili", "Dario", ""], ["Zonouz", "Saman", ""], ["Sritapan", "Vincent", ""]]}, {"id": "1907.00377", "submitter": "Tanmay Randhavane", "authors": "Tanmay Randhavane, Aniket Bera, Kyra Kapsaskis, Kurt Gray, Dinesh\n  Manocha", "title": "FVA: Modeling Perceived Friendliness of Virtual Agents Using Movement\n  Characteristics", "comments": "To appear in ISMAR 2019 Special Issue of TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for improving the friendliness and warmth of a\nvirtual agent in an AR environment by generating appropriate movement\ncharacteristics. Our algorithm is based on a novel data-driven friendliness\nmodel that is computed using a user-study and psychological characteristics. We\nuse our model to control the movements corresponding to the gaits, gestures,\nand gazing of friendly virtual agents (FVAs) as they interact with the user's\navatar and other agents in the environment. We have integrated FVA agents with\nan AR environment using with a Microsoft HoloLens. Our algorithm can generate\nplausible movements at interactive rates to increase the social presence. We\nalso investigate the perception of a user in an AR setting and observe that an\nFVA has a statistically significant improvement in terms of the perceived\nfriendliness and social presence of a user compared to an agent without the\nfriendliness modeling. We observe an increment of 5.71% in the mean responses\nto a friendliness measure and an improvement of 4.03% in the mean responses to\na social presence measure.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 13:04:43 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Randhavane", "Tanmay", ""], ["Bera", "Aniket", ""], ["Kapsaskis", "Kyra", ""], ["Gray", "Kurt", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1907.00443", "submitter": "Dhananjay Ram", "authors": "Dhananjay Ram, Lesly Miculicich, Herv\\'e Bourlard", "title": "Multilingual Bottleneck Features for Query by Example Spoken Term\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State of the art solutions to query by example spoken term detection\n(QbE-STD) usually rely on bottleneck feature representation of the query and\naudio document to perform dynamic time warping (DTW) based template matching.\nHere, we present a study on QbE-STD performance using several monolingual as\nwell as multilingual bottleneck features extracted from feed forward networks.\nThen, we propose to employ residual networks (ResNet) to estimate the\nbottleneck features and show significant improvements over the corresponding\nfeed forward network based features. The neural networks are trained on\nGlobalPhone corpus and QbE-STD experiments are performed on a very challenging\nQUESST 2014 database.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 20:14:19 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Ram", "Dhananjay", ""], ["Miculicich", "Lesly", ""], ["Bourlard", "Herv\u00e9", ""]]}, {"id": "1907.00483", "submitter": "Amit Kumar Jaiswal", "authors": "Amit Kumar Jaiswal, Haiming Liu and Ingo Frommholz", "title": "Effects of Foraging in Personalized Content-based Image Recommendation", "comments": "Accepted in Proceedings of the the 2nd International Workshop on\n  Explainable Recommendation and Search (EARS) at SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge of recommender systems is to help users locating\ninteresting items. Personalized recommender systems have become very popular as\nthey attempt to predetermine the needs of users and provide them with\nrecommendations to personalize their navigation. However, few studies have\naddressed the question of what drives the users' attention to specific content\nwithin the collection and what influences the selection of interesting items.\nTo this end, we employ the lens of Information Foraging Theory (IFT) to image\nrecommendation to demonstrate how the user could utilize visual bookmarks to\nlocate interesting images. We investigate a personalized content-based image\nrecommendation system to understand what affects user attention by reinforcing\nvisual attention cues based on IFT. We further find that visual bookmarks\n(cues) lead to a stronger scent of the recommended image collection. Our\nevaluation is based on the Pinterest image collection.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 22:16:32 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 12:43:53 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Jaiswal", "Amit Kumar", ""], ["Liu", "Haiming", ""], ["Frommholz", "Ingo", ""]]}, {"id": "1907.00684", "submitter": "Juliana Miehle", "authors": "Juliana Miehle, Louisa Pragst, Wolfgang Minker, Stefan Ultes", "title": "Enabling Dialogue Management with Dynamically Created Dialogue Actions", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to take up the challenge of realising user-adaptive system\nbehaviour, we present an extension for the existing OwlSpeak Dialogue Manager\nwhich enables the handling of dynamically created dialogue actions. This leads\nto an increase in flexibility which can be used for adaptation tasks. After the\nimplementation of the modifications and the integration of the Dialogue Manager\ninto a full Spoken Dialogue System, an evaluation of the system has been\ncarried out. The results indicate that the participants were able to conduct\nmeaningful dialogues and that the system performs satisfactorily, showing that\nthe implementation of the Dialogue Manager was successful.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:10:25 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Miehle", "Juliana", ""], ["Pragst", "Louisa", ""], ["Minker", "Wolfgang", ""], ["Ultes", "Stefan", ""]]}, {"id": "1907.00802", "submitter": "Takahiro Wada", "authors": "Takahiro Wada", "title": "Simultaneous Achievement of Driver Assistance and Skill Development in\n  Shared and Cooperative Controls", "comments": "Accepted for Cognition, Technology & Work (2018)", "journal-ref": null, "doi": "10.1007/s10111-018-0514-y", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advanced driver assistance systems have successfully reduced drivers'\nworkloads and increased safety. On the other hand, the excessive use of such\nsystems can impede the development of driving skills. However, there exist\ncollaborative driver assistance systems, including shared and cooperative\ncontrols, which can promote effective collaboration between an assistance\nsystem and a human operator under appropriate system settings. Given an\neffective collaboration setup, we address the goal of simultaneously developing\nor maintaining driving skills while reducing workload. As there has been a\npaucity of research on such systems and their methodologies, we discuss a\nmethodology applying shared and cooperative controls by considering related\nconcepts in the skill training field. Reverse parking assisted by haptic shared\ncontrol is presented as a means of increasing performance during assistance,\nwhile skill improvement following assistance is used to demonstrate the\npossibility of simultaneous achievement of driver assistance through the\nreduction of workload and skill improvement.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:09:31 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Wada", "Takahiro", ""]]}, {"id": "1907.00824", "submitter": "Hugo Scurto", "authors": "Hugo Scurto, Bavo Van Kerrebroeck, Baptiste Caramiaux, Fr\\'ed\\'eric\n  Bevilacqua", "title": "Designing Deep Reinforcement Learning for Human Parameter Exploration", "comments": "Author's version of the work. The definitive Version of Record was\n  published in ACM Transactions on Computer-Human Interaction (TOCHI)", "journal-ref": "ACM Trans. Comput.-Hum. Interact. 28, 1, Article 1 (January 2021),\n  35 pages (2021)", "doi": "10.1145/3414472", "report-no": null, "categories": "cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software tools for generating digital sound often present users with\nhigh-dimensional, parametric interfaces, that may not facilitate exploration of\ndiverse sound designs. In this paper, we propose to investigate artificial\nagents using deep reinforcement learning to explore parameter spaces in\npartnership with users for sound design. We describe a series of user-centred\nstudies to probe the creative benefits of these agents and adapting their\ndesign to exploration. Preliminary studies observing users' exploration\nstrategies with parametric interfaces and testing different agent exploration\nbehaviours led to the design of a fully-functioning prototype, called\nCo-Explorer, that we evaluated in a workshop with professional sound designers.\nWe found that the Co-Explorer enables a novel creative workflow centred on\nhuman-machine partnership, which has been positively received by practitioners.\nWe also highlight varied user exploration behaviors throughout partnering with\nour system. Finally, we frame design guidelines for enabling such\nco-exploration workflow in creative digital applications.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:32:47 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 11:50:04 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Scurto", "Hugo", ""], ["Van Kerrebroeck", "Bavo", ""], ["Caramiaux", "Baptiste", ""], ["Bevilacqua", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1907.00971", "submitter": "Philippe Esling", "authors": "Philippe Esling, Naotake Masuda, Adrien Bardet, Romeo Despres, Axel\n  Chemla--Romeu-Santos", "title": "Universal audio synthesizer control with normalizing flows", "comments": "DaFX 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.MM cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The ubiquity of sound synthesizers has reshaped music production and even\nentirely defined new music genres. However, the increasing complexity and\nnumber of parameters in modern synthesizers make them harder to master. Hence,\nthe development of methods allowing to easily create and explore with\nsynthesizers is a crucial need. Here, we introduce a novel formulation of audio\nsynthesizer control. We formalize it as finding an organized latent audio space\nthat represents the capabilities of a synthesizer, while constructing an\ninvertible mapping to the space of its parameters. By using this formulation,\nwe show that we can address simultaneously automatic parameter inference,\nmacro-control learning and audio-based preset exploration within a single\nmodel. To solve this new formulation, we rely on Variational Auto-Encoders\n(VAE) and Normalizing Flows (NF) to organize and map the respective auditory\nand parameter spaces. We introduce the disentangling flows, which allow to\nperform the invertible mapping between separate latent spaces, while steering\nthe organization of some latent dimensions to match target variation factors by\nsplitting the objective as partial density evaluation. We evaluate our proposal\nagainst a large set of baseline models and show its superiority in both\nparameter inference and audio reconstruction. We also show that the model\ndisentangles the major factors of audio variations as latent dimensions, that\ncan be directly used as macro-parameters. We also show that our model is able\nto learn semantic controls of a synthesizer by smoothly mapping to its\nparameters. Finally, we discuss the use of our model in creative applications\nand its real-time implementation in Ableton Live\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:49:07 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Esling", "Philippe", ""], ["Masuda", "Naotake", ""], ["Bardet", "Adrien", ""], ["Despres", "Romeo", ""], ["Chemla--Romeu-Santos", "Axel", ""]]}, {"id": "1907.00998", "submitter": "Alaadin Addas", "authors": "Alaadin Addas, Julie Thorpe, Amirali Salehi-Abari", "title": "Geographical Security Questions for Fallback Authentication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fallback authentication is the backup authentication method used when the\nprimary authentication method (e.g., passwords, fingerprints, etc.) fails.\nCurrently, widely-deployed fallback authentication methods (e.g., security\nquestions, email resets, and SMS resets) suffer from documented security and\nusability flaws that threaten the security of accounts. These flaws motivate us\nto design and study Geographical Security Questions (GeoSQ), a system for\nfallback authentication. GeoSQ is an Android application that utilizes\nautobiographical location data for fallback authentication. We performed\nsecurity and usability analyses of GeoSQ through an in-person two-session lab\nstudy (n=36,18 pairs). Our results indicate that GeoSQ exceeds the security of\nits counterparts, while its usability (specifically login time) has room for\nimprovement.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:09:39 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Addas", "Alaadin", ""], ["Thorpe", "Julie", ""], ["Salehi-Abari", "Amirali", ""]]}, {"id": "1907.01008", "submitter": "David Melhart", "authors": "David Melhart, Antonios Liapis, Georgios N. Yannakakis", "title": "PAGAN: Video Affect Annotation Made Easy", "comments": "Version accepted for International Conference on Affective Computing\n  & Intelligent Interaction (ACII), 2019", "journal-ref": "Proceedings of 8th International Conference on Affective Computing\n  & Intelligent Interaction (ACII 2019)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How could we gather affect annotations in a rapid, unobtrusive, and\naccessible fashion? How could we still make sure that these annotations are\nreliable enough for data-hungry affect modelling methods? This paper addresses\nthese questions by introducing PAGAN, an accessible, general-purpose, online\nplatform for crowdsourcing affect labels in videos. The design of PAGAN\novercomes the accessibility limitations of existing annotation tools, which\noften require advanced technical skills or even the on-site involvement of the\nresearcher. Such limitations often yield affective corpora that are restricted\nin size, scope and use, as the applicability of modern data-demanding machine\nlearning methods is rather limited. The description of PAGAN is accompanied by\nan exploratory study which compares the reliability of three continuous\nannotation tools currently supported by the platform. Our key results reveal\nhigher inter-rater agreement when annotation traces are processed in a relative\nmanner and collected via unbounded labelling.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 18:28:12 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Melhart", "David", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "1907.01221", "submitter": "Kun Zhao", "authors": "Kun Zhao, Takayuki Osogami and Tetsuro Morimura", "title": "Visual analytics for team-based invasion sports with significant events\n  and Markov reward process", "comments": "8 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In team-based invasion sports such as soccer and basketball, analytics is\nimportant for teams to understand their performance and for audiences to\nunderstand matches better. The present work focuses on performing visual\nanalytics to evaluate the value of any kind of event occurring in a sports\nmatch with a continuous parameter space. Here, the continuous parameter space\ninvolves the time, location, score, and other parameters. Because the\nspatiotemporal data used in such analytics is a low-level representation and\nhas a very large size, however, traditional analytics may need to discretize\nthe continuous parameter space (e.g., subdivide the playing area) or use a\nlocal feature to limit the analysis to specific events (e.g., only shots).\nThese approaches make evaluation impossible for any kind of event with a\ncontinuous parameter space. To solve this problem, we consider a whole match as\na Markov chain of significant events, so that event values can be estimated\nwith a continuous parameter space by solving the Markov chain with a machine\nlearning model. The significant events are first extracted by considering the\ntime-varying distribution of players to represent the whole match. Then, the\nextracted events are redefined as different states with the continuous\nparameter space and built as a Markov chain so that a Markov reward process can\nbe applied. Finally, the Markov reward process is solved by a customized\nfitted-value iteration algorithm so that the event values with the continuous\nparameter space can be predicted by a regression model. As a result, the event\nvalues can be visually inspected over the whole playing field under arbitrary\ngiven conditions. Experimental results with real soccer data show the\neffectiveness of the proposed system.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 08:11:16 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Zhao", "Kun", ""], ["Osogami", "Takayuki", ""], ["Morimura", "Tetsuro", ""]]}, {"id": "1907.01423", "submitter": "Haojian Jin", "authors": "Haojian Jin, Vita Chen, Ritwik Rajendra, Jason Hong", "title": "Enhancing Email Functionality using Late Bound Content", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Email is one of the most successful computer applications yet devised.\nCommunication features in email, however, have remained relatively static in\nyears. We investigate one way of expanding email functionality without\nmodifying the existing email infrastructure. We introduce email late bound\ncontent, a simple and generalizable technique that defers message content\nbinding through image lazy-loading. Parts of an email are converted into\nexternal images embedded in HTML code snippets, making it so that email clients\nwill defer the image download (i.e. content binding) until the moment users\nopen the email. This late bound content allows email senders and third party\nservices to update delivered emails. To illustrate the utilities of late bound\ncontent, we present four new example features and discuss the tradeoffs of\nemail content late binding.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 15:00:25 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Jin", "Haojian", ""], ["Chen", "Vita", ""], ["Rajendra", "Ritwik", ""], ["Hong", "Jason", ""]]}, {"id": "1907.01652", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Luisa Caldas, Luis Santos", "title": "RadVR: A 6DOF Virtual Reality Daylighting Analysis Tool", "comments": "Accepted to Automation in Construction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces RadVR, a virtual reality tool for daylighting analysis\nthat simultaneously combines qualitative assessments through immersive\nreal-time renderings with quantitative physically correct daylighting\nsimulations in a 6DOF virtual environment. By taking a 3D building model with\nmaterial properties as input, RadVR allows users to (1) perform\nphysically-based daylighting simulations via Radiance, (2) study sunlight in\ndifferent hours-of-the-year, (3) interact with a 9-point-in-time matrix for the\nmost representative times of the year, and (4) visualize, compare, and analyze\ndaylighting simulation results. With an end-to-end workflow, RadVR integrates\nwith 3D modeling software that is commonly used by building designers.\nAdditionally, by conducting user experiments we compare the proposed system\nwith DIVA for Rhino, a Radiance-based tool that uses conventional 2D-displays.\nThe results show that RadVR can provide promising assistance in spatial\nunderstanding tasks, navigation, and sun position analysis in virtual reality.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 21:15:02 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 06:19:56 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Caldas", "Luisa", ""], ["Santos", "Luis", ""]]}, {"id": "1907.01827", "submitter": "Hideyoshi Yanagisawa", "authors": "Kazutaka Ueda, Yuki Sakai, Hideyoshi Yanagisawa", "title": "Quantitative evaluation of sense of discrepancy to operation response\n  using event-related potential", "comments": "Submitted to iDECON2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aimed to develop a method to evaluate the sense of discrepancy to\nthe operation response quantitatively. We examined the availability of\nevent-related potential (P300), which is considered to reflect attention to\nstimulation, to evaluate the sense of discrepancy to the product response to\nthe user's action. In the experiment using subjective evaluation and P300 to\ninvestigate the sense of discrepancy due to the lack of operation response\n(sound and vibration) to the shutter operation of the mirrorless single-lens\ncamera, it was confirmed that P300 amplitude corresponds to the degree of the\nsubjective sense of discrepancy. Our results showed that the P300 amplitude\ncould evaluate the sense of discrepancy to the operation response.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:10:02 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Ueda", "Kazutaka", ""], ["Sakai", "Yuki", ""], ["Yanagisawa", "Hideyoshi", ""]]}, {"id": "1907.01921", "submitter": "Justin Edwards", "authors": "Allison Perrone, Justin Edwards", "title": "Chatbots as Unwitting Actors", "comments": null, "journal-ref": null, "doi": "10.1145/3342775.3342799", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatbots are popular for both task-oriented conversations and unstructured\nconversations with web users. Several different approaches to creating comedy\nand art exist across the field of computational creativity. Despite the\npopularity and ease of use of chatbots, there have not been any attempts by\nartists or comedians to use these systems for comedy performances. We present\ntwo initial attempts to do so from our comedy podcast and call for future work\ntoward both designing chatbots for performance and for performing alongside\nchatbots.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:10:10 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Perrone", "Allison", ""], ["Edwards", "Justin", ""]]}, {"id": "1907.01923", "submitter": "Justin Edwards", "authors": "Justin Edwards, Elaheh Sanoubari", "title": "A Need for Trust in Conversational Interface Research", "comments": null, "journal-ref": null, "doi": "10.1145/3342775.3342809", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across several branches of conversational interaction research including\ninteractions with social robots, embodied agents, and conversational\nassistants, users have identified trust as a critical part of those\ninteractions. Nevertheless, there is little agreement on what trust means\nwithin these sort of interactions or how trust can be measured. In this paper,\nwe explore some of the dimensions of trust as it has been understood in\nprevious work and we outline some of the ways trust has been measured in the\nhopes of furthering discussion of the concept across the field.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:12:23 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Edwards", "Justin", ""], ["Sanoubari", "Elaheh", ""]]}, {"id": "1907.01925", "submitter": "Justin Edwards", "authors": "Justin Edwards, He Liu, Tianyu Zhou, Sandy J. J. Gould, Leigh Clark,\n  Philip Doyle, Benjamin R. Cowan", "title": "Multitasking with Alexa Multitasking with Alexa: How Using Intelligent\n  Personal Assistants Impacts Language-based Primary Task Performance", "comments": null, "journal-ref": null, "doi": "10.1145/3342775.3342785", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent personal assistants (IPAs) are supposed to help us multitask. Yet\nthe impact of IPA use on multitasking is not clearly quantified, particularly\nin situations where primary tasks are also language based. Using a dual task\nparadigm, our study observes how IPA interactions impact two different types of\nwriting primary tasks; copying and generating content. We found writing tasks\nthat involve content generation, which are more cognitively demanding and share\nmore of the resources needed for IPA use, are significantly more disrupted by\nIPA interaction than less demanding tasks such as copying content. We discuss\nhow theories of cognitive resources, including multiple resource theory and\nworking memory, explain these results. We also outline the need for future work\nhow interruption length and relevance may impact primary task performance as\nwell as the need to identify effects of interruption timing in user and IPA led\ninterruptions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:13:35 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 08:31:39 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Edwards", "Justin", ""], ["Liu", "He", ""], ["Zhou", "Tianyu", ""], ["Gould", "Sandy J. J.", ""], ["Clark", "Leigh", ""], ["Doyle", "Philip", ""], ["Cowan", "Benjamin R.", ""]]}, {"id": "1907.01934", "submitter": "Hideyoshi Yanagisawa", "authors": "Dan Nanno, Hideyoshi Yanagisawa", "title": "Effect of assistive method on the sense of fulfillment with agency:\n  Modeling with flow and attribution theory", "comments": "PrePrint submitted to ASME", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Several assistive technologies for users' operations have been recently\ndeveloped. A user's sense of agency (SoA) decreases with increasing system\nassistance, possibly resulting in a decrease in the user's sense of\nfulfillment. This study aims to provide a design guideline for an assistive\nmethod to maintain and improve the sense of fulfillment with SoA. We propose a\nmathematical model describing the mechanisms by which the assistive method\naffects SoA and SoA induces a sense of fulfillment. The experience in the flow\nstate is assumed to be a sense of fulfillment. The assistance effect on the\nskill-challenge plane in flow theory is defined as an increase in skill and\ndecrease in challenge. The factor that separates the two effects from\nattribution theory is the locus of causality, which is matched to the judgement\nof agency (JoA) from the two-step account of agency. We hypothesized that the\nassistance increases the perception of skill and sense of fulfillment is\ngreater when the locus of causality is internal, rather than external. To\nverify this hypothesis, a game task experiment was conducted with assistance\nthat varied with the ease of recognition. We hypothesized that a player's JoA\nis internal for hard-to-recognize assistance, resulting in a high sense of\nfulfillment. Experimental results supported this hypothesis.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:28:52 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 02:44:12 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Nanno", "Dan", ""], ["Yanagisawa", "Hideyoshi", ""]]}, {"id": "1907.02035", "submitter": "Michael Correll", "authors": "Michael Correll, Enrico Bertini, Steven Franconeri", "title": "Truncating the Y-Axis: Threat or Menace?", "comments": null, "journal-ref": null, "doi": "10.1145/3313831.3376222", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bar charts with y-axes that don't begin at zero can visually exaggerate\neffect sizes. However, advice for whether or not to truncate the y-axis can be\nequivocal for other visualization types. In this paper we present examples of\nvisualizations where this y-axis truncation can be beneficial as well as\nharmful, depending on the communicative and analytic intent. We also present\nthe results of a series of crowd-sourced experiments in which we examine how\ny-axis truncation impacts subjective effect size across visualization types,\nand we explore alternative designs that more directly alert viewers to this\ntruncation. We find that the subjective impact of axis truncation is persistent\nacross visualizations designs, even for designs with explicit visual cues that\nindicate truncation has taken place. We suggest that designers consider the\nscale of the meaningful effect sizes and variation they intend to communicate,\nregardless of the visual encoding.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 16:55:57 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 17:39:11 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Correll", "Michael", ""], ["Bertini", "Enrico", ""], ["Franconeri", "Steven", ""]]}, {"id": "1907.02102", "submitter": "Tanmay Randhavane", "authors": "Tanmay Randhavane, Aniket Bera, Kyra Kapsaskis, Rahul Sheth, Kurt\n  Gray, Dinesh Manocha", "title": "EVA: Generating Emotional Behavior of Virtual Agents using Expressive\n  Features of Gait and Gaze", "comments": "In Proceedings of ACM Symposium on Applied Perception 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel, real-time algorithm, EVA, for generating virtual agents\nwith various perceived emotions. Our approach is based on using Expressive\nFeatures of gaze and gait to convey emotions corresponding to happy, sad,\nangry, or neutral. We precompute a data-driven mapping between gaits and their\nperceived emotions. EVA uses this gait emotion association at runtime to\ngenerate appropriate walking styles in terms of gaits and gaze. Using the EVA\nalgorithm, we can simulate gaits and gazing behaviors of hundreds of virtual\nagents in real-time with known emotional characteristics. We have evaluated the\nbenefits in different multi-agent VR simulation environments. Our studies\nsuggest that the use of expressive features corresponding to gait and gaze can\nconsiderably increase the sense of presence in scenarios with multiple virtual\nagents.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:54:57 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Randhavane", "Tanmay", ""], ["Bera", "Aniket", ""], ["Kapsaskis", "Kyra", ""], ["Sheth", "Rahul", ""], ["Gray", "Kurt", ""], ["Manocha", "Dinesh", ""]]}, {"id": "1907.02227", "submitter": "Anhong Guo", "authors": "Anhong Guo, Ece Kamar, Jennifer Wortman Vaughan, Hanna Wallach,\n  Meredith Ringel Morris", "title": "Toward Fairness in AI for People with Disabilities: A Research Roadmap", "comments": "ACM ASSETS 2019 Workshop on AI Fairness for People with Disabilities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI technologies have the potential to dramatically impact the lives of people\nwith disabilities (PWD). Indeed, improving the lives of PWD is a motivator for\nmany state-of-the-art AI systems, such as automated speech recognition tools\nthat can caption videos for people who are deaf and hard of hearing, or\nlanguage prediction algorithms that can augment communication for people with\nspeech or cognitive disabilities. However, widely deployed AI systems may not\nwork properly for PWD, or worse, may actively discriminate against them. These\nconsiderations regarding fairness in AI for PWD have thus far received little\nattention. In this position paper, we identify potential areas of concern\nregarding how several AI technology categories may impact particular disability\nconstituencies if care is not taken in their design, development, and testing.\nWe intend for this risk assessment of how various classes of AI might interact\nwith various classes of disability to provide a roadmap for future research\nthat is needed to gather data, test these hypotheses, and build more inclusive\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 05:29:49 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 18:39:41 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Guo", "Anhong", ""], ["Kamar", "Ece", ""], ["Vaughan", "Jennifer Wortman", ""], ["Wallach", "Hanna", ""], ["Morris", "Meredith Ringel", ""]]}, {"id": "1907.02288", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, Antonios Liapis, Georgios N. Yannakakis", "title": "From Pixels to Affect: A Study on Games and Player Experience", "comments": "Proceedings of the Intl. Conference on Affective Computing and\n  Intelligent Interaction. IEEE. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to predict the affect of a user just by observing her\nbehavioral interaction through a video? How can we, for instance, predict a\nuser's arousal in games by merely looking at the screen during play? In this\npaper we address these questions by employing three dissimilar deep\nconvolutional neural network architectures in our attempt to learn the\nunderlying mapping between video streams of gameplay and the player's arousal.\nWe test the algorithms in an annotated dataset of 50 gameplay videos of a\nsurvival shooter game and evaluate the deep learned models' capacity to\nclassify high vs low arousal levels. Our key findings with the demanding\nleave-one-video-out validation method reveal accuracies of over 78% on average\nand 98% at best. While this study focuses on games and player experience as a\ntest domain, the findings and methodology are directly relevant to any\naffective computing area, introducing a general and user-agnostic approach for\nmodeling affect.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 09:15:03 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 08:49:24 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "1907.02349", "submitter": "Jichen Zhu", "authors": "Jichen Zhu, Santiago Onta\\~n\\'on", "title": "Experience Management in Multi-player Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience Management studies AI systems that automatically adapt interactive\nexperiences such as games to tailor to specific players and to fulfill design\ngoals. Although it has been explored for several decades, existing work in\nexperience management has mostly focused on single-player experiences. This\npaper is a first attempt at identifying the main challenges to expand EM to\nmulti-player/multi-user games or experiences. We also make connections to\nrelated areas where solutions for similar problems have been proposed\n(especially group recommender systems) and discusses the potential impact and\napplications of multi-player EM.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 12:03:03 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Zhu", "Jichen", ""], ["Onta\u00f1\u00f3n", "Santiago", ""]]}, {"id": "1907.02426", "submitter": "Susanne Trick", "authors": "Susanne Trick, Dorothea Koert, Jan Peters, Constantin Rothkopf", "title": "Multimodal Uncertainty Reduction for Intention Recognition in\n  Human-Robot Interaction", "comments": "Submitted to IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assistive robots can potentially improve the quality of life and personal\nindependence of elderly people by supporting everyday life activities. To\nguarantee a safe and intuitive interaction between human and robot, human\nintentions need to be recognized automatically. As humans communicate their\nintentions multimodally, the use of multiple modalities for intention\nrecognition may not just increase the robustness against failure of individual\nmodalities but especially reduce the uncertainty about the intention to be\npredicted. This is desirable as particularly in direct interaction between\nrobots and potentially vulnerable humans a minimal uncertainty about the\nsituation as well as knowledge about this actual uncertainty is necessary.\nThus, in contrast to existing methods, in this work a new approach for\nmultimodal intention recognition is introduced that focuses on uncertainty\nreduction through classifier fusion. For the four considered modalities speech,\ngestures, gaze directions and scene objects individual intention classifiers\nare trained, all of which output a probability distribution over all possible\nintentions. By combining these output distributions using the Bayesian method\nIndependent Opinion Pool the uncertainty about the intention to be recognized\ncan be decreased. The approach is evaluated in a collaborative human-robot\ninteraction task with a 7-DoF robot arm. The results show that fused\nclassifiers which combine multiple modalities outperform the respective\nindividual base classifiers with respect to increased accuracy, robustness, and\nreduced uncertainty.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 14:33:49 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Trick", "Susanne", ""], ["Koert", "Dorothea", ""], ["Peters", "Jan", ""], ["Rothkopf", "Constantin", ""]]}, {"id": "1907.02872", "submitter": "Rebecca Faust", "authors": "Rebecca Faust, Katherine Isaacs, William Z. Bernstein, Michael Sharp,\n  and Carlos Scheidegger", "title": "Anteater: Interactive Visualization of Program Execution Values in\n  Context", "comments": "11 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Debugging is famously one the hardest parts in programming. In this paper, we\ntackle the question: what does a debugging environment look like when we take\ninteractive visualization as a central design principle? We introduce Anteater,\nan interactive visualization system for tracing and exploring the execution of\nPython programs. Existing systems often have visualization components built on\ntop of an existing infrastructure. In contrast, Anteater's organization of\ntrace data enables an intermediate representation which can be leveraged to\nautomatically synthesize a variety of visualizations and interactions. These\ninteractive visualizations help with tasks such as discovering important\nstructures in the execution and understanding and debugging unexpected\nbehaviors. To assess the utility of Anteater, we conducted a participant study\nwhere programmers completed tasks on their own python programs using Anteater.\nFinally, we discuss limitations and where further research is needed.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 14:53:31 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 22:11:18 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2020 19:48:39 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Faust", "Rebecca", ""], ["Isaacs", "Katherine", ""], ["Bernstein", "William Z.", ""], ["Sharp", "Michael", ""], ["Scheidegger", "Carlos", ""]]}, {"id": "1907.02889", "submitter": "A\\'ecio Solano Rodrigues Santos", "authors": "A\\'ecio Santos, Sonia Castelo, Cristian Felix, Jorge Piazentin Ono,\n  Bowen Yu, Sungsoo Hong, Cl\\'audio T. Silva, Enrico Bertini, Juliana Freire", "title": "Visus: An Interactive System for Automatic Machine Learning Model\n  Building and Curation", "comments": "Accepted for publication in the 2019 Workshop on Human-In-the-Loop\n  Data Analytics (HILDA'19), co-located with SIGMOD 2019", "journal-ref": null, "doi": "10.1145/3328519.3329134", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the demand for machine learning (ML) applications is booming, there is\na scarcity of data scientists capable of building such models. Automatic\nmachine learning (AutoML) approaches have been proposed that help with this\nproblem by synthesizing end-to-end ML data processing pipelines. However, these\nfollow a best-effort approach and a user in the loop is necessary to curate and\nrefine the derived pipelines. Since domain experts often have little or no\nexpertise in machine learning, easy-to-use interactive interfaces that guide\nthem throughout the model building process are necessary. In this paper, we\npresent Visus, a system designed to support the model building process and\ncuration of ML data processing pipelines generated by AutoML systems. We\ndescribe the framework used to ground our design choices and a usage scenario\nenabled by Visus. Finally, we discuss the feedback received in user testing\nsessions with domain experts.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:21:51 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Santos", "A\u00e9cio", ""], ["Castelo", "Sonia", ""], ["Felix", "Cristian", ""], ["Ono", "Jorge Piazentin", ""], ["Yu", "Bowen", ""], ["Hong", "Sungsoo", ""], ["Silva", "Cl\u00e1udio T.", ""], ["Bertini", "Enrico", ""], ["Freire", "Juliana", ""]]}, {"id": "1907.03050", "submitter": "Soheil Khorram", "authors": "Soheil Khorram, Melvin G McInnis, Emily Mower Provost", "title": "Jointly Aligning and Predicting Continuous Emotion Annotations", "comments": "IEEE Transactions on Affective Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-continuous dimensional descriptions of emotions (e.g., arousal, valence)\nallow researchers to characterize short-time changes and to capture long-term\ntrends in emotion expression. However, continuous emotion labels are generally\nnot synchronized with the input speech signal due to delays caused by\nreaction-time, which is inherent in human evaluations. To deal with this\nchallenge, we introduce a new convolutional neural network (multi-delay sinc\nnetwork) that is able to simultaneously align and predict labels in an\nend-to-end manner. The proposed network is a stack of convolutional layers\nfollowed by an aligner network that aligns the speech signal and emotion\nlabels. This network is implemented using a new convolutional layer that we\nintroduce, the delayed sinc layer. It is a time-shifted low-pass (sinc) filter\nthat uses a gradient-based algorithm to learn a single delay. Multiple delayed\nsinc layers can be used to compensate for a non-stationary delay that is a\nfunction of the acoustic space. We test the efficacy of this system on two\ncommon emotion datasets, RECOLA and SEWA, and show that this approach obtains\nstate-of-the-art speech-only results by learning time-varying delays while\npredicting dimensional descriptors of emotions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 23:49:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 22:40:43 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Khorram", "Soheil", ""], ["McInnis", "Melvin G", ""], ["Provost", "Emily Mower", ""]]}, {"id": "1907.03107", "submitter": "Lin Wang", "authors": "Lin Wang and Kuk-Jin Yoon", "title": "CoAug-MR: An MR-based Interactive Office Workstation Design System via\n  Augmented Multi-Person Collaboration", "comments": "10 pages, Paper in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital prototyping and evaluation using 3D modeling and digital human models\nare becoming more practical for customizing products to the preference of a\nuser. However, the 3D modeling is less accessible to casual users, and digital\nhuman models suffer from insufficient body data and less intuitive illustration\non how people use the product or how it accommodates to their body. Recently,\nVR-supported 'Do It Yourself' design has achieved real-time ergonomic\nevaluation with users themselves by capturing their poses, however, it lacks\nreliability and quality of design. In this paper, we explore a multi-person\ninteractive design approach that enables designers, users, and even ergonomists\nto collaborate to achieve effective and reliable design and prototyping tasks.\nMixed Reality that utilizes Hololens and motion tracking devices had been\ndeveloped to provide instant design feedback and evaluation and to experience\nprototyping in physical space. We evaluate the system based on the usability\nstudy, where casual users and designers are engaged in the interactive process\nof designing items with respect to the body information, the preference, and\nthe environment.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 10:19:04 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 12:07:11 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 08:21:41 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Wang", "Lin", ""], ["Yoon", "Kuk-Jin", ""]]}, {"id": "1907.03263", "submitter": "Aaditeshwar Seth", "authors": "Aaditeshwar Seth", "title": "Ensuring Responsible Outcomes from Technology", "comments": "Presented as an invited talk at IEEE COMSNETS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We attempt to make two arguments in this essay. First, through a case study\nof a mobile phone based voice-media service we have been running in rural\ncentral India for more than six years, we describe several implementation\ncomplexities we had to navigate towards realizing our intended vision of\nbringing social development through technology. Most of these complexities\narose in the interface of our technology with society, and we argue that even\nother technology providers can create similar processes to manage this\nsocio-technological interface and ensure intended outcomes from their\ntechnology use. We then build our second argument about how to ensure that the\norganizations behind both market driven technologies and those technologies\nthat are adopted by the state, pay due attention towards responsibly managing\nthe socio-technological interface of their innovations. We advocate for the\ntechnology engineers and researchers who work within these organizations, to\ntake up the responsibility and ensure that their labour leads to making the\nworld a better place especially for the poor and marginalized. We outline\npossible governance structures that can give more voice to the technology\ndevelopers to push their organizations towards ensuring that responsible\noutcomes emerge from their technology. We note that the examples we use to\nbuild our arguments are limited to contemporary information and communication\ntechnology (ICT) platforms used directly by end-users to share content with one\nanother, and hence our argument may not generalize to other ICTs in a\nstraightforward manner.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 09:55:20 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Seth", "Aaditeshwar", ""]]}, {"id": "1907.03282", "submitter": "Hideyoshi Yanagisawa", "authors": "Takuma Maki, Hideyoshi Yanagisawa", "title": "A methodology for multisensory product experience design using\n  cross-modal effect: A case of SLR camera", "comments": "Accepted to ICED2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Throughout the course of product experience, a user employs multiple senses,\nincluding vision, hearing, and touch. Previous cross-modal studies have shown\nthat multiple senses interact with each other and change perceptions. In this\npaper, we propose a methodology for designing multisensory product experiences\nby applying cross-modal effect to simultaneous stimuli. In this methodology, we\nfirst obtain a model of the comprehensive cognitive structure of user's\nmultisensory experience by applying Kansei modeling methodology and extract\nopportunities of cross-modal effect from the structure. Second, we conduct\nexperiments on these cross-modal effects and formulate them by obtaining a\nregression curve through analysis. Finally, we find solutions to improve the\nproduct sensory experience from the regression model of the target cross-modal\neffects. We demonstrated the validity of the methodology with SLR cameras as a\ncase study, which is a typical product with multisensory perceptions.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 13:19:06 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Maki", "Takuma", ""], ["Yanagisawa", "Hideyoshi", ""]]}, {"id": "1907.03324", "submitter": "Hilde Weerts MSc", "authors": "Hilde J.P. Weerts and Werner van Ipenburg and Mykola Pechenizkiy", "title": "A Human-Grounded Evaluation of SHAP for Alert Processing", "comments": "Will be published in proceedings of KDD workshop on Explainable AI\n  2019 (KDD-XAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past years, many new explanation methods have been proposed to achieve\ninterpretability of machine learning predictions. However, the utility of these\nmethods in practical applications has not been researched extensively. In this\npaper we present the results of a human-grounded evaluation of SHAP, an\nexplanation method that has been well-received in the XAI and related\ncommunities. In particular, we study whether this local model-agnostic\nexplanation method can be useful for real human domain experts to assess the\ncorrectness of positive predictions, i.e. alerts generated by a classifier. We\nperformed experimentation with three different groups of participants (159 in\ntotal), who had basic knowledge of explainable machine learning. We performed a\nqualitative analysis of recorded reflections of experiment participants\nperforming alert processing with and without SHAP information. The results\nsuggest that the SHAP explanations do impact the decision-making process,\nalthough the model's confidence score remains to be a leading source of\nevidence. We statistically test whether there is a significant difference in\ntask utility metrics between tasks for which an explanation was available and\ntasks in which it was not provided. As opposed to common intuitions, we did not\nfind a significant difference in alert processing performance when a SHAP\nexplanation is available compared to when it is not.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 17:50:06 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Weerts", "Hilde J. P.", ""], ["van Ipenburg", "Werner", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1907.03334", "submitter": "Hilde Weerts MSc", "authors": "Hilde J.P. Weerts and Werner van Ipenburg and Mykola Pechenizkiy", "title": "Case-Based Reasoning for Assisting Domain Experts in Processing Fraud\n  Alerts of Black-Box Machine Learning Models", "comments": "Will be published in proceedings of KDD workshop on Anomaly Detection\n  in Finance 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many contexts, it can be useful for domain experts to understand to what\nextent predictions made by a machine learning model can be trusted. In\nparticular, estimates of trustworthiness can be useful for fraud analysts who\nprocess machine learning-generated alerts of fraudulent transactions. In this\nwork, we present a case-based reasoning (CBR) approach that provides evidence\non the trustworthiness of a prediction in the form of a visualization of\nsimilar previous instances. Different from previous works, we consider\nsimilarity of local post-hoc explanations of predictions and show empirically\nthat our visualization can be useful for processing alerts. Furthermore, our\napproach is perceived useful and easy to use by fraud analysts at a major Dutch\nbank.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jul 2019 19:12:49 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Weerts", "Hilde J. P.", ""], ["van Ipenburg", "Werner", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1907.03390", "submitter": "Saeid Amiri", "authors": "Saeid Amiri, Sujay Bajracharya, Cihangir Goktolga, Jesse Thomason, and\n  Shiqi Zhang", "title": "Augmenting Knowledge through Statistical, Goal-oriented Human-Robot\n  Dialog", "comments": "In proceedings of International Conference on Intelligent Robots and\n  Systems (IROS) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some robots can interact with humans using natural language, and identify\nservice requests through human-robot dialog. However, few robots are able to\nimprove their language capabilities from this experience. In this paper, we\ndevelop a dialog agent for robots that is able to interpret user commands using\na semantic parser, while asking clarification questions using a probabilistic\ndialog manager. This dialog agent is able to augment its knowledge base and\nimprove its language capabilities by learning from dialog experiences, e.g.,\nadding new entities and learning new ways of referring to existing entities. We\nhave extensively evaluated our dialog system in simulation as well as with\nhuman participants through MTurk and real-robot platforms. We demonstrate that\nour dialog agent performs better in efficiency and accuracy in comparison to\nbaseline learning agents. Demo video can be found at\nhttps://youtu.be/DFB3jbHBqYE\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 02:58:38 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 20:08:41 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Amiri", "Saeid", ""], ["Bajracharya", "Sujay", ""], ["Goktolga", "Cihangir", ""], ["Thomason", "Jesse", ""], ["Zhang", "Shiqi", ""]]}, {"id": "1907.03877", "submitter": "Tiago Machado", "authors": "Tiago Machado, Dan Gopstein, Andy Nealen and Julian Togelius", "title": "Pitako -- Recommending Game Design Elements in Cicero", "comments": "Paper accepted in the IEEE Conference on Games 2019 (COG 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender Systems are widely and successfully applied in e-commerce. Could\nthey be used for design? In this paper, we introduce Pitako1, a tool that\napplies the Recommender System concept to assist humans in creative tasks. More\nspecifically, Pitako provides suggestions by taking games designed by humans as\ninputs, and recommends mechanics and dynamics as outputs. Pitako is implemented\nas a new system within the mixed-initiative AI-based Game Design Assistant,\nCicero. This paper discusses the motivation behind the implementation of Pitako\nas well as its technical details and presents usage examples. We believe that\nPitako can influence the use of recommender systems to help humans in their\ndaily tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 21:19:25 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Machado", "Tiago", ""], ["Gopstein", "Dan", ""], ["Nealen", "Andy", ""], ["Togelius", "Julian", ""]]}, {"id": "1907.03919", "submitter": "Matthew Brehmer", "authors": "Matthew Brehmer, Bongshin Lee, Petra Isenberg, Eun Kyoung Choe", "title": "A Comparative Evaluation of Animation and Small Multiples for Trend\n  Visualization on Mobile Phones", "comments": "Accepted for presentation at IEEE VIS 2019, October 20-25 in\n  Vancouver, Canada. To appear in IEEE Transactions on Visualization and\n  Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934397", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the efficacy of animated and small multiples variants of\nscatterplots on mobile phones for comparing trends in multivariate datasets.\nVisualization is increasingly prevalent in mobile applications and mobile-first\nwebsites, yet there is little prior visualization research dedicated to small\ndisplays. In this paper, we build upon previous experimental research carried\nout on larger displays that assessed animated and non-animated variants of\nscatterplots. Incorporating similar experimental stimuli and tasks, we\nconducted an experiment where 96 crowdworker participants performed nine trend\ncomparison tasks using their mobile phones. We found that those using a small\nmultiples design consistently completed tasks in less time, albeit with\nslightly less confidence than those using an animated design. The accuracy\nresults were more task-dependent, and we further interpret our results\naccording to the characteristics of the individual tasks, with a specific focus\non the trajectories of target and distractor data items in each task. We\nidentify cases that appear to favor either animation or small multiples,\nproviding new questions for further experimental research and implications for\nvisualization design on mobile devices. Lastly, we provide a reflection on our\nevaluation methodology.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 00:41:09 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 22:08:51 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Brehmer", "Matthew", ""], ["Lee", "Bongshin", ""], ["Isenberg", "Petra", ""], ["Choe", "Eun Kyoung", ""]]}, {"id": "1907.03994", "submitter": "Youwei Zeng", "authors": "Youwei Zeng, Dan Wu, Jie Xiong, Enze Yi, Ruiyang Gao, Daqing Zhang", "title": "FarSense: Pushing the Range Limit of WiFi-based Respiration Sensing with\n  CSI Ratio of Two Antennas", "comments": "This work is a pre-print version to appear at UbiComp 2019", "journal-ref": null, "doi": "10.1145/3351279", "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed the great potential of exploiting channel\nstate information retrieved from commodity WiFi devices for respiration\nmonitoring. However, existing approaches only work when the target is close to\nthe WiFi transceivers and the performance degrades significantly when the\ntarget is far away. On the other hand, most home environments only have one\nWiFi access point and it may not be located in the same room as the target.\nThis sensing range constraint greatly limits the application of the proposed\napproaches in real life.\n  This paper presents FarSense--the first real-time system that can reliably\nmonitor human respiration when the target is far away from the WiFi transceiver\npair. FarSense works well even when one of the transceivers is located in\nanother room, moving a big step towards real-life deployment. We propose two\nnovel schemes to achieve this goal: (1) Instead of applying the raw CSI\nreadings of individual antenna for sensing, we employ the ratio of CSI readings\nfrom two antennas, whose noise is mostly canceled out by the division operation\nto significantly increase the sensing range; (2) The division operation further\nenables us to utilize the phase information which is not usable with one single\nantenna for sensing. The orthogonal amplitude and phase are elaborately\ncombined to address the \"blind spots\" issue and further increase the sensing\nrange. Extensive experiments show that FarSense is able to accurately monitor\nhuman respiration even when the target is 8 meters away from the transceiver\npair, increasing the sensing range by more than 100%. We believe this is the\nfirst system to enable through-wall respiration sensing with commodity WiFi\ndevices and the proposed method could also benefit other sensing applications.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 05:56:02 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 02:32:52 GMT"}, {"version": "v3", "created": "Sat, 10 Aug 2019 05:35:43 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zeng", "Youwei", ""], ["Wu", "Dan", ""], ["Xiong", "Jie", ""], ["Yi", "Enze", ""], ["Gao", "Ruiyang", ""], ["Zhang", "Daqing", ""]]}, {"id": "1907.04104", "submitter": "Pavel Rojtberg", "authors": "Pavel Rojtberg", "title": "User Guidance for Interactive Camera Calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For building a Augmented Reality (AR) pipeline, the most crucial step is the\ncamera calibration as overall quality heavily depends on it. In turn camera\ncalibration itself is influenced most by the choice of camera-to-pattern poses\n- yet currently there is only little research on guiding the user to a specific\npose. We build upon our novel camera calibration framework that is capable to\ngenerate calibration poses in real-time and present a user study evaluating\ndifferent visualization methods to guide the user to a target pose. Using the\npresented method even novel users are capable to perform a precise camera\ncalibration in about 2 minutes.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 11:59:54 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Rojtberg", "Pavel", ""]]}, {"id": "1907.04112", "submitter": "Katar\\'ina Furmanov\\'a", "authors": "Katar\\'ina Furmanov\\'a, Adam Jur\\v{c}\\'ik, Barbora Kozl\\'ikov\\'a,\n  Helwig Hauser, Jan By\\v{s}ka", "title": "Multiscale Visual Drilldown for the Analysis of Large Ensembles of\n  Multi-Body Protein Complexes", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934333", "report-no": null, "categories": "cs.HC q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When studying multi-body protein complexes, biochemists use computational\ntools that can suggest hundreds or thousands of their possible spatial\nconfigurations. However, it is not feasible to experimentally verify more than\nonly a very small subset of them. In this paper, we propose a novel multiscale\nvisual drilldown approach that was designed in tight collaboration with\nproteomic experts, enabling a systematic exploration of the configuration\nspace. Our approach takes advantage of the hierarchical structure of the data\n-- from the whole ensemble of protein complex configurations to the individual\nconfigurations, their contact interfaces, and the interacting amino acids. Our\nnew solution is based on interactively linked 2D and 3D views for individual\nhierarchy levels and at each level, we offer a set of selection and filtering\noperations enabling the user to narrow down the number of configurations that\nneed to be manually scrutinized. Furthermore, we offer a dedicated filter\ninterface, which provides the users with an overview of the applied filtering\noperations and enables them to examine their impact on the explored ensemble.\nThis way, we maintain the history of the exploration process and thus enable\nthe user to return to an earlier point of the exploration. We demonstrate the\neffectiveness of our approach on two case studies conducted by collaborating\nproteomic experts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 12:31:31 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Furmanov\u00e1", "Katar\u00edna", ""], ["Jur\u010d\u00edk", "Adam", ""], ["Kozl\u00edkov\u00e1", "Barbora", ""], ["Hauser", "Helwig", ""], ["By\u0161ka", "Jan", ""]]}, {"id": "1907.04191", "submitter": "Philip Feldman", "authors": "Philip Feldman, Aaron Dant, Wayne Lutters", "title": "Belief places and spaces: Mapping cognitive environments", "comments": "19 pages, 9 figure. arXiv admin note: text overlap with\n  arXiv:1904.05216", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beliefs are not facts, but they are factive - they feel like facts. This\nproperty is what can make misinformation dangerous. Being able to deliberately\nnavigate through a landscape of often conflicting factive statements is\ndifficult when there is no way to show the relationships between them without\nincorporating the information in linear, narrative forms. In this paper, we\npresent a mechanism to produce maps of belief places, where populations agree\non salient features of fictional environments, and belief spaces, where\nsubgroups have related but distinct perspectives. Using a model developed using\nagent-based simulation, we show that by observing the repeated behaviors of\nhuman participants in the same social context, it is possible to build maps\nthat show the shared narrative environment overlaid with traces that show\nunique, individual or subgroup perspectives. Our contribution is a\nproof-of-concept system, based on the affordances of fantasy tabletop\nrole-playing games, which support multiple groups interacting with the same\ndungeon in a controlled, online environment. The techniques used in this\nprocess are mathematically straightforward, and should be generalizable to\nauto-generating larger-scale maps of belief spaces from other corpora, such as\ndiscussions on social media.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 11:44:25 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 15:44:50 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Feldman", "Philip", ""], ["Dant", "Aaron", ""], ["Lutters", "Wayne", ""]]}, {"id": "1907.04198", "submitter": "Jennifer Gago", "authors": "Jennifer J. Gago, Valentina Vasco, Bartek {\\L}ukawski, Ugo Pattacini,\n  Vadim Tikhanoff, Juan G. Victores, Carlos Balaguer", "title": "Sequence-to-Sequence Natural Language to Humanoid Robot Sign Language", "comments": "13 pages, 8 figures, conference", "journal-ref": null, "doi": "10.11128/arep.58", "report-no": null, "categories": "cs.RO cs.CL cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a study on natural language to sign language translation\nwith human-robot interaction application purposes. By means of the presented\nmethodology, the humanoid robot TEO is expected to represent Spanish sign\nlanguage automatically by converting text into movements, thanks to the\nperformance of neural networks. Natural language to sign language translation\npresents several challenges to developers, such as the discordance between the\nlength of input and output data and the use of non-manual markers. Therefore,\nneural networks and, consequently, sequence-to-sequence models, are selected as\na data-driven system to avoid traditional expert system approaches or temporal\ndependencies limitations that lead to limited or too complex translation\nsystems. To achieve these objectives, it is necessary to find a way to perform\nhuman skeleton acquisition in order to collect the signing input data. OpenPose\nand skeletonRetriever are proposed for this purpose and a 3D sensor\nspecification study is developed to select the best acquisition hardware.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 14:41:50 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Gago", "Jennifer J.", ""], ["Vasco", "Valentina", ""], ["\u0141ukawski", "Bartek", ""], ["Pattacini", "Ugo", ""], ["Tikhanoff", "Vadim", ""], ["Victores", "Juan G.", ""], ["Balaguer", "Carlos", ""]]}, {"id": "1907.04265", "submitter": "Janaki Sheth", "authors": "Janaki Sheth, Ariel Tankus, Michelle Tran, Nader Pouratian, Itzhak\n  Fried and William Speier", "title": "Translating neural signals to text using a Brain-Machine Interface", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-Computer Interfaces (BCI) help patients with faltering communication\nabilities due to neurodegenerative diseases produce text or speech output by\ndirect neural processing. However, practical implementation of such a system\nhas proven difficult due to limitations in speed, accuracy, and\ngeneralizability of the existing interfaces. To this end, we aim to create a\nBCI system that decodes text directly from neural signals. We implement a\nframework that initially isolates frequency bands in the input signal\nencapsulating differential information regarding production of various phonemic\nclasses. These bands then form a feature set that feeds into an LSTM which\ndiscerns at each time point probability distributions across all phonemes\nuttered by a subject. Finally, these probabilities are fed into a particle\nfiltering algorithm which incorporates prior knowledge of the English language\nto output text corresponding to the decoded word. Performance of this model on\ndata obtained from six patients shows encouragingly high levels of accuracy at\nspeeds and bit rates significantly higher than existing BCI communication\nsystems. Further, in producing an output, our network abstains from\nconstraining the reconstructed word to be from a given bag-of-words, unlike\nprevious studies. The success of our proposed approach, offers promise for the\nemployment of a BCI interface by patients in unfettered, naturalistic\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 16:07:38 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Sheth", "Janaki", ""], ["Tankus", "Ariel", ""], ["Tran", "Michelle", ""], ["Pouratian", "Nader", ""], ["Fried", "Itzhak", ""], ["Speier", "William", ""]]}, {"id": "1907.04390", "submitter": "Qinmeng Zou", "authors": "Frederic Magoules and Qinmeng Zou", "title": "A Novel Contactless Human Machine Interface based on Machine Learning", "comments": null, "journal-ref": "16th International Symposium on Distributed Computing and\n  Applications for Business Engineering and Science (DCABES), 2017, IEEE", "doi": "10.1109/dcabes.2017.37", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a global framework that enables contactless human\nmachine interaction using computer vision and machine learning techniques. The\nmain originality of our framework is that only a very simple image acquisition\ndevice, as a computer camera, is sufficient to establish a rich human machine\ninteraction as traditional devices such as mouse or keyboard. This framework is\nbased on well known computer vision techniques and efficient machine learning\ntechniques are used to detect and track user hand gestures so the end user can\ncontrol his computer using virtual interfaces with very simple gestures.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:16:57 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Magoules", "Frederic", ""], ["Zou", "Qinmeng", ""]]}, {"id": "1907.04393", "submitter": "Qinmeng Zou", "authors": "Frederic Magoules and Qinmeng Zou", "title": "GPU Accelerated Contactless Human Machine Interface for Driving Car", "comments": null, "journal-ref": "16th International Symposium on Distributed Computing and\n  Applications for Business Engineering and Science (DCABES), 2017, IEEE", "doi": "10.1109/dcabes.2017.9", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an original contactless human machine interface for\ndriving car. The proposed framework is based on the image sent by a simple\ncamera device, which is then processed by various computer vision algorithms.\nThese algorithms allow the isolation of the user's hand on the camera frame and\ntranslate its movements into orders sent to the computer in a real time\nprocess. The optimization of the implemented algorithms on graphics processing\nunit leads to real time interaction between the user, the computer and the\nmachine. The user can easily modify or create the interfaces displayed by the\nproposed framework to fit his personnel needs. A contactless driving car\ninterface is here produced to illustrate the principle of our framework.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 20:21:01 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Magoules", "Frederic", ""], ["Zou", "Qinmeng", ""]]}, {"id": "1907.04435", "submitter": "Fabio Miranda", "authors": "Fabio Miranda, Harish Doraiswamy, Marcos Lage, Luc Wilson, Mondrian\n  Hsieh, Claudio T. Silva", "title": "Shadow Accrual Maps: Efficient Accumulation of City-Scale Shadows Over\n  Time", "comments": "Video: https://www.youtube.com/watch?v=LsZv23d1LyM, Data:\n  https://github.com/ViDA-NYU/shadow-accrual-maps", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics (Volume:\n  25, Issue: 3, Mar. 2019)", "doi": "10.1109/TVCG.2018.2802945", "report-no": null, "categories": "cs.GR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale shadows from buildings in a city play an important role in\ndetermining the environmental quality of public spaces. They can be both\nbeneficial, such as for pedestrians during summer, and detrimental, by\nimpacting vegetation and by blocking direct sunlight. Determining the effects\nof shadows requires the accumulation of shadows over time across different\nperiods in a year. In this paper, we propose a simple yet efficient class of\napproach that uses the properties of sun movement to track the changing\nposition of shadows within a fixed time interval. We use this approach to\nextend two commonly used shadowing techniques, shadow maps and ray tracing, and\ndemonstrate the efficiency of our approach. Our technique is used to develop an\ninteractive visual analysis system, Shadow Profiler, targeted at city planners\nand architects that allows them to test the impact of shadows for different\ndevelopment scenarios. We validate the usefulness of this system through case\nstudies set in Manhattan, a dense borough of New York City.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:03:40 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Miranda", "Fabio", ""], ["Doraiswamy", "Harish", ""], ["Lage", "Marcos", ""], ["Wilson", "Luc", ""], ["Hsieh", "Mondrian", ""], ["Silva", "Claudio T.", ""]]}, {"id": "1907.04446", "submitter": "Travis Mandel", "authors": "Travis Mandel, Jahnu Best, Randall H. Tanaka, Hiram Temple, Chansen\n  Haili, Kayla Schlectinger, Roy Szeto", "title": "Let's Keep It Safe: Designing User Interfaces that Allow Everyone to\n  Contribute to AI Safety", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When AI systems are granted the agency to take impactful actions in the real\nworld, there is an inherent risk that these systems behave in ways that are\nharmful. Typically, humans specify constraints on the AI system to prevent\nharmful behavior; however, very little work has studied how best to facilitate\nthis difficult constraint specification process. In this paper, we study how to\ndesign user interfaces that make this process more effective and accessible,\nallowing people with a diversity of backgrounds and levels of expertise to\ncontribute to this task. We first present a task design in which workers\nevaluate the safety of individual state-action pairs, and propose several\nvariants of this task with improved task design and filtering mechanisms.\nAlthough this first design is easy to understand, it scales poorly to large\nstate spaces. Therefore, we develop a new user interface that allows workers to\nwrite constraint rules without any programming. Despite its simplicity, we show\nthat our rule construction interface retains full expressiveness. We present\nexperiments utilizing crowdworkers to help address an important real-world AI\nsafety problem in the domain of education. Our results indicate that our novel\nworker filtering and explanation methods outperform baseline approaches, and\nour rule-based interface allows workers to be much more efficient while\nimproving data quality.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:40:51 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Mandel", "Travis", ""], ["Best", "Jahnu", ""], ["Tanaka", "Randall H.", ""], ["Temple", "Hiram", ""], ["Haili", "Chansen", ""], ["Schlectinger", "Kayla", ""], ["Szeto", "Roy", ""]]}, {"id": "1907.04463", "submitter": "Nathan Brugnone", "authors": "Nathan Brugnone (1), Alex Gonopolskiy (2), Mark W. Moyle (3), Manik\n  Kuchroo (3), David van Dijk (3), Kevin R. Moon (4), Daniel Colon-Ramos (3),\n  Guy Wolf (5), Matthew J. Hirn (1) and Smita Krishnaswamy (3) ((1) Michigan\n  State University, (2) PicnicHealth, (3) Yale University, (4) Utah State\n  University, (5) Universit\\'e de Montr\\'eal)", "title": "Coarse Graining of Data via Inhomogeneous Diffusion Condensation", "comments": "14 pages, 7 figures", "journal-ref": "Proceedings of the 2019 IEEE International Conference on Big Data,\n  pages 2624-2633, 2019", "doi": "10.1109/BigData47090.2019.9006013", "report-no": null, "categories": "cs.HC cs.CV cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data often has emergent structure that exists at multiple levels of\nabstraction, which are useful for characterizing complex interactions and\ndynamics of the observations. Here, we consider multiple levels of abstraction\nvia a multiresolution geometry of data points at different granularities. To\nconstruct this geometry we define a time-inhomogeneous diffusion process that\neffectively condenses data points together to uncover nested groupings at\nlarger and larger granularities. This inhomogeneous process creates a deep\ncascade of intrinsic low pass filters on the data affinity graph that are\napplied in sequence to gradually eliminate local variability while adjusting\nthe learned data geometry to increasingly coarser resolutions. We provide\nvisualizations to exhibit our method as a continuously-hierarchical clustering\nwith directions of eliminated variation highlighted at each step. The utility\nof our algorithm is demonstrated via neuronal data condensation, where the\nconstructed multiresolution data geometry uncovers the organization, grouping,\nand connectivity between neurons.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 00:08:07 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 23:43:22 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 20:12:26 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Brugnone", "Nathan", ""], ["Gonopolskiy", "Alex", ""], ["Moyle", "Mark W.", ""], ["Kuchroo", "Manik", ""], ["van Dijk", "David", ""], ["Moon", "Kevin R.", ""], ["Colon-Ramos", "Daniel", ""], ["Wolf", "Guy", ""], ["Hirn", "Matthew J.", ""], ["Krishnaswamy", "Smita", ""]]}, {"id": "1907.04487", "submitter": "Minkesh Asati", "authors": "Minkesh Asati, Taizo Miyachi", "title": "A Short Virtual Reality Mindfulness Meditation Training For Regaining\n  Sustained Attention", "comments": "7 pages, 5 figures, ICSHM, IS-SHMPARIS-12049-11246", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to focus one's attention underlies success in many everyday\ntasks, but voluntary attention cannot be sustained for a long period of time.\nSeveral studies indicate that attention training using computer-based exercises\ncan lead to improved attention in children and adults. a major goal of recent\nresearch is to create a short (10 minutes) and effective VR Mindfulness\nmeditation particularly designed for regaining or improving sustained\nattention. In this study, we have created a custom virtually relaxing\nenvironment including an archery game with multiple targets. In the experiment,\nthe attention span of 12 adults are tested before and after the virtual reality\nsession by a non-action video game ([19]) score and Muse headband EEG-signals.\nAfter the 10-minute virtual reality session participants' game scores increased\n(according to game experience): for the beginner by 275%, for intermediate by\n107%, and for an expert by 17%. For Muse headband data, calm points increased\nby 250% irrespective of the participants gaming experiences. After the\nexperiment, all participants reported feeling recharged to continue their daily\nactivities.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 02:18:44 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Asati", "Minkesh", ""], ["Miyachi", "Taizo", ""]]}, {"id": "1907.04702", "submitter": "Andrey Krekhov", "authors": "Andrey Krekhov, Sebastian Cmentowski, Andre Waschk, and Jens Kr\\\"uger", "title": "Deadeye Visualization Revisited: Investigation of Preattentiveness and\n  Applicability in Virtual Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizations rely on highlighting to attract and guide our attention. To\nmake an object of interest stand out independently from a number of\ndistractors, the underlying visual cue, e.g., color, has to be preattentive. In\nour prior work, we introduced Deadeye as an instantly recognizable highlighting\ntechnique that works by rendering the target object for one eye only. In\ncontrast to prior approaches, Deadeye excels by not modifying any visual\nproperties of the target. However, in the case of 2D visualizations, the method\nrequires an additional setup to allow dichoptic presentation, which is a\nconsiderable drawback. As a follow-up to requests from the community, this\npaper explores Deadeye as a highlighting technique for 3D visualizations,\nbecause such stereoscopic scenarios support dichoptic presentation out of the\nbox. Deadeye suppresses binocular disparities for the target object, so we\ncannot assume the applicability of our technique as a given fact. With this\nmotivation, the paper presents quantitative evaluations of Deadeye in VR,\nincluding configurations with multiple heterogeneous distractors as an\nimportant robustness challenge. After confirming the preserved preattentiveness\n(all average accuracies above 90 %) under such real-world conditions, we\nexplore VR volume rendering as an example application scenario for Deadeye. We\ndepict a possible workflow for integrating our technique, conduct an\nexploratory survey to demonstrate benefits and limitations, and finally provide\nrelated design implications.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 13:17:00 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Krekhov", "Andrey", ""], ["Cmentowski", "Sebastian", ""], ["Waschk", "Andre", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "1907.04730", "submitter": "Andrey Krekhov", "authors": "Andrey Krekhov, Michael Michalski, and Jens Kr\\\"uger", "title": "Integrating Visualization Literacy into Computer Graphics Education\n  Using the Example of Dear Data", "comments": null, "journal-ref": null, "doi": "10.2312/eged.20191022", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of visual communication we are facing is rapidly increasing, and\nskills to process, understand, and generate visual representations are in high\ndemand. Especially students focusing on computer graphics and visualization can\nbenefit from a more diverse education on visual literacy, as they often have to\nwork on graphical representations for broad masses after their graduation. Our\nproposed teaching approach incorporates basic design thinking principles into\ntraditional visualization and graphics education. Our course was inspired by\nthe book Dear Data that was the subject of a lively discussion at the closing\ncapstone of IEEE VIS 2017. The paper outlines our 12-week teaching experiment\nand summarizes the results extracted from accompanying questionnaires and\ninterviews. In particular, we provide insights into the creation process and\npain points of visualization novices, discuss the observed interplay between\nvisualization tasks and design thinking, and finally draw design implications\nfor visual literacy education in general.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 13:57:20 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Krekhov", "Andrey", ""], ["Michalski", "Michael", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "1907.04739", "submitter": "Christoph Anderson", "authors": "Christoph Anderson, Judith Simone Heinisch, Sandra Ohly, Klaus David,\n  Veljko Pejovic", "title": "The Impact of Private and Work-Related Smartphone Usage on\n  Interruptibility", "comments": "6 pages, 3 figures", "journal-ref": "Adjunct Proceedings of the 2019 ACM International Joint Conference\n  on Pervasive and Ubiquitous Computing and the 2019 International Symposium on\n  Wearable Computers (UbiComp/ISWC '19 Adjunct)", "doi": "10.1145/3341162.3344845", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, the effects of interruptions through mobile notifications\nhave been extensively researched in the field of Human-Computer Interaction.\nBreakpoints in tasks and activities, cognitive load, and personality traits\nhave all been shown to correlate with individuals' interruptibility. However,\nconcepts that explain interruptibility in a broader sense are needed to provide\na holistic understanding of its characteristics. In this paper, we build upon\nthe theory of social roles to conceptualize and investigate the correlation\nbetween individuals' private and work-related smartphone usage and their\ninterruptibility. Through our preliminary study with four participants over 11\nweeks, we found that application sequences on smartphones correlate with\nindividuals' private and work roles. We observed that participants engaged in\nthese roles tend to follow specific interruptibility strategies - integrating,\ncombining, or segmenting private and work-related engagements. Understanding\nthese strategies breaks new ground for attention and interruption management\nsystems in ubiquitous computing.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 14:11:19 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Anderson", "Christoph", ""], ["Heinisch", "Judith Simone", ""], ["Ohly", "Sandra", ""], ["David", "Klaus", ""], ["Pejovic", "Veljko", ""]]}, {"id": "1907.05131", "submitter": "Christoph Kinkeldey", "authors": "Christoph Kinkeldey, Claudia M\\\"uller-Birn, Tom G\\\"ulenman, Jesse\n  Josua Benjamin and Aaron Halfaker", "title": "PreCall: A Visual Interface for Threshold Optimization in ML Model\n  Selection", "comments": "HCML Perspectives Workshop at CHI 2019, May 04, 2019, Glasgow", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning systems are ubiquitous in various kinds of digital\napplications and have a huge impact on our everyday life. But a lack of\nexplainability and interpretability of such systems hinders meaningful\nparticipation by people, especially by those without a technical background.\nInteractive visual interfaces (e.g., providing means for manipulating\nparameters in the user interface) can help tackle this challenge. In this paper\nwe present PreCall, an interactive visual interface for ORES, a machine\nlearning-based web service for Wikimedia projects such as Wikipedia. While ORES\ncan be used for a number of settings, it can be challenging to translate\nrequirements from the application domain into formal parameter sets needed to\nconfigure the ORES models. Assisting Wikipedia editors in finding damaging\nedits, for example, can be realized at various stages of automatization, which\nmight impact the precision of the applied model. Our prototype PreCall attempts\nto close this translation gap by interactively visualizing the relationship\nbetween major model metrics (recall, precision, false positive rate) and a\nparameter (the threshold between valuable and damaging edits). Furthermore,\nPreCall visualizes the probable results for the current model configuration to\nimprove the human's understanding of the relationship between metrics and\noutcome when using ORES. We describe PreCall's components and present a use\ncase that highlights the benefits of our approach. Finally, we pose further\nresearch questions we would like to discuss during the workshop.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 12:03:57 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Kinkeldey", "Christoph", ""], ["M\u00fcller-Birn", "Claudia", ""], ["G\u00fclenman", "Tom", ""], ["Benjamin", "Jesse Josua", ""], ["Halfaker", "Aaron", ""]]}, {"id": "1907.05220", "submitter": "Andrey Krekhov", "authors": "Andrey Krekhov, Sebastian Cmentowski, Jens Kr\\\"uger", "title": "The Illusion of Animal Body Ownership and Its Potential for Virtual\n  Reality Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality offers the unique possibility to experience a virtual\nrepresentation as our own body. In contrast to previous research that\npredominantly studied this phenomenon for humanoid avatars, our work focuses on\nvirtual animals. In this paper, we discuss different body tracking approaches\nto control creatures such as spiders or bats and the respective virtual body\nownership effects. Our empirical results demonstrate that virtual body\nownership is also applicable for nonhumanoids and can even outperform\nhuman-like avatars in certain cases. An additional survey confirms the general\ninterest of people in creating such experiences and allows us to initiate a\nbroad discussion regarding the applicability of animal embodiment for\neducational and entertainment purposes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 14:12:04 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Krekhov", "Andrey", ""], ["Cmentowski", "Sebastian", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "1907.05275", "submitter": "Yaohua Xie", "authors": "Yaohua Xie", "title": "Improving the resolution of microscope by deconvolution after dense scan", "comments": "This work has been patented, thereby is not open-source", "journal-ref": null, "doi": "10.5281/zenodo.3353772", "report-no": null, "categories": "eess.IV cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution microscopes (such as STED) illuminate samples with a tiny\nspot, and achieve very high resolution. But structures smaller than the spot\ncannot be resolved in this way. Therefore, we propose a technique to solve this\nproblem. It is termed \"Deconvolution after Dense Scan (DDS)\". First, a\npreprocessing stage is introduced to eliminate the optical uncertainty of the\nperipheral areas around the sample's ROI (Region of Interest). Then, the ROI is\nscanned densely together with its peripheral areas. Finally, the high\nresolution image is recovered by deconvolution. The proposed technique does not\nneed to modify the apparatus much, and is mainly performed by algorithm.\nSimulation experiments show that the technique can further improve the\nresolution of super-resolution microscopes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 03:35:50 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 01:57:19 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 03:05:19 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Xie", "Yaohua", ""]]}, {"id": "1907.05454", "submitter": "Dietmar Offenhuber", "authors": "Dietmar Offenhuber", "title": "Data by Proxy -- Material Traces as Autographic Visualizations", "comments": "accepted at IEEE Vis 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information visualization limits itself, per definition, to the domain of\nsymbolic information. This paper discusses arguments why the field should also\nconsider forms of data that are not symbolically encoded, including physical\ntraces and material indicators. Continuing a provocation presented by Pat\nHanrahan in his 2004 IEEE Vis capstone address, this paper compares physical\ntraces to visualizations and describes the techniques and visual practices for\nproducing, revealing, and interpreting them. By contrasting information\nvisualization with a speculative counter model of autographic visualization,\nthis paper examines the design principles for material data. Autographic\nvisualization addresses limitations of information visualization, such as the\ninability to directly reflect the material circumstances of data generation.\nThe comparison between the two models allows probing the epistemic assumptions\nbehind information visualization and uncovers linkages with the rich history of\nscientific visualization and trace reading.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 19:13:10 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Offenhuber", "Dietmar", ""]]}, {"id": "1907.05507", "submitter": "Alexandros Papangelis", "authors": "Alexandros Papangelis, Yi-Chia Wang, Piero Molino, Gokhan Tur", "title": "Collaborative Multi-Agent Dialogue Model Training Via Reinforcement\n  Learning", "comments": "SIGDIAL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first complete attempt at concurrently training conversational\nagents that communicate only via self-generated language. Using DSTC2 as seed\ndata, we trained natural language understanding (NLU) and generation (NLG)\nnetworks for each agent and let the agents interact online. We model the\ninteraction as a stochastic collaborative game where each agent (player) has a\nrole (\"assistant\", \"tourist\", \"eater\", etc.) and their own objectives, and can\nonly interact via natural language they generate. Each agent, therefore, needs\nto learn to operate optimally in an environment with multiple sources of\nuncertainty (its own NLU and NLG, the other agent's NLU, Policy, and NLG). In\nour evaluation, we show that the stochastic-game agents outperform deep\nlearning based supervised baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 22:05:48 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 11:28:40 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Papangelis", "Alexandros", ""], ["Wang", "Yi-Chia", ""], ["Molino", "Piero", ""], ["Tur", "Gokhan", ""]]}, {"id": "1907.05609", "submitter": "Qianwen Wang", "authors": "Qianwen Wang, Zhen Li, Siwei Fu, Weiwei Cui, Huamin Qu", "title": "Narvis: Authoring Narrative Slideshows for Introducing Data\n  Visualization Designs", "comments": "9 pages, published at IEEE InfoVis 2018,", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, vol. 25,\n  no. 1, pp. 779-788, Jan. 2019", "doi": "10.1109/TVCG.2018.2865232", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual designs can be complex in modern data visualization systems, which\nposes special challenges for explaining them to the non-experts. However, few\nif any presentation tools are tailored for this purpose. In this study, we\npresent Narvis, a slideshow authoring tool designed for introducing data\nvisualizations to non-experts. Narvis targets two types of end-users: teachers,\nexperts in data visualization who produce tutorials for explaining a data\nvisualization, and students, non-experts who try to understand visualization\ndesigns through tutorials. We present an analysis of requirements through close\ndiscussions with the two types of end-users. The resulting considerations guide\nthe design and implementation of Narvis. Additionally, to help teachers better\norganize their introduction slideshows, we specify a data visualization as a\nhierarchical combination of components, which are automatically detected and\nextracted by Narvis. The teachers craft an introduction slideshow through first\norganizing these components, and then explaining them sequentially. A series of\ntemplates are provided for adding annotations and animations to improve\nefficiency during the authoring process. We evaluate Narvis through a\nqualitative analysis of the authoring experience, and a preliminary evaluation\nof the generated slideshows.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 08:14:18 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Wang", "Qianwen", ""], ["Li", "Zhen", ""], ["Fu", "Siwei", ""], ["Cui", "Weiwei", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.05671", "submitter": "Graham Spinks", "authors": "Graham Spinks, Marie-Francine Moens", "title": "Justifying Diagnosis Decisions by Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.jbi.2019.103248", "report-no": null, "categories": "cs.LG cs.CL cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An integrated approach is proposed across visual and textual data to both\ndetermine and justify a medical diagnosis by a neural network. As deep learning\ntechniques improve, interest grows to apply them in medical applications. To\nenable a transition to workflows in a medical context that are aided by machine\nlearning, the need exists for such algorithms to help justify the obtained\noutcome so human clinicians can judge their validity. In this work, deep\nlearning methods are used to map a frontal X-Ray image to a continuous textual\nrepresentation. This textual representation is decoded into a diagnosis and the\nassociated textual justification that will help a clinician evaluate the\noutcome. Additionally, more explanatory data is provided for the diagnosis by\ngenerating a realistic X-Ray that belongs to the nearest alternative diagnosis.\nWith a clinical expert opinion study on a subset of the X-Ray data set from the\nIndiana University hospital network, we demonstrate that our justification\nmechanism significantly outperforms existing methods that use saliency maps.\nWhile performing multi-task training with multiple loss functions, our method\nachieves excellent diagnosis accuracy and captioning quality when compared to\ncurrent state-of-the-art single-task methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 10:51:48 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Spinks", "Graham", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1907.05674", "submitter": "Apdullah Yayik", "authors": "Apdullah Yay{\\i}k, Yakup Kutlu, G\\\"okhan Altan", "title": "Deep Learning with ConvNET Predicts Imagery Tasks Through EEG", "comments": "5 pages, 2 figures, springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning with convolutional neural networks (ConvNets) have dramatically\nimproved learning capabilities of computer vision applications just through\nconsidering raw data without any prior feature extraction. Nowadays, there is\nrising curiosity in interpreting and analyzing electroencephalography (EEG)\ndynamics with ConvNets. Our study focused on ConvNets of different structures,\nconstructed for predicting imagined left and right movements on a\nsubject-independent basis through raw EEG data. Results showed that recently\nadvanced methods in machine learning field, i.e. adaptive moments and batch\nnormalization together with dropout strategy, improved ConvNets predicting\nability, outperforming that of conventional fully-connected neural networks\nwith widely-used spectral features.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 11:10:05 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Yay\u0131k", "Apdullah", ""], ["Kutlu", "Yakup", ""], ["Altan", "G\u00f6khan", ""]]}, {"id": "1907.05800", "submitter": "Swakkhar Shatabda", "authors": "Md. Tashfiqul Bari, Tanvir Hassan, Raisa Tabassum, Zubaida Ahmed and\n  Swakkhar Shatabda", "title": "Find It: A Novel Way to Learn Through Play", "comments": null, "journal-ref": "International Joint Conference on Computational Intelligence\n  (IJCCI 2018)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Autism Spectrum Disorder (ASD) is the area where many researches enduring\nlike Magnetic Resonance Imaging (MRI), called diffusion tensor imaging, Early\nStart Denver Model (ESDM) to provide an easier life for the people diagnosed.\nAfter years and years of combined funding sources from public and private\nfunding, these researches show great promises in recent years. In this paper,\nwe have tried to show a way how children with Down Syndrome Autism can learn\nthrough game therapy. These game therapies have shown an immense number of\nimprovements among those children to learn alphabets along with developing\ntheir motor skills and memory challenges.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 15:34:50 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Bari", "Md. Tashfiqul", ""], ["Hassan", "Tanvir", ""], ["Tabassum", "Raisa", ""], ["Ahmed", "Zubaida", ""], ["Shatabda", "Swakkhar", ""]]}, {"id": "1907.05888", "submitter": "Apdullah Yayik", "authors": "Apdullah Yay{\\i}k, Yakup Kutlu, G\\\"okhan Altan", "title": "Regularized HessELM and Inclined Entropy Measurement for Congestive\n  Heart Failure Prediction", "comments": "9 pages, 3 figures, neuroprocessing letter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.NA eess.SP math.NA physics.med-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our study concerns with automated predicting of congestive heart failure\n(CHF) through the analysis of electrocardiography (ECG) signals. A novel\nmachine learning approach, regularized hessenberg decomposition based extreme\nlearning machine (R-HessELM), and feature models; squared, circled, inclined\nand grid entropy measurement were introduced and used for prediction of CHF.\nThis study proved that inclined entropy measurements features well represent\ncharacteristics of ECG signals and together with R-HessELM approach overall\naccuracy of 98.49% was achieved.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 11:11:02 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Yay\u0131k", "Apdullah", ""], ["Kutlu", "Yakup", ""], ["Altan", "G\u00f6khan", ""]]}, {"id": "1907.05931", "submitter": "Abdulaziz Alaboudi", "authors": "Abdulaziz Alaboudi and Thomas D. LaToza", "title": "An Exploratory Study of Live-Streamed Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In live-streamed programming, developers broadcast their development work on\nopen source projects using streaming media such as YouTube or Twitch. Sessions\nare first announced by a developer acting as the streamer, inviting other\ndevelopers to join and interact as watchers using chat. To better understand\nthe characteristics, motivations, and challenges in live-streamed programming,\nwe analyzed 20 hours of live-streamed programming videos and surveyed 7\nstreamers about their experiences. The results reveal that live-streamed\nprogramming shares some of the characteristics and benefits of pair\nprogramming, but differs in the nature of the relationship between the streamer\nand watchers. We also found that streamers are motivated by knowledge sharing,\nsocializing, and building an online identity, but face challenges with tool\nlimitations and maintaining engagement with watchers. We discuss the\nimplications of these findings, identify limitations with current tools, and\npropose design recommendations for new forms of tools to better supporting\nlive-streamed programming.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 19:56:33 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Alaboudi", "Abdulaziz", ""], ["LaToza", "Thomas D.", ""]]}, {"id": "1907.06005", "submitter": "Xiang Zhang", "authors": "Yu Gu, Xiang Zhang, Zhi Liu and Fuji Ren", "title": "BeSense: Leveraging WiFi Channel Data and Computational Intelligence for\n  Behavior Analysis", "comments": "11 pages accepted by IEEE Computational Intelligence Magazine", "journal-ref": null, "doi": "10.1109/MCI.2019.2937610", "report-no": null, "categories": "cs.HC cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever evolving informatics technology has gradually bounded human and\ncomputer in a compact way. Understanding user behavior becomes a key enabler in\nmany fields such as sedentary-related healthcare, human-computer interaction\n(HCI) and affective computing. Traditional sensor-based and vision-based user\nbehavior analysis approaches are obtrusive in general, hindering their usage in\nrealworld. Therefore, in this article, we first introduce WiFi signal as a new\nsource instead of sensor and vision for unobtrusive user behaviors analysis.\nThen we design BeSense, a contactless behavior analysis system leveraging\nsignal processing and computational intelligence over WiFi channel state\ninformation (CSI). We prototype BeSense on commodity low-cost WiFi devices and\nevaluate its performance in realworld environments. Experimental results have\nverified its effectiveness in recognizing user behaviors.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 03:31:14 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 10:48:13 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Gu", "Yu", ""], ["Zhang", "Xiang", ""], ["Liu", "Zhi", ""], ["Ren", "Fuji", ""]]}, {"id": "1907.06279", "submitter": "Seyedeh Zahra Razavi", "authors": "S. Zahra Razavi, Lenhart K. Schubert, Kimberly A. Van Orden, and\n  Mohammad Rafayet Ali", "title": "Discourse Behavior of Older Adults Interacting With a Dialogue Agent\n  Competent in Multiple Topics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some results concerning the dialogue behavior and inferred\nsentiment of a group of older adults interacting with a computer-based avatar.\nOur avatar is unique in its ability to hold natural dialogues on a wide range\nof everyday topics---27 topics in three groups, developed with the help of\ngerontologists. The three groups vary in ``degrees of intimacy\", and as such in\ndegrees of difficulty for the user. Each participant interacted with the avatar\nfor 7-9 sessions over a period of 3-4 weeks; analysis of the dialogues reveals\ncorrelations such as greater verbosity for more difficult topics, increasing\nverbosity with successive sessions, especially for more difficult topics,\nstronger sentiment on topics concerned with life goals rather than routine\nactivities, and stronger self-disclosure for more intimate topics. In addition\nto their intrinsic interest, these results also reflect positively on the\nsophistication of our dialogue system.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 20:41:46 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Razavi", "S. Zahra", ""], ["Schubert", "Lenhart K.", ""], ["Van Orden", "Kimberly A.", ""], ["Ali", "Mohammad Rafayet", ""]]}, {"id": "1907.06327", "submitter": "Rohan Lekhwani", "authors": "Rohan Lekhwani, Bhupendra Singh", "title": "FastV2C-HandNet: Fast Voxel to Coordinate Hand Pose Estimation with 3D\n  Convolutional Neural Networks", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand pose estimation from monocular depth images has been an important and\nchallenging problem in the Computer Vision community. In this paper, we present\na novel approach to estimate 3D hand joint locations from 2D depth images.\nUnlike most of the previous methods, our model captures the 3D spatial\ninformation from a depth image thereby giving it a greater understanding of the\ninput. We voxelize the input depth map to capture the 3D features of the input\nand perform 3D data augmentations to make our network robust to real-world\nimages. Our network is trained in an end-to-end manner which reduces time and\nspace complexity significantly when compared to other methods. Through\nextensive experiments, we show that our model outperforms state-of-the-art\nmethods with respect to the time it takes to train and predict 3D hand joint\nlocations. This makes our method more suitable for real-world hand pose\nestimation scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 04:04:01 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 07:06:18 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 14:31:45 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Lekhwani", "Rohan", ""], ["Singh", "Bhupendra", ""]]}, {"id": "1907.06360", "submitter": "Aaditeshwar Seth", "authors": "Aaditeshwar Seth", "title": "The Elusive Model of Technology, Media, Social Development, and\n  Financial Sustainability", "comments": "Case study prepared for a forthcoming book - Next Frontier Solutions:\n  Harnessing Technology for Social Good", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We recount in this essay the decade-long story of Gram Vaani, a social\nenterprise with a vision to build appropriate ICTs (Information and\nCommunication Technologies) for participatory media in rural and low-income\nsettings, to bring about social development and community empowerment. Other\nsocial enterprises will relate to the learning gained and the strategic pivots\nthat Gram Vaani had to undertake to survive and deliver on its mission, while\nsearching for a robust financial sustainability model. While we believe the\nideal model still remains elusive, we conclude this essay with an open question\nabout the reason to differentiate between different kinds of enterprises -\ncommercial or social, for-profit or not-for-profit - and argue that all\nenterprises should have an ethical underpinning to their work.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 08:20:02 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Seth", "Aaditeshwar", ""]]}, {"id": "1907.06399", "submitter": "Vanessa Pe\\~na-Araya", "authors": "Vanessa Pe\\~na-Araya, Emmanuel Pietriga, Anastasia Bezerianos", "title": "A Comparison of Visualizations for Identifying Correlation over Space\n  and Time", "comments": "Accepted for presentation at IEEE VIS 2019, to be held October 20-25\n  in Vancouver, Canada; will be published in a special issue of IEEE\n  Transactions on Visualization and Computer Graphics (TVCG)", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934807", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing the relationship between two or more variables over space and time\nis essential in many domains. For instance, looking, for different countries,\nat the evolution of both the life expectancy at birth and the fertility rate\nwill give an overview of their demographics. The choice of visual\nrepresentation for such multivariate data is key to enabling analysts to\nextract patterns and trends. Prior work has compared geo-temporal visualization\ntechniques for a single thematic variable that evolves over space and time, or\nfor two variables at a specific point in time. But how effective visualization\ntechniques are at communicating correlation between two variables that evolve\nover space and time remains to be investigated. We report on a study comparing\nthree techniques that are representative of different strategies to visualize\ngeo-temporal multivariate data: either juxtaposing all locations for a given\ntime step, or juxtaposing all time steps for a given location; and encoding\nthematic attributes either using symbols overlaid on top of map features, or\nusing visual channels of the map features themselves. Participants performed a\nseries of tasks that required them to identify if two variables were correlated\nover time and if there was a pattern in their evolution. Tasks varied in\ngranularity for both dimensions: time (all time steps, a subrange of steps, one\nstep only) and space (all locations, locations in a subregion, one location\nonly). Our results show that a visualization's effectiveness depends strongly\non the task to be carried out. Based on these findings we present a set of\ndesign guidelines about geo-temporal visualization techniques for communicating\ncorrelation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 09:49:47 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 13:16:26 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Pe\u00f1a-Araya", "Vanessa", ""], ["Pietriga", "Emmanuel", ""], ["Bezerianos", "Anastasia", ""]]}, {"id": "1907.06535", "submitter": "Jose Cambronero Sanchez", "authors": "Jos\\'e Pablo Cambronero and Jiasi Shen and J\\\"urgen Cito and Elena\n  Glassman and Martin Rinard", "title": "Characterizing Developer Use of Automatically Generated Patches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study that characterizes the way developers use automatically\ngenerated patches when fixing software defects. Our study tasked two groups of\ndevelopers with repairing defects in C programs. Both groups were provided with\nthe defective line of code. One was also provided with five automatically\ngenerated and validated patches, all of which modified the defective line of\ncode, and one of which was correct. Contrary to our initial expectations, the\ngroup with access to the generated patches did not produce more correct patches\nand did not produce patches in less time. We characterize the main behaviors\nobserved in experimental subjects: a focus on understanding the defect and the\nrelationship of the patches to the original source code. Based on this\ncharacterization, we highlight various potentially productive directions for\nfuture developer-centric automatic patch generation systems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 15:02:23 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 19:27:08 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Cambronero", "Jos\u00e9 Pablo", ""], ["Shen", "Jiasi", ""], ["Cito", "J\u00fcrgen", ""], ["Glassman", "Elena", ""], ["Rinard", "Martin", ""]]}, {"id": "1907.06563", "submitter": "Sudip Vhaduri", "authors": "Sudip Vhaduri and Christian Poellabauer", "title": "Summary: Multi-modal Biometric-based Implicit Authentication of Wearable\n  Device Users", "comments": "This will be published in the IEEE Biometrics Council newsletter,\n  volume 31, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Things (IoT) is increasingly empowering people with an\ninterconnected world of physical objects ranging from smart buildings to\nportable smart devices such as wearables. With recent advances in mobile\nsensing, wearables have become a rich collection of portable sensors and are\nable to provide various types of services including tracking of health and\nfitness, making financial transactions, and unlocking smart locks and vehicles.\nMost of these services are delivered based on users' confidential and personal\ndata, which are stored on these wearables. Existing explicit authentication\napproaches (i.e., PINs or pattern locks) for wearables suffer from several\nlimitations, including small or no displays, risk of shoulder surfing, and\nusers' recall burden. Oftentimes, users completely disable security features\nout of convenience. Therefore, there is a need for a burden-free (implicit)\nauthentication mechanism for wearable device users based on easily obtainable\nbiometric data. In this paper, we present an implicit wearable device user\nauthentication mechanism using combinations of three types of coarse-grain\nminute-level biometrics: behavioral (step counts), physiological (heart rate),\nand hybrid (calorie burn and metabolic equivalent of task). From our analysis\nof over 400 Fitbit users from a 17-month long health study, we are able to\nauthenticate subjects with average accuracy values of around .93 (sedentary)\nand .90 (non-sedentary) with equal error rates of .05 using binary SVM\nclassifiers. Our findings also show that the hybrid biometrics perform better\nthan other biometrics and behavioral biometrics do not have a significant\nimpact, even during non-sedentary periods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:06:50 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Vhaduri", "Sudip", ""], ["Poellabauer", "Christian", ""]]}, {"id": "1907.06637", "submitter": "Cheng-Zhi Anna Huang", "authors": "Cheng-Zhi Anna Huang, Curtis Hawthorne, Adam Roberts, Monica\n  Dinculescu, James Wexler, Leon Hong, Jacob Howcroft", "title": "The Bach Doodle: Approachable music composition with machine learning at\n  scale", "comments": "Proceedings of the 18th International Society for Music Information\n  Retrieval Conference, ISMIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.LG eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To make music composition more approachable, we designed the first AI-powered\nGoogle Doodle, the Bach Doodle, where users can create their own melody and\nhave it harmonized by a machine learning model Coconet (Huang et al., 2017) in\nthe style of Bach. For users to input melodies, we designed a simplified\nsheet-music based interface. To support an interactive experience at scale, we\nre-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the\nbrowser and reduced its runtime from 40s to 2s by adopting dilated depth-wise\nseparable convolutions and fusing operations. We also reduced the model\ndownload size to approximately 400KB through post-training weight quantization.\nWe calibrated a speed test based on partial model evaluation time to determine\nif the harmonization request should be performed locally or sent to remote TPU\nservers. In three days, people spent 350 years worth of time playing with the\nBach Doodle, and Coconet received more than 55 million queries. Users could\nchoose to rate their compositions and contribute them to a public dataset,\nwhich we are releasing with this paper. We hope that the community finds this\ndataset useful for applications ranging from ethnomusicological studies, to\nmusic education, to improving machine learning models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 23:39:12 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Huang", "Cheng-Zhi Anna", ""], ["Hawthorne", "Curtis", ""], ["Roberts", "Adam", ""], ["Dinculescu", "Monica", ""], ["Wexler", "James", ""], ["Hong", "Leon", ""], ["Howcroft", "Jacob", ""]]}, {"id": "1907.06725", "submitter": "Sayanti Roy", "authors": "Sayanti Roy, Emily Kieson, Charles Abramson, Christopher Crick", "title": "Mutual Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, collaborative robots have begun to train humans to achieve complex\ntasks, and the mutual information exchange between them can lead to successful\nrobot-human collaborations. In this paper we demonstrate the application and\neffectiveness of a new approach called mutual reinforcement learning (MRL),\nwhere both humans and autonomous agents act as reinforcement learners in a\nskill transfer scenario over continuous communication and feedback. An\nautonomous agent initially acts as an instructor who can teach a novice human\nparticipant complex skills using the MRL strategy. While teaching skills in a\nphysical (block-building) ($n=34$) or simulated (Tetris) environment ($n=31$),\nthe expert tries to identify appropriate reward channels preferred by each\nindividual and adapts itself accordingly using an exploration-exploitation\nstrategy. These reward channel preferences can identify important behaviors of\nthe human participants, because they may well exercise the same behaviors in\nsimilar situations later. In this way, skill transfer takes place between an\nexpert system and a novice human operator. We divided the subject population\ninto three groups and observed the skill transfer phenomenon, analyzing it with\nSimpson\"s psychometric model. 5-point Likert scales were also used to identify\nthe cognitive models of the human participants. We obtained a shared cognitive\nmodel which not only improves human cognition but enhances the robot's\ncognitive strategy to understand the mental model of its human partners while\nbuilding a successful robot-human collaborative framework.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 20:10:29 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 00:42:57 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 19:52:13 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Roy", "Sayanti", ""], ["Kieson", "Emily", ""], ["Abramson", "Charles", ""], ["Crick", "Christopher", ""]]}, {"id": "1907.06809", "submitter": "Aaditeshwar Seth", "authors": "Aaditeshwar Seth", "title": "Ethical Underpinnings in the Design and Management of ICT Projects", "comments": "Presented at the ACM HCAI Summer School, India 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With a view towards understanding why undesirable outcomes often arise in ICT\nprojects, we draw attention to three aspects in this essay. First, we present\nseveral examples to show that incorporating an ethical framework in the design\nof an ICT system is not sufficient in itself, and that ethics need to guide the\ndeployment and ongoing management of the projects as well. We present a\nframework that brings together the objectives, design, and deployment\nmanagement of ICT projects as being shaped by a common underlying ethical\nsystem. Second, we argue that power-based equality should be incorporated as a\nkey underlying ethical value in ICT projects, to ensure that the project does\nnot reinforce inequalities in power relationships between the actors directly\nor indirectly associated with the project. We present a method to model ICT\nprojects to make legible its influence on the power relationships between\nvarious actors in the ecosystem. Third, we discuss that the ethical values\nunderlying any ICT project ultimately need to be upheld by the project teams,\nwhere certain factors like political ideologies or dispersed teams may affect\nthe rigour with which these ethical values are followed. These three aspects of\nhaving an ethical underpinning to the design and management of ICT projects,\nthe need for having a power-based equality principle for ICT projects, and the\nimportance of socialization of the project teams, needs increasing attention in\ntoday's age of ICT platforms where millions and billions of users interact on\nthe same platform but which are managed by only a few people.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 02:30:03 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Seth", "Aaditeshwar", ""]]}, {"id": "1907.06831", "submitter": "Fan Yang", "authors": "Fan Yang, Mengnan Du, Xia Hu", "title": "Evaluating Explanation Without Ground Truth in Interpretable Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretable Machine Learning (IML) has become increasingly important in\nmany real-world applications, such as autonomous cars and medical diagnosis,\nwhere explanations are significantly preferred to help people better understand\nhow machine learning systems work and further enhance their trust towards\nsystems. However, due to the diversified scenarios and subjective nature of\nexplanations, we rarely have the ground truth for benchmark evaluation in IML\non the quality of generated explanations. Having a sense of explanation quality\nnot only matters for assessing system boundaries, but also helps to realize the\ntrue benefits to human users in practical settings. To benchmark the evaluation\nin IML, in this article, we rigorously define the problem of evaluating\nexplanations, and systematically review the existing efforts from\nstate-of-the-arts. Specifically, we summarize three general aspects of\nexplanation (i.e., generalizability, fidelity and persuasibility) with formal\ndefinitions, and respectively review the representative methodologies for each\nof them under different tasks. Further, a unified evaluation framework is\ndesigned according to the hierarchical needs from developers and end-users,\nwhich could be easily adopted for different scenarios in practice. In the end,\nopen problems are discussed, and several limitations of current evaluation\ntechniques are raised for future explorations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 04:25:39 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 21:13:50 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Yang", "Fan", ""], ["Du", "Mengnan", ""], ["Hu", "Xia", ""]]}, {"id": "1907.07032", "submitter": "Arunesh Mathur", "authors": "Arunesh Mathur, Gunes Acar, Michael J. Friedman, Elena Lucherini,\n  Jonathan Mayer, Marshini Chetty, Arvind Narayanan", "title": "Dark Patterns at Scale: Findings from a Crawl of 11K Shopping Websites", "comments": "32 pages, 11 figures, ACM Conference on Computer-Supported\n  Cooperative Work and Social Computing (CSCW 2019)", "journal-ref": "Proceedings of the ACM Human-Computer Interaction, Vol. 3, CSCW,\n  Article 81 (November 2019)", "doi": "10.1145/3359183", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dark patterns are user interface design choices that benefit an online\nservice by coercing, steering, or deceiving users into making unintended and\npotentially harmful decisions. We present automated techniques that enable\nexperts to identify dark patterns on a large set of websites. Using these\ntechniques, we study shopping websites, which often use dark patterns to\ninfluence users into making more purchases or disclosing more information than\nthey would otherwise. Analyzing ~53K product pages from ~11K shopping websites,\nwe discover 1,818 dark pattern instances, together representing 15 types and 7\nbroader categories. We examine these dark patterns for deceptive practices, and\nfind 183 websites that engage in such practices. We also uncover 22 third-party\nentities that offer dark patterns as a turnkey solution. Finally, we develop a\ntaxonomy of dark pattern characteristics that describes the underlying\ninfluence of the dark patterns and their potential harm on user\ndecision-making. Based on our findings, we make recommendations for\nstakeholders including researchers and regulators to study, mitigate, and\nminimize the use of these patterns.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 14:29:55 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 12:22:56 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Mathur", "Arunesh", ""], ["Acar", "Gunes", ""], ["Friedman", "Michael J.", ""], ["Lucherini", "Elena", ""], ["Mayer", "Jonathan", ""], ["Chetty", "Marshini", ""], ["Narayanan", "Arvind", ""]]}, {"id": "1907.07178", "submitter": "Juliana Ferreira J", "authors": "Rafael Brand\\~ao, Joel Carbonera, Clarisse de Souza, Juliana Ferreira,\n  Bernardo Gon\\c{c}alves, Carla Leit\\~ao", "title": "Mediation Challenges and Socio-Technical Gaps for Explainable Deep\n  Learning Applications", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presumed data owners' right to explanations brought about by the General\nData Protection Regulation in Europe has shed light on the social challenges of\nexplainable artificial intelligence (XAI). In this paper, we present a case\nstudy with Deep Learning (DL) experts from a research and development\nlaboratory focused on the delivery of industrial-strength AI technologies. Our\naim was to investigate the social meaning (i.e. meaning to others) that DL\nexperts assign to what they do, given a richly contextualized and familiar\ndomain of application. Using qualitative research techniques to collect and\nanalyze empirical data, our study has shown that participating DL experts did\nnot spontaneously engage into considerations about the social meaning of\nmachine learning models that they build. Moreover, when explicitly stimulated\nto do so, these experts expressed expectations that, with real-world DL\napplication, there will be available mediators to bridge the gap between\ntechnical meanings that drive DL work, and social meanings that AI technology\nusers assign to it. We concluded that current research incentives and values\nguiding the participants' scientific interests and conduct are at odds with\nthose required to face some of the scientific challenges involved in advancing\nXAI, and thus responding to the alleged data owners' right to explanations or\nsimilar societal demands emerging from current debates. As a concrete\ncontribution to mitigate what seems to be a more general problem, we propose\nthree preliminary XAI Mediation Challenges with the potential to bring together\ntechnical and social meanings of DL applications, as well as to foster much\nneeded interdisciplinary collaboration among AI and the Social Sciences\nresearchers.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 17:59:34 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Brand\u00e3o", "Rafael", ""], ["Carbonera", "Joel", ""], ["de Souza", "Clarisse", ""], ["Ferreira", "Juliana", ""], ["Gon\u00e7alves", "Bernardo", ""], ["Leit\u00e3o", "Carla", ""]]}, {"id": "1907.07232", "submitter": "Stephen Bottos", "authors": "Stephen Bottos, Balakumar Balasingam", "title": "A Novel Slip-Kalman Filter to Track the Progression of Reading Through\n  Eye-Gaze Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an approach to track the progression of eye-gaze\nwhile reading a block of text on computer screen. The proposed approach will\nhelp to accurately quantify reading, e.g., identifying the lines of text that\nwere read/skipped and estimating the time spent on each line, based on\ncommercially available inexpensive eye-tracking devices. The proposed approach\nis based on a novel Slip Kalman filter that is custom designed to track the\nprogression of reading. The performance of the proposed method is demonstrated\nusing 25 pages eye-tracking data collected using a commercial desk-mounted\neye-tracking device.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 19:42:18 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Bottos", "Stephen", ""], ["Balasingam", "Balakumar", ""]]}, {"id": "1907.07247", "submitter": "Justin Harris", "authors": "Justin D. Harris, Bo Waggoner", "title": "Decentralized & Collaborative AI on Blockchain", "comments": "Accepted to 2019 IEEE International Conference on Blockchain", "journal-ref": null, "doi": "10.1109/Blockchain.2019.00057", "report-no": null, "categories": "cs.CR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has recently enabled large advances in artificial\nintelligence, but these tend to be highly centralized. The large datasets\nrequired are generally proprietary; predictions are often sold on a per-query\nbasis; and published models can quickly become out of date without effort to\nacquire more data and re-train them. We propose a framework for participants to\ncollaboratively build a dataset and use smart contracts to host a continuously\nupdated model. This model will be shared publicly on a blockchain where it can\nbe free to use for inference. Ideal learning problems include scenarios where a\nmodel is used many times for similar input such as personal assistants, playing\ngames, recommender systems, etc. In order to maintain the model's accuracy with\nrespect to some test set we propose both financial and non-financial (gamified)\nincentive structures for providing good data. A free and open source\nimplementation for the Ethereum blockchain is provided at\nhttps://github.com/microsoft/0xDeCA10B.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 20:19:32 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Harris", "Justin D.", ""], ["Waggoner", "Bo", ""]]}, {"id": "1907.07296", "submitter": "Ross Maciejewski", "authors": "Yuxin Ma, Tiankai Xie, Jundong Li, Ross Maciejewski", "title": "Explaining Vulnerabilities to Adversarial Machine Learning through\n  Visual Analytics", "comments": "IEEE VAST (Transactions on Visualization and Computer Graphics), 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934631", "report-no": null, "categories": "cs.HC cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are currently being deployed in a variety of\nreal-world applications where model predictions are used to make decisions\nabout healthcare, bank loans, and numerous other critical tasks. As the\ndeployment of artificial intelligence technologies becomes ubiquitous, it is\nunsurprising that adversaries have begun developing methods to manipulate\nmachine learning models to their advantage. While the visual analytics\ncommunity has developed methods for opening the black box of machine learning\nmodels, little work has focused on helping the user understand their model\nvulnerabilities in the context of adversarial attacks. In this paper, we\npresent a visual analytics framework for explaining and exploring model\nvulnerabilities to adversarial attacks. Our framework employs a multi-faceted\nvisualization scheme designed to support the analysis of data poisoning attacks\nfrom the perspective of models, data instances, features, and local structures.\nWe demonstrate our framework through two case studies on binary classifiers and\nillustrate model vulnerabilities with respect to varying attack strategies.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 00:50:37 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 01:12:30 GMT"}, {"version": "v3", "created": "Tue, 1 Oct 2019 20:34:56 GMT"}, {"version": "v4", "created": "Thu, 3 Oct 2019 19:38:48 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Ma", "Yuxin", ""], ["Xie", "Tiankai", ""], ["Li", "Jundong", ""], ["Maciejewski", "Ross", ""]]}, {"id": "1907.07322", "submitter": "Thomas Searle", "authors": "Thomas Searle, Zeljko Kraljevic, Rebecca Bendayan, Daniel Bean,\n  Richard Dobson", "title": "MedCATTrainer: A Biomedical Free Text Annotation Interface with Active\n  Learning and Research Use Case Specific Customisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MedCATTrainer an interface for building, improving and customising\na given Named Entity Recognition and Linking (NER+L) model for biomedical\ndomain text. NER+L is often used as a first step in deriving value from\nclinical text. Collecting labelled data for training models is difficult due to\nthe need for specialist domain knowledge. MedCATTrainer offers an interactive\nweb-interface to inspect and improve recognised entities from an underlying\nNER+L model via active learning. Secondary use of data for clinical research\noften has task and context specific criteria. MedCATTrainer provides a further\ninterface to define and collect supervised learning training data for\nresearcher specific use cases. Initial results suggest our approach allows for\nefficient and accurate collection of research use case specific training data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:32:04 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Searle", "Thomas", ""], ["Kraljevic", "Zeljko", ""], ["Bendayan", "Rebecca", ""], ["Bean", "Daniel", ""], ["Dobson", "Richard", ""]]}, {"id": "1907.07327", "submitter": "Ross Harper", "authors": "Ross Harper and Joshua Southern", "title": "End-To-End Prediction of Emotion From Heartbeat Data Collected by a\n  Consumer Fitness Tracker", "comments": "7 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of emotion has the potential to revolutionize mental\nhealth and wellbeing. Recent work has been successful in predicting affect from\nunimodal electrocardiogram (ECG) data. However, to be immediately relevant for\nreal-world applications, physiology-based emotion detection must make use of\nubiquitous photoplethysmogram (PPG) data collected by affordable consumer\nfitness trackers. Additionally, applications of emotion detection in healthcare\nsettings will require some measure of uncertainty over model predictions. We\npresent here a Bayesian deep learning model for end-to-end classification of\nemotional valence, using only the unimodal heartbeat time series collected by a\nconsumer fitness tracker (Garmin V\\'ivosmart 3). We collected a new dataset for\nthis task, and report a peak F1 score of 0.7. This demonstrates a practical\nrelevance of physiology-based emotion detection `in the wild' today.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 09:24:23 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Harper", "Ross", ""], ["Southern", "Joshua", ""]]}, {"id": "1907.07428", "submitter": "Tomasz Ma\\'nkowski", "authors": "Piotr Kaczmarek, Tomasz Ma\\'nkowski and Jakub Tomczy\\'nski", "title": "putEMG -- a surface electromyography hand gesture recognition dataset", "comments": null, "journal-ref": null, "doi": "10.3390/s19163548", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a putEMG dataset intended for evaluation of hand\ngesture recognition methods based on sEMG signal. The dataset was acquired for\n44 able-bodied subjects and include 8 gestures (3 full hand gestures, 4\npinches, and idle). It consists of uninterrupted recordings of 24 sEMG channels\nfrom the subject's forearm, RGB video stream and depth camera images used for\nhand motion tracking. Moreover, exemplary processing scripts are also\npublished. putEMG dataset is available under Creative Commons\nAttribution-NonCommercial 4.0 International (CC BY-NC 4.0) license at:\nhttps://www.biolab.put.poznan.pl/putemg-dataset/. The dataset was validated\nregarding sEMG amplitudes and gesture recognition performance. The\nclassification was performed using state-of-the-art classifiers and feature\nsets. Accuracy of 90% was achieved for SVM classifier utilising RMS feature and\nfor LDA classifier using Hudgin's and Du's feature sets. Analysis of\nperformance for particular gestures showed that LDA/Du combination has\nsignificantly higher accuracy for full hand gestures, while SVM/RMS performs\nbetter for pinch gestures. Presented dataset can be used as a benchmark for\nvarious classification methods, evaluation of electrode localisation concepts,\nor development of classification methods invariant to user-specific features or\nelectrode displacement.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 10:29:01 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 10:49:16 GMT"}, {"version": "v3", "created": "Thu, 22 Aug 2019 08:30:38 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Kaczmarek", "Piotr", ""], ["Ma\u0144kowski", "Tomasz", ""], ["Tomczy\u0144ski", "Jakub", ""]]}, {"id": "1907.07466", "submitter": "Andrey Krekhov", "authors": "Andrey Krekhov, Sebastian Cmentowski, Katharina Emmerich, and Jens\n  Kr\\\"uger", "title": "Beyond Human: Animals as an Escape from Stereotype Avatars in Virtual\n  Reality Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality setups are particularly suited to create a tight bond between\nplayers and their avatars up to a degree where we start perceiving the virtual\nrepresentation as our own body. We hypothesize that such an illusion of virtual\nbody ownership (IVBO) has a particularly high, yet overlooked potential for\nnonhumanoid avatars. To validate our claim, we use the example of three very\ndifferent creatures---a scorpion, a rhino, and a bird---to explore possible\navatar controls and game mechanics based on specific animal abilities. A\nquantitative evaluation underpins the high game enjoyment arising from\nembodying such nonhuman morphologies, including additional body parts and\nobtaining respective superhuman skills, which allows us to derive a set of\nnovel design implications. Furthermore, the experiment reveals a correlation\nbetween IVBO and game enjoyment, which is a further indication that nonhumanoid\ncreatures offer a meaningful design space for VR games worth further\ninvestigation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 12:18:33 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Krekhov", "Andrey", ""], ["Cmentowski", "Sebastian", ""], ["Emmerich", "Katharina", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "1907.07496", "submitter": "Martin Maritsch", "authors": "Martin Maritsch, Caterina B\\'erub\\'e, Mathias Kraus, Vera Lehmann,\n  Thomas Z\\\"uger, Stefan Feuerriegel, Tobias Kowatsch, Felix Wortmann", "title": "Improving Heart Rate Variability Measurements from Consumer Smartwatches\n  with Machine Learning", "comments": "Adjunct Proceedings of the 2019 ACM International Joint Conference on\n  Pervasive and Ubiquitous Computing and the 2019 International Symposium on\n  Wearable Computers", "journal-ref": null, "doi": "10.1145/3341162.3346276", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reactions of the human body to physical exercise, psychophysiological\nstress and heart diseases are reflected in heart rate variability (HRV). Thus,\ncontinuous monitoring of HRV can contribute to determining and predicting\nissues in well-being and mental health. HRV can be measured in everyday life by\nconsumer wearable devices such as smartwatches which are easily accessible and\naffordable. However, they are arguably accurate due to the stability of the\nsensor. We hypothesize a systematic error which is related to the wearer\nmovement. Our evidence builds upon explanatory and predictive modeling: we find\na statistically significant correlation between error in HRV measurements and\nthe wearer movement. We show that this error can be minimized by bringing into\ncontext additional available sensor information, such as accelerometer data.\nThis work demonstrates our research-in-progress on how neural learning can\nminimize the error of such smartwatch HRV measurements.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:16:57 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Maritsch", "Martin", ""], ["B\u00e9rub\u00e9", "Caterina", ""], ["Kraus", "Mathias", ""], ["Lehmann", "Vera", ""], ["Z\u00fcger", "Thomas", ""], ["Feuerriegel", "Stefan", ""], ["Kowatsch", "Tobias", ""], ["Wortmann", "Felix", ""]]}, {"id": "1907.07564", "submitter": "Nishchay Sharma", "authors": "Madan Gopal Jhawar, Vipindeep Vangala, Nishchay Sharma, Ankur\n  Hayatnagarkar, Mansi Saxena, Swati Valecha", "title": "Conversational Help for Task Completion and Feature Discovery in\n  Personal Assistants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG cs.SD eess.AS stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Intelligent Personal Assistants (IPAs) have become widely popular in recent\ntimes. Most of the commercial IPAs today support a wide range of skills\nincluding Alarms, Reminders, Weather Updates, Music, News, Factual\nQuestioning-Answering, etc. The list grows every day, making it difficult to\nremember the command structures needed to execute various tasks. An IPA must\nhave the ability to communicate information about supported skills and direct\nusers towards the right commands needed to execute them. Users interact with\npersonal assistants in natural language. A query is defined to be a Help Query\nif it seeks information about a personal assistant's capabilities, or asks for\ninstructions to execute a task. In this paper, we propose an interactive system\nwhich identifies help queries and retrieves appropriate responses. Our system\ncomprises of a C-BiLSTM based classifier, which is a fusion of Convolutional\nNeural Networks (CNN) and Bidirectional LSTM (BiLSTM) architectures, to detect\nhelp queries and a semantic Approximate Nearest Neighbours (ANN) module to map\nthe query to an appropriate predefined response. Evaluation of our system on\nreal-world queries from a commercial IPA and a detailed comparison with popular\ntraditional machine learning and deep learning based models reveal that our\nsystem outperforms other approaches and returns relevant responses for help\nqueries.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 09:25:13 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Jhawar", "Madan Gopal", ""], ["Vangala", "Vipindeep", ""], ["Sharma", "Nishchay", ""], ["Hayatnagarkar", "Ankur", ""], ["Saxena", "Mansi", ""], ["Valecha", "Swati", ""]]}, {"id": "1907.07679", "submitter": "Ankit Gupta", "authors": "Ankit Gupta", "title": "User-Interactive Machine Learning Model for Identifying Structural\n  Relationships of Code Features", "comments": "This version has been removed by arXiv administrators as the\n  submitter did not have the right to agree to the license at the time of\n  submission. Author list truncated", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional machine learning based intelligent systems assist users by\nlearning patterns in data and making recommendations. However, these systems\nare limited in that the user has little means of understanding the rationale\nbehind the systems suggestions, communicating their own understanding of\npatterns, or correcting system behavior. In this project, we outline a model\nfor intelligent software based on a human computer feedback loop. The Machine\nLearning (ML) systems recommendations are reviewed by the user, and in turn,\nthis information shapes the systems decision making. Our model was applied to\ndeveloping an HTML editor that integrates ML with user interaction to ascertain\nstructural relationships between HTML document features and apply them for code\ncompletion. The editor utilizes the ID3 algorithm to build decision trees,\nsequences of rules for predicting code the user will type. The editor displays\nthe decision trees rules in the Interactive Rules Interface System (IRIS),\nwhich allows developers to prioritize, modify, or delete them. These\ninteractions alter the data processed by ID3, providing the developer some\ncontrol over the autocomplete system. Validation indicates that, absent user\ninteraction, the ML model is able to predict tags with 78.4 percent accuracy,\nattributes with 62.9 percent accuracy, and values with 12.8 percent accuracy.\nBased off of the results of the user study, user interaction with the rules\ninterface corrects feature relationships missed or mistaken by the automated\nprocess, enhancing autocomplete accuracy and developer productivity.\nAdditionally, interaction is proven to help developers work with greater\nawareness of code patterns. Our research demonstrates the viability of a\nsoftware integration of machine intelligence with human feedback.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 16:18:51 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Gupta", "Ankit", ""]]}, {"id": "1907.07717", "submitter": "Luyan Xu", "authors": "Luyan Xu, Xuan Zhou, Ujwal Gadiraju", "title": "Revealing the Role of User Moods in Struggling Search Tasks", "comments": "4 pages, 3 figures, SIGIR2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User-centered approaches have been extensively studied and used in the area\nof struggling search. Related research has targeted key aspects of users such\nas user satisfaction or frustration, and search success or failure, using a\nvariety of experimental methods including laboratory user studies, in-situ\nexplicit feedback from searchers and by using crowdsourcing. Such studies are\nvaluable in advancing the understanding of search difficulty from a user's\nperspective, and yield insights that can directly improve search systems and\ntheir evaluation. However, little is known about how user moods influence their\ninteractions with a search system or their perception of struggling. In this\nwork, we show that a user's own mood can systematically bias the user's\nperception, and experience while interacting with a search system and trying to\nsatisfy an information need. People who are in activated-pleasant /\nactivated-unpleasant moods tend to issue more queries than people in\ndeactivated or neutral moods. Those in an unpleasant mood perceive a higher\nlevel of difficulty. Our insights extend the current understanding of\nstruggling search tasks and have important implications on the design and\nevaluation of search systems supporting such tasks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 18:53:45 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Xu", "Luyan", ""], ["Zhou", "Xuan", ""], ["Gadiraju", "Ujwal", ""]]}, {"id": "1907.07759", "submitter": "Bin Guo", "authors": "Bin Guo, Yasan Ding, Yueheng Sun, Shuai Ma, Ke Li", "title": "The Mass, Fake News, and Cognition Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The wide spread of fake news in social networks is posing threats to social\nstability, economic development and political democracy etc. Numerous studies\nhave explored the effective detection approaches of online fake news, while few\nworks study the intrinsic propagation and cognition mechanisms of fake news.\nSince the development of cognitive science paves a promising way for the\nprevention of fake news, we present a new research area called Cognition\nSecurity (CogSec), which studies the potential impacts of fake news to human\ncognition, ranging from misperception, untrusted knowledge acquisition,\ntargeted opinion/attitude formation, to biased decision making, and\ninvestigates the effective ways for fake news debunking. CogSec is a\nmultidisciplinary research field that leverages knowledge from social science,\npsychology, cognition science, neuroscience, AI and computer science. We first\npropose related definitions to characterize CogSec and review the literature\nhistory. We further investigate the key research challenges and techniques of\nCogSec, including human-content cognition mechanism, social influence and\nopinion diffusion, fake news detection and malicious bot detection. Finally, we\nsummarize the open issues and future research directions, such as early\ndetection of fake news, explainable fake news debunking, social contagion and\ndiffusion models of fake news, and so on.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 22:40:35 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Guo", "Bin", ""], ["Ding", "Yasan", ""], ["Sun", "Yueheng", ""], ["Ma", "Shuai", ""], ["Li", "Ke", ""]]}, {"id": "1907.07835", "submitter": "Peixiang Zhong", "authors": "Peixiang Zhong, Di Wang, and Chunyan Miao", "title": "EEG-Based Emotion Recognition Using Regularized Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalography (EEG) measures the neuronal activities in different\nbrain regions via electrodes. Many existing studies on EEG-based emotion\nrecognition do not fully exploit the topology of EEG channels. In this paper,\nwe propose a regularized graph neural network (RGNN) for EEG-based emotion\nrecognition. RGNN considers the biological topology among different brain\nregions to capture both local and global relations among different EEG\nchannels. Specifically, we model the inter-channel relations in EEG signals via\nan adjacency matrix in a graph neural network where the connection and\nsparseness of the adjacency matrix are inspired by neuroscience theories of\nhuman brain organization. In addition, we propose two regularizers, namely\nnode-wise domain adversarial training (NodeDAT) and emotion-aware distribution\nlearning (EmotionDL), to better handle cross-subject EEG variations and noisy\nlabels, respectively. Extensive experiments on two public datasets, SEED and\nSEED-IV, demonstrate the superior performance of our model than\nstate-of-the-art models in most experimental settings. Moreover, ablation\nstudies show that the proposed adjacency matrix and two regularizers contribute\nconsistent and significant gain to the performance of our RGNN model. Finally,\ninvestigations on the neuronal activities reveal important brain regions and\ninter-channel relations for EEG-based emotion recognition.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 01:44:44 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 10:57:39 GMT"}, {"version": "v3", "created": "Sun, 12 Apr 2020 07:02:59 GMT"}, {"version": "v4", "created": "Wed, 13 May 2020 03:19:26 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Zhong", "Peixiang", ""], ["Wang", "Di", ""], ["Miao", "Chunyan", ""]]}, {"id": "1907.07861", "submitter": "Chen Chen", "authors": "Vivian Li, Alon Halevy, Adi Zief-Balteriski Ph.D, Wang-Chiew Tan,\n  George Mihaila, John Morales, Natalie Nuno, Huining Liu, Chen Chen, Xiaojuan\n  Ma, Shani Robins Ph.D., Jessica Johnson", "title": "Jo: The Smart Journal", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Jo, a mobile application that attempts to improve user's\nwell-being. Jo is a journaling application--users log their important moments\nvia short texts and optionally an attached photo. Unlike a static journal, Jo\nanalyzes these moments and helps users take action towards increased\nwell-being. For example, Jo annotates each moment with a set of values (e.g.,\nfamily, socialization, mindfulness), thereby giving the user insights about the\nbalance in their lives. In addition, Jo helps the user create reminders that\nenable them to create additional happy moments. We describe the results of\nfielding Jo in a study of 39 participants. The results illustrate the promise\nof a journaling application that provides personalized feedback, and points at\nfurther research.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 03:38:39 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Li", "Vivian", ""], ["Halevy", "Alon", ""], ["D", "Adi Zief-Balteriski Ph.", ""], ["Tan", "Wang-Chiew", ""], ["Mihaila", "George", ""], ["Morales", "John", ""], ["Nuno", "Natalie", ""], ["Liu", "Huining", ""], ["Chen", "Chen", ""], ["Ma", "Xiaojuan", ""], ["D.", "Shani Robins Ph.", ""], ["Johnson", "Jessica", ""]]}, {"id": "1907.08153", "submitter": "Jens Grubert", "authors": "Daniel Schneider, Alexander Otte, Travis Gesslein, Philipp Gagel,\n  Bastian Kuth, Mohamad Shahm Damlakhi, Oliver Dietz, Eyal Ofek, Michel Pahud,\n  Per Ola Kristensson, J\\\"org M\\\"uller, Jens Grubert", "title": "ReconViguRation: Reconfiguring Physical Keyboards in Virtual Reality", "comments": "to appear", "journal-ref": "In IEEE Transactions of Visualization and Computer Graphics\n  (TVCG), 2019", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical keyboards are common peripherals for personal computers and are\nefficient standard text entry devices. Recent research has investigated how\nphysical keyboards can be used in immersive head-mounted display-based Virtual\nReality (VR). So far, the physical layout of keyboards has typically been\ntransplanted into VR for replicating typing experiences in a standard desktop\nenvironment.\n  In this paper, we explore how to fully leverage the immersiveness of VR to\nchange the input and output characteristics of physical keyboard interaction\nwithin a VR environment. This allows individual physical keys to be\nreconfigured to the same or different actions and visual output to be\ndistributed in various ways across the VR representation of the keyboard.\n  We explore a set of input and output mappings for reconfiguring the virtual\npresentation of physical keyboards and probe the resulting design space by\nspecifically designing, implementing and evaluating nine VR-relevant\napplications: emojis, languages and special characters, application shortcuts,\nvirtual text processing macros, a window manager, a photo browser, a\nwhack-a-mole game, secure password entry and a virtual touch bar. We\ninvestigate the feasibility of the applications in a user study with 20\nparticipants and find that, among other things, they are usable in VR. We\ndiscuss the limitations and possibilities of remapping the input and output\ncharacteristics of physical keyboards in VR based on empirical findings and\nanalysis and suggest future research directions in this area.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 16:55:32 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Schneider", "Daniel", ""], ["Otte", "Alexander", ""], ["Gesslein", "Travis", ""], ["Gagel", "Philipp", ""], ["Kuth", "Bastian", ""], ["Damlakhi", "Mohamad Shahm", ""], ["Dietz", "Oliver", ""], ["Ofek", "Eyal", ""], ["Pahud", "Michel", ""], ["Kristensson", "Per Ola", ""], ["M\u00fcller", "J\u00f6rg", ""], ["Grubert", "Jens", ""]]}, {"id": "1907.08228", "submitter": "Indira Sen", "authors": "Indira Sen, Fabian Floeck, Katrin Weller, Bernd Weiss, Claudia Wagner", "title": "TED-On: A Total Error Framework for Digital Traces of Human Behavior on\n  Online Platforms", "comments": "20 pages, 2 figures, Longer version of paper set to appear in Public\n  Opinion Quarterly. Updating terminology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Peoples' activities and opinions recorded as digital traces online,\nespecially on social media and other web-based platforms, offer increasingly\ninformative pictures of the public. They promise to allow inferences about\npopulations beyond the users of the platforms on which the traces are recorded,\nrepresenting real potential for the Social Sciences and a complement to\nsurvey-based research. But the use of digital traces brings its own\ncomplexities and new error sources to the research enterprise. Recently,\nresearchers have begun to discuss the errors that can occur when digital traces\nare used to learn about humans and social phenomena. This article synthesizes\nthis discussion and proposes a systematic way to categorize potential errors,\ninspired by the Total Survey Error (TSE) Framework developed for survey\nmethodology. We introduce a conceptual framework to diagnose, understand, and\ndocument errors that may occur in studies based on such digital traces. While\nthere are clear parallels to the well-known error sources in the TSE framework,\nthe new \"Total Error Framework for Digital Traces of Human Behavior on Online\nPlatforms\" (TED-On) identifies several types of error that are specific to the\nuse of digital traces. By providing a standard vocabulary to describe these\nerrors, the proposed framework is intended to advance communication and\nresearch concerning the use of digital traces in scientific social research.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:18:48 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 16:42:26 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 18:03:44 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 16:52:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Sen", "Indira", ""], ["Floeck", "Fabian", ""], ["Weller", "Katrin", ""], ["Weiss", "Bernd", ""], ["Wagner", "Claudia", ""]]}, {"id": "1907.08325", "submitter": "Shusen Liu", "authors": "Shusen Liu, Di Wang, Dan Maljovec, Rushil Anirudh, Jayaraman J.\n  Thiagarajan, Sam Ade Jacobs, Brian C. Van Essen, David Hysom, Jae-Seung Yeom,\n  Jim Gaffney, Luc Peterson, Peter B. Robinson, Harsh Bhatia, Valerio Pascucci,\n  Brian K. Spears and Peer-Timo Bremer", "title": "Scalable Topological Data Analysis and Visualization for Evaluating\n  Data-Driven Models in Scientific Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid adoption of machine learning techniques for large-scale\napplications in science and engineering comes the convergence of two grand\nchallenges in visualization. First, the utilization of black box models (e.g.,\ndeep neural networks) calls for advanced techniques in exploring and\ninterpreting model behaviors. Second, the rapid growth in computing has\nproduced enormous datasets that require techniques that can handle millions or\nmore samples. Although some solutions to these interpretability challenges have\nbeen proposed, they typically do not scale beyond thousands of samples, nor do\nthey provide the high-level intuition scientists are looking for. Here, we\npresent the first scalable solution to explore and analyze high-dimensional\nfunctions often encountered in the scientific data analysis pipeline. By\ncombining a new streaming neighborhood graph construction, the corresponding\ntopology computation, and a novel data aggregation scheme, namely topology\naware datacubes, we enable interactive exploration of both the topological and\nthe geometric aspect of high-dimensional data. Following two use cases from\nhigh-energy-density (HED) physics and computational biology, we demonstrate how\nthese capabilities have led to crucial new insights in both applications.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 00:37:39 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Liu", "Shusen", ""], ["Wang", "Di", ""], ["Maljovec", "Dan", ""], ["Anirudh", "Rushil", ""], ["Thiagarajan", "Jayaraman J.", ""], ["Jacobs", "Sam Ade", ""], ["Van Essen", "Brian C.", ""], ["Hysom", "David", ""], ["Yeom", "Jae-Seung", ""], ["Gaffney", "Jim", ""], ["Peterson", "Luc", ""], ["Robinson", "Peter B.", ""], ["Bhatia", "Harsh", ""], ["Pascucci", "Valerio", ""], ["Spears", "Brian K.", ""], ["Bremer", "Peer-Timo", ""]]}, {"id": "1907.08345", "submitter": "Bahador Saket", "authors": "Bahador Saket, Lei Jiang, Charles Perin, and Alex Endert", "title": "Liger: Combining Interaction Paradigms for Visual Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization tools usually leverage a single interaction paradigm (e.g.,\nmanual view specification, visualization by demonstration, etc.), which fosters\nthe process of visualization construction. A large body of work has\ninvestigated the effectiveness of individual interaction paradigms, building an\nunderstanding of advantages and disadvantages of each in isolation. However,\nhow can we leverage the benefits of multiple interaction paradigms by combining\nthem into a single tool? We currently lack a holistic view of how interaction\nparadigms that use the same input modality (e.g., mouse) can be combined into a\nsingle tool and how people use such tools. To investigate opportunities and\nchallenges in combining paradigms, we first created a multi-paradigm prototype\n(Liger) that combines two mouse-based interaction paradigms (manual view\nspecification and visualization by demonstration) in a unified tool. We then\nconducted an exploratory study with Liger, providing initial evidence that\npeople 1) use both paradigms interchangeably, 2) seamlessly switch between\nparadigms based on the operation at hand, and 3) choose to successfully\ncomplete a single operation using a combination of both paradigms.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 02:29:31 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 00:49:34 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Saket", "Bahador", ""], ["Jiang", "Lei", ""], ["Perin", "Charles", ""], ["Endert", "Alex", ""]]}, {"id": "1907.08478", "submitter": "Robert Loftin", "authors": "Robert Loftin, Bei Peng, Matthew E. Taylor, Michael L. Littman and\n  David L. Roberts", "title": "Interactive Learning of Environment Dynamics for Sequential Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for robots and other artificial agents to efficiently learn to\nperform useful tasks defined by an end user, they must understand not only the\ngoals of those tasks, but also the structure and dynamics of that user's\nenvironment. While existing work has looked at how the goals of a task can be\ninferred from a human teacher, the agent is often left to learn about the\nenvironment on its own. To address this limitation, we develop an algorithm,\nBehavior Aware Modeling (BAM), which incorporates a teacher's knowledge into a\nmodel of the transition dynamics of an agent's environment. We evaluate BAM\nboth in simulation and with real human teachers, learning from a combination of\ntask demonstrations and evaluative feedback, and show that it can outperform\napproaches which do not explicitly consider this source of dynamics knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 12:15:53 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Loftin", "Robert", ""], ["Peng", "Bei", ""], ["Taylor", "Matthew E.", ""], ["Littman", "Michael L.", ""], ["Roberts", "David L.", ""]]}, {"id": "1907.08495", "submitter": "Jason Dykes", "authors": "Miriah Meyer and Jason Dykes", "title": "Criteria for Rigor in Visualization Design Study", "comments": "IEEE VIS (InfoVis) 2019 2012; ACM CCS - Human-centered computing,\n  Visualization, Visualization design and evaluation methods", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934539", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new perspective on research conducted through visualization\ndesign study that emphasizes design as a method of inquiry and the broad range\nof knowledge-contributions achieved through it as multiple, subjective, and\nsocially constructed. From this interpretivist position we explore the nature\nof visualization design study and develop six criteria for rigor. We propose\nthat rigor is established and judged according to the extent to which\nvisualization design study research and its reporting are INFORMED, REFLEXIVE,\nABUNDANT, PLAUSIBLE, RESONANT, and TRANSPARENT. This perspective and the\ncriteria were constructed through a four-year engagement with the discourse\naround rigor and the nature of knowledge in social science, information\nsystems, and design. We suggest methods from cognate disciplines that can\nsupport visualization researchers in meeting these criteria during the\nplanning, execution, and reporting of design study. Through a series of\ndeliberately provocative questions, we explore implications of this new\nperspective for design study research in visualization, concluding that as a\ndiscipline, visualization is not yet well positioned to embrace, nurture, and\nfully benefit from a rigorous, interpretivist approach to design study. The\nperspective and criteria we present are intended to stimulate dialogue and\ndebate around the nature of visualization design study and the broader\nunderpinnings of the discipline.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 12:57:00 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 08:55:46 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Meyer", "Miriah", ""], ["Dykes", "Jason", ""]]}, {"id": "1907.08586", "submitter": "Ariel Noyman", "authors": "Ariel Noyman, Yasushi Sakai, Kent Larson", "title": "CityScopeAR: Urban Design and Crowdsourced Engagement Platform", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Processes of urban planning, urban design and architecture are inherently\ntangible, iterative and collaborative. Nevertheless, the majority of tools in\nthese fields offer virtual environments and single user experience. This paper\npresents CityScopeAR: a computational-tangible mixed-reality platform designed\nfor collaborative urban design processes. It portrays the evolution of the tool\nand presents an overview of the history and limitations of notable CAD and TUI\nplatforms. As well, it depicts the development of a distributed networking\nsystem between TUIs and CityScopeAR, as a key in design collaboration. It\nshares the potential advantage of broad and decentralized community-engagement\nprocess using such tools. Finally, this paper demonstrates several real-world\ntests and deployments of CityScopeAR and proposes a path to future integration\nof AR/MR devices in urban design and public participation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 17:30:26 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Noyman", "Ariel", ""], ["Sakai", "Yasushi", ""], ["Larson", "Kent", ""]]}, {"id": "1907.08598", "submitter": "Amir Javadpour", "authors": "Amir Javadpour, HamidrezaMemarzadeh-Tehran", "title": "A Wearable Medical Sensor for Provisional Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thispaper presents the design and realization of a context-aware wireless\nhealth monitoring system for recording the heartbeat (HR) and respiration (RR)\nrate based on an indirect measurement approach. The system consists of a\ncontact-less medical sensor as well as a communication infrastructure for\nhandling the transmission and reception of the measured results. The\ncontact-less sensor includes a highly sensitive tri-axial accelerometer, an\naccurate temperature and air pressure sensor that enable one to inspect\npatients' health condition by continuously monitoring of two critical signs\nrelated to the cardiorespiratory system. The developed system can also be\nutilized in performing a number of long-term inspection on the heart and lungs\nwhile measuring the HR and RR values in addition to calculating the HR and RR\nratio, which is denoted by HRR. The obtained results show the potential of the\ndeveloped system for versatile monitoring applications applied to telemedicine\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 09:34:28 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Javadpour", "Amir", ""], ["HamidrezaMemarzadeh-Tehran", "", ""]]}, {"id": "1907.08661", "submitter": "Yichi Zhang", "authors": "Yichi Zhang, Yiting Zhang, Zhiyao Duan", "title": "Sound Search by Text Description or Vocal Imitation?", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching sounds by text labels is often difficult, as text descriptions\ncannot describe the audio content in detail. Query by vocal imitation bridges\nsuch gap and provides a novel way to sound search. Several algorithms for sound\nsearch by vocal imitation have been proposed and evaluated in a simulation\nenvironment, however, they have not been deployed into a real search engine nor\nevaluated by real users. This pilot work conducts a subjective study to compare\nthese two approaches to sound search, and tries to answer the question of which\napproach works better for what kinds of sounds. To do so, we developed two\nweb-based search engines for sound, one by vocal imitation (Vroom!) and the\nother by text description (TextSearch). We also developed an experimental\nframework to host these engines to collect statistics of user behaviors and\nratings. Results showed that Vroom! received significantly higher search\nsatisfaction ratings than TextSearch did for sound categories that were\ndifficult for subjects to describe by text. Results also showed a better\noverall ease-of-use rating for Vroom! than TextSearch on the limited sound\nlibrary in our experiments. These findings suggest advantages of\nvocal-imitation-based search for sound in practice.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 19:33:56 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Zhang", "Yichi", ""], ["Zhang", "Yiting", ""], ["Duan", "Zhiyao", ""]]}, {"id": "1907.08705", "submitter": "Mohammad Hadi Mehdizavareh", "authors": "Mohammad Hadi Mehdizavareh, Sobhan Hemati, Hamid Soltanian-Zadeh", "title": "Enhancing performance of subject-specific models via subject-independent\n  information for SSVEP-based BCIs", "comments": "22 pages, 8 figures, 1 table, 1 appendix, published in PLOS ONE\n  journal. This is a draft version. The published version is available in the\n  following link:\n  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0226048", "journal-ref": "PLOS ONE 15(1): e0226048 (2020)", "doi": "10.1371/journal.pone.0226048", "report-no": null, "categories": "q-bio.NC cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, brain-computer interface (BCI) systems developed based on\nsteady-state visual evoked potential (SSVEP) have attracted much attention due\nto their high information transfer rate (ITR) and increasing number of targets.\nHowever, SSVEP-based methods can be improved in terms of their accuracy and\ntarget detection time. We propose a new method based on canonical correlation\nanalysis (CCA) to integrate subject-specific models and subject-independent\ninformation and enhance BCI performance. We propose to use training data of\nother subjects to optimize hyperparameters for CCA-based model of a specific\nsubject. An ensemble version of the proposed method is also developed for a\nfair comparison with ensemble task-related component analysis (TRCA). The\nproposed method is compared with TRCA and extended CCA methods. A publicly\navailable, 35-subject SSVEP benchmark dataset is used for comparison studies\nand performance is quantified by classification accuracy and ITR. The ITR of\nthe proposed method is higher than those of TRCA and extended CCA. The proposed\nmethod outperforms extended CCA in all conditions and TRCA for time windows\ngreater than 0.3 s. The proposed method also outperforms TRCA when there are\nlimited training blocks and electrodes. This study illustrates that adding\nsubject-independent information to subject-specific models can improve\nperformance of SSVEP-based BCIs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 21:49:39 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 20:34:28 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Mehdizavareh", "Mohammad Hadi", ""], ["Hemati", "Sobhan", ""], ["Soltanian-Zadeh", "Hamid", ""]]}, {"id": "1907.08796", "submitter": "Leo Yu-Ho Lo", "authors": "Leo Yu-Ho Lo, Yao Ming, and Huamin Qu", "title": "Learning Vis Tools: Teaching Data Visualization Tutorials", "comments": "5 pages, 1 figure, IEEE VIS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching and advocating data visualization are among the most important\nactivities in the visualization community. With growing interest in data\nanalysis from business and science professionals, data visualization courses\nattract students across different disciplines. However, comprehensive\nvisualization training requires students to have a certain level of proficiency\nin programming, a requirement that imposes challenges on both teachers and\nstudents. With recent developments in visualization tools, we have managed to\novercome these obstacles by teaching a wide range of visualization and\nsupporting tools. Starting with GUI-based visualization tools and data analysis\nwith Python, students put visualization knowledge into practice with increasing\namounts of programming. At the end of the course, students can design and\nimplement visualizations with D3 and other programming-based visualization\ntools. Throughout the course, we continuously collect student feedback and\nrefine the teaching materials. This paper documents our teaching methods and\nconsiderations when designing the teaching materials.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 11:11:38 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 08:40:59 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Lo", "Leo Yu-Ho", ""], ["Ming", "Yao", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.08844", "submitter": "Asma Ghandeharioun", "authors": "Grace Leslie, Asma Ghandeharioun, Diane Y. Zhou, Rosalind W. Picard", "title": "Engineering Music to Slow Breathing and Invite Relaxed Physiology", "comments": "Accepted at 2019 8th International Conference on Affective Computing\n  and Intelligent Interaction (ACII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We engineered an interactive music system that influences a user's breathing\nrate to induce a relaxation response. This system generates ambient music\ncontaining periodic shifts in loudness that are determined by the user's own\nbreathing patterns. We evaluated the efficacy of this music intervention for\nparticipants who were engaged in an attention-demanding task, and thus\nexplicitly not focusing on their breathing or on listening to the music. We\nmeasured breathing patterns in addition to multiple peripheral and cortical\nindicators of physiological arousal while users experienced three different\ninteraction designs: (1) a \"Fixed Tempo\" amplitude modulation rate at six beats\nper minute; (2) a \"Personalized Tempo\" modulation rate fixed at 75\\% of each\nindividual's breathing rate baseline, and (3) a \"Personalized Envelope\" design\nin which the amplitude modulation matches each individual's breathing pattern\nin real-time. Our results revealed that each interactive music design slowed\ndown breathing rates, with the \"Personalized Tempo\" design having the largest\neffect, one that was more significant than the non-personalized design. The\nphysiological arousal indicators (electrodermal activity, heart rate, and slow\ncortical potentials measured in EEG) showed concomitant reductions, suggesting\nthat slowing users' breathing rates shifted them towards a more calmed state.\nThese results suggest that interactive music incorporating biometric data may\nhave greater effects on physiology than traditional recorded music.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 17:23:46 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Leslie", "Grace", ""], ["Ghandeharioun", "Asma", ""], ["Zhou", "Diane Y.", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "1907.08977", "submitter": "Saugat Bhattacharyya", "authors": "Saugat Bhattacharyya, Mitsuhiro Hayashibe", "title": "Systematic Enhancement of Functional Connectivity in Brain-Computer\n  Interfacing using Common Spatial Patterns and Tangent Space Mapping", "comments": "Pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional connectivity of cognitive tasks allows researchers to analyse the\ninteraction mapping occurring between different regions of the brain using\nelectroencephalography (EEG) signals. Standard practice in functional\nconnectivity involve studying the electrode pair interactions across several\ntrials. As the cognitive task always involves the human factor, it is\ninevitable to have lower quality data from the brain signals influenced by the\nsubject concentration or other mental states which can occur anytime over the\nwhole experimental trials. The connectivity among electrodes are heavily\ninfluenced by these low quality EEG. In this paper, we aim at enhancing the\nfunctional connectivity of mental tasks by implementing a classification step\nin the process to remove those incorrect EEG trials from the available set. The\nclassification step removes the trials which were mis-classified or had a low\nprobability of occurrence to extract only reliable EEG trials. Through our\napproach, we have successfully improved the separability among graph parameters\nfor different mental tasks. We also observe an improvement in the readability\nof the connectivity by focusing only on a group of selected channels rather\nthan employing all the channels.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jul 2019 14:04:33 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Bhattacharyya", "Saugat", ""], ["Hayashibe", "Mitsuhiro", ""]]}, {"id": "1907.09091", "submitter": "Weiwei Cui", "authors": "Weiwei Cui, Xiaoyu Zhang, Yun Wang, He Huang, Bei Chen, Lei Fang,\n  Haidong Zhang, Jian-Guan Lou, and Dongmei Zhang", "title": "Text-to-Viz: Automatic Generation of Infographics from\n  Proportion-Related Natural Language Statements", "comments": "InfoVis 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining data content with visual embellishments, infographics can\neffectively deliver messages in an engaging and memorable manner. Various\nauthoring tools have been proposed to facilitate the creation of infographics.\nHowever, creating a professional infographic with these authoring tools is\nstill not an easy task, requiring much time and design expertise. Therefore,\nthese tools are generally not attractive to casual users, who are either\nunwilling to take time to learn the tools or lacking in proper design expertise\nto create a professional infographic. In this paper, we explore an alternative\napproach: to automatically generate infographics from natural language\nstatements. We first conducted a preliminary study to explore the design space\nof infographics. Based on the preliminary study, we built a proof-of-concept\nsystem that automatically converts statements about simple proportion-related\nstatistics to a set of infographics with pre-designed styles. Finally, we\ndemonstrated the usability and usefulness of the system through sample results,\nexhibits, and expert reviews.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 02:47:13 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Cui", "Weiwei", ""], ["Zhang", "Xiaoyu", ""], ["Wang", "Yun", ""], ["Huang", "He", ""], ["Chen", "Bei", ""], ["Fang", "Lei", ""], ["Zhang", "Haidong", ""], ["Lou", "Jian-Guan", ""], ["Zhang", "Dongmei", ""]]}, {"id": "1907.09146", "submitter": "Gromit Yeuk-Yin Chan", "authors": "Gromit Yeuk-Yin Chan, Luis Gustavo Nonato, Alice Chu, Preeti Raghavan,\n  Viswanath Aluru, Claudio T. Silva", "title": "Motion Browser: Visualizing and Understanding Complex Upper Limb\n  Movement Under Obstetrical Brachial Plexus Injuries", "comments": "IEEE Transactions on Visualization and Computer Graphics (VAST 2019,\n  to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brachial plexus is a complex network of peripheral nerves that enables\nsensing from and control of the movements of the arms and hand. Nowadays, the\ncoordination between the muscles to generate simple movements is still not well\nunderstood, hindering the knowledge of how to best treat patients with this\ntype of peripheral nerve injury. To acquire enough information for medical data\nanalysis, physicians conduct motion analysis assessments with patients to\nproduce a rich dataset of electromyographic signals from multiple muscles\nrecorded with joint movements during real-world tasks. However, tools for the\nanalysis and visualization of the data in a succinct and interpretable manner\nare currently not available. Without the ability to integrate, compare, and\ncompute multiple data sources in one platform, physicians can only compute\nsimple statistical values to describe patient's behavior vaguely, which limits\nthe possibility to answer clinical questions and generate hypotheses for\nresearch. To address this challenge, we have developed \\systemname, an\ninteractive visual analytics system which provides an efficient framework to\nextract and compare muscle activity patterns from the patient's limbs and\ncoordinated views to help users analyze muscle signals, motion data, and video\ninformation to address different tasks. The system was developed as a result of\na collaborative endeavor between computer scientists and orthopedic surgery and\nrehabilitation physicians. We present case studies showing physicians can\nutilize the information displayed to understand how individuals coordinate\ntheir muscles to initiate appropriate treatment and generate new hypotheses for\nfuture research.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 06:15:54 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Chan", "Gromit Yeuk-Yin", ""], ["Nonato", "Luis Gustavo", ""], ["Chu", "Alice", ""], ["Raghavan", "Preeti", ""], ["Aluru", "Viswanath", ""], ["Silva", "Claudio T.", ""]]}, {"id": "1907.09293", "submitter": "David Powers", "authors": "David M W Powers", "title": "DREAMT -- Embodied Motivational Conversational Storytelling", "comments": "12 pages; to be presented as lightning talk plus poster at StoryNLP\n  on 1 August 2019 at ACL in Florence - poster pdf and powerpoint available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.MA cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Storytelling is fundamental to language, including culture, conversation and\ncommunication in their broadest senses. It thus emerges as an essential\ncomponent of intelligent systems, including systems where natural language is\nnot a primary focus or where we do not usually think of a story being involved.\nIn this paper we explore the emergence of storytelling as a requirement in\nembodied conversational agents, including its role in educational and health\ninterventions, as well as in a general-purpose computer interface for people\nwith disabilities or other constraints that prevent the use of traditional\nkeyboard and speech interfaces. We further present a characterization of\nstorytelling as an inventive fleshing out of detail according to a particular\npersonal perspective, and propose the DREAMT model to focus attention on the\ndifferent layers that need to be present in a character-driven storytelling\nsystem. Most if not all aspects of the DREAMT model have arisen from or been\nexplored in some aspect of our implemented research systems, but currently only\nat a primitive and relatively unintegrated level. However, this experience\nleads us to formalize and elaborate the DREAMT model mnemonically as follows: -\nDescription/Dialogue/Definition/Denotation - Realization/Representation/Role -\nExplanation/Education/Entertainment - Actualization/Activation -\nMotivation/Modelling - Topicalization/Transformation\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 01:49:37 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Powers", "David M W", ""]]}, {"id": "1907.09334", "submitter": "Jean-Pierre Lorre", "authors": "Jean-Pierre Lorr\\'e, Isabelle Ferran\\'e (IRIT), Francisco Madrigal\n  (CIMAT), Michalis Vazirgiannis (LIX), Christophe Bourguignat", "title": "LinTO : Assistant vocal open-source respectueux des donn\\'ees\n  personnelles pour les r\\'eunions d'entreprise", "comments": "in French. Applications Pratiques de l'Intelligence Artificielle, Jul\n  2019, Toulouse, France", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first results of the PIA \"Grands D\\'efis du\nNum\\'erique\" research project LinTO. The goal of this project is to develop a\nconversational assistant to help the company's employees, particularly during\nmeetings. LinTO is an interactive device equipped with microphones, a screen\nand a 360$^\\circ$ camera, which allows to control the room, query company's\ninformation system, helps facilitate the meeting and provides an environment to\naid minute writing. Distributed according to an open model that respects\nprivate data LinTO is the first open-source enterprise's assistant designed to\ncomply with the GDPR requirements.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:17:53 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lorr\u00e9", "Jean-Pierre", "", "IRIT"], ["Ferran\u00e9", "Isabelle", "", "IRIT"], ["Madrigal", "Francisco", "", "CIMAT"], ["Vazirgiannis", "Michalis", "", "LIX"], ["Bourguignat", "Christophe", ""]]}, {"id": "1907.09540", "submitter": "Ozan Ozdenizci", "authors": "Ozan Ozdenizci, Barry Oken, Tab Memmott, Melanie Fried-Oken, Deniz\n  Erdogmus", "title": "Adversarial Feature Learning in Brain Interfacing: An Experimental Study\n  on Eliminating Drowsiness Effects", "comments": "8th Graz Brain-Computer Interface Conference 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across- and within-recording variabilities in electroencephalographic (EEG)\nactivity is a major limitation in EEG-based brain-computer interfaces (BCIs).\nSpecifically, gradual changes in fatigue and vigilance levels during long EEG\nrecording durations and BCI system usage bring along significant fluctuations\nin BCI performances even when these systems are calibrated daily. We address\nthis in an experimental offline study from EEG-based BCI speller usage data\nacquired for one hour duration. As the main part of our methodological\napproach, we propose the concept of adversarial invariant feature learning for\nBCIs as a regularization approach on recently expanding EEG deep learning\narchitectures, to learn nuisance-invariant discriminative features. We\nempirically demonstrate the feasibility of adversarial feature learning on\neliminating drowsiness effects from event related EEG activity features, by\nusing temporal recording block ordering as the source of drowsiness\nvariability.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 19:28:37 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Ozdenizci", "Ozan", ""], ["Oken", "Barry", ""], ["Memmott", "Tab", ""], ["Fried-Oken", "Melanie", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1907.09567", "submitter": "Naftali Cohen", "authors": "Naftali Cohen, Tucker Balch, and Manuela Veloso", "title": "The Effect of Visual Design in Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC q-fin.CP q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial companies continuously analyze the state of the markets to rethink\nand adjust their investment strategies. While the analysis is done on the\ndigital form of data, decisions are often made based on graphical\nrepresentations in white papers or presentation slides. In this study, we\nexamine whether binary decisions are better to be decided based on the numeric\nor the visual representation of the same data. Using two data sets, a matrix of\nnumerical data with spatial dependencies and financial data describing the\nstate of the S&P index, we compare the results of supervised classification\nbased on the original numerical representation and the visual transformation of\nthe same data. We show that, for these data sets, the visual transformation\nresults in higher predictability skill compared to the original form of the\ndata. We suggest thinking of the visual representation of numeric data,\neffectively, as a combination of dimensional reduction and feature engineering\ntechniques. In particular, if the visual layout encapsulates the full\ncomplexity of the data. In this view, thoughtful visual design can guard\nagainst overfitting, or introduce new features -- all of which benefit the\nlearning process, and effectively lead to better recognition of meaningful\npatterns.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 20:47:56 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 14:41:00 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Cohen", "Naftali", ""], ["Balch", "Tucker", ""], ["Veloso", "Manuela", ""]]}, {"id": "1907.09594", "submitter": "Jungseock Joo", "authors": "Nan Xi, Di Ma, Marcus Liou, Zachary C. Steinert-Threlkeld, Jason\n  Anastasopoulos, Jungseock Joo", "title": "Understanding the Political Ideology of Legislators from Social Media\n  Images", "comments": "To appear in the Proceedings of International AAAI Conference on Web\n  and Social Media (ICWSM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CV cs.HC cs.MM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we seek to understand how politicians use images to express\nideological rhetoric through Facebook images posted by members of the U.S.\nHouse and Senate. In the era of social media, politics has become saturated\nwith imagery, a potent and emotionally salient form of political rhetoric which\nhas been used by politicians and political organizations to influence public\nsentiment and voting behavior for well over a century. To date, however, little\nis known about how images are used as political rhetoric. Using deep learning\ntechniques to automatically predict Republican or Democratic party affiliation\nsolely from the Facebook photographs of the members of the 114th U.S. Congress,\nwe demonstrate that predicted class probabilities from our model function as an\naccurate proxy of the political ideology of images along a left-right\n(liberal-conservative) dimension. After controlling for the gender and race of\npoliticians, our method achieves an accuracy of 59.28% from single photographs\nand 82.35% when aggregating scores from multiple photographs (up to 150) of the\nsame person. To better understand image content distinguishing liberal from\nconservative images, we also perform in-depth content analyses of the\nphotographs. Our findings suggest that conservatives tend to use more images\nsupporting status quo political institutions and hierarchy maintenance,\nfeaturing individuals from dominant social groups, and displaying greater\nhappiness than liberals.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:43:49 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Xi", "Nan", ""], ["Ma", "Di", ""], ["Liou", "Marcus", ""], ["Steinert-Threlkeld", "Zachary C.", ""], ["Anastasopoulos", "Jason", ""], ["Joo", "Jungseock", ""]]}, {"id": "1907.09728", "submitter": "Yao Ming", "authors": "Yao Ming and Panpan Xu and Huamin Qu and Liu Ren", "title": "Interpretable and Steerable Sequence Learning via Prototypes", "comments": "Accepted as a full paper at KDD 2019 on May 8, 2019", "journal-ref": "Proceedings of the 25th ACM SIGKDD International Conference on\n  Knowledge Discovery & Data Mining, KDD 2019", "doi": "10.1145/3292500.3330908", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in machine learning nowadays is to provide\npredictions with not only high accuracy but also user-friendly explanations.\nAlthough in recent years we have witnessed increasingly popular use of deep\nneural networks for sequence modeling, it is still challenging to explain the\nrationales behind the model outputs, which is essential for building trust and\nsupporting the domain experts to validate, critique and refine the model. We\npropose ProSeNet, an interpretable and steerable deep sequence model with\nnatural explanations derived from case-based reasoning. The prediction is\nobtained by comparing the inputs to a few prototypes, which are exemplar cases\nin the problem domain. For better interpretability, we define several criteria\nfor constructing the prototypes, including simplicity, diversity, and sparsity\nand propose the learning objective and the optimization procedure. ProSeNet\nalso provides a user-friendly approach to model steering: domain experts\nwithout any knowledge on the underlying model or parameters can easily\nincorporate their intuition and experience by manually refining the prototypes.\nWe conduct experiments on a wide range of real-world applications, including\npredictive diagnostics for automobiles, ECG, and protein sequence\nclassification and sentiment analysis on texts. The result shows that ProSeNet\ncan achieve accuracy on par with state-of-the-art deep learning models. We also\nevaluate the interpretability of the results with concrete case studies.\nFinally, through user study on Amazon Mechanical Turk (MTurk), we demonstrate\nthat the model selects high-quality prototypes which align well with human\nknowledge and can be interactively refined for better interpretability without\nloss of performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 07:28:28 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Ming", "Yao", ""], ["Xu", "Panpan", ""], ["Qu", "Huamin", ""], ["Ren", "Liu", ""]]}, {"id": "1907.09781", "submitter": "Dominik Kowald PhD", "authors": "Dominik Kowald, Elisabeth Lex, Markus Schedl", "title": "Modeling Artist Preferences of Users with Different Music Consumption\n  Patterns for Fair Music Recommendations", "comments": "EuroCSS'2019 Symposium, Zurich, Switzerland", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Music recommender systems have become central parts of popular streaming\nplatforms such as Last.fm, Pandora, or Spotify to help users find music that\nfits their preferences. These systems learn from the past listening events of\nusers to recommend music a user will likely listen to in the future. Here,\ncurrent algorithms typically employ collaborative filtering (CF) utilizing\nsimilarities between users' listening behaviors. Some approaches also combine\nCF with content features into hybrid recommender systems. While music\nrecommender systems can provide quality recommendations to listeners of\nmainstream music artists, recent research has shown that they tend to\ndiscriminate listeners of unorthodox, low-mainstream artists. This is foremost\ndue to the scarcity of usage data of low-mainstream music as music consumption\npatterns are biased towards popular artists. Thus, the objective of our work is\nto provide a novel approach for modeling artist preferences of users with\ndifferent music consumption patterns and listening habits.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 09:22:25 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Kowald", "Dominik", ""], ["Lex", "Elisabeth", ""], ["Schedl", "Markus", ""]]}, {"id": "1907.09873", "submitter": "Kim Baraka", "authors": "Kim Baraka, Patr\\'icia Alves-Oliveira, Tiago Ribeiro", "title": "An extended framework for characterizing social robots", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social robots are becoming increasingly diverse in their design, behavior,\nand usage. In this chapter, we provide a broad-ranging overview of the main\ncharacteristics that arise when one considers social robots and their\ninteractions with humans. We specifically contribute a framework for\ncharacterizing social robots along 7 dimensions that we found to be most\nrelevant to their design. These dimensions are: appearance, social\ncapabilities, purpose and application area, relational role, autonomy and\nintelligence, proximity, and temporal profile. Within each dimension, we\naccount for the variety of social robots through a combination of\nclassifications and/or explanations. Our framework builds on and goes beyond\nexisting frameworks, such as classifications and taxonomies found in the\nliterature. More specifically, it contributes to the unification,\nclarification, and extension of key concepts, drawing from a rich body of\nrelevant literature. This chapter is meant to serve as a resource for\nresearchers, designers, and developers within and outside the field of social\nrobotics. It is intended to provide them with tools to better understand and\nposition existing social robots, as well as to inform their future design.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 13:42:45 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Baraka", "Kim", ""], ["Alves-Oliveira", "Patr\u00edcia", ""], ["Ribeiro", "Tiago", ""]]}, {"id": "1907.09896", "submitter": "Jonny O'Dwyer", "authors": "Jonny O'Dwyer, Niall Murray, Ronan Flynn", "title": "Eye-based Continuous Affect Prediction", "comments": "Accepted paper (pre-print) for 2019 8th International Conference on\n  Affective Computing and Intelligent Interaction (ACII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye-based information channels include the pupils, gaze, saccades, fixational\nmovements, and numerous forms of eye opening and closure. Pupil size variation\nindicates cognitive load and emotion, while a person's gaze direction is said\nto be congruent with the motivation to approach or avoid stimuli. The eyelids\nare involved in facial expressions that can encode basic emotions.\nAdditionally, eye-based cues can have implications for human annotators of\nemotions or feelings. Despite these facts, the use of eye-based cues in\naffective computing is in its infancy, however, and this work is intended to\nstart to address this. Eye-based feature sets, incorporating data from all of\nthe aforementioned information channels, that can be estimated from video are\nproposed. Feature set refinement is provided by way of continuous arousal and\nvalence learning and prediction experiments on the RECOLA validation set. The\neye-based features are then combined with a speech feature set to provide\nconfirmation of their usefulness and assess affect prediction performance\ncompared with group-of-humans-level performance on the RECOLA test set. The\ncore contribution of this paper, a refined eye-based feature set, is shown to\nprovide benefits for affect prediction. It is hoped that this work stimulates\nfurther research into eye-based affective computing.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 14:18:30 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 15:50:03 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["O'Dwyer", "Jonny", ""], ["Murray", "Niall", ""], ["Flynn", "Ronan", ""]]}, {"id": "1907.09904", "submitter": "Baldwin Nsonga", "authors": "Baldwin Nsonga, Gerik Scheuermann, Stefan Gumhold, Jordi\n  Ventosa-Molina, Denis Koschichow, Jochen Fr\\\"ohlich", "title": "Analysis of the Near-Wall Flow in a Turbine Cascade by Splat\n  Visualization", "comments": "Accepted at IEEE Scientific Visualization (SciVis) 2019. To appear in\n  IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934367", "report-no": null, "categories": "physics.flu-dyn cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turbines are essential components of jet planes and power plants. Therefore,\ntheir efficiency and service life are of central engineering interest. In the\ncase of jet planes or thermal power plants, the heating of the turbines due to\nthe hot gas flow is critical. Besides effective cooling, it is a major goal of\nengineers to minimize heat transfer between gas flow and turbine by design.\nSince it is known that splat events have a substantial impact on the heat\ntransfer between flow and immersed surfaces, we adapt a splat detection and\nvisualization method to a turbine cascade simulation in this case study.\nBecause splat events are small phenomena, we use a direct numerical simulation\nresolving the turbulence in the flow as the base of our analysis. The outcome\nshows promising insights into splat formation and its relation to vortex\nstructures. This may lead to better turbine design in the future.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 14:25:01 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Nsonga", "Baldwin", ""], ["Scheuermann", "Gerik", ""], ["Gumhold", "Stefan", ""], ["Ventosa-Molina", "Jordi", ""], ["Koschichow", "Denis", ""], ["Fr\u00f6hlich", "Jochen", ""]]}, {"id": "1907.09919", "submitter": "Jonny O'Dwyer", "authors": "Jonny O'Dwyer", "title": "Speech, Head, and Eye-based Cues for Continuous Affect Prediction", "comments": "Accepted paper (pre-print) for 2019 8th International Conference on\n  Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous affect prediction involves the discrete time-continuous regression\nof affect dimensions. Dimensions to be predicted often include arousal and\nvalence. Continuous affect prediction researchers are now embracing multimodal\nmodel input. This provides motivation for researchers to investigate previously\nunexplored affective cues. Speech-based cues have traditionally received the\nmost attention for affect prediction, however, non-verbal inputs have\nsignificant potential to increase the performance of affective computing\nsystems and in addition, allow affect modelling in the absence of speech.\nHowever, non-verbal inputs that have received little attention for continuous\naffect prediction include eye and head-based cues. The eyes are involved in\nemotion displays and perception while head-based cues have been shown to\ncontribute to emotion conveyance and perception. Additionally, these cues can\nbe estimated non-invasively from video, using modern computer vision tools.\nThis work exploits this gap by comprehensively investigating head and eye-based\nfeatures and their combination with speech for continuous affect prediction.\nHand-crafted, automatically generated and CNN-learned features from these\nmodalities will be investigated for continuous affect prediction. The highest\nperforming feature sets and feature set combinations will answer how effective\nthese features are for the prediction of an individual's affective state.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 14:46:51 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 15:59:13 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["O'Dwyer", "Jonny", ""]]}, {"id": "1907.10036", "submitter": "Sara Evensen", "authors": "Sara Evensen, Yoshihiko Suhara, Alon Halevy, Vivian Li, Wang-Chiew\n  Tan, Saran Mumick", "title": "Happiness Entailment: Automating Suggestions for Well-Being", "comments": "ACII 2019, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding what makes people happy is a central topic in psychology. Prior\nwork has mostly focused on developing self-reporting assessment tools for\nindividuals and relies on experts to analyze the periodic reported assessments.\nOne of the goals of the analysis is to understand what actions are necessary to\nencourage modifications in the behaviors of the individuals to improve their\noverall well-being. In this paper, we outline a complementary approach; on the\nassumption that the user journals her happy moments as short texts, a system\ncan analyze these texts and propose sustainable suggestions for the user that\nmay lead to an overall improvement in her well-being. We prototype one\nnecessary component of such a system, the Happiness Entailment Recognition\n(HER) module, which takes as input a short text describing an event, a\ncandidate suggestion, and outputs a determination about whether the suggestion\nis more likely to be good for this user based on the event described. This\ncomponent is implemented as a neural network model with two encoders, one for\nthe user input and one for the candidate actionable suggestion, with additional\nlayers to capture psychologically significant features in the happy moment and\nsuggestion.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:46:02 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Evensen", "Sara", ""], ["Suhara", "Yoshihiko", ""], ["Halevy", "Alon", ""], ["Li", "Vivian", ""], ["Tan", "Wang-Chiew", ""], ["Mumick", "Saran", ""]]}, {"id": "1907.10181", "submitter": "Jacqueline Faherty", "authors": "Robert Hurt (Caltech/IPAC), Ryan Wyatt (California Academy of\n  Sciences), Mark Subbarao (Adler Planetarium/International Planetarium\n  Society), Kimberly Arcand (Chandra X-ray Center, Harvard & Smithsonian Center\n  for Astrophysics), Jacqueline K. Faherty (American Museum of Natural\n  History), Janice Lee (Caltech/IPAC), Brandon Lawton (STScI)", "title": "Making the Case for Visualization", "comments": "Astro 2020 white paper submission on the State of the Profession", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual representation of information is a fundamental tool for advancing our\nunderstanding of science. It enables the research community to extract new\nknowledge from complex datasets, and plays an equally vital role in\ncommunicating new results across a spectrum of public audiences. Visualizations\nwhich make research results accessible to the public have been popularized by\nthe press, and are used in formal education, informal learning settings, and\nall aspects of lifelong learning. In particular, visualizations of astronomical\ndata (hereafter astrovisualization or astroviz) have broadly captured the human\nimagination, and are in high demand.\n  Astrovisualization practitioners need a wide variety of specialized skills\nand expertise spanning multiple disciplines (art, science, technology). As\nastrophysics research continues to evolve into a more data rich science,\nastroviz is also evolving from artists conceptions to data-driven\nvisualizations, from two-dimensional images to three-dimensional prints,\nrequiring new skills for development. Currently astroviz practitioners are\nspread throughout the country. Due to the specialized nature of the field there\nare seldom enough practitioners at one location to form an effective research\ngroup for the exchange of knowledge on best practices and new techniques.\nBecause of the increasing importance of visualization in modern astrophysics,\nthe fact that the astroviz community is small and spread out in disparate\nlocations, and the rapidly evolving nature of this field, we argue for the\ncreation and nurturing of an Astroviz Community of Practice.\n  We first summarize our recommendations. We then describe the current make-up\nof astrovisualization practitioners, give an overview of the audiences they\nserve, and highlight technological considerations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 00:10:38 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Hurt", "Robert", "", "Caltech/IPAC"], ["Wyatt", "Ryan", "", "California Academy of\n  Sciences"], ["Subbarao", "Mark", "", "Adler Planetarium/International Planetarium\n  Society"], ["Arcand", "Kimberly", "", "Chandra X-ray Center, Harvard & Smithsonian Center\n  for Astrophysics"], ["Faherty", "Jacqueline K.", "", "American Museum of Natural\n  History"], ["Lee", "Janice", "", "Caltech/IPAC"], ["Lawton", "Brandon", "", "STScI"]]}, {"id": "1907.10380", "submitter": "Th\\'eis Bazin", "authors": "Th\\'eis Bazin and Ga\\\"etan Hadjeres", "title": "NONOTO: A Model-agnostic Web Interface for Interactive Music Composition\n  by Inpainting", "comments": "3 pages, 1 figure. Published as a conference paper at the 10th\n  International Conference on Computational Creativity (ICCC 2019), UNC\n  Charlotte, North Carolina", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inpainting-based generative modeling allows for stimulating human-machine\ninteractions by letting users perform stylistically coherent local editions to\nan object using a statistical model. We present NONOTO, a new interface for\ninteractive music generation based on inpainting models. It is aimed both at\nresearchers, by offering a simple and flexible API allowing them to connect\ntheir own models with the interface, and at musicians by providing\nindustry-standard features such as audio playback, real-time MIDI output and\nstraightforward synchronization with DAWs using Ableton Link.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 13:47:46 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Bazin", "Th\u00e9is", ""], ["Hadjeres", "Ga\u00ebtan", ""]]}, {"id": "1907.10428", "submitter": "Zixing Zhang", "authors": "Jing Han, Zixing Zhang, Zhao Ren, Bj\\\"orn Schuller", "title": "EmoBed: Strengthening Monomodal Emotion Recognition via Training with\n  Crossmodal Emotion Embeddings", "comments": null, "journal-ref": null, "doi": "10.1109/TAFFC.2019.2928297", "report-no": null, "categories": "cs.LG cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite remarkable advances in emotion recognition, they are severely\nrestrained from either the essentially limited property of the employed single\nmodality, or the synchronous presence of all involved multiple modalities.\nMotivated by this, we propose a novel crossmodal emotion embedding framework\ncalled EmoBed, which aims to leverage the knowledge from other auxiliary\nmodalities to improve the performance of an emotion recognition system at hand.\nThe framework generally includes two main learning components, i. e., joint\nmultimodal training and crossmodal training. Both of them tend to explore the\nunderlying semantic emotion information but with a shared recognition network\nor with a shared emotion embedding space, respectively. In doing this, the\nenhanced system trained with this approach can efficiently make use of the\ncomplementary information from other modalities. Nevertheless, the presence of\nthese auxiliary modalities is not demanded during inference. To empirically\ninvestigate the effectiveness and robustness of the proposed framework, we\nperform extensive experiments on the two benchmark databases RECOLA and\nOMG-Emotion for the tasks of dimensional emotion regression and categorical\nemotion classification, respectively. The obtained results show that the\nproposed framework significantly outperforms related baselines in monomodal\ninference, and are also competitive or superior to the recently reported\nsystems, which emphasises the importance of the proposed crossmodal learning\nfor emotion recognition.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 07:55:29 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Han", "Jing", ""], ["Zhang", "Zixing", ""], ["Ren", "Zhao", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1907.10588", "submitter": "Arnaud Martin", "authors": "Jean-Christophe Dubois (DRUID), Laetitia Gros, Mouloud Kharoune\n  (DRUID), Yolande Le Gall (DRUID), Arnaud Martin (DRUID), Zolt\\'an Mikl\\'os\n  (DRUID), Hosna Ouni (DRUID)", "title": "Measuring the Expertise of Workers for Crowdsourcing Applications", "comments": null, "journal-ref": "Advances in Knowledge Discovery and Management, pp.139-157, 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing platforms enable companies to propose tasks to a large crowd of\nusers. The workers receive a compensation for their work according to the\nserious of the tasks they managed to accomplish. The evaluation of the quality\nof responses obtained from the crowd remains one of the most important problems\nin this context. Several methods have been proposed to estimate the expertise\nlevel of crowd workers. We propose an innovative measure of expertise assuming\nthat we possess a dataset with an objective comparison of the items concerned.\nOur method is based on the definition of four factors with the theory of belief\nfunctions. We compare our method to the Fagin distance on a dataset from a real\nexperiment, where users have to assess the quality of some audio recordings.\nThen, we propose to fuse both the Fagin distance and our expertise measure.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 06:44:01 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Dubois", "Jean-Christophe", "", "DRUID"], ["Gros", "Laetitia", "", "DRUID"], ["Kharoune", "Mouloud", "", "DRUID"], ["Gall", "Yolande Le", "", "DRUID"], ["Martin", "Arnaud", "", "DRUID"], ["Mikl\u00f3s", "Zolt\u00e1n", "", "DRUID"], ["Ouni", "Hosna", "", "DRUID"]]}, {"id": "1907.10594", "submitter": "Nitish Nag", "authors": "Nitish Nag, Vaibhav Pandey, Likhita Navali, Prateek Mohan, Ramesh Jain", "title": "Synchronizing Geospatial Information for Personalized Health Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The health effects of air pollution have been subject to intense study in\nrecent decades. Exposure to pollutants such as airborne particulate matter and\nozone has been associated with increases in morbidity and mortality, especially\nwith regards to respiratory and cardiovascular diseases. Unfortunately,\nindividuals do not have readily accessible methods by which to track their\nexposure to pollution. This paper proposes how pollution parameters like CO,\nNO2, O3, PM2.5, PM10 and SO2 can be monitored for respiratory and\ncardiovascular personalized health during outdoor exercise events. Using\nlocation tracked activities, we synchronize them to public data sets of\npollution sensors. For improved accuracy in estimation, we use heart rate data\nto understand breathing volume mapped with the local air quality sensors via\nconstant GPS tracking.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 19:16:03 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Nag", "Nitish", ""], ["Pandey", "Vaibhav", ""], ["Navali", "Likhita", ""], ["Mohan", "Prateek", ""], ["Jain", "Ramesh", ""]]}, {"id": "1907.10658", "submitter": "Kevin Bowden", "authors": "Kevin K.Bowden, Jiaqi Wu, Wen Cui, Juraj Juraska, Vrindavan Harrison,\n  Brian Schwarzmann, Nick Santer, Marilyn Walker", "title": "SlugBot: Developing a Computational Model andFramework of a Novel\n  Dialogue Genre", "comments": "arXiv admin note: text overlap with arXiv:1801.01531", "journal-ref": null, "doi": "10.13140/RG.2.2.33543.96166", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most interesting aspects of the Amazon Alexa Prize competition is\nthat the framing of the competition requires the development of new\ncomputational models of dialogue and its structure. Traditional computational\nmodels of dialogue are of two types: (1) task-oriented dialogue, supported by\nAI planning models,or simplified planning models consisting of frames with\nslots to be filled; or (2)search-oriented dialogue where every user turn is\ntreated as a search query that may elaborate and extend current search results.\nAlexa Prize dialogue systems such as SlugBot must support conversational\ncapabilities that go beyond what these traditional models can do. Moreover,\nwhile traditional dialogue systems rely on theoretical computational models,\nthere are no existing computational theories that circumscribe the expected\nsystem and user behaviors in the intended conversational genre of the Alexa\nPrize Bots. This paper describes how UCSC's SlugBot team has combined the\ndevelopment of a novel computational theoretical model, Discourse Relation\nDialogue Model, with its implementation in a modular system in order to test\nand refine it. We highlight how our novel dialogue model has led us to create a\nnovel ontological resource, UniSlug, and how the structure of UniSlug determine\nshow we curate and structure content so that our dialogue manager implements\nand tests our novel computational dialogue model.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:16:58 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Bowden", "Kevin K.", ""], ["Wu", "Jiaqi", ""], ["Cui", "Wen", ""], ["Juraska", "Juraj", ""], ["Harrison", "Vrindavan", ""], ["Schwarzmann", "Brian", ""], ["Santer", "Nick", ""], ["Walker", "Marilyn", ""]]}, {"id": "1907.10664", "submitter": "Asma Ghandeharioun", "authors": "Asma Ghandeharioun, Daniel McDuff, Mary Czerwinski, Kael Rowan", "title": "Towards Understanding Emotional Intelligence for Behavior Change\n  Chatbots", "comments": "Accepted for presentation at 2019 8th International Conference on\n  Affective Computing and Intelligent Interaction (ACII). arXiv admin note:\n  substantial text overlap with arXiv:1812.11423", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural conversational interface that allows longitudinal symptom tracking\nwould be extremely valuable in health/wellness applications. However, the task\nof designing emotionally-aware agents for behavior change is still poorly\nunderstood. In this paper, we present the design and evaluation of an\nemotion-aware chatbot that conducts experience sampling in an empathetic\nmanner. We evaluate it through a human-subject experiment with N=39\nparticipants over the course of a week. Our results show that extraverts\npreferred the emotion-aware chatbot significantly more than introverts. Also,\nparticipants reported a higher percentage of positive mood reports when\ninteracting with the empathetic bot. Finally, we provide guidelines for the\ndesign of emotion-aware chatbots for potential use in mHealth contexts.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 01:32:23 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Ghandeharioun", "Asma", ""], ["McDuff", "Daniel", ""], ["Czerwinski", "Mary", ""], ["Rowan", "Kael", ""]]}, {"id": "1907.10699", "submitter": "Ravi Chugh", "authors": "Brian Hempel and Justin Lubin and Ravi Chugh", "title": "Sketch-n-Sketch: Output-Directed Programming for SVG", "comments": "UIST 2019 Paper + Appendix", "journal-ref": null, "doi": "10.1145/3332165.3347925", "report-no": null, "categories": "cs.HC cs.GR cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For creative tasks, programmers face a choice: Use a GUI and sacrifice\nflexibility, or write code and sacrifice ergonomics?\n  To obtain both flexibility and ease of use, a number of systems have explored\na workflow that we call output-directed programming. In this paradigm, direct\nmanipulation of the program's graphical output corresponds to writing code in a\ngeneral-purpose programming language, and edits not possible with the mouse can\nstill be enacted through ordinary text edits to the program. Such capabilities\nprovide hope for integrating graphical user interfaces into what are currently\ntext-centric programming environments.\n  To further advance this vision, we present a variety of new output-directed\ntechniques that extend the expressive power of Sketch-n-Sketch, an\noutput-directed programming system for creating programs that generate vector\ngraphics. To enable output-directed interaction at more stages of program\nconstruction, we expose intermediate execution products for manipulation and we\npresent a mechanism for contextual drawing. Looking forward to output-directed\nprogramming beyond vector graphics, we also offer generic refactorings through\nthe GUI, and our techniques employ a domain-agnostic provenance tracing scheme.\n  To demonstrate the improved expressiveness, we implement a dozen new\nparametric designs in Sketch-n-Sketch without text-based edits. Among these is\nthe first demonstration of building a recursive function in an output-directed\nprogramming setting.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 20:16:48 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 02:10:58 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 02:20:28 GMT"}, {"version": "v4", "created": "Sat, 10 Aug 2019 21:07:31 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Hempel", "Brian", ""], ["Lubin", "Justin", ""], ["Chugh", "Ravi", ""]]}, {"id": "1907.10739", "submitter": "Sebastian Gehrmann", "authors": "Sebastian Gehrmann, Hendrik Strobelt, Robert Kr\\\"uger, Hanspeter\n  Pfister, Alexander M. Rush", "title": "Visual Interaction with Deep Learning Models through Collaborative\n  Semantic Inference", "comments": "IEEE VIS 2019 (VAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automation of tasks can have critical consequences when humans lose agency\nover decision processes. Deep learning models are particularly susceptible\nsince current black-box approaches lack explainable reasoning. We argue that\nboth the visual interface and model structure of deep learning systems need to\ntake into account interaction design. We propose a framework of collaborative\nsemantic inference (CSI) for the co-design of interactions and models to enable\nvisual collaboration between humans and algorithms. The approach exposes the\nintermediate reasoning process of models which allows semantic interactions\nwith the visual metaphors of a problem, which means that a user can both\nunderstand and control parts of the model reasoning process. We demonstrate the\nfeasibility of CSI with a co-designed case study of a document summarization\nsystem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 21:37:29 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Gehrmann", "Sebastian", ""], ["Strobelt", "Hendrik", ""], ["Kr\u00fcger", "Robert", ""], ["Pfister", "Hanspeter", ""], ["Rush", "Alexander M.", ""]]}, {"id": "1907.10781", "submitter": "Hui Liu", "authors": "Hui Liu, Wentao Qin and Xiaojun Wan", "title": "INS: An Interactive Chinese News Synthesis System", "comments": "6 pages, 1 figure", "journal-ref": "Proceedings of NAACL-HLT 2019: Demonstrations, pages 18-23", "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, we are surrounded by more and more online news articles. Tens or\nhundreds of news articles need to be read if we wish to explore a hot news\nevent or topic. So it is of vital importance to automatically synthesize a\nbatch of news articles related to the event or topic into a new synthesis\narticle (or overview article) for reader's convenience. It is so challenging to\nmake news synthesis fully automatic that there is no successful solution by\nnow. In this paper, we put forward a novel Interactive News Synthesis system\n(i.e. INS), which can help generate news overview articles automatically or by\ninteracting with users. More importantly, INS can serve as a tool for editors\nto help them finish their jobs. In our experiments, INS performs well on both\ntopic representation and synthesis article generation. A user study also\ndemonstrates the usefulness and users' satisfaction with the INS tool. A demo\nvideo is available at \\url{https://youtu.be/7ItteKW3GEk}.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 01:01:27 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Liu", "Hui", ""], ["Qin", "Wentao", ""], ["Wan", "Xiaojun", ""]]}, {"id": "1907.10917", "submitter": "Chee Siang Ang", "authors": "Ben Nicholls, Chee Siang Ang, Eiman Kanjo, Panote Siriaraya, Woon-Hong\n  Yeo, Athanasios Tsanas", "title": "An EMG-based Eating Behaviour Monitoring System with Haptic Feedback to\n  Promote Mindful Eating", "comments": "v1: 13 + 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mindless eating, or the lack of awareness of the food we are consuming, has\nbeen linked to health problems attributed to unhealthy eating behaviour,\nincluding obesity. Traditional approaches used to moderate eating behaviour\noften rely on inaccurate self-logging, manual observations or bulky equipment.\nOverall, there is a need for an intelligent and lightweight system which can\nautomatically monitor eating behaviour and provide feedback. In this paper, we\ninvestigate: i) the development of an automated system for detecting eating\nbehaviour using wearable Electromyography (EMG) sensors, and ii) the\napplication of such a system in combination with real time wristband haptic\nfeedback to facilitate mindful eating. Data collected from 16 participants were\nused to develop an algorithm for detecting chewing and swallowing. We extracted\n18 features from EMG and presented those features to different classifiers. We\ndemonstrated that eating behaviour can be automatically assessed accurately\nusing the EMG-extracted features and a Support Vector Machine (SVM):\nF1-Score=0.94 for chewing classification, and F1-Score=0.86 for swallowing\nclassification. Based on this algorithm, we developed a system to enable\nparticipants to self-moderate their chewing behaviour using haptic feedback. An\nexperiment study was carried out with 20 additional participants showing that\nparticipants exhibited a lower rate of chewing when haptic feedback delivered\nin forms of wristband vibration was used compared to a baseline and non-haptic\ncondition (F (2,38)=58.243, p<0.001). These findings may have major\nimplications for research in eating behaviour, providing key new insights into\nthe impacts of automatic chewing detection and haptic feedback systems on\nmoderating eating behaviour with the aim to improve health outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 09:22:29 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Nicholls", "Ben", ""], ["Ang", "Chee Siang", ""], ["Kanjo", "Eiman", ""], ["Siriaraya", "Panote", ""], ["Yeo", "Woon-Hong", ""], ["Tsanas", "Athanasios", ""]]}, {"id": "1907.11000", "submitter": "Ludovik Coba <", "authors": "Ludovik Coba, Panagiotis Symeonidis, Markus Zanker", "title": "Personalised novel and explainable matrix factorisation", "comments": null, "journal-ref": "Data & Knowledge Engineering Volume 122, July 2019, Pages 142-158\n  https://www.sciencedirect.com/science/article/pii/S0169023X1830332X", "doi": "10.1016/j.datak.2019.06.003", "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recommendation systems personalise suggestions to individuals to help them in\ntheir decision making and exploration tasks. In the ideal case, these\nrecommendations, besides of being accurate, should also be novel and\nexplainable. However, up to now most platforms fail to provide both, novel\nrecommendations that advance users' exploration along with explanations to make\ntheir reasoning more transparent to them. For instance, a well-known\nrecommendation algorithm, such as matrix factorisation (MF), optimises only the\naccuracy criterion, while disregarding other quality criteria such as the\nexplainability or the novelty, of recommended items. In this paper, to the best\nof our knowledge, we propose a new model, denoted as NEMF, that allows to\ntrade-off the MF performance with respect to the criteria of novelty and\nexplainability, while only minimally compromising on accuracy. In addition, we\nrecommend a new explainability metric based on nDCG, which distinguishes a more\nexplainable item from a less explainable item. An initial user study indicates\nhow users perceive the different attributes of these \"user\" style explanations\nand our extensive experimental results demonstrate that we attain high accuracy\nby recommending also novel and explainable items.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 12:21:19 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Coba", "Ludovik", ""], ["Symeonidis", "Panagiotis", ""], ["Zanker", "Markus", ""]]}, {"id": "1907.11039", "submitter": "Nathan Hurley", "authors": "Nathan C. Hurley, Adrian D. Haimovich, R. Andrew Taylor, Bobak J.\n  Mortazavi", "title": "Visualization of Emergency Department Clinical Data for Interpretable\n  Patient Phenotyping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual summarization of clinical data collected on patients contained within\nthe electronic health record (EHR) may enable precise and rapid triage at the\ntime of patient presentation to an emergency department (ED). The triage\nprocess is critical in the appropriate allocation of resources and in\nanticipating eventual patient disposition, typically admission to the hospital\nor discharge home. EHR data are high-dimensional and complex, but offer the\nopportunity to discover and characterize underlying data-driven patient\nphenotypes. These phenotypes will enable improved, personalized therapeutic\ndecision making and prognostication. In this work, we focus on the challenge of\ntwo-dimensional patient projections. A low dimensional embedding offers visual\ninterpretability lost in higher dimensions. While linear dimensionality\nreduction techniques such as principal component analysis are often used\ntowards this aim, they are insufficient to describe the variance of patient\ndata. In this work, we employ the newly-described non-linear embedding\ntechnique called uniform manifold approximation and projection (UMAP). UMAP\nseeks to capture both local and global structures in high-dimensional data. We\nthen use Gaussian mixture models to identify clusters in the embedded data and\nuse the adjusted Rand index (ARI) to establish stability in the discovery of\nthese clusters. This technique is applied to five common clinical chief\ncomplaints from a real-world ED EHR dataset, describing the emergent properties\nof discovered clusters. We observe clinically-relevant cluster attributes,\nsuggesting that visual embeddings of EHR data using non-linear dimensionality\nreduction is a promising approach to reveal data-driven patient phenotypes. In\nthe five chief complaints, we find between 2 and 6 clusters, with the peak mean\npairwise ARI between subsequent training iterations to range from 0.35 to 0.74.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 19:01:56 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Hurley", "Nathan C.", ""], ["Haimovich", "Adrian D.", ""], ["Taylor", "R. Andrew", ""], ["Mortazavi", "Bobak J.", ""]]}, {"id": "1907.11040", "submitter": "Yong Wang", "authors": "Yong Wang, Zhihua Jin, Qianwen Wang, Weiwei Cui, Tengfei Ma and Huamin\n  Qu", "title": "DeepDrawing: A Deep Learning Approach to Graph Drawing", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node-link diagrams are widely used to facilitate network explorations.\nHowever, when using a graph drawing technique to visualize networks, users\noften need to tune different algorithm-specific parameters iteratively by\ncomparing the corresponding drawing results in order to achieve a desired\nvisual effect. This trial and error process is often tedious and\ntime-consuming, especially for non-expert users. Inspired by the powerful data\nmodelling and prediction capabilities of deep learning techniques, we explore\nthe possibility of applying deep learning techniques to graph drawing.\nSpecifically, we propose using a graph-LSTM-based approach to directly map\nnetwork structures to graph drawings. Given a set of layout examples as the\ntraining dataset, we train the proposed graph-LSTM-based model to capture their\nlayout characteristics. Then, the trained model is used to generate graph\ndrawings in a similar style for new networks. We evaluated the proposed\napproach on two special types of layouts (i.e., grid layouts and star layouts)\nand two general types of layouts (i.e., ForceAtlas2 and PivotMDS) in both\nqualitative and quantitative ways. The results provide support for the\neffectiveness of our approach. We also conducted a time cost assessment on the\ndrawings of small graphs with 20 to 50 nodes. We further report the lessons we\nlearned and discuss the limitations and future work.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 15:34:44 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 15:14:02 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 16:50:22 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Wang", "Yong", ""], ["Jin", "Zhihua", ""], ["Wang", "Qianwen", ""], ["Cui", "Weiwei", ""], ["Ma", "Tengfei", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.11042", "submitter": "Andreas Miaoudakis", "authors": "Constantinos Marios Angelopoulos, Vasilios Katos, Theodoros Kostoulas,\n  Andreas Miaoudakis, Nikolaos Petroulakis, George Alexandris, Giorgos\n  Demetriou, Giuditta Morandi, Karolina Waledzik, Urszula Rak, Marios\n  Panayiotou, Christos Iraklis Tsatsoulis", "title": "IDEAL-CITIES: A Trustworthy and Sustainable Framework for Circular Smart\n  Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reflecting upon the sustainability challenges cities will be facing in the\nnear future and the recent technological developments allowing cities to become\n\"smart\", we introduce IDEAL-CITIES; a framework aiming to provide an\narchitecture for cyber-physical systems to deliver a data-driven Circular\nEconomy model in a city context. In the IDEAL-CITIES ecosystem, the city's\nfinite resources as well as citizens will form the pool of intelligent assets\nin order to contribute to high utilization through crowdsourcing and real-time\ndecision making and planning. We describe two use cases as a vehicle to\ndemonstrate how a smart city can serve the Circular Economy paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:39:07 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Angelopoulos", "Constantinos Marios", ""], ["Katos", "Vasilios", ""], ["Kostoulas", "Theodoros", ""], ["Miaoudakis", "Andreas", ""], ["Petroulakis", "Nikolaos", ""], ["Alexandris", "George", ""], ["Demetriou", "Giorgos", ""], ["Morandi", "Giuditta", ""], ["Waledzik", "Karolina", ""], ["Rak", "Urszula", ""], ["Panayiotou", "Marios", ""], ["Tsatsoulis", "Christos Iraklis", ""]]}, {"id": "1907.11106", "submitter": "Mihai B\\^ace", "authors": "Mihai B\\^ace, Sander Staal, Andreas Bulling", "title": "How far are we from quantifying visual attention in mobile HCI?", "comments": "7 pages, 4 figures", "journal-ref": "IEEE Pervasive Computing, April-June 2020", "doi": "10.1109/MPRV.2020.2967736", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an ever-increasing number of mobile devices competing for our attention,\nquantifying when, how often, or for how long users visually attend to their\ndevices has emerged as a core challenge in mobile human-computer interaction.\nEncouraged by recent advances in automatic eye contact detection using machine\nlearning and device-integrated cameras, we provide a fundamental investigation\ninto the feasibility of quantifying visual attention during everyday mobile\ninteractions. We identify core challenges and sources of errors associated with\nsensing attention on mobile devices in the wild, including the impact of face\nand eye visibility, the importance of robust head pose estimation, and the need\nfor accurate gaze estimation. Based on this analysis, we propose future\nresearch directions and discuss how eye contact detection represents the\nfoundation for exciting new applications towards next-generation pervasive\nattentive user interfaces.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:38:52 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["B\u00e2ce", "Mihai", ""], ["Staal", "Sander", ""], ["Bulling", "Andreas", ""]]}, {"id": "1907.11115", "submitter": "Mihai B\\^ace", "authors": "Mihai B\\^ace, Sander Staal, Andreas Bulling", "title": "Accurate and Robust Eye Contact Detection During Everyday Mobile Device\n  Interactions", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of human attention is key to several tasks in mobile\nhuman-computer interaction (HCI), such as predicting user interruptibility,\nestimating noticeability of user interface content, or measuring user\nengagement. Previous works to study mobile attentive behaviour required\nspecial-purpose eye tracking equipment or constrained users' mobility. We\npropose a novel method to sense and analyse visual attention on mobile devices\nduring everyday interactions. We demonstrate the capabilities of our method on\nthe sample task of eye contact detection that has recently attracted increasing\nresearch interest in mobile HCI. Our method builds on a state-of-the-art method\nfor unsupervised eye contact detection and extends it to address challenges\nspecific to mobile interactive scenarios. Through evaluation on two current\ndatasets, we demonstrate significant performance improvements for eye contact\ndetection across mobile devices, users, or environmental conditions. Moreover,\nwe discuss how our method enables the calculation of additional attention\nmetrics that, for the first time, enable researchers from different domains to\nstudy and quantify attention allocation during mobile interactions in the wild.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:55:16 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["B\u00e2ce", "Mihai", ""], ["Staal", "Sander", ""], ["Bulling", "Andreas", ""]]}, {"id": "1907.11146", "submitter": "Justin Edwards", "authors": "Benjamin R. Cowan, Philip Doyle, Justin Edwards, Diego Garaialde, Ali\n  Hayes-Brady, Holly P. Branigan, Jo\\~ao Cabral, Leigh Clark", "title": "What's in an accent? The impact of accented synthetic speech on lexical\n  choice in human-machine dialogue", "comments": "In press, accepted at 1st International Conference on Conversational\n  User Interfaces (CUI 2019)", "journal-ref": null, "doi": "10.1145/3342775.3342786", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumptions we make about a dialogue partner's knowledge and\ncommunicative ability (i.e. our partner models) can influence our language\nchoices. Although similar processes may operate in human-machine dialogue, the\nrole of design in shaping these models, and their subsequent effects on\ninteraction are not clearly understood. Focusing on synthesis design, we\nconduct a referential communication experiment to identify the impact of\naccented speech on lexical choice. In particular, we focus on whether accented\nspeech may encourage the use of lexical alternatives that are relevant to a\npartner's accent, and how this is may vary when in dialogue with a human or\nmachine. We find that people are more likely to use American English terms when\nspeaking with a US accented partner than an Irish accented partner in both\nhuman and machine conditions. This lends support to the proposal that synthesis\ndesign can influence partner perception of lexical knowledge, which in turn\nguide user's lexical choices. We discuss the findings with relation to the\nnature and dynamics of partner models in human machine dialogue.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 15:39:11 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Cowan", "Benjamin R.", ""], ["Doyle", "Philip", ""], ["Edwards", "Justin", ""], ["Garaialde", "Diego", ""], ["Hayes-Brady", "Ali", ""], ["Branigan", "Holly P.", ""], ["Cabral", "Jo\u00e3o", ""], ["Clark", "Leigh", ""]]}, {"id": "1907.11179", "submitter": "Leigh Clark", "authors": "David R. Large, Gary Burnett, Leigh Clark", "title": "Lessons from Oz: Design Guidelines for Automotive Conversational User\n  Interfaces", "comments": "Accepted to the 11th International ACM Conference on Automotive User\n  Interfaces and Interactive Vehicular Applications (AutomotiveUI '19)", "journal-ref": null, "doi": "10.1145/3349263.3351314", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper draws from literature and our experience of conducting\nWizard-of-Oz (WoZ) studies using natural language, conversational user\ninterfaces (CUIs) in the automotive domain. These studies have revealed\npositive effects of using in-vehicle CUIs on issues such as: cognitive\ndemand/workload, passive task-related fatigue, trust, acceptance and\nenvironment engagement. A nascent set of human-centred design guidelines that\nhave emerged is presented. These are based on the analysis of users' behaviour\nand the positive benefits observed, and aim to make interactions with an\nin-vehicle agent interlocutor safe, effective, engaging and enjoyable, while\nconfirming with users' expectations. The guidelines can be used to inform the\ndesign of future in-vehicle CUIs or applied experimentally using WoZ\nmethodology, and will be evaluated and refined in ongoing work.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 16:39:02 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Large", "David R.", ""], ["Burnett", "Gary", ""], ["Clark", "Leigh", ""]]}, {"id": "1907.11260", "submitter": "Yen-Chia Hsu", "authors": "Yen-Chia Hsu, Illah Nourbakhsh", "title": "When Human-Computer Interaction Meets Community Citizen Science", "comments": "Viewpoints accepted by the Communications of the ACM", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-computer interaction (HCI) studies the design and use of interfaces and\ninteractive systems. HCI has been adopted successfully in modern commercial\nproducts. Recently, its use for promoting social good and pursuing\nsustainability, known as sustainable HCI, has begun to receive wide attention.\nConventionally, scientists and decision-makers apply top-down approaches to\nlead research activities that engage lay people in facilitating sustainability,\nsuch as saving energy. We introduce an alternative framework, Community Citizen\nScience (CCS), to closely connect research and social issues by empowering\ncommunities to produce scientific knowledge, represent their needs, address\ntheir concerns, and advocate for impact. CCS advances the current\nscience-oriented concept to a deeper level that aims to sustain community\nengagement when researchers are no longer involved after the intervention of\ninteractive systems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 18:11:23 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Hsu", "Yen-Chia", ""], ["Nourbakhsh", "Illah", ""]]}, {"id": "1907.11265", "submitter": "Enamul Hoque", "authors": "Enamul Hoque and Maneesh Agrawala", "title": "Searching the Visual Style and Structure of D3 Visualizations", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934431", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a search engine for D3 visualizations that allows queries based on\ntheir visual style and underlying structure. To build the engine we crawl a\ncollection of 7860 D3 visualizations from the Web and deconstruct each one to\nrecover its data, its data-encoding marks and the encodings describing how the\ndata is mapped to visual attributes of the marks. We also extract axes and\nother non-data-encoding attributes of marks (e.g., typeface, background color).\nOur search engine indexes this style and structure information as well as\nmetadata about the webpage containing the chart. We show how visualization\ndevelopers can search the collection to find visualizations that exhibit\nspecific design characteristics and thereby explore the space of possible\ndesigns. We also demonstrate how researchers can use the search engine to\nidentify commonly used visual design patterns and we perform such a demographic\ndesign analysis across our collection of D3 charts. A user study reveals that\nvisualization developers found our style and structure based search engine to\nbe significantly more useful and satisfying for finding different designs of D3\ncharts, than a baseline search engine that only allows keyword search over the\nwebpage containing a chart.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 18:26:36 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 18:22:33 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Hoque", "Enamul", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "1907.11358", "submitter": "Rafael Veras", "authors": "Rafael Veras and Christopher Collins", "title": "Discriminability Tests for Visualization Effectiveness and Scalability", "comments": "Accepted for presentation at IEEE VIS 2019, to be held October 20-25\n  in Vancouver, Canada; will be published in a special issue of IEEE\n  Transactions on Visualization and Computer Graphics (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scalability of a particular visualization approach is limited by the\nability for people to discern differences between plots made with different\ndatasets. Ideally, when the data changes, the visualization changes in\nperceptible ways. This relation breaks down when there is a mismatch between\nthe encoding and the character of the dataset being viewed. Unfortunately,\nvisualizations are often designed and evaluated without fully exploring how\nthey will respond to a wide variety of datasets. We explore the use of an image\nsimilarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for\ntesting the discriminability of a data visualization across a variety of\ndatasets. MS-SSIM is able to capture the similarity of two visualizations\nacross multiple scales, including low level granular changes and high level\npatterns. Significant data changes that are not captured by the MS-SSIM\nindicate visualizations of low discriminability and effectiveness. The\nmeasure's utility is demonstrated with two empirical studies. In the first, we\ncompare human similarity judgments and MS-SSIM scores for a collection of\nscatterplots. In the second, we compute the discriminability values for a set\nof basic visualizations and compare them with empirical measurements of\neffectiveness. In both cases, the analyses show that the computational measure\nis able to approximate empirical results. Our approach can be used to rank\ncompeting encodings on their discriminability and to aid in selecting\nvisualizations for a particular type of data distribution.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 01:57:36 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Veras", "Rafael", ""], ["Collins", "Christopher", ""]]}, {"id": "1907.11481", "submitter": "Shahid Latif", "authors": "Haris Mumtaz, Shahid Latif, Fabian Beck, and Daniel Weiskopf", "title": "Exploranative Code Quality Documents", "comments": "IEEE VIS VAST 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934669", "report-no": null, "categories": "cs.HC cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Good code quality is a prerequisite for efficiently developing maintainable\nsoftware. In this paper, we present a novel approach to generate exploranative\n(explanatory and exploratory) data-driven documents that report code quality in\nan interactive, exploratory environment. We employ a template-based natural\nlanguage generation method to create textual explanations about the code\nquality, dependent on data from software metrics. The interactive document is\nenriched by different kinds of visualization, including parallel coordinates\nplots and scatterplots for data exploration and graphics embedded into text. We\ndevise an interaction model that allows users to explore code quality with\nconsistent linking between text and visualizations; through integrated\nexplanatory text, users are taught background knowledge about code quality\naspects. Our approach to interactive documents was developed in a design study\nprocess that included software engineering and visual analytics experts.\nAlthough the solution is specific to the software engineering scenario, we\ndiscuss how the concept could generalize to multivariate data and report\nlessons learned in a broader scope.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:07:23 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 08:01:10 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Mumtaz", "Haris", ""], ["Latif", "Shahid", ""], ["Beck", "Fabian", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "1907.11498", "submitter": "Mohammed Khwaja", "authors": "Mohammed Khwaja and Aleksandar Matic", "title": "Personality is Revealed During Weekends: Towards Data Minimisation for\n  Smartphone Based Personality Classification", "comments": null, "journal-ref": "In 17th IFIP Conference on Human-Computer Interaction (INTERACT).\n  Springer, 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous literature has explored automatic personality modelling using\nsmartphone data for its potential to personalise mobile services. Although\npassive modelling of personality removes the burden of completing lengthy\nquestionnaires, the fact that such models typically require a few weeks or\nmonths of personal data can negatively impact user's engagement. In this study,\nwe explore the feasibility of reducing the duration of data collection in the\ncontext of personality classification. We found that only one or two weekends\ncan suffice for achieving state-of-the-art accuracy between 66% and 71% for\nclassifying the five personality traits. These results provide lessons for\npracticing \"data minimisation\" - a key principle of privacy laws.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 11:40:29 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 00:21:32 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Khwaja", "Mohammed", ""], ["Matic", "Aleksandar", ""]]}, {"id": "1907.11510", "submitter": "Fabien Ringeval", "authors": "Fabien Ringeval, Bj\\\"orn Schuller, Michel Valstar, NIcholas Cummins,\n  Roddy Cowie, Leili Tavabi, Maximilian Schmitt, Sina Alisamir, Shahin\n  Amiriparian, Eva-Maria Messner, Siyang Song, Shuo Liu, Ziping Zhao, Adria\n  Mallol-Ragolta, Zhao Ren, Mohammad Soleymani, Maja Pantic", "title": "AVEC 2019 Workshop and Challenge: State-of-Mind, Detecting Depression\n  with AI, and Cross-Cultural Affect Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.IR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Audio/Visual Emotion Challenge and Workshop (AVEC 2019) \"State-of-Mind,\nDetecting Depression with AI, and Cross-cultural Affect Recognition\" is the\nninth competition event aimed at the comparison of multimedia processing and\nmachine learning methods for automatic audiovisual health and emotion analysis,\nwith all participants competing strictly under the same conditions. The goal of\nthe Challenge is to provide a common benchmark test set for multimodal\ninformation processing and to bring together the health and emotion recognition\ncommunities, as well as the audiovisual processing communities, to compare the\nrelative merits of various approaches to health and emotion recognition from\nreal-life data. This paper presents the major novelties introduced this year,\nthe challenge guidelines, the data used, and the performance of the baseline\nsystems on the three proposed tasks: state-of-mind recognition, depression\nassessment with AI, and cross-cultural affect sensing, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 13:41:42 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Ringeval", "Fabien", ""], ["Schuller", "Bj\u00f6rn", ""], ["Valstar", "Michel", ""], ["Cummins", "NIcholas", ""], ["Cowie", "Roddy", ""], ["Tavabi", "Leili", ""], ["Schmitt", "Maximilian", ""], ["Alisamir", "Sina", ""], ["Amiriparian", "Shahin", ""], ["Messner", "Eva-Maria", ""], ["Song", "Siyang", ""], ["Liu", "Shuo", ""], ["Zhao", "Ziping", ""], ["Mallol-Ragolta", "Adria", ""], ["Ren", "Zhao", ""], ["Soleymani", "Mohammad", ""], ["Pantic", "Maja", ""]]}, {"id": "1907.11585", "submitter": "Justin Edwards", "authors": "Philip R. Doyle, Justin Edwards, Odile Dumbleton, Leigh Clark,\n  Benjamin R. Cowan", "title": "Mapping Perceptions of Humanness in Speech-Based Intelligent Personal\n  Assistant Interaction", "comments": null, "journal-ref": null, "doi": "10.1145/3338286.3340116", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanness is core to speech interface design. Yet little is known about how\nusers conceptualise perceptions of humanness and how people define their\ninteraction with speech interfaces through this. To map these perceptions n=21\nparticipants held dialogues with a human and two speech interface based\nintelligent personal assistants, and then reflected and compared their\nexperiences using the repertory grid technique. Analysis of the constructs show\nthat perceptions of humanness are multidimensional, focusing on eight key\nthemes: partner knowledge set, interpersonal connection, linguistic content,\npartner performance and capabilities, conversational interaction, partner\nidentity and role, vocal qualities and behavioral affordances. Through these\nthemes, it is clear that users define the capabilities of speech interfaces\ndifferently to humans, seeing them as more formal, fact based, impersonal and\nless authentic. Based on the findings, we discuss how the themes help to\nscaffold, categorise and target research and design efforts, considering the\nappropriateness of emulating humanness.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 14:12:03 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 14:31:14 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Doyle", "Philip R.", ""], ["Edwards", "Justin", ""], ["Dumbleton", "Odile", ""], ["Clark", "Leigh", ""], ["Cowan", "Benjamin R.", ""]]}, {"id": "1907.11656", "submitter": "Roger Moore", "authors": "Roger K. Moore", "title": "Vocal Interactivity in Crowds, Flocks and Swarms: Implications for Voice\n  User Interfaces", "comments": "Accepted at 2nd International Workshop on Vocal Interactivity\n  in-and-between Humans, Animals and Robots (VIHAR-2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen an explosion in the availability of Voice User\nInterfaces. However, user surveys suggest that there are issues with respect to\nusability, and it has been hypothesised that contemporary voice-enabled systems\nare missing crucial behaviours relating to user engagement and vocal\ninteractivity. However, it is well established that such ostensive behaviours\nare ubiquitous in the animal kingdom, and that vocalisation provides a means\nthrough which interaction may be coordinated and managed between individuals\nand within groups. Hence, this paper reports results from a study aimed at\nidentifying generic mechanisms that might underpin coordinated collective vocal\nbehaviour with a particular focus on closed-loop negative-feedback control as a\npowerful regulatory process. A computer-based real-time simulation of vocal\ninteractivity is described which has provided a number of insights, including\nthe enumeration of a number of key control variables that may be worthy of\nfurther investigation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 16:07:20 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Moore", "Roger K.", ""]]}, {"id": "1907.11662", "submitter": "Giacomo Ortali", "authors": "Panagiotis Lionakis, Giacomo Ortali, Ioannis G. Tollis", "title": "Adventures in Abstraction: Reachability in Hierarchical Drawings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present algorithms and experiments for the visualization of directed\ngraphs that focus on displaying their reachability information. Our algorithms\nare based on the concepts of the path and channel decomposition as proposed in\nthe framework presented in GD 2018 (pp. 579-592) and focus on showing the\nexistence of paths clearly. In this paper we customize these concepts and\npresent experimental results that clearly show the interplay between bends,\ncrossings and clarity. Additionally, our algorithms have direct applications to\nthe important problem of showing and storing transitivity information of very\nlarge graphs and databases. Only a subset of the edges is drawn, thus reducing\nthe visual complexity of the resulting drawing, and the memory requirements for\nstoring the transitivity information. Our algorithms require almost linear\ntime, $O(kn+m)$, where $k$ is the number of paths/channels, $n$ and $m$ is the\nnumber of vertices and edges, respectively. They produce progressively more\nabstract drawings of the input graph. No dummy vertices are introduced and the\nvertices of each path/channel are vertically aligned.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 16:16:41 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Lionakis", "Panagiotis", ""], ["Ortali", "Giacomo", ""], ["Tollis", "Ioannis G.", ""]]}, {"id": "1907.11741", "submitter": "Bel\\'en Sald\\'ias", "authors": "Belen Saldias and Rosalind W. Picard", "title": "Tweet Moodifier: Towards giving emotional awareness to Twitter users", "comments": "Accepted at 2019 8th International Conference on Affective Computing\n  and Intelligent Interaction (ACII)", "journal-ref": "2019 8th International Conference on Affective Computing and\n  Intelligent Interaction (ACII)", "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotional contagion in online social networks has been of great interest over\nthe past years. Previous studies have focused mainly on finding evidence of\naffect contagion in homophilic atmospheres. However, these studies have\noverlooked users' awareness of the sentiments they share and consume online. In\nthis paper, we present an experiment with Twitter users that aims to help them\nbetter understand which emotions they experience on this social network. We\nintroduce Tweet Moodifier (T-Moodifier), a Google Chrome extension that enables\nTwitter users to filter and make explicit (through colored visual marks) the\nemotional content in their News Feed. We compare behavioral changes between 55\nparticipants and 5089 of their public \"friends.\" The comparison period spans\nfrom two weeks before installing T-Moodifier to one week thereafter. The\nresults suggest that the use of T-Moodifier might help Twitter users increase\ntheir emotional awareness: T-Moodifier users who had access to emotional\nstatistics about their posts produced a significantly higher percentage of\nneutral content. This behavioral change suggests that people could behave\ndifferently while using real-time mechanisms that increase their affect\nreflection. Also, post-experience, those who completed both pre- and\npost-surveys could assert more confidently the main emotions they shared and\nperceived on Twitter. This shows T-Moodifier's potential to effectively make\nusers reflect on their News Feed.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 18:21:57 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 02:55:58 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 04:44:59 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Saldias", "Belen", ""], ["Picard", "Rosalind W.", ""]]}, {"id": "1907.11743", "submitter": "Doris Jung-Lin Lee", "authors": "Doris Jung-Lin Lee, Jaewoo Kim, Renxuan Wang, Aditya Parameswaran", "title": "SCATTERSEARCH: Visual Querying of Scatterplot Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scatterplots are one of the simplest and most commonly-used visualizations\nfor understanding quantitative, multidimensional data. However, since\nscatterplots only depict two attributes at a time, analysts often need to\nmanually generate and inspect large numbers of scatterplots to make sense of\nlarge datasets with many attributes. We present a visual query system for\nscatterplots, SCATTERSEARCH, that enables users to visually search and browse\nthrough large collections of scatterplots. Users can query for other\nvisualizations based on a region of interest or find other scatterplots that\n\"look similar'' to a selected one. We present two demo scenarios, provide a\nsystem overview of SCATTERSEARCH, and outline future directions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 18:33:45 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Lee", "Doris Jung-Lin", ""], ["Kim", "Jaewoo", ""], ["Wang", "Renxuan", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "1907.11762", "submitter": "Soumya Dutta", "authors": "Soumya Dutta and Ayan Biswas and James Ahrens", "title": "Multivariate Pointwise Information-Driven Data Sampling and\n  Visualization", "comments": "25 pages", "journal-ref": "Entropy, Volume 21, Issue 7, Year 2019", "doi": "10.3390/e21070699", "report-no": null, "categories": "cs.HC cs.GR cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With increasing computing capabilities of modern supercomputers, the size of\nthe data generated from the scientific simulations is growing rapidly. As a\nresult, application scientists need effective data summarization techniques\nthat can reduce large-scale multivariate spatiotemporal data sets while\npreserving the important data properties so that the reduced data can answer\ndomain-specific queries involving multiple variables with sufficient accuracy.\nWhile analyzing complex scientific events, domain experts often analyze and\nvisualize two or more variables together to obtain a better understanding of\nthe characteristics of the data features. Therefore, data summarization\ntechniques are required to analyze multi-variable relationships in detail and\nthen perform data reduction such that the important features involving multiple\nvariables are preserved in the reduced data. To achieve this, in this work, we\npropose a data sub-sampling algorithm for performing statistical data\nsummarization that leverages pointwise information theoretic measures to\nquantify the statistical association of data points considering multiple\nvariables and generates a sub-sampled data that preserves the statistical\nassociation among multi-variables. Using such reduced sampled data, we show\nthat multivariate feature query and analysis can be done effectively. The\nefficacy of the proposed multivariate association driven sampling algorithm is\npresented by applying it on several scientific data sets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 19:32:53 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Dutta", "Soumya", ""], ["Biswas", "Ayan", ""], ["Ahrens", "James", ""]]}, {"id": "1907.11989", "submitter": "Delaram Amiri", "authors": "Delaram Amiri, Arman Anzanpour, Iman Azimi, Amir M. Rahmani, Pasi\n  Liljeberg, Nikil Dutt, Marco Levorato", "title": "Optimizing Energy Efficiency of Wearable Sensors Using Fog-assisted\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the Internet of Things (IoT) technologies have enabled the\nuse of wearables for remote patient monitoring. Wearable sensors capture the\npatient's vital signs, and provide alerts or diagnosis based on the collected\ndata. Unfortunately, wearables typically have limited energy and computational\ncapacity, making their use challenging for healthcare applications where\nmonitoring must continue uninterrupted long time, without the need to charge or\nchange the battery. Fog computing can alleviate this problem by offloading\ncomputationally intensive tasks from the sensor layer to higher layers, thereby\nnot only meeting the sensors' limited computational capacity but also enabling\nthe use of local closed-loop energy optimization algorithms to increase the\nbattery life.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 23:18:48 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Amiri", "Delaram", ""], ["Anzanpour", "Arman", ""], ["Azimi", "Iman", ""], ["Rahmani", "Amir M.", ""], ["Liljeberg", "Pasi", ""], ["Dutt", "Nikil", ""], ["Levorato", "Marco", ""]]}, {"id": "1907.12001", "submitter": "Rouzbeh Shirvani", "authors": "Gloria Washington, Rouzbeh Shirvani", "title": "Towards Understanding and Modeling Empathy for Use in Motivational\n  Design Thinking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design Thinking workshops are used by companies to help generate new ideas\nfor technologies and products by engaging subjects in exercises to understand\ntheir users' wants and become more empathetic towards their needs. The \"aha\nmoment\" experienced during these thought-provoking, step outside the yourself\nactivities occurs when a group of persons iterate over several problems and\nconverge upon a solution that will fit seamlessly everyday life. With the\nincreasing use and cost of Design workshops being offered, it is important that\ntechnology be developed that can help identify empathy and its onset in humans.\nThis position paper presents an approach to modeling empathy using Gaussian\nmixture models and heart rate and skin conductance. This paper also presents an\nupdated approach to Design Thinking that helps to ensure participants are\nthinking outside of their own race's, culture's, or other affiliations'\nmotives.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 02:04:56 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Washington", "Gloria", ""], ["Shirvani", "Rouzbeh", ""]]}, {"id": "1907.12079", "submitter": "Hannah Kim", "authors": "Hannah Kim, Dongjin Choi, Barry Drake, Alex Endert, Haesun Park", "title": "TopicSifter: Interactive Search Space Reduction Through Targeted Topic\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic modeling is commonly used to analyze and understand large document\ncollections. However, in practice, users want to focus on specific aspects or\n\"targets\" rather than the entire corpus. For example, given a large collection\nof documents, users may want only a smaller subset which more closely aligns\nwith their interests, tasks, and domains. In particular, our paper focuses on\nlarge-scale document retrieval with high recall where any missed relevant\ndocuments can be critical. A simple keyword matching search is generally not\neffective nor efficient as 1) it is difficult to find a list of keyword queries\nthat can cover the documents of interest before exploring the dataset, 2) some\ndocuments may not contain the exact keywords of interest but may still be\nhighly relevant, and 3) some words have multiple meanings, which would result\nin irrelevant documents included in the retrieved subset. In this paper, we\npresent TopicSifter, a visual analytics system for interactive search space\nreduction. Our system utilizes targeted topic modeling based on nonnegative\nmatrix factorization and allows users to give relevance feedback in order to\nrefine their target and guide the topic modeling to the most relevant results.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 13:27:18 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Kim", "Hannah", ""], ["Choi", "Dongjin", ""], ["Drake", "Barry", ""], ["Endert", "Alex", ""], ["Park", "Haesun", ""]]}, {"id": "1907.12188", "submitter": "Nizamuddin Maitlo", "authors": "Nizamuddin Maitlo, Yanbo Wang, Chao Ping Chen, Lantian Mi, Wenbo Zhang", "title": "Hand-Gesture-Recognition Based Text Input Method for AR/VR Wearable\n  Devices", "comments": "Information is not correct need to rewrite", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Static and dynamic hand movements are basic way for human-machine\ninteractions. To recognize and classify these movements, first these movements\nare captured by the cameras mounted on the augmented reality (AR) or virtual\nreality (VR) wearable devices. The hand is segmented using segmentation method\nand its gestures are passed to hand gesture recognition algorithm, which\ndepends on depth-wise separable convolutional neural network for training,\ntesting and finally running smoothly on mobile AR/VR devices, while maintaining\nthe accuracy and balancing the load. A number of gestures are processed for\nidentification of right gesture and to classify the gesture and ignore the all\nintermittent gestures. With proposed method, a user can write letters and\nnumbers in air by just moving his/her hand in air. Gesture based operations are\nperformed, and trajectory of hand is recorded as handwritten text. Finally,\nthat handwritten text is processed for the text recognition.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 02:53:21 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 11:01:54 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Maitlo", "Nizamuddin", ""], ["Wang", "Yanbo", ""], ["Chen", "Chao Ping", ""], ["Mi", "Lantian", ""], ["Zhang", "Wenbo", ""]]}, {"id": "1907.12201", "submitter": "Dong Sun", "authors": "Dong Sun, Renfei Huang, Yuanzhe Chen, Yong Wang, Jia Zeng, Mingxuan\n  Yuan, Ting-Chuen Pong, and Huamin Qu", "title": "PlanningVis: A Visual Analytics Approach to Production Planning in Smart\n  Factories", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934275", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Production planning in the manufacturing industry is crucial for fully\nutilizing factory resources (e.g., machines, raw materials and workers) and\nreducing costs. With the advent of industry 4.0, plenty of data recording the\nstatus of factory resources have been collected and further involved in\nproduction planning, which brings an unprecedented opportunity to understand,\nevaluate and adjust complex production plans through a data-driven approach.\nHowever, developing a systematic analytics approach for production planning is\nchallenging due to the large volume of production data, the complex dependency\nbetween products, and unexpected changes in the market and the plant. Previous\nstudies only provide summarized results and fail to show details for\ncomparative analysis of production plans. Besides, the rapid adjustment to the\nplan in the case of an unanticipated incident is also not supported. In this\npaper, we propose PlanningVis, a visual analytics system to support the\nexploration and comparison of production plans with three levels of details: a\nplan overview presenting the overall difference between plans, a product view\nvisualizing various properties of individual products, and a production detail\nview displaying the product dependency and the daily production details in\nrelated factories. By integrating an automatic planning algorithm with\ninteractive visual explorations, PlanningVis can facilitate the efficient\noptimization of daily production planning as well as support a quick response\nto unanticipated incidents in manufacturing. Two case studies with real-world\ndata and carefully designed interviews with domain experts demonstrate the\neffectiveness and usability of PlanningVis.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 03:53:07 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 07:50:09 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 05:23:51 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Sun", "Dong", ""], ["Huang", "Renfei", ""], ["Chen", "Yuanzhe", ""], ["Wang", "Yong", ""], ["Zeng", "Jia", ""], ["Yuan", "Mingxuan", ""], ["Pong", "Ting-Chuen", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.12275", "submitter": "Wouter Klijn", "authors": "Wouter Klijn, Sandra Diaz-Pier, Abigail Morrison, Alexander Peyser", "title": "Staged deployment of interactive multi-application HPC workflows", "comments": "7 pages, 3 figures, The 2019 International Conference on High\n  Performance Computing & Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Running scientific workflows on a supercomputer can be a daunting task for a\nscientific domain specialist. Workflow management solutions (WMS) are a\nstandard method for reducing the complexity of application deployment on high\nperformance computing (HPC) infrastructure. We introduce the design for a\nmiddleware system that extends and combines the functionality from existing\nsolutions in order to create a high-level, staged user-centric\noperation/deployment model. This design addresses the requirements of several\nuse cases in the life sciences, with a focus on neuroscience. In this\nmanuscript we focus on two use cases: 1) three coupled neuronal simulators (for\nthree different space/time scales) with in-transit visualization and 2) a\nclosed-loop workflow optimized by machine learning, coupling a robot with a\nneural network simulation. We provide a detailed overview of the\napplication-integrated monitoring in relationship with the HPC job. We present\nhere a novel usage model for large scale interactive multi-application\nworkflows running on HPC systems which aims at reducing the complexity of\ndeployment and execution, thus enabling new science.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 08:41:16 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Klijn", "Wouter", ""], ["Diaz-Pier", "Sandra", ""], ["Morrison", "Abigail", ""], ["Peyser", "Alexander", ""]]}, {"id": "1907.12352", "submitter": "Tobias Isenberg", "authors": "Sarkis Halladjian, Haichao Miao, David Kou\\v{r}il, M. Eduard\n  Gr\\\"oller, Ivan Viola, Tobias Isenberg", "title": "ScaleTrotter: Illustrative Visual Travels Across Negative Scales", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934334", "report-no": null, "categories": "cs.GR cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ScaleTrotter, a conceptual framework for an interactive,\nmulti-scale visualization of biological mesoscale data and, specifically,\ngenome data. ScaleTrotter allows viewers to smoothly transition from the\nnucleus of a cell to the atomistic composition of the DNA, while bridging\nseveral orders of magnitude in scale. The challenges in creating an interactive\nvisualization of genome data are fundamentally different in several ways from\nthose in other domains like astronomy that require a multi-scale representation\nas well. First, genome data has intertwined scale levels---the DNA is an\nextremely long, connected molecule that manifests itself at all scale levels.\nSecond, elements of the DNA do not disappear as one zooms out---instead the\nscale levels at which they are observed group these elements differently.\nThird, we have detailed information and thus geometry for the entire dataset\nand for all scale levels, posing a challenge for interactive visual\nexploration. Finally, the conceptual scale levels for genome data are close in\nscale space, requiring us to find ways to visually embed a smaller scale into a\ncoarser one. We address these challenges by creating a new multi-scale\nvisualization concept. We use a scale-dependent camera model that controls the\nvisual embedding of the scales into their respective parents, the rendering of\na subset of the scale hierarchy, and the location, size, and scope of the view.\nIn traversing the scales, ScaleTrotter is roaming between 2D and 3D visual\nrepresentations that are depicted in integrated visuals. We discuss,\nspecifically, how this form of multi-scale visualization follows from the\nspecific characteristics of the genome data and describe its implementation.\nFinally, we discuss the implications of our work to the general illustrative\ndepiction of multi-scale data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 12:01:54 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Halladjian", "Sarkis", ""], ["Miao", "Haichao", ""], ["Kou\u0159il", "David", ""], ["Gr\u00f6ller", "M. Eduard", ""], ["Viola", "Ivan", ""], ["Isenberg", "Tobias", ""]]}, {"id": "1907.12364", "submitter": "Martin Striegel", "authors": "Martin Striegel, Carsten Rolfes, Johann Heyszl, Fabian Helfert,\n  Maximilian Hornung, Georg Sigl", "title": "EyeSec: A Retrofittable Augmented Reality Tool for Troubleshooting\n  Wireless Sensor Networks in the Field", "comments": "Published at the International Conference on Embedded Wireless\n  Systems and Networks (EWSN) 2019", "journal-ref": "Proceedings of the 2019 International Conference on Embedded\n  Wireless Systems and Networks, EWSN 2019, Beijing, China, February 25-27,\n  2019", "doi": null, "report-no": null, "categories": "cs.NI cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wireless Sensor Networks (WSNs) often lack interfaces for remote debugging.\nThus, fault diagnosis and troubleshooting are conducted at the deployment site.\nCurrently, WSN operators lack dedicated tools that aid them in this process.\nTherefore, we introduce EyeSec, a tool for WSN monitoring and maintenance in\nthe field. An Augmented Reality Device (AR Device) identifies sensor nodes\nusing optical markers. Portable Sniffer Units capture network traffic and\nextract information. With those data, the AR Device network topology and data\nflows between sensor nodes are visualized. Unlike previous tools, EyeSec is\nfully portable, independent of any given infrastructure and does not require\ndedicated and expensive AR hardware. Using passive inspection only, it can be\nretrofitted to already deployed WSNs. We implemented a proof of concept on\nlow-cost embedded hardware and commodity smart phones and demonstrate the usage\nof EyeSec within a WSN test bed using the 6LoWPAN transmission protocol.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 14:11:49 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Striegel", "Martin", ""], ["Rolfes", "Carsten", ""], ["Heyszl", "Johann", ""], ["Helfert", "Fabian", ""], ["Hornung", "Maximilian", ""], ["Sigl", "Georg", ""]]}, {"id": "1907.12413", "submitter": "Fabian Sperrle", "authors": "Fabian Sperrle, Rita Sevastjanova, Rebecca Kehlbeck and Mennatallah\n  El-Assady", "title": "VIANA: Visual Interactive Annotation of Argumentation", "comments": "Proceedings of IEEE Conference on Visual Analytics Science and\n  Technology (VAST), 2019", "journal-ref": "2019 IEEE Conference on Visual Analytics Science and Technology\n  (VAST)", "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argumentation Mining addresses the challenging tasks of identifying\nboundaries of argumentative text fragments and extracting their relationships.\nFully automated solutions do not reach satisfactory accuracy due to their\ninsufficient incorporation of semantics and domain knowledge. Therefore,\nexperts currently rely on time-consuming manual annotations. In this paper, we\npresent a visual analytics system that augments the manual annotation process\nby automatically suggesting which text fragments to annotate next. The accuracy\nof those suggestions is improved over time by incorporating linguistic\nknowledge and language modeling to learn a measure of argument similarity from\nuser interactions. Based on a long-term collaboration with domain experts, we\nidentify and model five high-level analysis tasks. We enable close reading and\nnote-taking, annotation of arguments, argument reconstruction, extraction of\nargument relations, and exploration of argument graphs. To avoid context\nswitches, we transition between all views through seamless morphing, visually\nanchoring all text- and graph-based layers. We evaluate our system with a\ntwo-stage expert user study based on a corpus of presidential debates. The\nresults show that experts prefer our system over existing solutions due to the\nspeedup provided by the automatic suggestions and the tight integration between\ntext and graph views.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 13:26:03 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Sperrle", "Fabian", ""], ["Sevastjanova", "Rita", ""], ["Kehlbeck", "Rebecca", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "1907.12549", "submitter": "Wei Yan Ph.D.", "authors": "Wei Yan", "title": "Augmented Reality Applied to LEGO Construction: AR-based Building\n  Instructions with High Accuracy & Precision and Realistic Object-Hand\n  Occlusions", "comments": "Accompanying project video:\n  https://www.youtube.com/watch?v=7JDW_lDv7FU and LinkedIn article:\n  https://www.linkedin.com/pulse/augmented-reality-lego-construction-iphone-wei-yan/\n  In version 4, algorithms are explained in more detail, more evaluations are\n  added, and references are updated, while the results of the performance of AR\n  instruction are unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BRICKxAR is a novel Augmented Reality (AR) instruction method for\nconstruction toys such as LEGO. With BRICKxAR, physical LEGO construction is\nguided by virtual bricks. Compared with the state-of-the-art, accuracy of the\nvirtual - physical model alignment is significantly improved through a new\ndesign of marker-based registration, which can achieve an average error less\nthan 1mm throughout the model. Realistic object occlusion is accomplished to\nreveal the true spatial relationship between physical and virtual bricks. LEGO\nplayers' hand detection and occlusion are realized to visualize the correct\nspatial relationship between real hands and virtual bricks, and allow virtual\nbricks to be \"grasped\" by real hands. The integration of these features makes\nAR instructions possible for small-parts assembly, validated through a working\nAR prototype for constructing LEGO Arc de Triomphe, quantitative measures of\nthe accuracies of registration and occlusions, and heuristic evaluation of AR\ninstruction features.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 17:44:14 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 17:22:15 GMT"}, {"version": "v3", "created": "Sun, 11 Aug 2019 21:19:53 GMT"}, {"version": "v4", "created": "Sun, 20 Dec 2020 00:29:10 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Yan", "Wei", ""]]}, {"id": "1907.12663", "submitter": "Aditeya Pandey", "authors": "Aditeya Pandey, Harsh Shukla, Geoffrey S. Young, Lei Qin, Amir A.\n  Zamani, Liangge Hsu, Raymond Huang, Cody Dunne, and Michelle A. Borkin", "title": "CerebroVis: Designing an Abstract yet Spatially Contextualized Cerebral\n  Arteries Network Visualization", "comments": "IEEE InfoVis 2019 ACM 2012 CCS-Human-centered\n  computing,Visualization,Visualization application domains,Information\n  visualization ACM 2012 CCS-Human-centered\n  computing,Visualization,Visualization techniques,Graph drawings", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934402", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blood circulation in the human brain is supplied through a network of\ncerebral arteries. If a clinician suspects a patient has a stroke or other\ncerebrovascular condition they order imaging tests. Neuroradiologists visually\nsearch the resulting scans for abnormalities. Their visual search tasks\ncorrespond to the abstract network analysis tasks of browsing and path\nfollowing. To assist neuroradiologists in identifying cerebral artery\nabnormalities we designed CerebroVis, a novel abstract---yet spatially\ncontextualized---cerebral artery network visualization. In this design study,\nwe contribute a novel framing and definition of the cerebral artery system in\nterms of network theory and characterize neuroradiologist domain goals as\nabstract visualization and network analysis tasks. Through an iterative,\nuser-centered design process we developed an abstract network layout technique\nwhich incorporates cerebral artery spatial context. The abstract visualization\nenables increased domain task performance over 3D geometry representations,\nwhile including spatial context helps preserve the user's mental map of the\nunderlying geometry. We provide open source implementations of our network\nlayout technique and prototype cerebral artery visualization tool. We\ndemonstrate the robustness of our technique by successfully laying out 61 open\nsource brain scans. We evaluate the effectiveness of our layout through a mixed\nmethods study with three neuroradiologists. In a formative controlled\nexperiment our study participants used CerebroVis and a conventional 3D\nvisualization to examine real cerebral artery imaging data and to identify a\nsimulated intracranial artery stenosis. Participants were more accurate at\nidentifying stenoses using CerebroVis (absolute risk difference 13%). A free\ncopy of this paper, the evaluation stimuli and data, and source code are\navailable at https://osf.io/e5sxt/.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 21:36:36 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 20:08:09 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Pandey", "Aditeya", ""], ["Shukla", "Harsh", ""], ["Young", "Geoffrey S.", ""], ["Qin", "Lei", ""], ["Zamani", "Amir A.", ""], ["Hsu", "Liangge", ""], ["Huang", "Raymond", ""], ["Dunne", "Cody", ""], ["Borkin", "Michelle A.", ""]]}, {"id": "1907.12687", "submitter": "Mor Vered", "authors": "Mor Vered, Frank Dignum and Tim Miller", "title": "Let's Make It Personal, A Challenge in Personalizing Medical Inter-Human\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current AI approaches have frequently been used to help personalize many\naspects of medical experiences and tailor them to a specific individuals'\nneeds. However, while such systems consider medically-relevant information,\nthey ignore socially-relevant information about how this diagnosis should be\ncommunicated and discussed with the patient. The lack of this capability may\nlead to mis-communication, resulting in serious implications, such as patients\nopting out of the best treatment. Consider a case in which the same treatment\nis proposed to two different individuals. The manner in which this treatment is\nmediated to each should be different, depending on the individual patient's\nhistory, knowledge, and mental state. While it is clear that this communication\nshould be conveyed via a human medical expert and not a software-based system,\nhumans are not always capable of considering all of the relevant aspects and\ntraversing all available information. We pose the challenge of creating\nIntelligent Agents (IAs) to assist medical service providers (MSPs) and\nconsumers in establishing a more personalized human-to-human dialogue.\nPersonalizing conversations will enable patients and MSPs to reach a solution\nthat is best for their particular situation, such that a relation of trust can\nbe built and commitment to the outcome of the interaction is assured. We\npropose a four-part conceptual framework for personalized social interactions,\nexpand on which techniques are available within current AI research and discuss\nwhat has yet to be achieved.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 23:37:25 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Vered", "Mor", ""], ["Dignum", "Frank", ""], ["Miller", "Tim", ""]]}, {"id": "1907.12748", "submitter": "Minjeong Shin", "authors": "Minjeong Shin, Alexander Soen, Benjamin T. Readshaw, Stephen M.\n  Blackburn, Mitchell Whitelaw, Lexing Xie", "title": "Influence Flowers of Academic Entities", "comments": "VAST 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Influence Flower, a new visual metaphor for the influence\nprofile of academic entities, including people, projects, institutions,\nconferences, and journals. While many tools quantify influence, we aim to\nexpose the flow of influence between entities. The Influence Flower is an\nego-centric graph, with a query entity placed in the centre. The petals are\nstyled to reflect the strength of influence to and from other entities of the\nsame or different type. For example, one can break down the incoming and\noutgoing influences of a research lab by research topics. The Influence Flower\nuses a recent snapshot of Microsoft Academic Graph, consisting of 212million\nauthors, their 176 million publications, and 1.2 billion citations. An\ninteractive web app, Influence Map, is constructed around this central metaphor\nfor searching and curating visualisations. We also propose a visual comparison\nmethod that highlights change in influence patterns over time. We demonstrate\nthrough several case studies that the Influence Flower supports data-driven\ninquiries about the following: researchers' careers over time; paper(s) and\nprojects, including those with delayed recognition; the interdisciplinary\nprofile of a research institution; and the shifting topical trends in\nconferences. We also use this tool on influence data beyond academic citations,\nby contrasting the academic and Twitter activities of a researcher.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 06:07:22 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Shin", "Minjeong", ""], ["Soen", "Alexander", ""], ["Readshaw", "Benjamin T.", ""], ["Blackburn", "Stephen M.", ""], ["Whitelaw", "Mitchell", ""], ["Xie", "Lexing", ""]]}, {"id": "1907.12879", "submitter": "Nicolas Holliman Professor", "authors": "Nicolas S. Holliman, Arzu Coltekin, Sara J. Fernstad, Michael D.\n  Simpson, Kevin J. Wilson, Andrew J. Woods", "title": "Visual Entropy and the Visualization of Uncertainty", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: It is possible to find many different visual representations of\ndata values in visualizations, it is less common to see visual representations\nthat include uncertainty, especially in visualizations intended for\nnon-technical audiences. Objective: our aim is to rigorously define and\nevaluate the novel use of visual entropy as a measure of shape that allows us\nto construct an ordered scale of glyphs for use in representing both\nuncertainty and value in 2D and 3D environments. Method: We use sample entropy\nas a numerical measure of visual entropy to construct a set of glyphs using R\nand Blender which vary in their complexity. Results: A Bradley-Terry analysis\nof a pairwise comparison of the glyphs shows participants (n=19) ordered the\nglyphs as predicted by the visual entropy score (linear regression R2 >0.97,\np<0.001). We also evaluate whether the glyphs can effectively represent\nuncertainty using a signal detection method, participants (n=15) were able to\nsearch for glyphs representing uncertainty with high sensitivity and low error\nrates. Conclusion: visual entropy is a novel cue for representing ordered data\nand provides a channel that allows the uncertainty of a measure to be presented\nalongside its mean value.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 13:18:27 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Holliman", "Nicolas S.", ""], ["Coltekin", "Arzu", ""], ["Fernstad", "Sara J.", ""], ["Simpson", "Michael D.", ""], ["Wilson", "Kevin J.", ""], ["Woods", "Andrew J.", ""]]}, {"id": "1907.12899", "submitter": "Roja Eini", "authors": "Adam Morrissett, Roja Eini, Mostafa Zaman, Nasibeh Zohrabi, Sherif\n  Abdelwahed", "title": "A Physical Testbed for Intelligent Transportation Systems", "comments": "7 pages, 8 figures, 1 table, going to be published in the proceedings\n  of 12th IEEE International Conference on Human System Interaction (HSI 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent transportation systems (ITSs) and other smart-city technologies\nare increasingly advancing in capability and complexity. While simulation\nenvironments continue to improve, their fidelity and ease of use can quickly\ndegrade as newer systems become increasingly complex. To remedy this, we\npropose a hardware- and software-based traffic management system testbed as\npart of a larger smart-city testbed. It comprises a network of connected\nvehicles, a network of intersection controllers, a variety of control services,\nand data analytics services. The main goal of our testbed is to provide\nresearchers and students with the means to develop novel traffic and vehicle\ncontrol algorithms with higher fidelity than what can be achieved with\nsimulation alone. Specifically, we are using the testbed to develop an\nintegrated management system that combines model-based control and data\nanalytics to improve the system performance over time. In this paper, we give a\ndetailed description of each component within the testbed and discuss its\ncurrent developmental state. Additionally, we present initial results and\npropose future work.\n  Index Terms: Smart city, Intelligent transportation systems,\nHuman-in-the-loop, Data analytics, Data visualization, Traffic network\nmanagement and control, Machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 15:49:44 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Morrissett", "Adam", ""], ["Eini", "Roja", ""], ["Zaman", "Mostafa", ""], ["Zohrabi", "Nasibeh", ""], ["Abdelwahed", "Sherif", ""]]}, {"id": "1907.12918", "submitter": "Haipeng Zeng", "authors": "Haipeng Zeng, Xingbo Wang, Aoyu Wu, Yong Wang, Quan Li, Alex Endert\n  and Huamin Qu", "title": "EmoCo: Visual Analysis of Emotion Coherence in Presentation Videos", "comments": "11 pages, 8 figures. Accepted by IEEE VAST 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934656", "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions play a key role in human communication and public presentations.\nHuman emotions are usually expressed through multiple modalities. Therefore,\nexploring multimodal emotions and their coherence is of great value for\nunderstanding emotional expressions in presentations and improving presentation\nskills. However, manually watching and studying presentation videos is often\ntedious and time-consuming. There is a lack of tool support to help conduct an\nefficient and in-depth multi-level analysis. Thus, in this paper, we introduce\nEmoCo, an interactive visual analytics system to facilitate efficient analysis\nof emotion coherence across facial, text, and audio modalities in presentation\nvideos. Our visualization system features a channel coherence view and a\nsentence clustering view that together enable users to obtain a quick overview\nof emotion coherence and its temporal evolution. In addition, a detail view and\nword view enable detailed exploration and comparison from the sentence level\nand word level, respectively. We thoroughly evaluate the proposed system and\nvisualization techniques through two usage scenarios based on TED Talk videos\nand interviews with two domain experts. The results demonstrate the\neffectiveness of our system in gaining insights into emotion coherence in\npresentations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 10:27:42 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 16:46:29 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Zeng", "Haipeng", ""], ["Wang", "Xingbo", ""], ["Wu", "Aoyu", ""], ["Wang", "Yong", ""], ["Li", "Quan", ""], ["Endert", "Alex", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.13178", "submitter": "Seth Johnson", "authors": "Seth Johnson, Francesca Samsel, Gregory Abram, Daniel Olson, Andrew J.\n  Solis, Bridger Herman, Phillip J. Wolfram, Christophe Lenglet, Daniel F.\n  Keefe", "title": "Artifact-Based Rendering: Harnessing Natural and Traditional Visual\n  Media for More Expressive and Engaging 3D Visualizations", "comments": "Published in IEEE VIS 2019, 9 pages of content with 2 pages of\n  references, 12 figures", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934260", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Artifact-Based Rendering (ABR), a framework of tools,\nalgorithms, and processes that makes it possible to produce real, data-driven\n3D scientific visualizations with a visual language derived entirely from\ncolors, lines, textures, and forms created using traditional physical media or\nfound in nature. A theory and process for ABR is presented to address three\ncurrent needs: (i) designing better visualizations by making it possible for\nnon-programmers to rapidly design and critique many alternative data-to-visual\nmappings; (ii) expanding the visual vocabulary used in scientific\nvisualizations to depict increasingly complex multivariate data; (iii) bringing\na more engaging, natural, and human-relatable handcrafted aesthetic to data\nvisualization. New tools and algorithms to support ABR include front-end\napplets for constructing artifact-based colormaps, optimizing 3D scanned meshes\nfor use in data visualization, and synthesizing textures from artifacts. These\nare complemented by an interactive rendering engine with custom algorithms and\ninterfaces that demonstrate multiple new visual styles for depicting point,\nline, surface, and volume data. A within-the-research-team design study\nprovides early evidence of the shift in visualization design processes that ABR\nis believed to enable when compared to traditional scientific visualization\nsystems. Qualitative user feedback on applications to climate science and brain\nimaging support the utility of ABR for scientific discovery and public\ncommunication.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 18:51:27 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 15:03:49 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Johnson", "Seth", ""], ["Samsel", "Francesca", ""], ["Abram", "Gregory", ""], ["Olson", "Daniel", ""], ["Solis", "Andrew J.", ""], ["Herman", "Bridger", ""], ["Wolfram", "Phillip J.", ""], ["Lenglet", "Christophe", ""], ["Keefe", "Daniel F.", ""]]}, {"id": "1907.13187", "submitter": "Ke Xu", "authors": "Ke Xu, Yun Wang, Leni Yang, Yifang Wang, Bo Qiao, Si Qin, Yong Xu,\n  Haidong Zhang, Huamin Qu", "title": "CloudDet: Interactive Visual Analysis of Anomalous Performances in Cloud\n  Computing Systems", "comments": "11 pages, 8 figures, IEEE VAST Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and analyzing potential anomalous performances in cloud computing\nsystems is essential for avoiding losses to customers and ensuring the\nefficient operation of the systems. To this end, a variety of automated\ntechniques have been developed to identify anomalies in cloud computing\nperformance. These techniques are usually adopted to track the performance\nmetrics of the system (e.g., CPU, memory, and disk I/O), represented by a\nmultivariate time series. However, given the complex characteristics of cloud\ncomputing data, the effectiveness of these automated methods is affected. Thus,\nsubstantial human judgment on the automated analysis results is required for\nanomaly interpretation. In this paper, we present a unified visual analytics\nsystem named CloudDet to interactively detect, inspect, and diagnose anomalies\nin cloud computing systems. A novel unsupervised anomaly detection algorithm is\ndeveloped to identify anomalies based on the specific temporal patterns of the\ngiven metrics data (e.g., the periodic pattern), the results of which are\nvisualized in our system to indicate the occurrences of anomalies. Rich\nvisualization and interaction designs are used to help understand the anomalies\nin the spatial and temporal context. We demonstrate the effectiveness of\nCloudDet through a quantitative evaluation, two case studies with real-world\ndata, and interviews with domain experts.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 19:12:46 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Xu", "Ke", ""], ["Wang", "Yun", ""], ["Yang", "Leni", ""], ["Wang", "Yifang", ""], ["Qiao", "Bo", ""], ["Qin", "Si", ""], ["Xu", "Yong", ""], ["Zhang", "Haidong", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.13193", "submitter": "Joyce Ma", "authors": "Joyce Ma, Kwan-Liu Ma, and Jennifer Frazier", "title": "Decoding a Complex Visualization in a Science Museum -- An Empirical\n  Study", "comments": "IEEE VIS (InfoVis/VAST/SciVis) 2019 ACM 2012 CCS - Human-centered\n  computing, Visualization, Empirical studies in visualization", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934401", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study describes a detailed analysis of museum visitors' decoding process\nas they used a visualization designed to support exploration of a large,\ncomplex dataset. Quantitative and qualitative analyses revealed that it took,\non average, 43 seconds for visitors to decode enough of the visualization to\nsee patterns and relationships in the underlying data represented, and 54\nseconds to arrive at their first correct data interpretation. Furthermore,\nvisitors decoded throughout and not only upon initial use of the visualization.\nThe study analyzed think-aloud data to identify issues visitors had mapping the\nvisual representations to their intended referents, examine why they occurred,\nand consider if and how these decoding issues were resolved. The paper also\ndescribes how multiple visual encodings both helped and hindered decoding and\nconcludes with implications on the design and adaptation of visualizations for\ninformal science learning venues.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 19:39:49 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 03:39:25 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ma", "Joyce", ""], ["Ma", "Kwan-Liu", ""], ["Frazier", "Jennifer", ""]]}, {"id": "1907.13214", "submitter": "Manny Rayner", "authors": "Nikos Tsourakis, Manny Rayner, Hanieh Habibi, Pierre-Emmanuel Gallais,\n  Cathy Chua, Matt Butterweck", "title": "Alexa as a CALL platform for children: Where do we start?", "comments": "4 pages. Based on talk given at enetCollect WG3 & WG5 Meeting,\n  Leiden, Holland, 2018", "journal-ref": "CEUR Workshop Proceedings Vol-2390 2019", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amazon's Alexa is now widely available and shows interesting potential as a\nplatform for hosting CALL games aimed at children. In this paper, we describe\nan initial informal experiment where we created some simple CALL games and made\nthem available to a few child testers. We report the children's and parents'\nreactions. Our overall conclusion is that, although Alexa has many positive\nfeatures, there are still fundamental platform issues in the current version\nthat make it very difficult to build compelling CALL games for children. The\ngames used will soon be freely available for download on the Alexa store.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 20:38:52 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Tsourakis", "Nikos", ""], ["Rayner", "Manny", ""], ["Habibi", "Hanieh", ""], ["Gallais", "Pierre-Emmanuel", ""], ["Chua", "Cathy", ""], ["Butterweck", "Matt", ""]]}, {"id": "1907.13274", "submitter": "Uehwan Kim", "authors": "Ue-Hwan Kim and Jong-Hwan Kim", "title": "A Stabilized Feedback Episodic Memory (SF-EM) and Home Service Provision\n  Framework for Robot and IoT Collaboration", "comments": "Accepted (Early Access)", "journal-ref": null, "doi": "10.1109/TCYB.2018.2882921", "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automated home referred to as Smart Home is expected to offer fully\ncustomized services to its residents, reducing the amount of home labor, thus\nimproving human beings' welfare. Service robots and Internet of Things (IoT)\nplay the key roles in the development of Smart Home. The service provision with\nthese two main components in a Smart Home environment requires: 1) learning and\nreasoning algorithms and 2) the integration of robot and IoT systems.\nConventional computational intelligence-based learning and reasoning algorithms\ndo not successfully manage dynamic changes in the Smart Home data, and the\nsimple integrations fail to fully draw the synergies from the collaboration of\nthe two systems. To tackle these limitations, we propose: 1) a stabilized\nmemory network with a feedback mechanism which can learn user behaviors in an\nincremental manner and 2) a robot-IoT service provision framework for a Smart\nHome which utilizes the proposed memory architecture as a learning and\nreasoning module and exploits synergies between the robot and IoT systems. We\nconduct a set of comprehensive experiments under various conditions to verify\nthe performance of the proposed memory architecture and the service provision\nframework and analyze the experiment results.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 01:27:34 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kim", "Ue-Hwan", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "1907.13285", "submitter": "Uehwan Kim", "authors": "Ue-Hwan Kim, Sahng-Min Yoo and Jong-Hwan Kim", "title": "I-Keyboard: Fully Imaginary Keyboard on Touch Devices Empowered by Deep\n  Neural Decoder", "comments": "Submitted to IEEE TRANSACTIONS ON CYBERNETICS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text-entry aims to provide an effective and efficient pathway for humans to\ndeliver their messages to computers. With the advent of mobile computing, the\nrecent focus of text-entry research has moved from physical keyboards to soft\nkeyboards. Current soft keyboards, however, increase the typo rate due to lack\nof tactile feedback and degrade the usability of mobile devices due to their\nlarge portion on screens. To tackle these limitations, we propose a fully\nimaginary keyboard (I-Keyboard) with a deep neural decoder (DND). The\ninvisibility of I-Keyboard maximizes the usability of mobile devices and DND\nempowered by a deep neural architecture allows users to start typing from any\nposition on the touch screens at any angle. To the best of our knowledge, the\neyes-free ten-finger typing scenario of I-Keyboard which does not necessitate\nboth a calibration step and a predefined region for typing is first explored in\nthis work. For the purpose of training DND, we collected the largest user data\nin the process of developing I-Keyboard. We verified the performance of the\nproposed I-Keyboard and DND by conducting a series of comprehensive simulations\nand experiments under various conditions. I-Keyboard showed 18.95% and 4.06%\nincreases in typing speed (45.57 WPM) and accuracy (95.84%), respectively over\nthe baseline.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 02:22:49 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kim", "Ue-Hwan", ""], ["Yoo", "Sahng-Min", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "1907.13295", "submitter": "Sahisnu Mazumder", "authors": "Sahisnu Mazumder, Bing Liu, Shuai Wang, Nianzu Ma", "title": "Lifelong and Interactive Learning of Factual Knowledge in Dialogues", "comments": "Published in SIGDIAL 2019", "journal-ref": null, "doi": "10.18653/v1/W19-5903", "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialogue systems are increasingly using knowledge bases (KBs) storing\nreal-world facts to help generate quality responses. However, as the KBs are\ninherently incomplete and remain fixed during conversation, it limits dialogue\nsystems' ability to answer questions and to handle questions involving entities\nor relations that are not in the KB. In this paper, we make an attempt to\npropose an engine for Continuous and Interactive Learning of Knowledge (CILK)\nfor dialogue systems to give them the ability to continuously and interactively\nlearn and infer new knowledge during conversations. With more knowledge\naccumulated over time, they will be able to learn better and answer more\nquestions. Our empirical evaluation shows that CILK is promising.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 03:11:33 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 02:25:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Mazumder", "Sahisnu", ""], ["Liu", "Bing", ""], ["Wang", "Shuai", ""], ["Ma", "Nianzu", ""]]}, {"id": "1907.13314", "submitter": "Mosab Khayat", "authors": "Mosab Khayat, Morteza Karimzadeh, David S. Ebert, Arif Ghafoor", "title": "The Validity, Generalizability and Feasibility of Summative Evaluation\n  Methods in Visual Analytics", "comments": "IEEE VIS (VAST) 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934264", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many evaluation methods have been used to assess the usefulness of Visual\nAnalytics (VA) solutions. These methods stem from a variety of origins with\ndifferent assumptions and goals, which cause confusion about their proofing\ncapabilities. Moreover, the lack of discussion about the evaluation processes\nmay limit our potential to develop new evaluation methods specialized for VA.\nIn this paper, we present an analysis of evaluation methods that have been used\nto summatively evaluate VA solutions. We provide a survey and taxonomy of the\nevaluation methods that have appeared in the VAST literature in the past two\nyears. We then analyze these methods in terms of validity and generalizability\nof their findings, as well as the feasibility of using them. We propose a new\nmetric called summative quality to compare evaluation methods according to\ntheir ability to prove usefulness, and make recommendations for selecting\nevaluation methods based on their summative quality in the VA domain.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 05:47:20 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 23:52:59 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Khayat", "Mosab", ""], ["Karimzadeh", "Morteza", ""], ["Ebert", "David S.", ""], ["Ghafoor", "Arif", ""]]}, {"id": "1907.13319", "submitter": "Mosab Khayat", "authors": "Mosab Khayat, Morteza Karimzadeh, Jieqiong Zhao, David S. Ebert", "title": "VASSL: A Visual Analytics Toolkit for Social Spambot Labeling", "comments": "IEEE VIS (VAST) 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934266", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms such as Twitter are filled with social spambots.\nDetecting these malicious accounts is essential, yet challenging, as they\ncontinually evolve and evade traditional detection techniques. In this work, we\npropose VASSL, a visual analytics system that assists in the process of\ndetecting and labeling spambots. Our tool enhances the performance and\nscalability of manual labeling by providing multiple connected views and\nutilizing dimensionality reduction, sentiment analysis and topic modeling\ntechniques, which offer new insights that enable the identification of\nspambots. The system allows users to select and analyze groups of accounts in\nan interactive manner, which enables the detection of spambots that may not be\nidentified when examined individually. We conducted a user study to objectively\nevaluate the performance of VASSL users, as well as capturing subjective\nopinions about the usefulness and the ease of use of the tool.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 06:05:55 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 23:53:55 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Khayat", "Mosab", ""], ["Karimzadeh", "Morteza", ""], ["Zhao", "Jieqiong", ""], ["Ebert", "David S.", ""]]}, {"id": "1907.13320", "submitter": "Yalong Yang", "authors": "Yalong Yang and Sarah Goodwin", "title": "What-Why Analysis of Expert Interviews: Analysing\n  Geographically-Embedded Flow Data", "comments": "Presented at IEEE Pacific Visualization Symposium (PacificVis 2019)", "journal-ref": null, "doi": "10.1109/PacificVis.2019.00022", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present our analysis of five expert interviews, each from a\ndifferent application domain. Such analysis is crucial to understanding the\nreal-world scenarios of analysing geographically-embedded flow data. The\nresults of our analysis show that similar high-level tasks were conducted in\ndifferent domains. To better describe the targets of these tasks, we proposed\nthree flow-targets for analysing geographically-embedded flow data: single\nflow, total flow and regional flow.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 06:16:28 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Yang", "Yalong", ""], ["Goodwin", "Sarah", ""]]}, {"id": "1907.13538", "submitter": "Zhutian Chen", "authors": "Zhutian Chen and Wei Zeng and Zhiguang Yang and Lingyun Yu and\n  Chi-Wing Fu and Huamin Qu", "title": "LassoNet: Deep Lasso-Selection of 3D Point Clouds", "comments": "10 pages", "journal-ref": "TVCG2019", "doi": "10.1109/TVCG.2019.2934332", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection is a fundamental task in exploratory analysis and visualization of\n3D point clouds. Prior researches on selection methods were developed mainly\nbased on heuristics such as local point density, thus limiting their\napplicability in general data. Specific challenges root in the great\nvariabilities implied by point clouds (e.g., dense vs. sparse), viewpoint\n(e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this\nwork, we introduce LassoNet, a new deep neural network for lasso selection of\n3D point clouds, attempting to learn a latent mapping from viewpoint and lasso\nto point cloud regions. To achieve this, we couple user-target points with\nviewpoint and lasso information through 3D coordinate transform and naive\nselection, and improve the method scalability via an intention filtering and\nfarthest point sampling. A hierarchical network is trained using a dataset with\nover 30K lasso-selection records on two different point cloud data. We conduct\na formal user study to compare LassoNet with two state-of-the-art\nlasso-selection methods. The evaluations confirm that our approach improves the\nselection effectiveness and efficiency across different combinations of 3D\npoint clouds, viewpoints, and lasso selections. Project Website:\nhttps://lassonet.github.io\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:51:53 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 11:45:48 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chen", "Zhutian", ""], ["Zeng", "Wei", ""], ["Yang", "Zhiguang", ""], ["Yu", "Lingyun", ""], ["Fu", "Chi-Wing", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.13550", "submitter": "Zhutian Chen", "authors": "Zhutian Chen, Yun Wang, Qianwen Wang, Yong Wang, and Huamin Qu", "title": "Towards Automated Infographic Design: Deep Learning-based\n  Auto-Extraction of Extensible Timeline", "comments": "10 pages, Automated Infographic Design, Deep Learning-based Approach,\n  Timeline Infographics, Multi-task Model", "journal-ref": "TVCG2019", "doi": "10.1109/TVCG.2019.2934810", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designers need to consider not only perceptual effectiveness but also visual\nstyles when creating an infographic. This process can be difficult and time\nconsuming for professional designers, not to mention non-expert users, leading\nto the demand for automated infographics design. As a first step, we focus on\ntimeline infographics, which have been widely used for centuries. We contribute\nan end-to-end approach that automatically extracts an extensible timeline\ntemplate from a bitmap image. Our approach adopts a deconstruction and\nreconstruction paradigm. At the deconstruction stage, we propose a multi-task\ndeep neural network that simultaneously parses two kinds of information from a\nbitmap timeline: 1) the global information, i.e., the representation, scale,\nlayout, and orientation of the timeline, and 2) the local information, i.e.,\nthe location, category, and pixels of each visual element on the timeline. At\nthe reconstruction stage, we propose a pipeline with three techniques, i.e.,\nNon-Maximum Merging, Redundancy Recover, and DL GrabCut, to extract an\nextensible template from the infographic, by utilizing the deconstruction\nresults. To evaluate the effectiveness of our approach, we synthesize a\ntimeline dataset (4296 images) and collect a real-world timeline dataset (393\nimages) from the Internet. We first report quantitative evaluation results of\nour approach over the two datasets. Then, we present examples of automatically\nextracted templates and timelines automatically generated based on these\ntemplates to qualitatively demonstrate the performance. The results confirm\nthat our approach can effectively extract extensible templates from real-world\ntimeline infographics.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:20:33 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 12:12:49 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chen", "Zhutian", ""], ["Wang", "Yun", ""], ["Wang", "Qianwen", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "1907.13568", "submitter": "Arvind Satyanarayan", "authors": "Arvind Satyanarayan, Bongshin Lee, Donghao Ren, Jeffrey Heer, John\n  Stasko, John Thompson, Matthew Brehmer, and Zhicheng Liu", "title": "Critical Reflections on Visualization Authoring Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emerging generation of visualization authoring systems support expressive\ninformation visualization without textual programming. As they vary in their\nvisualization models, system architectures, and user interfaces, it is\nchallenging to directly compare these systems using traditional evaluative\nmethods. Recognizing the value of contextualizing our decisions in the broader\ndesign space, we present critical reflections on three systems we developed --\nLyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that\nwould have been daunting within the constituent papers of these three systems.\nWe compare and contrast their (previously unmentioned) limitations and\ntrade-offs between expressivity and learnability. We also reflect on common\nassumptions that we made during the development of our systems, thereby\ninforming future research directions in visualization authoring systems.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:54:53 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Satyanarayan", "Arvind", ""], ["Lee", "Bongshin", ""], ["Ren", "Donghao", ""], ["Heer", "Jeffrey", ""], ["Stasko", "John", ""], ["Thompson", "John", ""], ["Brehmer", "Matthew", ""], ["Liu", "Zhicheng", ""]]}, {"id": "1907.13601", "submitter": "Jieqiong Zhao", "authors": "Jieqiong Zhao, Morteza Karimzadeh, Luke S. Snyder, Chittayong\n  Surakitbanharn, Zhenyu Cheryl Qian, David S. Ebert", "title": "MetricsVis: A Visual Analytics System for Evaluating Employee\n  Performance in Public Safety Agencies", "comments": "To appear in 2019 IEEE Transactions on Visualization and Computer\n  Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934603", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating employee performance in organizations with varying workloads and\ntasks is challenging. Specifically, it is important to understand how\nquantitative measurements of employee achievements relate to supervisor\nexpectations, what the main drivers of good performance are, and how to combine\nthese complex and flexible performance evaluation metrics into an accurate\nportrayal of organizational performance in order to identify shortcomings and\nimprove overall productivity. To facilitate this process, we summarize common\norganizational performance analyses into four visual exploration task\ncategories. Additionally, we develop MetricsVis, a visual analytics system\ncomposed of multiple coordinated views to support the dynamic evaluation and\ncomparison of individual, team, and organizational performance in public safety\norganizations. MetricsVis provides four primary visual components to expedite\nperformance evaluation: (1) a priority adjustment view to support direct\nmanipulation on evaluation metrics; (2) a reorderable performance matrix to\ndemonstrate the details of individual employees; (3) a group performance view\nthat highlights aggregate performance and individual contributions for each\ngroup; and (4) a projection view illustrating employees with similar\nspecialties to facilitate shift assignments and training. We demonstrate the\nusability of our framework with two case studies from medium-sized law\nenforcement agencies and highlight its broader applicability to other domains.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:06:47 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 00:34:32 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 18:53:15 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Zhao", "Jieqiong", ""], ["Karimzadeh", "Morteza", ""], ["Snyder", "Luke S.", ""], ["Surakitbanharn", "Chittayong", ""], ["Qian", "Zhenyu Cheryl", ""], ["Ebert", "David S.", ""]]}]