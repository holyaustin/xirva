[{"id": "2106.00077", "submitter": "Nicolas Holliman Professor", "authors": "Nicolas Steven Holliman", "title": "Automating Visualization Quality Assessment: a Case Study in Higher\n  Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case study in the use of machine+human mixed intelligence for\nvisualization quality assessment, applying automated visualization quality\nmetrics to support the human assessment of data visualizations produced as\ncoursework by students taking higher education courses. A set of image\ninformatics algorithms including edge congestion, visual saliency and colour\nanalysis generate machine analysis of student visualizations. The insight from\nthe image informatics outputs has proved helpful for the marker in assessing\nthe work and is also provided to the students as part of a written report on\ntheir work. Student and external reviewer comments suggest that the addition of\nthe image informatics outputs to the standard feedback document was a positive\nstep. We review the ethical challenges of working with assessment data and of\nautomating assessment processes.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 19:52:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Holliman", "Nicolas Steven", ""]]}, {"id": "2106.00152", "submitter": "Bei Wang", "authors": "Fangfei Lan, Michael Young, Lauren Anderson, Anders Ynnerman,\n  Alexander Bock, Michelle A. Borkin, Angus G. Forbes, Juna A. Kollmeier, Bei\n  Wang", "title": "Visualization in Astrophysics: Developing New Methods, Discovering Our\n  Universe, and Educating the Earth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a state-of-the-art report on visualization in astrophysics. We\nsurvey representative papers from both astrophysics and visualization and\nprovide a taxonomy of existing approaches based on data analysis tasks. The\napproaches are classified based on five categories: data wrangling, data\nexploration, feature identification, object reconstruction, as well as\neducation and outreach. Our unique contribution is to combine the diverse\nviewpoints from both astronomers and visualization experts to identify\nchallenges and opportunities for visualization in astrophysics. The main goal\nis to provide a reference point to bring modern data analysis and visualization\ntechniques to the rich datasets in astrophysics.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 00:09:06 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lan", "Fangfei", ""], ["Young", "Michael", ""], ["Anderson", "Lauren", ""], ["Ynnerman", "Anders", ""], ["Bock", "Alexander", ""], ["Borkin", "Michelle A.", ""], ["Forbes", "Angus G.", ""], ["Kollmeier", "Juna A.", ""], ["Wang", "Bei", ""]]}, {"id": "2106.00157", "submitter": "Bei Wang", "authors": "Lin Yan, Talha Bin Masood, Raghavendra Sridharamurthy, Farhan Rasheed,\n  Vijay Natarajan, Ingrid Hotz, Bei Wang", "title": "Scalar Field Comparison with Topological Descriptors: Properties and\n  Applications for Scientific Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In topological data analysis and visualization, topological descriptors such\nas persistence diagrams, merge trees, contour trees, Reeb graphs, and\nMorse-Smale complexes play an essential role in capturing the shape of scalar\nfield data. We present a state-of-the-art report on scalar field comparison\nusing topological descriptors. We provide a taxonomy of existing approaches\nbased on visualization tasks associated with three categories of data: single\nfields, time-varying fields, and ensembles. These tasks include symmetry\ndetection, periodicity detection, key event/feature detection, feature\ntracking, clustering, and structure statistics. Our main contributions include\nthe formulation of a set of desirable mathematical and computational properties\nof comparative measures, and the classification of visualization tasks and\napplications that are enabled by these measures.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 00:34:18 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Yan", "Lin", ""], ["Masood", "Talha Bin", ""], ["Sridharamurthy", "Raghavendra", ""], ["Rasheed", "Farhan", ""], ["Natarajan", "Vijay", ""], ["Hotz", "Ingrid", ""], ["Wang", "Bei", ""]]}, {"id": "2106.00163", "submitter": "Filipo Sharevski", "authors": "Emma Pieroni, Peter Jachim, Nathaniel Jachim, Filipo Sharevski", "title": "Parlermonium: A Data-Driven UX Design Evaluation of the Parler Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper evaluates Parler, the controversial social media platform, from\ntwo seemingly orthogonal perspectives: UX design perspective and data science.\nUX design researchers explore how users react to the interface/content of their\nsocial media feeds; Data science researchers analyze the misinformation flow in\nthese feeds to detect alternative narratives and state-sponsored disinformation\ncampaigns. We took a critical look into the intersection of these approaches to\nunderstand how Parler's interface itself is conductive to the flow of\nmisinformation and the perception of \"free speech\" among its audience. Parler\ndrew widespread attention leading up to and after the 2020 U.S. elections as\nthe \"alternative\" place for free speech, as a reaction to other mainstream\nsocial media platform which actively engaged in labeling misinformation with\ncontent warnings. Because platforms like Parler are disruptive to the social\nmedia landscape, we believe the evaluation uniquely uncovers the platform's\nconductivity to the spread of misinformation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 01:12:44 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Pieroni", "Emma", ""], ["Jachim", "Peter", ""], ["Jachim", "Nathaniel", ""], ["Sharevski", "Filipo", ""]]}, {"id": "2106.00326", "submitter": "Kimon Kieslich", "authors": "Kimon Kieslich, Birte Keller, Christopher Starke", "title": "AI-Ethics by Design. Evaluating Public Perception on the Importance of\n  Ethical Design Principles of AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the immense societal importance of ethically designing artificial\nintelligence (AI), little research on the public perceptions of ethical AI\nprinciples exists. This becomes even more striking when considering that\nethical AI development has the aim to be human-centric and of benefit for the\nwhole society. In this study, we investigate how ethical principles\n(explainability, fairness, security, accountability, accuracy, privacy, machine\nautonomy) are weighted in comparison to each other. This is especially\nimportant, since simultaneously considering ethical principles is not only\ncostly, but sometimes even impossible, as developers must make specific\ntrade-off decisions. In this paper, we give first answers on the relative\nimportance of ethical principles given a specific use case - the use of AI in\ntax fraud detection. The results of a large conjoint survey (n=1099) suggest\nthat, by and large, German respondents found the ethical principles equally\nimportant. However, subsequent cluster analysis shows that different preference\nmodels for ethically designed systems exist among the German population. These\nclusters substantially differ not only in the preferred attributes, but also in\nthe importance level of the attributes themselves. We further describe how\nthese groups are constituted in terms of sociodemographics as well as opinions\non AI. Societal implications as well as design challenges are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 09:01:14 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kieslich", "Kimon", ""], ["Keller", "Birte", ""], ["Starke", "Christopher", ""]]}, {"id": "2106.00410", "submitter": "Genta Indra Winata", "authors": "Genta Indra Winata, Holy Lovenia, Etsuko Ishii, Farhad Bin Siddique,\n  Yongsheng Yang, Pascale Fung", "title": "Nora: The Well-Being Coach", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current pandemic has forced people globally to remain in isolation and\npractice social distancing, which creates the need for a system to combat the\nresulting loneliness and negative emotions. In this paper we propose Nora, a\nvirtual coaching platform designed to utilize natural language understanding in\nits dialogue system and suggest other recommendations based on user\ninteractions. It is intended to provide assistance and companionship to people\nundergoing self-quarantine or work-from-home routines. Nora helps users gauge\ntheir well-being by detecting and recording the user's emotion, sentiment, and\nstress. Nora also recommends various workout, meditation, or yoga exercises to\nusers in support of developing a healthy daily routine. In addition, we provide\na social community inside Nora, where users can connect and share their\nexperiences with others undergoing a similar isolation procedure. Nora can be\naccessed from anywhere via a web link and has support for both English and\nMandarin.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 11:42:07 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Winata", "Genta Indra", ""], ["Lovenia", "Holy", ""], ["Ishii", "Etsuko", ""], ["Siddique", "Farhad Bin", ""], ["Yang", "Yongsheng", ""], ["Fung", "Pascale", ""]]}, {"id": "2106.00599", "submitter": "Michael Aupetit", "authors": "Mostafa Abbas, Ehsan Ullah, Abdelkader Baggag, Halima Bensmail,\n  Michael Sedlmair, Michael Aupetit", "title": "ClustRank: a Visual Quality Measure Trained on Perceptual Data for\n  Sorting Scatterplots by Cluster Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Visual quality measures (VQMs) are designed to support analysts by\nautomatically detecting and quantifying patterns in visualizations. We propose\na new data-driven technique called ClustRank that allows to rank scatterplots\naccording to visible grouping patterns. Our model first encodes scatterplots in\nthe parametric space of a Gaussian Mixture Model, and then uses a classifier\ntrained on human judgment data to estimate the perceptual complexity of\ngrouping patterns. The numbers of initial mixture components and final combined\ngroups determine the rank of the scatterplot. ClustRank improves on existing\nVQM techniques by mimicking human judgments on two-Gaussian cluster patterns\nand gives more accuracy when ranking general cluster patterns in scatterplots.\nWe demonstrate its benefit by analyzing kinship data for genome-wide\nassociation studies, a domain in which experts rely on the visual analysis of\nlarge sets of scatterplots. We make the three benchmark datasets and the\nClustRank VQM available for practical use and further improvements.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 16:07:50 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Abbas", "Mostafa", ""], ["Ullah", "Ehsan", ""], ["Baggag", "Abdelkader", ""], ["Bensmail", "Halima", ""], ["Sedlmair", "Michael", ""], ["Aupetit", "Michael", ""]]}, {"id": "2106.00613", "submitter": "Jian Cui", "authors": "Jian Cui, Zirui Lan, Yisi Liu, Ruilin Li, Fan Li, Olga Sourina, and\n  Wolfgang Mueller-Wittig", "title": "A Compact and Interpretable Convolutional Neural Network for\n  Cross-Subject Driver Drowsiness Detection from Single-Channel EEG", "comments": null, "journal-ref": null, "doi": "10.1016/j.ymeth.2021.04.017", "report-no": null, "categories": "eess.SP cs.HC cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Driver drowsiness is one of main factors leading to road fatalities and\nhazards in the transportation industry. Electroencephalography (EEG) has been\nconsidered as one of the best physiological signals to detect drivers drowsy\nstates, since it directly measures neurophysiological activities in the brain.\nHowever, designing a calibration-free system for driver drowsiness detection\nwith EEG is still a challenging task, as EEG suffers from serious mental and\nphysical drifts across different subjects. In this paper, we propose a compact\nand interpretable Convolutional Neural Network (CNN) to discover shared EEG\nfeatures across different subjects for driver drowsiness detection. We\nincorporate the Global Average Pooling (GAP) layer in the model structure,\nallowing the Class Activation Map (CAM) method to be used for localizing\nregions of the input signal that contribute most for classification. Results\nshow that the proposed model can achieve an average accuracy of 73.22% on 11\nsubjects for 2-class cross-subject EEG signal classification, which is higher\nthan conventional machine learning methods and other state-of-art deep learning\nmethods. It is revealed by the visualization technique that the model has\nlearned biologically explainable features, e.g., Alpha spindles and Theta\nburst, as evidence for the drowsy state. It is also interesting to see that the\nmodel uses artifacts that usually dominate the wakeful EEG, e.g., muscle\nartifacts and sensor drifts, to recognize the alert state. The proposed model\nillustrates a potential direction to use CNN models as a powerful tool to\ndiscover shared features related to different mental states across different\nsubjects from EEG signals.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 14:36:34 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Cui", "Jian", ""], ["Lan", "Zirui", ""], ["Liu", "Yisi", ""], ["Li", "Ruilin", ""], ["Li", "Fan", ""], ["Sourina", "Olga", ""], ["Mueller-Wittig", "Wolfgang", ""]]}, {"id": "2106.00727", "submitter": "Vladimir Ivanov", "authors": "Vladimir Ivanov, Anton Krivtsov, Sergey Strelkov, Dmitry Gulyaev,\n  Denis Godanyuk, Nikolay Kalakutsky, Artyom Pavlov, Marina Petropavloskaya,\n  Alexander Smirnov, Andrew Yaremenko", "title": "Surgical navigation systems based on augmented reality technologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This study considers modern surgical navigation systems based on augmented\nreality technologies. Augmented reality glasses are used to construct holograms\nof the patient's organs from MRI and CT data, subsequently transmitted to the\nglasses. This, in addition to seeing the actual patient, the surgeon gains\nvisualization inside the patient's body (bones, soft tissues, blood vessels,\netc.). The solutions developed at Peter the Great St. Petersburg Polytechnic\nUniversity allow reducing the invasiveness of the procedure and preserving\nhealthy tissues. This also improves the navigation process, making it easier to\nestimate the location and size of the tumor to be removed. We describe the\napplication of developed systems to different types of surgical operations\n(removal of a malignant brain tumor, removal of a cyst of the cervical spine).\nWe consider the specifics of novel navigation systems designed for anesthesia,\nfor endoscopic operations. Furthermore, we discuss the construction of novel\nvisualization systems for ultrasound machines. Our findings indicate that the\ntechnologies proposed show potential for telemedicine.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 11:09:12 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ivanov", "Vladimir", ""], ["Krivtsov", "Anton", ""], ["Strelkov", "Sergey", ""], ["Gulyaev", "Dmitry", ""], ["Godanyuk", "Denis", ""], ["Kalakutsky", "Nikolay", ""], ["Pavlov", "Artyom", ""], ["Petropavloskaya", "Marina", ""], ["Smirnov", "Alexander", ""], ["Yaremenko", "Andrew", ""]]}, {"id": "2106.00739", "submitter": "Ruben Tolosana", "authors": "Ruben Tolosana, Ruben Vera-Rodriguez, Carlos Gonzalez-Garcia, Julian\n  Fierrez, Santiago Rengifo, Aythami Morales, Javier Ortega-Garcia, Juan Carlos\n  Ruiz-Garcia, Sergio Romero-Tapiador, Jiajia Jiang, Songxuan Lai, Lianwen Jin,\n  Yecheng Zhu, Javier Galbally, Moises Diaz, Miguel Angel Ferrer, Marta\n  Gomez-Barrero, Ilya Hodashinsky, Konstantin Sarin, Artem Slezkin, Marina\n  Bardamova, Mikhail Svetlakov, Mohammad Saleem, Cintia Lia Sz\\\"ucs, Bence\n  Kovari, Falk Pulsmeyer, Mohamad Wehbi, Dario Zanca, Sumaiya Ahmad, Sarthak\n  Mishra and Suraiya Jabin", "title": "ICDAR 2021 Competition on On-Line Signature Verification", "comments": null, "journal-ref": "Proc. International Conference on Document Analysis and\n  Recognition 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes the experimental framework and results of the ICDAR 2021\nCompetition on On-Line Signature Verification (SVC 2021). The goal of SVC 2021\nis to evaluate the limits of on-line signature verification systems on popular\nscenarios (office/mobile) and writing inputs (stylus/finger) through\nlarge-scale public databases. Three different tasks are considered in the\ncompetition, simulating realistic scenarios as both random and skilled\nforgeries are simultaneously considered on each task. The results obtained in\nSVC 2021 prove the high potential of deep learning methods. In particular, the\nbest on-line signature verification system of SVC 2021 obtained Equal Error\nRate (EER) values of 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3).\n  SVC 2021 will be established as an on-going competition, where researchers\ncan easily benchmark their systems against the state of the art in an open\ncommon platform using large-scale public databases such as DeepSignDB and\nSVC2021_EvalDB, and standard experimental protocols.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 19:33:46 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tolosana", "Ruben", ""], ["Vera-Rodriguez", "Ruben", ""], ["Gonzalez-Garcia", "Carlos", ""], ["Fierrez", "Julian", ""], ["Rengifo", "Santiago", ""], ["Morales", "Aythami", ""], ["Ortega-Garcia", "Javier", ""], ["Ruiz-Garcia", "Juan Carlos", ""], ["Romero-Tapiador", "Sergio", ""], ["Jiang", "Jiajia", ""], ["Lai", "Songxuan", ""], ["Jin", "Lianwen", ""], ["Zhu", "Yecheng", ""], ["Galbally", "Javier", ""], ["Diaz", "Moises", ""], ["Ferrer", "Miguel Angel", ""], ["Gomez-Barrero", "Marta", ""], ["Hodashinsky", "Ilya", ""], ["Sarin", "Konstantin", ""], ["Slezkin", "Artem", ""], ["Bardamova", "Marina", ""], ["Svetlakov", "Mikhail", ""], ["Saleem", "Mohammad", ""], ["Sz\u00fccs", "Cintia Lia", ""], ["Kovari", "Bence", ""], ["Pulsmeyer", "Falk", ""], ["Wehbi", "Mohamad", ""], ["Zanca", "Dario", ""], ["Ahmad", "Sumaiya", ""], ["Mishra", "Sarthak", ""], ["Jabin", "Suraiya", ""]]}, {"id": "2106.00764", "submitter": "Dongyun Han", "authors": "Dongyun Han, Gorakh Parsad, Hwiyeon Kim, Jaekyom Shim, Oh-Sang Kwon,\n  Kyung A Son, Jooyoung Lee, Isaac Cho, and Sungahn Ko", "title": "HisVA: A Visual Analytics System for Studying History", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying history involves many difficult tasks. Examples include searching\nfor proper data in a large event space, understanding stories of historical\nevents by time and space, and finding relationships among events that may not\nbe apparent. Instructors who extensively use well-organized and well-argued\nmaterials (e.g., textbooks and online resources) can lead students to a narrow\nperspective in understanding history and prevent spontaneous investigation of\nhistorical events, with the students asking their own questions. In this work,\nwe proposed HisVA, a visual analytics system that allows the efficient\nexploration of historical events from Wikipedia using three views: event, map,\nand resource. HisVA provides an effective event exploration space, where users\ncan investigate relationships among historical events by reviewing and linking\nthem in terms of space and time. To evaluate our system, we present two usage\nscenarios, a user study with a qualitative analysis of user exploration\nstrategies, and %expert feedback with in-class deployment results.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:02:29 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 03:38:43 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Han", "Dongyun", ""], ["Parsad", "Gorakh", ""], ["Kim", "Hwiyeon", ""], ["Shim", "Jaekyom", ""], ["Kwon", "Oh-Sang", ""], ["Son", "Kyung A", ""], ["Lee", "Jooyoung", ""], ["Cho", "Isaac", ""], ["Ko", "Sungahn", ""]]}, {"id": "2106.00794", "submitter": "Nikita Nangia", "authors": "Nikita Nangia, Saku Sugawara, Harsh Trivedi, Alex Warstadt, Clara\n  Vania, Samuel R. Bowman", "title": "What Ingredients Make for an Effective Crowdsourcing Protocol for\n  Difficult NLU Data Collection Tasks?", "comments": "ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing is widely used to create data for common natural language\nunderstanding tasks. Despite the importance of these datasets for measuring and\nrefining model understanding of language, there has been little focus on the\ncrowdsourcing methods used for collecting the datasets. In this paper, we\ncompare the efficacy of interventions that have been proposed in prior work as\nways of improving data quality. We use multiple-choice question answering as a\ntestbed and run a randomized trial by assigning crowdworkers to write questions\nunder one of four different data collection protocols. We find that asking\nworkers to write explanations for their examples is an ineffective stand-alone\nstrategy for boosting NLU example difficulty. However, we find that training\ncrowdworkers, and then using an iterative process of collecting data, sending\nfeedback, and qualifying workers based on expert judgments is an effective\nmeans of collecting challenging data. But using crowdsourced, instead of expert\njudgments, to qualify workers and send feedback does not prove to be effective.\nWe observe that the data from the iterative protocol with expert assessments is\nmore challenging by several measures. Notably, the human--model gap on the\nunanimous agreement portion of this data is, on average, twice as large as the\ngap for the baseline protocol data.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:05:52 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Nangia", "Nikita", ""], ["Sugawara", "Saku", ""], ["Trivedi", "Harsh", ""], ["Warstadt", "Alex", ""], ["Vania", "Clara", ""], ["Bowman", "Samuel R.", ""]]}, {"id": "2106.00857", "submitter": "Jie Yang", "authors": "Sheng Tan, Jie Yang", "title": "Fine-grained Finger Gesture Recognition Using WiFi Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gesture recognition has become increasingly important in human-computer\ninteraction and can support different applications such as smart home, VR, and\ngaming. Traditional approaches usually rely on dedicated sensors that are worn\nby the user or cameras that require line of sight. In this paper, we present\nfine-grained finger gesture recognition by using commodity WiFi without\nrequiring user to wear any sensors. Our system takes advantages of the\nfine-grained Channel State Information available from commodity WiFi devices\nand the prevalence of WiFi network infrastructures. It senses and identifies\nsubtle movements of finger gestures by examining the unique patterns exhibited\nin the detailed CSI. We devise environmental noise removal mechanism to\nmitigate the effect of signal dynamic due to the environment changes. Moreover,\nwe propose to capture the intrinsic gesture behavior to deal with individual\ndiversity and gesture inconsistency. Lastly, we utilize multiple WiFi links and\nlarger bandwidth at 5GHz to achieve finger gesture recognition under multi-user\nscenario. Our experimental evaluation in different environments demonstrates\nthat our system can achieve over 90% recognition accuracy and is robust to both\nenvironment changes and individual diversity. Results also show that our system\ncan provide accurate gesture recognition under different scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 23:43:39 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tan", "Sheng", ""], ["Yang", "Jie", ""]]}, {"id": "2106.00859", "submitter": "Jie Yang", "authors": "Linghan Zhang, Jie Yang", "title": "A Continuous Liveness Detection for Voice Authentication on Smart\n  Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice biometrics is drawing increasing attention as it is a promising\nalternative to legacy passwords for user authentication. Recently, a growing\nbody of work shows that voice biometrics is vulnerable to spoofing through\nreplay attacks, where an adversary tries to spoof voice authentication systems\nby using a pre-recorded voice sample collected from a genuine user. To this\nend, we propose VoiceGesture, a liveness detection solution for voice\nauthentication on smart devices such as smartphones and smart speakers.\nVoiceGesture detects a live user by leveraging both the unique articulatory\ngesture of the user when speaking a passphrase and the audio hardware advances\non these smart devices. Specifically, our system re-uses a pair of built-in\nspeaker and microphone on a smart device as a Doppler radar, which transmits a\nhigh-frequency acoustic sound from the speaker and listens to the reflections\nat the microphone when a user speaks a passphrase. Then we extract Doppler\nshifts resulted from the user's articulatory gestures for liveness detection.\nVoiceGesture is practical as it requires neither cumbersome operations nor\nadditional hardware but a speaker and a microphone commonly available on smart\ndevices that support voice input. Our experimental evaluation with 21\nparticipants and different smart devices shows that VoiceGesture achieves over\n99% and around 98% detection accuracy for text-dependent and text-independent\nliveness detection, respectively. Results also show that VoiceGesture is robust\nto different device placements, low audio sampling frequency, and supports\nmedium range liveness detection on smart speakers in various use scenarios like\nsmart homes and smart vehicles.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 23:53:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Zhang", "Linghan", ""], ["Yang", "Jie", ""]]}, {"id": "2106.00860", "submitter": "Jie Yang", "authors": "Sheng Tan, Jie Yang", "title": "Object Sensing for Fruit Ripeness Detection Using WiFi Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents FruitSense, a novel fruit ripeness sensing system that\nleverages wireless signals to enable non-destructive and low-cost detection of\nfruit ripeness. Such a system can reuse existing WiFi devices in homes without\nthe need for additional sensors. It uses WiFi signals to sense the\nphysiological changes associated with fruit ripening for detecting the ripeness\nof fruit. FruitSense leverages the larger bandwidth at 5GHz (i.e., over 600MHz)\nto extract the multipath-independent signal components to characterize the\nphysiological compounds of the fruit. It then measures the similarity between\nthe extracted features and the ones in ripeness profiles for identifying the\nripeness level. We evaluate FruitSense in different multipath environments with\ntwo types of fruits (i.e, kiwi and avocado) under four levels of ripeness.\nExperimental results show that FruitSense can detect the ripeness levels of\nfruits with an accuracy of over 90%.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 23:58:57 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tan", "Sheng", ""], ["Yang", "Jie", ""]]}, {"id": "2106.00865", "submitter": "Jie Yang", "authors": "Sheng Tan, Jie Yang", "title": "Multi-User Activity Recognition and Tracking Using Commodity WiFi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents MultiTrack, a commodity WiFi-based human sensing system\nthat can track multiple users and recognize the activities of multiple users\nperforming them simultaneously. Such a system can enable easy and large-scale\ndeployment for multi-user tracking and sensing without the need for additional\nsensors through the use of existing WiFi devices (e.g., desktops, laptops, and\nsmart appliances). The basic idea is to identify and extract the signal\nreflection corresponding to each individual user with the help of multiple WiFi\nlinks and all the available WiFi channels at 5GHz. Given the extracted signal\nreflection of each user, MultiTrack examines the path of the reflected signals\nat multiple links to simultaneously track multiple users. It further\nreconstructs the signal profile of each user as if only a single user has\nperformed activity in the environment to facilitate multi-user activity\nrecognition. We evaluate MultiTrack in different multipath environments with up\nto 4 users for multi-user tracking and up to 3 users for activity recognition.\nExperimental results show that our system can achieve decimeter localization\naccuracy and over 92% activity recognition accuracy under multi-user scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 00:25:19 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tan", "Sheng", ""], ["Yang", "Jie", ""]]}, {"id": "2106.00891", "submitter": "Zhiwen Tang", "authors": "Zhiwen Tang, Hrishikesh Kulkarni, Grace Hui Yang", "title": "High-Quality Diversification for Task-Oriented Dialogue Systems", "comments": "Accepted by ACL-IJCNLP 2021 (Findings of ACL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many task-oriented dialogue systems use deep reinforcement learning (DRL) to\nlearn policies that respond to the user appropriately and complete the tasks\nsuccessfully. Training DRL agents with diverse dialogue trajectories prepare\nthem well for rare user requests and unseen situations. One effective\ndiversification method is to let the agent interact with a diverse set of\nlearned user models. However, trajectories created by these artificial user\nmodels may contain generation errors, which can quickly propagate into the\nagent's policy. It is thus important to control the quality of the\ndiversification and resist the noise. In this paper, we propose a novel\ndialogue diversification method for task-oriented dialogue systems trained in\nsimulators. Our method, Intermittent Short Extension Ensemble (I-SEE),\nconstrains the intensity to interact with an ensemble of diverse user models\nand effectively controls the quality of the diversification. Evaluations on the\nMultiwoz dataset show that I-SEE successfully boosts the performance of several\nstate-of-the-art DRL dialogue agents.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 02:10:07 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 01:45:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Tang", "Zhiwen", ""], ["Kulkarni", "Hrishikesh", ""], ["Yang", "Grace Hui", ""]]}, {"id": "2106.00931", "submitter": "Victor Chen", "authors": "Victor Chen, Xuhai Xu, Richard Li, Yuanchun Shi, Shwetak Patel, Yuntao\n  Wang", "title": "Understanding the Design Space of Mouth Microgestures", "comments": "14 page, 5 figures; Accepted to DIS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As wearable devices move toward the face (i.e. smart earbuds, glasses), there\nis an increasing need to facilitate intuitive interactions with these devices.\nCurrent sensing techniques can already detect many mouth-based gestures;\nhowever, users' preferences of these gestures are not fully understood. In this\npaper, we investigate the design space and usability of mouth-based\nmicrogestures. We first conducted brainstorming sessions (N=16) and compiled an\nextensive set of 86 user-defined gestures. Then, with an online survey (N=50),\nwe assessed the physical and mental demand of our gesture set and identified a\nsubset of 14 gestures that can be performed easily and naturally. Finally, we\nconducted a remote Wizard-of-Oz usability study (N=11) mapping gestures to\nvarious daily smartphone operations under a sitting and walking context. From\nthese studies, we develop a taxonomy for mouth gestures, finalize a practical\ngesture set for common applications, and provide design guidelines for future\nmouth-based gesture interactions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 04:31:45 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Chen", "Victor", ""], ["Xu", "Xuhai", ""], ["Li", "Richard", ""], ["Shi", "Yuanchun", ""], ["Patel", "Shwetak", ""], ["Wang", "Yuntao", ""]]}, {"id": "2106.00954", "submitter": "Zhe Liu", "authors": "Zhe Liu, Yufan Guo, Jalal Mahmud", "title": "When and Why does a Model Fail? A Human-in-the-loop Error Detection\n  Framework for Sentiment Analysis", "comments": "NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although deep neural networks have been widely employed and proven effective\nin sentiment analysis tasks, it remains challenging for model developers to\nassess their models for erroneous predictions that might exist prior to\ndeployment. Once deployed, emergent errors can be hard to identify in\nprediction run-time and impossible to trace back to their sources. To address\nsuch gaps, in this paper we propose an error detection framework for sentiment\nanalysis based on explainable features. We perform global-level feature\nvalidation with human-in-the-loop assessment, followed by an integration of\nglobal and local-level feature contribution analysis. Experimental results show\nthat, given limited human-in-the-loop intervention, our method is able to\nidentify erroneous model predictions on unseen data with high precision.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 05:45:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Zhe", ""], ["Guo", "Yufan", ""], ["Mahmud", "Jalal", ""]]}, {"id": "2106.01131", "submitter": "Lee Livsey", "authors": "Lee Livsey, Helen Petrie, Siamak F. Shahandashti, Aidan Fray", "title": "Performance and Usability of Visual and Verbal Verification of\n  Word-based Key Fingerprints", "comments": "This is an accepted manuscript to appear in the proceedings of the\n  15th International Symposium on Human Aspects of Information Security &\n  Assurance, HAISA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The security of messaging applications against person-in-the-middle attacks\nrelies on the authenticity of the exchanged keys. For users unable to meet in\nperson, a manual key fingerprint verification is necessary to ascertain key\nauthenticity. Such fingerprints can be exchanged visually or verbally, and it\nis not clear in which condition users perform best. This paper reports the\nresults of a 62-participant study that investigated differences in performance\nand perceived usability of visual and verbal comparisons of word-based key\nfingerprints, and the influence of the individual's cognitive learning style.\nThe results show visual comparisons to be more effective against non-security\ncritical errors and are perceived to provide increased confidence, yet\nparticipants perceive verbal comparisons to be easier and require less mental\neffort. Besides, limited evidence was found on the influence of the\nindividual's learning style on their performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:57:31 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Livsey", "Lee", ""], ["Petrie", "Helen", ""], ["Shahandashti", "Siamak F.", ""], ["Fray", "Aidan", ""]]}, {"id": "2106.01215", "submitter": "Talha Bin Masood", "authors": "Talha Bin Masood, Signe Sidwall Thygesen, Mathieu Linares, Alexei I.\n  Abrikosov, Vijay Natarajan, Ingrid Hotz", "title": "Visual Analysis of Electronic Densities and Transitions in Molecules", "comments": "15 pages, 9 figures, To appear in EuroVis 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CG physics.chem-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The study of electronic transitions within a molecule connected to the\nabsorption or emission of light is a common task in the process of the design\nof new materials. The transitions are complex quantum mechanical processes and\na detailed analysis requires a breakdown of these processes into components\nthat can be interpreted via characteristic chemical properties. We approach\nthese tasks by providing a detailed analysis of the electron density field.\nThis entails methods to quantify and visualize electron localization and\ntransfer from molecular subgroups combining spatial and abstract\nrepresentations. The core of our method uses geometric segmentation of the\nelectronic density field coupled with a graph-theoretic formulation of charge\ntransfer between molecular subgroups. The design of the methods has been guided\nby the goal of providing a generic and objective analysis following fundamental\nconcepts. We illustrate the proposed approach using several case studies\ninvolving the study of electronic transitions in different molecular systems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:07:02 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Masood", "Talha Bin", ""], ["Thygesen", "Signe Sidwall", ""], ["Linares", "Mathieu", ""], ["Abrikosov", "Alexei I.", ""], ["Natarajan", "Vijay", ""], ["Hotz", "Ingrid", ""]]}, {"id": "2106.01254", "submitter": "Tim Weninger PhD", "authors": "Paul Resnick, Yuqing Kong, Grant Schoenebeck, Tim Weninger", "title": "Survey Equivalence: A Procedure for Measuring Classifier Accuracy\n  Against Human Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many classification tasks, the ground truth is either noisy or subjective.\nExamples include: which of two alternative paper titles is better? is this\ncomment toxic? what is the political leaning of this news article? We refer to\nsuch tasks as survey settings because the ground truth is defined through a\nsurvey of one or more human raters. In survey settings, conventional\nmeasurements of classifier accuracy such as precision, recall, and\ncross-entropy confound the quality of the classifier with the level of\nagreement among human raters. Thus, they have no meaningful interpretation on\ntheir own. We describe a procedure that, given a dataset with predictions from\na classifier and K ratings per item, rescales any accuracy measure into one\nthat has an intuitive interpretation. The key insight is to score the\nclassifier not against the best proxy for the ground truth, such as a majority\nvote of the raters, but against a single human rater at a time. That score can\nbe compared to other predictors' scores, in particular predictors created by\ncombining labels from several other human raters. The survey equivalence of any\nclassifier is the minimum number of raters needed to produce the same expected\nscore as that found for the classifier.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 16:07:32 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Resnick", "Paul", ""], ["Kong", "Yuqing", ""], ["Schoenebeck", "Grant", ""], ["Weninger", "Tim", ""]]}, {"id": "2106.01503", "submitter": "Garrick Cabour", "authors": "Garrick Cabour, Andr\\'es Morales, \\'Elise Ledoux, Samuel Bassetto", "title": "Towards an Explanation Space to Align Humans and Explainable-AI Teamwork", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Providing meaningful and actionable explanations to end-users is a\nfundamental prerequisite for implementing explainable intelligent systems in\nthe real world. Explainability is a situated interaction between a user and the\nAI system rather than being static design principles. The content of\nexplanations is context-dependent and must be defined by evidence about the\nuser and its context. This paper seeks to operationalize this concept by\nproposing a formative architecture that defines the explanation space from a\nuser-inspired perspective. The architecture comprises five intertwined\ncomponents to outline explanation requirements for a task: (1) the end-users\nmental models, (2) the end-users cognitive process, (3) the user interface, (4)\nthe human-explainer agent, and the (5) agent process. We first define each\ncomponent of the architecture. Then we present the Abstracted Explanation\nSpace, a modeling tool that aggregates the architecture's components to support\ndesigners in systematically aligning explanations with the end-users work\npractices, needs, and goals. It guides the specifications of what needs to be\nexplained (content - end-users mental model), why this explanation is necessary\n(context - end-users cognitive process), to delimit how to explain it (format -\nhuman-explainer agent and user interface), and when should the explanations be\ngiven. We then exemplify the tool's use in an ongoing case study in the\naircraft maintenance domain. Finally, we discuss possible contributions of the\ntool, known limitations/areas for improvement, and future work to be done.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 23:17:29 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Cabour", "Garrick", ""], ["Morales", "Andr\u00e9s", ""], ["Ledoux", "\u00c9lise", ""], ["Bassetto", "Samuel", ""]]}, {"id": "2106.01627", "submitter": "Jan Wolff", "authors": "Jan Wolff", "title": "Piercing the Veil: Designs to Support Information Literacy on Social\n  Platforms", "comments": "Originally submitted to and presented at CHI'21 Workshop on\n  Technologies to Support Critical Thinking in an Age of Misinformation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this position paper we approach problems concerning critical digital and\ninformation literacy with ideas to provide more digestible explanations of\nabstract concepts through interface design. In particular, we focus on social\nmedia platforms where we see the possibility of counteracting the spread of\nmisinformation by providing users with more proficiency through our approaches.\nWe argue that the omnipresent trend to abstract away and hide information from\nusers via UI/UX design opposes their ability to self-learn. This leads us to\npropose a different framework in which we unify elegant and simple interfaces\nwith nudges that promote a look behind the curtain. Such designs serve to\nfoster a deeper understanding of employed technologies and aim to increase the\ncritical assessment of content encountered on social platforms. Furthermore, we\nconsider users with an intermediary skill level to be largely ignored in\ncurrent approaches, as they are given no tools to broaden their knowledge\nwithout consultation of expert material. The resulting stagnation is\nexemplified by the tactics of misinformation campaigns, which exploit the\nensuing lack of information literacy and critical thinking. We propose an\napproach to design that sufficiently emancipates users in both aspects by\npromoting a look behind the abstraction of UI/UX so that an autonomous learning\nprocess is given the chance to occur. Furthermore, we name ideas for future\nresearch within this area that take our considerations into account.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 06:56:09 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wolff", "Jan", ""]]}, {"id": "2106.01650", "submitter": "Gavin Suddrey", "authors": "Gavin Suddrey, Ben Talbot and Frederic Maire", "title": "Learning and Executing Re-usable Behaviour Trees from Natural Language\n  Instruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Domestic and service robots have the potential to transform industries such\nas health care and small-scale manufacturing, as well as the homes in which we\nlive. However, due to the overwhelming variety of tasks these robots will be\nexpected to complete, providing generic out-of-the-box solutions that meet the\nneeds of every possible user is clearly intractable. To address this problem,\nrobots must therefore not only be capable of learning how to complete novel\ntasks at run-time, but the solutions to these tasks must also be informed by\nthe needs of the user. In this paper we demonstrate how behaviour trees, a well\nestablished control architecture in the fields of gaming and robotics, can be\nused in conjunction with natural language instruction to provide a robust and\nmodular control architecture for instructing autonomous agents to learn and\nperform novel complex tasks. We also show how behaviour trees generated using\nour approach can be generalised to novel scenarios, and can be re-used in\nfuture learning episodes to create increasingly complex behaviours. We validate\nthis work against an existing corpus of natural language instructions,\ndemonstrate the application of our approach on both a simulated robot solving a\ntoy problem, as well as two distinct real-world robot platforms which,\nrespectively, complete a block sorting scenario, and a patrol scenario.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 07:47:06 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Suddrey", "Gavin", ""], ["Talbot", "Ben", ""], ["Maire", "Frederic", ""]]}, {"id": "2106.01730", "submitter": "Erwin Jos\\'e L\\'opez Pulgar\\'in Dr", "authors": "Erwin Jose Lopez Pulgarin, Guido Herrmann, Ute Leonards", "title": "Drivers' Manoeuvre Modelling and Prediction for Safe HRI", "comments": "Submitted to IEEE Transactions on Human-Machine Systems on\n  17-Jul-2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As autonomous machines such as robots and vehicles start performing tasks\ninvolving human users, ensuring a safe interaction between them becomes an\nimportant issue. Translating methods from human-robot interaction (HRI) studies\nto the interaction between humans and other highly complex machines (e.g.\nsemi-autonomous vehicles) could help advance the use of those machines in\nscenarios requiring human interaction. One method involves understanding human\nintentions and decision-making to estimate the human's present and near-future\nactions whilst interacting with a robot. This idea originates from the\npsychological concept of Theory of Mind, which has been broadly explored for\nrobotics and recently for autonomous and semi-autonomous vehicles. In this\nwork, we explored how to predict human intentions before an action is performed\nby combining data from human-motion, vehicle-state and human inputs (e.g.\nsteering wheel, pedals). A data-driven approach based on Recurrent Neural\nNetwork models was used to classify the current driving manoeuvre and to\npredict the future manoeuvre to be performed. A state-transition model was used\nwith a fixed set of manoeuvres to label data recorded during the trials for\nreal-time applications. Models were trained and tested using drivers of\ndifferent seat preferences, driving expertise and arm-length; precision and\nrecall metrics over 95% for manoeuvre identification and 86% for manoeuvre\nprediction were achieved, with prediction time-windows of up to 1 second for\nboth known and unknown test subjects. Compared to our previous results,\nperformance improved and manoeuvre prediction was possible for unknown test\nsubjects without knowing the current manoeuvre.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 10:07:55 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Pulgarin", "Erwin Jose Lopez", ""], ["Herrmann", "Guido", ""], ["Leonards", "Ute", ""]]}, {"id": "2106.01838", "submitter": "Jie Yang", "authors": "Zi Wang, Jie Yang", "title": "Acoustic-based Object Detection for Pedestrian Using Smartphone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Walking while using a smartphone is becoming a major pedestrian safety\nconcern as people may unknowingly bump into various obstacles that could lead\nto severe injuries. In this paper, we propose ObstacleWatch, an acoustic-based\nobstacle collision detection system to improve the safety of pedestrians who\nare engaged in smartphone usage while walking. ObstacleWatch leverages the\nadvanced audio hardware of the smartphone to sense the surrounding obstacles\nand infers fine-grained information about the frontal obstacle for collision\ndetection. In particular, our system emits well-designed inaudible beep signals\nfrom the smartphone built-in speaker and listens to the reflections with the\nstereo recording of the smartphone. By analyzing the reflected signals received\nat two microphones, ObstacleWatch is able to extract fine-grained information\nof the frontal obstacle including the distance, angle, and size for detecting\nthe possible collisions and to alert users. Our experimental evaluation under\ntwo real-world environments with different types of phones and obstacles shows\nthat ObstacleWatch achieves over 92% accuracy in predicting obstacle collisions\nwith distance estimation errors at about 2 cm. Results also show that\nObstacleWatch is robust to different sizes of objects and is compatible with\ndifferent phone models with low energy consumption.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 00:36:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Wang", "Zi", ""], ["Yang", "Jie", ""]]}, {"id": "2106.01840", "submitter": "Jie Yang", "authors": "Linghan Zhang, Jie Yang", "title": "A Continuous Liveness Detection System for Text-independent Speaker\n  Verification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice authentication is drawing increasing attention and becomes an\nattractive alternative to passwords for mobile authentication. Recent advances\nin mobile technology further accelerate the adoption of voice biometrics in an\narray of diverse mobile applications. However, recent studies show that voice\nauthentication is vulnerable to replay attacks, where an adversary can spoof a\nvoice authentication system using a pre-recorded voice sample collected from\nthe victim. In this paper, we propose VoiceLive, a liveness detection system\nfor both text-dependent and text-independent voice authentication on\nsmartphones. VoiceLive detects a live user by leveraging the user's unique\nvocal system and the stereo recording of smartphones. In particular, utilizing\nthe built-in gyroscope, loudspeaker, and microphone, VoiceLive first measures\nthe smartphone's distance and angle from the user, then it captures the\nposition-specific time-difference-of-arrival (TDoA) changes in a sequence of\nphoneme sounds to the two microphones of the phone, and uses such unique TDoA\ndynamic which doesn't exist under replay attacks for liveness detection.\nVoiceLive is practical as it doesn't require additional hardware but\ntwo-channel stereo recording that is supported by virtually all smartphones.\nOur experimental evaluation with 12 participants and different types of phones\nshows that VoiceLive achieves over 99% detection accuracy at around 1% Equal\nError Rate (EER) on the text-dependent system and around 99% accuracy and 2%\nEER on the text-independent one. Results also show that VoiceLive is robust to\ndifferent phone positions, i.e. the user is free to hold the smartphone with\ndistinct distances and angles.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 00:32:40 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Zhang", "Linghan", ""], ["Yang", "Jie", ""]]}, {"id": "2106.01998", "submitter": "Akbar Siami Namin", "authors": "Faranak Abri, Luis Felipe Gutierrez, Chaitra T. Kulkarni, Akbar Siami\n  Namin, Keith S. Jones", "title": "Toward Explainable Users: Using NLP to Enable AI to Understand Users'\n  Perceptions of Cyber Attacks", "comments": "20 pages, 3 figures, COMPSAC'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To understand how end-users conceptualize consequences of cyber security\nattacks, we performed a card sorting study, a well-known technique in Cognitive\nSciences, where participants were free to group the given consequences of\nchosen cyber attacks into as many categories as they wished using rationales\nthey see fit. The results of the open card sorting study showed a large amount\nof inter-participant variation making the research team wonder how the\nconsequences of security attacks were comprehended by the participants. As an\nexploration of whether it is possible to explain user's mental model and\nbehavior through Artificial Intelligence (AI) techniques, the research team\ncompared the card sorting data with the outputs of a number of Natural Language\nProcessing (NLP) techniques with the goal of understanding how participants\nperceived and interpreted the consequences of cyber attacks written in natural\nlanguages. The results of the NLP-based exploration methods revealed an\ninteresting observation implying that participants had mostly employed checking\nindividual keywords in each sentence to group cyber attack consequences\ntogether and less considered the semantics behind the description of\nconsequences of cyber attacks. The results reported in this paper are seemingly\nuseful and important for cyber attacks comprehension from user's perspectives.\nTo the best of our knowledge, this paper is the first introducing the use of AI\ntechniques in explaining and modeling users' behavior and their perceptions\nabout a context. The novel idea introduced here is about explaining users using\nAI.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:17:16 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Abri", "Faranak", ""], ["Gutierrez", "Luis Felipe", ""], ["Kulkarni", "Chaitra T.", ""], ["Namin", "Akbar Siami", ""], ["Jones", "Keith S.", ""]]}, {"id": "2106.02076", "submitter": "Justin Edwards", "authors": "Justin Edwards, Leigh Clark and Allison Perrone", "title": "LGBTQ-AI? Exploring Expressions of Gender and Sexual Orientation in\n  Chatbots", "comments": null, "journal-ref": null, "doi": "10.1145/3469595.3469597", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Chatbots are popular machine partners for task-oriented and social\ninteractions. Human-human computer-mediated communication research has explored\nhow people express their gender and sexuality in online social interactions,\nbut little is known about whether and in what way chatbots do the same. We\nconducted semi-structured interviews with 5 text-based conversational agents to\nexplore this topic Through these interviews, we identified 6 common themes\naround the expression of gender and sexual identity: identity description,\nidentity formation, peer acceptance, positive reflection, uncomfortable\nfeelings and off-topic responses. Chatbots express gender and sexuality\nexplicitly and through relation of experience and emotions, mimicking the human\nlanguage on which they are trained. It is nevertheless evident that chatbots\ndiffer from human dialogue partners as they lack the flexibility and\nunderstanding enabled by lived human experience. While chatbots are proficient\nin using language to express identity, they also display a lack of authentic\nexperiences of gender and sexuality.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 18:47:52 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Edwards", "Justin", ""], ["Clark", "Leigh", ""], ["Perrone", "Allison", ""]]}, {"id": "2106.02077", "submitter": "Justin Edwards", "authors": "Justin Edwards, Christian Janssen, Sandy Gould, and Benjamin R Cowan", "title": "Eliciting Spoken Interruptions to Inform Proactive Speech Agent Design", "comments": null, "journal-ref": null, "doi": "10.1145/3469595.3469618.", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current speech agent interactions are typically user-initiated, limiting the\ninteractions they can deliver. Future functionality will require agents to be\nproactive, sometimes interrupting users. Little is known about how these spoken\ninterruptions should be designed, especially in urgent interruption contexts.\nWe look to inform design of proactive agent interruptions through investigating\nhow people interrupt others engaged in complex tasks. We therefore developed a\nnew technique to elicit human spoken interruptions of people engaged in other\ntasks. We found that people interrupted sooner when interruptions were urgent.\nSome participants used access rituals to forewarn interruptions, but most\nrarely used them. People balanced speed and accuracy in timing interruptions,\noften using cues from the task they interrupted. People also varied phrasing\nand delivery of interruptions to reflect urgency. We discuss how our findings\ncan inform speech agent design and how our paradigm can help gain insight into\nhuman interruptions in new contexts.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 18:48:11 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Edwards", "Justin", ""], ["Janssen", "Christian", ""], ["Gould", "Sandy", ""], ["Cowan", "Benjamin R", ""]]}, {"id": "2106.02283", "submitter": "Maximilian Hils", "authors": "Maximilian Hils, Daniel W. Woods, Rainer B\\\"ohme (University of\n  Innsbruck)", "title": "Privacy Preference Signals: Past, Present and Future", "comments": null, "journal-ref": "Proceedings on Privacy Enhancing Technologies 2021", "doi": "10.2478/popets-2021-0069", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Privacy preference signals are digital representations of how users want\ntheir personal data to be processed. Such signals must be adopted by both the\nsender (users) and intended recipients (data processors). Adoption represents a\ncoordination problem that remains unsolved despite efforts dating back to the\n1990s. Browsers implemented standards like the Platform for Privacy Preferences\n(P3P) and Do Not Track (DNT), but vendors profiting from personal data faced\nfew incentives to receive and respect the expressed wishes of data subjects. In\nthe wake of recent privacy laws, a coalition of AdTech firms published the\nTransparency and Consent Framework (TCF), which defines an opt-in consent\nsignal. This paper integrates post-GDPR developments into the wider history of\nprivacy preference signals. Our main contribution is a high-frequency\nlongitudinal study describing how TCF signal gained dominance as of February\n2021. We explore which factors correlate with adoption at the website level.\nBoth the number of third parties on a website and the presence of Google Ads\nare associated with higher adoption of TCF. Further, we show that vendors acted\nas early adopters of TCF 2.0 and provide two case-studies describing how\nConsent Management Providers shifted existing customers to TCF 2.0. We sketch\nways forward for a pro-privacy signal.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 06:39:20 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 00:22:35 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 08:53:05 GMT"}, {"version": "v4", "created": "Wed, 14 Jul 2021 10:48:17 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Hils", "Maximilian", "", "University of\n  Innsbruck"], ["Woods", "Daniel W.", "", "University of\n  Innsbruck"], ["B\u00f6hme", "Rainer", "", "University of\n  Innsbruck"]]}, {"id": "2106.02325", "submitter": "Genta Indra Winata", "authors": "Etsuko Ishii, Genta Indra Winata, Samuel Cahyawijaya, Divesh Lala,\n  Tatsuya Kawahara, Pascale Fung", "title": "ERICA: An Empathetic Android Companion for Covid-19 Quarantine", "comments": "Accepted in SIGDIAL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past year, research in various domains, including Natural Language\nProcessing (NLP), has been accelerated to fight against the COVID-19 pandemic,\nyet such research has just started on dialogue systems. In this paper, we\nintroduce an end-to-end dialogue system which aims to ease the isolation of\npeople under self-quarantine. We conduct a control simulation experiment to\nassess the effects of the user interface, a web-based virtual agent called Nora\nvs. the android ERICA via a video call. The experimental results show that the\nandroid offers a more valuable user experience by giving the impression of\nbeing more empathetic and engaging in the conversation due to its nonverbal\ninformation, such as facial expressions and body gestures.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 08:14:43 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ishii", "Etsuko", ""], ["Winata", "Genta Indra", ""], ["Cahyawijaya", "Samuel", ""], ["Lala", "Divesh", ""], ["Kawahara", "Tatsuya", ""], ["Fung", "Pascale", ""]]}, {"id": "2106.02415", "submitter": "Andres Ferraro", "authors": "Andres Ferraro, Xavier Serra, Christine Bauer", "title": "What is fair? Exploring the artists' perspective on the fairness of\n  music streaming platforms", "comments": null, "journal-ref": "Proceedings of the 18th IFIP International Conference on\n  Human-Computer Interaction (INTERACT 2021)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Music streaming platforms are currently among the main sources of music\nconsumption, and the embedded recommender systems significantly influence what\nthe users consume. There is an increasing interest to ensure that those\nplatforms and systems are fair. Yet, we first need to understand what fairness\nmeans in such a context. Although artists are the main content providers for\nmusic platforms, there is a research gap concerning the artists' perspective.\nTo fill this gap, we conducted interviews with music artists to understand how\nthey are affected by current platforms and what improvements they deem\nnecessary. Using a Qualitative Content Analysis, we identify the aspects that\nthe artists consider relevant for fair platforms. In this paper, we discuss the\nfollowing aspects derived from the interviews: fragmented presentation,\nreaching an audience, transparency, influencing users' listening behavior,\npopularity bias, artists' repertoire size, quotas for local music, gender\nbalance, and new music. For some topics, our findings do not indicate a clear\ndirection about the best way how music platforms should act and function; for\nother topics, though, there is a clear consensus among our interviewees: for\nthese, the artists have a clear idea of the actions that should be taken so\nthat music platforms will be fair also for the artists.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 11:29:14 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Ferraro", "Andres", ""], ["Serra", "Xavier", ""], ["Bauer", "Christine", ""]]}, {"id": "2106.02604", "submitter": "Xiaowei Chen", "authors": "Xiaowei Chen", "title": "Does Persuasive Technology Make Smartphones More Addictive? -- An\n  Empirical Study of Chinese University Students", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  With the development of computer hardware, computers with persuasion have\nbecome more powerful and influential than ever. The latest trends show that\nPersuasive Technology integrates with cutting-edge technologies, such as\nNatural Language Processing, Big Data, and Machine Learning algorithms. As\npersuasion is becoming increasingly intelligent and subtle, it is urgent to\nreflect on the dark sides of Persuasive Technology. The study aims to\ninvestigate one of Persuasive Technology's accusations, making smartphones more\naddictive to its users. The study uses questionnaires and in-depth interviews\nto examine the impact of persuasive technologies on young smartphone users. The\nparticipants of the study are 18 to 26 years old Chinese university students.\nTen interviewees were sampled randomly from the survey results. Eight\ninterviewees shared their smartphone screen time for three consecutive weeks\nafter the interview. Among the 183 participants, 84.70% (n=155) spend over (or\nequal to) four hours per day on their smartphone, 44.26% (n=81) indicate that\nsmartphones negatively affect their studies or professional life. Ten\ninterviewees evaluated that they could reduce screen time by 37% if they could\navoid all persuasive functions. Five out of eight interviewees reduced their\nscreen time by 16.72% three weeks after the interviews by voluntarily turning\noff some persuasive functions on their smartphones. This study provides\nempirical evidence to argue that persuasive technologies increase users' screen\ntime and contribute to the addictive behaviours of young smartphone users. Some\ncommonly used persuasive design principles could have negative long term\nimpacts on users. To sum up, the ethical problems that Human-computer\ninteraction (HCI) designers face and users' neglected rights of acknowledgement\nwere discussed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 17:04:35 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 18:09:12 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chen", "Xiaowei", ""]]}, {"id": "2106.02692", "submitter": "David Gros", "authors": "David Gros, Yu Li, Zhou Yu", "title": "The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting\n  User Questions About Human or Non-Human Identity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Humans are increasingly interacting with machines through language, sometimes\nin contexts where the user may not know they are talking to a machine (like\nover the phone or a text chatbot). We aim to understand how system designers\nand researchers might allow their systems to confirm its non-human identity. We\ncollect over 2,500 phrasings related to the intent of ``Are you a robot?\". This\nis paired with over 2,500 adversarially selected utterances where only\nconfirming the system is non-human would be insufficient or disfluent. We\ncompare classifiers to recognize the intent and discuss the precision/recall\nand model complexity tradeoffs. Such classifiers could be integrated into\ndialog systems to avoid undesired deception. We then explore how both a\ngenerative research model (Blender) as well as two deployed systems (Amazon\nAlexa, Google Assistant) handle this intent, finding that systems often fail to\nconfirm their non-human identity. Finally, we try to understand what a good\nresponse to the intent would be, and conduct a user study to compare the\nimportant aspects when responding to this intent.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:04:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gros", "David", ""], ["Li", "Yu", ""], ["Yu", "Zhou", ""]]}, {"id": "2106.02715", "submitter": "Aleksandra Urman", "authors": "Aleksandra Urman, Mykola Makhortykh, Roberto Ulloa", "title": "Auditing Source Diversity Bias in Video Search Results Using Virtual\n  Agents", "comments": null, "journal-ref": "WWW '21: Companion Proceedings of the Web Conference 2021", "doi": "10.1145/3442442.3452306", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We audit the presence of domain-level source diversity bias in video search\nresults. Using a virtual agent-based approach, we compare outputs of four\nWestern and one non-Western search engines for English and Russian queries. Our\nfindings highlight that source diversity varies substantially depending on the\nlanguage with English queries returning more diverse outputs. We also find\ndisproportionately high presence of a single platform, YouTube, in top search\noutputs for all Western search engines except Google. At the same time, we\nobserve that Youtube's major competitors such as Vimeo or Dailymotion do not\nappear in the sampled Google's video search results. This finding suggests that\nGoogle might be downgrading the results from the main competitors of\nGoogle-owned Youtube and highlights the necessity for further studies focusing\non the presence of own-content bias in Google's search results.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:52:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Urman", "Aleksandra", ""], ["Makhortykh", "Mykola", ""], ["Ulloa", "Roberto", ""]]}, {"id": "2106.02777", "submitter": "Zach Van Hyfte", "authors": "Zach Van Hyfte and Avideh Zakhor", "title": "Immediate Proximity Detection Using Wi-Fi-Enabled Smartphones", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone apps for exposure notification and contact tracing have been shown\nto be effective in controlling the COVID-19 pandemic. However, Bluetooth Low\nEnergy tokens similar to those broadcast by existing apps can still be picked\nup far away from the transmitting device. In this paper, we present a new class\nof methods for detecting whether or not two Wi-Fi-enabled devices are in\nimmediate physical proximity, i.e. 2 or fewer meters apart, as established by\nthe U.S. Centers for Disease Control and Prevention (CDC). Our goal is to\nenhance the accuracy of smartphone-based exposure notification and contact\ntracing systems. We present a set of binary machine learning classifiers that\ntake as input pairs of Wi-Fi RSSI fingerprints. We empirically verify that a\nsingle classifier cannot generalize well to a range of different environments\nwith vastly different numbers of detectable Wi-Fi Access Points (APs). However,\nspecialized classifiers, tailored to situations where the number of detectable\nAPs falls within a certain range, are able to detect immediate physical\nproximity significantly more accurately. As such, we design three classifiers\nfor situations with low, medium, and high numbers of detectable APs. These\nclassifiers distinguish between pairs of RSSI fingerprints recorded 2 or fewer\nmeters apart and pairs recorded further apart but still in Bluetooth range. We\ncharacterize their balanced accuracy for this task to be between 66.8% and\n77.8%.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 02:17:01 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Van Hyfte", "Zach", ""], ["Zakhor", "Avideh", ""]]}, {"id": "2106.02921", "submitter": "Mikhail Jacob", "authors": "Mikhail Jacob, Brian Magerko", "title": "Empirically Evaluating Creative Arc Negotiation for Improvisational\n  Decision-making", "comments": "10 pages, 5 figures, 8 tables, accepted to ACM Creativity & Cognition\n  2021, Best Paper Award", "journal-ref": null, "doi": "10.1145/3450741.3465263", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Action selection from many options with few constraints is crucial for\nimprovisation and co-creativity. Our previous work proposed creative arc\nnegotiation to solve this problem, i.e., selecting actions to follow an\nauthor-defined `creative arc' or trajectory over estimates of novelty,\nunexpectedness, and quality for potential actions. The CARNIVAL agent\narchitecture demonstrated this approach for playing the Props game from improv\ntheatre in the Robot Improv Circus installation. This article evaluates the\ncreative arc negotiation experience with CARNIVAL through two crowdsourced\nobserver studies and one improviser laboratory study. The studies focus on\nsubjects' ability to identify creative arcs in performance and their preference\nfor creative arc negotiation compared to a random selection baseline. Our\nresults show empirically that observers successfully identified creative arcs\nin performances. Both groups also preferred creative arc negotiation in agent\ncreativity and logical coherence, while observers enjoyed it more too.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 15:20:36 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jacob", "Mikhail", ""], ["Magerko", "Brian", ""]]}, {"id": "2106.03036", "submitter": "Ritu Gala", "authors": "Ritu Gala, Revathi Vijayaraghavan, Valmik Nikam, Arvind Kiwelekar", "title": "Real-Time Cognitive Evaluation of Online Learners through Automatically\n  Generated Questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the increased adoption of E-learning platforms, keeping online learners\nengaged throughout a lesson is challenging. One approach to tackle this\nchallenge is to probe learn-ers periodically by asking questions. The paper\npresents an approach to generate questions from a given video lecture\nautomatically. The generated questions are aimed to evaluate learners'\nlower-level cognitive abilities. The approach automatically extracts text from\nvideo lectures to generates wh-kinds of questions. When learners respond with\nan answer, the proposed approach further evaluates the response and provides\nfeedback. Besides enhancing learner's engagement, this approach's main benefits\nare that it frees instructors from design-ing questions to check the\ncomprehension of a topic. Thus, instructors can spend this time productively on\nother activities.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 05:45:56 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gala", "Ritu", ""], ["Vijayaraghavan", "Revathi", ""], ["Nikam", "Valmik", ""], ["Kiwelekar", "Arvind", ""]]}, {"id": "2106.03060", "submitter": "Huansheng Ning Prof", "authors": "Sahraoui Dhelim, Liming Luke Chen, Nyothiri Aung, Wenyin Zhang,\n  Huansheng Ning", "title": "Big-Five, MPTI, Eysenck or HEXACO: The Ideal Personality Model for\n  Personality-aware Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personality-aware recommendation systems have been proven to achieve high\naccuracy compared to conventional recommendation systems. In addition to that,\npersonality-aware recommendation systems could help alleviate cold start and\ndata sparsity problems. Most of the existing works use Big-Five personality\nmodel to represent the user's personality, this is due to the popularity of\nBig-Five model in the literature of psychology. However, from personality\ncomputing perspective, the choice of the most suitable personality model that\nsatisfy the requirements of the recommendation application and the recommended\ncontent type still needs further investigation. In this paper, we study and\ncompare four personality-aware recommendation systems based on different\npersonality models, namely Big-Five, Eysenck and HEXACO from the personality\ntraits theory, and Myers-Briggs Type Indicator (MPTI) from the personality\ntypes theory. Following that, we propose a hybrid personality model for\nrecommendation that takes advantage of the personality traits models, as well\nas the personality types models. Through extensive experiments on\nrecommendation dataset, we prove the efficiency of the proposed model,\nespecially in cold start settings.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 08:17:55 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dhelim", "Sahraoui", ""], ["Chen", "Liming Luke", ""], ["Aung", "Nyothiri", ""], ["Zhang", "Wenyin", ""], ["Ning", "Huansheng", ""]]}, {"id": "2106.03075", "submitter": "Gil Shabat", "authors": "Dvir Ben Or, Michael Kolomenkin, Gil Shabat", "title": "DL-DDA -- Deep Learning based Dynamic Difficulty Adjustment with UX and\n  Gameplay constraints", "comments": "accepted to IEEE Conference on Games (CoG), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic difficulty adjustment ($DDA$) is a process of automatically changing\na game difficulty for the optimization of user experience. It is a vital part\nof almost any modern game. Most existing DDA approaches concentrate on the\nexperience of a player without looking at the rest of the players. We propose a\nmethod that automatically optimizes user experience while taking into\nconsideration other players and macro constraints imposed by the game. The\nmethod is based on deep neural network architecture that involves a count loss\nconstraint that has zero gradients in most of its support. We suggest a method\nto optimize this loss function and provide theoretical analysis for its\nperformance. Finally, we provide empirical results of an internal experiment\nthat was done on $200,000$ players and was found to outperform the\ncorresponding manual heuristics crafted by game design experts.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 09:47:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Or", "Dvir Ben", ""], ["Kolomenkin", "Michael", ""], ["Shabat", "Gil", ""]]}, {"id": "2106.03364", "submitter": "Zheng Wang", "authors": "Zheng Wang, Satoshi Suga, Edric John Cruz Nacpil, Bo Yang, and\n  Kimihiko Nakano", "title": "Effect of Adaptive and Fixed Shared Steering Control on Distracted\n  Driver Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.HC cs.RO cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driver distraction is a well-known cause for traffic collisions worldwide.\nStudies have indicated that shared steering control, which actively provides\nhaptic guidance torque on the steering wheel, effectively improves the\nperformance of distracted drivers. Recently, adaptive shared steering control\nbased on the physiological status of the driver has been developed, although\nits effect on distracted driver behavior remains unclear. To this end, a\nhigh-fidelity driving simulator experiment was conducted involving 18\nparticipants performing double lane changes. The experimental conditions\ncomprised two driver states: attentive and distracted. Under each condition,\nevaluations were performed on three types of haptic guidance: none (manual),\nfixed authority, and adaptive authority based on feedback from the forearm\nsurface electromyography of the driver. Evaluation results indicated that, for\nboth attentive and distracted drivers, haptic guidance with adaptive authority\nyielded lower driver workload and reduced lane departure risk than manual\ndriving and fixed authority. Moreover, there was a tendency for distracted\ndrivers to reduce grip strength on the steering wheel to follow the haptic\nguidance with fixed authority, resulting in a relatively shorter double lane\nchange duration.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 06:39:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Zheng", ""], ["Suga", "Satoshi", ""], ["Nacpil", "Edric John Cruz", ""], ["Yang", "Bo", ""], ["Nakano", "Kimihiko", ""]]}, {"id": "2106.03371", "submitter": "Waqas Ahmed", "authors": "Sheikh Muhamad Hizam, Waqas Ahmed, Muhammad Fahad, Habiba Akter, Ilham\n  Sentosa, Jawad Ali", "title": "User Behavior Assessment Towards Biometric Facial Recognition System: A\n  SEM-Neural Network Approach", "comments": "15 Pages, 04 Figures, 05 Tables, FICC-2021 Conference 29-30 April\n  2021, Vancouver, Canada", "journal-ref": "2021 - Advances in Intelligent Systems and Computing book series\n  (AISC, volume 1364)", "doi": "10.1007/978-3-030-73103-8_75", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A smart home is grounded on the sensors that endure automation, safety, and\nstructural integration. The security mechanism in digital setup possesses\nvibrant prominence and the biometric facial recognition system is novel\naddition to accrue the smart home features. Understanding the implementation of\nsuch technology is the outcome of user behavior modeling. However, there is the\npaucity of empirical research that explains the role of cognitive, functional,\nand social aspects of end-users acceptance behavior towards biometric facial\nrecognition systems at homes. Therefore, a causal research survey was conducted\nto comprehend the behavioral intention towards the use of a biometric facial\nrecognition system. Technology Acceptance Model (TAM)was implied with Perceived\nSystem Quality (PSQ) and Social Influence (SI)to hypothesize the conceptual\nframework. Data was collected from 475respondents through online\nquestionnaires. Structural Equation Modeling(SEM) and Artificial Neural Network\n(ANN) were employed to analyze the surveyed data. The results showed that all\nthe variables of the proposed framework significantly affected the behavioral\nintention to use the system. The PSQ appeared as the noteworthy predictor\ntowards biometric facial recognition system usability through regression and\nsensitivity analyses. A multi-analytical approach towards understanding the\ntechnology user behavior will support the efficient decision-making process in\nHuman-centric computing.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 06:52:26 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Hizam", "Sheikh Muhamad", ""], ["Ahmed", "Waqas", ""], ["Fahad", "Muhammad", ""], ["Akter", "Habiba", ""], ["Sentosa", "Ilham", ""], ["Ali", "Jawad", ""]]}, {"id": "2106.03797", "submitter": "Hoang D. Nguyen", "authors": "Alex To, Maican Liu, Muhammad Hazeeq Bin Muhammad Hairul, Joseph G.\n  Davis, Jeannie S.A. Lee, Henrik Hesse and Hoang D. Nguyen", "title": "Drone-based AI and 3D Reconstruction for Digital Twin Augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital Twin is an emerging technology at the forefront of Industry 4.0, with\nthe ultimate goal of combining the physical space and the virtual space. To\ndate, the Digital Twin concept has been applied in many engineering fields,\nproviding useful insights in the areas of engineering design, manufacturing,\nautomation, and construction industry. While the nexus of various technologies\nopens up new opportunities with Digital Twin, the technology requires a\nframework to integrate the different technologies, such as the Building\nInformation Model used in the Building and Construction industry. In this work,\nan Information Fusion framework is proposed to seamlessly fuse heterogeneous\ncomponents in a Digital Twin framework from the variety of technologies\ninvolved. This study aims to augment Digital Twin in buildings with the use of\nAI and 3D reconstruction empowered by unmanned aviation vehicles. We proposed a\ndrone-based Digital Twin augmentation framework with reusable and customisable\ncomponents. A proof of concept is also developed, and extensive evaluation is\nconducted for 3D reconstruction and applications of AI for defect detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 03:31:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["To", "Alex", ""], ["Liu", "Maican", ""], ["Hairul", "Muhammad Hazeeq Bin Muhammad", ""], ["Davis", "Joseph G.", ""], ["Lee", "Jeannie S. A.", ""], ["Hesse", "Henrik", ""], ["Nguyen", "Hoang D.", ""]]}, {"id": "2106.03988", "submitter": "Zohreh Shaghaghian", "authors": "Zohreh Shaghaghian, Wei Yan, Dezhen Song", "title": "Towards Learning Geometric Transformations through Play: An AR-powered\n  approach", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3463914.3463915", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the excessive developments of architectural parametric platforms,\nparametric design is often interpreted as an architectural style rather than a\ncomputational method. Also, the problem is still a lack of knowledge and skill\nabout the technical application of parametric design in architectural\nmodelling. Students often dive into utilizing complex digital modelling without\nhaving a competent pedagogical context to learn algorithmic thinking and the\ncorresponding logic behind digital and parametric modelling. The insufficient\nskills and superficial knowledge often result in utilizing the modelling\nsoftware through trial and error, not taking full advantage of what it has to\noffer. Geometric transformations as the fundamental functions of parametric\nmodelling is explored in this study to anchor learning essential components in\nparametric modelling. Students need to understand the differences between\nvariables, parameters, functions and their relations. Fologram, an Augmented\nReality tool, is utilized in this study to learn geometric transformation and\nits components in an intuitive way. A LEGO set is used as an editable physical\nmodel to improve spatial skill through hand movement beside an instant feedback\nin the physical environment.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 22:09:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Shaghaghian", "Zohreh", ""], ["Yan", "Wei", ""], ["Song", "Dezhen", ""]]}, {"id": "2106.04257", "submitter": "Dennis Wolf", "authors": "Dennis Wolf (1), Michael Rietzler (1), Laura Bottner (1) and Enrico\n  Rukzio (1) ((1) Ulm University, Ulm, Germany)", "title": "Augmenting Teleportation in Virtual Reality With Discrete Rotation\n  Angles", "comments": "14 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locomotion is one of the most essential interaction tasks in virtual reality\n(VR) with teleportation being widely accepted as the state-of-the-art\nlocomotion technique at the time of this writing. A major draw-back of\nteleportation is the accompanying physical rotation that is necessary to adjust\nthe users' orientation either before or after teleportation. This is a limiting\nfactor for tethered head-mounted displays (HMDs) and static body postures and\ncan induce additional simulator sickness for HMDs with three degrees-of-freedom\n(DOF) due to missing parallax cues. To avoid physical rotation, previous work\nproposed discrete rotation at fixed intervals (InPlace) as a controller-based\ntechnique with low simulator sickness, yet the impact of varying intervals on\nspatial disorientation, user presence and performance remains to be explored.\nAn unevaluated technique found in commercial VR games is reorientation during\nthe teleportation process (TeleTurn), which prevents physical rotation but\npotentially increases interaction time due to its continuous orientation\nselection. In an exploratory user study, where participants were free to apply\nboth techniques, we evaluated the impact of rotation parameters of either\ntechnique on user performance and preference. Our results indicate that\ndiscrete InPlace rotation introduced no significant spatial disorientation,\nwhile user presence scores were increased. Discrete TeleTurn and teleportation\nwithout rotation was ranked higher and achieved a higher presence score than\ncontinuous TeleTurn, which is the current state-of-the-art found in VR games.\nBased on observations, that participants avoided TeleTurn rotation when\ndiscrete InPlace rotation was available, we distilled guidelines for designing\nteleportation without physical rotation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:30:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wolf", "Dennis", "", "Ulm University, Ulm, Germany"], ["Rietzler", "Michael", "", "Ulm University, Ulm, Germany"], ["Bottner", "Laura", "", "Ulm University, Ulm, Germany"], ["Rukzio", "Enrico", "", "Ulm University, Ulm, Germany"]]}, {"id": "2106.04265", "submitter": "Christoph Anderson", "authors": "Christoph Anderson, Judith Simone Heinisch, Shohreh Deldari, Flora D.\n  Salim, Sandra Ohly, Klaus David, Veljko Pejovic", "title": "Towards Social Role-Based Interruptibility Management", "comments": "24 pages, 9 figures, first submitted on August 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing individuals' attention and interruptibility is still a challenging\ntask in the field of human-computer interaction. Individuals' intrinsic\ninterruptibility preferences are often established for and across different\nsocial roles and life domains, which have not yet been captured by modeling\nshort-term opportunities alone. This paper investigates the applicability of\nsocial role theory and boundary management as theoretical underpinnings for\nanalyzing social roles and their associated interruptibility preferences. We\nconducted an in-the-wild study with 16 participants for five weeks to collect\nindividuals' social roles, interruptibility preferences, application usage and\nspatio-temporal information. A paired t-test shows that interruptibility models\nare significantly improved by incorporating individuals' self-reported social\nroles, achieving a F1 score of 0.73 for classifying 4 different\ninterruptibility preferences. We design and evaluate social role classification\nmodels based on spatio-temporal and application based features. We then\ncombined social role and interruptibility classifiers in a novel two-stage\ninterruptibility model that first infers individuals' social roles to finally\npredict individuals' interruptibility preferences. The two-stage\ninterruptibility model achieves a F1 score of 0.70. Finally, we examine the\ninfluence of multi-device data on social role and interruptibility\nclassification performances. Our findings break new grounds and provide new\ninsights for the design of future interruption management systems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:51:29 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Anderson", "Christoph", ""], ["Heinisch", "Judith Simone", ""], ["Deldari", "Shohreh", ""], ["Salim", "Flora D.", ""], ["Ohly", "Sandra", ""], ["David", "Klaus", ""], ["Pejovic", "Veljko", ""]]}, {"id": "2106.04332", "submitter": "Negar Heidari", "authors": "Negar Heidari and Alexandros Iosifidis", "title": "Progressive Spatio-Temporal Bilinear Network with Monte Carlo Dropout\n  for Landmark-based Facial Expression Recognition with Uncertainty Estimation", "comments": "6 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have been widely used for feature learning in facial\nexpression recognition systems. However, small datasets and large intra-class\nvariability can lead to overfitting. In this paper, we propose a method which\nlearns an optimized compact network topology for real-time facial expression\nrecognition utilizing localized facial landmark features. Our method employs a\nspatio-temporal bilinear layer as backbone to capture the motion of facial\nlandmarks during the execution of a facial expression effectively. Besides, it\ntakes advantage of Monte Carlo Dropout to capture the model's uncertainty which\nis of great importance to analyze and treat uncertain cases. The performance of\nour method is evaluated on three widely used datasets and it is comparable to\nthat of video-based state-of-the-art methods while it has much less complexity.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 13:40:30 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Heidari", "Negar", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2106.04511", "submitter": "Deepak Kumar", "authors": "Deepak Kumar, Patrick Gage Kelley, Sunny Consolvo, Joshua Mason, Elie\n  Bursztein, Zakir Durumeric, Kurt Thomas, Michael Bailey", "title": "Designing Toxic Content Classification for a Diversity of Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we demonstrate how existing classifiers for identifying toxic\ncomments online fail to generalize to the diverse concerns of Internet users.\nWe survey 17,280 participants to understand how user expectations for what\nconstitutes toxic content differ across demographics, beliefs, and personal\nexperiences. We find that groups historically at-risk of harassment - such as\npeople who identify as LGBTQ+ or young adults - are more likely to to flag a\nrandom comment drawn from Reddit, Twitter, or 4chan as toxic, as are people who\nhave personally experienced harassment in the past. Based on our findings, we\nshow how current one-size-fits-all toxicity classification algorithms, like the\nPerspective API from Jigsaw, can improve in accuracy by 86% on average through\npersonalized model tuning. Ultimately, we highlight current pitfalls and new\ndesign directions that can improve the equity and efficacy of toxic content\nclassifiers for all users.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:45:15 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Kumar", "Deepak", ""], ["Kelley", "Patrick Gage", ""], ["Consolvo", "Sunny", ""], ["Mason", "Joshua", ""], ["Bursztein", "Elie", ""], ["Durumeric", "Zakir", ""], ["Thomas", "Kurt", ""], ["Bailey", "Michael", ""]]}, {"id": "2106.04661", "submitter": "Maximilian T. Fischer", "authors": "Maximilian T. Fischer", "title": "Towards a Survey of Visualization Methods for Power Grids", "comments": "11 pages, 6 figures, 1 table, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many visualization techniques have been developed in recent years, few\nhave been applied to power grid visualization. With the ongoing emergence of\nsmart and distributed grids, it becomes increasingly important to improve and\nmodernize legacy infrastructure. Visualization and simulation of power grids\nunder real-life scenarios can help with and sometimes even discover new\napproaches and techniques that significantly improve and simplify network\nanalysis, maintenance, and planning. This enables operators to spot key issues\nwhich are hard to detect otherwise. Creating visualizations for these problems\nis challenging as no obvious or trivial solutions exist, and many of them are\nassociated with multi-dimensional data. This paper aims to provide a\ncomprehensive overview of the methods developed for the visualization of power\ngrids while evaluating the advantages and disadvantages of the single\napproaches. An outlook discusses open research questions and possible further\nimprovements to the field.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 12:21:36 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Fischer", "Maximilian T.", ""]]}, {"id": "2106.04662", "submitter": "Kerstin Bach", "authors": "Kerstin Bach and Paul Jarle Mork", "title": "On the Explanation of Similarity for Developing and Deploying CBR\n  Systems", "comments": "5 pages, 5 figures, Proceedings of the Thirty-Third International\n  Florida Artificial Intelligence Research Society Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the early stages of developing Case-Based Reasoning (CBR) systems the\ndefinition of similarity measures is challenging since this task requires\ntransferring implicit knowledge of domain experts into knowledge\nrepresentations. While an entire CBR system is very explanatory, the similarity\nmeasure determines the ranking but do not necessarily show which features\ncontribute to high (or low) rankings. In this paper we present our work on\nopening the knowledge engineering process for similarity modelling. This work\npresent is a result of an interdisciplinary research collaboration between AI\nand public health researchers developing e-Health applications. During this\nwork explainability and transparency of the development process is crucial to\nallow in-depth quality assurance of the by the domain experts.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:43:27 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bach", "Kerstin", ""], ["Mork", "Paul Jarle", ""]]}, {"id": "2106.04675", "submitter": "Marios Constantinides", "authors": "Melanie Bancilhon, Marios Constantinides, Edyta Paulina Bogucka, Luca\n  Maria Aiello, Daniele Quercia", "title": "Streetonomics: Quantifying Culture Using Street Names", "comments": "17 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantifying a society's value system is important because it suggests what\npeople deeply care about -- it reflects who they actually are and, more\nimportantly, who they will like to be. This cultural quantification has been\ntypically done by studying literary production. However, a society's value\nsystem might well be implicitly quantified based on the decisions that people\ntook in the past and that were mediated by what they care about. It turns out\nthat one class of these decisions is visible in ordinary settings: it is\nvisible in street names. We studied the names of 4,932 honorific streets in the\ncities of Paris, Vienna, London and New York. We chose these four cities\nbecause they were important centers of cultural influence for the Western world\nin the 20th century. We found that street names greatly reflect the extent to\nwhich a society is gender biased, which professions are considered elite ones,\nand the extent to which a city is influenced by the rest of the world. This way\nof quantifying a society's value system promises to inform new methodologies in\nDigital Humanities; makes it possible for municipalities to reflect on their\npast to inform their future; and informs the design of everyday's educational\ntools that promote historical awareness in a playful way.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 20:42:42 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 07:37:01 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Bancilhon", "Melanie", ""], ["Constantinides", "Marios", ""], ["Bogucka", "Edyta Paulina", ""], ["Aiello", "Luca Maria", ""], ["Quercia", "Daniele", ""]]}, {"id": "2106.04684", "submitter": "Scott Cheng-Hsin Yang", "authors": "Tomas Folke, Scott Cheng-Hsin Yang, Sean Anderson, and Patrick Shafto", "title": "Explainable AI for medical imaging: Explaining pneumothorax diagnoses\n  with Bayesian Teaching", "comments": null, "journal-ref": "Proc. SPIE 11746, Artificial Intelligence and Machine Learning for\n  Multi-Domain Operations Applications III, 117462J (2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Limited expert time is a key bottleneck in medical imaging. Due to advances\nin image classification, AI can now serve as decision-support for medical\nexperts, with the potential for great gains in radiologist productivity and, by\nextension, public health. However, these gains are contingent on building and\nmaintaining experts' trust in the AI agents. Explainable AI may build such\ntrust by helping medical experts to understand the AI decision processes behind\ndiagnostic judgements. Here we introduce and evaluate explanations based on\nBayesian Teaching, a formal account of explanation rooted in the cognitive\nscience of human learning. We find that medical experts exposed to explanations\ngenerated by Bayesian Teaching successfully predict the AI's diagnostic\ndecisions and are more likely to certify the AI for cases when the AI is\ncorrect than when it is wrong, indicating appropriate trust. These results show\nthat Explainable AI can be used to support human-AI collaboration in medical\nimaging.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 20:49:11 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Folke", "Tomas", ""], ["Yang", "Scott Cheng-Hsin", ""], ["Anderson", "Sean", ""], ["Shafto", "Patrick", ""]]}, {"id": "2106.04688", "submitter": "Marios Constantinides", "authors": "Edyta Paulina Bogucka, Marios Constantinides, Luca Maria Aiello,\n  Daniele Quercia, Wonyoung So, Melanie Bancilhon", "title": "Cartographic Design of Cultural Maps", "comments": "9 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Throughout history, maps have been used as a tool to explore cities. They\nvisualize a city's urban fabric through its streets, buildings, and points of\ninterest. Besides purely navigation purposes, street names also reflect a\ncity's culture through its commemorative practices. Therefore, cultural maps\nthat unveil socio-cultural characteristics encoded in street names could\npotentially raise citizens' historical awareness. But designing effective\ncultural maps is challenging, not only due to data scarcity but also due to the\nlack of effective approaches to engage citizens with data exploration. To\naddress these challenges, we collected a dataset of 5,000 streets across the\ncities of Paris, Vienna, London, and New York, and built their cultural maps\ngrounded on cartographic storytelling techniques. Through data exploration\nscenarios, we demonstrated how cultural maps engage users and allow them to\ndiscover distinct patterns in the ways these cities are gender-biased,\ncelebrate various professions, and embrace foreign cultures.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 20:53:45 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bogucka", "Edyta Paulina", ""], ["Constantinides", "Marios", ""], ["Aiello", "Luca Maria", ""], ["Quercia", "Daniele", ""], ["So", "Wonyoung", ""], ["Bancilhon", "Melanie", ""]]}, {"id": "2106.05106", "submitter": "Atul Sahay", "authors": "Atul Sahay and Imon Mukherjee and Kavi Arya", "title": "An Efficient Point of Gaze Estimator for Low-Resolution Imaging Systems\n  Using Extracted Ocular Features Based Neural Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A user's eyes provide means for Human Computer Interaction (HCI) research as\nan important modal. The time to time scientific explorations of the eye has\nalready seen an upsurge of the benefits in HCI applications from gaze\nestimation to the measure of attentiveness of a user looking at a screen for a\ngiven time period. The eye tracking system as an assisting, interactive tool\ncan be incorporated by physically disabled individuals, fitted best for those\nwho have eyes as only a limited set of communication. The threefold objective\nof this paper is - 1. To introduce a neural network based architecture to\npredict users' gaze at 9 positions displayed in the 11.31{\\deg} visual range on\nthe screen, through a low resolution based system such as a webcam in real time\nby learning various aspects of eyes as an ocular feature set. 2.A collection of\ncoarsely supervised feature set obtained in real time which is also validated\nthrough the user case study presented in the paper for 21 individuals ( 17 men\nand 4 women ) from whom a 35k set of instances was derived with an accuracy\nscore of 82.36% and f1_score of 82.2% and 3.A detailed study over applicability\nand underlying challenges of such systems. The experimental results verify the\nfeasibility and validity of the proposed eye gaze tracking model.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:35:55 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sahay", "Atul", ""], ["Mukherjee", "Imon", ""], ["Arya", "Kavi", ""]]}, {"id": "2106.05147", "submitter": "Suzan Verberne", "authors": "Ioannis Chios and Suzan Verberne", "title": "Helping results assessment by adding explainable elements to the deep\n  relevance matching model", "comments": "Published in the 3rd International Workshop on ExplainAble\n  Recommendation and Search (EARS 2020), July 30, 2020 Xi'an, China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we address the explainability of web search engines. We propose\ntwo explainable elements on the search engine result page: a visualization of\nquery term weights and a visualization of passage relevance. The idea is that\nsearch engines that indicate to the user why results are retrieved are valued\nhigher by users and gain user trust. We deduce the query term weights from the\nterm gating network in the Deep Relevance Matching Model (DRMM) and visualize\nthem as a doughnut chart. In addition, we train a passage-level ranker with\nDRMM that selects the most relevant passage from each document and shows it as\nsnippet on the result page. Next to the snippet we show a document thumbnail\nwith this passage highlighted. We evaluate the proposed interface in an online\nuser study, asking users to judge the explainability and assessability of the\ninterface. We found that users judge our proposed interface significantly more\nexplainable and easier to assess than a regular search engine result page.\nHowever, they are not significantly better in selecting the relevant documents\nfrom the top-5. This indicates that the explainability of the search engine\nresult page leads to a better user experience. Thus, we conclude that the\nproposed explainable elements are promising as visualization for search engine\nusers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 15:41:54 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chios", "Ioannis", ""], ["Verberne", "Suzan", ""]]}, {"id": "2106.05211", "submitter": "Nour Almadhoun Alserr", "authors": "Nour Almadhoun Alserr, Gulce Kale, Onur Mutlu, Oznur Tastan, Erman\n  Ayday", "title": "Near-Optimal Privacy-Utility Tradeoff in Genomic Studies Using Selective\n  SNP Hiding", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Researchers need a rich trove of genomic datasets that they can\nleverage to gain a better understanding of the genetic basis of the human\ngenome and identify associations between phenotypes and specific parts of DNA.\nHowever, sharing genomic datasets that include sensitive genetic or medical\ninformation of individuals can lead to serious privacy-related consequences if\ndata lands in the wrong hands. Restricting access to genomic datasets is one\nsolution, but this greatly reduces their usefulness for research purposes. To\nallow sharing of genomic datasets while addressing these privacy concerns,\nseveral studies propose privacy-preserving mechanisms for data sharing.\nDifferential privacy (DP) is one of such mechanisms that formalize rigorous\nmathematical foundations to provide privacy guarantees while sharing aggregated\nstatistical information about a dataset. However, it has been shown that the\noriginal privacy guarantees of DP-based solutions degrade when there are\ndependent tuples in the dataset, which is a common scenario for genomic\ndatasets (due to the existence of family members). Results: In this work, we\nintroduce a near-optimal mechanism to mitigate the vulnerabilities of the\ninference attacks on differentially private query results from genomic datasets\nincluding dependent tuples. We propose a utility-maximizing and\nprivacy-preserving approach for sharing statistics by hiding selective SNPs of\nthe family members as they participate in a genomic dataset. By evaluating our\nmechanism on a real-world genomic dataset, we empirically demonstrate that our\nproposed mechanism can achieve up to 40% better privacy than state-of-the-art\nDP-based solutions, while near-optimally minimizing the utility loss.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:55:18 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Alserr", "Nour Almadhoun", ""], ["Kale", "Gulce", ""], ["Mutlu", "Onur", ""], ["Tastan", "Oznur", ""], ["Ayday", "Erman", ""]]}, {"id": "2106.05227", "submitter": "Pardis Emami-Naeini", "authors": "Pardis Emami-Naeini, Tiona Francisco, Tadayoshi Kohno, Franziska\n  Roesner", "title": "Understanding Privacy Attitudes and Concerns Towards Remote\n  Communications During the COVID-19 Pandemic", "comments": "To appear at the 17th Symposium on Usable Privacy and Security\n  (SOUPS'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since December 2019, the COVID-19 pandemic has caused people around the world\nto exercise social distancing, which has led to an abrupt rise in the adoption\nof remote communications for working, socializing, and learning from home. As\nremote communications will outlast the pandemic, it is crucial to protect\nusers' security and respect their privacy in this unprecedented setting, and\nthat requires a thorough understanding of their behaviors, attitudes, and\nconcerns toward various aspects of remote communications. To this end, we\nconducted an online study with 220 worldwide Prolific participants. We found\nthat privacy and security are among the most frequently mentioned factors\nimpacting participants' attitude and comfort level with conferencing tools and\nmeeting locations. Open-ended responses revealed that most participants lacked\nautonomy when choosing conferencing tools or using microphone/webcam in their\nremote meetings, which in several cases contradicted their personal privacy and\nsecurity preferences. Based on our findings, we distill several recommendations\non how employers, educators, and tool developers can inform and empower users\nto make privacy-protective decisions when engaging in remote communications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:17:06 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Emami-Naeini", "Pardis", ""], ["Francisco", "Tiona", ""], ["Kohno", "Tadayoshi", ""], ["Roesner", "Franziska", ""]]}, {"id": "2106.05357", "submitter": "Abhishek Santra", "authors": "Abhishek Santra, Kunal Samant, Endrit Memeti, Enamul Karim and Sharma\n  Chakravarthy", "title": "An Extensible Dashboard Architecture For Visualizing Base And Analyzed\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Any data analysis, especially the data sets that may be changing often or in\nreal-time, consists of at least three important synchronized components: i)\nfiguring out what to infer (objectives), ii) analysis or computation of\nobjectives, and iii) understanding of the results which may require drill-down\nand/or visualization. There is a lot of attention paid to the first two of the\nabove components as part of research whereas the understanding as well as\nderiving actionable decisions is quite tricky. Visualization is an important\nstep towards both understanding (even by non-experts) and inferring the actions\nthat need to be taken. As an example, for Covid-19, knowing regions (say, at\nthe county or state level) that have seen a spike or prone to a spike in cases\nin the near future may warrant additional actions with respect to gatherings,\nbusiness opening hours, etc. This paper focuses on an extensible architecture\nfor visualization of base as well as analyzed data. This paper proposes a\nmodular architecture of a dashboard for user-interaction, visualization\nmanagement, and complex analysis of base data. The contributions of this paper\nare: i) extensibility of the architecture providing flexibility to add\nadditional analysis, visualizations, and user interactions without changing the\nworkflow, ii) decoupling of the functional modules to ease and speedup\ndevelopment by different groups, and iii) address efficiency issues for display\nresponse time. This paper uses Multilayer Networks (or MLNs) for analysis. To\nshowcase the above, we present the implementation of a visualization dashboard,\ntermed CoWiz++ (for Covid Wizard), and elaborate on how web-based user\ninteraction and display components are interfaced seamlessly with the back end\nmodules.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 19:45:43 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Santra", "Abhishek", ""], ["Samant", "Kunal", ""], ["Memeti", "Endrit", ""], ["Karim", "Enamul", ""], ["Chakravarthy", "Sharma", ""]]}, {"id": "2106.05401", "submitter": "Bohan Jiang", "authors": "Bohan Jiang, Mansooreh Karami, Lu Cheng, Tyler Black and Huan Liu", "title": "Mechanisms and Attributes of Echo Chambers in Social Media", "comments": "10 pages, 2 figures, SBP-BRiMS 2021 (2021 International Conference on\n  Social Computing, Behavioral-Cultural Modeling, & Prediction and Behavior\n  Representation in Modeling and Simulation), working/late-breaking paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo chambers may exclude social media users from being exposed to other\nopinions, therefore, can cause rampant negative effects. Among abundant\nevidence are the 2016 and 2020 US presidential elections conspiracy theories\nand polarization, as well as the COVID-19 disinfodemic. To help better detect\necho chambers and mitigate its negative effects, this paper explores the\nmechanisms and attributes of echo chambers in social media. In particular, we\nfirst illustrate four primary mechanisms related to three main factors: human\npsychology, social networks, and automatic systems. We then depict common\nattributes of echo chambers with a focus on the diffusion of misinformation,\nspreading of conspiracy theory, creation of social trends, political\npolarization, and emotional contagion of users. We illustrate each mechanism\nand attribute in a multi-perspective of sociology, psychology, and social\ncomputing with recent case studies. Our analysis suggest an emerging need to\ndetect echo chambers and mitigate their negative effects.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:34:01 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 19:16:21 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Jiang", "Bohan", ""], ["Karami", "Mansooreh", ""], ["Cheng", "Lu", ""], ["Black", "Tyler", ""], ["Liu", "Huan", ""]]}, {"id": "2106.05532", "submitter": "Anjana Arunkumar", "authors": "Swaroop Mishra, Anjana Arunkumar", "title": "How Robust are Model Rankings: A Leaderboard Customization Approach for\n  Equitable Evaluation", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models that top leaderboards often perform unsatisfactorily when deployed in\nreal world applications; this has necessitated rigorous and expensive\npre-deployment model testing. A hitherto unexplored facet of model performance\nis: Are our leaderboards doing equitable evaluation? In this paper, we\nintroduce a task-agnostic method to probe leaderboards by weighting samples\nbased on their `difficulty' level. We find that leaderboards can be\nadversarially attacked and top performing models may not always be the best\nmodels. We subsequently propose alternate evaluation metrics. Our experiments\non 10 models show changes in model ranking and an overall reduction in\npreviously reported performance -- thus rectifying the overestimation of AI\nsystems' capabilities. Inspired by behavioral testing principles, we further\ndevelop a prototype of a visual analytics tool that enables leaderboard\nrevamping through customization, based on an end user's focus area. This helps\nusers analyze models' strengths and weaknesses, and guides them in the\nselection of a model best suited for their application scenario. In a user\nstudy, members of various commercial product development teams, covering 5\nfocus areas, find that our prototype reduces pre-deployment development and\ntesting effort by 41% on average.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 06:47:35 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Mishra", "Swaroop", ""], ["Arunkumar", "Anjana", ""]]}, {"id": "2106.05568", "submitter": "Julie Gerlings", "authors": "Julie Gerlings, Millie S{\\o}ndergaard Jensen and Arisa Shollo", "title": "Explainable AI, but explainable to whom?", "comments": "accepted book chapter for AI in Healthcare DOI TBD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in AI technologies have resulted in superior levels of AI-based\nmodel performance. However, this has also led to a greater degree of model\ncomplexity, resulting in 'black box' models. In response to the AI black box\nproblem, the field of explainable AI (xAI) has emerged with the aim of\nproviding explanations catered to human understanding, trust, and transparency.\nYet, we still have a limited understanding of how xAI addresses the need for\nexplainable AI in the context of healthcare. Our research explores the\ndiffering explanation needs amongst stakeholders during the development of an\nAI-system for classifying COVID-19 patients for the ICU. We demonstrate that\nthere is a constellation of stakeholders who have different explanation needs,\nnot just the 'user'. Further, the findings demonstrate how the need for xAI\nemerges through concerns associated with specific stakeholder groups i.e., the\ndevelopment team, subject matter experts, decision makers, and the audience.\nOur findings contribute to the expansion of xAI by highlighting that different\nstakeholders have different explanation needs. From a practical perspective,\nthe study provides insights on how AI systems can be adjusted to support\ndifferent stakeholders needs, ensuring better implementation and operation in a\nhealthcare context.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 07:47:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Gerlings", "Julie", ""], ["Jensen", "Millie S\u00f8ndergaard", ""], ["Shollo", "Arisa", ""]]}, {"id": "2106.05700", "submitter": "Pradipta Biswas", "authors": "Gowdham Prabhakar, Priyam Rajkhowa and Pradipta Biswas", "title": "A Wearable Virtual Touch System for Cars", "comments": "Journal on Multimodal User Interface 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In automotive domain, operation of secondary tasks like accessing\ninfotainment system, adjusting air conditioning vents, and side mirrors\ndistract drivers from driving. Though existing modalities like gesture and\nspeech recognition systems facilitate undertaking secondary tasks by reducing\nduration of eyes off the road, those often require remembering a set of\ngestures or screen sequences. In this paper, we have proposed two different\nmodalities for drivers to virtually touch the dashboard display using a laser\ntracker with a mechanical switch and an eye gaze switch. We compared\nperformances of our proposed modalities against conventional touch modality in\nautomotive environment by comparing pointing and selection times of\nrepresentative secondary task and also analysed effect on driving performance\nin terms of deviation from lane, average speed, variation in perceived workload\nand system usability. We did not find significant difference in driving and\npointing performance between laser tracking system and existing touchscreen\nsystem. Our result also showed that the driving and pointing performance of the\nvirtual touch system with eye gaze switch was significantly better than the\nsame with mechanical switch. We evaluated the efficacy of the proposed virtual\ntouch system with eye gaze switch inside a real car and investigated acceptance\nof the system by professional drivers using qualitative research. The\nquantitative and qualitative studies indicated importance of using multimodal\nsystem inside car and highlighted several criteria for acceptance of new\nautomotive user interface.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 12:37:40 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Prabhakar", "Gowdham", ""], ["Rajkhowa", "Priyam", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2106.05854", "submitter": "Neziha Akalin", "authors": "Neziha Akalin, Annica Kristoffersson, Amy Loutfi", "title": "Investigating the Multidisciplinary Perspective of Perceived Safety in\n  Human-Robot Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Safety is a key issue in human-robot interaction, but perceived safety has\nnot been well-studied in comparison to physical safety. In this paper, we\naddress the multidisciplinary perspective of perceived safety in human-robot\ninteraction. To investigate how the comfort of the user, sense of control of\nthe user, unpredictable robot behaviors, and trust impact the safety perception\nof the user, we designed a randomized controlled within-subject experiment. We\ndevised five different experimental conditions in which we investigated the\nrelationships between perceived safety and comfort, sense of control, and\ntrust. In each condition, we modified one factor. To extend our previous\nfindings, the participants were asked to answer questionnaires that measure\ncomfort, sense of control, trust, and perceived safety. The questionnaire\nresults show a strong correlation between these factors and the perceived\nsafety. Since these factors are the main factors that influence perceived\nsafety, they should be considered in human-robot interaction design decisions.\nThe effect of individual characteristics such as personality and gender on\nperceived safety was also discussed. Moreover, we analyzed the facial affect\nand physiological signals of the participants for predicting perceived safety\nfrom objective measures. The data from objective measures revealed that\nphysiological signals give better prediction of perceived safety rather than\nfacial affect data. We believe this article can play an important role in the\ngoal of better understanding perceived safety in human-robot interaction.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:56:12 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Akalin", "Neziha", ""], ["Kristoffersson", "Annica", ""], ["Loutfi", "Amy", ""]]}, {"id": "2106.05917", "submitter": "Adam Aviv", "authors": "David G. Balash and Dongkun Kim and Darikia Shaibekova and Rahel A.\n  Fainchtein and Micah Sherr and Adam J. Aviv", "title": "Examining the Examiners: Students' Privacy and Security Perceptions of\n  Online Proctoring Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response to the Covid-19 pandemic, educational institutions quickly\ntransitioned to remote learning. The problem of how to perform student\nassessment in an online environment has become increasingly relevant, leading\nmany institutions and educators to turn to online proctoring services to\nadminister remote exams. These services employ various student monitoring\nmethods to curb cheating, including restricted (\"lockdown\") browser modes,\nvideo/screen monitoring, local network traffic analysis, and eye tracking. In\nthis paper, we explore the security and privacy perceptions of the student\ntest-takers being proctored. We analyze user reviews of proctoring services'\nbrowser extensions and subsequently perform an online survey (n=102). Our\nfindings indicate that participants are concerned about both the amount and the\npersonal nature of the information shared with the exam proctoring companies.\nHowever, many participants also recognize a trade-off between pandemic safety\nconcerns and the arguably invasive means by which proctoring services ensure\nexam integrity. Our findings also suggest that institutional power dynamics and\nstudents' trust in their institutions may dissuade students' opposition to\nremote proctoring.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:03:13 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Balash", "David G.", ""], ["Kim", "Dongkun", ""], ["Shaibekova", "Darikia", ""], ["Fainchtein", "Rahel A.", ""], ["Sherr", "Micah", ""], ["Aviv", "Adam J.", ""]]}, {"id": "2106.06053", "submitter": "Tasos Spiliotopoulos", "authors": "Tasos Spiliotopoulos, Dave Horsfall, Magdalene Ng, Kovila Coopamootoo,\n  Aad van Moorsel, Karen Elliott", "title": "Identifying and Supporting Financially Vulnerable Consumers in a\n  Privacy-Preserving Manner: A Use Case Using Decentralised Identifiers and\n  Verifiable Credentials", "comments": "Published in the ACM CHI 2021 workshop on Designing for New Forms of\n  Vulnerability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vulnerable individuals have a limited ability to make reasonable financial\ndecisions and choices and, thus, the level of care that is appropriate to be\nprovided to them by financial institutions may be different from that required\nfor other consumers. Therefore, identifying vulnerability is of central\nimportance for the design and effective provision of financial services and\nproducts. However, validating the information that customers share and\nrespecting their privacy are both particularly important in finance and this\nposes a challenge for identifying and caring for vulnerable populations. This\nposition paper examines the potential of the combination of two emerging\ntechnologies, Decentralized Identifiers (DIDs) and Verifiable Credentials\n(VCs), for the identification of vulnerable consumers in finance in an\nefficient and privacy-preserving manner.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 21:05:34 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Spiliotopoulos", "Tasos", ""], ["Horsfall", "Dave", ""], ["Ng", "Magdalene", ""], ["Coopamootoo", "Kovila", ""], ["van Moorsel", "Aad", ""], ["Elliott", "Karen", ""]]}, {"id": "2106.06255", "submitter": "Hao Cheng", "authors": "Monika Sester, Mark Vollrath and Hao Cheng", "title": "Improving Take-over Situation by Active Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this short paper an idea is sketched, how to support drivers of an\nautonomous vehicle in taking back control of the vehicle after a longer section\nof autonomous cruising. The hypothesis is that a clear communication about the\nlocation and behavior of relevant objects in the environment will help the\ndriver to quickly grasp the situational context and thus support drivers in\nsafely handling the ongoing driving situation manually after take-over. Based\non this hypothesis, a research concept is sketched, which entails the necessary\ncomponents as well as the disciplines involved.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:14:59 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Sester", "Monika", ""], ["Vollrath", "Mark", ""], ["Cheng", "Hao", ""]]}, {"id": "2106.06261", "submitter": "Benedikt Hosp", "authors": "Benedikt Hosp, Myat Su Yin, peter Haddawy, Ratthapoom Watcharporas,\n  paphon Sa-ngasoonsong, Enkelejda Kasneci", "title": "States of confusion: Eye and Head tracking reveal surgeons' confusion\n  during arthroscopic surgery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  During arthroscopic surgeries, surgeons are faced with challenges like\ncognitive re-projection of the 2D screen output into the 3D operating site or\nnavigation through highly similar tissue. Training of these cognitive processes\ntakes much time and effort for young surgeons, but is necessary and crucial for\ntheir education. In this study we want to show how to recognize states of\nconfusion of young surgeons during an arthroscopic surgery, by looking at their\neye and head movements and feeding them to a machine learning model. With an\naccuracy of over 94\\% and detection speed of 0.039 seconds, our model is a step\ntowards online diagnostic and training systems for the perceptual-cognitive\nprocesses of surgeons during arthroscopic surgeries.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:22:49 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hosp", "Benedikt", ""], ["Yin", "Myat Su", ""], ["Haddawy", "peter", ""], ["Watcharporas", "Ratthapoom", ""], ["Sa-ngasoonsong", "paphon", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2106.06332", "submitter": "Vivek Ramachandran", "authors": "Vivek Ramachandran, Fabian Schilling, Amy R Wu, Dario Floreano", "title": "Smart Textiles that Teach: Fabric-Based Haptic Device Improves the Rate\n  of Motor Learning", "comments": null, "journal-ref": null, "doi": "10.1002/aisy.202100043", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  People learn motor activities best when they are conscious of their errors\nand make a concerted effort to correct them. While haptic interfaces can\nfacilitate motor training, existing interfaces are often bulky and do not\nalways ensure post-training skill retention. Here, we describe a programmable\nhaptic sleeve composed of textile-based electroadhesive clutches for skill\nacquisition and retention. We show its functionality in a motor learning study\nwhere users control a drone's movement using elbow joint rotation. Haptic\nfeedback is used to restrain elbow motion and make users aware of their errors.\nThis helps users consciously learn to avoid errors from occurring. While all\nsubjects exhibited similar performance during the baseline phase of motor\nlearning, those subjects who received haptic feedback from the haptic sleeve\ncommitted 23.5% fewer errors than subjects in the control group during the\nevaluation phase. The results show that the sleeve helps users retain and\ntransfer motor skills better than visual feedback alone. This work shows the\npotential for fabric-based haptic interfaces as a training aid for motor tasks\nin the fields of rehabilitation and teleoperation.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 12:08:55 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 18:47:10 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Ramachandran", "Vivek", ""], ["Schilling", "Fabian", ""], ["Wu", "Amy R", ""], ["Floreano", "Dario", ""]]}, {"id": "2106.06334", "submitter": "Maximilian T. Fischer", "authors": "Maximilian T. Fischer, Daniel Seebacher, Rita Sevastjanova, Daniel A.\n  Keim, Mennatallah El-Assady", "title": "CommAID: Visual Analytics for Communication Analysis through Interactive\n  Dynamics Modeling", "comments": "12 pages, 7 figures, Computer Graphics Forum 2021 (pre-peer reviewed\n  version)", "journal-ref": "Computer Graphics Forum, 40(3), 2021", "doi": "10.1111/cgf.14286", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication consists of both meta-information as well as content.\nCurrently, the automated analysis of such data often focuses either on the\nnetwork aspects via social network analysis or on the content, utilizing\nmethods from text-mining. However, the first category of approaches does not\nleverage the rich content information, while the latter ignores the\nconversation environment and the temporal evolution, as evident in the\nmeta-information. In contradiction to communication research, which stresses\nthe importance of a holistic approach, both aspects are rarely applied\nsimultaneously, and consequently, their combination has not yet received enough\nattention in automated analysis systems. In this work, we aim to address this\nchallenge by discussing the difficulties and design decisions of such a path as\nwell as contribute CommAID, a blueprint for a holistic strategy to\ncommunication analysis. It features an integrated visual analytics design to\nanalyze communication networks through dynamics modeling, semantic pattern\nretrieval, and a user-adaptable and problem-specific machine learning-based\nretrieval system. An interactive multi-level matrix-based visualization\nfacilitates a focused analysis of both network and content using inline visuals\nsupporting cross-checks and reducing context switches. We evaluate our approach\nin both a case study and through formative evaluation with eight law\nenforcement experts using a real-world communication corpus. Results show that\nour solution surpasses existing techniques in terms of integration level and\napplicability. With this contribution, we aim to pave the path for a more\nholistic approach to communication analysis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 12:13:38 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Fischer", "Maximilian T.", ""], ["Seebacher", "Daniel", ""], ["Sevastjanova", "Rita", ""], ["Keim", "Daniel A.", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "2106.06403", "submitter": "Christiane Plociennik", "authors": "Hooman Tavakoli, Snehal Walunj, Parsha Pahlevannejad, Christiane\n  Plociennik, and Martin Ruskowski", "title": "Small Object Detection for Near Real-Time Egocentric Perception in a\n  Manual Assembly Scenario", "comments": "Accepted for presentation at EPIC@CVPR2021 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting small objects in video streams of head-worn augmented reality\ndevices in near real-time is a huge challenge: training data is typically\nscarce, the input video stream can be of limited quality, and small objects are\nnotoriously hard to detect. In industrial scenarios, however, it is often\npossible to leverage contextual knowledge for the detection of small objects.\nFurthermore, CAD data of objects are typically available and can be used to\ngenerate synthetic training data. We describe a near real-time small object\ndetection pipeline for egocentric perception in a manual assembly scenario: We\ngenerate a training data set based on CAD data and realistic backgrounds in\nUnity. We then train a YOLOv4 model for a two-stage detection process: First,\nthe context is recognized, then the small object of interest is detected. We\nevaluate our pipeline on the augmented reality device Microsoft Hololens 2.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:59:44 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Tavakoli", "Hooman", ""], ["Walunj", "Snehal", ""], ["Pahlevannejad", "Parsha", ""], ["Plociennik", "Christiane", ""], ["Ruskowski", "Martin", ""]]}, {"id": "2106.06498", "submitter": "Matteo Antonio Scrugli", "authors": "Matteo Antonio Scrugli, Daniela Loi, Luigi Raffo, Paolo Meloni", "title": "An adaptive cognitive sensor node for ECG monitoring in the Internet of\n  Medical Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Internet of Medical Things (IoMT) paradigm is becoming mainstream in\nmultiple clinical trials and healthcare procedures. Cardiovascular diseases\nmonitoring, usually involving electrocardiogram (ECG) traces analysis, is one\nof the most promising and high-impact applications. Nevertheless, to fully\nexploit the potential of IoMT in this domain, some steps forward are needed.\nFirst, the edge-computing paradigm must be added to the picture. A certain\nlevel of near-sensor processing has to be enabled, to improve the scalability,\nportability, reliability, responsiveness of the IoMT nodes. Second, novel,\nincreasingly accurate, data analysis algorithms, such as those based on\nartificial intelligence and Deep Learning, must be exploited. To reach these\nobjectives, designers and programmers of IoMT nodes, have to face challenging\noptimization tasks, in order to execute fairly complex computing tasks on\nlow-power wearable and portable processing systems, with tight power and\nbattery lifetime budgets. In this work, we explore the implementation of a\ncognitive data analysis algorithm, based on a convolutional neural network\ntrained to classify ECG waveforms, on a resource-constrained\nmicrocontroller-based computing platform. To minimize power consumption, we add\nan adaptivity layer that dynamically manages the hardware and software\nconfiguration of the device to adapt it at runtime to the required operating\nmode. Our experimental results show that adapting the node setup to the\nworkload at runtime can save up to 50% power consumption. Our optimized and\nquantized neural network reaches an accuracy value higher than 97% for\narrhythmia disorders detection on MIT-BIH Arrhythmia dataset.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 16:49:10 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 15:15:18 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Scrugli", "Matteo Antonio", ""], ["Loi", "Daniela", ""], ["Raffo", "Luigi", ""], ["Meloni", "Paolo", ""]]}, {"id": "2106.06604", "submitter": "Mario Gleirscher", "authors": "Mario Gleirscher, Radu Calinescu, James Douthwaite, Benjamin Lesage,\n  Colin Paterson, Jonathan Aitken, Rob Alexander, James Law", "title": "Verified Synthesis of Optimal Safety Controllers for Human-Robot\n  Collaboration", "comments": "34 pages, 31 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.SE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a tool-supported approach for the synthesis, verification and\nvalidation of the control software responsible for the safety of the\nhuman-robot interaction in manufacturing processes that use collaborative\nrobots. In human-robot collaboration, software-based safety controllers are\nused to improve operational safety, e.g., by triggering shutdown mechanisms or\nemergency stops to avoid accidents. Complex robotic tasks and increasingly\nclose human-robot interaction pose new challenges to controller developers and\ncertification authorities. Key among these challenges is the need to assure the\ncorrectness of safety controllers under explicit (and preferably weak)\nassumptions. Our controller synthesis, verification and validation approach is\ninformed by the process, risk analysis, and relevant safety regulations for the\ntarget application. Controllers are selected from a design space of feasible\ncontrollers according to a set of optimality criteria, are formally verified\nagainst correctness criteria, and are translated into executable code and\nvalidated in a digital twin. The resulting controller can detect the occurrence\nof hazards, move the process into a safe state, and, in certain circumstances,\nreturn the process to an operational state from which it can resume its\noriginal task. We show the effectiveness of our software engineering approach\nthrough a case study involving the development of a safety controller for a\nmanufacturing work cell equipped with a collaborative robot.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:38:40 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gleirscher", "Mario", ""], ["Calinescu", "Radu", ""], ["Douthwaite", "James", ""], ["Lesage", "Benjamin", ""], ["Paterson", "Colin", ""], ["Aitken", "Jonathan", ""], ["Alexander", "Rob", ""], ["Law", "James", ""]]}, {"id": "2106.06655", "submitter": "Eleftherios Triantafyllidis Mr.", "authors": "Eleftherios Triantafyllidis, Wenbin Hu, Christopher McGreavy and\n  Zhibin Li", "title": "Metrics for 3D Object Pointing and Manipulation in Virtual Reality", "comments": "Accepted and to appear at the IEEE Robotics & Automation Magazine.\n  The manuscript has 14 pages, 6 figures, 5 tables, 10 equations and 22\n  references in total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the performance of human movements during teleoperation and virtual\nreality is a challenging problem, particularly in 3D space due to complex\nspatial settings. Despite the presence of a multitude of metrics, a compelling\nstandardized 3D metric is yet missing, aggravating inter-study comparability\nbetween different studies. Hence, evaluating human performance in virtual\nenvironments is a long-standing research goal, and a performance metric that\ncombines two or more metrics under one formulation remains largely unexplored,\nparticularly in higher dimensions. The absence of such a metric is primarily\nattributed to the discrepancies between pointing and manipulation, the complex\nspatial variables in 3D, and the combination of translational and rotational\nmovements altogether. In this work, four experiments were designed and\nconducted with progressively higher spatial complexity to study and compare\nexisting metrics thoroughly. The research goal was to quantify the difficulty\nof these 3D tasks and model human performance sufficiently in full 3D\nperipersonal space. Consequently, a new model extension has been proposed and\nits applicability has been validated across all the experimental results,\nshowing improved modelling and representation of human performance in combined\nmovements of 3D object pointing and manipulation tasks than existing work.\nLastly, the implications on 3D interaction, teleoperation and object task\ndesign in virtual reality are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 01:05:50 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Triantafyllidis", "Eleftherios", ""], ["Hu", "Wenbin", ""], ["McGreavy", "Christopher", ""], ["Li", "Zhibin", ""]]}, {"id": "2106.06831", "submitter": "Omri Suissa", "authors": "Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet", "title": "Toward the Optimized Crowdsourcing Strategy for OCR Post-Correction", "comments": "25 pages, 12 figures, 1 table", "journal-ref": "Aslib Journal of Information Management, Vol. 72 No. 2, pp.\n  179-197 (2020)", "doi": "10.1108/AJIM-07-2019-0189", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Digitization of historical documents is a challenging task in many digital\nhumanities projects. A popular approach for digitization is to scan the\ndocuments into images, and then convert images into text using Optical\nCharacter Recognition (OCR) algorithms. However, the outcome of OCR processing\nof historical documents is usually inaccurate and requires post-processing\nerror correction. This study investigates how crowdsourcing can be utilized to\ncorrect OCR errors in historical text collections, and which crowdsourcing\nmethodology is the most effective in different scenarios and for various\nresearch objectives. A series of experiments with different micro-task's\nstructures and text lengths was conducted with 753 workers on the Amazon's\nMechanical Turk platform. The workers had to fix OCR errors in a selected\nhistorical text. To analyze the results, new accuracy and efficiency measures\nhave been devised. The analysis suggests that in terms of accuracy, the optimal\ntext length is medium (paragraph-size) and the optimal structure of the\nexperiment is two-phase with a scanned image. In terms of efficiency, the best\nresults were obtained when using longer text in the single-stage structure with\nno image. The study provides practical recommendations to researchers on how to\nbuild the optimal crowdsourcing task for OCR post-correction. The developed\nmethodology can also be utilized to create golden standard historical texts for\nautomatic OCR post-correction. This is the first attempt to systematically\ninvestigate the influence of various factors on crowdsourcing-based OCR\npost-correction and propose an optimal strategy for this process.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 18:42:01 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Suissa", "Omri", ""], ["Elmalech", "Avshalom", ""], ["Zhitomirsky-Geffet", "Maayan", ""]]}, {"id": "2106.06907", "submitter": "Linan Huang", "authors": "Linan Huang and Quanyan Zhu", "title": "INADVERT: An Interactive and Adaptive Counterdeception Platform for\n  Attention Enhancement and Phishing Prevention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deceptive attacks exploiting the innate and the acquired vulnerabilities of\nhuman users have posed severe threats to information and infrastructure\nsecurity. This work proposes INADVERT, a systematic solution that generates\ninteractive visual aids in real-time to prevent users from inadvertence and\ncounter visual-deception attacks. Based on the eye-tracking outcomes and proper\ndata compression, the INADVERT platform automatically adapts the visual aids to\nthe user's varying attention status captured by the gaze location and duration.\nWe extract system-level metrics to evaluate the user's average attention level\nand characterize the magnitude and frequency of the user's mind-wandering\nbehaviors. These metrics contribute to an adaptive enhancement of the user's\nattention through reinforcement learning. To determine the optimal\nhyper-parameters in the attention enhancement mechanism, we develop an\nalgorithm based on Bayesian optimization to efficiently update the design of\nthe INADVERT platform and maximize the accuracy of the users' phishing\nrecognition.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 03:52:55 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Huang", "Linan", ""], ["Zhu", "Quanyan", ""]]}, {"id": "2106.07234", "submitter": "Walter Morales-Alvarez", "authors": "Walter Morales-Alvarez, Mohamed Marouf, Hadj. Hamma Tadjine, Cristina\n  Olaverri-Monreal", "title": "Real-World Evaluation of the Impact of Automated Driving System\n  Technology on Driver Gaze Behavior, Reaction Time and Trust", "comments": "Submitted and accepted in the IEEE Intelligent Vehicle Symposium 2021\n  Conference (IV2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in advanced driving assistance systems (ADAS) that rely\non some level of autonomy have led the automobile industry and research\ncommunity to investigate the impact they might have on driving performance.\nHowever, most of the research performed so far is based on simulated\nenvironments. In this study, we investigated the behavior of drivers in a\nvehicle with automated driving system (ADS) capabilities in a real-life driving\nscenario. We analyzed their response to a take over request (TOR) at two\ndifferent driving speeds while being engaged in non-driving-related tasks\n(NDRT). Results from the performed experiments showed that driver reaction time\nto a TOR, gaze behavior and self-reported trust in automation were affected by\nthe type of NDRT being concurrently performed and driver reaction time and gaze\nbehavior additionally depended on the driving or vehicle speed at the time of\nTOR.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 08:42:17 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Morales-Alvarez", "Walter", ""], ["Marouf", "Mohamed", ""], ["Tadjine", "Hadj. Hamma", ""], ["Olaverri-Monreal", "Cristina", ""]]}, {"id": "2106.07257", "submitter": "Abhishek Kaushik Mr.", "authors": "Mahak Sharma (1), Abhishek Kaushik (2), Rajesh Kumar (3), Sushant\n  Kumar Rai (3), Harshada Hanumant Desai (3) and Sargam Yadav (3) ((1) Vidhya\n  Bhawan Gandhiyan Institute of Educational Studies,(2) Dublin City University,\n  Ireland,(3) Dublin Business School, Dublin, Ireland)", "title": "Communication is the universal solvent: atreya bot -- an interactive bot\n  for chemical scientists", "comments": "IFIP 9.4 2021 1st Virtual Conference Conference Theme: Resilient\n  ICT4D May 25th 28th, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conversational agents are a recent trend in human-computer interaction,\ndeployed in multidisciplinary applications to assist the users. In this paper,\nwe introduce \"Atreya\", an interactive bot for chemistry enthusiasts,\nresearchers, and students to study the ChEMBL database. Atreya is hosted by\nTelegram, a popular cloud-based instant messaging application. This\nuser-friendly bot queries the ChEMBL database, retrieves the drug details for a\nparticular disease, targets associated with that drug, etc. This paper explores\nthe potential of using a conversational agent to assist chemistry students and\nchemical scientist in complex information seeking process.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 09:20:44 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sharma", "Mahak", ""], ["Kaushik", "Abhishek", ""], ["Kumar", "Rajesh", ""], ["Rai", "Sushant Kumar", ""], ["Desai", "Harshada Hanumant", ""], ["Yadav", "Sargam", ""]]}, {"id": "2106.07408", "submitter": "Pradipta Biswas", "authors": "Archana Hebbar, Abhay Pashilkar and Pradipta Biswas", "title": "Using Eye Tracker To Evaluate Cockpit Design -- A Flight Simulation\n  Study", "comments": null, "journal-ref": "Aviation 2022", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates applications of eye tracking in transport aircraft\ndesign evaluations. Piloted simulations were conducted for a complete flight\nprofile including take off, cruise and landing flight scenario using the\ntransport aircraft flight simulator at CSIR National Aerospace Laboratories.\nThirty-one simulation experiments were carried out with three pilots and\nengineers while recording the ocular parameters and the flight data.\nSimulations were repeated for high workload conditions like flying with\ndegraded visibility and during stall. Pilots visual scan behaviour and workload\nlevels were analysed using ocular parameters; while comparing with the\nstatistical deviations from the desired flight path. Conditions for fatigue\nwere also recreated through long duration simulations and signatures for the\nsame from the ocular parameters were assessed. Results from the study found\ncorrelation between the statistical inferences obtained from the ocular\nparameters with those obtained from the flight path deviations. The paper also\ndemonstrates an evaluators console that assists the designers or evaluators for\nbetter understanding of pilots attentional resource allocation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:31:59 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hebbar", "Archana", ""], ["Pashilkar", "Abhay", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2106.07553", "submitter": "Sara Kingsley", "authors": "Sara Kingsley", "title": "A Cognitive Science perspective for learning how to design meaningful\n  user experiences and human-centered technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper reviews literature in cognitive science, human-computer\ninteraction (HCI) and natural-language processing (NLP) to consider how\nanalogical reasoning (AR) could help inform the design of communication and\nlearning technologies, as well as online communities and digital platforms.\nFirst, analogical reasoning (AR) is defined, and use-cases of AR in the\ncomputing sciences are presented. The concept of schema is introduced, along\nwith use-cases in computing. Finally, recommendations are offered for future\nwork on using analogical reasoning and schema methods in the computing\nsciences.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:00:50 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Kingsley", "Sara", ""]]}, {"id": "2106.07615", "submitter": "Dipu Manandhar", "authors": "Dipu Manandhar, Hailin Jin, John Collomosse", "title": "Magic Layouts: Structural Prior for Component Detection in User\n  Interface Designs", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Magic Layouts; a method for parsing screenshots or hand-drawn\nsketches of user interface (UI) layouts. Our core contribution is to extend\nexisting detectors to exploit a learned structural prior for UI designs,\nenabling robust detection of UI components; buttons, text boxes and similar.\nSpecifically we learn a prior over mobile UI layouts, encoding common spatial\nco-occurrence relationships between different UI components. Conditioning\nregion proposals using this prior leads to performance gains on UI layout\nparsing for both hand-drawn UIs and app screenshots, which we demonstrate\nwithin the context an interactive application for rapidly acquiring digital\nprototypes of user experience (UX) designs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:20:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Manandhar", "Dipu", ""], ["Jin", "Hailin", ""], ["Collomosse", "John", ""]]}, {"id": "2106.07645", "submitter": "Soha Rostaminia", "authors": "Soha Rostaminia, S. Zohreh Homayounfar, Ali Kiaghadi, Trisha L.\n  Andrew, Deepak Ganesan", "title": "PhyMask: Robust Sensing of Brain Activity and Physiological Signals\n  During Sleep with an All-textile Eye Mask", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical-grade wearable sleep monitoring is a challenging problem since it\nrequires concurrently monitoring brain activity, eye movement, muscle activity,\ncardio-respiratory features and gross body movements. This requires multiple\nsensors to be worn at different locations as well as uncomfortable adhesives\nand discrete electronic components to be placed on the head. As a result,\nexisting wearables either compromise comfort or compromise accuracy in tracking\nsleep variables. We propose PhyMask, an all-textile sleep monitoring solution\nthat is practical and comfortable for continuous use and that acquires all\nsignals of interest to sleep solely using comfortable textile sensors placed on\nthe head. We show that PhyMask can be used to accurately measure sleep stages\nand advanced sleep markers such as spindles and k-complexes robustly in the\nreal-world setting. We validate PhyMask against polysomnography and show that\nit significantly outperforms two commercially-available sleep tracking\nwearables, Fitbit and Oura Ring.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 00:42:33 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Rostaminia", "Soha", ""], ["Homayounfar", "S. Zohreh", ""], ["Kiaghadi", "Ali", ""], ["Andrew", "Trisha L.", ""], ["Ganesan", "Deepak", ""]]}, {"id": "2106.07817", "submitter": "Stefan Winkler", "authors": "Vassilios Vonikakis and Stefan Winkler", "title": "Efficient Facial Expression Analysis For Dimensional Affect Recognition\n  Using Geometric Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite their continued popularity, categorical approaches to affect\nrecognition have limitations, especially in real-life situations. Dimensional\nmodels of affect offer important advantages for the recognition of subtle\nexpressions and more fine-grained analysis. We introduce a simple but effective\nfacial expression analysis (FEA) system for dimensional affect, solely based on\ngeometric features and Partial Least Squares (PLS) regression. The system\njointly learns to estimate Arousal and Valence ratings from a set of facial\nimages. The proposed approach is robust, efficient, and exhibits comparable\nperformance to contemporary deep learning models, while requiring a fraction of\nthe computational resources.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 00:28:16 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Vonikakis", "Vassilios", ""], ["Winkler", "Stefan", ""]]}, {"id": "2106.07867", "submitter": "Rajesh Kumar", "authors": "Mohit Agrawal and Pragyan Mehrotra and Rajesh Kumar and Rajiv Ratn\n  Shah", "title": "Defending Touch-based Continuous Authentication Systems from Active\n  Adversaries Using Generative Adversarial Networks", "comments": "2021 IEEE International Joint Conference on Biometrics (IJCB), 8\n  pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previous studies have demonstrated that commonly studied (vanilla)\ntouch-based continuous authentication systems (V-TCAS) are susceptible to\npopulation attack. This paper proposes a novel Generative Adversarial Network\nassisted TCAS (G-TCAS) framework, which showed more resilience to the\npopulation attack. G-TCAS framework was tested on a dataset of 117 users who\ninteracted with a smartphone and tablet pair. On average, the increase in the\nfalse accept rates (FARs) for V-TCAS was much higher (22%) than G-TCAS (13%)\nfor the smartphone. Likewise, the increase in the FARs for V-TCAS was 25%\ncompared to G-TCAS (6%) for the tablet.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 04:04:58 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Agrawal", "Mohit", ""], ["Mehrotra", "Pragyan", ""], ["Kumar", "Rajesh", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2106.07931", "submitter": "Thomas Beelen", "authors": "T. Beelen, E. Velner, R. Ordelman, K.P. Truong, V. Evers, T. Huibers", "title": "Does your robot know? Enhancing children's information retrieval through\n  spoken conversation with responsible robots", "comments": "IR4Children'21 workshop at SIGIR 2021 - http://www.fab4.science/IR4C/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we identify challenges in children's current information\nretrieval process, and propose conversational robots as an opportunity to ease\nthis process in a responsible way. Tools children currently use in this\nprocess, such as search engines on a computer or voice agents, do not always\nmeet their specific needs. The conversational robot we propose maintains\ncontext, asks clarifying questions, and gives suggestions in order to better\nmeet children's needs. Since children are often too trusting of robots, we\npropose to have the robot measure, monitor and adapt to the trust the child has\nin the robot. This way, we hope to induce a critical attitude with the children\nduring their information retrieval process.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 07:32:43 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Beelen", "T.", ""], ["Velner", "E.", ""], ["Ordelman", "R.", ""], ["Truong", "K. P.", ""], ["Evers", "V.", ""], ["Huibers", "T.", ""]]}, {"id": "2106.08008", "submitter": "Thorir Mar Ingolfsson", "authors": "Thorir Mar Ingolfsson, Andrea Cossettini, Xiaying Wang, Enrico\n  Tabanelli, Giuseppe Tagliavini, Philippe Ryvlin, Luca Benini, Simone Benatti", "title": "Towards Long-term Non-invasive Monitoring for Epilepsy via Wearable EEG\n  Devices", "comments": "4 pages, 3 figures, 2 tables, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the implementation of seizure detection algorithms based on a\nminimal number of EEG channels on a parallel ultra-low-power embedded platform.\nThe analyses are based on the CHB-MIT dataset, and include explorations of\ndifferent classification approaches (Support Vector Machines, Random Forest,\nExtra Trees, AdaBoost) and different pre/post-processing techniques to maximize\nsensitivity while guaranteeing no false alarms. We analyze global and\nsubject-specific approaches, considering all 23-electrodes or only 4 temporal\nchannels. For 8s window size and subject-specific approach, we report zero\nfalse positives and 100% sensitivity. These algorithms are parallelized and\noptimized for a parallel ultra-low power (PULP) platform, enabling 300h of\ncontinuous monitoring on a 300 mAh battery, in a wearable form factor and power\nbudget. These results pave the way for the implementation of affordable,\nwearable, long-term epilepsy monitoring solutions with low false-positive rates\nand high sensitivity, meeting both patient and caregiver requirements.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 09:37:09 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 10:12:42 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 09:28:03 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Ingolfsson", "Thorir Mar", ""], ["Cossettini", "Andrea", ""], ["Wang", "Xiaying", ""], ["Tabanelli", "Enrico", ""], ["Tagliavini", "Giuseppe", ""], ["Ryvlin", "Philippe", ""], ["Benini", "Luca", ""], ["Benatti", "Simone", ""]]}, {"id": "2106.08298", "submitter": "Jason R.C. Nurse Dr", "authors": "Suraj Sharma and Joseph Brennan and Jason R. C. Nurse", "title": "StockBabble: A Conversational Financial Agent to support Stock Market\n  Investors", "comments": "CUI 2021 - 3rd Conference on Conversational User Interfaces", "journal-ref": null, "doi": "10.1145/3469595.3469620", "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce StockBabble, a conversational agent designed to support\nunderstanding and engagement with the stock market. StockBabble's value and\nnovelty is in its ability to empower retail investors -- many of which may be\nnew to investing -- and supplement their informational needs using a\nuser-friendly agent. Users have the ability to query information on companies\nto retrieve a general and financial overview of a stock, including accessing\nthe latest news and trading recommendations. They can also request charts which\ncontain live prices and technical investment indicators, and add shares to a\npersonal portfolio to allow performance monitoring over time. To evaluate our\nagent's potential, we conducted a user study with 15 participants. In total,\n73% (11/15) of respondents said that they felt more confident in investing\nafter using StockBabble, and all 15 would consider recommending it to others.\nThese results are encouraging and suggest a wider appeal for such agents.\nMoreover, we believe this research can help to inform the design and\ndevelopment of future intelligent, financial personal assistants.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:19:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sharma", "Suraj", ""], ["Brennan", "Joseph", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "2106.08458", "submitter": "Min Hun Lee", "authors": "Min Hun Lee, Daniel P. Siewiorek, Asim Smailagic, Alexandre\n  Bernardino, Sergi Berm\\'udez i Badia", "title": "Enabling AI and Robotic Coaches for Physical Rehabilitation Therapy:\n  Iterative Design and Evaluation with Therapists and Post-Stroke Survivors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Artificial intelligence (AI) and robotic coaches promise the improved\nengagement of patients on rehabilitation exercises through social interaction.\nWhile previous work explored the potential of automatically monitoring\nexercises for AI and robotic coaches, the deployment of these systems remains a\nchallenge. Previous work described the lack of involving stakeholders to design\nsuch functionalities as one of the major causes. In this paper, we present our\nefforts on eliciting the detailed design specifications on how AI and robotic\ncoaches could interact with and guide patient's exercises in an effective and\nacceptable way with four therapists and five post-stroke survivors. Through\niterative questionnaires and interviews, we found that both post-stroke\nsurvivors and therapists appreciated the potential benefits of AI and robotic\ncoaches to achieve more systematic management and improve their self-efficacy\nand motivation on rehabilitation therapy. In addition, our evaluation sheds\nlight on several practical concerns (e.g. a possible difficulty with the\ninteraction for people with cognitive impairment, system failures, etc.). We\ndiscuss the value of early involvement of stakeholders and interactive\ntechniques that complement system failures, but also support a personalized\ntherapy session for the better deployment of AI and robotic exercise coaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:06:39 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Lee", "Min Hun", ""], ["Siewiorek", "Daniel P.", ""], ["Smailagic", "Asim", ""], ["Bernardino", "Alexandre", ""], ["Badia", "Sergi Berm\u00fadez i", ""]]}, {"id": "2106.08484", "submitter": "Alexandros Papangelis", "authors": "Alexandros Papangelis and Karthik Gopalakrishnan and Aishwarya\n  Padmakumar and Seokhwan Kim and Gokhan Tur and Dilek Hakkani-Tur", "title": "Generative Conversational Networks", "comments": "SIGDial 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent work in meta-learning and generative teaching networks, we\npropose a framework called Generative Conversational Networks, in which\nconversational agents learn to generate their own labelled training data (given\nsome seed data) and then train themselves from that data to perform a given\ntask. We use reinforcement learning to optimize the data generation process\nwhere the reward signal is the agent's performance on the task. The task can be\nany language-related task, from intent detection to full task-oriented\nconversations. In this work, we show that our approach is able to generalise\nfrom seed data and performs well in limited data and limited computation\nsettings, with significant gains for intent detection and slot tagging across\nmultiple datasets: ATIS, TOD, SNIPS, and Restaurants8k. We show an average\nimprovement of 35% in intent detection and 21% in slot tagging over a baseline\nmodel trained from the seed data. We also conduct an analysis of the novelty of\nthe generated data and provide generated examples for intent detection, slot\ntagging, and non-goal oriented conversations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 23:19:37 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 21:48:47 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Papangelis", "Alexandros", ""], ["Gopalakrishnan", "Karthik", ""], ["Padmakumar", "Aishwarya", ""], ["Kim", "Seokhwan", ""], ["Tur", "Gokhan", ""], ["Hakkani-Tur", "Dilek", ""]]}, {"id": "2106.08596", "submitter": "Van Thong Huynh", "authors": "VanThong Huynh, Guee-Sang Lee, Hyung-Jeong Yang, Soo-Huyng Kim", "title": "Temporal Convolution Networks with Positional Encoding for Evoked\n  Expression Estimation", "comments": "Oral presentation at AUVi Workshop - CVPR 2021\n  (https://sites.google.com/view/auvi-cvpr2021/program). Source code available\n  at https://github.com/th2l/EvokedExpression-tcnpe", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents an approach for Evoked Expressions from Videos (EEV)\nchallenge, which aims to predict evoked facial expressions from video. We take\nadvantage of pre-trained models on large-scale datasets in computer vision and\naudio signals to extract the deep representation of timestamps in the video. A\ntemporal convolution network, rather than an RNN like architecture, is used to\nexplore temporal relationships due to its advantage in memory consumption and\nparallelism. Furthermore, to address the missing annotations of some\ntimestamps, positional encoding is employed to ensure continuity of input data\nwhen discarding these timestamps during training. We achieved state-of-the-art\nresults on the EEV challenge with a Pearson correlation coefficient of 0.05477,\nthe first ranked performance in the EEV 2021 challenge.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 07:49:36 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Huynh", "VanThong", ""], ["Lee", "Guee-Sang", ""], ["Yang", "Hyung-Jeong", ""], ["Kim", "Soo-Huyng", ""]]}, {"id": "2106.08607", "submitter": "Dong Ma", "authors": "Dong Ma, Andrea Ferlini, Cecilia Mascolo", "title": "OESense: Employing Occlusion Effect for In-ear Human Sensing", "comments": null, "journal-ref": "Published at MobiSys 2021", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart earbuds are recognized as a new wearable platform for personal-scale\nhuman motion sensing. However, due to the interference from head movement or\nbackground noise, commonly-used modalities (e.g. accelerometer and microphone)\nfail to reliably detect both intense and light motions. To obviate this, we\npropose OESense, an acoustic-based in-ear system for general human motion\nsensing. The core idea behind OESense is the joint use of the occlusion effect\n(i.e., the enhancement of low-frequency components of bone-conducted sounds in\nan occluded ear canal) and inward-facing microphone, which naturally boosts the\nsensing signal and suppresses external interference. We prototype OESense as an\nearbud and evaluate its performance on three representative applications, i.e.,\nstep counting, activity recognition, and hand-to-face gesture interaction. With\ndata collected from 31 subjects, we show that OESense achieves 99.3% step\ncounting recall, 98.3% recognition recall for 5 activities, and 97.0% recall\nfor five tapping gestures on human face, respectively. We also demonstrate that\nOESense is compatible with earbuds' fundamental functionalities (e.g. music\nplayback and phone calls). In terms of energy, OESense consumes 746 mW during\ndata recording and recognition and it has a response latency of 40.85 ms for\ngesture recognition. Our analysis indicates such overhead is acceptable and\nOESense is potential to be integrated into future earbuds.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 08:05:59 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ma", "Dong", ""], ["Ferlini", "Andrea", ""], ["Mascolo", "Cecilia", ""]]}, {"id": "2106.08706", "submitter": "Ahmed Arif", "authors": "Laxmi Pandey, Ahmed Sabbir Arif", "title": "Silent Speech and Emotion Recognition from Vocal Tract Shape Dynamics in\n  Real-Time MRI", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.HC cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speech sounds of spoken language are obtained by varying configuration of the\narticulators surrounding the vocal tract. They contain abundant information\nthat can be utilized to better understand the underlying mechanism of human\nspeech production. We propose a novel deep neural network-based learning\nframework that understands acoustic information in the variable-length sequence\nof vocal tract shaping during speech production, captured by real-time magnetic\nresonance imaging (rtMRI), and translate it into text. The proposed framework\ncomprises of spatiotemporal convolutions, a recurrent network, and the\nconnectionist temporal classification loss, trained entirely end-to-end. On the\nUSC-TIMIT corpus, the model achieved a 40.6% PER at sentence-level, much better\ncompared to the existing models. To the best of our knowledge, this is the\nfirst study that demonstrates the recognition of entire spoken sentence based\non an individual's articulatory motions captured by rtMRI video. We also\nperformed an analysis of variations in the geometry of articulation in each\nsub-regions of the vocal tract (i.e., pharyngeal, velar and dorsal, hard\npalate, labial constriction region) with respect to different emotions and\ngenders. Results suggest that each sub-regions distortion is affected by both\nemotion and gender.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:20:02 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Pandey", "Laxmi", ""], ["Arif", "Ahmed Sabbir", ""]]}, {"id": "2106.08710", "submitter": "Jacky Cao", "authors": "Jacky Cao, Kit-Yung Lam, Lik-Hang Lee, Xiaoli Liu, Pan Hui, Xiang Su", "title": "Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence", "comments": "This work is currently under review in an international journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile Augmented Reality (MAR) integrates computer-generated virtual objects\nwith physical environments for mobile devices. MAR systems enable users to\ninteract with MAR devices, such as smartphones and head-worn wearables, and\nperforms seamless transitions from the physical world to a mixed world with\ndigital entities. These MAR systems support user experiences by using MAR\ndevices to provide universal accessibility to digital contents. Over the past\n20 years, a number of MAR systems have been developed, however, the studies and\ndesign of MAR frameworks have not yet been systematically reviewed from the\nperspective of user-centric design. This article presents the first effort of\nsurveying existing MAR frameworks (count: 37) and further discusses the latest\nstudies on MAR through a top-down approach: 1) MAR applications; 2) MAR\nvisualisation techniques adaptive to user mobility and contexts; 3) systematic\nevaluation of MAR frameworks including supported platforms and corresponding\nfeatures such as tracking, feature extraction plus sensing capabilities; and 4)\nunderlying machine learning approaches supporting intelligent operations within\nMAR systems. Finally, we summarise the development of emerging research fields,\ncurrent state-of-the-art, and discuss the important open challenges and\npossible theoretical and technical directions. This survey aims to benefit both\nresearchers and MAR system developers alike.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 11:26:37 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cao", "Jacky", ""], ["Lam", "Kit-Yung", ""], ["Lee", "Lik-Hang", ""], ["Liu", "Xiaoli", ""], ["Hui", "Pan", ""], ["Su", "Xiang", ""]]}, {"id": "2106.08761", "submitter": "Luke Guerdan", "authors": "Luke Guerdan, Alex Raymond, and Hatice Gunes", "title": "Toward Affective XAI: Facial Affect Analysis for Understanding\n  Explainable Human-AI Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As machine learning approaches are increasingly used to augment human\ndecision-making, eXplainable Artificial Intelligence (XAI) research has\nexplored methods for communicating system behavior to humans. However, these\napproaches often fail to account for the emotional responses of humans as they\ninteract with explanations. Facial affect analysis, which examines human facial\nexpressions of emotions, is one promising lens for understanding how users\nengage with explanations. Therefore, in this work, we aim to (1) identify which\nfacial affect features are pronounced when people interact with XAI interfaces,\nand (2) develop a multitask feature embedding for linking facial affect signals\nwith participants' use of explanations. Our analyses and results show that the\noccurrence and values of facial AU1 and AU4, and Arousal are heightened when\nparticipants fail to use explanations effectively. This suggests that facial\naffect analysis should be incorporated into XAI to personalize explanations to\nindividuals' interaction styles and to adapt explanations based on the\ndifficulty of the task performed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 13:14:21 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Guerdan", "Luke", ""], ["Raymond", "Alex", ""], ["Gunes", "Hatice", ""]]}, {"id": "2106.08813", "submitter": "Daniel Klug", "authors": "Daniel Le Compte, Daniel Klug", "title": "\"It's Viral!\" -- A study of the behaviors, practices, and motivations of\n  TikTok Social Activists", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social media platforms such as Facebook and Twitter are used for social\nactivism purposes, and TikTok is no different. We conducted 9 qualitative\nsemi-structured interviews with social activists who recently posted their\nvideos on TikTok to understand. This study presents an initial look into why\nTikTok is used by social activists, and what processes they use to carry out\nthis work. The interviews revealed the following main patterns: (1) content\ncreation practices are typical and expected, (2) motivation and inspiration for\nposting social activist content comes from a wide range of personal sources,\n(3) social activism has communities on TikTok that provides encouragement and\ndiscussion, (4) engagement and interaction with other activists and viewers is\na crucial part of content creation as well as social activism, and (5) the main\ndriving force behind picking TikTok over other platforms is the ability to\nspread messages much farther with less effort. These findings provide insight\ninto the unique factors that TikTok brings for social activists and\ncorroborates previous findings in understanding how social activists use social\nmedia for their purposes.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 14:26:28 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Compte", "Daniel Le", ""], ["Klug", "Daniel", ""]]}, {"id": "2106.08867", "submitter": "Tim Murray-Browne", "authors": "Tim Murray-Browne and Panagiotis Tigas", "title": "Latent Mappings: Generating Open-Ended Expressive Mappings Using\n  Variational Autoencoders", "comments": "Published at the International Conference on New Interfaces for\n  Musical Expression, June 2021. 3000 word short paper. 5 figures plus video\n  which may be seen at https://timmb.com/sonified-body-r-and-d-lab", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many contexts, creating mappings for gestural interactions can form part\nof an artistic process. Creators seeking a mapping that is expressive, novel,\nand affords them a sense of authorship may not know how to program it up in a\nsignal processing patch. Tools like Wekinator and MIMIC allow creators to use\nsupervised machine learning to learn mappings from example input/output\npairings. However, a creator may know a good mapping when they encounter it yet\nstart with little sense of what the inputs or outputs should be. We call this\nan open-ended mapping process. Addressing this need, we introduce the latent\nmapping, which leverages the latent space of an unsupervised machine learning\nalgorithm such as a Variational Autoencoder trained on a corpus of unlabelled\ngestural data from the creator. We illustrate it with Sonified Body, a system\nmapping full-body movement to sound which we explore in a residency with three\ndancers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:40:53 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Murray-Browne", "Tim", ""], ["Tigas", "Panagiotis", ""]]}, {"id": "2106.09006", "submitter": "Daniel Bailey", "authors": "Daniel V. Bailey and Philipp Markert and Adam J. Aviv", "title": "\"I have no idea what they're trying to accomplish:\" Enthusiastic and\n  Casual Signal Users' Understanding of Signal PINs", "comments": "To appear at Symposium on Usable Privacy and Security (SOUPS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We conducted an online study with $n = 235$ Signal users on their\nunderstanding and usage of PINs in Signal. In our study, we observe a split in\nPIN management and composition strategies between users who can explain the\npurpose of the Signal PINs (56%; enthusiasts) and users who cannot (44%; casual\nusers). Encouraging adoption of PINs by Signal appears quite successful: only\n14% opted-out of setting a PIN entirely. Among those who did set a PIN, most\nenthusiasts had long, complex alphanumeric PINs generated by and saved in a\npassword manager. Meanwhile more casual Signal users mostly relied on short\nnumeric-only PINs. Our results suggest that better communication about the\npurpose of the Signal PIN could help more casual users understand the features\nPINs enable (such as that it is not simply a personal identification number).\nThis communication could encourage a stronger security posture.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:55:46 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Bailey", "Daniel V.", ""], ["Markert", "Philipp", ""], ["Aviv", "Adam J.", ""]]}, {"id": "2106.09140", "submitter": "Laura Panfili", "authors": "Laura Panfili, Steve Duman, Andrew Nave, Katherine Phelps Ridgeway,\n  Nathan Eversole and Ruhi Sarikaya", "title": "Human-AI Interactions Through A Gricean Lens", "comments": null, "journal-ref": "Proceedings of the Linguistic Society of America 6 (2021) 288-302", "doi": "10.3765/plsa.v6i1.4971", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Grice's Cooperative Principle (1975) describes the implicit maxims that guide\nconversation between humans. As humans begin to interact with non-human\ndialogue systems more frequently and in a broader scope, an important question\nemerges: what principles govern those interactions? The present study addresses\nthis question by evaluating human-AI interactions using Grice's four maxims; we\ndemonstrate that humans do, indeed, apply these maxims to interactions with AI,\neven making explicit references to the AI's performance through a Gricean lens.\nTwenty-three participants interacted with an American English-speaking Alexa\nand rated and discussed their experience with an in-lab researcher. Researchers\nthen reviewed each exchange, identifying those that might relate to Grice's\nmaxims: Quantity, Quality, Manner, and Relevance. Many instances of explicit\nuser frustration stemmed from violations of Grice's maxims. Quantity violations\nwere noted for too little but not too much information, while Quality\nviolations were rare, indicating trust in Alexa's responses. Manner violations\nfocused on speed and humanness. Relevance violations were the most frequent,\nand they appear to be the most frustrating. While the maxims help describe many\nof the issues participants encountered, other issues do not fit neatly into\nGrice's framework. Participants were particularly averse to Alexa initiating\nexchanges or making unsolicited suggestions. To address this gap, we propose\nthe addition of human Priority to describe human-AI interaction. Humans and AIs\nare not conversational equals, and human initiative takes priority. We suggest\nthat the application of Grice's Cooperative Principles to human-AI interactions\nis beneficial both from an AI development perspective and as a tool for\ndescribing an emerging form of interaction.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 21:35:56 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Panfili", "Laura", ""], ["Duman", "Steve", ""], ["Nave", "Andrew", ""], ["Ridgeway", "Katherine Phelps", ""], ["Eversole", "Nathan", ""], ["Sarikaya", "Ruhi", ""]]}, {"id": "2106.09196", "submitter": "Haoran Xie", "authors": "Haoran Xie and Atsushi Watatani and Kazunori Miyata", "title": "CoreUI: Interactive Core Training System with 3D Human Shape", "comments": "9 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an interactive core training system for core training using a\nmonocular camera image as input in this paper. It is commonly expensive to\ncapture human pose using depth cameras or multiple cameras with conventional\napproaches. To solve this issue, we employ the skinned multi-person linear\nmodel of human shape to recover the 3D human pose from 2D images using pose\nestimation and human mesh recovery approaches. In order to support the user in\nmaintaining the correct postures from target poses in the training, we adopt 3D\nhuman shape estimation for both the target image and input camera video. We\npropose CoreUI, a user interface for providing visual guidance showing the\ndifferences among the estimated targets and current human shapes in core\ntraining, which are visualized by markers at ten body parts with color changes.\nFrom our user studies, the proposed core training system is effective and\nconvenient compared with the conventional guidance of 2D skeletons.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 01:19:30 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xie", "Haoran", ""], ["Watatani", "Atsushi", ""], ["Miyata", "Kazunori", ""]]}, {"id": "2106.09199", "submitter": "Jicheng Li", "authors": "Jicheng Li, Anjana Bhat, Roghayeh Barmaki", "title": "A Two-stage Multi-modal Affect Analysis Framework for Children with\n  Autism Spectrum Disorder", "comments": "8 pages including reference; 8 figures", "journal-ref": "The AAAI-21 Workshop On Affective Content Analysis; 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autism spectrum disorder (ASD) is a developmental disorder that influences\nthe communication and social behavior of a person in a way that those in the\nspectrum have difficulty in perceiving other people's facial expressions, as\nwell as presenting and communicating emotions and affect via their own faces\nand bodies. Some efforts have been made to predict and improve children with\nASD's affect states in play therapy, a common method to improve children's\nsocial skills via play and games. However, many previous works only used\npre-trained models on benchmark emotion datasets and failed to consider the\ndistinction in emotion between typically developing children and children with\nautism. In this paper, we present an open-source two-stage multi-modal approach\nleveraging acoustic and visual cues to predict three main affect states of\nchildren with ASD's affect states (positive, negative, and neutral) in\nreal-world play therapy scenarios, and achieved an overall accuracy of 72:40%.\nThis work presents a novel way to combine human expertise and machine\nintelligence for ASD affect recognition by proposing a two-stage schema.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 01:28:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Li", "Jicheng", ""], ["Bhat", "Anjana", ""], ["Barmaki", "Roghayeh", ""]]}, {"id": "2106.09338", "submitter": "Fareeda Nawaz", "authors": "Danyal Haroon, Hammad Arif, Ahmed Abdullah Tariq, fareeda nawaz, Dr.\n  Ihsan Ayyub Qazi and Dr. Maryam mustafa", "title": "Investigating Misinformation Dissemination on Social Media in Pakistan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fake news and misinformation are one of the most significant challenges\nbrought about by advances in communication technologies. We chose to research\nthe spread of fake news in Pakistan because of some unfortunate incidents that\ntook place during 2020. These included the downplaying of the severity of the\nCOVID-19 pandemic, and protests by right-wing political movements. We observed\nthat fake news and misinformation contributed significantly to these events and\nespecially affected low-literate and low-income populations. We conducted a\ncross-platform comparison of misinformation on WhatsApp, Twitter and YouTube\nwith a primary focus on messages shared in public WhatsApp groups, and analysed\nthe characteristics of misinformation, techniques used to make is believable,\nand how users respond to it. To the best of our knowledge, this is the first\nattempt to compare misinformation on all three platforms in Pakistan. Data\ncollected over a span of eight months helped us identify fake news and\nmisinformation related to politics, religion and health, among other\ncategories. Common elements which were used by fake news creators in Pakistan\nto make false content seem believable included: appeals to emotion, conspiracy\ntheories, political and religious polarization, incorrect facts and\nimpersonation of credible sources.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 09:19:45 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Haroon", "Danyal", ""], ["Arif", "Hammad", ""], ["Tariq", "Ahmed Abdullah", ""], ["nawaz", "fareeda", ""], ["Qazi", "Dr. Ihsan Ayyub", ""], ["mustafa", "Dr. Maryam", ""]]}, {"id": "2106.09509", "submitter": "Yuhao Zhu", "authors": "Joshua Romphf, Elias Neuman-Donihue, Gregory Heyworth, Yuhao Zhu", "title": "Resurrect3D: An Open and Customizable Platform for Visualizing and\n  Analyzing Cultural Heritage Artifacts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Art and culture, at their best, lie in the act of discovery and exploration.\nThis paper describes Resurrect3D, an open visualization platform for both\ncasual users and domain experts to explore cultural artifacts. To that end,\nResurrect3D takes two steps. First, it provides an interactive cultural\nheritage toolbox, providing not only commonly used tools in cultural heritage\nsuch as relighting and material editing, but also the ability for users to\ncreate an interactive \"story\": a saved session with annotations and\nvisualizations others can later replay. Second, Resurrect3D exposes a set of\nprogramming interfaces to extend the toolbox. Domain experts can develop custom\ntools that perform artifact-specific visualization and analysis.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:59:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Romphf", "Joshua", ""], ["Neuman-Donihue", "Elias", ""], ["Heyworth", "Gregory", ""], ["Zhu", "Yuhao", ""]]}, {"id": "2106.09763", "submitter": "Jeffrey Uhlmann", "authors": "Jeffrey Uhlmann", "title": "Sensory Modality Mapping for Game Adaptation and Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine methods for taking game-related information provided\nin one sensory modality and transforming it to another sensor modality in order\nto more effectively accommodate sensory-constrained players. We then consider\nmethods for the adaptation and design of games for which gameplay interactions\nare constrained to a subset of sensory modalities in ways that preserve a\ncommon level of novelty-of-experience for players with different sensory\ncapabilities. It is hoped that improved shared experiences can promote\ninteractions among a more diverse spectrum of players.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:45:44 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Uhlmann", "Jeffrey", ""]]}, {"id": "2106.09866", "submitter": "Eugene Yang", "authors": "Eugene Yang and David D. Lewis and Ophir Frieder", "title": "On Minimizing Cost in Legal Document Review Workflows", "comments": "10 pages, 3 figures. Accepted at DocEng 21", "journal-ref": null, "doi": "10.1145/3469096.3469872", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Technology-assisted review (TAR) refers to human-in-the-loop machine learning\nworkflows for document review in legal discovery and other high recall review\ntasks. Attorneys and legal technologists have debated whether review should be\na single iterative process (one-phase TAR workflows) or whether model training\nand review should be separate (two-phase TAR workflows), with implications for\nthe choice of active learning algorithm. The relative cost of manual labeling\nfor different purposes (training vs. review) and of different documents\n(positive vs. negative examples) is a key and neglected factor in this debate.\nUsing a novel cost dynamics analysis, we show analytically and empirically that\nthese relative costs strongly impact whether a one-phase or two-phase workflow\nminimizes cost. We also show how category prevalence, classification task\ndifficulty, and collection size impact the optimal choice not only of workflow\ntype, but of active learning method and stopping point.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 01:51:47 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yang", "Eugene", ""], ["Lewis", "David D.", ""], ["Frieder", "Ophir", ""]]}, {"id": "2106.09894", "submitter": "Ryan Kim", "authors": "Ryan Kim", "title": "Development of a conversing and body temperature scanning autonomously\n  navigating robot to help screen for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Throughout the COVID-19 pandemic, the most common symptom displayed by\npatients has been a fever, leading to the use of temperature scanning as a\npreemptive measure to detect potential carriers of the virus. Human employees\nwith handheld thermometers have been used to fulfill this task, however this\nputs them at risk as they cannot be physically distanced and the sequential\nnature of this method leads to great inconveniences and inefficiency. The\nproposed solution is an autonomously navigating robot capable of conversing and\nscanning people's temperature to detect fevers and help screen for COVID-19. To\nsatisfy this objective, the robot must be able to (1) navigate autonomously,\n(2) detect and track people, and (3) get individuals' temperature reading and\nconverse with them if it exceeds 38{\\deg}C. An autonomously navigating mobile\nrobot is used with a manipulator controlled using a face tracking algorithm,\nand an end effector consisting of a thermal camera, smartphone, and chatbot.\nThe goal is to develop a functioning solution that performs the above tasks. In\naddition, technical challenges encountered and their engineering solutions will\nbe presented, and recommendations will be made for enhancements that could be\nincorporated when approaching commercialization.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 03:30:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Kim", "Ryan", ""]]}, {"id": "2106.09906", "submitter": "Takahiro Tsumura", "authors": "Takahiro Tsumura, Seiji Yamada", "title": "Facilitation of human empathy through self-disclosure of anthropomorphic\n  agents", "comments": "20 pages, 8 figures, 2 tables, submitted to PLOS ONE Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As AI technologies progress, social acceptance of AI agents including\nintelligent virtual agents and robots is getting to be even more important for\nmore applications of AI in human society. One way to improve the relationship\nbetween humans and anthropomorphic agents is to have humans empathize with the\nagents. By empathizing, humans take positive and kind actions toward agents,\nand emphasizing makes it easier for humans to accept agents. In this study, we\nfocused on self-disclosure from agents to humans in order to realize\nanthropomorphic agents that elicit empathy from humans. Then, we experimentally\ninvestigated the possibility that an agent's self-disclosure facilitates human\nempathy. We formulate hypotheses and experimentally analyze and discuss the\nconditions in which humans have more empathy for agents. This experiment was\nconducted with a three-way mixed plan, and the factors were the agents'\nappearance (human, robot), self-disclosure (high-relevance self-disclosure,\nlow-relevance self-disclosure, no self-disclosure), and empathy before and\nafter a video stimulus. An analysis of variance was performed using data from\n576 participants. As a result, we found that the appearance factor did not have\na main effect, and self-disclosure, which is highly relevant to the scenario\nused, facilitated more human empathy with statistically significant difference.\nWe also found that no self-disclosure suppressed empathy. These results support\nour hypotheses.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 04:14:58 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Tsumura", "Takahiro", ""], ["Yamada", "Seiji", ""]]}, {"id": "2106.09937", "submitter": "Noble Mathews", "authors": "Noble Saji Mathews, Sridhar Chimalakonda", "title": "Detox Browser -- Towards Filtering Sensitive Content On the Web", "comments": "6 pages, 2 figures, CSCW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The annual consumption of web-based resources is increasing at a very fast\nrate, mainly due to an increase in affordability and accessibility of the\ninternet. Many are relying on the web to get diverse perspectives, but at the\nsame time, it can expose them to content that is harmful to their mental\nwell-being. Catchy headlines and emotionally charged articles increase the\nnumber of readers which in turn increases ad revenue for websites. When a user\nconsumes a large quantity of negative content, it adversely impacts the user's\nhappiness and has a significant impact on his/her mood and state of mind. Many\nstudies carried out during the COVID-19 pandemic has shown that people across\nthe globe irrespective of their country of origin have experienced higher\nlevels of anxiety and depression. Web filters can help in constructing a\ndigital environment that is more suitable for people prone to depression,\nanxiety and stress. A significant amount of work has been done in the field of\nweb filtering, but there has been limited focus on helping Highly Sensitive\nPersons (HSP's) or those with stress disorders induced by trauma. Through this\npaper, we propose detox Browser, a simple tool that enables end-users to tune\nout of or control their exposure to topics that can affect their mental well\nbeing. The extension makes use of sentiment analysis and keywords to filter out\nflagged content from google search results and warns users if any blacklisted\ntopics are detected when navigating across websites\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:28:17 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Mathews", "Noble Saji", ""], ["Chimalakonda", "Sridhar", ""]]}, {"id": "2106.09942", "submitter": "Wilk Oliveira", "authors": "Wilk Oliveira, Olena Pastushenko, Luiz Rodrigues, Armando M. Toda,\n  Paula T. Palomino, Juho Hamari, Seiji Isotani", "title": "Does gamification affect flow experience? A systematic literature review", "comments": "5th International GamiFIN Conference 2021 (GamiFIN 2021), April 7-10,\n  2021, Finland", "journal-ref": "2021 Proceedings of the 5th International GamiFIN Conference", "doi": null, "report-no": "CEUR-WS.org/Vol-2883/paper12.pdf", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, studies in different areas have used gamification to improve\nusers' flow experience. However, due to the high variety of the conducted\nstudies and the lack of secondary studies (e.g., systematic literature reviews)\nin this field, it is difficult to get the state-of-the-art of this research\ndomain. To address this problem, we conducted a systematic literature review to\nidentify i) which gamification design methods have been used in the studies\nabout gamification and Flow Theory, ii) which gamification elements have been\nused in these studies, iii) which methods have been used to evaluate the users'\nflow experience in gamified settings, and iv) how gamification affects users'\nflow experience. The main results show that there is growing interest to this\nfield, as the number of publications is increasing. The most significant\ninterest is in the area of gamification in education. However, there is no\nunanimity regarding the preferred method of the study or the effects of\ngamification on users' experience. Our results highlight the importance of\nconducting new experimental studies investigating how gamification affects the\nusers' flow experience in different gamified settings, applications and\ndomains.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 06:43:38 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Oliveira", "Wilk", ""], ["Pastushenko", "Olena", ""], ["Rodrigues", "Luiz", ""], ["Toda", "Armando M.", ""], ["Palomino", "Paula T.", ""], ["Hamari", "Juho", ""], ["Isotani", "Seiji", ""]]}, {"id": "2106.10148", "submitter": "Wilk Oliveira", "authors": "Ana Cl\\'audia Guimar\\~aes Santos, Wilk Oliveira, Juho Hamari, Seiji\n  Isotani", "title": "Do people's user types change over time? An exploratory study", "comments": "5th International GamiFIN Conference 2021 (GamiFIN 2021), April 7-10,\n  2021, Finland", "journal-ref": "2021 Proceedings of the 5th International GamiFIN Conference", "doi": null, "report-no": "CEUR-WS.org/Vol-2883/paper10.pdf", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, different studies have proposed and validated user models\n(e.g., Bartle, BrainHex, and Hexad) to represent the different user profiles in\ngames and gamified settings. However, the results of applying these user models\nin practice (e.g., to personalize gamified systems) are still contradictory.\nOne of the hypotheses for these results is that the user types can change over\ntime (i.e., user types are dynamic). To start to understand whether user types\ncan change over time, we conducted an exploratory study analyzing data from 74\nparticipants to identify if their user type (Achiever, Philanthropist,\nSocialiser, Free Spirit, Player, and Disruptor) had changed over time (six\nmonths). The results indicate that there is a change in the dominant user type\nof the participants, as well as the average scores in the Hexad sub-scales.\nThese results imply that all the scores should be considered when defining the\nHexad's user type and that the user types are dynamic. Our results contribute\nwith practical implications, indicating that the personalization currently made\n(generally static) may be insufficient to improve the users' experience,\nrequiring user types to be analyzed continuously and personalization to be done\ndynamically.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:25:21 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Santos", "Ana Cl\u00e1udia Guimar\u00e3es", ""], ["Oliveira", "Wilk", ""], ["Hamari", "Juho", ""], ["Isotani", "Seiji", ""]]}, {"id": "2106.10154", "submitter": "Paula Delgado-Santos", "authors": "Paula Delgado-Santos, Giuseppe Stragapede, Ruben Tolosana, Richard\n  Guest, Farzin Deravi and Ruben Vera-Rodriguez", "title": "A Survey of Privacy Vulnerabilities of Mobile Device Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The number of mobile devices, such as smartphones and smartwatches, is\nrelentlessly increasing to almost 6.8 billion by 2022, and along with it, the\namount of personal and sensitive data captured by them. This survey overviews\nthe state of the art of what personal and sensitive user attributes can be\nextracted from mobile device sensors, emphasising critical aspects such as\ndemographics, health and body features, activity and behaviour recognition,\netc. In addition, we review popular metrics in the literature to quantify the\ndegree of privacy, and discuss powerful privacy methods to protect the\nsensitive data while preserving data utility for analysis. Finally, open\nresearch questions a represented for further advancements in the field.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:42:09 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Delgado-Santos", "Paula", ""], ["Stragapede", "Giuseppe", ""], ["Tolosana", "Ruben", ""], ["Guest", "Richard", ""], ["Deravi", "Farzin", ""], ["Vera-Rodriguez", "Ruben", ""]]}, {"id": "2106.10275", "submitter": "Edelberto Franco Silva", "authors": "Edelberto Franco Silva and Bruno Jos\\'e Dembogurski and Gustavo Silva\n  Semaan", "title": "A Systematic Review of Computational Thinking in Early Ages", "comments": "a systematic review submitted to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, technology has become dominant in the daily lives of most people\naround the world. From children to older people, technology is present, helping\nin the most diverse daily tasks and allowing accessibility. However, many times\nthese people are just end-users, without any incentive to the development of\ncomputational thinking (CT). With advances in technologies, the abstraction of\ncoding, programming languages, and the hardware resources involved will become\na reality. However, while we have not progressed to this stage, it is necessary\nto encourage the development of CT teaching from an early age. This work will\npresent state of the art concerning teaching initiatives and tools on\nprogramming (e.g., ScratchJr), robotics (e.g., KIBO), and other playful tools\n(e.g., Happy Maps) for the development of CT in the early ages, specifically\nfilling the gap of CT at the kindergarten level. This survey presents a\nsystematic review of the literature, emphasizing computational and robotic\ntools used in preschool classes to develop the CT. The systematic review\nevaluated more than 60 papers from 2010 to December 2020, electing 31 papers\nand adding three papers from the qualitative stage. The paper's amount was\nclassified in taxonomy to show CT's principal tools and initiates applied to\nchildren early. To conclude this survey, an extensive discussion about the\nterms and authors related to this research area is present.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 19:00:10 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Silva", "Edelberto Franco", ""], ["Dembogurski", "Bruno Jos\u00e9", ""], ["Semaan", "Gustavo Silva", ""]]}, {"id": "2106.10331", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "Exoskeleton-Based Multimodal Action and Movement Recognition:\n  Identifying and Developing the Optimal Boosted Learning Approach", "comments": null, "journal-ref": "Journal of Advances in Artificial Intelligence and Machine\n  Learning. 2021; Volume 1, Issue 1, Article 4", "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper makes two scientific contributions to the field of\nexoskeleton-based action and movement recognition. First, it presents a novel\nmachine learning and pattern recognition-based framework that can detect a wide\nrange of actions and movements - walking, walking upstairs, walking downstairs,\nsitting, standing, lying, stand to sit, sit to stand, sit to lie, lie to sit,\nstand to lie, and lie to stand, with an overall accuracy of 82.63%. Second, it\npresents a comprehensive comparative study of different learning approaches -\nRandom Forest, Artificial Neural Network, Decision Tree, Multiway Decision\nTree, Support Vector Machine, k-NN, Gradient Boosted Trees, Decision Stump,\nAuto MLP, Linear Regression, Vector Linear Regression, Random Tree, Na\\\"ive\nBayes, Na\\\"ive Bayes (Kernel), Linear Discriminant Analysis, Quadratic\nDiscriminant Analysis, and Deep Learning applied to this framework. The\nperformance of each of these learning approaches was boosted by using the\nAdaBoost algorithm, and the Cross Validation approach was used for training and\ntesting. The results show that in boosted form, the k- NN classifier\noutperforms all the other boosted learning approaches and is, therefore, the\noptimal learning method for this purpose. The results presented and discussed\nuphold the importance of this work to contribute towards augmenting the\nabilities of exoskeleton-based assisted and independent living of the elderly\nin the future of Internet of Things-based living environments, such as Smart\nHomes. As a specific use case, we also discuss how the findings of our work are\nrelevant for augmenting the capabilities of the Hybrid Assistive Limb\nexoskeleton, a highly functional lower limb exoskeleton.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:43:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2106.10563", "submitter": "Sarah Fakhoury", "authors": "Sarah Fakhoury, Devjeet Roy, Harry Pines, Tyler Cleveland, Cole\n  Peterson, Venera Arnaoudova, Bonita Sharif, Jonathan Maletic", "title": "gazel: Supporting Source Code Edits in Eye-Tracking Studies", "comments": "4 pages, 2 figures", "journal-ref": "International Conference on Software Engineering (ICSE) 2021", "doi": "10.1109/ICSE-Companion52605.2021.00038", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye tracking tools are used in software engineering research to study various\nsoftware development activities. However, a major limitation of these tools is\ntheir inability to track gaze data for activities that involve source code\nediting. We present a novel solution to support eye tracking experiments for\ntasks involving source code edits as an extension of the iTrace community\ninfrastructure. We introduce the iTrace-Atom plugin and gazel -- a Python data\nprocessing pipeline that maps gaze information to changing source code elements\nand provides researchers with a way to query this dynamic data. iTrace-Atom is\nevaluated via a series of simulations and is over 99% accurate at high\neye-tracking speeds of over 1,000Hz. iTrace and gazel completely revolutionize\nthe way eye tracking studies are conducted in realistic settings with the\npresence of scrolling, context switching, and now editing. This opens the doors\nto support many day-to-day software engineering tasks such as bug fixing,\nadding new features, and refactoring.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 19:37:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fakhoury", "Sarah", ""], ["Roy", "Devjeet", ""], ["Pines", "Harry", ""], ["Cleveland", "Tyler", ""], ["Peterson", "Cole", ""], ["Arnaoudova", "Venera", ""], ["Sharif", "Bonita", ""], ["Maletic", "Jonathan", ""]]}, {"id": "2106.10882", "submitter": "Ali Abedi", "authors": "Ali Abedi and Shehroz Khan", "title": "Affect-driven Engagement Measurement from Videos", "comments": "13 pages, 8 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In education and intervention programs, person's engagement has been\nidentified as a major factor in successful program completion. Automatic\nmeasurement of person's engagement provides useful information for instructors\nto meet program objectives and individualize program delivery. In this paper,\nwe present a novel approach for video-based engagement measurement in virtual\nlearning programs. We propose to use affect states, continuous values of\nvalence and arousal extracted from consecutive video frames, along with a new\nlatent affective feature vector and behavioral features for engagement\nmeasurement. Deep learning-based temporal, and traditional\nmachine-learning-based non-temporal models are trained and validated on\nframe-level, and video-level features, respectively. In addition to the\nconventional centralized learning, we also implement the proposed method in a\ndecentralized federated learning setting and study the effect of model\npersonalization in engagement measurement. We evaluated the performance of the\nproposed method on the only two publicly available video engagement measurement\ndatasets, DAiSEE and EmotiW, containing videos of students in online learning\nprograms. Our experiments show a state-of-the-art engagement level\nclassification accuracy of 63.3% and correctly classifying disengagement videos\nin the DAiSEE dataset and a regression mean squared error of 0.0673 on the\nEmotiW dataset. Our ablation study shows the effectiveness of incorporating\naffect states in engagement measurement. We interpret the findings from the\nexperimental results based on psychology concepts in the field of engagement.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:49:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Abedi", "Ali", ""], ["Khan", "Shehroz", ""]]}, {"id": "2106.10970", "submitter": "Dimitris Spathis", "authors": "Benjamin Lucas Searle, Dimitris Spathis, Marios Constantinides,\n  Daniele Quercia, Cecilia Mascolo", "title": "Anticipatory Detection of Compulsive Body-focused Repetitive Behaviors\n  with Wearables", "comments": "Accepted to ACM MobileHCI 2021 (20 pages, dataset/code:\n  https://github.com/Bhorda/BFRBAnticipationDataset)", "journal-ref": null, "doi": "10.1145/3447526.3472061", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Body-focused repetitive behaviors (BFRBs), like face-touching or\nskin-picking, are hand-driven behaviors which can damage one's appearance, if\nnot identified early and treated. Technology for automatic detection is still\nunder-explored, with few previous works being limited to wearables with single\nmodalities (e.g., motion). Here, we propose a multi-sensory approach combining\nmotion, orientation, and heart rate sensors to detect BFRBs. We conducted a\nfeasibility study in which participants (N=10) were exposed to BFRBs-inducing\ntasks, and analyzed 380 mins of signals under an extensive evaluation of\nsensing modalities, cross-validation methods, and observation windows. Our\nmodels achieved an AUC > 0.90 in distinguishing BFRBs, which were more evident\nin observation windows 5 mins prior to the behavior as opposed to 1-min ones.\nIn a follow-up qualitative survey, we found that not only the timing of\ndetection matters but also models need to be context-aware, when designing\njust-in-time interventions to prevent BFRBs.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 10:45:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Searle", "Benjamin Lucas", ""], ["Spathis", "Dimitris", ""], ["Constantinides", "Marios", ""], ["Quercia", "Daniele", ""], ["Mascolo", "Cecilia", ""]]}, {"id": "2106.11008", "submitter": "Nikhil Garg", "authors": "Lizy Kanungo, Nikhil Garg, Anish Bhobe, Smit Rajguru, Veeky Baths", "title": "Wheelchair automation by a hybrid BCI system using SSVEP and eye blinks", "comments": "Accepted to 2021 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND\n  CYBERNETICS (SMC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a hybrid Brain Computer Interface system for the\nautomation of a wheelchair for the disabled. Herein a working prototype of a\nBCI-based wheelchair is detailed that can navigate inside a typical home\nenvironment with minimum structural modification and without any visual\nobstruction and discomfort to the user. The prototype is based on a combined\nmechanism of steady-state visually evoked potential and eye blinks. To elicit\nSSVEP, LEDs flickering at 13Hz and 15Hz were used to select the left and right\ndirection, respectively, and EEG data was recorded. In addition, the occurrence\nof three continuous blinks was used as an indicator for stopping an ongoing\naction. The wavelet packet denoising method was applied, followed by feature\nextraction methods such as Wavelet Packet Decomposition and Canonical\nCorrelation Analysis over narrowband reconstructed EEG signals. Bayesian\noptimization was used to obtain 5 fold cross-validations to optimize the\nhyperparameters of the Support Vector Machine. The resulting new model was\ntested and the average cross-validation accuracy 89.65% + 6.6% (SD) and testing\naccuracy 83.53% + 8.59% (SD) were obtained. The wheelchair was controlled by\nRaspberryPi through WiFi. The developed prototype demonstrated an average of\n86.97% success rate for all trials with 4.015s for each command execution. The\nprototype can be used efficiently in a home environment without causing any\ndiscomfort to the user.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:02:31 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 07:22:48 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Kanungo", "Lizy", ""], ["Garg", "Nikhil", ""], ["Bhobe", "Anish", ""], ["Rajguru", "Smit", ""], ["Baths", "Veeky", ""]]}, {"id": "2106.11345", "submitter": "Clod\\'eric Mars", "authors": "AI Redefined, Sai Krishna Gottipati, Sagar Kurandwad, Clod\\'eric Mars,\n  Gregory Szriftgiser and Fran\\c{c}ois Chabot", "title": "Cogment: Open Source Framework For Distributed Multi-actor Training,\n  Deployment & Operations", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Involving humans directly for the benefit of AI agents' training is getting\ntraction thanks to several advances in reinforcement learning and\nhuman-in-the-loop learning. Humans can provide rewards to the agent,\ndemonstrate tasks, design a curriculum, or act in the environment, but these\nbenefits also come with architectural, functional design and engineering\ncomplexities. We present Cogment, a unifying open-source framework that\nintroduces an actor formalism to support a variety of humans-agents\ncollaboration typologies and training approaches. It is also scalable out of\nthe box thanks to a distributed micro service architecture, and offers\nsolutions to the aforementioned complexities.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:21:26 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Redefined", "AI", ""], ["Gottipati", "Sai Krishna", ""], ["Kurandwad", "Sagar", ""], ["Mars", "Clod\u00e9ric", ""], ["Szriftgiser", "Gregory", ""], ["Chabot", "Fran\u00e7ois", ""]]}, {"id": "2106.11394", "submitter": "Felix Biessmann", "authors": "Felix Biessmann and Viktor Treu", "title": "A Turing Test for Transparency", "comments": "Published in Proceedings of the ICML Workshop on Theoretical\n  Foundations, Criticism, and Application Trends of Explainable AI held in\n  conjunction with the 38th International Conference on Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A central goal of explainable artificial intelligence (XAI) is to improve the\ntrust relationship in human-AI interaction. One assumption underlying research\nin transparent AI systems is that explanations help to better assess\npredictions of machine learning (ML) models, for instance by enabling humans to\nidentify wrong predictions more efficiently. Recent empirical evidence however\nshows that explanations can have the opposite effect: When presenting\nexplanations of ML predictions humans often tend to trust ML predictions even\nwhen these are wrong. Experimental evidence suggests that this effect can be\nattributed to how intuitive, or human, an AI or explanation appears. This\neffect challenges the very goal of XAI and implies that responsible usage of\ntransparent AI methods has to consider the ability of humans to distinguish\nmachine generated from human explanations. Here we propose a quantitative\nmetric for XAI methods based on Turing's imitation game, a Turing Test for\nTransparency. A human interrogator is asked to judge whether an explanation was\ngenerated by a human or by an XAI method. Explanations of XAI methods that can\nnot be detected by humans above chance performance in this binary\nclassification task are passing the test. Detecting such explanations is a\nrequirement for assessing and calibrating the trust relationship in human-AI\ninteraction. We present experimental results on a crowd-sourced text\nclassification task demonstrating that even for basic ML models and XAI\napproaches most participants were not able to differentiate human from machine\ngenerated explanations. We discuss ethical and practical implications of our\nresults for applications of transparent ML.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 20:09:40 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Biessmann", "Felix", ""], ["Treu", "Viktor", ""]]}, {"id": "2106.11676", "submitter": "Theodor Schnitzler", "authors": "Marvin Kowalewski, Franziska Herbert, Theodor Schnitzler, Markus\n  D\\\"urmuth", "title": "Proof-of-Vax: Studying User Preferences and Perception of Covid\n  Vaccination Certificates", "comments": "22 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital tools play an important role in fighting the current global COVID-19\npandemic. We conducted a representative online study in Germany on a sample of\n599 participants to evaluate the user perception of vaccination certificates.\nWe investigated five different variants of vaccination certificates, based on\ndeployed and planned designs in a between-group design, including paper-based\nand app-based variants. Our main results show that the willingness to use and\nadopt vaccination certificates is generally high. Overall, paper-based\nvaccination certificates were favored over app-based solutions. The willingness\nto use digital apps decreased significantly by a higher disposition to privacy,\nand increased by higher worries about the pandemic and acceptance of the\ncoronavirus vaccination. Vaccination certificates resemble an interesting use\ncase for studying privacy perceptions for health related data. We hope that our\nwork will be able to educate the currently ongoing design of vaccination\ncertificates, will give us deeper insights into privacy of health-related data\nand apps, and prepare us for future potential applications of vaccination\ncertificates and health apps in general.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 11:18:25 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Kowalewski", "Marvin", ""], ["Herbert", "Franziska", ""], ["Schnitzler", "Theodor", ""], ["D\u00fcrmuth", "Markus", ""]]}, {"id": "2106.11855", "submitter": "Joseph Breda", "authors": "Joseph Breda, Shwetak Patel", "title": "Intuitive and Ubiquitous Fever Monitoring Using Smartphones and\n  Smartwatches", "comments": "18 pages, 9 figures, Not yet submitted to conference (submitting\n  after clinical trials)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inside all smart devices, such as smartphones or smartwatches, there are\nthermally sensitive resistors known as thermistors which are used to monitor\nthe temperature of the device. These thermistors are sensitive to temperature\nchanges near their location on-device. While they are designed to measure the\ntemperature of the device components such as the battery, they can also sense\nchanges in the temperature of the ambient environment or thermal entities in\ncontact with the device. We have developed a model to estimate core body\ntemperature from signals sensed by these thermistors during a user interaction\nin which the user places the capacitive touchscreen of a smart device against a\nthermal site on their body such as their forehead. During the interaction, the\ndevice logs the temperature sensed by the thermistors as well as the raw\ncapacitance seen by the touch screen to capture features describing the rate of\nheat transfer from the body to the device and device-to-skin contact\nrespectively. These temperature and contact features are then used to model the\nrate of heat transferred from the user's body to the device and thus core-body\ntemperature of the user for ubiquitous and accessible fever monitoring using\nonly a smart device. We validate this system in a lab environment on a\nsimulated skin-like heat source with a temperature estimate mean absolute error\nof 0.743$^{\\circ}$F (roughly 0.4$^{\\circ}$C) and limit of agreement of\n$\\pm2.374^{\\circ}$F (roughly 1.3$^{\\circ}$C) which is comparable to some\noff-the-shelf peripheral and tympanic thermometers. We found a Pearson's\ncorrelation $R^2$ of 0.837 between ground truth temperature and temperature\nestimated by our system. We also deploy this system in an ongoing clinical\nstudy on a population of 7 participants in a clinical environment to show the\nsimilarity between simulated and clinical trials.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 15:27:00 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Breda", "Joseph", ""], ["Patel", "Shwetak", ""]]}, {"id": "2106.11897", "submitter": "Akhila Sri Manasa Venigalla", "authors": "Dheeraj Vagavolu, Akhila Sri Manasa Venigalla and Sridhar Chimalakonda", "title": "MuseumViz -- Towards Visualizing Online Museum Collections", "comments": "8 pages, 3 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growth of online museums for India's cultural heritage data,\nthere is limited increase in terms of visitors. Over the years, online museums\nadopted many techniques to improve the overall user experience. However, many\nIndian online museums display artifacts as lists and grids with basic search\nfunctionality, making it less visually appealing and difficult to comprehend.\nOur work aims to enhance the user experience of accessing Indian online museums\nby utilizing advancements in information visualization. Hence, we propose\nMuseumViz, a framework which processes data from online museums and visualizes\nit using four different interactive visualizations: the Network Graph,\nTreepMap, Polygon Chart and SunBurst Chart. We demonstrate MuseumViz on a total\nof 723 cultural heritage artifacts present in the Archaeological Survey of\nIndia, Goa. Based on our evaluation with 25 users, about 83% of them find it\neasier and more comprehensible to browse cultural heritage artifacts through\nMuseumViz.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:06:45 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Vagavolu", "Dheeraj", ""], ["Venigalla", "Akhila Sri Manasa", ""], ["Chimalakonda", "Sridhar", ""]]}, {"id": "2106.11900", "submitter": "Mohammad Arif Ul Alam", "authors": "Mohammad Arif Ul Alam", "title": "Person Re-identification Attack on Wearable Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person re-identification is a critical privacy attack in publicly shared\nhealthcare data as per Health Insurance Portability and Accountability Act\n(HIPAA) privacy rule. In this paper, we investigate the possibility of a new\ntype of privacy attack, Person Re-identification Attack (PRI-attack) on\npublicly shared privacy insensitive wearable data. We investigate user's\nspecific biometric signature in terms of two contextual biometric traits,\nphysiological (photoplethysmography and electrodermal activity) and physical\n(accelerometer) contexts. In this regard, we develop a Multi-Modal Siamese\nConvolutional Neural Network (mmSNN) model. The framework learns the spatial\nand temporal information individually and combines them together in a modified\nweighted cost with an objective of predicting a person's identity. We evaluated\nour proposed model using real-time collected data from 3 collected datasets and\none publicly available dataset. Our proposed framework shows that PPG-based\nbreathing rate and heart rate in conjunction with hand gesture contexts can be\nutilized by attackers to re-identify user's identity (max. 71%) from HIPAA\ncompliant wearable data. Given publicly placed camera can estimate heart rate\nand breathing rate along with hand gestures remotely, person re-identification\nusing them imposes a significant threat to future HIPAA compliant server which\nrequires a better encryption method to store wearable healthcare data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:10:17 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Alam", "Mohammad Arif Ul", ""]]}, {"id": "2106.11942", "submitter": "Abraham Smith", "authors": "Abraham George Smith, Jens Petersen, Cynthia Terrones-Campos, Anne\n  Kiil Berthelsen, Nora Jarrett Forbes, Sune Darkner, Lena Specht, and Ivan\n  Richter Vogelius", "title": "RootPainter3D: Interactive-machine-learning enables rapid and accurate\n  contouring for radiotherapy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Organ-at-risk contouring is still a bottleneck in radiotherapy, with many\ndeep learning methods falling short of promised results when evaluated on\nclinical data. We investigate the accuracy and time-savings resulting from the\nuse of an interactive-machine-learning method for an organ-at-risk contouring\ntask. We compare the method to the Eclipse contouring software and find strong\nagreement with manual delineations, with a dice score of 0.95. The annotations\ncreated using corrective-annotation also take less time to create as more\nimages are annotated, resulting in substantial time savings compared to manual\nmethods, with hearts that take 2 minutes and 2 seconds to delineate on average,\nafter 923 images have been delineated, compared to 7 minutes and 1 seconds when\ndelineating manually. Our experiment demonstrates that\ninteractive-machine-learning with corrective-annotation provides a fast and\naccessible way for non computer-scientists to train deep-learning models to\nsegment their own structures of interest as part of routine clinical workflows.\n  Source code is available at\n\\href{https://github.com/Abe404/RootPainter3D}{this HTTPS URL}.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:26:58 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Smith", "Abraham George", ""], ["Petersen", "Jens", ""], ["Terrones-Campos", "Cynthia", ""], ["Berthelsen", "Anne Kiil", ""], ["Forbes", "Nora Jarrett", ""], ["Darkner", "Sune", ""], ["Specht", "Lena", ""], ["Vogelius", "Ivan Richter", ""]]}, {"id": "2106.12113", "submitter": "Reuth Mirsky", "authors": "Reuth Mirsky and Xuesu Xiao and Justin Hart and Peter Stone", "title": "Prevention and Resolution of Conflicts in Social Navigation -- a Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the approaching goal of having robots collaborate in shared human-robot\nenvironments, navigation in this context becomes both crucial and desirable.\nRecent developments in robotics have encountered and tackled some of the\nchallenges of navigating in mixed human-robot environments, and in recent years\nwe observe a surge of related work that specifically targets the question of\nhow to handle conflicts between agents in social navigation. These\ncontributions offer models, algorithms, and evaluation metrics, however as this\nresearch area is inherently interdisciplinary, many of the relevant papers are\nnot comparable and there is no standard vocabulary between the researchers.\n  The main goal of this survey is to bridge this gap by proposing such a common\nlanguage, using it to survey existing work, and highlighting open problems. It\nstarts by defining a conflict in social navigation, and offers a detailed\ntaxonomy of its components. This survey then maps existing work while\ndiscussing papers using the framing of the proposed taxonomy. Finally, this\npaper propose some future directions and problems that are currently in the\nfrontier of social navigation to help focus research efforts.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 01:10:22 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Mirsky", "Reuth", ""], ["Xiao", "Xuesu", ""], ["Hart", "Justin", ""], ["Stone", "Peter", ""]]}, {"id": "2106.12183", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "A Review of Assistive Technologies for Activities of Daily Living of\n  Elderly", "comments": null, "journal-ref": "Book chapter in: Assisted Living: Current Issues and Challenges,\n  ISBN: 978-1-53618-446-4, 2020 Nova Science Publishers, Pp. 61 - 84", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the distinct features of this century has been the population of older\nadults which has been on a constant rise. Elderly people have several needs and\nrequirements due to physical disabilities, cognitive issues, weakened memory\nand disorganized behavior, that they face with increasing age. The extent of\nthese limitations also differs according to the varying diversities in elderly,\nwhich include age, gender, background, experience, skills, knowledge and so on.\nThese varying needs and challenges with increasing age, limits abilities of\nolder adults to perform Activities of Daily Living (ADLs) in an independent\nmanner. To add to it, the shortage of caregivers creates a looming need for\ntechnology-based services for elderly people, to assist them in performing\ntheir daily routine tasks to sustain their independent living and active aging.\nTo address these needs, this work consists of making three major contributions\nin this field. First, it provides a rather comprehensive review of assisted\nliving technologies aimed at helping elderly people to perform ADLs. Second,\nthe work discusses the challenges identified through this review, that\ncurrently exist in the context of implementation of assisted living services\nfor elderly care in Smart Homes and Smart Cities. Finally, the work also\noutlines an approach for implementation, extension and integration of the\nexisting works in this field for development of a much-needed framework that\ncan provide personalized assistance and user-centered behavior interventions to\nelderly as per their varying and ever-changing needs.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 06:17:49 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2106.12207", "submitter": "Utkarsh Soni", "authors": "Utkarsh Soni, Sarath Sreedharan, Subbarao Kambhampati", "title": "Not all users are the same: Providing personalized explanations for\n  sequential decision making problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in designing autonomous agents that can work\nalongside humans. Such agents will undoubtedly be expected to explain their\nbehavior and decisions. While generating explanations is an actively researched\ntopic, most works tend to focus on methods that generate explanations that are\none size fits all. As in the specifics of the user-model are completely\nignored. The handful of works that look at tailoring their explanation to the\nuser's background rely on having specific models of the users (either analytic\nmodels or learned labeling models). The goal of this work is thus to propose an\nend-to-end adaptive explanation generation system that begins by learning the\ndifferent types of users that the agent could interact with. Then during the\ninteraction with the target user, it is tasked with identifying the type on the\nfly and adjust its explanations accordingly. The former is achieved by a\ndata-driven clustering approach while for the latter, we compile our\nexplanation generation problem into a POMDP. We demonstrate the usefulness of\nour system on two domains using state-of-the-art POMDP solvers. We also report\nthe results of a user study that investigates the benefits of providing\npersonalized explanations in a human-robot interaction setting.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 07:46:19 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Soni", "Utkarsh", ""], ["Sreedharan", "Sarath", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2106.12264", "submitter": "Enrica Loria", "authors": "Enrica Loria and Alessia Antelmi and Johanna Pirker", "title": "Comparing the Structures and Characteristics of Different Game Social\n  Networks -- The Steam Case", "comments": "accepted for publication in IEEE Conference on Games (CoG) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most games, social connections are an essential part of the gaming\nexperience. Players connect in communities inside or around games and form\nfriendships, which can be translated into other games or even in the real\nworld. Recent research has investigated social phenomena within the player\nsocial network of several multiplayer games, yet we still know very little\nabout how these networks are shaped and formed. Specifically, we are unaware of\nhow the game type and its mechanics are related to its community structure and\nhow those structures vary in different games. This paper presents an initial\nanalysis of Steam users and how friendships on Steam are formed around 200\ngames. We examine the friendship graphs of these 200 games by dividing them\ninto clusters to compare their network properties and their specific\ncharacteristics (e.g., genre, game elements, and mechanics). We found how the\nSteam user-defined tags better characterized the clusters than the game genre,\nsuggesting that how players perceive and use the game also reflects how they\nconnect in the community. Moreover, team-based games are associated with more\ncohesive and clustered networks than games with a stronger single-player focus,\nsupporting the idea that playing together in teams more likely produces social\ncapital (i.e., Steam friendships).\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 09:41:30 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 07:56:41 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Loria", "Enrica", ""], ["Antelmi", "Alessia", ""], ["Pirker", "Johanna", ""]]}, {"id": "2106.12277", "submitter": "Enrica Loria", "authors": "Simone Petrosino, Enrica Loria, Johanna Pirker", "title": "#StayHome Playing LoL -- Analyzing Players' Activity and Social Bonds in\n  League of Legends During Covid-19 Lockdowns", "comments": "accepted for publication in Foundations of Digital Games (FDG) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are social beings. It is therefore not surprising that the social\ndistancing and movement restrictions associated with the Covid-19 pandemic had\nsevere consequences for the well-being of large sections of the population,\nleading to increased loneliness and related mental diseases. Many people found\nemotional shelter in online multiplayer games, which have already proven to be\ngreat social incubators. While the positive effect games had on individuals has\nbecome evident, we are still unaware of how and if games fostered the\nfundamental need for connectedness that people sought. In other words: how have\nthe social bonds and interaction patterns of players changed with the advent of\nthe pandemic? For this purpose, we analyzed one year of data from an online\nmultiplayer game (League of Legends) to observe the impact of Covid-19 on\nplayer assiduity and sociality in three different geographical regions (i.e.,\nEurope, North America, and South Korea). Our results show a strong relationship\nbetween the development of Covid-19 restrictions and player activity, together\nwith more robust and recurrent social bonds, especially for players committed\nto the game. Additionally, players with reinforced social bonds-i.e., people\nplayed with similar teammates-were more likely to stay in the game even once\nthe restrictions were lifted.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 09:55:25 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 07:55:26 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Petrosino", "Simone", ""], ["Loria", "Enrica", ""], ["Pirker", "Johanna", ""]]}, {"id": "2106.12314", "submitter": "Daniel Buschek", "authors": "Oliver Schmitt, Daniel Buschek", "title": "CharacterChat: Supporting the Creation of Fictional Characters through\n  Conversation and Progressive Manifestation with a Chatbot", "comments": "14 pages, 2 figures, 2 tables; ACM C&C 2021", "journal-ref": null, "doi": "10.1145/3450741.3465253", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CharacterChat, a concept and chatbot to support writers in\ncreating fictional characters. Concretely, writers progressively turn the bot\ninto their imagined character through conversation. We iteratively developed\nCharacterChat in a user-centred approach, starting with a survey on character\ncreation with writers (N=30), followed by two qualitative user studies (N=7 and\nN=8). Our prototype combines two modes: (1) Guided prompts help writers define\ncharacter attributes (e.g. User: \"Your name is Jane.\"), including suggestions\nfor attributes (e.g. Bot: \"What is my main motivation?\") and values, realised\nas a rule-based system with a concept network. (2) Open conversation with the\nchatbot helps writers explore their character and get inspiration, realised\nwith a language model that takes into account the defined character attributes.\nOur user studies reveal benefits particularly for early stages of character\ncreation, and challenges due to limited conversational capabilities. We\nconclude with lessons learned and ideas for future work.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 11:22:27 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Schmitt", "Oliver", ""], ["Buschek", "Daniel", ""]]}, {"id": "2106.12447", "submitter": "Judy Borowski", "authors": "Roland S. Zimmermann, Judy Borowski, Robert Geirhos, Matthias Bethge,\n  Thomas S. A. Wallis, Wieland Brendel", "title": "How Well do Feature Visualizations Support Causal Understanding of CNN\n  Activations?", "comments": "ICML 2021 XAI workshop version. Joint first and last authors. Project\n  website at\n  https://brendel-group.github.io/causal-understanding-via-visualizations/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One widely used approach towards understanding the inner workings of deep\nconvolutional neural networks is to visualize unit responses via activation\nmaximization. Feature visualizations via activation maximization are thought to\nprovide humans with precise information about the image features that cause a\nunit to be activated. If this is indeed true, these synthetic images should\nenable humans to predict the effect of an intervention, such as whether\noccluding a certain patch of the image (say, a dog's head) changes a unit's\nactivation. Here, we test this hypothesis by asking humans to predict which of\ntwo square occlusions causes a larger change to a unit's activation. Both a\nlarge-scale crowdsourced experiment and measurements with experts show that on\naverage, the extremely activating feature visualizations by Olah et al. (2017)\nindeed help humans on this task ($67 \\pm 4\\%$ accuracy; baseline performance\nwithout any visualizations is $60 \\pm 3\\%$). However, they do not provide any\nsignificant advantage over other visualizations (such as e.g. dataset samples),\nwhich yield similar performance ($66 \\pm 3\\%$ to $67 \\pm 3\\%$ accuracy). Taken\ntogether, we propose an objective psychophysical task to quantify the benefit\nof unit-level interpretability methods for humans, and find no evidence that\nfeature visualizations provide humans with better \"causal understanding\" than\nsimple alternative visualizations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 14:52:23 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zimmermann", "Roland S.", ""], ["Borowski", "Judy", ""], ["Geirhos", "Robert", ""], ["Bethge", "Matthias", ""], ["Wallis", "Thomas S. A.", ""], ["Brendel", "Wieland", ""]]}, {"id": "2106.12458", "submitter": "Sarah E Carter", "authors": "Sarah E. Carter", "title": "Is Downloading this App Consistent with my Values? Conceptualizing a\n  Value-Centered Privacy Assistant", "comments": "This is an Author Accepted Manuscript for I3E2021, to be held in\n  September 2021. The final version will be available at Springer. A link and\n  the DOI will be added in due course", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital privacy notices aim to provide users with information to make\ninformed decisions. They are, however, fraught with difficulties. Instead, I\npropose that data privacy decisions can be understood as an expression of user\nvalues. To optimize this value expression, I further propose the creation of a\nvalue-centered privacy assistant (VcPA). Here, I preliminary explore how a VcPA\ncould enhance user value expression by utilizing three user scenarios in the\ncontext of considering whether or not to download an environmental application,\nthe OpenLitterMap app. These scenarios are conceptually constructed from\nestablished privacy user groups - the privacy fundamentalists; the privacy\npragmatists; and the privacy unconcerned. I conclude that the VcPA best\nfacilitates user value expression of the privacy fundamentalists. In contrast,\nthe value expression of the privacy pragmatists and the privacy unconcerned\ncould be enhanced or hindered depending on the context and their internal\nstates. Possible implications for optimal VcPA design are also discussed.\nFollowing this initial conceptual exploration of VcPAs, further empirical\nresearch will be required to demonstrate the effectiveness of the VcPA system\nin real-world settings.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 15:08:58 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Carter", "Sarah E.", ""]]}, {"id": "2106.12505", "submitter": "Sam Kumar", "authors": "Sam Kumar, Michael P Andersen, David E. Culler", "title": "Mr. Plotter: Unifying Data Reduction Techniques in Storage and\n  Visualization Systems", "comments": "14 pages; Originally published in May 2018 as a technical report in\n  the UC Berkeley EECS Technical Report Series (see\n  https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-85.html)", "journal-ref": null, "doi": null, "report-no": "Technical Report No. UCB/EECS-2018-85", "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the rate of data collection continues to grow rapidly, developing\nvisualization tools that scale to immense data sets is a serious and\never-increasing challenge. Existing approaches generally seek to decouple\nstorage and visualization systems, performing just-in-time data reduction to\ntransparently avoid overloading the visualizer. We present a new architecture\nin which the visualizer and data store are tightly coupled. Unlike systems that\nread raw data from storage, the performance of our system scales linearly with\nthe size of the final visualization, essentially independent of the size of the\ndata. Thus, it scales to massive data sets while supporting interactive\nperformance (sub-100 ms query latency). This enables a new class of\nvisualization clients that automatically manage data, quickly and transparently\nrequesting data from the underlying database without requiring the user to\nexplicitly initiate queries. It lays a groundwork for supporting truly\ninteractive exploration of big data and opens new directions for research on\nscalable information visualization systems.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:23:18 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kumar", "Sam", ""], ["Andersen", "Michael P", ""], ["Culler", "David E.", ""]]}, {"id": "2106.12633", "submitter": "Anthony Steed", "authors": "Anthony Steed, Tuukka M. Takala, Daniel Archer, Wallace Lages, Robert\n  W. Lindeman", "title": "Directions for 3D User Interface Research from Consumer VR Games", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the continuing development of affordable immersive virtual reality (VR)\nsystems, there is now a growing market for consumer content. The current form\nof consumer systems is not dissimilar to the lab-based VR systems of the past\n30 years: the primary input mechanism is a head-tracked display and one or two\ntracked hands with buttons and joysticks on hand-held controllers. Over those\n30 years, a very diverse academic literature has emerged that covers design and\nergonomics of 3D user interfaces (3DUIs). However, the growing consumer market\nhas engaged a very broad range of creatives that have built a very diverse set\nof designs. Sometimes these designs adopt findings from the academic\nliterature, but other times they experiment with completely novel or\ncounter-intuitive mechanisms. In this paper and its online adjunct, we report\non novel 3DUI design patterns that are interesting from both design and\nresearch perspectives: they are highly novel, potentially broadly re-usable\nand/or suggest interesting avenues for evaluation. The supplemental material,\nwhich is a living document, is a crowd-sourced repository of interesting\npatterns. This paper is a curated snapshot of those patterns that were\nconsidered to be the most fruitful for further elaboration.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 19:05:01 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Steed", "Anthony", ""], ["Takala", "Tuukka M.", ""], ["Archer", "Daniel", ""], ["Lages", "Wallace", ""], ["Lindeman", "Robert W.", ""]]}, {"id": "2106.12645", "submitter": "Zachary Eberhart", "authors": "Zachary Eberhart, Aakash Bansal, Collin McMillan", "title": "A Wizard of Oz Study Simulating API Usage Dialogues with a Virtual\n  Assistant", "comments": null, "journal-ref": null, "doi": "10.1109/TSE.2020.3040935", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Assistant technology is rapidly proliferating to improve productivity\nin a variety of tasks. While several virtual assistants for everyday tasks are\nwell-known (e.g., Siri, Cortana, Alexa), assistants for specialty tasks such as\nsoftware engineering are rarer. One key reason software engineering assistants\nare rare is that very few experimental datasets are available and suitable for\ntraining the AI that is the bedrock of current virtual assistants. In this\npaper, we present a set of Wizard of Oz experiments that we designed to build a\ndataset for creating a virtual assistant. Our target is a hypothetical virtual\nassistant for helping programmers use APIs. In our experiments, we recruited 30\nprofessional programmers to complete programming tasks using two APIs. The\nprogrammers interacted with a simulated virtual assistant for help - the\nprogrammers were not aware that the assistant was actually operated by human\nexperts. We then annotated the dialogue acts in the corpus along four\ndimensions: illocutionary intent, API information type(s), backward-facing\nfunction, and traceability to specific API components. We observed a diverse\nrange of interactions that will facilitate the development of dialogue\nstrategies for virtual assistants for API usage.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 20:04:38 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Eberhart", "Zachary", ""], ["Bansal", "Aakash", ""], ["McMillan", "Collin", ""]]}, {"id": "2106.12767", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Dongjin Choi and Sara Evensen and \\c{C}a\\u{g}atay Demiralp and Estevam\n  Hruschka", "title": "TagRuler: Interactive Tool for Span-Level Data Programming by\n  Demonstration", "comments": "WWW'21 Demo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.DB cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite rapid developments in the field of machine learning research,\ncollecting high-quality labels for supervised learning remains a bottleneck for\nmany applications. This difficulty is exacerbated by the fact that\nstate-of-the-art models for NLP tasks are becoming deeper and more complex,\noften increasing the amount of training data required even for fine-tuning.\nWeak supervision methods, including data programming, address this problem and\nreduce the cost of label collection by using noisy label sources for\nsupervision. However, until recently, data programming was only accessible to\nusers who knew how to program. To bridge this gap, the Data Programming by\nDemonstration framework was proposed to facilitate the automatic creation of\nlabeling functions based on a few examples labeled by a domain expert. This\nframework has proven successful for generating high-accuracy labeling models\nfor document classification. In this work, we extend the DPBD framework to\nspan-level annotation tasks, arguably one of the most time-consuming NLP\nlabeling tasks. We built a novel tool, TagRuler, that makes it easy for\nannotators to build span-level labeling functions without programming and\nencourages them to explore trade-offs between different labeling models and\nactive learning strategies. We empirically demonstrated that an annotator could\nachieve a higher F1 score using the proposed tool compared to manual labeling\nfor different span-level annotation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 04:49:42 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Choi", "Dongjin", ""], ["Evensen", "Sara", ""], ["Demiralp", "\u00c7a\u011fatay", ""], ["Hruschka", "Estevam", ""]]}, {"id": "2106.12839", "submitter": "Zipeng Liu", "authors": "Zipeng Liu, Yang Wang, J\\\"urgen Bernard, Tamara Munzner", "title": "Visualizing Graph Neural Networks with CorGIE: Corresponding a Graph to\n  Its Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph neural networks (GNNs) are a class of powerful machine learning tools\nthat model node relations for making predictions of nodes or links. GNN\ndevelopers rely on quantitative metrics of the predictions to evaluate a GNN,\nbut similar to many other neural networks, it is difficult for them to\nunderstand if the GNN truly learns characteristics of a graph as expected. We\npropose an approach to corresponding an input graph to its node embedding (aka\nlatent space), a common component of GNNs that is later used for prediction. We\nabstract the data and tasks, and develop an interactive multi-view interface\ncalled CorGIE to instantiate the abstraction. As the key function in CorGIE, we\npropose the K-hop graph layout to show topological neighbors in hops and their\nclustering structure. To evaluate the functionality and usability of CorGIE, we\npresent how to use CorGIE in two usage scenarios, and conduct a case study with\ntwo GNN experts.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 08:59:53 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Liu", "Zipeng", ""], ["Wang", "Yang", ""], ["Bernard", "J\u00fcrgen", ""], ["Munzner", "Tamara", ""]]}, {"id": "2106.12857", "submitter": "Luigi Asprino", "authors": "Luigi Asprino, Christian Colonna, Misael Mongiov\\`i, Margherita\n  Porena, Valentina Presutti", "title": "Pattern-based Visualization of Knowledge Graphs", "comments": "16 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel approach to knowledge graph visualization based on\nontology design patterns. This approach relies on OPLa (Ontology Pattern\nLanguage) annotations and on a catalogue of visual frames, which are associated\nwith foundational ontology design patterns. We demonstrate that this approach\nsignificantly reduces the cognitive load required to users for visualizing and\ninterpreting a knowledge graph and guides the user in exploring it through\nmeaningful thematic paths provided by ontology patterns.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:43:15 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Asprino", "Luigi", ""], ["Colonna", "Christian", ""], ["Mongiov\u00ec", "Misael", ""], ["Porena", "Margherita", ""], ["Presutti", "Valentina", ""]]}, {"id": "2106.12880", "submitter": "Michael Winter", "authors": "Michael Winter and R\\\"udiger Pryss and Matthias Fink and Manfred\n  Reichert", "title": "Towards Measuring and Quantifying the Comprehensibility of Process\n  Models -- The Process Model Comprehension Framework", "comments": "16 pages, 5 Figures, 1 Table, additional materials in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Process models constitute crucial artifacts in modern information systems\nand, hence, the proper comprehension of these models is of utmost importance in\nthe utilization of such systems. Generally, process models are considered from\ntwo different perspectives: process modelers and readers. Both perspectives\nshare similarities and differences in the comprehension of process models\n(e.g., diverse experiences when working with process models). The literature\nproposed many rules and guidelines to ensure a proper comprehension of process\nmodels for both perspectives. As a novel contribution in this context, this\npaper introduces the Process Model Comprehension Framework (PMCF) as a first\nstep towards the measurement and quantification of the perspectives of process\nmodelers and readers as well as the interaction of both regarding the\ncomprehension of process models. Therefore, the PMCF describes an Evaluation\nTheory Tree based on the Communication Theory as well as the Conceptual\nModeling Quality Framework and considers a total of 96 quality metrics in order\nto quantify process model comprehension. Furthermore, the PMCF was evaluated in\na survey with 131 participants and has been implemented as well as applied\nsuccessfully in a practical case study including 33 participants. To conclude,\nthe PMCF allows for the identification of pitfalls and provides related\ninformation about how to assist process modelers as well as readers in order to\nfoster and enable a proper comprehension of process models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 10:27:28 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Winter", "Michael", ""], ["Pryss", "R\u00fcdiger", ""], ["Fink", "Matthias", ""], ["Reichert", "Manfred", ""]]}, {"id": "2106.12937", "submitter": "Alexandra Moringen", "authors": "Alexandra Moringen, S\\\"oren R\\\"uttgers, Luisa Zintgraf, Jason\n  Friedman, Helge Ritter", "title": "Optimizing piano practice with a utility-based scaffold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical part of learning to play the piano is the progression through a\nseries of practice units that focus on individual dimensions of the skill, such\nas hand coordination, correct posture, or correct timing. Ideally, a focus on a\nparticular practice method should be made in a way to maximize the learner's\nprogress in learning to play the piano. Because we each learn differently, and\nbecause there are many choices for possible piano practice tasks and methods,\nthe set of practice tasks should be dynamically adapted to the human learner.\nHowever, having a human teacher guide individual practice is not always\nfeasible since it is time consuming, expensive, and not always available.\nInstead, we suggest to optimize in the space of practice methods, the so-called\npractice modes. The proposed optimization process takes into account the skills\nof the individual learner and their history of learning. In this work we\npresent a modeling framework to guide the human learner through the learning\nprocess by choosing practice modes that have the highest expected utility\n(i.e., improvement in piano playing skill). To this end, we propose a human\nlearner utility model based on a Gaussian process, and exemplify the model\ntraining and its application for practice scaffolding on an example of\nsimulated human learners.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 14:05:00 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Moringen", "Alexandra", ""], ["R\u00fcttgers", "S\u00f6ren", ""], ["Zintgraf", "Luisa", ""], ["Friedman", "Jason", ""], ["Ritter", "Helge", ""]]}, {"id": "2106.13213", "submitter": "Paul Pu Liang", "authors": "Paul Pu Liang, Terrance Liu, Anna Cai, Michal Muszynski, Ryo Ishii,\n  Nicholas Allen, Randy Auerbach, David Brent, Ruslan Salakhutdinov,\n  Louis-Philippe Morency", "title": "Learning Language and Multimodal Privacy-Preserving Markers of Mood from\n  Mobile Data", "comments": "ACL 2021. arXiv admin note: substantial text overlap with\n  arXiv:2012.02359", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health conditions remain underdiagnosed even in countries with common\naccess to advanced medical care. The ability to accurately and efficiently\npredict mood from easily collectible data has several important implications\nfor the early detection, intervention, and treatment of mental health\ndisorders. One promising data source to help monitor human behavior is daily\nsmartphone usage. However, care must be taken to summarize behaviors without\nidentifying the user through personal (e.g., personally identifiable\ninformation) or protected (e.g., race, gender) attributes. In this paper, we\nstudy behavioral markers of daily mood using a recent dataset of mobile\nbehaviors from adolescent populations at high risk of suicidal behaviors. Using\ncomputational models, we find that language and multimodal representations of\nmobile typed text (spanning typed characters, words, keystroke timings, and app\nusage) are predictive of daily mood. However, we find that models trained to\npredict mood often also capture private user identities in their intermediate\nrepresentations. To tackle this problem, we evaluate approaches that obfuscate\nuser identity while remaining predictive. By combining multimodal\nrepresentations with privacy-preserving learning, we are able to push forward\nthe performance-privacy frontier.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 17:46:03 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Liang", "Paul Pu", ""], ["Liu", "Terrance", ""], ["Cai", "Anna", ""], ["Muszynski", "Michal", ""], ["Ishii", "Ryo", ""], ["Allen", "Nicholas", ""], ["Auerbach", "Randy", ""], ["Brent", "David", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2106.13265", "submitter": "Parthasarathy Suryanarayanan", "authors": "Parthasarathy Suryanarayanan, Prithwish Chakraborty, Piyush Madan,\n  Kibichii Bore, William Ogallo, Rachita Chandra, Mohamed Ghalwash, Italo\n  Buleje, Sekou Remy, Shilpa Mahatma, Pablo Meyer, Jianying Hu", "title": "Disease Progression Modeling Workbench 360", "comments": "Submitted to OHDSI Collaborator Showcase, 2021\n  (https://www.ohdsi.org/2021-collaborator-showcase)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we introduce Disease Progression Modeling workbench 360 (DPM360)\nopensource clinical informatics framework for collaborative research and\ndelivery of healthcare AI. DPM360, when fully developed, will manage the entire\nmodeling life cycle, from data analysis (e.g., cohort identification) to\nmachine learning algorithm development and prototyping. DPM360 augments the\nadvantages of data model standardization and tooling (OMOP-CDM, Athena, ATLAS)\nprovided by the widely-adopted OHDSI initiative with a powerful machine\nlearning training framework, and a mechanism for rapid prototyping through\nautomatic deployment of models as containerized services to a cloud\nenvironment.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 18:30:39 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Suryanarayanan", "Parthasarathy", ""], ["Chakraborty", "Prithwish", ""], ["Madan", "Piyush", ""], ["Bore", "Kibichii", ""], ["Ogallo", "William", ""], ["Chandra", "Rachita", ""], ["Ghalwash", "Mohamed", ""], ["Buleje", "Italo", ""], ["Remy", "Sekou", ""], ["Mahatma", "Shilpa", ""], ["Meyer", "Pablo", ""], ["Hu", "Jianying", ""]]}, {"id": "2106.13388", "submitter": "Bo Yang", "authors": "Bo Yang, Koichiro Inoue, Satoshi Kitazaki, and Kimihiko Nakano", "title": "Influences on Drivers' Understandings of Systems by Presenting Image\n  Recognition Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is essential to help drivers have appropriate understandings of level 2\nautomated driving systems for keeping driving safety. A human machine interface\n(HMI) was proposed to present real time results of image recognition by the\nautomated driving systems to drivers. It was expected that drivers could better\nunderstand the capabilities of the systems by observing the proposed HMI.\nDriving simulator experiments with 18 participants were preformed to evaluate\nthe effectiveness of the proposed system. Experimental results indicated that\nthe proposed HMI could effectively inform drivers of potential risks\ncontinuously and help drivers better understand the level 2 automated driving\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:10:03 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Yang", "Bo", ""], ["Inoue", "Koichiro", ""], ["Kitazaki", "Satoshi", ""], ["Nakano", "Kimihiko", ""]]}, {"id": "2106.13397", "submitter": "Youjia Zhou", "authors": "Youjia Zhou, Methun Kamruzzaman, Patrick Schnable, Bala\n  Krishnamoorthy, Ananth Kalyanaraman, Bei Wang", "title": "Pheno-Mapper: An Interactive Toolbox for the Visual Exploration of\n  Phenomics Data", "comments": "This is a preprint version. For a published version, please refer to\n  ACM DOI: 10.1145/3459930.3469511", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.HC math.AT q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput technologies to collect field data have made observations\npossible at scale in several branches of life sciences. The data collected can\nrange from the molecular level (genotypes) to physiological (phenotypic traits)\nand environmental observations (e.g., weather, soil conditions). These vast\nswathes of data, collectively referred to as phenomics data, represent a\ntreasure trove of key scientific knowledge on the dynamics of the underlying\nbiological system. However, extracting information and insights from these\ncomplex datasets remains a significant challenge owing to their\nmultidimensionality and lack of prior knowledge about their complex structure.\nIn this paper, we present Pheno-Mapper, an interactive toolbox for the\nexploratory analysis and visualization of large-scale phenomics data. Our\napproach uses the mapper framework to perform a topological analysis of the\ndata, and subsequently render visual representations with built-in data\nanalysis and machine learning capabilities. We demonstrate the utility of this\nnew tool on real-world plant (e.g., maize) phenomics datasets. In comparison to\nexisting approaches, the main advantage of Pheno-Mapper is that it provides\nrich, interactive capabilities in the exploratory analysis of phenomics data,\nand it integrates visual analytics with data analysis and machine learning in\nan easily extensible way. In particular, Pheno-Mapper allows the interactive\nselection of subpopulations guided by a topological summary of the data and\napplies data mining and machine learning to these selected subpopulations for\nin-depth exploration.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:55:45 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 05:08:50 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zhou", "Youjia", ""], ["Kamruzzaman", "Methun", ""], ["Schnable", "Patrick", ""], ["Krishnamoorthy", "Bala", ""], ["Kalyanaraman", "Ananth", ""], ["Wang", "Bei", ""]]}, {"id": "2106.13494", "submitter": "Katrin Glinka", "authors": "Katrin Glinka, Patrick Tobias Fischer, Claudia M\\\"uller-Birn, Silke\n  Krohn", "title": "Investigating Modes of Activity and Guidance for Mediating Museum\n  Exhibits in Mixed Reality", "comments": null, "journal-ref": "Kultur und Informatik: Extended Reality. Berlin: vwh (2020)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present an exploratory case study describing the design and realisation of\na ''pure mixed reality'' application in a museum setting, where we investigate\nthe potential of using Microsoft's HoloLens for object-centred museum\nmediation. Our prototype supports non-expert visitors observing a sculpture by\noffering interpretation that is linked to visual properties of the museum\nobject. The design and development of our research prototype is based on a\ntwo-stage visitor observation study and a formative study we conducted prior to\nthe design of the application. We present a summary of our findings from these\nstudies and explain how they have influenced our user-centred content creation\nand the interaction design of our prototype. We are specifically interested in\ninvestigating to what extent different constructs of initiative influence the\nlearning and user experience. Thus, we detail three modes of activity that we\nrealised in our prototype. Our case study is informed by research in the area\nof human-computer interaction, the humanities and museum practice. Accordingly,\nwe discuss core concepts, such as gaze-based interaction, object-centred\nlearning, presence, and modes of activity and guidance with a transdisciplinary\nperspective.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 08:17:34 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Glinka", "Katrin", ""], ["Fischer", "Patrick Tobias", ""], ["M\u00fcller-Birn", "Claudia", ""], ["Krohn", "Silke", ""]]}, {"id": "2106.13504", "submitter": "Alan Smeaton", "authors": "Hyowon Lee, Mingming Liu, Michael Scriney, Alan F. Smeaton", "title": "Usage-based Summaries of Learning Videos", "comments": "16th European Conference on Technology-Enhanced Learning (EC-TEL),\n  Bozen-Bolzano, Italy (online), September 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of the delivery of University education is now by synchronous or\nasynchronous video. For students, one of the challenges is managing the sheer\nvolume of such video material as video presentations of taught material are\ndifficult to abbreviate and summarise because they do not have highlights which\nstand out. Apart from video bookmarks there are no tools available to determine\nwhich parts of video content should be replayed at revision time or just before\nexaminations. We have developed and deployed a digital library for managing\nvideo learning material which has many dozens of hours of short-form video\ncontent from a range of taught courses for hundreds of students at\nundergraduate level. Through a web browser we allow students to access and play\nthese videos and we log their anonymised playback usage. From these logs we\nscore to each segment of each video based on the amount of playback it receives\nfrom across all students, whether the segment has been re-wound and re-played\nin the same student session, whether the on-screen window is the window in\nfocus on the student's desktop/laptop, and speed of playback. We also\nincorporate negative scoring if a video segment is skipped or fast-forward, and\noverarching all this we include a decay function based on recency of playback,\nso the most recent days of playback contribute more to the video segment\nscores. For each video in the library we present a usage-based graph which\nallows students to see which parts of each video attract the most playback from\ntheir peers, which helps them select material at revision time. Usage of the\nsystem is fully anonymised and GDPR-compliant.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 08:55:11 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Lee", "Hyowon", ""], ["Liu", "Mingming", ""], ["Scriney", "Michael", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2106.13512", "submitter": "Alan Smeaton", "authors": "Aparajita Dey-Plissonneau, Hyowon Lee, Vincent Pradier, Michael\n  Scriney, Alan F. Smeaton", "title": "The L2L System for Second Language Learning Using Visualised Zoom Calls\n  Among Students", "comments": "16th European Conference on Technology-Enhanced Learning (EC-TEL),\n  Bozen-Bolzano, Italy (online), September 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important part of second language learning is conversation which is best\npractised with speakers whose native language is the language being learned. We\nfacilitate this by pairing students from different countries learning each\nothers' native language. Mixed groups of students have Zoom calls, half in one\nlanguage and half in the other, in order to practice and improve their\nconversation skills. We use Zoom video recordings with audio transcripts\nenabled which generates recognised speech from which we extract timestamped\nutterances and calculate and visualise conversation metrics on a dashboard. A\ntimeline highlights each utterance, colour coded per student, with links to the\nvideo in a playback window. L2L was deployed for a semester and recorded almost\n250 hours of zoom meetings. The conversation metrics visualised on the\ndashboard are a beneficial asset for both students and lecturers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:06:58 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Dey-Plissonneau", "Aparajita", ""], ["Lee", "Hyowon", ""], ["Pradier", "Vincent", ""], ["Scriney", "Michael", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2106.13528", "submitter": "Tony Russell-Rose", "authors": "Tony Russell-Rose, Philip Gooch, Udo Kruschwitz", "title": "Interactive query expansion for professional search applications", "comments": "34 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge workers (such as healthcare information professionals, patent\nagents and recruitment professionals) undertake work tasks where search forms a\ncore part of their duties. In these instances, the search task is often complex\nand time-consuming and requires specialist expert knowledge to formulate\naccurate search strategies. Interactive features such as query expansion can\nplay a key role in supporting these tasks. However, generating query\nsuggestions within a professional search context requires that consideration be\ngiven to the specialist, structured nature of the search strategies they\nemploy. In this paper, we investigate a variety of query expansion methods\napplied to a collection of Boolean search strategies used in a variety of\nreal-world professional search tasks. The results demonstrate the utility of\ncontext-free distributional language models and the value of using linguistic\ncues such as ngram order to optimise the balance between precision and recall.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:40:46 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Russell-Rose", "Tony", ""], ["Gooch", "Philip", ""], ["Kruschwitz", "Udo", ""]]}, {"id": "2106.13544", "submitter": "Marwa Gadala", "authors": "Marwa Gadala (1), Lorenzo Strigini (1), Peter Ayton (2) ((1) City,\n  University of London, (2) University of Leeds)", "title": "Improving Human Decisions by Adjusting the Alerting Thresholds for\n  Computer Alerting Tools According to User and Task Characteristics", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Objective: To investigate whether performance (number of correct decisions)\nof humans supported by a computer alerting tool can be improved by tailoring\nthe tool's alerting threshold (sensitivity/specificity combination) according\nto user ability and task difficulty. Background: Many researchers implicitly\nassume that for each tool there exists a single ideal threshold. But research\nshows the effects of alerting tools on decision errors to vary depending on\nvariables such as user ability and task difficulty. These findings motivated\nour investigation. Method: Forty-seven participants edited text passages, aided\nby a simulated spell-checker tool. We experimentally manipulated passage\ndifficulty and tool alerting threshold, measured participants' editing and\ndictation ability, and counted participants' decision errors (false positives +\nfalse negatives). We investigated whether alerting threshold, user ability,\ntask difficulty and their interactions affected error count. Results: Which\nalerting threshold better helped a user depended on an interaction between user\nability and passage difficulty. Some effects were large: for higher ability\nusers, a more sensitive threshold reduced errors by 30%, on the easier\npassages. Participants were not significantly more likely to prefer the\nalerting threshold with which they performed better. Conclusion: Adjusting\nalerting thresholds for individual users' ability and task difficulty could\nsubstantially improve effects of automated alerts on user decisions. This\npotential deserves further investigation. Improvement size and rules for\nadjustment will be application-specific. Application: Computer alerting tools\nhave critical roles in many domains. Designers should assess potential benefits\nof adjustable alerting thresholds for their specific CAT application. Guidance\nfor choosing thresholds will be essential for realizing these benefits in\npractice.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 10:37:01 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gadala", "Marwa", ""], ["Strigini", "Lorenzo", ""], ["Ayton", "Peter", ""]]}, {"id": "2106.13675", "submitter": "Sanskar Jethi", "authors": "Sanskar Jethi, Avinash Kumar Choudhary, Yash Gupta, Abhishek Chaudhary", "title": "Creating and Implementing a Smart Speaker", "comments": null, "journal-ref": "IT in Industry, Vol. 9, No.3, 2021", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have seen significant advancements in Artificial Intelligence and Machine\nLearning in the 21st century. It has enabled a new technology where we can have\na human-like conversation with the machines. The most significant use of this\nspeech recognition and contextual understanding technology exists in the form\nof a Smart Speaker. We have a wide variety of Smart Speaker products available\nto us. This paper aims to decode its creation and explain the technology that\nmakes these Speakers, \"Smart.\"\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 09:26:53 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Jethi", "Sanskar", ""], ["Choudhary", "Avinash Kumar", ""], ["Gupta", "Yash", ""], ["Chaudhary", "Abhishek", ""]]}, {"id": "2106.13740", "submitter": "Magy Seif El-Nasr", "authors": "Magy Seif El-Nasr, Casper Harteveld, Paul Fombelle, Truong-Huy Nguyen,\n  Paola Rizzo, Dylan Schouten, Abdelrahman Madkour, Chaima Jemmali, Erica\n  Kleinman, Nithesh Javvaji, Zhaoqing Teng, Extra Ludic Inc", "title": "Advancing Methodology for Social Science Research Using Alternate\n  Reality Games: Proof-of-Concept Through Measuring Individual Differences and\n  Adaptability and their impact on Team Performance", "comments": null, "journal-ref": "DARPA Report, 2018", "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While work in fields of CSCW (Computer Supported Collaborative Work),\nPsychology and Social Sciences have progressed our understanding of team\nprocesses and their effect performance and effectiveness, current methods rely\non observations or self-report, with little work directed towards studying team\nprocesses with quantifiable measures based on behavioral data. In this report\nwe discuss work tackling this open problem with a focus on understanding\nindividual differences and its effect on team adaptation, and further explore\nthe effect of these factors on team performance as both an outcome and a\nprocess. We specifically discuss our contribution in terms of methods that\naugment survey data and behavioral data that allow us to gain more insight on\nteam performance as well as develop a method to evaluate adaptation and\nperformance across and within a group. To make this problem more tractable we\nchose to focus on specific types of environments, Alternate Reality Games\n(ARGs), and for several reasons. First, these types of games involve setups\nthat are similar to a real-world setup, e.g., communication through slack or\nemail. Second, they are more controllable than real environments allowing us to\nembed stimuli if needed. Lastly, they allow us to collect data needed to\nunderstand decisions and communications made through the entire duration of the\nexperience, which makes team processes more transparent than otherwise\npossible. In this report we discuss the work we did so far and demonstrate the\nefficacy of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:22:22 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["El-Nasr", "Magy Seif", ""], ["Harteveld", "Casper", ""], ["Fombelle", "Paul", ""], ["Nguyen", "Truong-Huy", ""], ["Rizzo", "Paola", ""], ["Schouten", "Dylan", ""], ["Madkour", "Abdelrahman", ""], ["Jemmali", "Chaima", ""], ["Kleinman", "Erica", ""], ["Javvaji", "Nithesh", ""], ["Teng", "Zhaoqing", ""], ["Inc", "Extra Ludic", ""]]}, {"id": "2106.13742", "submitter": "Magy Seif El-Nasr", "authors": "Truong-Huy Dinh Nguyen, Magy Seif El-Nasr, Alessandro Canossa", "title": "Glyph: Visualization Tool for Understanding Problem Solving Strategies\n  in Puzzle Games", "comments": null, "journal-ref": "Foundations of Digital Games, 2015", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding player strategies is a key question when analyzing player\nbehavior both for academic researchers and industry practitioners. For game\ndesigners and game user researchers, it is important to gauge the distance\nbetween intended strategies and emergent strategies; this comparison allows\nidentification of glitches or undesirable behaviors. For academic researchers\nusing games for serious purposes such as education, the strategies adopted by\nplayers are indicative of their cognitive progress in relation to serious\ngoals, such as learning process. Current techniques and systems created to\naddress these needs present a few drawbacks. Qualitative methods are difficult\nto scale upwards to include large number of players and are prone to subjective\nbiases. Other approaches such as visualization and analytical tools are either\ndesigned to provide an aggregated overview of the data, losing the nuances of\nindividual player behaviors, or, in the attempt of accounting for individual\nbehavior, are not specifically designed to reduce the visual cognitive load. In\nthis work, we propose a novel visualization technique that specifically\naddresses the tasks of comparing behavior sequences in order to capture an\noverview of the strategies enacted by players and at the same time examine\nindividual player behaviors to identify differences and outliers. This approach\nallows users to form hypotheses about player strategies and verify them. We\ndemonstrate the effectiveness of the technique through a case study: utilizing\na prototype system to investigate data collected from a commercial educational\npuzzle game. While the prototype's usability can be improved, initial testing\nresults show that core features of the system proved useful to our potential\nusers for understanding player strategies.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:27:38 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Nguyen", "Truong-Huy Dinh", ""], ["El-Nasr", "Magy Seif", ""], ["Canossa", "Alessandro", ""]]}, {"id": "2106.13747", "submitter": "Magy Seif El-Nasr", "authors": "Magy Seif El-Nasr, Shree Durga, Mariya Shiyko, Carmen Sceppa", "title": "Unpacking Adherence and Engagement in Pervasive Health Games", "comments": null, "journal-ref": "Foundations of Digital Games, 2015", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pervasive health games have a potential to impact health-related behaviors.\nAnd, similar to other types of interventions, engagement and adherence in\nhealth games is the keystone for examining their short- and long-term effects.\nMany health-based applications have turned to gamification principles\nspecifically to. enhance their engagement. However, according to many reports,\nonly 41% of participants are retained in single player games and 29% in social\ngames after 90 days. These statistics raise multiple questions about factors\ninfluencing adherence and engagement. This paper presents an in-depth\nmixed-methods investigation of game design factors affecting engagement with\nand adherence to a pervasive commercial health game, called SpaPlay. We\nanalyzed interview and game behavior log data using theoretical constructs of\nsustained engagement to identify design elements affecting engagement and\nadherence. Our findings indicate that design elements associated with autonomy.\nand relatedness from the Self-Determination Theory and integrability, a measure\nof how well activities align with a person's life style, are important factors\naffecting engagement and adherence.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:35:08 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["El-Nasr", "Magy Seif", ""], ["Durga", "Shree", ""], ["Shiyko", "Mariya", ""], ["Sceppa", "Carmen", ""]]}, {"id": "2106.13753", "submitter": "Magy Seif El-Nasr", "authors": "S. Durga, S. Hallinan, M. Seif El-Nasr, M. Shiyko, C. Sceppa", "title": "Investigating behavior change indicators and cognitive measures in\n  persuasive health games", "comments": null, "journal-ref": "Foundations of Digital Games, 2015", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Outcome-driven studies designed to evaluate potential effects of games and\napps designed to promote healthy eating and exercising remain limited either\ntargeting design or usability factors while omitting out health-based outcomes\naltogether, or tend to be too narrowly focuses on behavioral outcomes within a\nshort periods of time thereby less likely to influence longitudinal factors\nthat can help sustain healthy habits. In this paper we argue for a unified\napproach to tackle behavioral change through focusing on both health outcomes\nand cognitive precursors, such as players' attitudes and behaviors around\nhealthy eating and exercising, motivation stage and knowledge and awareness\nabout nutrition or physical activity. Key findings from a 3-month long game\nplay study, with 47 female participants indicate that there are clear shifts in\nplayers' perceptions about health and knowledge about eating. This paper\nextends our current understandings about approaches for evaluating health games\nand presents a unified approach to assess effectiveness of game-based health\ninterventions through combining health-based outcomes and shifts in players'\ncognitive precursors.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:43:01 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Durga", "S.", ""], ["Hallinan", "S.", ""], ["El-Nasr", "M. Seif", ""], ["Shiyko", "M.", ""], ["Sceppa", "C.", ""]]}, {"id": "2106.13777", "submitter": "Gabriel Appleby", "authors": "Gabriel Appleby, Mateus Espadoto, Rui Chen, Samuel Goree, Alexandru\n  Telea, Erik W Anderson, Remco Chang", "title": "HyperNP: Interactive Visual Exploration of Multidimensional Projection\n  Hyperparameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection algorithms such as t-SNE or UMAP are useful for the visualization\nof high dimensional data, but depend on hyperparameters which must be tuned\ncarefully. Unfortunately, iteratively recomputing projections to find the\noptimal hyperparameter value is computationally intensive and unintuitive due\nto the stochastic nature of these methods. In this paper we propose HyperNP, a\nscalable method that allows for real-time interactive hyperparameter\nexploration of projection methods by training neural network approximations.\nHyperNP can be trained on a fraction of the total data instances and\nhyperparameter configurations and can compute projections for new data and\nhyperparameters at interactive speeds. HyperNP is compact in size and fast to\ncompute, thus allowing it to be embedded in lightweight visualization systems\nsuch as web browsers. We evaluate the performance of the HyperNP across three\ndatasets in terms of performance and speed. The results suggest that HyperNP is\naccurate, scalable, interactive, and appropriate for use in real-world\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 17:28:14 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Appleby", "Gabriel", ""], ["Espadoto", "Mateus", ""], ["Chen", "Rui", ""], ["Goree", "Samuel", ""], ["Telea", "Alexandru", ""], ["Anderson", "Erik W", ""], ["Chang", "Remco", ""]]}, {"id": "2106.13921", "submitter": "Douglas Zytko", "authors": "Douglas Zytko, Zexin Ma, Jacob Gleason, Nathaniel Lundquist, Medina\n  Taylor", "title": "Immersive Stories for Health Information: Design Considerations from\n  Binge Drinking in VR", "comments": null, "journal-ref": "In: Toeppe K., Yan H., Chu S.K.W. (eds) Diversity, Divergence,\n  Dialogue. iConference 2021. Lecture Notes in Computer Science, vol 12645.\n  Springer, Cham", "doi": "10.1007/978-3-030-71292-1_25", "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive stories for health are 360-degree videos that intend to alter\nviewer perceptions about behaviors detrimental to health. They have potential\nto inform public health at scale, however, immersive story design is still in\nearly stages and largely devoid of best practices. This paper presents a focus\ngroup study with 147 viewers of an immersive story about binge drinking\nexperienced through VR headsets and mobile phones. The objective of the study\nis to identify aspects of immersive story design that influence attitudes\ntowards the health issue exhibited, and to understand how health information is\nconsumed in immersive stories. Findings emphasize the need for an immersive\nstory to provide reasoning behind character engagement in the focal health\nbehavior, to show the main character clearly engaging in the behavior, and to\nenable viewers to experience escalating symptoms of the behavior before the\npenultimate health consequence. Findings also show how the design of supporting\ncharacters can inadvertently distract viewers and lead them to justify the\ndetrimental behavior being exhibited. The paper concludes with design\nconsiderations for enabling immersive stories to better inform public\nperception of health issues.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 01:33:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zytko", "Douglas", ""], ["Ma", "Zexin", ""], ["Gleason", "Jacob", ""], ["Lundquist", "Nathaniel", ""], ["Taylor", "Medina", ""]]}, {"id": "2106.13967", "submitter": "Vasiliki Vasileiou", "authors": "Vasiliki I. Vasileiou, Nikolaos Kardaris, Petros Maragos", "title": "Exploring Temporal Context and Human Movement Dynamics for Online Action\n  Detection in Videos", "comments": "EUSIPCO-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, the interaction between humans and robots is constantly expanding,\nrequiring more and more human motion recognition applications to operate in\nreal time. However, most works on temporal action detection and recognition\nperform these tasks in offline manner, i.e. temporally segmented videos are\nclassified as a whole. In this paper, based on the recently proposed framework\nof Temporal Recurrent Networks, we explore how temporal context and human\nmovement dynamics can be effectively employed for online action detection. Our\napproach uses various state-of-the-art architectures and appropriately combines\nthe extracted features in order to improve action detection. We evaluate our\nmethod on a challenging but widely used dataset for temporal action\nlocalization, THUMOS'14. Our experiments show significant improvement over the\nbaseline method, achieving state-of-the art results on THUMOS'14.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 08:34:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Vasileiou", "Vasiliki I.", ""], ["Kardaris", "Nikolaos", ""], ["Maragos", "Petros", ""]]}, {"id": "2106.14068", "submitter": "Yalda Ghasemi", "authors": "Yalda Ghasemi, Ankit Singh, Myunghee Kim, Andrew Johnson, and Heejin\n  Jeong", "title": "Effects of Head-locked Augmented Reality on User's performance and\n  perceived workload", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An augmented reality (AR) environment includes a set of digital elements with\nwhich the users interact while performing certain tasks. Recent AR head-mounted\ndisplays allow users to select how these elements are presented. However, few\nstudies have been conducted to examine the effect of the way of presenting\naugmented content on user performance and workload. This study aims to evaluate\ntwo methods of presenting augmented content - world-locked and head-locked\nmodes in a data entry task. A total of eighteen participants performed the data\nentry task in this study. The effectiveness of each mode is evaluated in terms\nof task performance, muscle activity, perceived workload, and usability. The\nresults show that the task completion time is shorter and the typing speed is\nsignificantly faster in the head-locked mode while the world-locked mode\nachieved higher scores in terms of preference. The findings of this study can\nbe applied to AR user interfaces to improve content presentation and enhance\nthe user experience.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 17:35:39 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ghasemi", "Yalda", ""], ["Singh", "Ankit", ""], ["Kim", "Myunghee", ""], ["Johnson", "Andrew", ""], ["Jeong", "Heejin", ""]]}, {"id": "2106.14313", "submitter": "Wenchao Li", "authors": "Wenchao Li, Yun Wang, He Huang, Weiwei Cui, Haidong Zhang, Huamin Qu,\n  Dongmei Zhang", "title": "AniVis: Generating Animated Transitions Between Statistical Charts with\n  a Tree Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animated transitions help viewers understand changes between related\nvisualizations. To clearly present the underlying relations between statistical\ncharts, animation authors need to have a high level of expertise and a\nconsiderable amount of time to describe the relations with reasonable animation\nstages. We present AniVis, an automated approach for generating animated\ntransitions to demonstrate the changes between two statistical charts. AniVis\nmodels each statistical chart into a tree-based structure. Given an input chart\npair, the differences of data and visual properties of the chart pair are\nformalized as tree edit operations. The edit operations can be mapped to atomic\ntransition units. Through this approach, the animated transition between two\ncharts can be expressed as a set of transition units. Then, we conduct a\nformative study to understand people's preferences for animation sequences.\nBased on the study, we propose a set of principles and a sequence composition\nalgorithm to compose the transition units into a meaningful animation sequence.\nFinally, we synthesize these units together to deliver a smooth and intuitive\nanimated transition between charts. To test our approach, we present a\nprototype system and its generated results to illustrate the usage of our\nframework. We perform a comparative study to assess the transition sequence\nderived from the tree model. We further collect qualitative feedback to\nevaluate the effectiveness and usefulness of our method.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 19:17:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Li", "Wenchao", ""], ["Wang", "Yun", ""], ["Huang", "He", ""], ["Cui", "Weiwei", ""], ["Zhang", "Haidong", ""], ["Qu", "Huamin", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2106.14483", "submitter": "Azmi Can \\\"Ozgen", "authors": "Azmi Can \\\"Ozgen, Mahiye Uluya\\u{g}mur \\\"Ozt\\\"urk, Umut Bayraktar", "title": "Cheating Detection Pipeline for Online Interviews and Exams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote examination and job interviews have gained popularity and become\nindispensable because of both pandemics and the advantage of remote working\ncircumstances. Most companies and academic institutions utilize these systems\nfor their recruitment processes and also for online exams. However, one of the\ncritical problems of the remote examination systems is conducting the exams in\na reliable environment. In this work, we present a cheating analysis pipeline\nfor online interviews and exams. The system only requires a video of the\ncandidate, which is recorded during the exam. Then cheating detection pipeline\nis employed to detect another person, electronic device usage, and candidate\nabsence status. The pipeline consists of face detection, face recognition,\nobject detection, and face tracking algorithms. To evaluate the performance of\nthe pipeline we collected a private video dataset. The video dataset includes\nboth cheating activities and clean videos. Ultimately, our pipeline presents an\nefficient and fast guideline to detect and analyze cheating activities in an\nonline interview and exam video.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 08:52:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["\u00d6zgen", "Azmi Can", ""], ["\u00d6zt\u00fcrk", "Mahiye Uluya\u011fmur", ""], ["Bayraktar", "Umut", ""]]}, {"id": "2106.14505", "submitter": "Misbahu Sharfuddeen Zubair", "authors": "Misbahu S. Zubair and Salim Muhammad", "title": "Interactions and Actions in One Touch Gesture Mobile Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A player plays a game by sending messages into the game world using an\ninteraction technique. These messages are then translated into actions\nperformed on or by game objects towards achieving the game's objectives. A\ngame's interaction model is the bridge between the player's interaction and its\nin-game actions by defining what the player may and may not act upon at any\ngiven moment. This makes the choice of interaction technique, its associated\nactions, and interaction model critical for designing games that are engaging,\nimmersive, and intuitive to play. This paper presents a study focused on\nOne-Touch-Gesture mobile games, with the aim of identifying the touch gestures\nused in popular games of this type, the types of in-game actions associated\nwith these gestures, and the interaction models used by these games. The study\nwas conducted by reviewing 77 of the most popular games in the last two years\nthrough playtesting by two researchers. The results of the study contribute to\nexisting knowledge by providing an insight into the interactions and actions of\npopular 1TG games and providing a guide to aid in developing games of the type.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 09:38:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zubair", "Misbahu S.", ""], ["Muhammad", "Salim", ""]]}, {"id": "2106.14701", "submitter": "Jason R.C. Nurse Dr", "authors": "Betsy Uchendu and Jason R. C. Nurse and Maria Bada and Steven Furnell", "title": "Developing a cyber security culture: Current practices and future needs", "comments": null, "journal-ref": "Computers & Security, 2021", "doi": "10.1016/j.cose.2021.102387", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the creation of a strong security culture has been researched and\ndiscussed for decades, it continues to elude many businesses. Part of the\nchallenge faced is distilling pertinent, recent academic findings and research\ninto useful guidance. In this article, we aim to tackle this issue by\nconducting a state-of-the-art study into organisational cyber security culture\nresearch. This work investigates four questions, including how cyber security\nculture is defined, what factors are essential to building and maintaining such\na culture, the frameworks proposed to cultivate a security culture and the\nmetrics suggested to assess it. Through the application of the PRISMA\nsystematic literature review technique, we identify and analyse 58 research\narticles from the last 10 years (2010-2020). Our findings demonstrate that\nwhile there have been notable changes in the use of terms (e.g., information\nsecurity culture and cyber security culture), many of the most influential\nfactors across papers are similar. Top management support, policy and\nprocedures, and awareness for instance, are critical in engendering cyber\nsecurity culture. Many of the frameworks reviewed revealed common foundations,\nwith organisational culture playing a substantial role in crafting appropriate\ncyber security culture models. Questionnaires and surveys are the most used\ntool to measure cyber security culture, but there are also concerns as to\nwhether more dynamic measures are needed. For practitioners, this article\nhighlights factors and models essential to the creation and management of a\nrobust security culture. For research, we produce an up-to-date\ncharacterisation of the field and also define open issues deserving of further\nattention such as the role of change management processes and national culture\nin an enterprise's cyber security culture.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 13:31:33 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Uchendu", "Betsy", ""], ["Nurse", "Jason R. C.", ""], ["Bada", "Maria", ""], ["Furnell", "Steven", ""]]}, {"id": "2106.14736", "submitter": "Taras Kucherenko", "authors": "Taras Kucherenko, Rajmund Nagy, Patrik Jonell, Michael Neff, Hedvig\n  Kjellstr\\\"om, Gustav Eje Henter", "title": "Speech2Properties2Gestures: Gesture-Property Prediction as a Tool for\n  Generating Representational Gestures from Speech", "comments": "Accepted for publication at the ACM International Conference on\n  Intelligent Virtual Agents (IVA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for gesture generation, aiming to allow\ndata-driven approaches to produce more semantically rich gestures. Our approach\nfirst predicts whether to gesture, followed by a prediction of the gesture\nproperties. Those properties are then used as conditioning for a modern\nprobabilistic gesture-generation model capable of high-quality output. This\nempowers the approach to generate gestures that are both diverse and\nrepresentational.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:07:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kucherenko", "Taras", ""], ["Nagy", "Rajmund", ""], ["Jonell", "Patrik", ""], ["Neff", "Michael", ""], ["Kjellstr\u00f6m", "Hedvig", ""], ["Henter", "Gustav Eje", ""]]}, {"id": "2106.14739", "submitter": "Manuel Palermo", "authors": "Manuel Palermo, Sara Moccia, Lucia Migliorelli, Emanuele Frontoni,\n  Cristina P. Santos", "title": "Real-Time Human Pose Estimation on a Smart Walker using Convolutional\n  Neural Networks", "comments": "Accepted for publication in Expert Systems with Applications", "journal-ref": null, "doi": "10.1016/j.eswa.2021.115498", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Rehabilitation is important to improve quality of life for mobility-impaired\npatients. Smart walkers are a commonly used solution that should embed\nautomatic and objective tools for data-driven human-in-the-loop control and\nmonitoring. However, present solutions focus on extracting few specific metrics\nfrom dedicated sensors with no unified full-body approach. We investigate a\ngeneral, real-time, full-body pose estimation framework based on two RGB+D\ncamera streams with non-overlapping views mounted on a smart walker equipment\nused in rehabilitation. Human keypoint estimation is performed using a\ntwo-stage neural network framework. The 2D-Stage implements a detection module\nthat locates body keypoints in the 2D image frames. The 3D-Stage implements a\nregression module that lifts and relates the detected keypoints in both cameras\nto the 3D space relative to the walker. Model predictions are low-pass filtered\nto improve temporal consistency. A custom acquisition method was used to obtain\na dataset, with 14 healthy subjects, used for training and evaluating the\nproposed framework offline, which was then deployed on the real walker\nequipment. An overall keypoint detection error of 3.73 pixels for the 2D-Stage\nand 44.05mm for the 3D-Stage were reported, with an inference time of 26.6ms\nwhen deployed on the constrained hardware of the walker. We present a novel\napproach to patient monitoring and data-driven human-in-the-loop control in the\ncontext of smart walkers. It is able to extract a complete and compact body\nrepresentation in real-time and from inexpensive sensors, serving as a common\nbase for downstream metrics extraction solutions, and Human-Robot interaction\napplications. Despite promising results, more data should be collected on users\nwith impairments, to assess its performance as a rehabilitation tool in\nreal-world scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 14:11:48 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Palermo", "Manuel", ""], ["Moccia", "Sara", ""], ["Migliorelli", "Lucia", ""], ["Frontoni", "Emanuele", ""], ["Santos", "Cristina P.", ""]]}, {"id": "2106.14802", "submitter": "Maximilian T. Fischer", "authors": "Maximilian T. Fischer, Frederik L. Dennig, Daniel Seebacher, Daniel A.\n  Keim, Mennatallah El-Assady", "title": "Communication Analysis through Visual Analytics: Current Practices,\n  Challenges, and New Frontiers", "comments": "11 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automated analysis of digital human communication data often focuses on\nspecific aspects like content or network structure in isolation, while\nclassical communication research stresses the importance of a holistic analysis\napproach. This work aims to formalize digital communication analysis and\ninvestigate how classical results can be leveraged as part of visually\ninteractive systems, which offers new analysis opportunities to allow for less\nbiased, skewed, or incomplete results. For this, we construct a conceptual\nframework and design space based on the existing research landscape, technical\nconsiderations, and communication research that describes the properties,\ncapabilities, and composition of such systems through 30 criteria in four\nanalysis dimensions. We make the case how visual analytics principles are\nuniquely suited for a more holistic approach by tackling the automation\ncomplexity and leverage domain knowledge, paving the way to generate design\nguidelines for building such approaches. Our framework provides a common\nlanguage and description of communication analysis systems to support existing\nresearch, highlights relevant design areas while promoting and supporting the\nmutual exchange between researchers. Additionally, our framework identifies\nexisting gaps and highlights opportunities in research areas that are worth\ninvestigating further. With this contribution, we pave the path for the\nformalization of digital communication analysis through visual analytics.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 15:23:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fischer", "Maximilian T.", ""], ["Dennig", "Frederik L.", ""], ["Seebacher", "Daniel", ""], ["Keim", "Daniel A.", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "2106.14835", "submitter": "Anna Xambo", "authors": "Anna Xamb\\'o", "title": "Virtual Agents in Live Coding: A Short Review", "comments": "Preprint version submitted to eContact! (https://econtact.ca) for the\n  special issue 21.1 - Take Back the Stage: Live coding, live audiovisual,\n  laptop orchestra", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AI and live coding has been little explored. This article contributes with a\nshort review of different perspectives of using virtual agents in the practice\nof live coding looking at past and present as well as pointing to future\ndirections.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 16:23:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Xamb\u00f3", "Anna", ""]]}, {"id": "2106.14975", "submitter": "Wiebke Toussaint", "authors": "Wiebke Toussaint, Alejandra Gomez Ortega, Jered Vroon, Julian Harty,\n  G\\\"urkan Solmaz, Olya Kudina, Ella Peltonen, Jacky Bourgeois, Aaron Yi Ding", "title": "Design Considerations for Data Daemons: Co-creating Design Futures to\n  Explore Ethical Personal Data Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mobile applications and online service providers track our virtual and\nphysical behaviour more actively and with a broader scope than ever before.\nThis has given rise to growing concerns about ethical personal data management.\nEven though regulation and awareness around data ethics are increasing,\nend-users are seldom engaged when defining and designing what a future with\nethical personal data management should look like. We explore a participatory\nprocess that uses design futures, the Future workshop method and design\nfictions to envision ethical personal data management with end-users and\ndesigners. To engage participants effectively, we needed to bridge their\ndifferential expertise and make the abstract concepts of data and ethics\ntangible. By concretely presenting personal data management and control as\nfictitious entities called Data Daemons, we created a shared understanding of\nthese abstract concepts, and empowered non-expert end-users and designers to\nbecome actively engaged in the design process.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 20:48:09 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 08:49:29 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Toussaint", "Wiebke", ""], ["Ortega", "Alejandra Gomez", ""], ["Vroon", "Jered", ""], ["Harty", "Julian", ""], ["Solmaz", "G\u00fcrkan", ""], ["Kudina", "Olya", ""], ["Peltonen", "Ella", ""], ["Bourgeois", "Jacky", ""], ["Ding", "Aaron Yi", ""]]}, {"id": "2106.15005", "submitter": "Michael Correll", "authors": "Lyn Bartram, Michael Correll, Melanie Tory", "title": "Untidy Data: The Unreasonable Effectiveness of Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Working with data in table form is usually considered a preparatory and\ntedious step in the sensemaking pipeline; a way of getting the data ready for\nmore sophisticated visualization and analytical tools. But for many people,\nspreadsheets -- the quintessential table tool -- remain a critical part of\ntheir information ecosystem, allowing them to interact with their data in ways\nthat are hidden or abstracted in more complex tools. This is particularly true\nfor data workers: people who work with data as part of their job but do not\nidentify as professional analysts or data scientists. We report on a\nqualitative study of how these workers interact with and reason about their\ndata. Our findings show that data tables serve a broader purpose beyond data\ncleanup at the initial stage of a linear analytic flow: users want to see and\n\"get their hands on\" the underlying data throughout the analytics process,\nreshaping and augmenting it to support sensemaking. They reorganize, mark up,\nlayer on levels of detail, and spawn alternatives within the context of the\nbase data. These direct interactions and human-readable table representations\nform a rich and cognitively important part of building understanding of what\nthe data mean and what they can do with it. We argue that interactive tables\nare an important visualization idiom in their own right; that the direct data\ninteraction they afford offers a fertile design space for visual analytics; and\nthat sense making can be enriched by more flexible human-data interaction than\nis currently supported in visual analytics tools.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 22:27:12 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Bartram", "Lyn", ""], ["Correll", "Michael", ""], ["Tory", "Melanie", ""]]}, {"id": "2106.15049", "submitter": "Mabrook Al-Rakhami Mr.", "authors": "Mabrook S. Al-Rakhami, Abdu Gumaei1, Meteb Altaf, Mohammad Mehedi\n  Hassan, Bader Fahad Alkhamees, Khan Muhammad and Giancarlo Fortino", "title": "FallDeF5: A Fall Detection Framework Using 5G-based Deep Gated Recurrent\n  Unit Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fall prevalence is high among elderly people, which is challenging due to the\nsevere consequences of falling. This is why rapid assistance is a critical\ntask. Ambient assisted living (AAL) uses recent technologies such as 5G\nnetworks and the internet of medical things (IoMT) to address this research\narea. Edge computing can reduce the cost of cloud communication, including high\nlatency and bandwidth use, by moving conventional healthcare services and\napplications closer to end-users. Artificial intelligence (AI) techniques such\nas deep learning (DL) have been used recently for automatic fall detection, as\nwell as supporting healthcare services. However, DL requires a vast amount of\ndata and substantial processing power to improve its performance for the IoMT\nlinked to the traditional edge computing environment. This research proposes an\neffective fall detection framework based on DL algorithms and mobile edge\ncomputing (MEC) within 5G wireless networks, the aim being to empower\nIoMT-based healthcare applications. We also propose the use of a deep gated\nrecurrent unit (DGRU) neural network to improve the accuracy of existing\nDL-based fall detection methods. DGRU has the advantage of dealing with\ntime-series IoMT data, and it can reduce the number of parameters and avoid the\nvanishing gradient problem. The experimental results on two public datasets\nshow that the DGRU model of the proposed framework achieves higher accuracy\nrates compared to the current related works on the same datasets.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 01:31:35 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Al-Rakhami", "Mabrook S.", ""], ["Gumaei1", "Abdu", ""], ["Altaf", "Meteb", ""], ["Hassan", "Mohammad Mehedi", ""], ["Alkhamees", "Bader Fahad", ""], ["Muhammad", "Khan", ""], ["Fortino", "Giancarlo", ""]]}, {"id": "2106.15101", "submitter": "Harsh Sharma", "authors": "Stuti Sehgal (1), Harsh Sharma (1), Akshat Anand (1) ((1) Department\n  of Computer Science and Engineering, SRM Institute of Science and Technology,\n  Kattankulathur, Tamil Nadu, India)", "title": "Smart and Context-Aware System employing Emotions Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People have the ability to make sensible assumptions about other people's\nemotional states by being sympathetic, and because of our common sense of\nknowledge and the ability to think visually. Over the years, much research has\nbeen done on providing machines with the ability to detect human emotions and\nto develop automated emotional intelligence systems. The computer's ability to\ndetect human emotions is gaining popularity in creating sensitive systems such\nas learning environments, health care systems and real-world. Improving\npeople's health has been the subject of much research. This paper describes the\nformation as conceptual evidence of emotional acquisition and control in\nintelligent health settings. The authors of this paper aim for an\nunconventional approach with a friendly look to get emotional scenarios from\nthe system to establish a functional, non-intrusive and emotionally-sensitive\nenvironment where users can do their normal activities naturally and see the\nprogram only when pleasant mood activating services are received. The\ncontext-sensitive system interacts with users to detect and differentiate\nemotions through facial expressions or speech recognition, to make music\nrecommendations and mood color treatments with the services installed on their\nIoT devices.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 05:36:19 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Sehgal", "Stuti", ""], ["Sharma", "Harsh", ""], ["Anand", "Akshat", ""]]}, {"id": "2106.15297", "submitter": "Jussi Karlgren", "authors": "Jussi Karlgren, Lennart E. Fahl\\'en, Anders Wallberg, P\\\"ar Hansson,\n  Olov St{\\aa}hl, Jonas S\\\"oderberg, Karl-Petter {\\AA}kesson", "title": "Socially Intelligent Interfaces for Increased Energy Awareness in the\n  Home", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-540-78731-0_17", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes how home appliances might be enhanced to improve user\nawareness of energy usage. Households wish to lead comfortable and manageable\nlives. Balancing this reasonable desire with the environmental and political\ngoal of reducing electricity usage is a challenge that we claim is best met\nthrough the design of interfaces that allows users better control of their\nusage and unobtrusively informs them of the actions of their peers. A set of\ndesign principles along these lines is formulated in this paper. We have built\na fully functional prototype home appliance with a socially aware interface to\nsignal the aggregate usage of the users peer group according to these\nprinciples, and present the prototype in the paper.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:19:21 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Karlgren", "Jussi", ""], ["Fahl\u00e9n", "Lennart E.", ""], ["Wallberg", "Anders", ""], ["Hansson", "P\u00e4r", ""], ["St\u00e5hl", "Olov", ""], ["S\u00f6derberg", "Jonas", ""], ["\u00c5kesson", "Karl-Petter", ""]]}, {"id": "2106.15355", "submitter": "Ana Valeria Gonzalez", "authors": "Ana Valeria Gonzalez, Anna Rogers, Anders S{\\o}gaard", "title": "On the Interaction of Belief Bias and Explanations", "comments": "accepted at findings of ACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A myriad of explainability methods have been proposed in recent years, but\nthere is little consensus on how to evaluate them. While automatic metrics\nallow for quick benchmarking, it isn't clear how such metrics reflect human\ninteraction with explanations. Human evaluation is of paramount importance, but\nprevious protocols fail to account for belief biases affecting human\nperformance, which may lead to misleading conclusions. We provide an overview\nof belief bias, its role in human evaluation, and ideas for NLP practitioners\non how to account for it. For two experimental paradigms, we present a case\nstudy of gradient-based explainability introducing simple ways to account for\nhumans' prior beliefs: models of varying quality and adversarial examples. We\nshow that conclusions about the highest performing methods change when\nintroducing such controls, pointing to the importance of accounting for belief\nbias in evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 12:49:42 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gonzalez", "Ana Valeria", ""], ["Rogers", "Anna", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "2106.15481", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Xinhai Wei, Jian Zhao, Kwan-Liu Ma", "title": "Interactive Dimensionality Reduction for Comparative Analysis", "comments": "This manuscript is currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the similarities and differences between two or more groups of\ndatasets is a fundamental analysis task. For high-dimensional data,\ndimensionality reduction (DR) methods are often used to find the\ncharacteristics of each group. However, existing DR methods provide limited\ncapability and flexibility for such comparative analysis as each method is\ndesigned only for a narrow analysis target, such as identifying factors that\nmost differentiate groups. In this work, we introduce an interactive DR\nframework where we integrate our new DR method, called ULCA (unified linear\ncomparative analysis), with an interactive visual interface. ULCA unifies two\nDR schemes, discriminant analysis and contrastive learning, to support various\ncomparative analysis tasks. To provide flexibility for comparative analysis, we\ndevelop an optimization algorithm that enables analysts to interactively refine\nULCA results. Additionally, we provide an interactive visualization interface\nto examine ULCA results with a rich set of analysis libraries. We evaluate ULCA\nand the optimization algorithm to show their efficiency as well as present\nmultiple case studies using real-world datasets to demonstrate the usefulness\nof our framework.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:05:36 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Wei", "Xinhai", ""], ["Zhao", "Jian", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2106.15553", "submitter": "Anett Hoppe", "authors": "Anett Hoppe and David Morris and Ralph Ewerth", "title": "Evaluation of Automated Image Descriptions for Visually Impaired\n  Students", "comments": "6 pages, 12 references. Accepted for publication at the 22nd\n  International Conference on Artificial Intelligence in Education (AIED 2021),\n  June 14-16 2021, Utrecht, The Netherlands", "journal-ref": "Hoppe A., Morris D., Ewerth R. (2021) Evaluation of Automated\n  Image Descriptions for Visually Impaired Students. In: Roll I., McNamara D.,\n  Sosnovsky S., Luckin R., Dimitrova V. (eds) AIED 2021. LNCS vol 12749.\n  Springer, Cham", "doi": "10.1007/978-3-030-78270-2_35", "report-no": null, "categories": "cs.HC cs.CV cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illustrations are widely used in education, and sometimes, alternatives are\nnot available for visually impaired students. Therefore, those students would\nbenefit greatly from an automatic illustration description system, but only if\nthose descriptions were complete, correct, and easily understandable using a\nscreenreader. In this paper, we report on a study for the assessment of\nautomated image descriptions. We interviewed experts to establish evaluation\ncriteria, which we then used to create an evaluation questionnaire for sighted\nnon-expert raters, and description templates. We used this questionnaire to\nevaluate the quality of descriptions which could be generated with a\ntemplate-based automatic image describer. We present evidence that these\ntemplates have the potential to generate useful descriptions, and that the\nquestionnaire identifies problems with description templates.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:40:04 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Hoppe", "Anett", ""], ["Morris", "David", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2106.15599", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "Framework for an Intelligent Affect Aware Smart Home Environment for\n  Elderly People", "comments": null, "journal-ref": "International Journal of Recent Trends in Human Computer\n  Interaction (IJHCI), Volume - 9, Issue 1, 2019, pp. 23-43", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.ET cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The population of elderly people has been increasing at a rapid rate over the\nlast few decades and their population is expected to further increase in the\nupcoming future. Their increasing population is associated with their\nincreasing needs due to problems like physical disabilities, cognitive issues,\nweakened memory and disorganized behavior, that elderly people face with\nincreasing age. To reduce their financial burden on the world economy and to\nenhance their quality of life, it is essential to develop technology-based\nsolutions that are adaptive, assistive and intelligent in nature. Intelligent\nAffect Aware Systems that can not only analyze but also predict the behavior of\nelderly people in the context of their day to day interactions with technology\nin an IoT-based environment, holds immense potential for serving as a long-term\nsolution for improving the user experience of elderly in smart homes. This work\ntherefore proposes the framework for an Intelligent Affect Aware environment\nfor elderly people that can not only analyze the affective components of their\ninteractions but also predict their likely user experience even before they\nstart engaging in any activity in the given smart home environment. This\nforecasting of user experience would provide scope for enhancing the same,\nthereby increasing the assistive and adaptive nature of such intelligent\nsystems. To uphold the efficacy of this proposed framework for improving the\nquality of life of elderly people in smart homes, it has been tested on three\ndatasets and the results are presented and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:34:16 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2106.15606", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "Multimodal Approaches for Indoor Localization for Ambient Assisted\n  Living in Smart Homes", "comments": null, "journal-ref": "Journal of Information, Volume 12, Issue 3, 2021, Article 114", "doi": "10.3390/info12030114", "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work makes multiple scientific contributions to the field of Indoor\nLocalization for Ambient Assisted Living in Smart Homes. First, it presents a\nBig-Data driven methodology that studies the multimodal components of user\ninteractions and analyzes the data from Bluetooth Low Energy (BLE) beacons and\nBLE scanners to detect a user's indoor location in a specific activity-based\nzone during Activities of Daily Living. Second, it introduces a context\nindependent approach that can interpret the accelerometer and gyroscope data\nfrom diverse behavioral patterns to detect the zone-based indoor location of a\nuser in any Internet of Things (IoT)-based environment. These two approaches\nachieved performance accuracies of 81.36% and 81.13%, respectively, when tested\non a dataset. Third, it presents a methodology to detect the spatial\ncoordinates of a user's indoor position that outperforms all similar works in\nthis field, as per the associated root mean squared error - one of the\nperformance evaluation metrics in ISO/IEC18305:2016- an international standard\nfor testing Localization and Tracking Systems. Finally, it presents a\ncomprehensive comparative study that includes Random Forest, Artificial Neural\nNetwork, Decision Tree, Support Vector Machine, k-NN, Gradient Boosted Trees,\nDeep Learning, and Linear Regression, to address the challenge of identifying\nthe optimal machine learning approach for Indoor Localization.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:46:21 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2106.15609", "submitter": "Nirmalya Thakur", "authors": "Nirmalya Thakur and Chia Y. Han", "title": "An Ambient Intelligence-Based Human Behavior Monitoring Framework for\n  Ubiquitous Environments", "comments": null, "journal-ref": "Journal of Information, Volume 12, Issue 2, 2021, Article 81", "doi": "10.3390/info12020081", "report-no": null, "categories": "cs.HC cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This framework for human behavior monitoring aims to take a holistic approach\nto study, track, monitor, and analyze human behavior during activities of daily\nliving (ADLs). The framework consists of two novel functionalities. First, it\ncan perform the semantic analysis of user interactions on the diverse\ncontextual parameters during ADLs to identify a list of distinct behavioral\npatterns associated with different complex activities. Second, it consists of\nan intelligent decision-making algorithm that can analyze these behavioral\npatterns and their relationships with the dynamic contextual and spatial\nfeatures of the environment to detect any anomalies in user behavior that could\nconstitute an emergency. These functionalities of this interdisciplinary\nframework were developed by integrating the latest advancements and\ntechnologies in human-computer interaction, machine learning, Internet of\nThings, pattern recognition, and ubiquitous computing. The framework was\nevaluated on a dataset of ADLs, and the performance accuracies of these two\nfunctionalities were found to be 76.71% and 83.87%, respectively. The presented\nand discussed results uphold the relevance and immense potential of this\nframework to contribute towards improving the quality of life and assisted\nliving of the aging population in the future of Internet of Things (IoT)-based\nubiquitous living environments, e.g., smart homes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 17:50:54 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Thakur", "Nirmalya", ""], ["Han", "Chia Y.", ""]]}, {"id": "2106.15983", "submitter": "Tifanie Bouchara", "authors": "Valentin Bauer and Tifanie Bouchara and Patrick Bourdot", "title": "eXtended Reality for Autism Interventions: The importance of Mediation\n  and Sensory-Based Approaches", "comments": "submitted to Journal of Autism and Developmental Disorder", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  eXtended Reality (XR) autism research, ranging from Augmented Reality to\nVirtual Reality, focuses on socio-emotional abilities and high-functioning\nautism. However common autism interventions address the entire spectrum over\nsocial, sensory and mediation issues. To bridge the gap between autism research\nand real interventions, we compared existing literature on XR and autism with\nstakeholders' needs obtained by interviewing 34 skateholders, mainly\npractitioners. It allow us first to suggest XR use cases that could better\nsupport practitioners' interventions, and second to derive design guidelines\naccordingly. Findings demonstrate that collaborative XR sensory-based and\nmediation approaches would benefit the entire spectrum, and encourage to\nconsider the overall intervention context when designing XR protocols.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:11:23 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bauer", "Valentin", ""], ["Bouchara", "Tifanie", ""], ["Bourdot", "Patrick", ""]]}, {"id": "2106.16106", "submitter": "Yuri Klebanov", "authors": "Yuri Klebanov, Romi Mikulinsky, Tom Reznikov, Miles Pennington,\n  Yoshihiro Suda, Toshihiro Hiraoka, Shoichi Kanzaki", "title": "How can design help enhance trust calibration in public autonomous\n  vehicles?", "comments": "4 pages, 5 figures, IV 2021 Nagoya, Trust Calibration Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trust is a multilayered concept with critical relevance when it comes to\nintroducing new technologies. Understanding how humans will interact with\ncomplex vehicle systems and preparing for the functional, societal and\npsychological aspects of autonomous vehicles' entry into our cities is a\npressing concern. Design tools can help calibrate the adequate and affordable\nlevel of trust needed for a safe and positive experience. This study focuses on\npassenger interactions capable of enhancing the system trustworthiness and data\naccuracy in future shared public transportation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 14:54:25 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Klebanov", "Yuri", ""], ["Mikulinsky", "Romi", ""], ["Reznikov", "Tom", ""], ["Pennington", "Miles", ""], ["Suda", "Yoshihiro", ""], ["Hiraoka", "Toshihiro", ""], ["Kanzaki", "Shoichi", ""]]}, {"id": "2106.16122", "submitter": "Sebastian Kr\\\"ugel", "authors": "Sebastian Kr\\\"ugel, Andreas Ostermaier, Matthias Uhl", "title": "Zombies in the Loop? People are Insensitive to the Transparency of\n  AI-Powered Moral Advisors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Departing from the assumption that AI needs to be transparent to be trusted,\nwe find that users trustfully take ethical advice from a transparent and an\nopaque AI-powered algorithm alike. Even when transparency reveals information\nthat warns against the algorithm, they continue to accept its advice. We\nconducted online experiments where the participants took the role of\ndecision-makers who received AI-powered advice on how to deal with an ethical\ndilemma. We manipulated information about the algorithm to study its influence.\nOur findings suggest that AI is overtrusted rather than distrusted, and that\nusers need digital literacy to benefit from transparency.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:19:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kr\u00fcgel", "Sebastian", ""], ["Ostermaier", "Andreas", ""], ["Uhl", "Matthias", ""]]}]