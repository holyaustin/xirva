[{"id": "2101.00098", "submitter": "Yuan Liang", "authors": "Yuan Liang and Liang Qiu and Tiancheng Lu and Zhujun Fang and Dezhan\n  Tu and Jiawei Yang and Tiandong Zhao and Yiting Shao and Kun Wang and Xiang\n  'Anthony' Chen and Lei He", "title": "OralViewer: 3D Demonstration of Dental Surgeries for Patient Education\n  with Oral Cavity Reconstruction from a 2D Panoramic X-ray", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patient's understanding on forthcoming dental surgeries is required by\npatient-centered care and helps reduce fear and anxiety. Due to the gap of\nexpertise between patients and dentists, conventional techniques of patient\neducation are usually not effective for explaining surgical steps. In this\npaper, we present \\textit{OralViewer} -- the first interactive application that\nenables dentist's demonstration of dental surgeries in 3D to promote patients'\nunderstanding. \\textit{OralViewer} takes a single 2D panoramic dental X-ray to\nreconstruct patient-specific 3D teeth structures, which are then assembled with\nregistered gum and jaw bone models for complete oral cavity modeling. During\nthe demonstration, \\textit{OralViewer} enables dentists to show surgery steps\nwith virtual dental instruments that can animate effects on a 3D model in\nreal-time. A technical evaluation shows our deep learning based model achieves\na mean Intersection over Union (IoU) of 0.771 for 3D teeth reconstruction. A\npatient study with 12 participants shows \\textit{OralViewer} can improve\npatients' understanding of surgeries. An expert study with 3 board-certified\ndentists further verifies the clinical validity of our system.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 22:30:48 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liang", "Yuan", ""], ["Qiu", "Liang", ""], ["Lu", "Tiancheng", ""], ["Fang", "Zhujun", ""], ["Tu", "Dezhan", ""], ["Yang", "Jiawei", ""], ["Zhao", "Tiandong", ""], ["Shao", "Yiting", ""], ["Wang", "Kun", ""], ["Chen", "Xiang 'Anthony'", ""], ["He", "Lei", ""]]}, {"id": "2101.00235", "submitter": "Yuting Zhan", "authors": "Yuting Zhan, Hamed Haddadi", "title": "MoSen: Activity Modelling in Multiple-Occupancy Smart Homes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart home solutions increasingly rely on a variety of sensors for behavioral\nanalytics and activity recognition to provide context-aware applications and\npersonalized care. Optimizing the sensor network is one of the most important\napproaches to ensure classification accuracy and the system's efficiency.\nHowever, the trade-off between the cost and performance is often a challenge in\nreal deployments, particularly for multiple-occupancy smart homes or care\nhomes.\n  In this paper, using real indoor activity and mobility traces, floor plans,\nand synthetic multi-occupancy behavior models, we evaluate several\nmulti-occupancy household scenarios with 2-5 residents. We explore and quantify\nthe trade-offs between the cost of sensor deployments and expected labeling\naccuracy in different scenarios. Our evaluation across different scenarios show\nthat the performance of the desired context-aware task is affected by different\nlocalization resolutions, the number of residents, the number of sensors, and\nvarying sensor deployments. To aid in accelerating the adoption of practical\nsensor-based activity recognition technology, we design MoSen, a framework to\nsimulate the interaction dynamics between sensor-based environments and\nmultiple residents. By evaluating the factors that affect the performance of\nthe desired sensor network, we provide a sensor selection strategy and design\nmetrics for sensor layout in real environments. Using our selection strategy in\na 5-person scenario case study, we demonstrate that MoSen can significantly\nimprove overall system performance without increasing the deployment costs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 13:53:36 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhan", "Yuting", ""], ["Haddadi", "Hamed", ""]]}, {"id": "2101.00254", "submitter": "Oded Nov", "authors": "Oded Nov", "title": "Interface Features and Users' Well-Being: Measuring the Sensitivity of\n  Users' Well-Being to Resource Constraints and Feature Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Users increasingly face multiple interface features on one hand, and\nconstraints on available resources (e.g., time, attention) on the other.\nUnderstanding the sensitivity of users' well-being to feature type and resource\nconstraints, is critical for informed design. Building on microeconomic theory,\nand focusing on social information features, users' interface choices were\nconceptualized as an exchange of resources (e.g., time), in return for access\nto goods (social information features). We studied how sensitive users'\nwell-being is to features' type, and to their cost level and type. We found\nthat (1) increased cost of feature use leads to decreased well-being, (2)\nusers' well-being is a function of features' cost type, and (3) users'\nwell-being is sensitive to differences in feature type. The approach used here\nto quantify user well-being derived from interface features offers a basis for\nasynchronous feature comparison.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 15:52:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Nov", "Oded", ""]]}, {"id": "2101.00260", "submitter": "Eleftherios Triantafyllidis Mr.", "authors": "Eleftherios Triantafyllidis and Zhibin Li", "title": "The Challenges in Modeling Human Performance in 3D Space with Fitts' Law", "comments": "Accepted at ACM CHI 2021 Conference on Human Factors in Computing\n  Systems (CHI '21 Extended Abstracts)", "journal-ref": null, "doi": "10.1145/3411763.3443442", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth in virtual reality technologies, object interaction is\nbecoming increasingly more immersive, elucidating human perception and leading\nto promising directions towards evaluating human performance under different\nsettings. This spike in technological growth exponentially increased the need\nfor a human performance metric in 3D space. Fitts' law is perhaps the most\nwidely used human prediction model in HCI history attempting to capture human\nmovement in lower dimensions. Despite the collective effort towards deriving an\nadvanced extension of a 3D human performance model based on Fitts' law, a\nstandardized metric is still missing. Moreover, most of the extensions to date\nassume or limit their findings to certain settings, effectively disregarding\nimportant variables that are fundamental to 3D object interaction. In this\nreview, we investigate and analyze the most prominent extensions of Fitts' law\nand compare their characteristics pinpointing to potentially important aspects\nfor deriving a higher-dimensional performance model. Lastly, we mention the\ncomplexities, frontiers as well as potential challenges that may lay ahead.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jan 2021 16:03:45 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Triantafyllidis", "Eleftherios", ""], ["Li", "Zhibin", ""]]}, {"id": "2101.00433", "submitter": "Michael Saxon", "authors": "Michael Saxon, Sharon Levy, Xinyi Wang, Alon Albalak, William Yang\n  Wang", "title": "Modeling Discolsive Transparency in NLP Application Descriptions", "comments": "14 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nPrevious work has suggested that a trade-off exists between greater disclosive\ntransparency and user confusion, where 'too much information' clouds a reader's\nunderstanding of what a system description means. We address both of these\nissues by connecting disclosive transparency to a \"replication room\" thought\nexperiment, where the person describing the system attempts to convey the\nrequisite information for a third party to reconstruct it. In this setting, the\ndegree to which the necessary information is conveyed represents the\ndescription's transparency, and the level of expertise needed by the third\nparty corresponds to potential user confusion. We introduce two neural language\nmodel-based probabilistic metrics to model these factors, and demonstrate that\nthey correlate with user and expert opinions of system transparency, making\nthem a valid objective proxy. Finally, we apply these metrics to study the\nrelationships between transparency, confusion, and user perceptions in a corpus\nof NLP demo abstracts.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 11:46:17 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:42:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Saxon", "Michael", ""], ["Levy", "Sharon", ""], ["Wang", "Xinyi", ""], ["Albalak", "Alon", ""], ["Wang", "William Yang", ""]]}, {"id": "2101.00443", "submitter": "Sourav Garg", "authors": "Sourav Garg, Niko S\\\"underhauf, Feras Dayoub, Douglas Morrison,\n  Akansel Cosgun, Gustavo Carneiro, Qi Wu, Tat-Jun Chin, Ian Reid, Stephen\n  Gould, Peter Corke, Michael Milford", "title": "Semantics for Robotic Mapping, Perception and Interaction: A Survey", "comments": "81 pages, 1 figure, published in Foundations and Trends in Robotics,\n  2020", "journal-ref": "Foundations and Trends in Robotics: Vol. 8: No. 1-2, pp 1-224\n  (2020)", "doi": "10.1561/2300000059", "report-no": null, "categories": "cs.RO cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots to navigate and interact more richly with the world around them,\nthey will likely require a deeper understanding of the world in which they\noperate. In robotics and related research fields, the study of understanding is\noften referred to as semantics, which dictates what does the world \"mean\" to a\nrobot, and is strongly tied to the question of how to represent that meaning.\nWith humans and robots increasingly operating in the same world, the prospects\nof human-robot interaction also bring semantics and ontology of natural\nlanguage into the picture. Driven by need, as well as by enablers like\nincreasing availability of training data and computational resources, semantics\nis a rapidly growing research area in robotics. The field has received\nsignificant attention in the research literature to date, but most reviews and\nsurveys have focused on particular aspects of the topic: the technical research\nissues regarding its use in specific robotic topics like mapping or\nsegmentation, or its relevance to one particular application domain like\nautonomous driving. A new treatment is therefore required, and is also timely\nbecause so much relevant research has occurred since many of the key surveys\nwere published. This survey therefore provides an overarching snapshot of where\nsemantics in robotics stands today. We establish a taxonomy for semantics\nresearch in or relevant to robotics, split into four broad categories of\nactivity, in which semantics are extracted, used, or both. Within these broad\ncategories we survey dozens of major topics including fundamentals from the\ncomputer vision field and key robotics research areas utilizing semantics,\nincluding mapping, navigation and interaction with the world. The survey also\ncovers key practical considerations, including enablers like increased data\navailability and improved computational hardware, and major application areas\nwhere...\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 12:34:39 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Garg", "Sourav", ""], ["S\u00fcnderhauf", "Niko", ""], ["Dayoub", "Feras", ""], ["Morrison", "Douglas", ""], ["Cosgun", "Akansel", ""], ["Carneiro", "Gustavo", ""], ["Wu", "Qi", ""], ["Chin", "Tat-Jun", ""], ["Reid", "Ian", ""], ["Gould", "Stephen", ""], ["Corke", "Peter", ""], ["Milford", "Michael", ""]]}, {"id": "2101.00472", "submitter": "Iuliana Marin", "authors": "Iuliana Marin", "title": "Study of mental health and learning engagement during COVID-19 pandemic\n  based on an electroencephalogram headset", "comments": null, "journal-ref": "Proceedings of ICERI2020 Conference", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID pandemic and the measures which were taken had effect over the\nmental health of persons. The current paper proposes a concept that supports\nthe performance of students by analyzing three ways of distance learning,\nnamely text, text and illustrations, including charts, video. An\nelectroencephalogram headset allows the detection of brainwaves and the\ndeveloped web application enhances the process of distance learning. The\nelectrodes of the headset are placed at contact with the user head and monitor\nthe activity of the left and right frontal regions, along with the temporal\nlobe. Mood, focus, stress, relaxation, engagement, excitement and interest are\ntriggered as numerical values by using the headset. The users provide\ninformation about their daily activities, including learning and evaluation\nprocesses. According to the study, users had the highest long term attention\nwhile using text and illustrations, followed by watching videos. This is caused\nby the fact that the text contained the code for the programs which were\npresented in the video. Also, the users feel comfortable while using the\napplication and they started to pay more attention to the connection between\nstress, health, education and well being. The results triggered by the headset\nhad higher values while students studied for the first time using videos. When\nthey wanted to remember the information, the text and illustrations way of\nlearning was the best option. Based on the study outcomes, the instructional\ndesign can be enhanced. Moreover, the results improved as the students became\nmore equilibrated and confident in themselves. Teachers, professors and parents\nare able to collaborate and enhance training. While studying online under\nlockdown, students have found the proposed solution to be good because their\ninner state influences their productivity while solving problems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 16:13:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Marin", "Iuliana", ""]]}, {"id": "2101.00479", "submitter": "Ramesha Karunasena", "authors": "Ramesha Karunasena, Piumi Sandarenu, Madushi Pinto, Achala Athukorala,\n  Ranga Rodrigo, Peshala Jayasekara", "title": "DEVI: Open-source Human-Robot Interface for Interactive Receptionist\n  Systems", "comments": "Published in: 2019 IEEE 4th International Conference on Advanced\n  Robotics and Mechatronics (ICARM)", "journal-ref": null, "doi": "10.1109/ICARM.2019.8834299", "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humanoid robots that act as human-robot interfaces equipped with social\nskills can assist people in many of their daily activities. Receptionist robots\nare one such application where social skills and appearance are of utmost\nimportance. Many existing robot receptionist systems suffer from high cost and\nthey do not disclose internal architectures for further development for robot\nresearchers. Moreover, there does not exist customizable open-source robot\nreceptionist frameworks to be deployed for any given application. In this paper\nwe present an open-source robot receptionist intelligence core -- \"DEVI\"(means\n'lady' in Sinhala), that provides researchers with ease of creating customized\nrobot receptionists according to the requirements (cost, external appearance,\nand required processing power). Moreover, this paper also presents details on a\nprototype implementation of a physical robot using the DEVI system. The robot\ncan give directional guidance with physical gestures, answer basic queries\nusing a speech recognition and synthesis system, recognize and greet known\npeople using face recognition and register new people in its database, using a\nself-learning neural network. Experiments conducted with DEVI show the\neffectiveness of the proposed system.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 17:08:20 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Karunasena", "Ramesha", ""], ["Sandarenu", "Piumi", ""], ["Pinto", "Madushi", ""], ["Athukorala", "Achala", ""], ["Rodrigo", "Ranga", ""], ["Jayasekara", "Peshala", ""]]}, {"id": "2101.00496", "submitter": "Abhishek Das", "authors": "Abhishek Das, Vivek Dhuri, Aditya Desai, Suyash Ail, Ameya Kadam", "title": "Smart Car Features using Embedded Systems and IoT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There has been a tremendous rise in technological advances in the field of\nautomobiles and autonomous vehicles. With the increase in the number of driven\nvehicles, the safety concerns with the same have also risen. The cases of\naccidents and life-threatening injuries have skyrocketed. It has become a\nnecessity to provide adequate safety measures in automobiles. This project aims\nto develop a prototype for a smart vehicle system that provides real-time\nlocation of the vehicle on detection of a crash and alert the police station\nand relatives of the user, it has a panic button feature for a passenger's\nsafety. We also demonstrate a mechanism for cabin monitoring and an interactive\ninterface between a user and a car, where the user can inquire about the\ntemperature, humidity, and other variables inside the car remotely by sending a\ntext message to the GSM module which is present in the car. The GSM module\nconnects to the Arduino, which fetches the readings from sensors attached to it\nand sends it back to the user through a text message. We show the integration\nof MQ3 Alcohol sensor with Arduino for drunk driving prevention.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2021 18:41:46 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Das", "Abhishek", ""], ["Dhuri", "Vivek", ""], ["Desai", "Aditya", ""], ["Ail", "Suyash", ""], ["Kadam", "Ameya", ""]]}, {"id": "2101.00633", "submitter": "Md Naimul Hoque", "authors": "Md Naimul Hoque, and Klaus Mueller", "title": "Outcome-Explorer: A Causality Guided Interactive Visual Interface for\n  Interpretable Algorithmic Decision Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The widespread adoption of algorithmic decision-making systems has brought\nabout the necessity to interpret the reasoning behind these decisions. The\nmajority of these systems are complex black box models, and auxiliary models\nare often used to approximate and then explain their behavior. However, recent\nresearch suggests that such explanations are not overly accessible to\nnon-expert users and can lead to incorrect interpretation of the underlying\nmodel. In this paper, we show that a predictive and interactive model based on\ncausality is inherently interpretable, does not require any auxiliary model,\nand allows both expert and non-expert users to understand the model\ncomprehensively. To demonstrate our method we developed Outcome Explorer, a\ncausality guided interactive interface, and evaluated it by conducting\nthink-aloud sessions with three expert users and a user study with 18\nnon-expert users. All three expert users found our tool to be comprehensive in\nsupporting their explanation needs while the non-expert users were able to\nunderstand the inner workings of the model easily.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jan 2021 14:39:44 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 03:20:00 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hoque", "Md Naimul", ""], ["Mueller", "Klaus", ""]]}, {"id": "2101.00771", "submitter": "Jeremy Gordon", "authors": "Jeremy Gordon, Max Curran, John Chuang, Coye Cheshire", "title": "Covert Embodied Choice: Decision-Making and the Limits of Privacy Under\n  Biometric Surveillance", "comments": "12 pages. To be presented at CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445309", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithms engineered to leverage rich behavioral and biometric data to\npredict individual attributes and actions continue to permeate public and\nprivate life. A fundamental risk may emerge from misconceptions about the\nsensitivity of such data, as well as the agency of individuals to protect their\nprivacy when fine-grained (and possibly involuntary) behavior is tracked. In\nthis work, we examine how individuals adjust their behavior when incentivized\nto avoid the algorithmic prediction of their intent. We present results from a\nvirtual reality task in which gaze, movement, and other physiological signals\nare tracked. Participants are asked to decide which card to select without an\nalgorithmic adversary anticipating their choice. We find that while\nparticipants use a variety of strategies, data collected remains highly\npredictive of choice (80% accuracy). Additionally, a significant portion of\nparticipants became more predictable despite efforts to obfuscate, possibly\nindicating mistaken priors about the dynamics of algorithmic prediction.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 04:45:22 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Gordon", "Jeremy", ""], ["Curran", "Max", ""], ["Chuang", "John", ""], ["Cheshire", "Coye", ""]]}, {"id": "2101.00792", "submitter": "Pradipta Biswas", "authors": "Antony William Joseph, Jeevitha Shree DV, Kamal Preet Singh Saluja,\n  Abhishek Mukhopadhyay, Ramaswami Murugesh and Pradipta Biswas", "title": "Eye Tracking to Understand Impact of Aging on Mobile Phone Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Usage of smartphones and tablets have been increasing rapidly with\nmulti-touch interaction and powerful configurations. Performing tasks on mobile\nphones become more complex as people age, thereby increasing their cognitive\nworkload. In this context, we conducted an eye tracking study with 50\nparticipants between the age of 20 to 60 years and above, living in Bangalore,\nIndia. This paper focuses on visual nature of interaction with mobile user\ninterfaces. The study aims to investigate how aging affects user experience on\nmobile phones while performing complex tasks, and estimate cognitive workload\nusing eye tracking metrics. The study consisted of five tasks that were\nperformed on an android mobile phone under naturalistic scenarios using eye\ntracking glasses. We recorded ocular parameters like fixation rate, saccadic\nrate, average fixation duration, maximum fixation duration and standard\ndeviation of pupil dilation for left and right eyes respectively for each\nparticipant. Results from our study show that aging has a bigger effect on\nperformance of using mobile phones irrespective of any complex task given to\nthem. We noted that, participants aged between 50 to 60+ years had difficulties\nin completing tasks and showed increased cognitive workload. They took longer\nfixation duration to complete tasks which involved copy-paste operations.\nFurther, we identifed design implications and provided design recommendations\nfor designers and manufacturers.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 06:11:01 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Joseph", "Antony William", ""], ["DV", "Jeevitha Shree", ""], ["Saluja", "Kamal Preet Singh", ""], ["Mukhopadhyay", "Abhishek", ""], ["Murugesh", "Ramaswami", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2101.00794", "submitter": "Pradipta Biswas", "authors": "Somnath Arjun, KamalPreet Singh Saluja, Pradipta Biswas", "title": "Analysing ocular parameters for web browsing and graph visualization", "comments": "arXiv admin note: text overlap with arXiv:2005.05025", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a set of techniques to investigate eye gaze and fixation\npatterns while users interact with electronic user interfaces. In particular,\ntwo case studies are presented - one on analysing eye gaze while interacting\nwith deceptive materials in web pages and another on analysing graphs in\nstandard computer monitor and virtual reality displays. We analysed spatial and\ntemporal distributions of eye gaze fixations and sequence of eye gaze\nmovements. We used this information to propose new design guidelines to avoid\ndeceptive materials in web and user-friendly representation of data in 2D\ngraphs. In 2D graph study we identified that area graph has lowest number of\nclusters for user's gaze fixations and lowest average response time. The\nresults of 2D graph study were implemented in virtual and mixed reality\nenvironment. Along with this, it was ob-served that the duration while\ninteracting with deceptive materials in web pages is independent of the number\nof fixations. Furthermore, web-based data visualization tool for analysing eye\ntracking data from single and multiple users was developed.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 06:20:02 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Arjun", "Somnath", ""], ["Saluja", "KamalPreet Singh", ""], ["Biswas", "Pradipta", ""]]}, {"id": "2101.00812", "submitter": "Tatsuhito Hasegawa Dr.", "authors": "Tatsuhito Hasegawa", "title": "Smartphone Sensor-based Human Activity Recognition Robust to Different\n  Sampling Rates", "comments": "12 pages, 12 figures, accepted for publication on IEEE Sensors\n  Journal (2020)", "journal-ref": null, "doi": "10.1109/JSEN.2020.3038281", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a research field of human activity recognition that automatically\nrecognizes a user's physical activity through sensing technology incorporated\nin smartphones and other devices. When sensing daily activity, various\nmeasurement conditions, such as device type, possession method, wearing method,\nand measurement application, are often different depending on the user and the\ndate of the measurement. Models that predict activity from sensor values are\noften implemented by machine learning and are trained using a large amount of\nactivity-labeled sensor data measured from many users who provide labeled\nsensor data. However, collecting activity-labeled sensor data using each user's\nindividual smartphones causes data being measured in inconsistent environments\nthat may degrade the estimation accuracy of machine learning. In this study, I\npropose an activity recognition method that is robust to different sampling\nrates -- even in the measurement environment. The proposed method applies an\nadversarial network and data augmentation by downsampling to a common activity\nrecognition model to achieve the acquisition of feature representations that\nmake the sampling rate unspecifiable. Using the Human Activity Sensing\nConsortium (HASC), which is a dataset of basic activity recognition using\nsmartphone sensors, I conducted an evaluation experiment to simulate an\nenvironment in which various sampling rates were measured. As a result, I found\nthat estimation accuracy was reduced by the conventional method in the above\nenvironment and could be improved by my proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 07:33:52 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Hasegawa", "Tatsuhito", ""]]}, {"id": "2101.00874", "submitter": "Karola Marky", "authors": "Karola Marky, Andreas Wei{\\ss}, Thomas Kosch", "title": "Supporting Musical Practice Sessions Through HMD-Based Augmented Reality", "comments": null, "journal-ref": null, "doi": "10.18420/muc2019-ws-608", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Learning a musical instrument requires a lot of practice, which ideally,\nshould be done every day. During practice sessions, students are on their own\nin the overwhelming majority of the time, but access to experts that support\nstudents \"just-in-time\" is limited. Therefore, students commonly do not receive\nany feedback during their practice sessions. Adequate feedback, especially for\nbeginners, is highly important for three particular reasons: (1) preventing the\nacquirement of wrong motions, (2) avoiding frustration due to a steep learning\ncurve, and (3) potential health problems that arise from straining muscles or\njoints harmfully. In this paper, we envision the usage of head-mounted displays\nas assistance modality to support musical instrument learning. We propose a\nmodular concept for several assistance modes to help students during their\npractice sessions. Finally, we discuss hardware requirements and\nimplementations to realize the proposed concepts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 10:37:55 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Marky", "Karola", ""], ["Wei\u00df", "Andreas", ""], ["Kosch", "Thomas", ""]]}, {"id": "2101.01126", "submitter": "Elena Malakhovskaia", "authors": "E.K. Malakhovskaya, Y.P. Ekhlakov, P.V. Senchenko, A.A.Sidorov", "title": "Methodology for design of templates of text communication messages for\n  software marketing", "comments": "4 p", "journal-ref": null, "doi": "10.1088/1742-6596/1862/1/012014", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A methodology is proposed for design of templates of text communication\nmessages that are based on best practices of experts in software marketing,\nideas of marketing, communication theory, copywriting, media linguistics,\nsemiotics. Description of the subject area is based on conceptual modeling and\nproduction systems. For the purposes of testing, the methodology was used as\nthe basis of a software product. Decision support recommender system for design\nof communication messages for software marketing.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 06:45:47 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Malakhovskaya", "E. K.", ""], ["Ekhlakov", "Y. P.", ""], ["Senchenko", "P. V.", ""], ["Sidorov", "A. A.", ""]]}, {"id": "2101.01168", "submitter": "Wieslaw Kopec", "authors": "Rafa{\\l} Mas{\\l}yk, Kinga Skorupska, Piotr Gago, Marcin Niewi\\'nski,\n  Barbara Karpowicz, Anna Jaskulska, Katarzyna Abramczuk, Wies{\\l}aw Kope\\'c", "title": "Deploying Crowdsourcing for Workflow Driven Business Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this paper is to discuss how to integrate the possibilities\nof crowdsourcing platforms with systems supporting workflow to enable the\nengagement and interaction with business tasks of a wider group of people.\nThus, this work is an attempt to expand the functional capabilities of typical\nbusiness systems by allowing selected process tasks to be performed by\nunlimited human resources. Opening business tasks to crowdsourcing, within\nestablished Business Process Management Systems (BPMS) will improve the\nflexibility of company processes and allow for lower work-load and greater\nspecialization among the staff employed on-site. The presented conceptual work\nis based on the current international standards in this field, promoted by\nWorkflows Management Coalition. To this end, the functioning of business\nplatforms was analysed and their functionality was presented visually, followed\nby a proposal and a discussion of how to implement crowdsourcing into workflow\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 18:57:21 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Mas\u0142yk", "Rafa\u0142", ""], ["Skorupska", "Kinga", ""], ["Gago", "Piotr", ""], ["Niewi\u0144ski", "Marcin", ""], ["Karpowicz", "Barbara", ""], ["Jaskulska", "Anna", ""], ["Abramczuk", "Katarzyna", ""], ["Kope\u0107", "Wies\u0142aw", ""]]}, {"id": "2101.01285", "submitter": "Wieslaw Kopec", "authors": "Kinga Skorupska, Daniel Cnotkowski, Julia Paluch, Rafa{\\l} Mas{\\l}yk,\n  Anna Jaskulska, Monika Kornacka, Wies{\\l}aw Kope\\'c", "title": "All Factors Should Matter! Reference Checklist for Describing Research\n  Conditions in Pursuit of Comparable IVR Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant problem with immersive virtual reality (IVR) experiments is the\nability to compare research conditions. VR kits and IVR environments are\ncomplex and diverse but researchers from different fields, e.g. ICT,\npsychology, or marketing, often neglect to describe them with a level of detail\nsufficient to situate their research on the IVR landscape. Careful reporting of\nthese conditions may increase the applicability of research results and their\nimpact on the shared body of knowledge on HCI and IVR. Based on literature\nreview, our experience, practice and a synthesis of key IVR factors, in this\narticle we present a reference checklist for describing research conditions of\nIVR experiments. Including these in publications will contribute to the\ncomparability of IVR research and help other researchers decide to what extent\nreported results are relevant to their own research goals. The compiled\nchecklist is a ready-to-use reference tool and takes into account key hardware,\nsoftware and human factors as well as diverse factors connected to visual,\naudio, tactile, and other aspects of interaction.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 23:45:52 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 18:32:45 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Skorupska", "Kinga", ""], ["Cnotkowski", "Daniel", ""], ["Paluch", "Julia", ""], ["Mas\u0142yk", "Rafa\u0142", ""], ["Jaskulska", "Anna", ""], ["Kornacka", "Monika", ""], ["Kope\u0107", "Wies\u0142aw", ""]]}, {"id": "2101.01378", "submitter": "Muhammad Usman", "authors": "Muhammad Usman, Huanhuan Chen (School of Computer Science and\n  Technology, USTC, China)", "title": "Recent Trends in Food Intake Monitoring using Wearable Sensors", "comments": "27 Pages, 5 Figures - to be published in IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obesity and being over-weight add to the risk of some major life threatening\ndiseases. According to W.H.O., a considerable population suffers from these\ndisease whereas poor nutrition plays an important role in this context.\nTraditional food activity monitoring systems like Food Diaries allow manual\nrecord keeping of eating activities over time, and conduct nutrition analysis.\nHowever, these systems are prone to the problems of manual record keeping and\nbiased-reporting. Therefore, recently, the research community has focused on\ndesigning automatic food monitoring systems since the last decade which consist\nof one or multiple wearable sensors. These systems aim at providing different\nmacro and micro activity detections like chewing, swallowing, eating episodes,\nand food types as well as estimations like food mass and eating duration.\nResearchers have emphasized on high detection accuracy, low estimation errors,\nun-intrusive nature, low cost and real life implementation while designing\nthese systems, however a comprehensive automatic food monitoring system has yet\nnot been developed. Moreover, according to the best of our knowledge, there is\nno comprehensive survey in this field that delineates the automatic food\nmonitoring paradigm, covers a handful number of research studies, analyses\nthese studies against food intake monitoring tasks using various parameters,\nenlists the limitations and sets up future directions. In this research work,\nwe delineate the automatic food intake monitoring paradigm and present a survey\nof research studies. With special focus on studies with wearable sensors, we\nanalyze these studies against food activity monitoring tasks. We provide brief\ncomparison of these studies along with shortcomings based upon experimentation\nresults conducted under these studies. We setup future directions at the end to\nfacilitate the researchers working in this domain.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 07:05:00 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 04:28:22 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Usman", "Muhammad", "", "School of Computer Science and\n  Technology, USTC, China"], ["Chen", "Huanhuan", "", "School of Computer Science and\n  Technology, USTC, China"]]}, {"id": "2101.01401", "submitter": "Mohammad AL-Mousa Dr", "authors": "Ahmad Nabot, Firas Omar, Mohammed Almousa", "title": "Perceptions of Smartphone Users Acceptance and Adoption of Mobile\n  Commerce (MC) The Case of Jordan", "comments": "11 pages", "journal-ref": "Journal of Computer Science, 2020", "doi": "10.3844/jcssp.2020.532.542", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study investigates smartphone users perceptions of adopting and\naccepting Mobile Commerce (MC) based on users perceived adoption under the\nextended Technology Acceptance Model (TAM2) and Innovation Diffusion Theory\n(IDT) by providing research constructs for the domain of MC. Also, testing them\nwith reliability and validity and demonstrating their distinctiveness with\nhypothesis testing. The results show that consumer intention to adopt MC on a\nsmartphone was primarily influenced by Uncertainty Avoidance (UA), User\nExperience (UX), Perceived Ease Of Use (PEOU), Perceived Usefulness (PU) and\nCompatibility (CMP) as well as other constructs that positively determine\nattitude toward using a smartphone. For researchers, this study shows the\nbenefits of adapting TAM constructs into MC acceptance on a smartphone. The\nperceptions of MC adoption on a smartphone in this study investigated based on\na survey of specific people. For more reliability, a comprehensive study is\nneeded to show the attitudes of people from different environments.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 08:16:28 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Nabot", "Ahmad", ""], ["Omar", "Firas", ""], ["Almousa", "Mohammed", ""]]}, {"id": "2101.01524", "submitter": "Dakuo Wang", "authors": "Dakuo Wang and Liuping Wang and Zhan Zhang and Ding Wang and Haiyi Zhu\n  and Yvonne Gao and Xiangmin Fan and Feng Tian", "title": "\"Brilliant AI Doctor\" in Rural China: Tensions and Challenges in\n  AI-Powered CDSS Deployment", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445432", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) technology has been increasingly used in the\nimplementation of advanced Clinical Decision Support Systems (CDSS). Research\ndemonstrated the potential usefulness of AI-powered CDSS (AI-CDSS) in clinical\ndecision making scenarios. However, post-adoption user perception and\nexperience remain understudied, especially in developing countries. Through\nobservations and interviews with 22 clinicians from 6 rural clinics in China,\nthis paper reports the various tensions between the design of an AI-CDSS system\n(\"Brilliant Doctor\") and the rural clinical context, such as the misalignment\nwith local context and workflow, the technical limitations and usability\nbarriers, as well as issues related to transparency and trustworthiness of\nAI-CDSS. Despite these tensions, all participants expressed positive attitudes\ntoward the future of AI-CDSS, especially acting as \"a doctor's AI assistant\" to\nrealize a Human-AI Collaboration future in clinical settings. Finally we draw\non our findings to discuss implications for designing AI-CDSS interventions for\nrural clinical contexts in developing countries.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:32:48 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 22:44:26 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Wang", "Dakuo", ""], ["Wang", "Liuping", ""], ["Zhang", "Zhan", ""], ["Wang", "Ding", ""], ["Zhu", "Haiyi", ""], ["Gao", "Yvonne", ""], ["Fan", "Xiangmin", ""], ["Tian", "Feng", ""]]}, {"id": "2101.01583", "submitter": "Dakuo Wang", "authors": "Liuping Wang and Dakuo Wang and Feng Tian and Zhenhui Peng and\n  Xiangmin Fan and Zhan Zhang and Shuai Ma and Mo Yu and Xiaojuan Ma and Hongan\n  Wang", "title": "CASS: Towards Building a Social-Support Chatbot for Online Health\n  Community", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatbots systems, despite their popularity in today's HCI and CSCW research,\nfall short for one of the two reasons: 1) many of the systems use a rule-based\ndialog flow, thus they can only respond to a limited number of pre-defined\ninputs with pre-scripted responses; or 2) they are designed with a focus on\nsingle-user scenarios, thus it is unclear how these systems may affect other\nusers or the community. In this paper, we develop a generalizable chatbot\narchitecture (CASS) to provide social support for community members in an\nonline health community. The CASS architecture is based on advanced neural\nnetwork algorithms, thus it can handle new inputs from users and generate a\nvariety of responses to them. CASS is also generalizable as it can be easily\nmigrate to other online communities. With a follow-up field experiment, CASS is\nproven useful in supporting individual members who seek emotional support. Our\nwork also contributes to fill the research gap on how a chatbot may influence\nthe whole community's engagement.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 05:52:03 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 08:42:24 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 08:01:03 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Wang", "Liuping", ""], ["Wang", "Dakuo", ""], ["Tian", "Feng", ""], ["Peng", "Zhenhui", ""], ["Fan", "Xiangmin", ""], ["Zhang", "Zhan", ""], ["Ma", "Shuai", ""], ["Yu", "Mo", ""], ["Ma", "Xiaojuan", ""], ["Wang", "Hongan", ""]]}, {"id": "2101.01637", "submitter": "Chao Zhang", "authors": "Chao Zhang, Joaquin Vanschoren, Arlette van Wissen, Daniel Lakens,\n  Boris de Ruyter, and Wijnand A. IJsselsteijn", "title": "Theory-based Habit Modeling for Enhancing Behavior Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Psychological theories of habit posit that when a strong habit is formed\nthrough behavioral repetition, it can trigger behavior automatically in the\nsame environment. Given the reciprocal relationship between habit and behavior,\nchanging lifestyle behaviors (e.g., toothbrushing) is largely a task of\nbreaking old habits and creating new and healthy ones. Thus, representing\nusers' habit strengths can be very useful for behavior change support systems\n(BCSS), for example, to predict behavior or to decide when an intervention\nreaches its intended effect. However, habit strength is not directly observable\nand existing self-report measures are taxing for users. In this paper, built on\nrecent computational models of habit formation, we propose a method to enable\nintelligent systems to compute habit strength based on observable behavior. The\nhypothesized advantage of using computed habit strength for behavior prediction\nwas tested using data from two intervention studies, where we trained\nparticipants to brush their teeth twice a day for three weeks and monitored\ntheir behaviors using accelerometers. Through hierarchical cross-validation, we\nfound that for the task of predicting future brushing behavior, computed habit\nstrength clearly outperformed self-reported habit strength (in both studies)\nand was also superior to models based on past behavior frequency (in the larger\nsecond study). Our findings provide initial support for our theory-based\napproach of modeling user habits and encourages the use of habit computation to\ndeliver personalized and adaptive interventions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 16:42:59 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Zhang", "Chao", ""], ["Vanschoren", "Joaquin", ""], ["van Wissen", "Arlette", ""], ["Lakens", "Daniel", ""], ["de Ruyter", "Boris", ""], ["IJsselsteijn", "Wijnand A.", ""]]}, {"id": "2101.01652", "submitter": "Wieslaw Kopec", "authors": "Grzegorz Pochwatko, Barbara Karpowicz, Anna Chrzanowska, Wies{\\l}aw\n  Kope\\'c", "title": "Interpersonal distance in VR: reactions of older adults to the presence\n  of a virtual agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of virtual reality technology has increased its\navailability and, consequently, increased the number of its possible\napplications. The interest in the new medium has grown due to the entertainment\nindustry (games, VR experiences and movies). The number of freely available\ntraining and therapeutic applications is also increasing. Contrary to popular\nopinion, new technologies are also adopted by older adults. Creating virtual\nenvironments tailored to the needs and capabilities of older adults requires\nintense research on the behaviour of these participants in the most common\nsituations, towards commonly used elements of the virtual environment, in\ntypical sceneries. Comfortable immersion in a virtual environment is key to\nachieving the impression of presence. Presence is, in turn, necessary to obtain\nappropriate training, persuasive and therapeutic effects. A virtual agent (a\nhumanoid representation of an algorithm or artificial intelligence) is often an\nelement of the virtual environment interface. Maintaining an appropriate\ndistance to the agent is, therefore, a key parameter for the creator of the VR\nexperience. Older (65+) participants maintain greater distance towards an agent\n(a young white male) than younger ones (25-35). It may be caused by differences\nin the level of arousal, but also cultural norms. As a consequence, VR\ndevelopers are advised to use algorithms that maintain the agent at the\nappropriate distance, depending on the user's age.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 17:06:03 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Pochwatko", "Grzegorz", ""], ["Karpowicz", "Barbara", ""], ["Chrzanowska", "Anna", ""], ["Kope\u0107", "Wies\u0142aw", ""]]}, {"id": "2101.01718", "submitter": "Solomia Fedushko", "authors": "Solomiia Fedushko, Yuriy Syerov, Oleksandr Skybinskyi, Nataliya\n  Shakhovska, Zoryana Kunch", "title": "Efficiency of Using Utility for Usernames Verification in Online\n  Community Management", "comments": "10 pages, 6 figures", "journal-ref": "Efficiency of Using Utility for Username Verification in Online\n  Community Management. Proceedings of the International Workshop on Conflict\n  Management in Global Information Networks, 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study deals with the methods and means of checking the reliability of\nusernames of online communities on the basis of computer-linguistic analysis of\nthe results of their communicative interaction. The methodological basis of the\nstudy is a combination of general scientific methods and special approaches to\nthe study of the data verification of online communities in the Ukrainian\nsegment of the global information environment. The algorithm of functioning of\nthe utility Verifier of online community username is developed. The\ninformational model of the automated means of checking the usernames of online\ncommunity is designed. The utility Verifier of online community username data\nvalidation system approbation is realized in the online community. The\nindicator of the data verification system effectiveness is determined.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2021 19:42:59 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Fedushko", "Solomiia", ""], ["Syerov", "Yuriy", ""], ["Skybinskyi", "Oleksandr", ""], ["Shakhovska", "Nataliya", ""], ["Kunch", "Zoryana", ""]]}, {"id": "2101.01771", "submitter": "Efstratios Geronikolakis", "authors": "Efstratios Geronikolakis, George Papagiannakis", "title": "An XR rapid prototyping framework for interoperability across the\n  reality spectrum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of the Extended Reality (XR) spectrum, a superset of Mixed,\nAugmented and Virtual Reality, are gaining prominence and can be employed in a\nvariety of areas, such as virtual museums. Examples can be found in the areas\nof education, cultural heritage, health/treatment, entertainment, marketing,\nand more. The majority of computer graphics applications nowadays are used to\noperate only in one of the above realities. The lack of applications across the\nXR spectrum is a real shortcoming. There are many advantages resulting from\nthis problem's solution. Firstly, releasing an application across the XR\nspectrum could contribute in discovering its most suitable reality. Moreover,\nan application could be more immersive within a particular reality, depending\non its context. Furthermore, its availability increases to a broader range of\nusers. For instance, if an application is released both in Virtual and\nAugmented Reality, it is accessible to users that may lack the possession of a\nVR headset, but not of a mobile AR device. The question that arises at this\npoint, would be \"Is it possible for a full s/w application stack to be\nconverted across XR without sacrificing UI/UX in a semi-automatic way?\". It may\nbe quite difficult, depending on the architecture and application\nimplementation. Most companies nowadays support only one reality, due to their\nlack of UI/UX software architecture or resources to support the complete XR\nspectrum. In this work, we present an \"automatic reality transition\" in the\ncontext of virtual museum applications. We propose a development framework,\nwhich will automatically allow this XR transition. This framework transforms\nany XR project into different realities such as Augmented or Virtual. It also\nreduces the development time while increasing the XR availability of 3D\napplications, encouraging developers to release applications across the XR\nspectrum.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 20:27:47 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 13:32:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Geronikolakis", "Efstratios", ""], ["Papagiannakis", "George", ""]]}, {"id": "2101.01870", "submitter": "Hee-Seung Moon", "authors": "Hee-Seung Moon and Jiwon Seo", "title": "Optimal Action-based or User Prediction-based Haptic Guidance: Can You\n  Do Even Better?", "comments": "Accepted to ACM 2021 CHI Conference on Human Factors in Computing\n  Systems (CHI 2021)", "journal-ref": null, "doi": "10.1145/3411764.3445115", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently advanced robotics technology enables robots to assist users in\ntheir daily lives. Haptic guidance (HG) improves users' task performance\nthrough physical interaction between robots and users. It can be classified\ninto optimal action-based HG (OAHG), which assists users with an optimal\naction, and user prediction-based HG (UPHG), which assists users with their\nnext predicted action. This study aims to understand the difference between\nOAHG and UPHG and propose a combined HG (CombHG) that achieves optimal\nperformance by complementing each HG type, which has important implications for\nHG design. We propose implementation methods for each HG type using deep\nlearning-based approaches. A user study (n=20) in a haptic task environment\nindicated that UPHG induces better subjective evaluations, such as naturalness\nand comfort, than OAHG. In addition, the CombHG that we proposed further\ndecreases the disagreement between the user intention and HG, without reducing\nthe objective and subjective scores.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 04:59:19 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Moon", "Hee-Seung", ""], ["Seo", "Jiwon", ""]]}, {"id": "2101.01899", "submitter": "Maitree Leekha", "authors": "Vidit Jain, Maitree Leekha, Rajiv Ratn Shah, Jainendra Shukla", "title": "Exploring Semi-Supervised Learning for Predicting Listener Backchannels", "comments": "Accepted at CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445449", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Developing human-like conversational agents is a prime area in HCI research\nand subsumes many tasks. Predicting listener backchannels is one such\nactively-researched task. While many studies have used different approaches for\nbackchannel prediction, they all have depended on manual annotations for a\nlarge dataset. This is a bottleneck impacting the scalability of development.\nTo this end, we propose using semi-supervised techniques to automate the\nprocess of identifying backchannels, thereby easing the annotation process. To\nanalyze our identification module's feasibility, we compared the backchannel\nprediction models trained on (a) manually-annotated and (b) semi-supervised\nlabels. Quantitative analysis revealed that the proposed semi-supervised\napproach could attain 95% of the former's performance. Our user-study findings\nrevealed that almost 60% of the participants found the backchannel responses\npredicted by the proposed model more natural. Finally, we also analyzed the\nimpact of personality on the type of backchannel signals and validated our\nfindings in the user-study.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 07:30:38 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Jain", "Vidit", ""], ["Leekha", "Maitree", ""], ["Shah", "Rajiv Ratn", ""], ["Shukla", "Jainendra", ""]]}, {"id": "2101.02006", "submitter": "Abdallah Moubayed", "authors": "Abdallah Moubayed, MohammadNoor Injadat, Abdallah Shami, Hanan\n  Lutfiyya", "title": "Relationship between Student Engagement and Performance in e-Learning\n  Environment Using Association Rules", "comments": "1 Table, 1 Figure, published in 2018 IEEE World Engineering Education\n  Conference (EDUNINE)", "journal-ref": "2018 IEEE World Engineering Education Conference (EDUNINE), 2018,\n  pp. 1-6", "doi": "10.1109/EDUNINE.2018.8451005", "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of e-learning has emerged as a topic of interest in academia due to\nthe increased ease of accessing the Internet using using smart-phones and\nwireless devices. One of the challenges facing e-learning platforms is how to\nkeep students motivated and engaged. Moreover, it is also crucial to identify\nthe students that might need help in order to make sure their academic\nperformance doesn't suffer. To that end, this paper tries to investigate the\nrelationship between student engagement and their academic performance. Apriori\nassociation rules algorithm is used to derive a set of rules that relate\nstudent engagement to academic performance. Experimental results' analysis done\nusing confidence and lift metrics show that a positive correlation exists\nbetween students' engagement level and their academic performance in a blended\ne-learning environment. In particular, it is shown that higher engagement often\nleads to better academic performance. This cements the previous work that\nlinked engagement and academic performance in traditional classrooms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 17:00:23 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Moubayed", "Abdallah", ""], ["Injadat", "MohammadNoor", ""], ["Shami", "Abdallah", ""], ["Lutfiyya", "Hanan", ""]]}, {"id": "2101.02082", "submitter": "Yao Rong", "authors": "Yao Rong, Chao Han, Christian Hellert, Antje Loyal, Enkelejda Kasneci", "title": "Artificial Intelligence Methods in In-Cabin Use Cases: A Survey", "comments": "11 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As interest in autonomous driving increases, efforts are being made to meet\nrequirements for the high-level automation of vehicles. In this context, the\nfunctionality inside the vehicle cabin plays a key role in ensuring a safe and\npleasant journey for driver and passenger alike. At the same time, recent\nadvances in the field of artificial intelligence (AI) have enabled a whole\nrange of new applications and assistance systems to solve automated problems in\nthe vehicle cabin. This paper presents a thorough survey on existing work that\nutilizes AI methods for use-cases inside the driving cabin, focusing, in\nparticular, on application scenarios related to (1) driving safety and (2)\ndriving comfort. Results from the surveyed works show that AI technology has a\npromising future in tackling in-cabin tasks within the autonomous driving\naspect.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 15:08:39 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Rong", "Yao", ""], ["Han", "Chao", ""], ["Hellert", "Christian", ""], ["Loyal", "Antje", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2101.02244", "submitter": "Anamaria Crisan", "authors": "Anamaria Crisan, Michael Correll", "title": "User Ex Machina : Simulation as a Design Probe in Human-in-the-Loop Text\n  Analytics", "comments": "16 Pages, 9 Figures, CHI 2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic models are widely used analysis techniques for clustering documents and\nsurfacing thematic elements of text corpora. These models remain challenging to\noptimize and often require a \"human-in-the-loop\" approach where domain experts\nuse their knowledge to steer and adjust. However, the fragility,\nincompleteness, and opacity of these models means even minor changes could\ninduce large and potentially undesirable changes in resulting model. In this\npaper we conduct a simulation-based analysis of human-centered interactions\nwith topic models, with the objective of measuring the sensitivity of topic\nmodels to common classes of user actions. We find that user interactions have\nimpacts that differ in magnitude but often negatively affect the quality of the\nresulting modelling in a way that can be difficult for the user to evaluate. We\nsuggest the incorporation of sensitivity and \"multiverse\" analyses to topic\nmodel interfaces to surface and overcome these deficiencies.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2021 19:44:11 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Crisan", "Anamaria", ""], ["Correll", "Michael", ""]]}, {"id": "2101.02467", "submitter": "Evgeny Stemasov", "authors": "Evgeny Stemasov, Enrico Rukzio, Jan Gugenheimer", "title": "The Road to Ubiquitous Personal Fabrication: Modeling-Free Instead of\n  Increasingly Simple", "comments": "10 pages, 3 figures, accepted manuscript version to appear in IEEE\n  Pervasive Computing Volume 20, Issue 1", "journal-ref": "IEEE Pervasive Computing, Volume 20, Issue 1, IEEE, 2021, 1-9", "doi": "10.1109/MPRV.2020.3029650", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tools for personal digital fabrication (DF) are on the verge of reaching\nmass-adoption beyond technology enthusiasts, empowering consumers to fabricate\npersonalized artifacts. We argue that to achieve similar outreach and impact as\npersonal computing, personal fabrication research may have to venture beyond\never-simpler interfaces for creation, toward lowest-effort workflows for\nremixing. We surveyed novice-friendly DF workflows from the perspective of HCI.\nThrough this survey, we found two distinct approaches for this challenge: 1)\nsimplifying expert modeling tools (AutoCAD $\\to$ Tinkercad) and 2) enriching\ntools not involving primitive-based modeling with powerful customization (e.g.,\nThingiverse). Drawing parallel to content creation domains such as photography,\nwe argue that the bulk of content is created via remixing (2). In this article,\nwe argue that to be able to include the majority of the population in DF,\nresearch should embrace omission of workflow steps, shifting toward automation,\nremixing, and templates, instead of modeling from the ground up.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 10:13:14 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Stemasov", "Evgeny", ""], ["Rukzio", "Enrico", ""], ["Gugenheimer", "Jan", ""]]}, {"id": "2101.02555", "submitter": "Yehezkel Resheff", "authors": "Daniel Ben David, Yehezkel S. Resheff, Talia Tron", "title": "Explainable AI and Adoption of Financial Algorithmic Advisors: an\n  Experimental Study", "comments": "accepted: AIES '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study whether receiving advice from either a human or algorithmic advisor,\naccompanied by five types of Local and Global explanation labelings, has an\neffect on the readiness to adopt, willingness to pay, and trust in a financial\nAI consultant. We compare the differences over time and in various key\nsituations using a unique experimental framework where participants play a\nweb-based game with real monetary consequences. We observed that accuracy-based\nexplanations of the model in initial phases leads to higher adoption rates.\nWhen the performance of the model is immaculate, there is less importance\nassociated with the kind of explanation for adoption. Using more elaborate\nfeature-based or accuracy-based explanations helps substantially in reducing\nthe adoption drop upon model failure. Furthermore, using an autopilot increases\nadoption significantly. Participants assigned to the AI-labeled advice with\nexplanations were willing to pay more for the advice than the AI-labeled advice\nwith a No-explanation alternative. These results add to the literature on the\nimportance of XAI for algorithmic adoption and trust.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 09:34:38 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 14:26:48 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 06:44:15 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["David", "Daniel Ben", ""], ["Resheff", "Yehezkel S.", ""], ["Tron", "Talia", ""]]}, {"id": "2101.02565", "submitter": "Nico Feld", "authors": "Nico Feld", "title": "Augmentix -- An Augmented Reality System for asymmetric Teleteaching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Using augmented reality in education is already a common concept, as it has\nthe potential to turn learning into a motivational learning experience.\nHowever, current research only covers the students site of learning. Almost no\nresearch focuses on the teachers' site and whether augmented reality could\npotentially improve his/her workflow of teaching the students or not. Many\nresearchers do not differentiate between multiple user roles, like a student\nand a teacher. To allow investigation into these lacks of research, a teaching\nsystem \"Augmentix\" is presented, which includes a differentiation between the\ntwo user roles \"teacher\" and \"student\" to potentially enhances the teachers\nworkflow by using augmented reality. In this system's setting the student can\nexplore a virtual city in virtual reality and the teacher can guide him with\naugmented reality.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 14:43:51 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Feld", "Nico", ""]]}, {"id": "2101.02576", "submitter": "Tobias Drey", "authors": "Tobias Drey and Enrico Rukzio", "title": "Discussing the Risks of Adaptive Virtual Environments for User Autonomy", "comments": "Presented at the workshop \"Exploring Potentially Abusive Ethical,\n  Social and Political Implications of Mixed Reality Research in HCI\", CHI '20,\n  April 25-30, Honolulu, HI, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive virtual environments are an opportunity to support users and\nincrease their flow, presence, immersion, and overall experience. Possible\nfields of application are adaptive individual education, gameplay adjustment,\nprofessional work, and personalized content. But who benefits more from this\nadaptivity, the users who can enjoy a greater user experience or the companies\nor governments who are completely in control of the provided content. While the\nuser autonomy decreases for individuals, the power of institutions raises, and\nthe risk exists that personal opinions are precisely controlled. In this\nposition paper, we will argue that researchers should not only propose the\nbenefits of their work but also critically discuss what are possible abusive\nuse cases. Therefore, we will examine two use cases in the fields of\nprofessional work and personalized content and show possible abusive use.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 14:55:09 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Drey", "Tobias", ""], ["Rukzio", "Enrico", ""]]}, {"id": "2101.02768", "submitter": "Javad Rahimipour Anaraki", "authors": "Javad Rahimipour Anaraki, Chelsea Anne Rauh, Jason Leung, Tom Chau", "title": "EmoconLite: Bridging the Gap Between Emotiv and Play for Children With\n  Severe Disabilities", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interfaces (BCIs) allow users to control computer applications\nby modulating their brain activity. Since BCIs rely solely on brain activity,\nthey have enormous potential as an alternative access method for engaging\nchildren with severe disabilities and/or medical complexities in therapeutic\nrecreation and leisure. In particular, one commercially available BCI platform\nis the Emotiv EPOC headset, which is a portable and affordable\nelectroencephalography (EEG) device. Combined with the EmotivBCI software, the\nEmotiv system can generate a model to discern between different mental tasks\nbased on the user's EEG signals in real-time. While the Emotiv system shows\npromise for use by the pediatric population in the setting of a BCI clinic, it\nlacks integrated support that allows users to directly control computer\napplications using the generated classification output. To achieve this, users\nwould have to create their own program, which can be challenging for those who\nmay not be technologically inclined. To address this gap, we developed a freely\navailable and user-friendly BCI software application called EmoconLite. Using\nthe classification output from EmotivBCI, EmoconLite allows users to play\nYouTube video clips and a variety of video games from multiple platforms,\nultimately creating an end-to-end solution for users. Through its deployment in\nthe Holland Bloorview Kids Rehabilitation Hospital's BCI clinic, EmoconLite is\nbridging the gap between research and clinical practice, providing children\nwith access to BCI technology and supporting BCI-enabled play.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 21:24:12 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 21:17:49 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 03:28:38 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Anaraki", "Javad Rahimipour", ""], ["Rauh", "Chelsea Anne", ""], ["Leung", "Jason", ""], ["Chau", "Tom", ""]]}, {"id": "2101.02873", "submitter": "Rocky Chen", "authors": "Guanhua Ye, Hongzhi Yin, Tong Chen, Hongxu Chen, Lizhen Cui,\n  Xiangliang Zhang", "title": "FENet: A Frequency Extraction Network for Obstructive Sleep Apnea\n  Detection", "comments": "To appear in JBHI", "journal-ref": null, "doi": "10.1109/JBHI.2021.3050113", "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obstructive Sleep Apnea (OSA) is a highly prevalent but inconspicuous disease\nthat seriously jeopardizes the health of human beings. Polysomnography (PSG),\nthe gold standard of detecting OSA, requires multiple specialized sensors for\nsignal collection, hence patients have to physically visit hospitals and bear\nthe costly treatment for a single detection. Recently, many single-sensor\nalternatives have been proposed to improve the cost efficiency and convenience.\nAmong these methods, solutions based on RR-interval (i.e., the interval between\ntwo consecutive pulses) signals reach a satisfactory balance among comfort,\nportability and detection accuracy. In this paper, we advance RR-interval based\nOSA detection by considering its real-world practicality from energy\nperspectives. As photoplethysmogram (PPG) pulse sensors are commonly equipped\non smart wrist-worn wearable devices (e.g., smart watches and wristbands), the\nenergy efficiency of the detection model is crucial to fully support an\novernight observation on patients. This creates challenges as the PPG sensors\nare unable to keep collecting continuous signals due to the limited battery\ncapacity on smart wrist-worn devices. Therefore, we propose a novel Frequency\nExtraction Network (FENet), which can extract features from different frequency\nbands of the input RR-interval signals and generate continuous detection\nresults with downsampled, discontinuous RR-interval signals. With the help of\nthe one-to-multiple structure, FENet requires only one-third of the operation\ntime of the PPG sensor, thus sharply cutting down the energy consumption and\nenabling overnight diagnosis. Experimental results on real OSA datasets reveal\nthe state-of-the-art performance of FENet.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 06:42:36 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Ye", "Guanhua", ""], ["Yin", "Hongzhi", ""], ["Chen", "Tong", ""], ["Chen", "Hongxu", ""], ["Cui", "Lizhen", ""], ["Zhang", "Xiangliang", ""]]}, {"id": "2101.02936", "submitter": "Pradipta Biswas", "authors": "Pradipta Biswas, Pilar Orero, Manohar Swaminathan, Kavita Krishnaswamy\n  and Peter Robinson", "title": "Adaptive Accessible AR/VR Systems", "comments": "ACM CHI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Augmented, virtual and mixed reality technologies offer new ways of\ninteracting with digital media. However, such technologies are not well\nexplored for people with different ranges of abilities beyond a few specific\nnavigation and gaming applications. While new standardization activities are\ninvestigating accessibility issues with existing AR/VR systems, commercial\nsystems are still confined to specialized hardware and software limiting their\nwidespread adoption among people with disabilities as well as seniors. This\nproposal takes a novel approach by exploring the application of user\nmodel-based personalization for AR/VR systems to improve accessibility. The\nworkshop will be organized by experienced researchers in the field of human\ncomputer interaction, robotics control, assistive technology, and AR/VR\nsystems, and will consist of peer reviewed papers and hands-on demonstrations.\nKeynote speeches and demonstrations will cover latest accessibility research at\nMicrosoft, Google, Verizon and leading universities.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 10:01:21 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Biswas", "Pradipta", ""], ["Orero", "Pilar", ""], ["Swaminathan", "Manohar", ""], ["Krishnaswamy", "Kavita", ""], ["Robinson", "Peter", ""]]}, {"id": "2101.03267", "submitter": "Shili Sheng", "authors": "Shili Sheng, Erfan Pakdamanian, Kyungtae Han, Ziran Wang, John\n  Lenneman, and Lu Feng", "title": "Trust-Based Route Planning for Automated Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works consider the personalized route planning based on user\nprofiles, none of which accounts for human trust. We argue that human trust is\nan important factor to consider when planning routes for automated vehicles.\nThis paper presents the first trust-based route planning approach for automated\nvehicles. We formalize the human-vehicle interaction as a partially observable\nMarkov decision process (POMDP) and model trust as a partially observable state\nvariable of the POMDP, representing human's hidden mental state. We designed\nand conducted an online user study with 100 participants on the Amazon\nMechanical Turk platform to collect data of users' trust in automated vehicles.\nWe build data-driven models of trust dynamics and takeover decisions, which are\nincorporated in the POMDP framework. We compute optimal routes for automated\nvehicles by solving optimal policies in the POMDP planning. We evaluated the\nresulting routes via human subject experiments with 22 participants on a\ndriving simulator. The experimental results show that participants taking the\ntrust-based route generally resulted in higher cumulative POMDP rewards and\nreported more positive responses in the after-driving survey than those taking\nthe baseline trust-free route.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 01:17:25 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 14:10:50 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Sheng", "Shili", ""], ["Pakdamanian", "Erfan", ""], ["Han", "Kyungtae", ""], ["Wang", "Ziran", ""], ["Lenneman", "John", ""], ["Feng", "Lu", ""]]}, {"id": "2101.03394", "submitter": "Mohammad Aliannejadi", "authors": "Mohammad Aliannejadi and Hamed Zamani and Fabio Crestani and W. Bruce\n  Croft", "title": "Context-Aware Target Apps Selection and Recommendation for Enhancing\n  Personal Mobile Assistants", "comments": "Accepted to ACM TOIS, 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users install many apps on their smartphones, raising issues related to\ninformation overload for users and resource management for devices. Moreover,\nthe recent increase in the use of personal assistants has made mobile devices\neven more pervasive in users' lives. This paper addresses two research problems\nthat are vital for developing effective personal mobile assistants: target apps\nselection and recommendation. The former is the key component of a unified\nmobile search system: a system that addresses the users' information needs for\nall the apps installed on their devices with a unified mode of access. The\nlatter, instead, predicts the next apps that the users would want to launch.\nHere we focus on context-aware models to leverage the rich contextual\ninformation available to mobile devices. We design an in situ study to collect\nthousands of mobile queries enriched with mobile sensor data (now publicly\navailable for research purposes). With the aid of this dataset, we study the\nuser behavior in the context of these tasks and propose a family of\ncontext-aware neural models that take into account the sequential, temporal,\nand personal behavior of users. We study several state-of-the-art models and\nshow that the proposed models significantly outperform the baselines.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jan 2021 17:07:47 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Aliannejadi", "Mohammad", ""], ["Zamani", "Hamed", ""], ["Crestani", "Fabio", ""], ["Croft", "W. Bruce", ""]]}, {"id": "2101.03477", "submitter": "Peter Washington", "authors": "Peter Washington, Onur Cezmi Mutlu, Emilie Leblanc, Aaron Kline, Cathy\n  Hou, Brianna Chrisman, Nate Stockham, Kelley Paskov, Catalin Voss, Nick\n  Haber, Dennis Wall", "title": "Using Crowdsourcing to Train Facial Emotion Machine Learning Models with\n  Ambiguous Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Current emotion detection classifiers predict discrete emotions. However,\nliterature in psychology has documented that compound and ambiguous facial\nexpressions are often evoked by humans. As a stride towards development of\nmachine learning models that more accurately reflect compound and ambiguous\nemotions, we replace traditional one-hot encoded label representations with a\ncrowd's distribution of labels. We center our study on the Child Affective\nFacial Expression (CAFE) dataset, a gold standard dataset of pediatric facial\nexpressions which includes 100 human labels per image. We first acquire\ncrowdsourced labels for 207 emotions from CAFE and demonstrate that the\nconsensus labels from the crowd tend to match the consensus from the original\nCAFE raters, validating the utility of crowdsourcing. We then train two\nversions of a ResNet-152 classifier on CAFE images with two types of labels (1)\ntraditional one-hot encoding and (2) vector labels representing the crowd\ndistribution of responses. We compare the resulting output distributions of the\ntwo classifiers. While the traditional F1-score for the one-hot encoding\nclassifier is much higher (94.33% vs. 78.68%), the output probability vector of\nthe crowd-trained classifier much more closely resembles the distribution of\nhuman labels (t=3.2827, p=0.0014). For many applications of affective\ncomputing, reporting an emotion probability distribution that more closely\nresembles human interpretation can be more important than traditional machine\nlearning metrics. This work is a first step for engineers of interactive\nsystems to account for machine learning cases with ambiguous classes and we\nhope it will generate a discussion about machine learning with ambiguous labels\nand leveraging crowdsourcing as a potential solution.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 05:26:55 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Washington", "Peter", ""], ["Mutlu", "Onur Cezmi", ""], ["Leblanc", "Emilie", ""], ["Kline", "Aaron", ""], ["Hou", "Cathy", ""], ["Chrisman", "Brianna", ""], ["Stockham", "Nate", ""], ["Paskov", "Kelley", ""], ["Voss", "Catalin", ""], ["Haber", "Nick", ""], ["Wall", "Dennis", ""]]}, {"id": "2101.03478", "submitter": "Peter Washington", "authors": "Peter Washington, Aaron Kline, Onur Cezmi Mutlu, Emilie Leblanc, Cathy\n  Hou, Nate Stockham, Kelley Paskov, Brianna Chrisman, Dennis P. Wall", "title": "Activity Recognition with Moving Cameras and Few Training Examples:\n  Applications for Detection of Autism-Related Headbanging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Activity recognition computer vision algorithms can be used to detect the\npresence of autism-related behaviors, including what are termed \"restricted and\nrepetitive behaviors\", or stimming, by diagnostic instruments. The limited data\nthat exist in this domain are usually recorded with a handheld camera which can\nbe shaky or even moving, posing a challenge for traditional feature\nrepresentation approaches for activity detection which mistakenly capture the\ncamera's motion as a feature. To address these issues, we first document the\nadvantages and limitations of current feature representation techniques for\nactivity recognition when applied to head banging detection. We then propose a\nfeature representation consisting exclusively of head pose keypoints. We create\na computer vision classifier for detecting head banging in home videos using a\ntime-distributed convolutional neural network (CNN) in which a single CNN\nextracts features from each frame in the input sequence, and these extracted\nfeatures are fed as input to a long short-term memory (LSTM) network. On the\nbinary task of predicting head banging and no head banging within videos from\nthe Self Stimulatory Behaviour Dataset (SSBD), we reach a mean F1-score of\n90.77% using 3-fold cross validation (with individual fold F1-scores of 83.3%,\n89.0%, and 100.0%) when ensuring that no child who appeared in the train set\nwas in the test set for all folds. This work documents a successful technique\nfor training a computer vision classifier which can detect human motion with\nfew training examples and even when the camera recording the source clips is\nunstable. The general methods described here can be applied by designers and\ndevelopers of interactive systems towards other human motion and pose\nclassification problems used in mobile and ubiquitous interactive systems.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jan 2021 05:37:05 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Washington", "Peter", ""], ["Kline", "Aaron", ""], ["Mutlu", "Onur Cezmi", ""], ["Leblanc", "Emilie", ""], ["Hou", "Cathy", ""], ["Stockham", "Nate", ""], ["Paskov", "Kelley", ""], ["Chrisman", "Brianna", ""], ["Wall", "Dennis P.", ""]]}, {"id": "2101.03648", "submitter": "So Yeon Park", "authors": "So Yeon Park, Blair Kaneshiro", "title": "User perspectives on critical factors for collaborative playlists", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Collaborative playlists (CP) enable listeners to curate music together,\ntranslating long-standing social practices around music consumption into the\nage of streaming. Yet despite their role in connecting people through music, we\nlack an understanding of factors that are critical to CPs and their enjoyment.\nTo understand what users consider important to CPs and their usage, we\ninvestigated aspects that are perceived to be most useful and lacking in\ntoday's CP implementations. We conducted a survey to collect open-ended text\nresponses from real-world CP users. Using thematic analysis, we derived the\nCodebook of Critical CP Factors, which comprises eight aspects. We gained\ninsights into which aspects are particularly useful, and which are absent and\ndesired by current CP users. From these findings we propose design implications\nto inform further design of CP functionalities and platforms, and highlight\npotential benefits and challenges related to their adoption in current music\nservices.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 00:30:44 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Park", "So Yeon", ""], ["Kaneshiro", "Blair", ""]]}, {"id": "2101.03680", "submitter": "Aoyu Wu", "authors": "Aoyu Wu, Liwenhan Xie, Bongshin Lee, Yun Wang, Weiwei Cui, Huamin Qu", "title": "Learning to Automate Chart Layout Configurations Using Crowdsourced\n  Paired Comparison", "comments": "Accepted at ACM CHI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We contribute a method to automate parameter configurations for chart layouts\nby learning from human preferences. Existing charting tools usually determine\nthe layout parameters using predefined heuristics, producing sub-optimal\nlayouts. People can repeatedly adjust multiple parameters (e.g., chart size,\ngap) to achieve visually appealing layouts. However, this trial-and-error\nprocess is unsystematic and time-consuming, without a guarantee of improvement.\nTo address this issue, we develop Layout Quality Quantifier (LQ2), a machine\nlearning model that learns to score chart layouts from pairwise crowdsourcing\ndata. Combined with optimization techniques, LQ2 recommends layout parameters\nthat improve the charts' layout quality. We apply LQ2 on bar charts and conduct\nuser studies to evaluate its effectiveness by examining the quality of layouts\nit produces. Results show that LQ2 can generate more visually appealing layouts\nthan both laypeople and baselines. This work demonstrates the feasibility and\nusages of quantifying human preferences and aesthetics for chart layouts.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 02:49:46 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wu", "Aoyu", ""], ["Xie", "Liwenhan", ""], ["Lee", "Bongshin", ""], ["Wang", "Yun", ""], ["Cui", "Weiwei", ""], ["Qu", "Huamin", ""]]}, {"id": "2101.03706", "submitter": "Shuo Niu", "authors": "Shuo Niu, Ava Bartolome, Cat Mai, and Nguyen B. Ha", "title": "#StayHome #WithMe: How Do YouTubers Help with COVID-19 Loneliness?", "comments": "CHI Conference on Human Factors in Computing Systems (CHI '21), May\n  8--13, 2021, Yokohama, Japan", "journal-ref": "Proceedings of the 2021 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3411764.3445397", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loneliness threatens public mental wellbeing during COVID-19. In response,\nYouTube creators participated in the #StayHome #WithMe movement (SHWM) and made\nmyriad videos for people experiencing loneliness or boredom at home.\nUser-shared videos generate parasocial attachment and virtual connectedness.\nHowever, there is limited knowledge of how creators contributed videos during\ndisasters to provide social provisions as disaster-relief. Grounded on Weiss's\nloneliness theory, this work analyzed 1488 SHWM videos to examine video sharing\nas a pathway to social provisions. Findings suggested that skill and knowledge\nsharing, entertaining arts, homelife activities, live chatting, and gameplay\nwere the most popular video styles. YouTubers utilized parasocial relationships\nto form a space for staying away from the disaster. SHWM YouTubers provided\nfriend-like, mentor-like, and family-like provisions through videos in\ndifferent styles. Family-like provisions led to the highest overall viewer\nengagement. Based on the findings, design implications for supporting viewers'\nmental wellbeing in disasters are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 05:30:19 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2021 16:47:57 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 23:56:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Niu", "Shuo", ""], ["Bartolome", "Ava", ""], ["Mai", "Cat", ""], ["Ha", "Nguyen B.", ""]]}, {"id": "2101.03769", "submitter": "Pieter Wolfert", "authors": "Pieter Wolfert, Nicole Robinson, Tony Belpaeme", "title": "A Review of Evaluation Practices of Gesture Generation in Embodied\n  Conversational Agents", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embodied Conversational Agents (ECA) take on different forms, including\nvirtual avatars or physical agents, such as a humanoid robot. ECAs are often\ndesigned to produce nonverbal behaviour to complement or enhance its verbal\ncommunication. One form of nonverbal behaviour is co-speech gesturing, which\ninvolves movements that the agent makes with its arms and hands that is paired\nwith verbal communication. Co-speech gestures for ECAs can be created using\ndifferent generation methods, such as rule-based and data-driven processes.\nHowever, reports on gesture generation methods use a variety of evaluation\nmeasures, which hinders comparison. To address this, we conducted a systematic\nreview on co-speech gesture generation methods for iconic, metaphoric, deictic\nor beat gestures, including their evaluation methods. We reviewed 22 studies\nthat had an ECA with a human-like upper body that used co-speech gesturing in a\nsocial human-agent interaction, including a user study to evaluate its\nperformance. We found most studies used a within-subject design and relied on a\nform of subjective evaluation, but lacked a systematic approach. Overall,\nmethodological quality was low-to-moderate and few systematic conclusions could\nbe drawn. We argue that the field requires rigorous and uniform tools for the\nevaluation of co-speech gesture systems. We have proposed recommendations for\nfuture empirical evaluation, including standardised phrases and test scenarios\nto test generative models. We have proposed a research checklist that can be\nused to report relevant information for the evaluation of generative models as\nwell as to evaluate co-speech gesture use.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 08:56:23 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wolfert", "Pieter", ""], ["Robinson", "Nicole", ""], ["Belpaeme", "Tony", ""]]}, {"id": "2101.03970", "submitter": "Dakuo Wang", "authors": "Dakuo Wang and Q. Vera Liao and Yunfeng Zhang and Udayan Khurana and\n  Horst Samulowitz and Soya Park and Michael Muller and Lisa Amini", "title": "How Much Automation Does a Data Scientist Want?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science and machine learning (DS/ML) are at the heart of the recent\nadvancements of many Artificial Intelligence (AI) applications. There is an\nactive research thread in AI, \\autoai, that aims to develop systems for\nautomating end-to-end the DS/ML Lifecycle. However, do DS and ML workers really\nwant to automate their DS/ML workflow? To answer this question, we first\nsynthesize a human-centered AutoML framework with 6 User Role/Personas, 10\nStages and 43 Sub-Tasks, 5 Levels of Automation, and 5 Types of Explanation,\nthrough reviewing research literature and marketing reports. Secondly, we use\nthe framework to guide the design of an online survey study with 217 DS/ML\nworkers who had varying degrees of experience, and different user roles\n\"matching\" to our 6 roles/personas. We found that different user personas\nparticipated in distinct stages of the lifecycle -- but not all stages. Their\ndesired levels of automation and types of explanation for AutoML also varied\nsignificantly depending on the DS/ML stage and the user persona. Based on the\nsurvey results, we argue there is no rationale from user needs for complete\nautomation of the end-to-end DS/ML lifecycle. We propose new next steps for\nuser-controlled DS/ML automation.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jan 2021 04:09:08 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wang", "Dakuo", ""], ["Liao", "Q. Vera", ""], ["Zhang", "Yunfeng", ""], ["Khurana", "Udayan", ""], ["Samulowitz", "Horst", ""], ["Park", "Soya", ""], ["Muller", "Michael", ""], ["Amini", "Lisa", ""]]}, {"id": "2101.04035", "submitter": "Jesse Josua Benjamin", "authors": "Jesse Josua Benjamin, Arne Berger, Nick Merrill, James Pierce", "title": "Machine Learning Uncertainty as a Design Material: A\n  Post-Phenomenological Inquiry", "comments": "Accepted to ACM 2021 CHI Conference on Human Factors in Computing\n  Systems (CHI 2021)", "journal-ref": null, "doi": "10.1145/3411764.3445481", "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Design research is important for understanding and interrogating how emerging\ntechnologies shape human experience. However, design research with Machine\nLearning (ML) is relatively underdeveloped. Crucially, designers have not found\na grasp on ML uncertainty as a design opportunity rather than an obstacle. The\ntechnical literature points to data and model uncertainties as two main\nproperties of ML. Through post-phenomenology, we position uncertainty as one\ndefining material attribute of ML processes which mediate human experience. To\nunderstand ML uncertainty as a design material, we investigate four design\nresearch case studies involving ML. We derive three provocative concepts:\nthingly uncertainty: ML-driven artefacts have uncertain, variable relations to\ntheir environments; pattern leakage: ML uncertainty can lead to patterns\nshaping the world they are meant to represent; and futures creep: ML\ntechnologies texture human relations to time with uncertainty. Finally, we\noutline design research trajectories and sketch a post-phenomenological\napproach to human-ML relations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:11:19 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Benjamin", "Jesse Josua", ""], ["Berger", "Arne", ""], ["Merrill", "Nick", ""], ["Pierce", "James", ""]]}, {"id": "2101.04096", "submitter": "Jeremy Speth", "authors": "Jeremy Speth, Nathan Vance, Patrick Flynn, Kevin Bowyer, Adam Czajka", "title": "Remote Pulse Estimation in the Presence of Face Masks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote photoplethysmography (rPPG), a family of techniques for monitoring\nblood volume changes, may be especially useful for widespread contactless\nhealth monitoring using face video from consumer-grade visible-light cameras.\nThe COVID-19 pandemic has caused the widespread use of protective face masks.\nWe found that occlusions from cloth face masks increased the mean absolute\nerror of heart rate estimation by more than 80\\% when deploying methods\ndesigned on unmasked faces. We show that augmenting unmasked face videos by\nadding patterned synthetic face masks forces the model to attend to the\nperiocular and forehead regions, improving performance and closing the gap\nbetween masked and unmasked pulse estimation. To our knowledge, this paper is\nthe first to analyse the impact of face masks on the accuracy of pulse\nestimation and offers several novel contributions: (a) 3D CNN-based method\ndesigned for remote photoplethysmography in a presence of face masks, (b) two\npublicly available pulse estimation datasets acquired from 86 unmasked and 61\nmasked subjects, (c) evaluations of handcrafted algorithms and a 3D CNN trained\non videos of unmasked faces and with masks synthetically added, and (d) data\naugmentation method to add a synthetic mask to a face video.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 18:43:14 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 16:03:21 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Speth", "Jeremy", ""], ["Vance", "Nathan", ""], ["Flynn", "Patrick", ""], ["Bowyer", "Kevin", ""], ["Czajka", "Adam", ""]]}, {"id": "2101.04251", "submitter": "Michael Correll", "authors": "Rachael Zehrung and Astha Singhal and Michael Correll and Leilani\n  Battle", "title": "Vis Ex Machina: An Analysis of Trust in Human versus Algorithmically\n  Generated Visualization Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  More visualization systems are simplifying the data analysis process by\nautomatically suggesting relevant visualizations. However, little work has been\ndone to understand if users trust these automated recommendations. In this\npaper, we present the results of a crowd-sourced study exploring preferences\nand perceived quality of recommendations that have been positioned as either\nhuman-curated or algorithmically generated. We observe that while participants\ninitially prefer human recommenders, their actions suggest an indifference for\nrecommendation source when evaluating visualization recommendations. The\nrelevance of presented information (e.g., the presence of certain data fields)\nwas the most critical factor, followed by a belief in the recommender's ability\nto create accurate visualizations. Our findings suggest a general indifference\ntowards the provenance of recommendations, and point to idiosyncratic\ndefinitions of visualization quality and trustworthiness that may not be\ncaptured by simple measures. We suggest that recommendation systems should be\ntailored to the information-foraging strategies of specific users.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 01:06:39 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 22:13:34 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zehrung", "Rachael", ""], ["Singhal", "Astha", ""], ["Correll", "Michael", ""], ["Battle", "Leilani", ""]]}, {"id": "2101.04271", "submitter": "Kelly Mack", "authors": "Kelly Mack, Emma McDonnell, Dhruv Jain, Lucy Lu Wang, Jon E.\n  Froehlich, Leah Findlater", "title": "What Do We Mean by \"Accessibility Research\"? A Literature Survey of\n  Accessibility Papers in CHI and ASSETS from 1994 to 2019", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445412", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accessibility research has grown substantially in the past few decades, yet\nthere has been no literature review of the field. To understand current and\nhistorical trends, we created and analyzed a dataset of accessibility papers\nappearing at CHI and ASSETS since ASSETS' founding in 1994. We qualitatively\ncoded areas of focus and methodological decisions for the past 10 years\n(2010-2019, N=506 papers), and analyzed paper counts and keywords over the full\n26 years (N=836 papers). Our findings highlight areas that have received\ndisproportionate attention and those that are underserved--for example, over\n43% of papers in the past 10 years are on accessibility for blind and low\nvision people. We also capture common study characteristics, such as the roles\nof disabled and nondisabled participants as well as sample sizes (e.g., a\nmedian of 13 for participant groups with disabilities and older adults). We\nclose by critically reflecting on gaps in the literature and offering guidance\nfor future work in the field.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 03:00:44 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 22:10:55 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 20:03:04 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2021 21:10:48 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Mack", "Kelly", ""], ["McDonnell", "Emma", ""], ["Jain", "Dhruv", ""], ["Wang", "Lucy Lu", ""], ["Froehlich", "Jon E.", ""], ["Findlater", "Leah", ""]]}, {"id": "2101.04296", "submitter": "Anamaria Crisan", "authors": "Anamaria Crisan, Brittany Fiore-Gartland", "title": "Fits and Starts: Enterprise Use of AutoML and the Role of Humans in the\n  Loop", "comments": "CHI 2021 Conference, 15 pages, 3 figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AutoML systems can speed up routine data science work and make machine\nlearning available to those without expertise in statistics and computer\nscience. These systems have gained traction in enterprise settings where pools\nof skilled data workers are limited. In this study, we conduct interviews with\n29 individuals from organizations of different sizes to characterize how they\ncurrently use, or intend to use, AutoML systems in their data science work. Our\ninvestigation also captures how data visualization is used in conjunction with\nAutoML systems. Our findings identify three usage scenarios for AutoML that\nresulted in a framework summarizing the level of automation desired by data\nworkers with different levels of expertise. We surfaced the tension between\nspeed and human oversight and found that data visualization can do a poor job\nbalancing the two. Our findings have implications for the design and\nimplementation of human-in-the-loop visual analytics approaches.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 04:52:48 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Crisan", "Anamaria", ""], ["Fiore-Gartland", "Brittany", ""]]}, {"id": "2101.04449", "submitter": "Andrey Krekhov", "authors": "Katharina Emmerich, Andrey Krekhov, Sebastian Cmentowski, Jens Krueger", "title": "Streaming VR Games to the Broad Audience: A Comparison of the\n  First-Person and Third-Person Perspectives", "comments": "14 pages, 5 figures, 4 tables, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectatorship experience for virtual reality (VR) games differs strongly\nfrom its non-VR precursor. When watching non-VR games on platforms such as\nTwitch, spectators just see what the player sees, as the physical interaction\nis mostly unimportant for the overall impression. In VR, the immersive\nfull-body interaction is a crucial part of the player experience. Hence,\ncontent creators, such as streamers, often rely on green screens or similar\nsolutions to offer a mixed-reality third-person view to disclose their\nfull-body actions. Our work compares the most popular realizations of the\nfirst-person and the third-person perspective in an online survey (N=217) with\nthree different VR games. Contrary to the current trend to stream in\nthird-person, our key result is that most viewers prefer the first-person\nversion, which they attribute mostly to the better focus on in-game actions and\nhigher involvement. Based on the study insights, we provide design\nrecommendations for both perspectives.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 12:46:04 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Emmerich", "Katharina", ""], ["Krekhov", "Andrey", ""], ["Cmentowski", "Sebastian", ""], ["Krueger", "Jens", ""]]}, {"id": "2101.04459", "submitter": "Antonios Saravanos", "authors": "Antonios Saravanos (1), Stavros Zervoudakis (1), Dongnanzi Zheng (1),\n  Neil Stott (2), Bohdan Hawryluk (1), Donatella Delfino (1) ((1) New York\n  University, (2) Cambridge Judge Business School)", "title": "The Hidden Cost of Using Amazon Mechanical Turk for Research", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we investigate the attentiveness exhibited by participants\nsourced through Amazon Mechanical Turk (MTurk), thereby discovering a\nsignificant level of inattentiveness amongst the platform's top crowd workers\n(those classified as 'Master', with an 'Approval Rate' of 98% or more, and a\n'Number of HITS approved' value of 1,000 or more). A total of 564 individuals\nfrom the United States participated in our experiment. They were asked to read\na vignette outlining one of four hypothetical technology products and then\ncomplete a related survey. Three forms of attention check (logic, honesty, and\ntime) were used to assess attentiveness. Through this experiment we determined\nthat a total of 126 (22.3%) participants failed at least one of the three forms\nof attention check, with most (94) failing the honesty check - followed by the\nlogic check (31), and the time check (27). Thus, we established that\nsignificant levels of inattentiveness exist even among the most elite MTurk\nworkers. The study concludes by reaffirming the need for multiple forms of\ncarefully crafted attention checks, irrespective of whether participant quality\nis presumed to be high according to MTurk criteria such as 'Master', 'Approval\nRate', and 'Number of HITS approved'. Furthermore, we propose that researchers\nadjust their proposals to account for the effort and costs required to address\nparticipant inattentiveness.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 13:07:33 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 19:53:26 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 21:06:32 GMT"}, {"version": "v4", "created": "Thu, 10 Jun 2021 04:01:00 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Saravanos", "Antonios", ""], ["Zervoudakis", "Stavros", ""], ["Zheng", "Dongnanzi", ""], ["Stott", "Neil", ""], ["Hawryluk", "Bohdan", ""], ["Delfino", "Donatella", ""]]}, {"id": "2101.04658", "submitter": "Michael S. Crouch", "authors": "Mingde Zheng, Michael S. Crouch, Michael S. Eggleston", "title": "Surface Electromyography as a Natural Human-Machine Interface: A Review", "comments": "30 pages, 11 figures, preprint v2: Update author name, add references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Surface electromyography (sEMG) is a non-invasive method of measuring\nneuromuscular potentials generated when the brain instructs the body to perform\nboth fine and coarse locomotion. This technique has seen extensive\ninvestigation over the last two decades, with significant advances in both the\nhardware and signal processing methods used to collect and analyze sEMG\nsignals. While early work focused mainly on medical applications, there has\nbeen growing interest in utilizing sEMG as a sensing modality to enable\nnext-generation, high-bandwidth, and natural human-machine interfaces. In the\nfirst part of this review, we briefly overview the human skeletomuscular\nphysiology that gives rise to sEMG signals followed by a review of developments\nin sEMG acquisition hardware. Special attention is paid towards the fidelity of\nthese devices as well as form factor, as recent advances have pushed the limits\nof user comfort and high-bandwidth acquisition. In the second half of the\narticle, we explore work quantifying the information content of natural human\ngestures and then review the various signal processing and machine learning\nmethods developed to extract information in sEMG signals. Finally, we discuss\nthe future outlook in this field, highlighting the key gaps in current methods\nto enable seamless natural interactions between humans and machines.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 18:38:14 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 15:05:39 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Zheng", "Mingde", ""], ["Crouch", "Michael S.", ""], ["Eggleston", "Michael S.", ""]]}, {"id": "2101.04719", "submitter": "Upol Ehsan", "authors": "Upol Ehsan, Q. Vera Liao, Michael Muller, Mark O. Riedl, Justin D.\n  Weisz", "title": "Expanding Explainability: Towards Social Transparency in AI systems", "comments": "Accepted to CHI2021", "journal-ref": null, "doi": "10.1145/3411764.3445188", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As AI-powered systems increasingly mediate consequential decision-making,\ntheir explainability is critical for end-users to take informed and accountable\nactions. Explanations in human-human interactions are socially-situated. AI\nsystems are often socio-organizationally embedded. However, Explainable AI\n(XAI) approaches have been predominantly algorithm-centered. We take a\ndevelopmental step towards socially-situated XAI by introducing and exploring\nSocial Transparency (ST), a sociotechnically informed perspective that\nincorporates the socio-organizational context into explaining AI-mediated\ndecision-making. To explore ST conceptually, we conducted interviews with 29 AI\nusers and practitioners grounded in a speculative design scenario. We suggested\nconstitutive design elements of ST and developed a conceptual framework to\nunpack ST's effect and implications at the technical, decision-making, and\norganizational level. The framework showcases how ST can potentially calibrate\ntrust in AI, improve decision-making, facilitate organizational collective\nactions, and cultivate holistic explainability. Our work contributes to the\ndiscourse of Human-Centered XAI by expanding the design space of XAI.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:44:27 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Ehsan", "Upol", ""], ["Liao", "Q. Vera", ""], ["Muller", "Michael", ""], ["Riedl", "Mark O.", ""], ["Weisz", "Justin D.", ""]]}, {"id": "2101.04743", "submitter": "Yixuan Zhang", "authors": "Yixuan Zhang, Yifan Sun, Lace Padilla, Sumit Barua, Enrico Bertini,\n  Andrea G. Parker", "title": "Mapping the Landscape of COVID-19 Crisis Visualizations", "comments": "23 pages", "journal-ref": "CHI Conference on Human Factors in Computing Systems 2021", "doi": "10.1145/3411764.3445381", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response to COVID-19, a vast number of visualizations have been created to\ncommunicate information to the public. Information exposure in a public health\ncrisis can impact people's attitudes towards and responses to the crisis and\nrisks, and ultimately the trajectory of a pandemic. As such, there is a need\nfor work that documents, organizes, and investigates what COVID-19\nvisualizations have been presented to the public. We address this gap through\nan analysis of 668 COVID-19 visualizations. We present our findings through a\nconceptual framework derived from our analysis, that examines who, (uses) what\ndata, (to communicate) what messages, in what form, under what circumstances in\nthe context of COVID-19 crisis visualizations. We provide a set of factors to\nbe considered within each component of the framework. We conclude with\ndirections for future crisis visualization research.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 20:31:44 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zhang", "Yixuan", ""], ["Sun", "Yifan", ""], ["Padilla", "Lace", ""], ["Barua", "Sumit", ""], ["Bertini", "Enrico", ""], ["Parker", "Andrea G.", ""]]}, {"id": "2101.04794", "submitter": "Yue You", "authors": "Yue You, Yubo Kou, Xianghua Ding, Xinning Gui", "title": "The Medical Authority of AI: A Study of AI-enabled Consumer-facing\n  Health Technology", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.33097.98403", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, consumer-facing health technologies such as Artificial Intelligence\n(AI)-based symptom checkers (AISCs) have sprung up in everyday healthcare\npractice. AISCs solicit symptom information from users and provide medical\nsuggestions and possible diagnoses, a responsibility that people usually\nentrust with real-person authorities such as physicians and expert patients.\nThus, the advent of AISCs begs a question of whether and how they transform the\nnotion of medical authority in everyday healthcare practice. To answer this\nquestion, we conducted an interview study with thirty AISC users. We found that\nusers assess the medical authority of AISCs using various factors including\nautomated decisions and interaction design patterns of AISC apps, associations\nwith established medical authorities like hospitals, and comparisons with other\nhealth technologies. We reveal how AISCs are used in healthcare delivery,\ndiscuss how AI transforms conventional understandings of medical authority, and\nderive implications for designing AI-enabled health technology.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:02:40 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["You", "Yue", ""], ["Kou", "Yubo", ""], ["Ding", "Xianghua", ""], ["Gui", "Xinning", ""]]}, {"id": "2101.04796", "submitter": "Yue You", "authors": "Yue You, Xinning Gui", "title": "Self-Diagnosis through AI-enabled Chatbot-based Symptom Checkers: User\n  Experiences and Design Considerations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, there has been a growing interest in developing AI-enabled\nchatbot-based symptom checker (CSC) apps in the healthcare market. CSC apps\nprovide potential diagnoses for users and assist them with self-triaging based\non Artificial Intelligence (AI) techniques using human-like conversations.\nDespite the popularity of such CSC apps, little research has been done to\ninvestigate their functionalities and user experiences. To do so, we conducted\na feature review, a user review analysis, and an interview study. We found that\nthe existing CSC apps lack the functions to support the whole diagnostic\nprocess of an offline medical visit. We also found that users perceive the\ncurrent CSC apps to lack support for a comprehensive medical history, flexible\nsymptom input, comprehensible questions, and diverse diseases and user groups.\nBased on these results, we derived implications for the future features and\nconversational design of CSC apps.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 23:07:29 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["You", "Yue", ""], ["Gui", "Xinning", ""]]}, {"id": "2101.04834", "submitter": "Doris Xin", "authors": "Doris Xin, Eva Yiwei Wu, Doris Jung-Lin Lee, Niloufar Salehi, Aditya\n  Parameswaran", "title": "Whither AutoML? Understanding the Role of Automation in Machine Learning\n  Workflows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efforts to make machine learning more widely accessible have led to a rapid\nincrease in Auto-ML tools that aim to automate the process of training and\ndeploying machine learning. To understand how Auto-ML tools are used in\npractice today, we performed a qualitative study with participants ranging from\nnovice hobbyists to industry researchers who use Auto-ML tools. We present\ninsights into the benefits and deficiencies of existing tools, as well as the\nrespective roles of the human and automation in ML workflows. Finally, we\ndiscuss design implications for the future of Auto-ML tool development. We\nargue that instead of full automation being the ultimate goal of Auto-ML,\ndesigners of these tools should focus on supporting a partnership between the\nuser and the Auto-ML tool. This means that a range of Auto-ML tools will need\nto be developed to support varying user goals such as simplicity,\nreproducibility, and reliability.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 02:12:46 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Xin", "Doris", ""], ["Wu", "Eva Yiwei", ""], ["Lee", "Doris Jung-Lin", ""], ["Salehi", "Niloufar", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "2101.04843", "submitter": "Arunesh Mathur", "authors": "Arunesh Mathur, Jonathan Mayer, Mihir Kshirsagar", "title": "What Makes a Dark Pattern... Dark? Design Attributes, Normative\n  Considerations, and Measurement Methods", "comments": "27 pages, 4 figures", "journal-ref": "Proceedings of the 2021 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3411764.3445610", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There is a rapidly growing literature on dark patterns, user interface\ndesigns -- typically related to shopping or privacy -- that researchers deem\nproblematic. Recent work has been predominantly descriptive, documenting and\ncategorizing objectionable user interfaces. These contributions have been\ninvaluable in highlighting specific designs for researchers and policymakers.\nBut the current literature lacks a conceptual foundation: What makes a user\ninterface a dark pattern? Why are certain designs problematic for users or\nsociety?\n  We review recent work on dark patterns and demonstrate that the literature\ndoes not reflect a singular concern or consistent definition, but rather, a set\nof thematically related considerations. Drawing from scholarship in psychology,\neconomics, ethics, philosophy, and law, we articulate a set of normative\nperspectives for analyzing dark patterns and their effects on individuals and\nsociety. We then show how future research on dark patterns can go beyond\nsubjective criticism of user interface designs and apply empirical methods\ngrounded in normative perspectives.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 02:52:12 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Mathur", "Arunesh", ""], ["Mayer", "Jonathan", ""], ["Kshirsagar", "Mihir", ""]]}, {"id": "2101.04876", "submitter": "Aang Subiyakto", "authors": "Aang Subiyakto, Rohadatul Aisy, Bernadus Gunawan Sudarsono, Manorang\n  Sihotang, Didik Setiyadi, Asrul Sani", "title": "Empirical Evaluation of User Experience Using Lean Product and Process\n  Development: A Public Institution Case Study in Indonesia", "comments": "6 pages, 2 figures, The 2nd International Science and Mathematics\n  Conference (SMIC 2020) : Transforming Research and Education of Science and\n  Mathematics in the Digital Age", "journal-ref": null, "doi": "10.1063/5.0041676", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The easiness, speed, accuracy, security are the four main indicators of\ninformation quality that may be of concern to people for accessing information\nin a public institution dashboard system. To find out whether the data\ndisplayed on the system is easily understood by the users, an empirical\nevaluation study may indispensable to be performed. Evaluating user experience\nis one way to know whether a system is in accordance with user needs or not,\nwhether related to data, interfaces or system performance. This study assessed\nuser experiences based on the product pyramid and its development process in\nthe case of a public dashboard system in a public institution in Indonesia. The\nempirical experiment was conducted with about 15 participants using a scenario\nthat were deliberately designed and then gave answers to the single ease\nquestion (SEQ) and the system usability scale (SUS). The results show that SEQ\nmeasurement results obtained value of almost 5.7 which indicates that the\napplication measurement results have an easy level. The SUS results demonstrate\nthat the system was in the acceptable category with the value of around 81.3.\nAlthough findings of the study might not contribute theoretically, the findings\nmay demonstrate a practical consideration for the stakeholders, in terms of the\nnext system development stage.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 04:46:58 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Subiyakto", "Aang", ""], ["Aisy", "Rohadatul", ""], ["Sudarsono", "Bernadus Gunawan", ""], ["Sihotang", "Manorang", ""], ["Setiyadi", "Didik", ""], ["Sani", "Asrul", ""]]}, {"id": "2101.04880", "submitter": "Aang Subiyakto", "authors": "Aang Subiyakto, Yuliza Rahmi, Nia Kumaladewi, M. Qomarul Huda, Nidaul\n  Hasanati, Tri Haryanto", "title": "Investigating Quality of Institutional Repository Website Design Using\n  Usability Testing Framework", "comments": "7 pages, The 2nd International Science and Mathematics Conference\n  (SMIC 2020) : Transforming Research and Education of Science and Mathematics\n  in the Digital Age", "journal-ref": null, "doi": "10.1063/5.0041677", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Quality of website design is one of the influential factors of website\nsuccess. How the design helps the users using effectively and efficiently\nwebsite and satisfied at the end of the use. However, it is a common tendency\nthat websites are designed based on the developer's perspectives and lack\nconsidering user importance. Thus, the degree of website usability tends to be\nlow according to user perceptions. This study purposed to understand the user\nexperiences using an institutional repository (IR) website in a public\nuniversity in Indonesia. The research was performed based on usability testing\nframework as the usability testing method. About 12 participants were purposely\ninvolved concerning their key informant characteristics. Following three\nempirical data collection techniques (i.e., query technique, formal experiment,\nand thinking aloud), both descriptive analysis using usability scale matric and\ncontent analysis using qualitative data analysis (QDA) Miner Lite software were\nused in the data analysis stage. Lastly, several visual design recommendations\nwere then proposed at the end of the study. In terms of a case study, besides\nthe practical recommendations which may contextually useful for the next\nwebsite development; the clarity of the research design may also help scholars\nhow to combine more than one usability testing technique within a\nmulti-technique study design.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:04:36 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Subiyakto", "Aang", ""], ["Rahmi", "Yuliza", ""], ["Kumaladewi", "Nia", ""], ["Huda", "M. Qomarul", ""], ["Hasanati", "Nidaul", ""], ["Haryanto", "Tri", ""]]}, {"id": "2101.04893", "submitter": "Xiaoyi Zhang", "authors": "Xiaoyi Zhang, Lilian de Greef, Amanda Swearngin, Samuel White, Kyle\n  Murray, Lisa Yu, Qi Shan, Jeffrey Nichols, Jason Wu, Chris Fleizach, Aaron\n  Everitt, Jeffrey P. Bigham", "title": "Screen Recognition: Creating Accessibility Metadata for Mobile\n  Applications from Pixels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many accessibility features available on mobile platforms require\napplications (apps) to provide complete and accurate metadata describing user\ninterface (UI) components. Unfortunately, many apps do not provide sufficient\nmetadata for accessibility features to work as expected. In this paper, we\nexplore inferring accessibility metadata for mobile apps from their pixels, as\nthe visual interfaces often best reflect an app's full functionality. We\ntrained a robust, fast, memory-efficient, on-device model to detect UI elements\nusing a dataset of 77,637 screens (from 4,068 iPhone apps) that we collected\nand annotated. To further improve UI detections and add semantic information,\nwe introduced heuristics (e.g., UI grouping and ordering) and additional models\n(e.g., recognize UI content, state, interactivity). We built Screen Recognition\nto generate accessibility metadata to augment iOS VoiceOver. In a study with 9\nscreen reader users, we validated that our approach improves the accessibility\nof existing mobile apps, enabling even previously inaccessible apps to be used.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 05:56:15 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Zhang", "Xiaoyi", ""], ["de Greef", "Lilian", ""], ["Swearngin", "Amanda", ""], ["White", "Samuel", ""], ["Murray", "Kyle", ""], ["Yu", "Lisa", ""], ["Shan", "Qi", ""], ["Nichols", "Jeffrey", ""], ["Wu", "Jason", ""], ["Fleizach", "Chris", ""], ["Everitt", "Aaron", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "2101.04913", "submitter": "Daniel Diethei", "authors": "Daniel Diethei, Jasmin Niess, Carolin Stellmacher, Evropi Stefanidi,\n  Johannes Sch\\\"oning", "title": "Sharing Heartbeats: Motivations of Citizen Scientists in Times of Crises", "comments": null, "journal-ref": "In CHI Conference on Human Factors in Computing Systems (CHI 21),\n  May 8-13, 2021, Yokohama, Japan. ACM, New York, NY, USA, 16 pages", "doi": "10.1145/3411764.3445665", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of COVID-19 cases globally, many countries released digital\ntools to mitigate the effects of the pandemic. In Germany the Robert Koch\nInstitute (RKI) published the Corona-Data-Donation-App, a virtual citizen\nscience (VCS) project, to establish an early warning system for the prediction\nof potential COVID-19 hotspots using data from wearable devices. While work on\nmotivation for VCS projects in HCI often presents egoistic motives as\nprevailing, there is little research on such motives in crises situations. In\nthis paper, we explore the socio-psychological processes and motivations to\nshare personal data during a pandemic. Our findings indicate that collective\nmotives dominated among app reviews (n=464) and in in-depth interviews (n=10).\nWe contribute implications for future VCS tools in times of crises that\nhighlight the importance of communication, transparency and responsibility.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 07:30:32 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 15:05:38 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Diethei", "Daniel", ""], ["Niess", "Jasmin", ""], ["Stellmacher", "Carolin", ""], ["Stefanidi", "Evropi", ""], ["Sch\u00f6ning", "Johannes", ""]]}, {"id": "2101.04922", "submitter": "Mingyu Derek Ma", "authors": "Mingyu Derek Ma, Jiao Sun, Mu Yang, Kung-Hsiang Huang, Nuan Wen,\n  Shikhar Singh, Rujun Han and Nanyun Peng", "title": "EventPlus: A Temporal Event Understanding Pipeline", "comments": "To appear at NAACL 2021 (Demonstrations)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present EventPlus, a temporal event understanding pipeline that integrates\nvarious state-of-the-art event understanding components including event trigger\nand type detection, event argument detection, event duration and temporal\nrelation extraction. Event information, especially event temporal knowledge, is\na type of common sense knowledge that helps people understand how stories\nevolve and provides predictive hints for future events. EventPlus as the first\ncomprehensive temporal event understanding pipeline provides a convenient tool\nfor users to quickly obtain annotations about events and their temporal\ninformation for any user-provided document. Furthermore, we show EventPlus can\nbe easily adapted to other domains (e.g., biomedical domain). We make EventPlus\npublicly available to facilitate event-related information extraction and\ndownstream applications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:00:50 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 21:33:23 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ma", "Mingyu Derek", ""], ["Sun", "Jiao", ""], ["Yang", "Mu", ""], ["Huang", "Kung-Hsiang", ""], ["Wen", "Nuan", ""], ["Singh", "Shikhar", ""], ["Han", "Rujun", ""], ["Peng", "Nanyun", ""]]}, {"id": "2101.04954", "submitter": "Dazhen Deng", "authors": "Dazhen Deng, Jiang Wu, Jiachen Wang, Yihong Wu, Xiao Xie, Zheng Zhou,\n  Hui Zhang, Xiaolong Zhang, Yingcai Wu", "title": "EventAnchor: Reducing Human Interactions in Event Annotation of Racket\n  Sports Videos", "comments": null, "journal-ref": "Proceedings of the 2021 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/3411764.3445431", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The popularity of racket sports (e.g., tennis and table tennis) leads to high\ndemands for data analysis, such as notational analysis, on player performance.\nWhile sports videos offer many benefits for such analysis, retrieving accurate\ninformation from sports videos could be challenging. In this paper, we propose\nEventAnchor, a data analysis framework to facilitate interactive annotation of\nracket sports video with the support of computer vision algorithms. Our\napproach uses machine learning models in computer vision to help users acquire\nessential events from videos (e.g., serve, the ball bouncing on the court) and\noffers users a set of interactive tools for data annotation. An evaluation\nstudy on a table tennis annotation system built on this framework shows\nsignificant improvement of user performances in simple annotation tasks on\nobjects of interest and complex annotation tasks requiring domain knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 09:32:05 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 03:10:54 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Deng", "Dazhen", ""], ["Wu", "Jiang", ""], ["Wang", "Jiachen", ""], ["Wu", "Yihong", ""], ["Xie", "Xiao", ""], ["Zhou", "Zheng", ""], ["Zhang", "Hui", ""], ["Zhang", "Xiaolong", ""], ["Wu", "Yingcai", ""]]}, {"id": "2101.05004", "submitter": "Lina Rojas-Barahona", "authors": "Lina M. Rojas-Barahona", "title": "Is the User Enjoying the Conversation? A Case Study on the Impact on the\n  Reward Function", "comments": "Accepted at the Human in the Loop Dialogue Systems, 34st Conference\n  on Neural Information Processing Systems (NeurIPS 2020). Paper updated with\n  minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of user satisfaction in policy learning task-oriented dialogue\nsystems has long been a subject of research interest. Most current models for\nestimating the user satisfaction either (i) treat out-of-context short-texts,\nsuch as product reviews, or (ii) rely on turn features instead of on\ndistributed semantic representations. In this work we adopt deep neural\nnetworks that use distributed semantic representation learning for estimating\nthe user satisfaction in conversations. We evaluate the impact of modelling\ncontext length in these networks. Moreover, we show that the proposed\nhierarchical network outperforms state-of-the-art quality estimators.\nFurthermore, we show that applying these networks to infer the reward function\nin a Partial Observable Markov Decision Process (POMDP) yields to a great\nimprovement in the task success rate.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 11:13:07 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Rojas-Barahona", "Lina M.", ""]]}, {"id": "2101.05028", "submitter": "Giulia Perugia Dr.", "authors": "Giulia Perugia, Maike Paetzel-Pr\\\"usmann, Madelene Alanenp\\\"a\\\"a, and\n  Ginevra Castellano", "title": "I Can See it in Your Eyes: Gaze as an Implicit Cue of Uncanniness and\n  Task Performance in Repeated Interactions", "comments": "29 pages, 7 figures plus Appendix. This work has been submitted to\n  Frontiers in Robotics and AI (Human-Robot Interaction) and is currently under\n  review", "journal-ref": "Frontiers in Robotics and AI, 8, 78 (2021)", "doi": "10.3389/frobt.2021.645956", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past years, extensive research has been dedicated to developing\nrobust platforms and data-driven dialog models to support long-term human-robot\ninteractions. However, little is known about how people's perception of robots\nand engagement with them develop over time and how these can be accurately\nassessed through implicit and continuous measurement techniques. In this paper,\nwe explore this by involving participants in three interaction sessions with\nmultiple days of zero exposure in between. Each session consists of a joint\ntask with a robot as well as two short social chats with it before and after\nthe task. We measure participants' gaze patterns with a wearable eye-tracker\nand gauge their perception of the robot and engagement with it and the joint\ntask using questionnaires. Results disclose that aversion of gaze in a social\nchat is an indicator of a robot's uncanniness and that the more people gaze at\nthe robot in a joint task, the worse they perform. In contrast with most HRI\nliterature, our results show that gaze towards an object of shared attention,\nrather than gaze towards a robotic partner, is the most meaningful predictor of\nengagement in a joint task. Furthermore, the analyses of gaze patterns in\nrepeated interactions disclose that people's mutual gaze in a social chat\ndevelops congruently with their perceptions of the robot over time. These are\nkey findings for the HRI community as they entail that gaze behavior can be\nused as an implicit measure of people's perception of robots in a social chat\nand of their engagement and task performance in a joint task.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 12:20:12 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 13:52:04 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Perugia", "Giulia", ""], ["Paetzel-Pr\u00fcsmann", "Maike", ""], ["Alanenp\u00e4\u00e4", "Madelene", ""], ["Castellano", "Ginevra", ""]]}, {"id": "2101.05244", "submitter": "Shota Yamanaka Dr.", "authors": "Shota Yamanaka and Hiroki Usuba", "title": "Computing Touch-Point Ambiguity on Mobile Touchscreens for Modeling\n  Target Selection Times", "comments": "Revision based on MobileHCI 2020 reviews", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finger-Fitts law (FFitts law) is a model to predict touch-pointing times\nmodified from Fitts' law. It considers the absolute touch-point precision, or a\nfinger tremor factor sigma_a, to decrease the admissible target area and thus\nincrease the task difficulty. Among choices such as running an independent task\nor performing parameter optimization, there is no consensus on the best\nmethodology to measure sigma_a. This inconsistency could be harmful to HCI\nstudies such as evaluating pointing techniques and comparing user groups. By\nintegrating the results of our 1D and 2D touch-pointing experiments and\nreanalyses of previous studies' data, we examined the advantages and\ndisadvantages of each approach to compute sigma_a, and we found that using the\nparameter optimization method has overall the best prediction performance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:11:36 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 09:55:05 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yamanaka", "Shota", ""], ["Usuba", "Hiroki", ""]]}, {"id": "2101.05258", "submitter": "Jialun Aaron Jiang", "authors": "Jialun Aaron Jiang, Charles Kiene, Skyler Middler, Jed R. Brubaker,\n  Casey Fiesler", "title": "Moderation Challenges in Voice-based Online Communities on Discord", "comments": "23 pages", "journal-ref": "Proc. ACM Hum.-Comput. Interact.3, CSCW, Article 55 (November\n  2019), 23 pages", "doi": "10.1145/3359157", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Online community moderators are on the front lines of combating problems like\nhate speech and harassment, but new modes of interaction can introduce\nunexpected challenges. In this paper, we consider moderation practices and\nchallenges in the context of real-time, voice-based communication through 25\nin-depth interviews with moderators on Discord. Our findings suggest that the\naffordances of voice-based online communities change what it means to moderate\ncontent and interactions. Not only are there new ways to break rules that\nmoderators of text-based communities find unfamiliar, such as disruptive noise\nand voice raiding, but acquiring evidence of rule-breaking behaviors is also\nmore difficult due to the ephemerality of real-time voice. While moderators\nhave developed new moderation strategies, these strategies are limited and\noften based on hearsay and first impressions, resulting in problems ranging\nfrom unsuccessful moderation to false accusations. Based on these findings, we\ndiscuss how voice communication complicates current understandings and\nassumptions about moderation, and outline ways that platform designers and\nadministrators can design technology to facilitate moderation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 18:43:22 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Jiang", "Jialun Aaron", ""], ["Kiene", "Charles", ""], ["Middler", "Skyler", ""], ["Brubaker", "Jed R.", ""], ["Fiesler", "Casey", ""]]}, {"id": "2101.05272", "submitter": "Lisa-Marie Vortmann", "authors": "Lisa-Marie Vortmann, Leonid Schwenke, Felix Putze", "title": "Real or Virtual? Using Brain Activity Patterns to differentiate Attended\n  Targets during Augmented Reality Scenarios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Augmented Reality is the fusion of virtual components and our real\nsurroundings. The simultaneous visibility of generated and natural objects\noften requires users to direct their selective attention to a specific target\nthat is either real or virtual. In this study, we investigated whether this\ntarget is real or virtual by using machine learning techniques to classify\nelectroencephalographic (EEG) data collected in Augmented Reality scenarios. A\nshallow convolutional neural net classified 3 second data windows from 20\nparticipants in a person-dependent manner with an average accuracy above 70\\%\nif the testing data and training data came from different trials.\nPerson-independent classification was possible above chance level for 6 out of\n20 participants. Thus, the reliability of such a Brain-Computer Interface is\nhigh enough for it to be treated as a useful input mechanism for Augmented\nReality applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2021 19:08:39 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Vortmann", "Lisa-Marie", ""], ["Schwenke", "Leonid", ""], ["Putze", "Felix", ""]]}, {"id": "2101.05273", "submitter": "Dakuo Wang", "authors": "Dakuo Wang, Josh Andres, Justin Weisz, Erick Oduor, Casey Dugan", "title": "AutoDS: Towards Human-Centered Automation of Data Science", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445526", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science (DS) projects often follow a lifecycle that consists of\nlaborious tasks for data scientists and domain experts (e.g., data exploration,\nmodel training, etc.). Only till recently, machine learning(ML) researchers\nhave developed promising automation techniques to aid data workers in these\ntasks. This paper introduces AutoDS, an automated machine learning (AutoML)\nsystem that aims to leverage the latest ML automation techniques to support\ndata science projects. Data workers only need to upload their dataset, then the\nsystem can automatically suggest ML configurations, preprocess data, select\nalgorithm, and train the model. These suggestions are presented to the user via\na web-based graphical user interface and a notebook-based programming user\ninterface.\n  We studied AutoDS with 30 professional data scientists, where one group used\nAutoDS, and the other did not, to complete a data science project. As expected,\nAutoDS improves productivity; Yet surprisingly, we find that the models\nproduced by the AutoDS group have higher quality and less errors, but lower\nhuman confidence scores. We reflect on the findings by presenting design\nimplications for incorporating automation techniques into human work in the\ndata science lifecycle.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 08:35:14 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Wang", "Dakuo", ""], ["Andres", "Josh", ""], ["Weisz", "Justin", ""], ["Oduor", "Erick", ""], ["Dugan", "Casey", ""]]}, {"id": "2101.05300", "submitter": "David Shamma", "authors": "Julie Williamson, Jie Li, Vinoba Vinayagamoorthy, David A. Shamma,\n  Pablo Cesar", "title": "Proxemics and Social Interactions in an Instrumented Virtual Reality\n  Workshop", "comments": "20 pages, 9 figures, ACM CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445729", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual environments (VEs) can create collaborative and social spaces, which\nare increasingly important in the face of remote work and travel reduction.\nRecent advances, such as more open and widely available platforms, create new\npossibilities to observe and analyse interaction in VEs. Using a custom\ninstrumented build of Mozilla Hubs to measure position and orientation, we\nconducted an academic workshop to facilitate a range of typical workshop\nactivities. We analysed social interactions during a keynote, small group\nbreakouts, and informal networking/hallway conversations. Our mixed-methods\napproach combined environment logging, observations, and semi-structured\ninterviews. The results demonstrate how small and large spaces influenced group\nformation, shared attention, and personal space, where smaller rooms\nfacilitated more cohesive groups while larger rooms made small group formation\nchallenging but personal space more flexible. Beyond our findings, we show how\nthe combination of data and insights can fuel collaborative spaces' design and\ndeliver more effective virtual workshops.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:00:57 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Williamson", "Julie", ""], ["Li", "Jie", ""], ["Vinayagamoorthy", "Vinoba", ""], ["Shamma", "David A.", ""], ["Cesar", "Pablo", ""]]}, {"id": "2101.05303", "submitter": "Han Liu", "authors": "Han Liu, Vivian Lai, Chenhao Tan", "title": "Understanding the Effect of Out-of-distribution Examples and Interactive\n  Explanations on Human-AI Decision Making", "comments": "43 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although AI holds promise for improving human decision making in societally\ncritical domains, it remains an open question how human-AI teams can reliably\noutperform AI alone and human alone in challenging prediction tasks (also known\nas complementary performance). We explore two directions to understand the gaps\nin achieving complementary performance. First, we argue that the typical\nexperimental setup limits the potential of human-AI teams. To account for lower\nAI performance out-of-distribution than in-distribution because of distribution\nshift, we design experiments with different distribution types and investigate\nhuman performance for both in-distribution and out-of-distribution examples.\nSecond, we develop novel interfaces to support interactive explanations so that\nhumans can actively engage with AI assistance. Using virtual pilot studies and\nlarge-scale randomized experiments across three tasks, we demonstrate a clear\ndifference between in-distribution and out-of-distribution, and observe mixed\nresults for interactive explanations: while interactive explanations improve\nhuman perception of AI assistance's usefulness, they may reinforce human biases\nand lead to limited performance improvement. Overall, our work points out\ncritical challenges and future directions towards enhancing human performance\nwith AI assistance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 19:01:32 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 19:02:32 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 18:00:05 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Liu", "Han", ""], ["Lai", "Vivian", ""], ["Tan", "Chenhao", ""]]}, {"id": "2101.05385", "submitter": "Garrick Cabour", "authors": "Garrick Cabour, \\'Elise Ledoux and Samuel Bassetto", "title": "A Work-Centered Approach for Cyber-Physical-Social System Design:\n  Applications in Aerospace Industrial Inspection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Industrial inspection automation in aerospace presents numerous challenges\ndue to the dynamic, information-rich and regulated aspects of the domain. To\ndiagnose the condition of an aircraft component, expert inspectors rely on a\nsignificant amount of procedural and tacit knowledge (know-how). As systems\ncapabilities do not match high level human cognitive functions, the role of\nhumans in future automated work systems will remain important. A\nCyber-Physical-Social System (CPSS) is a suitable solution that envisions\nhumans and agents in a joint activity to enhance cognitive/computational\ncapabilities and produce better outcomes. This paper investigates how a\nwork-centred approach can support and guide the engineering process of a CPSS\nwith an industrial use case. We present a robust methodology that combines\nfieldwork inquiries and model-based engineering to elicit and formalize rich\nmental models into exploitable design patterns. Our results exhibit how\ninspectors process and apply knowledge to diagnose the component`s condition,\nhow they deal with the institution`s rules and operational constraints (norms,\nsafety policies, standard operating procedures). We suggest how these patterns\ncan be incorporated in software modules or can conceptualize Human-Agent\nTeaming requirements. We argue that this framework can corroborate the right\nfit between a system`s technical and ecological validity (system fit with\noperating context) that enhances data reliability, productivity-related factors\nand system acceptance by end-users.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2021 23:11:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Cabour", "Garrick", ""], ["Ledoux", "\u00c9lise", ""], ["Bassetto", "Samuel", ""]]}, {"id": "2101.05450", "submitter": "Shuhan Wei", "authors": "Xianghua Ding, Shuhan Wei, Xinning Gui, Ning Gu, and Peng Zhang", "title": "Data Engagement Reconsidered: A Study of Automatic Stress Tracking\n  Technology in Use", "comments": "13 pages, 2 figures, 1 table, Accepted at ACM 2021 CHI Conference on\n  Human Factors in Computing Systems (CHI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In today's fast-paced world, stress has become a growing health concern.\nWhile more automatic stress tracking technologies have recently become\navailable on wearable or mobile devices, there is still a limited understanding\nof how they are actually used in everyday life. This paper presents an\nempirical study of automatic stress-tracking technologies in use in China,\nbased on semi-structured interviews with 17 users. The study highlights three\nchallenges of stress-tracking data engagement that prevent effective technology\nusage: the lack of immediate awareness, the lack of pre-required knowledge, and\nthe lack of corresponding communal support. Drawing on the stress-tracking\npractices uncovered in the study, we bring these issues to the fore, and unpack\nassumptions embedded in related works on self-tracking and how data engagement\nis approached. We end by calling for a reconsideration of data engagement as\npart of self-tracking practices with technologies rather than simply looking at\nthe user interface.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 04:22:50 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Ding", "Xianghua", ""], ["Wei", "Shuhan", ""], ["Gui", "Xinning", ""], ["Gu", "Ning", ""], ["Zhang", "Peng", ""]]}, {"id": "2101.05507", "submitter": "Paul Knott PhD MPhys BSc", "authors": "Paul Knott, Micah Carroll, Sam Devlin, Kamil Ciosek, Katja Hofmann, A.\n  D. Dragan and Rohin Shah", "title": "Evaluating the Robustness of Collaborative Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for agents trained by deep reinforcement learning to work alongside\nhumans in realistic settings, we will need to ensure that the agents are\n\\emph{robust}. Since the real world is very diverse, and human behavior often\nchanges in response to agent deployment, the agent will likely encounter novel\nsituations that have never been seen during training. This results in an\nevaluation challenge: if we cannot rely on the average training or validation\nreward as a metric, then how can we effectively evaluate robustness? We take\ninspiration from the practice of \\emph{unit testing} in software engineering.\nSpecifically, we suggest that when designing AI agents that collaborate with\nhumans, designers should search for potential edge cases in \\emph{possible\npartner behavior} and \\emph{possible states encountered}, and write tests which\ncheck that the behavior of the agent in these edge cases is reasonable. We\napply this methodology to build a suite of unit tests for the Overcooked-AI\nenvironment, and use this test suite to evaluate three proposals for improving\nrobustness. We find that the test suite provides significant insight into the\neffects of these proposals that were generally not revealed by looking solely\nat the average validation reward.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:02:45 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Knott", "Paul", ""], ["Carroll", "Micah", ""], ["Devlin", "Sam", ""], ["Ciosek", "Kamil", ""], ["Hofmann", "Katja", ""], ["Dragan", "A. D.", ""], ["Shah", "Rohin", ""]]}, {"id": "2101.05508", "submitter": "Pengyuan Zhou", "authors": "Pengyuan Zhou, Pranvera Kortoci, Yui-Pan Yau, Tristan Braud, Xiujun\n  Wang, Benjamin Finley, Lik-Hang Lee, Sasu Tarkoma, Jussi Kangasharju, Pan Hui", "title": "Augmented Informative Cooperative Perception", "comments": "Submitted to ICDCS'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Connected vehicles, whether equipped with advanced driver-assistance systems\nor fully autonomous, are currently constrained to visual information in their\nlines-of-sight. A cooperative perception system among vehicles increases their\nsituational awareness by extending their perception ranges. Existing solutions\nimply significant network and computation load, as well as high flow of\nnot-always-relevant data received by vehicles. To address such issues, and thus\naccount for the inherently diverse informativeness of the data, we present\nAugmented Informative Cooperative Perception (AICP) as the first fast-filtering\nsystem which optimizes the informativeness of shared data at vehicles. AICP\ndisplays the filtered data to the drivers in augmented reality head-up display.\nTo this end, an informativeness maximization problem is presented for vehicles\nto select a subset of data to display to their drivers. Specifically, we\npropose (i) a dedicated system design with custom data structure and\nlight-weight routing protocol for convenient data encapsulation, fast\ninterpretation and transmission, and (ii) a comprehensive problem formulation\nand efficient fitness-based sorting algorithm to select the most valuable data\nto display at the application layer. We implement a proof-of-concept prototype\nof AICP with a bandwidth-hungry, latency-constrained real-life augmented\nreality application. The prototype realizes the informative-optimized\ncooperative perception with only 12.6 milliseconds additional latency. Next, we\ntest the networking performance of AICP at scale and show that AICP effectively\nfilter out less relevant packets and decreases the channel busy time.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 09:04:16 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Zhou", "Pengyuan", ""], ["Kortoci", "Pranvera", ""], ["Yau", "Yui-Pan", ""], ["Braud", "Tristan", ""], ["Wang", "Xiujun", ""], ["Finley", "Benjamin", ""], ["Lee", "Lik-Hang", ""], ["Tarkoma", "Sasu", ""], ["Kangasharju", "Jussi", ""], ["Hui", "Pan", ""]]}, {"id": "2101.05703", "submitter": "David Miguel Gon\\c{c}alves", "authors": "David Gon\\c{c}alves, Andr\\'e Rodrigues, Mike L. Richardson, Alexandra\n  A. de Sousa, Michael J. Proulx, Tiago Guerreiro", "title": "Exploring Asymmetric Roles in Mixed-Ability Gaming", "comments": "21 pages, 1 figure. Manuscript submitted to ACM Conference on Human\n  Factors in Computing Systems (CHI 21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The landscape of digital games is segregated by player ability. For example,\nsighted players have a multitude of highly visual games at their disposal,\nwhile blind players may choose from a variety of audio games. Attempts at\nimproving cross-ability access to any of those are often limited in the\nexperience they provide, or disregard multiplayer experiences. We explore\nability-based asymmetric roles as a design approach to create engaging and\nchallenging mixed-ability play. Our team designed and developed two\ncollaborative testbed games exploring asymmetric interdependent roles. In a\nremote study with 13 mixed-visual-ability pairs we assessed how roles affected\nperceptions of engagement, competence, and autonomy, using a mixed-methods\napproach. The games provided an engaging and challenging experience, in which\ndifferences in visual ability were not limiting. Our results underline how\nexperiences unequal by design can give rise to an equitable joint experience.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:27:27 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Gon\u00e7alves", "David", ""], ["Rodrigues", "Andr\u00e9", ""], ["Richardson", "Mike L.", ""], ["de Sousa", "Alexandra A.", ""], ["Proulx", "Michael J.", ""], ["Guerreiro", "Tiago", ""]]}, {"id": "2101.05718", "submitter": "Luiz Rodrigues", "authors": "Luiz Rodrigues, Armando M. Toda, Wilk Oliveira, Paula T. Palomino,\n  Julita Vassileva, Seiji Isotani", "title": "Automating Gamification Personalization: To the User and Beyond", "comments": "Submitted to IEEE Transactions on Learning Technologies. 14 pages, 2\n  figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalized gamification explores knowledge about the users to tailor\ngamification designs to improve one-size-fits-all gamification. The tailoring\nprocess should simultaneously consider user and contextual characteristics\n(e.g., activity to be done and geographic location), which leads to several\noccasions to tailor. Consequently, tools for automating gamification\npersonalization are needed. The problems that emerge are that which of those\ncharacteristics are relevant and how to do such tailoring are open questions,\nand that the required automating tools are lacking. We tackled these problems\nin two steps. First, we conducted an exploratory study, collecting\nparticipants' opinions on the game elements they consider the most useful for\ndifferent learning activity types (LAT) via survey. Then, we modeled opinions\nthrough conditional decision trees to address the aforementioned tailoring\nprocess. Second, as a product from the first step, we implemented a recommender\nsystem that suggests personalized gamification designs (which game elements to\nuse), addressing the problem of automating gamification personalization. Our\nfindings i) present empirical evidence that LAT, geographic locations, and\nother user characteristics affect users' preferences, ii) enable defining\ngamification designs tailored to user and contextual features simultaneously,\nand iii) provide technological aid for those interested in designing\npersonalized gamification. The main implications are that demographics,\ngame-related characteristics, geographic location, and LAT to be done, as well\nas the interaction between different kinds of information (user and contextual\ncharacteristics), should be considered in defining gamification designs and\nthat personalizing gamification designs can be improved with aid from our\nrecommender system.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 16:47:00 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Rodrigues", "Luiz", ""], ["Toda", "Armando M.", ""], ["Oliveira", "Wilk", ""], ["Palomino", "Paula T.", ""], ["Vassileva", "Julita", ""], ["Isotani", "Seiji", ""]]}, {"id": "2101.05766", "submitter": "Truong An Pham", "authors": "Truong An Pham, Junjue Wang, Yu Xiao, Padmanabhan Pillai, Roger\n  Iyengar, Roberta Klatzky, Mahadev Satyanarayanan", "title": "Ajalon: Simplifying the Authoring of Wearable Cognitive Assistants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable Cognitive Assistance (WCA) amplifies human cognition in real time\nthrough a wearable device and low-latency wireless access to edge computing\ninfrastructure. It is inspired by, and broadens, the metaphor of GPS navigation\ntools that provide real-time step-by-step guidance, with prompt error detection\nand correction. WCA applications are likely to be transformative in education,\nhealth care, industrial troubleshooting, manufacturing, and many other areas.\nToday, WCA application development is difficult and slow, requiring skills in\nareas such as machine learning and computer vision that are not widespread\namong software developers. This paper describes Ajalon, an authoring toolchain\nfor WCA applications that reduces the skill and effort needed at each step of\nthe development pipeline. Our evaluation shows that Ajalon significantly\nreduces the effort needed to create new WCA applications.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 18:17:11 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Pham", "Truong An", ""], ["Wang", "Junjue", ""], ["Xiao", "Yu", ""], ["Pillai", "Padmanabhan", ""], ["Iyengar", "Roger", ""], ["Klatzky", "Roberta", ""], ["Satyanarayanan", "Mahadev", ""]]}, {"id": "2101.05840", "submitter": "Rhema Linder", "authors": "Oleg Bezrukavnikov and Rhema Linder", "title": "A Neophyte With AutoML: Evaluating the Promises of Automatic Machine\n  Learning Tools", "comments": "10 pages, 3 tables, 3 figures. First author is a high school senior", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses modern Auto Machine Learning (AutoML) tools from the\nperspective of a person with little prior experience in Machine Learning (ML).\nThere are many AutoML tools both ready-to-use and under development, which are\ncreated to simplify and democratize usage of ML technologies in everyday life.\nOur position is that ML should be easy to use and available to a greater number\nof people. Prior research has identified the need for intuitive AutoML tools.\nThis work seeks to understand how well AutoML tools have achieved that goal in\npractice. We evaluate three AutoML Tools to evaluate the end-user experience\nand system performance. We evaluate the tools by having them create models from\na competition dataset on banking data. We report on their performance and the\ndetails of our experience. This process provides a unique understanding of the\nstate of the art of AutoML tools. Finally, we use these experiences to inform a\ndiscussion on how future AutoML tools can improve the user experience for\nneophytes of Machine Learning.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2021 19:28:57 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Bezrukavnikov", "Oleg", ""], ["Linder", "Rhema", ""]]}, {"id": "2101.06030", "submitter": "Yunlong Wang", "authors": "Samuel Rhys Cox, Yunlong Wang, Ashraf Abdul, Christian von der Weth,\n  Brian Y. Lim", "title": "Directed Diversity: Leveraging Language Embedding Distances for\n  Collective Creativity in Crowd Ideation", "comments": "CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445782", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowdsourcing can collect many diverse ideas by prompting ideators\nindividually, but this can generate redundant ideas. Prior methods reduce\nredundancy by presenting peers' ideas or peer-proposed prompts, but these\nrequire much human coordination. We introduce Directed Diversity, an automatic\nprompt selection approach that leverages language model embedding distances to\nmaximize diversity. Ideators can be directed towards diverse prompts and away\nfrom prior ideas, thus improving their collective creativity. Since there are\ndiverse metrics of diversity, we present a Diversity Prompting Evaluation\nFramework consolidating metrics from several research disciplines to analyze\nalong the ideation chain - prompt selection, prompt creativity, prompt-ideation\nmediation, and ideation creativity. Using this framework, we evaluated Directed\nDiversity in a series of a simulation study and four user studies for the use\ncase of crowdsourcing motivational messages to encourage physical activity. We\nshow that automated diverse prompting can variously improve collective\ncreativity across many nuanced metrics of diversity.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 09:36:05 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Cox", "Samuel Rhys", ""], ["Wang", "Yunlong", ""], ["Abdul", "Ashraf", ""], ["von der Weth", "Christian", ""], ["Lim", "Brian Y.", ""]]}, {"id": "2101.06120", "submitter": "Wenge Xu", "authors": "Wenge Xu, Hai-Ning Liang, Kangyou Yu, Nilufar Baghaei", "title": "Effect of Gameplay Uncertainty, Display Type, and Age on Virtual Reality\n  Exergames", "comments": "Accepted to ACM 2021 CHI Conference on Human Factors in Computing\n  Systems (CHI 2021)", "journal-ref": "CHI 2021", "doi": "10.1145/3411764.3445801", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Uncertainty is widely acknowledged as an engaging gameplay element but rarely\nused in exergames. In this research, we explore the role of uncertainty in\nexergames and introduce three uncertain elements (false-attacks, misses, and\ncritical hits) to an exergame. We conducted a study under two conditions\n(uncertain and certain), with two display types (virtual reality and large\ndisplay) and across young and middle-aged adults to measure their effect on\ngame performance, experience, and exertion. Results show that (1) our designed\nuncertain elements are instrumental in increasing exertion levels; (2) when\nplaying a motion-based first-person perspective exergame, virtual reality can\nimprove performance, while maintaining the same motion sickness level as a\nlarge display; and (3) exergames for middle-aged adults should be designed with\nage-related declines in mind, similar to designing for elderly adults. We also\nframed two design guidelines for exergames that have similar features to the\ngame used in this research.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:03:50 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Xu", "Wenge", ""], ["Liang", "Hai-Ning", ""], ["Yu", "Kangyou", ""], ["Baghaei", "Nilufar", ""]]}, {"id": "2101.06133", "submitter": "Jurriaan van Diggelen", "authors": "Jurriaan van Diggelen, Wiard Jorritsma, Bob van der Vecht", "title": "Teaming up with information agents", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the intricacies involved in designing a computer as a teampartner, we\ncan observe patterns in team behavior which allow us to describe at a general\nlevel how AI systems are to collaborate with humans. Whereas most work on\nhuman-machine teaming has focused on physical agents (e.g. robotic systems),\nour aim is to study how humans can collaborate with information agents. We\npropose some appropriate team design patterns, and test them using our\nCollaborative Intelligence Analysis (CIA) tool.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:26:12 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["van Diggelen", "Jurriaan", ""], ["Jorritsma", "Wiard", ""], ["van der Vecht", "Bob", ""]]}, {"id": "2101.06143", "submitter": "Fritz Lekschas", "authors": "Fritz Lekschas, Spyridon Ampanavos, Pao Siangliulue, Hanspeter\n  Pfister, Krzysztof Z. Gajos", "title": "Ask Me or Tell Me? Enhancing the Effectiveness of Crowdsourced Design\n  Feedback", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445507", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowdsourced design feedback systems are emerging resources for getting large\namounts of feedback in a short period of time. Traditionally, the feedback\ncomes in the form of a declarative statement, which often contains positive or\nnegative sentiment. Prior research has shown that overly negative or positive\nsentiment can strongly influence the perceived usefulness and acceptance of\nfeedback and, subsequently, lead to ineffective design revisions. To enhance\nthe effectiveness of crowdsourced design feedback, we investigate a new\napproach for mitigating the effects of negative or positive feedback by\ncombining open-ended and thought-provoking questions with declarative feedback\nstatements. We conducted two user studies to assess the effects of\nquestion-based feedback on the sentiment and quality of design revisions in the\ncontext of graphic design. We found that crowdsourced question-based feedback\ncontains more neutral sentiment than statement-based feedback. Moreover, we\nprovide evidence that presenting feedback as questions followed by statements\nleads to better design revisions than question- or statement-based feedback\nalone.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 14:41:04 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Lekschas", "Fritz", ""], ["Ampanavos", "Spyridon", ""], ["Siangliulue", "Pao", ""], ["Pfister", "Hanspeter", ""], ["Gajos", "Krzysztof Z.", ""]]}, {"id": "2101.06220", "submitter": "Jichen Zhu", "authors": "Jichen Zhu, Jennifer Villareale, Nithesh Javvaji, Sebastian Risi,\n  Mathias L\\\"owe, Rush Weigelt, Casper Harteveld", "title": "Player-AI Interaction: What Neural Network Games Reveal About AI as Play", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of artificial intelligence (AI) and machine learning (ML) bring\nhuman-AI interaction to the forefront of HCI research. This paper argues that\ngames are an ideal domain for studying and experimenting with how humans\ninteract with AI. Through a systematic survey of neural network games (n = 38),\nwe identified the dominant interaction metaphors and AI interaction patterns in\nthese games. In addition, we applied existing human-AI interaction guidelines\nto further shed light on player-AI interaction in the context of AI-infused\nsystems. Our core finding is that AI as play can expand current notions of\nhuman-AI interaction, which are predominantly productivity-based. In\nparticular, our work suggests that game and UX designers should consider flow\nto structure the learning curve of human-AI interaction, incorporate\ndiscovery-based learning to play around with the AI and observe the\nconsequences, and offer users an invitation to play to explore new forms of\nhuman-AI interaction.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 17:07:03 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 10:25:19 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zhu", "Jichen", ""], ["Villareale", "Jennifer", ""], ["Javvaji", "Nithesh", ""], ["Risi", "Sebastian", ""], ["L\u00f6we", "Mathias", ""], ["Weigelt", "Rush", ""], ["Harteveld", "Casper", ""]]}, {"id": "2101.06283", "submitter": "Young-Ho Kim", "authors": "Young-Ho Kim, Bongshin Lee, Arjun Srinivasan, Eun Kyoung Choe", "title": "Data@Hand: Fostering Visual Exploration of Personal Data on Smartphones\n  Leveraging Speech and Touch Interaction", "comments": "To appear in ACM CHI 2021 Conference on Human Factors in Computing\n  Systems; 16 pages, 6 figures, 5 tables", "journal-ref": "In CHI Conference on Human Factors in Computing Systems (CHI '21),\n  May 8-13, 2021, Yokohama, Japan", "doi": "10.1145/3411764.3445421", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most mobile health apps employ data visualization to help people view their\nhealth and activity data, but these apps provide limited support for visual\ndata exploration. Furthermore, despite its huge potential benefits, mobile\nvisualization research in the personal data context is sparse. This work aims\nto empower people to easily navigate and compare their personal health data on\nsmartphones by enabling flexible time manipulation with speech. We designed and\ndeveloped Data@Hand, a mobile app that leverages the synergy of two\ncomplementary modalities: speech and touch. Through an exploratory study with\n13 long-term Fitbit users, we examined how multimodal interaction helps\nparticipants explore their own health data. Participants successfully adopted\nmultimodal interaction (i.e., speech and touch) for convenient and fluid data\nexploration. Based on the quantitative and qualitative findings, we discuss\ndesign implications and opportunities with multimodal interaction for better\nsupporting visual data exploration on mobile devices.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 19:28:42 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Kim", "Young-Ho", ""], ["Lee", "Bongshin", ""], ["Srinivasan", "Arjun", ""], ["Choe", "Eun Kyoung", ""]]}, {"id": "2101.06305", "submitter": "Will Crichton", "authors": "Will Crichton, Maneesh Agrawala, Pat Hanrahan", "title": "The Role of Working Memory in Program Tracing", "comments": "To appear at CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445257", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program tracing, or mentally simulating a program on concrete inputs, is an\nimportant part of general program comprehension. Programs involve many kinds of\nvirtual state that must be held in memory, such as variable/value pairs and a\ncall stack. In this work, we examine the influence of short-term working memory\n(WM) on a person's ability to remember program state during tracing. We first\nconfirm that previous findings in cognitive psychology transfer to the\nprogramming domain: people can keep about 7 variable/value pairs in WM, and\npeople will accidentally swap associations between variables due to WM load. We\nuse a restricted focus viewing interface to further analyze the strategies\npeople use to trace through programs, and the relationship of tracing strategy\nto WM. Given a straight-line program, we find half of our participants traced a\nprogram from the top-down line-by-line (linearly), and the other half start at\nthe bottom and trace upward based on data dependencies (on-demand).\nParticipants with an on-demand strategy made more WM errors while tracing\nstraight-line code than with a linear strategy, but the two strategies\ncontained an equal number of WM errors when tracing code with functions. We\nconclude with the implications of these findings for the design of programming\ntools: first, programs should be analyzed to identify and refactor\nhuman-memory-intensive sections of code. Second, programming environments\nshould interactively visualize variable metadata to reduce WM load in\naccordance with a person's tracing strategy. Third, tools for program\ncomprehension should enable externalizing program state while tracing.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 21:30:17 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Crichton", "Will", ""], ["Agrawala", "Maneesh", ""], ["Hanrahan", "Pat", ""]]}, {"id": "2101.06315", "submitter": "Henry Dambanemuya", "authors": "Henry K. Dambanemuya and Em\\H{o}ke-\\'Agnes Horv\\'at", "title": "A Multi-Platform Study of Crowd Signals Associated with Successful\n  Online Fundraising", "comments": "To appear in the Proceedings of the ACM (PACM) Human-Computer\n  Interaction CSCW'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growing popularity of online fundraising (aka \"crowdfunding\") has\nattracted significant research on the subject. In contrast to previous studies\nthat attempt to predict the success of crowdfunded projects based on specific\ncharacteristics of the projects and their creators, we present a more general\napproach that focuses on crowd dynamics and is robust to the particularities of\ndifferent crowdfunding platforms. We rely on a multi-method analysis to\ninvestigate the correlates, predictive importance, and quasi-causal effects of\nfeatures that describe crowd dynamics in determining the success of crowdfunded\nprojects. By applying a multi-method analysis to a study of fundraising in\nthree different online markets, we uncover general crowd dynamics that\nultimately decide which projects will succeed. In all analyses and across the\nthree different platforms, we consistently find that funders' behavioural\nsignals (1) are significantly correlated with fundraising success; (2)\napproximate fundraising outcomes better than the characteristics of projects\nand their creators such as credit grade, company valuation, and subject domain;\nand (3) have significant quasi-causal effects on fundraising outcomes while\ncontrolling for potentially confounding project variables. By showing that\nuniversal features deduced from crowd behaviour are predictive of fundraising\nsuccess on different crowdfunding platforms, our work provides design-relevant\ninsights about novel types of collective decision-making online. This research\ninspires thus potential ways to leverage cues from the crowd and catalyses\nresearch into crowd-aware system design.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 22:40:10 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dambanemuya", "Henry K.", ""], ["Horv\u00e1t", "Em\u0151ke-\u00c1gnes", ""]]}, {"id": "2101.06322", "submitter": "Sarah Sch\\\"ottler", "authors": "Sarah Sch\\\"ottler, Yalong Yang, Hanspeter Pfister, Benjamin Bach", "title": "Visualizing and Interacting with Geospatial Networks: A Survey and\n  Design Space", "comments": "To be published in the Computer Graphics Forum (CGF) journal", "journal-ref": null, "doi": "10.1111/cgf.14198", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys visualization and interaction techniques for geospatial\nnetworks from a total of 95 papers. Geospatial networks are graphs where nodes\nand links can be associated with geographic locations. Examples can include\nsocial networks, trade and migration, as well as traffic and transport\nnetworks. Visualizing geospatial networks poses numerous challenges around the\nintegration of both network and geographical information as well as additional\ninformation such as node and link attributes, time, and uncertainty. Our\noverview analyzes existing techniques along four dimensions: i) the\nrepresentation of geographical information, ii) the representation of network\ninformation, iii) the visual integration of both, and iv) the use of\ninteraction. These four dimensions allow us to discuss techniques with respect\nto the trade-offs they make between showing information across all these\ndimensions and how they solve the problem of showing as much information as\nnecessary while maintaining readability of the visualization.\nhttps://geonetworks.github.io.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:12:15 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 10:49:31 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Sch\u00f6ttler", "Sarah", ""], ["Yang", "Yalong", ""], ["Pfister", "Hanspeter", ""], ["Bach", "Benjamin", ""]]}, {"id": "2101.06328", "submitter": "Alan Smeaton", "authors": "Hyowon Lee, Mingming Liu, Hamza Riaz, Navaneethan Rajasekaren, Michael\n  Scriney, Alan F. Smeaton", "title": "Attention Based Video Summaries of Live Online Zoom Classes", "comments": "Presented at AAAI-2021 Workshop on AI Education: \"Imagining\n  Post-COVID Education with AI\" (TIPCE-2021). 9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a system developed to help University students get more\nfrom their online lectures, tutorials, laboratory and other live sessions. We\ndo this by logging their attention levels on their laptops during live Zoom\nsessions and providing them with personalised video summaries of those live\nsessions. Using facial attention analysis software we create personalised video\nsummaries composed of just the parts where a student's attention was below some\nthreshold. We can also factor in other criteria into video summary generation\nsuch as parts where the student was not paying attention while others in the\nclass were, and parts of the video that other students have replayed\nextensively which a given student has not. Attention and usage based video\nsummaries of live classes are a form of personalised content, they are\neducational video segments recommended to highlight important parts of live\nsessions, useful in both topic understanding and in exam preparation. The\nsystem also allows a Professor to review the aggregated attention levels of\nthose in a class who attended a live session and logged their attention levels.\nThis allows her to see which parts of the live activity students were paying\nmost, and least, attention to. The Help-Me-Watch system is deployed and in use\nat our University in a way that protects student's personal data, operating in\na GDPR-compliant way.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jan 2021 23:28:52 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Lee", "Hyowon", ""], ["Liu", "Mingming", ""], ["Riaz", "Hamza", ""], ["Rajasekaren", "Navaneethan", ""], ["Scriney", "Michael", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2101.06444", "submitter": "Dmitry Alexandrovsky", "authors": "Dmitry Alexandrovsky, Susanne Putze, Valentin Schwind, Elisa D.\n  Mekler, Jan David Smeddinck, Denise Kahl, Antonio Kr\\\"uger, Rainer Malaka", "title": "Evaluating User Experiences in Mixed Reality", "comments": "Workshop proposal at CHI '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measure user experience in MR (i.e., AR/VR) user studies is essential.\nResearchers apply a wide range of measuring methods using objective (e.g.,\nbiosignals, time logging), behavioral (e.g., gaze direction, movement\namplitude), and subjective (e.g., standardized questionnaires) metrics. Many of\nthese measurement instruments were adapted from use-cases outside of MR but\nhave not been validated for usage in MR experiments. However, researchers are\nfaced with various challenges and design alternatives when measuring immersive\nexperiences. These challenges become even more diverse when running out-of-the\nlab studies. Measurement methods of VR experience recently received much\nattention. For example, research has started embedding questionnaires in the VE\nfor various applications, allowing users to stay closer to the ongoing\nexperience while filling out the survey. However, there is a diversity in the\ninteraction methods and practices on how the assessment procedure is conducted.\nThis diversity in methods underlines a missing shared agreement of standardized\nmeasurement tools for VR experiences. AR research strongly orients on the\nresearch methods from VR, e.g., using the same type of subjective\nquestionnaires. However, some crucial technical differences require careful\nconsiderations during the evaluation. This workshop at CHI 2021 provides a\nfoundation to exchange expertise and address challenges and opportunities of\nresearch methods in MR user studies. By this, our workshop launches a\ndiscussion of research methods that should lead to standardizing assessment\nmethods in MR user studies. The outcomes of the workshop will be aggregated\ninto a collective special issue journal article.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 12:38:01 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Alexandrovsky", "Dmitry", ""], ["Putze", "Susanne", ""], ["Schwind", "Valentin", ""], ["Mekler", "Elisa D.", ""], ["Smeddinck", "Jan David", ""], ["Kahl", "Denise", ""], ["Kr\u00fcger", "Antonio", ""], ["Malaka", "Rainer", ""]]}, {"id": "2101.06535", "submitter": "Emiliano De Cristofaro", "authors": "Chen Ling, Ihab AbuHilal, Jeremy Blackburn, Emiliano De Cristofaro,\n  Savvas Zannettou, and Gianluca Stringhini", "title": "Dissecting the Meme Magic: Understanding Indicators of Virality in Image\n  Memes", "comments": "To appear at the 24th ACM Conference on Computer-Supported Coop-\n  erative Work and Social Computing (CSCW 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the increasingly important role played by image memes, we do not yet\nhave a solid understanding of the elements that might make a meme go viral on\nsocial media. In this paper, we investigate what visual elements distinguish\nimage memes that are highly viral on social media from those that do not get\nre-shared, across three dimensions: composition, subjects, and target audience.\nDrawing from research in art theory, psychology, marketing, and neuroscience,\nwe develop a codebook to characterize image memes, and use it to annotate a set\nof 100 image memes collected from 4chan's Politically Incorrect Board (/pol/).\nOn the one hand, we find that highly viral memes are more likely to use a\nclose-up scale, contain characters, and include positive or negative emotions.\nOn the other hand, image memes that do not present a clear subject the viewer\ncan focus attention on, or that include long text are not likely to be\nre-shared by users.\n  We train machine learning models to distinguish between image memes that are\nlikely to go viral and those that are unlikely to be re-shared, obtaining an\nAUC of 0.866 on our dataset. We also show that the indicators of virality\nidentified by our model can help characterize the most viral memes posted on\nmainstream online social networks too, as our classifiers are able to predict\n19 out of the 20 most popular image memes posted on Twitter and Reddit between\n2016 and 2018. Overall, our analysis sheds light on what indicators\ncharacterize viral and non-viral visual content online, and set the basis for\ndeveloping better techniques to create or moderate content that is more likely\nto catch the viewer's attention.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jan 2021 22:36:51 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Ling", "Chen", ""], ["AbuHilal", "Ihab", ""], ["Blackburn", "Jeremy", ""], ["De Cristofaro", "Emiliano", ""], ["Zannettou", "Savvas", ""], ["Stringhini", "Gianluca", ""]]}, {"id": "2101.06968", "submitter": "Javier Fumanal-Idocin Mr.", "authors": "Javier Fumanal-Idocin, Yu-Kai Wang, Chin-Teng Lin, Javier Fern\\'andez,\n  Jose Antonio Sanz, Humberto Bustince", "title": "Motor-Imagery-Based Brain Computer Interface using Signal Derivation and\n  Aggregation Functions", "comments": "IEEE Transactions on Cybernetics (2021)", "journal-ref": null, "doi": "10.1109/TCYB.2021.3073210", "report-no": null, "categories": "cs.HC cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Brain Computer Interface technologies are popular methods of communication\nbetween the human brain and external devices. One of the most popular\napproaches to BCI is Motor Imagery. In BCI applications, the\nElectroEncephaloGraphy is a very popular measurement for brain dynamics because\nof its non-invasive nature. Although there is a high interest in the BCI topic,\nthe performance of existing systems is still far from ideal, due to the\ndifficulty of performing pattern recognition tasks in EEG signals. BCI systems\nare composed of a wide range of components that perform signal pre-processing,\nfeature extraction and decision making. In this paper, we define a BCI\nFramework, named Enhanced Fusion Framework, where we propose three different\nideas to improve the existing MI-based BCI frameworks. Firstly, we include aan\nadditional pre-processing step of the signal: a differentiation of the EEG\nsignal that makes it time-invariant. Secondly, we add an additional frequency\nband as feature for the system and we show its effect on the performance of the\nsystem. Finally, we make a profound study of how to make the final decision in\nthe system. We propose the usage of both up to six types of different\nclassifiers and a wide range of aggregation functions (including classical\naggregations, Choquet and Sugeno integrals and their extensions and overlap\nfunctions) to fuse the information given by the considered classifiers. We have\ntested this new system on a dataset of 20 volunteers performing motor\nimagery-based brain-computer interface experiments. On this dataset, the new\nsystem achieved a 88.80% of accuracy. We also propose an optimized version of\nour system that is able to obtain up to 90,76%. Furthermore, we find that the\npair Choquet/Sugeno integrals and overlap functions are the ones providing the\nbest results.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 10:14:01 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 08:41:37 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Fumanal-Idocin", "Javier", ""], ["Wang", "Yu-Kai", ""], ["Lin", "Chin-Teng", ""], ["Fern\u00e1ndez", "Javier", ""], ["Sanz", "Jose Antonio", ""], ["Bustince", "Humberto", ""]]}, {"id": "2101.07048", "submitter": "Andrey Krekhov", "authors": "Andrey Krekhov and Jens Krueger", "title": "Deadeye: A Novel Preattentive Visualization Technique Based on Dichoptic\n  Presentation", "comments": "10 pages, 8 figures, journal", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, vol. 25,\n  no. 1, pp. 936-945, Jan. 2019", "doi": "10.1109/TVCG.2018.2864498", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preattentive visual features such as hue or flickering can effectively draw\nattention to an object of interest -- for instance, an important feature in a\nscientific visualization. These features appear to pop out and can be\nrecognized by our visual system, independently from the number of distractors.\nMost cues do not take advantage of the fact that most humans have two eyes. In\ncases where binocular vision is applied, it is almost exclusively used to\nconvey depth by exposing stereo pairs. We present Deadeye, a novel preattentive\nvisualization technique based on presenting different stimuli to each eye. The\ntarget object is rendered for one eye only and is instantly detected by our\nvisual system. In contrast to existing cues, Deadeye does not modify any visual\nproperties of the target and, thus, is particularly suited for visualization\napplications. Our evaluation confirms that Deadeye is indeed perceived\npreattentively. We also explore a conjunction search based on our technique and\nshow that, in contrast to 3D depth, the task cannot be processed in parallel.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 12:53:57 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Krekhov", "Andrey", ""], ["Krueger", "Jens", ""]]}, {"id": "2101.07069", "submitter": "Jong-Seok Lee", "authors": "Seong-Eun Moon, Chun-Jui Chen, Cho-Jui Hsieh, Jane-Ling Wang,\n  Jong-Seok Lee", "title": "Emotional EEG Classification using Connectivity Features and\n  Convolutional Neural Networks", "comments": null, "journal-ref": "Neural Networks, vol. 132, pp. 96-107, Dec. 2020", "doi": "10.1016/j.neunet.2020.08.009", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Convolutional neural networks (CNNs) are widely used to recognize the user's\nstate through electroencephalography (EEG) signals. In the previous studies,\nthe EEG signals are usually fed into the CNNs in the form of high-dimensional\nraw data. However, this approach makes it difficult to exploit the brain\nconnectivity information that can be effective in describing the functional\nbrain network and estimating the perceptual state of the user. We introduce a\nnew classification system that utilizes brain connectivity with a CNN and\nvalidate its effectiveness via the emotional video classification by using\nthree different types of connectivity measures. Furthermore, two data-driven\nmethods to construct the connectivity matrix are proposed to maximize\nclassification performance. Further analysis reveals that the level of\nconcentration of the brain connectivity related to the emotional property of\nthe target video is correlated with classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 13:28:08 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Moon", "Seong-Eun", ""], ["Chen", "Chun-Jui", ""], ["Hsieh", "Cho-Jui", ""], ["Wang", "Jane-Ling", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2101.07124", "submitter": "Bhaskar Mitra", "authors": "Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani\n  and Fernando Diaz", "title": "Tip of the Tongue Known-Item Retrieval: A Case Study in Movie\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current information retrieval systems are effective for known-item\nretrieval where the searcher provides a precise name or identifier for the item\nbeing sought, systems tend to be much less effective for cases where the\nsearcher is unable to express a precise name or identifier. We refer to this as\ntip of the tongue (TOT) known-item retrieval, named after the cognitive state\nof not being able to retrieve an item from memory. Using movie search as a case\nstudy, we explore the characteristics of questions posed by searchers in TOT\nstates in a community question answering website. We analyze how searchers\nexpress their information needs during TOT states in the movie domain.\nSpecifically, what information do searchers remember about the item being\nsought and how do they convey this information? Our results suggest that\nsearchers use a combination of information about: (1) the content of the item\nsought, (2) the context in which they previously engaged with the item, and (3)\nprevious attempts to find the item using other resources (e.g., search\nengines). Additionally, searchers convey information by sometimes expressing\nuncertainty (i.e., hedging), opinions, emotions, and by performing relative\n(vs. absolute) comparisons with attributes of the item. As a result of our\nanalysis, we believe that searchers in TOT states may require specialized query\nunderstanding methods or document representations. Finally, our preliminary\nretrieval experiments show the impact of each information type presented in\ninformation requests on retrieval performance.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 15:33:46 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Arguello", "Jaime", ""], ["Ferguson", "Adam", ""], ["Fine", "Emery", ""], ["Mitra", "Bhaskar", ""], ["Zamani", "Hamed", ""], ["Diaz", "Fernando", ""]]}, {"id": "2101.07314", "submitter": "Takuma Yagi", "authors": "Takuma Yagi, Takumi Nishiyasu, Kunimasa Kawasaki, Moe Matsuki, Yoichi\n  Sato", "title": "GO-Finder: A Registration-Free Wearable System for Assisting Users in\n  Finding Lost Objects via Hand-Held Object Discovery", "comments": "13 pages, 13 figures, ACM IUI 2021", "journal-ref": null, "doi": "10.1145/3397481.3450664", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People spend an enormous amount of time and effort looking for lost objects.\nTo help remind people of the location of lost objects, various computational\nsystems that provide information on their locations have been developed.\nHowever, prior systems for assisting people in finding objects require users to\nregister the target objects in advance. This requirement imposes a cumbersome\nburden on the users, and the system cannot help remind them of unexpectedly\nlost objects. We propose GO-Finder (\"Generic Object Finder\"), a\nregistration-free wearable camera based system for assisting people in finding\nan arbitrary number of objects based on two key features: automatic discovery\nof hand-held objects and image-based candidate selection. Given a video taken\nfrom a wearable camera, Go-Finder automatically detects and groups hand-held\nobjects to form a visual timeline of the objects. Users can retrieve the last\nappearance of the object by browsing the timeline through a smartphone app. We\nconducted a user study to investigate how users benefit from using GO-Finder\nand confirmed improved accuracy and reduced mental load regarding the object\nsearch task by providing clear visual cues on object locations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 20:04:56 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 11:16:44 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Yagi", "Takuma", ""], ["Nishiyasu", "Takumi", ""], ["Kawasaki", "Kunimasa", ""], ["Matsuki", "Moe", ""], ["Sato", "Yoichi", ""]]}, {"id": "2101.07327", "submitter": "Hung-Wei Tseng", "authors": "Alec Rohloff and Zackary Allen and Kung-Min Lin and Joshua Okrend and\n  Chengyi Nie and Yu-Chia Liu and Hung-Wei Tseng", "title": "OpenUVR: an Open-Source System Framework for Untethered Virtual Reality\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.HC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in heterogeneous computing technologies enable the significant\npotential of virtual reality (VR) applications. To offer the best user\nexperience (UX), a system should adopt an untethered, wireless-network-based\narchitecture to transfer VR content between the user and the content generator.\nHowever, modern wireless network technologies make implementing such an\narchitecture challenging, as VR applications require superior video quality --\nwith high resolution, high frame rates, and very low latency.\n  This paper presents OpenUVR, an open-source framework that uses commodity\nhardware components to satisfy the demands of interactive, real-time VR\napplications. OpenUVR significantly improves UX through a redesign of the\nsystem stack and addresses the most time-sensitive issues associated with\nredundant memory copying in modern computing systems. OpenUVR presents a\ncross-layered VR datapath to avoid redundant data operations and computation\namong system components, OpenUVR customizes the network stack to eliminate\nunnecessary memory operations incurred by mismatching data formats in each\nlayer, and OpenUVR uses feedback from mobile devices to remove memory buffers.\n  Together, these modifications allow OpenUVR to reduce VR application delays\nto 14.32 ms, meeting the 20 ms minimum latency in avoiding motion sickness. As\nan open-source system that is fully compatible with commodity hardware, OpenUVR\noffers the research community an opportunity to develop, investigate, and\noptimize applications for untethered, high-performance VR architectures.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jan 2021 21:02:16 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Rohloff", "Alec", ""], ["Allen", "Zackary", ""], ["Lin", "Kung-Min", ""], ["Okrend", "Joshua", ""], ["Nie", "Chengyi", ""], ["Liu", "Yu-Chia", ""], ["Tseng", "Hung-Wei", ""]]}, {"id": "2101.07388", "submitter": "Bill Tomlinson", "authors": "Bill Tomlinson, Rebecca W. Black", "title": "Work Online, Welfare Calls, and Wine Night: Effects of the COVID-19\n  Pandemic on Individuals' Technology Use", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The COVID-19 pandemic has changed the ways many people use computational\nsystems. We conducted an empirical study, using qualitative and quantitative\nanalyses of free-response surveys completed by 62 US residents, to explore how\nCOVID-19 affected their computer use across work, education, home life, and\nsocial life. Nearly all participants experienced an increase in computer usage\nfor themselves or a family member in one or more of the four domains. The\nincreases involved both increasing frequency of existing uses as well as the\nadoption of new types of use. Changes in usage impacted many aspects of\npeople's lives, including relationships, affective experiences, and life\ntrajectories. Understanding these changes is important to the future of HCI, as\nthe field adapts to COVID-19 and potential future pandemics.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 00:43:00 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Tomlinson", "Bill", ""], ["Black", "Rebecca W.", ""]]}, {"id": "2101.07630", "submitter": "Andre Rodrigues", "authors": "Andr\\'e Rodrigues, Andr\\'e Santos, Kyle Montague, Tiago Guerreiro", "title": "Promoting Self-Efficacy Through an Effective Human-Powered Nonvisual\n  Smartphone Task Assistant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accessibility assessments typically focus on determining a binary measurement\nof task performance success/failure; and often neglect to acknowledge the\nnuances of those interactions. Although a large population of blind people find\nsmartphone interactions possible, many experiences take a significant toll and\ncan have a lasting negative impact on the individual and their willingness to\nstep out of technological comfort zones. There is a need to assist and support\nindividuals with the adoption and learning process of new tasks to mitigate\nthese negative experiences. We contribute with a human-powered nonvisual task\nassistant for smartphones to provide pervasive assistance. We argue, in\naddition to success, one must carefully consider promoting and evaluating\nfactors such as self-efficacy and the belief in one's own abilities to control\nand learn to use technology. In this paper, we show effective assistant\npositively affects self-efficacy when performing new tasks with smartphones,\naffects perceptions of accessibility and enables systemic task-based learning.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 14:07:33 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 15:11:42 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Rodrigues", "Andr\u00e9", ""], ["Santos", "Andr\u00e9", ""], ["Montague", "Kyle", ""], ["Guerreiro", "Tiago", ""]]}, {"id": "2101.07691", "submitter": "Rachel Freedman", "authors": "Rachel Freedman, Rohin Shah and Anca Dragan", "title": "Choice Set Misspecification in Reward Inference", "comments": "Presented at the IJCAI-PRICAI 2020 Workshop on Artificial\n  Intelligence Safety", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Specifying reward functions for robots that operate in environments without a\nnatural reward signal can be challenging, and incorrectly specified rewards can\nincentivise degenerate or dangerous behavior. A promising alternative to\nmanually specifying reward functions is to enable robots to infer them from\nhuman feedback, like demonstrations or corrections. To interpret this feedback,\nrobots treat as approximately optimal a choice the person makes from a choice\nset, like the set of possible trajectories they could have demonstrated or\npossible corrections they could have made. In this work, we introduce the idea\nthat the choice set itself might be difficult to specify, and analyze choice\nset misspecification: what happens as the robot makes incorrect assumptions\nabout the set of choices from which the human selects their feedback. We\npropose a classification of different kinds of choice set misspecification, and\nshow that these different classes lead to meaningful differences in the\ninferred reward and resulting performance. While we would normally expect\nmisspecification to hurt, we find that certain kinds of misspecification are\nneither helpful nor harmful (in expectation). However, in other situations,\nmisspecification can be extremely harmful, leading the robot to believe the\nopposite of what it should believe. We hope our results will allow for better\nprediction and response to the effects of misspecification in real-world reward\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 15:35:30 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Freedman", "Rachel", ""], ["Shah", "Rohin", ""], ["Dragan", "Anca", ""]]}, {"id": "2101.07708", "submitter": "Paul Rosen", "authors": "Alon Friedman and Paul Rosen", "title": "Leveraging Peer Review in Visualization Education: A Proposal for a New\n  Model", "comments": null, "journal-ref": "Pedagogy Data Visualization Workshop @ IEEE VIS, 2017", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visualization education, both science and humanities, the literature is\noften divided into two parts: the design aspect and the analysis of the\nvisualization. However, we find limited discussion on how to motivate and\nengage visualization students in the classroom. In the field of Writing\nStudies, researchers develop tools and frameworks for student peer review of\nwriting. Based on the literature review from the field of Writing Studies, this\npaper proposes a new framework to implement visualization peer review in the\nclassroom to engage today's students. This framework can be customized for\nincremental and double-blind review to inspire students and reinforce critical\nthinking about visualization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:18:48 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Friedman", "Alon", ""], ["Rosen", "Paul", ""]]}, {"id": "2101.07853", "submitter": "Kelly Mack", "authors": "Kelly Mack, Megan Hofmann, Udaya Lakshmi, Jerry Cao, Nayha Auradkar,\n  Rosa I. Arriaga, Scott E. Hudson, Jennifer Mankoff", "title": "Rapid Convergence: The Outcomes of Making PPE during a Healthcare Crisis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The NIH 3D Print Exchange is a public and open source repository for\nprimarily 3D printable medical device designs with contributions from\nexpert-amateur makers, engineers from industry and academia, and clinicians. In\nresponse to the COVID-19 pandemic, a collection was formed to foster\nsubmissions of low-cost, local manufacture of personal protective equipment\n(Personal Protective Equipment (PPE)). We systematically evaluated the 623\nsubmissions in this collection to understand: what makers contributed, how they\nwere made, who made them, and key characteristics of their designs. Our\nanalysis reveals an immediate design convergence to derivatives of a few\ninitial designs affiliated with NIH partners (e.g., universities, the Veteran's\nHealth Administration, America Makes) and major for-profit groups (e.g.,\nPrusa). The NIH worked to review safe and effective designs but was quickly\noverloaded by derivative works. We found that the vast majority were never\nreviewed (81.3%) while 10.4% of those reviewed were deemed safe for clinical\n(5.6%) or community use (4.8%). Our work contributes insights into: the\noutcomes of distributed, community-based, medical making; features the\ncommunity accepted as \"safe\" making; and how platforms can support regulated\nmaker activities in high-risk domains (e.g., healthcare).\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 20:37:53 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Mack", "Kelly", ""], ["Hofmann", "Megan", ""], ["Lakshmi", "Udaya", ""], ["Cao", "Jerry", ""], ["Auradkar", "Nayha", ""], ["Arriaga", "Rosa I.", ""], ["Hudson", "Scott E.", ""], ["Mankoff", "Jennifer", ""]]}, {"id": "2101.07891", "submitter": "Homagni Saha", "authors": "Homagni Saha, Fateme Fotouhif, Qisai Liu, Soumik Sarkar", "title": "A modular vision language navigation and manipulation framework for long\n  horizon compositional tasks in indoor environment", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new framework - MoViLan (Modular Vision and\nLanguage) for execution of visually grounded natural language instructions for\nday to day indoor household tasks. While several data-driven, end-to-end\nlearning frameworks have been proposed for targeted navigation tasks based on\nthe vision and language modalities, performance on recent benchmark data sets\nrevealed the gap in developing comprehensive techniques for long horizon,\ncompositional tasks (involving manipulation and navigation) with diverse object\ncategories, realistic instructions and visual scenarios with non-reversible\nstate changes. We propose a modular approach to deal with the combined\nnavigation and object interaction problem without the need for strictly aligned\nvision and language training data (e.g., in the form of expert demonstrated\ntrajectories). Such an approach is a significant departure from the traditional\nend-to-end techniques in this space and allows for a more tractable training\nprocess with separate vision and language data sets. Specifically, we propose a\nnovel geometry-aware mapping technique for cluttered indoor environments, and a\nlanguage understanding model generalized for household instruction following.\nWe demonstrate a significant increase in success rates for long-horizon,\ncompositional tasks over the baseline on the recently released benchmark data\nset-ALFRED.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 23:05:43 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Saha", "Homagni", ""], ["Fotouhif", "Fateme", ""], ["Liu", "Qisai", ""], ["Sarkar", "Soumik", ""]]}, {"id": "2101.07902", "submitter": "Andrew McNutt", "authors": "Andrew McNutt, Ravi Chugh", "title": "Integrated Visualization Editing via Parameterized Declarative Templates", "comments": "14 pages, 8 Figures, and a 4 page supplement with 4 figures and 1\n  table", "journal-ref": "CHI Conference on Human Factors in Computing Systems (CHI '21),\n  May 8-13, 2021, Yokohama, Japan", "doi": "10.1145/3411764.3445356", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interfaces for creating visualizations typically embrace one of several\ncommon forms. Textual specification enables fine-grained control, shelf\nbuilding facilitates rapid exploration, while chart choosing promotes immediacy\nand simplicity. Ideally these approaches could be unified to integrate the\nuser- and usage-dependent benefits found in each modality, yet these forms\nremain distinct. We propose parameterized declarative templates, a simple\nabstraction mechanism over JSON-based visualization grammars, as a foundation\nfor multimodal visualization editors. We demonstrate how templates can\nfacilitate organization and reuse by factoring the more than 160 charts that\nconstitute Vega-Lite's example gallery into approximately 40 templates. We\nexemplify the pliability of abstracting over charting grammars by implementing\n-- as a template -- the functionality of the shelf builder Polestar (a\nsimulacra of Tableau) and a set of templates that emulate the Google Sheets\nchart chooser. We show how templates support multimodal visualization editing\nby implementing a prototype and evaluating it through an approachability study.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 23:56:59 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 16:00:39 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["McNutt", "Andrew", ""], ["Chugh", "Ravi", ""]]}, {"id": "2101.07906", "submitter": "Daniel Martin", "authors": "Daniel Martin, Sandra Malpica, Diego Gutierrez, Belen Masia and Ana\n  Serrano", "title": "Multimodality in VR: A survey", "comments": "35 pages (24 pages not including references), 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Virtual reality (VR) is rapidly growing, with the potential to change the way\nwe create and consume content. In VR, users integrate multimodal sensory\ninformation they receive, to create a unified perception of the virtual world.\nIn this survey, we review the body of work addressing multimodality in VR, and\nits role and benefits in user experience, together with different applications\nthat leverage multimodality in many disciplines. These works thus encompass\nseveral fields of research, and demonstrate that multimodality plays a\nfundamental role in VR; enhancing the experience, improving overall\nperformance, and yielding unprecedented abilities in skill and knowledge\ntransfer.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 00:29:23 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 09:58:51 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Martin", "Daniel", ""], ["Malpica", "Sandra", ""], ["Gutierrez", "Diego", ""], ["Masia", "Belen", ""], ["Serrano", "Ana", ""]]}, {"id": "2101.07990", "submitter": "Haotian Li", "authors": "Haotian Li, Min Xu, Yong Wang, Huan Wei, Huamin Qu", "title": "A Visual Analytics Approach to Facilitate the Proctoring of Online Exams", "comments": "17 pages, 10 figures. Accepted at CHI2021", "journal-ref": null, "doi": "10.1145/3411764.3445294", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online exams have become widely used to evaluate students' performance in\nmastering knowledge in recent years, especially during the pandemic of\nCOVID-19. However, it is challenging to conduct proctoring for online exams due\nto the lack of face-to-face interaction. Also, prior research has shown that\nonline exams are more vulnerable to various cheating behaviors, which can\ndamage their credibility. This paper presents a novel visual analytics approach\nto facilitate the proctoring of online exams by analyzing the exam video\nrecords and mouse movement data of each student. Specifically, we detect and\nvisualize suspected head and mouse movements of students in three levels of\ndetail, which provides course instructors and teachers with convenient,\nefficient and reliable proctoring for online exams. Our extensive evaluations,\nincluding usage scenarios, a carefully-designed user study and expert\ninterviews, demonstrate the effectiveness and usability of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:27:40 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Li", "Haotian", ""], ["Xu", "Min", ""], ["Wang", "Yong", ""], ["Wei", "Huan", ""], ["Qu", "Huamin", ""]]}, {"id": "2101.07993", "submitter": "Crystal Lee", "authors": "Crystal Lee, Tanya Yang, Gabrielle Inchoco, Graham M. Jones, Arvind\n  Satyanarayan", "title": "Viral Visualizations: How Coronavirus Skeptics Use Orthodox Data\n  Practices to Promote Unorthodox Science Online", "comments": "To appear in ACM CHI 2021; 18 pages, 4 figures, 1 table", "journal-ref": null, "doi": "10.1145/3411764.3445211", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Controversial understandings of the coronavirus pandemic have turned data\nvisualizations into a battleground. Defying public health officials,\ncoronavirus skeptics on US social media spent much of 2020 creating data\nvisualizations showing that the government's pandemic response was excessive\nand that the crisis was over. This paper investigates how pandemic\nvisualizations circulated on social media, and shows that people who mistrust\nthe scientific establishment often deploy the same rhetorics of data-driven\ndecision-making used by experts, but to advocate for radical policy changes.\nUsing a quantitative analysis of how visualizations spread on Twitter and an\nethnographic approach to analyzing conversations about COVID data on Facebook,\nwe document an epistemological gap that leads pro- and anti-mask groups to draw\ndrastically different inferences from similar data. Ultimately, we argue that\nthe deployment of COVID data visualizations reflect a deeper sociopolitical\nrift regarding the place of science in public life.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:36:47 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Lee", "Crystal", ""], ["Yang", "Tanya", ""], ["Inchoco", "Gabrielle", ""], ["Jones", "Graham M.", ""], ["Satyanarayan", "Arvind", ""]]}, {"id": "2101.07996", "submitter": "Xin Liu", "authors": "Xin Liu, Yuang Li, Josh Fromm, Yuntao Wang, Ziheng Jiang, Alex\n  Mariakakis, Shwetak Patel", "title": "SplitSR: An End-to-End Approach to Super-Resolution on Mobile Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution (SR) is a coveted image processing technique for mobile apps\nranging from the basic camera apps to mobile health. Existing SR algorithms\nrely on deep learning models with significant memory requirements, so they have\nyet to be deployed on mobile devices and instead operate in the cloud to\nachieve feasible inference time. This shortcoming prevents existing SR methods\nfrom being used in applications that require near real-time latency. In this\nwork, we demonstrate state-of-the-art latency and accuracy for on-device\nsuper-resolution using a novel hybrid architecture called SplitSR and a novel\nlightweight residual block called SplitSRBlock. The SplitSRBlock supports\nchannel-splitting, allowing the residual blocks to retain spatial information\nwhile reducing the computation in the channel dimension. SplitSR has a hybrid\ndesign consisting of standard convolutional blocks and lightweight residual\nblocks, allowing people to tune SplitSR for their computational budget. We\nevaluate our system on a low-end ARM CPU, demonstrating both higher accuracy\nand up to 5 times faster inference than previous approaches. We then deploy our\nmodel onto a smartphone in an app called ZoomSR to demonstrate the first-ever\ninstance of on-device, deep learning-based SR. We conducted a user study with\n15 participants to have them assess the perceived quality of images that were\npost-processed by SplitSR. Relative to bilinear interpolation -- the existing\nstandard for on-device SR -- participants showed a statistically significant\npreference when looking at both images (Z=-9.270, p<0.01) and text (Z=-6.486,\np<0.01).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 06:47:41 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Liu", "Xin", ""], ["Li", "Yuang", ""], ["Fromm", "Josh", ""], ["Wang", "Yuntao", ""], ["Jiang", "Ziheng", ""], ["Mariakakis", "Alex", ""], ["Patel", "Shwetak", ""]]}, {"id": "2101.07999", "submitter": "Hiromu Yakura", "authors": "Hiromu Yakura", "title": "No More Handshaking: How have COVID-19 pushed the expansion of\n  computer-mediated communication in Japanese idol culture?", "comments": "To appear in ACM CHI Conference on Human Factors in Computing Systems\n  (CHI '21), May 8-13, 2021, Yokohama, Japan", "journal-ref": null, "doi": "10.1145/3411764.3445252", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Japanese idol culture, meet-and-greet events where fans were allowed to\nhandshake with an idol member for several seconds were regarded as its\nessential component until the spread of COVID-19. Now, idol groups are\nstruggling in the transition of such events to computer-mediated communication\nbecause these events had emphasized meeting face-to-face over communicating, as\nwe can infer from their length of time. I anticipated that investigating this\nemerging transition would provide implications because their communication has\na unique characteristic that is distinct from well-studied situations, such as\nworkplace communication and intimate relationships. Therefore, I first\nconducted a quantitative survey to develop a precise understanding of the\ntransition, and based on its results, had semi-structured interviews with idol\nfans about their perceptions of the transition. The survey revealed distinctive\napproaches, including one where fans gathered at a venue but were isolated from\nthe idol member by an acrylic plate and talked via a video call. Then the\ninterviews not only provided answers to why such an approach would be\nreasonable but also suggested the existence of a large gap between conventional\noffline events and emerging online events in their perceptions. Based on the\nresults, I discussed how we can develop interaction techniques to support this\ntransition and how we can apply it to other situations outside idol culture,\nsuch as computer-mediated performing arts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 07:14:59 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Yakura", "Hiromu", ""]]}, {"id": "2101.08021", "submitter": "Nico Ebert", "authors": "Nico Ebert, Kurt Alexander Ackermann, Bj\\\"orn Scheppler", "title": "Bolder is Better: Raising User Awareness through Salient and Concise\n  Privacy Notices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the question whether the recently proposed approach of\nconcise privacy notices in apps and on websites is effective in raising user\nawareness. To assess the effectiveness in a realistic setting, we included\nconcise notices in a fictitious but realistic fitness tracking app and asked\nparticipants recruited from an online panel to provide their feedback on the\nusability of the app as a cover story. Importantly, after giving feedback,\nusers were also asked to recall the data practices described in the notices.\nThe experimental setup included the variation of different levels of saliency\nand riskiness of the privacy notices. Based on a total sample of 2,274\nparticipants, our findings indicate that concise privacy notices are indeed a\npromising approach to raise user awareness for privacy information when\ndisplayed in a salient way, especially in case the notices describe risky data\npractices. Our results may be helpful for regulators, user advocates and\ntransparency-oriented companies in creating or enforcing better privacy\ntransparency towards average users that do not read traditional privacy\npolicies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 08:36:04 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ebert", "Nico", ""], ["Ackermann", "Kurt Alexander", ""], ["Scheppler", "Bj\u00f6rn", ""]]}, {"id": "2101.08046", "submitter": "Jack Ratcliffe", "authors": "Jack Ratcliffe, Francesco Soave, Nick Bryan-Kinns, Laurissa Tokarchuk,\n  Ildar Farkhatdinov", "title": "Extended Reality (XR) Remote Research: a Survey of Drawbacks and\n  Opportunities", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445170", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extended Reality (XR) technology - such as virtual and augmented reality - is\nnow widely used in Human Computer Interaction (HCI), social science and\npsychology experimentation. However, these experiments are predominantly\ndeployed in-lab with a co-present researcher. Remote experiments, without\nco-present researchers, have not flourished, despite the success of remote\napproaches for non-XR investigations. This paper summarises findings from a\n30-item survey of 46 XR researchers to understand perceived limitations and\nbenefits of remote XR experimentation. Our thematic analysis identifies\nconcerns common with non-XR remote research, such as participant recruitment,\nas well as XR-specific issues, including safety and hardware variability. We\nidentify potential positive affordances of XR technology, including leveraging\ndata collection functionalities builtin to HMDs (e.g. hand, gaze tracking) and\nthe portability and reproducibility of an experimental setting. We suggest that\nXR technology could be conceptualised as an interactive technology and a\ncapable data-collection device suited for remote experimentation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:02:29 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ratcliffe", "Jack", ""], ["Soave", "Francesco", ""], ["Bryan-Kinns", "Nick", ""], ["Tokarchuk", "Laurissa", ""], ["Farkhatdinov", "Ildar", ""]]}, {"id": "2101.08048", "submitter": "Reuben Binns Dr", "authors": "Nitin Agrawal, Reuben Binns, Max Van Kleek, Kim Laine, Nigel Shadbolt", "title": "Exploring Design and Governance Challenges in the Development of\n  Privacy-Preserving Computation", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445677", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Homomorphic encryption, secure multi-party computation, and differential\nprivacy are part of an emerging class of Privacy Enhancing Technologies which\nshare a common promise: to preserve privacy whilst also obtaining the benefits\nof computational analysis. Due to their relative novelty, complexity, and\nopacity, these technologies provoke a variety of novel questions for design and\ngovernance. We interviewed researchers, developers, industry leaders,\npolicymakers, and designers involved in their deployment to explore\nmotivations, expectations, perceived opportunities and barriers to adoption.\nThis provided insight into several pertinent challenges facing the adoption of\nthese technologies, including: how they might make a nebulous concept like\nprivacy computationally tractable; how to make them more usable by developers;\nand how they could be explained and made accountable to stakeholders and wider\nsociety. We conclude with implications for the development, deployment, and\nresponsible governance of these privacy-preserving computation techniques.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 10:07:17 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Agrawal", "Nitin", ""], ["Binns", "Reuben", ""], ["Van Kleek", "Max", ""], ["Laine", "Kim", ""], ["Shadbolt", "Nigel", ""]]}, {"id": "2101.08070", "submitter": "Yulia Goldenberg", "authors": "Yulia Goldenberg and Noam Tractinsky", "title": "Towards the Right Direction in BiDirectional User Interfaces", "comments": "Accepted to CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445461", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hundreds of millions of speakers of bidirectional (BiDi) languages rely on\nwriting systems that mix the native right-to-left script with left-to-right\nstrings. The global reach of interactive digital technologies requires special\nattention to these people, whose perception of interfaces is affected by this\nscript mixture. However, empirical research on this topic is scarce. Although\nleading software vendors provide guidelines for BiDi design, bidirectional\ninterfaces demonstrate inconsistent and incorrect directionality of UI\nelements, which may cause user confusion and errors. Through a websites'\nreview, we identified problematic UI items and considered reasons for their\nexistence. In an online survey with 234 BiDi speakers, we observed that in many\ncases, users' direction preferences were inconsistent with the guidelines. The\nfindings provide potential insights for design rules and empirical evidence for\nthe problem's complexity, suggesting the need for further empirical research\nand greater attention by the HCI community to the BiDi design problem.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 11:10:35 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 22:13:08 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Goldenberg", "Yulia", ""], ["Tractinsky", "Noam", ""]]}, {"id": "2101.08123", "submitter": "Panagiotis Kourtesis P.K.", "authors": "Panagiotis Kourtesis, Simona Collina, Leonidas A.A. Doumas, and Sarah\n  E. MacPherson", "title": "Technological Competence is a Precondition for Effective Implementation\n  of Virtual Reality Head Mounted Displays in Human Neuroscience: A\n  Technological Review and Meta-analysis", "comments": "Published in Frontiers in Human Neuroscience, 4 Figures, 4 Tables", "journal-ref": "2019,Frontiers in Human Neuroscience, 13, p.342", "doi": "10.3389/fnhum.2019.00342", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Immersive virtual reality (VR) emerges as a promising research and clinical\ntool. However, several studies suggest that VR induced adverse symptoms and\neffects (VRISE) may undermine the health and safety standards, and the\nreliability of the scientific results. In the current literature review, the\ntechnical reasons for the adverse symptomatology are investigated to provide\nsuggestions and technological knowledge for the implementation of VR\nhead-mounted display (HMD) systems in cognitive neuroscience. The technological\nsystematic literature indicated features pertinent to display, sound, motion\ntracking, navigation, ergonomic interactions, user experience, and computer\nhardware that should be considered by the researchers. Subsequently, a\nmeta-analysis of 44 neuroscientific or neuropsychological studies involving VR\nHMD systems was performed. The meta-analysis of the VR studies demonstrated\nthat new generation HMDs induced significantly less VRISE and marginally fewer\ndropouts.Importantly, the commercial versions of the new generation HMDs with\nergonomic interactions had zero incidents of adverse symptomatology and\ndropouts. HMDs equivalent to or greater than the commercial versions of\ncontemporary HMDs accompanied with ergonomic interactions are suitable for\nimplementation in cognitive neuroscience. In conclusion, researchers\ntechnological competency, along with meticulous methods and reports pertinent\nto software, hardware, and VRISE, are paramount to ensure the health and safety\nstandards and the reliability of neuroscientific results.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 13:48:11 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["Collina", "Simona", ""], ["Doumas", "Leonidas A. A.", ""], ["MacPherson", "Sarah E.", ""]]}, {"id": "2101.08146", "submitter": "Panagiotis Kourtesis P.K.", "authors": "Panagiotis Kourtesis, Simona Collina, Leonidas A.A. Doumas, and Sarah\n  E. MacPherson", "title": "Validation of the Virtual Reality Neuroscience Questionnaire: Maximum\n  Duration of Immersive Virtual Reality Sessions Without the Presence of\n  Pertinent Adverse Symptomatology", "comments": "Published in Frontier in Human Neuroscience", "journal-ref": "2019.Frontiers in human neuroscience, 13, p.417", "doi": "10.3389/fnhum.2019.00417", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Research suggests that the duration of a VR session modulates the presence\nand intensity of VRISE, but there are no suggestions regarding the appropriate\nmaximum duration of VR sessions. The implementation of high-end VR HMDs in\nconjunction with ergonomic VR software seems to mitigate the presence of VRISE\nsubstantially. However, a brief tool does not currently exist to appraise and\nreport both the quality of software features and VRISE intensity\nquantitatively. The VRNQ was developed to assess the quality of VR software in\nterms of user experience, game mechanics, in-game assistance, and VRISE. Forty\nparticipants aged between 28 and 43 years were recruited (18 gamers and 22\nnon-gamers) for the study. They participated in 3 different VR sessions until\nthey felt weary or discomfort and subsequently filled in the VRNQ. Our results\ndemonstrated that VRNQ is a valid tool for assessing VR software as it has good\nconvergent, discriminant, and construct validity. The maximum duration of VR\nsessions should be between 55-70 minutes when the VR software meets or exceeds\nthe parsimonious cut-offs of the VRNQ and the users are familiarized with the\nVR system. Also. the gaming experience does not seem to affect how long VR\nsessions should last. Also, while the quality of VR software substantially\nmodulates the maximum duration of VR sessions, age and education do not.\nFinally, deeper immersion, better quality of graphics and sound, and more\nhelpful in-game instructions and prompts were found to reduce VRISE intensity.\nThe VRNQ facilitates the brief assessment and reporting of the quality of VR\nsoftware features and/or the intensity of VRISE, while its minimum and\nparsimonious cut-offs may appraise the suitability of VR software. The findings\nof this study contribute to the establishment of rigorous VR methods that are\ncrucial for the viability of immersive VR as a research and clinical tool.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:10:44 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["Collina", "Simona", ""], ["Doumas", "Leonidas A. A.", ""], ["MacPherson", "Sarah E.", ""]]}, {"id": "2101.08155", "submitter": "Markus Wallinger", "authors": "Markus Wallinger, Ben Jacobsen, Stephen Kobourov and Martin\n  N\\\"ollenburg", "title": "On the Readability of Abstract Set Visualizations", "comments": "Supplementary material can be found on https://osf.io/nvd8e/", "journal-ref": "M. Wallinger, B. Jacobsen, S. Kobourov and M. Nollenburg, \"On the\n  Readability of Abstract Set Visualizations,\" in IEEE Transactions on\n  Visualization and Computer Graphics (2021)", "doi": "10.1109/TVCG.2021.3074615", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Set systems are used to model data that naturally arises in many contexts:\nsocial networks have communities, musicians have genres, and patients have\nsymptoms. Visualizations that accurately reflect the information in the\nunderlying set system make it possible to identify the set elements, the sets\nthemselves, and the relationships between the sets. In static contexts, such as\nprint media or infographics, it is necessary to capture this information\nwithout the help of interactions. With this in mind, we consider three\ndifferent systems for medium-sized set data, LineSets, EulerView, and\nMetroSets, and report the results of a controlled human-subjects experiment\ncomparing their effectiveness. Specifically, we evaluate the performance, in\nterms of time and error, on tasks that cover the spectrum of static set-based\ntasks. We also collect and analyze qualitative data about the three different\nvisualization systems. Our results include statistically significant\ndifferences, suggesting that MetroSets performs and scales better.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:26:15 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 13:50:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wallinger", "Markus", ""], ["Jacobsen", "Ben", ""], ["Kobourov", "Stephen", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "2101.08166", "submitter": "Panagiotis Kourtesis P.K.", "authors": "Panagiotis Kourtesis, Danai Korre, Simona Collina, Leonidas A.A.\n  Doumas, and Sarah E. MacPherson", "title": "Guidelines for the Development of Immersive Virtual Reality Software for\n  Cognitive Neuroscience and Neuropsychology: The Development of Virtual\n  Reality Everyday Assessment Lab (VR-EAL)", "comments": "Published in Frontier in Computer Science", "journal-ref": "Frontiers in Computer Science, 1, p.12 (2020)", "doi": "10.3389/fcomp.2019.00012", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Virtual reality (VR) head-mounted displays (HMD) appear to be effective\nresearch tools, which may address the problem of ecological validity in\nneuropsychological testing. However, their widespread implementation is\nhindered by VR induced symptoms and effects (VRISE) and the lack of skills in\nVR software development. This study offers guidelines for the development of VR\nsoftware in cognitive neuroscience and neuropsychology, by describing and\ndiscussing the stages of the development of Virtual Reality Everyday Assessment\nLab (VR-EAL), the first neuropsychological battery in immersive VR. Techniques\nfor evaluating cognitive functions within a realistic storyline are discussed.\nThe utility of various assets in Unity, software development kits, and other\nsoftware are described so that cognitive scientists can overcome challenges\npertinent to VRISE and the quality of the VR software. In addition, this pilot\nstudy attempts to evaluate VR-EAL in accordance with the necessary criteria for\nVR software for research purposes. The VR neuroscience questionnaire (VRNQ;\nKourtesis et al., 2019b) was implemented to appraise the quality of the three\nversions of VR-EAL in terms of user experience, game mechanics, in-game\nassistance, and VRISE. Twenty-five participants aged between 20 and 45 years\nwith 12-16 years of full-time education evaluated various versions of VR-EAL.\nThe final version of VR-EAL achieved high scores in every sub-score of the VRNQ\nand exceeded its parsimonious cut-offs. It also appeared to have better in-game\nassistance and game mechanics, while its improved graphics substantially\nincreased the quality of the user experience and almost eradicated VRISE. The\nresults substantially support the feasibility of the development of effective\nVR research and clinical software without the presence of VRISE during a\n60-minute VR session.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 14:55:57 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kourtesis", "Panagiotis", ""], ["Korre", "Danai", ""], ["Collina", "Simona", ""], ["Doumas", "Leonidas A. A.", ""], ["MacPherson", "Sarah E.", ""]]}, {"id": "2101.08235", "submitter": "Dae Hyun Kim", "authors": "Dae Hyun Kim, Vidya Setlur, Maneesh Agrawala", "title": "Towards Understanding How Readers Integrate Charts and Captions: A Case\n  Study with Line Charts", "comments": "To appear at CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445443", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Charts often contain visually prominent features that draw attention to\naspects of the data and include text captions that emphasize aspects of the\ndata. Through a crowdsourced study, we explore how readers gather takeaways\nwhen considering charts and captions together. We first ask participants to\nmark visually prominent regions in a set of line charts. We then generate text\ncaptions based on the prominent features and ask participants to report their\ntakeaways after observing chart-caption pairs. We find that when both the chart\nand caption describe a high-prominence feature, readers treat the doubly\nemphasized high-prominence feature as the takeaway; when the caption describes\na low-prominence chart feature, readers rely on the chart and report a\nhigher-prominence feature as the takeaway. We also find that external\ninformation that provides context, helps further convey the caption's message\nto the reader. We use these findings to provide guidelines for authoring\neffective chart-caption pairs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 18:11:35 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Kim", "Dae Hyun", ""], ["Setlur", "Vidya", ""], ["Agrawala", "Maneesh", ""]]}, {"id": "2101.08319", "submitter": "Malek Mouhoub", "authors": "Munira Al-Ageili and Malek Mouhoub", "title": "Communication Aid for Non-English Speaking Newcomers", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This research work is intended to assess the usability of Pictogram symbols\nand other visual symbols in an audio-visual strategy to facilitate and enhance\nthe use and learning of English as an additional language for Arabic-speaking\nSyrian refugees, with a potential for generalizing the process to speakers from\nother linguistic backgrounds. The adopted software for the project is\nPICTOPAGES, a versatile tool with 2,200 symbols, 78 animated symbols, and the\npotential for customization with photographs, thus augmenting its capability\nfor personalization and relevance. While PICTOPAGES is the intended basis for\nthis research, the concept and software will be adapted and modified as may be\nrequired. PICTOPAGES includes text, recorded speech, and symbols and is\ncurrently available for iPad. In the future, it may be adapted for use on\niPhone. A preliminary design using PICTOPAGES has been created for this\nresearch. The focus group includes, but is not limited to, newcomers who may\nhave limited to no English skills, limited resources, limited education, and\npotentially limited literacy in their native language, and perhaps high levels\nof distraction and frustration related to their recent experiences. Enhanced\ncommunication capability and confidence should enhance the participants\nemployment potential. Extensive interaction with respect to communication\nrequirements, selection or development of readily understandable symbols, and\nreal-world testing would be undertaken with an intended user group. A potential\nsubset of the focus group could involve members of the refugee community that,\nin addition to English language limitations, also have developmental or\nacquired disabilities that affect their ability to communicate verbally (per\nthe original intent of the software).\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 21:02:57 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Al-Ageili", "Munira", ""], ["Mouhoub", "Malek", ""]]}, {"id": "2101.08375", "submitter": "Mohammed Khwaja", "authors": "Svenja Pieritz, Mohammed Khwaja, A. Aldo Faisal, Aleksandar Matic", "title": "Personalised Recommendations in Mental Health Apps: The Impact of\n  Autonomy and Data Sharing", "comments": "To appear in the proceedings of the 2021 CHI Conference on Human\n  Factors in Computing Systems; 12 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent growth of digital interventions for mental well-being prompts a\ncall-to-arms to explore the delivery of personalised recommendations from a\nuser's perspective. In a randomised placebo study with a two-way factorial\ndesign, we analysed the difference between an autonomous user experience as\nopposed to personalised guidance, with respect to both users' preference and\ntheir actual usage of a mental well-being app. Furthermore, we explored users'\npreference in sharing their data for receiving personalised recommendations, by\njuxtaposing questionnaires and mobile sensor data. Interestingly, self-reported\nresults indicate the preference for personalised guidance, whereas behavioural\ndata suggests that a blend of autonomous choice and recommended activities\nresults in higher engagement. Additionally, although users reported a strong\npreference of filling out questionnaires instead of sharing their mobile data,\nthe data source did not have any impact on the actual app use. We discuss the\nimplications of our findings and provide takeaways for designers of mental\nwell-being applications.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 00:33:03 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Pieritz", "Svenja", ""], ["Khwaja", "Mohammed", ""], ["Faisal", "A. Aldo", ""], ["Matic", "Aleksandar", ""]]}, {"id": "2101.08419", "submitter": "Prerna Juneja", "authors": "Prerna Juneja, Tanushree Mitra", "title": "Auditing E-Commerce Platforms for Algorithmically Curated Vaccine\n  Misinformation", "comments": null, "journal-ref": "CHI Conference on Human Factors in Computing Systems 2021", "doi": "10.1145/3411764.3445250", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is a growing concern that e-commerce platforms are amplifying\nvaccine-misinformation. To investigate, we conduct two-sets of algorithmic\naudits for vaccine misinformation on the search and recommendation algorithms\nof Amazon -- world's leading e-retailer. First, we systematically audit\nsearch-results belonging to vaccine-related search-queries without logging into\nthe platform -- unpersonalized audits. We find 10.47% of search-results promote\nmisinformative health products. We also observe ranking-bias, with Amazon\nranking misinformative search-results higher than debunking search-results.\nNext, we analyze the effects of personalization due to account-history, where\nhistory is built progressively by performing various real-world user-actions,\nsuch as clicking a product. We find evidence of filter-bubble effect in\nAmazon's recommendations; accounts performing actions on misinformative\nproducts are presented with more misinformation compared to accounts performing\nactions on neutral and debunking products. Interestingly, once user clicks on a\nmisinformative product, homepage recommendations become more contaminated\ncompared to when user shows an intention to buy that product.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 03:16:29 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 20:15:18 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Juneja", "Prerna", ""], ["Mitra", "Tanushree", ""]]}, {"id": "2101.08621", "submitter": "Riku Arakawa", "authors": "Riku Arakawa and Hiromu Yakura", "title": "Mindless Attractor: A False-Positive Resistant Intervention for Drawing\n  Attention Using Auditory Perturbation", "comments": "To appear in ACM CHI Conference on Human Factors in Computing Systems\n  (CHI '21), May 8-13, 2021, Yokohama, Japan", "journal-ref": null, "doi": "10.1145/3411764.3445339", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explicitly alerting users is not always an optimal intervention, especially\nwhen they are not motivated to obey. For example, in video-based learning,\nlearners who are distracted from the video would not follow an alert asking\nthem to pay attention. Inspired by the concept of Mindless Computing, we\npropose a novel intervention approach, Mindless Attractor, that leverages the\nnature of human speech communication to help learners refocus their attention\nwithout relying on their motivation. Specifically, it perturbs the voice in the\nvideo to direct their attention without consuming their conscious awareness.\nOur experiments not only confirmed the validity of the proposed approach but\nalso emphasized its advantages in combination with a machine learning-based\nsensing module. Namely, it would not frustrate users even though the\nintervention is activated by false-positive detection of their attentive state.\nOur intervention approach can be a reliable way to induce behavioral change in\nhuman-AI symbiosis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 14:10:54 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Arakawa", "Riku", ""], ["Yakura", "Hiromu", ""]]}, {"id": "2101.08655", "submitter": "Leonardo Milhomem Franco Christino", "authors": "Leonardo Christino, Martha D. Ferreira, Asal Jalilvand and Fernando V.\n  Paulovich", "title": "Explainable Patterns: Going from Findings to Insights to Support Data\n  Analytics Democratization", "comments": "8 Figures, 10 pages, submitted to VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decades, massive efforts involving companies, non-profit\norganizations, governments, and others have been put into supporting the\nconcept of data democratization, promoting initiatives to educate people to\nconfront information with data. Although this represents one of the most\ncritical advances in our free world, access to data without concrete facts to\ncheck or the lack of an expert to help on understanding the existing patterns\nhampers its intrinsic value and lessens its democratization. So the benefits of\ngiving full access to data will only be impactful if we go a step further and\nsupport the Data Analytics Democratization, assisting users in transforming\nfindings into insights without the need of domain experts to promote\nunconstrained access to data interpretation and verification. In this paper, we\npresent Explainable Patterns (ExPatt), a new framework to support lay users in\nexploring and creating data storytellings, automatically generating plausible\nexplanations for observed or selected findings using an external (textual)\nsource of information, avoiding or reducing the need for domain experts. ExPatt\napplicability is confirmed via different use-cases involving world demographics\nindicators and Wikipedia as an external source of explanations, showing how it\ncan be used in practice towards the data analytics democratization.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2021 16:13:44 GMT"}], "update_date": "2021-01-22", "authors_parsed": [["Christino", "Leonardo", ""], ["Ferreira", "Martha D.", ""], ["Jalilvand", "Asal", ""], ["Paulovich", "Fernando V.", ""]]}, {"id": "2101.08846", "submitter": "Bryan Wang", "authors": "Bryan Wang, Mengyu Yang, Tovi Grossman", "title": "Soloist: Generating Mixed-Initiative Tutorials from Existing Guitar\n  Instructional Videos Through Audio Processing", "comments": "ACM CHI 2021 Camera-Ready, Single Column", "journal-ref": null, "doi": "10.1145/3411764.3445162", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning musical instruments using online instructional videos has become\nincreasingly prevalent. However, pre-recorded videos lack the instantaneous\nfeedback and personal tailoring that human tutors provide. In addition,\nexisting video navigations are not optimized for instrument learning, making\nthe learning experience encumbered. Guided by our formative interviews with\nguitar players and prior literature, we designed Soloist, a mixed-initiative\nlearning framework that automatically generates customizable curriculums from\noff-the-shelf guitar video lessons. Soloist takes raw videos as input and\nleverages deep-learning based audio processing to extract musical information.\nThis back-end processing is used to provide an interactive visualization to\nsupport effective video navigation and real-time feedback on the user's\nperformance, creating a guided learning experience. We demonstrate the\ncapabilities and specific use-cases of Soloist within the domain of learning\nelectric guitar solos using instructional YouTube videos. A remote user study,\nconducted to gather feedback from guitar players, shows encouraging results as\nthe users unanimously preferred learning with Soloist over unconverted\ninstructional videos.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 20:49:18 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Wang", "Bryan", ""], ["Yang", "Mengyu", ""], ["Grossman", "Tovi", ""]]}, {"id": "2101.08856", "submitter": "Zhilan Zhou", "authors": "Zhilan Zhou, Ximing Wen, Yue Wang, David Gotz", "title": "Modeling and Leveraging Analytic Focus During Exploratory Visual\n  Analysis", "comments": "Accepted to CHI2021", "journal-ref": null, "doi": "10.1145/3411764.3445674", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Visual analytics systems enable highly interactive exploratory data analysis.\nAcross a range of fields, these technologies have been successfully employed to\nhelp users learn from complex data. However, these same exploratory\nvisualization techniques make it easy for users to discover spurious findings.\nThis paper proposes new methods to monitor a user's analytic focus during\nvisual analysis of structured datasets and use it to surface relevant articles\nthat contextualize the visualized findings. Motivated by interactive analyses\nof electronic health data, this paper introduces a formal model of analytic\nfocus, a computational approach to dynamically update the focus model at the\ntime of user interaction, and a prototype application that leverages this model\nto surface relevant medical publications to users during visual analysis of a\nlarge corpus of medical records. Evaluation results with 24 users show that the\nmodeling approach has high levels of accuracy and is able to surface highly\nrelevant medical abstracts.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 21:18:33 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhou", "Zhilan", ""], ["Wen", "Ximing", ""], ["Wang", "Yue", ""], ["Gotz", "David", ""]]}, {"id": "2101.08886", "submitter": "Matteo Zallio Dr.", "authors": "Matteo Zallio, Paula Kelly, Barry Cryan, Damon Berry", "title": "A co-Design approach to develop a smart cooking appliance. Applying a\n  Domain Specific Language for a community supported appliance", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our environment, whether at work, in public spaces, or at home, is becoming\nmore connected, and increasingly responsive. Meal preparation even when it\ninvolves simply heating ready-made food can be perceived as a complex process\nfor people with disabilities. This research aimed to prototype, using a\nco-Design approach a Community Supported Appliance (CSA) by developing a Domain\nSpecific Language (DSL), precisely created for a semi-automated cooking\nprocess. The DSL was shaped and expressed in the idiom of the users and allowed\nthe CSA to support independence for users while performing daily cooking\nactivities.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 23:23:36 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 10:12:03 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 10:43:18 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zallio", "Matteo", ""], ["Kelly", "Paula", ""], ["Cryan", "Barry", ""], ["Berry", "Damon", ""]]}, {"id": "2101.09066", "submitter": "Luis Leiva", "authors": "Lukas Br\\\"uckner and Ioannis Arapakis and Luis A. Leiva", "title": "Query Abandonment Prediction with Recurrent Neural Models of Mouse\n  Cursor Movements", "comments": null, "journal-ref": "Proceedings of the 29th ACM Intl. Conf. on Information And\n  Knowledge Management (CIKM), 2020", "doi": "10.1145/3340531.3412126", "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most successful search queries do not result in a click if the user can\nsatisfy their information needs directly on the SERP. Modeling query\nabandonment in the absence of click-through data is challenging because search\nengines must rely on other behavioral signals to understand the underlying\nsearch intent. We show that mouse cursor movements make a valuable, low-cost\nbehavioral signal that can discriminate good and bad abandonment. We model\nmouse movements on SERPs using recurrent neural nets and explore several data\nrepresentations that do not rely on expensive hand-crafted features and do not\ndepend on a particular SERP structure. We also experiment with data resampling\nand augmentation techniques that we adopt for sequential data. Our results can\nhelp search providers to gauge user satisfaction for queries without clicks and\nultimately contribute to a better understanding of search engine performance.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 11:57:04 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Br\u00fcckner", "Lukas", ""], ["Arapakis", "Ioannis", ""], ["Leiva", "Luis A.", ""]]}, {"id": "2101.09086", "submitter": "Ioannis Arapakis", "authors": "Ioannis Arapakis, Souneil Park, Martin Pielot", "title": "Impact of Response Latency on User Behaviour in Mobile Web Search", "comments": "In Proceedings of the 2021 ACM SIGIR Conference on Human Information\n  Interaction and Retrieval (CHIIR '21), March 14-19, 2021, Canberra, Australia", "journal-ref": null, "doi": "10.1145/3406522.3446038", "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the efficiency and effectiveness of search systems have both\nbeen of great interest to the information retrieval community. However, an\nin-depth analysis of the interaction between the response latency and users'\nsubjective search experience in the mobile setting has been missing so far. To\naddress this gap, we conduct a controlled study that aims to reveal how\nresponse latency affects mobile web search. Our preliminary results indicate\nthat mobile web search users are four times more tolerant to response latency\nreported for desktop web search users. However, when exceeding a certain\nthreshold of 7-10 sec, the delays have a sizeable impact and users report\nfeeling significantly more tensed, tired, terrible, frustrated and sluggish,\nall which contribute to a worse subjective user experience.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:43:46 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Arapakis", "Ioannis", ""], ["Park", "Souneil", ""], ["Pielot", "Martin", ""]]}, {"id": "2101.09087", "submitter": "Ioannis Arapakis", "authors": "Luis A. Leiva, Ioannis Arapakis, Costas Iordanou", "title": "My Mouse, My Rules: Privacy Issues of Behavioral User Profiling via\n  Mouse Tracking", "comments": "In Proceedings of the 2021 ACM SIGIR Conference on Human Information\n  Interaction and Retrieval (CHIIR '21), March 14-19, 2021, Canberra, Australia", "journal-ref": null, "doi": "10.1145/3406522.3446011", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to stir debate about a disconcerting privacy issue on web\nbrowsing that could easily emerge because of unethical practices and\nuncontrolled use of technology. We demonstrate how straightforward is to\ncapture behavioral data about the users at scale, by unobtrusively tracking\ntheir mouse cursor movements, and predict user's demographics information with\nreasonable accuracy using five lines of code. Based on our results, we propose\nan adversarial method to mitigate user profiling techniques that make use of\nmouse cursor tracking, such as the recurrent neural net we analyze in this\npaper. We also release our data and a web browser extension that implements our\nadversarial method, so that others can benefit from this work in practice.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 12:49:03 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Leiva", "Luis A.", ""], ["Arapakis", "Ioannis", ""], ["Iordanou", "Costas", ""]]}, {"id": "2101.09138", "submitter": "Mawulolo Ameko", "authors": "Mawulolo K. Ameko, Sonia Baee, Laura E. Barnes", "title": "LonelyText: A Short Messaging Based Classification of Loneliness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loneliness does not only have emotional implications on a person but also on\nhis/her well-being. The study of loneliness has been challenging and largely\ninconclusive in findings because of the several factors that might correlate to\nthe phenomenon. We present one approach to predicting this event by discovering\npatterns of language associated with loneliness. Our results show insights and\npromising directions for mining text from instant messaging to predict\nloneliness.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 14:59:45 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Ameko", "Mawulolo K.", ""], ["Baee", "Sonia", ""], ["Barnes", "Laura E.", ""]]}, {"id": "2101.09157", "submitter": "Daniel Buschek", "authors": "Daniel Buschek, Martin Z\\\"urn, Malin Eiband", "title": "The Impact of Multiple Parallel Phrase Suggestions on Email Input and\n  Composition Behaviour of Native and Non-Native English Writers", "comments": "21 pages, 4 figures, ACM CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445372", "report-no": null, "categories": "cs.HC cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an in-depth analysis of the impact of multi-word suggestion\nchoices from a neural language model on user behaviour regarding input and text\ncomposition in email writing. Our study for the first time compares different\nnumbers of parallel suggestions, and use by native and non-native English\nwriters, to explore a trade-off of \"efficiency vs ideation\", emerging from\nrecent literature. We built a text editor prototype with a neural language\nmodel (GPT-2), refined in a prestudy with 30 people. In an online study\n(N=156), people composed emails in four conditions (0/1/3/6 parallel\nsuggestions). Our results reveal (1) benefits for ideation, and costs for\nefficiency, when suggesting multiple phrases; (2) that non-native speakers\nbenefit more from more suggestions; and (3) further insights into behaviour\npatterns. We discuss implications for research, the design of interactive\nsuggestion systems, and the vision of supporting writers with AI instead of\nreplacing them.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:32:32 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Buschek", "Daniel", ""], ["Z\u00fcrn", "Martin", ""], ["Eiband", "Malin", ""]]}, {"id": "2101.09161", "submitter": "Jonas Oppenlaender", "authors": "Jonas Oppenlaender, Elina Kuosmanen, Andr\\'es Lucero, Simo Hosio", "title": "Hardhats and Bungaloos: Comparing Crowdsourced Design Feedback with Peer\n  Design Feedback in the Classroom", "comments": "14 pages. CHI Conference on Human Factors in Computing Systems (CHI\n  '21), May 8-13, 2021", "journal-ref": null, "doi": "10.1145/3411764.3445380", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feedback is an important aspect of design education, and crowdsourcing has\nemerged as a convenient way to obtain feedback at scale. In this paper, we\ninvestigate how crowdsourced design feedback compares to peer design feedback\nwithin a design-oriented HCI class and across two metrics: perceived quality\nand perceived fairness. We also examine the perceived monetary value of\ncrowdsourced feedback, which provides an interesting contrast to the typical\nrequester-centric view of the value of labor on crowdsourcing platforms. Our\nresults reveal that the students (N=106) perceived the crowdsourced design\nfeedback as inferior to peer design feedback in multiple ways. However, they\nalso identified various positive aspects of the online crowds that peers cannot\nprovide. We discuss the meaning of the findings and provide suggestions for\nteachers in HCI and other researchers interested in crowd feedback systems on\nusing crowds as a potential complement to peers.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jan 2021 08:51:12 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Oppenlaender", "Jonas", ""], ["Kuosmanen", "Elina", ""], ["Lucero", "Andr\u00e9s", ""], ["Hosio", "Simo", ""]]}, {"id": "2101.09176", "submitter": "Luis Leiva", "authors": "Luis A. Leiva, Yunfei Xue, Avya Bansal, Hamed R. Tavakoli,\n  Tu\\u{g}\\c{c}e K\\\"oro\\u{g}lu, Niraj R. Dayama, Antti Oulasvirta", "title": "Understanding Visual Saliency in Mobile User Interfaces", "comments": null, "journal-ref": "Proceedings of the 22nd Intl. Conf. on Human-Computer Interaction\n  with Mobile Devices and Services (MobileHCI), 2020", "doi": "10.1145/3379503.3403557", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For graphical user interface (UI) design, it is important to understand what\nattracts visual attention. While previous work on saliency has focused on\ndesktop and web-based UIs, mobile app UIs differ from these in several\nrespects. We present findings from a controlled study with 30 participants and\n193 mobile UIs. The results speak to a role of expectations in guiding where\nusers look at. Strong bias toward the top-left corner of the display, text, and\nimages was evident, while bottom-up features such as color or size affected\nsaliency less. Classic, parameter-free saliency models showed a weak fit with\nthe data, and data-driven models improved significantly when trained\nspecifically on this dataset (e.g., NSS rose from 0.66 to 0.84). We also\nrelease the first annotated dataset for investigating visual saliency in mobile\nUIs.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 15:45:13 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Leiva", "Luis A.", ""], ["Xue", "Yunfei", ""], ["Bansal", "Avya", ""], ["Tavakoli", "Hamed R.", ""], ["K\u00f6ro\u011flu", "Tu\u011f\u00e7e", ""], ["Dayama", "Niraj R.", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "2101.09231", "submitter": "Fabio Valerio Massoli", "authors": "Donato Cafarelli, Fabio Valerio Massoli, Fabrizio Falchi, Claudio\n  Gennaro, Giuseppe Amato", "title": "Expression Recognition Analysis in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Facial Expression Recognition(FER) is one of the most important topic in\nHuman-Computer interactions(HCI). In this work we report details and\nexperimental results about a facial expression recognition method based on\nstate-of-the-art methods. We fine-tuned a SeNet deep learning architecture\npre-trained on the well-known VGGFace2 dataset, on the AffWild2 facial\nexpression recognition dataset. The main goal of this work is to define a\nbaseline for a novel method we are going to propose in the near future. This\npaper is also required by the Affective Behavior Analysis in-the-wild (ABAW)\ncompetition in order to evaluate on the test set this approach. The results\nreported here are on the validation set and are related on the Expression\nChallenge part (seven basic emotion recognition) of the competition. We will\nupdate them as soon as the actual results on the test set will be published on\nthe leaderboard.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 17:28:31 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Cafarelli", "Donato", ""], ["Massoli", "Fabio Valerio", ""], ["Falchi", "Fabrizio", ""], ["Gennaro", "Claudio", ""], ["Amato", "Giuseppe", ""]]}, {"id": "2101.09301", "submitter": "Ting Wang", "authors": "Xinyang Zhang, Ren Pang, Shouling Ji, Fenglong Ma, Ting Wang", "title": "i-Algebra: Towards Interactive Interpretability of Deep Neural Networks", "comments": "Accepted by the 35th AAAI Conference on Artificial Intelligence (AAAI\n  '21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing explanations for deep neural networks (DNNs) is essential for their\nuse in domains wherein the interpretability of decisions is a critical\nprerequisite. Despite the plethora of work on interpreting DNNs, most existing\nsolutions offer interpretability in an ad hoc, one-shot, and static manner,\nwithout accounting for the perception, understanding, or response of end-users,\nresulting in their poor usability in practice. In this paper, we argue that DNN\ninterpretability should be implemented as the interactions between users and\nmodels. We present i-Algebra, a first-of-its-kind interactive framework for\ninterpreting DNNs. At its core is a library of atomic, composable operators,\nwhich explain model behaviors at varying input granularity, during different\ninference stages, and from distinct interpretation perspectives. Leveraging a\ndeclarative query language, users are enabled to build various analysis tools\n(e.g., \"drill-down\", \"comparative\", \"what-if\" analysis) via flexibly composing\nsuch operators. We prototype i-Algebra and conduct user studies in a set of\nrepresentative analysis tasks, including inspecting adversarial inputs,\nresolving model inconsistency, and cleansing contaminated data, all\ndemonstrating its promising usability.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 19:22:57 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Zhang", "Xinyang", ""], ["Pang", "Ren", ""], ["Ji", "Shouling", ""], ["Ma", "Fenglong", ""], ["Wang", "Ting", ""]]}, {"id": "2101.09348", "submitter": "Franklin Mingzhe Li", "authors": "Franklin Mingzhe Li, Di Laura Chen, Mingming Fan, Khai N. Truong", "title": "\"I Choose Assistive Devices That Save My Face\" A Study on Perceptions of\n  Accessibility and Assistive Technology Use Conducted in China", "comments": "Proceedings of the 2021 CHI Conference on Human Factors in Computing\n  Systems (CHI '21)", "journal-ref": null, "doi": "10.1145/3411764.3445321", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the potential benefits of assistive technologies (ATs) for people\nwith various disabilities, only around 7% of Chinese with disabilities have had\nan opportunity to use ATs. Even for those who have used ATs, the abandonment\nrate was high. Although China has the world's largest population with\ndisabilities, prior research exploring how ATs are used and perceived, and why\nATs are abandoned have been conducted primarily in North America and Europe. In\nthis paper, we present an interview study conducted in China with 26 people\nwith various disabilities to understand their practices, challenges,\nperceptions, and misperceptions of using ATs. From the study, we learned about\nfactors that influence AT adoption practices (e.g., misuse of accessible\ninfrastructure, issues with replicating existing commercial ATs), challenges\nusing ATs in social interactions (e.g., Chinese stigma), and misperceptions\nabout ATs (e.g., ATs should overcome inaccessible social infrastructures).\nInformed by the findings, we derive a set of design considerations to bridge\nthe existing gaps in AT design (e.g., manual vs. electronic ATs) and to improve\nATs' social acceptability in China.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2021 22:06:29 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Li", "Franklin Mingzhe", ""], ["Chen", "Di Laura", ""], ["Fan", "Mingming", ""], ["Truong", "Khai N.", ""]]}, {"id": "2101.09385", "submitter": "Joshua Kroll", "authors": "Joshua A. Kroll", "title": "Outlining Traceability: A Principle for Operationalizing Accountability\n  in Computing Systems", "comments": "To be published in the Proceedings of the 2021 ACM Conference on\n  Fairness, Accountability, and Transparency (FAccT'21)", "journal-ref": null, "doi": "10.1145/3442188.3445937", "report-no": null, "categories": "cs.CY cs.AI cs.GL cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accountability is widely understood as a goal for well governed computer\nsystems, and is a sought-after value in many governance contexts. But how can\nit be achieved? Recent work on standards for governable artificial intelligence\nsystems offers a related principle: traceability. Traceability requires\nestablishing not only how a system worked but how it was created and for what\npurpose, in a way that explains why a system has particular dynamics or\nbehaviors. It connects records of how the system was constructed and what the\nsystem did mechanically to the broader goals of governance, in a way that\nhighlights human understanding of that mechanical operation and the decision\nprocesses underlying it. We examine the various ways in which the principle of\ntraceability has been articulated in AI principles and other policy documents\nfrom around the world, distill from these a set of requirements on software\nsystems driven by the principle, and systematize the technologies available to\nmeet those requirements. From our map of requirements to supporting tools,\ntechniques, and procedures, we identify gaps and needs separating what\ntraceability requires from the toolbox available for practitioners. This map\nreframes existing discussions around accountability and transparency, using the\nprinciple of traceability to show how, when, and why transparency can be\ndeployed to serve accountability goals and thereby improve the normative\nfidelity of systems and their development processes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 00:13:20 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Kroll", "Joshua A.", ""]]}, {"id": "2101.09498", "submitter": "Danding Wang", "authors": "Danding Wang, Wencan Zhang and Brian Y. Lim", "title": "Show or Suppress? Managing Input Uncertainty in Machine Learning Model\n  Explanations", "comments": "to be published in Artificial Intelligence Special Issue on\n  Explainable Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature attribution is widely used in interpretable machine learning to\nexplain how influential each measured input feature value is for an output\ninference. However, measurements can be uncertain, and it is unclear how the\nawareness of input uncertainty can affect the trust in explanations. We propose\nand study two approaches to help users to manage their perception of\nuncertainty in a model explanation: 1) transparently show uncertainty in\nfeature attributions to allow users to reflect on, and 2) suppress attribution\nto features with uncertain measurements and shift attribution to other features\nby regularizing with an uncertainty penalty. Through simulation experiments,\nqualitative interviews, and quantitative user evaluations, we identified the\nbenefits of moderately suppressing attribution uncertainty, and concerns\nregarding showing attribution uncertainty. This work adds to the understanding\nof handling and communicating uncertainty for model interpretability.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 13:10:48 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wang", "Danding", ""], ["Zhang", "Wencan", ""], ["Lim", "Brian Y.", ""]]}, {"id": "2101.09576", "submitter": "Efe Bozkir", "authors": "Hong Gao, Efe Bozkir, Lisa Hasenbein, Jens-Uwe Hahn, Richard\n  G\\\"ollner, Enkelejda Kasneci", "title": "Digital Transformations of Classrooms in Virtual Reality", "comments": "CHI Conference on Human Factors in Computing Systems (CHI '21)", "journal-ref": null, "doi": "10.1145/3411764.3445596", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapid developments in consumer-level head-mounted displays and computer\ngraphics, immersive VR has the potential to take online and remote learning\ncloser to real-world settings. However, the effects of such digital\ntransformations on learners, particularly for VR, have not been evaluated in\ndepth. This work investigates the interaction-related effects of sitting\npositions of learners, visualization styles of peer-learners and teachers, and\nhand-raising behaviors of virtual peer-learners on learners in an immersive VR\nclassroom, using eye tracking data. Our results indicate that learners sitting\nin the back of the virtual classroom may have difficulties extracting\ninformation. Additionally, we find indications that learners engage with\nlectures more efficiently if virtual avatars are visualized with realistic\nstyles. Lastly, we find different eye movement behaviors towards different\nperformance levels of virtual peer-learners, which should be investigated\nfurther. Our findings present an important baseline for design decisions for VR\nclassrooms.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jan 2021 20:15:17 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 14:55:33 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Gao", "Hong", ""], ["Bozkir", "Efe", ""], ["Hasenbein", "Lisa", ""], ["Hahn", "Jens-Uwe", ""], ["G\u00f6llner", "Richard", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "2101.09648", "submitter": "Maria De-Arteaga", "authors": "Maria De-Arteaga, Artur Dubrawski, Alexandra Chouldechova", "title": "Leveraging Expert Consistency to Improve Algorithmic Decision Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their promise of superior predictive power relative to human\nassessment, machine learning models are increasingly being used to support\nhigh-stakes decisions. However, the nature of the labels available for training\nthese models often hampers the usefulness of predictive models for decision\nsupport. In this paper, we explore the use of historical expert decisions as a\nrich--yet imperfect--source of information, and we show that it can be\nleveraged to mitigate some of the limitations of learning from observed labels\nalone. We consider the problem of estimating expert consistency indirectly when\neach case in the data is assessed by a single expert, and propose influence\nfunctions based methodology as a solution to this problem. We then incorporate\nthe estimated expert consistency into the predictive model meant for decision\nsupport through an approach we term label amalgamation. This allows the machine\nlearning models to learn from experts in instances where there is expert\nconsistency, and learn from the observed labels elsewhere. We show how the\nproposed approach can help mitigate common challenges of learning from observed\nlabels alone, reducing the gap between the construct that the algorithm\noptimizes for and the construct of interest to experts. After providing\nintuition and theoretical results, we present empirical results in the context\nof child maltreatment hotline screenings. Here, we find that (1) there are\nhigh-risk cases whose risk is considered by the experts but not wholly captured\nin the target labels used to train a deployed model, and (2) the proposed\napproach improves recall for these cases.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 05:40:29 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["De-Arteaga", "Maria", ""], ["Dubrawski", "Artur", ""], ["Chouldechova", "Alexandra", ""]]}, {"id": "2101.09824", "submitter": "Harini Suresh", "authors": "Harini Suresh, Steven R. Gomez, Kevin K. Nam, Arvind Satyanarayan", "title": "Beyond Expertise and Roles: A Framework to Characterize the Stakeholders\n  of Interpretable Machine Learning and their Needs", "comments": "In CHI Conference on Human Factors in Computing Systems (CHI '21)", "journal-ref": null, "doi": "10.1145/3411764.3445088", "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure accountability and mitigate harm, it is critical that diverse\nstakeholders can interrogate black-box automated systems and find information\nthat is understandable, relevant, and useful to them. In this paper, we eschew\nprior expertise- and role-based categorizations of interpretability\nstakeholders in favor of a more granular framework that decouples stakeholders'\nknowledge from their interpretability needs. We characterize stakeholders by\ntheir formal, instrumental, and personal knowledge and how it manifests in the\ncontexts of machine learning, the data domain, and the general milieu. We\nadditionally distill a hierarchical typology of stakeholder needs that\ndistinguishes higher-level domain goals from lower-level interpretability\ntasks. In assessing the descriptive, evaluative, and generative powers of our\nframework, we find our more nuanced treatment of stakeholders reveals gaps and\nopportunities in the interpretability literature, adds precision to the design\nand comparison of user studies, and facilitates a more reflexive approach to\nconducting this research.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 23:21:21 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Suresh", "Harini", ""], ["Gomez", "Steven R.", ""], ["Nam", "Kevin K.", ""], ["Satyanarayan", "Arvind", ""]]}, {"id": "2101.09841", "submitter": "Leslie Tiong", "authors": "Leslie Ching Ow Tiong and HeeJeong Jasmine Lee", "title": "E-cheating Prevention Measures: Detection of Cheating at Online\n  Examinations Using Deep Learning Approach -- A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study addresses the current issues in online assessments, which are\nparticularly relevant during the Covid-19 pandemic. Our focus is on academic\ndishonesty associated with online assessments. We investigated the prevalence\nof potential e-cheating using a case study and propose preventive measures that\ncould be implemented. We have utilised an e-cheating intelligence agent as a\nmechanism for detecting the practices of online cheating, which is composed of\ntwo major modules: the internet protocol (IP) detector and the behaviour\ndetector. The intelligence agent monitors the behaviour of the students and has\nthe ability to prevent and detect any malicious practices. It can be used to\nassign randomised multiple-choice questions in a course examination and be\nintegrated with online learning programs to monitor the behaviour of the\nstudents. The proposed method was tested on various data sets confirming its\neffectiveness. The results revealed accuracies of 68% for the deep neural\nnetwork (DNN); 92% for the long-short term memory (LSTM); 95% for the\nDenseLSTM; and, 86% for the recurrent neural network (RNN).\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 01:09:54 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Tiong", "Leslie Ching Ow", ""], ["Lee", "HeeJeong Jasmine", ""]]}, {"id": "2101.09869", "submitter": "Lelia Marie Hampton", "authors": "Lelia Marie Hampton", "title": "Black Feminist Musings on Algorithmic Oppression", "comments": "12 pages, accepted to ACM Conference on Fairness, Accountability, and\n  Transparency 2021", "journal-ref": null, "doi": "10.1145/3442188.3445929", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper unapologetically reflects on the critical role that Black feminism\ncan and should play in abolishing algorithmic oppression. Positioning\nalgorithmic oppression in the broader field of feminist science and technology\nstudies, I draw upon feminist philosophical critiques of science and technology\nand discuss histories and continuities of scientific oppression against\nhistorically marginalized people. Moreover, I examine the concepts of\ninvisibility and hypervisibility in oppressive technologies a l\\'a the\ncanonical double bind. Furthermore, I discuss what it means to call for\ndiversity as a solution to algorithmic violence, and I critique dialectics of\nthe fairness, accountability, and transparency community. I end by inviting you\nto envision and imagine the struggle to abolish algorithmic oppression by\nabolishing oppressive systems and shifting algorithmic development practices,\nincluding engaging our communities in scientific processes, centering\nmarginalized communities in design, and consensual data and algorithmic\npractices.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 03:04:05 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 01:54:26 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Hampton", "Lelia Marie", ""]]}, {"id": "2101.09978", "submitter": "Tianming Zhao", "authors": "Tianming Zhao (1), Chunyang Chen (2), Yuanning Liu (1), Xiaodong Zhu\n  (1) ((1) Jilin University, (2) Monash University)", "title": "GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial\n  Networks", "comments": "13 pages, 10 figures, accepted for publication at ICSE2021 Technical\n  Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical User Interface (GUI) is ubiquitous in almost all modern desktop\nsoftware, mobile applications, and online websites. A good GUI design is\ncrucial to the success of the software in the market, but designing a good GUI\nwhich requires much innovation and creativity is difficult even to well-trained\ndesigners. Besides, the requirement of the rapid development of GUI design also\naggravates designers' working load. So, the availability of various automated\ngenerated GUIs can help enhance the design personalization and specialization\nas they can cater to the taste of different designers. To assist designers, we\ndevelop a model GUIGAN to automatically generate GUI designs. Different from\nconventional image generation models based on image pixels, our GUIGAN is to\nreuse GUI components collected from existing mobile app GUIs for composing a\nnew design that is similar to natural-language generation. Our GUIGAN is based\non SeqGAN by modeling the GUI component style compatibility and GUI structure.\nThe evaluation demonstrates that our model significantly outperforms the best\nof the baseline methods by 30.77% in Frechet Inception distance (FID) and\n12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we\nprovide initial evidence of the usefulness of our approach for generating\nacceptable brand new GUI designs.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 09:42:58 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 04:42:42 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Zhao", "Tianming", "", "Jilin University"], ["Chen", "Chunyang", "", "Monash University"], ["Liu", "Yuanning", "", "Jilin University"], ["Zhu", "Xiaodong", "", "Jilin University"]]}, {"id": "2101.09999", "submitter": "Matteo Zallio Dr.", "authors": "Matteo Zallio", "title": "Democratizing information visualization. A study to map the value of\n  graphic design to easier knowledge transfer of scientific research", "comments": "13 pages, 2 images", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual representations are becoming important in science communication and\neducation. This explorative study investigates the perception of STEM\nresearchers, without any specific visual design background, and the value of\nvisual representations as tools to support the communication of technical and\nscientific knowledge among academics and a wider non-technical community. Early\nfindings show that visual representations can positively support scientists to\nshare research outcomes in a more compelling, visually clear, and impactful\nmanner, reaching a wider audience across different disciplines.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 10:29:17 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 23:36:19 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 10:43:33 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zallio", "Matteo", ""]]}, {"id": "2101.10020", "submitter": "Jichen Zhu", "authors": "Jichen Zhu, Diane H. Dallal, Robert C. Gray, Jennifer Villareale,\n  Santiago Onta\\~n\\'on, Evan M. Forman, Danielle Arigo", "title": "Personalization Paradox in Behavior Change Apps: Lessons from a Social\n  Comparison-Based Personalized App for Physical Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social comparison-based features are widely used in social computing apps.\nHowever, most existing apps are not grounded in social comparison theories and\ndo not consider individual differences in social comparison preferences and\nreactions. This paper is among the first to automatically personalize social\ncomparison targets. In the context of an m-health app for physical activity, we\nuse artificial intelligence (AI) techniques of multi-armed bandits. Results\nfrom our user study (n=53) indicate that there is some evidence that motivation\ncan be increased using the AI-based personalization of social comparison. The\ndetected effects achieved small-to-moderate effect sizes, illustrating the\nreal-world implications of the intervention for enhancing motivation and\nphysical activity. In addition to design implications for social comparison\nfeatures in social apps, this paper identified the personalization paradox, the\nconflict between user modeling and adaptation, as a key design challenge of\npersonalized applications for behavior change. Additionally, we propose\nresearch directions to mitigate this Personalization Paradox.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 11:39:32 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 14:24:12 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Zhu", "Jichen", ""], ["Dallal", "Diane H.", ""], ["Gray", "Robert C.", ""], ["Villareale", "Jennifer", ""], ["Onta\u00f1\u00f3n", "Santiago", ""], ["Forman", "Evan M.", ""], ["Arigo", "Danielle", ""]]}, {"id": "2101.10032", "submitter": "Yi-Chi Liao", "authors": "Yi-Chi Liao", "title": "Computational Workflows for Designing Input Devices", "comments": "CHI '21 Extended Abstracts, 6 pages", "journal-ref": null, "doi": "10.1145/3411763.3443428", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Input devices, such as buttons and sliders, are the foundation of any\ninterface. The typical user-centered design workflow requires the developers\nand users to go through many iterations of design, implementation, and\nanalysis. The procedure is inefficient, and human decisions highly bias the\nresults. While computational methods are used to assist various design tasks,\nthere has not been any holistic approach to automate the design of input\ncomponents. My thesis proposed a series of Computational Input Design\nworkflows: I envision a sample-efficient multi-objective optimization algorithm\nthat cleverly selects design instances, which are instantly deployed on\nphysical simulators. A meta-reinforcement learning user model then simulates\nthe user behaviors when using the design instance upon the simulators. The new\nworkflows derive Pareto-optimal designs with high efficiency and automation. I\ndemonstrate designing a push-button via the proposed methods. The resulting\ndesigns outperform the known baselines. The Computational Input Design process\ncan be generalized to other devices, such as joystick, touchscreen, mouse,\ncontroller, etc.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 12:05:44 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 22:36:09 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 21:35:35 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Liao", "Yi-Chi", ""]]}, {"id": "2101.10140", "submitter": "Sumit Mehra", "authors": "Sumit Mehra, Jantine van den Helder, Ben J.A. Kr\\\"ose, Raoul H.H.\n  Engelbert, Peter J.M. Weijs, Bart Visser", "title": "Predicting Exercise Adherence and Physical Activity in Older Adults\n  Based on Tablet Engagement: A Post-hoc Study", "comments": "this is a long paper (12 excluding references). The short paper has\n  been accepted at 16TH INTERNATIONAL CONFERENCE ON PERSUASIVE TECHNOLOGY,\n  12-14 APRIL 2021, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sufficient physical activity can prolong the ability of older adults to live\ninde-pendently. Community-based exercise programs can be enhanced by regularly\nperforming exercises at home. To support such a home-based exercise program, a\nblended intervention was developed that combined the use of a tablet\napplication with a personal coach. The purpose of the current study was to\nexplore to which extent tablet engagement predicted exercise adherence and\nphysical activity. The results show that older adults (n=133; M=71 years of\nage) that participated 6 months in a randomized controlled trial, performed at\naverage 12 home-based ex-ercised per week and exercised on average 3 days per\nweek, thereby meeting WHO guidelines. They used the tablet app on average 7\ntimes per week. Multiple linear regressions revealed that the use of the app\nstatistically predicted the num-ber of exercises that were performed and the\nnumber of exercise days. Physical activity, however, did not increase and also\ncould not be predicted by exercise frequency or app use. We conclude that\nengagement with a tablet can contribute to sustained exercise behavior.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 14:43:40 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mehra", "Sumit", ""], ["Helder", "Jantine van den", ""], ["Kr\u00f6se", "Ben J. A.", ""], ["Engelbert", "Raoul H. H.", ""], ["Weijs", "Peter J. M.", ""], ["Visser", "Bart", ""]]}, {"id": "2101.10191", "submitter": "Eric Larson", "authors": "Chatchai Wangwiwattana, Sunjoli Aggarwal, Eric C. Larson", "title": "Writers Gonna Wait: The Effectiveness of Notifications to Initiate\n  Aversive Action in Writing Procrastination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper evaluates the use of notifications to reduce\naversive-task-procrastination by helping initiate action. Specifically, we\nfocus on aversion to graded writing tasks. We evaluate software designs\ncommonly used by behavior change applications, such as goal setting and action\nsupport systems. We conduct a two-phase control trial experiment with 21\ncollege students tasked to write two 3000-word writing assignments (14 students\nfully completed the experiment). Participants use a customized text editor\ndesigned to continuously collect writing behavior. The results from the study\nreveal that notifications have minimal effect in encouraging users to get\nstarted. They can also increase negative effects on participants. Other\ntechniques, such as eliminating distraction and showing simple writing\nstatistics, yield higher satisfaction among participants as they complete the\nwriting task. Furthermore, the incorporation of text mining decreases aversion\nto the task and helps participants overcome writer's block. Finally, we discuss\nlessons learned from our evaluation that help quantify the difficulty of\nbehavior change for writing procrastination, with emphasis on goals for the HCI\ncommunity.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 15:53:54 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Wangwiwattana", "Chatchai", ""], ["Aggarwal", "Sunjoli", ""], ["Larson", "Eric C.", ""]]}, {"id": "2101.10245", "submitter": "Eric Larson", "authors": "Nibhrat Lohia, Raunak Mundada, Arya D. McCarthy, Eric C. Larson", "title": "AirWare: Utilizing Embedded Audio and Infrared Signals for In-Air\n  Hand-Gesture Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce AirWare, an in-air hand-gesture recognition system that uses the\nalready embedded speaker and microphone in most electronic devices, together\nwith embedded infrared proximity sensors. Gestures identified by AirWare are\nperformed in the air above a touchscreen or a mobile phone. AirWare utilizes\nconvolutional neural networks to classify a large vocabulary of hand gestures\nusing multi-modal audio Doppler signatures and infrared (IR) sensor\ninformation. As opposed to other systems which use high frequency Doppler\nradars or depth cameras to uniquely identify in-air gestures, AirWare does not\nrequire any external sensors. In our analysis, we use openly available APIs to\ninterface with the Samsung Galaxy S5 audio and proximity sensors for data\ncollection. We find that AirWare is not reliable enough for a deployable\ninteraction system when trying to classify a gesture set of 21 gestures, with\nan average true positive rate of only 50.5% per gesture. To improve\nperformance, we train AirWare to identify subsets of the 21 gestures vocabulary\nbased on possible usage scenarios. We find that AirWare can identify three\ngesture sets with average true positive rate greater than 80% using 4--7\ngestures per set, which comprises a vocabulary of 16 unique in-air gestures.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 17:20:23 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Lohia", "Nibhrat", ""], ["Mundada", "Raunak", ""], ["McCarthy", "Arya D.", ""], ["Larson", "Eric C.", ""]]}, {"id": "2101.10367", "submitter": "Samantha Robertson", "authors": "Samantha Robertson, Tonya Nguyen, Niloufar Salehi", "title": "Modeling Assumptions Clash with the Real World: Transparency, Equity,\n  and Community Challenges for Student Assignment Algorithms", "comments": null, "journal-ref": "CHI Conference on Human Factors in Computing Systems (CHI '21),\n  May 8--13, 2021, Yokohama, Japan", "doi": "10.1145/3411764.3445748", "report-no": null, "categories": "cs.HC cs.CY cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Across the United States, a growing number of school districts are turning to\nmatching algorithms to assign students to public schools. The designers of\nthese algorithms aimed to promote values such as transparency, equity, and\ncommunity in the process. However, school districts have encountered practical\nchallenges in their deployment. In fact, San Francisco Unified School District\nvoted to stop using and completely redesign their student assignment algorithm\nbecause it was not promoting educational equity in practice. We analyze this\nsystem using a Value Sensitive Design approach and find that one reason values\nare not met in practice is that the system relies on modeling assumptions about\nfamilies' priorities, constraints, and goals that clash with the real world.\nThese assumptions overlook the complex barriers to ideal participation that\nmany families face, particularly because of socioeconomic inequalities. We\nargue that direct, ongoing engagement with stakeholders is central to aligning\nalgorithmic values with real world conditions. In doing so we must broaden how\nwe evaluate algorithms while recognizing the limitations of purely algorithmic\nsolutions in addressing complex socio-political problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jan 2021 19:29:39 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Robertson", "Samantha", ""], ["Nguyen", "Tonya", ""], ["Salehi", "Niloufar", ""]]}, {"id": "2101.10495", "submitter": "Jayam Umesh Patel", "authors": "Jayam Patel, Tyagaraja Ramaswamy, Zhi Li, and Carlo Pinciroli", "title": "Transparency in Multi-Human Multi-Robot Interaction", "comments": "8 pages, submitted to IEEE Robotics and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transparency is a key factor in improving the performance of human-robot\ninteraction. A transparent interface allows humans to be aware of the state of\na robot and to assess the progress of the tasks at hand. When multi-robot\nsystems are involved, transparency is an even greater challenge, due to the\nlarger number of variables affecting the behavior of the robots as a whole.\nSignificant effort has been devoted to studying transparency when single\noperators interact with multiple robots. However, studies on transparency that\nfocus on multiple human operators interacting with a multi-robot systems are\nlimited. This paper aims to fill this gap by presenting a human-swarm\ninteraction interface with graphical elements that can be enabled and disabled.\nThrough this interface, we study which graphical elements are contribute to\ntransparency by comparing four \"transparency modes\": (i) no transparency (no\noperator receives information from the robots), (ii) central transparency (the\noperators receive information only relevant to their personal task), (iii)\nperipheral transparency (the operators share information on each others'\ntasks), and (iv) mixed transparency (both central and peripheral). We report\nthe results in terms of awareness, trust, and workload of a user study\ninvolving 18 participants engaged in a complex multi-robot task.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 00:13:58 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 20:05:03 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Patel", "Jayam", ""], ["Ramaswamy", "Tyagaraja", ""], ["Li", "Zhi", ""], ["Pinciroli", "Carlo", ""]]}, {"id": "2101.10534", "submitter": "Chirag Gupta", "authors": "Chirag Gupta", "title": "Modern Machine and Deep Learning Systems as a way to achieve\n  Man-Computer Symbiosis", "comments": "8 pages, 1 figure. Collaboration ongoing with coauthors for final\n  manuscript. To be submitted to the IEEE access for peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Man-Computer Symbiosis (MCS) was originally envisioned by the famous computer\npioneer J.C.R. Licklider in 1960, as a logical evolution of the then inchoate\nrelationship between computer and humans. In his paper, Licklider provided a\nset of criteria by which to judge if a Man-Computer System is a symbiotic one,\nand also provided some predictions about such systems in the near and far\nfuture. Since then, innovations in computer networks and the invention of the\nInternet were major developments towards that end. However, with most systems\nbased on conventional logical algorithms, many aspects of Licklider's MCS\nremained unfulfilled. This paper explores the extent to which modern machine\nlearning systems in general, and deep learning ones in particular best\nexemplify MCS systems, and why they are the prime contenders to achieve a true\nMan-Computer Symbiosis as described by Licklider in his original paper in the\nfuture. The case for deep learning is built by illustrating each point of the\noriginal criteria as well as the criteria laid by subsequent research into MCS\nsystems, with specific examples and applications provided to strengthen the\narguments. The efficacy of deep neural networks in achieving Artificial General\nIntelligence, which would be the perfect version of an MCS system is also\nexplored.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 17:55:21 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gupta", "Chirag", ""]]}, {"id": "2101.10580", "submitter": "Zhonghao Shi", "authors": "Zhonghao Shi, Thomas R Groechel, Shomik Jain, Kourtney Chima, Ognjen\n  Rudovic, Maja J Matari\\'c", "title": "Toward Personalized Affect-Aware Socially Assistive Robot Tutors in\n  Long-Term Interventions for Children with Autism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affect-aware socially assistive robotics (SAR) has shown great potential for\naugmenting interventions for children with autism spectrum disorders (ASD).\nHowever, current SAR cannot yet perceive the unique and diverse set of atypical\ncognitive-affective behaviors from children with ASD in an automatic and\npersonalized fashion in long-term (multi-session) real-world interactions. To\nbridge this gap, this work designed and validated personalized models of\narousal and valence for children with ASD using a multi-session in-home dataset\nof SAR interventions. By training machine learning (ML) algorithms with\nsupervised domain adaptation (s-DA), the personalized models were able to trade\noff between the limited individual data and the more abundant less personal\ndata pooled from other study participants. We evaluated the effects of\npersonalization on a long-term multimodal dataset consisting of 4 children with\nASD with a total of 19 sessions, and derived inter-rater reliability (IR)\nscores for binary arousal (IR = 83%) and valence (IR = 81%) labels between\nhuman annotators. Our results show that personalized Gradient Boosted Decision\nTrees (XGBoost) models with s-DA outperformed two non-personalized\nindividualized and generic model baselines not only on the weighted average of\nall sessions, but also statistically (p < .05) across individual sessions. This\nwork paves the way for the development of personalized autonomous SAR systems\ntailored toward individuals with atypical cognitive-affective and\nsocio-emotional needs.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 06:16:33 GMT"}, {"version": "v2", "created": "Sat, 30 Jan 2021 00:27:00 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shi", "Zhonghao", ""], ["Groechel", "Thomas R", ""], ["Jain", "Shomik", ""], ["Chima", "Kourtney", ""], ["Rudovic", "Ognjen", ""], ["Matari\u0107", "Maja J", ""]]}, {"id": "2101.10602", "submitter": "Shengchen Zhang", "authors": "Shengchen Zhang, Zixuan Wang, Chaoran Chen, Yi Dai, Lyumanshan Ye,\n  Xiaohua Sun", "title": "Patterns for Representing Knowledge Graphs to Communicate Situational\n  Knowledge of Service Robots", "comments": "12 pages, 13 figures. Published in CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445767", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Service robots are envisioned to be adaptive to their working environment\nbased on situational knowledge. Recent research focused on designing visual\nrepresentation of knowledge graphs for expert users. However, how to generate\nan understandable interface for non-expert users remains to be explored. In\nthis paper, we use knowledge graphs (KGs) as a common ground for knowledge\nexchange and develop a pattern library for designing KG interfaces for\nnon-expert users. After identifying the types of robotic situational knowledge\nfrom the literature, we present a formative study in which participants used\ncards to communicate the knowledge for given scenarios. We iteratively coded\nthe results and identified patterns for representing various types of\nsituational knowledge. To derive design recommendations for applying the\npatterns, we prototyped a lab service robot and conducted Wizard-of-Oz testing.\nThe patterns and recommendations could provide useful guidance in designing\nknowledge-exchange interfaces for robots.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 07:30:49 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Zhang", "Shengchen", ""], ["Wang", "Zixuan", ""], ["Chen", "Chaoran", ""], ["Dai", "Yi", ""], ["Ye", "Lyumanshan", ""], ["Sun", "Xiaohua", ""]]}, {"id": "2101.10681", "submitter": "Stephan Wiefling", "authors": "Stephan Wiefling, Markus D\\\"urmuth, Luigi Lo Iacono", "title": "What's in Score for Website Users: A Data-driven Long-term Study on\n  Risk-based Authentication Characteristics", "comments": "23 pages, 4 figures, 5 tables", "journal-ref": "25th International Conference on Financial Cryptography and Data\n  Security (FC '21). March 01-05, 2021", "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk-based authentication (RBA) aims to strengthen password-based\nauthentication rather than replacing it. RBA does this by monitoring and\nrecording additional features during the login process. If feature values at\nlogin time differ significantly from those observed before, RBA requests an\nadditional proof of identification. Although RBA is recommended in the NIST\ndigital identity guidelines, it has so far been used almost exclusively by\nmajor online services. This is partly due to a lack of open knowledge and\nimplementations that would allow any service provider to roll out RBA\nprotection to its users.\n  To close this gap, we provide a first in-depth analysis of RBA\ncharacteristics in a practical deployment. We observed N=780 users with 247\nunique features on a real-world online service for over 1.8 years. Based on our\ncollected data set, we provide (i) a behavior analysis of two RBA\nimplementations that were apparently used by major online services in the wild,\n(ii) a benchmark of the features to extract a subset that is most suitable for\nRBA use, (iii) a new feature that has not been used in RBA before, and (iv)\nfactors which have a significant effect on RBA performance. Our results show\nthat RBA needs to be carefully tailored to each online service, as even small\nconfiguration adjustments can greatly impact RBA's security and usability\nproperties. We provide insights on the selection of features, their weightings,\nand the risk classification in order to benefit from RBA after a minimum number\nof login attempts.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 10:14:59 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Wiefling", "Stephan", ""], ["D\u00fcrmuth", "Markus", ""], ["Iacono", "Luigi Lo", ""]]}, {"id": "2101.10706", "submitter": "Konstantinos Makantasis", "authors": "Konstantinos Makantasis, Antonios Liapis, Georgios N. Yannakakis", "title": "The Pixels and Sounds of Emotion: General-Purpose Representations of\n  Arousal in Games", "comments": "14 pages, 9 figures, IEEE Transactions on Affective Computing,\n  Konstantinos Makantasis was supported by the European Union's H2020 research\n  and innovation programme (Grant Agreement No. 101003397). Antonios Liapis and\n  Georgios N. Yannakakis were supported by the European Union's H2020 research\n  and innovation programme (Grant Agreement No. 951911)", "journal-ref": null, "doi": "10.1109/TAFFC.2021.3060877", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What if emotion could be captured in a general and subject-agnostic fashion?\nIs it possible, for instance, to design general-purpose representations that\ndetect affect solely from the pixels and audio of a human-computer interaction\nvideo? In this paper we address the above questions by evaluating the capacity\nof deep learned representations to predict affect by relying only on\naudiovisual information of videos. We assume that the pixels and audio of an\ninteractive session embed the necessary information required to detect affect.\nWe test our hypothesis in the domain of digital games and evaluate the degree\nto which deep classifiers and deep preference learning algorithms can learn to\npredict the arousal of players based only on the video footage of their\ngameplay. Our results from four dissimilar games suggest that general-purpose\nrepresentations can be built across games as the arousal models obtain average\naccuracies as high as 85% using the challenging leave-one-video-out\ncross-validation scheme. The dissimilar audiovisual characteristics of the\ntested games showcase the strengths and limitations of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:00:44 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 08:35:00 GMT"}, {"version": "v3", "created": "Thu, 18 Feb 2021 12:22:29 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Makantasis", "Konstantinos", ""], ["Liapis", "Antonios", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "2101.10710", "submitter": "Mohammad Naser Sabet Jahromi", "authors": "Satya M. Muddamsetty, Mohammad N. S. Jahromi, Andreea E. Ciontos,\n  Laura M. Fenoy, Thomas B. Moeslund", "title": "Introducing and assessing the explainable AI (XAI)method: SIDU", "comments": "Preprint-submitted to Journal of Pattern Recognition (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explainable Artificial Intelligence (XAI) has in recent years become a\nwell-suited framework to generate human understandable explanations of black\nbox models. In this paper, we present a novel XAI visual explanation algorithm\ndenoted SIDU that can effectively localize entire object regions responsible\nfor prediction in a full extend. We analyze its robustness and effectiveness\nthrough various computational and human subject experiments. In particular, we\nassess the SIDU algorithm using three different types of evaluations\n(Application, Human and Functionally-Grounded) to demonstrate its superior\nperformance. The robustness of SIDU is further studied in presence of\nadversarial attack on black box models to better understand its performance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 11:13:50 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Muddamsetty", "Satya M.", ""], ["Jahromi", "Mohammad N. S.", ""], ["Ciontos", "Andreea E.", ""], ["Fenoy", "Laura M.", ""], ["Moeslund", "Thomas B.", ""]]}, {"id": "2101.10932", "submitter": "Ce Zhang Mr.", "authors": "Ce Zhang, Young-Keun Kim, Azim Eskandarian", "title": "EEG-Inception: An Accurate and Robust End-to-End Neural Network for\n  EEG-based Motor Imagery Classification", "comments": "Provisionally Accepted by Journal of Neural Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of EEG-based motor imagery (MI) is a crucial non-invasive\napplication in brain-computer interface (BCI) research. This paper proposes a\nnovel convolutional neural network (CNN) architecture for accurate and robust\nEEG-based MI classification that outperforms the state-of-the-art methods. The\nproposed CNN model, namely EEG-Inception, is built on the backbone of the\nInception-Time network, which showed to be highly efficient and accurate for\ntime-series classification. Also, the proposed network is an end-to-end\nclassification, as it takes the raw EEG signals as the input and does not\nrequire complex EEG signal-preprocessing. Furthermore, this paper proposes a\nnovel data augmentation method for EEG signals to enhance the accuracy, at\nleast by 3%, and reduce overfitting with limited BCI datasets. The proposed\nmodel outperforms all the state-of-the-art methods by achieving the average\naccuracy of 88.4% and 88.6% on the 2008 BCI Competition IV 2a (four-classes)\nand 2b datasets (binary-classes), respectively. Furthermore, it takes less than\n0.025 seconds to test a sample suitable for real-time processing. Moreover, the\nclassification standard deviation for nine different subjects achieves the\nlowest value of 5.5 for the 2b dataset and 7.1 for the 2a dataset, which\nvalidates that the proposed method is highly robust. From the experiment\nresults, it can be inferred that the EEG-Inception network exhibits a strong\npotential as a subject-independent classifier for EEG-based MI tasks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jan 2021 19:03:10 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 22:19:11 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 15:51:01 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Ce", ""], ["Kim", "Young-Keun", ""], ["Eskandarian", "Azim", ""]]}, {"id": "2101.11000", "submitter": "Elizabeth Childs", "authors": "Amanuel Awoke, Hugo Burbelo, Elizabeth Childs, Ferzam Mohammad, Logan\n  Stevens, Nicholas Rewkowski, Dinesh Manocha", "title": "An Overview of Enhancing Distance Learning Through Augmented and Virtual\n  Reality Technologies", "comments": "12 pages, 7 figures, submitted to TVCG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although distance learning presents a number of interesting educational\nadvantages as compared to in-person instruction, it is not without its\ndownsides. We first assess the educational challenges presented by distance\nlearning as a whole, and identify 4 main challenges that distance learning\ncurrently presents as compared to in-person instruction: the lack of social\ninteraction, reduced student engagement and focus, reduced comprehension and\ninformation retention, and the lack of flexible and customizable instructor\nresources. After assessing each of these challenges in-depth, we examine how\nAR/VR technologies might serve to address each challenge along with their\ncurrent shortcomings, and finally outline the further research that is required\nto fully understand the potential of AR/VR technologies as they apply to\ndistance learning.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:56:25 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Awoke", "Amanuel", ""], ["Burbelo", "Hugo", ""], ["Childs", "Elizabeth", ""], ["Mohammad", "Ferzam", ""], ["Stevens", "Logan", ""], ["Rewkowski", "Nicholas", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2101.11054", "submitter": "Jennifer Jacobs", "authors": "Gabrielle Benabdallah, Samuelle Bourgault, Nadya Peek, Jennifer Jacobs", "title": "Remote Learners, Home Makers: How Digital Fabrication Was Taught Online\n  During a Pandemic", "comments": "to be published at CHI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital fabrication courses that relied on physical makerspaces were severely\ndisrupted by COVID-19. As universities shut down in Spring 2020, instructors\ndeveloped new models for digital fabrication at a distance. Through interviews\nwith faculty and students and examination of course materials, we recount the\nexperiences of eight remote digital fabrication courses. We found that learning\nwith hobbyist equipment and online social networks could emulate using\nindustrial equipment in shared workshops. Furthermore, at-home digital\nfabrication offered unique learning opportunities including more iteration,\nmachine tuning, and maintenance. These opportunities depended on new forms of\nlabor and varied based on student living situations. Our findings have\nimplications for remote and in-person digital fabrication instruction. They\nindicate how access to tools was important, but not as critical as providing\nopportunities for iteration; they show how remote fabrication exacerbated\nstudent inequities; and they suggest strategies for evaluating trade-offs in\nremote fabrication models with respect to learning objectives.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 19:53:16 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 06:15:08 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Benabdallah", "Gabrielle", ""], ["Bourgault", "Samuelle", ""], ["Peek", "Nadya", ""], ["Jacobs", "Jennifer", ""]]}, {"id": "2101.11089", "submitter": "Ana Paula Chaves", "authors": "Ana Paula Chaves, Jesse Egbert, Toby Hocking, Eck Doerry, Marco\n  Aurelio Gerosa", "title": "Chatbots language design: the influence of language variation on user\n  experience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatbots are often designed to mimic social roles attributed to humans.\nHowever, little is known about the impact on user's perceptions of using\nlanguage that fails to conform to the associated social role. Our research\ndraws on sociolinguistic theory to investigate how a chatbot's language choices\ncan adhere to the expected social role the agent performs within a given\ncontext. In doing so, we seek to understand whether chatbots design should\naccount for linguistic register. This research analyzes how register\ndifferences play a role in shaping the user's perception of the human-chatbot\ninteraction. Ultimately, we want to determine whether register-specific\nlanguage influences users' perceptions and experiences with chatbots. We\nproduced parallel corpora of conversations in the tourism domain with similar\ncontent and varying register characteristics and evaluated users' preferences\nof chatbot's linguistic choices in terms of appropriateness, credibility, and\nuser experience. Our results show that register characteristics are strong\npredictors of user's preferences, which points to the needs of designing\nchatbots with register-appropriate language to improve acceptance and users'\nperceptions of chatbot interactions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 21:23:21 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Chaves", "Ana Paula", ""], ["Egbert", "Jesse", ""], ["Hocking", "Toby", ""], ["Doerry", "Eck", ""], ["Gerosa", "Marco Aurelio", ""]]}, {"id": "2101.11101", "submitter": "Uttaran Bhattacharya", "authors": "Uttaran Bhattacharya and Nicholas Rewkowski and Abhishek Banerjee and\n  Pooja Guhan and Aniket Bera and Dinesh Manocha", "title": "Text2Gestures: A Transformer-Based Network for Generating Emotive Body\n  Gestures for Virtual Agents", "comments": "10 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Text2Gestures, a transformer-based learning method to\ninteractively generate emotive full-body gestures for virtual agents aligned\nwith natural language text inputs. Our method generates emotionally expressive\ngestures by utilizing the relevant biomechanical features for body expressions,\nalso known as affective features. We also consider the intended task\ncorresponding to the text and the target virtual agents' intended gender and\nhandedness in our generation pipeline. We train and evaluate our network on the\nMPI Emotional Body Expressions Database and observe that our network produces\nstate-of-the-art performance in generating gestures for virtual agents aligned\nwith the text for narration or conversation. Our network can generate these\ngestures at interactive rates on a commodity GPU. We conduct a web-based user\nstudy and observe that around 91% of participants indicated our generated\ngestures to be at least plausible on a five-point Likert Scale. The emotions\nperceived by the participants from the gestures are also strongly positively\ncorrelated with the corresponding intended emotions, with a minimum Pearson\ncoefficient of 0.77 in the valence dimension.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 22:07:20 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Bhattacharya", "Uttaran", ""], ["Rewkowski", "Nicholas", ""], ["Banerjee", "Abhishek", ""], ["Guhan", "Pooja", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2101.11102", "submitter": "Florence Jean Talirongan", "authors": "Jerry M. Lumasag, Hidear Talirongan, Florence Jean B. Talirongan,\n  Charies L. Labanza", "title": "Data driven Decision Support on Students Behavior using Fuzzy Based\n  Approach", "comments": "10 pages, 7 figures, 6 tables", "journal-ref": "Middle East Journal of Applied Science & Technology 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monitoring of students behavior in school needs further consideration in\norder to lessen the number of casualties in every term. The study designs a\ndata driven decision support on students behavior utilizing Fuzzy Based\nApproach. The study successfully produces common behavioral problems of the\nstudent and able to give interventions for the improvement of students\nbehavior. Student behavioral problems identified were absenteeism, tardiness\nand poor academic performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2021 07:36:44 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Lumasag", "Jerry M.", ""], ["Talirongan", "Hidear", ""], ["Talirongan", "Florence Jean B.", ""], ["Labanza", "Charies L.", ""]]}, {"id": "2101.11103", "submitter": "Toby Jia-Jun Li", "authors": "Toby Jia-Jun Li, Lindsay Popowski, Tom M. Mitchell, Brad A. Myers", "title": "Screen2Vec: Semantic Embedding of GUI Screens and GUI Components", "comments": "Accepted to CHI Conference on Human Factors in Computing Systems (CHI\n  2021)", "journal-ref": null, "doi": "10.1145/3411764.3445049", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Representing the semantics of GUI screens and components is crucial to\ndata-driven computational methods for modeling user-GUI interactions and mining\nGUI designs. Existing GUI semantic representations are limited to encoding\neither the textual content, the visual design and layout patterns, or the app\ncontexts. Many representation techniques also require significant manual data\nannotation efforts. This paper presents Screen2Vec, a new self-supervised\ntechnique for generating representations in embedding vectors of GUI screens\nand components that encode all of the above GUI features without requiring\nmanual annotation using the context of user interaction traces. Screen2Vec is\ninspired by the word embedding method Word2Vec, but uses a new two-layer\npipeline informed by the structure of GUIs and interaction traces and\nincorporates screen- and app-specific metadata. Through several sample\ndownstream tasks, we demonstrate Screen2Vec's key useful properties:\nrepresenting between-screen similarity through nearest neighbors,\ncomposability, and capability to represent user tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2021 17:17:34 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Toby Jia-Jun", ""], ["Popowski", "Lindsay", ""], ["Mitchell", "Tom M.", ""], ["Myers", "Brad A.", ""]]}, {"id": "2101.11218", "submitter": "Jennifer Jacobs", "authors": "Jingyi Li, Sonia Hashim, Jennifer Jacobs", "title": "What We Can Learn From Visual Artists About Software Development", "comments": "1 figure, CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.344568", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores software's role in visual art production by examining how\nartists use and develop software. We conducted interviews with professional\nartists who were collaborating with software developers, learning software\ndevelopment, and building and maintaining software. We found artists were\nmotivated to learn software development for intellectual growth and access to\ntechnical communities. Artists valued efficient workflows through skilled\nmanual execution and personal software development, but avoided high-level\nforms of software automation. Artists identified conflicts between their\npriorities and those of professional developers and computational art\ncommunities, which influenced how they used computational aesthetics in their\nwork. These findings contribute to efforts in systems engineering research to\nintegrate end-user programming and creativity support across software and\nphysical media, suggesting opportunities for artists as collaborators. Artists'\nexperiences writing software can guide technical implementations of\ndomain-specific representations, and their experiences in interdisciplinary\nproduction can aid inclusive community building around computational tools.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 06:04:55 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Li", "Jingyi", ""], ["Hashim", "Sonia", ""], ["Jacobs", "Jennifer", ""]]}, {"id": "2101.11231", "submitter": "Amy X. Zhang", "authors": "Jessica Wang, Amy Zhang, David Karger", "title": "Pano: Engaging with News using Moral Framing towards Bridging\n  Ideological Divides", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Society is showing signs of strong ideological polarization. When pushed to\nseek perspectives different from their own, people often reject diverse ideas\nor find them unfathomable. Work has shown that framing controversial issues\nusing the values of the audience can improve reception of opposing views. In\nthis paper, we present Pano, an interactive system motivated by moral framing\ntheory that educates news consumers to think in a framework of fundamental\nhuman values. Pano encourages users to challenge their understanding of\nopposing views through collaborative annotation and discussion of moral framing\nin articles and comments. We describe two iterations of Pano -- the first\ncovering a suite of ways to interact with news, and the second tailored toward\nannotation and discussion. We find that compared to a control, Pano users more\noften empathize with and re-frame arguments in the moral values of the opposing\nside, showing promise toward the ultimate goal of bridging the ideological\ndivide.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 07:20:48 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Wang", "Jessica", ""], ["Zhang", "Amy", ""], ["Karger", "David", ""]]}, {"id": "2101.11249", "submitter": "Sai Sukruth Bezugam", "authors": "Sai Sukruth Bezugam, Swatilekha Majumdar, Chetan Ralekar and Tapan\n  Kumar Gandhi", "title": "Efficient Video Summarization Framework using EEG and Eye-tracking\n  Signals", "comments": "10 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an efficient video summarization framework that will give\na gist of the entire video in a few key-frames or video skims. Existing video\nsummarization frameworks are based on algorithms that utilize computer vision\nlow-level feature extraction or high-level domain level extraction. However,\nbeing the ultimate user of the summarized video, humans remain the most\nneglected aspect. Therefore, the proposed paper considers human's role in\nsummarization and introduces human visual attention-based summarization\ntechniques. To understand human attention behavior, we have designed and\nperformed experiments with human participants using electroencephalogram (EEG)\nand eye-tracking technology. The EEG and eye-tracking data obtained from the\nexperimentation are processed simultaneously and used to segment frames\ncontaining useful information from a considerable video volume. Thus, the frame\nsegmentation primarily relies on the cognitive judgments of human beings. Using\nour approach, a video is summarized by 96.5% while maintaining higher precision\nand high recall factors. The comparison with the state-of-the-art techniques\ndemonstrates that the proposed approach yields ceiling-level performance with\nreduced computational cost in summarising the videos.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 08:13:19 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Bezugam", "Sai Sukruth", ""], ["Majumdar", "Swatilekha", ""], ["Ralekar", "Chetan", ""], ["Gandhi", "Tapan Kumar", ""]]}, {"id": "2101.11278", "submitter": "Elodie Bouzbib", "authors": "Elodie Bouzbib, Gilles Bailly, Sinan Haliyo, Pascal Frey", "title": "\"Can I Touch This?\": Survey of Virtual Reality Interactions via Haptic\n  Solutions", "comments": null, "journal-ref": null, "doi": "10.1145/3450522.3451323", "report-no": "32e Conf\\'erence Francophone sur l'Interaction Homme-Machine (IHM\n  '20.21), April 13--16, 2021, Virtual Event, France", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Haptic feedback has become crucial to enhance the user experiences in Virtual\nReality (VR). This justifies the sudden burst of novel haptic solutions\nproposed these past years in the HCI community. This article is a survey of\nVirtual Reality interactions, relying on haptic devices. We propose two\ndimensions to describe and compare the current haptic solutions: their degree\nof physicality, as well as their degree of actuation. We depict a compromise\nbetween the user and the designer, highlighting how the range of required or\nproposed stimulation in VR is opposed to the haptic interfaces flexibility and\ntheir deployment in real-life use-cases. This paper (1) outlines the variety of\nhaptic solutions and provides a novel perspective for analysing their\nassociated interactions, (2) highlights the limits of the current evaluation\ncriteria regarding these interactions, and finally (3) reflects the\ninteraction, operation and conception potentials of \"encountered-type of haptic\ndevices\".\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 09:16:17 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Bouzbib", "Elodie", ""], ["Bailly", "Gilles", ""], ["Haliyo", "Sinan", ""], ["Frey", "Pascal", ""]]}, {"id": "2101.11326", "submitter": "Kenta Yamamoto", "authors": "Kenta Yamamoto, Ippei Suzuki, Akihisa Shitara, Yoichi Ochiai", "title": "See-Through Captions: Real-Time Captioning on Transparent Display for\n  Deaf and Hard-of-Hearing People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Real-time captioning is a useful technique for deaf and hard-of-hearing (DHH)\npeople to talk to hearing people. With the improvement in device performance\nand the accuracy of automatic speech recognition (ASR), real-time captioning is\nbecoming an important tool for helping DHH people in their daily lives. To\nrealize higher-quality communication and overcome the limitations of mobile and\naugmented-reality devices, real-time captioning that can be used comfortably\nwhile maintaining nonverbal communication and preventing incorrect recognition\nis required. Therefore, we propose a real-time captioning system that uses a\ntransparent display. In this system, the captions are presented on both sides\nof the display to address the problem of incorrect ASR, and the highly\ntransparent display makes it possible to see both the body language and the\ncaptions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:27:51 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yamamoto", "Kenta", ""], ["Suzuki", "Ippei", ""], ["Shitara", "Akihisa", ""], ["Ochiai", "Yoichi", ""]]}, {"id": "2101.11333", "submitter": "Kostas Karpouzis", "authors": "George Tsatiris, Kostas Karpouzis", "title": "Developing for personalised learning: the long road from educational\n  objectives to development and feedback", "comments": "3 pages", "journal-ref": "ACM Interaction Design and Children (IDC) conference 2020,\n  Workshop on Technology-mediated personalized learning for younger learners:\n  concepts, methods and practice", "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the development needed to support the functional and\nteaching requirements of iRead, a 4-year EU-funded project which produced an\naward-winning serious game utilising lexical and syntactical game content. The\nmain functional requirement was that the game should retain different profiles\nfor each student, encapsulating both the respective language model (which\nlanguage features should be taught/used in the game first, before moving on to\nmore advanced ones) and the user model (mastery level for each feature, as\nreported by the student's performance in the game). In addition to this,\nresearchers and stakeholders stated additional requirements related to learning\nobjectives and strategies to make the game more interesting and successful;\nthese were implemented as a set of selection rules which take into account not\nonly the mastery level for each feature, but also respect the priorities set by\nteachers, helping avoid repetition of content and features, and maintaining a\nbalance between new content and revision of already mastered features to give\nstudents the sense of progress, while also reinforcing learning.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 11:50:41 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Tsatiris", "George", ""], ["Karpouzis", "Kostas", ""]]}, {"id": "2101.11435", "submitter": "Yakup Kutlu", "authors": "Apdullah Yayik, Yakup Kutlu", "title": "Online LDA based brain-computer interface system to aid disabled people", "comments": "13 pages, 4 figures, Natural and Engineering Sciences", "journal-ref": "Natural and Engineering Sciences, 2017", "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to develop brain-computer interface system based on\nelectroencephalography that can aid disabled people in daily life. The system\nrelies on one of the most effective event-related potential wave, P300, which\ncan be elicited by oddball paradigm. Developed application has a basic\ninteraction tool that enables disabled people to convey their needs to other\npeople selecting related objects. These objects pseudo-randomly flash in a\nvisual interface on computer screen. The user must focus on related object to\nconvey desired needs. The system can convey desired needs correctly by\ndetecting P300 wave in acquired 14-channel EEG signal and classifying using\nlinear discriminant analysis classifier just in 15 seconds. Experiments have\nbeen carried out on 19 volunteers to validate developed BCI system. As a\nresult, accuracy rate of 90.83% is achieved in online performance\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 08:17:05 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Yayik", "Apdullah", ""], ["Kutlu", "Yakup", ""]]}, {"id": "2101.11465", "submitter": "Jan Gogoll", "authors": "Till Feier, Jan Gogoll, Matthias Uhl", "title": "Hiding Behind Machines: When Blame Is Shifted to Artificial Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The transfer of tasks with sometimes far-reaching moral implications to\nautonomous systems raises a number of ethical questions. In addition to\nfundamental questions about the moral agency of these systems, behavioral\nissues arise. This article focuses on the responsibility of agents who decide\non our behalf. We investigate the empirically accessible question of whether\nthe production of moral outcomes by an agent is systematically judged\ndifferently when the agent is artificial and not human. The results of a\nlaboratory experiment suggest that decision-makers can actually rid themselves\nof guilt more easily by delegating to machines than by delegating to other\npeople. Our results imply that the availability of artificial agents could\nprovide stronger incentives for decision makers to delegate morally sensitive\ndecisions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 14:50:02 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Feier", "Till", ""], ["Gogoll", "Jan", ""], ["Uhl", "Matthias", ""]]}, {"id": "2101.11529", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Anirudh Thatipelli, Neel Trivedi, Ravi Kiran Sarvadevabhatla", "title": "NTU60-X: Towards Skeleton-based Recognition of Subtle Human Actions", "comments": "Code repository at https://github.com/skelemoa/ntu-x", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The lack of fine-grained joints such as hand fingers is a fundamental\nperformance bottleneck for state of the art skeleton action recognition models\ntrained on the largest action recognition dataset, NTU-RGBD. To address this\nbottleneck, we introduce a new skeleton based human action dataset - NTU60-X.\nIn addition to the 25 body joints for each skeleton as in NTU-RGBD, NTU60-X\ndataset includes finger and facial joints, enabling a richer skeleton\nrepresentation. We appropriately modify the state of the art approaches to\nenable training using the introduced dataset. Our results demonstrate the\neffectiveness of NTU60-X in overcoming the aforementioned bottleneck and\nimprove state of the art performance, overall and on hitherto worst performing\naction categories.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 16:33:51 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 10:19:33 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Thatipelli", "Anirudh", ""], ["Trivedi", "Neel", ""], ["Sarvadevabhatla", "Ravi Kiran", ""]]}, {"id": "2101.11556", "submitter": "Manisha Verma", "authors": "Manisha Verma, Kapil Thadani, and Shaunak Mishra", "title": "Powering COVID-19 community Q&A with Curated Side Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Community question answering and discussion platforms such as Reddit, Yahoo!\nanswers or Quora provide users the flexibility of asking open ended questions\nto a large audience, and replies to such questions maybe useful both to the\nuser and the community on certain topics such as health, sports or finance.\nGiven the recent events around COVID-19, some of these platforms have attracted\n2000+ questions from users about several aspects associated with the disease.\nGiven the impact of this disease on general public, in this work we investigate\nways to improve the ranking of user generated answers on COVID-19. We\nspecifically explore the utility of external technical sources of side\ninformation (such as CDC guidelines or WHO FAQs) in improving answer ranking on\nsuch platforms. We found that ranking user answers based on question-answer\nsimilarity is not sufficient, and existing models cannot effectively exploit\nexternal (side) information. In this work, we demonstrate the effectiveness of\ndifferent attention based neural models that can directly exploit side\ninformation available in technical documents or verified forums (e.g., research\npublications on COVID-19 or WHO website). Augmented with a temperature\nmechanism, the attention based neural models can selectively determine the\nrelevance of side information for a given user question, while ranking answers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 17:24:38 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Verma", "Manisha", ""], ["Thadani", "Kapil", ""], ["Mishra", "Shaunak", ""]]}, {"id": "2101.11691", "submitter": "Glenn Van Wallendael", "authors": "Niels Van Kets, Bart Moens, Klaas Bombeke, Wouter Durnez, Pieter-Jan\n  Maes, Glenn Van Wallendael, Lieven De Marez, Marc Leman, Peter Lambert", "title": "Art and Science Interaction Lab -- A highly flexible and modular\n  interaction science research facility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Art and Science Interaction Lab (ASIL) is a unique, highly flexible and\nmodular interaction science research facility to effectively bring, analyse and\ntest experiences and interactions in mixed virtual/augmented contexts as well\nas to conduct research on next-gen immersive technologies. It brings together\nthe expertise and creativity of engineers, performers, designers and scientists\ncreating solutions and experiences shaping the lives of people. The lab is\nequipped with state-of-the-art visual, auditory and user-tracking equipment,\nfully synchronized and connected to a central backend. This synchronization\nallows for highly accurate multi-sensor measurements and analysis.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 21:23:58 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Van Kets", "Niels", ""], ["Moens", "Bart", ""], ["Bombeke", "Klaas", ""], ["Durnez", "Wouter", ""], ["Maes", "Pieter-Jan", ""], ["Van Wallendael", "Glenn", ""], ["De Marez", "Lieven", ""], ["Leman", "Marc", ""], ["Lambert", "Peter", ""]]}, {"id": "2101.11743", "submitter": "Geza Kovacs", "authors": "Geza Kovacs, Zhengxuan Wu, Michael S. Bernstein", "title": "Not Now, Ask Later: Users Weaken Their Behavior Change Regimen Over\n  Time, But Expect To Re-Strengthen It Imminently", "comments": "To appear in ACM CHI Conference on Human Factors in Computing Systems\n  (CHI '21), May 8-13, 2021, Yokohama, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How effectively do we adhere to nudges and interventions that help us control\nour online browsing habits? If we have a temporary lapse and disable the\nbehavior change system, do we later resume our adherence, or has the dam\nbroken? In this paper, we investigate these questions through log analyses of\n8,000+ users on HabitLab, a behavior change platform that helps users reduce\ntheir time online. We find that, while users typically begin with\nhigh-challenge interventions, over time they allow themselves to slip into\neasier and easier interventions. Despite this, many still expect to return to\nthe harder interventions imminently: they repeatedly choose to be asked to\nchange difficulty again on the next visit, declining to have the system save\ntheir preference for easy interventions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 23:47:21 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kovacs", "Geza", ""], ["Wu", "Zhengxuan", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "2101.11778", "submitter": "Kai Lukoff", "authors": "Kai Lukoff, Ulrik Lyngs, Himanshu Zade, J. Vera Liao, James Choi,\n  Kaiyue Fan, Sean A. Munson, Alexis Hiniker", "title": "How the Design of YouTube Influences User Sense of Agency", "comments": "14 pages, 3 figures, Forthcoming at the CHI 2021 Conference", "journal-ref": null, "doi": "10.1145/3411764.3445467", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the attention economy, video apps employ design mechanisms like autoplay\nthat exploit psychological vulnerabilities to maximize watch time.\nConsequently, many people feel a lack of agency over their app use, which is\nlinked to negative life effects such as loss of sleep. Prior design research\nhas innovated external mechanisms that police multiple apps, such as lockout\ntimers. In this work, we shift the focus to how the internal mechanisms of an\napp can support user agency, taking the popular YouTube mobile app as a test\ncase. From a survey of 120 U.S. users, we find that autoplay and\nrecommendations primarily undermine sense of agency, while search and playlists\nsupport it. From 13 co-design sessions, we find that when users have a specific\nintention for how they want to use YouTube they prefer interfaces that support\ngreater agency. We discuss implications for how designers can help users\nreclaim a sense of agency over their media use.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 02:16:15 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Lukoff", "Kai", ""], ["Lyngs", "Ulrik", ""], ["Zade", "Himanshu", ""], ["Liao", "J. Vera", ""], ["Choi", "James", ""], ["Fan", "Kaiyue", ""], ["Munson", "Sean A.", ""], ["Hiniker", "Alexis", ""]]}, {"id": "2101.11824", "submitter": "Farnaz Jahanbakhsh", "authors": "Farnaz Jahanbakhsh, Amy X. Zhang, Adam J. Berinsky, Gordon Pennycook,\n  David G. Rand, David R. Karger", "title": "Exploring Lightweight Interventions at Posting Time to Reduce the\n  Sharing of Misinformation on Social Media", "comments": "In CSCW'21", "journal-ref": "Proc. ACM Hum.-Comput. Interact., Vol. 5, No. CSCW1, Article 18.\n  Publication date: April 2021", "doi": "10.1145/3449092", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When users on social media share content without considering its veracity,\nthey may unwittingly be spreading misinformation. In this work, we investigate\nthe design of lightweight interventions that nudge users to assess the accuracy\nof information as they share it. Such assessment may deter users from posting\nmisinformation in the first place, and their assessments may also provide\nuseful guidance to friends aiming to assess those posts themselves. In support\nof lightweight assessment, we first develop a taxonomy of the reasons why\npeople believe a news claim is or is not true; this taxonomy yields a checklist\nthat can be used at posting time. We conduct evaluations to demonstrate that\nthe checklist is an accurate and comprehensive encapsulation of people's\nfree-response rationales. In a second experiment, we study the effects of three\nbehavioral nudges -- 1) checkboxes indicating whether headings are accurate, 2)\ntagging reasons (from our taxonomy) that a post is accurate via a checklist and\n3) providing free-text rationales for why a headline is or is not accurate --\non people's intention of sharing the headline on social media. From an\nexperiment with 1668 participants, we find that both providing accuracy\nassessment and rationale reduce the sharing of false content. They also reduce\nthe sharing of true content, but to a lesser degree that yields an overall\ndecrease in the fraction of shared content that is false. Our findings have\nimplications for designing social media and news sharing platforms that draw\nfrom richer signals of content credibility contributed by users. In addition,\nour validated taxonomy can be used by platforms and researchers as a way to\ngather rationales in an easier fashion than free-response.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 05:45:01 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 05:41:55 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 05:38:49 GMT"}, {"version": "v4", "created": "Sat, 27 Feb 2021 19:40:25 GMT"}, {"version": "v5", "created": "Sun, 23 May 2021 04:56:44 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Jahanbakhsh", "Farnaz", ""], ["Zhang", "Amy X.", ""], ["Berinsky", "Adam J.", ""], ["Pennycook", "Gordon", ""], ["Rand", "David G.", ""], ["Karger", "David R.", ""]]}, {"id": "2101.11832", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta (Montreal AI Ethics Institute and Microsoft)", "title": "Making Responsible AI the Norm rather than the Exception", "comments": "A report prepared by the Montreal AI Ethics Institute for the\n  National Security Commission on Artificial Intelligence in response to their\n  Key Considerations for Responsible Development and Fielding of Artificial\n  Intelligence document; 26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This report prepared by the Montreal AI Ethics Institute provides\nrecommendations in response to the National Security Commission on Artificial\nIntelligence (NSCAI) Key Considerations for Responsible Development and\nFielding of Artificial Intelligence document. The report centres on the idea\nthat Responsible AI should be made the Norm rather than an Exception. It does\nso by utilizing the guiding principles of: (1) alleviating friction in existing\nworkflows, (2) empowering stakeholders to get buy-in, and (3) conducting an\neffective translation of abstract standards into actionable engineering\npractices. After providing some overarching comments on the document from the\nNSCAI, the report dives into the primary contribution of an actionable\nframework to help operationalize the ideas presented in the document from the\nNSCAI. The framework consists of: (1) a learning, knowledge, and information\nexchange (LKIE), (2) the Three Ways of Responsible AI, (3) an\nempirically-driven risk-prioritization matrix, and (4) achieving the right\nlevel of complexity. All components reinforce each other to move from\nprinciples to practice in service of making Responsible AI the norm rather than\nthe exception.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 06:39:01 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 08:23:05 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Gupta", "Abhishek", "", "Montreal AI Ethics Institute and Microsoft"]]}, {"id": "2101.11865", "submitter": "Hancheng Cao", "authors": "Hancheng Cao, Chia-Jung Lee, Shamsi Iqbal, Mary Czerwinski, Priscilla\n  Wong, Sean Rintel, Brent Hecht, Jaime Teevan, Longqi Yang", "title": "Large Scale Analysis of Multitasking Behavior During Remote Meetings", "comments": "In ACM CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445243", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual meetings are critical for remote work because of the need for\nsynchronous collaboration in the absence of in-person interactions. In-meeting\nmultitasking is closely linked to people's productivity and wellbeing. However,\nwe currently have limited understanding of multitasking in remote meetings and\nits potential impact. In this paper, we present what we believe is the most\ncomprehensive study of remote meeting multitasking behavior through an analysis\nof a large-scale telemetry dataset collected from February to May 2020 of U.S.\nMicrosoft employees and a 715-person diary study. Our results demonstrate that\nintrinsic meeting characteristics such as size, length, time, and type,\nsignificantly correlate with the extent to which people multitask, and\nmultitasking can lead to both positive and negative outcomes. Our findings\nsuggest important best-practice guidelines for remote meetings (e.g., avoid\nimportant meetings in the morning) and design implications for productivity\ntools (e.g., support positive remote multitasking).\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 08:33:23 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Cao", "Hancheng", ""], ["Lee", "Chia-Jung", ""], ["Iqbal", "Shamsi", ""], ["Czerwinski", "Mary", ""], ["Wong", "Priscilla", ""], ["Rintel", "Sean", ""], ["Hecht", "Brent", ""], ["Teevan", "Jaime", ""], ["Yang", "Longqi", ""]]}, {"id": "2101.11898", "submitter": "Patrik Jonell", "authors": "Patrik Jonell, Youngwoo Yoon, Pieter Wolfert, Taras Kucherenko, Gustav\n  Eje Henter", "title": "HEMVIP: Human Evaluation of Multiple Videos in Parallel", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many research areas, for example motion and gesture generation, objective\nmeasures alone do not provide an accurate impression of key stimulus traits\nsuch as perceived quality or appropriateness. The gold standard is instead to\nevaluate these aspects through user studies, especially subjective evaluations\nof video stimuli. Common evaluation paradigms either present individual stimuli\nto be scored on Likert-type scales, or ask users to compare and rate videos in\na pairwise fashion. However, the time and resources required for such\nevaluations scale poorly as the number of conditions to be compared increases.\nBuilding on standards used for evaluating the quality of multimedia codecs,\nthis paper instead introduces a framework for granular rating of multiple\ncomparable videos in parallel. This methodology essentially analyses all\ncondition pairs at once. Our contributions are 1) a proposed framework, called\nHEMVIP, for parallel and granular evaluation of multiple video stimuli and 2) a\nvalidation study confirming that results obtained using the tool are in close\nagreement with results of prior studies using conventional multiple pairwise\ncomparisons.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 10:00:34 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Jonell", "Patrik", ""], ["Yoon", "Youngwoo", ""], ["Wolfert", "Pieter", ""], ["Kucherenko", "Taras", ""], ["Henter", "Gustav Eje", ""]]}, {"id": "2101.11970", "submitter": "Diego Rojo", "authors": "Diego Rojo, Nyi Nyi Htun, Denis Parra, Robin De Croon and Katrien\n  Verbert", "title": "AHMoSe: A Knowledge-Based Visual Support System for Selecting Regression\n  Machine Learning Models", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision support systems have become increasingly popular in the domain of\nagriculture. With the development of Automated Machine Learning, agricultural\nexperts are now able to train, evaluate and make predictions using cutting edge\nmachine learning (ML) models without the need for much ML knowledge. Although\nthis automated approach has led to successful results in many scenarios, in\ncertain cases (e.g., when few labeled datasets are available) choosing among\ndifferent models with similar performance metrics is a difficult task.\nFurthermore, these systems do not commonly allow users to incorporate their\ndomain knowledge that could facilitate the task of model selection, and to gain\ninsight into the prediction system for eventual decision making. To address\nthese issues, in this paper we present AHMoSe, a visual support system that\nallows domain experts to better understand, diagnose and compare different\nregression models, primarily by enriching model-agnostic explanations with\ndomain knowledge. To validate AHMoSE, we describe a use case scenario in the\nviticulture domain, grape quality prediction, where the system enables users to\ndiagnose and select prediction models that perform better. We also discuss\nfeedback concerning the design of the tool from both ML and viticulture\nexperts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 12:55:06 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Rojo", "Diego", ""], ["Htun", "Nyi Nyi", ""], ["Parra", "Denis", ""], ["De Croon", "Robin", ""], ["Verbert", "Katrien", ""]]}, {"id": "2101.12038", "submitter": "Kostas Karpouzis", "authors": "Dimitris Kritikos and Kostas Karpouzis", "title": "From pixels to notes: a computational implementation of synaesthesia for\n  cultural artefacts", "comments": "AVI2CH 2020: Workshop on Advanced Visual Interfaces and Interactions\n  in Cultural Heritage, Ischia, Italy - Sept 28-October 2, 2020", "journal-ref": null, "doi": "10.1145/3399715.3400869", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synaesthesia is a condition that enables people to sense information in the\nform of several senses at once. This work describes a Python implementation of\na simulation of synaesthesia between listening to music and viewing a painting.\nBased on Scriabin's definition, we developed a deterministic process to produce\na melody after processing a painting, mimicking the production of notes from\ncolours in the field of view of persons experiencing synaesthesia.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 10:42:20 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Kritikos", "Dimitris", ""], ["Karpouzis", "Kostas", ""]]}, {"id": "2101.12044", "submitter": "Wilson Marc\\'ilio-Jr", "authors": "Wilson E. Marc\\'ilio-Jr, Danilo M. Eler, Rog\\'erio E. Garcia", "title": "Contrastive analysis for scatter plot-based representations of\n  dimensionality reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring multidimensional datasets is a ubiquitous part of the ones working\nwith data, where interpreting clusters is one of the main tasks. These\nmultidimensional datasets are usually encoded using scatter-plots\nrepresentations, where spatial proximity encodes similarity among data samples.\nIn the literature, techniques try to understand the scatter plot organization\nby visualizing the importance of the features for clusters definition with\ninteraction and layout enrichment strategies. However, the approaches used to\ninterpret dimensionality reduction usually do not differentiate clusters well,\nwhich hampers analysis where the focus is to understand the differences among\nclusters. This paper introduces a methodology to visually explore\nmultidimensional datasets and interpret clusters' formation based on the\ncontrastive analysis. We also introduce a bipartite graph to visually interpret\nand explore the relationship between the statistical variables used to\nunderstand how the attributes influenced cluster formation. Our methodology is\nvalidated through case studies. We explore a multivariate dataset of patients\nwith vertebral problems and two document collections, one related to news\narticles and other related to tweets about COVID-19 symptoms. Finally, we also\nvalidate our approach through quantitative results to demonstrate how it can be\nrobust enough to support multidimensional analysis.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jan 2021 01:16:31 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Marc\u00edlio-Jr", "Wilson E.", ""], ["Eler", "Danilo M.", ""], ["Garcia", "Rog\u00e9rio E.", ""]]}, {"id": "2101.12075", "submitter": "David H\\\"agele", "authors": "David H\\\"agele, Moataz Abdelaal, Ozgur S. Oguz, Marc Toussaint, Daniel\n  Weiskopf", "title": "Visualization of Nonlinear Programming for Robot Motion Planning", "comments": "8 pages, 6 figures", "journal-ref": "Proceedings of the 13th International Symposium on Visual\n  Information Communication and Interaction (2020), Article No. 10, Pages 1-8", "doi": "10.1145/3430036.3430050", "report-no": null, "categories": "cs.RO cs.HC cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear programming targets nonlinear optimization with constraints, which\nis a generic yet complex methodology involving humans for problem modeling and\nalgorithms for problem solving. We address the particularly hard challenge of\nsupporting domain experts in handling, understanding, and trouble-shooting\nhigh-dimensional optimization with a large number of constraints. Leveraging\nvisual analytics, users are supported in exploring the computation process of\nnonlinear constraint optimization. Our system was designed for robot motion\nplanning problems and developed in tight collaboration with domain experts in\nnonlinear programming and robotics. We report on the experiences from this\ndesign study, illustrate the usefulness for relevant example cases, and discuss\nthe extension to visual analytics for nonlinear programming in general.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 15:47:22 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["H\u00e4gele", "David", ""], ["Abdelaal", "Moataz", ""], ["Oguz", "Ozgur S.", ""], ["Toussaint", "Marc", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "2101.12110", "submitter": "Christian Boylston", "authors": "Christian Boylston, Beatriz Palacios, Plamen Tassev, Amy Bruckman", "title": "WallStreetBets: Positions or Ban", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  r/wallstreetbets (WallStreetBets or WSB) is a subreddit devoted to irreverent\nmemes and high-risk options trading. As of March 30, 2020, the subreddit boasts\na usership of nearly 1.1 millions subscribers and self-describes as \"if 4chan\nfound a Bloomberg terminal.\" This paper will utilize Amy Jo Kim's community\ndesign principles along with social psychology theory as frameworks to\nunderstand how this chaotic, oftentimes offensive community has developed one\nof the largest and most loyal user bases on the platform. We will further argue\nthat humor plays a vital role in promoting in-group cohesion and in providing\nan unconventional third place for traders (and thinly veiled gamblers) to seek\nsupport from each other in the form of vulgar, yet good-humored taunting.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 16:53:16 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Boylston", "Christian", ""], ["Palacios", "Beatriz", ""], ["Tassev", "Plamen", ""], ["Bruckman", "Amy", ""]]}, {"id": "2101.12202", "submitter": "Dhiraj Murthy", "authors": "Hassan Dashtian, Dhiraj Murthy", "title": "CML-COVID: A Large-Scale COVID-19 Twitter Dataset with Latent Topics,\n  Sentiment and Location Information", "comments": "6 pages, 4 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As a platform, Twitter has been a significant public space for discussion\nrelated to the COVID-19 pandemic. Public social media platforms such as Twitter\nrepresent important sites of engagement regarding the pandemic and these data\ncan be used by research teams for social, health, and other research.\nUnderstanding public opinion about COVID-19 and how information diffuses in\nsocial media is important for governments and research institutions. Twitter is\na ubiquitous public platform and, as such, has tremendous utility for\nunderstanding public perceptions, behavior, and attitudes related to COVID-19.\nIn this research, we present CML-COVID, a COVID-19 Twitter data set of\n19,298,967 million tweets from 5,977,653 unique individuals and summarize some\nof the attributes of these data. These tweets were collected between March 2020\nand July 2020 using the query terms coronavirus, covid and mask related to\nCOVID-19. We use topic modeling, sentiment analysis, and descriptive statistics\nto describe the tweets related to COVID-19 we collected and the geographical\nlocation of tweets, where available. We provide information on how to access\nour tweet dataset (archived using twarc).\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 18:59:10 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Dashtian", "Hassan", ""], ["Murthy", "Dhiraj", ""]]}, {"id": "2101.12273", "submitter": "Pegah Soleiman", "authors": "Pegah Soleiman (1), Hadi Moradi (1 and 2), Maryam Mahmoudi (3),\n  Mohyeddin Teymouri (4), Hamid Reza Pouretemad (5) ((1) School of ECE,\n  University of Tehran, North Karegar St., Tehran, Iran, (2) Intelligent\n  Systems Research Institute, SKKU, Suwon, South Korea, (3) Department of\n  Psychology, Allameh Tabatab\\'ai University, Tehran, Iran, (4) Department of\n  speech therapy, Iran university of medical sciences, Tehran, Iran (5)\n  Department of Psychology, Shahid Beheshti University, Tehran, Iran)", "title": "Teaching Turn-Taking Skills to Children with Autism using a Parrot-Like\n  Robot", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Robot Assisted Therapy is a new paradigm in many therapies such as the\ntherapy of children with autism spectrum disorder. In this paper we present the\nuse of a parrot-like robot as an assistive tool in turn taking therapy. The\ntherapy is designed in the form of a card game between a child with autism and\na therapist or the robot. The intervention was implemented in a single subject\nstudy format and the effect sizes for different turn taking variables are\ncalculated. The results show that the child robot interaction had larger effect\nsize than the child trainer effect size in most of the turn taking variables.\nFurthermore the therapist point of view on the proposed Robot Assisted Therapy\nis evaluated using a questionnaire. The therapist believes that the robot is\nappealing to children which may ease the therapy process. The therapist\nsuggested to add other functionalities and games to let children with autism to\nlearn more turn taking tasks and better generalize the learned tasks\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 20:56:35 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Soleiman", "Pegah", "", "1 and 2"], ["Moradi", "Hadi", "", "1 and 2"], ["Mahmoudi", "Maryam", ""], ["Teymouri", "Mohyeddin", ""], ["Pouretemad", "Hamid Reza", ""]]}, {"id": "2101.12284", "submitter": "Prasanth Murali", "authors": "Prasanth Murali, Javier Hernandez, Daniel McDuff, Kael Rowan, Jina\n  Suh, Mary Czerwinski", "title": "AffectiveSpotlight: Facilitating the Communication of Affective\n  Responses from Audience Members during Online Presentations", "comments": null, "journal-ref": null, "doi": "10.1145/3411764.3445235", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to monitor audience reactions is critical when delivering\npresentations. However, current videoconferencing platforms offer limited\nsolutions to support this. This work leverages recent advances in affect\nsensing to capture and facilitate communication of relevant audience signals.\nUsing an exploratory survey (N = 175), we assessed the most relevant audience\nresponses such as confusion, engagement, and head-nods. We then implemented\nAffectiveSpotlight, a Microsoft Teams bot that analyzes facial responses and\nhead gestures of audience members and dynamically spotlights the most\nexpressive ones. In a within-subjects study with 14 groups (N = 117), we\nobserved that the system made presenters significantly more aware of their\naudience, speak for a longer period of time, and self-assess the quality of\ntheir talk more similarly to the audience members, compared to two control\nconditions (randomly-selected spotlight and default platform UI). We provide\ndesign recommendations for future affective interfaces for online presentations\nbased on feedback from the study.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2021 21:18:57 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Murali", "Prasanth", ""], ["Hernandez", "Javier", ""], ["McDuff", "Daniel", ""], ["Rowan", "Kael", ""], ["Suh", "Jina", ""], ["Czerwinski", "Mary", ""]]}, {"id": "2101.12446", "submitter": "Matthew Olson", "authors": "Matthew L. Olson, Roli Khanna, Lawrence Neal, Fuxin Li, Weng-Keen Wong", "title": "Counterfactual State Explanations for Reinforcement Learning Agents via\n  Generative Deep Learning", "comments": "Full source code available at\n  https://github.com/mattolson93/counterfactual-state-explanations", "journal-ref": "Artificial Intelligence, 2021, 103455, ISSN 0004-3702", "doi": "10.1016/j.artint.2021.103455", "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Counterfactual explanations, which deal with \"why not?\" scenarios, can\nprovide insightful explanations to an AI agent's behavior. In this work, we\nfocus on generating counterfactual explanations for deep reinforcement learning\n(RL) agents which operate in visual input environments like Atari. We introduce\ncounterfactual state explanations, a novel example-based approach to\ncounterfactual explanations based on generative deep learning. Specifically, a\ncounterfactual state illustrates what minimal change is needed to an Atari game\nimage such that the agent chooses a different action. We also evaluate the\neffectiveness of counterfactual states on human participants who are not\nmachine learning experts. Our first user study investigates if humans can\ndiscern if the counterfactual state explanations are produced by the actual\ngame or produced by a generative deep learning approach. Our second user study\ninvestigates if counterfactual state explanations can help non-expert\nparticipants identify a flawed agent; we compare against a baseline approach\nbased on a nearest neighbor explanation which uses images from the actual game.\nOur results indicate that counterfactual state explanations have sufficient\nfidelity to the actual game images to enable non-experts to more effectively\nidentify a flawed RL agent compared to the nearest neighbor baseline and to\nhaving no explanation at all.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 07:43:41 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Olson", "Matthew L.", ""], ["Khanna", "Roli", ""], ["Neal", "Lawrence", ""], ["Li", "Fuxin", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "2101.12478", "submitter": "R\\'emi Petitpierre", "authors": "R\\'emi Petitpierre (Ecole polytechnique f\\'ed\\'erale de Lausanne,\n  EPFL, Switzerland)", "title": "Neural networks for semantic segmentation of historical city maps:\n  Cross-cultural performance and the impact of figurative diversity", "comments": "MSc thesis", "journal-ref": null, "doi": "10.13140/RG.2.2.10973.64484", "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we present a new semantic segmentation model for historical\ncity maps that surpasses the state of the art in terms of flexibility and\nperformance. Research in automatic map processing is largely focused on\nhomogeneous corpora or even individual maps, leading to inflexible algorithms.\nRecently, convolutional neural networks have opened new perspectives for the\ndevelopment of more generic tools. Based on two new maps corpora, the first one\ncentered on Paris and the second one gathering cities from all over the world,\nwe propose a method for operationalizing the figuration based on traditional\ncomputer vision algorithms that allows large-scale quantitative analysis. In a\nsecond step, we propose a semantic segmentation model based on neural networks\nand implement several improvements. Finally, we analyze the impact of map\nfiguration on segmentation performance and evaluate future ways to improve the\nrepresentational flexibility of neural networks. To conclude, we show that\nthese networks are able to semantically segment map data of a very large\nfigurative diversity with efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 09:08:12 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Petitpierre", "R\u00e9mi", "", "Ecole polytechnique f\u00e9d\u00e9rale de Lausanne,\n  EPFL, Switzerland"]]}, {"id": "2101.12511", "submitter": "Michael Aupetit", "authors": "Michael Aupetit", "title": "Aquanims: Area-Preserving Animated Transitions in Statistical Data\n  Graphics based on a Hydraulic Metaphor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose \"aquanims\" as new design metaphors for animated transitions that\npreserve displayed areas during the transformation. Animated transitions are\nused to facilitate understanding of graphical transformations between different\nvisualizations. Area is key information to preserve during filtering or\nordering transitions of area-based charts like bar charts, histograms,\ntreemaps, or mosaic plots. As liquids are incompressible fluids, we use a\nhydraulic metaphor to convey the sense of area preservation during animated\ntransitions: in aquanims, graphical objects can change shape, position, color,\nand even connectedness but not displayed area, as for a liquid contained in a\ntransparent vessel or transferred between such vessels communicating through\nhidden pipes. We present various aquanims for product plots like bar charts and\nhistograms to accommodate changes in data, in the ordering of bars or in a\nnumber of bins, and to provide animated tips. We also consider confusion\nmatrices visualized as fluctuation diagrams and mosaic plots, and show how\naquanims can be used to ease the understanding of different classification\nerrors of real data.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 10:35:42 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Aupetit", "Michael", ""]]}, {"id": "2101.12533", "submitter": "Anders Sundnes L{\\o}vlie", "authors": "Anders Sundnes L{\\o}vlie, Karin Ryding, Jocelyn Spence, Paulina\n  Rajkowska, Annika Waern, Tim Wray, Steve Benford, William Preston, Emily\n  Clare-Thorn", "title": "Playing games with Tito: Designing hybrid museum experiences for\n  critical play", "comments": "Accepted for publication in the ACM Journal on Computing and Cultural\n  Heritage, https://dl.acm.org/journal/jocch", "journal-ref": null, "doi": "10.1145/3446620", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article brings together two distinct, but related perspectives on\nplayful museum experiences: Critical play and hybrid design. The article\nexplores the challenges involved in combining these two perspectives, through\nthe design of two hybrid museum experiences that aimed to facilitate critical\nplay with/in the collections of the Museum of Yugoslavia and the highly\ncontested heritage they represent. Based on reflections from the design process\nas well as feedback from test users we describe a series of challenges:\nChallenging the norms of visitor behaviour, challenging the role of the\nartefact, and challenging the curatorial authority. In conclusion we outline\nsome possible design strategies to address these challenges.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 11:43:42 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["L\u00f8vlie", "Anders Sundnes", ""], ["Ryding", "Karin", ""], ["Spence", "Jocelyn", ""], ["Rajkowska", "Paulina", ""], ["Waern", "Annika", ""], ["Wray", "Tim", ""], ["Benford", "Steve", ""], ["Preston", "William", ""], ["Clare-Thorn", "Emily", ""]]}, {"id": "2101.12715", "submitter": "Tim Draws", "authors": "Tim Draws, Zolt\\'an Szl\\'avik, Benjamin Timmermans, Nava Tintarev,\n  Kush R. Varshney, Michael Hind", "title": "Disparate Impact Diminishes Consumer Trust Even for Advantaged Users", "comments": null, "journal-ref": "Persuasive Technology, Cham, 2021, p. 135-149", "doi": "10.1007/978-3-030-79460-6_11", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems aiming to aid consumers in their decision-making (e.g., by\nimplementing persuasive techniques) are more likely to be effective when\nconsumers trust them. However, recent research has demonstrated that the\nmachine learning algorithms that often underlie such technology can act\nunfairly towards specific groups (e.g., by making more favorable predictions\nfor men than for women). An undesired disparate impact resulting from this kind\nof algorithmic unfairness could diminish consumer trust and thereby undermine\nthe purpose of the system. We studied this effect by conducting a\nbetween-subjects user study investigating how (gender-related) disparate impact\naffected consumer trust in an app designed to improve consumers' financial\ndecision-making. Our results show that disparate impact decreased consumers'\ntrust in the system and made them less likely to use it. Moreover, we find that\ntrust was affected to the same degree across consumer groups (i.e., advantaged\nand disadvantaged users) despite both of these consumer groups recognizing\ntheir respective levels of personal benefit. Our findings highlight the\nimportance of fairness in consumer-oriented artificial intelligence systems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2021 18:25:09 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 17:38:45 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 16:27:23 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Draws", "Tim", ""], ["Szl\u00e1vik", "Zolt\u00e1n", ""], ["Timmermans", "Benjamin", ""], ["Tintarev", "Nava", ""], ["Varshney", "Kush R.", ""], ["Hind", "Michael", ""]]}]