[{"id": "1808.00048", "submitter": "Christos Rodosthenous", "authors": "Christos Rodosthenous and Loizos Michael", "title": "Web-STAR: A Visual Web-Based IDE for a Story Comprehension System", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Web-STAR, an online platform for story understanding built on top\nof the STAR reasoning engine for STory comprehension through ARgumentation. The\nplatform includes a web-based IDE, integration with the STAR system, and a web\nservice infrastructure to support integration with other systems that rely on\nstory understanding functionality to complete their tasks. The platform also\ndelivers a number of \"social\" features, including a community repository for\npublic story sharing with a built-in commenting system, and tools for\ncollaborative story editing that can be used for team development projects and\nfor educational purposes.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 05:09:27 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Rodosthenous", "Christos", ""], ["Michael", "Loizos", ""]]}, {"id": "1808.00118", "submitter": "Serguei Mokhov", "authors": "Serguei A. Mokhov, Miao Song, Jashanjot Singh, Joey Paquet, Mourad\n  Debbabi, Sudhir Mudur", "title": "Toward Multimodal Interaction in Scalable Visual Digital Evidence\n  Visualization Using Computer Vision Techniques and ISS", "comments": "reformatted; ICPRAI 2018 conference proceedings, pp. 151-157,\n  CENPARMI, Concordia University, Montreal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization requirements in Forensic Lucid have to do with different levels\nof case knowledge abstraction, representation, aggregation, as well as the\noperational aspects as the final long-term goal of this proposal. It\nencompasses anything from the finer detailed representation of hierarchical\ncontexts to Forensic Lucid programs, to the documented evidence and its\nmanagement, its linkage to programs, to evaluation, and to the management of\nGIPSY software networks. This includes an ability to arbitrarily switch between\nthose views combined with usable multimodal interaction. The purpose is to\ndetermine how the findings can be applied to Forensic Lucid and investigation\ncase management. It is also natural to want a convenient and usable evidence\nvisualization, its semantic linkage and the reasoning machinery for it. Thus,\nwe propose a scalable management, visualization, and evaluation of digital\nevidence using the modified interactive 3D documentary system - Illimitable\nSpace System - (ISS) to represent, semantically link, and provide a usable\ninterface to digital investigators that is navigable via different multimodal\ninteraction techniques using Computer Vision techniques including gestures, as\nwell as eye-gaze and audio.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 00:28:32 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Mokhov", "Serguei A.", ""], ["Song", "Miao", ""], ["Singh", "Jashanjot", ""], ["Paquet", "Joey", ""], ["Debbabi", "Mourad", ""], ["Mudur", "Sudhir", ""]]}, {"id": "1808.00121", "submitter": "Xiaodong Wu", "authors": "Xiaodong Wu, Lyn Bartram", "title": "Social Robots for People with Developmental Disabilities: A User Study\n  on Design Features of a Graphical User Interface", "comments": "Conference Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social robots, also known as service or assistant robots, have been developed\nto improve the quality of human life in recent years. The design of socially\ncapable and intelligent robots can vary, depending on the target user groups.\nIn this work, we assess the effect of social robots' roles, functions, and\ncommunication approaches in the context of a social agent providing service or\nentertainment to users with developmental disabilities. In this paper, we\ndescribe an exploratory study of interface design for a social robot that\nassists people suffering from developmental disabilities. We developed series\nof prototypes and tested one in a user study that included three residents with\nvarious function levels. This entire study had been recorded for the following\nqualitative data analysis. Results show that each design factor played a\ndifferent role in delivering information and in increasing engagement. We also\nnote that some of the fundamental design principles that would work for\nordinary users did not apply to our target user group. We conclude that social\nrobots could benefit our target users, and acknowledge that these robots were\nnot suitable for certain scenarios based on the feedback from our users.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 00:45:38 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Wu", "Xiaodong", ""], ["Bartram", "Lyn", ""]]}, {"id": "1808.00196", "submitter": "Yang Wang", "authors": "Jiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, David S. Ebert", "title": "Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of\n  Machine Learning Models", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2018.2864499", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretation and diagnosis of machine learning models have gained renewed\ninterest in recent years with breakthroughs in new approaches. We present\nManifold, a framework that utilizes visual analysis techniques to support\ninterpretation, debugging, and comparison of machine learning models in a more\ntransparent and interactive manner. Conventional techniques usually focus on\nvisualizing the internal logic of a specific model type (i.e., deep neural\nnetworks), lacking the ability to extend to a more complex scenario where\ndifferent model types are integrated. To this end, Manifold is designed as a\ngeneric framework that does not rely on or access the internal logic of the\nmodel and solely observes the input (i.e., instances or features) and the\noutput (i.e., the predicted result and probability distribution). We describe\nthe workflow of Manifold as an iterative process consisting of three major\nphases that are commonly involved in the model development and diagnosis\nprocess: inspection (hypothesis), explanation (reasoning), and refinement\n(verification). The visual components supporting these tasks include a\nscatterplot-based visual summary that overviews the models' outcome and a\ncustomizable tabular view that reveals feature discrimination. We demonstrate\ncurrent applications of the framework on the classification and regression\ntasks and discuss other potential machine learning use scenarios where Manifold\ncan be applied.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 07:04:08 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Zhang", "Jiawei", ""], ["Wang", "Yang", ""], ["Molino", "Piero", ""], ["Li", "Lezhi", ""], ["Ebert", "David S.", ""]]}, {"id": "1808.00356", "submitter": "Yongsung Kim", "authors": "Yongsung Kim, Adam Fourney, Ece Kamar", "title": "Studying Preferences and Concerns about Information Disclosure in Email\n  Notifications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of network-connected devices and applications has resulted\nin people receiving dozens, or hundreds, of notifications per day. When people\nare in the presence of others, each notification poses some risk of accidental\ninformation disclosure; onlookers may see notifications appear above the lock\nscreen of a mobile phone, on the periphery of a desktop or laptop display, or\nprojected onscreen during a presentation. In this paper, we quantify the\nprevalence of these accidental disclosures in the context of email\nnotifications, and we study people's relevant preferences and concerns. Our\nresults are compiled from an exploratory retrospective survey of 131\nrespondents, and a separate contextual-labeling study in which 169 participants\nlabeled 1,040 meeting-email pairs. We find that, for 53% of people, at least 1\nin 10 email notifications poses an information disclosure risk. We also find\nthat the real or perceived severity of these risks depend both on user\ncharacteristics and attributes of the meeting or email (e.g. the number of\nrecipients or attendees). We conclude by exploring machine learning algorithms\nto predict people's comfort levels given an email notification and a context,\nthen we present implications for the design of future contextually-relevant\nnotification systems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:07:14 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Kim", "Yongsung", ""], ["Fourney", "Adam", ""], ["Kamar", "Ece", ""]]}, {"id": "1808.00388", "submitter": "Stefan R\\\"abiger", "authors": "Stefan R\\\"abiger (1), Y\\\"ucel Sayg{\\i}n (1), Myra Spiliopoulou (2)\n  ((1) Sabanc{\\i} University (Istanbul, Turkey), (2) Otto-von-Guericke\n  University (Magdeburg, Germany))", "title": "How Does Tweet Difficulty Affect Labeling Performance of Annotators?", "comments": "13 pages, 4 figures, \"for dataset, see\n  https://www.researchgate.net/publication/325180810_Infsci2017_dataset\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Crowdsourcing is a popular means to obtain labeled data at moderate costs,\nfor example for tweets, which can then be used in text mining tasks. To\nalleviate the problem of low-quality labels in this context, multiple human\nfactors have been analyzed to identify and deal with workers who provide such\nlabels. However, one aspect that has been rarely considered is the inherent\ndifficulty of tweets to be labeled and how this affects the reliability of the\nlabels that annotators assign to such tweets. Therefore, we investigate in this\npreliminary study this connection using a hierarchical sentiment labeling task\non Twitter. We find that there is indeed a relationship between both factors,\nassuming that annotators have labeled some tweets before: labels assigned to\neasy tweets are more reliable than those assigned to difficult tweets.\nTherefore, training predictors on easy tweets enhances the performance by up to\n6% in our experiment. This implies potential improvements for active learning\ntechniques and crowdsourcing.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 15:55:07 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["R\u00e4biger", "Stefan", ""], ["Sayg\u0131n", "Y\u00fccel", ""], ["Spiliopoulou", "Myra", ""]]}, {"id": "1808.00703", "submitter": "Hammad Haleem", "authors": "Hammad Haleem, Yong Wang, Abishek Puri, Sahil Wadhwa and Huamin Qu", "title": "Evaluating the Readability of Force Directed Graph Layouts: A Deep\n  Learning Approach", "comments": "This work has been accepted at IEEE CG&A", "journal-ref": null, "doi": "10.1109/MCG.2018.2881501", "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Existing graph layout algorithms are usually not able to optimize all the\naesthetic properties desired in a graph layout. To evaluate how well the\ndesired visual features are reflected in a graph layout, many readability\nmetrics have been proposed in the past decades. However, the calculation of\nthese readability metrics often requires access to the node and edge\ncoordinates and is usually computationally inefficient, especially for dense\ngraphs. Importantly, when the node and edge coordinates are not accessible, it\nbecomes impossible to evaluate the graph layouts quantitatively. In this paper,\nwe present a novel deep learning-based approach to evaluate the readability of\ngraph layouts by directly using graph images. A convolutional neural network\narchitecture is proposed and trained on a benchmark dataset of graph images,\nwhich is composed of synthetically-generated graphs and graphs created by\nsampling from real large networks. Multiple representative readability metrics\n(including edge crossing, node spread, and group overlap) are considered in the\nproposed approach. We quantitatively compare our approach to traditional\nmethods and qualitatively evaluate our approach using a case study and\nvisualizing convolutional layers. This work is a first step towards using deep\nlearning based methods to evaluate images from the visualization field\nquantitatively.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 07:57:59 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 23:20:45 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Haleem", "Hammad", ""], ["Wang", "Yong", ""], ["Puri", "Abishek", ""], ["Wadhwa", "Sahil", ""], ["Qu", "Huamin", ""]]}, {"id": "1808.00876", "submitter": "Che-Wei Huang", "authors": "Che-Wei Huang and Shrikanth S. Narayanan", "title": "Normalization Before Shaking Toward Learning Symmetrically Distributed\n  Representation Without Margin in Speech Emotion Recognition", "comments": "Submission to The IEEE Transactions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.MM cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is crucial to the success of many practical deep learning\nmodels, in particular in a more often than not scenario where there are only a\nfew to a moderate number of accessible training samples. In addition to weight\ndecay, data augmentation and dropout, regularization based on multi-branch\narchitectures, such as Shake-Shake regularization, has been proven successful\nin many applications and attracted more and more attention. However, beyond\nmodel-based representation augmentation, it is unclear how Shake-Shake\nregularization helps to provide further improvement on classification tasks,\nlet alone the baffling interaction between batch normalization and shaking. In\nthis work, we present our investigation on Shake-Shake regularization, drawing\nconnections to the vicinal risk minimization principle and discriminative\nfeature learning in verification tasks. Furthermore, we identify a strong\nresemblance between batch normalized residual blocks and batch normalized\nrecurrent neural networks, where both of them share a similar convergence\nbehavior, which could be mitigated by a proper initialization of batch\nnormalization. Based on the findings, our experiments on speech emotion\nrecognition demonstrate simultaneously an improvement on the classification\naccuracy and a reduction on the generalization gap both with statistical\nsignificance.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 15:57:57 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 23:21:54 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Huang", "Che-Wei", ""], ["Narayanan", "Shrikanth S.", ""]]}, {"id": "1808.00981", "submitter": "Trevor Buteau", "authors": "Trevor Buteau and Damian Lyons", "title": "Constructionist Steps Towards an Autonomously Empathetic System", "comments": "Submitted for SIGCHI ICMI 2018's Late-Breaking-Work track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prior efforts to create an autonomous computer system capable of predicting\nwhat a human being is thinking or feeling from facial expression data have been\nlargely based on outdated, inaccurate models of how emotions work that rely on\nmany scientifically questionable assumptions. In our research, we are creating\nan empathetic system that incorporates the latest provable scientific\nunderstanding of emotions: that they are constructs of the human mind, rather\nthan universal expressions of distinct internal states. Thus, our system uses a\nuser-dependent method of analysis and relies heavily on contextual information\nto make predictions about what subjects are experiencing. Our system's accuracy\nand therefore usefulness are built on provable ground truths that prohibit the\ndrawing of inaccurate conclusions that other systems could too easily make.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 18:14:23 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Buteau", "Trevor", ""], ["Lyons", "Damian", ""]]}, {"id": "1808.01640", "submitter": "Birgitta Dresp-Langley", "authors": "Birgitta Dresp-Langley", "title": "Principles of perceptual grouping: implications for image-guided surgery", "comments": null, "journal-ref": "2015, Frontiers in Psychology, 6, 1565", "doi": null, "report-no": null, "categories": "cs.HC eess.IV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gestalt theory has provided perceptual science with a conceptual framework\nwhich has inspired researchers ever since, taking the field of perceptual\norganization into the 21st century. This opinion article discusses the\nimportance of rules of perceptual organization for the testing and design of\nvisual interface technology. It is argued that major Gestalt principles, such\nas the law of good continuation or the principle of Praegnanz (suggested\ntranslation: salience), taken as examples here, are important to our\nunderstanding of visual image processing by a human observer. Perceptual\nintegration of contrast information across collinear space, and the\norganization of objects in the 2D image plane into figure and ground are of a\nparticular importance here. Visual interfaces for image-guided surgery\nillustrate the criticality of these two types of perceptual processes for\nreliable decision making and action. It is concluded that Gestalt theory\ncontinues to generate powerful concepts and insights for perceptual science\nplaced within the context of major technological challenges of today.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 16:04:25 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Dresp-Langley", "Birgitta", ""]]}, {"id": "1808.01680", "submitter": "Toan Nguyen", "authors": "Toan Nguyen, Aditi Roy, Nasir Memon", "title": "Kid on The Phone! Toward Automatic Detection of Children on Mobile\n  Devices", "comments": "Under peer review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies have shown that children can be exposed to smart devices at a very\nearly age. This has important implications on research in children-computer\ninteraction, children online safety and early education. Many systems have been\nbuilt based on such research. In this work, we present multiple techniques to\nautomatically detect the presence of a child on a smart device, which could be\nused as the first step on such systems. Our methods distinguish children from\nadults based on behavioral differences while operating a touch-enabled modern\ncomputing device. Behavioral differences are extracted from data recorded by\nthe touchscreen and built-in sensors. To evaluate the effectiveness of the\nproposed methods, a new data set has been created from 50 children and adults\nwho interacted with off-the-shelf applications on smart phones. Results show\nthat it is possible to achieve 99% accuracy and less than 0.5% error rate after\n8 consecutive touch gestures using only touch information or 5 seconds of\nsensor reading. If information is used from multiple sensors, then only after 3\ngestures, similar performance could be achieved.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 19:59:35 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Nguyen", "Toan", ""], ["Roy", "Aditi", ""], ["Memon", "Nasir", ""]]}, {"id": "1808.01745", "submitter": "Denys Matthies", "authors": "Denys J.C. Matthies, Laura Milena Daza Parra, Bodo Urban", "title": "Scaling notifications beyond alerts: from subtly drawing attention up to\n  forcing the user to take action", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New computational devices, in particular wearable devices, offer the unique\nproperty of always being available and thus to be able to constantly update the\nuser with information, such as by notifications. While research has been done\nin sophisticated notifications, devices today mainly stick to a binary level of\ninformation, while they are either attention drawing or silent. In this paper,\nwe want to go further and propose scalable notifications, which adjust the\nintensity reaching from subtle to obtrusive and even going beyond that level,\nwhile forcing the user to take action. To illustrate the technical feasibility\nand validity of this concept, we developed three prototypes providing\nmechano-pressure, thermal, and electrical feedback and evaluated them in\ndifferent lab studies. Our first prototype provides subtle poking through to\nhigh and frequent pressure on the user's spine, which creates a significantly\nimproved back posture. In a second scenario, the users are enabled to perceive\nthe overuse of a drill by an increased temperature on the palm of a hand until\nthe heat is intolerable and the users are forced to eventually put down the\ntool. The last project comprises a speed control in a driving simulation, while\nelectric muscle stimulation on the users' legs conveys information on changing\nthe car's speed by a perceived tingling until the system independently forces\nthe foot to move. Although our selected scenarios are long way from being\nrealistic, we see these lab studies as a means to validate our\nproof-of-concept. In conclusion, all studies' findings support the feasibility\nof our concept of a scalable notification system, including the system of\nforced intervention. While we envisage the implementation of our\nproof-of-concept into future wearables, more realistic application scenarios\nare worthy of exploration.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 06:46:02 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Matthies", "Denys J. C.", ""], ["Parra", "Laura Milena Daza", ""], ["Urban", "Bodo", ""]]}, {"id": "1808.02324", "submitter": "Omid Mohamad Nezami", "authors": "Omid Mohamad Nezami, Mark Dras, Len Hamey, Deborah Richards, Stephen\n  Wan, Cecile Paris", "title": "Automatic Recognition of Student Engagement using Deep Learning and\n  Facial Expression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engagement is a key indicator of the quality of learning experience, and one\nthat plays a major role in developing intelligent educational interfaces. Any\nsuch interface requires the ability to recognise the level of engagement in\norder to respond appropriately; however, there is very little existing data to\nlearn from, and new data is expensive and difficult to acquire. This paper\npresents a deep learning model to improve engagement recognition from images\nthat overcomes the data sparsity challenge by pre-training on readily available\nbasic facial expression data, before training on specialised engagement data.\nIn the first of two steps, a facial expression recognition model is trained to\nprovide a rich face representation using deep learning. In the second step, we\nuse the model's weights to initialize our deep learning based model to\nrecognize engagement; we term this the engagement model. We train the model on\nour new engagement recognition dataset with 4627 engaged and disengaged\nsamples. We find that the engagement model outperforms effective deep learning\narchitectures that we apply for the first time to engagement recognition, as\nwell as approaches using histogram of oriented gradients and support vector\nmachines.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 12:38:20 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 11:32:39 GMT"}, {"version": "v3", "created": "Sat, 24 Nov 2018 09:01:53 GMT"}, {"version": "v4", "created": "Fri, 28 Jun 2019 03:29:39 GMT"}, {"version": "v5", "created": "Mon, 8 Jul 2019 13:29:02 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Nezami", "Omid Mohamad", ""], ["Dras", "Mark", ""], ["Hamey", "Len", ""], ["Richards", "Deborah", ""], ["Wan", "Stephen", ""], ["Paris", "Cecile", ""]]}, {"id": "1808.02444", "submitter": "Iryna Mintii", "authors": "Anna M. Horlo, Iryna S. Mintii", "title": "Adapting website design for people with color-blindness", "comments": "6 pages, 2 figures, in Ukrainian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of the study is the description of problem of developing web design\nfor people with color blindness. The objectives of the study are familiarising\nwith the exiting algorithms of simulation color blindness and searching the\nmost appropriate color models to realize a filter of disputed colors. The\nobject of the study is the convertation of color models and algorithms of\nfiltration. The subject of the study are methods of recognition disputed\ncolors. In the study were investigated the problems of color blind people,\nexamined the basic concepts of trichromatic color vision theory, substantiated\nthe necessity of changing different types of color models, given formulas\nconvertation from RGB-color model to HSL-color model, systematized the\nalgorithms of imitation and filtration of colors for different types of\ndichromacy: protanopia, deuteranopia and tritanopia. The results of the study\nare planned using in development of adapting website design for people with\ncolor blindness.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 06:11:53 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Horlo", "Anna M.", ""], ["Mintii", "Iryna S.", ""]]}, {"id": "1808.02502", "submitter": "Ethan Kerzner", "authors": "Ethan Kerzner and Sarah Goodwin and Jason Dykes and Sara Jones and\n  Miriah Meyer", "title": "A Framework for Creative-Visualization Opportunities Workshops", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2018.2865241", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied visualization researchers often work closely with domain\ncollaborators to explore new and useful applications of visualization. The\nearly stages of collaborations are typically time consuming for all\nstakeholders as researchers piece together an understanding of domain\nchallenges from disparate discussions and meetings. A number of recent\nprojects, however, report on the use of creative visualization-opportunities\n(CVO) workshops to accelerate the early stages of applied work, eliciting a\nwealth of requirements in a few days of focused work. Yet, there is no\nestablished guidance for how to use such workshops effectively. In this paper,\nwe present the results of a 2-year collaboration in which we analyzed the use\nof 17 workshops in 10 visualization contexts. Its primary contribution is a\nframework for CVO workshops that: 1) identifies a process model for using\nworkshops; 2) describes a structure of what happens within effective workshops;\n3) recommends 25 actionable guidelines for future workshops; and 4) presents an\nexample workshop and workshop methods. The creation of this framework\nexemplifies the use of critical reflection to learn about visualization in\npractice from diverse studies and experience.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 22:12:30 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Kerzner", "Ethan", ""], ["Goodwin", "Sarah", ""], ["Dykes", "Jason", ""], ["Jones", "Sara", ""], ["Meyer", "Miriah", ""]]}, {"id": "1808.02785", "submitter": "Adel Al-Dawood", "authors": "Adel Al-Dawood, Norah Abokhodair, Houda El Mimouni, Svetlana Yarosh", "title": "Against Marrying a Stranger Marital Matchmaking Technologies in Saudi\n  Arabia", "comments": "11 pages, 1 table, DIS 2017", "journal-ref": "In Proceedings of the 2017 Conference on Designing Interactive\n  Systems (DIS '17). ACM, New York, NY, USA, 1013-1024", "doi": "10.1145/3064663.3064683", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Websites and applications that match and connect individuals for romantic\npurposes are commonly used in the Western world. However, there have not been\nmany previous investigations focusing on cultural factors that affect the\nadoption of similar technologies in religiously conservative non-Western\ncultures. In this study, we examine the socio-technical and cultural factors\nthat influence the perceptions and use of matchmaking technologies in Saudi\nArabia. We report the methods and findings of interviews with 18 Saudi\nnationals (nine males and nine females) with diverse demographics and\nbackgrounds. We provide qualitatively generated insights into the major themes\nreported by our participants related to the common approaches to matchmaking,\nthe current role of technology, and concerns regarding matchmaking technologies\nin this cultural con-text. We relate these themes to specific implications for\ndesigning marital matchmaking technologies in Saudi Arabia and we outline\nopportunities for future investigations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 14:09:45 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Al-Dawood", "Adel", ""], ["Abokhodair", "Norah", ""], ["Mimouni", "Houda El", ""], ["Yarosh", "Svetlana", ""]]}, {"id": "1808.02956", "submitter": "Dongrui Wu", "authors": "Chenfeng Guo and Dongrui Wu", "title": "Feature Dimensionality Reduction for Video Affect Classification: A\n  Comparative Study", "comments": "1st Asian Affective Computing and Intelligent Interaction Conference,\n  Beijing, May 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective computing has become a very important research area in\nhuman-machine interaction. However, affects are subjective, subtle, and\nuncertain. So, it is very difficult to obtain a large number of labeled\ntraining samples, compared with the number of possible features we could\nextract. Thus, dimensionality reduction is critical in affective computing.\nThis paper presents our preliminary study on dimensionality reduction for\naffect classification. Five popular dimensionality reduction approaches are\nintroduced and compared. Experiments on the DEAP dataset showed that no\napproach can universally outperform others, and performing classification using\nthe raw features directly may not always be a bad choice.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:29:45 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Guo", "Chenfeng", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.03114", "submitter": "Alex B\\\"auerle", "authors": "Alex B\\\"auerle, Heiko Neumann and Timo Ropinski", "title": "Classifier-Guided Visual Correction of Noisy Labels for Image\n  Classification Tasks", "comments": null, "journal-ref": null, "doi": "10.1111/cgf.13973", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training data plays an essential role in modern applications of machine\nlearning. However, gathering labeled training data is time-consuming.\nTherefore, labeling is often outsourced to less experienced users, or\ncompletely automated. This can introduce errors, which compromise valuable\ntraining data, and lead to suboptimal training results. We thus propose a novel\napproach that uses the power of pretrained classifiers to visually guide users\nto noisy labels, and let them interactively check error candidates, to\niteratively improve the training data set. To systematically investigate\ntraining data, we propose a categorization of labeling errors into three\ndifferent types, based on an analysis of potential pitfalls in label\nacquisition processes. For each of these types, we present approaches to\ndetect, reason about, and resolve error candidates, as we propose measures and\nvisual guidance techniques to support machine learning users. Our approach has\nbeen used to spot errors in well-known machine learning benchmark data sets,\nand we tested its usability during a user evaluation. While initially developed\nfor images, the techniques presented in this paper are independent of the\nclassification algorithm, and can also be extended to many other types of\ntraining data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 12:34:33 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 12:07:13 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 12:20:21 GMT"}, {"version": "v4", "created": "Mon, 6 Apr 2020 13:55:12 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["B\u00e4uerle", "Alex", ""], ["Neumann", "Heiko", ""], ["Ropinski", "Timo", ""]]}, {"id": "1808.03281", "submitter": "Emilio Ferrara", "authors": "Adam Badawy and Kristina Lerman and Emilio Ferrara", "title": "Who Falls for Online Political Manipulation?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media, once hailed as a vehicle for democratization and the promotion\nof positive social change across the globe, are under attack for becoming a\ntool of political manipulation and spread of disinformation. A case in point is\nthe alleged use of trolls by Russia to spread malicious content in Western\nelections. This paper examines the Russian interference campaign in the 2016 US\npresidential election on Twitter. Our aim is twofold: first, we test whether\npredicting users who spread trolls' content is feasible in order to gain\ninsight on how to contain their influence in the future; second, we identify\nfeatures that are most predictive of users who either intentionally or\nunintentionally play a vital role in spreading this malicious content. We\ncollected a dataset with over 43 million elections-related posts shared on\nTwitter between September 16 and November 9, 2016, by about 5.7 million users.\nThis dataset includes accounts associated with the Russian trolls identified by\nthe US Congress. Proposed models are able to very accurately identify users who\nspread the trolls' content (average AUC score of 96%, using 10-fold\nvalidation). We show that political ideology, bot likelihood scores, and some\nactivity-related account meta data are the most predictive features of whether\na user spreads trolls' content or not.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 18:00:05 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Badawy", "Adam", ""], ["Lerman", "Kristina", ""], ["Ferrara", "Emilio", ""]]}, {"id": "1808.03338", "submitter": "Weixuan Chen", "authors": "Weixuan Chen, Daniel McDuff", "title": "DeepMag: Source Specific Motion Magnification Using Gradient Ascent", "comments": "24 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important physical phenomena involve subtle signals that are difficult\nto observe with the unaided eye, yet visualizing them can be very informative.\nCurrent motion magnification techniques can reveal these small temporal\nvariations in video, but require precise prior knowledge about the target\nsignal, and cannot deal with interference motions at a similar frequency. We\npresent DeepMag an end-to-end deep neural video-processing framework based on\ngradient ascent that enables automated magnification of subtle color and motion\nsignals from a specific source, even in the presence of large motions of\nvarious velocities. While the approach is generalizable, the advantages of\nDeepMag are highlighted via the task of video-based physiological\nvisualization. Through systematic quantitative and qualitative evaluation of\nthe approach on videos with different levels of head motion, we compare the\nmagnification of pulse and respiration to existing state-of-the-art methods.\nOur method produces magnified videos with substantially fewer artifacts and\nblurring whilst magnifying the physiological changes by a similar degree.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 20:36:57 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Chen", "Weixuan", ""], ["McDuff", "Daniel", ""]]}, {"id": "1808.03413", "submitter": "Zhenliang Zhang", "authors": "Zhenliang Zhang, Dongdong Weng, Haiyan Jiang, Yue Liu, Yongtian Wang", "title": "Inverse Augmented Reality: A Virtual Agent's Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework called inverse augmented reality (IAR) which describes\nthe scenario that a virtual agent living in the virtual world can observe both\nvirtual objects and real objects. This is different from the traditional\naugmented reality. The traditional virtual reality, mixed reality and augmented\nreality are all generated for humans, i.e., they are human-centered frameworks.\nOn the contrary, the proposed inverse augmented reality is a virtual\nagent-centered framework, which represents and analyzes the reality from a\nvirtual agent's perspective. In this paper, we elaborate the framework of\ninverse augmented reality to argue the equivalence of the virtual world and the\nphysical world regarding the whole physical structure.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 05:23:37 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Zhang", "Zhenliang", ""], ["Weng", "Dongdong", ""], ["Jiang", "Haiyan", ""], ["Liu", "Yue", ""], ["Wang", "Yongtian", ""]]}, {"id": "1808.03845", "submitter": "Nicholas Charles Landolfi", "authors": "Nicholas C. Landolfi and Anca D. Dragan", "title": "Social Cohesion in Autonomous Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous cars can perform poorly for many reasons. They may have perception\nissues, incorrect dynamics models, be unaware of obscure rules of human traffic\nsystems, or follow certain rules too conservatively. Regardless of the exact\nfailure mode of the car, often human drivers around the car are behaving\ncorrectly. For example, even if the car does not know that it should pull over\nwhen an ambulance races by, other humans on the road will know and will pull\nover. We propose to make socially cohesive cars that leverage the behavior of\nnearby human drivers to act in ways that are safer and more socially\nacceptable. The simple intuition behind our algorithm is that if all the humans\nare consistently behaving in a particular way, then the autonomous car probably\nshould too. We analyze the performance of our algorithm in a variety of\nscenarios and conduct a user study to assess people's attitudes towards\nsocially cohesive cars. We find that people are surprisingly tolerant of\nmistakes that cohesive cars might make in order to get the benefits of driving\nin a car with a safer, or even just more socially acceptable behavior.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 18:12:56 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 15:36:41 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Landolfi", "Nicholas C.", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1808.03977", "submitter": "Megan Strait", "authors": "Megan Strait", "title": "Conceptualization and Validation of a Novel Protocol for Investigating\n  the Uncanny Valley", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loosely based on principles of similarity-attraction, robots intended for\nsocial contexts are being designed with increasing human similarity to\nfacilitate their reception by and communication with human interactants.\nHowever, the observation of an uncanny valley - the phenomenon in which certain\nhumanlike entities provoke dislike instead of liking - has lead some to caution\nagainst this practice. Substantial evidence supports both of these contrasting\nperspectives on the design of social technologies. Yet, owing to both empirical\nand theoretical inconsistencies, the relationship between anthropomorphic\ndesign and people's liking of the technology remains poorly understood.\n  Here we present three studies which investigate people's explicit ratings of\nand behavior towards a large sample of real-world robots. The results show a\nprofound \"valley effect\" on people's \\emph{willingness} to interact with\nhumanlike robots, thus highlighting the formidable design challenge the uncanny\nvalley poses for social robotics. In addition to advancing uncanny valley\ntheory, Studies 2 and 3 contribute and validate a novel laboratory task for\nobjectively measuring people's perceptions of humanlike robots.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 18:14:13 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Strait", "Megan", ""]]}, {"id": "1808.04228", "submitter": "Zhan Yang", "authors": "Zhan Yang, Osolo Ian Raymond, ChengYuan Zhang, Ying Wan, Jun Long", "title": "DFTerNet: Towards 2-bit Dynamic Fusion Networks for Accurate Human\n  Activity Recognition", "comments": "19 pages, 5 figures, 6 tables, accepted by IEEE Access", "journal-ref": "IEEE ACCESS, vol. 6, pp. 56750-56764, 2018", "doi": "10.1109/ACCESS.2018.2873315", "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (DCNNs) are currently popular in human\nactivity recognition applications. However, in the face of modern artificial\nintelligence sensor-based games, many research achievements cannot be\npractically applied on portable devices. DCNNs are typically resource-intensive\nand too large to be deployed on portable devices, thus this limits the\npractical application of complex activity detection. In addition, since\nportable devices do not possess high-performance Graphic Processing Units\n(GPUs), there is hardly any improvement in Action Game (ACT) experience.\nBesides, in order to deal with multi-sensor collaboration, all previous human\nactivity recognition models typically treated the representations from\ndifferent sensor signal sources equally. However, distinct types of activities\nshould adopt different fusion strategies. In this paper, a novel scheme is\nproposed. This scheme is used to train 2-bit Convolutional Neural Networks with\nweights and activations constrained to {-0.5,0,0.5}. It takes into account the\ncorrelation between different sensor signal sources and the activity types.\nThis model, which we refer to as DFTerNet, aims at producing a more reliable\ninference and better trade-offs for practical applications. Our basic idea is\nto exploit quantization of weights and activations directly in pre-trained\nfilter banks and adopt dynamic fusion strategies for different activity types.\nExperiments demonstrate that by using dynamic fusion strategy can exceed the\nbaseline model performance by up to ~5% on activity recognition like\nOPPORTUNITY and PAMAP2 datasets. Using the quantization method proposed, we\nwere able to achieve performances closer to that of full-precision counterpart.\nThese results were also verified using the UniMiB-SHAR dataset. In addition,\nthe proposed method can achieve ~9x acceleration on CPUs and ~11x memory\nsaving.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 02:33:34 GMT"}, {"version": "v2", "created": "Sat, 29 Sep 2018 04:47:24 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Yang", "Zhan", ""], ["Raymond", "Osolo Ian", ""], ["Zhang", "ChengYuan", ""], ["Wan", "Ying", ""], ["Long", "Jun", ""]]}, {"id": "1808.04244", "submitter": "Dongrui Wu", "authors": "Dongrui Wu and Jian Huang", "title": "Affect Estimation in 3D Space Using Multi-Task Active Learning for\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquisition of labeled training samples for affective computing is usually\ncostly and time-consuming, as affects are intrinsically subjective, subtle and\nuncertain, and hence multiple human assessors are needed to evaluate each\naffective sample. Particularly, for affect estimation in the 3D space of\nvalence, arousal and dominance, each assessor has to perform the evaluations in\nthree dimensions, which makes the labeling problem even more challenging. Many\nsophisticated machine learning approaches have been proposed to reduce the data\nlabeling requirement in various other domains, but so far few have considered\naffective computing. This paper proposes two multi-task active learning for\nregression approaches, which select the most beneficial samples to label, by\nconsidering the three affect primitives simultaneously. Experimental results on\nthe VAM corpus demonstrated that our optimal sample selection approaches can\nresult in better estimation performance than random selection and several\ntraditional single-task active learning approaches. Thus, they can help\nalleviate the data labeling problem in affective computing, i.e., better\nestimation performance can be obtained from fewer labeling queries.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:39:46 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 19:58:15 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Wu", "Dongrui", ""], ["Huang", "Jian", ""]]}, {"id": "1808.04414", "submitter": "Fred Hohman", "authors": "James Abello, Fred Hohman, Varun Bezzam, Duen Horng Chau", "title": "Large Graph Exploration via Subgraph Discovery and Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are developing an interactive graph exploration system called Graph\nPlayground for making sense of large graphs. Graph Playground offers a fast and\nscalable edge decomposition algorithm, based on iterative vertex-edge peeling,\nto decompose million-edge graphs in seconds. Graph Playground introduces a\nnovel graph exploration approach and a 3D representation framework that\nsimultaneously reveals (1) peculiar subgraph structure discovered through the\ndecomposition's layers, (e.g., quasi-cliques), and (2) possible vertex roles in\nlinking such subgraph patterns across layers.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 19:35:44 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Abello", "James", ""], ["Hohman", "Fred", ""], ["Bezzam", "Varun", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1808.04760", "submitter": "Yuri G. Gordienko", "authors": "Sergii Stirenko, Gang Peng, Wei Zeng, Yuri Gordienko, Oleg Alienin,\n  Oleksandr Rokovyi, Nikita Gordienko", "title": "Parallel Statistical and Machine Learning Methods for Estimation of\n  Physical Load", "comments": "15 pages, 8 figures, accepted for 18th International Conference on\n  Algorithms and Architectures for Parallel Processing (ICA3PP) 15-17 November,\n  2018 (Guangzhou, China)", "journal-ref": "In: Vaidya J., Li J. (eds) Algorithms and Architectures for\n  Parallel Processing. ICA3PP 2018. Lecture Notes in Computer Science, vol\n  11334, 483-497. Springer, Cham", "doi": "10.1007/978-3-030-05051-1_33", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several statistical and machine learning methods are proposed to estimate the\ntype and intensity of physical load and accumulated fatigue . They are based on\nthe statistical analysis of accumulated and moving window data subsets with\nconstruction of a kurtosis-skewness diagram. This approach was applied to the\ndata gathered by the wearable heart monitor for various types and levels of\nphysical activities, and for people with various physical conditions. The\ndifferent levels of physical activities, loads, and fitness can be\ndistinguished from the kurtosis-skewness diagram, and their evolution can be\nmonitored. Several metrics for estimation of the instant effect and accumulated\neffect (physical fatigue) of physical loads were proposed. The data and results\npresented allow to extend application of these methods for modeling and\ncharacterization of complex human activity patterns, for example, to estimate\nthe actual and accumulated physical load and fatigue, model the potential\ndangerous development, and give cautions and advice in real time.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:47:32 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Stirenko", "Sergii", ""], ["Peng", "Gang", ""], ["Zeng", "Wei", ""], ["Gordienko", "Yuri", ""], ["Alienin", "Oleg", ""], ["Rokovyi", "Oleksandr", ""], ["Gordienko", "Nikita", ""]]}, {"id": "1808.04819", "submitter": "Kevin Hu", "authors": "Kevin Z. Hu, Michiel A. Bakker, Stephen Li, Tim Kraska, C\\'esar A.\n  Hidalgo", "title": "VizML: A Machine Learning Approach to Visualization Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualization should be accessible for all analysts with data, not just\nthe few with technical expertise. Visualization recommender systems aim to\nlower the barrier to exploring basic visualizations by automatically generating\nresults for analysts to search and select, rather than manually specify. Here,\nwe demonstrate a novel machine learning-based approach to visualization\nrecommendation that learns visualization design choices from a large corpus of\ndatasets and associated visualizations. First, we identify five key design\nchoices made by analysts while creating visualizations, such as selecting a\nvisualization type and choosing to encode a column along the X- or Y-axis. We\ntrain models to predict these design choices using one million\ndataset-visualization pairs collected from a popular online visualization\nplatform. Neural networks predict these design choices with high accuracy\ncompared to baseline models. We report and interpret feature importances from\none of these baseline models. To evaluate the generalizability and uncertainty\nof our approach, we benchmark with a crowdsourced test set, and show that the\nperformance of our model is comparable to human performance when predicting\nconsensus visualization type, and exceeds that of other ML-based systems.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 18:00:01 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Hu", "Kevin Z.", ""], ["Bakker", "Michiel A.", ""], ["Li", "Stephen", ""], ["Kraska", "Tim", ""], ["Hidalgo", "C\u00e9sar A.", ""]]}, {"id": "1808.05127", "submitter": "Zeon Trevor Fernando", "authors": "Luyan Xu, Zeon Trevor Fernando, Xuan Zhou, Wolfgang Nejdl", "title": "LogCanvas: Visualizing Search History Using Knowledge Graphs", "comments": "Proceedings of the 41st International ACM SIGIR Conference on\n  Research & Development in Information Retrieval, SIGIR '18 (demo), July 2018", "journal-ref": null, "doi": "10.1145/3209978.3210169", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this demo paper, we introduce LogCanvas, a platform for user search\nhistory visualisation. Different from the existing visualisation tools,\nLogCanvas focuses on helping users re-construct the semantic relationship among\ntheir search activities. LogCanvas segments a user's search history into\ndifferent sessions and generates a knowledge graph to represent the information\nexploration process in each session. A knowledge graph is composed of the most\nimportant concepts or entities discovered by each search query as well as their\nrelationships. It thus captures the semantic relationship among the queries.\nLogCanvas offers a session timeline viewer and a snippets viewer to enable\nusers to re-find their previous search results efficiently. LogCanvas also\nprovides a collaborative perspective to support a group of users in sharing\nsearch results and experience.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 15:20:33 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Xu", "Luyan", ""], ["Fernando", "Zeon Trevor", ""], ["Zhou", "Xuan", ""], ["Nejdl", "Wolfgang", ""]]}, {"id": "1808.05359", "submitter": "Zhenyue Qin", "authors": "Zhenyue Qin and Tom Gedeon and Sabrina Caldwell", "title": "Neural Networks Assist Crowd Predictions in Discerning the Veracity of\n  Emotional Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd predictions have demonstrated powerful performance in predicting future\nevents. We aim to understand crowd prediction efficacy in ascertaining the\nveracity of human emotional expressions. We discover that collective\ndiscernment can increase the accuracy of detecting emotion veracity from 63%,\nwhich is the average individual performance, to 80%. Constraining data to best\nperformers can further increase the result up to 92%. Neural networks can\nachieve an accuracy to 99.69% by aggregating participants' answers. That is,\nassigning positive and negative weights to high and low human predictors,\nrespectively. Furthermore, neural networks that are trained with one emotion\ndata can also produce high accuracies on discerning the veracity of other\nemotion types: our crowdsourced transfer of emotion learning is novel. We find\nthat our neural networks do not require a large number of participants,\nparticularly, 30 randomly selected, to achieve high accuracy predictions,\nbetter than any individual participant. Our proposed method of assembling\npeoples' predictions with neural networks can provide insights for applications\nsuch as fake news prevention and lie detection.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 06:55:25 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Qin", "Zhenyue", ""], ["Gedeon", "Tom", ""], ["Caldwell", "Sabrina", ""]]}, {"id": "1808.05464", "submitter": "Dongrui Wu", "authors": "He He and Dongrui Wu", "title": "Transfer Learning for Brain-Computer Interfaces: A Euclidean Space Data\n  Alignment Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: This paper targets a major challenge in developing practical\nEEG-based brain-computer interfaces (BCIs): how to cope with individual\ndifferences so that better learning performance can be obtained for a new\nsubject, with minimum or even no subject-specific data? Methods: We propose a\nnovel approach to align EEG trials from different subjects in the Euclidean\nspace to make them more similar, and hence improve the learning performance for\na new subject. Our approach has three desirable properties: 1) it aligns the\nEEG trials directly in the Euclidean space, and any signal processing, feature\nextraction and machine learning algorithms can then be applied to the aligned\ntrials; 2) its computational cost is very low; and, 3) it is unsupervised and\ndoes not need any label information from the new subject. Results: Both offline\nand simulated online experiments on motor imagery classification and\nevent-related potential classification verified that our proposed approach\noutperformed a state-of-the-art Riemannian space data alignment approach, and\nseveral approaches without data alignment. Conclusion: The proposed Euclidean\nspace EEG data alignment approach can greatly facilitate transfer learning in\nBCIs. Significance: Our proposed approach is effective, efficient, and easy to\nimplement. It could be an essential pre-processing step for EEG-based BCIs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 23:06:43 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 08:36:27 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["He", "He", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.05558", "submitter": "Robert Greinacher", "authors": "Robert Greinacher and Franziska Horn", "title": "The DALPHI annotation framework & how its pre-annotations can improve\n  annotator efficiency", "comments": "5 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Producing the required amounts of training data for machine learning and NLP\ntasks often involves human annotators doing very repetitive and monotonous\nwork. In this paper, we present and evaluate our novel annotation framework\nDALPHI, which facilitates the annotation process by providing the annotator\nwith suggestions generated by an automated, active-learning based assistance\nsystem. In a study with 66 participants, we demonstrate on the exemplary task\nof annotating named entities in text documents that with this assistance system\nthe annotation processes can be improved with respect to the quality and\nquantity of produced annotations, even if the pre-annotations provided by the\nassistance system are at a recall level of only 50%.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 15:58:08 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Greinacher", "Robert", ""], ["Horn", "Franziska", ""]]}, {"id": "1808.05579", "submitter": "Giuseppe Petracca", "authors": "Giuseppe Petracca, Jens Grossklags, Patrick McDaniel and Trent Jaeger", "title": "Regulating Access to System Sensors in Cooperating Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.OS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern operating systems such as Android, iOS, Windows Phone, and Chrome OS\nsupport a cooperating program abstraction. Instead of placing all functionality\ninto a single program, programs cooperate to complete tasks requested by users.\nHowever, untrusted programs may exploit interactions with other programs to\nobtain unauthorized access to system sensors either directly or through\nprivileged services. Researchers have proposed that programs should only be\nauthorized to access system sensors on a user-approved input event, but these\nmethods do not account for possible delegation done by the program receiving\nthe user input event. Furthermore, proposed delegation methods do not enable\nusers to control the use of their input events accurately. In this paper, we\npropose ENTRUST, a system that enables users to authorize sensor operations\nthat follow their input events, even if the sensor operation is performed by a\nprogram different from the program receiving the input event. ENTRUST tracks\nuser input as well as delegation events and restricts the execution of such\nevents to compute unambiguous delegation paths to enable accurate and reusable\nauthorization of sensor operations. To demonstrate this approach, we implement\nthe ENTRUST authorization system for Android. We find, via a laboratory user\nstudy, that attacks can be prevented at a much higher rate (54-64%\nimprovement); and via a field user study, that ENTRUST requires no more than\nthree additional authorizations per program with respect to the first-use\napproach, while incurring modest performance (<1%) and memory overheads (5.5 KB\nper program).\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 21:37:29 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Petracca", "Giuseppe", ""], ["Grossklags", "Jens", ""], ["McDaniel", "Patrick", ""], ["Jaeger", "Trent", ""]]}, {"id": "1808.05852", "submitter": "Dongrui Wu", "authors": "Yang Wang and Dongrui Wu", "title": "Real-time fMRI-based Brain Computer Interface: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the rapid development of neuroimaging technology has been\nproviding many powerful tools for cognitive neuroscience research. Among them,\nthe functional magnetic resonance imaging (fMRI), which has high spatial\nresolution, acceptable temporal resolution, simple calibration, and short\npreparation time, has been widely used in brain research. Compared with the\nelectroencephalogram (EEG), real-time fMRI-based brain computer interface\n(rtfMRI-BCI) not only can perform decoding analysis across the whole brain to\ncontrol external devices, but also allows a subject to voluntarily\nself-regulate specific brain regions. This paper reviews the basic architecture\nof rtfMRI-BCI, the emerging machine learning based data analysis approaches\n(also known as multi-voxel pattern analysis), and the applications and recent\nadvances of rtfMRI-BCI.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:44:56 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Wang", "Yang", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.05853", "submitter": "Dongrui Wu", "authors": "He He and Dongrui Wu", "title": "Transfer Learning Enhanced Common Spatial Pattern Filtering for Brain\n  Computer Interfaces (BCIs): Overview and a New Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electroencephalogram (EEG) is the most widely used input for brain\ncomputer interfaces (BCIs), and common spatial pattern (CSP) is frequently used\nto spatially filter it to increase its signal-to-noise ratio. However, CSP is a\nsupervised filter, which needs some subject-specific calibration data to\ndesign. This is time-consuming and not user-friendly. A promising approach for\nshortening or even completely eliminating this calibration session is transfer\nlearning, which leverages relevant data or knowledge from other subjects or\ntasks. This paper reviews three existing approaches for incorporating transfer\nlearning into CSP, and also proposes a new transfer learning enhanced CSP\napproach. Experiments on motor imagery classification demonstrate their\neffectiveness. Particularly, our proposed approach achieves the best\nperformance when the number of target domain calibration samples is small.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:52:12 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["He", "He", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.05902", "submitter": "Filipe Rodrigues", "authors": "Filipe Rodrigues, Mariana Louren\\c{c}o, Bernardete Ribeiro, Francisco\n  Pereira", "title": "Learning Supervised Topic Models for Classification and Regression from\n  Crowds", "comments": "14 pages", "journal-ref": "Rodrigues, F., Lourenco, M., Ribeiro, B. and Pereira, F.C., 2017.\n  Learning supervised topic models for classification and regression from\n  crowds. IEEE transactions on pattern analysis and machine intelligence,\n  39(12), pp.2409-2422", "doi": "10.1109/TPAMI.2017.2648786", "report-no": null, "categories": "stat.ML cs.CL cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing need to analyze large collections of documents has led to great\ndevelopments in topic modeling. Since documents are frequently associated with\nother related variables, such as labels or ratings, much interest has been\nplaced on supervised topic models. However, the nature of most annotation\ntasks, prone to ambiguity and noise, often with high volumes of documents, deem\nlearning under a single-annotator assumption unrealistic or unpractical for\nmost real-world applications. In this article, we propose two supervised topic\nmodels, one for classification and another for regression problems, which\naccount for the heterogeneity and biases among different annotators that are\nencountered in practice when learning from crowds. We develop an efficient\nstochastic variational inference algorithm that is able to scale to very large\ndatasets, and we empirically demonstrate the advantages of the proposed model\nover state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 15:32:24 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Rodrigues", "Filipe", ""], ["Louren\u00e7o", "Mariana", ""], ["Ribeiro", "Bernardete", ""], ["Pereira", "Francisco", ""]]}, {"id": "1808.06019", "submitter": "Dominik Moritz", "authors": "Dominik Moritz and Danyel Fisher", "title": "Visualizing a Million Time Series with the Density Line Chart", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data analysts often need to work with multiple series of\ndata---conventionally shown as line charts---at once. Few visual\nrepresentations allow analysts to view many lines simultaneously without\nbecoming overwhelming or cluttered. In this paper, we introduce the DenseLines\ntechnique to calculate a discrete density representation of time series.\nDenseLines normalizes time series by the arc length to compute accurate\ndensities. The derived density visualization allows users both to see the\naggregate trends of multiple series and to identify anomalous extrema.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 23:17:25 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 04:08:07 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Moritz", "Dominik", ""], ["Fisher", "Danyel", ""]]}, {"id": "1808.06055", "submitter": "Maneesh Bilalpur", "authors": "Maneesh Bilalpur, Mohan Kankanhalli, Stefan Winkler, and Ramanathan\n  Subramanian", "title": "EEG-based Evaluation of Cognitive Workload Induced by Acoustic\n  Parameters for Data Sonification", "comments": "Accepted for publication in the proceedings of the 20th ACM\n  International Conference on Multimodal Interaction, Colorado, USA", "journal-ref": null, "doi": "10.1145/3242969.3243016", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Visualization has been receiving growing attention recently, with\nubiquitous smart devices designed to render information in a variety of ways.\nHowever, while evaluations of visual tools for their interpretability and\nintuitiveness have been commonplace, not much research has been devoted to\nother forms of data rendering, eg, sonification. This work is the first to\nautomatically estimate the cognitive load induced by different acoustic\nparameters considered for sonification in prior studies. We examine cognitive\nload via (a) perceptual data-sound mapping accuracies of users for the\ndifferent acoustic parameters, (b) cognitive workload impressions explicitly\nreported by users, and (c) their implicit EEG responses compiled during the\nmapping task. Our main findings are that (i) low cognitive load-inducing (ie,\nmore intuitive) acoustic parameters correspond to higher mapping accuracies,\n(ii) EEG spectral power analysis reveals higher $\\alpha$ band power for low\ncognitive load parameters, implying a congruent relationship between explicit\nand implicit user responses, and (iii) Cognitive load classification with EEG\nfeatures achieves a peak F1-score of 0.64, confirming that reliable workload\nestimation is achievable with user EEG data compiled using wearable sensors.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 08:20:23 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Bilalpur", "Maneesh", ""], ["Kankanhalli", "Mohan", ""], ["Winkler", "Stefan", ""], ["Subramanian", "Ramanathan", ""]]}, {"id": "1808.06080", "submitter": "Anca Dumitrache", "authors": "Anca Dumitrache, Oana Inel, Lora Aroyo, Benjamin Timmermans, Chris\n  Welty", "title": "CrowdTruth 2.0: Quality Metrics for Crowdsourcing with Disagreement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Typically crowdsourcing-based approaches to gather annotated data use\ninter-annotator agreement as a measure of quality. However, in many domains,\nthere is ambiguity in the data, as well as a multitude of perspectives of the\ninformation examples. In this paper, we present ongoing work into the\nCrowdTruth metrics, that capture and interpret inter-annotator disagreement in\ncrowdsourcing. The CrowdTruth metrics model the inter-dependency between the\nthree main components of a crowdsourcing system -- worker, input data, and\nannotation. The goal of the metrics is to capture the degree of ambiguity in\neach of these three components. The metrics are available online at\nhttps://github.com/CrowdTruth/CrowdTruth-core .\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 12:35:58 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Dumitrache", "Anca", ""], ["Inel", "Oana", ""], ["Aroyo", "Lora", ""], ["Timmermans", "Benjamin", ""], ["Welty", "Chris", ""]]}, {"id": "1808.06211", "submitter": "Hussein Abbass A", "authors": "Aya Hussein and Hussein Abbass", "title": "Mixed Initiative Systems for Human-Swarm Interaction: Opportunities and\n  Challenges", "comments": "Author version, accepted at the 2018 IEEE Annual Systems Modelling\n  Conference, Canberra, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-swarm interaction (HSI) involves a number of human factors impacting\nhuman behaviour throughout the interaction. As the technologies used within HSI\nadvance, it is more tempting to increase the level of swarm autonomy within the\ninteraction to reduce the workload on humans. Yet, the prospective negative\neffects of high levels of autonomy on human situational awareness can hinder\nthis process. Flexible autonomy aims at trading-off these effects by changing\nthe level of autonomy within the interaction when required; with\nmixed-initiatives combining human preferences and automation's recommendations\nto select an appropriate level of autonomy at a certain point of time. However,\nthe effective implementation of mixed-initiative systems raises fundamental\nquestions on how to combine human preferences and automation recommendations,\nhow to realise the selected level of autonomy, and what the future impacts on\nthe cognitive states of a human are. We explore open challenges that hamper the\nprocess of developing effective flexible autonomy. We then highlight the\npotential benefits of using system modelling techniques in HSI by illustrating\nhow they provide HSI designers with an opportunity to evaluate different\nstrategies for assessing the state of the mission and for adapting the level of\nautonomy within the interaction to maximise mission success metrics.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 13:39:37 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Hussein", "Aya", ""], ["Abbass", "Hussein", ""]]}, {"id": "1808.06322", "submitter": "Wei Wang Dr.", "authors": "Zhiqing Luo, Wei Wang, Jiang Xiao, Qianyi Huang, Tao Jiang and Qian\n  Zhang", "title": "Authenticating On-Body Backscatter by Exploiting Propagation Signatures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vision of battery-free communication has made backscatter a compelling\ntechnology for on-body wearable and implantable devices. Recent advances have\nfacilitated the communication between backscatter tags and on-body smart\ndevices. These studies have focused on the communication dimension, while the\nsecurity dimension remains vulnerable. It has been demonstrated that wireless\nconnectivity can be exploited to send unauthorized commands or fake messages\nthat result in device malfunctioning. The key challenge in defending these\nattacks stems from the minimalist design in backscatter. Thus, in this paper,\nwe explore the feasibility of authenticating an on-body backscatter tag without\nmodifying its signal or protocol. We present SecureScatter, a physical-layer\nsolution that delegates the security of backscatter to an on-body smart device.\nTo this end, we profile the on-body propagation paths of backscatter links, and\nconstruct highly sensitive propagation signatures to identify on-body\nbackscatter links. We implement our design in a software radio and evaluate it\nwith different backscatter tags that work at 2.4 GHz and 900 MHz. Results show\nthat our system can identify on-body devices at 93.23% average true positive\nrate and 3.18% average false positive rate.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 06:38:39 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Luo", "Zhiqing", ""], ["Wang", "Wei", ""], ["Xiao", "Jiang", ""], ["Huang", "Qianyi", ""], ["Jiang", "Tao", ""], ["Zhang", "Qian", ""]]}, {"id": "1808.06398", "submitter": "Maarten Vanhoof", "authors": "Maarten Vanhoof, Fernando Reis, Zbigniew Smoreda, Thomas Ploetz", "title": "Detecting home locations from CDR data: introducing spatial uncertainty\n  to the state-of-the-art", "comments": "13 pages, 7 figures, contributed to the Mobile Tartu 2016 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-continuous location traces inferred from Call Detail Records (CDR) at\npopulation scale are increasingly becoming available for research and show\ngreat potential for automated detection of meaningful places. Yet, a majority\nof Home Detection Algorithms (HDAs) suffer from \"blind\" deployment of criteria\nto define homes and from limited possibilities for validation. In this paper,\nwe investigate the performance and capabilities of five popular criteria for\nhome detection based on a very large mobile phone dataset from France (~18\nmillion users, 6 months). Furthermore, we construct a data-driven framework to\nassess the spatial uncertainty related to the application of HDAs. Our findings\nappropriate spatial uncertainty in HDA and, in extension, for detection of\nmeaningful places. We show how spatial uncertainties on the individuals' level\ncan be assessed in absence of ground truth annotation, how they relate to\ntraditional, high-level validation practices and how they can be used to\nimprove results for, e.g., nation-wide population estimation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 11:41:51 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Vanhoof", "Maarten", ""], ["Reis", "Fernando", ""], ["Smoreda", "Zbigniew", ""], ["Ploetz", "Thomas", ""]]}, {"id": "1808.06533", "submitter": "Dongrui Wu", "authors": "He He and Dongrui Wu", "title": "Spatial Filtering for Brain Computer Interfaces: A Comparison between\n  the Common Spatial Pattern and Its Variant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electroencephalogram (EEG) is the most popular form of input for brain\ncomputer interfaces (BCIs). However, it can be easily contaminated by various\nartifacts and noise, e.g., eye blink, muscle activities, powerline noise, etc.\nTherefore, the EEG signals are often filtered both spatially and temporally to\nincrease the signal-to-noise ratio before they are fed into a machine learning\nalgorithm for recognition. This paper considers spatial filtering,\nparticularly, the common spatial pattern (CSP) filters for EEG classification.\nIn binary classification, CSP seeks a set of filters to maximize the variance\nfor one class while minimizing it for the other. We first introduce the\ntraditional solution, and then a new solution based on a slightly different\nobjective function. We performed comprehensive experiments on motor imagery to\ncompare the two approaches, and found that generally the traditional CSP\nsolution still gives better results. We also showed that adding regularization\nto the covariance matrices can improve the final classification performance, no\nmatter which objective function is used.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 22:39:13 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["He", "He", ""], ["Wu", "Dongrui", ""]]}, {"id": "1808.06543", "submitter": "Biswarup Mukherjee", "authors": "Ananya S. Dhawan, Biswarup Mukherjee, Shriniwas Patwardhan, Nima\n  Akhlaghi, Gyorgy Levay, Rahsaan Holley, Wilsaan Joiner, Michelle Harris-Love,\n  Siddhartha Sikdar", "title": "Proprioceptive Sonomyographic Control: A novel method of intuitive\n  proportional control of multiple degrees of freedom for upper-extremity\n  amputees", "comments": null, "journal-ref": "Scientific Reports, 9(9499), 2019", "doi": "10.1038/s41598-019-45459-7", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advances in multi-articulated prosthetic hands have outpaced\nthe methods available to amputees to intuitively control these devices.\nAmputees often cite difficulty of use as a key contributing factor for\nabandoning their prosthesis, creating a pressing need for improved control\ntechnology. A major challenge of traditional myoelectric control strategies\nusing surface electromyography electrodes has been the difficulty in achieving\nintuitive and robust proportional control of multiple degrees of freedom. In\nthis paper, we describe a new control method, proprioceptive sonomyographic\ncontrol that overcomes several limitations of myoelectric control. In\nsonomyography, muscle mechanical deformation is sensed using ultrasound, as\ncompared to electrical activation, and therefore the resulting control signals\ncan directly control the position of the end effector. Compared to myoelectric\ncontrol which controls the velocity of the end-effector device, sonomyographic\ncontrol is more congruent with residual proprioception in the residual limb. We\ntested our approach with 5 upper-extremity amputees and able-bodied subjects\nusing a virtual target achievement and holding task. Amputees and able-bodied\nparticipants demonstrated the ability to achieve positional control for 5\ndegrees of freedom with an hour of training. Our results demonstrate the\npotential of proprioceptive sonomyographic control for intuitive dexterous\ncontrol of multiarticulated prostheses.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 16:06:31 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Dhawan", "Ananya S.", ""], ["Mukherjee", "Biswarup", ""], ["Patwardhan", "Shriniwas", ""], ["Akhlaghi", "Nima", ""], ["Levay", "Gyorgy", ""], ["Holley", "Rahsaan", ""], ["Joiner", "Wilsaan", ""], ["Harris-Love", "Michelle", ""], ["Sikdar", "Siddhartha", ""]]}, {"id": "1808.06548", "submitter": "Akihito Noda", "authors": "Akihito Noda and Hiroyuki Shinoda", "title": "Inter-IC for Wearables (I2We): Power and Data Transfer over Double-sided\n  Conductive Textile", "comments": "11 pages, 16 figures, accepted for publication in IEEE TBioCAS", "journal-ref": "IEEE Transactions on Biomedical Circuits and Systems, vol. 13, no.\n  1, pp. 80-90, Feb. 2019", "doi": "10.1109/TBCAS.2018.2881219", "report-no": null, "categories": "eess.SP cs.HC physics.app-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a power and data transfer network on a conductive fabric material\nbased on an existing serial communication protocol, Inter-Integrated Circuit\n(I2C). We call the proposed network Inter-IC for Wearables (I2We). Continuous\ndc power and I2C-formatted data are simultaneously transferred to tiny sensor\nnodes distributed on a double-sided conductive textile. The textile has two\nconductive sides isolated from each other and is used as a single planar\ntransmission line. I2C data are transferred along with dc power supply based on\nfrequency division multiplexing (FDM). Two carriers are modulated with the\nclock (SCL) and the data (SDA) signals of I2C. A modulation and demodulation\ncircuit is designed to enable using off-the-shelf I2C-interfaced sensor ICs.\nOne significant originality of this work is that a special filter to enable\npassive modulation is designed by locating its impedance poles and zeros at\nappropriate frequencies. The proposed scheme enables flexible implementation of\nwearable sensor systems in which multiple off-the-shelf tiny sensors are\ndistributed all over a wear.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 00:35:03 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2019 14:19:41 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Noda", "Akihito", ""], ["Shinoda", "Hiroyuki", ""]]}, {"id": "1808.06679", "submitter": "Pablo Frank-Bolton Dr.", "authors": "Pablo Frank-Bolton, Rahul Simha", "title": "Annotation Scaffolds for Object Modeling and Manipulation", "comments": "31 pages, 46 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and evaluate an approach for human-in-the-loop specification of\nshape reconstruction with annotations for basic robot-object interactions. Our\nmethod is based on the idea of model annotation: the addition of simple cues to\nan underlying object model to specify shape and delineate a simple task. The\ngoal is to explore reducing the complexity of CAD-like interfaces so that\nnovice users can quickly recover an object's shape and describe a manipulation\ntask that is then carried out by a robot. The object modeling and interaction\nannotation capabilities are tested with a user study and compared against\nresults obtained using existing approaches. The approach has been analyzed\nusing a variety of shape comparison, grasping, and manipulation metrics, and\ntested with the PR2 robot platform, where it was shown to be successful.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 20:11:16 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Frank-Bolton", "Pablo", ""], ["Simha", "Rahul", ""]]}, {"id": "1808.07074", "submitter": "Tarek Richard Besold", "authors": "Tarek R. Besold and Sara L. Uckelman", "title": "The What, the Why, and the How of Artificial Explanations in Automated\n  Decision-Making", "comments": "working draft, comments/feedback welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing incorporation of Artificial Intelligence in the form of\nautomated systems into decision-making procedures highlights not only the\nimportance of decision theory for automated systems but also the need for these\ndecision procedures to be explainable to the people involved in them.\nTraditional realist accounts of explanation, wherein explanation is a relation\nthat holds (or does not hold) eternally between an explanans and an\nexplanandum, are not adequate to account for the notion of explanation required\nfor artificial decision procedures. We offer an alternative account of\nexplanation as used in the context of automated decision-making that makes\nexplanation an epistemic phenomenon, and one that is dependent on context. This\naccount of explanation better accounts for the way that we talk about, and use,\nexplanations and derived concepts, such as `explanatory power', and also allows\nus to differentiate between reasons or causes on the one hand, which do not\nneed to have an epistemic aspect, and explanations on the other, which do have\nsuch an aspect. Against this theoretical backdrop we then review existing\napproaches to explanation in Artificial Intelligence and Machine Learning, and\nsuggest desiderata which truly explainable decision systems should fulfill.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 18:06:22 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Besold", "Tarek R.", ""], ["Uckelman", "Sara L.", ""]]}, {"id": "1808.07314", "submitter": "Bruce Ferwerda", "authors": "Bruce Ferwerda and Mark Graus", "title": "Predicting Musical Sophistication from Music Listening Behaviors: A\n  Preliminary Study", "comments": "The Late-Breaking Results track part of the Twelfth ACM Conference on\n  Recommender Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Psychological models are increasingly being used to explain online behavioral\ntraces. Aside from the commonly used personality traits as a general user\nmodel, more domain dependent models are gaining attention. The use of domain\ndependent psychological models allows for more fine-grained identification of\nbehaviors and provide a deeper understanding behind the occurrence of those\nbehaviors. Understanding behaviors based on psychological models can provide an\nadvantage over data-driven approaches. For example, relying on psychological\nmodels allow for ways to personalize when data is scarce. In this preliminary\nwork we look at the relation between users' musical sophistication and their\nonline music listening behaviors and to what extent we can successfully predict\nmusical sophistication. An analysis of data from a study with 61 participants\nshows that listening behaviors can successfully be used to infer users' musical\nsophistication.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 11:16:59 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Ferwerda", "Bruce", ""], ["Graus", "Mark", ""]]}, {"id": "1808.07586", "submitter": "Michael Ekstrand", "authors": "Michael D. Ekstrand and Daniel Kluver", "title": "Exploring Author Gender in Book Rating and Recommendation", "comments": "Expanded version under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering algorithms find useful patterns in rating and\nconsumption data and exploit these patterns to guide users to good items. Many\nof the patterns in rating datasets reflect important real-world differences\nbetween the various users and items in the data; other patterns may be\nirrelevant or possibly undesirable for social or ethical reasons, particularly\nif they reflect undesired discrimination, such as discrimination in publishing\nor purchasing against authors who are women or ethnic minorities. In this work,\nwe examine the response of collaborative filtering recommender algorithms to\nthe distribution of their input data with respect to a dimension of social\nconcern, namely content creator gender. Using publicly-available book ratings\ndata, we measure the distribution of the genders of the authors of books in\nuser rating profiles and recommendation lists produced from this data. We find\nthat common collaborative filtering algorithms differ in the gender\ndistribution of their recommendation lists, and in the relationship of that\noutput distribution to user profile distribution.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 23:00:26 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2020 00:14:02 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Ekstrand", "Michael D.", ""], ["Kluver", "Daniel", ""]]}, {"id": "1808.07645", "submitter": "Huang Hu", "authors": "Huang Hu, Xianchao Wu, Bingfeng Luo, Chongyang Tao, Can Xu, Wei Wu,\n  Zhan Chen", "title": "Playing 20 Question Game with Policy-Based Reinforcement Learning", "comments": "Accepted by EMNLP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 20 Questions (Q20) game is a well known game which encourages deductive\nreasoning and creativity. In the game, the answerer first thinks of an object\nsuch as a famous person or a kind of animal. Then the questioner tries to guess\nthe object by asking 20 questions. In a Q20 game system, the user is considered\nas the answerer while the system itself acts as the questioner which requires a\ngood strategy of question selection to figure out the correct object and win\nthe game. However, the optimal policy of question selection is hard to be\nderived due to the complexity and volatility of the game environment. In this\npaper, we propose a novel policy-based Reinforcement Learning (RL) method,\nwhich enables the questioner agent to learn the optimal policy of question\nselection through continuous interactions with users. To facilitate training,\nwe also propose to use a reward network to estimate the more informative\nreward. Compared to previous methods, our RL method is robust to noisy answers\nand does not rely on the Knowledge Base of objects. Experimental results show\nthat our RL method clearly outperforms an entropy-based engineering system and\nhas competitive performance in a noisy-free simulation environment.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 06:34:32 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 09:47:54 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2019 06:28:09 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Hu", "Huang", ""], ["Wu", "Xianchao", ""], ["Luo", "Bingfeng", ""], ["Tao", "Chongyang", ""], ["Xu", "Can", ""], ["Wu", "Wei", ""], ["Chen", "Zhan", ""]]}, {"id": "1808.08081", "submitter": "Georgios Bakirtzis", "authors": "Georgios Bakirtzis, Brandon J. Simon, Cody H. Fleming, Carl R. Elks", "title": "Looking for a Black Cat in a Dark Room: Security Visualization for\n  Cyber-Physical System Design and Analysis", "comments": null, "journal-ref": null, "doi": "10.1109/VIZSEC.2018.8709187", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, there is a plethora of software security tools employing\nvisualizations that enable the creation of useful and effective interactive\nsecurity analyst dashboards. Such dashboards can assist the analyst to\nunderstand the data at hand and, consequently, to conceive more targeted\npreemption and mitigation security strategies. Despite the recent advances,\nmodel-based security analysis is lacking tools that employ effective\ndashboards---to manage potential attack vectors, system components, and\nrequirements. This problem is further exacerbated because model-based security\nanalysis produces significantly larger result spaces than security analysis\napplied to realized systems---where platform specific information, software\nversions, and system element dependencies are known. Therefore, there is a need\nto manage the analysis complexity in model-based security through better\nvisualization techniques. Towards that goal, we propose an interactive security\nanalysis dashboard that provides different views largely centered around the\nsystem, its requirements, and its associated attack vector space. This tool\nmakes it possible to start analysis earlier in the system lifecycle. We apply\nthis tool in a significant area of engineering design---the design of\ncyber-physical systems---where security violations can lead to safety hazards.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 10:43:58 GMT"}, {"version": "v2", "created": "Tue, 23 Oct 2018 07:58:40 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Bakirtzis", "Georgios", ""], ["Simon", "Brandon J.", ""], ["Fleming", "Cody H.", ""], ["Elks", "Carl R.", ""]]}, {"id": "1808.08138", "submitter": "Clarisse de Souza", "authors": "Juliana Soares Jansen Ferreira, Clarisse Sieckenius de Souza, Rafael\n  Rossi de Mello Brand\\~ao, Carla Faria Leit\\~ao", "title": "SigniFYI-CDN: merged communicability and usability methods to evaluate\n  notation-intensive interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present SigniFYI-CDN, an inspection method built from previously proposed\nmethods combining Semiotic Engineering and the Cognitive Dimensions of\nNotations. Compared to its predecessors, SigniFYI-CDN simplifies procedural\nsteps and supports them with more analytic scaffolds. It is especially fit for\nthe study of interaction with technologies where notations are created and used\nby various people, or by a single person in various, and potentially distant,\noccasions. In such cases, notations may serve several purposes, like (mutual)\ncomprehension, recall, coordination, negotiation, and documentation. We\nillustrate SigniFYI-CDN with highlights from the evaluation of a computer tool\nthat supports qualitative data analysis. Our contribution is a simpler tool for\nresearchers and practitioners to probe the power of combined communicability\nand usability analysis of interaction with increasingly complex data-intensive\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 13:41:36 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Ferreira", "Juliana Soares Jansen", ""], ["de Souza", "Clarisse Sieckenius", ""], ["Brand\u00e3o", "Rafael Rossi de Mello", ""], ["Leit\u00e3o", "Carla Faria", ""]]}, {"id": "1808.08157", "submitter": "Claudio Pinhanez", "authors": "Claudio S. Pinhanez, Heloisa Candello, Mauro C. Pichiliani, Marisa\n  Vasconcelos, Melina Guerra, Ma\\'ira G. de Bayser, Paulo Cavalin", "title": "Different but Equal: Comparing User Collaboration with Digital Personal\n  Assistants vs. Teams of Expert Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work compares user collaboration with conversational personal assistants\nvs. teams of expert chatbots. Two studies were performed to investigate whether\neach approach affects accomplishment of tasks and collaboration costs.\nParticipants interacted with two equivalent financial advice chatbot systems,\none composed of a single conversational adviser and the other based on a team\nof four experts chatbots. Results indicated that users had different forms of\nexperiences but were equally able to achieve their goals. Contrary to the\nexpected, there were evidences that in the teamwork situation that users were\nmore able to predict agent behavior better and did not have an overhead to\nmaintain common ground, indicating similar collaboration costs. The results\npoint towards the feasibility of either of the two approaches for user\ncollaboration with conversational agents.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 14:28:04 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Pinhanez", "Claudio S.", ""], ["Candello", "Heloisa", ""], ["Pichiliani", "Mauro C.", ""], ["Vasconcelos", "Marisa", ""], ["Guerra", "Melina", ""], ["de Bayser", "Ma\u00edra G.", ""], ["Cavalin", "Paulo", ""]]}, {"id": "1808.08177", "submitter": "Elissa Marie Redmiles", "authors": "Elissa M. Redmiles", "title": "\"Should I Worry?\" A Cross-Cultural Examination of Account Security\n  Incident Response", "comments": null, "journal-ref": "Proceedings of the IEEE Symposium on Security and Privacy (IEEE\n  S&P), 2019", "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Digital security technology is able to identify and prevent many threats to\nusers accounts. However, some threats remain that, to provide reliable\nsecurity, require human intervention: e.g., through users paying attention to\nwarning messages or completing secondary authentication procedures. While prior\nwork has broadly explored people's mental models of digital security threats,\nwe know little about users' precise, in-the-moment response process to\nin-the-wild threats. In this work, we conduct a series of qualitative\ninterviews (n=67) with users who had recently experienced suspicious login\nincidents on their real Facebook accounts in order to explore this process of\naccount security incident response. We find a common process across\nparticipants from five countries -- with differing online and offline cultures\n-- allowing us to identify areas for future technical development to best\nsupport user security. We provide additional insights on the unique nature of\nincident-response information seeking, known attacker threat models, and\nlessons learned from a large, cross-cultural qualitative study of digital\nsecurity.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 15:36:47 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 22:41:45 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 14:53:22 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Redmiles", "Elissa M.", ""]]}, {"id": "1808.08208", "submitter": "Nitish Nag", "authors": "Vaibhav Pandey, Nitish Nag, Ramesh Jain", "title": "Ubiquitous Event Mining to Enhance Personal Health", "comments": "Accepted to UBICOMP 2018, International Workshop on Integrating\n  Physical Activity and Health Aspects in Everyday Mobility. UbiComp / ISWC'18\n  Adjunct, October 8-12, 2018, Singapore, Singapore", "journal-ref": null, "doi": "10.1145/3267305.3267684", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in user interfaces, pattern recognition, and ubiquitous computing\ncontinue to pave the way for better navigation towards our health goals.\nQuantitative methods which can guide us towards our personal health goals will\nhelp us optimize our daily life actions, and environmental exposures.\nUbiquitous computing is essential for monitoring these factors and actuating\ntimely interventions in all relevant circumstances. We need to combine the\nevents recognized by different ubiquitous systems and derive actionable causal\nrelationships from an event ledger. Understanding of user habits and health\nshould be teleported between applications rather than these systems working in\nsilos, allowing systems to find the optimal guidance medium for required\ninterventions. We propose a method through which applications and devices can\nenhance the user experience by leveraging event relationships, leading the way\nto more relevant, useful, and, most importantly, pleasurable health guidance\nexperience.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 16:55:10 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Pandey", "Vaibhav", ""], ["Nag", "Nitish", ""], ["Jain", "Ramesh", ""]]}, {"id": "1808.08447", "submitter": "Chie Hieida", "authors": "Chie Hieida, Takato Horii and Takayuki Nagai", "title": "Deep Emotion: A Computational Model of Emotion Using Deep Neural\n  Networks", "comments": "29 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions are very important for human intelligence. For example, emotions are\nclosely related to the appraisal of the internal bodily state and external\nstimuli. This helps us to respond quickly to the environment. Another important\nperspective in human intelligence is the role of emotions in decision-making.\nMoreover, the social aspect of emotions is also very important. Therefore, if\nthe mechanism of emotions were elucidated, we could advance toward the\nessential understanding of our natural intelligence. In this study, a model of\nemotions is proposed to elucidate the mechanism of emotions through the\ncomputational model. Furthermore, from the viewpoint of partner robots, the\nmodel of emotions may help us to build robots that can have empathy for humans.\nTo understand and sympathize with people's feelings, the robots need to have\ntheir own emotions. This may allow robots to be accepted in human society. The\nproposed model is implemented using deep neural networks consisting of three\nmodules, which interact with each other. Simulation results reveal that the\nproposed model exhibits reasonable behavior as the basic mechanism of emotion.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 16:33:08 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Hieida", "Chie", ""], ["Horii", "Takato", ""], ["Nagai", "Takayuki", ""]]}, {"id": "1808.08531", "submitter": "Dongyu Liu", "authors": "Dongyu Liu, Weiwei Cui, Kai Jin, Yuxiao Guo, Huamin Qu", "title": "DeepTracker: Visualizing the Training Process of Convolutional Neural\n  Networks", "comments": "Published at ACM Transactions on Intelligent Systems and Technology\n  (in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep convolutional neural networks (CNNs) have achieved remarkable success in\nvarious fields. However, training an excellent CNN is practically a\ntrial-and-error process that consumes a tremendous amount of time and computer\nresources. To accelerate the training process and reduce the number of trials,\nexperts need to understand what has occurred in the training process and why\nthe resulting CNN behaves as such. However, current popular training platforms,\nsuch as TensorFlow, only provide very little and general information, such as\ntraining/validation errors, which is far from enough to serve this purpose. To\nbridge this gap and help domain experts with their training tasks in a\npractical environment, we propose a visual analytics system, DeepTracker, to\nfacilitate the exploration of the rich dynamics of CNN training processes and\nto identify the unusual patterns that are hidden behind the huge amount of\ntraining log. Specifically,we combine a hierarchical index mechanism and a set\nof hierarchical small multiples to help experts explore the entire training log\nfrom different levels of detail. We also introduce a novel cube-style\nvisualization to reveal the complex correlations among multiple types of\nheterogeneous training data including neuron weights, validation images, and\ntraining iterations. Three case studies are conducted to demonstrate how\nDeepTracker provides its users with valuable knowledge in an industry-level CNN\ntraining process, namely in our case, training ResNet-50 on the ImageNet\ndataset. We show that our method can be easily applied to other\nstate-of-the-art \"very deep\" CNN models.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 11:09:44 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Liu", "Dongyu", ""], ["Cui", "Weiwei", ""], ["Jin", "Kai", ""], ["Guo", "Yuxiao", ""], ["Qu", "Huamin", ""]]}, {"id": "1808.08711", "submitter": "Jeremy Frey", "authors": "Morgane Hamon, R\\'emy Ramadour, J\\'er\\'emy Frey", "title": "Exploring Biofeedback with a Tangible Interface Designed for Relaxation", "comments": "PhyCS - International Conference on Physiological Computing Systems,\n  Sep 2018, Seville, Spain. SCITEPRESS, 2018, http://www.phycs.org/?y=2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anxiety is a common health issue that can occur throughout one's existence.\nIn this pilot study we explore an alternative technique to regulate it:\nbiofeedback. The long-term objective is to offer an ecological device that\ncould help people cope with anxiety, by exposing their inner state in a\ncomprehensive manner. We propose a first iteration of this device, \"Inner\nFlower\", that uses heart rate to adapt a breathing guide to the user, and we\ninvestigate its efficiency and usability. Traditionally, such device requires\nuser's full attention. We propose an ambient modality during which the device\noperates in the peripheral vision. Beside comparing \"Ambient\" and \"Focus\"\nconditions, we also compare the biofeedback with a sham feedback (fixed\nbreathing guide). We found that the Focus group demonstrated higher relaxation\nand performance on a cognitive task (N-back). However, there was no noticeable\neffect of the Ambient feedback, and the biofeedback condition did not yield any\nsignificant difference when compared to the sham feedback. These results, while\npromising, highlight the pitfalls of any research related to biofeedback, where\nit is difficult to fully comprehend the underlying mechanisms of such\ntechnique.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 07:23:23 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Hamon", "Morgane", ""], ["Ramadour", "R\u00e9my", ""], ["Frey", "J\u00e9r\u00e9my", ""]]}, {"id": "1808.08713", "submitter": "Jeremy Frey", "authors": "J\\'er\\'emy Frey, Jessica Cauchard (IDC)", "title": "Remote Biofeedback Sharing, Opportunities and Challenges", "comments": "WellComp - UbiComp/ISWC'18 Adjunct, Oct 2018, Singapore, Singapore.\n  http://ubicomp.org/ubicomp2018/", "journal-ref": null, "doi": "10.1145/3267305.3267701", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biofeedback is commonly used to regulate one's state, for example to manage\nstress. The underlying idea is that by perceiving a feedback about their\nphysiological activity, a user can act upon it. In this paper we describe\nthrough two recent projects how biofeedback could be leveraged to share one's\nstate at distance. Such extension of biofeedback could answer to the need of\nbelonging, further widening the applications of the technology in terms of\nwell-being.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 07:29:05 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "IDC"], ["Cauchard", "Jessica", "", "IDC"]]}, {"id": "1808.08808", "submitter": "Shilin Zhu", "authors": "Shilin Zhu, Yilong Li", "title": "2DR: Towards Fine-Grained 2-D RFID Touch Sensing", "comments": "RFID Touch Sensing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.ET cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce 2DR, a single RFID tag which can seamlessly sense\ntwo-dimensional human touch using off-the-shelf RFID readers. Instead of using\na two-dimensional tag array to sense human finger touch on a surface, 2DR only\nuses one or two RFID chip(s), which reduces the manufacturing cost and makes\nthe tag more suitable for printing on flexible materials. The key idea behind\n2DR is to design a custom-shape antenna and classify human finger touch based\non unique phase information using statistical learning. We printed 2DR tag on\nFR-4 substrate and use off-the-shelf UHF-RFID readers (FCC frequency band) to\nsense different touch activities. Experiments show great potential of our\ndesign. Moreover, 2DR can be further extended to 3D by building stereoscopic\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 04:38:14 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Zhu", "Shilin", ""], ["Li", "Yilong", ""]]}, {"id": "1808.08954", "submitter": "Blake Hannaford", "authors": "Blake Hannaford and Randall Bly and Ian Humphreys and Mark Whipple", "title": "Behavior Trees as a Representation for Medical Procedures", "comments": "We are pleased to acknowledge support from National Science\n  Foundation grant #IIS-1637444. arXiv admin note: text overlap with\n  arXiv:1801.07864", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Effective collaboration between machines and clinicians requires\nflexible data structures to represent medical processes and clinical practice\nguidelines. Such a data structure could enable effective turn-taking between\nhuman and automated components of a complex treatment, accurate on-line\nmonitoring of clinical treatments (for example to detect medical errors), or\nautomated treatment systems (such as future medical robots) whose overall\ntreatment plan is understandable and auditable by human experts.\n  Materials and Methods: Behavior trees (BTs) emerged from video game\ndevelopment as a graphical language for modeling intelligent agent behavior.\nBTs have several properties which are attractive for modeling medical\nprocedures including human-readability, authoring tools, and composability.\n  Results: This paper will illustrate construction of BTs for exemplary medical\nprocedures and clinical protocols.\n  Discussion and Conclusion: Behavior Trees thus form a useful, and human\nauthorable/readable bridge between clinical practice guidelines and AI systems.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 16:56:53 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Hannaford", "Blake", ""], ["Bly", "Randall", ""], ["Humphreys", "Ian", ""], ["Whipple", "Mark", ""]]}, {"id": "1808.09068", "submitter": "Quan Li", "authors": "Quan Li, Ziming Wu, Lingling Yi, Kristanto Sean N, Huamin Qu, Xiaojuan\n  Ma", "title": "WeSeer: Visual Analysis for Better Information Cascade Prediction of\n  WeChat Articles", "comments": "IEEE Transactions on Visualization and Computer Graphics (TVCG), 2019\n  (To appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media, such as Facebook and WeChat, empowers millions of users to\ncreate, consume, and disseminate online information on an unprecedented scale.\nThe abundant information on social media intensifies the competition of WeChat\nPublic Official Articles (i.e., posts) for gaining user attention due to the\nzero-sum nature of attention. Therefore, only a small portion of information\ntends to become extremely popular while the rest remains unnoticed or quickly\ndisappears. Such a typical `long-tail' phenomenon is very common in social\nmedia. Thus, recent years have witnessed a growing interest in predicting the\nfuture trend in the popularity of social media posts and understanding the\nfactors that influence the popularity of the posts. Nevertheless, existing\npredictive models either rely on cumbersome feature engineering or\nsophisticated parameter tuning, which are difficult to understand and improve.\nIn this paper, we study and enhance a point process-based model by\nincorporating visual reasoning to support communication between the users and\nthe predictive model for a better prediction result. The proposed system\nsupports users to uncover the working mechanism behind the model and improve\nthe prediction accuracy accordingly based on the insights gained. We use\nrealistic WeChat articles to demonstrate the effectiveness of the system and\nverify the improved model on a large scale of WeChat articles. We also elicit\nand summarize the feedback from WeChat domain experts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 00:09:20 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Li", "Quan", ""], ["Wu", "Ziming", ""], ["Yi", "Lingling", ""], ["N", "Kristanto Sean", ""], ["Qu", "Huamin", ""], ["Ma", "Xiaojuan", ""]]}, {"id": "1808.09074", "submitter": "Quan Li", "authors": "Quan Li, Kristanto Sean Njotoprawiro, Hammad Haleem, Qiaoan Chen,\n  Chris Yi, Xiaojuan Ma", "title": "EmbeddingVis: A Visual Analytics Approach to Comparative Network\n  Embedding Inspection", "comments": "Proceedings of IEEE VIS 2018 (VAST 2018) (to appear), Berlin,\n  Germany, Oct 21-26, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing latent vector representation for nodes in a network through\nembedding models has shown its practicality in many graph analysis\napplications, such as node classification, clustering, and link prediction.\nHowever, despite the high efficiency and accuracy of learning an embedding\nmodel, people have little clue of what information about the original network\nis preserved in the embedding vectors. The abstractness of low-dimensional\nvector representation, stochastic nature of the construction process, and\nnon-transparent hyper-parameters all obscure understanding of network embedding\nresults. Visualization techniques have been introduced to facilitate embedding\nvector inspection, usually by projecting the embedding space to a\ntwo-dimensional display. Although the existing visualization methods allow\nsimple examination of the structure of embedding space, they cannot support\nin-depth exploration of the embedding vectors. In this paper, we design an\nexploratory visual analytics system that supports the comparative visual\ninterpretation of embedding vectors at the cluster, instance, and structural\nlevels. To be more specific, it facilitates comparison of what and how node\nmetrics are preserved across different embedding models and investigation of\nrelationships between node metrics and selected embedding vectors. Several case\nstudies confirm the efficacy of our system. Experts' feedback suggests that our\napproach indeed helps them better embrace the understanding of network\nembedding models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 00:48:52 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Li", "Quan", ""], ["Njotoprawiro", "Kristanto Sean", ""], ["Haleem", "Hammad", ""], ["Chen", "Qiaoan", ""], ["Yi", "Chris", ""], ["Ma", "Xiaojuan", ""]]}, {"id": "1808.09094", "submitter": "Jinwei Lin", "authors": "Jinwei Lin, Aimin Zhou", "title": "PyDraw: a GUI drawing generator based on Tkinter and its design concept", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of GUI is a great progress in the history of computer science\nand software design. GUI makes human computer interaction more simple and\ninteresting. Python, as a popular programming language in recent years, has not\nbeen realized in GUI design. Tkinter has the advantage of native support for\nPython, but there are too few visual GUI generators supporting Tkinter. This\narticle presents a GUI generator based on Tkinter framework, PyDraw. The design\nprinciple of PyDraw and the powerful design concept behind it are introduced in\ndetail. With PyDraw's GUI design philosophy, it can easily design a visual GUI\nrendering generator for any GUI framework with canvas functionality or\nprogramming language with screen display control. This article is committed to\nconveying PyDraw's GUI free design concept. Through experiments, we have proved\nthe practicability and efficiency of PyDrawd. In order to better convey the\ndesign concept of PyDraw, let more enthusiasts join PyDraw update and\nevolution, we have the source code of PyDraw. At the end of the article, we\nsummarize our experience and express our vision for future GUI design. We\nbelieve that the future GUI will play an important role in graphical software\nprogramming, the future of less code or even no code programming software\ndesign methods must become a focus and hot, free, like drawing GUI will be\nworth pursuing.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 02:44:05 GMT"}], "update_date": "2018-09-04", "authors_parsed": [["Lin", "Jinwei", ""], ["Zhou", "Aimin", ""]]}, {"id": "1808.09145", "submitter": "Joe Simons", "authors": "Joseph J.P. Simons", "title": "Psychological Frameworks for Persuasive Information and Communications\n  Technologies", "comments": null, "journal-ref": "Simons, Joseph JP. \"Psychological Frameworks for Persuasive\n  Information and Communications Technologies.\" IEEE Pervasive Computing 15,\n  no. 3 (2016): 68-76", "doi": "10.1109/MPRV.2016.52", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When developing devices to encourage positive change in users, social\npsychology can offer useful conceptual resources. This article outlines three\nmajor theories from the discipline and discusses their implications for\ndesigning persuasive technologies.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 07:24:15 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Simons", "Joseph J. P.", ""]]}, {"id": "1808.09296", "submitter": "Wolfgang Fuhl", "authors": "Wolfgang Fuhl, Enkelejda Kasneci", "title": "Eye movement velocity and gaze data generator for evaluation, robustness\n  testing and assess of eye tracking software and visualization tools", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.00970", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movements hold information about human perception, intention, and\ncognitive state. We propose a novel eye movement simulator that i)\nprobabilistically simulates saccade movements as gamma distributions\nconsidering different peak velocities and ii) models smooth pursuit onsets with\nthe sigmoid function. Additionally, it is capable of producing velocity and\ntwo-dimensional gaze sequences for static and dynamic scenes using saliency\nmaps or real fixation targets. Our approach is also capable of simulating any\nsampling rate, even with uctuations. The simulation is evaluated against\npublicly available annotated data. The simulator can be used in EyeTrace or\ndownloaded at http://ti.unituebingen. de/Projekte.1801.0.html.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 06:14:22 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 06:59:06 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Fuhl", "Wolfgang", ""], ["Kasneci", "Enkelejda", ""]]}, {"id": "1808.09488", "submitter": "Libby Hemphill", "authors": "Libby Hemphill, A.J. Million, Ingrid Erickson", "title": "Crafting Moral Infrastructures: How Nonprofits Use Facebook to Survive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present findings from interviews with 23 individuals affiliated with\nnon-profit organizations (NPOs) to understand how they deploy information and\ncommunication technologies (ICTs) in civic engagement efforts. Existing\nresearch about NPO ICT use is largely critical, but we did not find evidence\nthat NPOs fail to use tools effectively. Rather, we detail how various ICT use\non the part of NPOs intersects with unique affordance perceptions and adoption\ncauses. Overall, we find that existing theories about technology choice (e.g.,\ntask-technology fit, uses and gratifications) do not explain the assemblages\nNPOs describe. We argue that NPOs fashion infrastructures in accordance with\ntheir moral economy frameworks rather than selecting tools based on utility.\nTogether, the rhetorics of infrastructure and moral economies capture the\nmotivations and constraints our participants expressed and challenge how\nprevailing theories of ICT usage describe the non-profit landscape.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 18:46:48 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Hemphill", "Libby", ""], ["Million", "A. J.", ""], ["Erickson", "Ingrid", ""]]}, {"id": "1808.09496", "submitter": "Johanna Johansen Ms", "authors": "Johanna Johansen and Christian Johansen and Josef Noll", "title": "InfoInternet for Education in the Global South: A Study of Applications\n  Enabled by Free Information-only Internet Access in Technologically\n  Disadvantaged Areas (authors' version)", "comments": "16 pages, 1 figure, under review for a journal since March 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper summarises our work on studying educational applications enabled\nby the introduction of a new information layer called InfoInternet. This is an\ninitiative to facilitate affordable access to internet based information in\ncommunities with network scarcity or economic problems from the Global South.\nInfoInternet develops both networking solutions as well as business and social\nmodels, together with actors like mobile operators and government\norganisations. In this paper we identify and describe characteristics of\neducational applications, their specific users, and learning environment. We\nare interested in applications that make the adoption of Internet faster,\ncheaper, and wider in such communities. When developing new applications (or\nadopting existing ones) for such constrained environments, this work acts as\ninitial guidelines prior to field studies.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 19:05:19 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Johansen", "Johanna", ""], ["Johansen", "Christian", ""], ["Noll", "Josef", ""]]}, {"id": "1808.09572", "submitter": "Nicholas Waytowich", "authors": "Nicholas R. Waytowich, Vinicius G. Goecks, Vernon J. Lawhern", "title": "Cycle-of-Learning for Autonomous Systems from Human Interaction", "comments": "Presented at AI-HRI AAAI-FSS, 2018 (arXiv:1809.06606)", "journal-ref": null, "doi": null, "report-no": "AI-HRI/2018/05", "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss different types of human-robot interaction paradigms in the\ncontext of training end-to-end reinforcement learning algorithms. We provide a\ntaxonomy to categorize the types of human interaction and present our\nCycle-of-Learning framework for autonomous systems that combines different\nhuman-interaction modalities with reinforcement learning. Two key concepts\nprovided by our Cycle-of-Learning framework are how it handles the integration\nof the different human-interaction modalities (demonstration, intervention, and\nevaluation) and how to define the switching criteria between them.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 23:00:12 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 16:25:03 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Waytowich", "Nicholas R.", ""], ["Goecks", "Vinicius G.", ""], ["Lawhern", "Vernon J.", ""]]}, {"id": "1808.09732", "submitter": "Yi-Ting Huang", "authors": "Yi-Ting Huang, Meng Chang Chen, Yeali S. Sun", "title": "Development and Evaluation of a Personalized Computer-aided Question\n  Generation for English Learners to Improve Proficiency and Correct Mistakes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last several years, the field of computer assisted language learning\nhas increasingly focused on computer aided question generation. However, this\napproach often provides test takers with an exhaustive amount of questions that\nare not designed for any specific testing purpose. In this work, we present a\npersonalized computer aided question generation that generates multiple choice\nquestions at various difficulty levels and types, including vocabulary, grammar\nand reading comprehension. In order to improve the weaknesses of test takers,\nit selects questions depending on an estimated proficiency level and unclear\nconcepts behind incorrect responses. This results show that the students with\nthe personalized automatic quiz generation corrected their mistakes more\nfrequently than ones only with computer aided question generation. Moreover,\nstudents demonstrated the most progress between the pretest and post test and\ncorrectly answered more difficult questions. Finally, we investigated the\npersonalizing strategy and found that a student could make a significant\nprogress if the proposed system offered the vocabulary questions at the same\nlevel of his or her proficiency level, and if the grammar and reading\ncomprehension questions were at a level lower than his or her proficiency\nlevel.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 11:26:07 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Huang", "Yi-Ting", ""], ["Chen", "Meng Chang", ""], ["Sun", "Yeali S.", ""]]}, {"id": "1808.09735", "submitter": "Yi-Ting Huang", "authors": "Yi-Ting Huang, Meng Chang Chen, and Yeali S. Sun", "title": "Bringing personalized learning into computer-aided question generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel and statistical method of ability estimation\nbased on acquisition distribution for a personalized computer aided question\ngeneration. This method captures the learning outcomes over time and provides a\nflexible measurement based on the acquisition distributions instead of\nprecalibration. Compared to the previous studies, the proposed method is\nrobust, especially when an ability of a student is unknown. The results from\nthe empirical data show that the estimated abilities match the actual abilities\nof learners, and the pretest and post-test of the experimental group show\nsignificant improvement. These results suggest that this method can serves as\nthe ability estimation for a personalized computer-aided testing environment.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 11:31:53 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Huang", "Yi-Ting", ""], ["Chen", "Meng Chang", ""], ["Sun", "Yeali S.", ""]]}, {"id": "1808.09852", "submitter": "He Huang", "authors": "He Huang, Bokai Cao, Philip S. Yu, Chang-Dong Wang, Alex D. Leow", "title": "dpMood: Exploiting Local and Periodic Typing Dynamics for Personalized\n  Mood Prediction", "comments": "Published in ICDM'18 as a regular paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mood disorders are common and associated with significant morbidity and\nmortality. Early diagnosis has the potential to greatly alleviate the burden of\nmental illness and the ever increasing costs to families and society. Mobile\ndevices provide us a promising opportunity to detect the users' mood in an\nunobtrusive manner. In this study, we use a custom keyboard which collects\nkeystrokes' meta-data and accelerometer values. Based on the collected time\nseries data in multiple modalities, we propose a deep personalized mood\nprediction approach, called {\\pro}, by integrating convolutional and recurrent\ndeep architectures as well as exploring each individual's circadian rhythm.\nExperimental results not only demonstrate the feasibility and effectiveness of\nusing smart-phone meta-data to predict the presence and severity of mood\ndisturbances in bipolar subjects, but also show the potential of personalized\nmedical treatment for mood disorders.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 14:31:42 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Huang", "He", ""], ["Cao", "Bokai", ""], ["Yu", "Philip S.", ""], ["Wang", "Chang-Dong", ""], ["Leow", "Alex D.", ""]]}, {"id": "1808.09885", "submitter": "Aboubakr Aqle", "authors": "Aboubakr Aqle, Kamran Khowaja, Dena Al-Thani", "title": "Preliminary Evaluation of Interactive Search Engine Interface for\n  Visually Impaired Users", "comments": "10 pages, 8 figures, IEEE Access Journal", "journal-ref": null, "doi": "10.1109/ACCESS.2020.2977593", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work designs, evaluates, and improves a proposed search engine interface\nfor Visually Impaired VI users to efficiently perform web search activities.\nOur conceptual modeling technique is based on Formal Concept Analysis FCA that\nis used for data analysis. This approach highlights the hierarchized approach\nto represent the discovered concepts. It is combined with context interactive\nnavigation in an interface which is called interactive search engine\n(InteractSE). This interface aims to reduce the time and effort required by the\nVI users to browse search results. InteractSE was evaluated by experts using\nNielsen heuristics and Web Content Accessibility Guidelines WCAG 2.0 for its\nusability and accessibility. The analysis was carried out based on the\nusability problems identified and their average severity ratings. The results\nshow that the most frequently violated heuristics from Nielsen set are\nconsistency, documentation, and the average severity rating of all the problems\nis minor. The results also show that the most frequently violated WCAG 2\nguidelines are distinguishable, followed by navigable and affordance. The\naverage severity rating of all the problems found using WCAG 2 guidelines is\nalso minor. The results show that Nielsen heuristics and WCAG 2.0 guidelines\ncontributed to identifying several usability problems, which might have missed\nout if either of them was used alone.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 15:27:17 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 05:39:33 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2020 00:47:50 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Aqle", "Aboubakr", ""], ["Khowaja", "Kamran", ""], ["Al-Thani", "Dena", ""]]}, {"id": "1808.09900", "submitter": "F. Maxwell Harper", "authors": "Joshua Wissbroecker, F Maxwell Harper", "title": "Early Lessons from a Voice-Only Interface for Finding Movies", "comments": "Joshua Wissbroecker and F. Maxwell Harper. 2018. Early Lessons from a\n  Voice-Only Interface for Finding Movies. In Late-Breaking Results track part\n  of the Twelfth ACM Conference on Recommender Systems (RecSys '18), October\n  2-7, 2018, Vancouver, BC, Canada. ACM, New York, NY, USA, 2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The current generation of streaming media players often allow users to speak\ncommands (e.g., users can change the TV channel by pressing a button and saying\n\"ESPN\"). However, these devices typically support a narrow range of control-\nand search-oriented commands, and do not support deeper recommendation or\nexploration queries. To study spoken natural language interactions with\nrecommenders, we have built MovieLens TV, a movie recommender system with no\ninput modalities except voice. In this poster, we describe MovieLens TV, with a\nfocus on lessons learned building a prototype system around an off-the-shelf\nAmazon Echo.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 16:03:57 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Wissbroecker", "Joshua", ""], ["Harper", "F Maxwell", ""]]}, {"id": "1808.10639", "submitter": "Ma\\\"elick Claes", "authors": "Ma\\\"elick Claes, Mika M\\\"antyl\\\"a, Umar Farooq", "title": "On the Use of Emoticons in Open Source Software Development", "comments": "Short paper to be presented at the 12th International Symposium on\n  Empirical Software Engineering and Measurement (ESEM)", "journal-ref": null, "doi": "10.1145/3239235.3267434", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Using sentiment analysis to study software developers' behavior\ncomes with challenges such as the presence of a large amount of technical\ndiscussion unlikely to express any positive or negative sentiment. However,\nemoticons provide information about developer sentiments that can easily be\nextracted from software repositories. Aim: We investigate how software\ndevelopers use emoticons differently in issue trackers in order to better\nunderstand the differences between developers and determine to which extent\nemoticons can be used as in place of sentiment analysis. Method: We extract\nemoticons from 1.3M comments from Apache's issue tracker and 4.5M from\nMozilla's issue tracker using regular expressions built from a list of\nemoticons used by SentiStrength and Wikipedia. We check for statistical\ndifferences using Mann-Whitney U tests and determine the effect size with\nCliff's delta. Results: Overall Mozilla developers rely more on emoticons than\nApache developers. While the overall ratio of comments with emoticons is of 2%\nand 3.6% for Apache and Mozilla, some individual developers can have a ratio\nabove 20%. Looking specifically at Mozilla developers, we find that western\ndevelopers use significantly more emoticons (with large size effect) than\neastern developers. While the majority of emoticons are used to express joy, we\nfind that Mozilla developers use emoticons more frequently to express sadness\nand surprise than Apache developers. Finally, we find that developers use\noverall more emoticons during weekends than during weekdays, with the share of\nsad and surprised emoticons increasing during weekends. Conclusions: While\nemoticons are primarily used to express joy, the more occasional use of sad and\nsurprised emoticons can potentially be utilized to detect frustration in place\nof sentiment analysis among developers using emoticons frequently enough.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 09:09:34 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 14:10:08 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Claes", "Ma\u00eblick", ""], ["M\u00e4ntyl\u00e4", "Mika", ""], ["Farooq", "Umar", ""]]}, {"id": "1808.10852", "submitter": "Theerawit Wilaiprasitporn", "authors": "Patcharin Cheng, Phairot Autthasan, Boriwat Pijarana, Ekapol\n  Chuangsuwanich and Theerawit Wilaiprasitporn", "title": "Towards Asynchronous Motor Imagery-Based Brain-Computer Interfaces: a\n  joint training scheme using deep learning", "comments": null, "journal-ref": "TENCON 2018 - 2018 IEEE Region 10 Conference", "doi": "10.1109/TENCON.2018.8650546", "report-no": null, "categories": "eess.SP cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the deep learning (DL) approach is applied to a joint training\nscheme for asynchronous motor imagery-based Brain-Computer Interface (BCI). The\nproposed DL approach is a cascade of one-dimensional convolutional neural\nnetworks and fully-connected neural networks (CNN-FC). The focus is mainly on\nthree types of brain responses: non-imagery EEG (\\textit{background EEG}),\n(\\textit{pure imagery}) EEG, and EEG during the transitional period between\nbackground EEG and pure imagery (\\textit{transitional imagery}). The study of\ntransitional imagery signals should provide greater insight into real-world\nscenarios. It may be inferred that pure imagery and transitional EEG are high\nand low power EEG imagery, respectively. Moreover, the results from the CNN-FC\nare compared to the conventional approach for motor imagery-BCI, namely the\ncommon spatial pattern (CSP) for feature extraction and support vector machine\n(SVM) for classification (CSP-SVM). Under a joint training scheme, pure and\ntransitional imagery are treated as the same class, while background EEG is\nanother class. Ten-fold cross-validation is used to evaluate whether the joint\ntraining scheme significantly improves the performance task of classifying pure\nand transitional imagery signals from background EEG. Using sparse of just a\nfew electrode channels ($C_{z}$, $C_{3}$ and $C_{4}$), mean accuracy reaches\n71.52 % and 70.27 % for CNN-FC and CSP-SVM, respectively. On the other hand,\nmean accuracy without the joint training scheme achieve only 62.68 % and 52.41\n% for CNN-FC and CSP-SVM, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:30:05 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cheng", "Patcharin", ""], ["Autthasan", "Phairot", ""], ["Pijarana", "Boriwat", ""], ["Chuangsuwanich", "Ekapol", ""], ["Wilaiprasitporn", "Theerawit", ""]]}]