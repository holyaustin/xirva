[{"id": "1202.2158", "submitter": "Javad Azimi", "authors": "Javad Azimi, Ruofei Zhang, Yang Zhou, Vidhya Navalpakkam, Jianchang\n  Mao, Xiaoli Fern", "title": "The Impact of Visual Appearance on User Response in Online Display\n  Advertising", "comments": "This work was done while the first author was an Intern at Yahoo!\n  Labs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Display advertising has been a significant source of revenue for publishers\nand ad networks in online advertising ecosystem. One of the main goals in\ndisplay advertising is to maximize user response rate for advertising\ncampaigns, such as click through rates (CTR) or conversion rates. Although in\nthe online advertising industry we believe that the visual appearance of ads\n(creatives) matters for propensity of user response, there is no published work\nso far to address this topic via a systematic data-driven approach. In this\npaper we quantitatively study the relationship between the visual appearance\nand performance of creatives using large scale data in the world's largest\ndisplay ads exchange system, RightMedia. We designed a set of 43 visual\nfeatures, some of which are novel and some are inspired by related work. We\nextracted these features from real creatives served on RightMedia. We also\ndesigned and conducted a series of experiments to evaluate the effectiveness of\nvisual features for CTR prediction, ranking and performance classification.\nBased on the evaluation results, we selected a subset of features that have the\nmost important impact on CTR. We believe that the findings presented in this\npaper will be very useful for the online advertising industry in designing\nhigh-performance creatives. It also provides the research community with the\nfirst ever data set, initial insights into visual appearance's effect on user\nresponse propensity, and evaluation benchmarks for further study.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2012 00:26:22 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2012 22:23:52 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Azimi", "Javad", ""], ["Zhang", "Ruofei", ""], ["Zhou", "Yang", ""], ["Navalpakkam", "Vidhya", ""], ["Mao", "Jianchang", ""], ["Fern", "Xiaoli", ""]]}, {"id": "1202.3872", "submitter": "Thomas Pietrzak", "authors": "Thomas Pietrzak (INRIA Lille - Nord Europe), Andrew Crossan (GIST),\n  Brewster A. Stephen (GIST), Beno\\^it Martin (LITA), Isabelle Pecci (LITA)", "title": "Creating Usable Pin Array Tactons for Non-Visual Information", "comments": null, "journal-ref": "IEEE Transactions on Haptics 2, 2 (2009) 61-72", "doi": "10.1109/TOH.2009.6", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial information can be difficult to present to a visually impaired\ncomputer user. In this paper we examine a new kind of tactile cueing for\nnon-visual interaction as a potential solution, building on earlier work on\nvibrotactile Tactons. However, unlike vibrotactile Tactons, we use a pin array\nto stimulate the finger tip. Here, we describe how to design static and dynamic\nTactons by defining their basic components. We then present user tests\nexamining how easy it is to distinguish between different forms of pin array\nTactons demonstrating accurate Tacton sets to represent directions. These\nexperiments demonstrate usable patterns for static, wave and blinking pin array\nTacton sets for guiding a user in one of eight directions. A study is then\ndescribed that shows the benefits of structuring Tactons to convey information\nthrough multiple parameters of the signal. By using multiple independent\nparameters for a Tacton, this study demonstrates participants perceive more\ninformation through a single Tacton. Two applications using these Tactons are\nthen presented: a maze exploration application and an electric circuit\nexploration application designed for use by and tested with visually impaired\nusers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2012 10:49:23 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Pietrzak", "Thomas", "", "INRIA Lille - Nord Europe"], ["Crossan", "Andrew", "", "GIST"], ["Stephen", "Brewster A.", "", "GIST"], ["Martin", "Beno\u00eet", "", "LITA"], ["Pecci", "Isabelle", "", "LITA"]]}, {"id": "1202.3919", "submitter": "Thomas Pietrzak", "authors": "Sylvain Malacria, Thomas Pietrzak (INRIA Lille - Nord Europe),\n  Aur\\'elien Tabard (pIT), \\'Eric Lecolinet (LTCI)", "title": "U-Note: Capture the Class and Access it Everywhere", "comments": null, "journal-ref": "Interact 2011 (2011)", "doi": "10.1007/978-3-642-23774-4_50", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present U-Note, an augmented teaching and learning system leveraging the\nadvantages of paper while letting teachers and pupils benefit from the richness\nthat digital media can bring to a lecture. U-Note provides automatic linking\nbetween the notes of the pupils' notebooks and various events that occurred\nduring the class (such as opening digital documents, changing slides, writing\ntext on an interactive whiteboard...). Pupils can thus explore their notes in\nconjunction with the digital documents that were presented by the teacher\nduring the lesson. Additionally, they can also listen to what the teacher was\nsaying when a given note was written. Finally, they can add their own comments\nand documents to their notebooks to extend their lecture notes. We interviewed\nteachers and deployed questionnaires to identify both teachers and pupils'\nhabits: most of the teachers use (or would like to use) digital documents in\ntheir lectures but have problems in sharing these resources with their pupils.\nThe results of this study also show that paper remains the primary medium used\nfor knowledge keeping, sharing and editing by the pupils. Based on these\nobservations, we designed U-Note, which is built on three modules. U-Teach\ncaptures the context of the class: audio recordings, the whiteboard contents,\ntogether with the web pages, videos and slideshows displayed during the lesson.\nU-Study binds pupils' paper notes (taken with an Anoto digital pen) with the\ndata coming from U-Teach and lets pupils access the class materials at home,\nthrough their notebooks. U-Move lets pupils browse lecture materials on their\nsmartphone when they are not in front of a computer.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2012 14:45:02 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Malacria", "Sylvain", "", "INRIA Lille - Nord Europe"], ["Pietrzak", "Thomas", "", "INRIA Lille - Nord Europe"], ["Tabard", "Aur\u00e9lien", "", "pIT"], ["Lecolinet", "\u00c9ric", "", "LTCI"]]}, {"id": "1202.3926", "submitter": "Thomas Pietrzak", "authors": "Thomas Pietrzak (INRIA Lille - Nord Europe), Andrew Crossan (GIST),\n  Stephen A. Brewster (GIST), Beno\\^it Martin (LITA), Isabelle Pecci (LITA)", "title": "Exploring Geometric Shapes with Touch", "comments": null, "journal-ref": "Interact 2009 (2009)", "doi": "10.1007/978-3-642-03655-2_18", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new technique to help users to explore geometric shapes without\nvision. This technique is based on a guidance using directional cues with a pin\narray. This is an alternative to the usual technique that consists of raising\nthe pins corresponding to dark pixels around the cursor. In this paper we\ncompare the exploration of geometric shapes with our new technique in unimanual\nand bimanual conditions. The users made fewer errors in unimanual condition\nthan in bimanual condition. However they did not explore the shapes more\nquickly and there was no difference in confidence in their answer.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2012 15:03:54 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Pietrzak", "Thomas", "", "INRIA Lille - Nord Europe"], ["Crossan", "Andrew", "", "GIST"], ["Brewster", "Stephen A.", "", "GIST"], ["Martin", "Beno\u00eet", "", "LITA"], ["Pecci", "Isabelle", "", "LITA"]]}, {"id": "1202.4832", "submitter": "EPTCS", "authors": "Walther Neuper", "title": "Automated Generation of User Guidance by Combining Computation and\n  Deduction", "comments": "In Proceedings THedu'11, arXiv:1202.4535", "journal-ref": "EPTCS 79, 2012, pp. 82-101", "doi": "10.4204/EPTCS.79.5", "report-no": null, "categories": "cs.LO cs.HC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herewith, a fairly old concept is published for the first time and named\n\"Lucas Interpretation\". This has been implemented in a prototype, which has\nbeen proved useful in educational practice and has gained academic relevance\nwith an emerging generation of educational mathematics assistants (EMA) based\non Computer Theorem Proving (CTP).\n  Automated Theorem Proving (ATP), i.e. deduction, is the most reliable\ntechnology used to check user input. However ATP is inherently weak in\nautomatically generating solutions for arbitrary problems in applied\nmathematics. This weakness is crucial for EMAs: when ATP checks user input as\nincorrect and the learner gets stuck then the system should be able to suggest\npossible next steps.\n  The key idea of Lucas Interpretation is to compute the steps of a calculation\nfollowing a program written in a novel CTP-based programming language, i.e.\ncomputation provides the next steps. User guidance is generated by combining\ndeduction and computation: the latter is performed by a specific language\ninterpreter, which works like a debugger and hands over control to the learner\nat breakpoints, i.e. tactics generating the steps of calculation. The\ninterpreter also builds up logical contexts providing ATP with the data\nrequired for checking user input, thus combining computation and deduction.\n  The paper describes the concepts underlying Lucas Interpretation so that open\nquestions can adequately be addressed, and prerequisites for further work are\nprovided.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2012 06:41:51 GMT"}], "update_date": "2012-02-23", "authors_parsed": [["Neuper", "Walther", ""]]}, {"id": "1202.5469", "submitter": "Arkaitz Zubiaga", "authors": "Arkaitz Zubiaga", "title": "Enhancing Navigation on Wikipedia with Social Tags", "comments": "Wikimania 2009, 5th International Conference of the Wikimedia\n  Community", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Social tagging has become an interesting approach to improve search and\nnavigation over the actual Web, since it aggregates the tags added by different\nusers to the same resource in a collaborative way. This way, it results in a\nlist of weighted tags describing its resource. Combined to a classical\ntaxonomic classification system such as that by Wikipedia, social tags can\nenhance document navigation and search. On the one hand, social tags suggest\nalternative navigation ways, including pivot-browsing, popularity-driven\nnavigation, and filtering. On the other hand, it provides new metadata,\nsometimes uncovered by documents' content, that can substantially improve\ndocument search. In this work, the inclusion of an interface to add\nuser-defined tags describing Wikipedia articles is proposed, as a way to\nimprove article navigation and retrieval. As a result, a prototype on applying\ntags over Wikipedia is proposed in order to evaluate its effectiveness.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 19:31:43 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Zubiaga", "Arkaitz", ""]]}, {"id": "1202.6104", "submitter": "Nesrine Yahia Ben", "authors": "Nesrine Ben yahia, Narj\\`es Bellamine and Henda Ben Gh\\'ezala", "title": "On the Convergence of Collaboration and Knowledge Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Collaboration technology typically focuses on collaboration and group\nprocesses (cooperation, communication, coordination and coproduction).\nKnowledge Management (KM) technology typically focuses on content (creation,\nstorage, sharing and use of data, information and knowledge). Yet, to achieve\ntheir common goals, teams and organizations need both KM and collaboration\ntechnology to make that more effective and efficient. This paper is interested\nin knowledge management and collaboration regarding their convergence and their\nintegration. First, it contributes to a better understanding of the knowledge\nmanagement and collaboration concepts. Second, it focuses on KM and\ncollaboration convergence by presenting the different interpretation of this\nconvergence. Third, this paper proposes a generic framework of collaborative\nknowledge management.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 02:04:04 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["yahia", "Nesrine Ben", ""], ["Bellamine", "Narj\u00e8s", ""], ["Gh\u00e9zala", "Henda Ben", ""]]}, {"id": "1202.6106", "submitter": "Kazutaka Kurihara", "authors": "Kazutaka Kurihara, Koji Tsukada", "title": "SpeechJammer: A System Utilizing Artificial Speech Disturbance with\n  Delayed Auditory Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we report on a system, \"SpeechJammer\", which can be used to\ndisturb people's speech. In general, human speech is jammed by giving back to\nthe speakers their own utterances at a delay of a few hundred milliseconds.\nThis effect can disturb people without any physical discomfort, and disappears\nimmediately by stop speaking. Furthermore, this effect does not involve anyone\nbut the speaker. We utilize this phenomenon and implemented two prototype\nversions by combining a direction-sensitive microphone and a\ndirection-sensitive speaker, enabling the speech of a specific person to be\ndisturbed. We discuss practical application scenarios of the system, such as\nfacilitating and controlling discussions. Finally, we argue what system\nparameters should be examined in detail in future formal studies based on the\nlessons learned from our preliminary study.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 02:33:51 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2012 11:09:37 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2012 12:36:47 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Kurihara", "Kazutaka", ""], ["Tsukada", "Koji", ""]]}, {"id": "1202.6388", "submitter": "Jose Fontanari", "authors": "Jose F. Fontanari, Marie-Claude Bonniot-Cabanac, Michel Cabanac and\n  Leonid I. Perlovsky", "title": "A structural model of emotions of cognitive dissonances", "comments": null, "journal-ref": "Neural Networks 32 (2012) 57-64", "doi": "10.1016/j.neunet.2012.04.007", "report-no": null, "categories": "q-bio.NC cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive dissonance is the stress that comes from holding two conflicting\nthoughts simultaneously in the mind, usually arising when people are asked to\nchoose between two detrimental or two beneficial options. In view of the\nwell-established role of emotions in decision making, here we investigate\nwhether the conventional structural models used to represent the relationships\namong basic emotions, such as the Circumplex model of affect, can describe the\nemotions of cognitive dissonance as well. We presented a questionnaire to 34\nanonymous participants, where each question described a decision to be made\namong two conflicting motivations and asked the participants to rate\nanalogically the pleasantness and the intensity of the experienced emotion. We\nfound that the results were compatible with the predictions of the Circumplex\nmodel for basic emotions.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 21:44:22 GMT"}], "update_date": "2012-06-05", "authors_parsed": [["Fontanari", "Jose F.", ""], ["Bonniot-Cabanac", "Marie-Claude", ""], ["Cabanac", "Michel", ""], ["Perlovsky", "Leonid I.", ""]]}, {"id": "1202.6517", "submitter": "Michal Ciesla", "authors": "Michal Ciesla, Przemyslaw Koziol", "title": "Eye Pupil Location Using Webcam", "comments": "11 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three different algorithms used for eye pupil location were described and\ntested. Algorithm efficiency comparison was based on human faces images taken\nfrom the BioID database. Moreover all the eye localisation methods were\nimplemented in a dedicated application supporting eye movement based computer\ncontrol. In this case human face images were acquired by a webcam and processed\nin a real-time.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 11:17:10 GMT"}], "update_date": "2012-03-01", "authors_parsed": [["Ciesla", "Michal", ""], ["Koziol", "Przemyslaw", ""]]}, {"id": "1202.6609", "submitter": "Gilles Falquet", "authors": "Claudine M\\'etral, Nizar Ghoula, Gilles Falquet", "title": "Towards an Integrated Visualization Of Semantically Enriched 3D City\n  Models: An Ontology of 3D Visualization Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D city models - which represent in 3 dimensions the geometric elements of a\ncity - are increasingly used for an intended wide range of applications. Such\nuses are made possible by using semantically enriched 3D city models and by\npresenting such enriched 3D city models in a way that allows decision-making\nprocesses to be carried out from the best choices among sets of objectives, and\nacross issues and scales. In order to help in such a decision-making process we\nhave defined a framework to find the best visualization technique(s) for a set\nof potentially heterogeneous data that have to be visualized within the same 3D\ncity model, in order to perform a given task in a specific context. We have\nchosen an ontology-based approach. This approach and the specification and use\nof the resulting ontology of 3D visualization techniques are described in this\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 17:15:53 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2012 13:33:10 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["M\u00e9tral", "Claudine", ""], ["Ghoula", "Nizar", ""], ["Falquet", "Gilles", ""]]}]