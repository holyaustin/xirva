[{"id": "1412.0197", "submitter": "Selvarajah Thuseethan Mr.", "authors": "Selvarajah Thuseethan, Sivapalan Achchuthan, Sinnathamby Kuhanesan", "title": "Usability Evaluation of Learning Management Systems in Sri Lankan\n  Universities", "comments": "12 pages, 10 figures, Submitted for Advances in Human Computer\n  Interaction", "journal-ref": "Global Journal of Computer Science and Technology, 15(1) (2015)", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As far as Learning Management System is concerned, it offers an integrated\nplatform for educational materials, distribution and management of learning as\nwell as accessibility by a range of users including teachers, learners and\ncontent makers especially for distance learning. Usability evaluation is\nconsidered as one approach to assess the efficiency of e-Learning systems. It\nis used to evaluate how well technology and tools are working for users. There\nare some factors contributing as major reason why LMS is not used effectively\nand regularly. Learning Management Systems, as major part of e-Learning\nsystems, can benefit from usability research to evaluate the LMS ease of use\nand satisfaction among its handlers. Many academic institutions worldwide\nprefer using their own customized Learning Management Systems; this is the case\nwith Moodle, an open source Learning Management Systems platform designed and\noperated by most of the universities in Sri Lanka. This paper gives an overview\nof Learning Management Systems used in Sri Lankan universities, and evaluates\nits usability using some pre-defined usability standards. In addition it\nmeasures the effectiveness of Learning Management System by testing the\nLearning Management Systems. The findings and result of this study as well as\nthe testing are discussed and presented.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 09:10:06 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 05:29:41 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Thuseethan", "Selvarajah", ""], ["Achchuthan", "Sivapalan", ""], ["Kuhanesan", "Sinnathamby", ""]]}, {"id": "1412.1180", "submitter": "Robert McKay", "authors": "Joonseok Lee and R. I. (Bob) McKay", "title": "Optimizing a Personalized Multigram Cellphone Keypad", "comments": "18 pages (including header pages), 3 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": "TRSNUSC:2014:001", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current layouts for alphabetic input on mobile phone keypads are very\ninefficient. We propose a genetic algorithm (GA) to find a suitable keypad\nlayout for each user, based on their personal text history. It incorporates\ncodes for frequent multigrams, which may be directly input. This greatly\nreduces the average number of strokes required for typing. We optimize for\ntwo-handed use, the left thumb covering the leftmost rows and vice versa. The\nGA mini- mizes the number of strokes, consecutive use of the same key, and\nconsecutive use of the same hand. Using these criteria, the algorithm\nre-arranges the 26 alphabetic characters, plus 14 additional multigrams, on the\n10-key pad. We demonstrate that this arrangement can generate a more effective\nlayout, especially for SMS-style messages. Substantial savings are verified by\nboth computational analysis and human evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 03:59:25 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Lee", "Joonseok", "", "Bob"], ["I.", "R.", "", "Bob"], ["McKay", "", ""]]}, {"id": "1412.1251", "submitter": "Tarek Toumi", "authors": "Tarek Toumi and Abdelmadjid Zidani", "title": "From Human-Computer Interaction to Human-Robot Social Interaction", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 11,\n  Issue 1, No 1, 2014 1694-0814", "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-Robot Social Interaction became one of active research fields in which\nresearchers from different areas propose solutions and directives leading\nrobots to improve their interactions with humans. In this paper we propose to\nintroduce works in both human robot interaction and human computer interaction\nand to make a bridge between them, i.e. to integrate emotions and capabilities\nconcepts of the robot in human computer model to become adequate for human\nrobot interaction and discuss challenges related to the proposed model. Finally\nan illustration through real case of this model will be presented.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 09:40:46 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Toumi", "Tarek", ""], ["Zidani", "Abdelmadjid", ""]]}, {"id": "1412.1424", "submitter": "Amit Sharma", "authors": "Amit Sharma and Dan Cosley", "title": "Studying and Modeling the Connection between People's Preferences and\n  Content Sharing", "comments": "CSCW 2015", "journal-ref": null, "doi": "10.1145/2675133.2675151", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People regularly share items using online social media. However, people's\ndecisions around sharing---who shares what to whom and why---are not well\nunderstood. We present a user study involving 87 pairs of Facebook users to\nunderstand how people make their sharing decisions. We find that even when\nsharing to a specific individual, people's own preference for an item\n(individuation) dominates over the recipient's preferences (altruism). People's\nopen-ended responses about how they share, however, indicate that they do try\nto personalize shares based on the recipient. To explain these contrasting\nresults, we propose a novel process model of sharing that takes into account\npeople's preferences and the salience of an item. We also present encouraging\nresults for a sharing prediction model that incorporates both the senders' and\nthe recipients' preferences. These results suggest improvements to both\nalgorithms that support sharing in social media and to information diffusion\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 18:14:01 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Sharma", "Amit", ""], ["Cosley", "Dan", ""]]}, {"id": "1412.1772", "submitter": "Jeremy Frey", "authors": "J\\'er\\'emy Frey (INRIA Bordeaux - Sud-Ouest, LaBRI)", "title": "Heart Rate Monitoring as an Easy Way to Increase Engagement in\n  Human-Agent Interaction", "comments": "PhyCS - International Conference on Physiological Computing Systems,\n  Feb 2015, Angers, France. SCITEPRESS, \\&lt;http://www.phycs.org/\\&gt", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physiological sensors are gaining the attention of manufacturers and users.\nAs denoted by devices such as smartwatches or the newly released Kinect 2 --\nwhich can covertly measure heartbeats -- or by the popularity of smartphone\napps that track heart rate during fitness activities. Soon, physiological\nmonitoring could become widely accessible and transparent to users. We\ndemonstrate how one could take advantage of this situation to increase users'\nengagement and enhance user experience in human-agent interaction. We created\nan experimental protocol involving embodied agents -- \"virtual avatars\". Those\nagents were displayed alongside a beating heart. We compared a condition in\nwhich this feedback was simply duplicating the heart rates of users to another\ncondition in which it was set to an average heart rate. Results suggest a\nsuperior social presence of agents when they display feedback similar to users'\ninternal state. This physiological \"similarity-attraction\" effect may lead,\nwith little effort, to a better acceptance of agents and robots by the general\npublic.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 19:08:05 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "INRIA Bordeaux - Sud-Ouest, LaBRI"]]}, {"id": "1412.1790", "submitter": "Martin Hachet", "authors": "J\\'er\\'emy Frey (INRIA Bordeaux - Sud-Ouest, LaBRI), Renaud Gervais\n  (INRIA Bordeaux - Sud-Ouest, LaBRI), St\\'ephanie Fleck, Fabien Lotte (INRIA\n  Bordeaux - Sud-Ouest, LaBRI), Martin Hachet (INRIA Bordeaux - Sud-Ouest,\n  LaBRI)", "title": "Teegi: Tangible EEG Interface", "comments": "to appear in UIST-ACM User Interface Software and Technology\n  Symposium, Oct 2014, Honolulu, United States", "journal-ref": null, "doi": "10.1145/2642918.2647368", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Teegi, a Tangible ElectroEncephaloGraphy (EEG) Interface that\nenables novice users to get to know more about something as complex as brain\nsignals, in an easy, en- gaging and informative way. To this end, we have\ndesigned a new system based on a unique combination of spatial aug- mented\nreality, tangible interaction and real-time neurotech- nologies. With Teegi, a\nuser can visualize and analyze his or her own brain activity in real-time, on a\ntangible character that can be easily manipulated, and with which it is\npossible to interact. An exploration study has shown that interacting with\nTeegi seems to be easy, motivating, reliable and infor- mative. Overall, this\nsuggests that Teegi is a promising and relevant training and mediation tool for\nthe general public.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 20:04:08 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Frey", "J\u00e9r\u00e9my", "", "INRIA Bordeaux - Sud-Ouest, LaBRI"], ["Gervais", "Renaud", "", "INRIA Bordeaux - Sud-Ouest, LaBRI"], ["Fleck", "St\u00e9phanie", "", "INRIA\n  Bordeaux - Sud-Ouest, LaBRI"], ["Lotte", "Fabien", "", "INRIA\n  Bordeaux - Sud-Ouest, LaBRI"], ["Hachet", "Martin", "", "INRIA Bordeaux - Sud-Ouest,\n  LaBRI"]]}, {"id": "1412.2122", "submitter": "V\\'ictor Ponce-L\\'opez", "authors": "V\\'ictor Ponce-L\\'opez, Sergio Escalera, Marc P\\'erez, Oriol Jan\\'es,\n  Xavier Bar\\'o", "title": "Non-Verbal Communication Analysis in Victim-Offender Mediations", "comments": "Please, find the supplementary video material at:\n  http://sunai.uoc.edu/~vponcel/video/VOMSessionSample.mp4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a non-invasive ambient intelligence framework for\nthe semi-automatic analysis of non-verbal communication applied to the\nrestorative justice field. In particular, we propose the use of computer vision\nand social signal processing technologies in real scenarios of Victim-Offender\nMediations, applying feature extraction techniques to multi-modal\naudio-RGB-depth data. We compute a set of behavioral indicators that define\ncommunicative cues from the fields of psychology and observational methodology.\nWe test our methodology on data captured in real world Victim-Offender\nMediation sessions in Catalonia in collaboration with the regional government.\nWe define the ground truth based on expert opinions when annotating the\nobserved social responses. Using different state-of-the-art binary\nclassification approaches, our system achieves recognition accuracies of 86%\nwhen predicting satisfaction, and 79% when predicting both agreement and\nreceptivity. Applying a regression strategy, we obtain a mean deviation for the\npredictions between 0.5 and 0.7 in the range [1-5] for the computed social\nsignals.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 12:56:43 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 18:12:48 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Ponce-L\u00f3pez", "V\u00edctor", ""], ["Escalera", "Sergio", ""], ["P\u00e9rez", "Marc", ""], ["Jan\u00e9s", "Oriol", ""], ["Bar\u00f3", "Xavier", ""]]}, {"id": "1412.2855", "submitter": "Jagmohan Chauhan", "authors": "Jagmohan Chauhan, Hassan Jameel Asghar, Mohamed Ali Kaafar, Anirban\n  Mahanti", "title": "Gesture-based Continuous Authentication for Wearable Devices: the Google\n  Glass Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the feasibility of touch gesture behavioural biometrics for implicit\nauthentication of users on a smartglass (Google Glass) by proposing a\ncontinuous authentication system using two classifiers: SVM with RBF kernel,\nand a new classifier based on Chebyshev's concentration inequality. Based on\ndata collected from 30 volunteers, we show that such authentication is feasible\nboth in terms of classification accuracy and computational load on\nsmartglasses. We achieve a classification accuracy of up to 99% with only 75\ntraining samples using behavioural biometric data from four different types of\ntouch gestures. To show that our system can be generalized, we test its\nperformance on touch data from smartphones and found the accuracy to be similar\nto smartglasses. Finally, our experiments on the permanence of gestures show\nthat the negative impact of changing user behaviour with time on classification\naccuracy can be best alleviated by periodically replacing older training\nsamples with new randomly chosen samples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 04:49:07 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 08:43:23 GMT"}, {"version": "v3", "created": "Sun, 6 Dec 2015 09:07:13 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2016 03:28:07 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2016 02:33:08 GMT"}, {"version": "v6", "created": "Thu, 25 Feb 2016 09:53:03 GMT"}, {"version": "v7", "created": "Sat, 23 Apr 2016 10:54:54 GMT"}, {"version": "v8", "created": "Mon, 9 May 2016 03:39:21 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Chauhan", "Jagmohan", ""], ["Asghar", "Hassan Jameel", ""], ["Kaafar", "Mohamed Ali", ""], ["Mahanti", "Anirban", ""]]}, {"id": "1412.2880", "submitter": "Iza Marfisi-Schottman", "authors": "Iza Marfisi-Schottman, S\\'ebastien George, Franck Tarpin-Bernard (LIG)", "title": "Evaluating Learning Games during their Conception", "comments": "European Conference on Game Based Learning, ECGBL, Oct 2014, Berlin,\n  Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning Games (LGs) are educational environments based on a playful approach\nto learning. Their use has proven to be promising in many domains, but is at\npresent restricted by the time consuming and costly nature of the developing\nprocess. In this paper, we propose a set of quality indicators that can help\nthe conception team to evaluate the quality of their LG during the designing\nprocess, and before it is developed. By doing so, the designers can identify\nand repair problems in the early phases of the conception and therefore reduce\nthe alteration phases, that occur after testing the LG's prototype. These\nquality indicators have been validated by 6 LG experts that used them to assess\nthe quality of 24 LGs in the process of being designed. They have also proven\nto be useful as design guidelines for novice LG designers.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 08:00:41 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Marfisi-Schottman", "Iza", "", "LIG"], ["George", "S\u00e9bastien", "", "LIG"], ["Tarpin-Bernard", "Franck", "", "LIG"]]}, {"id": "1412.3768", "submitter": "Alexia Schulz", "authors": "Andrea Brennen, David Danico, Raul Harnasch, Maureen Hunter, Richard\n  Larkin, Jeremy Mineweaser, Kevin Nam, David O'Gwynn, Harry Phan, Alexia\n  Schulz, Michael Snyder, Diane Staheli, Tamara Yu", "title": "A novel display for situational awareness at a network operations center", "comments": "Received honorable mention in VAST 2013 Challenge, appears in VAST\n  2013 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As modern industry shifts toward significant globalization, robust and\nadaptable network capability is increasingly vital to the success of business\nenterprises. Large quantities of information must be distilled and presented in\na single integrated picture in order to maintain the health, security and\nperformance of global networks. We present a design for a network situational\nawareness display that visually aggregates large quantities of data, identifies\nproblems in a network, assesses their impact on critical company mission areas\nand clarifies the utilization of resources. This display facilitates the\nprioritization of network problems as they arise by explicitly depicting how\nproblems interrelate. It also serves to coordinate mitigation strategies with\nmembers of a team.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 19:19:16 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Brennen", "Andrea", ""], ["Danico", "David", ""], ["Harnasch", "Raul", ""], ["Hunter", "Maureen", ""], ["Larkin", "Richard", ""], ["Mineweaser", "Jeremy", ""], ["Nam", "Kevin", ""], ["O'Gwynn", "David", ""], ["Phan", "Harry", ""], ["Schulz", "Alexia", ""], ["Snyder", "Michael", ""], ["Staheli", "Diane", ""], ["Yu", "Tamara", ""]]}, {"id": "1412.4179", "submitter": "Michael Lissack", "authors": "Terrence Letiche, Michael Lissack", "title": "What About Feedback?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role of immediate feedback in-group conversations has received scant\nattention in the recent literature. While studies from the early 1990's\nsuggested that \"added information\" in the form of non-verbal cues would allow\nvideo conferencing to \"augment\" the audio-only conference in terms of\neffectiveness, stunningly little follow-on research has been done reflective of\nthe current state of computer mediated communication, video conferencing, \"live\nwalls\", etc. This article contrasts three studies of immediate feedback in\nin-person settings as the basis for suggesting a new research program -\nresearch to look at potential effects of augmenting video-conferencing with an\nimmediate feedback channel.\n", "versions": [{"version": "v1", "created": "Sat, 13 Dec 2014 02:11:44 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Letiche", "Terrence", ""], ["Lissack", "Michael", ""]]}, {"id": "1412.5207", "submitter": "Anish Acharya", "authors": "Anish Acharya", "title": "Are We Ready for Driver-less Vehicles? Security vs. Privacy- A Social\n  Perspective", "comments": "17 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At this moment Autonomous cars are probably the biggest and most talked about\ntechnology in the Robotics Research Community. In spite of great technological\nadvances over past few years a full edged autonomous car is still far from\nreality. This article talks about the existing system and discusses the\npossibility of a Computer Vision enabled driving being superior than the LiDar\nbased system. A detailed overview of privacy violations that might arise from\nautonomous driving has been discussed in detail both from a technical as well\nas legal perspective. It has been proved through evidence and arguments that\nefficient and accurate estimation and efficient solution of the constraint\nsatisfaction problem addressed in the case of autonomous cars are negatively\ncorrelated with the preserving the privacy of the user. It is a very difficult\ntrade-off since both are very important aspects and has to be taken into\naccount. The fact that one cannot compromise with the safety issues of the car\nmakes it inevitable to run into serious privacy concerns that might have\nadverse social and political effects.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 21:48:53 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Acharya", "Anish", ""]]}, {"id": "1412.5401", "submitter": "Alex Volinsky", "authors": "Ilya V. Osipov, Alex A. Volinsky, Vadim V. Grishin", "title": "Gamification, virality and retention in educational online platform.\n  Measurable indicators and market entry strategy", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2015.060402", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes gamification, virality and retention in the freemium\neducational online platform with 40,000 users as an example. Relationships\nbetween virality and retention parameters as measurable metrics are calculated\nand discussed using real examples. Virality and monetization can be both\ncompeting and complementary mechanisms for the system growth. The K-growth\nfactor, which combines both virality and retention, is proposed as the metrics\nof the overall freemium system performance in terms of the user base growth.\nThis approach can be tested using a small number of users to assess the system\npotential performance. If the K-growth factor is less than one, the product\nneeds further development. If the K-growth factor it is greater than one, the\nsystem retains existing and attracts new users, thus a large scale market\nlaunch can be successful.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 14:13:34 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Osipov", "Ilya V.", ""], ["Volinsky", "Alex A.", ""], ["Grishin", "Vadim V.", ""]]}, {"id": "1412.5583", "submitter": "Salvatore Iaconesi", "authors": "Salvatore Iaconesi, Oriana Persico", "title": "Visualising Emotional Landmarks in Cities", "comments": "6 pages, 5 figures, IV2014 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different people and cultures associate different emotional states to\ndifferent parts and spaces of cities. These vary according to individuals,\ntheir cultures and also to the time of day, day of week, season, special\noccasions and more. Recurring patterns may occur in correspondence of the\nplaces in which people work, study, entertain themselves, consume, relate, wait\nor just take a break. What can we learn from these patterns? Trying to find\npossible answers to this question passes through the possibility to visualize\nand represent the configurations of emotional expressions in urban spaces,\nacross time, geography, theme, cultures and other dimensions. We have developed\nways in which it is possible to harvest people's geo-located (or geo-locatable)\nemotional expressions from major social networks and to visualize them\naccording to a variety of different modalities. In this paper we will present a\nseries of these types of visualizations, and the ways in which they can be used\nto gain better understandings of these emotional patterns as they arise, from\npoints of view which derive from anthropology, urbanism, sociology, politics\nand also arts and poetics. The paper will focus on the ways in which the data\nis harvested from different social networks, then categorized and annotated\nwith meta-data describing the emotional states, the languages in which people\nexpress themselves, the geographic locations, the themes expressed. A\nmethodology for representing this information across a variety of domains\n(time, space, emotion, theme) will then be presented in detail. A reflection on\npossible usage cases for anthropology, urbanism, policy-making, arts and design\nwill end the contribution, as well as the description of series of open issues\nand the indication of possible next-steps for research.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 20:14:49 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Iaconesi", "Salvatore", ""], ["Persico", "Oriana", ""]]}, {"id": "1412.6029", "submitter": "Jie Fu", "authors": "Jie Fu and Ufuk Topcu", "title": "Pareto efficiency in synthesizing shared autonomy policies with temporal\n  logic constraints", "comments": "8 pages, 5 figures, submitted to ICRA 2015 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In systems in which control authority is shared by an autonomous controller\nand a human operator, it is important to find solutions that achieve a\ndesirable system performance with a reasonable workload for the human operator.\nWe formulate a shared autonomy system capable of capturing the interaction and\nswitching control between an autonomous controller and a human operator, as\nwell as the evolution of the operator's cognitive state during control\nexecution. To trade-off human's effort and the performance level, e.g.,\nmeasured by the probability of satisfying the underlying temporal logic\nspecification, a two-stage policy synthesis algorithm is proposed for\ngenerating Pareto efficient coordination and control policies with respect to\nuser specified weights. We integrate the Tchebychev scalarization method for\nmulti-objective optimization methods to obtain a better coverage of the set of\nPareto efficient solutions than linear scalarization methods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 19:49:03 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Fu", "Jie", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1412.6079", "submitter": "Hamid Izadinia", "authors": "Fereshteh Sadeghi, Hamid Izadinia", "title": "Decoding the Text Encoding", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word clouds and text visualization is one of the recent most popular and\nwidely used types of visualizations. Despite the attractiveness and simplicity\nof producing word clouds, they do not provide a thorough visualization for the\ndistribution of the underlying data. Therefore, it is important to redesign\nword clouds for improving their design choices and to be able to do further\nstatistical analysis on data. In this paper we have proposed a fully automatic\nredesigning algorithm for word cloud visualization. Our proposed method is able\nto decode an input word cloud visualization and provides the raw data in the\nform of a list of (word, value) pairs. To the best of our knowledge our work is\nthe first attempt to extract raw data from word cloud visualization. We have\ntested our proposed method both qualitatively and quantitatively. The results\nof our experiments show that our algorithm is able to extract the words and\ntheir weights effectively with considerable low error rate.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 04:34:16 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Sadeghi", "Fereshteh", ""], ["Izadinia", "Hamid", ""]]}, {"id": "1412.6140", "submitter": "Andres Monroy-Hernandez", "authors": "Yuheng Hu, Shelly D. Farnham, Andres Monroy-Hernandez", "title": "Whoo.ly: Facilitating Information Seeking For Hyperlocal Communities\n  Using Social Media", "comments": "ACM CHI Conference on Human Factors in Computing Systems 2013, best\n  paper honorable mention, 10 pages, 4 figures", "journal-ref": null, "doi": "10.1145/2470654.2466478", "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media systems promise powerful opportunities for people to connect to\ntimely, relevant information at the hyper local level. Yet, finding the\nmeaningful signal in noisy social media streams can be quite daunting to users.\nIn this paper, we present and evaluate Whoo.ly, a web service that provides\nneighborhood-specific information based on Twitter posts that were\nautomatically inferred to be hyperlocal. Whoo.ly automatically extracts and\nsummarizes hyperlocal information about events, topics, people, and places from\nthese Twitter posts. We provide an overview of our design goals with Whoo.ly\nand describe the system including the user interface and our unique event\ndetection and summarization algorithms. We tested the usefulness of the system\nas a tool for finding neighborhood information through a comprehensive user\nstudy. The outcome demonstrated that most participants found Whoo.ly easier to\nuse than Twitter and they would prefer it as a tool for exploring their\nneighborhoods.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 21:41:59 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Hu", "Yuheng", ""], ["Farnham", "Shelly D.", ""], ["Monroy-Hernandez", "Andres", ""]]}, {"id": "1412.6159", "submitter": "Hamed Haddadi", "authors": "Richard Mortier, Hamed Haddadi, Tristan Henderson, Derek McAuley, Jon\n  Crowcroft", "title": "Human-Data Interaction: The Human Face of the Data-Driven Society", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing generation and collection of personal data has created a\ncomplex ecosystem, often collaborative but sometimes combative, around\ncompanies and individuals engaging in the use of these data. We propose that\nthe interactions between these agents warrants a new topic of study: Human-Data\nInteraction (HDI). In this paper we discuss how HDI sits at the intersection of\nvarious disciplines, including computer science, statistics, sociology,\npsychology and behavioural economics. We expose the challenges that HDI raises,\norganised into three core themes of legibility, agency and negotiability, and\nwe present the HDI agenda to open up a dialogue amongst interested parties in\nthe personal and big data ecosystems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 19:11:53 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2015 07:59:40 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Mortier", "Richard", ""], ["Haddadi", "Hamed", ""], ["Henderson", "Tristan", ""], ["McAuley", "Derek", ""], ["Crowcroft", "Jon", ""]]}, {"id": "1412.6378", "submitter": "Pierre de Buyl", "authors": "Bastian Venthur, Benjamin Blankertz", "title": "Wyrm, A Pythonic Toolbox for Brain-Computer Interfacing", "comments": "Part of the Proceedings of the 7th European Conference on Python in\n  Science (EuroSciPy 2014), Pierre de Buyl and Nelle Varoquaux editors, (2014)", "journal-ref": null, "doi": null, "report-no": "euroscipy-proceedings2014-04", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A Brain-Computer Interface (BCI) is a system that measures central nervous\nsystem activity and translates the recorded data into an output suitable for a\ncomputer to use as an input signal. Such a BCI system consists of three parts,\nthe signal acquisition, the signal processing and the feedback/stimulus\npresentation. In this paper we present Wyrm, a signal processing toolbox for\nBCI in Python. Wyrm is applicable to a broad range of neuroscientific problems\nand capable for running online experiments in real time and off-line data\nanalysis and visualisation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 15:34:01 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Venthur", "Bastian", ""], ["Blankertz", "Benjamin", ""]]}, {"id": "1412.6605", "submitter": "Reza Rawassizadeh", "authors": "Stefan Foell, Gerd Kortuem, Reza Rawassizadeh, Marcus Handte, Umer\n  Iqbal, Pedro Marron", "title": "Micro-Navigation for Urban Bus Passengers: Using the Internet of Things\n  to Improve the Public Transport Experience", "comments": "Urb-IoT 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public bus services are widely deployed in cities around the world because\nthey provide cost-effective and economic public transportation. However, from a\npassenger point of view urban bus systems can be complex and difficult to\nnavigate, especially for disadvantaged users, i.e. tourists, novice users,\nolder people, and people with impaired cognitive or physical abilities. We\npresent Urban Bus Navigator (UBN), a reality-aware urban navigation system for\nbus passengers with the ability to recognize and track the physical public\ntransport infrastructure such as buses. Unlike traditional location-aware\nmobile transport applications, UBN acts as a true navigation assistant for\npublic transport users. Insights from a six-month long trial in Madrid indicate\nthat UBN removes barriers for public transport usage and has a positive impact\non how people feel about public transport journeys.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 05:10:20 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 04:19:28 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Foell", "Stefan", ""], ["Kortuem", "Gerd", ""], ["Rawassizadeh", "Reza", ""], ["Handte", "Marcus", ""], ["Iqbal", "Umer", ""], ["Marron", "Pedro", ""]]}, {"id": "1412.6706", "submitter": "Dustin Arendt", "authors": "Dustin L. Arendt and Leslie M. Blaha", "title": "SVEN: Informative Visual Representation of Complex Dynamic Structure", "comments": "Python, JavaScript, & HTML source contained within ancillary folder\n  under MPL2.0 license", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GR cs.HC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs change over time, and typically variations on the small multiples or\nanimation pattern is used to convey this dynamism visually. However, both of\nthese classical techniques have significant drawbacks, so a new approach,\nStoryline Visualization of Events on a Network (SVEN) is proposed. SVEN builds\non storyline techniques, conveying nodes as contiguous lines over time. SVEN\nencodes time in a natural manner, along the horizontal axis, and optimizes the\nvertical placement of storylines to decrease clutter (line crossings,\nstraightness, and bends) in the drawing. This paper demonstrates SVEN on\nseveral different flavors of real-world dynamic data, and outlines the\nremaining near-term future work.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 23:21:46 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Arendt", "Dustin L.", ""], ["Blaha", "Leslie M.", ""]]}, {"id": "1412.7677", "submitter": "Roshan Ragel", "authors": "C.B Bulumulla and R. G. Ragel", "title": "LineCAPTCHA Mobile: A User Friendly Replacement for Unfriendly Reverse\n  Turing Tests for Mobile Devices (ICIAfS14)", "comments": "The 7th International Conference on Information and Automation for\n  Sustainability (ICIAfS) 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As smart phones and tablets are becoming ubiquitous and taking over as the\nprimary choice for accessing the Internet worldwide, ensuring a secure gateway\nto the servers serving such devices become essential. CAPTCHAs play an\nimportant role in identifying human users in internet to prevent unauthorized\nbot attacks. Even though there are numerous CAPTCHA alternatives available\ntoday, there are certain drawbacks attached with each alternative, making them\nharder to find a general solution for the necessity of a CAPTCHA mechanism.\nWith the advancing technology and expertise in areas such as AI, cryptography\nand image processing, it has come to a stage where the chase between making and\nbreaking CAPTCHAs are even now. This has led the humans with a hard time\ndeciphering the CAPTCHA mechanisms. In this paper, we adapt a novel CAPTCHA\nmechanism named as LineCAPTCHA to mobile devices. LineCAPTCHA is a new reverse\nTuring test based on drawing on top of Bezier curves within noisy backgrounds.\nThe major objective of this paper is to report the implementation and\nevaluation of LineCAPTCHA on a mobile platform. At the same time we impose\ncertain security standards and security aspects for establishing LineCAPTCHAs\nwhich are obtained through extensive measures. Independency from factors such\nas the fluency in English language, age and easily understandable nature of it\ninclines the usability of LineCAPTCHA. We believe that such independency will\nfavour the main target of LineCAPTCHA, user friendliness and usability.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 14:49:14 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Bulumulla", "C. B", ""], ["Ragel", "R. G.", ""]]}, {"id": "1412.7932", "submitter": "Raunaq Vohra", "authors": "Kratarth Goel, Raunaq Vohra, Anant Kamath, and Veeky Baths", "title": "Home Automation Using SSVEP & Eye-Blink Detection Based Brain-Computer\n  Interface", "comments": "2 pages, 1 table, published at IEEE SMC 2014", "journal-ref": null, "doi": "10.1109/SMC.2014.6974563", "report-no": null, "categories": "cs.HC cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel brain computer interface based home\nautomation system using two responses - Steady State Visually Evoked Potential\n(SSVEP) and the eye-blink artifact, which is augmented by a Bluetooth based\nindoor localization system, to greatly increase the number of controllable\ndevices. The hardware implementation of this system to control a table lamp and\ntable fan using brain signals has also been discussed and state-of-the-art\nresults have been achieved.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 12:50:07 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Goel", "Kratarth", ""], ["Vohra", "Raunaq", ""], ["Kamath", "Anant", ""], ["Baths", "Veeky", ""]]}, {"id": "1412.7977", "submitter": "Tim Evans", "authors": "D.S. Reiss, J.J. Price and T.S. Evans", "title": "Sculplexity: Sculptures of Complexity using 3D printing", "comments": "Free access to article on European Physics Letters", "journal-ref": "European Physics Letters 2013, 104, 48001", "doi": "10.1209/0295-5075/104/48001", "report-no": "Imperial/TP/13/TSE/1", "categories": "physics.ed-ph cs.HC math.HO physics.pop-ph physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to convert models of complex systems such as 2D cellular automata\ninto a 3D printed object. Our method takes into account the limitations\ninherent to 3D printing processes and materials. Our approach automates the\ngreater part of this task, bypassing the use of CAD software and the need for\nmanual design. As a proof of concept, a physical object representing a modified\nforest fire model was successfully printed. Automated conversion methods\nsimilar to the ones developed here can be used to create objects for research,\nfor demonstration and teaching, for outreach, or simply for aesthetic pleasure.\nAs our outputs can be touched, they may be particularly useful for those with\nvisual disabilities.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 14:22:51 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Reiss", "D. S.", ""], ["Price", "J. J.", ""], ["Evans", "T. S.", ""]]}]