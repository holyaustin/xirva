[{"id": "1804.00229", "submitter": "Alina Striner", "authors": "Alina Striner", "title": "Can Multisensory Cues in VR Help Train Pattern Recognition to Citizen\n  Scientists?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the internet of things (IoT) has integrated physical and digital\ntechnologies, designing for multiple sensory media (mulsemedia) has become more\nattainable. Designing technology for multiple senses has the capacity to\nimprove virtual realism, extend our ability to process information, and more\neasily transfer knowledge between physical and digital environments. HCI\nresearchers are beginning to explore the viability of integrating multimedia\ninto virtual experiences, however research has yet to consider whether\nmulsemedia truly enhances realism, immersion and knowledge transfer. My work\ndeveloping StreamBED, a VR training platform to train citizen science water\nmonitors plans to consider the role of mulsemedia in immersion and learning\ngoals. Future findings about the role of mulsemedia in learning contexts will\npotentially allow learners to experience, connect to, learn from spaces that\nare impossible to experience firsthand.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 00:00:51 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Striner", "Alina", ""]]}, {"id": "1804.00245", "submitter": "Sara Bunian", "authors": "Sara Bunian, Alessandro Canossa, Randy Colvin, Magy Seif El-Nasr", "title": "Modeling Individual Differences in Game Behavior using HMM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Player modeling is an important concept that has gained much attention in\ngame research due to its utility in developing adaptive techniques to target\nbetter designs for engagement and retention. Previous work has explored\nmodeling individual differences using machine learning algorithms per- formed\non aggregated game actions. However, players' individual differences may be\nbetter manifested through sequential patterns of the in-game player's actions.\nWhile few works have explored sequential analysis of player data, none have\nexplored the use of Hidden Markov Models (HMM) to model individual differences,\nwhich is the topic of this paper. In par- ticular, we developed a modeling\napproach using data col- lected from players playing a Role-Playing Game (RPG).\nOur proposed approach is two fold: 1. We present a Hidden Markov Model (HMM) of\nplayer in-game behaviors to model individual differences, and 2. using the\noutput of the HMM, we generate behavioral features used to classify real world\nplayers' characteristics, including game expertise and the big five personality\ntraits. Our results show predictive power for some of personality traits, such\nas game expertise and conscientiousness, but the most influential factor was\ngame expertise.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 01:43:48 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Bunian", "Sara", ""], ["Canossa", "Alessandro", ""], ["Colvin", "Randy", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "1804.00401", "submitter": "Carsten Binnig", "authors": "Prasetya Utama, Nathaniel Weir, Fuat Basik, Carsten Binnig, Ugur\n  Cetintemel, Benjamin H\\\"attasch, Amir Ilkhechi, Shekar Ramaswamy, Arif Usta", "title": "An End-to-end Neural Natural Language Interface for Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to extract insights from new data sets is critical for decision\nmaking. Visual interactive tools play an important role in data exploration\nsince they provide non-technical users with an effective way to visually\ncompose queries and comprehend the results. Natural language has recently\ngained traction as an alternative query interface to databases with the\npotential to enable non-expert users to formulate complex questions and\ninformation needs efficiently and effectively. However, understanding natural\nlanguage questions and translating them accurately to SQL is a challenging\ntask, and thus Natural Language Interfaces for Databases (NLIDBs) have not yet\nmade their way into practical tools and commercial products.\n  In this paper, we present DBPal, a novel data exploration tool with a natural\nlanguage interface. DBPal leverages recent advances in deep models to make\nquery understanding more robust in the following ways: First, DBPal uses a deep\nmodel to translate natural language statements to SQL, making the translation\nprocess more robust to paraphrasing and other linguistic variations. Second, to\nsupport the users in phrasing questions without knowing the database schema and\nthe query features, DBPal provides a learned auto-completion model that\nsuggests partial query extensions to users during query formulation and thus\nhelps to write complex queries.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 05:36:38 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Utama", "Prasetya", ""], ["Weir", "Nathaniel", ""], ["Basik", "Fuat", ""], ["Binnig", "Carsten", ""], ["Cetintemel", "Ugur", ""], ["H\u00e4ttasch", "Benjamin", ""], ["Ilkhechi", "Amir", ""], ["Ramaswamy", "Shekar", ""], ["Usta", "Arif", ""]]}, {"id": "1804.00651", "submitter": "Cairong Zhang", "authors": "Cairong Zhang, Guijin Wang, Hengkai Guo, Xinghao Chen, Fei Qiao,\n  Huazhong Yang", "title": "Interactive Hand Pose Estimation: Boosting accuracy in localizing\n  extended finger joints", "comments": "Original publication available on\n  https://doi.org/10.2352/ISSN.2470-1173.2018.2.VIPC-251", "journal-ref": "Electronic Imaging, Visual Information Processing and\n  Communication IX (2018), pp. 251-1-251-6(6)", "doi": "10.2352/ISSN.2470-1173.2018.2.VIPC-251", "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate 3D hand pose estimation plays an important role in Human Machine\nInteraction (HMI). In the reality of HMI, joints in fingers stretching out,\nespecially corresponding fingertips, are much more important than other joints.\nWe propose a novel method to refine stretching-out finger joint locations after\nobtaining rough hand pose estimation. It first detects which fingers are\nstretching out, then neighbor pixels of certain joint vote for its new location\nbased on random forests. The algorithm is tested on two public datasets: MSRA15\nand ICVL. After the refinement stage of stretching-out fingers, errors of\npredicted HMI finger joint locations are significantly reduced. Mean error of\nall fingertips reduces around 5mm (relatively more than 20%). Stretching-out\nfingertip locations are even more precise, which in MSRA15 reduces 10.51mm\n(relatively 41.4%).\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 17:59:38 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 15:31:40 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Zhang", "Cairong", ""], ["Wang", "Guijin", ""], ["Guo", "Hengkai", ""], ["Chen", "Xinghao", ""], ["Qiao", "Fei", ""], ["Yang", "Huazhong", ""]]}, {"id": "1804.00656", "submitter": "Claudio Sanhueza", "authors": "Claudio Sanhueza, Francia Jim\\'enez, Regina Berretta and Pablo Moscato", "title": "mQAPViz: A divide-and-conquer multi-objective optimization algorithm to\n  compute large data visualizations", "comments": "Proceeding GECCO 18 Proceedings of the Genetic and Evolutionary\n  Computation Conference. Pages 737-744 Kyoto, Japan - July 15 - 19, 2018", "journal-ref": null, "doi": "10.1145/3205455.3205457", "report-no": null, "categories": "cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for data visualizations are essential tools for transforming data\ninto useful narratives. Unfortunately, very few visualization algorithms can\nhandle the large datasets of many real-world scenarios. In this study, we\naddress the visualization of these datasets as a Multi-Objective Optimization\nProblem. We propose mQAPViz, a divide-and-conquer multi-objective optimization\nalgorithm to compute large-scale data visualizations. Our method employs the\nMulti-Objective Quadratic Assignment Problem (mQAP) as the mathematical\nfoundation to solve the visualization task at hand. The algorithm applies\nadvanced sampling techniques originating from the field of machine learning and\nefficient data structures to scale to millions of data objects. The algorithm\nallocates objects onto a 2D grid layout. Experimental results on real-world and\nlarge datasets demonstrate that mQAPViz is a competitive alternative to\nexisting techniques.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 09:55:10 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 07:01:14 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 03:10:30 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Sanhueza", "Claudio", ""], ["Jim\u00e9nez", "Francia", ""], ["Berretta", "Regina", ""], ["Moscato", "Pablo", ""]]}, {"id": "1804.00974", "submitter": "Birgitta Dresp-Langley", "authors": "A.U. Batmaz, M. de Mathelin and Birgitta Dresp-Langley", "title": "Seeing virtual while acting real: Visual display and strategy effects on\n  the time and precision of eye-hand coordination", "comments": "arXiv admin note: text overlap with arXiv:1803.11283", "journal-ref": "2017, PLoS ONE 12(8): e0183789", "doi": "10.1371/journal.pone.0183789", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effects of computer generated 2D and 3D views on the time and precision of\nbare-handed or tool-mediated eye-hand coordination were investigated in a\npick-and-place-task with complete novices. All of them scored well above\naverage in spatial perspective taking ability and performed the task with their\ndominant hand. Two groups of novices, four men and four women in each group,\nhad to place a small object in a precise order on the centre of five targets on\na Real-world Action Field (RAF), as swiftly as possible and as precisely as\npossible, using a tool or not (control). Each individual session consisted of\nfour visual display conditions. The order of conditions was counterbalanced\nbetween individuals and sessions. Subjects looked at what their hands were\ndoing 1) directly in front of them (natural top-down view) 2) in topdown 2D\nfisheye camera view 3) in top-down undistorted 2D view or 4) in 3D stereoscopic\ntop-down view (head-mounted OCULUS DK 2). It was made sure that object\nmovements in all image conditions matched the real-world movements in time and\nspace. One group was looking at the 2D images with the monitor positioned\nsideways (sub-optimal); the other group was looking at the monitor placed\nstraight ahead of them (near-optimal). All image viewing conditions had\nsignificantly detrimental effects on time (seconds) and precision (pixels) of\ntask execution when compared with natural direct viewing.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 12:55:58 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Batmaz", "A. U.", ""], ["de Mathelin", "M.", ""], ["Dresp-Langley", "Birgitta", ""]]}, {"id": "1804.01382", "submitter": "Chaochen Wu", "authors": "Chaochen Wu", "title": "Vanlearning: A Machine Learning SaaS Application for People Without\n  Programming Backgrounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although we have tons of machine learning tools to analyze data, most of them\nrequire users have some programming backgrounds. Here we introduce a SaaS\napplication which allows users analyze their data without any coding and even\nwithout any knowledge of machine learning. Users can upload, train, predict and\ndownload their data by simply clicks their mouses. Our system uses data\npre-processor and validator to relieve the computational cost of our server.\nThe simple architecture of Vanlearning helps developers can easily maintain and\nextend it.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 01:17:16 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Wu", "Chaochen", ""]]}, {"id": "1804.01774", "submitter": "Tomislav Petkovi\\'c", "authors": "Tomislav Petkovi\\'c, Ivan Markovi\\'c, Ivan Petrovi\\'c", "title": "Human Intention Recognition in Flexible Robotized Warehouses based on\n  Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid growth of e-commerce increases the need for larger warehouses and\ntheir automation, thus using robots as assistants to human workers becomes a\npriority. In order to operate efficiently and safely, robot assistants or the\nsupervising system should recognize human intentions. Theory of mind (ToM) is\nan intuitive conception of other agents' mental state, i.e., beliefs and\ndesires, and how they cause behavior. In this paper we present a ToM-based\nalgorithm for human intention recognition in flexible robotized warehouses. We\nhave placed the warehouse worker in a simulated 2D environment with three\npotential goals. We observe agent's actions and validate them with respect to\nthe goal locations using a Markov decision process framework. Those\nobservations are then processed by the proposed hidden Markov model framework\nwhich estimated agent's desires. We demonstrate that the proposed framework\npredicts human warehouse worker's desires in an intuitive manner and in the end\nwe discuss the simulation results.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 10:53:58 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Petkovi\u0107", "Tomislav", ""], ["Markovi\u0107", "Ivan", ""], ["Petrovi\u0107", "Ivan", ""]]}, {"id": "1804.02173", "submitter": "Egor Lakomkin", "authors": "Egor Lakomkin, Mohammad Ali Zamani, Cornelius Weber, Sven Magg and\n  Stefan Wermter", "title": "On the Robustness of Speech Emotion Recognition for Human-Robot\n  Interaction with Deep Neural Networks", "comments": "Submitted to IROS'18, Madrid, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech emotion recognition (SER) is an important aspect of effective\nhuman-robot collaboration and received a lot of attention from the research\ncommunity. For example, many neural network-based architectures were proposed\nrecently and pushed the performance to a new level. However, the applicability\nof such neural SER models trained only on in-domain data to noisy conditions is\ncurrently under-researched. In this work, we evaluate the robustness of\nstate-of-the-art neural acoustic emotion recognition models in human-robot\ninteraction scenarios. We hypothesize that a robot's ego noise, room\nconditions, and various acoustic events that can occur in a home environment\ncan significantly affect the performance of a model. We conduct several\nexperiments on the iCub robot platform and propose several novel ways to reduce\nthe gap between the model's performance during training and testing in\nreal-world conditions. Furthermore, we observe large improvements in the model\nperformance on the robot and demonstrate the necessity of introducing several\ndata augmentation techniques like overlaying background noise and loudness\nvariations to improve the robustness of the neural approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 09:03:29 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Lakomkin", "Egor", ""], ["Zamani", "Mohammad Ali", ""], ["Weber", "Cornelius", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "1804.02329", "submitter": "Mahdi Miraz", "authors": "Mahdi H. Miraz, Maaruf Ali and Peter S. Excell", "title": "Cross-cultural Usability Issues in E/M-Learning", "comments": null, "journal-ref": "Annals of Emerging Technologies in Computing (AETiC), Print ISSN:\n  2516-0281, Online ISSN: 2516-029X, pp. 46-55, Vol. 2, No. 2, 1st April 2018,\n  Published by International Association of Educators and Researchers (IAER)", "doi": "10.33166/AETiC.2018.02.005", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper gives an overview of electronic learning (E-Learning) and mobile\nlearning (M-Learning) adoption and diffusion trends, as well as their\nparticular traits, characteristics and issues, especially in terms of\ncross-cultural and universal usability. E-Learning and M-Learning models using\nweb services and cloud computing, as well as associated security concerns are\nall addressed. The benefits and enhancements that accrue from using mobile and\nother internet devices for the purposes of learning in academia are discussed.\nThe differences between traditional classroom-based learning, distance\nlearning, E-Learning and M-Learning models are compared and some conclusions\nare drawn.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 21:21:22 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Miraz", "Mahdi H.", ""], ["Ali", "Maaruf", ""], ["Excell", "Peter S.", ""]]}, {"id": "1804.02462", "submitter": "David Watkins-Valls", "authors": "David Watkins-Valls, Chaiwen Chou, Caroline Weinberg, Jacob Varley,\n  Kenneth Lyons, Sanjay Joshi, Lynne Weber, Joel Stein, Peter Allen", "title": "Human Robot Interface for Assistive Grasping", "comments": "8 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes a new human-in-the-loop (HitL) assistive grasping system\nfor individuals with varying levels of physical capabilities. We investigated\nthe feasibility of using four potential input devices with our assistive\ngrasping system interface, using able-bodied individuals to define a set of\nquantitative metrics that could be used to assess an assistive grasping system.\nWe then took these measurements and created a generalized benchmark for\nevaluating the effectiveness of any arbitrary input device into a HitL grasping\nsystem. The four input devices were a mouse, a speech recognition device, an\nassistive switch, and a novel sEMG device developed by our group that was\nconnected either to the forearm or behind the ear of the subject. These\npreliminary results provide insight into how different interface devices\nperform for generalized assistive grasping tasks and also highlight the\npotential of sEMG based control for severely disabled individuals.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 21:37:51 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Watkins-Valls", "David", ""], ["Chou", "Chaiwen", ""], ["Weinberg", "Caroline", ""], ["Varley", "Jacob", ""], ["Lyons", "Kenneth", ""], ["Joshi", "Sanjay", ""], ["Weber", "Lynne", ""], ["Stein", "Joel", ""], ["Allen", "Peter", ""]]}, {"id": "1804.02527", "submitter": "Shixia Liu", "authors": "Jaegul Choo and Shixia Liu", "title": "Visual Analytics for Explainable Deep Learning", "comments": "IEEE Computer Graphics and Applications, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep learning has been advancing the state of the art in artificial\nintelligence to a new level, and humans rely on artificial intelligence\ntechniques more than ever. However, even with such unprecedented advancements,\nthe lack of explanation regarding the decisions made by deep learning models\nand absence of control over their internal processes act as major drawbacks in\ncritical decision-making processes, such as precision medicine and law\nenforcement. In response, efforts are being made to make deep learning\ninterpretable and controllable by humans. In this paper, we review visual\nanalytics, information visualization, and machine learning perspectives\nrelevant to this aim, and discuss potential challenges and future research\ndirections.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 07:52:04 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Choo", "Jaegul", ""], ["Liu", "Shixia", ""]]}, {"id": "1804.02657", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Issei Tachibana", "title": "Emotion Orientated Recommendation System for Hiroshima Tourist by Fuzzy\n  Petri Net", "comments": "6 pages, 10 figures, Proc. of IEEE 6th International Workshop on\n  Computational Intelligence and Applications (IWCIA2013)", "journal-ref": null, "doi": "10.1109/IWCIA.2013.6624776", "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed an Android Smartophone application software for tourist\ninformation system. Especially, the agent system recommends the sightseeing\nspot and local hospitality corresponding to the current feelings. The system\nsuch as concierge can estimate user's emotion and mood by Emotion Generating\nCalculations and Mental State Transition Network. In this paper, the system\ndecides the next candidates for spots and foods by the reasoning of fuzzy Petri\nNet in order to make more smooth communication between human and smartphone.\nThe system was developed for Hiroshima Tourist Information and described some\nhospitality about the concierge system.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 09:18:56 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ichimura", "Takumi", ""], ["Tachibana", "Issei", ""]]}, {"id": "1804.02677", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Shin Kamada", "title": "Early Discovery of Chronic Non-attenders by Using NFC Attendance\n  Management System", "comments": "6 pages, 9 figures, Proc. of IEEE 6th International Workshop on\n  Computational Intelligence and Applications (IWCIA2013)", "journal-ref": null, "doi": "10.1109/IWCIA.2013.6624813", "report-no": null, "categories": "cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Near Field Communication (NFC) standards cover communications protocols and\ndata exchange formats. They are based on existing radio-frequency\nidentification (RFID) standards. In Japan, Felica card is a popular way to\nidentify the unique ID. Recently, the attendance management system (AMS) with\nRFID technology has been developed as a part of Smart University, which is the\neducational infrastructure using high technologies, such as ICT. However, the\nreader/writer for Felica is too expensive to build the AMS. NFC technology\nincludes not only Felica but other type of IC chips. The Android OS 2.3 and the\nlater can provide access to NFC functionality. Therefore, we developed AMS for\nuniversity with NFC on Nexus 7. Because Nexus 7 is a low cost smart tablet, a\nteacher can determine to use familiarly. Especially, this paper describes the\nmethod of early discovery for chronic non-attenders by using the AMS system on\n2 or more Nexus 7 which is connected each other via peer-to-peer communication.\nThe attendance situation collected from different Nexus 7 is merged into a\nSQLite file and then, the document is reported to operate with the trunk system\nin educational affairs section.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 11:58:54 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ichimura", "Takumi", ""], ["Kamada", "Shin", ""]]}, {"id": "1804.02813", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Kosuke Tanabe, Toshiyuki Yamashita", "title": "An Adaptive Learning Method of Personality Trait Based Mood in Mental\n  State Transition Network by Recurrent Neural Network", "comments": "6 pages, 9 figures, Proc. of IEEE 7th International Workshop on\n  Computational Intelligence and Applications (IWCIA2014)", "journal-ref": null, "doi": "10.1109/IWCIA.2014.6988081", "report-no": null, "categories": "cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental State Transition Network (MSTN) is a basic concept of approximating to\nhuman psychological and mental responses. A stimulus calculated by Emotion\nGenerating Calculations (EGC) method can cause the transition of mood from an\nemotional state to others. In this paper, the agent can interact with human to\nrealize smooth communication by an adaptive learning method of the user's\npersonality trait based mood. The learning method consists of the profit\nsharing (PS) method and the recurrent neural network (RNN). An emotion for\nsensor inputs to MSTN is calculated by EGC and the variance of emotion leads to\nthe change of mental state, and then the sequence of states forms an episode.\nIn order to learn the tendency of personality trait effectively, the\nineffective rules should be removed from the episode. PS method finds out a\ndetour in episode and should be deleted. Furthermore, RNN works to realize the\nvariance of user's mood. Some experimental results were shown the success of\nrepresenting a various human's delicate emotion.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 04:41:27 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Ichimura", "Takumi", ""], ["Tanabe", "Kosuke", ""], ["Yamashita", "Toshiyuki", ""]]}, {"id": "1804.02960", "submitter": "Simone Gelmini", "authors": "Simone Gelmini, Silvia Strada, Mara Tanelli, Sergio Savaresi and\n  Vincenzo Biase", "title": "Analysis and development of a novel algorithm for the in-vehicle\n  hand-usage of a smartphone", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Smartphone usage while driving is unanimously considered to be a really\ndangerous habit due to strong correlation with road accidents. In this paper,\nthe problem of detecting whether the driver is using the phone during a trip is\naddressed. To do this, high-frequency data from the triaxial inertial\nmeasurement unit (IMU) integrated in almost all modern phone is processed\nwithout relying on external inputs so as to provide a self-contained approach.\nBy resorting to a frequency-domain analysis, it is possible to extract from the\nraw signals the useful information needed to detect when the driver is using\nthe phone, without being affected by the effects that vehicle motion has on the\nsame signals. The selected features are used to train a Support Vector Machine\n(SVM) algorithm. The performance of the proposed approach are analyzed and\ntested on experimental data collected during mixed naturalistic driving\nscenarios, proving the effectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 11:42:06 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 09:53:41 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Gelmini", "Simone", ""], ["Strada", "Silvia", ""], ["Tanelli", "Mara", ""], ["Savaresi", "Sergio", ""], ["Biase", "Vincenzo", ""]]}, {"id": "1804.03048", "submitter": "Cagatay Demiralp", "authors": "Marco Cavallo and \\c{C}a\\u{g}atay Demiralp", "title": "Clustrophile 2: Guided Visual Clustering Analysis", "comments": "IEEE VIS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data clustering is a common unsupervised learning method frequently used in\nexploratory data analysis. However, identifying relevant structures in\nunlabeled, high-dimensional data is nontrivial, requiring iterative\nexperimentation with clustering parameters as well as data features and\ninstances. The number of possible clusterings for a typical dataset is vast,\nand navigating in this vast space is also challenging. The absence of\nground-truth labels makes it impossible to define an optimal solution, thus\nrequiring user judgment to establish what can be considered a satisfiable\nclustering result. Data scientists need adequate interactive tools to\neffectively explore and navigate the large clustering space so as to improve\nthe effectiveness of exploratory clustering analysis. We introduce\n\\textit{Clustrophile~2}, a new interactive tool for guided clustering analysis.\n\\textit{Clustrophile~2} guides users in clustering-based exploratory analysis,\nadapts user feedback to improve user guidance, facilitates the interpretation\nof clusters, and helps quickly reason about differences between clusterings. To\nthis end, \\textit{Clustrophile~2} contributes a novel feature, the Clustering\nTour, to help users choose clustering parameters and assess the quality of\ndifferent clustering results in relation to current analysis goals and user\nexpectations. We evaluate \\textit{Clustrophile~2} through a user study with 12\ndata scientists, who used our tool to explore and interpret sub-cohorts in a\ndataset of Parkinson's disease patients. Results suggest that\n\\textit{Clustrophile~2} improves the speed and effectiveness of exploratory\nclustering analysis for both experts and non-experts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 15:05:56 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 04:25:02 GMT"}, {"version": "v3", "created": "Sat, 8 Sep 2018 02:57:14 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1804.03126", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "Victor Dibia and \\c{C}a\\u{g}atay Demiralp", "title": "Data2Vis: Automatic Generation of Data Visualizations Using Sequence to\n  Sequence Recurrent Neural Networks", "comments": "IEEE VDS'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapidly creating effective visualizations using expressive grammars is\nchallenging for users who have limited time and limited skills in statistics\nand data visualization. Even high-level, dedicated visualization tools often\nrequire users to manually select among data attributes, decide which\ntransformations to apply, and specify mappings between visual encoding\nvariables and raw or transformed attributes.\n  In this paper we introduce Data2Vis, a neural translation model for\nautomatically generating visualizations from given datasets. We formulate\nvisualization generation as a sequence to sequence translation problem where\ndata specifications are mapped to visualization specifications in a declarative\nlanguage (Vega-Lite). To this end, we train a multilayered attention-based\nrecurrent neural network (RNN) with long short-term memory (LSTM) units on a\ncorpus of visualization specifications.\n  Qualitative results show that our model learns the vocabulary and syntax for\na valid visualization specification, appropriate transformations (count, bins,\nmean) and how to use common data selection patterns that occur within data\nvisualizations. Data2Vis generates visualizations that are comparable to\nmanually-created visualizations in a fraction of the time, with potential to\nlearn more complex visualization strategies at scale.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 17:48:23 GMT"}, {"version": "v2", "created": "Thu, 19 Apr 2018 05:18:37 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 22:02:37 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Dibia", "Victor", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1804.03194", "submitter": "Andreas Henelius", "authors": "Andreas Henelius and Emilia Oikarinen and Kai Puolam\\\"aki", "title": "Human-Guided Data Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outcome of the explorative data analysis (EDA) phase is vital for\nsuccessful data analysis. EDA is more effective when the user interacts with\nthe system used to carry out the exploration. In the recently proposed paradigm\nof iterative data mining the user controls the exploration by inputting\nknowledge in the form of patterns observed during the process. The system then\nshows the user views of the data that are maximally informative given the\nuser's current knowledge. Although this scheme is good at showing surprising\nviews of the data to the user, there is a clear shortcoming: the user cannot\nsteer the process. In many real cases we want to focus on investigating\nspecific questions concerning the data. This paper presents the Human Guided\nData Exploration framework, generalising previous research. This framework\nallows the user to incorporate existing knowledge into the exploration process,\nfocus on exploring a subset of the data, and compare different complex\nhypotheses concerning relations in the data. The framework utilises a\ncomputationally efficient constrained randomisation scheme. To showcase the\nframework, we developed a free open-source tool, using which the empirical\nevaluation on real-world datasets was carried out. Our evaluation shows that\nthe ability to focus on particular subsets and being able to compare hypotheses\nare important additions to the interactive iterative data mining process.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:29:41 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Henelius", "Andreas", ""], ["Oikarinen", "Emilia", ""], ["Puolam\u00e4ki", "Kai", ""]]}, {"id": "1804.03209", "submitter": "Pete Warden", "authors": "Pete Warden", "title": "Speech Commands: A Dataset for Limited-Vocabulary Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Describes an audio dataset of spoken words designed to help train and\nevaluate keyword spotting systems. Discusses why this task is an interesting\nchallenge, and why it requires a specialized dataset that is different from\nconventional datasets used for automatic speech recognition of full sentences.\nSuggests a methodology for reproducible and comparable accuracy metrics for\nthis task. Describes how the data was collected and verified, what it contains,\nprevious versions and properties. Concludes by reporting baseline results of\nmodels trained on this dataset.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:58:17 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Warden", "Pete", ""]]}, {"id": "1804.03211", "submitter": "Jens Grubert", "authors": "Michel Pahud and Eyal Ofek and Nathalie Henry Riche and Christophe\n  Hurter and Jens Grubert", "title": "Mobiles as Portals for Interacting with Virtual Data Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a set of techniques leveraging mobile devices as lenses to\nexplore, interact and annotate n-dimensional data visualizations. The\ndemocratization of mobile devices, with their arrays of integrated sensors,\nopens up opportunities to create experiences for anyone to explore and interact\nwith large information spaces anywhere. In this paper, we propose to revisit\nideas behind the Chameleon prototype of Fitzmaurice et al. initially envisioned\nin the 90s for navigation, before spatially-aware devices became mainstream. We\nalso take advantage of other input modalities such as pen and touch to not only\nnavigate the space using the mobile as a lens, but interact and annotate it by\nadding toolglasses.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 20:03:30 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Pahud", "Michel", ""], ["Ofek", "Eyal", ""], ["Riche", "Nathalie Henry", ""], ["Hurter", "Christophe", ""], ["Grubert", "Jens", ""]]}, {"id": "1804.03258", "submitter": "Azra Bihorac", "authors": "Meghan Brennan, Sahil Puri, Tezcan Ozrazgat-Baslanti, Rajendra Bhat,\n  Zheng Feng, Petar Momcilovic, Xiaolin Li, Daisy Zhe Wang, Azra Bihorac", "title": "Comparing Clinical Judgment with MySurgeryRisk Algorithm for\n  Preoperative Risk Assessment: A Pilot Study", "comments": "21 pages, 4 tables", "journal-ref": "Surgery 165(5):1035-1045 (2019)", "doi": "10.1016/j.surg.2019.01.002", "report-no": "PMCID: PMC6502657", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Major postoperative complications are associated with increased\nshort and long-term mortality, increased healthcare cost, and adverse long-term\nconsequences. The large amount of data contained in the electronic health\nrecord (EHR) creates barriers for physicians to recognize patients most at\nrisk. We hypothesize, if presented in an optimal format, information from\ndata-driven predictive risk algorithms for postoperative complications can\nimprove physician risk assessment. Methods: Prospective, non-randomized,\ninterventional pilot study of twenty perioperative physicians at a quarterly\nacademic medical center. Using 150 clinical cases we compared physicians' risk\nassessment before and after interaction with MySurgeryRisk, a validated\nmachine-learning algorithm predicting preoperative risk for six major\npostoperative complications using EHR data. Results: The area under the curve\n(AUC) of MySurgeryRisk algorithm ranged between 0.73 and 0.85 and was\nsignificantly higher than physicians' risk assessments (AUC between 0.47 and\n0.69) for all postoperative complications except cardiovascular complications.\nThe AUC for repeated physician's risk assessment improved by 2% to 5% for all\ncomplications with the exception of thirty-day mortality. Physicians' risk\nassessment for acute kidney injury and intensive care unit admission longer\nthan 48 hours significantly improved after knowledge exchange, resulting in net\nreclassification improvement of 12.4% and 16%, respectively. Conclusions: The\nvalidated MySurgeryRisk algorithm predicted postoperative complications with\nequal or higher accuracy than pilot cohort of physicians using available\nclinical preoperative data. The interaction with algorithm significantly\nimproved physicians' risk assessment.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 22:13:34 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Brennan", "Meghan", ""], ["Puri", "Sahil", ""], ["Ozrazgat-Baslanti", "Tezcan", ""], ["Bhat", "Rajendra", ""], ["Feng", "Zheng", ""], ["Momcilovic", "Petar", ""], ["Li", "Xiaolin", ""], ["Wang", "Daisy Zhe", ""], ["Bihorac", "Azra", ""]]}, {"id": "1804.03261", "submitter": "Carolina Nobre", "authors": "Carolina Nobre, Marc Streit, Alexander Lex", "title": "Juniper: A Tree+Table Approach to Multivariate Graph Visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Analyzing large, multivariate graphs is an important problem in many domains,\nyet such graphs are challenging to visualize. In this paper, we introduce a\nnovel, scalable, tree+table multivariate graph visualization technique, which\nmakes many tasks related to multivariate graph analysis easier to achieve. The\ncore principle we follow is to selectively query for nodes or subgraphs of\ninterest and visualize these subgraphs as a spanning tree of the graph. The\ntree is laid out linearly, which enables us to juxtapose the nodes with a table\nvisualization where diverse attributes can be shown. We also use this table as\nan adjacency matrix, so that the resulting technique is a hybrid\nnode-link/adjacency matrix technique. We implement this concept in Juniper and\ncomplement it with a set of interaction techniques that enable analysts to\ndynamically grow, restructure, and aggregate the tree, as well as change the\nlayout or show paths between nodes. We demonstrate the utility of our tool in\nusage scenarios for different multivariate networks: a bipartite network of\nscholars, papers, and citation metrics and a multitype network of story\ncharacters, places, books, etc.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 22:21:50 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 23:23:17 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Nobre", "Carolina", ""], ["Streit", "Marc", ""], ["Lex", "Alexander", ""]]}, {"id": "1804.03263", "submitter": "Yen-Chia Hsu", "authors": "Yen-Chia Hsu, Jennifer Cross, Paul Dille, Illah Nourbakhsh, Leann\n  Leiter, Ryan Grode", "title": "Visualization Tool for Environmental Sensing and Public Health Data", "comments": "Accepted by 2018 ACM Conference Companion Publication on Designing\n  Interactive Systems (DIS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assist residents affected by oil and gas development, public health\nprofessionals in a non-profit organization have collected community data,\nincluding symptoms, air quality, and personal stories. However, the\norganization was unable to aggregate and visualize these data computationally.\nWe present the Environmental Health Channel, an interactive web-based tool for\nvisualizing environmental sensing and public health data. This tool enables\ndiscussing and disseminating scientific evidence to reveal local environmental\nand health impacts of industrial activities.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 22:31:06 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Hsu", "Yen-Chia", ""], ["Cross", "Jennifer", ""], ["Dille", "Paul", ""], ["Nourbakhsh", "Illah", ""], ["Leiter", "Leann", ""], ["Grode", "Ryan", ""]]}, {"id": "1804.03293", "submitter": "Yen-Chia Hsu", "authors": "Yen-Chia Hsu, Paul Dille, Jennifer Cross, Beatrice Dias, Randy\n  Sargent, Illah Nourbakhsh", "title": "Community-Empowered Air Quality Monitoring System", "comments": "Accepted by 2017 ACM Conference on Human Factors in Computing Systems\n  (CHI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing information technology to democratize scientific knowledge and\nsupport citizen empowerment is a challenging task. In our case, a local\ncommunity suffered from air pollution caused by industrial activity. The\nresidents lacked the technological fluency to gather and curate diverse\nscientific data to advocate for regulatory change. We collaborated with the\ncommunity in developing an air quality monitoring system which integrated\nheterogeneous data over a large spatial and temporal scale. The system afforded\nstrong scientific evidence by using animated smoke images, air quality data,\ncrowdsourced smell reports, and wind data. In our evaluation, we report\npatterns of sharing smoke images among stakeholders. Our survey study shows\nthat the scientific knowledge provided by the system encourages agonistic\ndiscussions with regulators, empowers the community to support policy making,\nand rebalances the power relationship between stakeholders.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 01:12:51 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Hsu", "Yen-Chia", ""], ["Dille", "Paul", ""], ["Cross", "Jennifer", ""], ["Dias", "Beatrice", ""], ["Sargent", "Randy", ""], ["Nourbakhsh", "Illah", ""]]}, {"id": "1804.03307", "submitter": "Yen-Chia Hsu", "authors": "Yen-Chia Hsu, Paul Dille, Randy Sargent, Christopher Bartley, Illah\n  Nourbakhsh", "title": "A Web-based Large-scale Timelapse Editor for Creating and Sharing Guided\n  Video Tours and Interactive Slideshows", "comments": "Accepted by 2015 IEEE Information Visualization Posters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists, journalists, and photographers have used advanced camera\ntechnology to capture extremely high-resolution timelapse and developed\ninformation visualization tools for data exploration and analysis. However, it\ntakes a great deal of effort for professionals to form and tell stories after\nexploring data, since these tools usually provide little aids in creating\nvisual elements. We present a web-based timelapse editor to support the\ncreation of guided video tours and interactive slideshows from a collection of\nlarge-scale spatial and temporal images. Professionals can embed these two\nvisual elements into web pages in conjunction with various forms of digital\nmedia to tell multimodal and interactive stories.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 01:44:33 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Hsu", "Yen-Chia", ""], ["Dille", "Paul", ""], ["Sargent", "Randy", ""], ["Bartley", "Christopher", ""], ["Nourbakhsh", "Illah", ""]]}, {"id": "1804.03506", "submitter": "Ch. Md. Rakin Haider", "authors": "Ch. Md. Rakin Haider and Mohammed Eunus Ali", "title": "Can We Predict the Scenic Beauty of Locations from Geo-tagged Flickr\n  Images?", "comments": "5 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel technique to determine the aesthetic score\nof a location from social metadata of Flickr photos. In particular, we built\nmachine learning classifiers to predict the class of a location where each\nclass corresponds to a set of locations having equal aesthetic rating. These\nmodels are trained on two empirically build datasets containing locations in\ntwo different cities (Rome and Paris) where aesthetic ratings of locations were\ngathered from TripAdvisor.com. In this work we exploit the idea that in a\nlocation with higher aesthetic rating, it is more likely for an user to capture\na photo and other users are more likely to interact with that photo. Our models\nachieved as high as 79.48% accuracy (78.60% precision and 79.27% recall) on\nRome dataset and 73.78% accuracy(75.62% precision and 78.07% recall) on Paris\ndataset. The proposed technique can facilitate urban planning, tour planning\nand recommending aesthetically pleasing paths.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 07:14:04 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Haider", "Ch. Md. Rakin", ""], ["Ali", "Mohammed Eunus", ""]]}, {"id": "1804.03517", "submitter": "Tao Chen", "authors": "Tao Chen, Chen Yuan, Guangyi Liu, Renchang Dai", "title": "Graph based Platform for Electricity Market Study, Education and\n  Training", "comments": "To be published (Accepted) in: Proceedings of the Power and Energy\n  Society General Meeting (PESGM), Portland, OR, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the further development of deregulated electricity market in many other\ncountries around the world, a lot of challenges have been identified for market\ndata management, network topology processing and fast market-clearance\nmechanism design. In this paper, a graph computing framework based on\nTigerGraph database is proposed to solve a security constrained unit commitment\n(SCUC) and security constrained economic dispatch (SCED) problem, with\nparallelized graph power flow (PGPF) and innovative LU decomposition\ntechniques, for electricity market-clearance. It also provides a comprehensive\nvisualization platform to demonstrate the market clearing results vividly, such\nas locational marginal price (LMP), and is able to be utilized for electricity\nmarket operators' education and training purpose.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 19:50:39 GMT"}], "update_date": "2018-04-11", "authors_parsed": [["Chen", "Tao", ""], ["Yuan", "Chen", ""], ["Liu", "Guangyi", ""], ["Dai", "Renchang", ""]]}, {"id": "1804.03994", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Kousuke Tanabe", "title": "An Estimation of Favorite Value in Emotion Generating Calculation by\n  Fuzzy Petri Net", "comments": "6 pages, 7 figures, Proc. of IEEE 6th International Workshop on\n  Computational Intelligence and Applications (IWCIA2013). arXiv admin note:\n  substantial text overlap with arXiv:1804.02657; text overlap with\n  arXiv:1804.02813", "journal-ref": null, "doi": "10.1109/IWCIA.2013.6624777", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion Generating Calculations (EGC) method based on the Emotion Eliciting\nCondition Theory can decide whether an event arouses pleasure or not and\nquantify the degree under the event. An event in the form of Case Frame\nrepresentation is classified into 12 types of calculations. However, the weak\npoint in EGC is Favorite Value (FV) as the personal taste information. In order\nto improve the problem, this paper challenges to establish a learning method to\nlearn speaker's taste information from dialog. Especially, the learning method\nemploys Fuzzy Petri Net to find an appropriate FV to a word which has the\nunknown FV. This paper discusses the effective learning method to improve a\nweak point of EGC when a missing value of FV exists.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 02:43:35 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Ichimura", "Takumi", ""], ["Tanabe", "Kousuke", ""]]}, {"id": "1804.03997", "submitter": "Dominik Sch\\\"urmann", "authors": "Arne Br\\\"usch, Ngu Nguyen, Dominik Sch\\\"urmann, Stephan Sigg and Lars\n  Wolf", "title": "Security Properties of Gait for Mobile Device Pairing", "comments": null, "journal-ref": "IEEE Transactions on Mobile Computing (2019)", "doi": "10.1109/TMC.2019.2897933", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gait has been proposed as a feature for mobile device pairing across\narbitrary positions on the human body. Results indicate that the correlation in\ngait-based features across different body locations is sufficient to establish\nsecure device pairing. However, the population size of the studies is limited\nand powerful attackers with e.g. capability of video recording are not\nconsidered. We present a concise discussion of security properties of\ngait-based pairing schemes including a discussion of popular quantization\nschemes, classification and analysis of attack surfaces, discussion of\nstatistical properties of generated sequences, an entropy analysis, as well as\npossible threats and security weaknesses of gait-based pairing systems. For one\nof the schemes considered, we present modifications to fix an identified\nsecurity flaw. As a general limitation of gait-based authentication or pairing\nsystems, we further demonstrate that an adversary with video support can create\nkey sequences that are sufficiently close to on-body generated acceleration\nsequences to breach gait-based security mechanisms.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 14:12:18 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 17:19:20 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2018 21:57:42 GMT"}, {"version": "v4", "created": "Mon, 11 Feb 2019 09:56:13 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Br\u00fcsch", "Arne", ""], ["Nguyen", "Ngu", ""], ["Sch\u00fcrmann", "Dominik", ""], ["Sigg", "Stephan", ""], ["Wolf", "Lars", ""]]}, {"id": "1804.04053", "submitter": "Egor Lakomkin", "authors": "Egor Lakomkin, Mohammad Ali Zamani, Cornelius Weber, Sven Magg, Stefan\n  Wermter", "title": "EmoRL: Continuous Acoustic Emotion Classification using Deep\n  Reinforcement Learning", "comments": "Accepted to the IEEE International Conference on Robotics and\n  Automation (ICRA'18), Brisbane, Australia, May 21-25, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustically expressed emotions can make communication with a robot more\nefficient. Detecting emotions like anger could provide a clue for the robot\nindicating unsafe/undesired situations. Recently, several deep neural\nnetwork-based models have been proposed which establish new state-of-the-art\nresults in affective state evaluation. These models typically start processing\nat the end of each utterance, which not only requires a mechanism to detect the\nend of an utterance but also makes it difficult to use them in a real-time\ncommunication scenario, e.g. human-robot interaction. We propose the EmoRL\nmodel that triggers an emotion classification as soon as it gains enough\nconfidence while listening to a person speaking. As a result, we minimize the\nneed for segmenting the audio signal for classification and achieve lower\nlatency as the audio signal is processed incrementally. The method is\ncompetitive with the accuracy of a strong baseline model, while allowing much\nearlier prediction.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 09:21:11 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Lakomkin", "Egor", ""], ["Zamani", "Mohammad Ali", ""], ["Weber", "Cornelius", ""], ["Magg", "Sven", ""], ["Wermter", "Stefan", ""]]}, {"id": "1804.04111", "submitter": "Yongbin Sun", "authors": "Jonathan Dyssel Stets, Yongbin Sun, Wiley Corning, Scott Greenwald", "title": "Visualization and Labeling of Point Clouds in Virtual Reality", "comments": "2 pages, 3 figures", "journal-ref": "SA '17 SIGGRAPH Asia 2017 Posters Article No. 31", "doi": "10.1145/3145690.3145729", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Virtual Reality (VR) application for labeling and handling point\ncloud data sets. A series of room-scale point clouds are recorded as a video\nsequence using a Microsoft Kinect. The data can be played and paused, and\nframes can be skipped just like in a video player. The user can walk around and\ninspect the data while it is playing or paused. Using the tracked hand-held\ncontroller, the user can select and label individual parts of the point cloud.\nThe points are highlighted with a color when they are labeled. With a tracking\nalgorithm, the labeled points can be tracked from frame to frame to ease the\nlabeling process. Our sample data is an RGB point cloud recording of two people\njuggling with pins. Here, the user can select and label, for example, the\njuggler pins as shown in Figure 1. Each juggler pin is labeled with various\ncolors to indicate di erent labels.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:35:26 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Stets", "Jonathan Dyssel", ""], ["Sun", "Yongbin", ""], ["Corning", "Wiley", ""], ["Greenwald", "Scott", ""]]}, {"id": "1804.04118", "submitter": "Eshed Ohn-Bar", "authors": "Eshed Ohn-Bar and Kris Kitani and Chieko Asakawa", "title": "Personalized Dynamics Models for Adaptive Assistive Navigation Systems", "comments": "Oral Presentation in 2nd Conference on Robot Learning (CoRL, 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an assistive system that guides visually impaired users through\nspeech and haptic feedback to their destination. Existing robotic and\nubiquitous navigation technologies (e.g., portable, ground, or wearable\nsystems) often operate in a generic, user-agnostic manner. However, to minimize\nconfusion and navigation errors, our real-world analysis reveals a crucial need\nto adapt the instructional guidance across different end-users with diverse\nmobility skills. To address this practical issue in scalable system design, we\npropose a novel model-based reinforcement learning framework for personalizing\nthe system-user interaction experience. When incrementally adapting the system\nto new users, we propose to use a weighted experts model for addressing\ndata-efficiency limitations in transfer learning with deep models. A real-world\ndataset of navigation by blind users is used to show that the proposed approach\nallows for (1) more accurate long-term human behavior prediction (up to 20\nseconds into the future) through improved reasoning over personal mobility\ncharacteristics, interaction with surrounding obstacles, and the current\nnavigation goal, and (2) quick adaptation at the onset of learning, when data\nis limited.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 17:55:00 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 12:20:33 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Ohn-Bar", "Eshed", ""], ["Kitani", "Kris", ""], ["Asakawa", "Chieko", ""]]}, {"id": "1804.04256", "submitter": "Louis Faust", "authors": "Louis Faust, Priscilla Jim\\'enez, David Hachen, Omar Lizardo, Aaron\n  Striegel, Nitesh V. Chawla", "title": "Long-term Compliance Habits: What Early Data Tells Us", "comments": "Accepted to A Short Workshop on Next Steps Towards Long Term Self\n  Tracking, https://longtermtracking.offis.de/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise in popularity of physical activity trackers provides extensive\nopportunities for research on personal health, however, barriers such as\ncompliance attrition can lead to substantial losses in data. As such, insights\ninto student's compliance habits could support researcher's decisions when\ndesigning long-term studies. In this paper, we examined 392 students on a\ncollege campus currently two and a half years into an ongoing study. We find\nthat compliance data from as early as one month correlated with student's\nlikelihood of dropping out of the study (p < .001) and compliance long-term (p\n< .001). The findings in this paper identify long-term compliance habits and\nthe viability of their early detection.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 23:39:06 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Faust", "Louis", ""], ["Jim\u00e9nez", "Priscilla", ""], ["Hachen", "David", ""], ["Lizardo", "Omar", ""], ["Striegel", "Aaron", ""], ["Chawla", "Nitesh V.", ""]]}, {"id": "1804.04290", "submitter": "Yuling Li", "authors": "Yuling Li, Kun Liu, Wei He, Yixin Yin, Rolf Johansson, Kai Zhang", "title": "Bilateral Teleoperation of Multiple Robots under Scheduling\n  Communication", "comments": "13 pages, 12 figures, 4 tables, submitted to IEEE Transactions on\n  Control Systems Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.HC math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, bilateral teleoperation of multiple slaves coupled to a single\nmaster under scheduling communication is investigated. The sampled-data\ntransmission between the master and the multiple slaves is fulfilled over a\ndelayed communication network, and at each sampling instant, only one slave is\nallowed to transmit its current information to the master side according to\nsome scheduling protocols. To achieve the master-slave synchronization,\nRound-Robin scheduling protocol and Try-Once-Discard scheduling protocol are\nemployed, respectively. By designing a scheduling-communication-based\ncontroller, some sufficient stability criteria related to the controller gain\nmatrices, sampling intervals, and communication delays are obtained for the\nclosed-loop teleoperation system under Round-Robin and Try-Once-Discard\nscheduling protocols, respectively. Finally, simulation studies are given to\nvalidate the effectiveness of the proposed results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 02:32:33 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Li", "Yuling", ""], ["Liu", "Kun", ""], ["He", "Wei", ""], ["Yin", "Yixin", ""], ["Johansson", "Rolf", ""], ["Zhang", "Kai", ""]]}, {"id": "1804.04361", "submitter": "Emmanouil Tsardoulias", "authors": "Panagiotis Doxopoulos, Konstantinos L. Panayiotou, Emmanouil G.\n  Tsardoulias, Andreas L. Symeonidis", "title": "Creating an extrovert robotic assistant via IoT networking devices", "comments": "Accepted in ICCR17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The communication and collaboration of Cyber-Physical Systems, including\nmachines and robots, among themselves and with humans, is expected to attract\nresearchers' interest for the years to come. A key element of the new\nrevolution is the Internet of Things (IoT). IoT infrastructures enable\ncommunication between different connected devices using internet protocols. The\nintegration of robots in an IoT platform can improve robot capabilities by\nproviding access to other devices and resources. In this paper we present an\nIoT-enabled application including a NAO robot which can communicate through an\nIoT platform with a reflex measurement system and a hardware node that provides\nrobotics-oriented services in the form of RESTful web services. An activity\nreminder application is also included, illustrating the extension capabilities\nof the system.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 07:46:08 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Doxopoulos", "Panagiotis", ""], ["Panayiotou", "Konstantinos L.", ""], ["Tsardoulias", "Emmanouil G.", ""], ["Symeonidis", "Andreas L.", ""]]}, {"id": "1804.04538", "submitter": "Priya  Ranjan", "authors": "Anju Mishra, Shanu Sharma, Sanjay Kumar, Priya Ranjan, and Amit\n  Ujlayan", "title": "Automated Classification of Hand-grip action on Objects using Machine\n  Learning", "comments": "This is a report on an ongoing project", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain computer interface is the current area of research to provide\nassistance to disabled persons. To cope up with the growing needs of BCI\napplications, this paper presents an automated classification scheme for\nhandgrip actions on objects by using Electroencephalography (EEG) data. The\npresented approach focuses on investigation of classifying correct and\nincorrect handgrip responses for objects by using EEG recorded patterns. The\nmethod starts with preprocessing of data, followed by extraction of relevant\nfeatures from the epoch data in the form of discrete wavelet transform (DWT),\nand entropy measures. After computing feature vectors, artificial neural\nnetwork classifiers used to classify the patterns into correct and incorrect\nhandgrips on different objects. The proposed method was tested on real dataset,\nwhich contains EEG recordings from 14 persons. The results showed that the\nproposed approach is effective and may be useful to develop a variety of BCI\nbased devices to control hand movements.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 11:51:43 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Mishra", "Anju", ""], ["Sharma", "Shanu", ""], ["Kumar", "Sanjay", ""], ["Ranjan", "Priya", ""], ["Ujlayan", "Amit", ""]]}, {"id": "1804.04833", "submitter": "Andr\\'es Lucero", "authors": "Andr\\'es Lucero", "title": "Living Without a Mobile Phone: An Autoethnography", "comments": "12 pages", "journal-ref": null, "doi": "10.1145/3196709.3196731", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an autoethnography of my experiences living without a\nmobile phone. What started as an experiment motivated by a personal need to\nreduce stress, has resulted in two voluntary mobile phone breaks spread over\nnine years (i.e., 2002-2008 and 2014-2017). Conducting this autoethnography is\nthe means to assess if the lack of having a phone has had any real impact in my\nlife. Based on formative and summative analyses, four meaningful units or\nthemes were identified (i.e., social relationships, everyday work, research\ncareer, and location and security), and judged using seven criteria for\nsuccessful ethnography from existing literature. Furthermore, I discuss factors\nthat allow me to make the choice of not having a mobile phone, as well as the\nrelevance that the lessons gained from not having a mobile phone have on the\nlives of people who are involuntarily disconnected from communication\ninfrastructures.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 08:31:13 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Lucero", "Andr\u00e9s", ""]]}, {"id": "1804.04855", "submitter": "Gabriela Villalobos-Z\\'u\\~niga", "authors": "Gabriela Villalobos-Z\\'u\\~niga, Mauro Cherubini", "title": "Activity Self-Tracking with Smart Phones: How to Approach Odd\n  Measurements?", "comments": "A Short Workshop on Next Steps Towards Long Term Self Tracking,\n  CHI'18 Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking physical activity reliably is becoming central to many research\nefforts. In the last years specialized hardware has been proposed to measure\nmovement. However, asking study participants to carry additional devices has\ndrawbacks. We focus on using mobile devices as motion sensors. In the paper we\ndetail several issues that we found while using this technique in a\nlongitudinal study involving hundreds of participants for several months. We\nhope to sparkle a lively discussion at the workshop and attract interest in\nthis method from other researchers.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 09:28:18 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Villalobos-Z\u00fa\u00f1iga", "Gabriela", ""], ["Cherubini", "Mauro", ""]]}, {"id": "1804.04868", "submitter": "Andr\\'e Calero Valdez", "authors": "Andr\\'e Calero Valdez, Martina Ziefle", "title": "The Users' Perspective on the Privacy-Utility Trade-offs in Health\n  Recommender Systems", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": "10.1016/j.ijhcs.2018.04.003", "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Privacy is a major good for users of personalized services such as\nrecommender systems. When applied to the field of health informatics, privacy\nconcerns of users may be amplified, but the possible utility of such services\nis also high. Despite availability of technologies such as k-anonymity,\ndifferential privacy, privacy-aware recommendation, and personalized privacy\ntrade-offs, little research has been conducted on the users' willingness to\nshare health data for usage in such systems. In two conjoint-decision studies\n(sample size n=521), we investigate importance and utility of\nprivacy-preserving techniques related to sharing of personal health data for\nk-anonymity and differential privacy. Users were asked to pick a preferred\nsharing scenario depending on the recipient of the data, the benefit of sharing\ndata, the type of data, and the parameterized privacy. Users disagreed with\nsharing data for commercial purposes regarding mental illnesses and with high\nde-anonymization risks but showed little concern when data is used for\nscientific purposes and is related to physical illnesses. Suggestions for\nhealth recommender system development are derived from the findings.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 10:03:09 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Valdez", "Andr\u00e9 Calero", ""], ["Ziefle", "Martina", ""]]}, {"id": "1804.04946", "submitter": "Takumi Ichimura", "authors": "Takumi Ichimura, Issei Tachibana", "title": "Affective Recommendation System for Tourists by Using Emotion Generating\n  Calculations", "comments": "6 pages, 10 figures. arXiv admin note: substantial text overlap with\n  arXiv:1804.02657 and arXiv:1804.03994", "journal-ref": "Proc. of IEEE 7th International Workshop on Computational\n  Intelligence and Applications (IWCIA2014)", "doi": "10.1109/IWCIA.2014.6987727", "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emotion orientated intelligent interface consists of Emotion Generating\nCalculations (EGC) and Mental State Transition Network (MSTN). We have\ndeveloped the Android EGC application software which the agent works to\nevaluate the feelings in the conversation. In this paper, we develop the\ntourist information system which can estimate the user's feelings at the\nsightseeing spot. The system can recommend the sightseeing spot and the local\nfood corresponded to the user's feeling. The system calculates the\nrecommendation list by the estimate function which consists of Google search\nresults, the important degree of a term at the sightseeing website, and the the\naroused emotion by EGC. In order to show the effectiveness, this paper\ndescribes the experimental results for some situations during Hiroshima\nsightseeing.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 04:55:27 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Ichimura", "Takumi", ""], ["Tachibana", "Issei", ""]]}, {"id": "1804.05021", "submitter": "Julien Gori", "authors": "Julien Gori, Olivier Rioul", "title": "A Feedback Information-Theoretic Transmission Scheme (FITTS) for\n  Modeling Trajectory Variability in Aimed Movements", "comments": null, "journal-ref": "Biological Cybernetics (2020), 114(6), 621-641", "doi": "10.1007/s00422-020-00853-7", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectories in human aimed movements are inherently variable. Using the\nconcept of positional variance profiles, such trajectories are shown to be\ndecomposable into two phases: In a first phase, the variance of the limb\nposition over many trajectories increases rapidly; in a second phase, it then\ndecreases steadily. A new theoretical model, where the aiming task is seen as a\nShannon-like communication problem, is developed to describe the second phase:\nInformation is transmitted from a source (determined by the position at the end\nof the first phase), to a destination (the movement's end-point) over a channel\nperturbed by Gaussian noise, with the presence of a noiseless feedback link.\nInformation-theoretic considerations show that the positional variance\ndecreases exponentially with a rate equal to the channel capacity C. Two\nexisting datasets for simple pointing tasks are re-analyzed and observations on\nreal data confirm our model. The first phase has constant duration and C is\nfound constant across instructions and task parameters, which thus\ncharacterizes the participant's performance. Our model provides a clear\nunderstanding of the speed-accuracy tradeoff in aimed movements: Since the\nparticipant's capacity is fixed, a higher prescribed accuracy necessarily\nrequires a longer second phase resulting in an increased overall movement time.\nThe well-known Fitts' law is also recovered using this approach.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 16:45:54 GMT"}, {"version": "v2", "created": "Sat, 15 Sep 2018 15:41:57 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 09:03:54 GMT"}, {"version": "v4", "created": "Sun, 13 Dec 2020 16:23:13 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gori", "Julien", ""], ["Rioul", "Olivier", ""]]}, {"id": "1804.05560", "submitter": "Naman Goel", "authors": "Naman Goel, Boi Faltings", "title": "Deep Bayesian Trust : A Dominant and Fair Incentive Mechanism for Crowd", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important class of game-theoretic incentive mechanisms for eliciting\neffort from a crowd are the peer based mechanisms, in which workers are paid by\nmatching their answers with one another. The other classic mechanism is to have\nthe workers solve some gold standard tasks and pay them according to their\naccuracy on gold tasks. This mechanism ensures stronger incentive compatibility\nthan the peer based mechanisms but assigning gold tasks to all workers becomes\ninefficient at large scale. We propose a novel mechanism that assigns gold\ntasks to only a few workers and exploits transitivity to derive accuracy of the\nrest of the workers from their peers' accuracy. We show that the resulting\nmechanism ensures a dominant notion of incentive compatibility and fairness.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 09:04:11 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 22:26:16 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Goel", "Naman", ""], ["Faltings", "Boi", ""]]}, {"id": "1804.05788", "submitter": "Samarth Tripathi", "authors": "Samarth Tripathi, Sarthak Tripathi and Homayoon Beigi", "title": "Multi-Modal Emotion recognition on IEMOCAP Dataset using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion recognition has become an important field of research in Human\nComputer Interactions as we improve upon the techniques for modelling the\nvarious aspects of behaviour. With the advancement of technology our\nunderstanding of emotions are advancing, there is a growing need for automatic\nemotion recognition systems. One of the directions the research is heading is\nthe use of Neural Networks which are adept at estimating complex functions that\ndepend on a large number and diverse source of input data. In this paper we\nattempt to exploit this effectiveness of Neural networks to enable us to\nperform multimodal Emotion recognition on IEMOCAP dataset using data from\nSpeech, Text, and Motion capture data from face expressions, rotation and hand\nmovements. Prior research has concentrated on Emotion detection from Speech on\nthe IEMOCAP dataset, but our approach is the first that uses the multiple modes\nof data offered by IEMOCAP for a more robust and accurate emotion detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 16:58:37 GMT"}, {"version": "v2", "created": "Mon, 12 Nov 2018 08:46:57 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 20:10:26 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Tripathi", "Samarth", ""], ["Tripathi", "Sarthak", ""], ["Beigi", "Homayoon", ""]]}, {"id": "1804.05821", "submitter": "Samantha Krening", "authors": "Samantha Krening", "title": "Newtonian Action Advice: Integrating Human Verbal Instruction with\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A goal of Interactive Machine Learning (IML) is to enable people without\nspecialized training to teach agents how to perform tasks. Many of the existing\nmachine learning algorithms that learn from human instructions are evaluated\nusing simulated feedback and focus on how quickly the agent learns. While this\nis valuable information, it ignores important aspects of the human-agent\ninteraction such as frustration. In this paper, we present the Newtonian Action\nAdvice agent, a new method of incorporating human verbal action advice with\nReinforcement Learning (RL) in a way that improves the human-agent interaction.\nIn addition to simulations, we validated the Newtonian Action Advice algorithm\nby conducting a human-subject experiment. The results show that Newtonian\nAction Advice can perform better than Policy Shaping, a state-of-the-art IML\nalgorithm, both in terms of RL metrics like cumulative reward and human factors\nmetrics like frustration.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:45:32 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Krening", "Samantha", ""]]}, {"id": "1804.06339", "submitter": "Atish Pawar", "authors": "Atish Pawar, Sahib Budhiraja, Daniel Kivi, Vijay Mago", "title": "Are we on the same learning curve: Visualization of Semantic Similarity\n  of Course Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The course description provided by instructors is an important piece of\ninformation as it defines what is expected from the instructor and what he/she\nis going to deliver during a particular course. One of the key components of a\ncourse description is the Learning Outcomes section. The contents of this\nsection are used by program managers who are tasked to compare and match two\ndifferent courses during the development of Transfer Agreements between\ndifferent institutions. This research introduces the development of visual\ntools for understanding the two different courses and making comparisons. We\ndesigned methods to extract the text from a course description document,\ndeveloped an algorithm to perform semantic analysis, and displayed the results\nin a web interface. We are able to achieve the intermediate results of the\nresearch which includes extracting, analyzing and visualizing the data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 16:07:25 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Pawar", "Atish", ""], ["Budhiraja", "Sahib", ""], ["Kivi", "Daniel", ""], ["Mago", "Vijay", ""]]}, {"id": "1804.06383", "submitter": "Andrew Silva", "authors": "Siddhartha Banerjee, Andrew Silva, Karen Feigh, Sonia Chernova", "title": "Effects of Interruptibility-Aware Robot Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots become increasingly prevalent in human environments, there will\ninevitably be times when a robot needs to interrupt a human to initiate an\ninteraction. Our work introduces the first interruptibility-aware mobile robot\nsystem, and evaluates the effects of interruptibility-awareness on human task\nperformance, robot task performance, and on human interpretation of the robot's\nsocial aptitude. Our results show that our robot is effective at predicting\ninterruptibility at high accuracy, allowing it to interrupt at more appropriate\ntimes. Results of a large-scale user study show that while participants are\nable to maintain task performance even in the presence of interruptions,\ninterruptibility-awareness improves the robot's task performance and improves\nparticipant social perception of the robot.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 17:26:30 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Banerjee", "Siddhartha", ""], ["Silva", "Andrew", ""], ["Feigh", "Karen", ""], ["Chernova", "Sonia", ""]]}, {"id": "1804.06481", "submitter": "Yao Zhou", "authors": "Yao Zhou, Arun Reddy Nelakurthi, Jingrui He", "title": "Unlearn What You Have Learned: Adaptive Crowd Teaching with\n  Exponentially Decayed Memory Learners", "comments": "10 pages, KDD 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing demand for large amount of labeled data, crowdsourcing\nhas been used in many large-scale data mining applications. However, most\nexisting works in crowdsourcing mainly focus on label inference and incentive\ndesign. In this paper, we address a different problem of adaptive crowd\nteaching, which is a sub-area of machine teaching in the context of\ncrowdsourcing. Compared with machines, human beings are extremely good at\nlearning a specific target concept (e.g., classifying the images into given\ncategories) and they can also easily transfer the learned concepts into similar\nlearning tasks. Therefore, a more effective way of utilizing crowdsourcing is\nby supervising the crowd to label in the form of teaching. In order to perform\nthe teaching and expertise estimation simultaneously, we propose an adaptive\nteaching framework named JEDI to construct the personalized optimal teaching\nset for the crowdsourcing workers. In JEDI teaching, the teacher assumes that\neach learner has an exponentially decayed memory. Furthermore, it ensures\ncomprehensiveness in the learning process by carefully balancing teaching\ndiversity and learner's accurate learning in terms of teaching usefulness.\nFinally, we validate the effectiveness and efficacy of JEDI teaching in\ncomparison with the state-of-the-art techniques on multiple data sets with both\nsynthetic learners and real crowdsourcing workers.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 22:02:02 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 19:01:46 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Zhou", "Yao", ""], ["Nelakurthi", "Arun Reddy", ""], ["He", "Jingrui", ""]]}, {"id": "1804.06550", "submitter": "Marcos Baez", "authors": "Svetlana Nikitina, Sara Callaioli, Marcos Baez", "title": "Smart Conversational Agents for Reminiscence", "comments": null, "journal-ref": null, "doi": "10.1145/3195555.3195567", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the requirements and early system design for a\nsmart conversational agent that can assist older adults in the reminiscence\nprocess. The practice of reminiscence has well documented benefits for the\nmental, social and emotional well-being of older adults. However, the\ntechnology support, valuable in many different ways, is still limited in terms\nof need of co-located human presence, data collection capabilities, and ability\nto support sustained engagement, thus missing key opportunities to improve care\npractices, facilitate social interactions, and bring the reminiscence practice\ncloser to those with less opportunities to engage in co-located sessions with a\n(trained) companion. We discuss conversational agents and cognitive services as\nthe platform for building the next generation of reminiscence applications, and\nintroduce the concept application of a smart reminiscence agent.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 04:48:01 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Nikitina", "Svetlana", ""], ["Callaioli", "Sara", ""], ["Baez", "Marcos", ""]]}, {"id": "1804.06948", "submitter": "Boris Ba\\v{c}i\\'c", "authors": "Boris Ba\\v{c}i\\'c", "title": "Towards the next generation of exergames: Flexible and personalised\n  assessment-based identification of tennis swings", "comments": "This unpublished manuscript is accepted by the IJCNN 2018\n  (http://www.ecomp.poli.br/~wcci2018/) and uploaded to ArXiv.org preprint\n  server. The camera ready copy with DOI should be available in IEEE Xplore\n  sometimes after the conference presentation and copyright transfer to IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Current exergaming sensors and inertial systems attached to sports equipment\nor the human body can provide quantitative information about the movement or\nimpact e.g. with the ball. However, the scope of these technologies is not to\nqualitatively assess sports technique at a personalised level, similar to a\ncoach during training or replay analysis. The aim of this paper is to\ndemonstrate a novel approach to automate identification of tennis swings\nexecuted with erroneous technique without recorded ball impact. The presented\nspatiotemporal transformations relying on motion gradient vector flow and\npolynomial regression with RBF classifier, can identify previously unseen\nerroneous swings (84.5-94.6%). The presented solution is able to learn from a\nsmall dataset and capture two subjective swing-technique assessment criteria\nfrom a coach. Personalised and flexible assessment criteria required for\nplayers of diverse skill levels and various coaching scenarios were\ndemonstrated by assigning different labelling criteria for identifying similar\nspatiotemporal patterns of tennis swings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 23:44:01 GMT"}, {"version": "v2", "created": "Thu, 26 Apr 2018 06:55:23 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Ba\u010di\u0107", "Boris", ""]]}, {"id": "1804.07890", "submitter": "Bill Howe", "authors": "Ke Yang, Julia Stoyanovich, Abolfazl Asudeh, Bill Howe, HV Jagadish,\n  Gerome Miklau", "title": "A Nutritional Label for Rankings", "comments": "4 pages, SIGMOD demo, 3 figuress, ACM SIGMOD 2018", "journal-ref": null, "doi": "10.1145/3183713.3193568", "report-no": null, "categories": "cs.CY cs.DB cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Algorithmic decisions often result in scoring and ranking individuals to\ndetermine credit worthiness, qualifications for college admissions and\nemployment, and compatibility as dating partners. While automatic and seemingly\nobjective, ranking algorithms can discriminate against individuals and\nprotected groups, and exhibit low diversity. Furthermore, ranked results are\noften unstable --- small changes in the input data or in the ranking\nmethodology may lead to drastic changes in the output, making the result\nuninformative and easy to manipulate. Similar concerns apply in cases where\nitems other than individuals are ranked, including colleges, academic\ndepartments, or products.\n  In this demonstration we present Ranking Facts, a Web-based application that\ngenerates a \"nutritional label\" for rankings. Ranking Facts is made up of a\ncollection of visual widgets that implement our latest research results on\nfairness, stability, and transparency for rankings, and that communicate\ndetails of the ranking methodology, or of the output, to the end user. We will\nshowcase Ranking Facts on real datasets from different domains, including\ncollege rankings, criminal risk assessment, and financial services.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2018 05:00:58 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Yang", "Ke", ""], ["Stoyanovich", "Julia", ""], ["Asudeh", "Abolfazl", ""], ["Howe", "Bill", ""], ["Jagadish", "HV", ""], ["Miklau", "Gerome", ""]]}, {"id": "1804.08386", "submitter": "Steve Mann", "authors": "Steve Mann, Tom Furness, Yu Yuan, Jay Iorio, and Zixin Wang", "title": "All Reality: Virtual, Augmented, Mixed (X), Mediated (X,Y), and\n  Multimediated Reality", "comments": "14 pages, 20 figures, expanded version of a much shorter paper\n  submitted to ACM Multimedia 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contributions of this paper are: (1) a taxonomy of the \"Realities\"\n(Virtual, Augmented, Mixed, Mediated, etc.), and (2) some new kinds of\n\"reality\" that come from nature itself, i.e. that expand our notion beyond\nsynthetic realities to include also phenomenological realities.\n  VR (Virtual Reality) replaces the real world with a simulated experience\n(virtual world). AR (Augmented Reality) allows a virtual world to be\nexperienced while also experiencing the real world at the same time. Mixed\nReality provides blends that interpolate between real and virtual worlds in\nvarious proportions, along a \"Virtuality\" axis, and extrapolate to an \"X-axis\".\nMediated Reality goes a step further by mixing/blending and also modifying\nreality. This modifying of reality introduces a second axis. Mediated Reality\nis useful as a seeing aid (e.g. modifying reality to make it easier to\nunderstand), and for psychology experiments like Stratton's 1896 upside-down\neyeglasses experiment.\n  We propose Multimediated Reality as a multidimensional multisensory mediated\nreality that includes not just interactive multimedia-based reality for our\nfive senses, but also includes additional senses (like sensory sonar, sensory\nradar, etc.), as well as our human actions/actuators. These extra senses are\nmapped to our human senses using synthetic synesthesia. This allows us to\ndirectly experience real (but otherwise invisible) phenomena, such as wave\npropagation and wave interference patterns, so that we can see radio waves and\nsound waves and how they interact with objects and each other. Multimediated\nreality is multidimensional, multimodal, multisensory, and multiscale. It is\nalso multidisciplinary, in that we must consider not just the user, but also\nhow the technology affects others, e.g. how its physical appearance affects\nsocial situations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 15:40:39 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Mann", "Steve", ""], ["Furness", "Tom", ""], ["Yuan", "Yu", ""], ["Iorio", "Jay", ""], ["Wang", "Zixin", ""]]}, {"id": "1804.08396", "submitter": "Mehdi Mohammadi", "authors": "Mehdi Mohammadi, Ala Al-Fuqaha, Jun-Seok Oh", "title": "Path Planning in Support of Smart Mobility Applications using Generative\n  Adversarial Networks", "comments": "8 pages, submitted to IEEE SmartData-2018 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.NI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes and evaluates the use of Generative Adversarial Networks\n(GANs) for path planning in support of smart mobility applications such as\nindoor and outdoor navigation applications, individualized wayfinding for\npeople with disabilities (e.g., vision impairments, physical disabilities,\netc.), path planning for evacuations, robotic navigations, and path planning\nfor autonomous vehicles. We propose an architecture based on GANs to recommend\naccurate and reliable paths for navigation applications. The proposed system\ncan use crowd-sourced data to learn the trajectories and infer new ones. The\nsystem provides users with generated paths that help them navigate from their\nlocal environment to reach a desired location. As a use case, we experimented\nwith the proposed method in support of a wayfinding application in an indoor\nenvironment. Our experiments assert that the generated paths are correct and\nreliable. The accuracy of the classification task for the generated paths is up\nto 99% and the quality of the generated paths has a mean opinion score of 89%.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 13:21:31 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Mohammadi", "Mehdi", ""], ["Al-Fuqaha", "Ala", ""], ["Oh", "Jun-Seok", ""]]}, {"id": "1804.08458", "submitter": "Justin Weisz", "authors": "Saad Ismail, Justin G. Manweiler, Justin D. Weisz", "title": "CardKit: A Card-Based Programming Framework for Drones", "comments": null, "journal-ref": null, "doi": null, "report-no": "RC25674", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drones are being used in many industries for a variety of applications,\nincluding inspecting bridges, surveying farm land, and delivering cargo.\nAutomating these kinds of scenarios requires more than following a sequence of\nGPS waypoints; they require integrating on-device hardware with real-time\nanalysis to provide feedback and control to the drone. Currently, implementing\nthese kinds of advanced scenarios is a complex task, requiring skilled software\nengineers programming with drone APIs. We envision an alternate model to enable\ndrone operators to orchestrate advanced behaviors using a card-based approach.\nWe describe the design of our card-based programming model, position it\nrelative to other visual programming metaphors, share results from our paper\nprototype user study, and discuss our learnings from its implementation.\nResults suggest that a wide range of scenarios can be implemented with moderate\nmental effort and learning, balanced by intuitiveness and engagement.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 14:16:04 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Ismail", "Saad", ""], ["Manweiler", "Justin G.", ""], ["Weisz", "Justin D.", ""]]}, {"id": "1804.08732", "submitter": "Alina Striner", "authors": "Alina Striner, Jennifer Preece", "title": "StreamBED: Training Citizen Scientists to Make Qualitative Judgments\n  Using Embodied Virtual Reality Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental citizen science frequently relies on experience-based\nassessment, however volunteers are not trained to make qualitative judgments.\nEmbodied learning in virtual reality (VR) has been explored as a way to train\nbehavior, but has not fully been considered as a way to train judgment. This\npreliminary research explores embodied learning in VR through the design,\nevaluation, and redesign of StreamBED, a water quality monitoring training\nenvironment that teaches volunteers to make qualitative assessments by\nexploring, assessing and comparing virtual watersheds.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 20:57:47 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Striner", "Alina", ""], ["Preece", "Jennifer", ""]]}, {"id": "1804.08737", "submitter": "Alina Striner", "authors": "Alina Striner, Lennart E. Nacke, Elizabeth Bonsignore, Matthew Louis\n  Mauriello, Zachary O. Toups, Carlea Holl-Jensen, Heather Kelley", "title": "\"It was Colonel Mustard in the Study with the Candlestick\": Using\n  Artifacts to Create An Alternate Reality Game-The Unworkshop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Workshops are used for academic social networking, but connections can be\nsuperficial and result in few enduring collaborations. This unworkshop offers a\nnovel interactive format to create deep connections, peer- learning, and\nproduces a technology-enhanced experience. Participants will generate\ninteractive technological artifacts before the unworkshop, which will be used\ntogether and orchestrated at the unworkshop to engage all participants in an\nalternate reality game set in local places at the conference.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 21:08:10 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Striner", "Alina", ""], ["Nacke", "Lennart E.", ""], ["Bonsignore", "Elizabeth", ""], ["Mauriello", "Matthew Louis", ""], ["Toups", "Zachary O.", ""], ["Holl-Jensen", "Carlea", ""], ["Kelley", "Heather", ""]]}, {"id": "1804.08895", "submitter": "Andreas Tarnowsky", "authors": "Andreas Tarnowsky, Jan Jamaszyk, Daniel Brandes, Franz-Erich Wolter", "title": "Open Tactile - An open, modular hardware system for controlling tactile\n  displays", "comments": "17 pages, 10 figures. This paper has not been submitted to any\n  journal yet. It is intended to supplement the information given at the\n  opentactile.org project pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tactile displays have a wide potential field of applications, ranging from\nenhancing Virtual-Reality scenarios up to aiding telesurgery as well as in\nfundamental psychological and neurophysiological research. In this paper, we\ndescribe an open source hardware and software architecture that is designed to\ndrive a variety of different tactile displays. For demonstration purposes, a\ntactile computer mouse featuring a simple tactile display, based on lateral\npiezoelectric (PZT) actuators, is presented. Even though we will focus on\ndriving mechanical actuators in this paper, the system can be extended to\ndifferent working principles. The suggested architecture is supplied with a\ncustom, easy to use, software stack allowing a simple definition of tactile\nscenarios as well as user studies while being especially tailored to\nnon-computer scientists. By releasing the OpenTactile system under MIT license\nwe hope to ease the burden of controlling tactile displays as well as designing\nand reproducing the related experiments.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 08:33:59 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Tarnowsky", "Andreas", ""], ["Jamaszyk", "Jan", ""], ["Brandes", "Daniel", ""], ["Wolter", "Franz-Erich", ""]]}, {"id": "1804.09044", "submitter": "Vladimir Ivanov", "authors": "Joseph Alexander Brown, Vladimir Ivanov, Alan Rogers, Giancarlo Succi,\n  Alexander Tormasov and Jooyong Yi", "title": "Toward a Better Understanding of How to Develop Software Under Stress -\n  Drafting the Lines for Future Research", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The software is often produced under significant time constraints. Our idea\nis to understand the effects of various software development practices on the\nperformance of developers working in stressful environments, and identify the\nbest operating conditions for software developed under stressful conditions\ncollecting data through questionnaires, non-invasive software measurement tools\nthat can collect measurable data about software engineers and the software they\ndevelop, without intervening their activities, and biophysical sensors and then\ntry to recreated also in different processes or key development practices such\nconditions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 13:48:38 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Brown", "Joseph Alexander", ""], ["Ivanov", "Vladimir", ""], ["Rogers", "Alan", ""], ["Succi", "Giancarlo", ""], ["Tormasov", "Alexander", ""], ["Yi", "Jooyong", ""]]}, {"id": "1804.09154", "submitter": "Pier Luca Lanzi", "authors": "Edoardo Giacomello and Pier Luca Lanzi and Daniele Loiacono", "title": "DOOM Level Generation using Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We applied Generative Adversarial Networks (GANs) to learn a model of DOOM\nlevels from human-designed content. Initially, we analysed the levels and\nextracted several topological features. Then, for each level, we extracted a\nset of images identifying the occupied area, the height map, the walls, and the\nposition of game objects. We trained two GANs: one using plain level images,\none using both the images and some of the features extracted during the\npreliminary analysis. We used the two networks to generate new levels and\ncompared the results to assess whether the network trained using also the\ntopological features could generate levels more similar to human-designed ones.\nOur results show that GANs can capture intrinsic structure of DOOM levels and\nappears to be a promising approach to level generation in first person shooter\ngames.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 17:20:52 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Giacomello", "Edoardo", ""], ["Lanzi", "Pier Luca", ""], ["Loiacono", "Daniele", ""]]}, {"id": "1804.09452", "submitter": "Siddharth Siddharth", "authors": "Siddharth Siddharth, Tzyy-Ping Jung, and Terrence J. Sejnowski", "title": "Multi-modal Approach for Affective Computing", "comments": "Published in IEEE 40th International Engineering in Medicine and\n  Biology Conference (EMBC) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Throughout the past decade, many studies have classified human emotions using\nonly a single sensing modality such as face video, electroencephalogram (EEG),\nelectrocardiogram (ECG), galvanic skin response (GSR), etc. The results of\nthese studies are constrained by the limitations of these modalities such as\nthe absence of physiological biomarkers in the face-video analysis, poor\nspatial resolution in EEG, poor temporal resolution of the GSR etc. Scant\nresearch has been conducted to compare the merits of these modalities and\nunderstand how to best use them individually and jointly. Using multi-modal\nAMIGOS dataset, this study compares the performance of human emotion\nclassification using multiple computational approaches applied to face videos\nand various bio-sensing modalities. Using a novel method for compensating\nphysiological baseline we show an increase in the classification accuracy of\nvarious approaches that we use. Finally, we present a multi-modal\nemotion-classification approach in the domain of affective computing research.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 09:45:22 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 23:55:22 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Siddharth", "Siddharth", ""], ["Jung", "Tzyy-Ping", ""], ["Sejnowski", "Terrence J.", ""]]}, {"id": "1804.09552", "submitter": "Kyongsik Yun", "authors": "Kyongsik Yun, Joseph Osborne, Madison Lee, Thomas Lu, Edward Chow", "title": "Automatic speech recognition for launch control center communication\n  using recurrent neural networks with data augmentation and custom language\n  model", "comments": "SPIE 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcribing voice communications in NASA's launch control center is\nimportant for information utilization. However, automatic speech recognition in\nthis environment is particularly challenging due to the lack of training data,\nunfamiliar words in acronyms, multiple different speakers and accents, and\nconversational characteristics of speaking. We used bidirectional deep\nrecurrent neural networks to train and test speech recognition performance. We\nshowed that data augmentation and custom language models can improve speech\nrecognition accuracy. Transcribing communications from the launch control\ncenter will help the machine analyze information and accelerate knowledge\ngeneration.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 10:28:57 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Yun", "Kyongsik", ""], ["Osborne", "Joseph", ""], ["Lee", "Madison", ""], ["Lu", "Thomas", ""], ["Chow", "Edward", ""]]}, {"id": "1804.10201", "submitter": "Anis Davoudi", "authors": "Anis Davoudi, Kumar Rohit Malhotra, Benjamin Shickel, Scott Siegel,\n  Seth Williams, Matthew Ruppert, Emel Bihorac, Tezcan Ozrazgat-Baslanti,\n  Patrick J. Tighe, Azra Bihorac, Parisa Rashidi", "title": "The Intelligent ICU Pilot Study: Using Artificial Intelligence\n  Technology for Autonomous Patient Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, many critical care indices are repetitively assessed and recorded\nby overburdened nurses, e.g. physical function or facial pain expressions of\nnonverbal patients. In addition, many essential information on patients and\ntheir environment are not captured at all, or are captured in a non-granular\nmanner, e.g. sleep disturbance factors such as bright light, loud background\nnoise, or excessive visitations. In this pilot study, we examined the\nfeasibility of using pervasive sensing technology and artificial intelligence\nfor autonomous and granular monitoring of critically ill patients and their\nenvironment in the Intensive Care Unit (ICU). As an exemplar prevalent\ncondition, we also characterized delirious and non-delirious patients and their\nenvironment. We used wearable sensors, light and sound sensors, and a\nhigh-resolution camera to collected data on patients and their environment. We\nanalyzed collected data using deep learning and statistical analysis. Our\nsystem performed face detection, face recognition, facial action unit\ndetection, head pose detection, facial expression recognition, posture\nrecognition, actigraphy analysis, sound pressure and light level detection, and\nvisitation frequency detection. We were able to detect patient's face (Mean\naverage precision (mAP)=0.94), recognize patient's face (mAP=0.80), and their\npostures (F1=0.94). We also found that all facial expressions, 11 activity\nfeatures, visitation frequency during the day, visitation frequency during the\nnight, light levels, and sound pressure levels during the night were\nsignificantly different between delirious and non-delirious patients\n(p-value<0.05). In summary, we showed that granular and autonomous monitoring\nof critically ill patients and their environment is feasible and can be used\nfor characterizing critical care conditions and related environment factors.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 21:24:46 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 18:25:32 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Davoudi", "Anis", ""], ["Malhotra", "Kumar Rohit", ""], ["Shickel", "Benjamin", ""], ["Siegel", "Scott", ""], ["Williams", "Seth", ""], ["Ruppert", "Matthew", ""], ["Bihorac", "Emel", ""], ["Ozrazgat-Baslanti", "Tezcan", ""], ["Tighe", "Patrick J.", ""], ["Bihorac", "Azra", ""], ["Rashidi", "Parisa", ""]]}, {"id": "1804.10202", "submitter": "Hao Fang", "authors": "Hao Fang, Hao Cheng, Maarten Sap, Elizabeth Clark, Ari Holtzman, Yejin\n  Choi, Noah A. Smith, Mari Ostendorf", "title": "Sounding Board: A User-Centric and Content-Driven Social Chatbot", "comments": "5 pages, 3 figures, NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Sounding Board, a social chatbot that won the 2017 Amazon Alexa\nPrize. The system architecture consists of several components including spoken\nlanguage processing, dialogue management, language generation, and content\nmanagement, with emphasis on user-centric and content-driven design. We also\nshare insights gained from large-scale online logs based on 160,000\nconversations with real-world users.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 08:11:16 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Fang", "Hao", ""], ["Cheng", "Hao", ""], ["Sap", "Maarten", ""], ["Clark", "Elizabeth", ""], ["Holtzman", "Ari", ""], ["Choi", "Yejin", ""], ["Smith", "Noah A.", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1804.10367", "submitter": "Araz Taeihagh", "authors": "Hazel Si Min Lim and Araz Taeihagh", "title": "Autonomous Vehicles for Smart and Sustainable Cities: An In-Depth\n  Exploration of Privacy and Cybersecurity Implications", "comments": null, "journal-ref": "Energies 11, no. 5: 1062 (2018)", "doi": "10.3390/en11051062", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amidst rapid urban development, sustainable transportation solutions are\nrequired to meet the increasing demands for mobility whilst mitigating the\npotentially negative social, economic, and environmental impacts. This study\nanalyses autonomous vehicles (AVs) as a potential transportation solution for\nsmart and sustainable development. We identified privacy and cybersecurity\nrisks of AVs as crucial to the development of smart and sustainable cities and\nexamined the steps taken by governments around the world to address these\nrisks. We highlight the literature that supports why AVs are essential for\nsmart and sustainable development. We then identify the aspects of privacy and\ncybersecurity in AVs that are important for smart and sustainable development.\nLastly, we review the efforts taken by federal governments in the US, the UK,\nChina, Australia, Japan, Singapore, South Korea, Germany, France, and the EU,\nand by US state governments to address AV-related privacy and cybersecurity\nrisks in-depth. Overall, the actions taken by governments to address privacy\nrisks are mainly in the form of regulations or voluntary guidelines. To address\ncybersecurity risks, governments have mostly resorted to regulations that are\nnot specific to AVs and are conducting research and fostering research\ncollaborations with the private sector.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 07:29:34 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Lim", "Hazel Si Min", ""], ["Taeihagh", "Araz", ""]]}, {"id": "1804.10381", "submitter": "Shabnam Sadeghi Esfahlani", "authors": "Shabnam Sadeghi Esfahlani, George Wilson", "title": "Development of Rehabilitation System (ReHabgame) through Monte-Carlo\n  Tree Search Algorithm", "comments": "8 pages ,6 figures", "journal-ref": "Computing Conference, London, UK, 2017; 978-1-5090-5443-5;\n  978-1-5090-5444-2", "doi": "10.1109/SAI.2017.8252217", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational Intelligence (CI) in computer games plays an important role\nthat could simulate various aspects of real-life problems. CI in real-time\ndecision-making games can provide a platform for the examination of tree search\nalgorithms. In this paper, we present a rehabilitation serious game (ReHabgame)\nin which the Monte-Carlo Tree Search (MCTS) algorithm is utilized. The game is\ndesigned to combat the physical impairment of post-stroke/brain injury\ncasualties in order to improve upper limb movement. Through the process of\nReHabgame the player chooses paths via upper limb according to his/her movement\nability to reach virtual goal objects. The system adjusts the difficulty level\nof the game based on the player's quality of activity through MCTS. It learns\nfrom the movements made by a player and generates further subsequent objects\nfor collection. The system collects orientation, muscle and joint activity data\nand utilizes them to make decisions. Players data are collected through Kinect\nXbox One and Myo Armband. The results show the effectiveness of the MCTS in the\nReHabgame that progresses from highly achievable paths to the less achievable\nones, thus configuring and personalizing the rehabilitation process.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 08:15:42 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Esfahlani", "Shabnam Sadeghi", ""], ["Wilson", "George", ""]]}, {"id": "1804.10392", "submitter": "Shabnam Sadeghi Esfahlani", "authors": "Shabnam Sadeghi Esfahlani, Silvia Cirstea, Alireza Sanaei, George\n  Wilson", "title": "An adaptive self-organizing fuzzy logic controller in a serious game for\n  motor impairment rehabilitation", "comments": "8 pages, 7 figures, 17096690", "journal-ref": "19-21 June 2017; 978-1-5090-1412-5; 978-1-5090-1413-2; 2163-5145", "doi": "10.1109/ISIE.2017.8001435", "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rehabilitation robotics combined with video game technology provides a means\nof assisting in the rehabilitation of patients with neuromuscular disorders by\nperforming various facilitation movements. The current work presents ReHabGame,\na serious game using a fusion of implemented technologies that can be easily\nused by patients and therapists to assess and enhance sensorimotor performance\nand also increase the activities in the daily lives of patients. The game\nallows a player to control avatar movements through a Kinect Xbox, Myo armband\nand rudder foot pedal, and involves a series of reach-grasp-collect tasks whose\ndifficulty levels are learnt by a fuzzy interface. The orientation, angular\nvelocity, head and spine tilts and other data generated by the player are\nmonitored and saved, whilst the task completion is calculated by solving an\ninverse kinematics algorithm which orientates the upper limb joints of the\navatar. The different values in upper body quantities of movement provide fuzzy\ninput from which crisp output is determined and used to generate an appropriate\nsubsequent rehabilitation game level. The system can thus provide personalised,\nautonomously-learnt rehabilitation programmes for patients with neuromuscular\ndisorders with superior predictions to guide the development of improved\nclinical protocols compared to traditional theraputic activities.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 08:39:43 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Esfahlani", "Shabnam Sadeghi", ""], ["Cirstea", "Silvia", ""], ["Sanaei", "Alireza", ""], ["Wilson", "George", ""]]}, {"id": "1804.10521", "submitter": "Bradly Alicea", "authors": "Bradly Alicea", "title": "An Integrative Introduction to Human Augmentation Science", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human Augmentation (HA) spans several technical fields and methodological\napproaches, including Experimental Psychology, Human-Computer Interaction,\nPsychophysiology, and Artificial Intelligence. Augmentation involves various\nstrategies for optimizing and controlling cognitive states, which requires an\nunderstanding of biological plasticity, dynamic cognitive processes, and models\nof adaptive systems. As an instructive lesson, we will explore a few HA-related\nconcepts and outstanding issues. Next, we focus on inducing and controlling HA\nusing experimental methods by introducing three techniques for HA\nimplementation: learning augmentation, augmentation using physical media, and\nextended phenotype modeling. To conclude, we will review integrative approaches\nto augmentation, which transcend specific functions.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 14:29:27 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Alicea", "Bradly", ""]]}, {"id": "1804.10651", "submitter": "Maxwell Scale Uwadia Osagie", "authors": "Osagie Scale Uwadia Maxwell, K.O. Obahiagbon, Osagie Joy Amenze and\n  John-Otumu M. A", "title": "5PEN TECHNOLOGY: A New Dawn in Homogeneous and Heterogeneous Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research work is a pair review into the conceptual frame work and\ninnovation into Pen-style Personal Network Gadget Package (P-ISM) as inevitable\ntool to easy, fast and convenient access to the internet. Computing activities\nhave increased the degree of people using personal computers (PCs), complicated\npackages and all form of social media applications (Apps.) have emerged within\nthis short period. Meeting these trends (day to day activities) in more\nconvenient form has led to the modern sophisticated garget such as Pen-Style\nNetwork Gadget Package (P-ISM) prototype. The growth in internet affects our\nlives in much better way than we know and its sustainability made 5 pen\ntechnology innovations a salt after.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 16:29:37 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Maxwell", "Osagie Scale Uwadia", ""], ["Obahiagbon", "K. O.", ""], ["Amenze", "Osagie Joy", ""], ["A", "John-Otumu M.", ""]]}, {"id": "1804.10685", "submitter": "Alina Striner", "authors": "Alina Striner", "title": "Yes and...? Using Improv to Design for Narrative in Lights Out", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed reality experiences often require detailed narrative that can be used\nto craft physical and virtual design components. This work elaborates on a\nmentoring experience at the Carnegie Mellon's ETC to consider how improv games\nmay be used ideate and iterate on storytelling experiences.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 19:42:55 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Striner", "Alina", ""]]}, {"id": "1804.10861", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg and Sergej Sizov", "title": "Neuroscientific User Models: The Source of Uncertain User Feedback and\n  Potentials for Improving Recommendation and Personalisation", "comments": "User Noise, Human Uncertainty, Collaborative Filtering, User Models,\n  Bayesian Brain, Probabilistic Population Codes, Cognitive Agency, Neural\n  Coding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research revealed a considerable lack of reliability for user feedback\nwhen interacting with adaptive systems, often denoted as user noise or human\nuncertainty. Moreover, this lack of reliability holds striking impacts for the\nassessment of adaptive systems and personalisation approaches. Whenever\nresearch on this topic is done, there is a very strong system-centric view in\nwhich user variation is something undesirable and should be modelled with the\neye to eliminate. However, the possibilities of extracting additional\ninformation were only insufficiently considered so far.\n  In this contribution we consider the neuroscientific theory of the Bayesian\nbrain in order to develop novel user models with the power of turning the\nvariability of user behaviour into additional information for improving\nrecommendation and personalisation. To this end, we first introduce an adaptive\nmodel in which populations of neurons provide an estimation for a feedback to\nbe submitted. Subsequently, we present various decoder functions with which\nneuronal activity can be translated into quantitative decisions. The interplay\nof cognition model and decoder functions lead to different model-based\nproperties of decision-making. This will help to associate users to different\nclusters on the basis of their individual neural characteristics and thinking\npatterns. By means of user experiments and simulations, we show that this\ninformation can be used to improve the standard collaborative filtering.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 02:17:15 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1804.10938", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias, Panagiotis Tzirakis, Mihalis A. Nicolaou,\n  Athanasios Papaioannou, Guoying Zhao, Bj\\\"orn Schuller, Irene Kotsia,\n  Stefanos Zafeiriou", "title": "Deep Affect Prediction in-the-wild: Aff-Wild Database and Challenge,\n  Deep Architectures, and Beyond", "comments": null, "journal-ref": null, "doi": "10.1007/s11263-019-01158-4", "report-no": null, "categories": "cs.CV cs.AI cs.HC eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic understanding of human affect using visual signals is of great\nimportance in everyday human-machine interactions. Appraising human emotional\nstates, behaviors and reactions displayed in real-world settings, can be\naccomplished using latent continuous dimensions (e.g., the circumplex model of\naffect). Valence (i.e., how positive or negative is an emotion) & arousal\n(i.e., power of the activation of the emotion) constitute popular and effective\naffect representations. Nevertheless, the majority of collected datasets this\nfar, although containing naturalistic emotional states, have been captured in\nhighly controlled recording conditions. In this paper, we introduce the\nAff-Wild benchmark for training and evaluating affect recognition algorithms.\nWe also report on the results of the First Affect-in-the-wild Challenge that\nwas organized in conjunction with CVPR 2017 on the Aff-Wild database and was\nthe first ever challenge on the estimation of valence and arousal in-the-wild.\nFurthermore, we design and extensively train an end-to-end deep neural\narchitecture which performs prediction of continuous emotion dimensions based\non visual cues. The proposed deep learning architecture, AffWildNet, includes\nconvolutional & recurrent neural network layers, exploiting the invariant\nproperties of convolutional features, while also modeling temporal dynamics\nthat arise in human behavior via the recurrent layers. The AffWildNet produced\nstate-of-the-art results on the Aff-Wild Challenge. We then exploit the AffWild\ndatabase for learning features, which can be used as priors for achieving best\nperformances both for dimensional, as well as categorical emotion recognition,\nusing the RECOLA, AFEW-VA and EmotiW datasets, compared to all other methods\ndesigned for the same goal. The database and emotion recognition models are\navailable at http://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 14:18:07 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 01:27:00 GMT"}, {"version": "v3", "created": "Tue, 8 May 2018 09:50:53 GMT"}, {"version": "v4", "created": "Sat, 1 Sep 2018 13:26:39 GMT"}, {"version": "v5", "created": "Fri, 1 Feb 2019 12:39:52 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Tzirakis", "Panagiotis", ""], ["Nicolaou", "Mihalis A.", ""], ["Papaioannou", "Athanasios", ""], ["Zhao", "Guoying", ""], ["Schuller", "Bj\u00f6rn", ""], ["Kotsia", "Irene", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "1804.10999", "submitter": "Brandon Dang", "authors": "Brandon Dang, Martin J. Riedl, Matthew Lease", "title": "But Who Protects the Moderators? The Case of Crowdsourced Image\n  Moderation", "comments": "To be presented at the 6th AAAI Conference on Human Computation and\n  Crowdsourcing (HCOMP 2018) and the 6th ACM Collective Intelligence Conference\n  (CI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though detection systems have been developed to identify obscene content such\nas pornography and violence, artificial intelligence is simply not good enough\nto fully automate this task yet. Due to the need for manual verification,\nsocial media companies may hire internal reviewers, contract specialized\nworkers from third parties, or outsource to online labor markets for the\npurpose of commercial content moderation. These content moderators are often\nfully exposed to extreme content and may suffer lasting psychological and\nemotional damage. In this work, we aim to alleviate this problem by\ninvestigating the following question: How can we reveal the minimum amount of\ninformation to a human reviewer such that an objectionable image can still be\ncorrectly identified? We design and conduct experiments in which blurred\ngraphic and non-graphic images are filtered by human moderators on Amazon\nMechanical Turk (AMT). We observe how obfuscation affects the moderation\nexperience with respect to image classification accuracy, interface usability,\nand worker emotional well-being.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 23:00:54 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 20:56:09 GMT"}, {"version": "v3", "created": "Sat, 9 Jun 2018 21:39:04 GMT"}, {"version": "v4", "created": "Sun, 5 Jan 2020 04:07:35 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Dang", "Brandon", ""], ["Riedl", "Martin J.", ""], ["Lease", "Matthew", ""]]}, {"id": "1804.11177", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Jiechao Xiong, Xiaochun Cao, Qingming Huang, and Yuan Yao", "title": "From Social to Individuals: a Parsimonious Path of Multi-level Models\n  for Crowdsourced Preference Aggregation", "comments": "Accepted by IEEE Transactions on Pattern Analysis and Machine\n  Intelligence as a regular paper. arXiv admin note: substantial text overlap\n  with arXiv:1607.03401", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowdsourced preference aggregation, it is often assumed that all the\nannotators are subject to a common preference or social utility function which\ngenerates their comparison behaviors in experiments. However, in reality\nannotators are subject to variations due to multi-criteria, abnormal, or a\nmixture of such behaviors. In this paper, we propose a parsimonious\nmixed-effects model, which takes into account both the fixed effect that the\nmajority of annotators follows a common linear utility model, and the random\neffect that some annotators might deviate from the common significantly and\nexhibit strongly personalized preferences. The key algorithm in this paper\nestablishes a dynamic path from the social utility to individual variations,\nwith different levels of sparsity on personalization. The algorithm is based on\nthe Linearized Bregman Iterations, which leads to easy parallel implementations\nto meet the need of large-scale data analysis. In this unified framework, three\nkinds of random utility models are presented, including the basic linear model\nwith L2 loss, Bradley-Terry model, and Thurstone-Mosteller model. The validity\nof these multi-level models are supported by experiments with both simulated\nand real-world datasets, which shows that the parsimonious multi-level models\nexhibit improvements in both interpretability and predictive precision compared\nwith traditional HodgeRank.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 03:56:22 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Cao", "Xiaochun", ""], ["Huang", "Qingming", ""], ["Yao", "Yuan", ""]]}, {"id": "1804.11247", "submitter": "Shabnam Sadeghi Esfahlani", "authors": "Shabnam Sadeghi Esfahlani, Tommy Thompson, Ali D. Parsa, Ian Brown,\n  Silvia Cirstea", "title": "ReHabgame A non-immersive virtual reality rehabilitation system with\n  applications in neuroscience", "comments": "29 pages, 10 figures", "journal-ref": "Heliyon. 2018 Feb 12;4(2):e00526", "doi": "10.1016/j.heliyon.2018.e00526", "report-no": "PMCID: PMC5857620 PMID: 29560446", "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes the use of a non-immersive virtual reality rehabilitation\nsystem ReHabgame developed using Microsoft Kinect and the Thalmic Labs Myo\ngesture control armband. The ReHabgame was developed based on two third-person\nvideo games that provide a feasible possibility of assessing postural control\nand functional reach tests. It accurately quantifies specific postural control\nmechanisms including timed standing balance, functional reach tests using\nreal-time anatomical landmark orientation, joint velocity, and acceleration\nwhile end trajectories were calculated using an inverse kinematics algorithm.\nThe game was designed to help patients with neurological impairment to be\nsubjected to physiotherapy activity and practice postures of daily activities.\nThe subjective experience of the ReHabgame was studied through the development\nof an Engagement Questionnaire (EQ) for qualitative, quantitative and Rasch\nmodel. The Monte-Carlo Tree Search (MCTS) and Random object (ROG) generator\nalgorithms were used to adapt the physical and gameplay intensity in the\nReHabgame based on the Motor Assessment Scale (MAS) and Hierarchical Scoring\nSystem (HSS). Rasch analysis was conducted to assess the psychometric\ncharacteristics of the ReHabgame and to identify if these are any misfitting\nitems in the game. Rasch rating scale model (RSM) was used to assess the\nengagement of players in the ReHabgame and evaluate the effectiveness and\nattractiveness of the game. The results showed that the scales assessing the\nrehabilitation process met Rasch expectations of reliability, and\nunidimensionality. Infit and outfit mean squares values are in the range of\n(0.68 1.52) for all considered 16 items. The Root Mean Square Residual (RMSR)\nand the person separation reliability were acceptable. The item/person map\nshowed that the persons and items were clustered symmetrically.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 07:51:32 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Esfahlani", "Shabnam Sadeghi", ""], ["Thompson", "Tommy", ""], ["Parsa", "Ali D.", ""], ["Brown", "Ian", ""], ["Cirstea", "Silvia", ""]]}]