[{"id": "1908.00003", "submitter": "Matthias Miller", "authors": "Matthias Miller, Alexandra Bonnici, Mennatallah El-Assady", "title": "Augmenting Music Sheets with Harmonic Fingerprints", "comments": "(9+1) pages; 5 figures; User Study", "journal-ref": null, "doi": "10.1145/3342558.3345395", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional Music Notation (CMN) is the well-established foundation for the\nwritten communication of musical information, such as rhythm, harmony, or\ntimbre. However, CMN suffers from the complexity of its visual encoding and the\nneed for extensive training to acquire proficiency and legibility. While\nalternative notations using additional visual variables (such as color to\nimprove pitch identification) have been proposed, the music community does not\nreadily accept notation systems that vary widely from the CMN. Therefore, to\nsupport student musicians in understanding the harmonic relationship of notes,\ninstead of replacing the CMN, we present a visualization technique that\naugments a digital music sheet with a harmonic fingerprint glyph. Our design\nexploits the circle of fifths - a fundamental concept in music theory, as a\nvisual metaphor. By attaching these visual glyphs to each bar of a selected\ncomposition we provide additional information about the salient harmonic\nfeatures available in a musical piece. We conducted a user study to analyze the\nperformance of experts and non-experts in an identification and comparison task\nof recurring patterns. The evaluation shows that the harmonic fingerprint\nsupports these tasks without the need for close-reading, as when compared to a\nnot-annotated music sheet.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 08:17:43 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Miller", "Matthias", ""], ["Bonnici", "Alexandra", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "1908.00073", "submitter": "Cindy Xiong", "authors": "Cindy Xiong, Cristina R. Ceja, Casimir J.H. Ludwig, and Steven\n  Franconeri", "title": "Biased Average Position Estimates in Line and Bar Graphs:\n  Underestimation, Overestimation, and Perceptual Pull", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In visual depictions of data, position (i.e., the vertical height of a line\nor a bar) is believed to be the most precise way to encode information compared\nto other encodings (e.g., hue). Not only are other encodings less precise than\nposition, but they can also be prone to systematic biases (e.g., color category\nboundaries can distort perceived differences between hues). By comparison,\nposition's high level of precision may seem to protect it from such biases. In\ncontrast, across three empirical studies, we show that while position may be a\nprecise form of data encoding, it can also produce systematic biases in how\nvalues are visually encoded, at least for reports of average position across a\nshort delay. In displays with a single line or a single set of bars, reports of\naverage positions were significantly biased, such that line positions were\nunderestimated and bar positions were overestimated. In displays with multiple\ndata series (i.e., multiple lines and/or sets of bars), this systematic bias\nstill persisted. We also observed an effect of \"perceptual pull\", where the\naverage position estimate for each series was 'pulled' toward the other. These\nfindings suggest that, although position may still be the most precise form of\nvisual data encoding, it can also be systematically biased.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 20:14:25 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Xiong", "Cindy", ""], ["Ceja", "Cristina R.", ""], ["Ludwig", "Casimir J. H.", ""], ["Franconeri", "Steven", ""]]}, {"id": "1908.00085", "submitter": "Ana Lucic", "authors": "Ana Lucic, Hinda Haned, Maarten de Rijke", "title": "Why Does My Model Fail? Contrastive Local Explanations for Retail\n  Forecasting", "comments": "To appear in ACM FAT* 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various business settings, there is an interest in using more complex\nmachine learning techniques for sales forecasting. It is difficult to convince\nanalysts, along with their superiors, to adopt these techniques since the\nmodels are considered to be \"black boxes,\" even if they perform better than\ncurrent models in use. We examine the impact of contrastive explanations about\nlarge errors on users' attitudes towards a \"black-box'\" model. We propose an\nalgorithm, Monte Carlo Bounds for Reasonable Predictions. Given a large error,\nMC-BRP determines (1) feature values that would result in a reasonable\nprediction, and (2) general trends between each feature and the target, both\nbased on Monte Carlo simulations. We evaluate on a real dataset with real users\nby conducting a user study with 75 participants to determine if explanations\ngenerated by MC-BRP help users understand why a prediction results in a large\nerror, and if this promotes trust in an automatically-learned model. Our study\nshows that users are able to answer objective questions about the model's\npredictions with overall 81.1% accuracy when provided with these contrastive\nexplanations. We show that users who saw MC-BRP explanations understand why the\nmodel makes large errors in predictions significantly more than users in the\ncontrol group. We also conduct an in-depth analysis on the difference in\nattitudes between Practitioners and Researchers, and confirm that our results\nhold when conditioning on the users' background.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 12:57:06 GMT"}, {"version": "v2", "created": "Wed, 27 Nov 2019 14:51:52 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Lucic", "Ana", ""], ["Haned", "Hinda", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1908.00086", "submitter": "Zitao Liu", "authors": "Guowei Xu, Wenbiao Ding, Jiliang Tang, Songfan Yang, Gale Yan Huang,\n  Zitao Liu", "title": "Learning Effective Embeddings From Crowdsourced Labels: An Educational\n  Case Study", "comments": null, "journal-ref": "2019 IEEE 35th International Conference on Data Engineering", "doi": "10.1109/ICDE.2019.00208", "report-no": null, "categories": "cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representation has been proven to be helpful in numerous machine\nlearning tasks. The success of the majority of existing representation learning\napproaches often requires a large amount of consistent and noise-free labels.\nHowever, labels are not accessible in many real-world scenarios and they are\nusually annotated by the crowds. In practice, the crowdsourced labels are\nusually inconsistent among crowd workers given their diverse expertise and the\nnumber of crowdsourced labels is very limited. Thus, directly adopting\ncrowdsourced labels for existing representation learning algorithms is\ninappropriate and suboptimal. In this paper, we investigate the above problem\nand propose a novel framework of \\textbf{R}epresentation \\textbf{L}earning with\ncrowdsourced \\textbf{L}abels, i.e., \"RLL\", which learns representation of data\nwith crowdsourced labels by jointly and coherently solving the challenges\nintroduced by limited and inconsistent labels. The proposed representation\nlearning framework is evaluated in two real-world education applications. The\nexperimental results demonstrate the benefits of our approach on learning\nrepresentation from limited labeled data from the crowds, and show RLL is able\nto outperform state-of-the-art baselines. Moreover, detailed experiments are\nconducted on RLL to fully understand its key components and the corresponding\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 03:01:06 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Xu", "Guowei", ""], ["Ding", "Wenbiao", ""], ["Tang", "Jiliang", ""], ["Yang", "Songfan", ""], ["Huang", "Gale Yan", ""], ["Liu", "Zitao", ""]]}, {"id": "1908.00087", "submitter": "Thilo Spinner", "authors": "Thilo Spinner, Udo Schlegel, Hanna Sch\\\"afer and Mennatallah El-Assady", "title": "explAIner: A Visual Analytics Framework for Interactive and Explainable\n  Machine Learning", "comments": "9 pages paper, 2 pages references, 5 pages supplementary material\n  (ancillary files)", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics (2019)", "doi": "10.1109/TVCG.2019.2934629", "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for interactive and explainable machine learning that\nenables users to (1) understand machine learning models; (2) diagnose model\nlimitations using different explainable AI methods; as well as (3) refine and\noptimize the models. Our framework combines an iterative XAI pipeline with\neight global monitoring and steering mechanisms, including quality monitoring,\nprovenance tracking, model comparison, and trust building. To operationalize\nthe framework, we present explAIner, a visual analytics system for interactive\nand explainable machine learning that instantiates all phases of the suggested\npipeline within the commonly used TensorBoard environment. We performed a\nuser-study with nine participants across different expertise levels to examine\ntheir perception of our workflow and to collect suggestions to fill the gap\nbetween our system and framework. The evaluation confirms that our tightly\nintegrated system leads to an informed machine learning process while\ndisclosing opportunities for further extensions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 15:04:59 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 12:54:46 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Spinner", "Thilo", ""], ["Schlegel", "Udo", ""], ["Sch\u00e4fer", "Hanna", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "1908.00113", "submitter": "Lin Yan", "authors": "Lin Yan, Yusu Wang, Elizabeth Munch, Ellen Gasparovic, Bei Wang", "title": "A Structural Average of Labeled Merge Trees for Uncertainty\n  Visualization", "comments": "IEEE VIS (SciVis) 2019 ACM 2012 CCS - Human-centered computing,\n  Visualization, Visualization design and evaluation methods", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934242", "report-no": null, "categories": "cs.CG cs.HC math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical phenomena in science and engineering are frequently modeled using\nscalar fields. In scalar field topology, graph-based topological descriptors\nsuch as merge trees, contour trees, and Reeb graphs are commonly used to\ncharacterize topological changes in the (sub)level sets of scalar fields. One\nof the biggest challenges and opportunities to advance topology-based\nvisualization is to understand and incorporate uncertainty into such\ntopological descriptors to effectively reason about their underlying data. In\nthis paper, we study a structural average of a set of labeled merge trees and\nuse it to encode uncertainty in data. Specifically, we compute a 1-center tree\nthat minimizes its maximum distance to any other tree in the set under a\nwell-defined metric called the interleaving distance. We provide heuristic\nstrategies that compute structural averages of merge trees whose labels do not\nfully agree. We further provide an interactive visualization system that\nresembles a numerical calculator that takes as input a set of merge trees and\noutputs a tree as their structural average. We also highlight structural\nsimilarities between the input and the average and incorporate uncertainty\ninformation for visual exploration. We develop a novel measure of uncertainty,\nreferred to as consistency, via a metric-space view of the input trees.\nFinally, we demonstrate an application of our framework through merge trees\nthat arise from ensembles of scalar fields. Our work is the first to employ\ninterleaving distances and consistency to study a global, mathematically\nrigorous, structural average of merge trees in the context of uncertainty\nvisualization.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 21:43:51 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 19:04:23 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Yan", "Lin", ""], ["Wang", "Yusu", ""], ["Munch", "Elizabeth", ""], ["Gasparovic", "Ellen", ""], ["Wang", "Bei", ""]]}, {"id": "1908.00176", "submitter": "Yongsu Ahn", "authors": "Yongsu Ahn and Yu-Ru Lin", "title": "FairSight: Visual Analytics for Fairness in Decision Making", "comments": "10 pages, 8 figures, IEEE VIS (VAST) 2019", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 2020", "doi": "10.1109/TVCG.2019.2934262", "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven decision making related to individuals has become increasingly\npervasive, but the issue concerning the potential discrimination has been\nraised by recent studies. In response, researchers have made efforts to propose\nand implement fairness measures and algorithms, but those efforts have not been\ntranslated to the real-world practice of data-driven decision making. As such,\nthere is still an urgent need to create a viable tool to facilitate fair\ndecision making. We propose FairSight, a visual analytic system to address this\nneed; it is designed to achieve different notions of fairness in ranking\ndecisions through identifying the required actions -- understanding, measuring,\ndiagnosing and mitigating biases -- that together lead to fairer decision\nmaking. Through a case study and user study, we demonstrate that the proposed\nvisual analytic and diagnostic modules in the system are effective in\nunderstanding the fairness-aware decision pipeline and obtaining more fair\noutcomes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 01:59:54 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2019 04:03:51 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Ahn", "Yongsu", ""], ["Lin", "Yu-Ru", ""]]}, {"id": "1908.00181", "submitter": "Zhuochen Jin", "authors": "Zhuochen Jin, Nan Cao, Yang Shi, Hanghang Tong, Yingcai Wu", "title": "EcoLens: Visual Analysis of Urban Region Dynamics Using Traffic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of urbanization during the past decades has\nsignificantly improved people's lives but also introduced new challenges on\neffective functional urban planning and transportation management. The\nfunctional regions defined based on a static boundary rarely reflect an\nindividual's daily experience of the space in which they live and visit for a\nvariety of purposes. Fortunately, the increasing availability of spatiotemporal\ndata provides unprecedented opportunities for understanding the structure of an\nurban area in terms of people's activity pattern and how they form the latent\nregions over time. These ecological regions, where people temporarily share a\nsimilar moving behavior during a short period of time, could provide insights\ninto urban planning and smart-city services. However, existing solutions are\nlimited in their capacity of capturing the evolutionary patterns of dynamic\nlatent regions within urban context. In this work, we introduce an interactive\nvisual analysis approach, EcoLens, that allows analysts to progressively\nexplore and analyze the complex dynamic segmentation patterns of a city using\ntraffic data. We propose an extended non-negative Matrix Factorization based\nalgorithm smoothed over both spatial and temporal dimensions to capture the\nspatiotemporal dynamics of the city. The algorithm also ensures the\northogonality of its result to facilitate the interpretation of different\npatterns. A suite of visualizations is designed to illustrate the dynamics of\ncity segmentation and the corresponding interactions are added to support the\nexploration of the segmentation patterns over time. We evaluate the\neffectiveness of our system via case studies using a real-world dataset and a\nqualitative interview with the domain expert.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 23:50:15 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Jin", "Zhuochen", ""], ["Cao", "Nan", ""], ["Shi", "Yang", ""], ["Tong", "Hanghang", ""], ["Wu", "Yingcai", ""]]}, {"id": "1908.00192", "submitter": "Jagoda Walny", "authors": "Jagoda Walny, Christian Frisson, Mieka West, Doris Kosminsky, S{\\o}ren\n  Knudsen, Sheelagh Carpendale, Wesley Willett", "title": "Data Changes Everything: Challenges and Opportunities in Data\n  Visualization Design Handoff", "comments": "11 pages, 11 figures. To appear in IEEE Transactions on Visualization\n  and Computer Graphics. To be presented at the IEEE VIS 2019 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex data visualization design projects often entail collaboration between\npeople with different visualization-related skills. For example, many teams\ninclude both designers who create new visualization designs and developers who\nimplement the resulting visualization software. We identify gaps between data\ncharacterization tools, visualization design tools, and development platforms\nthat pose challenges for designer-developer teams working to create new data\nvisualizations. While it is common for commercial interaction design tools to\nsupport collaboration between designers and developers, creating data\nvisualizations poses several unique challenges that are not supported by\ncurrent tools. In particular, visualization designers must characterize and\nbuild an understanding of the underlying data, then specify layouts, data\nencodings, and other data-driven parameters that will be robust across many\ndifferent data values. In larger teams, designers must also clearly communicate\nthese mappings and their dependencies to developers, clients, and other\ncollaborators. We report observations and reflections from five large\nmultidisciplinary visualization design projects and highlight six data-specific\nvisualization challenges for design specification and handoff. These challenges\ninclude adapting to changing data, anticipating edge cases in data,\nunderstanding technical challenges, articulating data-dependent interactions,\ncommunicating data mappings, and preserving the integrity of data mappings\nacross iterations. Based on these observations, we identify opportunities for\nfuture tools for prototyping, testing, and communicating data-driven designs,\nwhich might contribute to more successful and collaborative data visualization\ndesign.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 03:11:38 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Walny", "Jagoda", ""], ["Frisson", "Christian", ""], ["West", "Mieka", ""], ["Kosminsky", "Doris", ""], ["Knudsen", "S\u00f8ren", ""], ["Carpendale", "Sheelagh", ""], ["Willett", "Wesley", ""]]}, {"id": "1908.00215", "submitter": "Cindy Xiong", "authors": "Cindy Xiong, Joel Shapiro, Jessica Hullman, and Steven Franconeri", "title": "Illusion of Causality in Visualized Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Students who eat breakfast more frequently tend to have a higher grade point\naverage. From this data, many people might confidently state that a\nbefore-school breakfast program would lead to higher grades. This is a\nreasoning error, because correlation does not necessarily indicate causation --\nX and Y can be correlated without one directly causing the other. While this\nerror is pervasive, its prevalence might be amplified or mitigated by the way\nthat the data is presented to a viewer. Across three crowdsourced experiments,\nwe examined whether how simple data relations are presented would mitigate this\nreasoning error. The first experiment tested examples similar to the\nbreakfast-GPA relation, varying in the plausibility of the causal link. We\nasked participants to rate their level of agreement that the relation was\ncorrelated, which they rated appropriately as high. However, participants also\nexpressed high agreement with a causal interpretation of the data. Levels of\nsupport for the causal interpretation were not equally strong across\nvisualization types: causality ratings were highest for text descriptions and\nbar graphs, but weaker for scatter plots. But is this effect driven by bar\ngraphs aggregating data into two groups or by the visual encoding type? We\nisolated data aggregation versus visual encoding type and examined their\nindividual effect on perceived causality. Overall, different visualization\ndesigns afford different cognitive reasoning affordances across the same data.\nHigh levels of data aggregation by graphs tend to be associated with higher\nperceived causality in data. Participants perceived line and dot visual\nencodings as more causal than bar encodings. Our results demonstrate how some\nvisualization designs trigger stronger causal links while choosing others can\nhelp mitigate unwarranted perceptions of causality.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 05:11:48 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Xiong", "Cindy", ""], ["Shapiro", "Joel", ""], ["Hullman", "Jessica", ""], ["Franconeri", "Steven", ""]]}, {"id": "1908.00220", "submitter": "Laurent Lessard", "authors": "Ragini Rathore, Zachary Leggon, Laurent Lessard, Karen B. Schloss", "title": "Estimating Color-Concept Associations from Image Statistics", "comments": "IEEE VIS InfoVis 2019 ACM 2012 CSS: 1) Human-centered computing,\n  Human computer interaction (HCI), Empirical studies in HCI 2) Human-centered\n  computing, Human computer interaction (HCI), HCI design and evaluation\n  methods, Laboratory experiments 3) Human-centered computing, Visualization,\n  Empirical studies in visualization", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934536", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To interpret the meanings of colors in visualizations of categorical\ninformation, people must determine how distinct colors correspond to different\nconcepts. This process is easier when assignments between colors and concepts\nin visualizations match people's expectations, making color palettes\nsemantically interpretable. Efforts have been underway to optimize color\npalette design for semantic interpretablity, but this requires having good\nestimates of human color-concept associations. Obtaining these data from humans\nis costly, which motivates the need for automated methods. We developed and\nevaluated a new method for automatically estimating color-concept associations\nin a way that strongly correlates with human ratings. Building on prior studies\nusing Google Images, our approach operates directly on Google Image search\nresults without the need for humans in the loop. Specifically, we evaluated\nseveral methods for extracting raw pixel content of the images in order to best\nestimate color-concept associations obtained from human ratings. The most\neffective method extracted colors using a combination of cylindrical sectors\nand color categories in color space. We demonstrate that our approach can\naccurately estimate average human color-concept associations for different\nfruits using only a small set of images. The approach also generalizes\nmoderately well to more complicated recycling-related concepts of objects that\ncan appear in any color.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 05:48:42 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 19:26:40 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Rathore", "Ragini", ""], ["Leggon", "Zachary", ""], ["Lessard", "Laurent", ""], ["Schloss", "Karen B.", ""]]}, {"id": "1908.00234", "submitter": "Bradly Alicea", "authors": "Hrishikesh Kulkarni, Bradly Alicea", "title": "Cultural association based on machine learning for team formation", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Culture is core to human civilization, and is essential for human\nintellectual achievements in social context. Culture also influences how humans\nwork together, perform particular task and overall lifestyle and dealing with\nother groups of civilization. Thus, culture is concerned with establishing\nshared ideas, particularly those playing a key role in success. Does it impact\non how two individuals can work together in achieving certain goals? In this\npaper, we establish a means to derive cultural association and map it to\nculturally mediated success. Human interactions with the environment are\ntypically in the form of expressions. Association between culture and behavior\nproduce similar beliefs which lead to common principles and actions, while\ncultural similarity as a set of common expressions and responses. To measure\ncultural association among different candidates, we propose the use of a\nGraphical Association Method (GAM). The behaviors of candidates are captured\nthrough series of expressions and represented in the graphical form. The\nassociation among corresponding node and core nodes is used for the same. Our\napproach provides a number of interesting results and promising avenues for\nfuture applications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 06:57:35 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Kulkarni", "Hrishikesh", ""], ["Alicea", "Bradly", ""]]}, {"id": "1908.00277", "submitter": "Zhaosong Huang", "authors": "Zhaosong Huang, Ye Zhao, Wei Chen, Shengjie Gao, Kejie Yu, Weixia Xu,\n  Mingjie Tang, Minfeng Zhu, and Mingliang Xu", "title": "A Natural-language-based Visual Query Approach of Uncertain Human\n  Trajectories", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934671", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual querying is essential for interactively exploring massive trajectory\ndata. However, the data uncertainty imposes profound challenges to fulfill\nadvanced analytics requirements. On the one hand, many underlying data does not\ncontain accurate geographic coordinates, e.g., positions of a mobile phone only\nrefer to the regions (i.e., mobile cell stations) in which it resides, instead\nof accurate GPS coordinates. On the other hand, domain experts and general\nusers prefer a natural way, such as using a natural language sentence, to\naccess and analyze massive movement data. In this paper, we propose a visual\nanalytics approach that can extract spatial-temporal constraints from a textual\nsentence and support an effective query method over uncertain mobile trajectory\ndata. It is built up on encoding massive, spatially uncertain trajectories by\nthe semantic information of the POIs and regions covered by them, and then\nstoring the trajectory documents in text database with an effective indexing\nscheme. The visual interface facilitates query condition specification,\nsituation-aware visualization, and semantic exploration of large trajectory\ndata. Usage scenarios on real-world human mobility datasets demonstrate the\neffectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 09:00:59 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 01:50:00 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Huang", "Zhaosong", ""], ["Zhao", "Ye", ""], ["Chen", "Wei", ""], ["Gao", "Shengjie", ""], ["Yu", "Kejie", ""], ["Xu", "Weixia", ""], ["Tang", "Mingjie", ""], ["Zhu", "Minfeng", ""], ["Xu", "Mingliang", ""]]}, {"id": "1908.00286", "submitter": "Floris Den Hengst", "authors": "Floris den Hengst, Mark Hoogendoorn, Frank van Harmelen, Joost Bosman", "title": "Reinforcement Learning for Personalized Dialogue Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language systems have been of great interest to the research community and\nhave recently reached the mass market through various assistant platforms on\nthe web. Reinforcement Learning methods that optimize dialogue policies have\nseen successes in past years and have recently been extended into methods that\npersonalize the dialogue, e.g. take the personal context of users into account.\nThese works, however, are limited to personalization to a single user with whom\nthey require multiple interactions and do not generalize the usage of context\nacross users. This work introduces a problem where a generalized usage of\ncontext is relevant and proposes two Reinforcement Learning (RL)-based\napproaches to this problem. The first approach uses a single learner and\nextends the traditional POMDP formulation of dialogue state with features that\ndescribe the user context. The second approach segments users by context and\nthen employs a learner per context. We compare these approaches in a benchmark\nof existing non-RL and RL-based methods in three established and one novel\napplication domain of financial product recommendation. We compare the\ninfluence of context and training experiences on performance and find that\nlearning approaches generally outperform a handcrafted gold standard.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 09:19:27 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Hengst", "Floris den", ""], ["Hoogendoorn", "Mark", ""], ["van Harmelen", "Frank", ""], ["Bosman", "Joost", ""]]}, {"id": "1908.00379", "submitter": "Sebastian Cmentowski", "authors": "Sebastian Cmentowski, Andrey Krekhov, Jens Kr\\\"uger", "title": "Outstanding: A Multi-Perspective Travel Approach for Virtual Reality\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In virtual reality games, players dive into fictional environments and can\nexperience a compelling and immersive world. State-of-the-art VR systems allow\nfor natural and intuitive navigation through physical walking. However, the\ntracking space is still limited, and viable alternatives \\comm{or extensions\n}are required to reach further virtual destinations. Our work focuses on the\nexploration of vast open worlds -- an area where existing local navigation\napproaches such as the arc-based teleport are not ideally suited and\nworld-in-miniature techniques potentially reduce presence. We present a novel\nalternative for open environments: Our idea is to equip players with the\nability to switch from first-person to a third-person bird's eye perspective on\ndemand. From above, players can command their avatar and initiate travels over\nlarge distance. Our evaluation reveals a significant increase in spatial\norientation while avoiding cybersickness and preserving presence, enjoyment,\nand competence. We summarize our findings in a set of comprehensive design\nguidelines to help developers integrate our technique.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 13:22:37 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Cmentowski", "Sebastian", ""], ["Krekhov", "Andrey", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "1908.00385", "submitter": "Pritam Sarkar", "authors": "Pritam Sarkar, Kyle Ross, Aaron J. Ruberto, Dirk Rodenburg, Paul\n  Hungler, Ali Etemad", "title": "Classification of Cognitive Load and Expertise for Adaptive Simulation\n  using Deep Multitask Learning", "comments": "2019 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses, in any current or future\n  media, including reprinting/republishing this material for advertising or\n  promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "journal-ref": null, "doi": "10.1109/ACII.2019.8925507", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Simulations are a pedagogical means of enabling a risk-free way for\nhealthcare practitioners to learn, maintain, or enhance their knowledge and\nskills. Such simulations should provide an optimum amount of cognitive load to\nthe learner and be tailored to their levels of expertise. However, most current\nsimulations are a one-type-fits-all tool used to train different learners\nregardless of their existing skills, expertise, and ability to handle cognitive\nload. To address this problem, we propose an end-to-end framework for a trauma\nsimulation that actively classifies a participant's level of cognitive load and\nexpertise for the development of a dynamically adaptive simulation. To\nfacilitate this solution, trauma simulations were developed for the collection\nof electrocardiogram (ECG) signals of both novice and expert practitioners. A\nmultitask deep neural network was developed to utilize this data and classify\nhigh and low cognitive load, as well as expert and novice participants. A\nleave-one-subject-out (LOSO) validation was used to evaluate the effectiveness\nof our model, achieving an accuracy of 89.4% and 96.6% for classification of\ncognitive load and expertise, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 02:30:42 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Sarkar", "Pritam", ""], ["Ross", "Kyle", ""], ["Ruberto", "Aaron J.", ""], ["Rodenburg", "Dirk", ""], ["Hungler", "Paul", ""], ["Etemad", "Ali", ""]]}, {"id": "1908.00387", "submitter": "Dylan Cashman", "authors": "Dylan Cashman and Adam Perer and Remco Chang and Hendrik Strobelt", "title": "Ablate, Variate, and Contemplate: Visual Analytics for Discovering\n  Neural Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models require the configuration of many layers and parameters\nin order to get good results. However, there are currently few systematic\nguidelines for how to configure a successful model. This means model builders\noften have to experiment with different configurations by manually programming\ndifferent architectures (which is tedious and time consuming) or rely on purely\nautomated approaches to generate and train the architectures (which is\nexpensive). In this paper, we present Rapid Exploration of Model Architectures\nand Parameters, or REMAP, a visual analytics tool that allows a model builder\nto discover a deep learning model quickly via exploration and rapid\nexperimentation of neural network architectures. In REMAP, the user explores\nthe large and complex parameter space for neural network architectures using a\ncombination of global inspection and local experimentation. Through a visual\noverview of a set of models, the user identifies interesting clusters of\narchitectures. Based on their findings, the user can run ablation and variation\nexperiments to identify the effects of adding, removing, or replacing layers in\na given architecture and generate new models accordingly. They can also\nhandcraft new models using a simple graphical interface. As a result, a model\nbuilder can build deep learning models quickly, efficiently, and without manual\nprogramming. We inform the design of REMAP through a design study with four\ndeep learning model builders. Through a use case, we demonstrate that REMAP\nallows users to discover performant neural network architectures efficiently\nusing visual exploration and user-defined semi-automated searches through the\nmodel space.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 18:41:03 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Cashman", "Dylan", ""], ["Perer", "Adam", ""], ["Chang", "Remco", ""], ["Strobelt", "Hendrik", ""]]}, {"id": "1908.00403", "submitter": "Yating Wei", "authors": "Yating Wei, Honghui Mei, Ying Zhao, Shuyue Zhou, Bingru Lin, Haojing\n  Jiang and Wei Chen", "title": "Evaluating Perceptual Bias During Geometric Scaling of Scatterplots", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2019.2934208", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scatterplots are frequently scaled to fit display areas in multi-view and\nmulti-device data analysis environments. A common method used for scaling is to\nenlarge or shrink the entire scatterplot together with the inside points\nsynchronously and proportionally. This process is called geometric scaling.\nHowever, geometric scaling of scatterplots may cause a perceptual bias, that\nis, the perceived and physical values of visual features may be dissociated\nwith respect to geometric scaling. For example, if a scatterplot is projected\nfrom a laptop to a large projector screen, then observers may feel that the\nscatterplot shown on the projector has fewer points than that viewed on the\nlaptop. This paper presents an evaluation study on the perceptual bias of\nvisual features in scatterplots caused by geometric scaling. The study focuses\non three fundamental visual features (i.e., numerosity, correlation, and\ncluster separation) and three hypotheses that are formulated on the basis of\nour experience. We carefully design three controlled experiments by using\nwell-prepared synthetic data and recruit participants to complete the\nexperiments on the basis of their subjective experience. With a detailed\nanalysis of the experimental results, we obtain a set of instructive findings.\nFirst, geometric scaling causes a bias that has a linear relationship with the\nscale ratio. Second, no significant difference exists between the biases\nmeasured from normally and uniformly distributed scatterplots. Third, changing\nthe point radius can correct the bias to a certain extent. These findings can\nbe used to inspire the design decisions of scatterplots in various scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 13:53:36 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 03:03:57 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Wei", "Yating", ""], ["Mei", "Honghui", ""], ["Zhao", "Ying", ""], ["Zhou", "Shuyue", ""], ["Lin", "Bingru", ""], ["Jiang", "Haojing", ""], ["Chen", "Wei", ""]]}, {"id": "1908.00456", "submitter": "Jack Bandy", "authors": "Jack Bandy and Nicholas Diakopoulos", "title": "Auditing News Curation Systems: A Case Study Examining Algorithmic and\n  Editorial Logic in Apple News", "comments": "Preprint, to appear in Proceedings of the Fourteenth International\n  AAAI Conference on Web and Social Media (ICWSM 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents an audit study of Apple News as a sociotechnical news\ncuration system that exercises gatekeeping power in the media. We examine the\nmechanisms behind Apple News as well as the content presented in the app,\noutlining the social, political, and economic implications of both aspects. We\nfocus on the Trending Stories section, which is algorithmically curated, and\nthe Top Stories section, which is human-curated. Results from a crowdsourced\naudit showed minimal content personalization in the Trending Stories section,\nand a sock-puppet audit showed no location-based content adaptation. Finally,\nwe perform an extended two-month data collection to compare the human-curated\nTop Stories section with the algorithmically curated Trending Stories section.\nWithin these two sections, human curation outperformed algorithmic curation in\nseveral measures of source diversity, concentration, and evenness. Furthermore,\nalgorithmic curation featured more \"soft news\" about celebrities and\nentertainment, while editorial curation featured more news about policy and\ninternational events. To our knowledge, this study provides the first\ndata-backed characterization of Apple News in the United States.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 15:28:07 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Bandy", "Jack", ""], ["Diakopoulos", "Nicholas", ""]]}, {"id": "1908.00475", "submitter": "Mennatallah El-Assady", "authors": "Mennatallah El-Assady, Rebecca Kehlbeck, Christopher Collins, Daniel\n  Keim, Oliver Deussen", "title": "Semantic Concept Spaces: Guided Topic Model Refinement using\n  Word-Embedding Projections", "comments": null, "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework that allows users to incorporate the semantics of\ntheir domain knowledge for topic model refinement while remaining\nmodel-agnostic. Our approach enables users to (1) understand the semantic space\nof the model, (2) identify regions of potential conflicts and problems, and (3)\nreadjust the semantic relation of concepts based on their understanding,\ndirectly influencing the topic modeling. These tasks are supported by an\ninteractive visual analytics workspace that uses word-embedding projections to\ndefine concept regions which can then be refined. The user-refined concepts are\nindependent of a particular document collection and can be transferred to\nrelated corpora. All user interactions within the concept space directly affect\nthe semantic relations of the underlying vector space model, which, in turn,\nchange the topic modeling. In addition to direct manipulation, our system\nguides the users' decision-making process through recommended interactions that\npoint out potential improvements. This targeted refinement aims at minimizing\nthe feedback required for an efficient human-in-the-loop process. We confirm\nthe improvements achieved through our approach in two user studies that show\ntopic model quality improvements through our visual knowledge externalization\nand learning process.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:02:04 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["El-Assady", "Mennatallah", ""], ["Kehlbeck", "Rebecca", ""], ["Collins", "Christopher", ""], ["Keim", "Daniel", ""], ["Deussen", "Oliver", ""]]}, {"id": "1908.00494", "submitter": "Georg Regal", "authors": "Georg Regal, David Sellitsch, Elke Mattheiss, Manfred Tscheligi", "title": "POINTS -- Playful objects for inclusive, personalized movement games", "comments": "Workshop Augmented Humanity using Wearable and Mobile Devices for\n  Health and Wellbeing, MobileHCI2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promoting exercise and promoting fun in sport and activity is a common goal\nof schools. However, children and adolescents do not exercise enough, which can\nfavor a number of chronic illnesses. Exercise and sports often require\ncoordination of visual perception and reaction, which is an additional barrier\nfor visually impaired (blind and partially sighted) people. Due to their highly\nmotivating appeal, games promoting physical activity (exertion games) have\nbecome increasingly popular. Although accessible exertion games have been\ndeveloped, they do not consider the different abilities of players. Especially\nin team sports player roles that consider individual abilities can foster\ninclusion. To personalize roles and assign certain abilities to players,\nwearable technology can play an important role. In this position paper we\npresent ideas how digital objects can be used to design exertion games for\nvisually impaired students and we reflect how wearable technology can be used\nfor personalized player roles.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 16:44:31 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Regal", "Georg", ""], ["Sellitsch", "David", ""], ["Mattheiss", "Elke", ""], ["Tscheligi", "Manfred", ""]]}, {"id": "1908.00559", "submitter": "Kyle Hall", "authors": "Kyle Wm. Hall, Adam J. Bradley, Uta Hinrichs, Samuel Huron, Jo Wood,\n  Christopher Collins, Sheelagh Carpendale", "title": "Design by Immersion: A Transdisciplinary Approach to Problem-Driven\n  Visualizations", "comments": "The paper has been accepted to IEEE VIS (InfoVis) 2019, and will\n  appear IEEE TVCG. ACM 2012 CCS - Human-centered computing, Visualization,\n  Visualization design and evaluation methods", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934790", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While previous work exists on how to conduct and disseminate insights from\nproblem-driven visualization projects and design studies, the literature does\nnot address how to accomplish these goals in transdisciplinary teams in ways\nthat advance all disciplines involved. In this paper we introduce and define a\nnew methodological paradigm we call design by immersion, which provides an\nalternative perspective on problem-driven visualization work. Design by\nimmersion embeds transdisciplinary experiences at the center of the\nvisualization process by having visualization researchers participate in the\nwork of the target domain (or domain experts participate in visualization\nresearch). Based on our own combined experiences of working on\ncross-disciplinary, problem-driven visualization projects, we present six case\nstudies that expose the opportunities that design by immersion enables,\nincluding (1) exploring new domain-inspired visualization design spaces, (2)\nenriching domain understanding through personal experiences, and (3) building\nstrong transdisciplinary relationships. Furthermore, we illustrate how the\nprocess of design by immersion opens up a diverse set of design activities that\ncan be combined in different ways depending on the type of collaboration,\nproject, and goals. Finally, we discuss the challenges and potential pitfalls\nof design by immersion.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 18:03:22 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 14:56:49 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Hall", "Kyle Wm.", ""], ["Bradley", "Adam J.", ""], ["Hinrichs", "Uta", ""], ["Huron", "Samuel", ""], ["Wood", "Jo", ""], ["Collins", "Christopher", ""], ["Carpendale", "Sheelagh", ""]]}, {"id": "1908.00576", "submitter": "Michael Blumenschein", "authors": "Matthias Miller, Xuan Zhang, Johannes Fuchs, Michael Blumenschein", "title": "Evaluating Ordering Strategies of Star Glyph Axes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Star glyphs are a well-researched visualization technique to represent\nmulti-dimensional data. They are often used in small multiple settings for a\nvisual comparison of many data points. However, their overall visual appearance\nis strongly influenced by the ordering of dimensions. To this end, two\northogonal categories of layout strategies are proposed in the literature:\norder dimensions by similarity to get homogeneously shaped glyphs vs. order by\ndissimilarity to emphasize spikes and salient shapes. While there is evidence\nthat salient shapes support clustering tasks, evaluation, and direct comparison\nof data-driven ordering strategies has not received much research attention. We\ncontribute an empirical user study to evaluate the efficiency, effectiveness,\nand user confidence in visual clustering tasks using star glyphs. In comparison\nto similarity-based ordering, our results indicate that dissimilarity-based\nstar glyph layouts support users better in clustering tasks, especially when\nclutter is present.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 18:45:17 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Miller", "Matthias", ""], ["Zhang", "Xuan", ""], ["Fuchs", "Johannes", ""], ["Blumenschein", "Michael", ""]]}, {"id": "1908.00580", "submitter": "Jorge Alberto Wagner Filho", "authors": "Jorge A. Wagner Filho, Wolfgang Stuerzlinger and Luciana Nedel", "title": "Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive\n  Trajectory Data Exploration", "comments": "IEEE VIS InfoVis 2019 (Transactions on Visualization and Computer\n  Graphics)", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934415", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Space-Time Cube enables analysts to clearly observe spatio-temporal\nfeatures in movement trajectory datasets in geovisualization. However, its\ngeneral usability is impacted by a lack of depth cues, a reported steep\nlearning curve, and the requirement for efficient 3D navigation. In this work,\nwe investigate a Space-Time Cube in the Immersive Analytics domain. Based on a\nreview of previous work and selecting an appropriate exploration metaphor, we\nbuilt a prototype environment where the cube is coupled to a virtual\nrepresentation of the analyst's real desk, and zooming and panning in space and\ntime are intuitively controlled using mid-air gestures. We compared our\nimmersive environment to a desktop-based implementation in a user study with 20\nparticipants across 7 tasks of varying difficulty, which targeted different\nuser interface features. To investigate how performance is affected in the\npresence of clutter, we explored two scenarios with different numbers of\ntrajectories. While the quantitative performance was similar for the majority\nof tasks, large differences appear when we analyze the patterns of interaction\nand consider subjective metrics. The immersive version of the Space-Time Cube\nreceived higher usability scores, much higher user preference, and was rated to\nhave a lower mental workload, without causing participants discomfort in\n25-minute-long VR sessions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 18:59:21 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 18:42:54 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Filho", "Jorge A. Wagner", ""], ["Stuerzlinger", "Wolfgang", ""], ["Nedel", "Luciana", ""]]}, {"id": "1908.00605", "submitter": "S{\\o}ren Knudsen", "authors": "Gonzalo Gabriel M\\'endez and Jagoda Walny and S{\\o}ren Knudsen and\n  Charles Perin and Samuel Huron and Jo Vermeulen and Richard Pusch and\n  Sheelagh Carpendale", "title": "ReConstructor: A Scalable Constructive Visualization Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructive approaches to visualization authoring have been shown to offer\nadvantages such as providing options for flexible outputs, scaffolding and\nideation of new data mappings, personalized exploration of data, as well as\nsupporting data understanding and literacy. However, visualization authoring\ntools based on a constructive approach do not scale well to larger datasets. As\nconstruction often involves manipulating small pieces of data and visuals, it\nrequires a significant amount of time, effort, and repetitive steps. We present\nReConstructor, an authoring tool in which a visualization is constructed by\ninstantiating its structural and functional components through four interaction\nelements (objects, modifiers, activators, and tools). This design preserves\nmost of the benefits of a constructive process while avoiding scalability\nissues by allowing designers to propagate individual mapping steps to all the\nelements of a visualization. We also discuss the perceived benefits of our\napproach and propose avenues for future research in this area.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 20:09:08 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["M\u00e9ndez", "Gonzalo Gabriel", ""], ["Walny", "Jagoda", ""], ["Knudsen", "S\u00f8ren", ""], ["Perin", "Charles", ""], ["Huron", "Samuel", ""], ["Vermeulen", "Jo", ""], ["Pusch", "Richard", ""], ["Carpendale", "Sheelagh", ""]]}, {"id": "1908.00611", "submitter": "Daniel Weiskopf", "authors": "Daniel Weiskopf", "title": "Vis4Vis: Visualization for (Empirical) Visualization Research", "comments": "This is a preprint of a chapter for a planned book that was initiated\n  by participants of the Dagstuhl Seminar 18041 (\"Foundations of Data\n  Visualization\") and that is expected to be published by Springer. The final\n  book chapter will differ from this preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriate evaluation is a key component in visualization research. It is\ntypically based on empirical studies that assess visualization components or\ncomplete systems. While such studies often include the user of the\nvisualization, empirical research is not necessarily restricted to user studies\nbut may also address the technical performance of a visualization system such\nas its computational speed or memory consumption. Any such empirical experiment\nfaces the issue that the underlying visualization is becoming increasingly\nsophisticated, leading to an increasingly difficult evaluation in complex\nenvironments. Therefore, many of the established methods of empirical studies\ncan no longer capture the full complexity of the evaluation. One promising\nsolution is the use of data-rich observations that we can acquire during\nstudies to obtain more reliable interpretations of empirical research. For\nexample, we have been witnessing an increasing availability and use of\nphysiological sensor information from eye tracking, electrodermal activity\nsensors, electroencephalography, etc. Other examples are various kinds of logs\nof user activities such as mouse, keyboard, or touch interaction. Such\ndata-rich empirical studies promise to be especially useful for studies in the\nwild and similar scenarios outside of the controlled laboratory environment.\nHowever, with the growing availability of large, complex, time-dependent,\nheterogeneous, and unstructured observational data, we are facing the new\nchallenge of how we can analyze such data. This challenge can be addressed by\nestablishing the subfield of visualization for visualization (Vis4Vis):\nvisualization as a means of analyzing and communicating data from empirical\nstudies to advance visualization research.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 20:23:53 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Weiskopf", "Daniel", ""]]}, {"id": "1908.00629", "submitter": "Danielle Szafir", "authors": "Stephen Smart, Keke Wu, Danielle Albers Szafir", "title": "Color Crafting: Automating the Construction of Designer Quality Color\n  Ramps", "comments": "IEEE VIS, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualizations often encode numeric data using sequential and diverging color\nramps. Effective ramps use colors that are sufficiently discriminable, align\nwell with the data, and are aesthetically pleasing. Designers rely on years of\nexperience to create high-quality color ramps. However, it is challenging for\nnovice visualization developers that lack this experience to craft effective\nramps as most guidelines for constructing ramps are loosely defined qualitative\nheuristics that are often difficult to apply. Our goal is to enable\nvisualization developers to readily create effective color encodings using a\nsingle seed color. We do this using an algorithmic approach that models\ndesigner practices by analyzing patterns in the structure of designer-crafted\ncolor ramps. We construct these models from a corpus of 222 expert-designed\ncolor ramps, and use the results to automatically generate ramps that mimic\ndesigner practices. We evaluate our approach through an empirical study\ncomparing the outputs of our approach with designer-crafted color ramps. Our\nmodels produce ramps that support accurate and aesthetically pleasing\nvisualizations at least as well as designer ramps and that outperform\nconventional mathematical approaches.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 21:06:23 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Smart", "Stephen", ""], ["Wu", "Keke", ""], ["Szafir", "Danielle Albers", ""]]}, {"id": "1908.00630", "submitter": "Stephen Redmond", "authors": "Stephen Redmond", "title": "Visual cues in estimation of part-to-whole comparison", "comments": "Camera-ready version of final accepted paper for IEEE VIS 2019 Short\n  Papers track", "journal-ref": null, "doi": "10.1109/VISUAL.2019.8933718", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pie charts were first published in 1801 by William Playfair and have caused\nsome controversy since. Despite the suggestions of many experts against their\nuse, several empirical studies have shown that pie charts are at least as good\nas alternatives. From Brinton to Few on one side and Eells to Kosara on the\nother, there appears to have been a hundred-year war waged on the humble pie.\nIn this paper a set of experiments are reported that compare the performance of\npie charts and horizontal bar charts with various visual cues. Amazon's\nMechanical Turk service was employed to perform the tasks of estimating\nsegments in various part-to-whole charts. The results lead to recommendations\nfor data visualization professionals in developing dashboards.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 21:07:16 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 13:38:58 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Redmond", "Stephen", ""]]}, {"id": "1908.00661", "submitter": "Jacob Ritchie", "authors": "Amira Chalbi, Jacob Ritchie, Deokgun Park, Jungu Choi, Nicolas\n  Roussel, Niklas Elmqvist and Fanny Chevalier", "title": "Common Fate for Animated Transitions in Visualization", "comments": "11 pages, 5 figures, InfoVis Conference / IEEE TVCG, the first two\n  authors contributed equally to the work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Law of Common Fate from Gestalt psychology states that visual objects\nmoving with the same velocity along parallel trajectories will be perceived by\na human observer as grouped. However, the concept of common fate is much\nbroader than mere velocity; in this paper we explore how common fate results\nfrom coordinated changes in luminance and size. We present results from a\ncrowdsourced graphical perception study where we asked workers to make\nperceptual judgments on a series of trials involving four graphical objects\nunder the influence of conflicting static and dynamic visual factors (position,\nsize and luminance) used in conjunction. Our results yield the following\nrankings for visual grouping: motion > (dynamic luminance, size, luminance);\ndynamic size > (dynamic luminance, position); and dynamic luminance > size. We\nalso conducted a follow-up experiment to evaluate the three dynamic visual\nfactors in a more ecologically valid setting, using both a Gapminder-like\nanimated scatterplot and a thematic map of election data. The results indicate\nthat in practice the relative grouping strengths of these factors may depend on\nvarious parameters including the visualization characteristics and the\nunderlying data. We discuss design implications for animated transitions in\ndata visualization.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 23:33:20 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Chalbi", "Amira", ""], ["Ritchie", "Jacob", ""], ["Park", "Deokgun", ""], ["Choi", "Jungu", ""], ["Roussel", "Nicolas", ""], ["Elmqvist", "Niklas", ""], ["Chevalier", "Fanny", ""]]}, {"id": "1908.00662", "submitter": "Yalong Yang", "authors": "Yalong Yang", "title": "Visualising Geographically-Embedded Origin-Destination Flows: in 2D and\n  immersive environments", "comments": "PhD Thesis, Monash University, Australia, December 2018. Update:\n  corrected typos in arXiv comments", "journal-ref": null, "doi": "10.26180/5c087e9980d8a", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This thesis develops and evaluates effective techniques for visualisation of\nflows (e.g. of people, trade, knowledge) between places on geographic maps.\nThis geographically-embedded flow data contains information about geographic\nlocations, and flows from origin locations to destination locations. We\nexplored the design space of OD flow visualisation in both 2D and immersive\nenvironments. We do so by creating novel OD flow visualisations in both\nenvironments, and then conducting controlled user studies to evaluate different\ndesigns.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 23:57:09 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 23:51:49 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Yang", "Yalong", ""]]}, {"id": "1908.00671", "submitter": "Jieqiong Zhao", "authors": "Jieqiong Zhao, Morteza Karimzadeh, Ali Masjedi, Taojun Wang, Xiwen\n  Zhang, Melba M. Crawford, David S. Ebert", "title": "FeatureExplorer: Interactive Feature Selection and Exploration of\n  Regression Models for Hyperspectral Images", "comments": "To appear in IEEE VIS 2019 Short Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is used in machine learning to improve predictions,\ndecrease computation time, reduce noise, and tune models based on limited\nsample data. In this article, we present FeatureExplorer, a visual analytics\nsystem that supports the dynamic evaluation of regression models and importance\nof feature subsets through the interactive selection of features in\nhigh-dimensional feature spaces typical of hyperspectral images. The\ninteractive system allows users to iteratively refine and diagnose the model by\nselecting features based on their domain knowledge, interchangeable\n(correlated) features, feature importance, and the resulting model performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 00:50:04 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Zhao", "Jieqiong", ""], ["Karimzadeh", "Morteza", ""], ["Masjedi", "Ali", ""], ["Wang", "Taojun", ""], ["Zhang", "Xiwen", ""], ["Crawford", "Melba M.", ""], ["Ebert", "David S.", ""]]}, {"id": "1908.00679", "submitter": "Bahador Saket", "authors": "Bahador Saket, Samuel Huron, Charles Perin, and Alex Endert", "title": "Investigating Direct Manipulation of Graphical Encodings as a Method for\n  User Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate direct manipulation of graphical encodings as a method for\ninteracting with visualizations. There is an increasing interest in developing\nvisualization tools that enable users to perform operations by directly\nmanipulating graphical encodings rather than external widgets such as\ncheckboxes and sliders. Designers of such tools must decide which direct\nmanipulation operations should be supported, and identify how each operation\ncan be invoked. However, we lack empirical guidelines for how people convey\ntheir intended operations using direct manipulation of graphical encodings. We\naddress this issue by conducting a qualitative study that examines how\nparticipants perform 15 operations using direct manipulation of standard\ngraphical encodings. From this study, we 1) identify a list of strategies\npeople employ to perform each operation, 2) observe commonalities in strategies\nacross operations, and 3) derive implications to help designers leverage direct\nmanipulation of graphical encoding as a method for user interaction.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 02:05:35 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Saket", "Bahador", ""], ["Huron", "Samuel", ""], ["Perin", "Charles", ""], ["Endert", "Alex", ""]]}, {"id": "1908.00680", "submitter": "Matt Whitlock", "authors": "Matt Whitlock, Keke Wu, Danielle Szafir", "title": "Designing for Mobile and Immersive Visual Analytics in the Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collection and analysis in the field is critical for operations in\ndomains such as environmental science and public safety. However, field workers\ncurrently face data- and platform-oriented issues in efficient data collection\nand analysis in the field, such as limited connectivity, screen space, and\nattentional resources. In this paper, we explore how visual analytics tools\nmight transform field practices by more deeply integrating data into these\noperations. We use a design probe coupling mobile, cloud and immersive\nanalytics components to guide interviews with ten experts from five domains to\nexplore how visual analytics could support data collection and analysis needs\nin the field. The results identify shortcomings of current approaches and\ntarget scenarios and design considerations for future field analysis systems.\nWe embody these findings in FieldView, an extensible, open-source prototype\ndesigned to support critical use cases for situated field analysis. Our\nfindings suggest the potential for integrating mobile and immersive\ntechnologies to enhance data's utility for various field operations and new\ndirections for visual analytics tools to transform fieldwork.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 02:11:37 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Whitlock", "Matt", ""], ["Wu", "Keke", ""], ["Szafir", "Danielle", ""]]}, {"id": "1908.00681", "submitter": "Bowen Yu", "authors": "Bowen Yu, Claudio T. Silva", "title": "FlowSense: A Natural Language Interface for Visual Data Exploration\n  within a Dataflow System", "comments": "Published in IEEE Transactions on Visualization and Computer Graphics", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934668", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dataflow visualization systems enable flexible visual data exploration by\nallowing the user to construct a dataflow diagram that composes query and\nvisualization modules to specify system functionality. However learning\ndataflow diagram usage presents overhead that often discourages the user. In\nthis work we design FlowSense, a natural language interface for dataflow\nvisualization systems that utilizes state-of-the-art natural language\nprocessing techniques to assist dataflow diagram construction. FlowSense\nemploys a semantic parser with special utterance tagging and special utterance\nplaceholders to generalize to different datasets and dataflow diagrams. It\nexplicitly presents recognized dataset and diagram special utterances to the\nuser for dataflow context awareness. With FlowSense the user can expand and\nadjust dataflow diagrams more conveniently via plain English. We apply\nFlowSense to the VisFlow subset-flow visualization system to enhance its\nusability. We evaluate FlowSense by one case study with domain experts on a\nreal-world data analysis problem and a formal user study.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 02:19:21 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 18:30:35 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yu", "Bowen", ""], ["Silva", "Claudio T.", ""]]}, {"id": "1908.00688", "submitter": "Yuan-Fang Li", "authors": "Ying Yang, Michael Wybrow, Yuan-Fang Li, Tobias Czauderna, Yongqun He", "title": "OntoPlot: A Novel Visualisation for Non-hierarchical Associations in\n  Large Ontologies", "comments": "Accepted at IEEE InfoVis 2019", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934557", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontologies are formal representations of concepts and complex relationships\namong them. They have been widely used to capture comprehensive domain\nknowledge in areas such as biology and medicine, where large and complex\nontologies can contain hundreds of thousands of concepts. Especially due to the\nlarge size of ontologies, visualisation is useful for authoring, exploring and\nunderstanding their underlying data. Existing ontology visualisation tools\ngenerally focus on the hierarchical structure, giving much less emphasis to\nnon-hierarchical associations. In this paper we present OntoPlot, a novel\nvisualisation specifically designed to facilitate the exploration of all\nconcept associations whilst still showing an ontology's large hierarchical\nstructure. This hybrid visualisation combines icicle plots, visual compression\ntechniques and interactivity, improving space-efficiency and reducing visual\nstructural complexity. We conducted a user study with domain experts to\nevaluate the usability of OntoPlot, comparing it with the de facto ontology\neditor Prot{\\'e}g{\\'e}. The results confirm that OntoPlot attains our design\ngoals for association-related tasks and is strongly favoured by domain experts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 02:58:04 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 09:17:24 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Yang", "Ying", ""], ["Wybrow", "Michael", ""], ["Li", "Yuan-Fang", ""], ["Czauderna", "Tobias", ""], ["He", "Yongqun", ""]]}, {"id": "1908.00744", "submitter": "Pablo Barros", "authors": "Pablo Barros, Stefan Wermter, Alessandra Sciutti", "title": "Towards Learning How to Properly Play UNO with the iCub Robot", "comments": "Workshops on Naturalistic Non-Verbal and Affective Human-Robot\n  Interactions co-located with ICDL-EPIROB 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While interacting with another person, our reactions and behavior are much\naffected by the emotional changes within the temporal context of the\ninteraction. Our intrinsic affective appraisal comprising perception,\nself-assessment, and the affective memories with similar social experiences\nwill drive specific, and in most cases addressed as proper, reactions within\nthe interaction. This paper proposes the roadmap for the development of\nmultimodal research which aims to empower a robot with the capability to\nprovide proper social responses in a Human-Robot Interaction (HRI) scenario.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 08:11:46 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Barros", "Pablo", ""], ["Wermter", "Stefan", ""], ["Sciutti", "Alessandra", ""]]}, {"id": "1908.00754", "submitter": "Abon Chaudhuri", "authors": "Abon Chaudhuri", "title": "A Visual Technique to Analyze Flow of Information in a Machine Learning\n  System", "comments": "Published in Visualization and Data Analysis (VDA), part of IS&T\n  Electronic Imaging Symposium 2018", "journal-ref": null, "doi": "10.2352/ISSN.2470-1173.2018.01.VDA-380", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) algorithms and machine learning based software systems\nimplicitly or explicitly involve complex flow of information between various\nentities such as training data, feature space, validation set and results.\nUnderstanding the statistical distribution of such information and how they\nflow from one entity to another influence the operation and correctness of such\nsystems, especially in large-scale applications that perform classification or\nprediction in real time. In this paper, we propose a visual approach to\nunderstand and analyze flow of information during model training and serving\nphases. We build the visualizations using a technique called Sankey Diagram -\nconventionally used to understand data flow among sets - to address various use\ncases of in a machine learning system. We demonstrate how the proposed\ntechnique, tweaked and twisted to suit a classification problem, can play a\ncritical role in better understanding of the training data, the features, and\nthe classifier performance. We also discuss how this technique enables\ndiagnostic analysis of model predictions and comparative analysis of\npredictions from multiple classifiers. The proposed concept is illustrated with\nthe example of categorization of millions of products in the e-commerce domain\n- a multi-class hierarchical classification problem.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 08:31:36 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Chaudhuri", "Abon", ""]]}, {"id": "1908.00758", "submitter": "Guy Shtar", "authors": "Guy Shtar, Bracha Shapira, Lior Rokach", "title": "Clustering Wi-Fi Fingerprints for Indoor-Outdoor Detection", "comments": null, "journal-ref": null, "doi": "10.1007/s11276-018-1753-9", "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a method for continuous indoor-outdoor environment\ndetection on mobile devices based solely on WiFi fingerprints. Detection of\nindoor outdoor switching is an important part of identifying a user's context,\nand it provides important information for upper layer context aware mobile\napplications such as recommender systems, navigation tools, etc. Moreover,\nfuture indoor positioning systems are likely to use Wi-Fi fingerprints, and\ntherefore Wi-Fi receivers will be on most of the time. In contrast to existing\nresearch, we believe that these fingerprints should be leveraged, and they\nserve as the basis of the proposed method. Using various machine learning\nalgorithms, we train a supervised classifier based on features extracted from\nthe raw fingerprints, clusters, and cluster transition graph. The contribution\nof each of the features to the method is assessed. Our method assumes no prior\nknowledge of the environment, and a training set consisting of the data\ncollected for just a few hours on a single device is sufficient in order to\nprovide indoor-outdoor classification, even in an unknown location or when\nusing new devices. We evaluate our method in an experiment involving 12\nparticipants during their daily routine, with a total of 828 hours' worth of\ndata collected by the participants. We report a predictive performance of the\nAUC (area under the curve) of 0.94 using the gradient boosting machine ensemble\nlearning method. We show that our method can be used for other context\ndetection tasks such as learning and recognizing a given building or room.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 08:42:47 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Shtar", "Guy", ""], ["Shapira", "Bracha", ""], ["Rokach", "Lior", ""]]}, {"id": "1908.00903", "submitter": "Jessica Gisela Magallanes Castaneda", "authors": "Jessica Magallanes, Lindsey van Gemeren, Steven Wood, Maria-Cruz\n  Villa-Uriol", "title": "Analyzing Time Attributes in Temporal Event Sequences", "comments": "To appear in IEEE VIS 2019 Short Papers conference proceedings and\n  IEEE Xplore. 4 pages + references. 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event data is present in a variety of domains such as electronic health\nrecords, daily living activities and web clickstream records. Current\nvisualization methods to explore event data focus on discovering sequential\npatterns but present limitations when studying time attributes in event\nsequences. Time attributes are especially important when studying waiting times\nor lengths of visit in patient flow analysis. We propose a visual analytics\nmethodology that allows the identification of trends and outliers in respect of\nduration and time of occurrence in event sequences. The proposed method\npresents event data using a single Sequential and Time Patterns overview.\nUser-driven alignment by multiple events, sorting by sequence similarity and a\nnovel visual encoding of events allows the comparison of time trends across and\nwithin sequences. The proposed visualization allows the derivation of findings\nthat otherwise could not be obtained using traditional visualizations. The\nproposed methodology has been applied to a real-world dataset provided by\nSheffield Teaching Hospitals NHS Foundation Trust, for which four classes of\nconclusions were derived.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 15:21:02 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Magallanes", "Jessica", ""], ["van Gemeren", "Lindsey", ""], ["Wood", "Steven", ""], ["Villa-Uriol", "Maria-Cruz", ""]]}, {"id": "1908.00948", "submitter": "Stefan Lattner", "authors": "Stefan Lattner and Maarten Grachten", "title": "High-Level Control of Drum Track Generation Using Learned Patterns of\n  Rhythmic Interaction", "comments": "Paper accepted at the IEEE Workshop on Applications of Signal\n  Processing to Audio and Acoustics (WASPAA 2019), New Paltz, New York, U.S.A.,\n  October 20-23; 6 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spurred by the potential of deep learning, computational music generation has\ngained renewed academic interest. A crucial issue in music generation is that\nof user control, especially in scenarios where the music generation process is\nconditioned on existing musical material. Here we propose a model for\nconditional kick drum track generation that takes existing musical material as\ninput, in addition to a low-dimensional code that encodes the desired relation\nbetween the existing material and the new material to be generated. These\nrelational codes are learned in an unsupervised manner from a music dataset. We\nshow that codes can be sampled to create a variety of musically plausible kick\ndrum tracks and that the model can be used to transfer kick drum patterns from\none song to another. Lastly, we demonstrate that the learned codes are largely\ninvariant to tempo and time-shift.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 16:39:02 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Lattner", "Stefan", ""], ["Grachten", "Maarten", ""]]}, {"id": "1908.00977", "submitter": "Elisabeth Lex", "authors": "Elisabeth Lex, Dominik Kowald", "title": "The Impact of Time on Hashtag Reuse in Twitter: A Cognitive-Inspired\n  Hashtag Recommendation Approach", "comments": "49. GI-Jahrestagung INFORMATIK 2019, Best of Data Science Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In our work [KPL17], we study temporal usage patterns of Twitter hashtags,\nand we use the Base-Level Learning (BLL) equation from the cognitive\narchitecture ACT-R [An04] to model how a person reuses her own, individual\nhashtags as well as hashtags from her social network. The BLL equation accounts\nfor the time-dependent decay of item exposure in human memory. According to\nBLL, the usefulness of a piece of information (e.g., a hashtag) is defined by\nhow frequently and how recently it was used in the past, following a\ntime-dependent decay that is best modeled with a power-law distribution. We\nused the BLL equation in our previous work to recommend tags in social\nbookmarking systems [KL16]. Here [KPL17], we adopt the BLL equation to model\ntemporal reuse patterns of individual (i.e., reusing own hashtags) and social\nhashtags (i.e., reusing hashtags, which has been previously used by a followee)\nand to build a cognitive-inspired hashtag recommendation algorithm. We\ndemonstrate the efficacy of our approach in two empirical social networks\ncrawled from Twitter, i.e., CompSci and Random (for details about the datasets,\nsee [KPL17]). Our results show that our approach can outperform current\nstate-of-the-art hashtag recommendation approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 11:40:05 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Lex", "Elisabeth", ""], ["Kowald", "Dominik", ""]]}, {"id": "1908.00981", "submitter": "Sakib Khan", "authors": "Sakib Mahmud Khan, Mashrur Chowdhury", "title": "Situation-Aware Left-Turning Connected and Automated Vehicle Operation\n  at Signalized Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One challenging aspect of the Connected and Automated Vehicle (CAV) operation\nin mixed traffic is the development of a situation-awareness module for CAVs.\nWhile operating on public roads, CAVs need to assess their surroundings,\nespecially the intentions of non-CAVs. Generally, CAVs demonstrate a defensive\ndriving behavior, and CAVs expect other non-autonomous entities on the road\nwill follow the traffic rules or common driving behavior. However, the presence\nof aggressive human drivers in the surrounding environment, who may not follow\ntraffic rules and behave abruptly, can lead to serious safety consequences. In\nthis paper, we have addressed the CAV and non-CAV interaction by evaluating a\nsituation-awareness module for left-turning CAV operations in an urban area.\nExisting literature does not consider the intent of the following vehicle for a\nCAVs left-turning movement, and existing CAV controllers do not assess the\nfollowing non-CAVs intents. Based on our simulation study, the situation-aware\nCAV controller module reduces up to 27% of the abrupt braking of the following\nnon-CAVs for scenarios with different opposing through movement compared to the\nbase scenario with the autonomous vehicle, without considering the following\nvehicles intent. The analysis shows that the average travel time reductions for\nthe opposite through traffic volumes of 600, 800, and 1000 vehicle/hour/lane\nare 58%, 52%, and 62%, respectively, for the aggressive human driver following\nthe CAV if the following vehicles intent is considered by a CAV in making a\nleft turn at an intersection.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2019 16:44:14 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 19:21:11 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Khan", "Sakib Mahmud", ""], ["Chowdhury", "Mashrur", ""]]}, {"id": "1908.01133", "submitter": "Ardavan Bidgoli", "authors": "Ardavan Bidgoli, Eunsu Kang, Daniel Cardoso Llach", "title": "Machinic Surrogates: Human-Machine Relationships in Computational\n  Creativity", "comments": "25th International Symposium on Electronic Art, ISEA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in artificial intelligence (AI) and its sub-branch\nmachine learning (ML) promise machines that go beyond the boundaries of\nautomation and behave autonomously. Applications of these machines in creative\npractices such as art and design entail relationships between users and\nmachines that have been described as a form of collaboration or co-creation\nbetween computational and human agents. This paper uses examples from art and\ndesign to argue that this frame is incomplete as it fails to acknowledge the\nsocio-technical nature of AI systems, and the different human agencies involved\nin their design, implementation, and operation. Situating applications of\nAI-enabled tools in creative practices in a spectrum between automation and\nautonomy, this paper distinguishes different kinds of human engagement elicited\nby systems deemed automated or autonomous. Reviewing models of artistic\ncollaboration during the late 20th century, it suggests that collaboration is\nat the core of these artistic practices. We build upon the growing literature\nof machine learning and art to look for the human agencies inscribed in works\nof computational creativity, and expand the co-creation frame to incorporate\nemerging forms of human-human collaboration mediated through technical\nartifacts such as algorithms and data.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2019 08:15:59 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Bidgoli", "Ardavan", ""], ["Kang", "Eunsu", ""], ["Llach", "Daniel Cardoso", ""]]}, {"id": "1908.01277", "submitter": "Yalong Yang", "authors": "Linda Woodburn, Yalong Yang and Kim Marriott", "title": "Interactive Visualisation of Hierarchical Quantitative Data: An\n  Evaluation", "comments": "Presented at IEEE VIS 2019 in Vancouver, Canada and included in the\n  VIS 2019 conference proceedings. Improved the image quality in the paper", "journal-ref": null, "doi": "10.1109/VISUAL.2019.8933545", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have compared three common visualisations for hierarchical quantitative\ndata, treemaps, icicle plots and sunburst charts as well as a semicircular\nvariant of sunburst charts we call the sundown chart. In a pilot study, we\nfound that the sunburst chart was least preferred. In a controlled study with\n12 participants, we compared treemaps, icicle plots and sundown charts. Treemap\nwas the least preferred and had a slower performance on a basic navigation task\nand slower performance and accuracy in hierarchy understanding tasks. The\nicicle plot and sundown chart had similar performance with slight user\npreference for the icicle plot.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 06:16:08 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 19:33:02 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 22:10:55 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Woodburn", "Linda", ""], ["Yang", "Yalong", ""], ["Marriott", "Kim", ""]]}, {"id": "1908.01344", "submitter": "Lesandro Ponciano", "authors": "Lesandro Ponciano and Thiago Emmanuel Pereira", "title": "Characterising Volunteers' Task Execution Patterns Across Projects on\n  Multi-Project Citizen Science Platforms", "comments": "XVIII Brazilian Symposium on Human Factors in Computing Systems\n  (IHC'19), October 21-25, 2019, Vit\\'oria, ES, Brazil", "journal-ref": "In Proceedings of IHC 2019. ACM, US. 11 pages (2019)", "doi": "10.1145/3357155.3358441", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Citizen science projects engage people in activities that are part of a\nscientific research effort. On multi-project citizen science platforms,\nscientists can create projects consisting of tasks. Volunteers, in turn,\nparticipate in executing the project's tasks. Such type of platforms seeks to\nconnect volunteers and scientists' projects, adding value to both. However,\nlittle is known about volunteer's cross-project engagement patterns and the\nbenefits of such patterns for scientists and volunteers. This work proposes a\nGoal, Question, and Metric (GQM) approach to analyse volunteers' cross-project\ntask execution patterns and employs the Semiotic Inspection Method (SIM) to\nanalyse the communicability of the platform's cross-project features. In doing\nso, it investigates what are the features of platforms to foster volunteers'\ncross-project engagement, to what extent multi-project platforms facilitate the\nattraction of volunteers to perform tasks in new projects, and to what extent\nmulti-project participation increases engagement on the platforms. Results from\nanalyses on real platforms show that volunteers tend to explore multiple\nprojects, but they perform tasks regularly in just a few of them; few projects\nattract much attention from volunteers; volunteers recruited from other\nprojects on the platform tend to get more engaged than those recruited outside\nthe platform. System inspection shows that platforms still lack personalised\nand explainable recommendations of projects and tasks. The findings are\ntranslated into useful claims about how to design and manage multi-project\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 14:00:49 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 01:07:07 GMT"}, {"version": "v3", "created": "Wed, 9 Oct 2019 04:29:55 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Ponciano", "Lesandro", ""], ["Pereira", "Thiago Emmanuel", ""]]}, {"id": "1908.01523", "submitter": "Raoul de Charette", "authors": "Raoul de Charette, Sotiris Manitsaris", "title": "3D Reconstruction of Deformable Revolving Object under Heavy Hand\n  Interaction", "comments": "7 pages, 10 figures. Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reconstruct 3D deformable object through time, in the context of a live\npottery making process where the crafter molds the object. Because the object\nsuffers from heavy hand interaction, and is being deformed, classical\ntechniques cannot be applied. We use particle energy optimization to estimate\nthe object profile and benefit of the object radial symmetry to increase the\nrobustness of the reconstruction to both occlusion and noise. Our method works\nwith an unconstrained scalable setup with one or more depth sensors. We\nevaluate on our database (released upon publication) on a per-frame and\ntemporal basis and shows it significantly outperforms state-of-the-art\nachieving 7.60mm average object reconstruction error. Further ablation studies\ndemonstrate the effectiveness of our method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 09:00:54 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["de Charette", "Raoul", ""], ["Manitsaris", "Sotiris", ""]]}, {"id": "1908.01536", "submitter": "Liam Hiley BSc", "authors": "Liam Hiley and Alun Preece and Yulia Hicks and David Marshall and\n  Harrison Taylor", "title": "Discriminating Spatial and Temporal Relevance in Deep Taylor\n  Decompositions for Explainable Activity Recognition", "comments": "5 pages, 2 figures, published at IJCAI19 ExAI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current techniques for explainable AI have been applied with some success to\nimage processing. The recent rise of research in video processing has called\nfor similar work n deconstructing and explaining spatio-temporal models. While\nmany techniques are designed for 2D convolutional models, others are inherently\napplicable to any input domain. One such body of work, deep Taylor\ndecomposition, propagates relevance from the model output distributively onto\nits input and thus is not restricted to image processing models. However, by\nexploiting a simple technique that removes motion information, we show that it\nis not the case that this technique is effective as-is for representing\nrelevance in non-image tasks. We instead propose a discriminative method that\nproduces a na\\\"ive representation of both the spatial and temporal relevance of\na frame as two separate objects. This new discriminative relevance model\nexposes relevance in the frame attributed to motion, that was previously\nambiguous in the original explanation. We observe the effectiveness of this\ntechnique on a range of samples from the UCF-101 action recognition dataset,\ntwo of which are demonstrated in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 09:42:25 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 14:36:13 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Hiley", "Liam", ""], ["Preece", "Alun", ""], ["Hicks", "Yulia", ""], ["Marshall", "David", ""], ["Taylor", "Harrison", ""]]}, {"id": "1908.01697", "submitter": "Jessica Hullman", "authors": "Jessica Hullman", "title": "Why Authors Don't Visualize Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clear presentation of uncertainty is an exception rather than rule in media\narticles, data-driven reports, and consumer applications, despite proposed\ntechniques for communicating sources of uncertainty in data. This work\nconsiders, Why do so many visualization authors choose not to visualize\nuncertainty? I contribute a detailed characterization of practices,\nassociations, and attitudes related to uncertainty communication among\nvisualization authors, derived from the results of surveying 90 authors who\nregularly create visualizations for others as part of their work, and\ninterviewing thirteen influential visualization designers. My results highlight\nchallenges that authors face and expose assumptions and inconsistencies in\nbeliefs about the role of uncertainty in visualization. In particular, a clear\ncontradiction arises between authors' acknowledgment of the value of depicting\nuncertainty and the norm of omitting direct depiction of uncertainty. To help\nexplain this contradiction, I present a rhetorical model of uncertainty\nomission in visualization-based communication. I also adapt a formal\nstatistical model of how viewers judge the strength of a signal in a\nvisualization to visualization-based communication, to argue that uncertainty\ncommunication necessarily reduces degrees of freedom in viewers' statistical\ninferences. I conclude with recommendations for how visualization research on\nuncertainty communication could better serve practitioners' current needs and\nvalues while deepening understanding of assumptions that reinforce uncertainty\nomission.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 15:43:20 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Hullman", "Jessica", ""]]}, {"id": "1908.01699", "submitter": "David Awad", "authors": "David Awad", "title": "Thoth: Improved Rapid Serial Visual Presentation using Natural Language\n  Processing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thoth is a tool designed to combine many different types of speed reading\ntechnology. The largest insight is using natural language parsing for more\noptimal rapid serial visual presentation and more effective reading\ninformation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 15:45:39 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Awad", "David", ""]]}, {"id": "1908.01963", "submitter": "Hannah Bowman", "authors": "Dana Conard, Blake Vollmer, Corbin Shatto, Hannah Bowman and Sara\n  Kassis", "title": "Using the Makerspace to Create Educational Open-source Software for\n  Electrical Circuits: A Learning Experience", "comments": "Presented as poster at International Symposium on Academic\n  Makerspaces(ISAM) 2018 - https://isam2018.hemi-makers.org/papers-posters/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.ed-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual learning environments are a useful modality for engaging students in\nthe classroom by affording them a sense of presence and immersion. The\nmotivation of this project was to create an open-source augmented reality\nelectrical circuit application for use in lower division engineering courses to\nteach students about electricity fundamentals. Softwares that are readily\navailable for use on virtual and augmented reality devices do not typically\napply to all disciplines and do not necessarily have a pedagogical or\naccessibility focus. Considering this lack of appropriate educational\napplications for the current virtual and augmented reality devices, a team of\ninterdisciplinary students was assembled to create such software. With\nextensive usability studies, the application was designed for quick adoption\nand improve accessibility by providing multimodal access such as voice\nassistant, gray scaling for depth perception and daltonize the app. The\nsoftware is available as part of VITaL Laboratory, Sonoma State University.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 05:35:21 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Conard", "Dana", ""], ["Vollmer", "Blake", ""], ["Shatto", "Corbin", ""], ["Bowman", "Hannah", ""], ["Kassis", "Sara", ""]]}, {"id": "1908.02005", "submitter": "Honghui Mei Mr.", "authors": "Honghui Mei, Wei Chen, Yating Wei, Yuanzhe Hu, Shuyue Zhou, Bingru\n  Lin, Ying Zhao, Jiazhi Xia", "title": "RSATree: Distribution-Aware Data Representation of Large-Scale Tabular\n  Datasets for Flexible Visual Query", "comments": "VIS 2019 (InfoVis) accepted", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934800", "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysts commonly investigate the data distributions derived from statistical\naggregations of data that are represented by charts, such as histograms and\nbinned scatterplots, to visualize and analyze a large-scale dataset. Aggregate\nqueries are implicitly executed through such a process. Datasets are constantly\nextremely large; thus, the response time should be accelerated by calculating\npredefined data cubes. However, the queries are limited to the predefined\nbinning schema of preprocessed data cubes. Such limitation hinders analysts'\nflexible adjustment of visual specifications to investigate the implicit\npatterns in the data effectively. Particularly, RSATree enables arbitrary\nqueries and flexible binning strategies by leveraging three schemes, namely, an\nR-tree-based space partitioning scheme to catch the data distribution, a\nlocality-sensitive hashing technique to achieve locality-preserving random\naccess to data items, and a summed area table scheme to support interactive\nquery of aggregated values with a linear computational complexity. This study\npresents and implements a web-based visual query system that supports visual\nspecification, query, and exploration of large-scale tabular data with\nuser-adjustable granularities. We demonstrate the efficiency and utility of our\napproach by performing various experiments on real-world datasets and analyzing\ntime and space complexity.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 08:17:43 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 01:17:08 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Mei", "Honghui", ""], ["Chen", "Wei", ""], ["Wei", "Yating", ""], ["Hu", "Yuanzhe", ""], ["Zhou", "Shuyue", ""], ["Lin", "Bingru", ""], ["Zhao", "Ying", ""], ["Xia", "Jiazhi", ""]]}, {"id": "1908.02052", "submitter": "Yalong Yang", "authors": "Yalong Yang, Tim Dwyer, Sarah Goodwin and Kim Marriott", "title": "Many-to-Many Geographically-Embedded Flow Visualisation: An Evaluation", "comments": "Presented at IEEE Conference on Information Visualization (InfoVis\n  2016). Awarded Best Paper Honorable Mention. Part of PhD thesis\n  arXiv:1908.00662", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 23:1\n  (2017) 411-420", "doi": "10.1109/TVCG.2016.2598885", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Showing flows of people and resources between multiple geographic locations\nis a challenging visualisation problem. We conducted two quantitative user\nstudies to evaluate different visual representations for such dense\nmany-to-many flows. In our first study we compared a bundled node-link flow map\nrepresentation and OD Maps [37] with a new visualisation we call MapTrix. Like\nOD Maps, MapTrix overcomes the clutter associated with a traditional flow map\nwhile providing geographic embedding that is missing in standard OD matrix\nrepresentations. We found that OD Maps and MapTrix had similar performance\nwhile bundled node-link flow map representations did not scale at all well. Our\nsecond study compared participant performance with OD Maps and MapTrix on\nlarger data sets. Again performance was remarkably similar.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 10:18:20 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Goodwin", "Sarah", ""], ["Marriott", "Kim", ""]]}, {"id": "1908.02088", "submitter": "Yalong Yang", "authors": "Yalong Yang, Bernhard Jenny, Tim Dwyer, Kim Marriott, Haohui Chen and\n  Maxime Cordeil", "title": "Maps and Globes in Virtual Reality", "comments": "Presented at 20th EG/VGTC Conference on Visualization (EuroVis 2018).\n  Part of PhD thesis arXiv:1908.00662", "journal-ref": "Computer Graphics Forum 37:3 (2018) 427-438", "doi": "10.1111/cgf.13431", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores different ways to render world-wide geographic maps in\nvirtual reality (VR). We compare: (a) a 3D exocentric globe, where the user's\nviewpoint is outside the globe; (b) a flat map (rendered to a plane in VR); (c)\nan egocentric 3D globe, with the viewpoint inside the globe; and (d) a curved\nmap, created by projecting the map onto a section of a sphere which curves\naround the user. In all four visualisations the geographic centre can be\nsmoothly adjusted with a standard handheld VR controller and the user, through\na head-tracked headset, can physically move around the visualisation. For\ndistance comparison, exocentric globe is more accurate than egocentric globe\nand flat map. For area comparison, more time is required with exocentric and\negocentric globes than with flat and curved maps. For direction estimation, the\nexocentric globe is more accurate and faster than the other visual\npresentations. Our study participants had a weak preference for the exocentric\nglobe. Generally, the curved map had benefits over the flat map. In almost all\ncases the egocentric globe was found to be the least effective visualisation.\nOverall, our results provide support for the use of exocentric globes for\ngeographic visualisation in mixed-reality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:45:51 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "Yalong", ""], ["Jenny", "Bernhard", ""], ["Dwyer", "Tim", ""], ["Marriott", "Kim", ""], ["Chen", "Haohui", ""], ["Cordeil", "Maxime", ""]]}, {"id": "1908.02089", "submitter": "Yalong Yang", "authors": "Yalong Yang, Tim Dwyer, Bernhard Jenny, Kim Marriott, Maxime Cordeil\n  and Haohui Chen", "title": "Origin-Destination Flow Maps in Immersive Environments", "comments": "Presented at IEEE Conference on Information Visualization (InfoVis\n  2018). Part of PhD thesis arXiv:1908.00662", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics 25:1\n  (2019) 693-703", "doi": "10.1109/TVCG.2018.2865192", "report-no": null, "categories": "cs.HC cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immersive virtual- and augmented-reality headsets can overlay a flat image\nagainst any surface or hang virtual objects in the space around the user. The\ntechnology is rapidly improving and may, in the long term, replace traditional\nflat panel displays in many situations. When displays are no longer\nintrinsically flat, how should we use the space around the user for abstract\ndata visualisation? In this paper, we ask this question with respect to\norigin-destination flow data in a global geographic context. We report on the\nfindings of three studies exploring different spatial encodings for flow maps.\nThe first experiment focuses on different 2D and 3D encodings for flows on flat\nmaps. We find that participants are significantly more accurate with raised\nflow paths whose height is proportional to flow distance but fastest with\ntraditional straight line 2D flows. In our second and third experiment, we\ncompared flat maps, 3D globes and a novel interactive design we call MapsLink,\ninvolving a pair of linked flat maps. We find that participants took\nsignificantly more time with MapsLink than other flow maps while the 3D globe\nwith raised flows was the fastest, most accurate, and most preferred method.\nOur work suggests that careful use of the third spatial dimension can resolve\nvisual clutter in complex flow maps.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 11:57:41 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Jenny", "Bernhard", ""], ["Marriott", "Kim", ""], ["Cordeil", "Maxime", ""], ["Chen", "Haohui", ""]]}, {"id": "1908.02119", "submitter": "Zulkarnaen Hatala", "authors": "Zulkarnaen Hatala", "title": "Practical Speech Recognition with HTK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical aspects of developing an Automatic Speech Recognition System\n(ASR) with HTK are reviewed. Steps are explained concerning hardware, software,\nlibraries, applications and computer programs used. The common procedure to\nrapidly apply speech recognition system is summarized. The procedure is\nillustrated, to implement a speech based electrical switch in home automation\nfor the Indonesian language. The main key of the procedure is to match the\nenvironment for training and testing using the training data recorded from the\ntesting program, HVite. Often the silence detector of HTK is wrongly triggered\nby noises because the microphone is too sensitive. This problem is mitigated by\nsimply scaling down the volume. In this sub-word phone-based speech\nrecognition, noise is included in the training database and labelled\nparticularly. Illustration of the procedure is applied to a home automation\napplication. Electrical switches are controlled by Indonesian speech\nrecognizer. The results show 100% command completion rate.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 13:12:57 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Hatala", "Zulkarnaen", ""]]}, {"id": "1908.02282", "submitter": "Srijith Rajamohan", "authors": "Srijith Rajamohan, Alana Romanella, Amit Ramesh", "title": "A Weakly-Supervised Attention-based Visualization Tool for Assessing\n  Political Affiliation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we seek to finetune a weakly-supervised expert-guided Deep\nNeural Network (DNN) for the purpose of determining political affiliations. In\nthis context, stance detection is used for determining political affiliation or\nideology which is framed in the form of relative proximities between entities\nin a low-dimensional space. An attention-based mechanism is used to provide\nmodel interpretability. A Deep Neural Network for Natural Language\nUnderstanding (NLU) using static and contextual embeddings is trained and\nevaluated. Various techniques to visualize the projections generated from the\nnetwork are evaluated for visualization efficiency. An overview of the pipeline\nfrom data ingestion, processing and generation of visualization is given here.\nA web-based framework created to faciliate this interaction and exploration is\npresented here. Preliminary results of this study are summarized and future\nwork is outlined.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 18:14:06 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Rajamohan", "Srijith", ""], ["Romanella", "Alana", ""], ["Ramesh", "Amit", ""]]}, {"id": "1908.02402", "submitter": "Lei Shu", "authors": "Lei Shu, Piero Molino, Mahdi Namazifar, Hu Xu, Bing Liu, Huaixiu\n  Zheng, Gokhan Tur", "title": "Flexibly-Structured Model for Task-Oriented Dialogues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel end-to-end architecture for task-oriented\ndialogue systems. It is based on a simple and practical yet very effective\nsequence-to-sequence approach, where language understanding and state tracking\ntasks are modeled jointly with a structured copy-augmented sequential decoder\nand a multi-label decoder for each slot. The policy engine and language\ngeneration tasks are modeled jointly following that. The copy-augmented\nsequential decoder deals with new or unknown values in the conversation, while\nthe multi-label decoder combined with the sequential decoder ensures the\nexplicit assignment of values to slots. On the generation part, slot binary\nclassifiers are used to improve performance. This architecture is scalable to\nreal-world scenarios and is shown through an empirical evaluation to achieve\nstate-of-the-art performance on both the Cambridge Restaurant dataset and the\nStanford in-car assistant dataset\\footnote{The code is available at\n\\url{https://github.com/uber-research/FSDM}}\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2019 23:56:25 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Shu", "Lei", ""], ["Molino", "Piero", ""], ["Namazifar", "Mahdi", ""], ["Xu", "Hu", ""], ["Liu", "Bing", ""], ["Zheng", "Huaixiu", ""], ["Tur", "Gokhan", ""]]}, {"id": "1908.02409", "submitter": "Anhong Guo", "authors": "Anhong Guo, Ilter Canberk, Hannah Murphy, Andr\\'es Monroy-Hern\\'andez,\n  Rajan Vaish", "title": "Blocks: Collaborative and Persistent Augmented Reality Experiences", "comments": "ACM UbiComp 2019", "journal-ref": null, "doi": "10.1145/3351241", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Blocks, a mobile application that enables people to co-create AR\nstructures that persist in the physical environment. Using Blocks, end users\ncan collaborate synchronously or asynchronously, whether they are colocated or\nremote. Additionally, the AR structures can be tied to a physical location or\ncan be accessed from anywhere. We evaluated how people used Blocks through a\nseries of lab and field deployment studies with over 160 participants, and\nexplored the interplay between two collaborative dimensions: space and time. We\nfound that participants preferred creating structures synchronously with\ncolocated collaborators. Additionally, they were most active when they created\nstructures that were not restricted by time or place. Unlike most of today's AR\nexperiences, which focus on content consumption, this work outlines new design\nopportunities for persistent and collaborative AR experiences that empower\nanyone to collaborate and create AR content.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 00:49:35 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 17:53:37 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Guo", "Anhong", ""], ["Canberk", "Ilter", ""], ["Murphy", "Hannah", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""], ["Vaish", "Rajan", ""]]}, {"id": "1908.02412", "submitter": "Bin Guo", "authors": "Bin Guo, Huihui Chen, Yan Liu, Chao Chen, Qi Han, Zhiwen Yu", "title": "From Crowdsourcing to Crowdmining: Using Implicit Human Intelligence for\n  Better Understanding of Crowdsourced Data", "comments": "12 pages, accepted by World Wide Web Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of mobile social networks, more and more crowdsourced\ndata are generated on the Web or collected from real-world sensing. The\nfragment, heterogeneous, and noisy nature of online/offline crowdsourced data,\nhowever, makes it difficult to be understood. Traditional content-based\nanalyzing methods suffer from potential issues such as computational\nintensiveness and poor performance. To address them, this paper presents\nCrowdMining. In particular, we observe that the knowledge hidden in the process\nof data generation, regarding individual/crowd behavior patterns (e.g.,\nmobility patterns, community contexts such as social ties and structure) and\ncrowd-object interaction patterns (flickering or tweeting patterns) are\nneglected in crowdsourced data mining. Therefore, a novel approach that\nleverages implicit human intelligence (implicit HI) for crowdsourced data\nmining and understanding is proposed. Two studies titled CrowdEvent and\nCrowdRoute are presented to showcase its usage, where implicit HIs are\nextracted either from online or offline crowdsourced data. A generic model for\nCrowdMining is further proposed based on a set of existing studies. Experiments\nbased on real-world datasets demonstrate the effectiveness of CrowdMining.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 01:09:17 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Guo", "Bin", ""], ["Chen", "Huihui", ""], ["Liu", "Yan", ""], ["Chen", "Chao", ""], ["Han", "Qi", ""], ["Yu", "Zhiwen", ""]]}, {"id": "1908.02432", "submitter": "Roman Ibrahimov", "authors": "Roman Ibrahimov, Evgeny Tsykunov, Vladimir Shirokun, Andrey Somov, and\n  Dzmitry Tsetserukou", "title": "DronePick: Object Picking and Delivery Teleoperation with the Drone\n  Controlled by a Wearable Tactile Display", "comments": "Accepted to the 28th IEEE International Conference on Robot and Human\n  Interactive Communication (RO-MAN 2019), IEEE copyright, 6 pages, 6 figures,\n  2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on the teleoperation system DronePick which provides remote object\npicking and delivery by a human-controlled quadcopter. The main novelty of the\nproposed system is that the human user continuously gets the visual and haptic\nfeedback for accurate teleoperation. DronePick consists of a quadcopter\nequipped with a magnetic grabber, a tactile glove with finger motion tracking\nsensor, hand tracking system, and the Virtual Reality (VR) application. The\nhuman operator teleoperates the quadcopter by changing the position of the\nhand. The proposed vibrotactile patterns representing the location of the\nremote object relative to the quadcopter are delivered to the glove. It helps\nthe operator to determine when the quadcopter is right above the object. When\nthe \"pick\" command is sent by clasping the hand in the glove, the quadcopter\ndecreases its altitude and the magnetic grabber attaches the target object. The\nwhole scenario is in parallel simulated in VR. The air flow from the quadcopter\nand the relative positions of VR objects help the operator to determine the\nexact position of the delivered object to be picked. The experiments showed\nthat the vibrotactile patterns were recognized by the users at the high\nrecognition rates: the average 99% recognition rate and the average 2.36s\nrecognition time. The real-life implementation of DronePick featuring object\npicking and delivering to the human was developed and tested.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 03:49:46 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Ibrahimov", "Roman", ""], ["Tsykunov", "Evgeny", ""], ["Shirokun", "Vladimir", ""], ["Somov", "Andrey", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "1908.02502", "submitter": "Danqing Shi", "authors": "Danqing Shi, Yang Shi, Xinyue Xu, Nan Chen, Siwei Fu, Hongjin Wu, Nan\n  Cao", "title": "Task-Oriented Optimal Sequencing of Visualization Charts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A chart sequence is used to describe a series of visualization charts\ngenerated in the exploratory analysis by data analysts. It provides information\ndetails in each chart as well as a logical relationship among charts. While\nexisting research targets on generating chart sequences that match human's\nperceptions, little attention has been paid to formulate task-oriented\nconnections between charts in a chart design space. We present a novel chart\nsequencing method based on reinforcement learning to capture the connections\nbetween charts in the context of three major analysis tasks, including\ncorrelation analysis, anomaly detection, and cluster analysis. The proposed\nmethod formulates a chart sequencing procedure as an optimization problem,\nwhich seeks an optimal policy to sequencing charts for the specific analysis\ntask. In our method, a novel reward function is introduced, which takes both\nthe analysis task and the factor of human cognition into consideration. We\nconducted one case study and two user studies to evaluate the effectiveness of\nour method under the application scenarios of visualization demonstration,\nsequencing charts for reasoning analysis results, and making a chart design\nchoice. The study results showed the power of our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 09:43:09 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Shi", "Danqing", ""], ["Shi", "Yang", ""], ["Xu", "Xinyue", ""], ["Chen", "Nan", ""], ["Fu", "Siwei", ""], ["Wu", "Hongjin", ""], ["Cao", "Nan", ""]]}, {"id": "1908.02548", "submitter": "Will Nash", "authors": "W.T. Nash, C.J. Powell, T. Drummond, N. Birbilis", "title": "Automated Corrosion Detection Using Crowd Sourced Training for Deep\n  Learning", "comments": "presubmission, computer vision, deep learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automated detection of corrosion from images (i.e., photographs) or video\n(i.e., drone footage) presents significant advantages in terms of corrosion\nmonitoring. Such advantages include access to remote locations, mitigation of\nrisk to inspectors, cost savings and monitoring speed. The automated detection\nof corrosion requires deep learning to approach human level artificial\nintelligence (A.I.). The training of a deep learning model requires intensive\nimage labelling, and in order to generate a large database of labelled images,\ncrowd sourced labelling via a dedicated website was sought. The website\n(corrosiondetector.com) permits any user to label images, with such labelling\nthen contributing to the training of a cloud based A.I. model - with such a\ncloud-based model then capable of assessing any fresh (or uploaded) image for\nthe presence of corrosion. In other words, the website includes both the crowd\nsourced training process, but also the end use of the evolving model. Herein,\nthe results and findings from the website (corrosiondetector.com) over the\nperiod of approximately one month, are reported.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 00:44:15 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Nash", "W. T.", ""], ["Powell", "C. J.", ""], ["Drummond", "T.", ""], ["Birbilis", "N.", ""]]}, {"id": "1908.02588", "submitter": "Luke Snyder", "authors": "Luke S. Snyder, Yi-Shan Lin, Morteza Karimzadeh, Dan Goldwasser, and\n  David S. Ebert", "title": "Interactive Learning for Identifying Relevant Tweets to Support\n  Real-time Situational Awareness", "comments": "12 pages, 8 figures, 3 tables, IEEE VIS VAST 2019, TVCG", "journal-ref": null, "doi": "10.1109/TVCG.2019.2934614", "report-no": null, "categories": "cs.SI cs.CL cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various domain users are increasingly leveraging real-time social media data\nto gain rapid situational awareness. However, due to the high noise in the\ndeluge of data, effectively determining semantically relevant information can\nbe difficult, further complicated by the changing definition of relevancy by\neach end user for different events. The majority of existing methods for short\ntext relevance classification fail to incorporate users' knowledge into the\nclassification process. Existing methods that incorporate interactive user\nfeedback focus on historical datasets. Therefore, classifiers cannot be\ninteractively retrained for specific events or user-dependent needs in\nreal-time. This limits real-time situational awareness, as streaming data that\nis incorrectly classified cannot be corrected immediately, permitting the\npossibility for important incoming data to be incorrectly classified as well.\nWe present a novel interactive learning framework to improve the classification\nprocess in which the user iteratively corrects the relevancy of tweets in\nreal-time to train the classification model on-the-fly for immediate predictive\nimprovements. We computationally evaluate our classification model adapted to\nlearn at interactive rates. Our results show that our approach outperforms\nstate-of-the-art machine learning models. In addition, we integrate our\nframework with the extended Social Media Analytics and Reporting Toolkit\n(SMART) 2.0 system, allowing the use of our interactive learning framework\nwithin a visual analytics system tailored for real-time situational awareness.\nTo demonstrate our framework's effectiveness, we provide domain expert feedback\nfrom first responders who used the extended SMART 2.0 system.\n", "versions": [{"version": "v1", "created": "Thu, 1 Aug 2019 09:01:19 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 19:11:52 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Snyder", "Luke S.", ""], ["Lin", "Yi-Shan", ""], ["Karimzadeh", "Morteza", ""], ["Goldwasser", "Dan", ""], ["Ebert", "David S.", ""]]}, {"id": "1908.02627", "submitter": "Fabian Sperrle", "authors": "Fabian Sperrle, J\\\"urgen Bernard, Michael Sedlmair, Daniel Keim,\n  Mennatallah El-Assady", "title": "Speculative Execution for Guided Visual Analytics", "comments": null, "journal-ref": "Machine Learning from User Interactions for Visualization and\n  Analytics: An IEEE VIS 2018 workshop", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the concept of Speculative Execution for Visual Analytics and\ndiscuss its effectiveness for model exploration and optimization. Speculative\nExecution enables the automatic generation of alternative, competing model\nconfigurations that do not alter the current model state unless explicitly\nconfirmed by the user. These alternatives are computed based on either user\ninteractions or model quality measures and can be explored using\ndelta-visualizations. By automatically proposing modeling alternatives, systems\nemploying Speculative Execution can shorten the gap between users and models,\nreduce the confirmation bias and speed up optimization processes. In this\npaper, we have assembled five application scenarios showcasing the potential of\nSpeculative Execution, as well as a potential for further research.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 13:31:37 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Sperrle", "Fabian", ""], ["Bernard", "J\u00fcrgen", ""], ["Sedlmair", "Michael", ""], ["Keim", "Daniel", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "1908.02941", "submitter": "Vincent Falconieri", "authors": "Vincent Falconieri", "title": "VisJSClassificator -- Manual Visual Collaborative Classification\n  Graph-based Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysts need to classify, search and correlate numerous images. Automatic\nclassification tools improve the efficiency of such tasks. However, classified\ndata is a prerequisite to develop these tools. Labelling tools are of great use\nin case of already known classes, but seemed limited for Open Set\nClassification. This paper presents a manual and collaborative classification\ntool, which uses graph representation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 06:05:49 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Falconieri", "Vincent", ""]]}, {"id": "1908.02949", "submitter": "Patrick Stotko", "authors": "Patrick Stotko, Stefan Krumpen, Max Schwarz, Christian Lenz, Sven\n  Behnke, Reinhard Klein, Michael Weinmann", "title": "A VR System for Immersive Teleoperation and Live Exploration with a\n  Mobile Robot", "comments": "The final version of record is available at\n  http://dx.doi.org/10.1109/IROS40897.2019.8968598", "journal-ref": "In proceedings of IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS), pages 3630-3637, 2019", "doi": "10.1109/IROS40897.2019.8968598", "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications like disaster management and industrial inspection often require\nexperts to enter contaminated places. To circumvent the need for physical\npresence, it is desirable to generate a fully immersive individual live\nteleoperation experience. However, standard video-based approaches suffer from\na limited degree of immersion and situation awareness due to the restriction to\nthe camera view, which impacts the navigation. In this paper, we present a\nnovel VR-based practical system for immersive robot teleoperation and scene\nexploration. While being operated through the scene, a robot captures RGB-D\ndata that is streamed to a SLAM-based live multi-client telepresence system.\nHere, a global 3D model of the already captured scene parts is reconstructed\nand streamed to the individual remote user clients where the rendering for e.g.\nhead-mounted display devices (HMDs) is performed. We introduce a novel\nlightweight robot client component which transmits robot-specific data and\nenables a quick integration into existing robotic systems. This way, in\ncontrast to first-person exploration systems, the operators can explore and\nnavigate in the remote site completely independent of the current position and\nview of the capturing robot, complementing traditional input devices for\nteleoperation. We provide a proof-of-concept implementation and demonstrate the\ncapabilities as well as the performance of our system regarding interactive\nobject measurements and bandwidth-efficient data streaming and visualization.\nFurthermore, we show its benefits over purely video-based teleoperation in a\nuser study revealing a higher degree of situation awareness and a more precise\nnavigation in challenging environments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 06:51:30 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 08:12:07 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Stotko", "Patrick", ""], ["Krumpen", "Stefan", ""], ["Schwarz", "Max", ""], ["Lenz", "Christian", ""], ["Behnke", "Sven", ""], ["Klein", "Reinhard", ""], ["Weinmann", "Michael", ""]]}, {"id": "1908.03118", "submitter": "Patrick Stotko", "authors": "Patrick Stotko, Stefan Krumpen, Michael Weinmann, Reinhard Klein", "title": "Efficient 3D Reconstruction and Streaming for Group-Scale Multi-Client\n  Live Telepresence", "comments": "The final version of record is available at\n  http://dx.doi.org/10.1109/ISMAR.2019.00018", "journal-ref": "In proceedings of IEEE International Symposium on Mixed and\n  Augmented Reality (ISMAR), pages 19-25, 2019", "doi": "10.1109/ISMAR.2019.00018", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharing live telepresence experiences for teleconferencing or remote\ncollaboration receives increasing interest with the recent progress in\ncapturing and AR/VR technology. Whereas impressive telepresence systems have\nbeen proposed on top of on-the-fly scene capture, data transmission and\nvisualization, these systems are restricted to the immersion of single or up to\na low number of users into the respective scenarios. In this paper, we direct\nour attention on immersing significantly larger groups of people into\nlive-captured scenes as required in education, entertainment or collaboration\nscenarios. For this purpose, rather than abandoning previous approaches, we\npresent a range of optimizations of the involved reconstruction and streaming\ncomponents that allow the immersion of a group of more than 24 users within the\nsame scene - which is about a factor of 6 higher than in previous work -\nwithout introducing further latency or changing the involved consumer hardware\nsetup. We demonstrate that our optimized system is capable of generating\nhigh-quality scene reconstructions as well as providing an immersive viewing\nexperience to a large group of people within these live-captured scenes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2019 15:27:10 GMT"}, {"version": "v2", "created": "Mon, 13 Jan 2020 10:39:45 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Stotko", "Patrick", ""], ["Krumpen", "Stefan", ""], ["Weinmann", "Michael", ""], ["Klein", "Reinhard", ""]]}, {"id": "1908.03380", "submitter": "Jie Jiang", "authors": "Jie Jiang, Riccardo Pozza, Nigel Gilbert, Klaus Moessner", "title": "MakeSense: An IoT Testbed for Social Research of Indoor Activities", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increasing interest in deploying IoT devices to study human\nbehaviour in locations such as homes and offices. Such devices can be deployed\nin a laboratory or `in the wild' in natural environments. The latter allows one\nto collect behavioural data that is not contaminated by the artificiality of a\nlaboratory experiment. Using IoT devices in ordinary environments also brings\nthe benefits of reduced cost, as compared with lab experiments, and less\ndisturbance to the participants' daily routines which in turn helps with\nrecruiting them into the research. However, in this case, it is essential to\nhave an IoT infrastructure that can be easily and swiftly installed and from\nwhich real-time data can be securely and straightforwardly collected. In this\npaper, we present MakeSense, an IoT testbed that enables real-world\nexperimentation for large scale social research on indoor activities through\nreal-time monitoring and/or situation-aware applications. The testbed features\nquick setup, flexibility in deployment, the integration of a range of IoT\ndevices, resilience, and scalability. We also present two case studies to\ndemonstrate the use of the testbed, one in homes and one in offices.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 09:16:18 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Jiang", "Jie", ""], ["Pozza", "Riccardo", ""], ["Gilbert", "Nigel", ""], ["Moessner", "Klaus", ""]]}, {"id": "1908.03503", "submitter": "Johanna Johansen Ms", "authors": "Johanna Johansen and Simone Fischer-H\\\"ubner", "title": "Making GDPR Usable: A Model to Support Usability Evaluations of Privacy", "comments": "41 pages, 2 figures, 1 table, and appendixes", "journal-ref": "In IFIP Advances in Information and Communication Technology, vol\n  576. Springer (2020)", "doi": "10.1007/978-3-030-42504-3_18", "report-no": null, "categories": "cs.HC cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce a new model for evaluating privacy that builds on the criteria\nproposed by the EuroPriSe certification scheme by adding usability criteria.\nOur model is visually represented through a cube, called Usable Privacy Cube\n(or UP Cube), where each of its three axes of variability captures,\nrespectively: rights of the data subjects, privacy principles, and usable\nprivacy criteria. We slightly reorganize the criteria of EuroPriSe to fit with\nthe UP Cube model, i.e., we show how EuroPriSe can be viewed as a combination\nof only rights and principles, forming the two axes at the basis of our UP\nCube. In this way we also want to bring out two perspectives on privacy: that\nof the data subjects and, respectively, that of the controllers/processors. We\ndefine usable privacy criteria based on usability goals that we have extracted\nfrom the whole text of the General Data Protection Regulation. The criteria are\ndesigned to produce measurements of the level of usability with which the goals\nare reached. Precisely, we measure effectiveness, efficiency, and satisfaction,\nconsidering both the objective and the perceived usability outcomes, producing\nmeasures of accuracy and completeness, of resource utilization (e.g., time,\neffort, financial), and measures resulting from satisfaction scales. In the\nlong run, the UP Cube is meant to be the model behind a new certification\nmethodology capable of evaluating the usability of privacy, to the benefit of\ncommon users. For industries, considering also the usability of privacy would\nallow for greater business differentiation, beyond GDPR compliance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 15:46:15 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 08:52:37 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 22:23:14 GMT"}, {"version": "v4", "created": "Wed, 19 Feb 2020 14:05:25 GMT"}, {"version": "v5", "created": "Sun, 17 May 2020 15:09:19 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Johansen", "Johanna", ""], ["Fischer-H\u00fcbner", "Simone", ""]]}, {"id": "1908.03518", "submitter": "Mutasem Alsmadi Khalil", "authors": "Ibrahim Almarashdeh, Mutasem K. Alsmadi, Tamer Farag, Abdullah S.\n  Albahussain, Usama A Badawi, Njoud Altuwaijri, Hala Almaimoni, Fatima Asiry,\n  Shahad Alowaid, Muneerah Alshabanah, Daniah Alrajhi, Amirah Al Fraihet,\n  Ghaith Jaradat", "title": "Real-Time Elderly Healthcare Monitoring Expert System Using Wireless\n  Sensor Network", "comments": "7 pages,5 figures", "journal-ref": "International Journal of Applied Engineering Research 2018", "doi": null, "report-no": null, "categories": "cs.HC cs.CY eess.SP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Elderly chronic diseases are the main cause of death in the world, accounting\n60% of all death. Because elderly with chronic diseases at the early stages has\nno observed symptoms, and then symptoms starts to appear, it is critical to\nobserve the symptoms as early as possible to avoid any complication. This paper\npresents an expert system for an Elderly Health Care (EHC) at elderly home\ntailored for the specific needs of Elderly. The proposed EHC aims to develop an\nintegrated and multidisciplinary method to employ communication technologies\nand information for covering real health needs of elderly people, mainly of\npeople at high risk due to social and geographic isolation in addition to\nspecific chronic diseases. The proposed EHC provides personalized intervention\nplans covering chronic diseases such as (body temperature (BT), blood pressure\n(BP), and Heart beat rate (HR)). The processes and architecture of the proposed\nEHC are based on the server side and three main clients, one for the elderly\nand another two for the nurse and the physicians whom take care of them. The\nproposed EHC model is discussed for proving the usefulness and effectiveness of\nthe expert system.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 08:57:01 GMT"}], "update_date": "2019-08-12", "authors_parsed": [["Almarashdeh", "Ibrahim", ""], ["Alsmadi", "Mutasem K.", ""], ["Farag", "Tamer", ""], ["Albahussain", "Abdullah S.", ""], ["Badawi", "Usama A", ""], ["Altuwaijri", "Njoud", ""], ["Almaimoni", "Hala", ""], ["Asiry", "Fatima", ""], ["Alowaid", "Shahad", ""], ["Alshabanah", "Muneerah", ""], ["Alrajhi", "Daniah", ""], ["Fraihet", "Amirah Al", ""], ["Jaradat", "Ghaith", ""]]}, {"id": "1908.03591", "submitter": "Sebastian Cmentowski", "authors": "Sebastian Cmentowski, Andrey Krekhov, Ann-Marie M\\\"uller, Jens\n  Kr\\\"uger", "title": "Toward a Taxonomy of Inventory Systems for Virtual Reality Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality (VR) games are gradually becoming more elaborated and\nfeature-rich, but fail to reach the complexity of traditional digital games.\nOne common feature that is used to extend and organize complex gameplay is the\nin-game inventory, which allows players to obtain and carry new tools and items\nthroughout their journey. However, VR imposes additional requirements and\nchallenges that impede the implementation of this important feature and hinder\ngames to unleash their full potential. Our current work focuses on the design\nspace of inventories in VR games. We introduce this sparsely researched topic\nby constructing a first taxonomy of the underlying design considerations and\nbuilding blocks. Furthermore, we present three different inventories that were\ndesigned using our taxonomy and evaluate them in an early qualitative study.\nThe results underline the importance of our research and reveal promising\ninsights that show the huge potential for VR games.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 18:35:00 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Cmentowski", "Sebastian", ""], ["Krekhov", "Andrey", ""], ["M\u00fcller", "Ann-Marie", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "1908.03628", "submitter": "Yash Mehta", "authors": "Yash Mehta, Navonil Majumder, Alexander Gelbukh, Erik Cambria", "title": "Recent Trends in Deep Learning Based Personality Detection", "comments": null, "journal-ref": "Artif Intell Rev 53 (2020) 2313-2339", "doi": "10.1007/s10462-019-09770-z", "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the automatic prediction of personality traits has received a lot\nof attention. Specifically, personality trait prediction from multimodal data\nhas emerged as a hot topic within the field of affective computing. In this\npaper, we review significant machine learning models which have been employed\nfor personality detection, with an emphasis on deep learning-based methods.\nThis review paper provides an overview of the most popular approaches to\nautomated personality detection, various computational datasets, its industrial\napplications, and state-of-the-art machine learning models for personality\ndetection with specific focus on multimodal approaches. Personality detection\nis a very broad and diverse topic: this survey only focuses on computational\napproaches and leaves out psychological studies on personality detection.\n", "versions": [{"version": "v1", "created": "Wed, 7 Aug 2019 19:16:50 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 19:46:57 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Mehta", "Yash", ""], ["Majumder", "Navonil", ""], ["Gelbukh", "Alexander", ""], ["Cambria", "Erik", ""]]}, {"id": "1908.04087", "submitter": "Elena Sibirtseva", "authors": "Yuan Gao, Elena Sibirtseva, Ginevra Castellano and Danica Kragic", "title": "Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in\n  Human-Robot Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In socially assistive robotics, an important research area is the development\nof adaptation techniques and their effect on human-robot interaction. We\npresent a meta-learning based policy gradient method for addressing the problem\nof adaptation in human-robot interaction and also investigate its role as a\nmechanism for trust modelling. By building an escape room scenario in mixed\nreality with a robot, we test our hypothesis that bi-directional trust can be\ninfluenced by different adaptation algorithms. We found that our proposed model\nincreased the perceived trustworthiness of the robot and influenced the\ndynamics of gaining human's trust. Additionally, participants evaluated that\nthe robot perceived them as more trustworthy during the interactions with the\nmeta-learning based adaptation compared to the previously studied statistical\nadaptation model.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 11:06:28 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Gao", "Yuan", ""], ["Sibirtseva", "Elena", ""], ["Castellano", "Ginevra", ""], ["Kragic", "Danica", ""]]}, {"id": "1908.04088", "submitter": "Francesco Osborne", "authors": "Andrea Mannocci, Francesco Osborne, Enrico Motta", "title": "The Evolution of IJHCS and CHI: A Quantitative Analysis", "comments": null, "journal-ref": "International Journal of Human-Computer Studies 2019", "doi": "10.1016/j.ijhcs.2019.05.009", "report-no": null, "categories": "cs.HC cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on the International Journal of Human-Computer Studies\n(IJHCS) as a domain of analysis, to gain insights about its evolution in the\npast 50 years and what this evolution tells us about the research landscape\nassociated with the journal. To this purpose we use techniques from the field\nof Science of Science and analyse the relevant scholarly data to identify a\nvariety of phenomena, including significant geopolitical patterns, the key\ntrends that emerge from a topic-centric analysis, and the insights that can be\ndrawn from an analysis of citation data. Because the area of Human-Computer\nInteraction (HCI) has always been a central focus for IJHCS, we also include in\nthe analysis the CHI conference, which is the premiere scientific venue in HCI.\nAnalysing both venues provides more data points to our study and allows us to\nconsider two alternative viewpoints on the evolution of HCI research.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 11:07:09 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Mannocci", "Andrea", ""], ["Osborne", "Francesco", ""], ["Motta", "Enrico", ""]]}, {"id": "1908.04237", "submitter": "Daniel Mutembesa", "authors": "Daniel Mutembesa, Christopher Omongo and Ernest Mwebaze", "title": "Crowdsourcing real-time viral disease and pest information. A case of\n  nation-wide cassava disease surveillance in a developing country", "comments": "9 pages, 5 figures, Proceedings of AAAI HCOMP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most developing countries, a huge proportion of the national food basket\nis supported by small subsistence agricultural systems. A major challenge to\nthese systems is disease and pest attacks which have a devastating effect on\nthe smallholder farmers that depend on these systems for their livelihoods. A\nkey component of any proposed solution is a good disease surveillance network.\nHowever, current surveillance efforts are unable to provide sufficient data for\nmonitoring such phenomena over a vast geographic area efficiently and\neffectively due to limited resources, both human and financial. Crowdsourcing\nwith farmer crowds that have access to mobile phones offers a viable option to\nprovide all year round real-time surveillance data on viral disease and pest\nincidence and severity. This work presents a mobile ad hoc surveillance system\nfor monitoring viral diseases and pests in cassava. We present results from a\npilot in Uganda where this system was deployed for 76 weeks. We discuss the\nparticipation behaviours of the crowds with mobile smartphones as well as the\neffects of several incentives applied.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2019 07:46:38 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Mutembesa", "Daniel", ""], ["Omongo", "Christopher", ""], ["Mwebaze", "Ernest", ""]]}, {"id": "1908.04342", "submitter": "Nilavra Bhattacharya", "authors": "Nilavra Bhattacharya, Qing Li, Danna Gurari", "title": "Why Does a Visual Question Have Different Answers?", "comments": null, "journal-ref": "The IEEE International Conference on Computer Vision (ICCV) 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering is the task of returning the answer to a question\nabout an image. A challenge is that different people often provide different\nanswers to the same visual question. To our knowledge, this is the first work\nthat aims to understand why. We propose a taxonomy of nine plausible reasons,\nand create two labelled datasets consisting of ~45,000 visual questions\nindicating which reasons led to answer differences. We then propose a novel\nproblem of predicting directly from a visual question which reasons will cause\nanswer differences as well as a novel algorithm for this purpose. Experiments\ndemonstrate the advantage of our approach over several related baselines on two\ndiverse datasets. We publicly share the datasets and code at\nhttps://vizwiz.org.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 19:19:48 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 18:55:01 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Bhattacharya", "Nilavra", ""], ["Li", "Qing", ""], ["Gurari", "Danna", ""]]}, {"id": "1908.04617", "submitter": "Mohammed Khwaja", "authors": "Mohammed Khwaja, Sumer S. Vaid, Sara Zannone, Gabriella M. Harari, A.\n  Aldo Faisal, Aleksandar Matic", "title": "Modeling Personality vs. Modeling Personalidad: In-the-wild Mobile Data\n  Analysis in Five Countries Suggests Cultural Impact on Personality Models", "comments": "Presented at Ubicomp 2019. London, UK. September 2019", "journal-ref": "Proceedings of the ACM on Interactive, Mobile, Wearable and\n  Ubiquitous Technologies (IMWUT) Vol 3 Issue 3. 2019", "doi": "10.1145/3351246", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor data collected from smartphones provides the possibility to passively\ninfer a user's personality traits. Such models can be used to enable technology\npersonalization, while contributing to our substantive understanding of how\nhuman behavior manifests in daily life. A significant challenge in personality\nmodeling involves improving the accuracy of personality inferences, however,\nresearch has yet to assess and consider the cultural impact of users' country\nof residence on model replicability. We collected mobile sensing data and\nself-reported Big Five traits from 166 participants (54 women and 112 men)\nrecruited in five different countries (UK, Spain, Colombia, Peru, and Chile)\nfor 3 weeks. We developed machine learning based personality models using\nculturally diverse datasets -- representing different countries -- and we show\nthat such models can achieve state-of-the-art accuracy when tested in new\ncountries, ranging from 63% (Agreeableness) to 71% (Extraversion) of\nclassification accuracy. Our results indicate that using country-specific\ndatasets can improve the classification accuracy between 3% and 7% for\nExtraversion, Agreeableness, and Conscientiousness. We show that these findings\nhold regardless of gender and age balance in the dataset. Interestingly, using\ngender- or age- balanced datasets as well as gender-separated datasets improve\ntrait prediction by up to 17%. We unpack differences in personality models\nacross the five countries, highlight the most predictive data categories\n(location, noise, unlocks, accelerometer), and provide takeaways to\ntechnologists and social scientists interested in passive personality\nassessment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 12:56:57 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Khwaja", "Mohammed", ""], ["Vaid", "Sumer S.", ""], ["Zannone", "Sara", ""], ["Harari", "Gabriella M.", ""], ["Faisal", "A. Aldo", ""], ["Matic", "Aleksandar", ""]]}, {"id": "1908.04629", "submitter": "Tiago Machado", "authors": "Tiago Machado, Daniel Gopstein, Oded Nov, Angela Wang, Andy Nealen,\n  Julian Togelius", "title": "Evaluation of a Recommender System for Assisting Novice Game Designers", "comments": "The 15th AAAI Conference on Artificial Intelligence and Interactive\n  Digital Entertainment (AIIDE 19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game development is a complex task involving multiple disciplines and\ntechnologies. Developers and researchers alike have suggested that AI-driven\ngame design assistants may improve developer workflow. We present a recommender\nsystem for assisting humans in game design as well as a rigorous human subjects\nstudy to validate it. The AI-driven game design assistance system suggests game\nmechanics to designers based on characteristics of the game being developed. We\nbelieve this method can bring creative insights and increase users'\nproductivity. We conducted quantitative studies that showed the recommender\nsystem increases users' levels of accuracy and computational affect, and\ndecreases their levels of workload.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 13:22:51 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Machado", "Tiago", ""], ["Gopstein", "Daniel", ""], ["Nov", "Oded", ""], ["Wang", "Angela", ""], ["Nealen", "Andy", ""], ["Togelius", "Julian", ""]]}, {"id": "1908.04692", "submitter": "David Puljiz", "authors": "David Puljiz, Erik St\\\"ohr, Katharina S. Riesterer, Bj\\\"orn Hein,\n  Torsten Kr\\\"oger", "title": "General Hand Guidance Framework using Microsoft HoloLens", "comments": "As accepted to IROS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hand guidance emerged from the safety requirements for collaborative robots,\nnamely possessing joint-torque sensors. Since then it has proven to be a\npowerful tool for easy trajectory programming, allowing lay-users to reprogram\nrobots intuitively. Going beyond, a robot can learn tasks by user\ndemonstrations through kinesthetic teaching, enabling robots to generalise\ntasks and further reducing the need for reprogramming. However, hand guidance\nis still mostly relegated to collaborative robots. Here we propose a method\nthat doesn't require any sensors on the robot or in the robot cell, by using a\nMicrosoft HoloLens augmented reality head mounted display. We reference the\nrobot using a registration algorithm to match the robot model to the spatial\nmesh. The in-built hand tracking and localisation capabilities are then used to\ncalculate the position of the hands relative to the robot. By decomposing the\nhand movements into orthogonal rotations and propagating it down through the\nkinematic chain, we achieve a generalised hand guidance without the need to\nbuild a dynamic model of the robot itself. We tested our approach on a commonly\nused industrial manipulator, the KUKA KR-5.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 15:05:42 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Puljiz", "David", ""], ["St\u00f6hr", "Erik", ""], ["Riesterer", "Katharina S.", ""], ["Hein", "Bj\u00f6rn", ""], ["Kr\u00f6ger", "Torsten", ""]]}, {"id": "1908.04832", "submitter": "Kevin Bowden", "authors": "Kevin K. Bowden, Jiaqi Wu, Wen Cui, Juraj Juraska, Vrindavan Harrison,\n  Brian Schwarzmann, Nicholas Santer, Steve Whittaker, Marilyn Walker", "title": "Entertaining and Opinionated but Too Controlling: A Large-Scale User\n  Study of an Open Domain Alexa Prize System", "comments": "To appear in 1st International Conference on Conversational User\n  Interfaces (CUI 2019)", "journal-ref": null, "doi": "10.1145/3342775.3342792", "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational systems typically focus on functional tasks such as scheduling\nappointments or creating todo lists. Instead we design and evaluate SlugBot\n(SB), one of 8 semifinalists in the 2018 AlexaPrize, whose goal is to support\ncasual open-domain social inter-action. This novel application requires both\nbroad topic coverage and engaging interactive skills. We developed a new\ntechnical approach to meet this demanding situation by crowd-sourcing novel\ncontent and introducing playful conversational strategies based on storytelling\nand games. We collected over 10,000 conversations during August 2018 as part of\nthe Alexa Prize competition. We also conducted an in-lab follow-up qualitative\nevaluation. Over-all users found SB moderately engaging; conversations averaged\n3.6 minutes and involved 26 user turns. However, users reacted very differently\nto different conversation subtypes. Storytelling and games were evaluated\npositively; these were seen as entertaining with predictable interactive\nstructure. They also led users to impute personality and intelligence to SB. In\ncontrast, search and general Chit-Chat induced coverage problems; here users\nfound it hard to infer what topics SB could understand, with these\nconversations seen as being too system-driven. Theoretical and design\nimplications suggest a move away from conversational systems that simply\nprovide factual information. Future systems should be designed to have their\nown opinions with personal stories to share, and SB provides an example of how\nwe might achieve this.\n", "versions": [{"version": "v1", "created": "Tue, 13 Aug 2019 19:10:36 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Bowden", "Kevin K.", ""], ["Wu", "Jiaqi", ""], ["Cui", "Wen", ""], ["Juraska", "Juraj", ""], ["Harrison", "Vrindavan", ""], ["Schwarzmann", "Brian", ""], ["Santer", "Nicholas", ""], ["Whittaker", "Steve", ""], ["Walker", "Marilyn", ""]]}, {"id": "1908.04955", "submitter": "Joseph Campbell", "authors": "Joseph Campbell, Simon Stepputtis, Heni Ben Amor", "title": "Probabilistic Multimodal Modeling for Human-Robot Interaction Tasks", "comments": "Project website:\n  http://interactive-robotics.engineering.asu.edu/interaction-primitives\n  Accompanying video: https://youtu.be/r5AqfxTDfLA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-robot interaction benefits greatly from multimodal sensor inputs as\nthey enable increased robustness and generalization accuracy. Despite this\nobservation, few HRI methods are capable of efficiently performing inference\nfor multimodal systems. In this work, we introduce a reformulation of\nInteraction Primitives which allows for learning from demonstration of\ninteraction tasks, while also gracefully handling nonlinearities inherent to\nmultimodal inference in such scenarios. We also empirically show that our\nmethod results in more accurate, more robust, and faster inference than\nstandard Interaction Primitives and other common methods in challenging HRI\nscenarios.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 04:58:20 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Campbell", "Joseph", ""], ["Stepputtis", "Simon", ""], ["Amor", "Heni Ben", ""]]}, {"id": "1908.05108", "submitter": "Xiang Zhang", "authors": "Yu Gu, Xiang Zhang, Zhi Liu and Fuji Ren", "title": "WiFi-based Real-time Breathing and Heart Rate Monitoring during Sleep", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good quality sleep is essential for good health and sleep monitoring becomes\na vital research topic. This paper provides a low cost, continuous and\ncontactless WiFi-based vital signs (breathing and heart rate) monitoring\nmethod. In particular, we set up the antennas based on Fresnel diffraction\nmodel and signal propagation theory, which enhances the detection of weak\nbreathing/heartbeat motion. We implement a prototype system using the off-shelf\ndevices and a real-time processing system to monitor vital signs in real time.\nThe experimental results indicate the accurate breathing rate and heart rate\ndetection performance. To the best of our knowledge, this is the first work to\nuse a pair of WiFi devices and omnidirectional antennas to achieve real-time\nindividual breathing rate and heart rate monitoring in different sleeping\npostures.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 13:08:14 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Gu", "Yu", ""], ["Zhang", "Xiang", ""], ["Liu", "Zhi", ""], ["Ren", "Fuji", ""]]}, {"id": "1908.05133", "submitter": "Houtan Jebelli", "authors": "Byungjoo Choi, Gaang Lee, Houtan Jebelli, SangHyun Lee", "title": "Assessing Workers Perceived Risk During Construction Task Using A\n  Wristband-Type Biosensor", "comments": null, "journal-ref": "Proceedings of the Creative Construction Conference (CCC 2019)", "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction industry has demonstrated a high frequency and severity of\naccidents. Construction accidents are the result of the interaction between\nunsafe work conditions and workers unsafe behaviors. Given this relation,\nperceived risk is determined by an individual response to a potential work\nhazard during the work. As such, risk perception is critical to understand\nworkers unsafe behaviors. Established methods of assessing workers perceived\nrisk have mainly relied on surveys and interviews. However, these post-hoc\nmethods, which are limited to monitoring dynamic changes in risk perception and\nconducting surveys at a construction site, may prove cumbersome to workers.\nAdditionally, these methods frequently suffer from self-reported bias. To\novercome the limitations of previous subjective measures, this study aims to\ndevelop a framework for the objective and continuous prediction of construction\nworkers perceived risk using physiological signals [e.g., electrodermal\nactivity (EDA)] acquired from workers wristband-type biosensors. To achieve\nthis objective, physiological signals were collected from eight construction\nworkers while they performed regular tasks in the field. Various filtering\nmethods were applied to exclude noises recorded in the signal and to extract\nvarious features of the signals as workers experienced different risk levels.\nThen, a supervised machine-learning model was trained to explore the\napplicability of the collected physiological signals for the prediction of risk\nperception. The results showed that features based on EDA data collected from\nwristbands are feasible and useful to the process of continuously monitoring\nworkers perceived risk during ongoing work. This study contributes to an\nin-depth understanding of construction workers perceived risk by developing a\nnoninvasive means of continuously monitoring workers perceived risk.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 14:02:05 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Choi", "Byungjoo", ""], ["Lee", "Gaang", ""], ["Jebelli", "Houtan", ""], ["Lee", "SangHyun", ""]]}, {"id": "1908.05188", "submitter": "Lukas Dominique Josef Fiederer", "authors": "Lukas D.J. Fiederer, Hisham Alwanni, Martin V\\\"olker, Oliver Schnell,\n  J\\\"urgen Beck, Tonio Ball", "title": "A Research Framework for Virtual Reality Neurosurgery Based on\n  Open-Source Tools", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully immersive virtual reality (VR) has the potential to improve\nneurosurgical planning. For example, it may offer 3D visualizations of relevant\nanatomical structures with complex shapes, such as blood vessels and tumors.\nHowever, there is a lack of research tools specifically tailored for this area.\nWe present a research framework for VR neurosurgery based on open-source tools\nand preliminary evaluation results. We showcase the potential of such a\nframework using clinical data of two patients and research data of one subject.\nAs a first step toward practical evaluations, two certified senior\nneurosurgeons positively assessed the usefulness of the VR visualizations using\nhead-mounted displays. The methods and findings described in our study thus\nprovide a foundation for research and development aiming at versatile and\nuser-friendly VR tools for improving neurosurgical planning and training.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 16:04:40 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Fiederer", "Lukas D. J.", ""], ["Alwanni", "Hisham", ""], ["V\u00f6lker", "Martin", ""], ["Schnell", "Oliver", ""], ["Beck", "J\u00fcrgen", ""], ["Ball", "Tonio", ""]]}, {"id": "1908.05203", "submitter": "Irawan Nurhas", "authors": "Irawan Nurhas, Stefan Geisler, Jan Pawlowski", "title": "Why Should the Q-method be Integrated Into the Design Science Research?\n  A Systematic Mapping Study", "comments": "10th Scandinavian Conference on Information Systems, Nokia, Finland;\n  Url: https://aisel.aisnet.org/scis2019/9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Q-method has been utilized over time in various areas, including\ninformation systems. In this study, we used a systematic mapping to illustrate\nhow the Q-method was applied within Information Systems (IS) community and\nproposing towards the integration of Q-method into the Design Sciences Research\n(DSR) process as a tool for future research DSR-based IS studies. In this\nmapping study, we collected peer-reviewed journals from Basket-of-Eight\njournals and the digital library of the Association for Information Systems\n(AIS). Then we grouped the publications according to the process of DSR, and\ndifferent variables for preparing Q-method from IS publications. We found that\nthe potential of the Q-methodology can be used to support each main research\nstage of DSR processes and can serve as the useful tool to evaluate a system in\nthe IS topic of system analysis and design\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 16:21:48 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 08:07:16 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Nurhas", "Irawan", ""], ["Geisler", "Stefan", ""], ["Pawlowski", "Jan", ""]]}, {"id": "1908.05240", "submitter": "Cesar Salas Rommel", "authors": "Cesar R. Salas Guerra", "title": "Epistemological approach in immersive virtual environments and the\n  neurophysiology learning process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Currently virtual reality (VR) usage in training processes is increasing due\nto their usefulness in the learning processes based on visual information\nempowered. The information in virtual environments is perceived by sight, sound\nand touch, but the relationship or impact that these stimuli can have on the\noscillatory activity of the brain such as the processing, propagation and\nsynchronization of information still needs to be established in relation to the\ncognitive load of attention. Therefore, this study seeks to identify the\nsuggested epistemological basis through literature review and current research\nagendas in the relationship that exists between the immersive virtual\nenvironment and the neurophysiology of learning processes by means of the\nanalysis of visual information. The suggested dimensional modeling of this\nresearch is composed by the theory of information processing which allows the\nincorporation of learning through stimuli with the use of attention, perception\nand storage by means of information management and the Kolb's learning model\nwhich defines the perception and processing of information as dimensions of\nlearning. Regarding to the neurophysiology of learning, the literature has\nestablished he links between the prefrontal cortex and working memory within\nthe process of information management. The challenges and advances discussed in\nthis research are based in the relationship between the identified constructs\n(Income Stimuli, Information Management and Cognitive Processing) and the\nestablishment of a research agenda on how to identify the necessary indicators\nto measure memory and attention in the virtual immersion environments.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2019 05:41:11 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Guerra", "Cesar R. Salas", ""]]}, {"id": "1908.05341", "submitter": "Ozge Yalcin N", "authors": "\\\"Ozge Nilay Yal\\c{c}{\\i}n", "title": "Evaluating Empathy in Artificial Agents", "comments": "This is a pre-print of an article accepted to ACII 2019 conference\n  (http://acii-conf.org/2019/). The final authenticated version will be\n  published on IEEExplore as proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The novel research area of computational empathy is in its infancy and moving\ntowards developing methods and standards. One major problem is the lack of\nagreement on the evaluation of empathy in artificial interactive systems. Even\nthough the existence of well-established methods from psychology, psychiatry\nand neuroscience, the translation between these methods and computational\nempathy is not straightforward. It requires a collective effort to develop\nmetrics that are more suitable for interactive artificial agents. This paper is\naimed as an attempt to initiate the dialogue on this important problem. We\nexamine the evaluation methods for empathy in humans and provide suggestions\nfor the development of better metrics to evaluate empathy in artificial agents.\nWe acknowledge the difficulty of arriving at a single solution in a vast\nvariety of interactive systems and propose a set of systematic approaches that\ncan be used with a variety of applications and systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 20:38:52 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Yal\u00e7\u0131n", "\u00d6zge Nilay", ""]]}, {"id": "1908.05409", "submitter": "Hancheng Cao", "authors": "Hancheng Cao, Zhilong Chen, Fengli Xu, Tao Wang, Yujian Xu, Lianglun\n  Zhang, Yong Li", "title": "When Your Friends Become Sellers: An Empirical Study of Social Commerce\n  Site Beidian", "comments": "This work has been accepted by the International AAAI Conference on\n  Web and Social Media (ICWSM) as a full paper and will appear in the\n  proceedings of ICWSM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past few years have witnessed the emergence and phenomenal success of\nstrong-tie based social commerce. Embedded in social networking sites, these\nE-Commerce platforms transform ordinary people into sellers, where they\nadvertise and sell products to their friends and family in online social\nnetworks. These sites can acquire millions of users within a short time, and\nare growing fast at an accelerated rate. However, little is known about how\nthese social commerce develop as a blend of social relationship and economic\ntransactions. In this paper we present the first measurement study on the\nfull-scale data of Beidian, one of the fastest growing social commerce sites in\nChina, which involves 11.8 million users. We first analyzed the topological\nstructure of the Beidian platform and highlighted its decentralized nature. We\nthen studied the site's rapid growth and its growth mechanism via invitation\ncascade. Finally, we investigated purchasing behavior on Beidian, where we\nfocused on user proximity and loyalty, which contributes to the site's high\nconversion rate. As the consequences of interactions between strong ties and\neconomic logics, emerging social commerce demonstrates significant property\ndeviations from all known social networks and E-Commerce in terms of network\nstructure, dynamics and user behavior. To the best of our knowledge, this work\nis the first quantitative study on the network characteristics and dynamics of\nemerging social commerce platforms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 03:51:17 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Cao", "Hancheng", ""], ["Chen", "Zhilong", ""], ["Xu", "Fengli", ""], ["Wang", "Tao", ""], ["Xu", "Yujian", ""], ["Zhang", "Lianglun", ""], ["Li", "Yong", ""]]}, {"id": "1908.05505", "submitter": "Kathryn McKeough", "authors": "Nicholas Ruta, Naoko Sawada, Katy McKeough, Michael Behrisch, Johanna\n  Beyer", "title": "SAX Navigator: Time Series Exploration through Hierarchical Clustering", "comments": "IEEE Vis 2019 Short Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing many long time series is challenging to do by hand. Clustering time\nseries enables data analysts to discover relevance between and anomalies among\nmultiple time series. However, even after reasonable clustering, analysts have\nto scrutinize correlations between clusters or similarities within a cluster.\nWe developed SAX Navigator, an interactive visualization tool, that allows\nusers to hierarchically explore global patterns as well as individual\nobservations across large collections of time series data. Our visualization\nprovides a unique way to navigate time series that involves a \"vocabulary of\npatterns\" developed by using a dimensionality reduction technique,Symbolic\nAggregate approXimation(SAX). With SAX, the time series data clusters\nefficiently and is quicker to query at scale. We demonstrate the ability of SAX\nNavigator to analyze patterns in large time series data based on three case\nstudies for an astronomy data set. We verify the usability of our system\nthrough a think-aloud study with an astronomy domain scientist.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 12:04:01 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Ruta", "Nicholas", ""], ["Sawada", "Naoko", ""], ["McKeough", "Katy", ""], ["Behrisch", "Michael", ""], ["Beyer", "Johanna", ""]]}, {"id": "1908.05897", "submitter": "Sanchari Das", "authors": "Sanchari Das, Andrew Kim, Zachary Tingle, and Christena Nippert-Eng", "title": "All About Phishing: Exploring User Research through a Systematic\n  Literature Review", "comments": "Proceedings of the Thirteenth International Symposium on Human\n  Aspects of Information Security & Assurance (HAISA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phishing is a well-known cybersecurity attack that has rapidly increased in\nrecent years. It poses legitimate risks to businesses, government agencies, and\nall users due to sensitive data breaches, subsequent financial and productivity\nlosses, and social and personal inconvenience. Often, these attacks use social\nengineering techniques to deceive end-users, indicating the importance of\nuser-focused studies to help prevent future attacks. We provide a detailed\noverview of phishing research that has focused on users by conducting a\nsystematic literature review of peer-reviewed academic papers published in ACM\nDigital Library. Although published work on phishing appears in this data set\nas early as 2004, we found that of the total number of papers on phishing (N =\n367) only 13.9% (n = 51) focus on users by employing user study methodologies\nsuch as interviews, surveys, and in-lab studies. Even within this small subset\nof papers, we note a striking lack of attention to reporting important\ninformation about methods and participants (e.g., the number and nature of\nparticipants), along with crucial recruitment biases in some of the research.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:07:30 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Das", "Sanchari", ""], ["Kim", "Andrew", ""], ["Tingle", "Zachary", ""], ["Nippert-Eng", "Christena", ""]]}, {"id": "1908.05901", "submitter": "Sanchari Das", "authors": "Sanchari Das, Bingxing Wang, Zachary Tingle, and L. Jean Camp", "title": "Evaluating User Perception of Multi-Factor Authentication: A Systematic\n  Review", "comments": "Proceedings of the Thirteenth International Symposium on Human\n  Aspects of Information Security & Assurance (HAISA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security vulnerabilities of traditional single factor authentication has\nbecome a major concern for security practitioners and researchers. To mitigate\nsingle point failures, new and technologically advanced Multi-Factor\nAuthentication (MFA) tools have been developed as security solutions. However,\nthe usability and adoption of such tools have raised concerns. An obvious\nsolution can be viewed as conducting user studies to create more user-friendly\nMFA tools. To learn more, we performed a systematic literature review of\nrecently published academic papers (N = 623) that primarily focused on MFA\ntechnologies. While majority of these papers (m = 300) proposed new MFA tools,\nonly 9.1% of papers performed any user evaluation research. Our meta-analysis\nof user focused studies (n = 57) showed that researchers found lower adoption\nrate to be inevitable for MFAs, while avoidance was pervasive among mandatory\nuse. Furthermore, we noted several reporting and methodological discrepancies\nin the user focused studies. We identified trends in participant recruitment\nthat is indicative of demographic biases.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:14:32 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Das", "Sanchari", ""], ["Wang", "Bingxing", ""], ["Tingle", "Zachary", ""], ["Camp", "L. Jean", ""]]}, {"id": "1908.05902", "submitter": "Sanchari Das", "authors": "Sanchari Das, Bingxing Wang, and L. Jean Camp", "title": "MFA is a Waste of Time! Understanding Negative Connotation Towards MFA\n  Applications via User Generated Content", "comments": "Proceedings of the Thirteenth International Symposium on Human\n  Aspects of Information Security & Assurance (HAISA 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional single-factor authentication possesses several critical security\nvulnerabilities due to single-point failure feature. Multi-factor\nauthentication (MFA), intends to enhance security by providing additional\nverification steps. However, in practical deployment, users often experience\ndissatisfaction while using MFA, which leads to non-adoption. In order to\nunderstand the current design and usability issues with MFA, we analyze\naggregated user generated comments (N = 12,500) about application-based MFA\ntools from major distributors, such as, Amazon, Google Play, Apple App Store,\nand others. While some users acknowledge the security benefits of MFA, majority\nof them still faced problems with initial configuration, system design\nunderstanding, limited device compatibility, and risk trade-offs leading to\nnon-adoption of MFA. Based on these results, we provide actionable\nrecommendations in technological design, initial training, and risk\ncommunication to improve the adoption and user experience of MFA.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:15:56 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Das", "Sanchari", ""], ["Wang", "Bingxing", ""], ["Camp", "L. Jean", ""]]}, {"id": "1908.05913", "submitter": "Jiyoung Lee", "authors": "Jiyoung Lee, Seungryong Kim, Sunok Kim, Jungin Park, Kwanghoon Sohn", "title": "Context-Aware Emotion Recognition Networks", "comments": "International Conference on Computer Vision (ICCV) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional techniques for emotion recognition have focused on the facial\nexpression analysis only, thus providing limited ability to encode context that\ncomprehensively represents the emotional responses. We present deep networks\nfor context-aware emotion recognition, called CAER-Net, that exploit not only\nhuman facial expression but also context information in a joint and boosting\nmanner. The key idea is to hide human faces in a visual scene and seek other\ncontexts based on an attention mechanism. Our networks consist of two\nsub-networks, including two-stream encoding networks to seperately extract the\nfeatures of face and context regions, and adaptive fusion networks to fuse such\nfeatures in an adaptive fashion. We also introduce a novel benchmark for\ncontext-aware emotion recognition, called CAER, that is more appropriate than\nexisting benchmarks both qualitatively and quantitatively. On several\nbenchmarks, CAER-Net proves the effect of context for emotion recognition. Our\ndataset is available at http://caer-dataset.github.io.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 09:59:15 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Lee", "Jiyoung", ""], ["Kim", "Seungryong", ""], ["Kim", "Sunok", ""], ["Park", "Jungin", ""], ["Sohn", "Kwanghoon", ""]]}, {"id": "1908.06046", "submitter": "Hyocheol Ro", "authors": "Hyocheol Ro, Yoon Jung Park, Tack-Don Han", "title": "A Projection-based Augmented Reality for Elderly People with Dementia", "comments": "Accepted paper in ACM Designing Interactive Systems (DIS) 2019\n  Workshop Design HCI Dementia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As aging societies grow, researchers are actively studying care systems\nconcerning the life and diseases of the elderly. Among these diseases, dementia\nmakes it difficult to maintain daily life due to the degradation of cognitive\nfunctioning, memory, and reasoning, as well as the ability to perform actions.\nMoreover, dementia does not have a perfect cure, though therapy and care can\nslow its onset and provide patients with physical and mental support. In this\npaper, we developed a projection-based augmented reality system robot that can\ncover 360 degrees of space. We also propose an application that supports\ncontinuous monitoring of dementia patients to address the difficulties they\nface in daily life. The system is also designed to provide therapy\napplications, such as entertainment and spatial art, to provide mental care\naids for the patients.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 16:27:51 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Ro", "Hyocheol", ""], ["Park", "Yoon Jung", ""], ["Han", "Tack-Don", ""]]}, {"id": "1908.06167", "submitter": "Os Keyes", "authors": "Os Keyes, Josephine Hoy, Margaret Drouhard", "title": "Human-Computer Insurrection: Notes on an Anarchist HCI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HCI community has worked to expand and improve our consideration of the\nsocietal implications of our work and our corresponding responsibilities.\nDespite this increased engagement, HCI continues to lack an explicitly\narticulated politic, which we argue re-inscribes and amplifies systemic\noppression. In this paper, we set out an explicit political vision of an HCI\ngrounded in emancipatory autonomy - an anarchist HCI, aimed at dismantling all\noppressive systems by mandating suspicion of and a reckoning with imbalanced\ndistributions of power. We outline some of the principles and accountability\nmechanisms that constitute an anarchist HCI. We offer a potential framework for\nradically reorienting the field towards creating prefigurative counterpower -\nsystems and spaces that exemplify the world we wish to see, as we go about\nbuilding the revolution in increment.\n", "versions": [{"version": "v1", "created": "Sat, 10 Aug 2019 17:51:54 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Keyes", "Os", ""], ["Hoy", "Josephine", ""], ["Drouhard", "Margaret", ""]]}, {"id": "1908.06171", "submitter": "Yantong Wang", "authors": "Yu Gu and Yantong Wang and Zhi Liu and Jun Liu and Jie Li", "title": "SleepGuardian: An RF-based Healthcare System Guarding Your Sleep from\n  Afar", "comments": "17 pages accepted by IEEE Network Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever accelerating process of urbanization urges more and more population\ninto the swelling cities. While city residents are enjoying an entertaining\nlife supported by advanced informatics techniques like 5G and cloud computing,\nthe same technologies have also gradually deprived their sleep, which is\ncrucial for their wellness. Therefore, sleep monitoring has drawn significant\nattention from both research and industry communities. In this article, we\nfirst review the sleep monitoring issue and point out three essential\nproperties of an ideal sleep healthcare system, i.e., realtime guarding,\nfine-grained logging, and cost-effectiveness. Based on the analysis, we present\nSleepGuardian, a Radio Frequence (RF) based sleep healthcare system leveraging\nsignal processing, edge computing and machine learning.SleepGuardian offers an\noffline sleep logging service and an online abnormality warning service. The\noffline service provides a fine-grained sleep log like timing and regularity of\nbed time, onset of sleep and night time awakenings. The online service keeps\nguarding the subject for any abnormal behaviors during sleep like intensive\nbody twitches and a sudden seizure attack. Once an abnormality happens,it will\nautomatically warn the designated contacts like a nearby emergency room or a\ncloseby relative.We prototype SleepGuardian with low-cost WiFi devices and\nevaluate it in real scenarios. Experimental results demonstrate that\nSleepGuardian is very effective.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 08:09:33 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Gu", "Yu", ""], ["Wang", "Yantong", ""], ["Liu", "Zhi", ""], ["Liu", "Jun", ""], ["Li", "Jie", ""]]}, {"id": "1908.06173", "submitter": "Emilio Ferrara", "authors": "Emilio Ferrara", "title": "The History of Digital Spam", "comments": null, "journal-ref": "Communications of the ACM, August 2019, Vol. 62 No. 8, Pages\n  82-91, 2019", "doi": "10.1145/3299768", "report-no": null, "categories": "cs.CY cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spam!: that's what Lorrie Faith Cranor and Brian LaMacchia exclaimed in the\ntitle of a popular call-to-action article that appeared twenty years ago on\nCommunications of the ACM. And yet, despite the tremendous efforts of the\nresearch community over the last two decades to mitigate this problem, the\nsense of urgency remains unchanged, as emerging technologies have brought new\ndangerous forms of digital spam under the spotlight. Furthermore, when spam is\ncarried out with the intent to deceive or influence at scale, it can alter the\nvery fabric of society and our behavior. In this article, I will briefly review\nthe history of digital spam: starting from its quintessential incarnation, spam\nemails, to modern-days forms of spam affecting the Web and social media, the\nsurvey will close by depicting future risks associated with spam and abuse of\nnew technologies, including Artificial Intelligence (e.g., Digital Humans).\nAfter providing a taxonomy of spam, and its most popular applications emerged\nthroughout the last two decades, I will review technological and regulatory\napproaches proposed in the literature, and suggest some possible solutions to\ntackle this ubiquitous digital epidemic moving forward.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2019 00:14:54 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Ferrara", "Emilio", ""]]}, {"id": "1908.06402", "submitter": "Evgeny Burnaev", "authors": "Anton Smerdov and Evgeny Burnaev and Andrey Somov", "title": "eSports Pro-Players Behavior During the Game Events: Statistical\n  Analysis of Data Obtained Using the Smart Chair", "comments": "8 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's competition between the professional eSports teams is so strong that\nin-depth analysis of players' performance literally crucial for creating a\npowerful team. There are two main approaches to such an estimation: obtaining\nfeatures and metrics directly from the in-game data or collecting detailed\ninformation about the player including data on his/her physical training. While\nthe correlation between the player's skill and in-game data has already been\ncovered in many papers, there are very few works related to analysis of eSports\nathlete's skill through his/her physical behavior. We propose the smart chair\nplatform which is to collect data on the person's behavior on the chair using\nan integrated accelerometer, a gyroscope and a magnetometer. We extract the\nimportant game events to define the players' physical reactions to them. The\nobtained data are used for training machine learning models in order to\ndistinguish between the low-skilled and high-skilled players. We extract and\nfigure out the key features during the game and discuss the results.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 09:05:18 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Smerdov", "Anton", ""], ["Burnaev", "Evgeny", ""], ["Somov", "Andrey", ""]]}, {"id": "1908.06403", "submitter": "Evgeny Burnaev", "authors": "Alexander Korotin and Nikita Khromov and Anton Stepanov and Andrey\n  Lange and Evgeny Burnaev and Andrey Somov", "title": "Towards Understanding of eSports Athletes' Potentialities: The Sensing\n  System for Data Collection and Analysis", "comments": "7 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  eSports is a developing multidisciplinary research area. At present, there is\na lack of relevant data collected from real eSports athletes and lack of\nplatforms which could be used for the data collection and further analysis. In\nthis paper, we present a sensing system for enabling the data collection from\nprofessional athletes. Also, we report on the case study about collecting and\nanalyzing the gaze data from Monolith professional eSports team specializing in\nCounter-Strike: Global Offensive (CS:GO) discipline. We perform a comparative\nstudy on assessing the gaze of amateur players and professional athletes. The\nresults of our work are vital for ensuring eSports data collection and the\nfollowing analysis in the scope of scouting or assessing the eSports players\nand athletes.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 09:05:46 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Korotin", "Alexander", ""], ["Khromov", "Nikita", ""], ["Stepanov", "Anton", ""], ["Lange", "Andrey", ""], ["Burnaev", "Evgeny", ""], ["Somov", "Andrey", ""]]}, {"id": "1908.06404", "submitter": "Evgeny Burnaev", "authors": "Anton Stepanov and Andrey Lange and Nikita Khromov and Alexander\n  Korotin and Evgeny Burnaev and Andrey Somov", "title": "Sensors and Game Synchronization for Data Analysis in eSports", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  eSports industry has greatly progressed within the last decade in terms of\naudience and fund rising, broadcasting, networking and hardware. Since the\nnumber and quality of professional team has evolved too, there is a reasonable\nneed in improving skills and training process of professional eSports athletes.\nIn this work, we demonstrate a system able to collect heterogeneous data\n(physiological, environmental, video, telemetry) and guarantying\nsynchronization with 10 ms accuracy. In particular, we demonstrate how to\nsynchronize various sensors and ensure post synchronization, i.e. logged video,\na so-called demo file, with the sensors data. Our experimental results achieved\non the CS:GO game discipline show up to 3 ms accuracy of the time\nsynchronization of the gaming computer.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 09:18:53 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Stepanov", "Anton", ""], ["Lange", "Andrey", ""], ["Khromov", "Nikita", ""], ["Korotin", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Somov", "Andrey", ""]]}, {"id": "1908.06407", "submitter": "Evgeny Burnaev", "authors": "Anton Smerdov and Anastasia Kiskun and Rostislav Shaniiazov and Andrey\n  Somov and Evgeny Burnaev", "title": "Understanding Cyber Athletes Behaviour Through a Smart Chair: CS:GO and\n  Monolith Team Scenario", "comments": "6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  eSports is the rapidly developing multidisciplinary domain. However, research\nand experimentation in eSports are in the infancy. In this work, we propose a\nsmart chair platform - an unobtrusive approach to the collection of data on the\neSports athletes and data further processing with machine learning methods. The\nuse case scenario involves three groups of players: `cyber athletes' (Monolith\nteam), semi-professional players and newbies all playing CS:GO discipline. In\nparticular, we collect data from the accelerometer and gyroscope integrated in\nthe chair and apply machine learning algorithms for the data analysis. Our\nresults demonstrate that the professional athletes can be identified by their\nbehaviour on the chair while playing the game.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 09:29:50 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Smerdov", "Anton", ""], ["Kiskun", "Anastasia", ""], ["Shaniiazov", "Rostislav", ""], ["Somov", "Andrey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1908.06484", "submitter": "Rodolfo Migon Favaretto", "authors": "Rodolfo Migon Favaretto and Victor Araujo and Soraia Raupp Musse and\n  Felipe Vilanova and Angelo Brandelli Costa", "title": "A Software to Detect OCC Emotion, Big-Five Personality and Hofstede\n  Cultural Dimensions of Pedestrians from Video Sequences", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a video analysis application to detect personality,\nemotion and cultural aspects from pedestrians in video sequences, along with a\nvisualizer of features. The proposed model considers a series of\ncharacteristics of the pedestrians and the crowd, such as number and size of\ngroups, distances, speeds, among others, and performs the mapping of these\ncharacteristics in personalities, emotions and cultural aspects, considering\nthe Cultural Dimensions of Hofstede (HCD), the Big-Five Personality Model\n(OCEAN) and the OCC Emotional Model. The main hypothesis is that there is a\nrelationship between so-called intrinsic human variables (such as emotion) and\nthe way people behave in space and time. The software was tested in a set of\nvideos from different countries and results seem promising in order to identify\nthese three different levels of psychological traits in the filmed sequences.\nIn addition, the data of the people present in the videos can be seen in a\ncrowd viewer.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 17:17:03 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Favaretto", "Rodolfo Migon", ""], ["Araujo", "Victor", ""], ["Musse", "Soraia Raupp", ""], ["Vilanova", "Felipe", ""], ["Costa", "Angelo Brandelli", ""]]}, {"id": "1908.06814", "submitter": "Victor Morel", "authors": "Victor Morel (CITI, PRIVATICS), Ra\\'ul Pardo", "title": "SoK: Three Facets of Privacy Policies", "comments": null, "journal-ref": "Workshop on Privacy in the Electronic Society, Nov 2020, Virtual,\n  France", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Privacy policies are the main way to obtain information related to personal\ndata collection and processing. Originally, privacy policies were presented as\ntextual documents. However, the unsuitability of this format for the needs of\ntoday's society gave birth to other means of expression. In this paper, we\nsystematically study the different means of expression of privacy policies. In\ndoing so, we have explored the three main categories, which we call facets, ie,\nnatural language, graphical and machine-readable privacy policies. Each of\nthese facets focuses on the particular needs of the communities they come from,\nie, law experts, organizations and privacy advocates, and academics,\nrespectively. We then analyze the benefits and limitations of each facet, and\nexplain why solutions based on a single facet do not cover the needs of other\ncommunities. Finally, we set guidelines and discuss challenges of an approach\nto expressing privacy policies which brings together the benefits of each facet\nas an attempt to overcome their limitations.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 14:09:46 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 09:22:10 GMT"}, {"version": "v3", "created": "Mon, 25 Nov 2019 11:04:56 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 12:42:23 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Morel", "Victor", "", "CITI, PRIVATICS"], ["Pardo", "Ra\u00fal", ""]]}, {"id": "1908.06877", "submitter": "Manny Rayner", "authors": "Cathy Chua and Hanieh Habibi and Manny Rayner and Nikos Tsourakis", "title": "Decentralising power: how we are trying to keep CALLector ethical", "comments": "6 pages; based on talk presented at enetCollect WG3 & WG5 Meeting,\n  Leiden, Holland, 2018", "journal-ref": "CEUR Workshop proceedings vol 2390 http://ceur-ws.org/Vol-2390/\n  2019", "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a brief overview of the CALLector project, and consider ethical\nquestions arising from its overall goal of creating a social network to support\ncreation and use of online CALL resources. We argue that these questions are\nbest addressed in a decentralised, pluralistic open source architecture.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 20:29:46 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Chua", "Cathy", ""], ["Habibi", "Hanieh", ""], ["Rayner", "Manny", ""], ["Tsourakis", "Nikos", ""]]}, {"id": "1908.06922", "submitter": "Bum Chul Kwon", "authors": "Hwiyeon Kim, Juyoung Oh, Yunha Han, Sungahn Ko, Matthew Brehmer, Bum\n  Chul Kwon", "title": "Thumbnails for Data Stories: A Survey of Current Practices", "comments": "To appear at IEEE VIS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When people browse online news, small thumbnail images accompanying links to\narticles attract their attention and help them to decide which articles to\nread. As an increasing proportion of online news can be construed as data\njournalism, we have witnessed a corresponding increase in the incorporation of\nvisualization in article thumbnails. However, there is little research to\nsupport alternative design choices for visualization thumbnails, which include\nresizing, cropping, simplifying, and embellishing charts appearing within the\nbody of the associated article. We therefore sought to better understand these\ndesign choices and determine what makes a visualization thumbnail inviting and\ninterpretable. This paper presents our findings from a survey of visualization\nthumbnails collected online and from conversations with data journalists and\nnews graphics designers. Our study reveals that there exists an uncharted\ndesign space, one that is in need of further empirical study. Our work can thus\nbe seen as a first step toward providing structured guidance on how to design\nthumbnails for data stories.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 16:48:48 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Kim", "Hwiyeon", ""], ["Oh", "Juyoung", ""], ["Han", "Yunha", ""], ["Ko", "Sungahn", ""], ["Brehmer", "Matthew", ""], ["Kwon", "Bum Chul", ""]]}, {"id": "1908.07047", "submitter": "Daniel Mutembesa", "authors": "Daniel Mutembesa, Ernest Mwebaze, Solomon Nsumba, Christopher Omongo,\n  Humphrey Mutaasa", "title": "Mobile community sensing with smallholder farmers in a developing\n  nation; A scaled pilot for crop health monitoring", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previously, crowdsourcing experiments in surveillance of crop diseases and\npest have been trialed as small scale community sensing campaigns with select\ncohort of smallholder farmers, extension and experts. While those pilots have\ndemonstrated the viability of community sensing with mobile phones to collect\nmassive amounts of real-time data all year round, to compliment low-resourced\nagricultural expert surveys, they are limited in generalising ideas for scaled\nimplementations of a community sensing system with farmer communities. This\nwork presents a case of scaled deployment of the mobile ad hoc surveillance for\ncrowdsourcing real-time surveillance data on cassava from over 175 smallholder\nfarmers across Uganda. This paper describes a modified mobile ad hoc\nsurveillance ecosystem to suite smallholder farmer agents, a communication\nmodel and data collection model designed to cover the spatial interests for the\nscale of surveillance, a deployment plan, the training methodology and\nincentives structure. The paper also presents very early results of\ncontributions from farmer agents, that could be usable in monitoring the\nmovement of planting materials between districts, mapping cassava varieties,\nmultiplication sites, and communities with little or no access to agricultural\nextension services, and possibly guide precision expert surveys in areas of\nhigh disease incidence.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 19:46:47 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Mutembesa", "Daniel", ""], ["Mwebaze", "Ernest", ""], ["Nsumba", "Solomon", ""], ["Omongo", "Christopher", ""], ["Mutaasa", "Humphrey", ""]]}, {"id": "1908.07107", "submitter": "Debanjan Borthakur", "authors": "Debanjan Borthakur, Victoria Grace, Paul Batchelor, Harishchandra\n  Dubey, Kunal Mankodiya", "title": "Fuzzy C-Means Clustering and Sonification of HRV Features", "comments": "5 pages, 5 figures", "journal-ref": "2019 the IEEE/ACM 4th International Conference on Connected\n  Health: Applications, Systems and Engineering Technologies: EdgeDL\n  WorkshopAt: Washington, D.C, sep- 25-27", "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SD eess.AS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear and non-linear measures of heart rate variability (HRV) are widely\ninvestigated as non-invasive indicators of health. Stress has a profound impact\non heart rate, and different meditation techniques have been found to modulate\nheartbeat rhythm. This paper aims to explore the process of identifying\nappropriate metrices from HRV analysis for sonification. Sonification is a type\nof auditory display involving the process of mapping data to acoustic\nparameters. This work explores the use of auditory display in aiding the\nanalysis of HRV leveraged by unsupervised machine learning techniques.\nUnsupervised clustering helps select the appropriate features to improve the\nsonification interpretability. Vocal synthesis sonification techniques are\nemployed to increase comprehension and learnability of the processed data\ndisplayed through sound. These analyses are early steps in building a real-time\nsound-based biofeedback training system.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 23:44:01 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 21:47:27 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Borthakur", "Debanjan", ""], ["Grace", "Victoria", ""], ["Batchelor", "Paul", ""], ["Dubey", "Harishchandra", ""], ["Mankodiya", "Kunal", ""]]}, {"id": "1908.07123", "submitter": "Siqi Wu", "authors": "Siqi Wu, Marian-Andrei Rizoiu, Lexing Xie", "title": "Estimating Attention Flow in Online Video Networks", "comments": "CSCW 2019, the code and datasets are publicly available at\n  https://github.com/avalanchesiqi/networked-popularity", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online videos have shown tremendous increase in Internet traffic. Most video\nhosting sites implement recommender systems, which connect the videos into a\ndirected network and conceptually act as a source of pathways for users to\nnavigate. At present, little is known about how human attention is allocated\nover such large-scale networks, and about the impacts of the recommender\nsystems. In this paper, we first construct the Vevo network -- a YouTube video\nnetwork with 60,740 music videos interconnected by the recommendation links,\nand we collect their associated viewing dynamics. This results in a total of\n310 million views every day over a period of 9 weeks. Next, we present\nlarge-scale measurements that connect the structure of the recommendation\nnetwork and the video attention dynamics. We use the bow-tie structure to\ncharacterize the Vevo network and we find that its core component (23.1% of the\nvideos), which occupies most of the attention (82.6% of the views), is made out\nof videos that are mainly recommended among themselves. This is indicative of\nthe links between video recommendation and the inequality of attention\nallocation. Finally, we address the task of estimating the attention flow in\nthe video recommendation network. We propose a model that accounts for the\nnetwork effects for predicting video popularity, and we show it consistently\noutperforms the baselines. This model also identifies a group of artists\ngaining attention because of the recommendation network. Altogether, our\nobservations and our models provide a new set of tools to better understand the\nimpacts of recommender systems on collective social attention.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 01:37:26 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 08:47:27 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 04:55:02 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Wu", "Siqi", ""], ["Rizoiu", "Marian-Andrei", ""], ["Xie", "Lexing", ""]]}, {"id": "1908.07124", "submitter": "Akinari Onishi", "authors": "Akinari Onishi", "title": "Landmark Map: An Extension of the Self-Organizing Map for a\n  User-Intended Nonlinear Projection", "comments": null, "journal-ref": null, "doi": "10.1016/j.neucom.2019.12.125", "report-no": null, "categories": "cs.NE cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The self-organizing map (SOM) is an unsupervised artificial neural network\nthat is widely used in, e.g., data mining and visualization. Supervised and\nsemi-supervised learning methods have been proposed for the SOM. However, their\nteacher labels do not describe the relationship between the data and the\nlocation of nodes. This study proposes a landmark map (LAMA), which is an\nextension of the SOM that utilizes several landmarks, e.g., pairs of nodes and\ndata points. LAMA is designed to obtain a user-intended nonlinear projection to\nachieve, e.g., the landmark-oriented data visualization. To reveal the learning\nproperties of LAMA, the Zoo dataset from the UCI Machine Learning Repository\nand an artificial formant dataset were analyzed. The analysis results of the\nZoo dataset indicated that LAMA could provide a new data view such as the\nlandmark-centered data visualization. Furthermore, the artificial formant data\nanalysis revealed that LAMA successfully provided the intended nonlinear\nprojection associating articular movement with vertical and horizontal movement\nof a computer cursor. Potential applications of LAMA include data mining,\nrecommendation systems, and human-computer interaction.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 01:51:14 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Onishi", "Akinari", ""]]}, {"id": "1908.07144", "submitter": "Anhong Guo", "authors": "Anhong Guo, Junhan Kong, Michael Rivera, Frank F. Xu, Jeffrey P.\n  Bigham", "title": "StateLens: A Reverse Engineering Solution for Making Existing Dynamic\n  Touchscreens Accessible", "comments": "ACM UIST 2019", "journal-ref": null, "doi": "10.1145/3332165.3347873", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind people frequently encounter inaccessible dynamic touchscreens in their\neveryday lives that are difficult, frustrating, and often impossible to use\nindependently. Touchscreens are often the only way to control everything from\ncoffee machines and payment terminals, to subway ticket machines and in-flight\nentertainment systems. Interacting with dynamic touchscreens is difficult\nnon-visually because the visual user interfaces change, interactions often\noccur over multiple different screens, and it is easy to accidentally trigger\ninterface actions while exploring the screen. To solve these problems, we\nintroduce StateLens - a three-part reverse engineering solution that makes\nexisting dynamic touchscreens accessible. First, StateLens reverse engineers\nthe underlying state diagrams of existing interfaces using point-of-view videos\nfound online or taken by users using a hybrid crowd-computer vision pipeline.\nSecond, using the state diagrams, StateLens automatically generates\nconversational agents to guide blind users through specifying the tasks that\nthe interface can perform, allowing the StateLens iOS application to provide\ninteractive guidance and feedback so that blind users can access the interface.\nFinally, a set of 3D-printed accessories enable blind people to explore\ncapacitive touchscreens without the risk of triggering accidental touches on\nthe interface. Our technical evaluation shows that StateLens can accurately\nreconstruct interfaces from stationary, hand-held, and web videos; and, a user\nstudy of the complete system demonstrates that StateLens successfully enables\nblind users to access otherwise inaccessible dynamic touchscreens.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 03:23:35 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Guo", "Anhong", ""], ["Kong", "Junhan", ""], ["Rivera", "Michael", ""], ["Xu", "Frank F.", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1908.07316", "submitter": "Yixuan Zhang", "authors": "Yixuan Zhang, Sara Di Bartolomeo, Fangfang Sheng, Holly Jimison, Cody\n  Dunne", "title": "Evaluating Alignment Approaches in Superimposed Time-Series and Temporal\n  Event-Sequence Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composite temporal event sequence visualizations have included sentinel event\nalignment techniques to cope with data volume and variety. Prior work has\ndemonstrated the utility of using single-event alignment for understanding the\nprecursor, co-occurring, and aftereffect events surrounding a sentinel event.\nHowever, the usefulness of single-event alignment has not been sufficiently\nevaluated in composite visualizations. Furthermore, recently proposed\ndual-event alignment techniques have not been empirically evaluated. In this\nwork, we designed tasks around temporal event sequence and timing analysis and\nconducted a controlled experiment on Amazon Mechanical Turk to examine four\nsentinel event alignment approaches: no sentinel event alignment (NoAlign),\nsingle-event alignment (SingleAlign), dual-event alignment with left\njustification (DualLeft), and dual-event alignment with stretch justification\n(DualStretch). Differences between approaches were most pronounced with more\nrows of data. For understanding intermediate events between two sentinel\nevents, dual-event alignment was the clear winner for correctness---71% vs. 18%\nfor NoAlign and SingleAlign. For understanding the duration between two\nsentinel events, NoAlign was the clear winner: correctness---88% vs. 36% for\nDualStretch---completion time---55 seconds vs. 101 seconds for DualLeft---and\nerror---1.5% vs. 8.4% for DualStretch. For understanding precursor and\naftereffect events, there was no significant difference among approaches. A\nfree copy of this paper, the evaluation stimuli and data, and source code are\navailable at https://osf.io/78fs5\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 12:58:14 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Zhang", "Yixuan", ""], ["Di Bartolomeo", "Sara", ""], ["Sheng", "Fangfang", ""], ["Jimison", "Holly", ""], ["Dunne", "Cody", ""]]}, {"id": "1908.07333", "submitter": "Leah Findlater", "authors": "Leah Findlater, Steven Goodman, Yuhang Zhao, Shiri Azenkot, Margot\n  Hanley", "title": "Fairness Issues in AI Systems that Augment Sensory Abilities", "comments": "4 pages. Accepted to the ACM ASSETS 2019 Workshop on AI Fairness for\n  People with Disabilities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems that augment sensory abilities are increasingly employing AI and\nmachine learning (ML) approaches, with applications ranging from object\nrecognition and scene description tools for blind users to sound awareness\ntools for d/Deaf users. However, unlike many other AI-enabled technologies,\nthese systems provide information that is already available to non-disabled\npeople. In this paper, we discuss unique AI fairness challenges that arise in\nthis context, including accessibility issues with data and models, ethical\nimplications in deciding what sensory information to convey to the user, and\nprivacy concerns both for the primary user and for others.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 20:25:49 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Findlater", "Leah", ""], ["Goodman", "Steven", ""], ["Zhao", "Yuhang", ""], ["Azenkot", "Shiri", ""], ["Hanley", "Margot", ""]]}, {"id": "1908.07407", "submitter": "Ala Eldin Omer Mr", "authors": "Ala Eldin Omer, George Shaker, Safieddin Safavi-Naeini, Georges\n  Alqui\\'e, Frederique Deshours, and Hamid Kokabi", "title": "Triple-Poles Complementary Split Ring Resonator for Sensing Diabetics\n  Glucose Levels at cm-Band", "comments": "4 pages, 5 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microwave sensors are very promising for sensing the blood glucose levels\nnon-invasively for their non-ionizing nature, miniaturized sizing, and low\nhealth risks for diabetics. All these features offer the possibility for\nrealizing a portable non-invasive glucose sensor for monitoring glucose levels\nin real time. In this article, we propose a triple poles complementary split\nring resonator (CSRR) produced on a FR4 substrate in microstrip technology in\nthe cm-wave band (1-6 GHz). The proposed bio-sensor can detect the small\nvariations in the dielectric properties (relative permittivity and dielectric\nlosses) of glucose in the blood mimicking aqueous solutions due their intense\ninteraction with the electromagnetic field at harmonic resonances. The\nresonator exhibits higher sensitivity performance at the different resonances\ncompared to the single and double-poles counterparts as demonstrated by\nsimulations in a 3D full-wave EM solver.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2019 05:59:42 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Omer", "Ala Eldin", ""], ["Shaker", "George", ""], ["Safavi-Naeini", "Safieddin", ""], ["Alqui\u00e9", "Georges", ""], ["Deshours", "Frederique", ""], ["Kokabi", "Hamid", ""]]}, {"id": "1908.07471", "submitter": "Aditya Bharadwaj", "authors": "Aditya Bharadwaj, David Gwizdala, Yoonjin Kim, Kurt Luther, T. M.\n  Murali", "title": "Flud: a hybrid crowd-algorithm approach for visualizing biological\n  networks", "comments": "This manuscript is currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern experiments in many disciplines generate large quantities of network\n(graph) data. Researchers require aesthetic layouts of these networks that\nclearly convey the domain knowledge and meaning. However, the problem remains\nchallenging due to multiple conflicting aesthetic criteria and complex\ndomain-specific constraints. In this paper, we present a strategy for\ngenerating visualizations that can help network biologists understand the\nprotein interactions that underlie processes that take place in the cell.\nSpecifically, we have developed Flud, an online game with a purpose (GWAP) that\nallows humans with no expertise to design biologically meaningful graph layouts\nwith the help of algorithmically generated suggestions. Further, we propose a\nnovel hybrid approach for graph layout wherein crowdworkers and a simulated\nannealing algorithm build on each other's progress. To showcase the\neffectiveness of Flud, we recruited crowd workers on Amazon Mechanical Turk to\nlay out complex networks that represent signaling pathways. Our results show\nthat the proposed hybrid approach outperforms state-of-the-art techniques for\ngraphs with a large number of feedback loops. We also found that the\nalgorithmically generated suggestions guided the players when they are stuck\nand helped them improve their score. Finally, we discuss broader implications\nfor mixed-initiative interactions in human computation games.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 16:12:39 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Bharadwaj", "Aditya", ""], ["Gwizdala", "David", ""], ["Kim", "Yoonjin", ""], ["Luther", "Kurt", ""], ["Murali", "T. M.", ""]]}, {"id": "1908.07479", "submitter": "Alessio Arleo PhD", "authors": "Alessio Arleo, Christos Tsigkanos, Chao Jia, Roger A. Leite, Ilir\n  Murturi, Manfred Klaffenboeck, Schahram Dustdar, Michael Wimmer, Silvia\n  Miksch, and Johannes Sorger", "title": "Sabrina: Modeling and Visualization of Economy Data with Incremental\n  Domain Knowledge", "comments": null, "journal-ref": null, "doi": "10.1109/VISUAL.2019.8933598", "report-no": null, "categories": "q-fin.GN cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investment planning requires knowledge of the financial landscape on a large\nscale, both in terms of geo-spatial and industry sector distribution. There is\nplenty of data available, but it is scattered across heterogeneous sources\n(newspapers, open data, etc.), which makes it difficult for financial analysts\nto understand the big picture. In this paper, we present Sabrina, a financial\ndata analysis and visualization approach that incorporates a pipeline for the\ngeneration of firm-to-firm financial transaction networks. The pipeline is\ncapable of fusing the ground truth on individual firms in a region with\n(incremental) domain knowledge on general macroscopic aspects of the economy.\nSabrina unites these heterogeneous data sources within a uniform visual\ninterface that enables the visual analysis process. In a user study with three\ndomain experts, we illustrate the usefulness of Sabrina, which eases their\nanalysis process.\n", "versions": [{"version": "v1", "created": "Mon, 5 Aug 2019 21:51:24 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 10:22:10 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Arleo", "Alessio", ""], ["Tsigkanos", "Christos", ""], ["Jia", "Chao", ""], ["Leite", "Roger A.", ""], ["Murturi", "Ilir", ""], ["Klaffenboeck", "Manfred", ""], ["Dustdar", "Schahram", ""], ["Wimmer", "Michael", ""], ["Miksch", "Silvia", ""], ["Sorger", "Johannes", ""]]}, {"id": "1908.07519", "submitter": "Wenjin Tao", "authors": "Wenjin Tao, Ming C. Leu, Zhaozheng Yin", "title": "Multi-Modal Recognition of Worker Activity for Human-Centered\n  Intelligent Manufacturing", "comments": "17 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a human-centered intelligent manufacturing system, sensing and\nunderstanding of the worker's activity are the primary tasks. In this paper, we\npropose a novel multi-modal approach for worker activity recognition by\nleveraging information from different sensors and in different modalities.\nSpecifically, a smart armband and a visual camera are applied to capture\nInertial Measurement Unit (IMU) signals and videos, respectively. For the IMU\nsignals, we design two novel feature transform mechanisms, in both frequency\nand spatial domains, to assemble the captured IMU signals as images, which\nallow using convolutional neural networks to learn the most discriminative\nfeatures. Along with the above two modalities, we propose two other modalities\nfor the video data, at the video frame and video clip levels, respectively.\nEach of the four modalities returns a probability distribution on activity\nprediction. Then, these probability distributions are fused to output the\nworker activity classification result. A worker activity dataset of 6\nactivities is established, which at present contains 6 common activities in\nassembly tasks, i.e., grab a tool/part, hammer a nail, use a power-screwdriver,\nrest arms, turn a screwdriver, and use a wrench. The developed multi-modal\napproach is evaluated on this dataset and achieves recognition accuracies as\nhigh as 97% and 100% in the leave-one-out and half-half experiments,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 15:46:07 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Tao", "Wenjin", ""], ["Leu", "Ming C.", ""], ["Yin", "Zhaozheng", ""]]}, {"id": "1908.07544", "submitter": "Katherine Isaacs", "authors": "Katherine E. Isaacs, Todd Gamblin", "title": "Preserving Command Line Workflow for a Package Management System using\n  ASCII DAG Visualization", "comments": null, "journal-ref": "Volume: 25, Issue: 9, Sept. 1 2019, Pages: 2804 - 2820", "doi": "10.1109/TVCG.2018.2859974", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Package managers provide ease of access to applications by removing the\ntime-consuming and sometimes completely prohibitive barrier of successfully\nbuilding, installing, and maintaining the software for a system. A package\ndependency contains dependencies between all packages required to build and run\nthe target software. Package management system developers, package maintainers,\nand users may consult the dependency graph when a simple listing is\ninsufficient for their analyses. However, users working in a remote command\nline environment must disrupt their workflow to visualize dependency graphs in\ngraphical programs, possibly needing to move files between devices or incur\nforwarding lag. Such is the case for users of Spack, an open source package\nmanagement system originally developed to ease the complex builds required by\nsupercomputing environments. To preserve the command line workflow of Spack, we\ndevelop an interactive ASCII visualization for its dependency graphs. Through\ninterviews with Spack maintainers, we identify user goals and corresponding\nvisual tasks for dependency graphs. We evaluate the use of our visualization\nthrough a command line-centered study, comparing it to the system's two\nexisting approaches. We observe that despite the limitations of the ASCII\nrepresentation, our visualization is preferred by participants when approached\nfrom a command line interface workflow.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 18:01:18 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Isaacs", "Katherine E.", ""], ["Gamblin", "Todd", ""]]}, {"id": "1908.07572", "submitter": "Michal Luria", "authors": "Michal Luria, John Zimmerman, Jodi Forlizzi", "title": "Championing Research Through Design in HRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in conducting research on the intersection of the CHI\nand Human-Robot Interaction (HRI) communities is in addressing the gap of\nacceptable design research methods between the two. While HRI is focused on\ninteraction with robots and includes design research in its scope, the\ncommunity is not as accustomed to exploratory design methods as the CHI\ncommunity. This workshop paper argues for bringing exploratory design, and\nspecifically Research through Design (RtD) methods that have been established\nin CHI for the past decade to the foreground of HRI. RtD can enable design\nresearchers in the field of HRI to conduct exploratory design work that asks\nwhat is the right thing to design and share it within the community.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 19:10:21 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Luria", "Michal", ""], ["Zimmerman", "John", ""], ["Forlizzi", "Jodi", ""]]}, {"id": "1908.07577", "submitter": "Michal Luria", "authors": "Michal Luria, Amit Zoran, Jodi Forlizzi", "title": "Challenges of Designing HCI for Negative Emotions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions that are perceived as \"negative\" are inherent in the human\nexperience. Yet not much work in the field of HCI has looked into the role of\nthese emotions in interaction with technology. As technology is becoming more\nsocial, personal and emotional by mediating our relationships and generating\nnew social entities (such as conversational agents and robots), it is valuable\nto consider how it can support people's negative emotions and behaviors.\nResearch in Psychology shows that interacting with negative emotions correctly\ncan benefit well-being, yet the boundary between helpful and harmful is\ndelicate. This workshop paper looks at the opportunities of designing for\nnegative affect, and the challenge of \"causing no harm\" that arises in an\nattempt to do so.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 19:23:34 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Luria", "Michal", ""], ["Zoran", "Amit", ""], ["Forlizzi", "Jodi", ""]]}, {"id": "1908.07752", "submitter": "Alexander Rind", "authors": "Alexander Rind, Markus Wagner, and Wolfgang Aigner (St. Poelten\n  University of Applied Sciences, Austria)", "title": "Towards a Structural Framework for Explicit Domain Knowledge in Visual\n  Analytics", "comments": "8 pages, 5 figures", "journal-ref": "Proc. IEEE Workshop on Visual Analytics in Healthcare (VAHC),\n  pages 33-40, 20 Oct 2019", "doi": "10.1109/VAHC47919.2019.8945032", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinicians and other analysts working with healthcare data are in need for\nbetter support to cope with large and complex data. While an increasing number\nof visual analytics environments integrates explicit domain knowledge as a\nmeans to deliver a precise representation of the available data, theoretical\nwork so far has focused on the role of knowledge in the visual analytics\nprocess. There has been little discussion about how such explicit domain\nknowledge can be structured in a generalized framework. This paper collects\ndesiderata for such a structural framework, proposes how to address these\ndesiderata based on the model of linked data, and demonstrates the\napplicability in a visual analytics environment for physiotherapy.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 08:48:50 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 09:55:24 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Rind", "Alexander", "", "St. Poelten\n  University of Applied Sciences, Austria"], ["Wagner", "Markus", "", "St. Poelten\n  University of Applied Sciences, Austria"], ["Aigner", "Wolfgang", "", "St. Poelten\n  University of Applied Sciences, Austria"]]}, {"id": "1908.07792", "submitter": "Amyra Meidiana", "authors": "Amyra Meidiana, Seok-Hee Hong, Peter Eades, and Daniel Keim", "title": "A Quality Metric for Visualization of Clusters in Graphs", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, graph quality metrics focus on readability, but recent studies\nshow the need for metrics which are more specific to the discovery of patterns\nin graphs. Cluster analysis is a popular task within graph analysis, yet there\nis no metric yet explicitly quantifying how well a drawing of a graph\nrepresents its cluster structure. We define a clustering quality metric\nmeasuring how well a node-link drawing of a graph represents the clusters\ncontained in the graph. Experiments with deforming graph drawings verify that\nour metric effectively captures variations in the visual cluster quality of\ngraph drawings. We then use our metric to examine how well different graph\ndrawing algorithms visualize cluster structures in various graphs; the results\ncon-firm that some algorithms which have been specifically designed to show\ncluster structures perform better than other algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 10:57:30 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Meidiana", "Amyra", ""], ["Hong", "Seok-Hee", ""], ["Eades", "Peter", ""], ["Keim", "Daniel", ""]]}, {"id": "1908.07816", "submitter": "Yubo Xie", "authors": "Yubo Xie, Ekaterina Svikhnushina, Pearl Pu", "title": "A Multi-Turn Emotionally Engaging Dialog Model", "comments": "Accepted to IUI 2020 user2agent workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-domain dialog systems (also known as chatbots) have increasingly drawn\nattention in natural language processing. Some of the recent work aims at\nincorporating affect information into sequence-to-sequence neural dialog\nmodeling, making the response emotionally richer, while others use hand-crafted\nrules to determine the desired emotion response. However, they do not\nexplicitly learn the subtle emotional interactions captured in human dialogs.\nIn this paper, we propose a multi-turn dialog system aimed at learning and\ngenerating emotional responses that so far only humans know how to do. Compared\nwith two baseline models, offline experiments show that our method performs the\nbest in perplexity scores. Further human evaluations confirm that our chatbot\ncan keep track of the conversation context and generate emotionally more\nappropriate responses while performing equally well on grammar.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2019 12:52:53 GMT"}, {"version": "v2", "created": "Sat, 14 Sep 2019 17:00:07 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 16:30:45 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Xie", "Yubo", ""], ["Svikhnushina", "Ekaterina", ""], ["Pu", "Pearl", ""]]}, {"id": "1908.07894", "submitter": "Alessandra Milani", "authors": "Alessandra Milani and Fernando Paulovich and Isabel Manssour", "title": "Visualization in the preprocessing phase: an interview study with\n  enterprise professionals", "comments": "11 pages with references; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current information age has increasingly required organizations to become\ndata-driven. However, analyzing and managing raw data is still a challenging\npart of the data mining process. Even though we can find interview studies\nproposing design implications or recommendations for future visualization\nsolutions in the data mining scope, they cover the entire workflow and do not\nfully focus on the challenges during the preprocessing phase and on how\nvisualization can support it. Moreover, they do not organize a final list of\ninsights consolidating the findings of other related studies. Hence, to better\nunderstand the current practice of enterprise professionals in data mining\nworkflows, in particular during the preprocessing phase, and how visualization\nsupports this process, we conducted semi-structured interviews with thirteen\ndata analysts. The discussion about the challenges and opportunities based on\nthe responses of the interviewees resulted in a list of ten insights. This list\nwas compared with the closest related works, improving the reliability of our\nfindings and providing background, as a consolidated set of requirements, for\nfuture visualization research papers applied to visual data exploration in data\nmining. Furthermore, we provide greater details on the profile of the data\nanalysts, the main challenges they face, and the opportunities that arise while\nthey are engaged in data mining projects in diverse organizational areas.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 14:33:03 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Milani", "Alessandra", ""], ["Paulovich", "Fernando", ""], ["Manssour", "Isabel", ""]]}, {"id": "1908.08131", "submitter": "Roger Moore", "authors": "Roger K. Moore", "title": "A 'Canny' Approach to Spoken Language Interfaces", "comments": "Presented at the CHI 2019 Workshop on Mapping Theoretical and\n  Methodological Perspectives for Understanding Speech Interface Interactions,\n  4-9 May 2019, Glasgow, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice-enabled artefacts such as Amazon Echo are very popular, but there\nappears to be a 'habitability gap' whereby users fail to engage with the full\ncapabilities of the device. This position paper draws a parallel with the\n'uncanny valley' effect, thereby proposing a solution based on aligning the\nvisual, vocal, behavioural and cognitive affordances of future voice-enabled\ndevices.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 22:36:37 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Moore", "Roger K.", ""]]}, {"id": "1908.08381", "submitter": "Xiangyun Lei", "authors": "Xiangyun Lei, Fred Hohman, Duen Horng Chau, Andrew J. Medford", "title": "ElectroLens: Understanding Atomistic Simulations Through\n  Spatially-resolved Visualization of High-dimensional Features", "comments": "accepted to IEEE visualization 2019 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG physics.chem-ph physics.comp-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, machine learning (ML) has gained significant popularity in\nthe field of chemical informatics and electronic structure theory. These\ntechniques often require researchers to engineer abstract \"features\" that\nencode chemical concepts into a mathematical form compatible with the input to\nmachine-learning models. However, there is no existing tool to connect these\nabstract features back to the actual chemical system, making it difficult to\ndiagnose failures and to build intuition about the meaning of the features. We\npresent ElectroLens, a new visualization tool for high-dimensional\nspatially-resolved features to tackle this problem. The tool visualizes\nhigh-dimensional data sets for atomistic and electron environment features by a\nseries of linked 3D views and 2D plots. The tool is able to connect different\nderived features and their corresponding regions in 3D via interactive\nselection. It is built to be scalable, and integrate with existing\ninfrastructure.\n", "versions": [{"version": "v1", "created": "Tue, 20 Aug 2019 19:48:31 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 01:27:16 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Lei", "Xiangyun", ""], ["Hohman", "Fred", ""], ["Chau", "Duen Horng", ""], ["Medford", "Andrew J.", ""]]}, {"id": "1908.08412", "submitter": "Alessandra Tappini", "authors": "Lorenzo Angori, Walter Didimo, Fabrizio Montecchiani, Daniele\n  Pagliuca, Alessandra Tappini", "title": "ChordLink: A New Hybrid Visualization Model", "comments": "Appears in the Proceedings of the 27th International Symposium on\n  Graph Drawing and Network Visualization (GD 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world networks are globally sparse but locally dense. Typical\nexamples are social networks, biological networks, and information networks.\nThis double structural nature makes it difficult to adopt a homogeneous\nvisualization model that clearly conveys an overview of the network and the\ninternal structure of its communities at the same time. As a consequence, the\nuse of hybrid visualizations has been proposed. For instance, NodeTrix combines\nnode-link and matrix-based representations (Henry et al., 2007). In this paper\nwe describe ChordLink, a hybrid visualization model that embeds chord diagrams,\nused to represent dense subgraphs, into a node-link diagram, which shows the\nglobal network structure. The visualization is intuitive and makes it possible\nto interactively highlight the structure of a community while keeping the rest\nof the layout stable. We discuss the intriguing algorithmic challenges behind\nthe ChordLink model, present a prototype system, and illustrate case studies on\nreal-world networks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 14:41:45 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Angori", "Lorenzo", ""], ["Didimo", "Walter", ""], ["Montecchiani", "Fabrizio", ""], ["Pagliuca", "Daniele", ""], ["Tappini", "Alessandra", ""]]}, {"id": "1908.08532", "submitter": "Nate Phillips Mr.", "authors": "Nate Phillips, Kristen Massey, Mohammed Safayet Arefin, and J. Edward\n  Swan II", "title": "Design, Assembly, Calibration, and Measurement of an Augmented Reality\n  Haploscope", "comments": "Accepted and presented at the IEEE VR 2018 Workshop on Perceptual and\n  Cognitive Issues in AR (PERCAR); pre-print version", "journal-ref": "Proceedings of PERCAR: The Fifth IEEE Virtual Reality Workshop on\n  Perceptual and Cognitive Issues in AR, 2019 IEEE Conference on Virtual\n  Reality and 3D User Interfaces, Osaka, Japan, March 23-27, pages 1770-1774,\n  2019", "doi": "10.1109/VR.2019.8798335", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A haploscope is an optical system which produces a carefully controlled\nvirtual image. Since the development of Wheatstone's original stereoscope in\n1838, haploscopes have been used to measure perceptual properties of human\nstereoscopic vision. This paper presents an augmented reality (AR) haploscope,\nwhich allows the viewing of virtual objects superimposed against the real\nworld. Our lab has used generations of this device to make a careful series of\nperceptual measurements of AR phenomena, which have been described in\npublications over the previous 8 years. This paper systematically describes the\ndesign, assembly, calibration, and measurement of our AR haploscope. These\nmethods have been developed and improved in our lab over the past 10 years.\nDespite the fact that 180 years have elapsed since the original report of\nWheatstone's stereoscope, we have not previously found a paper that describes\nthese kinds of details.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 23:32:57 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Phillips", "Nate", ""], ["Massey", "Kristen", ""], ["Arefin", "Mohammed Safayet", ""], ["Swan", "J. Edward", "II"]]}, {"id": "1908.08597", "submitter": "Danielle Bragg", "authors": "Danielle Bragg, Oscar Koller, Mary Bellard, Larwan Berke, Patrick\n  Boudrealt, Annelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa\n  Kacorri, Tessa Verhoef, Christian Vogler, Meredith Ringel Morris", "title": "Sign Language Recognition, Generation, and Translation: An\n  Interdisciplinary Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CL cs.CY cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing successful sign language recognition, generation, and translation\nsystems requires expertise in a wide range of fields, including computer\nvision, computer graphics, natural language processing, human-computer\ninteraction, linguistics, and Deaf culture. Despite the need for deep\ninterdisciplinary knowledge, existing research occurs in separate disciplinary\nsilos, and tackles separate portions of the sign language processing pipeline.\nThis leads to three key questions: 1) What does an interdisciplinary view of\nthe current landscape reveal? 2) What are the biggest challenges facing the\nfield? and 3) What are the calls to action for people working in the field? To\nhelp answer these questions, we brought together a diverse group of experts for\na two-day workshop. This paper presents the results of that interdisciplinary\nworkshop, providing key background that is often overlooked by computer\nscientists, a review of the state-of-the-art, a set of pressing challenges, and\na call to action for the research community.\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2019 21:05:17 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Bragg", "Danielle", ""], ["Koller", "Oscar", ""], ["Bellard", "Mary", ""], ["Berke", "Larwan", ""], ["Boudrealt", "Patrick", ""], ["Braffort", "Annelies", ""], ["Caselli", "Naomi", ""], ["Huenerfauth", "Matt", ""], ["Kacorri", "Hernisa", ""], ["Verhoef", "Tessa", ""], ["Vogler", "Christian", ""], ["Morris", "Meredith Ringel", ""]]}, {"id": "1908.08641", "submitter": "Matt Cooper", "authors": "Matt Cooper, Jun Ki Lee, Jacob Beck, Joshua D. Fishman, Michael\n  Gillett, Zo\\\"e Papakipos, Aaron Zhang, Jerome Ramos, Aansh Shah, and Michael\n  L. Littman", "title": "Stackelberg Punishment and Bully-Proofing Autonomous Vehicles", "comments": "10 pages, The 11th International Conference on Social Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutually beneficial behavior in repeated games can be enforced via the threat\nof punishment, as enshrined in game theory's well-known \"folk theorem.\" There\nis a cost, however, to a player for generating these disincentives. In this\nwork, we seek to minimize this cost by computing a \"Stackelberg punishment,\" in\nwhich the player selects a behavior that sufficiently punishes the other player\nwhile maximizing its own score under the assumption that the other player will\nadopt a best response. This idea generalizes the concept of a Stackelberg\nequilibrium. Known efficient algorithms for computing a Stackelberg equilibrium\ncan be adapted to efficiently produce a Stackelberg punishment. We demonstrate\nan application of this idea in an experiment involving a virtual autonomous\nvehicle and human participants. We find that a self-driving car with a\nStackelberg punishment policy discourages human drivers from bullying in a\ndriving scenario requiring social negotiation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 02:29:17 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Cooper", "Matt", ""], ["Lee", "Jun Ki", ""], ["Beck", "Jacob", ""], ["Fishman", "Joshua D.", ""], ["Gillett", "Michael", ""], ["Papakipos", "Zo\u00eb", ""], ["Zhang", "Aaron", ""], ["Ramos", "Jerome", ""], ["Shah", "Aansh", ""], ["Littman", "Michael L.", ""]]}, {"id": "1908.08740", "submitter": "Maximilian Felde", "authors": "Maximilian Felde and Gerd Stumme", "title": "Interactive Collaborative Exploration using Incomplete Contexts", "comments": "38 pages (31 pages + 7 pages appendix), 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known knowledge acquisition method in the field of Formal Concept\nAnalysis (FCA) is attribute exploration. It is used to reveal dependencies in a\nset of attributes with help of a domain expert. In most applications no single\nexpert is capable (time- and knowledge-wise) of exploring the knowledge domain\nalone. However, there is up to now no theory that models the interaction of\nmultiple experts for the task of attribute exploration with incomplete\nknowledge. To this end, we to develop a theoretical framework that allows\nmultiple experts to explore domains together. We use a representation of\nincomplete knowledge as three-valued contexts. We then adapt the corresponding\nversion of attribute exploration to fit the setting of multiple experts. We\nsuggest formalizations for key components like expert knowledge, interaction\nand collaboration strategy. In particular, we define an order that allows to\ncompare the results of different exploration strategies on the same task with\nrespect to their information completeness. Furthermore we discuss other ways of\ncomparing collaboration strategies and suggest avenues for future research.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 09:49:43 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 10:50:53 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Felde", "Maximilian", ""], ["Stumme", "Gerd", ""]]}, {"id": "1908.08893", "submitter": "Paul Rosen", "authors": "Ghulam Jilani Quadri and Paul Rosen", "title": "You Can't Publish Replication Studies (and How to Anyways)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility has been increasingly encouraged by communities of science in\norder to validate experimental conclusions, and replication studies represent a\nsignificant opportunity to vision scientists wishing contribute new perceptual\nmodels, methods, or insights to the visualization community. Unfortunately, the\nnotion of replication of previous studies does not lend itself to how we\ncommunicate research findings. Simple put, studies that re-conduct and confirm\nearlier results do not hold any novelty, a key element to the modern research\npublication system. Nevertheless, savvy researchers have discovered ways to\nproduce replication studies by embedding them into other sufficiently novel\nstudies. In this position paper, we define three methods -- re-evaluation,\nexpansion, and specialization -- for embedding a replication study into a novel\npublished work. Within this context, we provide a non-exhaustive case study on\nreplications of Cleveland and McGill's seminal work on graphical perception. As\nit turns out, numerous replication studies have been carried out based on that\nwork, which have both confirmed prior findings and shined new light on our\nunderstanding of human perception. Finally, we discuss how publishing a true\nreplication study should be avoided, while providing suggestions for how vision\nscientists and others can still use replication studies as a vehicle to\nproducing visualization research publications.\n", "versions": [{"version": "v1", "created": "Fri, 23 Aug 2019 16:21:28 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Quadri", "Ghulam Jilani", ""], ["Rosen", "Paul", ""]]}, {"id": "1908.08931", "submitter": "Claudio Pinhanez", "authors": "Claudio Pinhanez", "title": "Machine Teaching by Domain Experts: Towards More Humane,Inclusive, and\n  Intelligent Machine Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper argues that a possible way to escape from the limitations of\ncurrent machine learning (ML) systems is to allow their development directly by\ndomain experts without the mediation of ML experts. This could be accomplished\nby making ML systems interactively teachable using concepts, definitions, and\nsimilar high level knowledge constructs. Pointing to the recent advances in\nmachine teaching technology, we list key technical challenges specific for such\nexpert-centric ML systems, and suggest that they are more humane and possibly\nmore intelligent than traditional ML systems in many domains. We then argue\nthat ML systems could also benefit greatly from being built by a community of\nexperts as much as open source software did, creating more inclusive systems,\nin terms of enabling different points-of-view about the same corpus of\nknowledge. Advantages of the community approach over current ways to build ML\nsystems, as well as specific challenges this approach raises, are also\ndiscussed in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2019 20:47:18 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Pinhanez", "Claudio", ""]]}, {"id": "1908.08939", "submitter": "Meredith Ringel Morris", "authors": "Meredith Ringel Morris", "title": "AI and Accessibility: A Discussion of Ethical Considerations", "comments": "Preprint of a \"Viewpoint\" column that was published in the\n  Communications of the ACM in May/June 2020", "journal-ref": null, "doi": "10.1145/3356727", "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization, more than one billion people\nworldwide have disabilities. The field of disability studies defines disability\nthrough a social lens; people are disabled to the extent that society creates\naccessibility barriers. AI technologies offer the possibility of removing many\naccessibility barriers; for example, computer vision might help people who are\nblind better sense the visual world, speech recognition and translation\ntechnologies might offer real time captioning for people who are hard of\nhearing, and new robotic systems might augment the capabilities of people with\nlimited mobility. Considering the needs of users with disabilities can help\ntechnologists identify high-impact challenges whose solutions can advance the\nstate of AI for all users; however, ethical challenges such as inclusivity,\nbias, privacy, error, expectation setting, simulated data, and social\nacceptability must be considered.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 17:00:25 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 07:28:59 GMT"}, {"version": "v3", "created": "Wed, 3 Jun 2020 16:33:42 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Morris", "Meredith Ringel", ""]]}, {"id": "1908.09082", "submitter": "Felix Hamza-Lup", "authors": "Felix Hamza-Lup", "title": "Kinesthetic Learning -- Haptic User Interfaces for Gyroscopic Precession\n  Simulation", "comments": null, "journal-ref": "Journal of Human Computer Interaction (RO-CHI) Vol. 11(3) (2018)\n  185-204", "doi": null, "report-no": null, "categories": "cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some forces in nature are difficult to comprehend due to their non-intuitive\nand abstract nature. Forces driving gyroscopic precession are invisible, yet\ntheir effect is very important in a variety of applications, from space\nnavigation to motion tracking. Current technological advancements in haptic\ninterfaces, enables development of revolutionary user interfaces, combining\nmultiple modalities: tactile, visual and auditory. Tactile augmented user\ninterfaces have been deployed in a variety of areas, from surgical training to\nelementary education. This research provides an overview of haptic user\ninterfaces in higher education, and presents the development and assessment of\na haptic-user interface that supports the learner's understanding of gyroscopic\nprecession forces. The visual-haptic simulator proposed, is one module from a\nseries of simulators targeted at complex concept representation, using\nmulti-modal user interfaces. Various higher education domains, from classical\nphysics to mechanical engineering, will benefit from the mainstream adoption of\nmulti-modal interfaces for hands-on training and content delivery. Experimental\nresults are promising, and underline the valuable impact that haptic user\ninterfaces have on enabling abstract concepts understanding, through\nkinesthetic learning and hands-on practice.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 03:24:14 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hamza-Lup", "Felix", ""]]}, {"id": "1908.09089", "submitter": "Felix Hamza-Lup", "authors": "Felix G. Hamza-Lup, Ionut E. Iacob, Sushmita Khan", "title": "Web-enabled Intelligent System for Continuous Sensor Data Processing and\n  Visualization", "comments": null, "journal-ref": "ACM (2019)", "doi": "10.1145/3329714.3338127", "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of sensors deployed in recent years in various setups and\ntheir data is readily available in dedicated databases or in the cloud. Of\nparticular interest is real-time data processing and 3D visualization in\nweb-based user interfaces that facilitate spatial information understanding and\nsharing, hence helping the decision making process for all the parties\ninvolved. In this research, we provide a prototype system for near real-time,\ncontinuous X3D-based visualization of processed sensor data for two significant\napplications: thermal monitoring for residential/commercial buildings and\nnitrogen cycle monitoring in water beds for aquaponics systems. As sensors are\nsparsely placed, in each application, where they collect data for large periods\n(of up to one year), we employ a Finite Differences Method and a Neural\nNetworks model to approximate data distribution in the entire volume.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 04:06:01 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Hamza-Lup", "Felix G.", ""], ["Iacob", "Ionut E.", ""], ["Khan", "Sushmita", ""]]}, {"id": "1908.09165", "submitter": "Ian Goldberg", "authors": "Matthew Lakier, Dimcho Karakashev, Yixin Wang, Ian Goldberg", "title": "Augmented Unlocking Techniques for Smartphones Using Pre-Touch\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphones store a significant amount of personal and private information,\nand are playing an increasingly important role in people's lives. It is\nimportant for authentication techniques to be more resistant against two known\nattacks called shoulder surfing and smudge attacks. In this work, we propose a\nnew technique called 3D Pattern. Our 3D Pattern technique takes advantage of a\nnew input paradigm called pre-touch, which could soon allow smartphones to\nsense a user's finger position at some distance from the screen. We implement\nthe technique and evaluate it in a pilot study (n=6) by comparing it to PIN and\npattern locks. Our results show that although our prototype takes about 8\nseconds to authenticate, it is immune to smudge attacks and promises to be more\nresistant to shoulder surfing.\n", "versions": [{"version": "v1", "created": "Sat, 24 Aug 2019 16:41:42 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Lakier", "Matthew", ""], ["Karakashev", "Dimcho", ""], ["Wang", "Yixin", ""], ["Goldberg", "Ian", ""]]}, {"id": "1908.09348", "submitter": "Joseph Gabbard", "authors": "Joseph L. Gabbard, J. Edward Swan II, Adam Zarger", "title": "Color Blending in Outdoor Optical See-through AR: The Effect of\n  Real-world Backgrounds on User Interface Color", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been noted anecdotally and through a small number of formal studies\nthat ambient lighting conditions and dynamic real-world backgrounds affect the\nusability of optical see-through augmented reality (AR) displays; especially so\nin outdoor environments. Our previous work examined these effects using painted\nposters as representative real-world backgrounds. In this paper, we present a\nstudy that employs an experimental testbed that allows AR graphics to be\noverlaid onto real-world backgrounds as well as painted posters. Our results\nindicate that color blending effects of physical materials as backgrounds are\nnearly the same as their corresponding poster backgrounds, even though the\ncolors of each pair are only a metameric match. More importantly, our results\nsuggest that given the current capabilities of optical see-through head-mounted\ndisplays (oHMDs), the implications are, at a minimum, a reduced color gamut\navailable to user interface (UI) designers. In worse cases, there are unknown\nor unexpected color interactions that no UI or system designers can plan for;\nsignificantly crippling the usability of the UI or altering the semantic\ninterpretation of graphical elements. Further, our results support the concept\nof an adaptive AR system which can dynamically alter the color of UI elements\nbased on predicted background color interactions. These interactions can be\nstudied and predicted through methods such as those presented in this work.\n", "versions": [{"version": "v1", "created": "Sun, 25 Aug 2019 15:56:28 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Gabbard", "Joseph L.", ""], ["Swan", "J. Edward", "II"], ["Zarger", "Adam", ""]]}, {"id": "1908.09984", "submitter": "Yixuan Zhang", "authors": "Elizabeth Stowell, Yixuan Zhang, Carmen Castaneda-Sceppa, Margie\n  Lachman, Andrea G. Parker", "title": "Caring for Alzheimer's Disease Caregivers: A Qualitative Study\n  Investigating Opportunities for Exergame Innovation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of informal caregivers for family members with Alzheimer's Disease\n(AD) is rising dramatically in the United States. AD caregivers\ndisproportionately experience numerous health problems and are often isolated\nwith little support. An active lifestyle can help prevent and mitigate physical\nand psychological health concerns amongst AD caregivers. Research has\ndemonstrated how pervasive exergames can encourage physical activity (PA) in\nthe general population, yet little work has explored how these tools can\naddress the significant PA barriers that AD caregivers face. To identify\nopportunities for design, we conducted semi-structured interviews and\nparticipatory design sessions with 14 informal caregivers of family members\nwith AD. Our findings characterize how becoming an AD caregiver profoundly\nimpacts one's ability to be active, perspectives on being active, and the ways\nthat exergames might best support this population. We discuss implications for\ndesign and how our findings challenge existing technological approaches to PA\npromotion.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 01:55:59 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Stowell", "Elizabeth", ""], ["Zhang", "Yixuan", ""], ["Castaneda-Sceppa", "Carmen", ""], ["Lachman", "Margie", ""], ["Parker", "Andrea G.", ""]]}, {"id": "1908.10013", "submitter": "Yantong Wang", "authors": "Yu Gu and Yantong Wang and Tao Liu and Yusheng Ji and Zhi Liu and Peng\n  Li and Xiaoyan Wang and Xin An and Fuji Ren", "title": "EmoSense: Computational Intelligence Driven Emotion Sensing via Wireless\n  Channel Data", "comments": "11 pages, accepted by IEEE Transactions On Emerging Topics In\n  Computational Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion is well-recognized as a distinguished symbol of human beings, and it\nplays a crucial role in our daily lives. Existing vision-based or sensor-based\nsolutions are either obstructive to use or rely on specialized hardware,\nhindering their applicability. This paper introduces EmoSense, a\nfirst-of-its-kind wireless emotion sensing system driven by computational\nintelligence. The basic methodology is to explore the physical expression of\nemotions from wireless channel response via data mining. The design and\nimplementation of EmoSense {face} two major challenges: extracting physical\nexpression from wireless channel data and recovering emotion from the\ncorresponding physical expression. For the former, we present a Fresnel zone\nbased theoretical model depicting the fingerprint of the physical expression on\nchannel response. For the latter, we design an efficient computational\nintelligence driven mechanism to recognize emotion from the corresponding\nfingerprints. We prototyped EmoSense on the commodity WiFi infrastructure and\ncompared it with main-stream sensor-based and vision-based approaches in the\nreal-world scenario. The numerical study over $3360$ cases confirms that\nEmoSense achieves a comparable performance to the vision-based and sensor-based\nrivals under different scenarios. EmoSense only leverages the low-cost and\nprevalent WiFi infrastructures and thus constitutes a tempting solution for\nemotion sensing.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 04:10:03 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Gu", "Yu", ""], ["Wang", "Yantong", ""], ["Liu", "Tao", ""], ["Ji", "Yusheng", ""], ["Liu", "Zhi", ""], ["Li", "Peng", ""], ["Wang", "Xiaoyan", ""], ["An", "Xin", ""], ["Ren", "Fuji", ""]]}, {"id": "1908.10048", "submitter": "Rainer B\\\"ohme", "authors": "Dominique Machuletz, Rainer B\\\"ohme", "title": "Multiple Purposes, Multiple Problems: A User Study of Consent Dialogs\n  after GDPR", "comments": null, "journal-ref": "Proceedings on Privacy Enhancing Technologies 2020", "doi": "10.2478/popets-2020-0037", "report-no": null, "categories": "cs.HC cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The European Union's General Data Protection Regulation (GDPR) requires\nwebsites to ask for consent to the use of cookies for \\emph{specific purposes}.\nThis enlarges the relevant design space for consent dialogs. Websites could try\nto maximize click-through rates and positive consent decision, even at the risk\nof users agreeing to more purposes than intended. We evaluate a practice\nobserved on popular websites by conducting an experiment with one control and\ntwo treatment groups ($N=150$ university students in two countries). We\nhypothesize that users' consent decision is influenced by (1) the number of\noptions, connecting to the theory of choice proliferation, and (2) the presence\nof a highlighted default button (``select all''), connecting to theories of\nsocial norms and deception in consumer research. The results show that\nparticipants who see a default button accept cookies for more purposes than the\ncontrol group, while being less able to correctly recall their choice. After\nbeing reminded of their choice, they regret it more often and perceive the\nconsent dialog as more deceptive than the control group. Whether users are\npresented one or three purposes has no significant effect on their decisions\nand perceptions. We discuss the results and outline policy implications.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 06:55:11 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 20:48:50 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Machuletz", "Dominique", ""], ["B\u00f6hme", "Rainer", ""]]}, {"id": "1908.10078", "submitter": "Wieslaw Kopec", "authors": "Kinga Skorupska, Manuel N\\'u\\~nez, Wies{\\l}aw Kope\\'c, Rados{\\l}aw\n  Nielek", "title": "A Comparative Study of Younger and Older Adults' Interaction with a\n  Crowdsourcing Android TV App for Detecting Errors in TEDx Video Subtitles", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-29387-1_25", "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we report the results of a pilot study comparing the older and\nyounger adults' interaction with an Android TV application which enables users\nto detect errors in video subtitles. Overall, the interaction with the\nTV-mediated crowdsourcing system relying on language profficiency was seen as\nintuitive, fun and accessible, but also cognitively demanding; more so for\nyounger adults who focused on the task of detecting errors, than for older\nadults who concentrated more on the meaning and edutainment aspect of the\nvideos. We also discuss participants' motivations and preliminary\nrecommendations for the design of TV-enabled crowdsourcing tasks and subtitle\nQA systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 08:37:16 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Skorupska", "Kinga", ""], ["N\u00fa\u00f1ez", "Manuel", ""], ["Kope\u0107", "Wies\u0142aw", ""], ["Nielek", "Rados\u0142aw", ""]]}, {"id": "1908.10125", "submitter": "Dimitri Ognibene", "authors": "Dimitri Ognibene, Lorenzo Mirante, Letizia Marchegiani", "title": "Proactive Intention Recognition for Joint Human-Robot Search and Rescue\n  Missions through Monte-Carlo Planning in POMDP Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proactively perceiving others' intentions is a crucial skill to effectively\ninteract in unstructured, dynamic and novel environments. This work proposes a\nfirst step towards embedding this skill in support robots for search and rescue\nmissions. Predicting the responders' intentions, indeed, will enable\nexploration approaches which will identify and prioritise areas that are more\nrelevant for the responder and, thus, for the task, leading to the development\nof safer, more robust and efficient joint exploration strategies. More\nspecifically, this paper presents an active intention recognition paradigm to\nperceive, even under sensory constraints, not only the target's position but\nalso the first responder's movements, which can provide information on his/her\nintentions (e.g. reaching the position where he/she expects the target to be).\nThis mechanism is implemented by employing an extension of Monte-Carlo-based\nplanning techniques for partially observable environments, where the reward\nfunction is augmented with an entropy reduction bonus. We test in simulation\nseveral configurations of reward augmentation, both information theoretic and\nnot, as well as belief state approximations and obtain substantial improvements\nover the basic approach.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 10:56:59 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Ognibene", "Dimitri", ""], ["Mirante", "Lorenzo", ""], ["Marchegiani", "Letizia", ""]]}, {"id": "1908.10127", "submitter": "Ke Chen", "authors": "Ke Chen", "title": "Learning-Based Video Game Development in MLP@UoM: An Overview", "comments": "8 pages, 15 figures Invited paper presented as a keynote speech,\n  ICEEIE'19 in Bali, Indonesia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, video games not only prevail in entertainment but also have\nbecome an alternative methodology for knowledge learning, skill acquisition and\nassistance for medical treatment as well as health care in education,\nvocational/military training and medicine. On the other hand, video games also\nprovide an ideal test bed for AI researches. To a large extent, however, video\ngame development is still a laborious yet costly process, and there are many\ntechnical challenges ranging from game generation to intelligent agent\ncreation. Unlike traditional methodologies, in Machine Learning and Perception\nLab at the University of Manchester (MLP@UoM), we advocate applying machine\nlearning to different tasks in video game development to address several\nchallenges systematically. In this paper, we overview the main progress made in\nMLP@UoM recently and have an outlook on the future research directions in\nlearning-based video game development arising from our works.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 11:05:15 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Chen", "Ke", ""]]}, {"id": "1908.10233", "submitter": "Jonas H\\\"ochst", "authors": "Lars Baumg\\\"artner, Jonas H\\\"ochst, Patrick Lampe, Ragnar Mogk, Artur\n  Sterz, Pascal Weisenburger, Mira Mezini, Bernd Freisleben", "title": "Smart Street Lights and Mobile Citizen Apps for Resilient Communication\n  in a Digital City", "comments": "2019 IEEE Global Humanitarian Technology Conference (GHTC)", "journal-ref": "2019 IEEE Global Humanitarian Technology Conference (GHTC)", "doi": "10.1109/GHTC46095.2019.9033134", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, nearly four billion people live in urban areas. Since this trend\nis increasing, natural disasters or terrorist attacks in such areas affect an\nincreasing number of people. While information and communication technology is\ncrucial for the operation of urban infrastructures and the well-being of its\ninhabitants, current technology is quite vulnerable to disruptions of various\nkinds. In future smart cities, a more resilient urban infrastructure is\nimperative to handle the increasing number of hazardous situations. We present\na novel resilient communication approach based on smart street lights as part\nof the public infrastructure. It supports people in their everyday life and\nadapts its functionality to the challenges of emergency situations. Our\napproach relies on various environmental sensors and in-situ processing for\nautomatic situation assessment, and a range of communication mechanisms (e.g.,\npublic WiFi hotspot functionality and mesh networking) for maintaining a\ncommunication network. Furthermore, resilience is not only achieved based on\ninfrastructure deployed by a digital city's municipality, but also based on\nintegrating citizens through software that runs on their mobile devices (e.g.,\nsmartphones and tablets). Web-based zero-installation and platform-agnostic\napps can switch to device-to-device communication to continue benefiting people\neven during a disaster situation. Our approach, featuring a covert channel for\nprofessional responders and the zero-installation app, is evaluated through a\nprototype implementation based on a commercially available street light.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 14:30:40 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Baumg\u00e4rtner", "Lars", ""], ["H\u00f6chst", "Jonas", ""], ["Lampe", "Patrick", ""], ["Mogk", "Ragnar", ""], ["Sterz", "Artur", ""], ["Weisenburger", "Pascal", ""], ["Mezini", "Mira", ""], ["Freisleben", "Bernd", ""]]}, {"id": "1908.10307", "submitter": "Youngjun Cho", "authors": "Youngjun Cho, Nadia Bianchi-Berthouze", "title": "Physiological and Affective Computing through Thermal Imaging: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thermal imaging-based physiological and affective computing is an emerging\nresearch area enabling technologies to monitor our bodily functions and\nunderstand psychological and affective needs in a contactless manner. However,\nup to recently, research has been mainly carried out in very controlled lab\nsettings. As small size and even low-cost versions of thermal video cameras\nhave started to appear on the market, mobile thermal imaging is opening its\ndoor to ubiquitous and real-world applications. Here we review the literature\non the use of thermal imaging to track changes in physiological cues relevant\nto affective computing and the technological requirements set so far. In doing\nso, we aim to establish computational and methodological pipelines from thermal\nimages of the human skin to affective states and outline the research\nopportunities and challenges to be tackled to make ubiquitous real-life thermal\nimaging-based affect monitoring a possibility.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 16:30:51 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Cho", "Youngjun", ""], ["Bianchi-Berthouze", "Nadia", ""]]}, {"id": "1908.10410", "submitter": "Daniel Probst", "authors": "Daniel Probst and Jean-Louis Reymond", "title": "Visualization of Very Large High-Dimensional Data Sets as Minimum\n  Spanning Trees", "comments": "33 pages, 14 figures, 1 table, supplementary information included", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chemical sciences are producing an unprecedented amount of large,\nhigh-dimensional data sets containing chemical structures and associated\nproperties. However, there are currently no algorithms to visualize such data\nwhile preserving both global and local features with a sufficient level of\ndetail to allow for human inspection and interpretation. Here, we propose a\nsolution to this problem with a new data visualization method, TMAP, capable of\nrepresenting data sets of up to millions of data points and arbitrary high\ndimensionality as a two-dimensional tree (http://tmap.gdb.tools).\nVisualizations based on TMAP are better suited than t-SNE or UMAP for the\nexploration and interpretation of large data sets due to their tree-like\nnature, increased local and global neighborhood and structure preservation, and\nthe transparency of the methods the algorithm is based on. We apply TMAP to the\nmost used chemistry data sets including databases of molecules such as ChEMBL,\nFDB17, the Natural Products Atlas, DSSTox, as well as to the MoleculeNet\nbenchmark collection of data sets. We also show its broad applicability with\nfurther examples from biology, particle physics, and literature.\n", "versions": [{"version": "v1", "created": "Fri, 16 Aug 2019 15:14:19 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 10:43:40 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 14:32:02 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Probst", "Daniel", ""], ["Reymond", "Jean-Louis", ""]]}, {"id": "1908.10411", "submitter": "Matthias Miller", "authors": "Matthias Miller, Hanna Sch\\\"afer, Matthias Kraus, Marc Leman, Daniel\n  Keim, Mennatallah El-Assady", "title": "Framing Visual Musicology through Methodology Transfer", "comments": "5 pages, position paper, 3 figures, 4th Workshop on Visualization for\n  the Digital Humanities, VIS4DH, IEEE VIS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper, we frame the field of Visual Musicology by providing\nan overview of well-established musicological sub-domains and their\ncorresponding analytic and visualization tasks. To foster collaborative,\ninterdisciplinary research, we discuss relevant data and domain\ncharacteristics. We give a description of the problem space, as well as the\ndesign space of musicology and discuss how existing problem-design mappings or\nsolutions from other fields can be transferred to musicology. We argue that,\nthrough methodology transfer, established methods can be exploited to solve\ncurrent musicological problems and show exemplary mappings from analytics\nfields related to text, geospatial, time-series, and other high-dimensional\ndata to musicology. Finally, we point out open challenges, discuss research\ngaps, and highlight future research opportunities.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2019 09:10:43 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Miller", "Matthias", ""], ["Sch\u00e4fer", "Hanna", ""], ["Kraus", "Matthias", ""], ["Leman", "Marc", ""], ["Keim", "Daniel", ""], ["El-Assady", "Mennatallah", ""]]}, {"id": "1908.10414", "submitter": "Sushant Kafle", "authors": "Sushant Kafle, Abraham Glasser, Sedeeq Al-khazraji, Larwan Berke,\n  Matthew Seita, Matt Huenerfauth", "title": "Artificial Intelligence Fairness in the Context of Accessibility\n  Research on Intelligent Systems for People who are Deaf or Hard of Hearing", "comments": "6 pages, ACM ASSETS 2019 Workshop on AI Fairness for People with\n  Disabilities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss issues of Artificial Intelligence (AI) fairness for people with\ndisabilities, with examples drawn from our research on human-computer\ninteraction (HCI) for AI-based systems for people who are Deaf or Hard of\nHearing (DHH). In particular, we discuss the need for inclusion of data from\npeople with disabilities in training sets, the lack of interpretability of AI\nsystems, ethical responsibilities of access technology researchers and\ncompanies, the need for appropriate evaluation metrics for AI-based access\ntechnologies (to determine if they are ready to be deployed and if they can be\ntrusted by users), and the ways in which AI systems influence human behavior\nand influence the set of abilities needed by users to successfully interact\nwith computing systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2019 19:13:55 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 16:56:28 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kafle", "Sushant", ""], ["Glasser", "Abraham", ""], ["Al-khazraji", "Sedeeq", ""], ["Berke", "Larwan", ""], ["Seita", "Matthew", ""], ["Huenerfauth", "Matt", ""]]}, {"id": "1908.10560", "submitter": "Xiaodong Cai", "authors": "Xiaodong Cai, Jingyi Ma, Wei Liu, Hemin Han, Lili Ma", "title": "Efficient Convolutional Neural Network for FMCW Radar Based Hand Gesture\n  Recognition", "comments": "Poster in Ubicomp 2019", "journal-ref": null, "doi": "10.1145/3341162.3343768", "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FMCW radar could detect object's range, speed and Angleof-Arrival, advantages\nare robust to bad weather, good range resolution, and good speed resolution. In\nthis paper, we consider the FMCW radar as a novel interacting interface on\nlaptop. We merge sequences of object's range, speed, azimuth information into\nsingle input, then feed to a convolution neural network to learn spatial and\ntemporal patterns. Our model achieved 96% accuracy on test set and real-time\ntest.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 06:01:42 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Cai", "Xiaodong", ""], ["Ma", "Jingyi", ""], ["Liu", "Wei", ""], ["Han", "Hemin", ""], ["Ma", "Lili", ""]]}, {"id": "1908.10585", "submitter": "Katrien Laenen", "authors": "Katrien Laenen and Marie-Francine Moens", "title": "Attention-based Fusion for Outfit Recommendation", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper describes an attention-based fusion method for outfit\nrecommendation which fuses the information in the product image and description\nto capture the most important, fine-grained product features into the item\nrepresentation. We experiment with different kinds of attention mechanisms and\ndemonstrate that the attention-based fusion improves item understanding. We\noutperform state-of-the-art outfit recommendation results on three benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 07:41:53 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Laenen", "Katrien", ""], ["Moens", "Marie-Francine", ""]]}, {"id": "1908.10808", "submitter": "R.Stuart Geiger", "authors": "R. Stuart Geiger", "title": "The Rise and Fall of the Note: Changing Paper Lengths in ACM CSCW,\n  2000-2018", "comments": "10 pages. To appear in PACMHCI, to be presented at ACM CSCW 2019. v3\n  fixes typos and updates some statistics", "journal-ref": "PACMHCI 3, CSCW (2019) 222", "doi": "10.1145/3359324", "report-no": null, "categories": "cs.HC cs.CY cs.DL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, I quantitatively examine various trends in the lengths of\npublished papers in ACM CSCW from 2000-2018, focusing on several major\ntransitions in editorial and reviewing policy. The focus is on the rise and\nfall of the 4-page note, which was introduced in 2004 as a separate submission\ntype to the 10-page double-column \"full paper\" format. From 2004-2012, 4-page\nnotes of 2,500 to 4,500 words consistently represented about 20-35\\% of all\npublications. In 2013, minimum and maximum page lengths were officially\nremoved, with no formal distinction made between full papers and notes. The\nnote soon completely disappeared as a distinct genre, which co-occurred with a\ntrend in steadily rising paper lengths. I discuss such findings both as they\ndirectly relate to local concerns in CSCW and in the context of longstanding\ntheoretical discussions around genre theory and how socio-technical structures\nand affordances impact participation in distributed, computer-mediated\norganizations and user-generated content platforms. There are many possible\nexplanations for the decline of the note and the emergence of longer and longer\npapers, which I identify for future work. I conclude by addressing the\nimplications of such findings for the CSCW community, particularly given how\ngenre norms impact what kinds of scholarship and scholars thrive in CSCW, as\nwell as whether new top-down rules or bottom-up guidelines ought to be\ndeveloped around paper lengths and different kinds of contributions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 16:18:19 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 14:55:58 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 00:08:42 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Geiger", "R. Stuart", ""]]}, {"id": "1908.10870", "submitter": "Shuo Zhang", "authors": "Jason Shuo Zhang, Chenhao Tan, and Qin Lv", "title": "Intergroup Contact in the Wild: Characterizing Language Differences\n  between Intergroup and Single-group Members in NBA-related Discussion Forums", "comments": "ACM Conference on Computer-Supported Cooperative Work and Social\n  Computing (CSCW), 2019", "journal-ref": null, "doi": "10.1145/3359295", "report-no": null, "categories": "cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intergroup contact has long been considered as an effective strategy to\nreduce prejudice between groups. However, recent studies suggest that exposure\nto opposing groups in online platforms can exacerbate polarization. To further\nunderstand the behavior of individuals who actively engage in intergroup\ncontact in practice, we provide a large-scale observational study of intragroup\nbehavioral differences between members with and without intergroup contact. We\nleverage the existing structure of NBA-related discussion forums on Reddit to\nstudy the context of professional sports. We identify fans of each NBA team as\nmembers of a group and trace whether they have intergroup contact. Our results\nshow that members with intergroup contact use more negative and abusive\nlanguage in their affiliated group than those without such contact, after\ncontrolling for activity levels. We further quantify different levels of\nintergroup contact and show that there may exist nonlinear mechanisms regarding\nhow intergroup contact relates to intragroup behavior. Our findings provide\ncomplementary evidence to experimental studies in a novel context and also shed\nlight on possible reasons for the different outcomes in prior studies.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 18:00:03 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Zhang", "Jason Shuo", ""], ["Tan", "Chenhao", ""], ["Lv", "Qin", ""]]}, {"id": "1908.10948", "submitter": "Binxuan Huang", "authors": "Binxuan Huang, Kathleen M. Carley", "title": "A Large-Scale Empirical Study of Geotagging Behavior on Twitter", "comments": "Accepted by ASONAM 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geotagging on social media has become an important proxy for understanding\npeople's mobility and social events. Research that uses geotags to infer public\nopinions relies on several key assumptions about the behavior of geotagged and\nnon-geotagged users. However, these assumptions have not been fully validated.\nLack of understanding the geotagging behavior prohibits people further\nutilizing it. In this paper, we present an empirical study of geotagging\nbehavior on Twitter based on more than 40 billion tweets collected from 20\nmillion users. There are three main findings that may challenge these common\nassumptions. Firstly, different groups of users have different geotagging\npreferences. For example, less than 3% of users speaking in Korean are\ngeotagged, while more than 40% of users speaking in Indonesian use geotags.\nSecondly, users who report their locations in profiles are more likely to use\ngeotags, which may affects the generability of those location prediction\nsystems on non-geotagged users. Thirdly, strong homophily effect exists in\nusers' geotagging behavior, that users tend to connect to friends with similar\ngeotagging preferences.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 21:18:40 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Huang", "Binxuan", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "1908.10954", "submitter": "Allen Yilun Lin", "authors": "Isaac Johnson, Allen Yilun Lin, Toby Jia-Jun Li, Andrew Hall, Aaron\n  Halfaker, Johannes Sch\\\"oning, Brent Hecht", "title": "Not at Home on the Range: Peer Production and the Urban/Rural Divide", "comments": "10 pages, published on CHI'16", "journal-ref": "Proceedings of the 2016 CHI Conference on Human Factors in\n  Computing Systems", "doi": "10.1145/2858036.2858123", "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia articles about places, OpenStreetMap features, and other forms of\npeer-produced content have become critical sources of geographic knowledge for\nhumans and intelligent technologies. In this paper, we explore the\neffectiveness of the peer production model across the rural/urban divide, a\ndivide that has been shown to be an important factor in many online social\nsystems. We find that in both Wikipedia and OpenStreetMap, peer-produced\ncontent about rural areas is of systematically lower quality, is less likely to\nhave been produced by contributors who focus on the local area, and is more\nlikely to have been generated by automated software agents (i.e. bots). We then\ncodify the systemic challenges inherent to characterizing rural phenomena\nthrough peer production and discuss potential solutions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2019 21:42:14 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Johnson", "Isaac", ""], ["Lin", "Allen Yilun", ""], ["Li", "Toby Jia-Jun", ""], ["Hall", "Andrew", ""], ["Halfaker", "Aaron", ""], ["Sch\u00f6ning", "Johannes", ""], ["Hecht", "Brent", ""]]}, {"id": "1908.11344", "submitter": "Xiangyang He", "authors": "Xiangyang He, Yubo Tao, Qirui Wang, Hai Lin", "title": "Multivariate Spatial Data Visualization: A Survey", "comments": "16 pages, 5 figures. Corresponding author: Yubo Tao", "journal-ref": "Journal of Visualization, (2019), 1-16", "doi": "10.1007/s12650-019-00584-3", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate spatial data plays an important role in computational science\nand engineering simulations. The potential features and hidden relationships in\nmultivariate data can assist scientists to gain an in-depth understanding of a\nscientific process, verify a hypothesis and further discover a new physical or\nchemical law. In this paper, we present a comprehensive survey of the\nstate-of-the-art techniques for multivariate spatial data visualization. We\nfirst introduce the basic concept and characteristics of multivariate spatial\ndata, and describe three main tasks in multivariate data visualization: feature\nclassification, fusion visualization, and correlation analysis. Finally, we\nprospect potential research topics for multivariate data visualization\naccording to the current research.\n", "versions": [{"version": "v1", "created": "Sun, 18 Aug 2019 06:07:17 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["He", "Xiangyang", ""], ["Tao", "Yubo", ""], ["Wang", "Qirui", ""], ["Lin", "Hai", ""]]}, {"id": "1908.11566", "submitter": "Jessica McBroom", "authors": "Jessica McBroom, Irena Koprinska and Kalina Yacef", "title": "A Survey of Automated Programming Hint Generation -- The HINTS Framework", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated tutoring systems offer the flexibility and scalability necessary to\nfacilitate the provision of high quality and universally accessible programming\neducation. In order to realise the full potential of these systems, recent work\nhas proposed a diverse range of techniques for automatically generating hints\nto assist students with programming exercises. This paper integrates these\napparently disparate approaches into a coherent whole. Specifically, it\nemphasises that all hint techniques can be understood as a series of simpler\ncomponents with similar properties. Using this insight, it presents a simple\nframework for describing such techniques, the Hint Iteration by Narrow-down and\nTransformation Steps (HINTS) framework, and it surveys recent work in the\ncontext of this framework. It discusses important implications of the survey\nand framework, including the need to further develop evaluation methods and the\nimportance of considering hint technique components when designing,\ncommunicating and evaluating hint systems. Ultimately, this paper is designed\nto facilitate future opportunities for the development, extension and\ncomparison of automated programming hint techniques in order to maximise their\neducational potential.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 07:06:28 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["McBroom", "Jessica", ""], ["Koprinska", "Irena", ""], ["Yacef", "Kalina", ""]]}, {"id": "1908.11614", "submitter": "Teresa Castle-Green", "authors": "Teresa Castle-Green, Stuart Reeves, Joel E. Fischer and Boriana Koleva", "title": "Designing with Data: A Case Study", "comments": "Proceedings of the CHI 2019 Workshop on New Directions for the IoT:\n  Automate, Share, Build, and Care, (arXiv:cs/0101200)", "journal-ref": null, "doi": null, "report-no": "IOTD/2019/11", "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  As the Internet of Things continues to take hold in the commercial world, the\nteams designing these new technologies are constantly evolving and turning\ntheir hand to uncharted territory. This is especially key within the field of\nsecondary service design as businesses attempt to utilize and find value in the\nsensor data being produced by connected products. This paper discusses the ways\nin which a commercial design team use smart thermostat data to prototype an\nadvice-giving chatbot. The team collaborate to produce a chat sequence through\ncareful ordering of data & reasoning about customer reactions. The paper\ncontributes important insights into design methods being used in practice\nwithin the under researched areas of chatbot prototyping and secondary service\ndesign.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 09:38:06 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Castle-Green", "Teresa", ""], ["Reeves", "Stuart", ""], ["Fischer", "Joel E.", ""], ["Koleva", "Boriana", ""]]}, {"id": "1908.11706", "submitter": "Pablo Barros", "authors": "Pablo Barros, Nikhil Churamani, Angelica Lim, Stefan Wermter", "title": "The OMG-Empathy Dataset: Evaluating the Impact of Affective Behavior in\n  Storytelling", "comments": "2019 8th International Conference on Affective Computing and\n  Intelligent Interaction (ACII)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Processing human affective behavior is important for developing intelligent\nagents that interact with humans in complex interaction scenarios. A large\nnumber of current approaches that address this problem focus on classifying\nemotion expressions by grouping them into known categories. Such strategies\nneglect, among other aspects, the impact of the affective responses from an\nindividual on their interaction partner thus ignoring how people empathize\ntowards each other. This is also reflected in the datasets used to train models\nfor affective processing tasks. Most of the recent datasets, in particular, the\nones which capture natural interactions (\"in-the-wild\" datasets), are designed,\ncollected, and annotated based on the recognition of displayed affective\nreactions, ignoring how these displayed or expressed emotions are perceived. In\nthis paper, we propose a novel dataset composed of dyadic interactions\ndesigned, collected and annotated with a focus on measuring the affective\nimpact that eight different stories have on the listener. Each video of the\ndataset contains around 5 minutes of interaction where a speaker tells a story\nto a listener. After each interaction, the listener annotated, using a valence\nscale, how the story impacted their affective state, reflecting how they\nempathized with the speaker as well as the story. We also propose different\nevaluation protocols and a baseline that encourages participation in the\nadvancement of the field of artificial empathy and emotion contagion.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 12:53:51 GMT"}], "update_date": "2019-09-02", "authors_parsed": [["Barros", "Pablo", ""], ["Churamani", "Nikhil", ""], ["Lim", "Angelica", ""], ["Wermter", "Stefan", ""]]}, {"id": "1908.11752", "submitter": "Filipo Sharevski", "authors": "Filipo Sharevski, Paige Treebridge, Jessica Westbrook", "title": "Manipulation of Perceived Politeness in a Web-based Email Discourse\n  Through a Malicious Browser Extension", "comments": null, "journal-ref": "ACM New Security Paradigms Workshop NSPW 2019", "doi": "10.1145/3368860.3368863", "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a specific man-in-the-middle exploit: Ambient Tactical\nDeception (ATD) in online communication, realized via a malicious web browser\nextension. Extensions manipulate web content in unobtrusive ways as ambient\nintermediaries of the overall browsing experience. In our previous work, we\ndemonstrated that it is possible to employ tactical deception by making covert\nchanges in the text content of a web page, regardless of the source. In this\nwork, we investigated the application of ATD in a web-based email discourse\nwhere the objective is to manipulate the interpersonal perception without the\nknowledge of the involved parties. We focus on web-based email text because it\nis asynchronous and usually revised for clarity and politeness. Previous\nresearch has demonstrated that people's perception of politeness in online\ncommunication is based on three factors: the degree of imposition, the power of\nthe receiver over the sender, and the social distance between them. We\ninterviewed participants about their perception of these factors to establish\nthe plausibility of ATD for email discourse. The results indicate that by\ncovertly altering the politeness strategy in an email, it is possible for an\nATD attacker to manipulate the receiver's perception on all of the politeness\nfactors. Our findings support the Brown and Levinson's politeness theory and\nWalther's hyperpersonal model of email communication.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 14:17:56 GMT"}, {"version": "v2", "created": "Tue, 24 Dec 2019 19:14:31 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sharevski", "Filipo", ""], ["Treebridge", "Paige", ""], ["Westbrook", "Jessica", ""]]}]