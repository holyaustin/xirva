[{"id": "1706.00007", "submitter": "Bo Wu", "authors": "Bo Wu and Bin Hu and Hai Lin", "title": "A Learning Based Optimal Human Robot Collaboration with Linear Temporal\n  Logic Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers an optimal task allocation problem for human robot\ncollaboration in human robot systems with persistent tasks. Such human robot\nsystems consist of human operators and intelligent robots collaborating with\neach other to accomplish complex tasks that cannot be done by either part\nalone. The system objective is to maximize the probability of successfully\nexecuting persistent tasks that are formulated as linear temporal logic\nspecifications and minimize the average cost between consecutive visits of a\nparticular proposition. This paper proposes to model the human robot\ncollaboration under a framework with the composition of multiple Markov\nDecision Process (MDP) with possibly unknown transition probabilities, which\ncharacterizes how human cognitive states, such as human trust and fatigue,\nstochastically change with the robot performance. Under the unknown MDP models,\nan algorithm is developed to learn the model and obtain an optimal task\nallocation policy that minimizes the expected average cost for each task cycle\nand maximizes the probability of satisfying linear temporal logic constraints.\nMoreover, this paper shows that the difference between the optimal policy based\non the learned model and that based on the underlying ground truth model can be\nbounded by arbitrarily small constant and large confidence level with\nsufficient samples. The case study of an assembly process demonstrates the\neffectiveness and benefits of our proposed learning based human robot\ncollaboration.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 17:52:48 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Wu", "Bo", ""], ["Hu", "Bin", ""], ["Lin", "Hai", ""]]}, {"id": "1706.00056", "submitter": "Tyler Kaczmarek", "authors": "Bruce Berg, Tyler Kaczmarek, Alfred Kobsa, and Gene Tsudik", "title": "Lights, Camera, Action! Exploring Effects of Visual Distractions on\n  Completion of Security Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human errors in performing security-critical tasks are typically blamed on\nthe complexity of those tasks. However, such errors can also occur because of\n(possibly unexpected) sensory distractions. A sensory distraction that produces\nnegative effects can be abused by the adversary that controls the environment.\nMeanwhile, a distraction with positive effects can be artificially introduced\nto improve user performance.\n  The goal of this work is to explore the effects of visual stimuli on the\nperformance of security-critical tasks. To this end, we experimented with a\nlarge number of subjects who were exposed to a range of unexpected visual\nstimuli while attempting to perform Bluetooth Pairing. Our results clearly\ndemonstrate substantially increased task completion times and markedly lower\ntask success rates. These negative effects are noteworthy, especially, when\ncontrasted with prior results on audio distractions which had positive effects\non performance of similar tasks. Experiments were conducted in a novel (fully\nautomated and completely unattended) experimental environment. This yielded\nmore uniform experiments, better scalability and significantly lower financial\nand logistical burdens. We discuss this experience, including benefits and\nlimitations of the unattended automated experiment paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 19:27:07 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Berg", "Bruce", ""], ["Kaczmarek", "Tyler", ""], ["Kobsa", "Alfred", ""], ["Tsudik", "Gene", ""]]}, {"id": "1706.00069", "submitter": "Qiyu Zhi", "authors": "Qiyu Zhi, Ronald Metoyer", "title": "Recognizing Handwritten Source Code", "comments": "7 pages, 6 figures, Proceedings of the 2017 Graphics Interface\n  conference", "journal-ref": null, "doi": "10.20380/GI2017.21", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supporting programming on touchscreen devices requires effective text input\nand editing methods. Unfortunately, the virtual keyboard can be inefficient and\nuses valuable screen space on already small devices. Recent advances in stylus\ninput make handwriting a potentially viable text input solution for programming\non touchscreen devices. The primary barrier, however, is that handwriting\nrecognition systems are built to take advantage of the rules of natural\nlanguage, not those of a programming language. In this paper, we explore this\nparticular problem of handwriting recognition for source code. We collect and\nmake publicly available a dataset of handwritten Python code samples from 15\nparticipants and we characterize the typical recognition errors for this\nhandwritten Python source code when using a state-of-the-art handwriting\nrecognition tool. We present an approach to improve the recognition accuracy by\naugmenting a handwriting recognizer with the programming language grammar\nrules. Our experiment on the collected dataset shows an 8.6% word error rate\nand a 3.6% character error rate which outperforms standard handwriting\nrecognition systems and compares favorably to typing source code on virtual\nkeyboards.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:07:12 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Zhi", "Qiyu", ""], ["Metoyer", "Ronald", ""]]}, {"id": "1706.00070", "submitter": "Azza Abouzied", "authors": "Matteo Brucato, Azza Abouzied, Chris Blauvelt", "title": "Redistributing Funds across Charitable Crowdfunding Campaigns", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On Kickstarter only 36% of crowdfunding campaigns successfully raise\nsufficient funds for their projects. In this paper, we explore the possibility\nof redistribution of crowdfunding donations to increase the chances of success.\nWe define several intuitive redistribution policies and, using data from a real\ncrowdfunding platform, LaunchGood, we assess the potential improvement in\ncampaign fundraising success rates. We find that an aggressive redistribution\nscheme can boost campaign success rates from 37% to 79%, but such\nchoice-agnostic redistribution schemes come at the cost of disregarding donor\npreferences. Taking inspiration from offline giving societies and donor clubs,\nwe build a case for choice preserving redistribution schemes that strike a\nbalance between increasing the number of successful campaigns and respecting\ngiving preference. We find that choice-preserving redistribution can easily\nachieve campaign success rates of 48%. Finally, we discuss the implications of\nthese different redistribution schemes for the various stakeholders in the\ncrowdfunding ecosystem.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 20:14:21 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Brucato", "Matteo", ""], ["Abouzied", "Azza", ""], ["Blauvelt", "Chris", ""]]}, {"id": "1706.00130", "submitter": "Huan Ling", "authors": "Huan Ling, Sanja Fidler", "title": "Teaching Machines to Describe Images via Natural Language Feedback", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots will eventually be part of every household. It is thus critical to\nenable algorithms to learn from and be guided by non-expert users. In this\npaper, we bring a human in the loop, and enable a human teacher to give\nfeedback to a learning agent in the form of natural language. We argue that a\ndescriptive sentence can provide a much stronger learning signal than a numeric\nreward in that it can easily point to where the mistakes are and how to correct\nthem. We focus on the problem of image captioning in which the quality of the\noutput can easily be judged by non-experts. We propose a hierarchical\nphrase-based captioning model trained with policy gradients, and design a\nfeedback network that provides reward to the learner by conditioning on the\nhuman-provided feedback. We show that by exploiting descriptive feedback our\nmodel learns to perform better than when given independently written human\ncaptions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 00:24:55 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 16:47:40 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ling", "Huan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1706.00138", "submitter": "Nalin Asanka Gamagedara Arachchilage", "authors": "Chamila Wijayarathna, Nalin Asanka Gamagedara Arachchilage, Jill Slay", "title": "Using Cognitive Dimensions Questionnaire to Evaluate the Usability of\n  Security APIs", "comments": "4, 28th Annual Workshop of the Psychology of Programming Interest\n  Group (PPIG), Delft, Netherlands, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usability issues that exist in security APIs cause programmers to embed those\nsecurity APIs incorrectly to the applications they develop. This results in\nintroduction of security vulnerabilities to those applications. One of the main\nreasons for security APIs to be not usable is currently there is no proper\nmethod by which the usability issues of security APIs can be identified. We\nconducted a study to assess the effectiveness of the cognitive dimensions\nquestionnaire based usability evaluation methodology in evaluating the\nusability of security APIs. We used a cognitive dimensions based generic\nquestionnaire to collect feedback from programmers who participated in the\nstudy. Results revealed interesting facts about the prevailing usability issues\nin four commonly used security APIs and the capability of the methodology to\nidentify those issues.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 00:56:52 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 07:30:40 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Wijayarathna", "Chamila", ""], ["Arachchilage", "Nalin Asanka Gamagedara", ""], ["Slay", "Jill", ""]]}, {"id": "1706.00176", "submitter": "Anh Nguyen", "authors": "Anh Nguyen", "title": "3DTouch: Towards a Wearable 3D Input Device for 3D Applications", "comments": "MS thesis, University of Wyoming, ProQuest", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-dimensional (3D) applications have come to every corner of life. We\npresent 3DTouch, a novel 3D wearable input device worn on the fingertip for\ninteracting with 3D applications. 3DTouch is self-contained, and designed to\nuniversally work on various 3D platforms. The device employs touch input for\nthe benefits of passive haptic feedback, and movement stability. Moreover, with\ntouch interaction, 3DTouch is conceptually less fatiguing to use over many\nhours than 3D spatial input devices such as Kinect. Our approach relies on\nrelative positioning technique using an optical laser sensor and a 9-DOF\ninertial measurement unit. We implemented a set of 3D interaction techniques\nincluding selection, translation, and rotation using 3DTouch. An evaluation\nalso demonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees\nfor subtle touch interaction in 3D space. With 3DTouch project, we would like\nto provide an input device that reduces the gap between 3D applications and\nusers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 06:43:54 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Nguyen", "Anh", ""]]}, {"id": "1706.01123", "submitter": "Dandan Huang", "authors": "Dandan Huang, Melanie Tory and Lyn Bartram", "title": "A Field Study of On-Calendar Visualizations", "comments": null, "journal-ref": "Proceeding GI '16 Proceedings of the 42nd Graphics Interface\n  Conference Pages 13-20, 2016", "doi": "10.20380/GI2016.03", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feedback tools help people to monitor information about themselves to improve\ntheir health, sustainability practices, or personal well-being. Yet reasoning\nabout personal data (e.g., pedometer counts, blood pressure readings, or home\nelectricity consumption) to gain a deep understanding of your current practices\nand how to change can be challenging with the data alone. We integrate\nquantitative feedback data within a personal digital calendar; this approach\naims to make the feedback data readily accessible and more comprehensible. We\nreport on an eight-week field study of an on-calendar visualization tool.\nResults showed that a personal calendar can provide rich context for people to\nreason about their feedback data. The on-calendar visualization enabled people\nto quickly identify and reason about regular patterns and anomalies. Based on\nour results, we also derived a model of the behavior feedback process that\nextends existing technology adoption models. With that, we reflected on\npotential barriers for the ongoing use of feedback tools.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 18:33:57 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Huang", "Dandan", ""], ["Tory", "Melanie", ""], ["Bartram", "Lyn", ""]]}, {"id": "1706.01248", "submitter": "Jingyi Wang", "authors": "Jingyi Wang, Jiro Tanaka", "title": "Aiding autobiographic memory by using wearable devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we investigate the effectiveness of two distinct techniques\n(Special Moment Approach & Spatial Frequency Approach) for reviewing the\nlifelogs, which were collected by lifeloggers who were willing to use a\nwearable camera and a bracelet simultaneously for two days. Generally, Special\nmoment approach is a technique for extracting episodic events and Spatial\nfrequency approach is a technique for associating visual with temporal and\nlocation information, especially heat map is applied as the spatial data for\nexpressing frequency awareness. Based on that, the participants were asked to\nfill in two post-study questionnaires for evaluating the effectiveness of those\ntwo techniques and their combination. The preliminary result showed the\npositive potential of exploring individual lifelogs using our approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 09:23:47 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wang", "Jingyi", ""], ["Tanaka", "Jiro", ""]]}, {"id": "1706.01340", "submitter": "Robin Ruede", "authors": "Robin Ruede, Markus M\\\"uller, Sebastian St\\\"uker, Alex Waibel", "title": "Yeah, Right, Uh-Huh: A Deep Learning Backchannel Predictor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CV cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using supporting backchannel (BC) cues can make human-computer interaction\nmore social. BCs provide a feedback from the listener to the speaker indicating\nto the speaker that he is still listened to. BCs can be expressed in different\nways, depending on the modality of the interaction, for example as gestures or\nacoustic cues. In this work, we only considered acoustic cues. We are proposing\nan approach towards detecting BC opportunities based on acoustic input features\nlike power and pitch. While other works in the field rely on the use of a\nhand-written rule set or specialized features, we made use of artificial neural\nnetworks. They are capable of deriving higher order features from input\nfeatures themselves. In our setup, we first used a fully connected feed-forward\nnetwork to establish an updated baseline in comparison to our previously\nproposed setup. We also extended this setup by the use of Long Short-Term\nMemory (LSTM) networks which have shown to outperform feed-forward based setups\non various tasks. Our best system achieved an F1-Score of 0.37 using power and\npitch features. Adding linguistic information using word2vec, the score\nincreased to 0.39.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 17:05:26 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ruede", "Robin", ""], ["M\u00fcller", "Markus", ""], ["St\u00fcker", "Sebastian", ""], ["Waibel", "Alex", ""]]}, {"id": "1706.01521", "submitter": "Nathan Hodas", "authors": "Lyndsey Franklin, Kristina Lerman, Nathan Hodas", "title": "Will Break for Productivity: Generalized Symptoms of Cognitive Depletion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the symptoms of cognitive depletion as they relate\nto generalized knowledge workers. We unify previous findings within a single\nanalytical model of cognitive depletion. Our purpose is to develop a model that\nwill help us predict when a person has reached a sufficient state of cognitive\ndepletion such that taking a break or some other restorative action will\nbenefit both his or her own wellbeing and the quality of his or her\nperformance. We provide a definition of each symptom in our model as well as\nthe effect it would have on a knowledge worker's ability to work productively.\nWe discuss methods to detect each symptom that do not require self assessment.\nUnderstanding symptoms of cognitive depletion provides the ability to support\nhuman knowledge workers by reducing the stress involved with cognitive and work\noverload while maintaining or improving the quality of their performance.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 20:02:19 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Franklin", "Lyndsey", ""], ["Lerman", "Kristina", ""], ["Hodas", "Nathan", ""]]}, {"id": "1706.01523", "submitter": "Nathan Hodas", "authors": "Lyndsey Franklin, Nathan Hodas", "title": "Cognitive Depletion in the Wild: a Case Study of NMR Spectroscopy\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NMR spectroscopy analysis is a detail-oriented analytic feat that typically\nrequires specific domain expertise and hours of concentration. This work\npresents an ethnographic-style study of this analysis process in the context of\nevaluating the symptoms of cognitive depletion. The repeated, non-trivial\ndecisions required by and the time-consuming nature of NMR spectroscopy\nanalysis make it an ideal, real-world scenario to study the symptoms of\ncognitive depletion, its effect on workflow and performance, and potential\nstrategies for mitigating its deleterious effects.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 20:05:10 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Franklin", "Lyndsey", ""], ["Hodas", "Nathan", ""]]}, {"id": "1706.01728", "submitter": "Jelena Mladenovic", "authors": "Jelena Mladenovi\\'c (Potioc), J\\'er\\'emy Frey (Potioc), Manon\n  Bonnet-Save (Potioc), J\\'er\\'emie Mattout, Fabien Lotte (LaBRI, Potioc)", "title": "The Impact of Flow in an EEG-based Brain Computer Interface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major issues in Brain Computer Interfaces (BCIs) include low usability and\npoor user performance. This paper tackles them by ensuring the users to be in a\nstate of immersion, control and motivation, called state of flow. Indeed, in\nvarious disciplines, being in the state of flow was shown to improve\nperformances and learning. Hence, we intended to draw BCI users in a flow state\nto improve both their subjective experience and their performances. In a Motor\nImagery BCI game, we manipulated flow in two ways: 1) by adapting the task\ndifficulty and 2) by using background music. Results showed that the difficulty\nadaptation induced a higher flow state, however music had no effect. There was\na positive correlation between subjective flow scores and offline performance,\nalthough the flow factors had no effect (adaptation) or negative effect (music)\non online performance. Overall, favouring the flow state seems a promising\napproach for enhancing users' satisfaction, although its complexity requires\nmore thorough investigations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 12:21:44 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Mladenovi\u0107", "Jelena", "", "Potioc"], ["Frey", "J\u00e9r\u00e9my", "", "Potioc"], ["Bonnet-Save", "Manon", "", "Potioc"], ["Mattout", "J\u00e9r\u00e9mie", "", "LaBRI, Potioc"], ["Lotte", "Fabien", "", "LaBRI, Potioc"]]}, {"id": "1706.01919", "submitter": "Nathan Hodas", "authors": "Lyndsey Franklin, Kyungsik Han, Zhuanyi Huang, Dustin Arendt, Nathan\n  Hodas", "title": "Understanding Cognitive Depletion in Novice NMR Analysts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of a user study with novice NMR analysts (N=19)\ninvolving a gamified simulation of the NMR analysis process. Participants\nsolved randomly generated spectrum puzzles for up to three hours. We used eye\ntracking, event logging, and observations to record symptoms of cognitive\ndepletion while participants worked. Analysis of results indicate that we can\ndetect both signs of learning and signs of cognitive depletion in participants\nover the course of the three hours. Participants' break strategies did not\npredict or reflect game scores, but certain symptoms appear predictive of\nbreaks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 18:31:30 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Franklin", "Lyndsey", ""], ["Han", "Kyungsik", ""], ["Huang", "Zhuanyi", ""], ["Arendt", "Dustin", ""], ["Hodas", "Nathan", ""]]}, {"id": "1706.02149", "submitter": "Yoji Yamato", "authors": "Yoji Yamato", "title": "Experiments of posture estimation on vehicles using wearable\n  acceleration sensors", "comments": "4 pages, 4 figures, The 3rd IEEE International Conference on Big Data\n  Security on Cloud (BigDataSecurity 2017), pp.14-17, Beijing, May 2017", "journal-ref": "The 3rd IEEE International Conference on Big Data Security on\n  Cloud (BigDataSecurity 2017), pp.14-17, May 2017. (c) 2017\n  BigDataSecurity2017", "doi": null, "report-no": null, "categories": "cs.HC cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study methods to estimate drivers' posture in vehicles\nusing acceleration data of wearable sensor and conduct a field test. Recently,\nsensor technologies have been progressed. Solutions of safety management to\nanalyze vital data acquired from wearable sensor and judge work status are\nproposed. To prevent huge accidents, demands for safety management of bus and\ntaxi are high. However, acceleration of vehicles is added to wearable sensor in\nvehicles, and there is no guarantee to estimate drivers' posture accurately.\nTherefore, in this paper, we study methods to estimate driving posture using\nacceleration data acquired from T-shirt type wearable sensor hitoe, conduct\nfield tests and implement a sample application.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 05:48:11 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 04:16:07 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Yamato", "Yoji", ""]]}, {"id": "1706.02499", "submitter": "Burak Benligiray", "authors": "Burak Benligiray, Cihan Topal, Cuneyt Akinlar", "title": "SliceType: Fast Gaze Typing with a Merging Keyboard", "comments": null, "journal-ref": "Journal on Multimodal User Interfaces, 2018", "doi": "10.1007/s12193-018-0285-z", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jitter is an inevitable by-product of gaze detection. Because of this, gaze\ntyping tends to be a slow and frustrating process. In this paper, we propose\nSliceType, a soft keyboard that is optimized for gaze input. Our main design\nobjective is to use the screen area more efficiently by allocating a larger\narea to the target keys. We achieve this by determining the keys that will not\nbe used for the next input, and allocating their space to the adjacent keys\nwith a merging animation. Larger keys are faster to navigate towards, and easy\nto dwell on in the presence of eye tracking jitter. As a result, the user types\nfaster and more comfortably. In addition, we employ a word completion scheme\nthat complements gaze typing mechanics. A character and a related prediction is\ndisplayed at each key. Dwelling at a key enters the character, and\ndouble-dwelling enters the prediction. While dwelling on a key to enter a\ncharacter, the user reads the related prediction effortlessly. The improvements\nprovided by these features are quantified using the Fitts' law. The performance\nof the proposed keyboard is compared with two other soft keyboards designed for\ngaze typing, Dasher and GazeTalk. 37 novice users gaze-typed a piece of text\nusing all three keyboards. The results of the experiment show that the proposed\nkeyboard allows faster typing, and is more preferred by the users.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 10:06:52 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 13:39:05 GMT"}, {"version": "v3", "created": "Sun, 18 Mar 2018 19:14:36 GMT"}, {"version": "v4", "created": "Thu, 27 Dec 2018 13:59:19 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Benligiray", "Burak", ""], ["Topal", "Cihan", ""], ["Akinlar", "Cuneyt", ""]]}, {"id": "1706.02542", "submitter": "Chris Larson", "authors": "Chris Larson, Josef Spjut, Ross Knepper, Robert Shepherd", "title": "A Deformable Interface for Human Touch Recognition using Stretchable\n  Carbon Nanotube Dielectric Elastomer Sensors and Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User interfaces provide an interactive window between physical and virtual\nenvironments. A new concept in the field of human-computer interaction is a\nsoft user interface; a compliant surface that facilitates touch interaction\nthrough deformation. Despite the potential of these interfaces, they currently\nlack a signal processing framework that can efficiently extract information\nfrom their deformation. Here we present OrbTouch, a device that uses\nstatistical learning algorithms, based on convolutional neural networks, to map\ndeformations from human touch to categorical labels (i.e., gestures) and touch\nlocation using stretchable capacitor signals as inputs. We demonstrate this\napproach by using the device to control the popular game Tetris. OrbTouch\nprovides a modular, robust framework to interpret deformation in soft media,\nlaying a foundation for new modes of human computer interaction through shape\nchanging solids.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 12:20:44 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 13:17:27 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 14:45:12 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Larson", "Chris", ""], ["Spjut", "Josef", ""], ["Knepper", "Ross", ""], ["Shepherd", "Robert", ""]]}, {"id": "1706.02637", "submitter": "Jun Sun", "authors": "Korok Sengupta, Jun Sun, Raphael Menges, Chandan Kumar, Steffen Staab", "title": "Analyzing the Impact of Cognitive Load in Evaluating Gaze-based Typing", "comments": "6 pages, 4 figures, IEEE CBMS 2017", "journal-ref": null, "doi": "10.1109/CBMS.2017.134", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaze-based virtual keyboards provide an effective interface for text entry by\neye movements. The efficiency and usability of these keyboards have\ntraditionally been evaluated with conventional text entry performance measures\nsuch as words per minute, keystrokes per character, backspace usage, etc.\nHowever, in comparison to the traditional text entry approaches, gaze-based\ntyping involves natural eye movements that are highly correlated with human\nbrain cognition. Employing eye gaze as an input could lead to excessive mental\ndemand, and in this work we argue the need to include cognitive load as an eye\ntyping evaluation measure. We evaluate three variations of gaze-based virtual\nkeyboards, which implement variable designs in terms of word suggestion\npositioning. The conventional text entry metrics indicate no significant\ndifference in the performance of the different keyboard designs. However, STFT\n(Short-time Fourier Transform) based analysis of EEG signals indicate variances\nin the mental workload of participants while interacting with these designs.\nMoreover, the EEG analysis provides insights into the user's cognition\nvariation for different typing phases and intervals, which should be considered\nin order to improve eye typing usability.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 15:21:23 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 13:45:34 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Sengupta", "Korok", ""], ["Sun", "Jun", ""], ["Menges", "Raphael", ""], ["Kumar", "Chandan", ""], ["Staab", "Steffen", ""]]}, {"id": "1706.02757", "submitter": "Jekaterina Novikova Dr.", "authors": "Jekaterina Novikova, Christian Dondrup, Ioannis Papaioannou and Oliver\n  Lemon", "title": "Sympathy Begins with a Smile, Intelligence Begins with a Word: Use of\n  Multimodal Features in Spoken Human-Robot Interaction", "comments": "Robo-NLP workshop at ACL 2017. 9 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of social signals, from human facial expressions or prosody of\nspeech, is a popular research topic in human-robot interaction studies. There\nis also a long line of research in the spoken dialogue community that\ninvestigates user satisfaction in relation to dialogue characteristics.\nHowever, very little research relates a combination of multimodal social\nsignals and language features detected during spoken face-to-face human-robot\ninteraction to the resulting user perception of a robot. In this paper we show\nhow different emotional facial expressions of human users, in combination with\nprosodic characteristics of human speech and features of human-robot dialogue,\ncorrelate with users' impressions of the robot after a conversation. We find\nthat happiness in the user's recognised facial expression strongly correlates\nwith likeability of a robot, while dialogue-related features (such as number of\nhuman turns or number of sentences per robot utterance) correlate with\nperceiving a robot as intelligent. In addition, we show that facial expression,\nemotional features, and prosody are better predictors of human ratings related\nto perceived robot likeability and anthropomorphism, while linguistic and\nnon-linguistic features more often predict perceived robot intelligence and\ninterpretability. As such, these characteristics may in future be used as an\nonline reward signal for in-situ Reinforcement Learning based adaptive\nhuman-robot dialogue systems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 20:33:00 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Novikova", "Jekaterina", ""], ["Dondrup", "Christian", ""], ["Papaioannou", "Ioannis", ""], ["Lemon", "Oliver", ""]]}, {"id": "1706.03179", "submitter": "Emilio Ferrara", "authors": "Emilio Ferrara, Nazanin Alipourfard, Keith Burghardt, Chiranth Gopal,\n  Kristina Lerman", "title": "Dynamics of Content Quality in Collaborative Knowledge Production", "comments": null, "journal-ref": "Proceedings of the Eleventh International AAAI Conference on Web\n  and Social Media (ICWSM 2017), pp. 520-523, 2017", "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the dynamics of user performance in collaborative knowledge\nproduction by studying the quality of answers to questions posted on Stack\nExchange. We propose four indicators of answer quality: answer length, the\nnumber of code lines and hyperlinks to external web content it contains, and\nwhether it is accepted by the asker as the most helpful answer to the question.\nAnalyzing millions of answers posted over the period from 2008 to 2014, we\nuncover regular short-term and long-term changes in quality. In the short-term,\nquality deteriorates over the course of a single session, with each successive\nanswer becoming shorter, with fewer code lines and links, and less likely to be\naccepted. In contrast, performance improves over the long-term, with more\nexperienced users producing higher quality answers. These trends are not a\nconsequence of data heterogeneity, but rather have a behavioral origin. Our\nfindings highlight the complex interplay between short-term deterioration in\nperformance, potentially due to mental fatigue or attention depletion, and\nlong-term performance improvement due to learning and skill acquisition, and\nits impact on the quality of user-generated content.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 04:17:30 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Ferrara", "Emilio", ""], ["Alipourfard", "Nazanin", ""], ["Burghardt", "Keith", ""], ["Gopal", "Chiranth", ""], ["Lerman", "Kristina", ""]]}, {"id": "1706.03249", "submitter": "Rishabh Mehrotra", "authors": "Rishabh Mehrotra and Prasanta Bhattacharya", "title": "Characterizing and Predicting Supply-side Engagement on\n  Crowd-contributed Video Sharing Platforms", "comments": "8 pages, ICTIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video sharing and entertainment websites have rapidly grown in popularity and\nnow constitute some of the most visited websites on the Internet. Despite the\nactive user engagement on these online video-sharing platforms, most of recent\nresearch on online media platforms have restricted themselves to networking\nbased social media sites, like Facebook or Twitter. We depart from previous\nstudies in the online media space that have focused exclusively on demand-side\nuser engagement, by modeling the supply-side of the crowd-contributed videos on\nthis platform. The current study is among the first to perform a large-scale\nempirical study using longitudinal video upload data from a large online video\nplatform. The modeling and subsequent prediction of video uploads is made\ncomplicated by the heterogeneity of video types (e.g. popular vs. niche video\ngenres), and the inherent time trend effects associated with media uploads. We\nidentify distinct genre-clusters from our dataset and employ a self-exciting\nHawkes point-process model on each of these clusters to fully specify and\nestimate the video upload process. Additionally, we go beyond prediction to\ndisentangle potential factors that govern user engagement and determine the\nvideo upload rates, which improves our analysis with additional explanatory\npower. Our findings show that using a relatively parsimonious point-process\nmodel, we are able to achieve higher model fit, and predict video uploads to\nthe platform with a higher accuracy than competing models. The findings from\nthis study can benefit platform owners in better understanding how their\nsupply-side users engage with their site over time. We also offer a robust\nmethod for performing media upload prediction that is likely to be\ngeneralizable across media platforms which demonstrate similar temporal and\ngenre-level heterogeneity.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 16:26:48 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Mehrotra", "Rishabh", ""], ["Bhattacharya", "Prasanta", ""]]}, {"id": "1706.03311", "submitter": "Kristin Siu", "authors": "Kristin Siu, Alexander Zook, Mark O. Riedl", "title": "A Framework for Exploring and Evaluating Mechanics in Human Computation\n  Games", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human computation games (HCGs) are a crowdsourcing approach to solving\ncomputationally-intractable tasks using games. In this paper, we describe the\nneed for generalizable HCG design knowledge that accommodates the needs of both\nplayers and tasks. We propose a formal representation of the mechanics in HCGs,\nproviding a structural breakdown to visualize, compare, and explore the space\nof HCG mechanics. We present a methodology based on small-scale design\nexperiments using fixed tasks while varying game elements to observe effects on\nboth the player experience and the human computation task completion. Finally\nwe discuss applications of our framework using comparisons of prior HCGs and\nrecent design experiments. Ultimately, we wish to enable easier exploration and\ndevelopment of HCGs, helping these games provide meaningful player experiences\nwhile solving difficult problems.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 06:16:49 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Siu", "Kristin", ""], ["Zook", "Alexander", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1706.03741", "submitter": "Paul Christiano", "authors": "Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\n  Dario Amodei", "title": "Deep reinforcement learning from human preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sophisticated reinforcement learning (RL) systems to interact usefully\nwith real-world environments, we need to communicate complex goals to these\nsystems. In this work, we explore goals defined in terms of (non-expert) human\npreferences between pairs of trajectory segments. We show that this approach\ncan effectively solve complex RL tasks without access to the reward function,\nincluding Atari games and simulated robot locomotion, while providing feedback\non less than one percent of our agent's interactions with the environment. This\nreduces the cost of human oversight far enough that it can be practically\napplied to state-of-the-art RL systems. To demonstrate the flexibility of our\napproach, we show that we can successfully train complex novel behaviors with\nabout an hour of human time. These behaviors and environments are considerably\nmore complex than any that have been previously learned from human feedback.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:23:59 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 20:25:56 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 20:18:41 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Christiano", "Paul", ""], ["Leike", "Jan", ""], ["Brown", "Tom B.", ""], ["Martic", "Miljan", ""], ["Legg", "Shane", ""], ["Amodei", "Dario", ""]]}, {"id": "1706.03851", "submitter": "Ramya Hebbalaguppe", "authors": "Ramakrishna Perla and Ramya Hebbalaguppe", "title": "Google Cardboard Dates Augmented Reality : Issues, Challenges and Future\n  Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Google's frugal Cardboard solution for immersive Virtual Reality\nexperiences has come a long way in the VR market. The Google Cardboard VR\napplications will support us in the fields such as education, virtual tourism,\nentertainment, gaming, design etc. Recently, Qualcomm's Vuforia SDK has\nintroduced support for developing mixed reality applications for Google\nCardboard which can combine Virtual and Augmented Reality to develop exciting\nand immersive experiences. In this work, we present a comprehensive review of\nGoogle Cardboard for AR and also highlight its technical and subjective\nlimitations by conducting a feasibility study through the inspection of a\nDesktop computer use-case. Additionally, we recommend the future avenues for\nthe Google Cardboard in AR. This work also serves as a guide for Android/iOS\ndevelopers as there are no published scholarly articles or well documented\nstudies exclusively on Google Cardboard with both user and developer's\nexperience captured at one place.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 06:26:25 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Perla", "Ramakrishna", ""], ["Hebbalaguppe", "Ramya", ""]]}, {"id": "1706.03930", "submitter": "Chi Hong", "authors": "Chi Hong", "title": "Generative Models for Learning from Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose generative probabilistic models for label\naggregation. We use Gibbs sampling and a novel variational inference algorithm\nto perform the posterior inference. Empirical results show that our methods\nconsistently outperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 07:28:26 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 07:28:14 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 09:18:04 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hong", "Chi", ""]]}, {"id": "1706.04148", "submitter": "Massimo Quadrana", "authors": "Massimo Quadrana, Alexandros Karatzoglou, Bal\\'azs Hidasi and Paolo\n  Cremonesi", "title": "Personalizing Session-based Recommendations with Hierarchical Recurrent\n  Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1145/3109859.3109896", "report-no": null, "categories": "cs.LG cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Session-based recommendations are highly relevant in many modern on-line\nservices (e.g. e-commerce, video streaming) and recommendation settings.\nRecently, Recurrent Neural Networks have been shown to perform very well in\nsession-based settings. While in many session-based recommendation domains user\nidentifiers are hard to come by, there are also domains in which user profiles\nare readily available. We propose a seamless way to personalize RNN models with\ncross-session information transfer and devise a Hierarchical RNN model that\nrelays end evolves latent hidden states of the RNNs across user sessions.\nResults on two industry datasets show large improvements over the session-only\nRNNs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:33:52 GMT"}, {"version": "v2", "created": "Wed, 14 Jun 2017 08:02:51 GMT"}, {"version": "v3", "created": "Sat, 8 Jul 2017 16:52:18 GMT"}, {"version": "v4", "created": "Fri, 14 Jul 2017 16:42:43 GMT"}, {"version": "v5", "created": "Wed, 23 Aug 2017 18:42:56 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Quadrana", "Massimo", ""], ["Karatzoglou", "Alexandros", ""], ["Hidasi", "Bal\u00e1zs", ""], ["Cremonesi", "Paolo", ""]]}, {"id": "1706.04427", "submitter": "Kristof Friess", "authors": "Kristof Friess and H.C. Volker Herwig", "title": "Classification of Smart Environment Scenarios in combination with a\n  Human-Wearable-Environment-Communication using wireless connectivity", "comments": "14pages, Keywords: Wireless Network, Smart Environment,\n  Wearable-Computing, UbiComp, Interaction-Scenarios", "journal-ref": null, "doi": "10.5121/csit.2017.70704", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of computer technology has been rapid. Not so long ago, the\nfirst computer was developed which was large and bulky. Now, the latest\ngeneration of smartphones has a calculation power, which would have been\nconsidered those of supercomputers in 1990. For a smart environment, the person\nrecognition and re-recognition is an important topic. The distribution of new\ntechnologies like wearable computing is a new approach to the field of person\nrecognition and re-recognition. This article lays out the idea of identifying\nand re-identifying wearable computing devices by listening to their wireless\ncommunication connectivity like Wi-Fi and Bluetooth and building a\nclassification of interaction scenarios for the combination of\nhuman-wearable-environment.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 12:16:57 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Friess", "Kristof", ""], ["Herwig", "H. C. Volker", ""]]}, {"id": "1706.04524", "submitter": "Julia Kiseleva", "authors": "Julia Kiseleva and Maarten de Rijke", "title": "Evaluating Personal Assistants on Mobile devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The iPhone was introduced only a decade ago in 2007 but has fundamentally\nchanged the way we interact with online information. Mobile devices differ\nradically from classic command-based and point-and-click user interfaces, now\nallowing for gesture-based interaction using fine-grained touch and swipe\nsignals. Due to the rapid growth in the use of voice-controlled intelligent\npersonal assistants on mobile devices, such as Microsoft's Cortana, Google Now,\nand Apple's Siri, mobile devices have become personal, allowing us to be online\nall the time, and assist us in any task, both in work and in our daily lives,\nmaking context a crucial factor to consider.\n  Mobile usage is now exceeding desktop usage, and is still growing at a rapid\nrate, yet our main ways of training and evaluating personal assistants are\nstill based on (and framed in) classical desktop interactions, focusing on\nexplicit queries, clicks, and dwell time spent. However, modern user\ninteraction with mobile devices is radically different due to touch screens\nwith a gesture- and voice-based control and the varying context of use, e.g.,\nin a car, by bike, often invalidating the assumptions underlying today's user\nsatisfaction evaluation.\n  There is an urgent need to understand voice- and gesture-based interaction,\ntaking all interaction signals and context into account in appropriate ways. We\npropose a research agenda for developing methods to evaluate and improve\ncontext-aware user satisfaction with mobile interactions using gesture-based\nsignals at scale.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 14:57:18 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "1706.04979", "submitter": "Md. Iqbal Hossain", "authors": "Md Iqbal Hossain and Stephen Kobourov", "title": "Research Topics Map: rtopmap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a system for visualizing and analyzing worldwide\nresearch topics, {\\tt rtopmap}. We gather data from google scholar academic\nresearch profiles, putting together a weighted topics graph, consisting of over\n35,000 nodes and 646,000 edges. The nodes correspond to self-reported research\ntopics, and edges correspond to co-occurring topics in google scholar profiles.\nThe {\\tt rtopmap} system supports zooming/panning/searching and other\ngoogle-maps-based interactive features. With the help of map overlays, we also\nvisualize the strengths and weaknesses of different academic institutions in\nterms of human resources (e.g., number of researchers in different areas), as\nwell as scholarly output (e.g., citation counts in different areas). Finally,\nwe also visualize what parts of the map are associated with different academic\ndepartments, or with specific documents (such as research papers, or calls for\nproposals). The system itself is available at\n\\url{http://rtopmap.arl.arizona.edu/}.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 17:29:53 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Hossain", "Md Iqbal", ""], ["Kobourov", "Stephen", ""]]}, {"id": "1706.05286", "submitter": "Irvan Arief Ang", "authors": "Irvan B. Arief-Ang, Flora D. Salim and Margaret Hamilton", "title": "CD-HOC: Indoor Human Occupancy Counting using Carbon Dioxide Sensor Data", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human occupancy information is crucial for any modern Building Management\nSystem (BMS). Implementing pervasive sensing and leveraging Carbon Dioxide data\nfrom BMS sensor, we present Carbon Dioxide - Human Occupancy Counter (CD-HOC),\na novel way to estimate the number of people within a closed space from a\nsingle carbon dioxide sensor. CD-HOC de-noises and pre-processes the carbon\ndioxide data. We utilise both seasonal-trend decomposition based on Loess and\nseasonal-trend decomposition with moving average to factorise carbon dioxide\ndata. For each trend, seasonal and irregular component, we model different\nregression algorithms to predict each respective human occupancy component\nvalue. We propose a zero pattern adjustment model to increase the accuracy and\nfinally, we use additive decomposition to reconstruct the prediction value. We\nrun our model in two different locations that have different contexts. The\nfirst location is an academic staff room and the second is a cinema theatre.\nOur results show an average of 4.33% increment in accuracy for the small room\nwith 94.68% indoor human occupancy counting and 8.46% increase for the cinema\ntheatre in comparison to the accuracy of the baseline method, support vector\nregression.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 14:28:32 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Arief-Ang", "Irvan B.", ""], ["Salim", "Flora D.", ""], ["Hamilton", "Margaret", ""]]}, {"id": "1706.05718", "submitter": "Charisee Chiw", "authors": "Charisee Chiw, Gordon Kindlmann, and John Reppy", "title": "An exploration to visualize finite element data with a DSL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scientific community use PDEs to model a range of problems. The people in\nthis domain are interested in visualizing their results, but existing\nmechanisms for visualization can not handle the full richness of computations\nin the domain. We did an exploration to see how Diderot, a domain specific\nlanguage for scientific visualization and image analysis, could be used to\nsolve this problem.\n  We demonstrate our first and modest approach of visualizing FE data with\nDiderot and provide examples. Using Diderot, we do a simple sampling and a\nvolume rendering of a FE field. These examples showcase Diderot's ability to\nprovide a visualization result for Firedrake. This paper describes the\nextension of the Diderot language to include FE data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 20:25:47 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Chiw", "Charisee", ""], ["Kindlmann", "Gordon", ""], ["Reppy", "John", ""]]}, {"id": "1706.05993", "submitter": "Hosnieh Sattar", "authors": "Hosnieh Sattar, Mario Fritz, Andreas Bulling", "title": "Visual Decoding of Targets During Visual Search From Human Eye Fixations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does human gaze reveal about a users' intents and to which extend can\nthese intents be inferred or even visualized? Gaze was proposed as an implicit\nsource of information to predict the target of visual search and, more\nrecently, to predict the object class and attributes of the search target. In\nthis work, we go one step further and investigate the feasibility of combining\nrecent advances in encoding human gaze information using deep convolutional\nneural networks with the power of generative image models to visually decode,\ni.e. create a visual representation of, the search target. Such visual decoding\nis challenging for two reasons: 1) the search target only resides in the user's\nmind as a subjective visual pattern, and can most often not even be described\nverbally by the person, and 2) it is, as of yet, unclear if gaze fixations\ncontain sufficient information for this task at all. We show, for the first\ntime, that visual representations of search targets can indeed be decoded only\nfrom human gaze fixations. We propose to first encode fixations into a semantic\nrepresentation and then decode this representation into an image. We evaluate\nour method on a recent gaze dataset of 14 participants searching for clothing\nin image collages and validate the model's predictions using two human studies.\nOur results show that 62% (Chance level = 10%) of the time users were able to\nselect the categories of the decoded image right. In our second studies we show\nthe importance of a local gaze encoding for decoding visual search targets of\nuser\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 14:52:30 GMT"}, {"version": "v2", "created": "Tue, 20 Jun 2017 05:28:51 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 11:19:10 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Sattar", "Hosnieh", ""], ["Fritz", "Mario", ""], ["Bulling", "Andreas", ""]]}, {"id": "1706.06120", "submitter": "Junming Yin", "authors": "Xuan Wei, Daniel Dajun Zeng, Junming Yin", "title": "Multi-Label Annotation Aggregation in Crowdsourcing", "comments": "The paper needs more refinement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a means of human-based computation, crowdsourcing has been widely used to\nannotate large-scale unlabeled datasets. One of the obvious challenges is how\nto aggregate these possibly noisy labels provided by a set of heterogeneous\nannotators. Another challenge stems from the difficulty in evaluating the\nannotator reliability without even knowing the ground truth, which can be used\nto build incentive mechanisms in crowdsourcing platforms. When each instance is\nassociated with many possible labels simultaneously, the problem becomes even\nharder because of its combinatorial nature. In this paper, we present new\nflexible Bayesian models and efficient inference algorithms for multi-label\nannotation aggregation by taking both annotator reliability and label\ndependency into account. Extensive experiments on real-world datasets confirm\nthat the proposed methods outperform other competitive alternatives, and the\nmodel can recover the type of the annotators with high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 18:03:15 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 03:14:53 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wei", "Xuan", ""], ["Zeng", "Daniel Dajun", ""], ["Yin", "Junming", ""]]}, {"id": "1706.06176", "submitter": "Nicholas Firth", "authors": "Nicholas C. Firth, Emma Harding, Mary Pat Sullivan, Sebastian J.\n  Crutch, Daniel C. Alexander", "title": "ESCAPE - Echo SCraper and ClAssifier of PErsons: A novel tool to\n  facilitate using voice-controlled devices for research", "comments": "10 pages, 3 figures, currently in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart devices have become common place in many homes, and these devices can\nbe utilized to provide support for people with mental or physical deficits.\nVoice-controlled assistants are a class of smart device that collect a large\namount of data in the home. In this work we present Echo SCraper and ClAssifier\nof Persons (ESCAPE), an open source software for the extraction of Amazon Echo\ninteraction data, and speaker recognition on that data. We show that ESCAPE is\nable to extract data from a voice-controlled assistant and classify with\naccuracy who is talking, based on a small number of labeled audio data. Using\nESCAPE to extract interactions recorded over 3 months in the first author's\nhome yields a rich dataset of transcribed audio recordings. Our results\ndemonstrate that using this software the Amazon Echo can be used to study\nparticipants in a naturalistic setting with minimal intrusion. We also discuss\nthe potential for usage of voice-controlled devices together with ESCAPE to\nunderstand how diseases affect individuals, and how these data can be used to\nmonitor disease processes in general.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 10:39:07 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Firth", "Nicholas C.", ""], ["Harding", "Emma", ""], ["Sullivan", "Mary Pat", ""], ["Crutch", "Sebastian J.", ""], ["Alexander", "Daniel C.", ""]]}, {"id": "1706.06410", "submitter": "Zeljko Carevic", "authors": "Zeljko Carevic, Maria Lusky, Wilko van Hoek and Philipp Mayr", "title": "Investigating Exploratory Search Activities based on the Stratagem Level\n  in Digital Libraries", "comments": null, "journal-ref": null, "doi": "10.1007/s00799-017-0226-6", "report-no": null, "categories": "cs.DL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the results of a user study on exploratory search\nactivities in a social science digital library. We conducted a user study with\n32 participants with a social sciences background -- 16 postdoctoral\nresearchers and 16 students -- who were asked to solve a task on searching\nrelated work to a given topic. The exploratory search task was performed in a\n10-minutes time slot. The use of certain search activities is measured and\ncompared to gaze data recorded with an eye tracking device. We use a novel tree\ngraph representation to visualise the users' search patterns and introduce a\nway to combine multiple search session trees. The tree graph representation is\ncapable to create one single tree for multiple users and to identify common\nsearch patterns. In addition, the information behaviour of students and\npostdoctoral researchers is being compared. The results show that search\nactivities on the stratagem level are frequently utilised by both user groups.\nThe most heavily used search activities were keyword search, followed by\nbrowsing through references and citations, and author searching. The eye\ntracking results showed an intense examination of documents metadata,\nespecially on the level of citations and references. When comparing the group\nof students and postdoctoral researchers we found significant differences\nregarding gaze data on the area of the journal name of the seed document. In\ngeneral, we found a tendency of the postdoctoral researchers to examine the\nmetadata records more intensively with regards to dwell time and the number of\nfixations.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 13:20:49 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Carevic", "Zeljko", ""], ["Lusky", "Maria", ""], ["van Hoek", "Wilko", ""], ["Mayr", "Philipp", ""]]}, {"id": "1706.06579", "submitter": "Peikun Xiong", "authors": "Peikun Xiong, Chen Sun and Dongsheng Cai", "title": "\"Synchronize\" to VR Body: Full Body Illusion in VR Space", "comments": "4 pages, 4 figures, Eurographics 2017,Conference short paper", "journal-ref": "Eurographics 2017, 4pp,(2017)", "doi": "10.2312/egsh.20171002", "report-no": "009-012", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) becomes accessible to mimic a \"real-like\" world now.\nPeople who have a VR experience usually can be impressed by the immersive\nfeeling, they might consider themselves are actually existed in the VR space.\nSelf-consciousness is important for people to identify their own characters in\nVR space, and illusory ownership can help people to \"build\" their \"bodies\". The\nrubber hand illusion can convince us a fake hand made by rubber is a part of\nour bodies under certain circumstances. Researches about autoscopic phenomena\nextend this illusory to the so-called full body illusion. We conducted 3 type\nof experiments to study the illusory ownership in VR space as it shows in\nFigure 1, and we learned: Human body must receive the synchronized visual\nsignal and somatosensory stimulus at the same time; The visual signal must be\nthe first person perceptive; the subject and the virtual body needs to be the\nsame height as much as possible. All these illusory ownerships accompanied by\nthe body temperature decreases, where the body is stimulated.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 07:56:43 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Xiong", "Peikun", ""], ["Sun", "Chen", ""], ["Cai", "Dongsheng", ""]]}, {"id": "1706.06954", "submitter": "Claudia Schulz", "authors": "Christos Rodosthenous and Loizos Michael", "title": "Web-STAR: Towards a Visual Web-Based IDE for a Story Comprehension\n  System", "comments": "Proceedings of the 2nd International Workshop on User-Oriented Logic\n  Paradigms (IULP 2017), Editors: Claudia Schulz and Stefan Ellmauthaler", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present Web-STAR, an online platform for story understanding\nbuilt on top of the STAR (STory comprehension through ARgumentation) reasoning\nengine. This platform includes a web-based IDE, integration with the STAR\nsystem and a web service infrastructure to support integration with other\nsystems that rely on story understanding functionality to complete their tasks.\nThe platform also delivers a number of \"social\" features like public story\nsharing with a built-in commenting system, a public repository for sharing\nstories with the community and collaboration tools that can be used from both\nproject team members for development and educators for teaching. Moreover, we\ndiscuss the ongoing work on adding new features and functionality to this\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 10:02:13 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Rodosthenous", "Christos", ""], ["Michael", "Loizos", ""]]}, {"id": "1706.06987", "submitter": "Hannah Morrison", "authors": "Hannah Morrison, Chris Martens", "title": "A Generative Model of Group Conversation", "comments": "Accepted submission for the Workshop on Non-Player Characters and\n  Social Believability in Games at FDG 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversations with non-player characters (NPCs) in games are typically\nconfined to dialogue between a human player and a virtual agent, where the\nconversation is initiated and controlled by the player. To create richer, more\nbelievable environments for players, we need conversational behavior to reflect\ninitiative on the part of the NPCs, including conversations that include\nmultiple NPCs who interact with one another as well as the player. We describe\na generative computational model of group conversation between agents, an\nabstract simulation of discussion in a small group setting. We define\nconversational interactions in terms of rules for turn taking and interruption,\nas well as belief change, sentiment change, and emotional response, all of\nwhich are dependent on agent personality, context, and relationships. We\nevaluate our model using a parameterized expressive range analysis, observing\ncorrelations between simulation parameters and features of the resulting\nconversations. This analysis confirms, for example, that character\npersonalities will predict how often they speak, and that heterogeneous groups\nof characters will generate more belief change.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 16:19:59 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Morrison", "Hannah", ""], ["Martens", "Chris", ""]]}, {"id": "1706.07757", "submitter": "Mehdi Ghayoumi", "authors": "Mehdi Ghayoumi and Arvind Bansal", "title": "Improved Human Emotion Recognition Using Symmetry of Facial Key Points\n  with Dihedral Group", "comments": "7", "journal-ref": "IJASCSE Volume 6 Issue 01 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes how to deploy dihedral group theory to detect Facial\nKey Points (FKP) symmetry to recognize emotions. The method can be applied in\nmany other areas which those have the same data texture.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 14:54:14 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Ghayoumi", "Mehdi", ""], ["Bansal", "Arvind", ""]]}, {"id": "1706.08096", "submitter": "Adrien Coppens", "authors": "Adrien Coppens", "title": "Merging real and virtual worlds: An analysis of the state of the art and\n  practical evaluation of Microsoft Hololens", "comments": "Submitted in fulfillment of the requirements for the degree of Master\n  in Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Achieving a symbiotic blending between reality and virtuality is a dream that\nhas been lying in the minds of many people for a long time. Advances in various\ndomains constantly bring us closer to making that dream come true. Augmented\nreality as well as virtual reality are in fact trending terms and are expected\nto further progress in the years to come.\n  This master's thesis aims to explore these areas and starts by defining\nnecessary terms such as augmented reality (AR) or virtual reality (VR). Usual\ntaxonomies to classify and compare the corresponding experiences are then\ndiscussed.\n  In order to enable those applications, many technical challenges need to be\ntackled, such as accurate motion tracking with 6 degrees of freedom (positional\nand rotational), that is necessary for compelling experiences and to prevent\nuser sickness. Additionally, augmented reality experiences typically rely on\nimage processing to position the superimposed content. To do so, \"paper\"\nmarkers or features extracted from the environment are often employed. Both\nsets of techniques are explored and common solutions and algorithms are\npresented.\n  After investigating those technical aspects, I carry out an objective\ncomparison of the existing state-of-the-art and state-of-the-practice in those\ndomains, and I discuss present and potential applications in these areas. As a\npractical validation, I present the results of an application that I have\ndeveloped using Microsoft HoloLens, one of the more advanced affordable\ntechnologies for augmented reality that is available today. Based on the\nexperience and lessons learned during this development, I discuss the\nlimitations of current technologies and present some avenues of future\nresearch.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 13:10:39 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Coppens", "Adrien", ""]]}, {"id": "1706.08205", "submitter": "Sayan Sarcar", "authors": "Sayan Sarcar, Ahmed Sabbir Arif, Ali Mazalek", "title": "Metrics for Bengali Text Entry Research", "comments": "This paper has been accepted and presented as a position paper at ACM\n  CHI 2015 workshop on \"Text entry on the edge\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the intention of bringing uniformity to Bengali text entry research,\nhere we present a new approach for calculating the most popular English text\nentry evaluation metrics for Bengali. To demonstrate our approach, we conducted\na user study where we evaluated four popular Bengali text entry techniques.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 02:07:11 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Sarcar", "Sayan", ""], ["Arif", "Ahmed Sabbir", ""], ["Mazalek", "Ali", ""]]}, {"id": "1706.08274", "submitter": "Ziniu Hu", "authors": "Ziniu Hu, Yun Ma, Qiaozhu Mei, Jian Tang", "title": "Roaming across the Castle Tunnels: an Empirical Study of Inter-App\n  Navigation Behaviors of Android Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile applications (a.k.a., apps), which facilitate a large variety of tasks\non mobile devices, have become indispensable in our everyday lives.\nAccomplishing a task may require the user to navigate among various apps.\nUnlike Web pages that are inherently interconnected through hyperlinks, mobile\napps are usually isolated building blocks, and the lack of direct links between\napps has largely compromised the efficiency of task completion. In this paper,\nwe present the first in-depth empirical study of inter-app navigation behaviors\nof smartphone users based on a comprehensive dataset collected through a\nsizable user study over three months. We propose a model to distinguish\ninformational pages and transitional pages, based on which a large number of\ninter-app navigation are identified. We reveal that developing 'tunnels'\nbetween of isolated apps has a huge potential to reduce the cost of navigation.\nOur analysis provides various practical implications on how to improve\napp-navigation experiences from both the operating system's perspective and the\ndeveloper's perspective.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 08:24:21 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Hu", "Ziniu", ""], ["Ma", "Yun", ""], ["Mei", "Qiaozhu", ""], ["Tang", "Jian", ""]]}, {"id": "1706.08319", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg and Sergej Sizov", "title": "Bayesian Brain meets Bayesian Recommender - Towards Systems with Empathy\n  for the Human Nature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the modern theory of the Bayesian brain from\ncognitive neurosciences in the light of recommender systems and expose\npotentials for our community. In particular, we elaborate on noisy user\nfeedback and the thus resulting multicomponent user models, which have indeed a\nbiological origin. In real user experiments we observe the impact of both\nfactors directly in a repeated rating task along with recommendation. As a\nconsequence, this contribution supports the plausibility of contemporary\ntheories of mind in the context of recommender systems and can be understood as\na solicitation to integrate ideas of cognitive neurosciences into our systems\nin order to further improve the prediction of human behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 11:02:34 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 14:11:41 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1706.08461", "submitter": "Lorenzo Sabattini", "authors": "Lorenzo Sabattini, Valeria Villani, Julia N. Czerniak, Alexander\n  Mertens, Cesare Fantuzzi", "title": "Methodological Approach for the Design of a Complex Inclusive\n  Human-Machine System", "comments": "Proceedings of the IEEE Conference on Automation Science and\n  Engineering (CASE) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern industrial automatic machines and robotic cells are equipped with\nhighly complex human-machine interfaces (HMIs) that often prevent human\noperators from an effective use of the automatic systems. In particular, this\napplies to vulnerable users, such as those with low experience or education\nlevel, the elderly and the disabled. To tackle this issue, it becomes necessary\nto design user-oriented HMIs, which adapt to the capabilities and skills of\nusers, thus compensating their limitations and taking full advantage of their\nknowledge. In this paper, we propose a methodological approach to the design of\ncomplex adaptive human-machine systems that might be inclusive of all users, in\nparticular the vulnerable ones. The proposed approach takes into account both\nthe technical requirements and the requirements for ethical, legal and social\nimplications (ELSI) for the design of automatic systems. The technical\nrequirements derive from a thorough analysis of three use cases taken from the\nEuropean project INCLUSIVE. To achieve the ELSI requirements, the MEESTAR\napproach is combined with the specific legal issues for occupational systems\nand requirements of the target users.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 16:30:20 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Sabattini", "Lorenzo", ""], ["Villani", "Valeria", ""], ["Czerniak", "Julia N.", ""], ["Mertens", "Alexander", ""], ["Fantuzzi", "Cesare", ""]]}, {"id": "1706.08467", "submitter": "Lorenzo Sabattini", "authors": "Valeria Villani, Lorenzo Sabattini, Julia N. Czerniak, Alexander\n  Mertens, Birgit Vogel-Heuser, Cesare Fantuzzi", "title": "Towards Modern Inclusive Factories: A Methodology for the Development of\n  Smart Adaptive Human-Machine Interfaces", "comments": "Proceedings of the IEEE International Conference on Emerging\n  Technologies And Factory Automation (ETFA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern manufacturing systems typically require high degrees of flexibility,\nin terms of ability to customize the production lines to the constantly\nchanging market requests. For this purpose, manufacturing systems are required\nto be able to cope with changes in the types of products, and in the size of\nthe production batches. As a consequence, the human-machine interfaces (HMIs)\nare typically very complex, and include a wide range of possible operational\nmodes and commands. This generally implies an unsustainable cognitive workload\nfor the human operators, in addition to a non-negligible training effort. To\novercome this issue, in this paper we present a methodology for the design of\nadaptive human-centred HMIs for industrial machines and robots. The proposed\napproach relies on three pillars: measurement of user's capabilities,\nadaptation of the information presented in the HMI, and training of the user.\nThe results expected from the application of the proposed methodology are\ninvestigated in terms of increased customization and productivity of\nmanufacturing processes, and wider acceptance of automation technologies. The\nproposed approach has been devised in the framework of the European project\nINCLUSIVE.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 16:38:10 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Villani", "Valeria", ""], ["Sabattini", "Lorenzo", ""], ["Czerniak", "Julia N.", ""], ["Mertens", "Alexander", ""], ["Vogel-Heuser", "Birgit", ""], ["Fantuzzi", "Cesare", ""]]}, {"id": "1706.08866", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg and Sergej Sizov", "title": "Re-Evaluating the Netflix Prize - Human Uncertainty and its Impact on\n  Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the statistical soundness of comparative\nassessments within the field of recommender systems in terms of reliability and\nhuman uncertainty. From a controlled experiment, we get the insight that users\nprovide different ratings on same items when repeatedly asked. This volatility\nof user ratings justifies the assumption of using probability densities instead\nof single rating scores. As a consequence, the well-known accuracy metrics\n(e.g. MAE, MSE, RMSE) yield a density themselves that emerges from convolution\nof all rating densities. When two different systems produce different RMSE\ndistributions with significant intersection, then there exists a probability of\nerror for each possible ranking. As an application, we examine possible ranking\nerrors of the Netflix Prize. We are able to show that all top rankings are more\nor less subject to high probabilities of error and that some rankings may be\ndeemed to be caused by mere chance rather than system quality.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 14:09:06 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1706.09089", "submitter": "Jing Jin", "authors": "Jing Jin, Brendan Z. Allison, Yu Zhang, Yan Chen, Sijie Zhou, Yi Dong,\n  Xingyu Wang and Andrzej Chchocki", "title": "Continuous use of ERP-based BCIs with different visual angles in ALS\n  patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Amyotrophic lateral sclerosis (ALS) is a rare disease, but is also\none of the most common motor neuron diseases, and people of all races and\nethnic backgrounds are affected. There is currently no cure. Brain computer\ninterfaces (BCIs) can establish a communication channel directly between the\nbrain and an external device by recognizing brain activities that reflect user\nintent. Therefore, this technology could help ALS patients in promoting\nfunctional independence through BCI-based speller systems and motor assistive\ndevices. Methods: In this paper, two kinds of ERP-based speller systems were\ntested on 18 ALS patients to: (1) assess performance when they spelled 42\ncharacters online continuously, without a break; and (2) to compare performance\nbetween a matrix-based speller paradigm (MS-P, mean visual angle 6 degree) and\na new speller paradigm that used a larger visual angle called the large visual\nangle speller paradigm (LS-P, mean visual angle 8 degree). Results: Although\nresults showed that there were no significant differences between the two\nparadigms in accuracy trend over continuous use (p>0.05), the fatigue during\nthe LS-P condition was significantly lower than that of MS-P (p<0.05). Results\nalso showed that continuous use slightly reduced the performance of this\nERP-based BCI. Conclusion: 15 subjects obtained higher than 80% feedback\naccuracy (online output accuracy) and 9 subjects obtained higher than 90%\nfeedback accuracy in one of the two paradigms, thus validating the BCI\napproaches in this study. Significance: Most ALS subjects in this study could\nspell effectively after continuous use of an ERP-based BCI. The new LS-P\ndisplay may be easier for subjects to use, resulting in lower fatigue.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 00:52:17 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Jin", "Jing", ""], ["Allison", "Brendan Z.", ""], ["Zhang", "Yu", ""], ["Chen", "Yan", ""], ["Zhou", "Sijie", ""], ["Dong", "Yi", ""], ["Wang", "Xingyu", ""], ["Chchocki", "Andrzej", ""]]}, {"id": "1706.09554", "submitter": "Christ Bartneck", "authors": "Christoph Bartneck, Michael J. Lyons, Martin Saerbeck", "title": "The Relationship Between Emotion Models and Artificial Intelligence", "comments": "Proceedings of the SAB2008 Workshop on The Role of Emotion in\n  Adaptive Behavior and Cognitive Robotics, Osaka", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotions play a central role in most forms of natural human interaction so we\nmay expect that computational methods for the processing and expression of\nemotions will play a growing role in human-computer interaction. The OCC model\nhas established itself as the standard model for emotion synthesis. A large\nnumber of studies employed the OCC model to generate emotions for their\nembodied characters. Many developers of such characters believe that the OCC\nmodel will be all they ever need to equip their character with emotions. This\nstudy reflects on the limitations of the OCC model specifically, and on the\nemotion models in general due to their dependency on artificial intelligence.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 02:39:42 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Bartneck", "Christoph", ""], ["Lyons", "Michael J.", ""], ["Saerbeck", "Martin", ""]]}, {"id": "1706.09855", "submitter": "Dominik J\\\"ackle", "authors": "Dominik J\\\"ackle, Johannes Fuchs, Harald Reiterer", "title": "Topology-Preserving Off-screen Visualization: Effects of Projection\n  Strategy and Intrusion Adaption", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing amount of data being visualized in large information\nspaces, methods providing data-driven context have become indispensable.\nOff-screen visualization techniques, therefore, have been extensively\nresearched for their ability to overcome the inherent trade-off between\noverview and detail. The general idea is to project off-screen located objects\nback to the available screen real estate. Detached visual cues, such as halos\nor arrows, encode information on position and distance, but fall short showing\nthe topology of off-screen objects. For that reason, state of the art\ntechniques integrate visual cues into a dedicated border region. As yet, the\ndimensions of the navigated space are not reflected properly, which is why we\npropose to adapt the intrusion of the border pursuant to the position in space.\nMoreover, off-screen objects are projected to the border region using one out\nof two projection methods: Radial or Orthographic. We describe a controlled\nexperiment to investigate the effect of the adaptive border intrusion to the\ntopology as well as the users' intuition regarding the projection strategy. The\nresults of our experiment suggest to use the orthographic projection strategy\nfor unconnected point data in an adaptive border design. We further discuss the\nresults including the given informal feedback of participants.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 17:03:55 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["J\u00e4ckle", "Dominik", ""], ["Fuchs", "Johannes", ""], ["Reiterer", "Harald", ""]]}, {"id": "1706.10060", "submitter": "Radoslaw Nielek", "authors": "Radoslaw Nielek, Marta Lutostanska, Wieslaw Kopec, Adam Wierzbicki", "title": "Turned 70? It is time to start editing Wikipedia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Success of Wikipedia would not be possible without the contributions of\nmillions of anonymous Internet users who edit articles, correct mistakes, add\nlinks or pictures. At the same time Wikipedia editors are currently overworked\nand there is always more tasks waiting to be completed than people willing to\nvolunteer. The paper explores the possibility of involving the elderly in the\nWikipedia editing process. Older adults were asked to complete various tasks on\nWikipedia. Based on the observations made during these activities as well as\nin-depth interviews, a list of recommendation has been crafted. It turned out\nthat older adults are willing to contribute to Wikiepdia but substantial\nchanges have to be made in the Wikipedia editor.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 08:35:06 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Nielek", "Radoslaw", ""], ["Lutostanska", "Marta", ""], ["Kopec", "Wieslaw", ""], ["Wierzbicki", "Adam", ""]]}, {"id": "1706.10097", "submitter": "Matthew Lease", "authors": "Akash Mankar, Riddhi J. Shah, and Matthew Lease", "title": "Design Activism for Minimum Wage Crowd Work", "comments": "This is an extended online version of the paper accepted to the 5th\n  AAAI Conference on Human Computation and Crowdsourcing (HCOMP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entry-level crowd work is often reported to pay less than minimum wage. While\nthis may be appropriate or even necessary, due to various legal, economic, and\npragmatic factors, some Requesters and workers continue to question this status\nquo. To promote further discussion on the issue, we survey Requesters and\nworkers whether they would support restricting tasks to require minimum wage\npay. As a form of design activism, we confronted workers with this dilemma\ndirectly by posting a dummy Mechanical Turk task which told them that they\ncould not work on it because it paid less than their local minimum wage, and we\ninvited their feedback. Strikingly, for those workers expressing an opinion,\ntwo-thirds of Indians favored the policy while two-thirds of Americans opposed\nit. Though a majority of Requesters supported minimum wage pay, only 20\\% would\nenforce it. To further empower Requesters, and to ensure that effort or\nignorance are not barriers to change, we provide a simple public API to make it\neasy to find a worker's local minimum wage by his/her IP address.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 10:06:34 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 17:37:09 GMT"}, {"version": "v3", "created": "Sun, 27 Aug 2017 05:48:57 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Mankar", "Akash", ""], ["Shah", "Riddhi J.", ""], ["Lease", "Matthew", ""]]}, {"id": "1706.10151", "submitter": "Luca Buoncompagni", "authors": "Luca Buoncompagni, Alessio Capitanelli, Fulvio Mastrogiovanni", "title": "A ROS multi-ontology references services: OWL reasoners and application\n  prototyping issues", "comments": "Presented to the IEEE RO-MAN 2017 Workshop on \"Autonomous Robot\n  Ontology\", Lisbon Portugal, August 28, 2017. Published in: Proceedings of the\n  5th Italian Workshop on Artificial Intelligence and Robotics (AIRO) A\n  workshop of the XVII International Conference of the Italian Association for\n  Artificial Intelligence (AIXIA), Trento, Italy (2018). CEUR-WS, Vol-2352,\n  pages 36-41", "journal-ref": "In proceedings of AIRO@AIXIA 2018, Trento, Italy\n  (http://ceur-ws.org/Vol-2352/short7.pdf)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a ROS Multi Ontology References (ARMOR) service, a\ngeneral-purpose and scalable interface between robot architectures and OWL\nreasoners. ARMOR addresses synchronisation and communication issues among\nheterogeneous and distributed software components. As a guiding scenario, we\nconsider a prototyping approach for the use of symbolic reasoning in\nhuman-robot interaction applications.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 11:47:39 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 12:30:44 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Buoncompagni", "Luca", ""], ["Capitanelli", "Alessio", ""], ["Mastrogiovanni", "Fulvio", ""]]}, {"id": "1706.10177", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodi\\'c, Alessia Amelio, Ivo R. Draganov", "title": "Statistical Analysis of Dice CAPTCHA Usability", "comments": "9 pages, 5 figures, 52nd International Scientific Conference on\n  Information, Communication and Energy Systems and Technologies (ICEST 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CR cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the elements of the CAPTCHA usability are analyzed. CAPTCHA, as\na time progressive element in computer science, has been under constant\ninterest of ordinary, professional as well as the scientific users of the\nInternet. The analysis is given based on the usability elements of CAPTCHA\nwhich are abbreviated as user-centric approach to the CAPTCHA. To demonstrate\nit, the specific type of Dice CAPTCHA is used in the experiment. The experiment\nis conducted on 190 Internet users with different demographic characteristics\non laptop and tablet computers. The obtained results are statistically\nprocessed. At the end, the results are compared and conclusion of their use is\ndrawn.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:00:25 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Brodi\u0107", "Darko", ""], ["Amelio", "Alessia", ""], ["Draganov", "Ivo R.", ""]]}, {"id": "1706.10223", "submitter": "Bart{\\l}omiej Balcerzak", "authors": "Bart{\\l}omiej Balcerzak, Wies{\\l}aw Kope\\'c, Rados{\\l}aw Nielek,\n  Sebastian Kruk, Kamil Warpechowski, Mateusz Wasik and Marek W\\k{e}grzyn", "title": "Press F1 for help: participatory design for dealing with on-line and\n  real life security of older adults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the report on the design and development of a\nplatform for the inter-generational exchange of favors. This platform was\ndesigned using participatory design approach during a 24-hour hackathon by a\nteam consisting of younger programmers and older adults. The findings of this\nreport show that inter-generational cooperation in which the older adults serve\nas representatives of the end user, not only improves the design and\ndevelopment of the application, but also provides an effective method for\ndesigning and applying solutions aimed at improving the security of older\nadults while using online and mobile tools.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 14:42:01 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Balcerzak", "Bart\u0142omiej", ""], ["Kope\u0107", "Wies\u0142aw", ""], ["Nielek", "Rados\u0142aw", ""], ["Kruk", "Sebastian", ""], ["Warpechowski", "Kamil", ""], ["Wasik", "Mateusz", ""], ["W\u0119grzyn", "Marek", ""]]}, {"id": "1706.10297", "submitter": "Johnson Keiriz", "authors": "Johnson J.G. Keiriz, Liang Zhan, Morris Chukhman, Olu Ajilore, Alex D.\n  Leow, and Angus G. Forbes", "title": "Exploring the Human Connectome Topology in Group Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually comparing brain networks, or connectomes, is an essential task in\nthe field of neuroscience. Especially relevant to the field of clinical\nneuroscience, group studies that examine differences between populations or\nchanges over time within a population enable neuroscientists to reason about\neffective diagnoses and treatments for a range of neuropsychiatric disorders.\nIn this paper, we specifically explore how visual analytics tools can be used\nto facilitate various clinical neuroscience tasks, in which observation and\nanalysis of meaningful patterns in the connectome can support patient diagnosis\nand treatment. We conduct a survey of visualization tasks that enable clinical\nneuroscience activities, and further explore how existing connectome\nvisualization tools support or fail to support these tasks. Based on our\ninvestigation of these tasks, we introduce a novel visualization tool,\nNeuroCave, to support group studies analyses. We discuss how our design\ndecisions (the use of immersive visualization, the use of hierarchical\nclustering and dimensionality reduction techniques, and the choice of visual\nencodings) are motivated by these tasks. We evaluate NeuroCave through two use\ncases that illustrate the utility of interactive connectome visualization in\nclinical neuroscience contexts. In the first use case, we study sex differences\nusing functional connectomes and discover hidden connectome patterns associated\nwith well-known cognitive differences in spatial and verbal abilities. In the\nsecond use case, we show how the utility of visualizing the brain in different\ntopological space coupled with clustering information can reveal the brain's\nintrinsic structure.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 17:59:14 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Keiriz", "Johnson J. G.", ""], ["Zhan", "Liang", ""], ["Chukhman", "Morris", ""], ["Ajilore", "Olu", ""], ["Leow", "Alex D.", ""], ["Forbes", "Angus G.", ""]]}]