[{"id": "2008.00002", "submitter": "Nicolas Tempelmeier", "authors": "Nicolas Tempelmeier, Anzumana Sander, Udo Feuerhake, Martin\n  L\\\"ohdefink, Elena Demidova", "title": "TA-Dash: An Interactive Dashboard for Spatial-Temporal Traffic Analytics\n  -- Demo Paper", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a large number of research efforts aimed at the development\nof machine learning models to predict complex spatial-temporal mobility\npatterns and their impact on road traffic and infrastructure. However, the\nutility of these models is often diminished due to the lack of accessible user\ninterfaces to view and analyse prediction results. In this paper, we present\nthe Traffic Analytics Dashboard ( TA-Dash), an interactive dashboard that\nenables the visualisation of complex spatial-temporal urban traffic patterns.\nWe demonstrate the utility of TA-Dash at the example of two recently proposed\nspatial-temporal models for urban traffic and urban road infrastructure\nanalysis. In particular, the use cases include the analysis, prediction and\nvisualisation of the impact of planned special events on urban road traffic as\nwell as the analysis and visualisation of structural dependencies within urban\nroad networks. The lightweight TA-Dash dashboard aims to address non-expert\nusers involved in urban traffic management and mobility service planning. The\nTA-Dash builds on a flexible layer-based architecture that is easily adaptable\nto the visualisation of new models.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 11:54:55 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Tempelmeier", "Nicolas", ""], ["Sander", "Anzumana", ""], ["Feuerhake", "Udo", ""], ["L\u00f6hdefink", "Martin", ""], ["Demidova", "Elena", ""]]}, {"id": "2008.00058", "submitter": "Alireza Karduni", "authors": "Alireza Karduni, Doug Markant, Ryan Wesslen, Wenwen Dou", "title": "A Bayesian cognition approach for belief updating of correlation\n  judgement through uncertainty visualizations", "comments": "9 pages, 8 figures, accepted at IEEE Information Visualization 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding correlation judgement is important to designing effective\nvisualizations of bivariate data. Prior work on correlation perception has not\nconsidered how factors including prior beliefs and uncertainty representation\nimpact such judgements. The present work focuses on the impact of uncertainty\ncommunication when judging bivariate visualizations. Specifically, we model how\nusers update their beliefs about variable relationships after seeing a\nscatterplot with and without uncertainty representation. To model and evaluate\nthe belief updating, we present three studies. Study 1 focuses on a proposed\n''Line + Cone'' visual elicitation method for capturing users' beliefs in an\naccurate and intuitive fashion. The findings reveal that our proposed method of\nbelief solicitation reduces complexity and accurately captures the users'\nuncertainty about a range of bivariate relationships. Study 2 leverages the\n``Line + Cone'' elicitation method to measure belief updating on the\nrelationship between different sets of variables when seeing correlation\nvisualization with and without uncertainty representation. We compare changes\nin users beliefs to the predictions of Bayesian cognitive models which provide\nnormative benchmarks for how users should update their prior beliefs about a\nrelationship in light of observed data. The findings from Study 2 revealed that\none of the visualization conditions with uncertainty communication led to users\nbeing slightly more confident about their judgement compared to visualization\nwithout uncertainty information. Study 3 builds on findings from Study 2 and\nexplores differences in belief update when the bivariate visualization is\ncongruent or incongruent with users' prior belief. Our results highlight the\neffects of incorporating uncertainty representation, and the potential of\nmeasuring belief updating on correlation judgement with Bayesian cognitive\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 19:51:00 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Karduni", "Alireza", ""], ["Markant", "Doug", ""], ["Wesslen", "Ryan", ""], ["Dou", "Wenwen", ""]]}, {"id": "2008.00101", "submitter": "Liyiming Ke", "authors": "Liyiming Ke, Ajinkya Kamat, Jingqiang Wang, Tapomayukh Bhattacharjee,\n  Christoforos Mavrogiannis, Siddhartha S. Srinivasa", "title": "Telemanipulation with Chopsticks: Analyzing Human Factors in User\n  Demonstrations", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chopsticks constitute a simple yet versatile tool that humans have used for\nthousands of years to perform a variety of challenging tasks ranging from food\nmanipulation to surgery. Applying such a simple tool in a diverse repertoire of\nscenarios requires significant adaptability. Towards developing autonomous\nmanipulators with comparable adaptability to humans, we study chopsticks-based\nmanipulation to gain insights into human manipulation strategies. We conduct a\nwithin-subjects user study with 25 participants, evaluating three different\ndata-collection methods: normal chopsticks, motion-captured chopsticks, and a\nnovel chopstick telemanipulation interface. We analyze factors governing human\nperformance across a variety of challenging chopstick-based grasping tasks.\nAlthough participants rated teleoperation as the least comfortable and most\ndifficult-to-use method, teleoperation enabled users to achieve the highest\nsuccess rates on three out of five objects considered. Further, we notice that\nsubjects quickly learned and adapted to the teleoperation interface. Finally,\nwhile motion-captured chopsticks could provide a better reflection of how\nhumans use chopsticks, the teleoperation interface can produce quality\non-hardware demonstrations from which the robot can directly learn.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 22:26:42 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Ke", "Liyiming", ""], ["Kamat", "Ajinkya", ""], ["Wang", "Jingqiang", ""], ["Bhattacharjee", "Tapomayukh", ""], ["Mavrogiannis", "Christoforos", ""], ["Srinivasa", "Siddhartha S.", ""]]}, {"id": "2008.00137", "submitter": "Shawn Jones", "authors": "Shawn M. Jones, Martin Klein, Michele C. Weigle, Michael L. Nelson", "title": "MementoEmbed and Raintale for Web Archive Storytelling", "comments": "54 pages, 5 tables, 46 figures", "journal-ref": "Presented at the Web Archiving and Digital Libraries 2020 Workshop", "doi": null, "report-no": null, "categories": "cs.DL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For traditional library collections, archivists can select a representative\nsample from a collection and display it in a featured physical or digital\nlibrary space. Web archive collections may consist of thousands of archived\npages, or mementos. How should an archivist display this sample to drive\nvisitors to their collection? Search engines and social media platforms often\nrepresent web pages as cards consisting of text snippets, titles, and images.\nWeb storytelling is a popular method for grouping these cards in order to\nsummarize a topic. Unfortunately, social media platforms are not archive-aware\nand fail to consistently create a good experience for mementos. They also allow\nno UI alterations for their cards. Thus, we created MementoEmbed to generate\ncards for individual mementos and Raintale for creating entire stories that\narchivists can export to a variety of formats.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 00:52:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jones", "Shawn M.", ""], ["Klein", "Martin", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "2008.00139", "submitter": "Shawn Jones", "authors": "Shawn M. Jones, Alexander C. Nwala, Martin Klein, Michele C. Weigle,\n  Michael L. Nelson", "title": "SHARI -- An Integration of Tools to Visualize the Story of the Day", "comments": "19 pages, 16 figures, 1 Table", "journal-ref": "Presented at the Web Archiving and Digital Libraries 2020 Workshop", "doi": null, "report-no": null, "categories": "cs.DL cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tools such as Google News and Flipboard exist to convey daily news, but what\nabout the past? In this paper, we describe how to combine several existing\ntools with web archive holdings to perform news analysis and visualization of\nthe \"biggest story\" for a given date. StoryGraph clusters news articles\ntogether to identify a common news story. Hypercane leverages ArchiveNow to\nstore URLs produced by StoryGraph in web archives. Hypercane analyzes these\nURLs to identify the most common terms, entities, and highest quality images\nfor social media storytelling. Raintale then uses the output of these tools to\nproduce a visualization of the news story for a given day. We name this process\nSHARI (StoryGraph Hypercane ArchiveNow Raintale Integration).\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 01:02:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jones", "Shawn M.", ""], ["Nwala", "Alexander C.", ""], ["Klein", "Martin", ""], ["Weigle", "Michele C.", ""], ["Nelson", "Michael L.", ""]]}, {"id": "2008.00142", "submitter": "Yea Seul Kim", "authors": "Yea-Seul Kim, Paula Kayongo, Madeleine Grunde-McLaughlin and Jessica\n  Hullman", "title": "Bayesian-Assisted Inference from Visualized Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian view of data interpretation suggests that a visualization user\nshould update their existing beliefs about a parameter's value in accordance\nwith the amount of information about the parameter value captured by the new\nobservations. Extending recent work applying Bayesian models to understand and\nevaluate belief updating from visualizations, we show how the predictions of\nBayesian inference can be used to guide more rational belief updating. We\ndesign a Bayesian inference-assisted uncertainty analogy that numerically\nrelates uncertainty in observed data to the user's subjective uncertainty, and\na posterior visualization that prescribes how a user should update their\nbeliefs given their prior beliefs and the observed data. In a pre-registered\nexperiment on 4,800 people, we find that when a newly observed data sample is\nrelatively small (N=158), both techniques reliably improve people's Bayesian\nupdating on average compared to the current best practice of visualizing\nuncertainty in the observed data. For large data samples (N=5208), where\npeople's updated beliefs tend to deviate more strongly from the prescriptions\nof a Bayesian model, we find evidence that the effectiveness of the two forms\nof Bayesian assistance may depend on people's proclivity toward trusting the\nsource of the data. We discuss how our results provide insight into individual\nprocesses of belief updating and subjective uncertainty, and how understanding\nthese aspects of interpretation paves the way for more sophisticated\ninteractive visualizations for analysis and communication.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 01:12:25 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 22:32:29 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kim", "Yea-Seul", ""], ["Kayongo", "Paula", ""], ["Grunde-McLaughlin", "Madeleine", ""], ["Hullman", "Jessica", ""]]}, {"id": "2008.00151", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Jian Zhao, Francine Chen, Kwan-Liu Ma", "title": "A Visual Analytics Framework for Contrastive Network Analysis", "comments": "To appear in IEEE Conference on Visual Analytics Science and\n  Technology (VAST) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.GR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common network analysis task is comparison of two networks to identify\nunique characteristics in one network with respect to the other. For example,\nwhen comparing protein interaction networks derived from normal and cancer\ntissues, one essential task is to discover protein-protein interactions unique\nto cancer tissues. However, this task is challenging when the networks contain\ncomplex structural (and semantic) relations. To address this problem, we design\nContraNA, a visual analytics framework leveraging both the power of machine\nlearning for uncovering unique characteristics in networks and also the\neffectiveness of visualization for understanding such uniqueness. The basis of\nContraNA is cNRL, which integrates two machine learning schemes, network\nrepresentation learning (NRL) and contrastive learning (CL), to generate a\nlow-dimensional embedding that reveals the uniqueness of one network when\ncompared to another. ContraNA provides an interactive visualization interface\nto help analyze the uniqueness by relating embedding results and network\nstructures as well as explaining the learned features by cNRL. We demonstrate\nthe usefulness of ContraNA with two case studies using real-world datasets. We\nalso evaluate through a controlled user study with 12 participants on network\ncomparison tasks. The results show that participants were able to both\neffectively identify unique characteristics from complex networks and interpret\nthe results obtained from cNRL.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 02:18:10 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 01:46:51 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Zhao", "Jian", ""], ["Chen", "Francine", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2008.00380", "submitter": "Sharmin Majumder", "authors": "Sharmin Majumder, Nasser Kehtarnavaz", "title": "Vision and Inertial Sensing Fusion for Human Action Recognition : A\n  Review", "comments": "14 pages,4 figures,2 tables. Submitted to IEEE Sensors Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human action recognition is used in many applications such as video\nsurveillance, human computer interaction, assistive living, and gaming. Many\npapers have appeared in the literature showing that the fusion of vision and\ninertial sensing improves recognition accuracies compared to the situations\nwhen each sensing modality is used individually. This paper provides a survey\nof the papers in which vision and inertial sensing are used simultaneously\nwithin a fusion framework in order to perform human action recognition. The\nsurveyed papers are categorized in terms of fusion approaches, features,\nclassifiers, as well as multimodality datasets considered. Challenges as well\nas possible future directions are also stated for deploying the fusion of these\ntwo sensing modalities under realistic conditions.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 02:06:44 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Majumder", "Sharmin", ""], ["Kehtarnavaz", "Nasser", ""]]}, {"id": "2008.00666", "submitter": "Jiacheng Pan", "authors": "Jiacheng Pan, Wei Chen, Xiaodong Zhao, Shuyue Zhou, Wei Zeng, Minfeng\n  Zhu, Jian Chen, Siwei Fu, Yingcai Wu", "title": "Exemplar-based Layout Fine-tuning for Node-link Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design and evaluate a novel layout fine-tuning technique for node-link\ndiagrams that facilitates exemplar-based adjustment of a group of substructures\nin batching mode. The key idea is to transfer user modifications on a local\nsubstructure to other substructures in the whole graph that are topologically\nsimilar to the exemplar. We first precompute a canonical representation for\neach substructure with node embedding techniques and then use it for on-the-fly\nsubstructure retrieval. We design and develop a light-weight interactive system\nto enable intuitive adjustment, modification transfer, and visual graph\nexploration. We also report some results of quantitative comparisons, three\ncase studies, and a within-participant user study.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 06:36:05 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 08:32:39 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 05:10:57 GMT"}, {"version": "v4", "created": "Thu, 20 Aug 2020 02:33:29 GMT"}, {"version": "v5", "created": "Thu, 3 Sep 2020 10:23:02 GMT"}, {"version": "v6", "created": "Sun, 6 Sep 2020 10:38:57 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Pan", "Jiacheng", ""], ["Chen", "Wei", ""], ["Zhao", "Xiaodong", ""], ["Zhou", "Shuyue", ""], ["Zeng", "Wei", ""], ["Zhu", "Minfeng", ""], ["Chen", "Jian", ""], ["Fu", "Siwei", ""], ["Wu", "Yingcai", ""]]}, {"id": "2008.00699", "submitter": "Harold Soh", "authors": "Joshua Lee, Jeffrey Fong, Bing Cai Kok, Harold Soh", "title": "Getting to Know One Another: Calibrating Intent, Capabilities and Trust\n  for Human-Robot Collaboration", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common experience suggests that agents who know each other well are better\nable to work together. In this work, we address the problem of calibrating\nintention and capabilities in human-robot collaboration. In particular, we\nfocus on scenarios where the robot is attempting to assist a human who is\nunable to directly communicate her intent. Moreover, both agents may have\ndiffering capabilities that are unknown to one another. We adopt a\ndecision-theoretic approach and propose the TICC-POMDP for modeling this\nsetting, with an associated online solver. Experiments show our approach leads\nto better team performance both in simulation and in a real-world study with\nhuman subjects.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 08:04:15 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lee", "Joshua", ""], ["Fong", "Jeffrey", ""], ["Kok", "Bing Cai", ""], ["Soh", "Harold", ""]]}, {"id": "2008.01051", "submitter": "Ruikun Luo", "authors": "Ruikun Luo, Na Du, X. Jessie Yang", "title": "Enhancing autonomy transparency: an option-centric rationale approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While the advances in artificial intelligence and machine learning empower a\nnew generation of autonomous systems for assisting human performance, one major\nconcern arises from the human factors perspective: Humans have difficulty\ndeciphering autonomy-generated solutions and increasingly perceive autonomy as\na mysterious black box. The lack of transparency contributes to the lack of\ntrust in autonomy and sub-optimal team performance. To enhance autonomy\ntransparency, this study proposed an option-centric rationale display and\nevaluated its effectiveness. We developed a game Treasure Hunter wherein a\nhuman uncovers a map for treasures with the help from an intelligent assistant,\nand conducted a human-in-the-loop experiment with 34 participants. Results\nindicated that by conveying the intelligent assistant's decision-making\nrationale via the option-centric rationale display, participants had higher\ntrust in the system and calibrated their trust faster. Additionally, higher\ntrust led to higher acceptance of recommendations from the intelligent\nassistant, and in turn higher task performance.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:37:16 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Luo", "Ruikun", ""], ["Du", "Na", ""], ["Yang", "X. Jessie", ""]]}, {"id": "2008.01139", "submitter": "Iain Cruickshank", "authors": "Iain J. Cruickshank and Kathleen M. Carley", "title": "Characterizing Communities of Hashtag Usage on Twitter During the 2020\n  COVID-19 Pandemic by Multi-view Clustering", "comments": "Pending review with Journal of Applied Network Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has produced a flurry of online activity on social\nmedia sites. As such, analysis of social media data during the COVID-19\npandemic can produce unique insights into discussion topics and how those\ntopics evolve over the course of the pandemic. In this study, we propose\nanalyzing discussion topics on Twitter by clustering hashtags. In order to\nobtain high-quality clusters of the Twitter hashtags, we also propose a novel\nmulti-view clustering technique that incorporates multiple different data types\nthat can be used to describe how users interact with hashtags. The results of\nour multi-view clustering show that there are distinct temporal and topical\ntrends present within COVID-19 twitter discussion. In particular, we find that\nsome topical clusters of hashtags shift over the course of the pandemic, while\nothers are persistent throughout, and that there are distinct temporal trends\nin hashtag usage. This study is the first to use multi-view clustering to\nanalyze hashtags and the first analysis of the greater trends of discussion\noccurring online during the COVID-19 pandemic.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 19:14:37 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Cruickshank", "Iain J.", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "2008.01177", "submitter": "Shizhao Sun", "authors": "Chunyao Qian, Shizhao Sun, Weiwei Cui, Jian-Guang Lou, Haidong Zhang,\n  and Dongmei Zhang", "title": "Retrieve-Then-Adapt: Example-based Automatic Generation for\n  Proportion-related Infographics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infographic is a data visualization technique which combines graphic and\ntextual descriptions in an aesthetic and effective manner. Creating\ninfographics is a difficult and time-consuming process which often requires\nsignificant attempts and adjustments even for experienced designers, not to\nmention novice users with limited design expertise. Recently, a few approaches\nhave been proposed to automate the creation process by applying predefined\nblueprints to user information. However, predefined blueprints are often hard\nto create, hence limited in volume and diversity. In contrast, good\ninfogrpahics have been created by professionals and accumulated on the Internet\nrapidly. These online examples often represent a wide variety of design styles,\nand serve as exemplars or inspiration to people who like to create their own\ninfographics. Based on these observations, we propose to generate infographics\nby automatically imitating examples. We present a two-stage approach, namely\nretrieve-then-adapt. In the retrieval stage, we index online examples by their\nvisual elements. For a given user information, we transform it to a concrete\nquery by sampling from a learned distribution about visual elements, and then\nfind appropriate examples in our example library based on the similarity\nbetween example indexes and the query. For a retrieved example, we generate an\ninitial drafts by replacing its content with user information. However, in many\ncases, user information cannot be perfectly fitted to retrieved examples.\nTherefore, we further introduce an adaption stage. Specifically, we propose a\nMCMC-like approach and leverage recursive neural networks to help adjust the\ninitial draft and improve its visual appearance iteratively, until a\nsatisfactory result is obtained. We implement our approach on\nproportion-related infographics, and demonstrate its effectiveness by sample\nresults and expert reviews.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 07:55:00 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Qian", "Chunyao", ""], ["Sun", "Shizhao", ""], ["Cui", "Weiwei", ""], ["Lou", "Jian-Guang", ""], ["Zhang", "Haidong", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2008.01181", "submitter": "Diego Felipe Paez Granados", "authors": "Yang Chen, Diego Paez-Granados, Hideki Kadone, Kenji Suzuki", "title": "Control Interface for Hands-free Navigation of Standing Mobility\n  Vehicles based on Upper-Body Natural Movements", "comments": "2020 IEEE. Personal use of this material is permitted. Permission\n  from IEEE must be obtained for all other uses", "journal-ref": "IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS), Oct.25-29, 2020 - Las Vegas, USA", "doi": "10.1109/IROS45743.2020.9340875", "report-no": null, "categories": "cs.RO cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and evaluate a novel human-machine interface (HMI)\nfor controlling a standing mobility vehicle or person carrier robot, aiming for\na hands-free control through upper-body natural postures derived from gaze\ntracking while walking. We target users with lower-body impairment with\nremaining upper-body motion capabilities. The developed HMI bases on a sensing\narray for capturing body postures; an intent recognition algorithm for\ncontinuous mapping of body motions to robot control space; and a personalizing\nsystem for multiple body sizes and shapes. We performed two user studies:\nfirst, an analysis of the required body muscles involved in navigating with the\nproposed control; and second, an assessment of the HMI compared with a standard\njoystick through quantitative and qualitative metrics in a narrow circuit task.\nWe concluded that the main user control contribution comes from Rectus\nAbdominis and Erector Spinae muscle groups at different levels. Finally, the\ncomparative study showed that a joystick still outperforms the proposed HMI in\nusability perceptions and controllability metrics, however, the smoothness of\nuser control was similar in jerk and fluency. Moreover, users' perceptions\nshowed that hands-free control made it more anthropomorphic, animated, and even\nsafer.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 20:40:58 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Chen", "Yang", ""], ["Paez-Granados", "Diego", ""], ["Kadone", "Hideki", ""], ["Suzuki", "Kenji", ""]]}, {"id": "2008.01273", "submitter": "Shuo Zhang", "authors": "Upasana Dutta, Rhett Hanscom, Jason Shuo Zhang, Richard Han, Tamara\n  Lehman, Qin Lv, Shivakant Mishra", "title": "Analyzing Twitter Users' Behavior Before and After Contact by the\n  Internet Research Agency", "comments": "Accepted to CSCW 2021", "journal-ref": null, "doi": "10.1145/3449164", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms have been exploited to conduct election interference\nin recent years. In particular, the Russian-backed Internet Research Agency\n(IRA) has been identified as a key source of misinformation spread on Twitter\nprior to the 2016 U.S. presidential election. The goal of this research is to\nunderstand whether general Twitter users changed their behavior in the year\nfollowing first contact from an IRA account. We compare the before and after\nbehavior of contacted users to determine whether there were differences in\ntheir mean tweet count, the sentiment of their tweets, and the frequency and\nsentiment of tweets mentioning @realDonaldTrump or @HillaryClinton. Our results\nindicate that users overall exhibited statistically significant changes in\nbehavior across most of these metrics, and that those users that engaged with\nthe IRA generally showed greater changes in behavior.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 02:04:17 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 22:03:49 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Dutta", "Upasana", ""], ["Hanscom", "Rhett", ""], ["Zhang", "Jason Shuo", ""], ["Han", "Richard", ""], ["Lehman", "Tamara", ""], ["Lv", "Qin", ""], ["Mishra", "Shivakant", ""]]}, {"id": "2008.01513", "submitter": "Naoki Wake", "authors": "Naoki Wake, Riku Arakawa, Iori Yanokura, Takuya Kiyokawa, Kazuhiro\n  Sasabuchi, Jun Takamatsu, Katsushi Ikeuchi", "title": "A Learning-from-Observation Framework: One-Shot Robot Teaching for\n  Grasp-Manipulation-Release Household Operations", "comments": "6 pages, 6 figures. Submitted to and accepted by IEEE/SICE SII 2021.\n  Last updated October 20th, 2020", "journal-ref": null, "doi": "10.1109/IEEECONF49454.2021.9382750", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A household robot is expected to perform various manipulative operations with\nan understanding of the purpose of the task. To this end, a desirable robotic\napplication should provide an on-site robot teaching framework for non-experts.\nHere we propose a Learning-from-Observation (LfO) framework for\ngrasp-manipulation-release class household operations (GMR-operations). The\nframework maps human demonstrations to predefined task models through one-shot\nteaching. Each task model contains both high-level knowledge regarding the\ngeometric constraints and low-level knowledge related to human postures. The\nkey idea is to design a task model that 1) covers various GMR-operations and 2)\nincludes human postures to achieve tasks. We verify the applicability of our\nframework by testing an operational LfO system with a real robot. In addition,\nwe quantify the coverage of the task model by analyzing online videos of\nhousehold operations. In the context of one-shot robot teaching, the\ncontribution of this study is a framework that 1) covers various GMR-operations\nand 2) mimics human postures during the operations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 13:26:25 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 17:49:28 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 15:19:48 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 09:42:16 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wake", "Naoki", ""], ["Arakawa", "Riku", ""], ["Yanokura", "Iori", ""], ["Kiyokawa", "Takuya", ""], ["Sasabuchi", "Kazuhiro", ""], ["Takamatsu", "Jun", ""], ["Ikeuchi", "Katsushi", ""]]}, {"id": "2008.01613", "submitter": "Haotian Li", "authors": "Haotian Li, Huan Wei, Yong Wang, Yangqiu Song, Huamin Qu", "title": "Peer-inspired Student Performance Prediction in Interactive Online\n  Question Pools with Graph Neural Network", "comments": "8 pages, 8 figures. Accepted at CIKM 2020", "journal-ref": null, "doi": "10.1145/3340531.3412733", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student performance prediction is critical to online education. It can\nbenefit many downstream tasks on online learning platforms, such as estimating\ndropout rates, facilitating strategic intervention, and enabling adaptive\nonline learning. Interactive online question pools provide students with\ninteresting interactive questions to practice their knowledge in online\neducation. However, little research has been done on student performance\nprediction in interactive online question pools. Existing work on student\nperformance prediction targets at online learning platforms with predefined\ncourse curriculum and accurate knowledge labels like MOOC platforms, but they\nare not able to fully model knowledge evolution of students in interactive\nonline question pools. In this paper, we propose a novel approach using Graph\nNeural Networks (GNNs) to achieve better student performance prediction in\ninteractive online question pools. Specifically, we model the relationship\nbetween students and questions using student interactions to construct the\nstudent-interaction-question network and further present a new GNN model,\ncalled R^2GCN, which intrinsically works for the heterogeneous networks, to\nachieve generalizable student performance prediction in interactive online\nquestion pools. We evaluate the effectiveness of our approach on a real-world\ndataset consisting of 104,113 mouse trajectories generated in the\nproblem-solving process of over 4000 students on 1631 questions. The experiment\nresults show that our approach can achieve a much higher accuracy of student\nperformance prediction than both traditional machine learning approaches and\nGNN models.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 14:55:32 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 07:47:01 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Li", "Haotian", ""], ["Wei", "Huan", ""], ["Wang", "Yong", ""], ["Song", "Yangqiu", ""], ["Qu", "Huamin", ""]]}, {"id": "2008.01645", "submitter": "Takanori Fujiwara", "authors": "Takanori Fujiwara, Shilpika, Naohisa Sakamoto, Jorji Nonaka, Keiji\n  Yamamoto, and Kwan-Liu Ma", "title": "A Visual Analytics Framework for Reviewing Multivariate Time-Series Data\n  with Dimensionality Reduction", "comments": "To appear in IEEE Transactions on Visualization and Computer Graphics\n  and IEEE VIS 2020 (VAST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven problem solving in many real-world applications involves analysis\nof time-dependent multivariate data, for which dimensionality reduction (DR)\nmethods are often used to uncover the intrinsic structure and features of the\ndata. However, DR is usually applied to a subset of data that is either\nsingle-time-point multivariate or univariate time-series, resulting in the need\nto manually examine and correlate the DR results out of different data subsets.\nWhen the number of dimensions is large either in terms of the number of time\npoints or attributes, this manual task becomes too tedious and infeasible. In\nthis paper, we present MulTiDR, a new DR framework that enables processing of\ntime-dependent multivariate data as a whole to provide a comprehensive overview\nof the data. With the framework, we employ DR in two steps. When treating the\ninstances, time points, and attributes of the data as a 3D array, the first DR\nstep reduces the three axes of the array to two, and the second DR step\nvisualizes the data in a lower-dimensional space. In addition, by coupling with\na contrastive learning method and interactive visualizations, our framework\nenhances analysts' ability to interpret DR results. We demonstrate the\neffectiveness of our framework with four case studies using real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 04:22:43 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 00:37:25 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Fujiwara", "Takanori", ""], ["Shilpika", "", ""], ["Sakamoto", "Naohisa", ""], ["Nonaka", "Jorji", ""], ["Yamamoto", "Keiji", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2008.01659", "submitter": "Alireza Abedin", "authors": "Alireza Abedin, Farbod Motlagh, Qinfeng Shi, Seyed Hamid Rezatofighi,\n  Damith Chinthana Ranasinghe", "title": "Towards Deep Clustering of Human Activities from Wearables", "comments": "Accepted at ISWC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our ability to exploit low-cost wearable sensing modalities for critical\nhuman behaviour and activity monitoring applications in health and wellness is\nreliant on supervised learning regimes; here, deep learning paradigms have\nproven extremely successful in learning activity representations from annotated\ndata. However, the costly work of gathering and annotating sensory activity\ndatasets is labor-intensive, time consuming and not scalable to large volumes\nof data. While existing unsupervised remedies of deep clustering leverage\nnetwork architectures and optimization objectives that are tailored for static\nimage datasets, deep architectures to uncover cluster structures from raw\nsequence data captured by on-body sensors remains largely unexplored. In this\npaper, we develop an unsupervised end-to-end learning strategy for the\nfundamental problem of human activity recognition (HAR) from wearables. Through\nextensive experiments, including comparisons with existing methods, we show the\neffectiveness of our approach to jointly learn unsupervised representations for\nsensory data and generate cluster assignments with strong semantic\ncorrespondence to distinct human activities.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 13:55:24 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 05:35:27 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Abedin", "Alireza", ""], ["Motlagh", "Farbod", ""], ["Shi", "Qinfeng", ""], ["Rezatofighi", "Seyed Hamid", ""], ["Ranasinghe", "Damith Chinthana", ""]]}, {"id": "2008.01719", "submitter": "Sayamindu Dasgupta", "authors": "Sayamindu Dasgupta and Benjamin Mako Hill", "title": "Designing for Critical Algorithmic Literacies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As pervasive data collection and powerful algorithms increasingly shape\nchildren's experience of the world and each other, their ability to interrogate\ncomputational algorithms has become crucially important. A growing body of work\nhas attempted to articulate a set of \"literacies\" to describe the intellectual\ntools that children can use to understand, interrogate, and critique the\nalgorithmic systems that shape their lives. Unfortunately, because many\nalgorithms are invisible, only a small number of children develop the\nliteracies required to critique these systems. How might designers support the\ndevelopment of critical algorithmic literacies? Based on our experience\ndesigning two data programming systems, we present four design principles that\nwe argue can help children develop literacies that allow them to understand not\nonly how algorithms work, but also to critique and question them.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 17:51:02 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Dasgupta", "Sayamindu", ""], ["Hill", "Benjamin Mako", ""]]}, {"id": "2008.01723", "submitter": "Keith Burghardt", "authors": "Keith Burghardt, Nazgol Tavabi, Emilio Ferrara, Shrikanth Narayanan,\n  Kristina Lerman", "title": "Having a Bad Day? Detecting the Impact of Atypical Life Events Using\n  Wearable Sensors", "comments": "10 pages, 4 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Life events can dramatically affect our psychological state and work\nperformance. Stress, for example, has been linked to professional\ndissatisfaction, increased anxiety, and workplace burnout. We explore the\nimpact of positive and negative life events on a number of psychological\nconstructs through a multi-month longitudinal study of hospital and aerospace\nworkers. Through causal inference, we demonstrate that positive life events\nincrease positive affect, while negative events increase stress, anxiety and\nnegative affect. While most events have a transient effect on psychological\nstates, major negative events, like illness or attending a funeral, can reduce\npositive affect for multiple days. Next, we assess whether these events can be\ndetected through wearable sensors, which can cheaply and unobtrusively monitor\nhealth-related factors. We show that these sensors paired with embedding-based\nlearning models can be used ``in the wild'' to capture atypical life events in\nhundreds of workers across both datasets. Overall our results suggest that\nautomated interventions based on physiological sensing may be feasible to help\nworkers regulate the negative effects of life events.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 17:55:54 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Burghardt", "Keith", ""], ["Tavabi", "Nazgol", ""], ["Ferrara", "Emilio", ""], ["Narayanan", "Shrikanth", ""], ["Lerman", "Kristina", ""]]}, {"id": "2008.01769", "submitter": "Xiang 'Anthony' Chen", "authors": "Xiang 'Anthony' Chen", "title": "FaceOff: Detecting Face Touching with a Wrist-Worn Accelerometer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the CDC, one key step of preventing oneself from contracting\ncoronavirus (COVID-19) is to avoid touching eyes, nose, and mouth with unwashed\nhands. However, touching one's face is a frequent and spontaneous\nbehavior---one study observed subjects touching their faces on average 23 times\nper hour. Creative solutions have emerged amongst some recent commercial and\nhobbyists' projects, yet most either are closed-source or lack validation in\nperformance. We develop FaceOff---a sensing technique using a commodity\nwrist-worn accelerometer to detect face-touching behavior based on the specific\nmotion pattern of raising one's hand towards the face. We report a survey\n(N=20) that elicits different ways people touch their faces, an algorithm that\ntemporally ensembles data-driven models to recognize when a face touching\nbehavior occurs and results from a preliminary user testing (N=3 for a total of\nabout 90 minutes).\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:11:51 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Chen", "Xiang 'Anthony'", ""]]}, {"id": "2008.01980", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer, Andreas Breiter", "title": "More Than Accuracy: Towards Trustworthy Machine Learning Interfaces for\n  Object Recognition", "comments": "UMAP '20: Proceedings of the 28th ACM Conference on User Modeling,\n  Adaptation and Personalization", "journal-ref": "UMAP 2020: Proceedings of the 28th ACM Conference on User\n  Modeling, Adaptation and Personalization", "doi": "10.1145/3340631.3394873", "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the user experience of visualizations of a machine\nlearning (ML) system that recognizes objects in images. This is important since\neven good systems can fail in unexpected ways as misclassifications on\nphoto-sharing websites showed. In our study, we exposed users with a background\nin ML to three visualizations of three systems with different levels of\naccuracy. In interviews, we explored how the visualization helped users assess\nthe accuracy of systems in use and how the visualization and the accuracy of\nthe system affected trust and reliance. We found that participants do not only\nfocus on accuracy when assessing ML systems. They also take the perceived\nplausibility and severity of misclassification into account and prefer seeing\nthe probability of predictions. Semantically plausible errors are judged as\nless severe than errors that are implausible, which means that system accuracy\ncould be communicated through the types of errors.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 07:56:37 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Heuer", "Hendrik", ""], ["Breiter", "Andreas", ""]]}, {"id": "2008.01988", "submitter": "Hendrik Heuer", "authors": "Hendrik Heuer, Andreas Breiter", "title": "How Fake News Affect Trust in the Output of a Machine Learning System\n  for News Curation", "comments": "This is a pre-print of an article published in MISDOOM 2020 - 2nd\n  Multidisciplinary International Symposium on Disinformation in Open Online\n  Media", "journal-ref": "MISDOOM 2020 - 2nd Multidisciplinary International Symposium on\n  Disinformation in Open Online Media", "doi": null, "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People are increasingly consuming news curated by machine learning (ML)\nsystems. Motivated by studies on algorithmic bias, this paper explores which\nrecommendations of an algorithmic news curation system users trust and how this\ntrust is affected by untrustworthy news stories like fake news. In a study with\n82 vocational school students with a background in IT, we found that users are\nable to provide trust ratings that distinguish trustworthy recommendations of\nquality news stories from untrustworthy recommendations. However, a single\nuntrustworthy news story combined with four trustworthy news stories is rated\nsimilarly as five trustworthy news stories. The results could be a first\nindication that untrustworthy news stories benefit from appearing in a\ntrustworthy context. The results also show the limitations of users' abilities\nto rate the recommendations of a news curation system. We discuss the\nimplications of this for the user experience of interactive machine learning\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 08:17:20 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Heuer", "Hendrik", ""], ["Breiter", "Andreas", ""]]}, {"id": "2008.02016", "submitter": "Ankit Kariryaa", "authors": "Ankit Kariryaa, Tony Veale and Johannes Sch\\\"oning", "title": "Activity and mood-based routing for autonomous vehicles", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A significant amount of our daily lives is dedicated to driving, leading to\nan unavoidable exposure to driving-related stress. The rise of autonomous\nvehicles will likely lessen the extent of this stress and enhance the routine\ntraveling experience. Yet, no matter how diverse they may be, current routing\ncriteria are limited to considering only the passive preferences of a vehicle's\nusers. Thus, to enhance the overall driving experience in autonomous vehicles,\nwe advocate here for the diversification of routing criteria, by additionally\nemphasizing activity- and mood-based requirements.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 09:29:32 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Kariryaa", "Ankit", ""], ["Veale", "Tony", ""], ["Sch\u00f6ning", "Johannes", ""]]}, {"id": "2008.02114", "submitter": "Andrea Papenmeier", "authors": "Andrea Papenmeier, Alfred Sliwa, Dagmar Kern, Daniel Hienert, Ahmet\n  Aker and Norbert Fuhr", "title": "'A Modern Up-To-Date Laptop' -- Vagueness in Natural Language Queries\n  for Product Search", "comments": null, "journal-ref": "Proceedings of the 2020 ACM on Designing Interactive Systems\n  Conference, 2020, pp. 2077-2089", "doi": "10.1145/3357236.3395489", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of voice assistants and an increase in mobile search usage,\nnatural language has become an important query language. So far, most of the\ncurrent systems are not able to process these queries because of the vagueness\nand ambiguity in natural language. Users have adapted their query formulation\nto what they think the search engine is capable of, which adds to their\ncognitive burden. With our research, we contribute to the design of interactive\nsearch systems by investigating the genuine information need in a product\nsearch scenario. In a crowd-sourcing experiment, we collected 132 information\nneeds in natural language. We examine the vagueness of the formulations and\ntheir match to retailer-generated content and user-generated product reviews.\nOur findings reveal high variance on the level of vagueness and the potential\nof user reviews as a source for supporting users with rather vague search\nintents.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 13:07:01 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Papenmeier", "Andrea", ""], ["Sliwa", "Alfred", ""], ["Kern", "Dagmar", ""], ["Hienert", "Daniel", ""], ["Aker", "Ahmet", ""], ["Fuhr", "Norbert", ""]]}, {"id": "2008.02234", "submitter": "Chuhao Liu", "authors": "Chuhao Liu, Shaojie Shen", "title": "An Augmented Reality Interaction Interface for Autonomous Drone", "comments": "6 pages, 6 figures, IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human drone interaction in autonomous navigation incorporates spatial\ninteraction tasks, including reconstructed 3D map from the drone and human\ndesired target position. Augmented Reality (AR) devices can be powerful\ninteractive tools for handling these spatial interactions. In this work, we\nbuild an AR interface that displays the reconstructed 3D map from the drone on\nphysical surfaces in front of the operator. Spatial target positions can be\nfurther set on the 3D map by intuitive head gaze and hand gesture. The AR\ninterface is deployed to interact with an autonomous drone to explore an\nunknown environment. A user study is further conducted to evaluate the overall\ninteraction performance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 17:02:22 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Liu", "Chuhao", ""], ["Shen", "Shaojie", ""]]}, {"id": "2008.02311", "submitter": "Ranjay Krishna", "authors": "Pranav Khadpe, Ranjay Krishna, Li Fei-Fei, Jeffrey Hancock, Michael\n  Bernstein", "title": "Conceptual Metaphors Impact Perceptions of Human-AI Collaboration", "comments": "CSCW 2020", "journal-ref": "PACM HCI Volume 4 CSCW 2, 2020", "doi": "10.1145/3415234", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emergence of conversational artificial intelligence (AI) agents, it\nis important to understand the mechanisms that influence users' experiences of\nthese agents. We study a common tool in the designer's toolkit: conceptual\nmetaphors. Metaphors can present an agent as akin to a wry teenager, a toddler,\nor an experienced butler. How might a choice of metaphor influence our\nexperience of the AI agent? Sampling metaphors along the dimensions of warmth\nand competence---defined by psychological theories as the primary axes of\nvariation for human social perception---we perform a study (N=260) where we\nmanipulate the metaphor, but not the behavior, of a Wizard-of-Oz conversational\nagent. Following the experience, participants are surveyed about their\nintention to use the agent, their desire to cooperate with the agent, and the\nagent's usability. Contrary to the current tendency of designers to use high\ncompetence metaphors to describe AI products, we find that metaphors that\nsignal low competence lead to better evaluations of the agent than metaphors\nthat signal high competence. This effect persists despite both high and low\ncompetence agents featuring human-level performance and the wizards being blind\nto condition. A second study confirms that intention to adopt decreases rapidly\nas competence projected by the metaphor increases. In a third study, we assess\neffects of metaphor choices on potential users' desire to try out the system\nand find that users are drawn to systems that project higher competence and\nwarmth. These results suggest that projecting competence may help attract new\nusers, but those users may discard the agent unless it can quickly correct with\na lower competence metaphor. We close with a retrospective analysis that finds\nsimilar patterns between metaphors and user attitudes towards past\nconversational agents such as Xiaoice, Replika, Woebot, Mitsuku, and Tay.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 18:39:56 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Khadpe", "Pranav", ""], ["Krishna", "Ranjay", ""], ["Fei-Fei", "Li", ""], ["Hancock", "Jeffrey", ""], ["Bernstein", "Michael", ""]]}, {"id": "2008.02323", "submitter": "Saurabh Adya", "authors": "Saurabh Adya, Vineet Garg, Siddharth Sigtia, Pramod Simha, Chandra\n  Dhir", "title": "Hybrid Transformer/CTC Networks for Hardware Efficient Voice Triggering", "comments": "INTERSPEECH, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the design of two-pass voice trigger detection systems. We focus\non the networks in the second pass that are used to re-score candidate segments\nobtained from the first-pass. Our baseline is an acoustic model(AM), with\nBiLSTM layers, trained by minimizing the CTC loss. We replace the BiLSTM layers\nwith self-attention layers. Results on internal evaluation sets show that\nself-attention networks yield better accuracy while requiring fewer parameters.\nWe add an auto-regressive decoder network on top of the self-attention layers\nand jointly minimize the CTC loss on the encoder and the cross-entropy loss on\nthe decoder. This design yields further improvements over the baseline. We\nretrain all the models above in a multi-task learning(MTL) setting, where one\nbranch of a shared network is trained as an AM, while the second branch\nclassifies the whole sequence to be true-trigger or not. Results demonstrate\nthat networks with self-attention layers yield $\\sim$60% relative reduction in\nfalse reject rates for a given false-alarm rate, while requiring 10% fewer\nparameters. When trained in the MTL setup, self-attention networks yield\nfurther accuracy improvements. On-device measurements show that we observe 70%\nrelative reduction in inference time. Additionally, the proposed network\narchitectures are $\\sim$5X faster to train.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 19:16:33 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Adya", "Saurabh", ""], ["Garg", "Vineet", ""], ["Sigtia", "Siddharth", ""], ["Simha", "Pramod", ""], ["Dhir", "Chandra", ""]]}, {"id": "2008.02354", "submitter": "Yukino Baba", "authors": "Yukino Baba, Jiyi Li, Hisashi Kashima", "title": "CrowDEA: Multi-view Idea Prioritization with Crowds", "comments": "Accepted in HCOMP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set of ideas collected from crowds with regard to an open-ended\nquestion, how can we organize and prioritize them in order to determine the\npreferred ones based on preference comparisons by crowd evaluators? As there\nare diverse latent criteria for the value of an idea, multiple ideas can be\nconsidered as \"the best\". In addition, evaluators can have different preference\ncriteria, and their comparison results often disagree.\n  In this paper, we propose an analysis method for obtaining a subset of ideas,\nwhich we call frontier ideas, that are the best in terms of at least one latent\nevaluation criterion. We propose an approach, called CrowDEA, which estimates\nthe embeddings of the ideas in the multiple-criteria preference space, the best\nviewpoint for each idea, and preference criterion for each evaluator, to obtain\na set of frontier ideas. Experimental results using real datasets containing\nnumerous ideas or designs demonstrate that the proposed approach can\neffectively prioritize ideas from multiple viewpoints, thereby detecting\nfrontier ideas. The embeddings of ideas learned by the proposed approach\nprovide a visualization that facilitates observation of the frontier ideas. In\naddition, the proposed approach prioritizes ideas from a wider variety of\nviewpoints, whereas the baselines tend to use to the same viewpoints; it can\nalso handle various viewpoints and prioritize ideas in situations where only a\nlimited number of evaluators or labels are available.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 07:41:18 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 14:22:13 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Baba", "Yukino", ""], ["Li", "Jiyi", ""], ["Kashima", "Hisashi", ""]]}, {"id": "2008.02487", "submitter": "Iv\\'an L\\'opez-Espejo", "authors": "Santi Prieto, Alfonso Ortega, Iv\\'an L\\'opez-Espejo, Eduardo Lleida", "title": "Shouted Speech Compensation for Speaker Verification Robust to Vocal\n  Effort Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of speaker verification systems degrades when vocal effort\nconditions between enrollment and test (e.g., shouted vs. normal speech) are\ndifferent. This is a potential situation in non-cooperative speaker\nverification tasks. In this paper, we present a study on different methods for\nlinear compensation of embeddings making use of Gaussian mixture models to\ncluster shouted and normal speech domains. These compensation techniques are\nborrowed from the area of robustness for automatic speech recognition and, in\nthis work, we apply them to compensate the mismatch between shouted and normal\nconditions in speaker verification. Before compensation, shouted condition is\nautomatically detected by means of logistic regression. The process is\ncomputationally light and it is performed in the back-end of an x-vector\nsystem. Experimental results show that applying the proposed approach in the\npresence of vocal effort mismatch yields up to 13.8% equal error rate relative\nimprovement with respect to a system that applies neither shouted speech\ndetection nor compensation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 07:25:57 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Prieto", "Santi", ""], ["Ortega", "Alfonso", ""], ["L\u00f3pez-Espejo", "Iv\u00e1n", ""], ["Lleida", "Eduardo", ""]]}, {"id": "2008.02543", "submitter": "Leevi Raivio", "authors": "Leevi Raivio, Han He, Johanna Virkki, Heikki Huttunen", "title": "Handwritten Character Recognition from Wearable Passive RFID", "comments": "Submitted to ICPR2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the recognition of handwritten characters from data\ncaptured by a novel wearable electro-textile sensor panel. The data is\ncollected sequentially, such that we record both the stroke order and the\nresulting bitmap. We propose a preprocessing pipeline that fuses the sequence\nand bitmap representations together. The data is collected from ten subjects\ncontaining altogether 7500 characters. We also propose a convolutional neural\nnetwork architecture, whose novel upsampling structure enables successful use\nof conventional ImageNet pretrained networks, despite the small input size of\nonly 10x10 pixels. The proposed model reaches 72\\% accuracy in experimental\ntests, which can be considered good accuracy for this challenging dataset. Both\nthe data and the model are released to the public.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 09:45:29 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Raivio", "Leevi", ""], ["He", "Han", ""], ["Virkki", "Johanna", ""], ["Huttunen", "Heikki", ""]]}, {"id": "2008.02582", "submitter": "Andrey Krekhov", "authors": "Andrey Krekhov, Daniel Preu{\\ss}, Sebastian Cmentowski, and Jens\n  Kr\\\"uger", "title": "Silhouette Games: An Interactive One-Way Mirror Approach to Watching\n  Players in VR", "comments": "11 pages, 7 figures, 2 tables, conference paper, to be published in\n  CHI PLAY '20: Proceedings of the Annual Symposium on Computer-Human\n  Interaction in Play", "journal-ref": null, "doi": "10.1145/3410404.3414247", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Watching others play is a key ingredient of digital games and an important\naspect of games user research. However, spectatorship is not very popular in\nvirtual reality, as such games strongly rely on one's feelings of presence. In\nother words, the head-mounted display creates a barrier between the player and\nthe audience. We contribute an alternative watching approach consisting of two\nmajor components: a dynamic view frustum that renders the game scene from the\ncurrent spectator position and a one-way mirror in front of the screen. This\nmirror, together with our silhouetting algorithm, allows seeing the player's\nreflection at the correct position in the virtual world. An exploratory survey\nemphasizes the overall positive experience of the viewers in our setup. In\nparticular, the participants enjoyed their ability to explore the virtual\nsurrounding via physical repositioning and to observe the blended player during\nobject manipulations. Apart from requesting a larger screen, the participants\nexpressed a strong need to interact with the player. Consequently, we suggest\nutilizing our technology as a foundation for novel playful experiences with the\noverarching goal to transform the passive spectator into a collocated player.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 11:25:10 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Krekhov", "Andrey", ""], ["Preu\u00df", "Daniel", ""], ["Cmentowski", "Sebastian", ""], ["Kr\u00fcger", "Jens", ""]]}, {"id": "2008.02778", "submitter": "Omar Delarosa", "authors": "Omar Delarosa, Hang Dong, Mindy Ruan, Ahmed Khalifa, Julian Togelius", "title": "Mixed-Initiative Level Design with RL Brush", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces RL Brush, a level-editing tool for tile-based games\ndesigned for mixed-initiative co-creation. The tool uses\nreinforcement-learning-based models to augment manual human level-design\nthrough the addition of AI-generated suggestions. Here, we apply RL Brush to\ndesigning levels for the classic puzzle game Sokoban. We put the tool online\nand tested it in 39 different sessions. The results show that users using the\nAI suggestions stay around longer and their created levels on average are more\nplayable and more complex than without.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:25:14 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 23:27:39 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 20:18:28 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Delarosa", "Omar", ""], ["Dong", "Hang", ""], ["Ruan", "Mindy", ""], ["Khalifa", "Ahmed", ""], ["Togelius", "Julian", ""]]}, {"id": "2008.02840", "submitter": "Siddharth Reddy", "authors": "Siddharth Reddy, Sergey Levine, Anca D. Dragan", "title": "Assisted Perception: Optimizing Observations to Communicate State", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to help users estimate the state of the world in tasks like robotic\nteleoperation and navigation with visual impairments, where users may have\nsystematic biases that lead to suboptimal behavior: they might struggle to\nprocess observations from multiple sensors simultaneously, receive delayed\nobservations, or overestimate distances to obstacles. While we cannot directly\nchange the user's internal beliefs or their internal state estimation process,\nour insight is that we can still assist them by modifying the user's\nobservations. Instead of showing the user their true observations, we\nsynthesize new observations that lead to more accurate internal state estimates\nwhen processed by the user. We refer to this method as assistive state\nestimation (ASE): an automated assistant uses the true observations to infer\nthe state of the world, then generates a modified observation for the user to\nconsume (e.g., through an augmented reality interface), and optimizes the\nmodification to induce the user's new beliefs to match the assistant's current\nbeliefs. We evaluate ASE in a user study with 12 participants who each perform\nfour tasks: two tasks with known user biases -- bandwidth-limited image\nclassification and a driving video game with observation delay -- and two with\nunknown biases that our method has to learn -- guided 2D navigation and a lunar\nlander teleoperation video game. A different assistance strategy emerges in\neach domain, such as quickly revealing informative pixels to speed up image\nclassification, using a dynamics model to undo observation delay in driving,\nidentifying nearby landmarks for navigation, and exaggerating a visual\nindicator of tilt in the lander game. The results show that ASE substantially\nimproves the task performance of users with bandwidth constraints, observation\ndelay, and other unknown biases.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 19:08:05 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Reddy", "Siddharth", ""], ["Levine", "Sergey", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2008.02863", "submitter": "Homayoon Beigi", "authors": "Sitong Zhou and Homayoon Beigi", "title": "A Transfer Learning Method for Speech Emotion Recognition from Automatic\n  Speech Recognition", "comments": "4 pages, 3 tables and 1 figure", "journal-ref": null, "doi": null, "report-no": "RTI-20200330-01", "categories": "eess.AS cs.HC cs.LG cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a transfer learning method in speech emotion recognition\nbased on a Time-Delay Neural Network (TDNN) architecture. A major challenge in\nthe current speech-based emotion detection research is data scarcity. The\nproposed method resolves this problem by applying transfer learning techniques\nin order to leverage data from the automatic speech recognition (ASR) task for\nwhich ample data is available. Our experiments also show the advantage of\nspeaker-class adaptation modeling techniques by adopting identity-vector\n(i-vector) based features in addition to standard Mel-Frequency Cepstral\nCoefficient (MFCC) features.[1] We show the transfer learning models\nsignificantly outperform the other methods without pretraining on ASR. The\nexperiments performed on the publicly available IEMOCAP dataset which provides\n12 hours of motional speech data. The transfer learning was initialized by\nusing the Ted-Lium v.2 speech dataset providing 207 hours of audio with the\ncorresponding transcripts. We achieve the highest significantly higher accuracy\nwhen compared to state-of-the-art, using five-fold cross validation. Using only\nspeech, we obtain an accuracy 71.7% for anger, excitement, sadness, and\nneutrality emotion content.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 20:37:22 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 18:56:24 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Zhou", "Sitong", ""], ["Beigi", "Homayoon", ""]]}, {"id": "2008.02912", "submitter": "Aaron Hertzmann", "authors": "Camilo Fosco, Vincent Casser, Amish Kumar Bedi, Peter O'Donovan, Aaron\n  Hertzmann, Zoya Bylinskii", "title": "Predicting Visual Importance Across Graphic Design Types", "comments": null, "journal-ref": "Proceedings of UIST 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a Unified Model of Saliency and Importance (UMSI),\nwhich learns to predict visual importance in input graphic designs, and\nsaliency in natural images, along with a new dataset and applications. Previous\nmethods for predicting saliency or visual importance are trained individually\non specialized datasets, making them limited in application and leading to poor\ngeneralization on novel image classes, while requiring a user to know which\nmodel to apply to which input. UMSI is a deep learning-based model\nsimultaneously trained on images from different design classes, including\nposters, infographics, mobile UIs, as well as natural images, and includes an\nautomatic classification module to classify the input. This allows the model to\nwork more effectively without requiring a user to label the input. We also\nintroduce Imp1k, a new dataset of designs annotated with importance\ninformation. We demonstrate two new design interfaces that use importance\nprediction, including a tool for adjusting the relative importance of design\nelements, and a tool for reflowing designs to new aspect ratios while\npreserving visual importance. The model, code, and importance dataset are\navailable at https://predimportance.mit.edu .\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 00:12:18 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Fosco", "Camilo", ""], ["Casser", "Vincent", ""], ["Bedi", "Amish Kumar", ""], ["O'Donovan", "Peter", ""], ["Hertzmann", "Aaron", ""], ["Bylinskii", "Zoya", ""]]}, {"id": "2008.02919", "submitter": "Afsaneh Doryab", "authors": "Momin M. Malik, Afsaneh Doryab, Michael Merrill, J\\\"urgen Pfeffer,\n  Anind K. Dey", "title": "Can Smartphone Co-locations Detect Friendship? It Depends How You Model\n  It", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study to detect friendship, its strength, and its change from\nsmartphone location data collectedamong members of a fraternity. We extract a\nrich set of co-location features and build classifiers that detectfriendships\nand close friendship at 30% above a random baseline. We design cross-validation\nschema to testour model performance in specific application settings, finding\nit robust to seeing new dyads and to temporalvariance.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 00:55:10 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 19:14:31 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 02:15:13 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Malik", "Momin M.", ""], ["Doryab", "Afsaneh", ""], ["Merrill", "Michael", ""], ["Pfeffer", "J\u00fcrgen", ""], ["Dey", "Anind K.", ""]]}, {"id": "2008.02999", "submitter": "Philipp V. Rouast", "authors": "Philipp V. Rouast and Marc T. P. Adam", "title": "Single-stage intake gesture detection using CTC loss and extended prefix\n  beam search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate detection of individual intake gestures is a key step towards\nautomatic dietary monitoring. Both inertial sensor data of wrist movements and\nvideo data depicting the upper body have been used for this purpose. The most\nadvanced approaches to date use a two-stage approach, in which (i) frame-level\nintake probabilities are learned from the sensor data using a deep neural\nnetwork, and then (ii) sparse intake events are detected by finding the maxima\nof the frame-level probabilities. In this study, we propose a single-stage\napproach which directly decodes the probabilities learned from sensor data into\nsparse intake detections. This is achieved by weakly supervised training using\nConnectionist Temporal Classification (CTC) loss, and decoding using a novel\nextended prefix beam search decoding algorithm. Benefits of this approach\ninclude (i) end-to-end training for detections, (ii) simplified timing\nrequirements for intake gesture labels, and (iii) improved detection\nperformance compared to existing approaches. Across two separate datasets, we\nachieve relative $F_1$ score improvements between 1.9% and 6.2% over the\ntwo-stage approach for intake detection and eating/drinking detection tasks,\nfor both video and inertial sensors.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 06:04:25 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 01:05:45 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Rouast", "Philipp V.", ""], ["Adam", "Marc T. P.", ""]]}, {"id": "2008.03174", "submitter": "Jinghui Cheng", "authors": "Audrey Labrie, Jinghui Cheng", "title": "Adapting Nielsen's Usability Heuristics to the Context of Mobile\n  Augmented Reality", "comments": "3 pages, UIST '20 Poster", "journal-ref": null, "doi": "10.1145/3379350.3416167", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR) is an emerging technology in mobile app design during\nrecent years. However, usability challenges in these apps are prominent. There\nare currently no established guidelines for designing and evaluating\ninteractions in AR as there are in traditional user interfaces. In this work,\nwe aimed to examine the usability of current mobile AR applications and\ninterpreting classic usability heuristics in the context of mobile AR.\nParticularly, we focused on AR home design apps because of their popularity and\nability to incorporate important mobile AR interaction schemas. Our findings\nindicated that it is important for the designers to consider the unfamiliarity\nof AR technology to the vast users and to take technological limitations into\nconsideration when designing mobile AR apps. Our work serves as a first step\nfor establishing more general heuristics and guidelines for mobile AR.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 13:32:18 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Labrie", "Audrey", ""], ["Cheng", "Jinghui", ""]]}, {"id": "2008.03202", "submitter": "Hendrik Heuer", "authors": "Oscar Alvarado, Hendrik Heuer, Vero Vanden Abeele, Andreas Breiter,\n  Katrien Verbert", "title": "Middle-Aged Video Consumers' Beliefs About Algorithmic Recommendations\n  on YouTube", "comments": "To appear in the October 2020 issue of PACM HCI, to be presented at\n  ACM CSCW 2020. The two first authors Oscar Alvarado and Hendrik Heuer\n  contributed equally to this work", "journal-ref": "Proc. ACM Hum.-Comput. Interact. 4, CSCW2, Article 121 (October\n  2020)", "doi": "10.1145/3415192", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User beliefs about algorithmic systems are constantly co-produced through\nuser interaction and the complex socio-technical systems that generate\nrecommendations. Identifying these beliefs is crucial because they influence\nhow users interact with recommendation algorithms. With no prior work on user\nbeliefs of algorithmic video recommendations, practitioners lack relevant\nknowledge to improve the user experience of such systems. To address this\nproblem, we conducted semi-structured interviews with middle-aged YouTube video\nconsumers to analyze their user beliefs about the video recommendation system.\nOur analysis revealed different factors that users believe influence their\nrecommendations. Based on these factors, we identified four groups of user\nbeliefs: Previous Actions, Social Media, Recommender System, and Company\nPolicy. Additionally, we propose a framework to distinguish the four main\nactors that users believe influence their video recommendations: the current\nuser, other users, the algorithm, and the organization. This framework provides\na new lens to explore design suggestions based on the agency of these four\nactors. It also exposes a novel aspect previously unexplored: the effect of\ncorporate decisions on the interaction with algorithmic recommendations. While\nwe found that users are aware of the existence of the recommendation system on\nYouTube, we show that their understanding of this system is limited.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 14:35:50 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Alvarado", "Oscar", ""], ["Heuer", "Hendrik", ""], ["Abeele", "Vero Vanden", ""], ["Breiter", "Andreas", ""], ["Verbert", "Katrien", ""]]}, {"id": "2008.03298", "submitter": "Ivan Gordeev", "authors": "Ivan Gordeev", "title": "FitsGeo -- Python package for PHITS geometry development and\n  visualization", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC hep-ex", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An easy way to define and visualize geometry for PHITS input files\nintroduced. Suggested FitsGeo Python package helps to define surfaces as Python\nobjects and manipulate them conveniently. VPython assists to view defined\ngeometry interactively which boosts geometry development and helps with\ncomplicated cases. Every class that sets the surface object has methods with\nsome extra properties. As well as geometry generation for PHITS input,\nadditional modules developed for material and cell definition. Any user with a\nvery basic knowledge of Python can define the geometry in a convenient way and\nuse it in further research related to particle transport.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 09:54:21 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Gordeev", "Ivan", ""]]}, {"id": "2008.03406", "submitter": "Helen Jiang", "authors": "Helen Jiang", "title": "Security for People with Mental Illness in Telehealth Systems: A\n  Proposal", "comments": "Accepted to 5th Workshop on Inclusive Privacy and Security (WIPS) at\n  16th Symposium on Usable Privacy and Security (SOUPS), August 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mental health crisis is looming large, and needs to be addressed. But\nacross age groups, even just in the United States, more than 50% of people with\nany mental illness (AMI) did not seek or receive any service or treatment. The\nproliferation of telehealth and telepsychiatry tools and systems can help\naddress this crisis, but outside of traditional regulatory aspects on privacy,\ne.g. Health Insurance Portability and Accountability Act (HIPPA), there does\nnot seem to be enough attention on the security needs, concerns, or user\nexperience of people with AMI using those telehealth systems. In this text, I\ntry to explore some priority security properties for telehealth systems used by\npeople with AMI for mental heath services (MHS). I will also suggest some key\nsteps in a proposed process for designing and building security mechanisms into\nsuch systems, so that security is accessible and usable to patients with AMI,\nand these systems can achieve their goals of ameliorate this mental health\ncrisis.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 00:38:45 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Jiang", "Helen", ""]]}, {"id": "2008.03550", "submitter": "Ken (Kezhi) Li", "authors": "Robert Spence, Chukwuma Uduku, Kezhi Li, Nick Oliver, Pantelis\n  Georgiou", "title": "A novel hand-held interface supporting the self-management of Type 1\n  diabetes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes the interaction design of a hand-held interface\nsupporting the self-management of Type 1 diabetes. It addresses\nwell-established clinical and human-computer interaction requirements.\n  The design exploits three opportunities. One is associated with visible\ncontext, whether conspicuous or inconspicuous. A second arises from the design\nfreedom made possible by the user's anticipated focus of attention during\ncertain interactions. A third opportunity to provide valuable functionality\narises from wearable sensors and machine learning algorithms. The resulting\ninterface permits ``What if?'' questions: it allows a user to dynamically and\nmanually explore predicted short-term (e.g., 2 hours) relationships between an\nintended meal, blood glucose level and recommended insulin dosage, and thereby\nreadily make informed food and exercise decisions. Design activity has been\ninformed throughout by focus groups comprising people with Type 1 diabetes in\naddition to experts in diabetes, interaction design and machine learning. The\ndesign is being implemented prior to a clinical trial.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 16:00:46 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Spence", "Robert", ""], ["Uduku", "Chukwuma", ""], ["Li", "Kezhi", ""], ["Oliver", "Nick", ""], ["Georgiou", "Pantelis", ""]]}, {"id": "2008.03717", "submitter": "Antonios Minas Krasakis", "authors": "Antonios Minas Krasakis, Mohammad Aliannejadi, Nikos Voskarides,\n  Evangelos Kanoulas", "title": "Analysing the Effect of Clarifying Questions on Document Ranking in\n  Conversational Search", "comments": "Proceedings of the 2020 ACM SIGIR International Conference on the\n  Theory of Information Retrieval (ICTIR '20), September 14-17, 2020", "journal-ref": null, "doi": "10.1145/3409256.3409817", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on conversational search highlights the importance of\nmixed-initiative in conversations. To enable mixed-initiative, the system\nshould be able to ask clarifying questions to the user. However, the ability of\nthe underlying ranking models (which support conversational search) to account\nfor these clarifying questions and answers has not been analysed when ranking\ndocuments, at large. To this end, we analyse the performance of a lexical\nranking model on a conversational search dataset with clarifying questions. We\ninvestigate, both quantitatively and qualitatively, how different aspects of\nclarifying questions and user answers affect the quality of ranking. We argue\nthat there needs to be some fine-grained treatment of the entire conversational\nround of clarification, based on the explicit feedback which is present in such\nmixed-initiative settings. Informed by our findings, we introduce a simple\nheuristic-based lexical baseline, that significantly outperforms the existing\nnaive baselines. Our work aims to enhance our understanding of the challenges\npresent in this particular task and inform the design of more appropriate\nconversational ranking models.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 12:55:16 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 10:21:13 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Krasakis", "Antonios Minas", ""], ["Aliannejadi", "Mohammad", ""], ["Voskarides", "Nikos", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2008.03880", "submitter": "Boris Ivanovic", "authors": "Boris Ivanovic, Karen Leung, Edward Schmerling, Marco Pavone", "title": "Multimodal Deep Generative Models for Trajectory Prediction: A\n  Conditional Variational Autoencoder Approach", "comments": "8 pages, 3 figures, 2 tables. IEEE Robotics and Automation Letters\n  (RA-L), 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human behavior prediction models enable robots to anticipate how humans may\nreact to their actions, and hence are instrumental to devising safe and\nproactive robot planning algorithms. However, modeling complex interaction\ndynamics and capturing the possibility of many possible outcomes in such\ninteractive settings is very challenging, which has recently prompted the study\nof several different approaches. In this work, we provide a self-contained\ntutorial on a conditional variational autoencoder (CVAE) approach to human\nbehavior prediction which, at its core, can produce a multimodal probability\ndistribution over future human trajectories conditioned on past interactions\nand candidate robot future actions. Specifically, the goals of this tutorial\npaper are to review and build a taxonomy of state-of-the-art methods in human\nbehavior prediction, from physics-based to purely data-driven methods, provide\na rigorous yet easily accessible description of a data-driven, CVAE-based\napproach, highlight important design characteristics that make this an\nattractive model to use in the context of model-based planning for human-robot\ninteractions, and provide important design considerations when using this class\nof models.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 03:18:27 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 00:13:47 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Ivanovic", "Boris", ""], ["Leung", "Karen", ""], ["Schmerling", "Edward", ""], ["Pavone", "Marco", ""]]}, {"id": "2008.03892", "submitter": "Juan Quiroz", "authors": "Juan C. Quiroz, Tristan Bongolan, Kiran Ijaz", "title": "Alexa Depression and Anxiety Self-tests: A Preliminary Analysis of User\n  Experience and Trust", "comments": "Mental Health Sensing & Intervention Workshop - UbiComp 2020", "journal-ref": null, "doi": "10.1145/3410530.3414374", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health resources available via websites and mobile apps provide\nsupport such as advice, journaling, and elements from cognitive behavioral\ntherapy. The proliferation of spoken conversational agents, such as Alexa,\nSiri, and Google Home, has led to an increasing interest in developing mental\nhealth apps for these devices. We present the pilot study outcomes of an Alexa\nSkill that allows users to conduct depression and anxiety self-tests. Ten\nparticipants were given access to the Alexa Skill for two-weeks, followed by an\nonline evaluation of the Skill's usability and trust. Our preliminary\nevaluation suggests that participants trusted the Skill and scored the\nusability and user experience as average. Usage of the Skill was low, with most\nparticipants using the Skill only once. In view of work-in-progress, we also\npresent a discussion of implementation and study design challenges to guide the\ncurrent literature on designing spoken conversational agents for mental health\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 04:12:40 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Quiroz", "Juan C.", ""], ["Bongolan", "Tristan", ""], ["Ijaz", "Kiran", ""]]}, {"id": "2008.03982", "submitter": "Lei Shi", "authors": "Lei Shi, Alexandra Cristea, Ahmad Alamri, Armando M. Toda, Wilk\n  Oliveira", "title": "Social Interactions Clustering MOOC Students: An Exploratory Study", "comments": "The 20th IEEE International Conference on Advanced Learning\n  Technologies (ICALT2020), page 172-174", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  An exploratory study on social interactions of MOOC students in FutureLearn\nwas conducted, to answer \"how can we cluster students based on their social\ninteractions?\" Comments were categorized based on how students interacted with\nthem, e.g., how a student's comment received replies from peers. Statistical\nmodelling and machine learning were used to analyze comment categorization,\nresulting in 3 strong and stable clusters.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:32:38 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Shi", "Lei", ""], ["Cristea", "Alexandra", ""], ["Alamri", "Ahmad", ""], ["Toda", "Armando M.", ""], ["Oliveira", "Wilk", ""]]}, {"id": "2008.04236", "submitter": "Amy X. Zhang", "authors": "Amy X. Zhang, Grant Hugh, Michael S. Bernstein", "title": "PolicyKit: Building Governance in Online Communities", "comments": "to be published in ACM UIST 2020", "journal-ref": null, "doi": "10.1145/3379337.3415858", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The software behind online community platforms encodes a governance model\nthat represents a strikingly narrow set of governance possibilities focused on\nmoderators and administrators. When online communities desire other forms of\ngovernment, such as ones that take many members' opinions into account or that\ndistribute power in non-trivial ways, communities must resort to laborious\nmanual effort. In this paper, we present PolicyKit, a software infrastructure\nthat empowers online community members to concisely author a wide range of\ngovernance procedures and automatically carry out those procedures on their\nhome platforms. We draw on political science theory to encode community\ngovernance into policies, or short imperative functions that specify a\nprocedure for determining whether a user-initiated action can execute. Actions\nthat can be governed by policies encompass everyday activities such as posting\nor moderating a message, but actions can also encompass changes to the policies\nthemselves, enabling the evolution of governance over time. We demonstrate the\nexpressivity of PolicyKit through implementations of governance models such as\na random jury deliberation, a multi-stage caucus, a reputation system, and a\npromotion procedure inspired by Wikipedia's Request for Adminship (RfA)\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 16:17:15 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 18:46:10 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Zhang", "Amy X.", ""], ["Hugh", "Grant", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "2008.04373", "submitter": "Lei Shi", "authors": "Lei Shi, Alexandra I. Cristea, Armando M. Toda, Wilk Oliveira", "title": "Exploring Navigation Styles in a FutureLearn MOOC", "comments": "The 16th International Conference on Intelligent Tutoring Systems\n  (ITS2020)", "journal-ref": null, "doi": "10.1007/978-3-030-49663-0_7", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents for the first time a detailed analysis of fine-grained\nnavigation style identification in MOOCs backed by a large number of active\nlearners. The result shows 1) whilst the sequential style is clearly in\nevidence, the global style is less prominent; 2) the majority of the learners\ndo not belong to either category; 3) navigation styles are not as stable as\nbelieved in the literature; and 4) learners can, and do, swap between\nnavigation styles with detrimental effects. The approach is promising, as it\nprovides insight into online learners' temporal engagement, as well as a tool\nto identify vulnerable learners, which potentially benefit personalised\ninterventions (from teachers or automatic help) in Intelligent Tutoring Systems\n(ITS).\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 19:12:21 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Shi", "Lei", ""], ["Cristea", "Alexandra I.", ""], ["Toda", "Armando M.", ""], ["Oliveira", "Wilk", ""]]}, {"id": "2008.04505", "submitter": "Ke Wang", "authors": "Junlan Chen, Ke Wang, Huanhuan Bao, Tao Chen", "title": "A Design of Cooperative Overtaking Based on Complex Lane Detection and\n  Collision Risk Estimation", "comments": null, "journal-ref": "IEEE Access, 2019, 7: 87951-87959", "doi": "10.1109/ACCESS.2019.2922113", "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative overtaking is believed to have the capability of improving road\nsafety and traffic efficiency by means of the real-time information exchange\nbetween traffic participants, including road infrastructures, nearby vehicles\nand others. In this paper, we focused on the critical issues of modeling,\ncomputation, and analysis of cooperative overtaking and made it playing a key\nrole in the road overtaking area. In detail, for the purpose of extending the\nawareness of the surrounding environment, the lane markings in front of ego\nvehicle were detected and modeled with Bezier curve using an onboard camera.\nWhile the nearby vehicle positions were obtained through the vehicle-to-vehicle\ncommunication scheme making assure of the accuracy of localization. Then,\nGaussian-based conflict potential field was proposed to guarantee the\novertaking safety, which can quantitatively estimate the oncoming collision\ndanger. To support the proposed method, many experiments were conducted on the\nhuman-in-the-loop simulation platform. The results demonstrated that our\nproposed method achieves better performance, especially in some unpredictable\nnature road circumstances.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 04:01:34 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Chen", "Junlan", ""], ["Wang", "Ke", ""], ["Bao", "Huanhuan", ""], ["Chen", "Tao", ""]]}, {"id": "2008.04543", "submitter": "Verena Biener", "authors": "Travis Gesslein, Verena Biener, Philipp Gagel, Daniel Schneider, Per\n  Ola Kristensson, Eyal Ofek, Michel Pahud, Jens Grubert", "title": "Pen-based Interaction with Spreadsheets in Mobile Virtual Reality", "comments": "10 pages, 11 figures, ISMAR 2020", "journal-ref": "In 2020 IEEE International Symposium on Mixed and Augmented\n  Reality (ISMAR) 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) can enhance the display and interaction of mobile\nknowledge work and in particular, spreadsheet applications. While spreadsheets\nare widely used yet are challenging to interact with, especially on mobile\ndevices, using them in VR has not been explored in depth. A special uniqueness\nof the domain is the contrast between the immersive and large display space\nafforded by VR, contrasted by the very limited interaction space that may be\nafforded for the information worker on the go, such as an airplane seat or a\nsmall work-space. To close this gap, we present a tool-set for enhancing\nspreadsheet interaction on tablets using immersive VR headsets and pen-based\ninput. This combination opens up many possibilities for enhancing the\nproductivity for spreadsheet interaction. We propose to use the space around\nand in front of the tablet for enhanced visualization of spreadsheet data and\nmeta-data. For example, extending sheet display beyond the bounds of the\nphysical screen, or easier debugging by uncovering hidden dependencies between\nsheet's cells. Combining the precise on-screen input of a pen with spatial\nsensing around the tablet, we propose tools for the efficient creation and\nediting of spreadsheets functions such as off-the-screen layered menus,\nvisualization of sheets dependencies, and gaze-and-touch-based switching\nbetween spreadsheet tabs. We study the feasibility of the proposed tool-set\nusing a video-based online survey and an expert-based assessment of indicative\nhuman performance potential.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 06:39:35 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Gesslein", "Travis", ""], ["Biener", "Verena", ""], ["Gagel", "Philipp", ""], ["Schneider", "Daniel", ""], ["Kristensson", "Per Ola", ""], ["Ofek", "Eyal", ""], ["Pahud", "Michel", ""], ["Grubert", "Jens", ""]]}, {"id": "2008.04559", "submitter": "Verena Biener", "authors": "Verena Biener, Daniel Schneider, Travis Gesslein, Alexander Otte,\n  Bastian Kuth, Per Ola Kristensson, Eyal Ofek, Michel Pahud, Jens Grubert", "title": "Breaking the Screen: Interaction Across Touchscreen Boundaries in\n  Virtual Reality for Mobile Knowledge Workers", "comments": "10 pages, 8 figures, ISMAR 2020", "journal-ref": "In IEEE transactions on visualization and computer graphics, 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual Reality (VR) has the potential to transform knowledge work. One\nadvantage of VR knowledge work is that it allows extending 2D displays into the\nthird dimension, enabling new operations, such as selecting overlapping objects\nor displaying additional layers of information. On the other hand, mobile\nknowledge workers often work on established mobile devices, such as tablets,\nlimiting interaction with those devices to a small input space. This challenge\nof a constrained input space is intensified in situations when VR knowledge\nwork is situated in cramped environments, such as airplanes and touchdown\nspaces.\n  In this paper, we investigate the feasibility of interacting jointly between\nan immersive VR head-mounted display and a tablet within the context of\nknowledge work. Specifically, we 1) design, implement and study how to interact\nwith information that reaches beyond a single physical touchscreen in VR; 2)\ndesign and evaluate a set of interaction concepts; and 3) build example\napplications and gather user feedback on those applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 07:21:20 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Biener", "Verena", ""], ["Schneider", "Daniel", ""], ["Gesslein", "Travis", ""], ["Otte", "Alexander", ""], ["Kuth", "Bastian", ""], ["Kristensson", "Per Ola", ""], ["Ofek", "Eyal", ""], ["Pahud", "Michel", ""], ["Grubert", "Jens", ""]]}, {"id": "2008.04638", "submitter": "Marco Comunita'", "authors": "Marco Comunit\\`a, Andrea Gerino, Veranika Lim, Lorenzo Picinali", "title": "PlugSonic: a web- and mobile-based platform for binaural audio and sonic\n  narratives", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC cs.MM eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PlugSonic is a suite of web- and mobile-based applications for the curation\nand experience of binaural interactive soundscapes and sonic narratives. It was\ndeveloped as part of the PLUGGY EU project (Pluggable Social Platform for\nHeritage Awareness and Participation) and consists of two main applications:\nPlugSonic Sample, to edit and apply audio effects, and PlugSonic Soundscape, to\ncreate and experience binaural soundscapes. The audio processing within\nPlugSonic is based on the Web Audio API and the 3D Tune-In Toolkit, while the\nexploration of soundscapes in a physical space is obtained using Apple's ARKit.\nIn this paper we present the design choices, the user involvement processes and\nthe implementation details. The main goal of PlugSonic is technology\ndemocratisation; PlugSonic users - whether institutions or citizens - are all\ngiven the instruments needed to create, process and experience 3D soundscapes\nand sonic narrative; without the need for specific devices, external tools\n(software and/or hardware), specialised knowledge or custom development. The\nevaluation, which was conducted with inexperienced users on three tasks -\ncreation, curation and experience - demonstrates how PlugSonic is indeed a\nsimple, effective, yet powerful tool.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 11:42:49 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Comunit\u00e0", "Marco", ""], ["Gerino", "Andrea", ""], ["Lim", "Veranika", ""], ["Picinali", "Lorenzo", ""]]}, {"id": "2008.04698", "submitter": "Jason R.C. Nurse Dr", "authors": "Anjuli R. K. Shere and Jason R. C. Nurse and Ivan Flechais", "title": "Security should be there by default: Investigating how journalists\n  perceive and respond to risks from the Internet of Things", "comments": "5th European Workshop on Usable Security, at 2020 IEEE European\n  Symposium on Security and Privacy (EuroS&P)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Journalists have long been the targets of both physical and cyber-attacks\nfrom well-resourced adversaries. Internet of Things (IoT) devices are arguably\na new avenue of threat towards journalists through both targeted and\ngeneralised cyber-physical exploitation. This study comprises three parts:\nFirst, we interviewed 11 journalists and surveyed 5 further journalists, to\ndetermine the extent to which journalists perceive threats through the IoT,\nparticularly via consumer IoT devices. Second, we surveyed 34 cyber security\nexperts to establish if and how lay-people can combat IoT threats. Third, we\ncompared these findings to assess journalists' knowledge of threats, and\nwhether their protective mechanisms would be effective against experts'\ndepictions and predictions of IoT threats. Our results indicate that\njournalists generally are unaware of IoT-related risks and are not adequately\nprotecting themselves; this considers cases where they possess IoT devices, or\nwhere they enter IoT-enabled environments (e.g., at work or home). Expert\nrecommendations spanned both immediate and long-term mitigation methods,\nincluding practical actions that are technical and socio-political in nature.\nHowever, all proposed individual mitigation methods are likely to be short-term\nsolutions, with 26 of 34 (76.5%) of cyber security experts responding that\nwithin the next five years it will not be possible for the public to opt-out of\ninteraction with the IoT.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:41:22 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Shere", "Anjuli R. K.", ""], ["Nurse", "Jason R. C.", ""], ["Flechais", "Ivan", ""]]}, {"id": "2008.04723", "submitter": "Akinari Onishi", "authors": "Akinari Onishi", "title": "Economical Visual Attention Test for Elderly Drivers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traffic accidents involving elderly drivers are an issue in a super-aging\nsociety. A quick and low-cost aptitude test is required to reduce the number of\ntraffic accidents. This study proposed an oddball-serial visual search task\nthat assesses the individual's performance by his or her responses to the\npresence of cued stimuli on the screen. Task difficulty varied by changing the\nnumber of simultaneous stimuli; Accordingly, low performers were detected. In\naddition, performance correlated with age. This implies that individual\ncharacteristics related to driving performance that decline with age can be\ndetected by the proposed task. Since the task requires low-cost devices\n(computer and response button), it is feasible for use as a quick and low-cost\naptitude test for elderly drivers.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 14:37:51 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Onishi", "Akinari", ""]]}, {"id": "2008.04773", "submitter": "Shamal Faily", "authors": "Shamal Faily, Claudia Iacob, Raian Ali, and Duncan Ki-Aries", "title": "Identifying Implicit Vulnerabilities through Personas as Goal Models", "comments": "SECPRE 2020 workshop pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When used in requirements processes and tools, personas have the potential to\nidentify vulnerabilities resulting from misalignment between user expectations\nand system goals. Typically, however, this potential is unfulfilled as personas\nand system goals are captured with different mindsets, by different teams, and\nfor different purposes. If personas are visualised as goal models, it may be\neasier for stakeholders to see implications of their goals being satisfied or\ndenied, and designers to incorporate the creation and analysis of such models\ninto the broader RE tool-chain. This paper outlines a tool-supported approach\nfor finding implicit vulnerabilities from user and system goals by reframing\npersonas as social goal models. We illustrate this approach with a case study\nwhere previously hidden vulnerabilities based on human behaviour were\nidentified.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 15:23:51 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 03:54:27 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Faily", "Shamal", ""], ["Iacob", "Claudia", ""], ["Ali", "Raian", ""], ["Ki-Aries", "Duncan", ""]]}, {"id": "2008.04811", "submitter": "Lei Shi", "authors": "Lei Shi, Alexandra I. Cristea, Armando M. Toda, Wilk Oliveira", "title": "Social Engagement versus Learning Engagement -- An Exploratory Study of\n  FutureLearn Learners", "comments": "The 14th IEEE International Conference on Intelligent Systems and\n  Knowledge Engineering (ISKE 2019), pages 542-549", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Massive Open Online Courses (MOOCs) continue to see increasing enrolment, but\nonly a small percent of enrolees completes the MOOCs. Whilst a lot of research\nhas focused on predicting completion, there is little research analysing the\nostensible contradiction between the MOOC's popularity and the apparent\ndisengagement of learners. Specifically, it is important to analyse engagement\nnot just in learning, but also from a social perspective. This is especially\ncrucial, as MOOCs offer a growing amount of activities, which can be classified\nas social interactions. Thus, this study is particularly concerned with how\nlearners interact with peers, along with their study progression in MOOCs.\nAdditionally, unlike most existing studies that are mainly focused on learning\noutcomes, this study adopts a fine-grained temporal approach to exploring how\nlearners progress within a MOOC. The study was conducted on the less explored\nFutureLearn platform, which employs a social constructivist approach and\npromotes collaborative learning. The preliminary results suggest potential\ninteresting fine-grained predictive models for learner behaviour, involving\nweekly monitoring of social, non-social behaviour of active students (further\nclassified as completers and non-completers).\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:09:10 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Shi", "Lei", ""], ["Cristea", "Alexandra I.", ""], ["Toda", "Armando M.", ""], ["Oliveira", "Wilk", ""]]}, {"id": "2008.04855", "submitter": "Praveen Damacharla", "authors": "Praveen Damacharla, Ahmad Y. Javaid, Jennie J. Gallimore, Vijay K.\n  Devabhaktuni", "title": "Common Metrics to Benchmark Human-Machine Teams (HMT): A Review", "comments": null, "journal-ref": "in IEEE Access, vol. 6, pp. 38637-38655, 2018", "doi": "10.1109/ACCESS.2018.2853560", "report-no": null, "categories": "cs.CY cs.HC cs.LG cs.RO cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A significant amount of work is invested in human-machine teaming (HMT)\nacross multiple fields. Accurately and effectively measuring system performance\nof an HMT is crucial for moving the design of these systems forward. Metrics\nare the enabling tools to devise a benchmark in any system and serve as an\nevaluation platform for assessing the performance, along with the verification\nand validation, of a system. Currently, there is no agreed-upon set of\nbenchmark metrics for developing HMT systems. Therefore, identification and\nclassification of common metrics are imperative to create a benchmark in the\nHMT field. The key focus of this review is to conduct a detailed survey aimed\nat identification of metrics employed in different segments of HMT and to\ndetermine the common metrics that can be used in the future to benchmark HMTs.\nWe have organized this review as follows: identification of metrics used in\nHMTs until now, and classification based on functionality and measuring\ntechniques. Additionally, we have also attempted to analyze all the identified\nmetrics in detail while classifying them as theoretical, applied, real-time,\nnon-real-time, measurable, and observable metrics. We conclude this review with\na detailed analysis of the identified common metrics along with their usage to\nbenchmark HMTs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:57:52 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Damacharla", "Praveen", ""], ["Javaid", "Ahmad Y.", ""], ["Gallimore", "Jennie J.", ""], ["Devabhaktuni", "Vijay K.", ""]]}, {"id": "2008.04873", "submitter": "Iretiayo Akinola", "authors": "Zizhao Wang, Junyao Shi, Iretiayo Akinola and Peter Allen", "title": "Maximizing BCI Human Feedback using Active Learning", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in \\textit{Learning from Human Feedback} present an\neffective way to train robot agents via inputs from non-expert humans, without\na need for a specially designed reward function. However, this approach needs a\nhuman to be present and attentive during robot learning to provide evaluative\nfeedback. In addition, the amount of feedback needed grows with the level of\ntask difficulty and the quality of human feedback might decrease over time\nbecause of fatigue. To overcome these limitations and enable learning more\nrobot tasks with higher complexities, there is a need to maximize the quality\nof expensive feedback received and reduce the amount of human cognitive\ninvolvement required. In this work, we present an approach that uses active\nlearning to smartly choose queries for the human supervisor based on the\nuncertainty of the robot and effectively reduces the amount of feedback needed\nto learn a given task. We also use a novel multiple buffer system to improve\nrobustness to feedback noise and guard against catastrophic forgetting as the\nrobot learning evolves. This makes it possible to learn tasks with more\ncomplexity using lesser amounts of human feedback compared to previous methods.\nWe demonstrate the utility of our proposed method on a robot arm reaching task\nwhere the robot learns to reach a location in 3D without colliding with\nobstacles. Our approach is able to learn this task faster, with less human\nfeedback and cognitive involvement, compared to previous methods that do not\nuse active learning.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 17:26:18 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Wang", "Zizhao", ""], ["Shi", "Junyao", ""], ["Akinola", "Iretiayo", ""], ["Allen", "Peter", ""]]}, {"id": "2008.05054", "submitter": "Hee-Seung Moon", "authors": "Hee-Seung Moon and Jiwon Seo", "title": "Sample-Efficient Training of Robotic Guide Using Human Path Prediction\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a robot that engages with people is challenging, because it is\nexpensive to involve people in a robot training process requiring numerous data\nsamples. This paper proposes a human path prediction network (HPPN) and an\nevolution strategy-based robot training method using virtual human movements\ngenerated by the HPPN, which compensates for this sample inefficiency problem.\nWe applied the proposed method to the training of a robotic guide for visually\nimpaired people, which was designed to collect multimodal human response data\nand reflect such data when selecting the robot's actions. We collected 1,507\nreal-world episodes for training the HPPN and then generated over 100,000\nvirtual episodes for training the robot policy. User test results indicate that\nour trained robot accurately guides blindfolded participants along a goal path.\nIn addition, by the designed reward to pursue both guidance accuracy and human\ncomfort during the robot policy training process, our robot leads to improved\nsmoothness in human motion while maintaining the accuracy of the guidance. This\nsample-efficient training method is expected to be widely applicable to all\nrobots and computing machinery that physically interact with humans.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 01:15:38 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Moon", "Hee-Seung", ""], ["Seo", "Jiwon", ""]]}, {"id": "2008.05064", "submitter": "Praveen Damacharla", "authors": "Praveen Damacharla, Parashar Dhakal, Sebastian Stumbo, Ahmad Y.\n  Javaid, Subhashini Ganapathy, David A. Malek, Douglas C. Hodge, Vijay\n  Devabhaktuni", "title": "Effects of Voice-Based Synthetic Assistant on Performance of Emergency\n  Care Provider in Training", "comments": null, "journal-ref": "Int J Artif Intell Educ, 29, 122-143, 2018", "doi": "10.1007/s40593-018-0166-3", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As part of a perennial project, our team is actively engaged in developing\nnew synthetic assistant (SA) technologies to assist in training combat medics\nand medical first responders. It is critical that medical first responders are\nwell trained to deal with emergencies more effectively. This would require\nreal-time monitoring and feedback for each trainee. Therefore, we introduced a\nvoice-based SA to augment the training process of medical first responders and\nenhance their performance in the field. The potential benefits of SAs include a\nreduction in training costs and enhanced monitoring mechanisms. Despite the\nincreased usage of voice-based personal assistants (PAs) in day-to-day life,\nthe associated effects are commonly neglected for a study of human factors.\nTherefore, this paper focuses on performance analysis of the developed\nvoice-based SA in emergency care provider training for a selected emergency\ntreatment scenario. The research discussed in this paper follows design science\nin developing proposed technology; at length, we discussed architecture and\ndevelopment and presented working results of voice-based SA. The empirical\ntesting was conducted on two groups as user studies using statistical analysis\ntools, one trained with conventional methods and the other with the help of SA.\nThe statistical results demonstrated the amplification in training efficacy and\nperformance of medical responders powered by SA. Furthermore, the paper also\ndiscusses the accuracy and time of task execution (t) and concludes with the\nguidelines for resolving the identified problems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 01:55:28 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Damacharla", "Praveen", ""], ["Dhakal", "Parashar", ""], ["Stumbo", "Sebastian", ""], ["Javaid", "Ahmad Y.", ""], ["Ganapathy", "Subhashini", ""], ["Malek", "David A.", ""], ["Hodge", "Douglas C.", ""], ["Devabhaktuni", "Vijay", ""]]}, {"id": "2008.05132", "submitter": "Jieshan Chen", "authors": "Jieshan Chen, Mulong Xie, Zhenchang Xing, Chunyang Chen, Xiwei Xu,\n  Liming Zhu and Guoqiang Li", "title": "Object Detection for Graphical User Interface: Old Fashioned or Deep\n  Learning or a Combination?", "comments": "13 pages, accepted to ESEC/FSE '20", "journal-ref": null, "doi": "10.1145/3368089.3409691", "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting Graphical User Interface (GUI) elements in GUI images is a\ndomain-specific object detection task. It supports many software engineering\ntasks, such as GUI animation and testing, GUI search and code generation.\nExisting studies for GUI element detection directly borrow the mature methods\nfrom computer vision (CV) domain, including old fashioned ones that rely on\ntraditional image processing features (e.g., canny edge, contours), and deep\nlearning models that learn to detect from large-scale GUI data. Unfortunately,\nthese CV methods are not originally designed with the awareness of the unique\ncharacteristics of GUIs and GUI elements and the high localization accuracy of\nthe GUI element detection task. We conduct the first large-scale empirical\nstudy of seven representative GUI element detection methods on over 50k GUI\nimages to understand the capabilities, limitations and effective designs of\nthese methods. This study not only sheds the light on the technical challenges\nto be addressed but also informs the design of new GUI element detection\nmethods. We accordingly design a new GUI-specific old-fashioned method for\nnon-text GUI element detection which adopts a novel top-down coarse-to-fine\nstrategy, and incorporate it with the mature deep learning model for GUI text\ndetection.Our evaluation on 25,000 GUI images shows that our method\nsignificantly advances the start-of-the-art performance in GUI element\ndetection.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 06:36:33 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 12:57:33 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Chen", "Jieshan", ""], ["Xie", "Mulong", ""], ["Xing", "Zhenchang", ""], ["Chen", "Chunyang", ""], ["Xu", "Xiwei", ""], ["Zhu", "Liming", ""], ["Li", "Guoqiang", ""]]}, {"id": "2008.05209", "submitter": "Lei Shi", "authors": "Ahmed Alamri, Zhongtian Sun, Alexandra I. Cristea, Gautham\n  Senthilnathan, Lei Shi, Craig Stewart", "title": "Is MOOC Learning Different for Dropouts? A Visually-Driven,\n  Multi-granularity Explanatory ML Approach", "comments": "Intelligent Tutoring Systems. ITS 2020. Lecture Notes in Computer\n  Science, vol 12149", "journal-ref": null, "doi": "10.1007/978-3-030-49663-0_42", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Millions of people have enrolled and enrol (especially in the Covid-19\npandemic world) in MOOCs. However, the retention rate of learners is\nnotoriously low. The majority of the research work on this issue focuses on\npredicting the dropout rate, but very few use explainable learning patterns as\npart of this analysis. However, visual representation of learning patterns\ncould provide deeper insights into learners' behaviour across different\ncourses, whilst numerical analyses can -- and arguably, should -- be used to\nconfirm the latter. Thus, this paper proposes and compares different\ngranularity visualisations for learning patterns (based on clickstream data)\nfor both course completers and non-completers. In the large-scale MOOCs we\nanalysed, across various domains, our fine-grained, fish-eye visualisation\napproach showed that non-completers are more likely to jump forward in their\nlearning sessions, often on a 'catch-up' path, whilst completers exhibit linear\nbehaviour. For coarser, bird-eye granularity visualisation, we observed\nlearners' transition between types of learning activity, obtaining typed\ntransition graphs. The results, backed up by statistical significance analysis\nand machine learning, provide insights for course instructors to maintain\nengagement of learners by adapting the course design to not just 'dry'\npredicted values, but explainable, visually viable paths extracted.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 09:59:18 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Alamri", "Ahmed", ""], ["Sun", "Zhongtian", ""], ["Cristea", "Alexandra I.", ""], ["Senthilnathan", "Gautham", ""], ["Shi", "Lei", ""], ["Stewart", "Craig", ""]]}, {"id": "2008.05228", "submitter": "Jugoslav Stojcheski", "authors": "Jugoslav Stojcheski, Valkyrie Felso, Falk Lieder", "title": "Optimal to-do list gamification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What should I work on first? What can wait until later? Which projects should\nI prioritize and which tasks are not worth my time? These are challenging\nquestions that many people face every day. People's intuitive strategy is to\nprioritize their immediate experience over the long-term consequences. This\nleads to procrastination and the neglect of important long-term projects in\nfavor of seemingly urgent tasks that are less important. Optimal gamification\nstrives to help people overcome these problems by incentivizing each task by a\nnumber of points that communicates how valuable it is in the long-run.\nUnfortunately, computing the optimal number of points with standard dynamic\nprogramming methods quickly becomes intractable as the number of a person's\nprojects and the number of tasks required by each project increase. Here, we\nintroduce and evaluate a scalable method for identifying which tasks are most\nimportant in the long run and incentivizing each task according to its\nlong-term value. Our method makes it possible to create to-do list gamification\napps that can handle the size and complexity of people's to-do lists in the\nreal world.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:59:13 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Stojcheski", "Jugoslav", ""], ["Felso", "Valkyrie", ""], ["Lieder", "Falk", ""]]}, {"id": "2008.05305", "submitter": "Maryam Alimardani", "authors": "Maryam Alimardani, Linda Kemmeren, Kazuki Okumura, Kazuo Hiraki", "title": "Robot-Assisted Mindfulness Practice: Analysis of Neurophysiological\n  Responses and Affective State Change", "comments": "accepted for conference RoMAN2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mindfulness is the state of paying attention to the present moment on purpose\nand meditation is the technique to obtain this state. This study aims to\ndevelop a robot assistant that facilitates mindfulness training by means of a\nBrain Computer Interface (BCI) system. To achieve this goal, we collected EEG\nsignals from two groups of subjects engaging in a meditative vs. nonmeditative\nhuman robot interaction (HRI) and evaluated cerebral hemispheric asymmetry,\nwhich is recognized as a well defined indicator of emotional states. Moreover,\nusing self reported affective states, we strived to explain asymmetry changes\nbased on pre and post experiment mood alterations. We found that unlike earlier\nmeditation studies, the frontocentral activations in alpha and theta frequency\nbands were not influenced by robot guided mindfulness practice, however there\nwas a significantly greater right sided activity in the occipital gamma band of\nMeditation group, which is attributed to increased sensory awareness and open\nmonitoring. In addition, there was a significant main effect of Time on\nparticipants self reported affect, indicating an improved mood after\ninteraction with the robot regardless of the interaction type. Our results\nsuggest that EEG responses during robot-guided meditation hold promise in\nrealtime detection and neurofeedback of mindful state to the user, however the\nexperienced neurophysiological changes may differ based on the meditation\npractice and recruited tools. This study is the first to report EEG changes\nduring mindfulness practice with a robot. We believe that our findings driven\nfrom an ecologically valid setting, can be used in development of future BCI\nsystems that are integrated with social robots for health applications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 13:29:15 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Alimardani", "Maryam", ""], ["Kemmeren", "Linda", ""], ["Okumura", "Kazuki", ""], ["Hiraki", "Kazuo", ""]]}, {"id": "2008.05370", "submitter": "Shyam Tailor", "authors": "Shyam A. Tailor and Jagmohan Chauhan and Cecilia Mascolo", "title": "A First Step Towards On-Device Monitoring of Body Sounds in the Wild", "comments": "4 page version to appear at the WellComp Workshop at Ubicomp 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Body sounds provide rich information about the state of the human body and\ncan be useful in many medical applications. Auscultation, the practice of\nlistening to body sounds, has been used for centuries in respiratory and\ncardiac medicine to diagnose or track disease progression. To date, however,\nits use has been confined to clinical and highly controlled settings. Our work\naddresses this limitation: we devise a chest-mounted wearable for continuous\nmonitoring of body sounds, that leverages data processing algorithms that run\non-device. We concentrate on the detection of heart sounds to perform heart\nrate monitoring. To improve robustness to ambient noise and motion artefacts,\nour device uses an algorithm that explicitly segments the collected audio into\nthe phases of the cardiac cycle. Our pilot study with 9 users demonstrates that\nit is possible to obtain heart rate estimates that are competitive with\ncommercial heart rate monitors, with low enough power consumption for\ncontinuous use.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 15:10:58 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Tailor", "Shyam A.", ""], ["Chauhan", "Jagmohan", ""], ["Mascolo", "Cecilia", ""]]}, {"id": "2008.05399", "submitter": "Zhiyun Ren", "authors": "Ziwei Fan, Evan Burgun, Zhiyun Ren, Titus Schleyer, Xia Ning", "title": "Improving information retrieval from electronic health records using\n  dynamic and multi-collaborative filtering", "comments": "18 pages, 11 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the rapid growth of information available about individual patients,\nmost physicians suffer from information overload when they review patient\ninformation in health information technology systems. In this manuscript, we\npresent a novel hybrid dynamic and multi-collaborative filtering method to\nimprove information retrieval from electronic health records. This method\nrecommends relevant information from electronic health records for physicians\nduring patient visits. It models information search dynamics using a Markov\nmodel. It also leverages the key idea of collaborative filtering, originating\nfrom Recommender Systems, to prioritize information based on various\nsimilarities among physicians, patients and information items. We tested this\nnew method using real electronic health record data from the Indiana Network\nfor Patient Care. Our experimental results demonstrated that for 46.7% of\ntesting cases, this new method is able to correctly prioritize relevant\ninformation among top-5 recommendations that physicians are truly interested\nin.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 15:46:33 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Fan", "Ziwei", ""], ["Burgun", "Evan", ""], ["Ren", "Zhiyun", ""], ["Schleyer", "Titus", ""], ["Ning", "Xia", ""]]}, {"id": "2008.05473", "submitter": "Lei Shi", "authors": "Armando M. Toda, Ana C. T. Klock, Wilk Oliveira, Paula T. Palomino,\n  Luiz Rodrigues, Lei Shi, Ig Bittencourt, Isabela Gasparini, Seiji Isotani,\n  Alexandra I. Cristea", "title": "Analysing gamification elements in educational environments using an\n  existing Gamification taxonomy", "comments": "Smart Learn. Environ. 6, 16 (2019)", "journal-ref": null, "doi": "10.1186/s40561-019-0106-1", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gamification has been widely employed in the educational domain over the past\neight years when the term became a trend. However, the literature states that\ngamification still lacks formal definitions to support the design and analysis\nof gamified strategies. This paper analysed the game elements employed in\ngamified learning environments through a previously proposed and evaluated\ntaxonomy while detailing and expanding this taxonomy. In the current paper, we\ndescribe our taxonomy in-depth as well as expand it. Our new structured results\ndemonstrate an extension of the proposed taxonomy which results from this\nprocess, is divided into five dimensions, related to the learner and the\nlearning environment. Our main contribution is the detailed taxonomy that can\nbe used to design and evaluate gamification design in learning environments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:21:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Toda", "Armando M.", ""], ["Klock", "Ana C. T.", ""], ["Oliveira", "Wilk", ""], ["Palomino", "Paula T.", ""], ["Rodrigues", "Luiz", ""], ["Shi", "Lei", ""], ["Bittencourt", "Ig", ""], ["Gasparini", "Isabela", ""], ["Isotani", "Seiji", ""], ["Cristea", "Alexandra I.", ""]]}, {"id": "2008.05505", "submitter": "Tiffany D. Do", "authors": "Tiffany D. Do, Joseph J. LaViola Jr., Ryan P. McMahan", "title": "The Effects of Object Shape, Fidelity, Color, and Luminance on Depth\n  Perception in Handheld Mobile Augmented Reality", "comments": "9 pages, In proceedings of IEEE International Symposium on Mixed and\n  Augmented Reality (ISMAR) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth perception of objects can greatly affect a user's experience of an\naugmented reality (AR) application. Many AR applications require depth matching\nof real and virtual objects and have the possibility to be influenced by depth\ncues. Color and luminance are depth cues that have been traditionally studied\nin two-dimensional (2D) objects. However, there is little research\ninvestigating how the properties of three-dimensional (3D) virtual objects\ninteract with color and luminance to affect depth perception, despite the\nsubstantial use of 3D objects in visual applications. In this paper, we present\nthe results of a paired comparison experiment that investigates the effects of\nobject shape, fidelity, color, and luminance on depth perception of 3D objects\nin handheld mobile AR. The results of our study indicate that bright colors are\nperceived as nearer than dark colors for a high-fidelity, simple 3D object,\nregardless of hue. Additionally, bright red is perceived as nearer than any\nother color. These effects were not observed for a low-fidelity version of the\nsimple object or for a more-complex 3D object. High-fidelity objects had more\nperceptual differences than low-fidelity objects, indicating that fidelity\ninteracts with color and luminance to affect depth perception. These findings\nreveal how the properties of 3D models influence the effects of color and\nluminance on depth perception in handheld mobile AR and can help developers\nselect colors for their applications.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 18:12:05 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Do", "Tiffany D.", ""], ["LaViola", "Joseph J.", "Jr."], ["McMahan", "Ryan P.", ""]]}, {"id": "2008.05691", "submitter": "Kelly Mack", "authors": "Kelly Mack, Danielle Bragg, Meredith Ringel Morris, Maarten W. Bos,\n  Isabelle Albi, Andr\\'es Monroy-Hern\\'andez", "title": "Social App Accessibility for Deaf Signers", "comments": null, "journal-ref": null, "doi": "10.1145/3415196", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media platforms support the sharing of written text, video, and audio.\nAll of these formats may be inaccessible to people who are deaf or hard of\nhearing (DHH), particularly those who primarily communicate via sign language,\npeople who we call Deaf signers. We study how Deaf signers engage with social\nplatforms, focusing on how they share content and the barriers they face. We\nemploy a mixed-methods approach involving seven in-depth interviews and a\nsurvey of a larger population (n = 60). We find that Deaf signers share the\nmost in written English, despite their desire to share in sign language. We\nfurther identify key areas of difficulty in consuming content (e.g., lack of\ncaptions for spoken content in videos) and producing content (e.g., captioning\nsigned videos, signing into a phone camera) on social media platforms. Our\nresults both provide novel insights into social media use by Deaf signers and\nreinforce prior findings on DHH communication more generally, while revealing\npotential ways to make social media platforms more accessible to Deaf signers.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 05:00:03 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 01:37:19 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Mack", "Kelly", ""], ["Bragg", "Danielle", ""], ["Morris", "Meredith Ringel", ""], ["Bos", "Maarten W.", ""], ["Albi", "Isabelle", ""], ["Monroy-Hern\u00e1ndez", "Andr\u00e9s", ""]]}, {"id": "2008.05799", "submitter": "Marcus Scheunemann", "authors": "Marcus M. Scheunemann and Raymond H. Cuijpers and Christoph Salge", "title": "Warmth and Competence to Predict Human Preference of Robot Behavior in\n  Physical Human-Robot Interaction", "comments": "8 pages, 4 figures, 29th IEEE International Conference on Robot &\n  Human Interactive Communication (RO-MAN) 2020", "journal-ref": null, "doi": "10.1109/RO-MAN47096.2020.9223478", "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.IT cs.RO math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A solid methodology to understand human perception and preferences in\nhuman-robot interaction (HRI) is crucial in designing real-world HRI. Social\ncognition posits that the dimensions Warmth and Competence are central and\nuniversal dimensions characterizing other humans. The Robotic Social Attribute\nScale (RoSAS) proposes items for those dimensions suitable for HRI and\nvalidated them in a visual observation study. In this paper we complement the\nvalidation by showing the usability of these dimensions in a behavior based,\nphysical HRI study with a fully autonomous robot. We compare the findings with\nthe popular Godspeed dimensions Animacy, Anthropomorphism, Likeability,\nPerceived Intelligence and Perceived Safety. We found that Warmth and\nCompetence, among all RoSAS and Godspeed dimensions, are the most important\npredictors for human preferences between different robot behaviors. This\npredictive power holds even when there is no clear consensus preference or\nsignificant factor difference between conditions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 10:19:47 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Scheunemann", "Marcus M.", ""], ["Cuijpers", "Raymond H.", ""], ["Salge", "Christoph", ""]]}, {"id": "2008.05847", "submitter": "Lei Shi", "authors": "Armando Toda, Paula Palomino, Luiz Rodrigues, Wilk Oliveira, Lei Shi,\n  Seiji Isotani, Alexandra Cristea", "title": "Validating the Effectiveness of Data-Driven Gamification\n  Recommendations: An Exploratory Study", "comments": null, "journal-ref": null, "doi": "10.5753/cbie.sbie.2019.763", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Gamification design has benefited from data-driven approaches to creating\nstrategies based on students characteristics. However, these strategies need\nfurther validation to verify their effectiveness in e-learning environments.\nThe exploratory study presented in this paper thus aims at verifying how\ndata-driven gamified strategies are perceived by the students, i.e., the users\nof e-learning environments. In this study, we conducted a survey presenting 25\npredefined strategies, based on a previous study, to students and analysed each\nstrategys perceived relevance, instanced in an e-learning environment. Our\nresults show that students perceive Acknowledgement, Objective and Progression\nas important elements in a gamified e-learning environment. We also provide new\ninsights about existing elements and design recommendations for domain\nspecialists.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:25:03 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Toda", "Armando", ""], ["Palomino", "Paula", ""], ["Rodrigues", "Luiz", ""], ["Oliveira", "Wilk", ""], ["Shi", "Lei", ""], ["Isotani", "Seiji", ""], ["Cristea", "Alexandra", ""]]}, {"id": "2008.05849", "submitter": "Lei Shi", "authors": "Ahmed Alamri, Mohammad Alshehri, Alexandra I. Cristea, Filipe D.\n  Pereira, Elaine Oliveira, Lei Shi, Craig Stewart", "title": "Predicting MOOCs Dropout Using Only Two Easily Obtainable Features from\n  the First Week's Activities", "comments": "Intelligent Tutoring Systems. ITS 2019. Lecture Notes in Computer\n  Science, vol 11528. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-030-22244-4_20", "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While Massive Open Online Course (MOOCs) platforms provide knowledge in a new\nand unique way, the very high number of dropouts is a significant drawback.\nSeveral features are considered to contribute towards learner attrition or lack\nof interest, which may lead to disengagement or total dropout. The jury is\nstill out on which factors are the most appropriate predictors. However, the\nliterature agrees that early prediction is vital to allow for a timely\nintervention. Whilst feature-rich predictors may have the best chance for high\naccuracy, they may be unwieldy. This study aims to predict learner dropout\nearly-on, from the first week, by comparing several machine-learning\napproaches, including Random Forest, Adaptive Boost, XGBoost and GradientBoost\nClassifiers. The results show promising accuracies (82%-94%) using as little as\n2 features. We show that the accuracies obtained outperform state of the art\napproaches, even when the latter deploy several features.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:44:49 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Alamri", "Ahmed", ""], ["Alshehri", "Mohammad", ""], ["Cristea", "Alexandra I.", ""], ["Pereira", "Filipe D.", ""], ["Oliveira", "Elaine", ""], ["Shi", "Lei", ""], ["Stewart", "Craig", ""]]}, {"id": "2008.05850", "submitter": "Lei Shi", "authors": "Lei Shi, Alexandra I. Cristea, Armando M. Toda, Wilk Oliveira", "title": "Revealing the Hidden Patterns: A Comparative Study on Profiling\n  Subpopulations of MOOC Students", "comments": "Information Systems Development: Information Systems Beyond 2020\n  (ISD2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Massive Open Online Courses (MOOCs) exhibit a remarkable heterogeneity of\nstudents. The advent of complex \"big data\" from MOOC platforms is a challenging\nyet rewarding opportunity to deeply understand how students are engaged in\nMOOCs. Past research, looking mainly into overall behavior, may have missed\npatterns related to student diversity. Using a large dataset from a MOOC\noffered by FutureLearn, we delve into a new way of investigating hidden\npatterns through both machine learning and statistical modelling. In this\npaper, we report on clustering analysis of student activities and comparative\nanalysis on both behavioral patterns and demographical patterns between student\nsubpopulations in the MOOC. Our approach allows for a deeper understanding of\nhow MOOC students behave and achieve. Our findings may be used to design\nadaptive strategies towards an enhanced MOOC experience\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:38:50 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Shi", "Lei", ""], ["Cristea", "Alexandra I.", ""], ["Toda", "Armando M.", ""], ["Oliveira", "Wilk", ""]]}, {"id": "2008.05855", "submitter": "Oded Nov", "authors": "Oded Nov, Yindalon Aphinyanaphongs, Yvonne W. Lui, Devin Mann,\n  Maurizio Porfiri, Mark Riedl, John-Ross Rizzo, Batia Wiesenfeld", "title": "The Transformation of Patient-Clinician Relationships With AI-Based\n  Medical Advice: A \"Bring Your Own Algorithm\" Era in Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the dramatic trends at the intersection of computing and healthcare\nhas been patients' increased access to medical information, ranging from\nself-tracked physiological data to genetic data, tests, and scans. Increasingly\nhowever, patients and clinicians have access to advanced machine learning-based\ntools for diagnosis, prediction, and recommendation based on large amounts of\ndata, some of it patient-generated. Consequently, just as organizations have\nhad to deal with a \"Bring Your Own Device\" (BYOD) reality in which employees\nuse their personal devices (phones and tablets) for some aspects of their work,\na similar reality of \"Bring Your Own Algorithm\" (BYOA) is emerging in\nhealthcare with its own challenges and support demands. BYOA is changing\npatient-clinician interactions and the technologies, skills and workflows\nrelated to them. In this paper we argue that: (1) BYOA is changing the\npatient-clinician relationship and the nature of expert work in healthcare, and\n(2) better patient-clinician-information-interpretation relationships can be\nfacilitated with solutions that integrate technological and organizational\nperspectives.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 12:22:16 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Nov", "Oded", ""], ["Aphinyanaphongs", "Yindalon", ""], ["Lui", "Yvonne W.", ""], ["Mann", "Devin", ""], ["Porfiri", "Maurizio", ""], ["Riedl", "Mark", ""], ["Rizzo", "John-Ross", ""], ["Wiesenfeld", "Batia", ""]]}, {"id": "2008.05872", "submitter": "Peter Kan", "authors": "Anna Sebernegg, Peter K\\'an, Hannes Kaufmann", "title": "Motion Similarity Modeling -- A State of the Art Report", "comments": null, "journal-ref": null, "doi": null, "report-no": "VR-TR-001", "categories": "cs.GR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of human motion opens up a wide range of possibilities, such as\nrealistic training simulations or authentic motions in robotics or animation.\nOne of the problems underlying motion analysis is the meaningful comparison of\nactions based on similarity measures. Since the motion analysis is\napplication-dependent, it is essential to find the appropriate motion\nsimilarity method for the particular use case. This state of the art report\nprovides an overview of human motion analysis and different similarity modeling\nmethods, while mainly focusing on approaches that work with 3D motion data. The\nsurvey summarizes various similarity aspects and features of motion and\ndescribes approaches to measuring the similarity between two actions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 13:08:30 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Sebernegg", "Anna", ""], ["K\u00e1n", "Peter", ""], ["Kaufmann", "Hannes", ""]]}, {"id": "2008.05914", "submitter": "Janet Rafner", "authors": "Janet Rafner, Arthur Hjorth, Sebastian Risi, Lotte Philipsen, Charles\n  Dumas, Michael Mose Biskj{\\ae}r, Lior Noy, Kristian Tyl\\'en, Carsten\n  Bergenholtz, Jesse Lynch, Blanka Zana, Jacob Sherson", "title": "crea.blender: A Neural Network-Based Image Generation Game to Assess\n  Creativity", "comments": "4 page, 6 figures, CHI Play", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a pilot study on crea.blender, a novel co-creative game designed\nfor large-scale, systematic assessment of distinct constructs of human\ncreativity. Co-creative systems are systems in which humans and computers\n(often with Machine Learning) collaborate on a creative task. This\nhuman-computer collaboration raises questions about the relevance and level of\nhuman creativity and involvement in the process. We expand on, and explore\naspects of these questions in this pilot study. We observe participants play\nthrough three different play modes in crea.blender, each aligned with\nestablished creativity assessment methods. In these modes, players \"blend\"\nexisting images into new images under varying constraints. Our study indicates\nthat crea.blender provides a playful experience, affords players a sense of\ncontrol over the interface, and elicits different types of player behavior,\nsupporting further study of the tool for use in a scalable, playful, creativity\nassessment.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 13:52:04 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 09:12:32 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Rafner", "Janet", ""], ["Hjorth", "Arthur", ""], ["Risi", "Sebastian", ""], ["Philipsen", "Lotte", ""], ["Dumas", "Charles", ""], ["Biskj\u00e6r", "Michael Mose", ""], ["Noy", "Lior", ""], ["Tyl\u00e9n", "Kristian", ""], ["Bergenholtz", "Carsten", ""], ["Lynch", "Jesse", ""], ["Zana", "Blanka", ""], ["Sherson", "Jacob", ""]]}, {"id": "2008.05959", "submitter": "Philippe Esling", "authors": "Philippe Esling, Ninon Devis", "title": "Creativity in the era of artificial intelligence", "comments": "Keynote paper - JIM Conference 2020 - 12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Creativity is a deeply debated topic, as this concept is arguably\nquintessential to our humanity. Across different epochs, it has been infused\nwith an extensive variety of meanings relevant to that era. Along these, the\nevolution of technology have provided a plurality of novel tools for creative\npurposes. Recently, the advent of Artificial Intelligence (AI), through deep\nlearning approaches, have seen proficient successes across various\napplications. The use of such technologies for creativity appear in a natural\ncontinuity to the artistic trend of this century. However, the aura of a\ntechnological artefact labeled as intelligent has unleashed passionate and\nsomewhat unhinged debates on its implication for creative endeavors. In this\npaper, we aim to provide a new perspective on the question of creativity at the\nera of AI, by blurring the frontier between social and computational sciences.\nTo do so, we rely on reflections from social science studies of creativity to\nview how current AI would be considered through this lens. As creativity is a\nhighly context-prone concept, we underline the limits and deficiencies of\ncurrent AI, requiring to move towards artificial creativity. We argue that the\nobjective of trying to purely mimic human creative traits towards a\nself-contained ex-nihilo generative machine would be highly counterproductive,\nputting us at risk of not harnessing the almost unlimited possibilities offered\nby the sheer computational power of artificial agents.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:07:34 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Esling", "Philippe", ""], ["Devis", "Ninon", ""]]}, {"id": "2008.06030", "submitter": "Nicolas Rougier", "authors": "Nicolas P. Rougier", "title": "On the design of text editors", "comments": "5 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text editors are written by and for developers. They come with a large set of\ndefault and implicit choices in terms of layout, typography, colorization and\ninteraction that hardly change from one editor to the other. It is not clear if\nthese implicit choices derive from the ignorance of alternatives or if they\nderive from developers' habits, reproducing what they are used to. The goal of\nthis article is to characterize these implicit choices and to illustrate what\nare some alternatives without prescribing one or the other.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:40:48 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 09:51:05 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Rougier", "Nicolas P.", ""]]}, {"id": "2008.06099", "submitter": "David Baum", "authors": "David Baum, Stefan Bechert, Ulrich Eisenecker, Isabelle Meichsner,\n  Richard M\\\"uller", "title": "Identifying Usability Issues of Software Analytics Applications in\n  Immersive Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software analytics in augmented reality (AR) is said to have great potential.\nOne reason why this potential is not yet fully exploited may be usability\nproblems of the AR user interfaces. We present an iterative and qualitative\nusability evaluation with 15 subjects of a state-of-the-art application for\nsoftware analytics in AR. We could identify and resolve numerous usability\nissues. Most of them were caused by applying conventional user interface\nelements, such as dialog windows, buttons, and scrollbars. The used city\nvisualization, however, did not cause any usability issues. Therefore, we argue\nthat future work should focus on making conventional user interface elements in\nAR obsolete by integrating their functionality into the immersive\nvisualization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 20:15:28 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Baum", "David", ""], ["Bechert", "Stefan", ""], ["Eisenecker", "Ulrich", ""], ["Meichsner", "Isabelle", ""], ["M\u00fcller", "Richard", ""]]}, {"id": "2008.06465", "submitter": "Ugur Kursuncu", "authors": "Thilini Wijesiriwardene, Hale Inan, Ugur Kursuncu, Manas Gaur, Valerie\n  L. Shalin, Krishnaprasad Thirunarayan, Amit Sheth, I. Budak Arpinar", "title": "ALONE: A Dataset for Toxic Behavior among Adolescents on Twitter", "comments": "Accepted: Social Informatics 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The convenience of social media has also enabled its misuse, potentially\nresulting in toxic behavior. Nearly 66% of internet users have observed online\nharassment, and 41% claim personal experience, with 18% facing severe forms of\nonline harassment. This toxic communication has a significant impact on the\nwell-being of young individuals, affecting mental health and, in some cases,\nresulting in suicide. These communications exhibit complex linguistic and\ncontextual characteristics, making recognition of such narratives challenging.\nIn this paper, we provide a multimodal dataset of toxic social media\ninteractions between confirmed high school students, called ALONE (AdoLescents\nON twittEr), along with descriptive explanation. Each instance of interaction\nincludes tweets, images, emoji and related metadata. Our observations show that\nindividual tweets do not provide sufficient evidence for toxic behavior, and\nmeaningful use of context in interactions can enable highlighting or\nexonerating tweets with purported toxicity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 17:02:55 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Wijesiriwardene", "Thilini", ""], ["Inan", "Hale", ""], ["Kursuncu", "Ugur", ""], ["Gaur", "Manas", ""], ["Shalin", "Valerie L.", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Sheth", "Amit", ""], ["Arpinar", "I. Budak", ""]]}, {"id": "2008.06610", "submitter": "Joey Huang", "authors": "Kylie Peppler, Joey Huang, Michael C. Richey, Michael Ginda, Katy\n  B\\\"orner, Haden Quinlan, A. John Hart", "title": "Key principles for workforce upskilling via online learning: a learning\n  analytics study of a professional course in additive manufacturing", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Effective adoption of online platforms for teaching, learning, and skill\ndevelopment is essential to both academic institutions and workplaces. Adoption\nof online learning has been abruptly accelerated by COVID19 pandemic, drawing\nattention to research on pedagogy and practice for effective online\ninstruction. Online learning requires a multitude of skills and resources\nspanning from learning management platforms to interactive assessment tools,\ncombined with multimedia content, presenting challenges to instructors and\norganizations. This study focuses on ways that learning sciences and visual\nlearning analytics can be used to design, and to improve, online workforce\ntraining in advanced manufacturing. Scholars and industry experts, educational\nresearchers, and specialists in data analysis and visualization collaborated to\nstudy the performance of a cohort of 900 professionals enrolled in an online\ntraining course focused on additive manufacturing. The course was offered\nthrough MITxPro, MIT Open Learning is a professional learning organization\nwhich hosts in a dedicated instance of the edX platform. This study combines\nlearning objective analysis and visual learning analytics to examine the\nrelationships among learning trajectories, engagement, and performance. The\nresults demonstrate how visual learning analytics was used for targeted course\nmodification, and interpretation of learner engagement and performance, such as\nby more direct mapping of assessments to learning objectives, and to expected\nand actual time needed to complete each segment of the course. The study also\nemphasizes broader strategies for course designers and instructors to align\ncourse assignments, learning objectives, and assessment measures with learner\nneeds and interests, and argues for a synchronized data infrastructure to\nfacilitate effective just in time learning and continuous improvement of online\ncourses.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 00:30:56 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Peppler", "Kylie", ""], ["Huang", "Joey", ""], ["Richey", "Michael C.", ""], ["Ginda", "Michael", ""], ["B\u00f6rner", "Katy", ""], ["Quinlan", "Haden", ""], ["Hart", "A. John", ""]]}, {"id": "2008.06616", "submitter": "Edward Burnell", "authors": "Edward Burnell, Priya P. Pillai, Maria C. Yang", "title": "Maps, Mirrors, and Participants: Design Lenses for Sociomateriality in\n  Engineering Organizations", "comments": "9 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When you use a computer it also uses you, and in that relationship forms a\nnew entity of melded agencies, a \"centaur\" inseparably human and nonhuman.\nNetworks of interaction in an organization similarly form \"organizational\ncentaurs\", melding humans, technologies, and organizations into an inseparable\nsociomateriality. By developing a convex optimization toolkit for conceptual\nengineering we sought to shape these centaurs. How do organizations go from a\nhigh-level concept (\"let's make an airplane\") to a \"design\", and in that\nprocess what blurred lines between humans and computers bring opportunities for\nresearch? We present three metaphors that have been useful lenses across our\nfield sites: considering design models as maps shows how centaurs apportioned\nlegitimacy; looking at design models as mirrors illuminates how they sought\nvalidation in their perspectives; and treating design models as participants\nrecognizes their opinions and agency as equivalent to other entities in these\ncentaurs.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 01:00:05 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 04:29:43 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Burnell", "Edward", ""], ["Pillai", "Priya P.", ""], ["Yang", "Maria C.", ""]]}, {"id": "2008.06678", "submitter": "Aoyu Wu", "authors": "Aoyu Wu, Wai Tong, Tim Dwyer, Bongshin Lee, Petra Isenberg, Huamin Qu", "title": "MobileVisFixer: Tailoring Web Visualizations for Mobile Phones\n  Leveraging an Explainable Reinforcement Learning Framework", "comments": "Accepted at IEEE VIS 2020 (Info VIS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We contribute MobileVisFixer, a new method to make visualizations more\nmobile-friendly. Although mobile devices have become the primary means of\naccessing information on the web, many existing visualizations are not\noptimized for small screens and can lead to a frustrating user experience.\nCurrently, practitioners and researchers have to engage in a tedious and\ntime-consuming process to ensure that their designs scale to screens of\ndifferent sizes, and existing toolkits and libraries provide little support in\ndiagnosing and repairing issues. To address this challenge, MobileVisFixer\nautomates a mobile-friendly visualization re-design process with a novel\nreinforcement learning framework. To inform the design of MobileVisFixer, we\nfirst collected and analyzed SVG-based visualizations on the web, and\nidentified five common mobile-friendly issues. MobileVisFixer addresses four of\nthese issues on single-view Cartesian visualizations with linear or discrete\nscales by a Markov Decision Process model that is both generalizable across\nvarious visualizations and fully explainable. MobileVisFixer deconstructs\ncharts into declarative formats, and uses a greedy heuristic based on Policy\nGradient methods to find solutions to this difficult, multi-criteria\noptimization problem in reasonable time. In addition, MobileVisFixer can be\neasily extended with the incorporation of optimization algorithms for data\nvisualizations. Quantitative evaluation on two real-world datasets demonstrates\nthe effectiveness and generalizability of our method.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 08:24:45 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wu", "Aoyu", ""], ["Tong", "Wai", ""], ["Dwyer", "Tim", ""], ["Lee", "Bongshin", ""], ["Isenberg", "Petra", ""], ["Qu", "Huamin", ""]]}, {"id": "2008.06723", "submitter": "Lionel Robert", "authors": "Connor Esterwood, Lionel P. Robert", "title": "Personality in Healthcare Human Robot Interaction (H-HRI): A Literature\n  Review and Brief Critique", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": "10.1145/3406499.3415075", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots are becoming an important way to deliver health care, and personality\nis vital to understanding their effectiveness. Despite this, there is a lack of\na systematic overarching understanding of personality in health care human\nrobot interaction (H-HRI). To address this, the authors conducted a review that\nidentified 18 studies on personality in H-HRI. This paper presents the results\nof that systematic literature review. Insights are derived from this review\nregarding the methodologies, outcomes, and samples utilized. The authors of\nthis review discuss findings across this literature while identifying several\ngaps worthy of attention. Overall, this paper is an important starting point in\nunderstanding personality in H-HRI.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 14:14:56 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Esterwood", "Connor", ""], ["Robert", "Lionel P.", ""]]}, {"id": "2008.06798", "submitter": "Geoffrey Yu", "authors": "Geoffrey X. Yu, Tovi Grossman, Gennady Pekhimenko", "title": "Skyline: Interactive In-Editor Computational Performance Profiling for\n  Deep Neural Network Training", "comments": "14 pages, 5 figures. Appears in the proceedings of UIST'20", "journal-ref": null, "doi": "10.1145/3379337.3415890", "report-no": null, "categories": "cs.HC cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a state-of-the-art deep neural network (DNN) is a\ncomputationally-expensive and time-consuming process, which incentivizes deep\nlearning developers to debug their DNNs for computational performance. However,\neffectively performing this debugging requires intimate knowledge about the\nunderlying software and hardware systems---something that the typical deep\nlearning developer may not have. To help bridge this gap, we present Skyline: a\nnew interactive tool for DNN training that supports in-editor computational\nperformance profiling, visualization, and debugging. Skyline's key contribution\nis that it leverages special computational properties of DNN training to\nprovide (i) interactive performance predictions and visualizations, and (ii)\ndirectly manipulatable visualizations that, when dragged, mutate the batch size\nin the code. As an in-editor tool, Skyline allows users to leverage these\ndiagnostic features to debug the performance of their DNNs during development.\nAn exploratory qualitative user study of Skyline produced promising results;\nall the participants found Skyline to be useful and easy to use.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 22:17:00 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 14:57:58 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Yu", "Geoffrey X.", ""], ["Grossman", "Tovi", ""], ["Pekhimenko", "Gennady", ""]]}, {"id": "2008.06895", "submitter": "Sidong Feng", "authors": "Chunyang Chen, Sidong Feng, Zhengyang Liu, Zhenchang Xing, Shengdong\n  Zhao", "title": "From Lost to Found: Discover Missing UI Design Semantics through\n  Recovering Missing Tags", "comments": "22 pages, The 23rd ACM Conference on Computer-Supported Cooperative\n  Work and Social Computing", "journal-ref": null, "doi": "10.1145/3415194", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design sharing sites provide UI designers with a platform to share their\nworks and also an opportunity to get inspiration from others' designs. To\nfacilitate management and search of millions of UI design images, many design\nsharing sites adopt collaborative tagging systems by distributing the work of\ncategorization to the community. However, designers often do not know how to\nproperly tag one design image with compact textual description, resulting in\nunclear, incomplete, and inconsistent tags for uploaded examples which impede\nretrieval, according to our empirical study and interview with four\nprofessional designers. Based on a deep neural network, we introduce a novel\napproach for encoding both the visual and textual information to recover the\nmissing tags for existing UI examples so that they can be more easily found by\ntext queries. We achieve 82.72% accuracy in the tag prediction. Through a\nsimulation test of 5 queries, our system on average returns hundreds more\nresults than the default Dribbble search, leading to better relatedness,\ndiversity and satisfaction.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 12:21:58 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 09:28:41 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Chen", "Chunyang", ""], ["Feng", "Sidong", ""], ["Liu", "Zhengyang", ""], ["Xing", "Zhenchang", ""], ["Zhao", "Shengdong", ""]]}, {"id": "2008.07204", "submitter": "Jessica Maria Echterhoff", "authors": "Jessica Maria Echterhoff and Edward J. Wang", "title": "PAR: Personal Activity Radius Camera View for Contextual Sensing", "comments": "15 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual sensing using wearable cameras has seen a variety of different\ncamera angles proposed to capture a wide gamut of different visual scenes. In\nthis paper, we propose a new camera view that aims to capture the same visual\ninformation as many of the camera positions and orientations combined from a\nsingle camera view point. The camera, mounted on the corner of a glasses frame\nis pointing downwards towards the floor, a field-of-view we named Personal\nActivity Radius (PAR). The PAR field-of-view captures the visual information\naround a wearer's personal bubble, including items they interact with, their\nbody motion, their surrounding environment, etc. In our evaluation, we tested\nthe PAR view's interpretability by human labelers in two different activity\ntracking scenarios: food related behaviors and exercise tracking. Human\nlabelers achieved an overall high level of precision in identifying body\nmotions in exercise tracking of 91% precision and eating/drinking motions at\n96% precision. Item interaction identification reached a precision of 86%\nprecision for labeling grocery categories. We show a high level on the device\nsetup and contextual views we were able to capture with the device. We see that\nthe camera wide angle captures different activities such as driving, shopping,\ngym exercises, walking and eating and can observe the specific interaction item\nof the user as well as the immediate contextual surrounding.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 10:37:33 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Echterhoff", "Jessica Maria", ""], ["Wang", "Edward J.", ""]]}, {"id": "2008.07207", "submitter": "David Melhart", "authors": "David Melhart, Daniele Gravina, Georgios N. Yannakakis", "title": "Moment-to-moment Engagement Prediction through the Eyes of the Observer:\n  PUBG Streaming on Twitch", "comments": "Version accepted for the Conference on the Foundations of Digital\n  Games 2020 - Malta", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to predict moment-to-moment gameplay engagement based solely\non game telemetry? Can we reveal engaging moments of gameplay by observing the\nway the viewers of the game behave? To address these questions in this paper,\nwe reframe the way gameplay engagement is defined and we view it, instead,\nthrough the eyes of a game's live audience. We build prediction models for\nviewers' engagement based on data collected from the popular battle royale game\nPlayerUnknown's Battlegrounds as obtained from the Twitch streaming service. In\nparticular, we collect viewers' chat logs and in-game telemetry data from\nseveral hundred matches of five popular streamers (containing over 100,000 game\nevents) and machine learn the mapping between gameplay and viewer chat\nfrequency during play, using small neural network architectures. Our key\nfindings showcase that engagement models trained solely on 40 gameplay features\ncan reach accuracies of up to 80% on average and 84% at best. Our models are\nscalable and generalisable as they perform equally well within- and\nacross-streamers, as well as across streamer play styles.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 10:40:34 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Melhart", "David", ""], ["Gravina", "Daniele", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "2008.07227", "submitter": "Xinhuan Shu", "authors": "Xinhuan Shu, Aoyu Wu, Junxiu Tang, Benjamin Bach, Yingcai Wu, and\n  Huamin Qu", "title": "What Makes a Data-GIF Understandable?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GIFs are enjoying increasing popularity on social media as a format for\ndata-driven storytelling with visualization; simple visual messages are\nembedded in short animations that usually last less than 15 seconds and are\nplayed in automatic repetition. In this paper, we ask the question, \"What makes\na data-GIF understandable?\" While other storytelling formats such as data\nvideos, infographics, or data comics are relatively well studied, we have\nlittle knowledge about the design factors and principles for \"data-GIFs\". To\nclose this gap, we provide results from semi-structured interviews and an\nonline study with a total of 118 participants investigating the impact of\ndesign decisions on the understandability of data-GIFs. The study and our\nconsequent analysis are informed by a systematic review and structured design\nspace of 108 data-GIFs that we found online. Our results show the impact of\ndesign dimensions from our design space such as animation encoding, context\npreservation, or repetition on viewers' understanding of the GIF's core\nmessage. The paper concludes with a list of suggestions for creating more\neffective Data-GIFs.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 11:27:38 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 08:19:47 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Shu", "Xinhuan", ""], ["Wu", "Aoyu", ""], ["Tang", "Junxiu", ""], ["Bach", "Benjamin", ""], ["Wu", "Yingcai", ""], ["Qu", "Huamin", ""]]}, {"id": "2008.07299", "submitter": "Maximilian T. Fischer", "authors": "Maximilian T. Fischer, Devanshu Arya, Dirk Streeb, Daniel Seebacher,\n  Daniel A. Keim, Marcel Worring", "title": "Visual Analytics for Temporal Hypergraph Model Exploration", "comments": "11 pages, 6 figures, IEEE VIS VAST 2020 - IEEE Transactions on\n  Visualization and Computer Graphics", "journal-ref": "IEEE Transactions on Visualization and Computer Graphics, 2020", "doi": "10.1109/TVCG.2020.3030408", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many processes, from gene interaction in biology to computer networks to\nsocial media, can be modeled more precisely as temporal hypergraphs than by\nregular graphs. This is because hypergraphs generalize graphs by extending\nedges to connect any number of vertices, allowing complex relationships to be\ndescribed more accurately and predict their behavior over time. However, the\ninteractive exploration and seamless refinement of such hypergraph-based\nprediction models still pose a major challenge. We contribute Hyper-Matrix, a\nnovel visual analytics technique that addresses this challenge through a tight\ncoupling between machine-learning and interactive visualizations. In\nparticular, the technique incorporates a geometric deep learning model as a\nblueprint for problem-specific models while integrating visualizations for\ngraph-based and category-based data with a novel combination of interactions\nfor an effective user-driven exploration of hypergraph models. To eliminate\ndemanding context switches and ensure scalability, our matrix-based\nvisualization provides drill-down capabilities across multiple levels of\nsemantic zoom, from an overview of model predictions down to the content. We\nfacilitate a focused analysis of relevant connections and groups based on\ninteractive user-steering for filtering and search tasks, a dynamically\nmodifiable partition hierarchy, various matrix reordering techniques, and\ninteractive model feedback. We evaluate our technique in a case study and\nthrough formative evaluation with law enforcement experts using real-world\ninternet forum communication data. The results show that our approach surpasses\nexisting solutions in terms of scalability and applicability, enables the\nincorporation of domain knowledge, and allows for fast search-space traversal.\nWith the technique, we pave the way for the visual analytics of temporal\nhypergraphs in a wide variety of domains.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 13:28:31 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 11:00:45 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fischer", "Maximilian T.", ""], ["Arya", "Devanshu", ""], ["Streeb", "Dirk", ""], ["Seebacher", "Daniel", ""], ["Keim", "Daniel A.", ""], ["Worring", "Marcel", ""]]}, {"id": "2008.07331", "submitter": "Shuby Deshpande", "authors": "Shuby Deshpande, Benjamin Eysenbach, Jeff Schneider", "title": "Interactive Visualization for Debugging RL", "comments": "Builds on preliminary work presented at ICML 2020 (WHI)\n  arXiv:2007.05577. An interactive demo of the system can be at\n  https://tinyurl.com/y5gv5t4m", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization tools for supervised learning allow users to interpret,\nintrospect, and gain an intuition for the successes and failures of their\nmodels. While reinforcement learning practitioners ask many of the same\nquestions, existing tools are not applicable to the RL setting as these tools\naddress challenges typically found in the supervised learning regime. In this\nwork, we design and implement an interactive visualization tool for debugging\nand interpreting RL algorithms. Our system addresses many features missing from\nprevious tools such as (1) tools for supervised learning often are not\ninteractive; (2) while debugging RL policies researchers use state\nrepresentations that are different from those seen by the agent; (3) a\nframework designed to make the debugging RL policies more conducive. We provide\nan example workflow of how this system could be used, along with ideas for\nfuture extensions.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 15:28:18 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 22:27:29 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Deshpande", "Shuby", ""], ["Eysenbach", "Benjamin", ""], ["Schneider", "Jeff", ""]]}, {"id": "2008.07520", "submitter": "Anastassia Loukina", "authors": "Anastassia Loukina, Keelan Evanini, Matthew Mulholland, Ian Blood, and\n  Klaus Zechner", "title": "Do face masks introduce bias in speech technologies? The case of\n  automated scoring of speaking proficiency", "comments": null, "journal-ref": "Proceedings of Interspeech 2020, 1942-1946", "doi": "10.21437/Interspeech.2020-1264", "report-no": null, "categories": "eess.AS cs.CL cs.HC cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has led to a dramatic increase in the use of face masks\nworldwide. Face coverings can affect both acoustic properties of the signal as\nwell as speech patterns and have unintended effects if the person wearing the\nmask attempts to use speech processing technologies. In this paper we explore\nthe impact of wearing face masks on the automated assessment of English\nlanguage proficiency. We use a dataset from a large-scale speaking test for\nwhich test-takers were required to wear face masks during the test\nadministration, and we compare it to a matched control sample of test-takers\nwho took the same test before the mask requirements were put in place. We find\nthat the two samples differ across a range of acoustic measures and also show a\nsmall but significant difference in speech patterns. However, these differences\ndo not lead to differences in human or automated scores of English language\nproficiency. Several measures of bias showed no differences in scores between\nthe two groups.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:58:29 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:10:15 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Loukina", "Anastassia", ""], ["Evanini", "Keelan", ""], ["Mulholland", "Matthew", ""], ["Blood", "Ian", ""], ["Zechner", "Klaus", ""]]}, {"id": "2008.07615", "submitter": "Hooman Hedayati", "authors": "Hooman Hedayati, Ryo Suzuki1, Daniel Leithinger, Daniel Szafir", "title": "PufferBot: Actuated Expandable Structures for Aerial Robots", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PufferBot, an aerial robot with an expandable structure that may\nexpand to protect a drone's propellers when the robot is close to obstacles or\ncollocated humans. PufferBot is made of a custom 3D-printed expandable scissor\nstructure, which utilizes a one degree of freedom actuator with rack and pinion\nmechanism. We propose four designs for the expandable structure, each with\nunique characterizations for different situations. Finally, we present three\nmotivating scenarios in which PufferBot may extend the utility of existing\nstatic propeller guard structures. The supplementary video can be found at:\nhttps://youtu.be/XtPepCxWcBg\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 20:54:32 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Hedayati", "Hooman", ""], ["Suzuki1", "Ryo", ""], ["Leithinger", "Daniel", ""], ["Szafir", "Daniel", ""]]}, {"id": "2008.07643", "submitter": "Fajrian Yunus", "authors": "Fajrian Yunus, Chlo\\'e Clavel, Catherine Pelachaud", "title": "Sequence-to-Sequence Predictive Model: From Prosody To Communicative\n  Gestures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CL cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communicative gestures and speech acoustic are tightly linked. Our objective\nis to predict the timing of gestures according to the acoustic. That is, we\nwant to predict when a certain gesture occurs. We develop a model based on a\nrecurrent neural network with attention mechanism. The model is trained on a\ncorpus of natural dyadic interaction where the speech acoustic and the gesture\nphases and types have been annotated. The input of the model is a sequence of\nspeech acoustic and the output is a sequence of gesture classes. The classes we\nare using for the model output is based on a combination of gesture phases and\ngesture types. We use a sequence comparison technique to evaluate the model\nperformance. We find that the model can predict better certain gesture classes\nthan others. We also perform ablation studies which reveal that fundamental\nfrequency is a relevant feature for gesture prediction task. In another\nsub-experiment, we find that including eyebrow movements as acting as beat\ngesture improves the performance. Besides, we also find that a model trained on\nthe data of one given speaker also works for the other speaker of the same\nconversation. We also perform a subjective experiment to measure how\nrespondents judge the naturalness, the time consistency, and the semantic\nconsistency of the generated gesture timing of a virtual agent. Our respondents\nrate the output of our model favorably.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 21:55:22 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 21:03:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yunus", "Fajrian", ""], ["Clavel", "Chlo\u00e9", ""], ["Pelachaud", "Catherine", ""]]}, {"id": "2008.07668", "submitter": "Hooman Hedayati", "authors": "Hooman Hedayati, Annika Muehlbradt, Daniel J. Szafir, Sean Andrist", "title": "REFORM: Recognizing F-formations for Social Robots", "comments": "IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing and understanding conversational groups, or F-formations, is a\ncritical task for situated agents designed to interact with humans.\nF-formations contain complex structures and dynamics, yet are used intuitively\nby people in everyday face-to-face conversations. Prior research exploring ways\nof identifying F-formations has largely relied on heuristic algorithms that may\nnot capture the rich dynamic behaviors employed by humans. We introduce REFORM\n(REcognize F-FORmations with Machine learning), a data-driven approach for\ndetecting F-formations given human and agent positions and orientations. REFORM\ndecomposes the scene into all possible pairs and then reconstructs F-formations\nwith a voting-based scheme. We evaluated our approach across three datasets:\nthe SALSA dataset, a newly collected human-only dataset, and a new set of acted\nhuman-robot scenarios, and found that REFORM yielded improved accuracy over a\nstate-of-the-art F-formation detection algorithm. We also introduce symmetry\nand tightness as quantitative measures to characterize F-formations.\nSupplementary video: https://youtu.be/Fp7ETdkKvdA , Dataset available at:\ngithub.com/cu-ironlab/Babble\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 23:32:05 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Hedayati", "Hooman", ""], ["Muehlbradt", "Annika", ""], ["Szafir", "Daniel J.", ""], ["Andrist", "Sean", ""]]}, {"id": "2008.07702", "submitter": "Michael Oppermann", "authors": "Michael Oppermann, Robert Kincaid, Tamara Munzner", "title": "VizCommender: Computing Text-Based Similarity in Visualization\n  Repositories for Content-Based Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud-based visualization services have made visual analytics accessible to a\nmuch wider audience than ever before. Systems such as Tableau have started to\namass increasingly large repositories of analytical knowledge in the form of\ninteractive visualization workbooks. When shared, these collections can form a\nvisual analytic knowledge base. However, as the size of a collection increases,\nso does the difficulty in finding relevant information. Content-based\nrecommendation (CBR) systems could help analysts in finding and managing\nworkbooks relevant to their interests. Toward this goal, we focus on text-based\ncontent that is representative of the subject matter of visualizations rather\nthan the visual encodings and style. We discuss the challenges associated with\ncreating a CBR based on visualization specifications and explore more\nconcretely how to implement the relevance measures required using Tableau\nworkbook specifications as the source of content data. We also demonstrate what\ninformation can be extracted from these visualization specifications and how\nvarious natural language processing techniques can be used to compute\nsimilarity between workbooks as one way to measure relevance. We report on a\ncrowd-sourced user study to determine if our similarity measure mimics human\njudgement. Finally, we choose latent Dirichlet allocation (LDA) as a specific\nmodel and instantiate it in a proof-of-concept recommender tool to demonstrate\nthe basic function of our similarity measure.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 02:26:27 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Oppermann", "Michael", ""], ["Kincaid", "Robert", ""], ["Munzner", "Tamara", ""]]}, {"id": "2008.07738", "submitter": "Helen Jiang", "authors": "Helen Jiang, Erwen Senge", "title": "Usable Security for ML Systems in Mental Health: A Framework", "comments": "Accepted to Designing AI in Support of Good Mental Health (GOOD)\n  Workshop at KDD 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the applications and demands of Machine learning (ML) systems in mental\nhealth are growing, there is little discussion nor consensus regarding a\nuniquely challenging aspect: building security methods and requirements into\nthese ML systems, and keep the ML system usable for end-users. This question of\nusable security is very important, because the lack of consideration in either\nsecurity or usability would hinder large-scale user adoption and active usage\nof ML systems in mental health applications.\n  In this short paper, we introduce a framework of four pillars, and a set of\ndesired properties which can be used to systematically guide and evaluate\nsecurity-related designs, implementations, and deployments of ML systems for\nmental health. We aim to weave together threads from different domains,\nincorporate existing views, and propose new principles and requirements, in an\neffort to lay out a clear framework where criteria and expectations are\nestablished, and are used to make security mechanisms usable for end-users of\nthose ML systems in mental health. Together with this framework, we present\nseveral concrete scenarios where different usable security cases and profiles\nin ML-systems in mental health applications are examined and evaluated.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 04:44:47 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Jiang", "Helen", ""], ["Senge", "Erwen", ""]]}, {"id": "2008.07764", "submitter": "Amyra Meidiana", "authors": "Amyra Meidiana, Seok-Hee Hong, Peter Eades", "title": "New Quality Metrics for Dynamic Graph Drawing", "comments": "Appears in the Proceedings of the 28th International Symposium on\n  Graph Drawing and Network Visualization (GD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present new quality metrics for dynamic graph drawings.\nNamely, we present a new framework for change faithfulness metrics for dynamic\ngraph drawings, which compare the ground truth change in dynamic graphs and the\ngeometric change in drawings. More specifically, we present two specific\ninstances, cluster change faithfulness metrics and distance change faithfulness\nmetrics. We first validate the effectiveness of our new metrics using\ndeformation experiments. Then we compare various graph drawing algorithms using\nour metrics. Our experiments confirm that the best cluster (resp. distance)\nfaithful graph drawing algorithms are also cluster (resp. distance) change\nfaithful.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 06:53:19 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 13:37:15 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Meidiana", "Amyra", ""], ["Hong", "Seok-Hee", ""], ["Eades", "Peter", ""]]}, {"id": "2008.07795", "submitter": "Stephan Wiefling", "authors": "Stephan Wiefling, Tanvi Patil, Markus D\\\"urmuth and Luigi Lo Iacono", "title": "Evaluation of Risk-based Re-Authentication Methods", "comments": "14 pages, 5 figures. Paper accepted for IFIP SEC 2020. Keywords:\n  Risk-based Authentication (RBA), Re-authentication, Usable Security", "journal-ref": "35th IFIP TC-11 International Conference on Information Security\n  and Privacy Protection (IFIP SEC 2020). IFIP Advances in Information and\n  Communication Technology, vol. 580, pp. 280-294. Springer, Cham", "doi": "10.1007/978-3-030-58201-2_19", "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk-based Authentication (RBA) is an adaptive security measure that improves\nthe security of password-based authentication by protecting against credential\nstuffing, password guessing, or phishing attacks. RBA monitors extra features\nduring login and requests for an additional authentication step if the observed\nfeature values deviate from the usual ones in the login history. In\nstate-of-the-art RBA re-authentication deployments, users receive an email with\na numerical code in its body, which must be entered on the online service.\nAlthough this procedure has a major impact on RBA's time exposure and\nusability, these aspects were not studied so far.\n  We introduce two RBA re-authentication variants supplementing the de facto\nstandard with a link-based and another code-based approach. Then, we present\nthe results of a between-group study (N=592) to evaluate these three\napproaches. Our observations show with significant results that there is\npotential to speed up the RBA re-authentication process without reducing\nneither its security properties nor its security perception. The link-based\nre-authentication via \"magic links\", however, makes users significantly more\nanxious than the code-based approaches when perceived for the first time. Our\nevaluations underline the fact that RBA re-authentication is not a uniform\nprocedure. We summarize our findings and provide recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 08:08:12 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Wiefling", "Stephan", ""], ["Patil", "Tanvi", ""], ["D\u00fcrmuth", "Markus", ""], ["Iacono", "Luigi Lo", ""]]}, {"id": "2008.07819", "submitter": "Tianqi Ma", "authors": "Tianqi Ma, Lin Zhang, Xiumin Diao, Ou Ma", "title": "ConvGRU in Fine-grained Pitching Action Recognition for Action Outcome\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of the action outcome is a new challenge for a robot\ncollaboratively working with humans. With the impressive progress in video\naction recognition in recent years, fine-grained action recognition from video\ndata turns into a new concern. Fine-grained action recognition detects subtle\ndifferences of actions in more specific granularity and is significant in many\nfields such as human-robot interaction, intelligent traffic management, sports\ntraining, health caring. Considering that the different outcomes are closely\nconnected to the subtle differences in actions, fine-grained action recognition\nis a practical method for action outcome prediction. In this paper, we explore\nthe performance of convolutional gate recurrent unit (ConvGRU) method on a\nfine-grained action recognition tasks: predicting outcomes of ball-pitching.\nBased on sequences of RGB images of human actions, the proposed approach\nachieved the performance of 79.17% accuracy, which exceeds the current\nstate-of-the-art result. We also compared different network implementations and\nshowed the influence of different image sampling methods, different fusion\nmethods and pre-training, etc. Finally, we discussed the advantages and\nlimitations of ConvGRU in such action outcome prediction and fine-grained\naction recognition tasks.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 09:27:17 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ma", "Tianqi", ""], ["Zhang", "Lin", ""], ["Diao", "Xiumin", ""], ["Ma", "Ou", ""]]}, {"id": "2008.07862", "submitter": "David Baum", "authors": "David Baum", "title": "Exploring the Design Space of Aesthetics with the Repertory Grid\n  Technique", "comments": "Appears in the Proceedings of the 28th International Symposium on\n  Graph Drawing and Network Visualization (GD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By optimizing aesthetics, graph diagrams can be generated that are easier to\nread and understand. However, the challenge lies in identifying suitable\naesthetics. We present a novel approach based on repertory grids to explore the\ndesign space of aesthetics systematically. We applied our approach with three\nindependent groups of participants to systematically identify graph aesthetics.\nIn all three cases, we were able to reproduce the aesthetics with positively\nevaluated influence on readability without any prior knowledge. We also applied\nour approach to two- and three-dimensional domain-specific software\nvisualizations to demonstrate its versatility. In this case, we were also able\nto acquire several aesthetics that are relevant for perceiving the\nvisualization.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:25:26 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Baum", "David", ""]]}, {"id": "2008.07944", "submitter": "Yalong Yang", "authors": "Vahan Yoghourdjian, Yalong Yang, Tim Dwyer, Lee Lawrence, Michael\n  Wybrow, Kim Marriott", "title": "Scalability of Network Visualisation from a Cognitive Load Perspective", "comments": "To be presented at IEEE Conference on Information Visualization\n  (InfoVis 2020)", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030459", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node-link diagrams are widely used to visualise networks. However, even the\nbest network layout algorithms ultimately result in 'hairball' visualisations\nwhen the graph reaches a certain degree of complexity, requiring simplification\nthrough aggregation or interaction (such as filtering) to remain usable. Until\nnow, there has been little data to indicate at what level of complexity\nnode-link diagrams become ineffective or how visual complexity affects\ncognitive load. To this end, we conducted a controlled study to understand\nworkload limits for a task that requires a detailed understanding of the\nnetwork topology---finding the shortest path between two nodes. We tested\nperformance on graphs with 25 to 175 nodes with varying density. We collected\nperformance measures (accuracy and response time), subjective feedback, and\nphysiological measures (EEG, pupil dilation, and heart rate variability). To\nthe best of our knowledge, this is the first network visualisation study to\ninclude physiological measures. Our results show that people have significant\ndifficulty finding the shortest path in high-density node-link diagrams with\nmore than 50 nodes and even low-density graphs with more than 100 nodes. From\nour collected EEG data we observe functional differences in brain activity\nbetween hard and easy tasks. We found that cognitive load increased up to a\ncertain level of difficulty after which it decreased, likely because\nparticipants had given up. We also explored the effects of global network\nlayout features such as size or number of crossings, and features of the\nshortest path such as length or straightness on task difficulty. We found that\nglobal features generally had a greater impact than those of the shortest path.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:20:00 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Yoghourdjian", "Vahan", ""], ["Yang", "Yalong", ""], ["Dwyer", "Tim", ""], ["Lawrence", "Lee", ""], ["Wybrow", "Michael", ""], ["Marriott", "Kim", ""]]}, {"id": "2008.08165", "submitter": "Bahareh Sarrafzadeh", "authors": "Bahareh Sarrafzadeh, Sujay Kumar Jauhar, Michael Gamon, Edward Lank,\n  and Ryen White", "title": "Characterizing Stage-Aware Writing Assistance in Collaborative Document\n  Authoring", "comments": "Accepted for publication at CSCW 2020", "journal-ref": "CSCW 2020", "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Writing is a complex non-linear process that begins with a mental model of\nintent, and progresses through an outline of ideas, to words on paper (and\ntheir subsequent refinement). Despite past research in understanding writing,\nWeb-scale consumer and enterprise collaborative digital writing environments\nare yet to greatly benefit from intelligent systems that understand the stages\nof document evolution, providing opportune assistance based on authors'\nsituated actions and context. In this paper, we present three studies that\nexplore temporal stages of document authoring. We first survey information\nworkers at a large technology company about their writing habits and\npreferences, concluding that writers do in fact conceptually progress through\nseveral distinct phases while authoring documents. We also explore,\nqualitatively, how writing stages are linked to document lifespan. We\nsupplement these qualitative findings with an analysis of the longitudinal user\ninteraction logs of a popular digital writing platform over several million\ndocuments. Finally, as a first step towards facilitating an intelligent digital\nwriting assistant, we conduct a preliminary investigation into the utility of\nuser interaction log data for predicting the temporal stage of a document. Our\nresults support the benefit of tools tailored to writing stages, identify\nprimary tasks associated with these stages, and show that it is possible to\npredict stages from anonymous interaction logs. Together, these results argue\nfor the benefit and feasibility of more tailored digital writing assistance.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 21:48:04 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Sarrafzadeh", "Bahareh", ""], ["Jauhar", "Sujay Kumar", ""], ["Gamon", "Michael", ""], ["Lank", "Edward", ""], ["White", "Ryen", ""]]}, {"id": "2008.08193", "submitter": "Sudip Poddar", "authors": "Sudip Poddar, Anirban Mukhopadhyay", "title": "EXCLUVIS: A MATLAB GUI Software for Comparative Study of Clustering and\n  Visualization of Gene Expression Data", "comments": "19 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a popular data mining technique that aims to partition an input\nspace into multiple homogeneous regions. There exist several clustering\nalgorithms in the literature. The performance of a clustering algorithm depends\non its input parameters which can substantially affect the behavior of the\nalgorithm. Cluster validity indices determine the partitioning that best fits\nthe underlying data. In bioinformatics, microarray gene expression technology\nhas made it possible to measure the gene expression levels of thousands of\ngenes simultaneously. Many genomic studies, which aim to analyze the functions\nof some genes, highly rely on some clustering technique for grouping similarly\nexpressed genes in one cluster or partitioning tissue samples based on similar\nexpression values of genes. In this work, an application package called\nEXCLUVIS (gene EXpression data CLUstering and VISualization) has been developed\nusing MATLAB Graphical User Interface (GUI) environment for analyzing the\nperformances of different clustering algorithms on gene expression datasets. In\nthis application package, the user needs to select a number of parameters such\nas internal validity indices, external validity indices and number of clusters\nfrom the active windows for evaluating the performance of the clustering\nalgorithms. EXCLUVIS compares the performances of K-means, fuzzy C-means,\nhierarchical clustering and multiobjective evolutionary clustering algorithms.\nHeatmap and cluster profile plots are used for visualizing the results.\nEXCLUVIS allows the users to easily find the goodness of clustering solutions\nas well as provides visual representations of the clustering outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 23:34:57 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Poddar", "Sudip", ""], ["Mukhopadhyay", "Anirban", ""]]}, {"id": "2008.08202", "submitter": "Yubo Kou", "authors": "Yubo Kou and Xinning Gui", "title": "Mediating Community-AI Interaction through Situated Explanation: The\n  Case of AI-Led Moderation", "comments": null, "journal-ref": "PACMHCI, Vol 4, No. CSCW2, Article 102 (October 2020). 27 pages", "doi": "10.1145/3415173", "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) has become prevalent in our everyday\ntechnologies and impacts both individuals and communities. The explainable AI\n(XAI) scholarship has explored the philosophical nature of explanation and\ntechnical explanations, which are usually driven by experts in lab settings and\ncan be challenging for laypersons to understand. In addition, existing XAI\nresearch tends to focus on the individual level. Little is known about how\npeople understand and explain AI-led decisions in the community context.\nDrawing from XAI and activity theory, a foundational HCI theory, we theorize\nhow explanation is situated in a community's shared values, norms, knowledge,\nand practices, and how situated explanation mediates community-AI interaction.\nWe then present a case study of AI-led moderation, where community members\ncollectively develop explanations of AI-led decisions, most of which are\nautomated punishments. Lastly, we discuss the implications of this framework at\nthe intersection of CSCW, HCI, and XAI.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 00:13:12 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Kou", "Yubo", ""], ["Gui", "Xinning", ""]]}, {"id": "2008.08247", "submitter": "Kun Zhou", "authors": "Kun Zhou, Wayne Xin Zhao, Hui Wang, Sirui Wang, Fuzheng Zhang,\n  Zhongyuan Wang and Ji-Rong Wen", "title": "Leveraging Historical Interaction Data for Improving Conversational\n  Recommender System", "comments": "Accepted as CIKM short paper", "journal-ref": null, "doi": "10.1145/3340531.3412098", "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, conversational recommender system (CRS) has become an emerging and\npractical research topic. Most of the existing CRS methods focus on learning\neffective preference representations for users from conversation data alone.\nWhile, we take a new perspective to leverage historical interaction data for\nimproving CRS. For this purpose, we propose a novel pre-training approach to\nintegrating both item-based preference sequence (from historical interaction\ndata) and attribute-based preference sequence (from conversation data) via\npre-training methods. We carefully design two pre-training tasks to enhance\ninformation fusion between item- and attribute-based preference. To improve the\nlearning performance, we further develop an effective negative sample generator\nwhich can produce high-quality negative samples. Experiment results on two\nreal-world datasets have demonstrated the effectiveness of our approach for\nimproving CRS.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:43:50 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhou", "Kun", ""], ["Zhao", "Wayne Xin", ""], ["Wang", "Hui", ""], ["Wang", "Sirui", ""], ["Zhang", "Fuzheng", ""], ["Wang", "Zhongyuan", ""], ["Wen", "Ji-Rong", ""]]}, {"id": "2008.08282", "submitter": "Eren Cakmak", "authors": "Eren Cakmak, Udo Schlegel, Dominik J\\\"ackle, Daniel Keim, and Tobias\n  Schreck", "title": "Multiscale Snapshots: Visual Analysis of Temporal Summaries in Dynamic\n  Graphs", "comments": "IEEE Transactions on Visualization and Computer Graphics (TVCG), to\n  appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The overview-driven visual analysis of large-scale dynamic graphs poses a\nmajor challenge. We propose Multiscale Snapshots, a visual analytics approach\nto analyze temporal summaries of dynamic graphs at multiple temporal scales.\nFirst, we recursively generate temporal summaries to abstract overlapping\nsequences of graphs into compact snapshots. Second, we apply graph embeddings\nto the snapshots to learn low-dimensional representations of each sequence of\ngraphs to speed up specific analytical tasks (e.g., similarity search). Third,\nwe visualize the evolving data from a coarse to fine-granular snapshots to\nsemi-automatically analyze temporal states, trends, and outliers. The approach\nenables to discover similar temporal summaries (e.g., recurring states),\nreduces the temporal data to speed up automatic analysis, and to explore both\nstructural and temporal properties of a dynamic graph. We demonstrate the\nusefulness of our approach by a quantitative evaluation and the application to\na real-world dataset.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 06:18:48 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 18:48:30 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Cakmak", "Eren", ""], ["Schlegel", "Udo", ""], ["J\u00e4ckle", "Dominik", ""], ["Keim", "Daniel", ""], ["Schreck", "Tobias", ""]]}, {"id": "2008.08300", "submitter": "Zhichao Xu", "authors": "Zhichao Xu, Shuhong Chen", "title": "Using Sampling Strategy to Assist Consensus Sequence Analysis", "comments": "under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consensus Sequences of event logs are often used in process mining to quickly\ngrasp the core sequence of events to be performed in a process, or to represent\nthe backbone of the process for doing other analyses. However, it is still not\nclear how many traces are enough to properly represent the underlying process.\nIn this paper, we propose a novel sampling strategy to determine the number of\ntraces necessary to produce a representative consensus sequence. We show how to\nestimate the difference between the predefined Expert Model and the real\nprocesses carried out. This difference level can be used as reference for\ndomain experts to adjust the Expert Model. In addition, we apply this strategy\nto several real-world workflow activity datasets as a case study. We show a\nsample curve fitting task to help readers better understand our proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 07:12:09 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 23:03:06 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Xu", "Zhichao", ""], ["Chen", "Shuhong", ""]]}, {"id": "2008.08353", "submitter": "Furui Cheng", "authors": "Furui Cheng, Yao Ming, Huamin Qu", "title": "DECE: Decision Explorer with Counterfactual Explanations for Machine\n  Learning Models", "comments": "10 pages, 7 figures. The paper will be published on IEEE Transactions\n  on Visualization and Computer Graphics (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With machine learning models being increasingly applied to various\ndecision-making scenarios, people have spent growing efforts to make machine\nlearning models more transparent and explainable. Among various explanation\ntechniques, counterfactual explanations have the advantages of being\nhuman-friendly and actionable -- a counterfactual explanation tells the user\nhow to gain the desired prediction with minimal changes to the input. Besides,\ncounterfactual explanations can also serve as efficient probes to the models'\ndecisions. In this work, we exploit the potential of counterfactual\nexplanations to understand and explore the behavior of machine learning models.\nWe design DECE, an interactive visualization system that helps understand and\nexplore a model's decisions on individual instances and data subsets,\nsupporting users ranging from decision-subjects to model developers. DECE\nsupports exploratory analysis of model decisions by combining the strengths of\ncounterfactual explanations at instance- and subgroup-levels. We also introduce\na set of interactions that enable users to customize the generation of\ncounterfactual explanations to find more actionable ones that can suit their\nneeds. Through three use cases and an expert interview, we demonstrate the\neffectiveness of DECE in supporting decision exploration tasks and instance\nexplanations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 09:44:47 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Cheng", "Furui", ""], ["Ming", "Yao", ""], ["Qu", "Huamin", ""]]}, {"id": "2008.08688", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Rubaiat Habib Kazi, Li-Yi Wei, Stephen DiVerdi, Wilmot Li,\n  Daniel Leithinger", "title": "RealitySketch: Embedding Responsive Graphics and Visualizations in AR\n  through Dynamic Sketching", "comments": "UIST 2020", "journal-ref": null, "doi": "10.1145/3379337.3415892", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present RealitySketch, an augmented reality interface for sketching\ninteractive graphics and visualizations. In recent years, an increasing number\nof AR sketching tools enable users to draw and embed sketches in the real\nworld. However, with the current tools, sketched contents are inherently\nstatic, floating in mid air without responding to the real world. This paper\nintroduces a new way to embed dynamic and responsive graphics in the real\nworld. In RealitySketch, the user draws graphical elements on a mobile AR\nscreen and binds them with physical objects in real-time and improvisational\nways, so that the sketched elements dynamically move with the corresponding\nphysical motion. The user can also quickly visualize and analyze real-world\nphenomena through responsive graph plots or interactive visualizations. This\npaper contributes to a set of interaction techniques that enable capturing,\nparameterizing, and visualizing real-world motion without pre-defined programs\nand configurations. Finally, we demonstrate our tool with several application\nscenarios, including physics education, sports training, and in-situ tangible\ninterfaces.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 22:16:19 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Suzuki", "Ryo", ""], ["Kazi", "Rubaiat Habib", ""], ["Wei", "Li-Yi", ""], ["DiVerdi", "Stephen", ""], ["Li", "Wilmot", ""], ["Leithinger", "Daniel", ""]]}, {"id": "2008.08695", "submitter": "Ryo Suzuki", "authors": "Ryo Suzuki, Hooman Hedayati, Clement Zheng, James Bohn, Daniel Szafir,\n  Ellen Yi-Luen Do, Mark D. Gross, Daniel Leithinger", "title": "RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm\n  Robots", "comments": "CHI 2020", "journal-ref": null, "doi": "10.1145/3313831.3376523", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RoomShift is a room-scale dynamic haptic environment for virtual reality,\nusing a small swarm of robots that can move furniture. RoomShift consists of\nnine shape-changing robots: Roombas with mechanical scissor lifts. These robots\ndrive beneath a piece of furniture to lift, move and place it. By augmenting\nvirtual scenes with physical objects, users can sit on, lean against, place and\notherwise interact with furniture with their whole body; just as in the real\nworld. When the virtual scene changes or users navigate within it, the swarm of\nrobots dynamically reconfigures the physical environment to match the virtual\ncontent. We describe the hardware and software implementation, applications in\nvirtual tours and architectural design and interaction techniques.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 22:47:50 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Suzuki", "Ryo", ""], ["Hedayati", "Hooman", ""], ["Zheng", "Clement", ""], ["Bohn", "James", ""], ["Szafir", "Daniel", ""], ["Do", "Ellen Yi-Luen", ""], ["Gross", "Mark D.", ""], ["Leithinger", "Daniel", ""]]}, {"id": "2008.08791", "submitter": "Monica Perusquia-Hernandez", "authors": "Monica Perusquia-Hernandez, Felix Dollack, Chun Kwang Tan, Shushi\n  Namba, Saho Ayabe-Kanamura, Kenji Suzuki", "title": "Facial movement synergies and Action Unit detection from distal wearable\n  Electromyography and Computer Vision", "comments": "11 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distal facial Electromyography (EMG) can be used to detect smiles and frowns\nwith reasonable accuracy. It capitalizes on volume conduction to detect\nrelevant muscle activity, even when the electrodes are not placed directly on\nthe source muscle. The main advantage of this method is to prevent occlusion\nand obstruction of the facial expression production, whilst allowing EMG\nmeasurements. However, measuring EMG distally entails that the exact source of\nthe facial movement is unknown. We propose a novel method to estimate specific\nFacial Action Units (AUs) from distal facial EMG and Computer Vision (CV). This\nmethod is based on Independent Component Analysis (ICA), Non-Negative Matrix\nFactorization (NNMF), and sorting of the resulting components to determine\nwhich is the most likely to correspond to each CV-labeled action unit (AU).\nPerformance on the detection of AU06 (Orbicularis Oculi) and AU12 (Zygomaticus\nMajor) was estimated by calculating the agreement with Human Coders. The\nresults of our proposed algorithm showed an accuracy of 81% and a Cohen's Kappa\nof 0.49 for AU6; and accuracy of 82% and a Cohen's Kappa of 0.53 for AU12. This\ndemonstrates the potential of distal EMG to detect individual facial movements.\nUsing this multimodal method, several AU synergies were identified. We\nquantified the co-occurrence and timing of AU6 and AU12 in posed and\nspontaneous smiles using the human-coded labels, and for comparison, using the\ncontinuous CV-labels. The co-occurrence analysis was also performed on the\nEMG-based labels to uncover the relationship between muscle synergies and the\nkinematics of visible facial movement.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 06:09:03 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Perusquia-Hernandez", "Monica", ""], ["Dollack", "Felix", ""], ["Tan", "Chun Kwang", ""], ["Namba", "Shushi", ""], ["Ayabe-Kanamura", "Saho", ""], ["Suzuki", "Kenji", ""]]}, {"id": "2008.08806", "submitter": "Christoph Schmidt", "authors": "Christoph Schmidt and Paul Rosenthal and Heidrun Schumann", "title": "Varying Annotations in the Steps of the Visual Analysis", "comments": "9 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotations in Visual Analytics (VA) have become a common means to support\nthe analysis by integrating additional information into the VA system. That\nadditional information often depends on the current process step in the visual\nanalysis. For example, the data preprocessing step has data structuring\noperations while the data exploration step focuses on user interaction and\ninput. Describing suitable annotations to meet the goals of the different steps\nis challenging. To tackle this issue, we identify individual annotations for\neach step and outline their gathering and design properties for the visual\nanalysis of heterogeneous clinical data. We integrate our annotation design\ninto a visual analysis tool to show its applicability to data from the\nophthalmic domain. In interviews and application sessions with experts we asses\nits usefulness for the analysis of patients with different medications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 07:06:26 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Schmidt", "Christoph", ""], ["Rosenthal", "Paul", ""], ["Schumann", "Heidrun", ""]]}, {"id": "2008.08821", "submitter": "Alessio Arleo PhD", "authors": "Alessio Arleo, Walter Didimo, Giuseppe Liotta, Silvia Miksch, and\n  Fabrizio Montecchiani", "title": "VAIM: Visual Analytics for Influence Maximization", "comments": "Appears in the Proceedings of the 28th International Symposium on\n  Graph Drawing and Network Visualization (GD 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In social networks, individuals' decisions are strongly influenced by\nrecommendations from their friends and acquaintances. The influence\nmaximization (IM) problem asks to select a seed set of users that maximizes the\ninfluence spread, i.e., the expected number of users influenced through a\nstochastic diffusion process triggered by the seeds. In this paper, we present\nVAIM, a visual analytics system that supports users in analyzing the\ninformation diffusion process determined by different IM algorithms. By using\nVAIM one can: (i) simulate the information spread for a given seed set on a\nlarge network, (ii) analyze and compare the effectiveness of different seed\nsets, and (iii) modify the seed sets to improve the corresponding influence\nspread.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 07:53:01 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Arleo", "Alessio", ""], ["Didimo", "Walter", ""], ["Liotta", "Giuseppe", ""], ["Miksch", "Silvia", ""], ["Montecchiani", "Fabrizio", ""]]}, {"id": "2008.08965", "submitter": "Evalds Urtans", "authors": "Evalds Urtans, Ariel Tabaks", "title": "asya: Mindful verbal communication using deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.HC cs.LG cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  asya is a mobile application that consists of deep learning models which\nanalyze spectra of a human voice and do noise detection, speaker diarization,\ngender detection, tempo estimation, and classification of emotions using only\nvoice. All models are language agnostic and capable of running in real-time.\nOur speaker diarization models have accuracy over 95% on the test data set.\nThese models can be applied for a variety of areas like customer service\nimprovement, sales effective conversations, psychology and couples therapy.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 13:37:49 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Urtans", "Evalds", ""], ["Tabaks", "Ariel", ""]]}, {"id": "2008.08973", "submitter": "Evita Bakopoulou", "authors": "Evita Bakopoulou, Anastasia Shuba, Athina Markopoulou", "title": "Exposures Exposed: A Measurement and User Study to Assess Mobile Data\n  Privacy in Context", "comments": "arXiv admin note: text overlap with arXiv:1803.01261", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile devices have access to personal, potentially sensitive data, and there\nis a large number of mobile applications and third-party libraries that\ntransmit this information over the network to remote servers (including app\ndeveloper servers and third party servers). In this paper, we are interested in\nbetter understanding of not just the extent of personally identifiable\ninformation (PII) exposure, but also its context i.e., functionality of the\napp, destination server, encryption used, etc.) and the risk perceived by\nmobile users today. To that end we take two steps. First, we perform a\nmeasurement study: we collect a new dataset via manual and automatic testing\nand capture the exposure of 16 PII types from 400 most popular Android apps. We\nanalyze these exposures and provide insights into the extent and patterns of\nmobile apps sharing PII, which can be later used for prediction and prevention.\nSecond, we perform a user study with 220 participants on Amazon Mechanical\nTurk: we summarize the results of the measurement study in categories, present\nthem in a realistic context, and assess users' understanding, concern, and\nwillingness to take action. To the best of our knowledge, our user study is the\nfirst to collect and analyze user input in such fine granularity and on actual\n(not just potential or permitted) privacy exposures on mobile devices. Although\nmany users did not initially understand the full implications of their PII\nbeing exposed, after being better informed through the study, they became\nappreciative and interested in better privacy practices.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 02:42:30 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Bakopoulou", "Evita", ""], ["Shuba", "Anastasia", ""], ["Markopoulou", "Athina", ""]]}, {"id": "2008.09063", "submitter": "Iyolita Islam", "authors": "Muhammad Nazrul Islam, Iyolita Islam, Kazi MD. Munim, A.K.M. Najmul\n  Islam", "title": "A review on the mobile applications developed for COVID-19: An\n  exploratory analysis", "comments": "11 pages, 3 figures, 4 tables", "journal-ref": "IEEE Access (Volume: 8) 2020 145601 - 145610", "doi": "10.1109/ACCESS.2020.3015102", "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this research is to explore the existing mobile applications\ndeveloped for the COVID-19 pandemic. To obtain this research objective, firstly\nthe related applications were selected through the systematic search technique\nin the popular application stores. Secondly, data related to the app\nobjectives, functionalities provided by the app, user ratings, and user reviews\nwere extracted. Thirdly, the extracted data were analyzed through the affinity\ndiagram, noticing-collecting-thinking, and descriptive analysis. As outcomes,\nthe review provides a state-of-the-art view of mobile apps developed for\nCOVID-19 by revealing nine functionalities or features. It revealed ten factors\nrelated to information systems design characteristics that can guide future app\ndesign. The review outcome highlights the need for new development and further\nrefinement of the existing applications considering not only the revealed\nobjectives and their associated functionalities, but also revealed design\ncharacteristics such as reliability, performance, usefulness, supportive,\nsecurity, privacy, flexibility, responsiveness, ease of use, and cultural\nsensitivity.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 16:34:24 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Islam", "Muhammad Nazrul", ""], ["Islam", "Iyolita", ""], ["Munim", "Kazi MD.", ""], ["Islam", "A. K. M. Najmul", ""]]}, {"id": "2008.09100", "submitter": "Mahsan Nourani", "authors": "Mahsan Nourani, Joanie T. King, Eric D. Ragan", "title": "The Role of Domain Expertise in User Trust and the Impact of First\n  Impressions with Intelligent Systems", "comments": "Accepted and to appear in the Proceedings of the AAAI Conference on\n  Human Computation and Crowdsourcing (HCOMP) 2020", "journal-ref": null, "doi": null, "report-no": "https://www.aaai.org/ojs/index.php/HCOMP/article/view/7469", "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-specific intelligent systems are meant to help system users in their\ndecision-making process. Many systems aim to simultaneously support different\nusers with varying levels of domain expertise, but prior domain knowledge can\naffect user trust and confidence in detecting system errors. While it is also\nknown that user trust can be influenced by first impressions with intelligent\nsystems, our research explores the relationship between ordering bias and\ndomain expertise when encountering errors in intelligent systems. In this\npaper, we present a controlled user study to explore the role of domain\nknowledge in establishing trust and susceptibility to the influence of first\nimpressions on user trust. Participants reviewed an explainable image\nclassifier with a constant accuracy and two different orders of observing\nsystem errors (observing errors in the beginning of usage vs. in the end). Our\nfindings indicate that encountering errors early-on can cause negative first\nimpressions for domain experts, negatively impacting their trust over the\ncourse of interactions. However, encountering correct outputs early helps more\nknowledgable users to dynamically adjust their trust based on their\nobservations of system performance. In contrast, novice users suffer from\nover-reliance due to their lack of proper knowledge to detect errors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:41:02 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Nourani", "Mahsan", ""], ["King", "Joanie T.", ""], ["Ragan", "Eric D.", ""]]}, {"id": "2008.09233", "submitter": "John Wenskovitch Jr.", "authors": "John Wenskovitch and Chris North", "title": "An Examination of Grouping and Spatial Organization Tasks for\n  High-Dimensional Data Exploration", "comments": "11 pages, 10 figures, to be presented at IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do analysts think about grouping and spatial operations? This overarching\nquestion incorporates a number of points for investigation, including\nunderstanding how analysts begin to explore a dataset, the types of\ngrouping/spatial structures created and the operations performed on them, the\nrelationship between grouping and spatial structures, the decisions analysts\nmake when exploring individual observations, and the role of external\ninformation. This work contributes the design and results of such a study, in\nwhich a group of participants are asked to organize the data contained within\nan unfamiliar quantitative dataset. We identify several overarching approaches\ntaken by participants to design their organizational space, discuss the\ninteractions performed by the participants, and propose design recommendations\nto improve the usability of future high-dimensional data exploration tools that\nmake use of grouping (clustering) and spatial (dimension reduction) operations.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 23:51:18 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Wenskovitch", "John", ""], ["North", "Chris", ""]]}, {"id": "2008.09254", "submitter": "EPTCS", "authors": "Marco T. Moraz\\'an (Seton Hall University), Joshua M. Schappel (Seton\n  Hall University), Sachin Mahashabde (Seton Hall University)", "title": "Visual Designing and Debugging of Deterministic Finite-State Machines in\n  FSM", "comments": "In Proceedings TFPIE 2019 and 2020, arXiv:2008.08923", "journal-ref": "EPTCS 321, 2020, pp. 55-77", "doi": "10.4204/EPTCS.321.4", "report-no": null, "categories": "cs.HC cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a visualization tool for designing and debugging\ndeterministic finite-state machines in FSM -- a domain specific language for\nthe automata theory classroom. Like other automata visualization tools, users\ncan edit machines and observe their execution, given some input. Unlike other\nautomata visualization tools, the user is not burdened nor distracted with\nrendering a machine as a graph. Furthermore, emphasis is placed on the design\nof machines and this article presents a novel design recipe for deterministic\nfinite-state machines. In support of the design process, the visualization tool\nallows for each state to be associated with an invariant predicate. During\nmachine execution, the visualization tool indicates if the proposed invariant\nholds or does not hold after each transition. In this manner, students can\nvalidate and debug their machines before attempting to prove partial\ncorrectness or submitting for grading. In addition, any machine edited with the\nvisualization tool can be rendered as executable code. The interface of the\nvisualization tool along with extended examples of its use are presented.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 01:22:20 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Moraz\u00e1n", "Marco T.", "", "Seton Hall University"], ["Schappel", "Joshua M.", "", "Seton\n  Hall University"], ["Mahashabde", "Sachin", "", "Seton Hall University"]]}, {"id": "2008.09367", "submitter": "Martin N\\\"ollenburg", "authors": "Ben Jacobsen, Markus Wallinger, Stephen Kobourov and Martin\n  N\\\"ollenburg", "title": "MetroSets: Visualizing Sets as Metro Maps", "comments": "19 pages; accepted for IEEE INFOVIS 2020; for associated live system,\n  see http://metrosets.ac.tuwien.ac.at", "journal-ref": "IEEE TVCG 27(2):1257-1267 (2021)", "doi": "10.1109/TVCG.2020.3030475", "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose MetroSets, a new, flexible online tool for visualizing set systems\nusing the metro map metaphor. We model a given set system as a hypergraph\n$\\mathcal{H} = (V, \\mathcal{S})$, consisting of a set $V$ of vertices and a set\n$\\mathcal{S}$, which contains subsets of $V$ called hyperedges. Our system then\ncomputes a metro map representation of $\\mathcal{H}$, where each hyperedge $E$\nin $\\mathcal{S}$ corresponds to a metro line and each vertex corresponds to a\nmetro station. Vertices that appear in two or more hyperedges are drawn as\ninterchanges in the metro map, connecting the different sets. MetroSets is\nbased on a modular 4-step pipeline which constructs and optimizes a path-based\nhypergraph support, which is then drawn and schematized using metro map layout\nalgorithms. We propose and implement multiple algorithms for each step of the\nMetroSet pipeline and provide a functional prototype with easy-to-use preset\nconfigurations. Furthermore, using several real-world datasets, we perform an\nextensive quantitative evaluation of the impact of different pipeline stages on\ndesirable properties of the generated maps, such as octolinearity,\nmonotonicity, and edge uniformity.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:22:09 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 13:58:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Jacobsen", "Ben", ""], ["Wallinger", "Markus", ""], ["Kobourov", "Stephen", ""], ["N\u00f6llenburg", "Martin", ""]]}, {"id": "2008.09391", "submitter": "Nicol\\'as E. D\\'iaz Ferreyra", "authors": "Nicolas E. Diaz Ferreyra, Rene Meis and Maritta Heisel", "title": "Learning from Online Regrets: From Deleted Posts to Risk Awareness in\n  Social Network Sites", "comments": null, "journal-ref": null, "doi": "10.1145/3314183.3323849", "report-no": null, "categories": "cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social Network Sites (SNSs) like Facebook or Instagram are spaces where\npeople expose their lives to wide and diverse audiences. This practice can lead\nto unwanted incidents such as reputation damage, job loss or harassment when\npieces of private information reach unintended recipients. As a consequence,\nusers often regret to have posted private information in these platforms and\nproceed to delete such content after having a negative experience. Risk\nawareness is a strategy that can be used to persuade users towards safer\nprivacy decisions. However, many risk awareness technologies for SNSs assume\nthat information about risks is retrieved and measured by an expert in the\nfield. Consequently, risk estimation is an activity that is often passed over\ndespite its importance. In this work we introduce an approach that employs\ndeleted posts as risk information vehicles to measure the frequency and\nconsequence level of self-disclosure patterns in SNSs. In this method,\nconsequence is reported by the users through an ordinal scale and used later on\nto compute a risk criticality index. We thereupon show how this index can serve\nin the design of adaptive privacy nudges for SNSs.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 09:43:18 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Ferreyra", "Nicolas E. Diaz", ""], ["Meis", "Rene", ""], ["Heisel", "Maritta", ""]]}, {"id": "2008.09472", "submitter": "Mawulolo Ameko", "authors": "Mawulolo K. Ameko, Miranda L. Beltzer, Lihua Cai, Mehdi Boukhechba,\n  Bethany A. Teachman, Laura E. Barnes", "title": "Offline Contextual Multi-armed Bandits for Mobile Health Interventions:\n  A Case Study on Emotion Regulation", "comments": "Accepted at RecSys 2020", "journal-ref": null, "doi": "10.1145/3383313.3412244", "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delivering treatment recommendations via pervasive electronic devices such as\nmobile phones has the potential to be a viable and scalable treatment medium\nfor long-term health behavior management. But active experimentation of\ntreatment options can be time-consuming, expensive and altogether unethical in\nsome cases. There is a growing interest in methodological approaches that allow\nan experimenter to learn and evaluate the usefulness of a new treatment\nstrategy before deployment. We present the first development of a treatment\nrecommender system for emotion regulation using real-world historical mobile\ndigital data from n = 114 high socially anxious participants to test the\nusefulness of new emotion regulation strategies. We explore a number of offline\ncontextual bandits estimators for learning and propose a general framework for\nlearning algorithms. Our experimentation shows that the proposed doubly robust\noffline learning algorithms performed significantly better than baseline\napproaches, suggesting that this type of recommender algorithm could improve\nemotion regulation. Given that emotion regulation is impaired across many\nmental illnesses and such a recommender algorithm could be scaled up easily,\nthis approach holds potential to increase access to treatment for many people.\nWe also share some insights that allow us to translate contextual bandit models\nto this complex real-world data, including which contextual features appear to\nbe most important for predicting emotion regulation strategy effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 13:41:24 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Ameko", "Mawulolo K.", ""], ["Beltzer", "Miranda L.", ""], ["Cai", "Lihua", ""], ["Boukhechba", "Mehdi", ""], ["Teachman", "Bethany A.", ""], ["Barnes", "Laura E.", ""]]}, {"id": "2008.09533", "submitter": "Md Momen Bhuiyan", "authors": "Md Momen Bhuiyan, Amy X. Zhang, Connie Moon Sehat and Tanushree Mitra", "title": "Investigating Differences in Crowdsourced News Credibility Assessment:\n  Raters, Tasks, and Expert Criteria", "comments": null, "journal-ref": null, "doi": "10.1145/3415164", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misinformation about critical issues such as climate change and vaccine\nsafety is oftentimes amplified on online social and search platforms. The\ncrowdsourcing of content credibility assessment by laypeople has been proposed\nas one strategy to combat misinformation by attempting to replicate the\nassessments of experts at scale. In this work, we investigate news credibility\nassessments by crowds versus experts to understand when and how ratings between\nthem differ. We gather a dataset of over 4,000 credibility assessments taken\nfrom 2 crowd groups---journalism students and Upwork workers---as well as 2\nexpert groups---journalists and scientists---on a varied set of 50 news\narticles related to climate science, a topic with widespread disconnect between\npublic opinion and expert consensus. Examining the ratings, we find differences\nin performance due to the makeup of the crowd, such as rater demographics and\npolitical leaning, as well as the scope of the tasks that the crowd is assigned\nto rate, such as the genre of the article and partisanship of the publication.\nFinally, we find differences between expert assessments due to differing expert\ncriteria that journalism versus science experts use---differences that may\ncontribute to crowd discrepancies, but that also suggest a way to reduce the\ngap by designing crowd tasks tailored to specific expert criteria. From these\nfindings, we outline future research directions to better design crowd\nprocesses that are tailored to specific crowds and types of content.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 15:19:18 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Bhuiyan", "Md Momen", ""], ["Zhang", "Amy X.", ""], ["Sehat", "Connie Moon", ""], ["Mitra", "Tanushree", ""]]}, {"id": "2008.09576", "submitter": "Jonathan Zong", "authors": "Jonathan Zong, Dhiraj Barnwal, Rupayan Neogy, Arvind Satyanarayan", "title": "Lyra 2: Designing Interactive Visualizations by Demonstration", "comments": "11 pages, 5 figures. To be published in IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent graphical interfaces offer direct manipulation mechanisms for\nauthoring visualizations, but are largely restricted to static output. To\nauthor interactive visualizations, users must instead turn to textual\nspecification; but, such approaches impose a higher technical burden than their\ngraphical counterparts. To bridge this gap, we introduce interaction design by\ndemonstration: a novel method for authoring interaction techniques via direct\nmanipulation. Users perform an interaction (e.g., button clicks, drags, or key\npresses) directly on the visualization they are editing. The system interprets\nthis performance using a set of heuristics, and produces suggestions of\npossible interaction designs. Heuristics account for properties of the\ninteraction (e.g., target and event type) as well as the visualization (e.g.,\nmark and scale types, and multiple views). Interaction design suggestions are\ndisplayed as thumbnails; users can preview and test these suggestions,\niteratively refine them through additional demonstrations, and finally apply\nand customize them via property inspectors. To evaluate our approach, we\ninstantiate it in Lyra, an existing visualization design environment. We\ndemonstrate its expressive extent with a gallery of diverse examples, and\nevaluate its usability through a first-use study and via an analysis of its\ncognitive dimensions. We find that, in Lyra, interaction design by\ndemonstration enables users to rapidly express a wide range of interactive\nvisualizations.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 16:39:53 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 18:27:49 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Zong", "Jonathan", ""], ["Barnwal", "Dhiraj", ""], ["Neogy", "Rupayan", ""], ["Satyanarayan", "Arvind", ""]]}, {"id": "2008.09632", "submitter": "Jun Yuan", "authors": "Jun Yuan, Changjian Chen, Weikai Yang, Mengchen Liu, Jiazhi Xia,\n  Shixia Liu", "title": "A Survey of Visual Analytics Techniques for Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual analytics for machine learning has recently evolved as one of the most\nexciting areas in the field of visualization. To better identify which research\ntopics are promising and to learn how to apply relevant techniques in visual\nanalytics, we systematically review 259 papers published in the last ten years\ntogether with representative works before 2010. We build a taxonomy, which\nincludes three first-level categories: techniques before model building,\ntechniques during model building, and techniques after model building. Each\ncategory is further characterized by representative analysis tasks, and each\ntask is exemplified by a set of recent influential works. We also discuss and\nhighlight research challenges and promising potential future research\nopportunities useful for visual analytics researchers.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:13:01 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Yuan", "Jun", ""], ["Chen", "Changjian", ""], ["Yang", "Weikai", ""], ["Liu", "Mengchen", ""], ["Xia", "Jiazhi", ""], ["Liu", "Shixia", ""]]}, {"id": "2008.09656", "submitter": "Sara Kingsley", "authors": "Sara Kingsley, Clara Wang, Alex Mikhalenko, Proteeti Sinha, Chinmay\n  Kulkarni", "title": "Auditing Digital Platforms for Discrimination in Economic Opportunity\n  Advertising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital platforms, including social networks, are major sources of economic\ninformation. Evidence suggests that digital platforms display different\nsocioeconomic opportunities to demographic groups. Our work addresses this\nissue by presenting a methodology and software to audit digital platforms for\nbias and discrimination. To demonstrate, an audit of the Facebook platform and\nadvertising network was conducted. Between October 2019 and May 2020, we\ncollected 141,063 ads from the Facebook Ad Library API. Using machine learning\nclassifiers, each ad was automatically labeled by the primary marketing\ncategory (housing, employment, credit, political, other). For each of the\ncategories, we analyzed the distribution of the ad content by age group and\ngender. From the audit findings, we considered and present the limitations,\nneeds, infrastructure and policies that would enable researchers to conduct\nmore systematic audits in the future and advocate for why this work must be\ndone. We also discuss how biased distributions impact what socioeconomic\nopportunities people have, especially when on digital platforms some\ndemographic groups are disproportionately excluded from the population(s) that\nreceive(s) content regulated by law.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 19:18:34 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Kingsley", "Sara", ""], ["Wang", "Clara", ""], ["Mikhalenko", "Alex", ""], ["Sinha", "Proteeti", ""], ["Kulkarni", "Chinmay", ""]]}, {"id": "2008.09670", "submitter": "Anna Solovyova", "authors": "Anna Solovyova, Sergiy Danylov, Shpenkov Oleksii, Aleksandr Kravchenko", "title": "Early Autism Spectrum Disorders Diagnosis Using Eye-Tracking Technology", "comments": "11 pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the number of children with diagnosed autism spectrum disorder (ASD)\ncontinues to rise from year to year, there is still no universal approach to\nautism diagnosis and treatment. A great variety of different tools and\napproaches for the on-site diagnostic are available right now, however, a big\npercent of parents have no access to them and they tend to search for the\navailable tools and correction programs on the Internet. Lack of money, absence\nof qualified specialists, and low level of trust to the correction methods are\nthe main issues that affect the in-time diagnoses of ASD and which need to be\nsolved to get the early treatment for the little patients. Understanding the\nimportance of this issue our team decided to investigate new methods of the\nonline autism diagnoses and develop the algorithm that will be able to predict\nthe chances of ASD according to the information from the gaze activity of the\nchild. The results that we got during the experiments show supported our idea\nthat eye-tracking technology is one of the most promising tools for the early\ndetection of the eye-movement features that can be markers of the ASD.\nMoreover, we have conducted a series of experiments to ensure that our approach\nhas a reliable result on the cheap webcam systems. Thus, this approach can be\nused as an additional first screening tool for the home monitoring of the early\nchild development and ASD connected disorders monitoring. The further\ndevelopment of eye-tracking based autism diagnosis has a big potential of usage\nand can be further implemented in the daily practice for practical specialists\nand parents.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 20:22:55 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Solovyova", "Anna", ""], ["Danylov", "Sergiy", ""], ["Oleksii", "Shpenkov", ""], ["Kravchenko", "Aleksandr", ""]]}, {"id": "2008.09688", "submitter": "Xi Wang", "authors": "Xi Wang, Zoya Bylinskii, Aaron Hertzmann, Robert Pepperell", "title": "Toward Quantifying Ambiguities in Artistic Images", "comments": null, "journal-ref": "ACM Trans. Applied Perception, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been hypothesized that perceptual ambiguities play an important\nrole in aesthetic experience: a work with some ambiguity engages a viewer more\nthan one that does not. However, current frameworks for testing this theory are\nlimited by the availability of stimuli and data collection methods. This paper\npresents an approach to measuring the perceptual ambiguity of a collection of\nimages. Crowdworkers are asked to describe image content, after different\nviewing durations. Experiments are performed using images created with\nGenerative Adversarial Networks, using the Artbreeder website. We show that\ntext processing of viewer responses can provide a fine-grained way to measure\nand describe image ambiguities.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 21:40:16 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Wang", "Xi", ""], ["Bylinskii", "Zoya", ""], ["Hertzmann", "Aaron", ""], ["Pepperell", "Robert", ""]]}, {"id": "2008.09830", "submitter": "Takayuki Itoh", "authors": "Hinako Sassa, Maxime Cordeil, Mitsuo Yoshida, Takayuki Itoh", "title": "Brushing Feature Values in Immersive Graph Visualization Environment", "comments": "5 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are a variety of graphs where multidimensional feature values are\nassigned to the nodes. Visualization of such datasets is not an easy task since\nthey are complex and often huge. Immersive Analytics is a powerful approach to\nsupport the interactive exploration of such large and complex data. Many recent\nstudies on graph visualization have applied immersive analytics frameworks.\nHowever, there have been few studies on immersive analytics for visualization\nof multidimensional attributes associated with the input graphs. This paper\npresents a new immersive analytics system that supports the interactive\nexploration of multidimensional feature values assigned to the nodes of input\ngraphs. The presented system displays label-axes corresponding to the\ndimensions of feature values, and label-edges that connect label-axes and\ncorresponding to the nodes. The system supports brushing operations which\ncontrols the display of edges that connect a label-axis and nodes of the graph.\nThis paper introduces visualization examples with a graph dataset of Twitter\nusers and reviews by experts on graph data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 13:14:27 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Sassa", "Hinako", ""], ["Cordeil", "Maxime", ""], ["Yoshida", "Mitsuo", ""], ["Itoh", "Takayuki", ""]]}, {"id": "2008.09883", "submitter": "Ghalib Tahir", "authors": "Ghalib Ahmed Tahir, Chu Kiong Loo, Foong Ming Moy and Nadine Kong", "title": "A Review of Critical Features and General Issues of Freely Available\n  mHealth Apps For Dietary Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obesity is known to lower the quality of life substantially. It is often\nassociated with increased chances of non-communicable diseases such as\ndiabetes, cardiovascular problems, various cancers, etc. Evidence suggests that\ndiet-related mobile applications play a vital role in assisting individuals in\nmaking healthier choices and keeping track of food intake. However, due to an\nabundance of similar applications, it becomes pertinent to evaluate each of\nthem in terms of functionality, usability, and possible design issues to truly\ndetermine state-of-the-art solutions for the future. Since these applications\ninvolve implementing multiple user requirements and recommendations from\ndifferent dietitians, the evaluation becomes quite complex. Therefore, this\nstudy aims to review existing dietary applications at length to highlight key\nfeatures and problems that enhance or undermine an application's usability. For\nthis purpose, we have examined the published literature from various scientific\ndatabases of the PUBMED, CINAHL (January 2010-December 2019) and Science Direct\n(2010-2019). We followed PRISMA guidelines, and out of our findings, fifty-six\nprimary studies met our inclusion criteria after identification, screening,\neligibility and full-text evaluation. We analyzed 35 apps from the selected\nstudies and extracted the data of each of the identified apps.Following our\ndetailed analysis on the comprehensiveness of freely available mHealth\napplications, we specified potential future research challenges and stated\nrecommendations to help grow clinically accurate diet-related applications.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 17:27:49 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 08:42:46 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 18:15:37 GMT"}, {"version": "v4", "created": "Sun, 11 Jul 2021 15:03:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Tahir", "Ghalib Ahmed", ""], ["Loo", "Chu Kiong", ""], ["Moy", "Foong Ming", ""], ["Kong", "Nadine", ""]]}, {"id": "2008.09941", "submitter": "Yalong Yang", "authors": "Yalong Yang, Maxime Cordeil, Johanna Beyer, Tim Dwyer, Kim Marriott,\n  Hanspeter Pfister", "title": "Embodied Navigation in Immersive Abstract Data Visualization: Is\n  Overview+Detail or Zooming Better for 3D Scatterplots?", "comments": "To be presented at IEEE Conference on Information Visualization\n  (InfoVis 2020)", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030427", "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract data has no natural scale and so interactive data visualizations\nmust provide techniques to allow the user to choose their viewpoint and scale.\nSuch techniques are well established in desktop visualization tools. The two\nmost common techniques are zoom+pan and overview+detail. However, how best to\nenable the analyst to navigate and view abstract data at different levels of\nscale in immersive environments has not previously been studied. We report the\nfindings of the first systematic study of immersive navigation techniques for\n3D scatterplots. We tested four conditions that represent our best attempt to\nadapt standard 2D navigation techniques to data visualization in an immersive\nenvironment while still providing standard immersive navigation techniques\nthrough physical movement and teleportation. We compared room-sized\nvisualization versus a zooming interface, each with and without an overview. We\nfind significant differences in participants' response times and accuracy for a\nnumber of standard visual analysis tasks. Both zoom and overview provide\nbenefits over standard locomotion support alone (i.e., physical movement and\npointer teleportation). However, which variation is superior, depends on the\ntask. We obtain a more nuanced understanding of the results by analyzing them\nin terms of a time-cost model for the different components of navigation:\nway-finding, travel, number of travel steps, and context switching.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 01:41:14 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Yang", "Yalong", ""], ["Cordeil", "Maxime", ""], ["Beyer", "Johanna", ""], ["Dwyer", "Tim", ""], ["Marriott", "Kim", ""], ["Pfister", "Hanspeter", ""]]}, {"id": "2008.10039", "submitter": "Toshiyuki Yokoyama", "authors": "Toshiyuki T. Yokoyama, Masashi Okada, Tadahiro Taniguchi", "title": "Visual Exploration System for Analyzing Trends in Annual Recruitment\n  Using Time-varying Graphs", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0247587", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annual recruitment data of new graduates are manually analyzed by human\nresources specialists (HR) in industries, which signifies the need to evaluate\nthe recruitment strategy of HR specialists. Every year, different applicants\nsend in job applications to companies. The relationships between applicants'\nattributes (e.g., English skill or academic credential) can be used to analyze\nthe changes in recruitment trends across multiple years' data. However, most\nattributes are unnormalized and thus require thorough preprocessing. Such\nunnormalized data hinder the effective comparison of the relationship between\napplicants in the early stage of data analysis. Thus, a visual exploration\nsystem is highly needed to gain insight from the overview of the relationship\nbetween applicants across multiple years. In this study, we propose the\nPolarizing Attributes for Network Analysis of Correlation on Entities\nAssociation (Panacea) visualization system. The proposed system integrates a\ntime-varying graph model and dynamic graph visualization for heterogeneous\ntabular data. Using this system, human resource specialists can interactively\ninspect the relationships between two attributes of prospective employees\nacross multiple years. Further, we demonstrate the usability of Panacea with\nrepresentative examples for finding hidden trends in real-world datasets and\nthen describe HR specialists' feedback obtained throughout Panacea's\ndevelopment. The proposed Panacea system enables HR specialists to visually\nexplore the annual recruitment of new graduates.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 13:26:29 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Yokoyama", "Toshiyuki T.", ""], ["Okada", "Masashi", ""], ["Taniguchi", "Tadahiro", ""]]}, {"id": "2008.10122", "submitter": "Varun Badrinath Krishna", "authors": "Varun Badrinath Krishna", "title": "Ballroom Dance Movement Recognition Using a Smart Watch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inertial Measurement Unit (IMU) sensors are being increasingly used to detect\nhuman gestures and movements. Using a single IMU sensor, whole body movement\nrecognition remains a hard problem because movements may not be adequately\ncaptured by the sensor. In this paper, we present a whole body movement\ndetection study using a single smart watch in the context of ballroom dancing.\nDeep learning representations are used to classify well-defined sequences of\nmovements, called \\emph{figures}. Those representations are found to outperform\nensembles of random forests and hidden Markov models. The classification\naccuracy of 85.95\\% was improved to 92.31\\% by modeling a dance as a\nfirst-order Markov chain of figures and correcting estimates of the immediately\npreceding figure.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 22:36:28 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 05:25:56 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Krishna", "Varun Badrinath", ""]]}, {"id": "2008.10148", "submitter": "Md. Shirajum Munir", "authors": "Md. Shirajum Munir, Sarder Fakhrul Abedin, Ki Tae Kim, Do Hyeon Kim,\n  Md. Golam Rabiul Alam, and Choong Seon Hong", "title": "Drive Safe: Cognitive-Behavioral Mining for Intelligent Transportation\n  Cyber-Physical System", "comments": "Submitted to IEEE Transactions on Intelligent Transportation Systems,\n  Special Issue on Technologies for risk mitigation and support of impaired\n  drivers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a cognitive behavioral-based driver mood repairment\nplatform in intelligent transportation cyber-physical systems (IT-CPS) for road\nsafety. In particular, we propose a driving safety platform for distracted\ndrivers, namely \\emph{drive safe}, in IT-CPS. The proposed platform recognizes\nthe distracting activities of the drivers as well as their emotions for mood\nrepair. Further, we develop a prototype of the proposed drive safe platform to\nestablish proof-of-concept (PoC) for the road safety in IT-CPS. In the\ndeveloped driving safety platform, we employ five AI and statistical-based\nmodels to infer a vehicle driver's cognitive-behavioral mining to ensure safe\ndriving during the drive. Especially, capsule network (CN), maximum likelihood\n(ML), convolutional neural network (CNN), Apriori algorithm, and Bayesian\nnetwork (BN) are deployed for driver activity recognition, environmental\nfeature extraction, mood recognition, sequential pattern mining, and content\nrecommendation for affective mood repairment of the driver, respectively.\nBesides, we develop a communication module to interact with the systems in\nIT-CPS asynchronously. Thus, the developed drive safe PoC can guide the vehicle\ndrivers when they are distracted from driving due to the cognitive-behavioral\nfactors. Finally, we have performed a qualitative evaluation to measure the\nusability and effectiveness of the developed drive safe platform. We observe\nthat the P-value is 0.0041 (i.e., < 0.05) in the ANOVA test. Moreover, the\nconfidence interval analysis also shows significant gains in prevalence value\nwhich is around 0.93 for a 95% confidence level. The aforementioned statistical\nresults indicate high reliability in terms of driver's safety and mental state.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 01:19:40 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Munir", "Md. Shirajum", ""], ["Abedin", "Sarder Fakhrul", ""], ["Kim", "Ki Tae", ""], ["Kim", "Do Hyeon", ""], ["Alam", "Md. Golam Rabiul", ""], ["Hong", "Choong Seon", ""]]}, {"id": "2008.10282", "submitter": "Praphula Jain Mr.", "authors": "Praphula Kumar Jain and Rajendra Pamula", "title": "A systematic literature review on machine learning applications for\n  consumer sentiment analysis using online reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer sentiment analysis is a recent fad for social media related\napplications such as healthcare, crime, finance, travel, and academics.\nDisentangling consumer perception to gain insight into the desired objective\nand reviews is significant. With the advancement of technology, a massive\namount of social web-data increasing in terms of volume, subjectivity, and\nheterogeneity, becomes challenging to process it manually. Machine learning\ntechniques have been utilized to handle this difficulty in real-life\napplications. This paper presents the study to find out the usefulness, scope,\nand applicability of this alliance of Machine Learning techniques for consumer\nsentiment analysis on online reviews in the domain of hospitality and tourism.\nWe have shown a systematic literature review to compare, analyze, explore, and\nunderstand the attempts and direction in a proper way to find research gaps to\nillustrating the future scope of this pairing. This work is contributing to the\nextant literature in two ways; firstly, the primary objective is to read and\nanalyze the use of machine learning techniques for consumer sentiment analysis\non online reviews in the domain of hospitality and tourism. Secondly, in this\nwork, we presented a systematic approach to identify, collect observational\nevidence, results from the analysis, and assimilate observations of all related\nhigh-quality research to address particular research queries referring to the\ndescribed research area.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 09:30:14 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Jain", "Praphula Kumar", ""], ["Pamula", "Rajendra", ""]]}, {"id": "2008.10640", "submitter": "Julian Pistorius", "authors": "Julian L. Pistorius (The University of Arizona), Chris Martin\n  (Exosphere Project), Sanjana Sudarshan (Indiana University), David S. LeBauer\n  (The University of Arizona)", "title": "Exosphere -- Bringing The Cloud Closer", "comments": "6 pages, 4 figures, submitted to \"SC20 Workshop - SuperCompCloud: 3rd\n  Workshop on Interoperability of Supercomputing and Cloud Technologies\",\n  https://sites.google.com/view/supercompcloud", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exosphere provides researcher-friendly software for managing computing\nworkloads on OpenStack cloud infrastructure. Exosphere is a user-friendly\nalternative to Horizon, the default OpenStack graphical interface. Exosphere\ncan be used with most research cloud infrastructure, requiring near-zero custom\nintegration work.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 18:19:20 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 22:36:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Pistorius", "Julian L.", "", "The University of Arizona"], ["Martin", "Chris", "", "Exosphere Project"], ["Sudarshan", "Sanjana", "", "Indiana University"], ["LeBauer", "David S.", "", "The University of Arizona"]]}, {"id": "2008.10681", "submitter": "Adam Aviv", "authors": "Timothy J. Forman and Adam J. Aviv", "title": "Double Patterns: A Usable Solution to Increase the Security of Android\n  Unlock Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Android unlock patterns remain quite common. Our study, as well as others,\nfinds that roughly 25\\% of respondents use a pattern when unlocking their\nphone. Despite known security issues, the design of the pattern interface\nremains unchanged since first launch. We propose Double Patterns, a natural and\neasily adoptable advancement on Android unlock patterns that maintains the core\ndesign features, but instead of selecting a single pattern, a user selects two,\nconcurrent Android unlock patterns entered one-after-the-other super-imposed on\nthe same 3x3 grid. We evaluated Double Patterns for both security and usability\nby conducting an online study with $n=634$ participants in three treatments: a\ncontrol treatment, a first pattern entry blocklist, and a blocklist for both\npatterns. We find that in all settings, user chosen Double Patterns are more\nsecure than traditional patterns based on standard guessability metrics, more\nsimilar to that of 4-/6-digit PINs, and even more difficult to guess for a\nsimulated attacker. Users express positive sentiments in qualitative feedback,\nparticularly those who currently (or previously) used Android unlock patterns,\nand overall, participants found the Double Pattern interface quite usable, with\nhigh recall retention and comparable entry times to traditional patterns. In\nparticular, current Android pattern users, the target population for Double\nPatterns, reported SUS scores in the 80th percentile and high perceptions of\nsecurity and usability in responses to open- and closed-questions. Based on\nthese findings, we would recommend adding Double Patterns as an advancement to\nAndroid patterns, much like allowing for added PIN length.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 20:02:39 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Forman", "Timothy J.", ""], ["Aviv", "Adam J.", ""]]}, {"id": "2008.10697", "submitter": "Adam Aviv", "authors": "Hassan Khan and Jason Ceci and Jonah Stegman and Adam J. Aviv and\n  Rozita Dara and Ravi Kuber", "title": "Widely Reused and Shared, Infrequently Updated, and Sometimes Inherited:\n  A Holistic View of PIN Authentication in Digital Lives and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personal Identification Numbers (PINs) are widely used as an access control\nmechanism for digital assets (e.g., smartphones), financial assets (e.g., ATM\ncards), and physical assets (e.g., locks for garage doors or homes). Using\nsemi-structured interviews (n=35), participants reported on PIN usage for\ndifferent types of assets, including how users choose, share, inherit, and\nreuse PINs, as well as behaviour following the compromise of a PIN. We find\nthat memorability is the most important criterion when choosing a PIN, more so\nthan security or concerns of reuse. Updating or changing a PIN is very\nuncommon, even when a PIN is compromised. Participants reported sharing PINs\nfor one type of asset with acquaintances but inadvertently reused them for\nother assets, thereby subjecting themselves to potential risks. Participants\nalso reported using PINs originally set by previous homeowners for physical\ndevices (e.g., alarm or keypad door entry systems). While aware of the risks of\nnot updating PINs, this did not always deter participants from using inherited\nPINs, as they were often missing instructions on how to update them. %While\naware of the risks of not updating PINs, participants continued using these\nPINs, as they were often missing instructions on how to update them.Given the\nexpected increase in PIN-protected assets (e.g., loyalty cards, smart locks,\nand web apps), we provide suggestions and future research directions to better\nsupport users with multiple digital and non-digital assets and more secure\nhuman-device interaction when utilizing PINs.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 20:29:59 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Khan", "Hassan", ""], ["Ceci", "Jason", ""], ["Stegman", "Jonah", ""], ["Aviv", "Adam J.", ""], ["Dara", "Rozita", ""], ["Kuber", "Ravi", ""]]}, {"id": "2008.10723", "submitter": "Arpit Narechania", "authors": "Arpit Narechania, Arjun Srinivasan, and John Stasko", "title": "NL4DV: A Toolkit for Generating Analytic Specifications for Data\n  Visualization from Natural Language Queries", "comments": "11 pages, 10 figures. Proceedings of IEEE VIS'2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030378", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language interfaces (NLIs) have shown great promise for visual data\nanalysis, allowing people to flexibly specify and interact with visualizations.\nHowever, developing visualization NLIs remains a challenging task, requiring\nlow-level implementation of natural language processing (NLP) techniques as\nwell as knowledge of visual analytic tasks and visualization design. We present\nNL4DV, a toolkit for natural language-driven data visualization. NL4DV is a\nPython package that takes as input a tabular dataset and a natural language\nquery about that dataset. In response, the toolkit returns an analytic\nspecification modeled as a JSON object containing data attributes, analytic\ntasks, and a list of Vega-Lite specifications relevant to the input query. In\ndoing so, NL4DV aids visualization developers who may not have a background in\nNLP, enabling them to create new visualization NLIs or incorporate natural\nlanguage input within their existing systems. We demonstrate NL4DV's usage and\ncapabilities through four examples: 1) rendering visualizations using natural\nlanguage in a Jupyter notebook, 2) developing a NLI to specify and edit\nVega-Lite charts, 3) recreating data ambiguity widgets from the DataTone\nsystem, and 4) incorporating speech input to create a multimodal visualization\nsystem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 21:54:43 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 17:12:32 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 20:58:46 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Narechania", "Arpit", ""], ["Srinivasan", "Arjun", ""], ["Stasko", "John", ""]]}, {"id": "2008.10759", "submitter": "Connor Brooks", "authors": "Connor Brooks and Daniel Szafir", "title": "Visualization of Intended Assistance for Acceptance of Shared Control", "comments": "To appear at IROS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In shared control, advances in autonomous robotics are applied to help\nempower a human user in operating a robotic system. While these systems have\nbeen shown to improve efficiency and operation success, users are not always\naccepting of the new control paradigm produced by working with an assistive\ncontroller. This mismatch between performance and acceptance can prevent users\nfrom taking advantage of the benefits of shared control systems for robotic\noperation. To address this mismatch, we develop multiple types of\nvisualizations for improving both the legibility and perceived predictability\nof assistive controllers, then conduct a user study to evaluate the impact that\nthese visualizations have on user acceptance of shared control systems. Our\nresults demonstrate that shared control visualizations must be designed\ncarefully to be effective, with users requiring visualizations that improve\nboth legibility and predictability of the assistive controller in order to\nvoluntarily relinquish control.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 00:30:17 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Brooks", "Connor", ""], ["Szafir", "Daniel", ""]]}, {"id": "2008.10772", "submitter": "Ben Kaiser", "authors": "Ben Kaiser, Jerry Wei, Elena Lucherini, Kevin Lee, J. Nathan Matias,\n  Jonathan Mayer", "title": "Adapting Security Warnings to Counter Online Disinformation", "comments": "To be published in USENIX Security '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disinformation is proliferating on the internet, and platforms are responding\nby attaching warnings to content. There is little evidence, however, that these\nwarnings help users identify or avoid disinformation. In this work, we adapt\nmethods and results from the information security warning literature in order\nto design and evaluate effective disinformation warnings.\n  In an initial laboratory study, we used a simulated search task to examine\ncontextual and interstitial disinformation warning designs. We found that users\nroutinely ignore contextual warnings, but users notice interstitial\nwarnings--and respond by seeking information from alternative sources.\n  We then conducted a follow-on crowdworker study with eight interstitial\nwarning designs. We confirmed a significant impact on user information-seeking\nbehavior, and we found that a warning's design could effectively inform users\nor convey a risk of harm. We also found, however, that neither user\ncomprehension nor fear of harm moderated behavioral effects.\n  Our work provides evidence that disinformation warnings can -- when designed\nwell -- help users identify and avoid disinformation. We show a path forward\nfor designing effective warnings, and we contribute repeatable methods for\nevaluating behavioral effects. We also surface a possible dilemma:\ndisinformation warnings might be able to inform users and guide behavior, but\nthe behavioral effects might result from user experience friction, not informed\ndecision making.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 01:10:57 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 15:56:11 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2020 22:55:30 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 15:36:03 GMT"}, {"version": "v5", "created": "Fri, 16 Oct 2020 18:19:45 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Kaiser", "Ben", ""], ["Wei", "Jerry", ""], ["Lucherini", "Elena", ""], ["Lee", "Kevin", ""], ["Matias", "J. Nathan", ""], ["Mayer", "Jonathan", ""]]}, {"id": "2008.10915", "submitter": "Di Weng", "authors": "Di Weng, Chengbo Zheng, Zikun Deng, Mingze Ma, Jie Bao, Yu Zheng,\n  Mingliang Xu, Yingcai Wu", "title": "Towards Better Bus Networks: A Visual Analytics Approach", "comments": "IEEE VIS VAST 2020", "journal-ref": null, "doi": "10.1109/TVCG.2020.3030458", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bus routes are typically updated every 3-5 years to meet constantly changing\ntravel demands. However, identifying deficient bus routes and finding their\noptimal replacements remain challenging due to the difficulties in analyzing a\ncomplex bus network and the large solution space comprising alternative routes.\nMost of the automated approaches cannot produce satisfactory results in\nreal-world settings without laborious inspection and evaluation of the\ncandidates. The limitations observed in these approaches motivate us to\ncollaborate with domain experts and propose a visual analytics solution for the\nperformance analysis and incremental planning of bus routes based on an\nexisting bus network. Developing such a solution involves three major\nchallenges, namely, a) the in-depth analysis of complex bus route networks, b)\nthe interactive generation of improved route candidates, and c) the effective\nevaluation of alternative bus routes. For challenge a, we employ an\noverview-to-detail approach by dividing the analysis of a complex bus network\ninto three levels to facilitate the efficient identification of deficient\nroutes. For challenge b, we improve a route generation model and interpret the\nperformance of the generation with tailored visualizations. For challenge c, we\nincorporate a conflict resolution strategy in the progressive decision-making\nprocess to assist users in evaluating the alternative routes and finding the\nmost optimal one. The proposed system is evaluated with two usage scenarios\nbased on real-world data and received positive feedback from the experts.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 09:49:38 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 12:22:51 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 09:25:11 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Weng", "Di", ""], ["Zheng", "Chengbo", ""], ["Deng", "Zikun", ""], ["Ma", "Mingze", ""], ["Bao", "Jie", ""], ["Zheng", "Yu", ""], ["Xu", "Mingliang", ""], ["Wu", "Yingcai", ""]]}, {"id": "2008.11015", "submitter": "Mengyu Zhou", "authors": "Mengyu Zhou, Qingtao Li, Xinyi He, Yuejiang Li, Yibo Liu, Wei Ji, Shi\n  Han, Yining Chen, Daxin Jiang, Dongmei Zhang", "title": "Table2Charts: Recommending Charts by Learning Shared Table\n  Representations", "comments": "9 + 2(appendix) pages, accepted by KDD'21 conference", "journal-ref": null, "doi": "10.1145/3447548.3467279", "report-no": null, "categories": "cs.DB cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common for people to create different types of charts to explore a\nmulti-dimensional dataset (table). However, to recommend commonly composed\ncharts in real world, one should take the challenges of efficiency, imbalanced\ndata and table context into consideration. In this paper, we propose\nTable2Charts framework which learns common patterns from a large corpus of\n(table, charts) pairs. Based on deep Q-learning with copying mechanism and\nheuristic searching, Table2Charts does table-to-sequence generation, where each\nsequence follows a chart template. On a large spreadsheet corpus with 165k\ntables and 266k charts, we show that Table2Charts could learn a shared\nrepresentation of table fields so that recommendation tasks on different chart\ntypes could mutually enhance each other. Table2Charts outperforms other chart\nrecommendation systems in both multi-type task (with doubled recall numbers\nR@3=0.61 and R@1=0.43) and human evaluations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 15:06:26 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 18:21:42 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 14:08:32 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 11:57:20 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhou", "Mengyu", ""], ["Li", "Qingtao", ""], ["He", "Xinyi", ""], ["Li", "Yuejiang", ""], ["Liu", "Yibo", ""], ["Ji", "Wei", ""], ["Han", "Shi", ""], ["Chen", "Yining", ""], ["Jiang", "Daxin", ""], ["Zhang", "Dongmei", ""]]}, {"id": "2008.11069", "submitter": "Volker Strobel", "authors": "Volker Strobel and Alexandra Kirsch", "title": "MyPDDL: Tools for efficiently creating PDDL domains and problems", "comments": "In: Vallati M., Kitchin D. (eds) Knowledge Engineering Tools and\n  Techniques for AI Planning. Springer, Cham (2020). arXiv admin note: text\n  overlap with arXiv:1511.07500", "journal-ref": null, "doi": "10.1007/978-3-030-38561-3_4", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Planning Domain Definition Language (PDDL) is the state-of-the-art\nlanguage for specifying planning problems in artificial intelligence research.\nWriting and maintaining these planning problems, however, can be time-consuming\nand error prone. To address this issue, we present myPDDL-a modular toolkit for\ndeveloping and manipulating PDDL domains and problems. To evaluate myPDDL, we\ncompare its features to existing knowledge engineering tools for PDDL. In a\nuser test, we additionally assess two of its modules, namely the syntax\nhighlighting feature and the type diagram generator. The users of syntax\nhighlighting detected 36% more errors than non-users in an erroneous domain\nfile. The average time on task for questions on a PDDL type hierarchy was\nreduced by 48% when making the type diagram generator available. This implies\nthat myPDDL can support knowledge engineers well in the PDDL design and\nanalysis process.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 11:28:25 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Strobel", "Volker", ""], ["Kirsch", "Alexandra", ""]]}, {"id": "2008.11147", "submitter": "Sonia Jaffe", "authors": "Denae Ford and Margaret-Anne Storey and Thomas Zimmermann and\n  Christian Bird and Sonia Jaffe and Chandra Maddila and Jenna L. Butler and\n  Brian Houck and Nachiappan Nagappan", "title": "A Tale of Two Cities: Software Developers Working from Home During the\n  COVID-19 Pandemic", "comments": "35 pages, 1 figure, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The COVID-19 pandemic has shaken the world to its core and has provoked an\novernight exodus of developers that normally worked in an office setting to\nworking from home. The magnitude of this shift and the factors that have\naccompanied this new unplanned work setting go beyond what the software\nengineering community has previously understood to be remote work. To find out\nhow developers and their productivity were affected, we distributed two surveys\n(with a combined total of 3,634 responses that answered all required questions)\n-- weeks apart to understand the presence and prevalence of the benefits,\nchallenges, and opportunities to improve this special circumstance of remote\nwork. From our thematic qualitative analysis and statistical quantitative\nanalysis, we find that there is a dichotomy of developer experiences influenced\nby many different factors (that for some are a benefit, while for others a\nchallenge). For example, a benefit for some was being close to family members\nbut for others having family members share their working space and interrupting\ntheir focus, was a challenge. Our surveys led to powerful narratives from\nrespondents and revealed the scale at which these experiences exist to provide\ninsights as to how the future of (pandemic) remote work can evolve.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:27:21 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 18:36:05 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Ford", "Denae", ""], ["Storey", "Margaret-Anne", ""], ["Zimmermann", "Thomas", ""], ["Bird", "Christian", ""], ["Jaffe", "Sonia", ""], ["Maddila", "Chandra", ""], ["Butler", "Jenna L.", ""], ["Houck", "Brian", ""], ["Nagappan", "Nachiappan", ""]]}, {"id": "2008.11226", "submitter": "Ce Zhang Mr.", "authors": "Ce Zhang, Azim Eskandarian", "title": "A Survey and Tutorial of EEG-Based Brain Monitoring for Driver State\n  Analysis", "comments": "Accepted by IEEE/CAA Journal of Automatica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drivers cognitive and physiological states affect their ability to control\ntheir vehicles. Thus, these driver states are important to the safety of\nautomobiles. The design of advanced driver assistance systems (ADAS) or\nautonomous vehicles will depend on their ability to interact effectively with\nthe driver. A deeper understanding of the driver state is, therefore,\nparamount. EEG is proven to be one of the most effective methods for driver\nstate monitoring and human error detection. This paper discusses EEG-based\ndriver state detection systems and their corresponding analysis algorithms over\nthe last three decades. First, the commonly used EEG system setup for driver\nstate studies is introduced. Then, the EEG signal preprocessing, feature\nextraction, and classification algorithms for driver state detection are\nreviewed. Finally, EEG-based driver state monitoring research is reviewed\nin-depth, and its future development is discussed. It is concluded that the\ncurrent EEG-based driver state monitoring algorithms are promising for safety\napplications. However, many improvements are still required in EEG artifact\nreduction, real-time processing, and between-subject classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 18:21:35 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Zhang", "Ce", ""], ["Eskandarian", "Azim", ""]]}, {"id": "2008.11250", "submitter": "Michael Correll", "authors": "Michael Correll", "title": "What Do We Actually Learn from Evaluations in the \"Heroic Era\" of\n  Visualization?", "comments": null, "journal-ref": null, "doi": "10.1109/BELIV51497.2020.00013", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We often point to the relative increase in the amount and sophistication of\nevaluations of visualization systems versus the earliest days of the field as\nevidence that we are maturing as a field. I am not so convinced. In particular,\nI feel that evaluations of visualizations, as they are ordinarily performed in\nthe field or asked for by reviewers, fail to tell us very much that is useful\nor transferable about visualization systems, regardless of the statistical\nrigor or ecological validity of the evaluation. Through a series of thought\nexperiments, I show how our current conceptions of visualization evaluations\ncan be incomplete, capricious, or useless for the goal of furthering the field,\nmore in line with the \"heroic age\" of medical science than the rigorous\nevidence-based field we might aspire to be. I conclude by suggesting that our\nmodels for designing evaluations, and our priorities as a field, should be\nrevisited.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 20:03:00 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Correll", "Michael", ""]]}, {"id": "2008.11282", "submitter": "Andrew Wentzel", "authors": "Andrew Wentzel, Guadalupe Canahuate, Lisanne van Dijk, Abdallah\n  Mohamed, Clifton David Fuller, G.Elisabeta Marai", "title": "Explainable Spatial Clustering: Leveraging Spatial Data in Radiation\n  Oncology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in data collection in radiation therapy have led to an abundance of\nopportunities for applying data mining and machine learning techniques to\npromote new data-driven insights. In light of these advances, supporting\ncollaboration between machine learning experts and clinicians is important for\nfacilitating better development and adoption of these models. Although many\nmedical use-cases rely on spatial data, where understanding and visualizing the\nunderlying structure of the data is important, little is known about the\ninterpretability of spatial clustering results by clinical audiences. In this\nwork, we reflect on the design of visualizations for explaining novel\napproaches to clustering complex anatomical data from head and neck cancer\npatients. These visualizations were developed, through participatory design,\nfor clinical audiences during a multi-year collaboration with radiation\noncologists and statisticians. We distill this collaboration into a set of\nlessons learned for creating visual and explainable spatial clustering for\nclinical users.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 21:31:41 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 21:38:25 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Wentzel", "Andrew", ""], ["Canahuate", "Guadalupe", ""], ["van Dijk", "Lisanne", ""], ["Mohamed", "Abdallah", ""], ["Fuller", "Clifton David", ""], ["Marai", "G. Elisabeta", ""]]}, {"id": "2008.11310", "submitter": "Michael Correll", "authors": "Enrico Bertini, Michael Correll, Steven Franconeri", "title": "Why Shouldn't All Charts Be Scatter Plots? Beyond Precision-Driven\n  Visualizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A central concept in information visualization research and practice is the\nnotion of visual variable effectiveness, or the perceptual precision at which\nvalues are decoded given visual channels of encoding. Formative work from\nCleveland & McGill has shown that position along a common axis is the most\neffective visual variable for comparing individual values. One natural\nconclusion is that any chart that is not a dot plot or scatterplot is deficient\nand should be avoided. In this paper we refute a caricature of this\n\"scatterplots only\" argument as a way to call for new perspectives on how\ninformation visualization is researched, taught, and evaluated.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 23:51:09 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 19:26:36 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 21:06:28 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Bertini", "Enrico", ""], ["Correll", "Michael", ""], ["Franconeri", "Steven", ""]]}, {"id": "2008.11319", "submitter": "Haotian Li", "authors": "Ka Wing Tsang and Haotian Li and Fuk Ming Lam and Yifan Mu and Yong\n  Wang and Huamin Qu", "title": "TradAO: A Visual Analytics System for Trading Algorithm Optimization", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the wide applications of algorithmic trading, it has become critical for\ntraders to build a winning trading algorithm to beat the market. However, due\nto the lack of efficient tools, traders mainly rely on their memory to manually\ncompare the algorithm instances of a trading algorithm and further select the\nbest trading algorithm instance for the real trading deployment. We work\nclosely with industry practitioners to discover and consolidate user\nrequirements and develop an interactive visual analytics system for trading\nalgorithm optimization. Structured expert interviews are conducted to\nevaluateTradAOand a representative case study is documented for illustrating\nthe system effectiveness. To the best of our knowledge, previous financial data\nvisual analyses have mainly aimed to assist investment managers in investment\nportfolio analysis but have neglected the need of traders in developing trading\nalgorithms for portfolio execution.TradAOis the first visual analytics system\nthat assists users in comprehensively exploring the performances of a trading\nalgorithm with different parameter settings.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 00:49:22 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Tsang", "Ka Wing", ""], ["Li", "Haotian", ""], ["Lam", "Fuk Ming", ""], ["Mu", "Yifan", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2008.11564", "submitter": "Jennifer Rogers", "authors": "Jen Rogers, Austin H. Patton, Luke Harmon, Alexander Lex, Miriah Meyer", "title": "Insights From Experiments With Rigor in an EvoBio Design Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Design study is an established approach of conducting problem-driven\nvisualization research. The academic visualizationcommunity has produced a\nlarge body of work for reporting on design studies, informed by a handful of\ntheoretical frameworks, andapplied to a broad range of application areas. The\nresult is an abundance of reported insights into visualization design, with\nanemphasis on novel visualization techniques and systems as the primary\ncontribution of these studies. In recent work we proposeda new, interpretivist\nperspective on design study and six companion criteria for rigor that highlight\nthe opportunities for researchersto contribute knowledge that extends beyond\nvisualization idioms and software. In this work we conducted a year-long\ncollaborationwith evolutionary biologists to develop an interactive tool for\nvisual exploration of multivariate datasets and phylogenetic trees. Duringthis\ndesign study we experimented with methods to support three of the rigor\ncriteria:ABUNDANT,REFLEXIVE, andTRANSPARENT. As aresult we contribute two novel\nvisualization techniques for the analysis of multivariate phylogenetic\ndatasets, three methodologicalrecommendations for conducting design studies\ndrawn from reflections over our process of experimentation, and two writing\ndevices forreporting interpretivist design study. We offer this work as an\nexample for implementing the rigor criteria to produce a diverse range\nofknowledge contributions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 13:50:08 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Rogers", "Jen", ""], ["Patton", "Austin H.", ""], ["Harmon", "Luke", ""], ["Lex", "Alexander", ""], ["Meyer", "Miriah", ""]]}, {"id": "2008.11695", "submitter": "Vignesh Prasad", "authors": "Vignesh Prasad, Ruth Stock-Homburg, Jan Peters", "title": "Advances in Human-Robot Handshaking", "comments": "Accepted at The 12th International Conference on Social Robotics\n  (ICSR 2020) 12 Pages, 1 Figure", "journal-ref": null, "doi": "10.1007/978-3-030-62056-1_40", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The use of social, anthropomorphic robots to support humans in various\nindustries has been on the rise. During Human-Robot Interaction (HRI),\nphysically interactive non-verbal behaviour is key for more natural\ninteractions. Handshaking is one such natural interaction used commonly in many\nsocial contexts. It is one of the first non-verbal interactions which takes\nplace and should, therefore, be part of the repertoire of a social robot. In\nthis paper, we explore the existing state of Human-Robot Handshaking and\ndiscuss possible ways forward for such physically interactive behaviours.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 17:35:06 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Prasad", "Vignesh", ""], ["Stock-Homburg", "Ruth", ""], ["Peters", "Jan", ""]]}, {"id": "2008.11721", "submitter": "Hua Shen", "authors": "Hua Shen and Ting-Hao Kenneth Huang", "title": "How Useful Are the Machine-Generated Interpretations to General Users? A\n  Human Evaluation on Guessing the Incorrectly Predicted Labels", "comments": "Accepted by The 8th AAAI Conference on Human Computation and\n  Crowdsourcing (HCOMP 2020) https://github.com/huashen218/GuessWrongLabel", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining to users why automated systems make certain mistakes is important\nand challenging. Researchers have proposed ways to automatically produce\ninterpretations for deep neural network models. However, it is unclear how\nuseful these interpretations are in helping users figure out why they are\ngetting an error. If an interpretation effectively explains to users how the\nunderlying deep neural network model works, people who were presented with the\ninterpretation should be better at predicting the model's outputs than those\nwho were not. This paper presents an investigation on whether or not showing\nmachine-generated visual interpretations helps users understand the incorrectly\npredicted labels produced by image classifiers. We showed the images and the\ncorrect labels to 150 online crowd workers and asked them to select the\nincorrectly predicted labels with or without showing them the machine-generated\nvisual interpretations. The results demonstrated that displaying the visual\ninterpretations did not increase, but rather decreased, the average guessing\naccuracy by roughly 10%.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 14:02:05 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 02:42:23 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Shen", "Hua", ""], ["Huang", "Ting-Hao Kenneth", ""]]}, {"id": "2008.11785", "submitter": "Guy Marshall", "authors": "Guy Clarke Marshall, Caroline Jay and Andr\\'e Freitas", "title": "Understanding scholarly Natural Language Processing system diagrams\n  through application of the Richards-Engelhardt framework", "comments": "16 pages, 5 figures, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilise Richards-Engelhardt framework as a tool for understanding Natural\nLanguage Processing systems diagrams. Through four examples from scholarly\nproceedings, we find that the application of the framework to this ecological\nand complex domain is effective for reflecting on these diagrams. We argue for\nvocabulary to describe multiple-codings, semiotic variability, and\ninconsistency or misuse of visual encoding principles in diagrams. Further, for\napplication to scholarly Natural Language Processing systems, and perhaps\nsystems diagrams more broadly, we propose the addition of \"Grouping by Object\"\nas a new visual encoding principle, and \"Emphasising\" as a new visual encoding\ntype.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 20:06:30 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Marshall", "Guy Clarke", ""], ["Jay", "Caroline", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2008.11834", "submitter": "Adam Williams", "authors": "Adam S. Williams, Sarah Coler, Francisco Ortega", "title": "Conversations On Multimodal Input Design With Older Adults", "comments": "Presented at the CHI 2020 Designing Interactions for the Ageing\n  Populations Addressing Global Challenges Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal input systems can help bridge the wide range of physical abilities\nfound in older generations. After conducting a survey/interview session with a\ngroup of older adults at an assisted living community we believe that gesture\nand speech should be the main inputs for that system. Additionally,\ncollaborative design of new systems was found to be useful for facilitating\nconversations around input design with this demographic.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 21:46:45 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Williams", "Adam S.", ""], ["Coler", "Sarah", ""], ["Ortega", "Francisco", ""]]}, {"id": "2008.11844", "submitter": "Zhiyan Zhou", "authors": "Siwei Li, Zhiyan Zhou, Anish Upadhayay, Omar Shaikh, Scott Freitas,\n  Haekyu Park, Zijie J. Wang, Susanta Routray, Matthew Hull and Duen Horng Chau", "title": "Argo Lite: Open-Source Interactive Graph Exploration and Visualization\n  in Browsers", "comments": "CIKM'20 Resource Track (October 19-23, 2020), 6 pages, 6 figures", "journal-ref": null, "doi": "10.1145/3340531.3412877", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph data have become increasingly common. Visualizing them helps people\nbetter understand relations among entities. Unfortunately, existing graph\nvisualization tools are primarily designed for single-person desktop use,\noffering limited support for interactive web-based exploration and online\ncollaborative analysis. To address these issues, we have developed Argo Lite, a\nnew in-browser interactive graph exploration and visualization tool. Argo Lite\nenables users to publish and share interactive graph visualizations as URLs and\nembedded web widgets. Users can explore graphs incrementally by adding more\nrelated nodes, such as highly cited papers cited by or citing a paper of\ninterest in a citation network. Argo Lite works across devices and platforms,\nleveraging WebGL for high-performance rendering. Argo Lite has been used by\nover 1,000 students at Georgia Tech's Data and Visual Analytics class. Argo\nLite may serve as a valuable open-source tool for advancing multiple CIKM\nresearch areas, from data presentation, to interfaces for information systems\nand more.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 22:10:34 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Li", "Siwei", ""], ["Zhou", "Zhiyan", ""], ["Upadhayay", "Anish", ""], ["Shaikh", "Omar", ""], ["Freitas", "Scott", ""], ["Park", "Haekyu", ""], ["Wang", "Zijie J.", ""], ["Routray", "Susanta", ""], ["Hull", "Matthew", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2008.11899", "submitter": "Xiao Xie", "authors": "Xiao Xie, Moqi He, Yingcai Wu", "title": "CausalFlow: Visual Analytics of Causality in Event Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the relation of events plays an important role in different\ndomains, such as identifying the reasons for users' certain actions from\napplication logs as well as explaining sports players' behaviors according to\nhistorical records. Co-occurrence has been widely used to characterize the\nrelation of events. However, insights provided by the co-occurrence relation\nare vague, which leads to difficulties in addressing domain problems. In this\npaper, we use causation to model the relation of events and present a\nvisualization approach for conducting the causation analysis of event\nsequences. We integrate automatic causal discovery methods into the approach\nand propose a model for detecting event causalities. Considering the\ninterpretability, we design a novel visualization named causal flow to\nintegrate the detected causality into timeline-based event sequence\nvisualizations. With this design, users can understand the occurrence of\ncertain events and identify the causal pathways. We further implement an\ninteractive system to help users comprehensively analyze event sequences. Two\ncase studies are provided to evaluate the usability of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 03:20:24 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Xie", "Xiao", ""], ["He", "Moqi", ""], ["Wu", "Yingcai", ""]]}, {"id": "2008.11989", "submitter": "Dongming Han", "authors": "Dongming Han, Wei Chen, Rusheng Pan, Yijing Liu, Jiehui Zhou, Ying Xu,\n  Tianye Zhang, Changjie Fan, Jianrong Tao and Xiaolong (Luke) Zhang", "title": "GraphFederator: Federated Visual Analysis for Multi-party Graphs", "comments": "12 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GraphFederator, a novel approach to construct joint\nrepresentations of multi-party graphs and supports privacy-preserving visual\nanalysis of graphs. Inspired by the concept of federated learning, we\nreformulate the analysis of multi-party graphs into a decentralization process.\nThe new federation framework consists of a shared module that is responsible\nfor joint modeling and analysis, and a set of local modules that run on\nrespective graph data. Specifically, we propose a federated graph\nrepresentation model (FGRM) that is learned from encrypted characteristics of\nmulti-party graphs in local modules. We also design multiple visualization\nviews for joint visualization, exploration, and analysis of multi-party graphs.\nExperimental results with two datasets demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 08:36:10 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Han", "Dongming", "", "Luke"], ["Chen", "Wei", "", "Luke"], ["Pan", "Rusheng", "", "Luke"], ["Liu", "Yijing", "", "Luke"], ["Zhou", "Jiehui", "", "Luke"], ["Xu", "Ying", "", "Luke"], ["Zhang", "Tianye", "", "Luke"], ["Fan", "Changjie", "", "Luke"], ["Tao", "Jianrong", "", "Luke"], ["Xiaolong", "", "", "Luke"], ["Zhang", "", ""]]}, {"id": "2008.12024", "submitter": "Alexander Sch\\\"afer", "authors": "Jason Rambach, Gergana Lilligreen, Alexander Sch\\\"afer, Ramya\n  Bankanal, Alexander Wiebel, Didier Stricker", "title": "A survey on applications of augmented, mixed and virtual reality for\n  nature and environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Augmented reality (AR), virtual reality (VR) and mixed reality (MR) are\ntechnologies of great potential due to the engaging and enriching experiences\nthey are capable of providing. Their use is rapidly increasing in diverse\nfields such as medicine, manufacturing or entertainment. However, the\npossibilities that AR, VR and MR offer in the area of environmental\napplications are not yet widely explored. In this paper we present the outcome\nof a survey meant to discover and classify existing AR/VR/MR applications that\ncan benefit the environment or increase awareness on environmental issues. We\nperformed an exhaustive search over several online publication access platforms\nand past proceedings of major conferences in the fields of AR/VR/MR. Identified\nrelevant papers were filtered based on novelty, technical soundness, impact and\ntopic relevance, and classified into different categories. Referring to the\nselected papers, we discuss how the applications of each category are\ncontributing to environmental protection, preservation and sensitization\npurposes. We further analyse these approaches as well as possible future\ndirections in the scope of existing and upcoming AR/VR/MR enabling\ntechnologies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 09:59:27 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 08:47:26 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Rambach", "Jason", ""], ["Lilligreen", "Gergana", ""], ["Sch\u00e4fer", "Alexander", ""], ["Bankanal", "Ramya", ""], ["Wiebel", "Alexander", ""], ["Stricker", "Didier", ""]]}, {"id": "2008.12095", "submitter": "Katya Kudashkina", "authors": "Katya Kudashkina, Patrick M. Pilarski, Richard S. Sutton", "title": "Document-editing Assistants and Model-based Reinforcement Learning as a\n  Path to Conversational AI", "comments": "Currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent assistants that follow commands or answer simple questions, such\nas Siri and Google search, are among the most economically important\napplications of AI. Future conversational AI assistants promise even greater\ncapabilities and a better user experience through a deeper understanding of the\ndomain, the user, or the user's purposes. But what domain and what methods are\nbest suited to researching and realizing this promise? In this article we argue\nfor the domain of voice document editing and for the methods of model-based\nreinforcement learning. The primary advantages of voice document editing are\nthat the domain is tightly scoped and that it provides something for the\nconversation to be about (the document) that is delimited and fully accessible\nto the intelligent assistant. The advantages of reinforcement learning in\ngeneral are that its methods are designed to learn from interaction without\nexplicit instruction and that it formalizes the purposes of the assistant.\nModel-based reinforcement learning is needed in order to genuinely understand\nthe domain of discourse and thereby work efficiently with the user to achieve\ntheir goals. Together, voice document editing and model-based reinforcement\nlearning comprise a promising research direction for achieving conversational\nAI.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 13:05:51 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Kudashkina", "Katya", ""], ["Pilarski", "Patrick M.", ""], ["Sutton", "Richard S.", ""]]}, {"id": "2008.12096", "submitter": "Bernd Dudzik", "authors": "Bernd Dudzik, Joost Broekens, Mark Neerincx, Hayley Hung", "title": "A Blast From the Past: Personalizing Predictions of Video-Induced\n  Emotions using Personal Memories as Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in the accurate prediction of viewers' emotional responses to\nvideo stimuli in real-world applications is accounting for person- and\nsituation-specific variation. An important contextual influence shaping\nindividuals' subjective experience of a video is the personal memories that it\ntriggers in them. Prior research has found that this memory influence explains\nmore variation in video-induced emotions than other contextual variables\ncommonly used for personalizing predictions, such as viewers' demographics or\npersonality. In this article, we show that (1) automatic analysis of text\ndescribing their video-triggered memories can account for variation in viewers'\nemotional responses, and (2) that combining such an analysis with that of a\nvideo's audiovisual content enhances the accuracy of automatic predictions. We\ndiscuss the relevance of these findings for improving on state of the art\napproaches to automated affective video analysis in personalized contexts.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 13:06:10 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Dudzik", "Bernd", ""], ["Broekens", "Joost", ""], ["Neerincx", "Mark", ""], ["Hung", "Hayley", ""]]}, {"id": "2008.12145", "submitter": "William Cheung", "authors": "William Cheung and Sudip Vhaduri", "title": "Context-Dependent Implicit Authentication for Wearable Device User", "comments": "7 pages, 5 figures, accepted at IEEE International Symposium on\n  Personal, Indoor and Mobile Radio Communications (PIMRC). arXiv admin note:\n  substantial text overlap with arXiv:2008.10779", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As market wearables are becoming popular with a range of services, including\nmaking financial transactions, accessing cars, etc. that they provide based on\nvarious private information of a user, security of this information is becoming\nvery important. However, users are often flooded with PINs and passwords in\nthis internet of things (IoT) world. Additionally, hard-biometric, such as\nfacial or finger recognition, based authentications are not adaptable for\nmarket wearables due to their limited sensing and computation capabilities.\nTherefore, it is a time demand to develop a burden-free implicit authentication\nmechanism for wearables using the less-informative soft-biometric data that are\neasily obtainable from the market wearables. In this work, we present a\ncontext-dependent soft-biometric-based wearable authentication system utilizing\nthe heart rate, gait, and breathing audio signals. From our detailed analysis,\nwe find that a binary support vector machine (SVM) with radial basis function\n(RBF) kernel can achieve an average accuracy of $0.94 \\pm 0.07$, $F_1$ score of\n$0.93 \\pm 0.08$, an equal error rate (EER) of about $0.06$ at a lower\nconfidence threshold of 0.52, which shows the promise of this work.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 04:34:19 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Cheung", "William", ""], ["Vhaduri", "Sudip", ""]]}, {"id": "2008.12147", "submitter": "Oded Nov", "authors": "Graham Dove, Martina Balestra, Devin Mann, Oded Nov", "title": "Good for the Many or Best for the Few? A Dilemma in the Design of\n  Algorithmic Advice", "comments": "To appear in Proceedings of the ACM on Human-Computer Interaction, 4\n  (CSCW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications in a range of domains, including route planning and well-being,\noffer advice based on the social information available in prior users'\naggregated activity. When designing these applications, is it better to offer:\na) advice that if strictly adhered to is more likely to result in an individual\nsuccessfully achieving their goal, even if fewer users will choose to adopt it?\nor b) advice that is likely to be adopted by a larger number of users, but\nwhich is sub-optimal with regard to any particular individual achieving their\ngoal? We identify this dilemma, characterized as Goal-Directed vs.\nAdoption-Directed advice, and investigate the design questions it raises\nthrough an online experiment undertaken in four advice domains (financial\ninvestment, making healthier lifestyle choices, route planning, training for a\n5k run), with three user types, and across two levels of uncertainty. We report\nfindings that suggest a preference for advice favoring individual goal\nattainment over higher user adoption rates, albeit with significant variation\nacross advice domains; and discuss their design implications.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 14:14:45 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Dove", "Graham", ""], ["Balestra", "Martina", ""], ["Mann", "Devin", ""], ["Nov", "Oded", ""]]}, {"id": "2008.12401", "submitter": "John Thomson", "authors": "Sizhe Yuen, John D. Thomson, Oliver Don", "title": "Automatic Player Identification in Dota 2", "comments": "11 pages, 13 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dota 2 is a popular, multiplayer online video game. Like many online games,\nplayers are mostly anonymous, being tied only to online accounts which can be\nreadily obtained, sold and shared between multiple people. This makes it\ndifficult to track or ban players who exhibit unwanted behavior online. In this\npaper, we present a machine learning approach to identify players based a\n`digital fingerprint' of how they play the game, rather than by account. We use\ndata on mouse movements, in-game statistics and game strategy extracted from\nmatch replays and show that for best results, all of these are necessary. We\nare able to obtain an accuracy of prediction of 95\\% for the problem of\npredicting if two different matches were played by the same player.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 22:58:01 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Yuen", "Sizhe", ""], ["Thomson", "John D.", ""], ["Don", "Oliver", ""]]}, {"id": "2008.12431", "submitter": "Xuancong Wang", "authors": "Xuancong Wang, Nikola Vouk, Creighton Heaukulani, Thisum Buddhika,\n  Wijaya Martanto, Jimmy Lee, Robert JT Morris", "title": "HOPES -- An Integrative Digital Phenotyping Platform for Data\n  Collection, Monitoring and Machine Learning", "comments": "This document includes both the main paper and its supplementary\n  material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the development of, and early experiences with, comprehensive\nDigital Phenotyping platform: Health Outcomes through Positive Engagement and\nSelf-Empowerment (HOPES). HOPES is based on the open-source Beiwe platform but\nadds a much wider range of data collection, including the integration of\nwearable data sources and further sensor collection from the smartphone.\nRequirements were in part derived from a concurrent clinical trial for\nschizophrenia. This trial required development of significant capabilities in\nHOPES in security, privacy, ease-of-use and scalability, based on a careful\ncombination of public cloud and on-premises operation. We describe new data\npipelines to clean, process, present and analyze data. This includes a set of\ndashboards customized to the needs of the research study operations and for\nclinical care. A test use of HOPES is described by analyzing the digital\nbehaviors of 20 participants during the SARS-CoV-2 pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 01:43:16 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Wang", "Xuancong", ""], ["Vouk", "Nikola", ""], ["Heaukulani", "Creighton", ""], ["Buddhika", "Thisum", ""], ["Martanto", "Wijaya", ""], ["Lee", "Jimmy", ""], ["Morris", "Robert JT", ""]]}, {"id": "2008.12566", "submitter": "Guy Marshall", "authors": "Guy Clarke Marshall, Andr\\'e Freitas, Caroline Jay", "title": "How Researchers Use Diagrams in Communicating Neural Network Systems", "comments": "19 pages, 6 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are a prevalent and effective machine learning component, and\ntheir application is leading to significant scientific progress in many\ndomains. As the field of neural network systems is fast growing, it is\nimportant to understand how advances are communicated. Diagrams are key to\nthis, appearing in almost all papers describing novel systems. This paper\nreports on a study into the use of neural network system diagrams, through\ninterviews, card sorting, and qualitative feedback structured around\necologically-derived examples. We find high diversity of usage, perception and\npreference in both creation and interpretation of diagrams, examining this in\nthe context of existing design, information visualisation, and user experience\nguidelines. Considering the interview data alongside existing guidance, we\npropose guidelines aiming to improve the way in which neural network system\ndiagrams are constructed.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 10:21:03 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 09:59:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Marshall", "Guy Clarke", ""], ["Freitas", "Andr\u00e9", ""], ["Jay", "Caroline", ""]]}, {"id": "2008.12632", "submitter": "Fernando M. Ortiz", "authors": "Fernando Molano Ortiz, Matteo Sammarco, Lu\\'is Henrique M. K. Costa,\n  Marcin Detyniecki", "title": "Vehicle Telematics Via Exteroceptive Sensors: A Survey", "comments": "18 pages, 4 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whereas a very large number of sensors are available in the automotive field,\ncurrently just a few of them, mostly proprioceptive ones, are used in\ntelematics, automotive insurance, and mobility safety research. In this paper,\nwe show that exteroceptive sensors, like microphones or cameras, could replace\nproprioceptive ones in many fields. Our main motivation is to provide the\nreader with alternative ideas for the development of telematics applications\nwhen proprioceptive sensors are unusable for technological issues, privacy\nconcerns, or lack of availability in commercial devices. We first introduce a\ntaxonomy of sensors in telematics. Then, we review in detail all exteroceptive\nsensors of some interest for vehicle telematics, highlighting advantages,\ndrawbacks, and availability in off-the-shelf devices. Successively, we present\na list of notable telematics services and applications in research and industry\nlike driving profiling or vehicular safety. For each of them, we report the\nmost recent and important works relying on exteroceptive sensors, as long as\nthe available datasets. We conclude showing open challenges using exteroceptive\nsensors both for industry and research.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 17:58:13 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Ortiz", "Fernando Molano", ""], ["Sammarco", "Matteo", ""], ["Costa", "Lu\u00eds Henrique M. K.", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "2008.12735", "submitter": "Donald Honeycutt", "authors": "Donald R. Honeycutt, Mahsan Nourani, Eric D. Ragan", "title": "Soliciting Human-in-the-Loop User Feedback for Interactive Machine\n  Learning Reduces User Trust and Impressions of Model Accuracy", "comments": "Accepted and to appear in the Proceedings of the AAAI Conference on\n  Human Computation and Crowdsourcing (HCOMP) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-initiative systems allow users to interactively provide feedback to\npotentially improve system performance. Human feedback can correct model errors\nand update model parameters to dynamically adapt to changing data.\nAdditionally, many users desire the ability to have a greater level of control\nand fix perceived flaws in systems they rely on. However, how the ability to\nprovide feedback to autonomous systems influences user trust is a largely\nunexplored area of research. Our research investigates how the act of providing\nfeedback can affect user understanding of an intelligent system and its\naccuracy. We present a controlled experiment using a simulated object detection\nsystem with image data to study the effects of interactive feedback collection\non user impressions. The results show that providing human-in-the-loop feedback\nlowered both participants' trust in the system and their perception of system\naccuracy, regardless of whether the system accuracy improved in response to\ntheir feedback. These results highlight the importance of considering the\neffects of allowing end-user feedback on user trust when designing intelligent\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 16:46:41 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Honeycutt", "Donald R.", ""], ["Nourani", "Mahsan", ""], ["Ragan", "Eric D.", ""]]}, {"id": "2008.12747", "submitter": "Alexandra Lee", "authors": "Alexandra Lee, Daniel Archambault, Miguel A. Nacenta", "title": "The Effectiveness of Interactive Visualization Techniques for Time\n  Navigation of Dynamic Graphs on Large Displays", "comments": null, "journal-ref": null, "doi": "10.1109/TVCG.2020.3030446", "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks can be challenging to analyze visually, especially if they\nspan a large time range during which new nodes and edges can appear and\ndisappear. Although it is straightforward to provide interfaces for\nvisualization that represent multiple states of the network (i.e., multiple\ntimeslices) either simultaneously (e.g., through small multiples) or\ninteractively (e.g., through interactive animation), these interfaces might not\nsupport tasks in which disjoint timeslices need to be compared. Since these\ntasks are key for understanding the dynamic aspects of the network,\nunderstanding which interactive visualizations best support these tasks is\nimportant. We present the results of a series of laboratory experiments\ncomparing two traditional approaches (small multiples and interactive\nanimation), with a more recent approach based on interactive timeslicing. The\ntasks were performed on a large display through a touch interface. Participants\ncompleted 24 trials of three tasks with all techniques. The results show that\ninteractive timeslicing brings benefit when comparing distant points in time,\nbut less benefits when analyzing contiguous intervals of time.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 17:13:50 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Lee", "Alexandra", ""], ["Archambault", "Daniel", ""], ["Nacenta", "Miguel A.", ""]]}, {"id": "2008.12855", "submitter": "Vaibhav Pandey", "authors": "Ali Rostami, Vaibhav Pandey, Nitish Nag, Vesper Wang, Ramesh Jain", "title": "Personal Food Model", "comments": null, "journal-ref": "Proceedings of the 28th ACM International Conference on Multimedia\n  (MM '20), October 12--16, 2020, Seattle, WA, USA", "doi": "10.1145/3394171.3414691", "report-no": null, "categories": "cs.MM cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Food is central to life. Food provides us with energy and foundational\nbuilding blocks for our body and is also a major source of joy and new\nexperiences. A significant part of the overall economy is related to food. Food\nscience, distribution, processing, and consumption have been addressed by\ndifferent communities using silos of computational approaches. In this paper,\nwe adopt a person-centric multimedia and multimodal perspective on food\ncomputing and show how multimedia and food computing are synergistic and\ncomplementary.\n  Enjoying food is a truly multimedia experience involving sight, taste, smell,\nand even sound, that can be captured using a multimedia food logger. The\nbiological response to food can be captured using multimodal data streams using\navailable wearable devices. Central to this approach is the Personal Food\nModel. Personal Food Model is the digitized representation of the food-related\ncharacteristics of an individual. It is designed to be used in food\nrecommendation systems to provide eating-related recommendations that improve\nthe user's quality of life. To model the food-related characteristics of each\nperson, it is essential to capture their food-related enjoyment using a\nPreferential Personal Food Model and their biological response to food using\ntheir Biological Personal Food Model. Inspired by the power of 3-dimensional\ncolor models for visual processing, we introduce a 6-dimensional taste-space\nfor capturing culinary characteristics as well as personal preferences. We use\nevent mining approaches to relate food with other life and biological events to\nbuild a predictive model that could also be used effectively in emerging food\nrecommendation systems.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 21:36:09 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Rostami", "Ali", ""], ["Pandey", "Vaibhav", ""], ["Nag", "Nitish", ""], ["Wang", "Vesper", ""], ["Jain", "Ramesh", ""]]}, {"id": "2008.12875", "submitter": "Raul Arrabales", "authors": "Ra\\'ul Arrabales", "title": "Perla: A Conversational Agent for Depression Screening in Digital\n  Ecosystems. Design, Implementation and Validation", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most depression assessment tools are based on self-report questionnaires,\nsuch as the Patient Health Questionnaire (PHQ-9). These psychometric\ninstruments can be easily adapted to an online setting by means of electronic\nforms. However, this approach lacks the interacting and engaging features of\nmodern digital environments. With the aim of making depression screening more\navailable, attractive and effective, we developed Perla, a conversational agent\nable to perform an interview based on the PHQ-9. We also conducted a validation\nstudy in which we compared the results obtained by the traditional self-report\nquestionnaire with Perla's automated interview. Analyzing the results from this\nstudy we draw two significant conclusions: firstly, Perla is much preferred by\nInternet users, achieving more than 2.5 times more reach than a traditional\nform-based questionnaire; secondly, her psychometric properties (Cronbach's\nalpha of 0.81, sensitivity of 96% and specificity of 90%) are excellent and\ncomparable to the traditional well-established depression screening\nquestionnaires.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 23:09:04 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 10:00:45 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Arrabales", "Ra\u00fal", ""]]}, {"id": "2008.13028", "submitter": "Guizhen Wang", "authors": "Guizhen Wang, Jingjing Guo, Mingjie Tang, Jos\\'e Florencio de Queiroz\n  Neto, Calvin Yau, Anas Daghistani, Morteza Karimzadeh, Walid G. Aref, David\n  S. Ebert", "title": "STULL: Unbiased Online Sampling for Visual Exploration of Large\n  Spatiotemporal Data", "comments": "IEEE VIS (InfoVis/VAST/SciVis) 2020 ACM 2012 CCS - Human-centered\n  computing, Visualization, Visualization design and evaluation methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online sampling-supported visual analytics is increasingly important, as it\nallows users to explore large datasets with acceptable approximate answers at\ninteractive rates. However, existing online spatiotemporal sampling techniques\nare often biased, as most researchers have primarily focused on reducing\ncomputational latency. Biased sampling approaches select data with unequal\nprobabilities and produce results that do not match the exact data\ndistribution, leading end users to incorrect interpretations. In this paper, we\npropose a novel approach to perform unbiased online sampling of large\nspatiotemporal data. The proposed approach ensures the same probability of\nselection to every point that qualifies the specifications of a user's\nmultidimensional query. To achieve unbiased sampling for accurate\nrepresentative interactive visualizations, we design a novel data index and an\nassociated sample retrieval plan. Our proposed sampling approach is suitable\nfor a wide variety of visual analytics tasks, e.g., tasks that run aggregate\nqueries of spatiotemporal data. Extensive experiments confirm the superiority\nof our approach over a state-of-the-art spatial online sampling technique,\ndemonstrating that within the same computational time, data samples generated\nin our approach are at least 50% more accurate in representing the actual\nspatial distribution of the data and enable approximate visualizations to\npresent closer visual appearances to the exact ones.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 18:12:08 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Guizhen", ""], ["Guo", "Jingjing", ""], ["Tang", "Mingjie", ""], ["Neto", "Jos\u00e9 Florencio de Queiroz", ""], ["Yau", "Calvin", ""], ["Daghistani", "Anas", ""], ["Karimzadeh", "Morteza", ""], ["Aref", "Walid G.", ""], ["Ebert", "David S.", ""]]}, {"id": "2008.13040", "submitter": "Daniel Klug", "authors": "Daniel Klug", "title": "\"It took me almost 30 minutes to practice this\". Performance and\n  Production Practices in Dance Challenge Videos on TikTok", "comments": "29 pages, incl. images NCA 106th Annual Convention: Communication at\n  the Crossroads", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  TikTok is a music-based video sharing social media app famous for users\ncreating short meme and dance videos. TikTok videos are largely based on\npopular song snippets, which is why lip syncing and dance moves evolve as\nsignificant user performance practices in videos. User prosumption has not yet\nbeen studied regarding the characteristics of TikTok. This paper is based on\nsocial practice and performance theory, social media studies, and participatory\nonline video culture. It uses the #distantdance challenge on TikTok to analyze\nproduction practices and strategies of users through qualitative video product\nanalysis. 92 videos were coded and categorized regarding their visual content\n(who participated in which way) and paratextual elements (used tags and\ncaptions). The visual and (para-)textual elements were then analyzed regarding\nindicators that allow to draw conclusions on users' video creation strategies\nand performance practices in participating in the #distantdance challenge. The\nresults show videos are mainly performed by single white female teenagers\nwearing casual outfits in their bedrooms. Users shared their experiences about\nlearning and performing the dance in video captions. While users prepared\nsettings and outfits for their performance, the majority of performances seems\nrather unplanned or spontaneous. This indicates most videos might be part of a\nseries of user attempts to master the dance challenge resulting in posting the\nfirst successful video performance to TikTok. In addition to the dance moves,\nparticipants also added gestures as closing elements to their performances.\nThis indicates their knowledge of using signals as part of an online community\nwhile at the same time manifesting their belongingness to the community. These\nfirst results of a qualitative product analysis illustrate some of users'\nmotivations and effort to participate in TikTok dance challenges.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 19:23:25 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Klug", "Daniel", ""]]}, {"id": "2008.13057", "submitter": "Po-Ming Law", "authors": "Po-Ming Law, Alex Endert, John Stasko", "title": "What are Data Insights to Professional Visualization Users?", "comments": "Published as IEEE VIS 2020 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many visualization researchers have attempted to define data insights,\nlittle is known about how visualization users perceive them. We interviewed 23\nprofessional users of end-user visualization platforms (e.g., Tableau and Power\nBI) about their experiences with data insights. We report on seven\ncharacteristics of data insights based on interviewees' descriptions. Grounded\nin these characteristics, we propose practical implications for creating tools\nthat aim to automatically communicate data insights to users.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 21:25:16 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 21:25:49 GMT"}, {"version": "v3", "created": "Sun, 4 Oct 2020 08:23:23 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Law", "Po-Ming", ""], ["Endert", "Alex", ""], ["Stasko", "John", ""]]}, {"id": "2008.13060", "submitter": "Po-Ming Law", "authors": "Po-Ming Law, Alex Endert, John Stasko", "title": "Characterizing Automated Data Insights", "comments": "Published as IEEE VIS 2020 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many researchers have explored tools that aim to recommend data insights to\nusers. These tools automatically communicate a rich diversity of data insights\nand offer such insights for many different purposes. However, there is a lack\nof structured understanding concerning what researchers of these tools mean by\n\"insight\" and what tasks in the analysis workflow these tools aim to support.\nWe conducted a systematic review of existing systems that seek to recommend\ndata insights. Grounded in the review, we propose 12 types of automated\ninsights and four purposes of automating insights. We further discuss the\ndesign opportunities emerged from our analysis.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 21:28:16 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 19:46:36 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Law", "Po-Ming", ""], ["Endert", "Alex", ""], ["Stasko", "John", ""]]}, {"id": "2008.13133", "submitter": "Tom\\'as Alves", "authors": "Tom\\'as Alves, B\\'arbara Ramalho, Joana Henriques-Calado, Daniel\n  Gon\\c{c}alves, Sandra Gama", "title": "Exploring How Personality Models Information Visualization Preferences", "comments": "5 pages, 1 figure, 4 tables, short paper, IEEE VIS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent research on information visualization has shown how individual\ndifferences act as a mediator on how users interact with visualization systems.\nWe focus our exploratory study on whether personality has an effect on user\npreferences regarding idioms used for hierarchy, evolution over time, and\ncomparison contexts. Specifically, we leverage all personality variables from\nthe Five-Factor Model and the three dimensions from Locus of Control (LoC) with\ncorrelation and clustering approaches. The correlation-based method suggested\nthat Neuroticism, Openness to Experience, Agreeableness, several facets from\neach trait, and the External dimensions from LoC mediate how much individuals\nprefer certain idioms. In addition, our results from the cluster-based analysis\nshowed that Neuroticism, Extraversion, Conscientiousness, and all dimensions\nfrom LoC have an effect on preferences for idioms in hierarchy and evolution\ncontexts. Our results support the incorporation of in-depth personality\nsynergies with InfoVis into the design pipeline of visualization systems.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 10:15:01 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 10:34:22 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Alves", "Tom\u00e1s", ""], ["Ramalho", "B\u00e1rbara", ""], ["Henriques-Calado", "Joana", ""], ["Gon\u00e7alves", "Daniel", ""], ["Gama", "Sandra", ""]]}, {"id": "2008.13221", "submitter": "Vinicius G. Goecks", "authors": "Vinicius G. Goecks", "title": "Human-in-the-Loop Methods for Data-Driven and Reinforcement Learning\n  Systems", "comments": "PhD thesis, Aerospace Engineering, Texas A&M (2020). For more\n  information, see https://vggoecks.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent successes combine reinforcement learning algorithms and deep neural\nnetworks, despite reinforcement learning not being widely applied to robotics\nand real world scenarios. This can be attributed to the fact that current\nstate-of-the-art, end-to-end reinforcement learning approaches still require\nthousands or millions of data samples to converge to a satisfactory policy and\nare subject to catastrophic failures during training. Conversely, in real world\nscenarios and after just a few data samples, humans are able to either provide\ndemonstrations of the task, intervene to prevent catastrophic actions, or\nsimply evaluate if the policy is performing correctly. This research\ninvestigates how to integrate these human interaction modalities to the\nreinforcement learning loop, increasing sample efficiency and enabling\nreal-time reinforcement learning in robotics and real world scenarios. This\nnovel theoretical foundation is called Cycle-of-Learning, a reference to how\ndifferent human interaction modalities, namely, task demonstration,\nintervention, and evaluation, are cycled and combined to reinforcement learning\nalgorithms. Results presented in this work show that the reward signal that is\nlearned based upon human interaction accelerates the rate of learning of\nreinforcement learning algorithms and that learning from a combination of human\ndemonstrations and interventions is faster and more sample efficient when\ncompared to traditional supervised learning algorithms. Finally,\nCycle-of-Learning develops an effective transition between policies learned\nusing human demonstrations and interventions to reinforcement learning. The\ntheoretical foundation developed by this research opens new research paths to\nhuman-agent teaming scenarios where autonomous agents are able to learn from\nhuman teammates and adapt to mission performance metrics in real-time and in\nreal world scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 17:28:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Goecks", "Vinicius G.", ""]]}, {"id": "2008.13262", "submitter": "Daria Trinitatova", "authors": "Aysien Ivanov, Daria Trinitatova, Dzmitry Tsetserukou", "title": "LinkRing: A Wearable Haptic Display for Delivering Multi-contact and\n  Multi-modal Stimuli at the Finger Pads", "comments": "8 pages, Accepted to the EuroHaptics Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LinkRing is a novel wearable tactile display for providing multi-contact and\nmulti-modal stimuli at the finger. The system of two five-bar linkage\nmechanisms is designed to operate with two independent contact points, which\ncombined can provide such stimulation as shear force and twist stimuli,\nslippage, and pressure. The proposed display has a lightweight and easy to wear\nstructure. Two experiments were carried out in order to determine the\nsensitivity of the finger surface, the first one aimed to determine the\nlocation of the contact points, and the other for discrimination the slippage\nwith varying rates. The results of the experiments showed a high level of\npattern recognition.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 20:07:27 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ivanov", "Aysien", ""], ["Trinitatova", "Daria", ""], ["Tsetserukou", "Dzmitry", ""]]}, {"id": "2008.13306", "submitter": "Subhashis Hazarika", "authors": "Subhashis Hazarika, Ayan Biswas, Phillip J. Wolfram, Earl Lawrence,\n  Nathan Urban", "title": "Relationship-aware Multivariate Sampling Strategy for Scientific\n  Simulation Data", "comments": "To appear as IEEE Vis 2020 Shortpaper", "journal-ref": null, "doi": "10.1109/VIS47514.2020.00015", "report-no": "2020 IEEE Visualization Conference (VIS)", "categories": "cs.LG cs.GR cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing computational power of current supercomputers, the size\nof data produced by scientific simulations is rapidly growing. To reduce the\nstorage footprint and facilitate scalable post-hoc analyses of such scientific\ndata sets, various data reduction/summarization methods have been proposed over\nthe years. Different flavors of sampling algorithms exist to sample the\nhigh-resolution scientific data, while preserving important data properties\nrequired for subsequent analyses. However, most of these sampling algorithms\nare designed for univariate data and cater to post-hoc analyses of single\nvariables. In this work, we propose a multivariate sampling strategy which\npreserves the original variable relationships and enables different\nmultivariate analyses directly on the sampled data. Our proposed strategy\nutilizes principal component analysis to capture the variance of multivariate\ndata and can be built on top of any existing state-of-the-art sampling\nalgorithms for single variables. In addition, we also propose variants of\ndifferent data partitioning schemes (regular and irregular) to efficiently\nmodel the local multivariate relationships. Using two real-world multivariate\ndata sets, we demonstrate the efficacy of our proposed multivariate sampling\nstrategy with respect to its data reduction capabilities as well as the ease of\nperforming efficient post-hoc multivariate analyses.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 00:52:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hazarika", "Subhashis", ""], ["Biswas", "Ayan", ""], ["Wolfram", "Phillip J.", ""], ["Lawrence", "Earl", ""], ["Urban", "Nathan", ""]]}, {"id": "2008.13321", "submitter": "Fabio Miranda", "authors": "Fabio Miranda, Maryam Hosseini, Marcos Lage, Harish Doraiswamy, Graham\n  Dove, Claudio T. Silva", "title": "Urban Mosaic: Visual Exploration of Streetscapes Using Large-Scale Image\n  Data", "comments": "Video: https://www.youtube.com/watch?v=Nrhk7lb3GUo", "journal-ref": "CHI '20: Proceedings of the 2020 CHI Conference on Human Factors\n  in Computing Systems", "doi": "10.1145/3313831.3376399", "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Urban planning is increasingly data driven, yet the challenge of designing\nwith data at a city scale and remaining sensitive to the impact at a human\nscale is as important today as it was for Jane Jacobs. We address this\nchallenge with Urban Mosaic,a tool for exploring the urban fabric through a\nspatially and temporally dense data set of 7.7 million street-level images from\nNew York City, captured over the period of a year. Working in collaboration\nwith professional practitioners, we use Urban Mosaic to investigate questions\nof accessibility and mobility, and preservation and retrofitting. In doing so,\nwe demonstrate how tools such as this might provide a bridge between the city\nand the street, by supporting activities such as visual comparison of\ngeographically distant neighborhoods,and temporal analysis of unfolding urban\ndevelopment.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 02:23:12 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Miranda", "Fabio", ""], ["Hosseini", "Maryam", ""], ["Lage", "Marcos", ""], ["Doraiswamy", "Harish", ""], ["Dove", "Graham", ""], ["Silva", "Claudio T.", ""]]}, {"id": "2008.13335", "submitter": "Thanh Tran", "authors": "Thanh Tran, Di You, Kyumin Lee", "title": "Quaternion-Based Self-Attentive Long Short-Term User Preference Encoding\n  for Recommendation", "comments": null, "journal-ref": "CIKM 2020", "doi": "10.1145/3340531.3411926", "report-no": null, "categories": "cs.IR cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quaternion space has brought several benefits over the traditional Euclidean\nspace: Quaternions (i) consist of a real and three imaginary components,\nencouraging richer representations; (ii) utilize Hamilton product which better\nencodes the inter-latent interactions across multiple Quaternion components;\nand (iii) result in a model with smaller degrees of freedom and less prone to\noverfitting. Unfortunately, most of the current recommender systems rely on\nreal-valued representations in Euclidean space to model either user's long-term\nor short-term interests. In this paper, we fully utilize Quaternion space to\nmodel both user's long-term and short-term preferences. We first propose a\nQUaternion-based self-Attentive Long term user Encoding (QUALE) to study the\nuser's long-term intents. Then, we propose a QUaternion-based self-Attentive\nShort term user Encoding (QUASE) to learn the user's short-term interests. To\nenhance our models' capability, we propose to fuse QUALE and QUASE into one\nmodel, namely QUALSE, by using a Quaternion-based gating mechanism. We further\ndevelop Quaternion-based Adversarial learning along with the Bayesian\nPersonalized Ranking (QABPR) to improve our model's robustness. Extensive\nexperiments on six real-world datasets show that our fused QUALSE model\noutperformed 11 state-of-the-art baselines, improving 8.43% at HIT@1 and 10.27%\nat NDCG@1 on average compared with the best baseline.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 03:22:14 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Tran", "Thanh", ""], ["You", "Di", ""], ["Lee", "Kyumin", ""]]}, {"id": "2008.13369", "submitter": "Leena Mathur", "authors": "Leena Mathur and Maja J Matari\\'c", "title": "Introducing Representations of Facial Affect in Automated Multimodal\n  Deception Detection", "comments": "10 pages, Accepted at ACM International Conference on Multimodal\n  Interaction (ICMI), October 2020", "journal-ref": "Proceedings of the 2020 International Conference on Multimodal\n  Interaction (ICMI)", "doi": "10.1145/3382507.3418864", "report-no": null, "categories": "cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated deception detection systems can enhance health, justice, and\nsecurity in society by helping humans detect deceivers in high-stakes\nsituations across medical and legal domains, among others. This paper presents\na novel analysis of the discriminative power of dimensional representations of\nfacial affect for automated deception detection, along with interpretable\nfeatures from visual, vocal, and verbal modalities. We used a video dataset of\npeople communicating truthfully or deceptively in real-world, high-stakes\ncourtroom situations. We leveraged recent advances in automated emotion\nrecognition in-the-wild by implementing a state-of-the-art deep neural network\ntrained on the Aff-Wild database to extract continuous representations of\nfacial valence and facial arousal from speakers. We experimented with unimodal\nSupport Vector Machines (SVM) and SVM-based multimodal fusion methods to\nidentify effective features, modalities, and modeling approaches for detecting\ndeception. Unimodal models trained on facial affect achieved an AUC of 80%, and\nfacial affect contributed towards the highest-performing multimodal approach\n(adaptive boosting) that achieved an AUC of 91% when tested on speakers who\nwere not part of training sets. This approach achieved a higher AUC than\nexisting automated machine learning approaches that used interpretable visual,\nvocal, and verbal features to detect deception in this dataset, but did not use\nfacial affect. Across all videos, deceptive and truthful speakers exhibited\nsignificant differences in facial valence and facial arousal, contributing\ncomputational support to existing psychological theories on affect and\ndeception. The demonstrated importance of facial affect in our models informs\nand motivates the future development of automated, affect-aware machine\nlearning approaches for modeling and detecting deception and other social\nbehaviors in-the-wild.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 05:12:57 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Mathur", "Leena", ""], ["Matari\u0107", "Maja J", ""]]}, {"id": "2008.13381", "submitter": "Ziran Wang", "authors": "Ziran Wang and Kyungtae Han and Prashant Tiwari", "title": "Augmented Reality-Based Advanced Driver-Assistance System for Connected\n  Vehicles", "comments": "2020 IEEE International Conference on Systems, Man, and Cybernetics\n  (SMC 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.MM cs.SY eess.IV eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of advanced communication technology, connected vehicles\nbecome increasingly popular in our transportation systems, which can conduct\ncooperative maneuvers with each other as well as road entities through\nvehicle-to-everything communication. A lot of research interests have been\ndrawn to other building blocks of a connected vehicle system, such as\ncommunication, planning, and control. However, less research studies were\nfocused on the human-machine cooperation and interface, namely how to visualize\nthe guidance information to the driver as an advanced driver-assistance system\n(ADAS). In this study, we propose an augmented reality (AR)-based ADAS, which\nvisualizes the guidance information calculated cooperatively by multiple\nconnected vehicles. An unsignalized intersection scenario is adopted as the use\ncase of this system, where the driver can drive the connected vehicle crossing\nthe intersection under the AR guidance, without any full stop at the\nintersection. A simulation environment is built in Unity game engine based on\nthe road network of San Francisco, and human-in-the-loop (HITL) simulation is\nconducted to validate the effectiveness of our proposed system regarding travel\ntime and energy consumption.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 06:14:28 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Ziran", ""], ["Han", "Kyungtae", ""], ["Tiwari", "Prashant", ""]]}, {"id": "2008.13404", "submitter": "Travis Greene", "authors": "Travis Greene, Galit Shmueli", "title": "Beyond Our Behavior: The GDPR and Humanistic Personalization", "comments": "submitted to FAccTRec 2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personalization should take the human person seriously. This requires a\ndeeper understanding of how recommender systems can shape both our\nself-understanding and identity. We unpack key European humanistic and\nphilosophical ideas underlying the General Data Protection Regulation (GDPR)\nand propose a new paradigm of humanistic personalization. Humanistic\npersonalization responds to the IEEE's call for Ethically Aligned Design (EAD)\nand is based on fundamental human capacities and values. Humanistic\npersonalization focuses on narrative accuracy: the subjective fit between a\nperson's self-narrative and both the input (personal data) and output of a\nrecommender system. In doing so, we re-frame the distinction between implicit\nand explicit data collection as one of nonconscious (\"organismic\") behavior and\nconscious (\"reflective\") action. This distinction raises important ethical and\ninterpretive issues related to agency, self-understanding, and political\nparticipation. Finally, we discuss how an emphasis on narrative accuracy can\nreduce opportunities for epistemic injustice done to data subjects.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 07:40:09 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Greene", "Travis", ""], ["Shmueli", "Galit", ""]]}, {"id": "2008.13419", "submitter": "Linh K\\\"astner", "authors": "Linh K\\\"astner, Leon Eversberg, Marina Mursa, Jens Lambrecht", "title": "Integrative Object and Pose to Task Detection for an\n  Augmented-Reality-based Human Assistance System using Neural Networks", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a result of an increasingly automatized and digitized industry, processes\nare becoming more complex. Augmented Reality has shown considerable potential\nin assisting workers with complex tasks by enhancing user understanding and\nexperience with spatial information. However, the acceptance and integration of\nAR into industrial processes is still limited due to the lack of established\nmethods and tedious integration efforts. Meanwhile, deep neural networks have\nachieved remarkable results in computer vision tasks and bear great prospects\nto enrich Augmented Reality applications . In this paper, we propose an\nAugmented-Reality-based human assistance system to assist workers in complex\nmanual tasks where we incorporate deep neural networks for computer vision\ntasks. More specifically, we combine Augmented Reality with object and action\ndetectors to make workflows more intuitive and flexible. To evaluate our system\nin terms of user acceptance and efficiency, we conducted several user studies.\nWe found a significant reduction in time to task completion in untrained\nworkers and a decrease in error rate. Furthermore, we investigated the users\nlearning curve with our assistance system.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:24:06 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["K\u00e4stner", "Linh", ""], ["Eversberg", "Leon", ""], ["Mursa", "Marina", ""], ["Lambrecht", "Jens", ""]]}, {"id": "2008.13509", "submitter": "Md Ashfaqur Rahman", "authors": "Md Ashfaqur Rahman", "title": "Torrit: A GUI-Based Power System Simulation Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adequate education on power system operations and controls requires a\nhands-on experience on a graphical user interface (GUI) based software. At\npresent, most commercial software do not have free editions with high\nflexibility and most freeware do not have good interfaces. This paper\nintroduces a GUI-based application called \"Torrit\" for executing operations of\npower systems, especially for transmission systems. It is written in Python for\nit's rapid development ability. Torrit's main window includes a single canvas\nwith some standard graphical interactions like create, delete, copy, move,\ndouble click etc. The beta version of this application is the focus of this\npaper that allows executing, saving and re-opening a project in three different\nmodes: per unit computations, power flow, and state estimation. However, it is\nstill in a rudimentary stage and many extensions are planned for future to\nmatch the needs of both academia and industry.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 14:11:38 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Rahman", "Md Ashfaqur", ""]]}, {"id": "2008.13510", "submitter": "Michael Fischer", "authors": "Michael H. Fischer, Giovanni Campagna, Euirim Choi, Monica S. Lam", "title": "Multi-Modal End-User Programming of Web-Based Virtual Assistant Skills", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Alexa can perform over 100,000 skills on paper, its capability covers\nonly a fraction of what is possible on the web. To reach the full potential of\nan assistant, it is desirable that individuals can create skills to automate\ntheir personal web browsing routines. Many seemingly simple routines, however,\nsuch as monitoring COVID-19 stats for their hometown, detecting changes in\ntheir child's grades online, or sending personally-addressed messages to a\ngroup, cannot be automated without conventional programming concepts such as\nconditional and iterative evaluation. This paper presents VASH (Voice Assistant\nScripting Helper), a new system that empowers users to create useful web-based\nvirtual assistant skills without learning a formal programming language. With\nVASH, the user demonstrates their task of interest in the browser and issues a\nfew voice commands, such as naming the skills and adding conditions on the\naction. VASH turns these multi-modal specifications into skills that can be\ninvoked invoice on a virtual assistant. These skills are represented in a\nformal programming language we designed called WebTalk, which supports\nparameterization, function invocation, conditionals, and iterative execution.\nVASH is a fully working prototype that works on the Chrome browser on\nreal-world websites. Our user study shows that users have many web routines\nthey wish to automate, 81% of which can be expressed using VASH. We found that\nVASH Is easy to learn, and that a majority of the users in our study want to\nuse our system.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 19:19:01 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Fischer", "Michael H.", ""], ["Campagna", "Giovanni", ""], ["Choi", "Euirim", ""], ["Lam", "Monica S.", ""]]}, {"id": "2008.13526", "submitter": "Sami Khenissi", "authors": "Sami Khenissi and Mariem Boujelbene and Olfa Nasraoui", "title": "Theoretical Modeling of the Iterative Properties of User Discovery in a\n  Collaborative Filtering Recommender System", "comments": "Accepted in Recsys2020. Code available at:\n  https://github.com/samikhenissi/TheoretUserModeling", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The closed feedback loop in recommender systems is a common setting that can\nlead to different types of biases. Several studies have dealt with these biases\nby designing methods to mitigate their effect on the recommendations. However,\nmost existing studies do not consider the iterative behavior of the system\nwhere the closed feedback loop plays a crucial role in incorporating different\nbiases into several parts of the recommendation steps.\n  We present a theoretical framework to model the asymptotic evolution of the\ndifferent components of a recommender system operating within a feedback loop\nsetting, and derive theoretical bounds and convergence properties on\nquantifiable measures of the user discovery and blind spots. We also validate\nour theoretical findings empirically using a real-life dataset and empirically\ntest the efficiency of a basic exploration strategy within our theoretical\nframework.\n  Our findings lay the theoretical basis for quantifying the effect of feedback\nloops and for designing Artificial Intelligence and machine learning algorithms\nthat explicitly incorporate the iterative nature of feedback loops in the\nmachine learning and recommendation process.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 20:30:39 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Khenissi", "Sami", ""], ["Boujelbene", "Mariem", ""], ["Nasraoui", "Olfa", ""]]}, {"id": "2008.13585", "submitter": "Jacopo de Berardinis", "authors": "Jacopo de Berardinis, Gabriella Pizzuto, Francesco Lanza, Antonio\n  Chella, Jorge Meira, Angelo Cangelosi", "title": "At Your Service: Coffee Beans Recommendation From a Robot Assistant", "comments": "Extended version of submission to ACM HAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in the field of machine learning, precisely algorithms for\nrecommendation systems, robot assistants are envisioned to become more present\nin the hospitality industry. Additionally, the COVID-19 pandemic has also\nhighlighted the need to have more service robots in our everyday lives, to\nminimise the risk of human to-human transmission. One such example would be\ncoffee shops, which have become intrinsic to our everyday lives. However,\nserving an excellent cup of coffee is not a trivial feat as a coffee blend\ntypically comprises rich aromas, indulgent and unique flavours and a lingering\naftertaste. Our work addresses this by proposing a computational model which\nrecommends optimal coffee beans resulting from the user's preferences.\nSpecifically, given a set of coffee bean properties (objective features), we\napply different supervised learning techniques to predict coffee qualities\n(subjective features). We then consider an unsupervised learning method to\nanalyse the relationship between coffee beans in the subjective feature space.\nEvaluated on a real coffee beans dataset based on digitised reviews, our\nresults illustrate that the proposed computational model gives up to 92.7\npercent recommendation accuracy for coffee beans prediction. From this, we\npropose how this computational model can be deployed on a service robot to\nreliably predict customers' coffee bean preferences, starting from the user\ninputting their coffee preferences to the robot recommending the coffee beans\nthat best meet the user's likings.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 14:03:25 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["de Berardinis", "Jacopo", ""], ["Pizzuto", "Gabriella", ""], ["Lanza", "Francesco", ""], ["Chella", "Antonio", ""], ["Meira", "Jorge", ""], ["Cangelosi", "Angelo", ""]]}, {"id": "2008.13592", "submitter": "Gabriela Molina Le\\'on", "authors": "Gabriela Molina Le\\'on, Michael Lischka and Andreas Breiter", "title": "Mapping the Global South: Equal-Area Projections for Choropleth Maps", "comments": "Accepted at IEEE VIS 2020; abstract corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choropleth maps are among the most common visualization techniques used to\npresent geographical data. These maps require an equal-area projection but\nthere are no clear criteria for selecting one. We collaborated with 20 social\nscientists researching on the Global South, interested in using choropleth\nmaps, to investigate their design choices according to their research tasks. We\nasked them to design world choropleth maps through a survey, and analyzed their\nanswers both qualitatively and quantitatively. The results suggest that the\ndesign choices of map projection, center, scale, and color scheme, were\ninfluenced by their personal research goals and the tasks. The projection was\nconsidered the most important choice and the Equal Earth projection was the\nmost common projection used. Our study takes the first substantial step in\ninvestigating projection choices for world choropleth maps in applied\nvisualization research.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 13:30:26 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 10:12:04 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Le\u00f3n", "Gabriela Molina", ""], ["Lischka", "Michael", ""], ["Breiter", "Andreas", ""]]}, {"id": "2008.13749", "submitter": "Elaine Huynh", "authors": "Elaine Huynh, Angela Nyhout, Patricia Ganea and Fanny Chevalier", "title": "Designing Narrative-Focused Role-Playing Games for Visualization\n  Literacy in Young Children", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on game design and education research, this paper introduces\nnarrative-focused role-playing games as a way to promote visualization literacy\nin young children. Visualization literacy skills are vital in understanding the\nworld around us and constructing meaningful visualizations, yet, how to better\ndevelop these skills at an early age remains largely overlooked and\nunderstudied. Only recently has the visualization community started to fill\nthis gap, resulting in preliminary studies and development of educational tools\nfor use in early education. We add to these efforts through the exploration of\ngamification to support learning, and identify an opportunity to apply\nrole-playing game-based designs by leveraging the presence of narratives in\ndata-related problems involving visualizations. We study the effects of\nincluding narrative elements on learning through a technology probe, grounded\nin a set of design considerations stemming from visualization, game design, and\neducation science. We create two versions of a game -- one with narrative\nelements and one without -- and evaluate our instances on 33 child participants\nbetween 11- to 13-years old using a between-subjects study design. Despite\nparticipants requiring double the amount of time to complete their game due to\nadditional elements, the inclusion of such elements were found to improve\nengagement without sacrificing learning; our results indicate no significant\ndifferences in development of graph-reading skills, but significant differences\nin engagement and overall enjoyment of the game. We report observations and\nqualitative feedback collected, and note areas for improvement and room for\nfuture wook.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:12:49 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Huynh", "Elaine", ""], ["Nyhout", "Angela", ""], ["Ganea", "Patricia", ""], ["Chevalier", "Fanny", ""]]}, {"id": "2008.13769", "submitter": "Maitree Leekha", "authors": "Shahid Nawaz Khan, Maitree Leekha, Jainendra Shukla, Rajiv Ratn Shah", "title": "Vyaktitv: A Multimodal Peer-to-Peer Hindi Conversations based Dataset\n  for Personality Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically detecting personality traits can aid several applications, such\nas mental health recognition and human resource management. Most datasets\nintroduced for personality detection so far have analyzed these traits for each\nindividual in isolation. However, personality is intimately linked to our\nsocial behavior. Furthermore, surprisingly little research has focused on\npersonality analysis using low resource languages. To this end, we present a\nnovel peer-to-peer Hindi conversation dataset- Vyaktitv. It consists of\nhigh-quality audio and video recordings of the participants, with Hinglish\ntextual transcriptions for each conversation. The dataset also contains a rich\nset of socio-demographic features, like income, cultural orientation, amongst\nseveral others, for all the participants. We release the dataset for public\nuse, as well as perform preliminary statistical analysis along the different\ndimensions. Finally, we also discuss various other applications and tasks for\nwhich the dataset can be employed.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:44:28 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Khan", "Shahid Nawaz", ""], ["Leekha", "Maitree", ""], ["Shukla", "Jainendra", ""], ["Shah", "Rajiv Ratn", ""]]}]