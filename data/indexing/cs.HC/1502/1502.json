[{"id": "1502.00317", "submitter": "Mounia Lalmas Dr", "authors": "David Warnock and Mounia Lalmas", "title": "An Exploration of Cursor tracking Data", "comments": "Mouse, Cursor, Tracking, Engagement, Involvement, Aesthetics,\n  Self-report Measure, Experimental Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cursor tracking data contains information about website visitors which may\nprovide new ways to understand visitors and their needs. This paper presents an\nAmazon Mechanical Turk study where participants were tracked as they used\nmodified variants of the Wikipedia and BBC News websites. Participants were\nasked to complete reading and information-finding tasks. The results showed\nthat it was possible to differentiate between users reading content and users\nlooking for information based on cursor data. The effects of website\naesthetics, user interest and cursor hardware were also analysed which showed\nit was possible to identify hardware from cursor data, but no relationship\nbetween cursor data and engagement was found. The implications of these\nresults, from the impact on web analytics to the design of experiments to\nassess user engagement, are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 21:42:02 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 14:25:07 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Warnock", "David", ""], ["Lalmas", "Mounia", ""]]}, {"id": "1502.00354", "submitter": "Nesreen Ahmed", "authors": "Nesreen K. Ahmed and Ryan A. Rossi", "title": "A Web-based Interactive Visual Graph Analytics Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a web-based visual graph analytics platform for\ninteractive graph mining, visualization, and real-time exploration of networks.\nGraphVis is fast, intuitive, and flexible, combining interactive visualizations\nwith analytic techniques to reveal important patterns and insights for sense\nmaking, reasoning, and decision making. Networks can be visualized and explored\nwithin seconds by simply drag-and-dropping a graph file into the web browser.\nThe structure, properties, and patterns of the network are computed\nautomatically and can be instantly explored in real-time. At the heart of\nGraphVis lies a multi-level interactive network visualization and analytics\nengine that allows for real-time graph mining and exploration across multiple\nlevels of granularity simultaneously. Both the graph analytic and visualization\ntechniques (at each level of granularity) are dynamic and interactive, with\nimmediate and continuous visual feedback upon every user interaction (e.g.,\nchange of a slider for filtering). Furthermore, nodes, edges, and subgraphs are\neasily inserted, deleted or exported via a number of novel techniques and tools\nthat make it extremely easy and flexible for exploring, testing hypothesis, and\nunderstanding networks in real-time over the web. A number of interactive\nvisual graph analytic techniques are also proposed including interactive role\ndiscovery methods, community detection, as well as a number of novel block\nmodels for generating graphs with community structure. Finally, we also\nhighlight other key aspects including filtering, querying, ranking,\nmanipulating, exporting, partitioning, as well as tools for dynamic network\nanalysis and visualization, interactive graph generators, and a variety of\nmulti-level network analysis, summarization, and statistical techniques.\n", "versions": [{"version": "v1", "created": "Mon, 2 Feb 2015 04:26:01 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Ahmed", "Nesreen K.", ""], ["Rossi", "Ryan A.", ""]]}, {"id": "1502.00756", "submitter": "Shonal Chaudhry", "authors": "Shonal Chaudhry and Rohitash Chandra", "title": "Design of a Mobile Face Recognition System for Visually Impaired Persons", "comments": "Added author names in sections 1 and 2. Certain details in sections 3\n  and 4 are now clearer. Removed external camera from implementation, results\n  unaffected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is estimated that 285 million people globally are visually impaired. A\nmajority of these people live in developing countries and are among the elderly\npopulation. One of the most difficult tasks faced by the visually impaired is\nidentification of people. While naturally, voice recognition is a common method\nof identification, it is an intuitive and difficult process. The rise of\ncomputation capability of mobile devices gives motivation to develop\napplications that can assist visually impaired persons. With the availability\nof mobile devices, these people can be assisted by an additional method of\nidentification through intelligent software based on computer vision\ntechniques. In this paper, we present the design and implementation of a face\ndetection and recognition system for the visually impaired through the use of\nmobile computing. This mobile system is assisted by a server-based support\nsystem. The system was tested on a custom video database. Experiment results\nshow high face detection accuracy and promising face recognition accuracy in\nsuitable conditions. The challenges of the system lie in better recognition\ntechniques for difficult situations in terms of lighting and weather.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 06:38:22 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 09:58:48 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Chaudhry", "Shonal", ""], ["Chandra", "Rohitash", ""]]}, {"id": "1502.01312", "submitter": "Renato Fabbri", "authors": "Vilson Vieira, Guilherme Lunhani, Geraldo Magela de Castro Rocha\n  Junior, Caleb Mascarenhas Luporini, Daniel Penalva, Ricardo Fabbri, Renato\n  Fabbri", "title": "Vivace: a collaborative live coding language and platform", "comments": null, "journal-ref": "Proceedings of the 16th Brazilian Symposium on Computer Music,\n  SBCM 2017", "doi": null, "report-no": "ISSN 2175-6759", "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Live coding is a performance and creative technique based on improvised and\ninteractive coding. Many recent endeavors have focused in live coding both\nbecause of aesthetics and as a way to alleviate performance drawbacks when the\nmusical instrument is a computer. This paper describes the principles and the\ndesign of Vivace, a live coding language and environment built with Web\ntechnologies to be executed on web browsers. The approach is compelling by 1)\nallowing many performers to code simultaneously, 2) the synthesis of audio and\nvideo, 3) a very simple syntax, 4) being a multiplatform software. We also\nstrive to contextualize Vivace by means of historical and usage summaries\nincluding a live coding sub-genre.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 17:15:20 GMT"}, {"version": "v2", "created": "Sun, 29 Oct 2017 01:30:06 GMT"}, {"version": "v3", "created": "Tue, 31 Oct 2017 02:18:16 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Vieira", "Vilson", ""], ["Lunhani", "Guilherme", ""], ["Junior", "Geraldo Magela de Castro Rocha", ""], ["Luporini", "Caleb Mascarenhas", ""], ["Penalva", "Daniel", ""], ["Fabbri", "Ricardo", ""], ["Fabbri", "Renato", ""]]}, {"id": "1502.02796", "submitter": "Rajib Rana", "authors": "Rajib Rana, Margee Hume, John Reilly, Raja Jurdak, Jeffrey Soar", "title": "Opportunistic and Context-aware Affect Sensing on Smartphones: The\n  Concept, Challenges and Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opportunistic affect sensing offers unprecedented potential for capturing\nspontaneous affect ubiquitously, obviating biases inherent in the laboratory\nsetting. Facial expression and voice are two major affective displays, however\nmost affect sensing systems on smartphone avoid them due to extensive power\nrequirement. Encouragingly, due to the recent advent of low-power DSP (Digital\nSignal Processing) co-processor and GPU (Graphics Processing Unit) technology,\naudio and video sensing are becoming more feasible. To properly evaluate\nopportunistically captured facial expression and voice, contextual information\nabout the dynamic audio-visual stimuli needs to be inferred. This paper\ndiscusses recent advances of affect sensing on the smartphone and identifies\nthe key barriers and potential solutions of implementing opportunistic and\ncontext-aware affect sensing on smartphone platforms.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 06:37:31 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 00:44:54 GMT"}, {"version": "v3", "created": "Sun, 6 Sep 2015 04:11:04 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Rana", "Rajib", ""], ["Hume", "Margee", ""], ["Reilly", "John", ""], ["Jurdak", "Raja", ""], ["Soar", "Jeffrey", ""]]}, {"id": "1502.03302", "submitter": "Abdulsalam Yassine Dr.", "authors": "Pallavi Kuhad, Abdulsalam Yassine, Shervin Shirmohammadi", "title": "Using Distance Estimation and Deep Learning to Simplify Calibration in\n  Food Calorie Measurement", "comments": "This paper has been withdrawn due to errors in equation 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High calorie intake in the human body on the one hand, has proved harmful in\nnumerous occasions leading to several diseases and on the other hand, a\nstandard amount of calorie intake has been deemed essential by dieticians to\nmaintain the right balance of calorie content in human body. As such,\nresearchers have proposed a variety of automatic tools and systems to assist\nusers measure their calorie in-take. In this paper, we consider the category of\nthose tools that use image processing to recognize the food, and we propose a\nmethod for fully automatic and user-friendly calibration of the dimension of\nthe food portion sizes, which is needed in order to measure food portion weight\nand its ensuing amount of calories. Experimental results show that our method,\nwhich uses deep learning, mobile cloud computing, distance estimation and size\ncalibration inside a mobile device, leads to an accuracy improvement to 95% on\naverage compared to previous work\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 13:27:01 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 11:45:11 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Kuhad", "Pallavi", ""], ["Yassine", "Abdulsalam", ""], ["Shirmohammadi", "Shervin", ""]]}, {"id": "1502.03530", "submitter": "Renien Joseph", "authors": "Renien John Joseph", "title": "Single Page Application and Canvas Drawing", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, with the impact of AJAX a new way of web development techniques\nhave been emerged. Hence, with the help of this model, single-page web\napplication was introduced which can be updated/replaced independently. Today\nwe have a new challenge of building a powerful single-page application using\nthe currently emerged technologies. Gaining an understanding of navigational\nmodel and user interface structure of the source application is the first step\nto successfully build a single- page application. In this paper, it explores\nnot only building powerful single-page application but also Two Dimensional\n(2D) drawings on images and videos. Moreover, in this research it clearly\nexpress the findings on 2D multi-points polygon drawing concepts on client\nside; real-time data binding in between drawing module on image, video and view\npages.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 04:05:25 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Joseph", "Renien John", ""]]}, {"id": "1502.03678", "submitter": "Ragaad AlTarawneh", "authors": "Ragaad AlTarawneh, Jens Bauer, Nicole Menck, Shah Rukh Humayoun, Achim\n  Ebert", "title": "assistME: A Platform for Assisting Engineers in Maintaining the Factory\n  Pipeline", "comments": "7 pages, 3 figures, MOBILEng/2014/02", "journal-ref": null, "doi": null, "report-no": "MOBILEng/2014/02", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper, we present our approach of utilizing mobile devices\n(i.e., mobile phones and tablets) for assisting engineers and experts in\nunderstanding and maintaining the factory pipelines. For this, we present a\nplatform, called assistME, that is composed of three main components: the\nassistME Server, the assistME mobile infrastructure, and the co-assistME\ncollaborative environment. In order to get full utilization of the assistME\nplatform, we assume that an initial setup is made in the factory in such a way\nthat it is equipped with different sensors to collect data about specific\nevents in the factory pipeline together with the corresponding locations of\nthese events. The assistME Server works as a central control unit in the\nplatform and collects data from the installed sensors in the factory pipeline.\nIn the case of any unexpected behavior or any critical situation in the factory\npipeline, notification and other details are sent to the related group of\nengineers and experts through the assistME mobile app. Further, the co-assistME\ncollaborative environment, equipped with a large shared screen and multiple\nmobile devices, helps the engineers and experts to collaborate with to\nunderstand and analyze the current situation in the factory pipeline in order\nto maintain it accurately.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 14:40:01 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["AlTarawneh", "Ragaad", ""], ["Bauer", "Jens", ""], ["Menck", "Nicole", ""], ["Humayoun", "Shah Rukh", ""], ["Ebert", "Achim", ""]]}, {"id": "1502.03723", "submitter": "Helio M. de Oliveira", "authors": "H.M. de Oliveira, J. Ranhel, R.B.A. Alves", "title": "Simulation of Color Blindness and a Proposal for Using Google Glass as\n  Color-correcting Tool", "comments": "4 pages, 6 figures, XXIV Congresso Brasileiro de Engenharia\n  Biomedica, Uberlandia, MG, Brazil, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human visual color response is driven by specialized cells called cones,\nwhich exist in three types, viz. R, G, and B. Software is developed to simulate\nhow color images are displayed for different types of color blindness.\nSpecified the default color deficiency associated with a user, it generates a\npreview of the rainbow (in the visible range, from red to violet) and shows up,\nside by side with a colorful image provided as input, the display correspondent\ncolorblind. The idea is to provide an image processing after image acquisition\nto enable a better perception ofcolors by the color blind. Examples of\npseudo-correction are shown for the case of Protanopia (red blindness). The\nsystem is adapted into a screen of an i-pad or a cellphone in which the\ncolorblind observe the camera, the image processed with color detail previously\nimperceptible by his naked eye. As prospecting, wearable computer glasses could\nbe manufactured to provide a corrected image playback. The approach can also\nprovide augmented reality for human vision by adding the UV or IR responses as\na new feature of Google Glass.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 16:36:55 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["de Oliveira", "H. M.", ""], ["Ranhel", "J.", ""], ["Alves", "R. B. A.", ""]]}, {"id": "1502.04245", "submitter": "Joseph Williams", "authors": "Joseph Jay Williams, Korinn Ostrow, Xiaolu Xiong, Elena Glassman, Juho\n  Kim, Samuel G. Maldonado, Na Li, Justin Reich, Neil Hefferman", "title": "Using and Designing Platforms for In Vivo Education Experiments", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contrast to typical laboratory experiments, the everyday use of online\neducational resources by large populations and the prevalence of software\ninfrastructure for A/B testing leads us to consider how platforms can embed in\nvivo experiments that do not merely support research, but ensure practical\nimprovements to their educational components. Examples are presented of\nrandomized experimental comparisons conducted by subsets of the authors in\nthree widely used online educational platforms Khan Academy, edX, and\nASSISTments. We suggest design principles for platform technology to support\nrandomized experiments that lead to practical improvements enabling Iterative\nImprovement and Collaborative Work and explain the benefit of their\nimplementation by WPI co-authors in the ASSISTments platform.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 21:16:12 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Williams", "Joseph Jay", ""], ["Ostrow", "Korinn", ""], ["Xiong", "Xiaolu", ""], ["Glassman", "Elena", ""], ["Kim", "Juho", ""], ["Maldonado", "Samuel G.", ""], ["Li", "Na", ""], ["Reich", "Justin", ""], ["Hefferman", "Neil", ""]]}, {"id": "1502.04247", "submitter": "Joseph Williams", "authors": "Joseph Jay Williams, Juho Kim, Brian C. Keegan", "title": "Supporting Instructors in Collaborating with Researchers using MOOClets", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most education and workplace learning takes place in classroom contexts far\nremoved from laboratories or field sites with special arrangements for\nscientific research. But digital online resources provide a novel opportunity\nfor large scale efforts to bridge the real world and laboratory settings which\nsupport data collection and randomized A/B experiments comparing different\nversions of content or interactions [2]. However, there are substantial\ntechnological and practical barriers in aligning instructors and researchers to\nuse learning technologies like blended lessons/exercises & MOOCs as both a\nservice for students and a realistic context to conduct research. This paper\nexplains how the concept of a MOOClet can facilitate research-practitioner\ncollaborations. MOOClets [3] are defined as modular components of a digital\nresource that can be implemented in technology to: (1) allow modification to\ncreate multiple versions, (2) allow experimental comparison and personalization\nof different versions, (3) reliably specify what data are collected. We suggest\na framework in which instructors specify what kinds of changes to lessons,\nexercises, and emails they would be willing to adopt, and what data they will\ncollect and make available. Researchers can then: (1) specify or design\nexperiments that compare the effects of different versions on quantifiable\noutcomes. (2) Explore algorithms for maximizing particular outcomes by choosing\nalternative versions of a MOOClet based on the input variables available. We\npresent a prototype survey tool for instructors intended to facilitate\npractitioner researcher matches and successful collaborations.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 21:23:01 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Williams", "Joseph Jay", ""], ["Kim", "Juho", ""], ["Keegan", "Brian C.", ""]]}, {"id": "1502.04326", "submitter": "Sergey Andreyev", "authors": "Sergey Andreyev", "title": "Interface Transformation from Ruling to Obedience", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is about one feature which was partly introduced 30 years ago\nwith the development of multi windows operating systems. It is about the\nmovability of screen objects not according to some predetermined algorithm but\nby the direct user action. Many years ago it was introduced on a very limited\nbasis and nothing was improved since then. Smartphones and tablets give direct\naccess to screen elements but on a very limited set of commands (scroll and\nzoom). There is an easy to use algorithm which turns any screen object into\nmovable / resizable. This algorithm uses only mouse to turn screens of normal\nPCs into touchscreens, but this simple change means a revolution in our work\nwith computers.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 16:05:21 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Andreyev", "Sergey", ""]]}, {"id": "1502.04485", "submitter": "Alberto Casagrande", "authors": "Alberto Casagrande and Joanna Jarmolowska and Marcello Turconi and\n  Francesco Fabris and Pierpaolo Busan and Piero Paolo Battaglini", "title": "PolyMorph: Increasing P300 Spelling Efficiency by Selection Matrix\n  Polymorphism and Sentence-Based Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P300 is an electric signal emitted by brain about 300 milliseconds after a\nrare, but relevant-for-the-user event. One of the applications of this signal\nis sentence spelling that enables subjects who lost the control of their motor\npathways to communicate by selecting characters in a matrix containing all the\nalphabet symbols. Although this technology has made considerable progress in\nthe last years, it still suffers from both low communication rate and high\nerror rate. This article presents a P300 speller, named PolyMorph, that\nintroduces two major novelties in the field: the selection matrix polymorphism,\nthat reduces the size of the selection matrix itself by removing useless\nsymbols, and sentence-based predictions, that exploit all the spelt characters\nof a sentence to determine the probability of a word. In order to measure the\neffectiveness of the presented speller, we describe two sets of tests: the\nfirst one in vivo and the second one in silico. The results of these\nexperiments suggest that the use of PolyMorph in place of the naive\ncharacter-by-character speller both increases the number of spelt characters\nper time unit and reduces the error rate.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 10:27:25 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 16:33:32 GMT"}, {"version": "v3", "created": "Mon, 23 Feb 2015 13:48:06 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Casagrande", "Alberto", ""], ["Jarmolowska", "Joanna", ""], ["Turconi", "Marcello", ""], ["Fabris", "Francesco", ""], ["Busan", "Pierpaolo", ""], ["Battaglini", "Piero Paolo", ""]]}, {"id": "1502.04609", "submitter": "Derek Greene", "authors": "Derek Greene, Daniel Archambault, V\\'aclav Bel\\'ak, P\\'adraig\n  Cunningham", "title": "TextLuas: Tracking and Visualizing Document and Term Clusters in Dynamic\n  Text Data", "comments": "21 page version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large volumes of text data collected over time, a key knowledge discovery\ntask is identifying and tracking clusters. These clusters may correspond to\nemerging themes, popular topics, or breaking news stories in a corpus.\nTherefore, recently there has been increased interest in the problem of\nclustering dynamic data. However, there exists little support for the\ninteractive exploration of the output of these analysis techniques,\nparticularly in cases where researchers wish to simultaneously explore both the\nchange in cluster structure over time and the change in the textual content\nassociated with clusters. In this paper, we propose a model for tracking\ndynamic clusters characterized by the evolutionary events of each cluster.\nMotivated by this model, the TextLuas system provides an implementation for\ntracking these dynamic clusters and visualizing their evolution using a metro\nmap metaphor. To provide overviews of cluster content, we adapt the tag cloud\nrepresentation to the dynamic clustering scenario. We demonstrate the TextLuas\nsystem on two different text corpora, where they are shown to elucidate the\nevolution of key themes. We also describe how TextLuas was applied to a problem\nin bibliographic network research.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 19:13:02 GMT"}], "update_date": "2016-08-07", "authors_parsed": [["Greene", "Derek", ""], ["Archambault", "Daniel", ""], ["Bel\u00e1k", "V\u00e1clav", ""], ["Cunningham", "P\u00e1draig", ""]]}, {"id": "1502.04744", "submitter": "Pulkit Budhiraja", "authors": "Pulkit Budhiraja, Rajinder Sodhi, Brett Jones, Kevin Karsch, Brian\n  Bailey and David Forsyth", "title": "Where's My Drink? Enabling Peripheral Real World Interactions While\n  Using HMDs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Head Mounted Displays (HMDs) allow users to experience virtual reality with a\ngreat level of immersion. However, even simple physical tasks like drinking a\nbeverage can be difficult and awkward while in a virtual reality experience. We\nexplore mixed reality renderings that selectively incorporate the physical\nworld into the virtual world for interactions with physical objects. We\nconducted a user study comparing four rendering techniques that balances\nimmersion in a virtual world with ease of interaction with the physical world.\nFinally, we discuss the pros and cons of each approach, suggesting guidelines\nfor future rendering techniques that bring physical objects into virtual\nreality.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 22:50:03 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Budhiraja", "Pulkit", ""], ["Sodhi", "Rajinder", ""], ["Jones", "Brett", ""], ["Karsch", "Kevin", ""], ["Bailey", "Brian", ""], ["Forsyth", "David", ""]]}, {"id": "1502.05534", "submitter": "Kalyan Nagaraj", "authors": "Kalyan Nagaraj and Amulyashree Sridhar", "title": "NeuroSVM: A Graphical User Interface for Identification of Liver\n  Patients", "comments": "9 pages, 6 figures", "journal-ref": "IJCSIT. 5(6): 8280-8284 (2014)", "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosis of liver infection at preliminary stage is important for better\ntreatment. In todays scenario devices like sensors are used for detection of\ninfections. Accurate classification techniques are required for automatic\nidentification of disease samples. In this context, this study utilizes data\nmining approaches for classification of liver patients from healthy\nindividuals. Four algorithms (Naive Bayes, Bagging, Random forest and SVM) were\nimplemented for classification using R platform. Further to improve the\naccuracy of classification a hybrid NeuroSVM model was developed using SVM and\nfeed-forward artificial neural network (ANN). The hybrid model was tested for\nits performance using statistical parameters like root mean square error (RMSE)\nand mean absolute percentage error (MAPE). The model resulted in a prediction\naccuracy of 98.83%. The results suggested that development of hybrid model\nimproved the accuracy of prediction. To serve the medicinal community for\nprediction of liver disease among patients, a graphical user interface (GUI)\nhas been developed using R. The GUI is deployed as a package in local\nrepository of R platform for users to perform prediction.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 11:45:37 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Nagaraj", "Kalyan", ""], ["Sridhar", "Amulyashree", ""]]}, {"id": "1502.06492", "submitter": "Brian Thomas", "authors": "Brian Thomas and Edward Shaya", "title": "A User Interface for Semantically Oriented Data Mining of Astronomy\n  Repositories", "comments": "ADASS ASP Conference Series, Vol. 394, Proceedings of the conference\n  held 23-26 September, 2007, in Kensington Town Hall, London, United Kingdom.\n  Edited by Robert W. Argyle, Peter S. Bunclark, and James R. Lewis., p.361", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a user-friendly, but powerful interface for the data mining of\nscientific repositories. We present the tool in use with actual astronomy data\nand show how it may be used to achieve many different types of powerful\nsemantic queries. The tool itself hides the gory details of query formulation,\nand data retrieval from the user, and allows the user to create workflows which\nmay be used to transform the data into a convenient form.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 16:33:12 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Thomas", "Brian", ""], ["Shaya", "Edward", ""]]}, {"id": "1502.06501", "submitter": "Brian Thomas", "authors": "Brian Thomas, Edward Shaya, Zenping Huang, Peter Teuben", "title": "Knowledge Discovery Framework for the Virtual Observatory", "comments": "ADASS XVI ASP Conference Series, Vol. 376, proceedings of the\n  conference held 15-18 October 2006 in Tucson, Arizona, USA. Edited by Richard\n  A. Shaw, Frank Hill and David J. Bell., p.563", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a framework that allows a scientist-user to easily query for\ninformation across all Virtual Observatory (VO) repositories and pull it back\nfor analysis. This framework hides the gory details of meta-data remediation\nand data formatting from the user, allowing them to get on with search,\nretrieval and analysis of VO data as if they were drawn from a single source\nusing a science based terminology rather than a data-centric one.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 16:57:48 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Thomas", "Brian", ""], ["Shaya", "Edward", ""], ["Huang", "Zenping", ""], ["Teuben", "Peter", ""]]}, {"id": "1502.06519", "submitter": "Haipeng Cai", "authors": "Haipeng Cai", "title": "Enhancing Programming Interface to Effectively Meet Multiple Information\n  Needs of Developers", "comments": "Position paper; 10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In the past decades, integrated development environments (IDEs) have been\nlargely advanced to facilitate common software engineering tasks. Yet, with\ngrowing information needs driven by increasing complexity in developing modern\nhigh-quality software, developers often need to switch among multiple user\ninterfaces, even across different applications, in their development process,\nwhich breaks their mental workflow thus tends to adversely affect their working\nefficiency and productivity. This position paper discusses challenges faced by\ncurrent IDE designs mainly from working context transitions of developers\nduring the process of seeking multiple information needs for their development\ntasks. It remarks the primary blockades behind and initially explores some\nhigh-level design considerations for overcoming such challenges in the\nnext-generation IDEs. Specifically, a few design enhancements on top of modern\nIDEs are envisioned, attempting to reduce the overheads of frequent context\nswitching commonly seen in the multitasking of developers.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 17:23:56 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Cai", "Haipeng", ""]]}, {"id": "1502.06792", "submitter": "Maximilian Speicher", "authors": "Maximilian Speicher", "title": "What is Usability? A Characterization based on ISO 9241-11 and ISO/IEC\n  25010", "comments": "Technical Report; Department of Computer Science, Technische\n  Universit\\\"at Chemnitz; also available from\n  https://www.tu-chemnitz.de/informatik/service/ib/2015.php.en", "journal-ref": null, "doi": null, "report-no": "CSR-15-02", "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to Brooke* \"Usability does not exist in any absolute sense; it can\nonly be defined with reference to particular contexts.\" That is, one cannot\nspeak of usability without specifying what that particular usability is\ncharacterized by. Driven by the feedback of a reviewer at an international\nconference, I explore in which way one can precisely specify the kind of\nusability they are investigating in a given setting. Finally, I come up with a\nformalism that defines usability as a quintuple comprising the elements level\nof usability metrics, product, users, goals and context of use. Providing\nconcrete values for these elements then constitutes the investigated type of\nusability. The use of this formalism is demonstrated in two case studies.\n  * J. Brooke. SUS: A \"quick and dirty\" usability scale. In P. W. Jordan, B.\nThomas, B. A. Weerdmeester, and A. L. McClelland, editors, Usability Evaluation\nin Industry. Taylor and Francis, 1996.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 13:06:28 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Speicher", "Maximilian", ""]]}, {"id": "1502.07762", "submitter": "Tomasz Rutkowski", "authors": "Tomasz M. Rutkowski, Hiromu Mori, Takumi Kodama, and Hiroyuki Shinoda", "title": "Airborne Ultrasonic Tactile Display Brain-computer Interface -- A Small\n  Robotic Arm Online Control Study", "comments": "2 pages, 1 figure, accepted for 10th AEARU Workshop on Computer\n  Science and Web Technology February 25-27, 2015, University of Tsukuba, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We report on an extended robot control application of a contact-less and\nairborne ultrasonic tactile display (AUTD) stimulus-based brain-computer\ninterface (BCI) paradigm, which received last year The Annual BCI Research\nAward 2014. In the award winning human communication augmentation paradigm the\nsix palm positions are used to evoke somatosensory brain responses, in order to\ndefine a novel contactless tactile BCI. An example application of a small robot\nmanagement is also presented in which the users control a small robot online.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 08:21:10 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Rutkowski", "Tomasz M.", ""], ["Mori", "Hiromu", ""], ["Kodama", "Takumi", ""], ["Shinoda", "Hiroyuki", ""]]}, {"id": "1502.07792", "submitter": "Sabrina Nusrat", "authors": "Sabrina Nusrat, Stephen Kobourov", "title": "Visualizing Cartograms: Goals and Task Taxonomy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartograms are maps in which areas of geographic regions (countries, states)\nappear in proportion to some variable of interest (population, income).\nCartograms are popular visualizations for geo-referenced data that have been\naround for over a century. Newspapers, magazines, textbooks, blogs, and\npresentations frequently employ cartograms to show voting results, popularity,\nand in general, geographic patterns. Despite the popularity of cartograms and\nthe large number of cartogram variants, there are very few studies evaluating\nthe effectiveness of cartograms in conveying information. In order to design\ncartograms as a useful visualization tool and to be able to compare the\neffectiveness of cartograms generated by different methods, we need to study\nthe nature of information conveyed and the specific tasks that can be performed\non cartograms. In this paper we consider a set of cartogram visualization\ntasks, based on standard taxonomies from cartography and information\nvisualization. We then propose a cartogram task taxonomy that can be used to\norganize not only the tasks considered here but also other tasks that might be\nadded later.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 23:47:08 GMT"}, {"version": "v2", "created": "Sun, 19 Apr 2015 19:31:49 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Nusrat", "Sabrina", ""], ["Kobourov", "Stephen", ""]]}]