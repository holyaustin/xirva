[{"id": "1607.00623", "submitter": "Kaveh Hassani", "authors": "Kaveh Hassani and Won-Sook Lee", "title": "Visualizing Natural Language Descriptions: A Survey", "comments": "Due to copyright most of the figures only appear in the journal\n  version", "journal-ref": "ACM Computing Surveys, Volume 49 Issue 1, Article No. 17, June\n  2016", "doi": "10.1145/2932710", "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural language interface exploits the conceptual simplicity and\nnaturalness of the language to create a high-level user-friendly communication\nchannel between humans and machines. One of the promising applications of such\ninterfaces is generating visual interpretations of semantic content of a given\nnatural language that can be then visualized either as a static scene or a\ndynamic animation. This survey discusses requirements and challenges of\ndeveloping such systems and reports 26 graphical systems that exploit natural\nlanguage interfaces and addresses both artificial intelligence and\nvisualization aspects. This work serves as a frame of reference to researchers\nand to enable further advances in the field.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 10:30:40 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Hassani", "Kaveh", ""], ["Lee", "Won-Sook", ""]]}, {"id": "1607.01075", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan and Gerald Knapp", "title": "Affect Intensity Estimation Using Multiple Modalities", "comments": "4 pages, 2 figures, 2 tables, Published as short paper in (Florida\n  Artificial Intelligence Research Society Conference (2014)) Flairs 27\n  Conference. Peer reviewed and published paper, Key words: affect, intensity,\n  multimodal, Kinect, emotion, face, body, hand, head, speech", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the challenges in affect recognition is accurate estimation of the\nemotion intensity level. This research proposes development of an affect\nintensity estimation model based on a weighted sum of classification confidence\nlevels, displacement of feature points and speed of feature point motion. The\nparameters of the model were calculated from data captured using multiple\nmodalities such as face, body posture, hand movement and speech. A preliminary\nstudy was conducted to compare the accuracy of the model with the annotated\nintensity levels. An emotion intensity scale ranging from 0 to 1 along the\narousal dimension in the emotion space was used. Results indicated speech and\nhand modality significantly contributed in improving accuracy in emotion\nintensity estimation using the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 00:03:17 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.01076", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan, Gerald Knapp", "title": "Aggressive actions and anger detection from multiple modalities using\n  Kinect", "comments": "11 pages, 2 figures, 5 tables, in peer review with ACM TIST, Key\n  words: Aggression, multimodal anger recognition, Kinect", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prison facilities, mental correctional institutions, sports bars and places\nof public protest are prone to sudden violence and conflicts. Surveillance\nsystems play an important role in mitigation of hostile behavior and\nimprovement of security by detecting such provocative and aggressive\nactivities. This research proposed using automatic aggressive behavior and\nanger detection to improve the effectiveness of the surveillance systems. An\nemotion and aggression aware component will make the surveillance system highly\nresponsive and capable of alerting the security guards in real time. This\nresearch proposed facial expression, head, hand and body movement and speech\ntracking for detecting anger and aggressive actions. Recognition was achieved\nusing support vector machines and rule based features. The multimodal affect\nrecognition precision rate for anger improved by 15.2% and recall rate improved\nby 11.7% when behavioral rule based features were used in aggressive action\ndetection.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 00:04:45 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.01077", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan and Gerald Knapp", "title": "EmoFit: Affect Monitoring System for Sedentary Jobs", "comments": "9 pages, 10 figures, Preprint, arXiv.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotional and physical well-being at workplace is important for a positive\nwork environment and higher productivity. Jobs such as software programming\nlead to a sedentary lifestyle and require high interaction with computers.\nWorking at the same job for years can cause a feeling of intellectual\nstagnation and lack of drive. Many employees experience lack of motivation,\nmild to extreme depression due to reasons such as aversion towards job\nresponsibilities and incompatibility with coworkers or boss. This research\nproposed an affect monitoring system EmoFit that would play the role of\npsychological and physical health trainer. The day to day computer activity and\nbody language was analyzed to detect the physical and emotional well-being of\nthe user. Keystrokes, activity interruptions, eye tracking, facial expressions,\nbody posture and speech were monitored to gauge the users health. The system\nalso provided activities such as at-desk exercise and stress relief game and\nmotivational quotes in an attempt to promote users well-being. The experimental\nresults and positive feedback from test subjects showed that EmoFit would help\nimprove emotional and physical well-being at jobs that involve significant\ncomputer usage.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 00:08:21 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.01115", "submitter": "Suyog Jain", "authors": "Suyog Dutt Jain, Kristen Grauman", "title": "Click Carving: Segmenting Objects in Video with Point Clicks", "comments": "A preliminary version of the material in this document was filed as\n  University of Texas technical report no. UT AI16-01", "journal-ref": null, "doi": null, "report-no": "University of Texas Technical Report UT AI16-01", "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel form of interactive video object segmentation where a few\nclicks by the user helps the system produce a full spatio-temporal segmentation\nof the object of interest. Whereas conventional interactive pipelines take the\nuser's initialization as a starting point, we show the value in the system\ntaking the lead even in initialization. In particular, for a given video frame,\nthe system precomputes a ranked list of thousands of possible segmentation\nhypotheses (also referred to as object region proposals) using image and motion\ncues. Then, the user looks at the top ranked proposals, and clicks on the\nobject boundary to carve away erroneous ones. This process iterates (typically\n2-3 times), and each time the system revises the top ranked proposal set, until\nthe user is satisfied with a resulting segmentation mask. Finally, the mask is\npropagated across the video to produce a spatio-temporal object tube. On three\nchallenging datasets, we provide extensive comparisons with both existing work\nand simpler alternative methods. In all, the proposed Click Carving approach\nstrikes an excellent balance of accuracy and human effort. It outperforms all\nsimilarly fast methods, and is competitive or better than those requiring 2 to\n12 times the effort.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 05:35:22 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Jain", "Suyog Dutt", ""], ["Grauman", "Kristen", ""]]}, {"id": "1607.01419", "submitter": "Kangjin Kim", "authors": "Wei Wei and Kangjin Kim and Georgios Fainekos", "title": "Extended LTLvis Motion Planning interface (Extended Technical Report)", "comments": "8 pages, 15 figures, a technical report for the 2016 IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an extended version of the Linear Temporal Logic (LTL)\ngraphical interface. It is a sketch based interface built on the Android\nplatform which makes the LTL control interface more straightforward and\nfriendly to nonexpert users. By predefining a set of areas of interest, this\ninterface can quickly and efficiently create plans that satisfy extended plan\ngoals in LTL. The interface can also allow users to customize the paths for\nthis plan by sketching a set of reference trajectories. Given the custom paths\nby the user, the LTL specification and the environment, the interface generates\na plan balancing the customized paths and the LTL specifications. We also show\nexperimental results with the implemented interface.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 21:14:54 GMT"}, {"version": "v2", "created": "Sun, 24 Jul 2016 20:46:26 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Wei", "Wei", ""], ["Kim", "Kangjin", ""], ["Fainekos", "Georgios", ""]]}, {"id": "1607.01443", "submitter": "Dan Calacci", "authors": "Dan Calacci, Oren Lederman, David Shrier, Alex 'Sandy' Pentland", "title": "Breakout: An Open Measurement and Intervention Tool for Distributed Peer\n  Learning Groups", "comments": "Presented at SBP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Breakout, a group interaction platform for online courses that\nenables the creation and measurement of face-to-face peer learning groups in\nonline settings. Breakout is designed to help students easily engage in\nsynchronous, video breakout session based peer learning in settings that\notherwise force students to rely on asynchronous text-based communication. The\nplatform also offers data collection and intervention tools for studying the\ncommunication patterns inherent in online learning environments. The goals of\nthe system are twofold: to enhance student engagement in online learning\nsettings and to create a platform for research into the relationship between\ndistributed group interaction patterns and learning outcomes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 00:11:10 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Calacci", "Dan", ""], ["Lederman", "Oren", ""], ["Shrier", "David", ""], ["Pentland", "Alex 'Sandy'", ""]]}, {"id": "1607.01492", "submitter": "Ali Tarhini", "authors": "Ali Tarhini", "title": "The Effects of Cultural dimensions and Demographic Characteristics on\n  E-learning Acceptance", "comments": "unpublished thesis", "journal-ref": "Computer Science, BURA, (2016), 1-190", "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study aims to develop and test an amalgamated conceptual framework based\non Technology Acceptance Model (TAM) and other models from social psychology,\nsuch as theory of reasoned action and TAM2 that captures the salient factors\ninfluencing the user adoption and acceptance of web-based learning systems.\nThis framework has been applied to the study of higher educational institutions\nin the context of developing as well as developed countries (e.g. Lebanon and\nUK). Additionally, the framework investigates the moderating effect of\nHofstedes four cultural dimensions at the individual level and a set of\nindividual differences (age, gender, experience and educational level) on the\nkey determinants that affect the behavioral intention to use e-learning. A\ntotal of 1197 questionnaires were received from students who were using\nweb-based learning systems at higher educational institutions in Lebanon and\nthe UK with opposite scores on cultural dimensions. Confirmatory Factor\nAnalysis (CFA) was used to perform reliability and validity checks, and\nStructural Equation Modeling (SEM) in conjunction with multi-group analysis\nmethod was used to test the hypothesized conceptual model. Our findings suggest\nthat individual, social, cultural and organizational factors are important to\nconsider in explaining students behavioral intention and usage of e-learning\nenvironments. The findings of this research contribute to the literature by\nvalidating and supporting the applicability of our extended TAM in the Lebanese\nand British contexts and provide several prominent implications to both theory\nand practice on the individual, organizational and societal levels.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 07:00:35 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Tarhini", "Ali", ""]]}, {"id": "1607.01752", "submitter": "Pavel Kucherbaev", "authors": "Pavel Kucherbaev, Azad Abad, Stefano Tranquillini, Florian Daniel,\n  Maurizio Marchese, Fabio Casati", "title": "CrowdCafe - Mobile Crowdsourcing Platform", "comments": "Was published before as a part of the phd thesis by Pavel Kucherbaev\n  http://eprints-phd.biblio.unitn.it/1716/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a mobile crowdsourcing platform CrowdCafe, where\npeople can perform microtasks using their smartphones while they ride a bus,\ntravel by train, stand in a queue or wait for an appointment. These microtasks\nare executed in exchange for rewards provided by local stores, such as coffee,\ndesserts and bus tickets. We present the concept, the implementation and the\nevaluation by conducting a study with 52 participants, having 1108 tasks\ncompleted.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 19:26:20 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Kucherbaev", "Pavel", ""], ["Abad", "Azad", ""], ["Tranquillini", "Stefano", ""], ["Daniel", "Florian", ""], ["Marchese", "Maurizio", ""], ["Casati", "Fabio", ""]]}, {"id": "1607.02174", "submitter": "Faiza Khattak Faiza Khattak", "authors": "Faiza Khan Khattak, Ansaf Salleb-Aouissi", "title": "Toward a Robust Crowd-labeling Framework using Expert Evaluation and\n  Pairwise Comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd-labeling emerged from the need to label large-scale and complex data, a\ntedious, expensive, and time-consuming task. One of the main challenges in the\ncrowd-labeling task is to control for or determine in advance the proportion of\nlow-quality/malicious labelers. If that proportion grows too high, there is\noften a phase transition leading to a steep, non-linear drop in labeling\naccuracy as noted by Karger et al. [2014]. To address these challenges, we\npropose a new framework called Expert Label Injected Crowd Estimation (ELICE)\nand extend it to different versions and variants that delay phase transition\nleading to a better labeling accuracy. ELICE automatically combines and boosts\nbulk crowd labels supported by labels from experts for limited number of\ninstances from the dataset. The expert-labels help to estimate the individual\nability of crowd labelers and difficulty of each instance, both of which are\nused to aggregate the labels. Empirical evaluation shows the superiority of\nELICE as compared to other state-of-the-art methods. We also derive a lower\nbound on the number of expert-labeled instances needed to estimate the crowd\nability and dataset difficulty as well as to get better quality labels.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 21:23:20 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Khattak", "Faiza Khan", ""], ["Salleb-Aouissi", "Ansaf", ""]]}, {"id": "1607.02652", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan and Gerald Knapp", "title": "Multimodal Affect Recognition using Kinect", "comments": "9 pages, 2 tables, 1 figure, Peer reviewed in ACM TIST", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affect (emotion) recognition has gained significant attention from\nresearchers in the past decade. Emotion-aware computer systems and devices have\nmany applications ranging from interactive robots, intelligent online tutor to\nemotion based navigation assistant. In this research data from multiple\nmodalities such as face, head, hand, body and speech was utilized for affect\nrecognition. The research used color and depth sensing device such as Kinect\nfor facial feature extraction and tracking human body joints. Temporal features\nacross multiple frames were used for affect recognition. Event driven decision\nlevel fusion was used to combine the results from each individual modality\nusing majority voting to recognize the emotions. The study also implemented\naffect recognition by matching the features to the rule based emotion templates\nper modality. Experiments showed that multimodal affect recognition rates using\ncombination of emotion templates and supervised learning were better compared\nto recognition rates based on supervised learning alone. Recognition rates\nobtained using temporal feature were higher compared to recognition rates\nobtained using position based features only.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 20:01:33 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.02660", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan and Gerald Knapp", "title": "Augmenting Supervised Emotion Recognition with Rule-Based Decision Model", "comments": "8 pages, 6 figures, 23 tables, IEEE TAC (in review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this research is development of rule based decision model for\nemotion recognition. This research also proposes using the rules for augmenting\ninter-corporal recognition accuracy in multimodal systems that use supervised\nlearning techniques. The classifiers for such learning based recognition\nsystems are susceptible to over fitting and only perform well on intra-corporal\ndata. To overcome the limitation this research proposes using rule based model\nas an additional modality. The rules were developed using raw feature data from\nvisual channel, based on human annotator agreement and existing studies that\nhave attributed movement and postures to emotions. The outcome of the rule\nevaluations was combined during the decision phase of emotion recognition\nsystem. The results indicate rule based emotion recognition augment recognition\naccuracy of learning based systems and also provide better recognition rate\nacross inter corpus emotion test data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 20:34:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.03366", "submitter": "Cindy Grimm", "authors": "Brendon John and Jackson Carter and Javier Ruiz and Sai Krishna Allani\n  and Saurabh Dixit and Cindy M. Grimm and Ravi Balasubramanian", "title": "Human-Planned Robotic Grasp Ranges: Capture and Validation", "comments": "8 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leveraging human grasping skills to teach a robot to perform a manipulation\ntask is appealing, but there are several limitations to this approach:\ntime-inefficient data capture procedures, limited generalization of the data to\nother grasps and objects, and inability to use that data to learn more about\nhow humans perform and evaluate grasps. This paper presents a data capture\nprotocol that partially addresses these deficiencies by asking participants to\nspecify ranges over which a grasp is valid. The protocol is verified both\nqualitatively through online survey questions (where 95.38% of within-range\ngrasps are identified correctly with the nearest extreme grasp) and\nquantitatively by showing that there is small variation in grasps ranges from\ndifferent participants as measured by joint angles, contact points, and\nposition. We demonstrate that these grasp ranges are valid through testing on a\nphysical robot (93.75% of grasps interpolated from grasp ranges are\nsuccessful).\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 14:21:19 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["John", "Brendon", ""], ["Carter", "Jackson", ""], ["Ruiz", "Javier", ""], ["Allani", "Sai Krishna", ""], ["Dixit", "Saurabh", ""], ["Grimm", "Cindy M.", ""], ["Balasubramanian", "Ravi", ""]]}, {"id": "1607.03401", "submitter": "Qianqian Xu", "authors": "Qianqian Xu, Jiechao Xiong, Xiaochun Cao, and Yuan Yao", "title": "Parsimonious Mixed-Effects HodgeRank for Crowdsourced Preference\n  Aggregation", "comments": "10 pages, ACM Multimedia (full paper) accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In crowdsourced preference aggregation, it is often assumed that all the\nannotators are subject to a common preference or utility function which\ngenerates their comparison behaviors in experiments. However, in reality\nannotators are subject to variations due to multi-criteria, abnormal, or a\nmixture of such behaviors. In this paper, we propose a parsimonious\nmixed-effects model based on HodgeRank, which takes into account both the fixed\neffect that the majority of annotators follows a common linear utility model,\nand the random effect that a small subset of annotators might deviate from the\ncommon significantly and exhibits strongly personalized preferences. HodgeRank\nhas been successfully applied to subjective quality evaluation of multimedia\nand resolves pairwise crowdsourced ranking data into a global consensus ranking\nand cyclic conflicts of interests. As an extension, our proposed methodology\nfurther explores the conflicts of interests through the random effect in\nannotator specific variations. The key algorithm in this paper establishes a\ndynamic path from the common utility to individual variations, with different\nlevels of parsimony or sparsity on personalization, based on newly developed\nLinearized Bregman Algorithms with Inverse Scale Space method. Finally the\nvalidity of the methodology are supported by experiments with both simulated\nexamples and three real-world crowdsourcing datasets, which shows that our\nproposed method exhibits better performance (i.e. smaller test error) compared\nwith HodgeRank due to its parsimonious property.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 15:30:10 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Xu", "Qianqian", ""], ["Xiong", "Jiechao", ""], ["Cao", "Xiaochun", ""], ["Yao", "Yuan", ""]]}, {"id": "1607.03417", "submitter": "Graeme Craig Jenkinson", "authors": "Brain Glass and Graeme Jenkinson and Yuqi Liu and M. Angela Sasse and\n  Frank Stajano", "title": "The usability canary in the security coal mine: A cognitive framework\n  for evaluation and design of usable authentication solutions", "comments": null, "journal-ref": null, "doi": "10.14722/eurousec.2016.23007", "report-no": null, "categories": "cs.HC cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past 15 years, researchers have identified an increasing number of\nsecurity mechanisms that are so unusable that the intended users either\ncircumvent them or give up on a service rather than suffer the security. With\nhindsight, the reasons can be identified easily enough: either the security\ntask itself is too cumbersome and/or time-consuming, or it creates high\nfriction with the users` primary task. The aim of the research presented here\nis to equip designers who select and implement security mechanisms with a\nmethod for identifying the ``best fit`` security mechanism at the design stage.\nSince many usability problems have been identified with authentication, we\nfocus on ``best fit`` authentication, and present a framework that allows\nsecurity designers not only to model the workload associated with a particular\nauthentication method, but more importantly to model it in the context of the\nuser`s primary task. We draw on results from cognitive psychology to create a\nmethod that allows a designer to understand the impact of a particular\nauthentication method on user productivity and satisfaction. In a validation\nstudy using a physical mockup of an airline check-in kiosk, we demonstrate that\nthe model can predict user performance and satisfaction. Furthermore, design\nexperts suggested personalized order recommendations which were similar to our\nmodel`s predictions. Our model is the first that supports identification of a\nholistic fit between the task of user authentication and the context in which\nit is performed. When applied to new systems, we believe it will help designers\nunderstand the usability impact of their security choices and thus develop\nsolutions that maximize both.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 15:54:32 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Glass", "Brain", ""], ["Jenkinson", "Graeme", ""], ["Liu", "Yuqi", ""], ["Sasse", "M. Angela", ""], ["Stajano", "Frank", ""]]}, {"id": "1607.03502", "submitter": "Manuel J. A. Eugster", "authors": "Manuel J. A. Eugster, Tuukka Ruotsalo, Michiel M. Spap\\'e, Oswald\n  Barral, Niklas Ravaja, Giulio Jacucci, Samuel Kaski", "title": "Natural brain-information interfaces: Recommending information by\n  relevance inferred from human brain signals", "comments": null, "journal-ref": "Scientific Reports 6, Article number: 38580 (2016)", "doi": "10.1038/srep38580", "report-no": null, "categories": "cs.IR cs.HC q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding relevant information from large document collections such as the\nWorld Wide Web is a common task in our daily lives. Estimation of a user's\ninterest or search intention is necessary to recommend and retrieve relevant\ninformation from these collections. We introduce a brain-information interface\nused for recommending information by relevance inferred directly from brain\nsignals. In experiments, participants were asked to read Wikipedia documents\nabout a selection of topics while their EEG was recorded. Based on the\nprediction of word relevance, the individual's search intent was modeled and\nsuccessfully used for retrieving new, relevant documents from the whole English\nWikipedia corpus. The results show that the users' interests towards digital\ncontent can be modeled from the brain signals evoked by reading. The introduced\nbrain-relevance paradigm enables the recommendation of information without any\nexplicit user interaction, and may be applied across diverse\ninformation-intensive applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 20:17:00 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Eugster", "Manuel J. A.", ""], ["Ruotsalo", "Tuukka", ""], ["Spap\u00e9", "Michiel M.", ""], ["Barral", "Oswald", ""], ["Ravaja", "Niklas", ""], ["Jacucci", "Giulio", ""], ["Kaski", "Samuel", ""]]}, {"id": "1607.03578", "submitter": "Mohammad Moghadamfalahi", "authors": "Mohammad Moghadamfalahi, Murat Akcakaya, Hooman Nezamfar, Jamshid\n  Sourati, Deniz Erdogmus", "title": "An Active RBSE Framework to Generate Optimal Stimulus Sequences in a BCI\n  for Spelling", "comments": "10 pages, 6 figures, Will be submitted to IEEE transactions on Signal\n  Processing", "journal-ref": null, "doi": "10.1109/TSP.2017.2728500", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of brain computer interfaces (BCIs) employs noninvasive recordings of\nelectroencephalography (EEG) signals to enable users with severe speech and\nmotor impairments to interact with their environment and social network. For\nexample, EEG based BCIs for typing popularly utilize event related potentials\n(ERPs) for inference. Presentation paradigm design in current ERP-based letter\nby letter typing BCIs typically query the user with an arbitrary subset\ncharacters. However, the typing accuracy and also typing speed can potentially\nbe enhanced with more informed subset selection and flash assignment. In this\nmanuscript, we introduce the active recursive Bayesian state estimation\n(active-RBSE) framework for inference and sequence optimization. Prior to\npresentation in each iteration, rather than showing a subset of randomly\nselected characters, the developed framework optimally selects a subset based\non a query function. Selected queries are made adaptively specialized for users\nduring each intent detection. Through a simulation-based study, we assess the\neffect of active-RBSE on the performance of a language-model assisted typing\nBCI in terms of typing speed and accuracy. To provide a baseline for\ncomparison, we also utilize standard presentation paradigms namely, row and\ncolumn matrix presentation paradigm and also random rapid serial visual\npresentation paradigms. The results show that utilization of active-RBSE can\nenhance the online performance of the system, both in terms of typing accuracy\nand speed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 03:00:54 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Moghadamfalahi", "Mohammad", ""], ["Akcakaya", "Murat", ""], ["Nezamfar", "Hooman", ""], ["Sourati", "Jamshid", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "1607.03922", "submitter": "Gerard Djengomemgoto", "authors": "Djengomemgoto Gerard", "title": "Development of Graphical User Interface For Microwave Filter Design", "comments": "BSc Thesis (30th December, 2013), Universiti Teknologi PETRONAS,\n  Malaysia. arXiv admin note: text overlap with arXiv:1206.3509 by other\n  authors without attribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This research project aims at developing a low-cost, time-effective and a\nstand-alone graphical user interface (GUI) that will be used to design\nmicrowave filters. Throughout the projects, the main theory behind the\ntechnology of microwave filters, their generalized mathematical equations and\nthe analysis of their different circuit topologies have been reviewed. This\nreview helps to extract the necessary information needed for the design of\nmicrowave filters. Besides, the guiding principles and the underlying\nengineering factors for a successful and information-oriented GUI were also\nhighlighted. To carry out the project, the High-Level GUI Development\nEnvironment (GUIDE) together with a structured programming approach have been\nused to design the GUI, and to program its related functionalities. The\nfrequency responses are generated by using the generalized equation of each\nfilter class and type; and by using their different circuit topologies (Shunt\nor Series topology). The GUI can also provide reactive element values from\ngiven specification. Moreover, the features for the design of ultra-wideband\n(UWB) band-pass filter, capacitively coupled and combline filter are also\nincorporated into the stand-alone application. The finalized prototype will\nserve both industries and educational institutions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 14:50:10 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Gerard", "Djengomemgoto", ""]]}, {"id": "1607.04653", "submitter": "Mathieu Delangle", "authors": "Mathieu Delangle, Jean Fran\\c{c}ois Petiot, Emilie Poirson", "title": "Assessing the differences between numerical methods, CAD evaluations and\n  real experiments for the assessement of reach envelopes of the human body", "comments": "arXiv admin note: text overlap with arXiv:1512.08557", "journal-ref": "ICED ''Design for Life'', Jul 2015, Milan, Italy", "doi": null, "report-no": null, "categories": "physics.med-ph cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical models and computer-aided modeling software are tools commonly used\nto assess the accessibility of an environment, based on static human body\ndimensions. In this paper, the limits of validity of these approaches are\nassessed by comparing the reach envelopes obtained by these methods to those\nobtained experimentally. First, the accessibility areas of forty adult\nsubjects, which may correspond to the distance of reachability of products,\nwere evaluated by performing an accessibility task comprising 168 reach points.\nSecond, anthropometric characteristics of participants were recorded and used\nto perform the reach assessment by a numerical method, and then by a CAD-based\nanalysis, where the reach was predicted using the software's maximum\nreach-envelope generation. In spite of the simple nature of the presented\ndesign problem (two-dimensional), the results show important differences\nbetween the three methods. The study of the number of reached points shows that\nthe CAD-based assessment provides more accurate results than the numerical\nmodel. Nevertheless, the shapes envelopes comparison indicates that the maximum\nreach envelopes obtained with the CAD analysis are not always consistent with\nthose obtained experimentally, closely linked to the hand location. Results\nindicate that the CAD model used to obtain maximum reaches gave predictions\nthat underestimate the reach ability.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2015 13:14:59 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Delangle", "Mathieu", ""], ["Petiot", "Jean Fran\u00e7ois", ""], ["Poirson", "Emilie", ""]]}, {"id": "1607.04760", "submitter": "Ary Setijadi Prihatmanto", "authors": "Setyaki Sholata Sya, Ary Setijadi Prihatmanto", "title": "Design and implementation of image processing system for Lumen social\n  robot-humanoid as an exhibition guide for Electrical Engineering Days 2015", "comments": "Lumen, image processing system, face detection, face recognition,\n  face tracking, human detection", "journal-ref": null, "doi": "10.13140/RG.2.1.3432.0889/1", "report-no": null, "categories": "cs.CV cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Lumen Social Robot is a humanoid robot development with the purpose that it\ncould be a good friend to all people. In this year, the Lumen Social Robot is\nbeing developed into a guide in the exhibition and in the seminar of the Final\nExam of undergraduate and graduate students in Electrical Engineering ITB,\nnamed Electrical Engineering Days 2015. In order to be the guide in that\noccasion, Lumen is supported by several things. They are Nao robot components,\nservers, and multiple processor systems. The image processing system is a\nprocessing application system that allows Lumen to recognize and determine an\nobject from the image taken from the camera eye. The image processing system is\nprovided with four modules. They are face detection module to detect a person's\nface, face recognition module to recognize a person's face, face tracking\nmodule to follow a person's face, and human detection module to detect humans\nbased on the upper parts of person's body. Face detection module and human\ndetection module are implemented by using the library harcascade.xml on EMGU\nCV. Face recognition module is implemented by adding the database for the face\nthat has been detected and store it in that database. Face tracking module is\nimplemented by using the Smooth Gaussian filter to the image.\n  -----\n  Lumen Sosial Robot merupakan sebuah pengembangan robot humanoid agar dapat\nmenjadi teman bagi banyak orang. Sistem pengolahan citra merupakan sistem\naplikasi pengolah yang bertujuan Lumen dapat mengenali dan mengetahui suatu\nobjek pada citra yang diambil dari camera mata Lumen. System pengolahan citra\ndilengkapi dengan empat buah modul, yaitu modul face detection untuk mendeteksi\nwajah seseorang, modul face recognition untuk mengenali wajah orang tersebut,\nmodul face tracking untuk mengikuti wajah seseorang, dan modul human detection\nuntuk mendeteksi manusia berdasarkan bagian tubuh atas orang\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 16:34:09 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Sya", "Setyaki Sholata", ""], ["Prihatmanto", "Ary Setijadi", ""]]}, {"id": "1607.04763", "submitter": "Ary Setijadi Prihatmanto", "authors": "Ahmad Syarif, Ary Setijadi Prihatmanto", "title": "Design and implementation of computational platform for social-humanoid\n  robot Lumen as an exhibition guide in Electrical Engineering Days 2015", "comments": "Keywords: robot, humanoid, server, RabbitMQ, Fuzzy Logic Controller,\n  AI", "journal-ref": null, "doi": "10.13140/RG.2.1.2317.9762", "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social Robot Lumen is an Artificial Intelligence development project that\naims to create an Artificial Intelligence (AI) which allows a humanoid robot to\ncommunicate with human being naturally. In this study, Lumen will be developed\nto be a tour guide in Electrical Engineering Days 2015 exhibition. In\ndeveloping an AI, there are a lot of modules that need to be developed\nseparately. To make the development easier, we need a computational platform\nwhich becomes basis for all developers to give easiness in developing the\nmodules in parallel way. That computational platform that developed by the\nwriter is called Lumen Server. Lumen Server has two main function, which are to\nbe a bridge between all Lumen intelligence modules with NAO robot, and to be\nthe communication bridge between those Lumen intelligence modules. For the\nsecond function, Lumen Server implements the AMQP protocol using RabbitMQ.\nBesides that, writer also developed a control system for robot movement called\nLumen Motion. Lumen motion is implemented by modelling the movement of NAO\nrobot and also by creating a control system using fuzzy logic controller.\nWriter also developed a program that connects all Lumen intelligence modules so\nthat Lumen can act like a tour guide. The implementation of this program uses\nFSM and event-driven program. From implementation result, all the features\nwhich were designed are successfully implemented. By the developing of this\ncomputational platform, it can ease the development of Lumen in the future. For\nnext development, it must be focused on creating integration system so that\nLumen can be more responsive to the environment.\n  -----\n  Sosial Robot Lumen adalah proyek pengembangan kecerdasan buatan yang\nbertujuan untuk menciptakan kecerdasan buatan atau artificial intelligence (AI)\nyang memungkinkan robot untuk dapat berkomunikasi dengan manusia secara alami.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 16:46:33 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Syarif", "Ahmad", ""], ["Prihatmanto", "Ary Setijadi", ""]]}, {"id": "1607.04765", "submitter": "Ary Setijadi Prihatmanto", "authors": "Putri Nhirun Rikasofiadewi, Ary Setijadi Prihatmanto", "title": "Design and implementation of audio communication system for\n  social-humanoid robot Lumen as an exhibition guide in Electrical Engineering\n  Days 2015", "comments": "Keywords: robot, audio, communication system, speech recognition,\n  speech synthesizer, gender identification, Fast Fourier Transform. Kata\n  kunci: robot, audio, system komunikasi, speech recognition, speech\n  synthesizer, gender identification, Fast Fourier Transform", "journal-ref": null, "doi": "10.13140/RG.2.1.3759.7682/1", "report-no": "13209027", "categories": "cs.HC cs.RO cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social Robot Lumen is a humanoid robot created to act like human and be human\nfriend. In this study, Lumen scenario is limited on Lumen as an exhibition\nguide in Electrical Engineering Days 2015, a seminar and exhibition of\nelectrical engineering undergraduate and graduate student of Bandung Institute\nof Technology. To be an exhibition guide, Lumen is equipped by Nao robot, a\nserver, and processing applications. Audio communication system is one of the\nprocessing applications. The purpose of the system is to create verbal\ncommunication that allow Lumen to receive human voice and respond naturally to\nit. To be able to communicate like a human, audio communication system is built\nwith speech recognition module to transform speech data into text, speech\nsynthesizer module to transform text data into speech, and gender\nidentification module to distinguish adult female and male voice. Speech\nrecognition module is implemented using Google Speech Recognition API, speech\nsynthesizer module is implemented using Acapela engine, and gender\nidentification module implemented by utilizing speech signal feature that is\nextracted using Fast Fourier Transform algorithm. Hardware used for\nimplementation are Nao robot, computer, and wireless modem.\n  -----\n  Lumen Robot Sosial Robot merupakan robot humanoid yang diciptakan agar dapat\nbersikap seperti manusia dan menjadi teman bagi manusia. Sistem komunikasi\naudio merupakan salah satu aplikasi pengolah yang bertujuan agar Lumen dapat\nmenerima suara manusia dan meresponnya dengan natural, yaitu seperti cara\nmanusia merespon manusia lainnya. Untuk dapat berkomunikasi seperti manusia,\nsistem komunikasi audio dilengkapi dengan tiga buah modul: speech recognition\nuntuk mengubah data suara menjadi teks, speech synthesizer untuk mengubah data\nteks menjadi suara, dan gender identification untuk membedakan suara wanita dan\npria.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 16:50:54 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Rikasofiadewi", "Putri Nhirun", ""], ["Prihatmanto", "Ary Setijadi", ""]]}, {"id": "1607.05028", "submitter": "Benjamin Cowley PhD", "authors": "Benjamin Ultan Cowley and Darryl Charles", "title": "Adaptive Artificial Intelligence in Games: Issues, Requirements, and a\n  Solution through Behavlets-based General Player Modelling", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the last of a series of three academic essays which deal with the\nquestion of how and why to build a generalized player model. We propose that a\ngeneral player model needs parameters for subjective experience of play,\nincluding: player psychology, game structure, and actions of play. Based on\nthis proposition, we pose three linked research questions: RQ1 what is a\nnecessary and sufficient foundation to a general player model?; RQ2 can such a\nfoundation improve performance of a computational intelligence- based player\nmodel?; and RQ3 can such a player model improve efficacy of adaptive artificial\nintelligence in games?\n  We set out the arguments behind these research questions in each of the three\nessays, presented as three preprints. The third essay, in this preprint,\npresents the argument that adaptive game artificial intelligence will be\nenhanced by a generalised player model. This is because games are inherently\nhuman artefacts which therefore, require some encoding of the human perspective\nin order to effectively autonomously respond to the individual player. The\nplayer model informs the necessary constraints on the adaptive artificial\nintelligence. A generalised player model is not only more efficient than a\nper-game solution, but also allows comparison between games which makes it a\nuseful tool for studying play in general. We describe the concept and meaning\nof an adaptive game. We propose requirements for functional adaptive AI,\narguing from first principles drawn from the games research literature. We\npropose solutions to these requirements, based on a formal model approach to\nour existing 'Behavlets' method for psychologically-derived player modelling:\n  Cowley, B., & Charles, D. (2016). Behavlets: a Method for Practical Player\nModelling using Psychology-Based Player Traits and Domain Specific Features.\nUser Modeling and User-Adapted Interaction, 26(2), 257-306.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 11:47:30 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 06:22:31 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Cowley", "Benjamin Ultan", ""], ["Charles", "Darryl", ""]]}, {"id": "1607.05162", "submitter": "Jean-Daniel Fekete", "authors": "Jean-Daniel Fekete and Romain Primet", "title": "Progressive Analytics: A Computation Paradigm for Exploratory Data\n  Analysis", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring data requires a fast feedback loop from the analyst to the system,\nwith a latency below about 10 seconds because of human cognitive limitations.\nWhen data becomes large or analysis becomes complex, sequential computations\ncan no longer be completed in a few seconds and data exploration is severely\nhampered. This article describes a novel computation paradigm called\nProgressive Computation for Data Analysis or more concisely Progressive\nAnalytics, that brings at the programming language level a low-latency\nguarantee by performing computations in a progressive fashion. Moving this\nprogressive computation at the language level relieves the programmer of\nexploratory data analysis systems from implementing the whole analytics\npipeline in a progressive way from scratch, streamlining the implementation of\nscalable exploratory data analysis systems. This article describes the new\nparadigm through a prototype implementation called ProgressiVis, and explains\nthe requirements it implies through examples.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:24:41 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Fekete", "Jean-Daniel", ""], ["Primet", "Romain", ""]]}, {"id": "1607.05174", "submitter": "Roger Moore", "authors": "Roger K. Moore", "title": "Is spoken language all-or-nothing? Implications for future speech-based\n  human-machine interaction", "comments": "To appear in K. Jokinen & G. Wilcock (Eds.), Dialogues with Social\n  Robots - Enablements, Analyses, and Evaluation. Springer Lecture Notes in\n  Electrical Engineering (LNEE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen significant market penetration for voice-based\npersonal assistants such as Apple's Siri. However, despite this success, user\ntake-up is frustratingly low. This position paper argues that there is a\nhabitability gap caused by the inevitable mismatch between the capabilities and\nexpectations of human users and the features and benefits provided by\ncontemporary technology. Suggestions are made as to how such problems might be\nmitigated, but a more worrisome question emerges: \"is spoken language\nall-or-nothing\"? The answer, based on contemporary views on the special nature\nof (spoken) language, is that there may indeed be a fundamental limit to the\ninteraction that can take place between mismatched interlocutors (such as\nhumans and machines). However, it is concluded that interactions between native\nand non-native speakers, or between adults and children, or even between humans\nand dogs, might provide critical inspiration for the design of future\nspeech-based human-machine interaction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:44:34 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Moore", "Roger K.", ""]]}, {"id": "1607.05327", "submitter": "Alberto Brunete", "authors": "Sandra Costa, Alberto Brunete, Byung-Chull Bae and Nikolaos Mavridis", "title": "Emotional Storytelling using Virtual and Robotic Agents", "comments": "14 pages, 10 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to create effective storytelling agents three fundamental questions\nmust be answered: first, is a physically embodied agent preferable to a virtual\nagent or a voice-only narration? Second, does a human voice have an advantage\nover a synthesised voice? Third, how should the emotional trajectory of the\ndifferent characters in a story be related to a storyteller's facial\nexpressions during storytelling time, and how does this correlate with the\napparent emotions on the faces of the listeners? The results of two specially\ndesigned studies indicate that the physically embodied robot produces more\nattention to the listener as compared to a virtual embodiment, that a human\nvoice is preferable over the current state of the art of text-to-speech, and\nthat there is a complex yet interesting relation between the emotion lines of\nthe story, the facial expressions of the narrating agent, and the emotions of\nthe listener, and that the empathising of the listener is evident through its\nfacial expressions. This work constitutes an important step towards emotional\nstorytelling robots that can observe their listeners and adapt their style in\norder to maximise their effectiveness.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 21:04:45 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Costa", "Sandra", ""], ["Brunete", "Alberto", ""], ["Bae", "Byung-Chull", ""], ["Mavridis", "Nikolaos", ""]]}, {"id": "1607.05654", "submitter": "Tommy Nilsson", "authors": "Tommy Nilsson, Alan Blackwell, Carl Hogsden, David Scruton", "title": "Ghosts! A Location-Based Bluetooth LE Mobile Game for Museum Exploration", "comments": "10 pages, 6th Global Conference: Videogame Cultures and the Future of\n  Interactive Entertainment. 2016. In Lindsey Joyce and Brian Quinn (Eds.)\n  Mapping the Digital: Cultures and Territories of Play, Oxford,\n  Inter-Disciplinary Press, pp. 129-138", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BLE (Bluetooth Low Energy) is a new wireless communication technology that,\nthanks to reduced power consumption, promises to facilitate communication\nbetween computing devices and help us harness their power in environments and\ncontexts previously untouched by information technology. Museums and other\nfacilities housing various cultural content are a particularly interesting area\nof application. The University of Cambridge Museums consortium has put\nconsiderable effort into researching the potential uses of emerging\ntechnologies such as BLE to unlock new experiences enriching the way we engage\nwith cultural information. As a part of this research initiative, our ambition\nhas been to examine the challenges and opportunities introduced by the\nintroduction of a BLE-centred system into the museum context. We present an\nassessment of the potential offered by this technology and of the design\napproaches that might yield the best results when developing BLE-centred\nexperiences for museum environments. A pivotal part of our project consisted of\ndesigning, developing and evaluating a prototype mobile location-based\nBLE-centred game. A number of technical problems, such as unstable and\nfluctuating signal strength, were encountered throughout the project lifecycle.\nInstead of attempting to eliminate such problems, we argued in favour of\nembracing them and turning them into a cornerstone of the gameplay. Our study\nsuggested that this alternative seamful design approach yields particularly\ngood results when deploying the technology in public environments. The project\noutcome also demonstrated the potential of BLE-centred solutions to reach out\nand engage new demographics, especially children, extending their interest in\nmuseum visits.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 16:47:55 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Nilsson", "Tommy", ""], ["Blackwell", "Alan", ""], ["Hogsden", "Carl", ""], ["Scruton", "David", ""]]}, {"id": "1607.05832", "submitter": "Varvara Kollia", "authors": "Varvara Kollia", "title": "Personalization Effect on Emotion Recognition from Physiological Data:\n  An Investigation of Performance on Different Setups and Classifiers", "comments": "8 pages, 7 png figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of emotion recognition from physiological\nsignals. Features are extracted and ranked based on their effect on\nclassification accuracy. Different classifiers are compared. The inter-subject\nvariability and the personalization effect are thoroughly investigated, through\ntrial-based and subject-based cross-validation. Finally, a personalized model\nis introduced, that would allow for enhanced emotional state prediction, based\non the physiological data of subjects that exhibit a certain degree of\nsimilarity, without the requirement of further feedback.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 06:32:16 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Kollia", "Varvara", ""]]}, {"id": "1607.05895", "submitter": "Uwe Aickelin", "authors": "Grazziela P. Figueredo, Christian Wagner, Jonathan M. Garibaldi, Uwe\n  Aickelin", "title": "Adaptive Data Communication Interface: A User-Centric Visual Data\n  Interpretation Framework", "comments": "The 9th IEEE International Conference on Big Data Science and\n  Engineering (IEEE BigDataSE-15), pp. 128 - 135, 2015", "journal-ref": null, "doi": "10.1109/Trustcom.2015.571", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this position paper, we present ideas about creating a next generation\nframework towards an adaptive interface for data communication and\nvisualisation systems. Our objective is to develop a system that accepts large\ndata sets as inputs and provides user-centric, meaningful visual information to\nassist owners to make sense of their data collection. The proposed framework\ncomprises four stages: (i) the knowledge base compilation, where we search and\ncollect existing state-ofthe-art visualisation techniques per domain and user\npreferences; (ii) the development of the learning and inference system, where\nwe apply artificial intelligence techniques to learn, predict and recommend new\ngraphic interpretations (iii) results evaluation; and (iv) reinforcement and\nadaptation, where valid outputs are stored in our knowledge base and the system\nis iteratively tuned to address new demands. These stages, as well as our\noverall vision, limitations and possible challenges are introduced in this\narticle. We also discuss further extensions of this framework for other\nknowledge discovery tasks.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 10:01:31 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Figueredo", "Grazziela P.", ""], ["Wagner", "Christian", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1607.05946", "submitter": "Eduardo Duarte", "authors": "Eduardo Duarte, Pedro Bordonhos, Paulo Dias, Beatriz Sousa Santos", "title": "Living Globe: Tridimensional interactive visualization of world\n  demographic data", "comments": "11 pages, 6 figures, submitted to HCII 2016 Conference (Toronto,\n  Canada), published on Human Interface and the Management of Information:\n  Information, Design and Interaction Volume 9734 of the series Lecture Notes\n  in Computer Science, pages 14-24", "journal-ref": null, "doi": "10.1007/978-3-319-40349-6_2", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Living Globe, an application for visualization of demo-\ngraphic data supporting the temporal comparison of data from several countries\nrepresented on a 3D globe. Living Globe allows the visual exploration of the\nfollowing demographic data: total population, population density and growth,\ncrude birth and death rates, life expectancy, net migration and population per-\ncentage of different age groups. While offering unexperienced users a default\nmapping of these data variables into visual variables, Living Globe allows more\nadvanced users to select the mapping, increasing its flexibility. The main\naspects of the Living Globe model and prototype are described as well as the\nevaluation results obtained using heuristic evaluation and usability testing.\nSome conclusions and ideas for future work are also presented.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 13:28:41 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Duarte", "Eduardo", ""], ["Bordonhos", "Pedro", ""], ["Dias", "Paulo", ""], ["Santos", "Beatriz Sousa", ""]]}, {"id": "1607.06232", "submitter": "James Lockwood", "authors": "James Lockwood, Susan Bergin", "title": "A neurofeedback system to promote learner engagement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This report describes a series of experiments that track novice programmer's\nengagement during two attention based tasks. The tasks required participants to\nwatch a tutorial video on introductory programming and to attend to a simple\nmaze game whilst wearing an electroencephalogram (EEG)device called the Emotiv\nEPOC. The EPOC's proprietary software includes a system which tracks emotional\nstate (specifically: engagement, excitement, meditation, frustration, valence\nand long-term excitement). Using this data, a software application written in\nthe Processing language was developed to track user's engagement levels and\nimplement a neurofeedback based intervention when engagement fell below an\nacceptable level. The aim of the intervention was to prompt learners who\ndisengaged with the task to re-engage. The intervention used during the video\ntutorial was to pause the video if a participant disengaged significantly.\nHowever other interventions such as slowing the video down, playing a noise or\ndarkening/brightening the screen could also be used. For the maze game, the\ncaterpillar moving through the maze slowed in line with disengagement and moved\nmore quickly once the learner re-engaged. The approach worked very well and\nsuccessfully re-engaged participants, although a number of improvements could\nbe made. A number of interesting findings on the comparative engagement levels\nof different groups e.g. by gender and by age etc. were identified and provide\nuseful pointers for future research studies.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 08:35:27 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Lockwood", "James", ""], ["Bergin", "Susan", ""]]}, {"id": "1607.06264", "submitter": "Alejandro Betancourt", "authors": "Alejandro Betancourt, Pietro Morerio, Emilia Barakova, Lucio\n  Marcenaro, Matthias Rauterberg, Carlo Regazzoni", "title": "Left/Right Hand Segmentation in Egocentric Videos", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2016.09.005", "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras allow people to record their daily activities from a\nuser-centered (First Person Vision) perspective. Due to their favorable\nlocation, wearable cameras frequently capture the hands of the user, and may\nthus represent a promising user-machine interaction tool for different\napplications. Existent First Person Vision methods handle hand segmentation as\na background-foreground problem, ignoring two important facts: i) hands are not\na single \"skin-like\" moving element, but a pair of interacting cooperative\nentities, ii) close hand interactions may lead to hand-to-hand occlusions and,\nas a consequence, create a single hand-like segment. These facts complicate a\nproper understanding of hand movements and interactions. Our approach extends\ntraditional background-foreground strategies, by including a\nhand-identification step (left-right) based on a Maxwell distribution of angle\nand position. Hand-to-hand occlusions are addressed by exploiting temporal\nsuperpixels. The experimental results show that, in addition to a reliable\nleft/right hand-segmentation, our approach considerably improves the\ntraditional background-foreground hand-segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:06:05 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Betancourt", "Alejandro", ""], ["Morerio", "Pietro", ""], ["Barakova", "Emilia", ""], ["Marcenaro", "Lucio", ""], ["Rauterberg", "Matthias", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "1607.06359", "submitter": "Fatema Akbar", "authors": "Fatema Akbar and Ingmar Weber", "title": "#Sleep_as_Android: Feasibility of Using Sleep Logs on Twitter for Sleep\n  Studies", "comments": "This is a preprint of an article accepted to appear at IEEE ICHI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media enjoys a growing popularity as a platform to seek and share\npersonal health information. For sleep studies using data from social media,\nmost researchers focused on inferring sleep-related artifacts from\nself-reported anecdotal pointers to sleep patterns or issues such as insomnia.\nThe data shared by \"quantified-selfers\" on social media presents an opportunity\nto study more quantitative and objective measures of sleep. We propose and\nvalidate the approach of collecting and analyzing sleep logs that are generated\nand shared through a sleep-tracking mobile application. We highlight the value\nof this data by combining it with users' social media data. The results provide\na validation of using social media for sleep studies as the collected sleep\ndata is aligned with sleep data from other sources. The results of combining\nsocial media data with sleep data provide preliminary evidence that higher\nsocial media activity is associated with lower sleep duration and quality.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 15:18:27 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Akbar", "Fatema", ""], ["Weber", "Ingmar", ""]]}, {"id": "1607.06587", "submitter": "Jens Grubert", "authors": "Iris Seidinger and Jens Grubert", "title": "3D Character Customization for Non-Professional Users in Handheld\n  Augmented Reality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In gaming, customizing individual characters, can create personal bonds\nbetween players and their characters. Hence, character customization is a\nstandard component in many games. While mobile Augmented Reality (AR) games\nbecome popular, to date, no 3D character editor for AR games exists. We\ninvestigate the feasibility of 3D character customization for smartphone-based\nAR in an iterative design process.\n  Specifically, we present findings from creating AR prototypes in a handheld\nAR setting. In a first user study, we found that a tangible AR prototype\nresulted in higher hedonistic measures than a camera-based approach. In a\nfollow up study, we compared the tangible AR prototype with a non-AR\ntouchscreen version for selection, scaling, translation and rotation tasks in a\n3D character customization setting. The tangible AR version resulted in\nsignificantly better results for stimulation and novelty measures than the\nnon-AR version. At the same time, it maintained a proficient level in pragmatic\nmeasures such as accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 08:00:55 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Seidinger", "Iris", ""], ["Grubert", "Jens", ""]]}, {"id": "1607.06671", "submitter": "Marc Lazareff", "authors": "Marc Lazareff (Chatillon)", "title": "The Python user interface of the elsA cfd software: a coupling framework\n  for external steering layers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC physics.flu-dyn", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Python--elsA user interface of the elsA cfd (Computational Fluid\nDynamics) software has been developed to allow users to specify simulations\nwith confidence, through a global context of description objects grouped inside\nscripts. The software main features are generated documentation, context\nchecking and completion, and helpful error management. Further developments\nhave used this foundation as a coupling framework, allowing (thanks to the\ndescriptive approach) the coupling of external algorithms with the cfd solver\nin a simple and abstract way, leading to more success in complex simulations.\nAlong with the description of the technical part of the interface, we try to\ngather the salient points pertaining to the psychological viewpoint of user\nexperience (ux). We point out the differences between user interfaces and pure\ndata management systems such as cgns.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 13:21:02 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 12:56:27 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 10:22:01 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Lazareff", "Marc", "", "Chatillon"]]}, {"id": "1607.06875", "submitter": "Steve Doubleday", "authors": "Steve Doubleday, Sean Trott, Jerome Feldman", "title": "Processing Natural Language About Ongoing Actions", "comments": "6 pages, 8 figures. Updated with PIPE citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actions may not proceed as planned; they may be interrupted, resumed or\noverridden. This is a challenge to handle in a natural language understanding\nsystem. We describe extensions to an existing implementation for the control of\nautonomous systems by natural language, to enable such systems to handle\nincoming language requests regarding actions. Language Communication with\nAutonomous Systems (LCAS) has been extended with support for X-nets,\nparameterized executable schemas representing actions. X-nets enable the system\nto control actions at a desired level of granularity, while providing a\nmechanism for language requests to be processed asynchronously. Standard\nsemantics supported include requests to stop, continue, or override the\nexisting action. The specific domain demonstrated is the control of motion of a\nsimulated robot, but the approach is general, and could be applied to other\ndomains.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 01:46:09 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 13:32:01 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Doubleday", "Steve", ""], ["Trott", "Sean", ""], ["Feldman", "Jerome", ""]]}, {"id": "1607.06878", "submitter": "Tejaswini Ganapathi", "authors": "Tejaswini Ganapathi and David Vining and Roland Bassett and Naveen\n  Garg and Mia Markey", "title": "A Human Computer Interaction Solution for Radiology Reporting:\n  Evaluation of the Factors of Variation", "comments": "17 pages, 7 figures, 1 table, In preparation for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this research is to evaluate the human and technical factors\nrequired to create a human-computer interface (HCI) for a structured reporting\nsolution based on eye-gaze and speech signals. Gaze and speech signals from\nradiologists acquired during simulated image interpretation and dictation\nsessions were analyzed to determine a) variation of temporal relationship\nbetween eye gaze and speech in a dictation environment, and b) variation in eye\nmovements for a particular image interpretation task among radiologists.\nKnowledge of these factors provides information regarding the complexity of the\nimage interpretation or dictation task, and provides information that can be\nused to design a HCI for use in diagnostic radiology. Our ultimate goal is to\nuse these data to create an HCI to automate the generation of a particular type\nof structured radiology report. Our data indicate that the a) temporal\nrelationships between eye gaze and speech and b) scan paths substantially vary\namong radiologists, thus implying that an HCI system based on eye gaze and\nspeech for automating the capture of data for structured reporting processes\nshould be customized for each user. The image resolution and layout, image\ncontent, and order of targets during an image interpretation session are not\nrelevant factors to consider when designing an HCI. Our findings can be applied\nto the design of other HCI solutions for radiological applications that involve\nvisual inspection and verbal descriptions of image findings.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 02:00:06 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 10:02:44 GMT"}, {"version": "v3", "created": "Wed, 3 Aug 2016 12:46:52 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Ganapathi", "Tejaswini", ""], ["Vining", "David", ""], ["Bassett", "Roland", ""], ["Garg", "Naveen", ""], ["Markey", "Mia", ""]]}, {"id": "1607.06896", "submitter": "Piotr Br\\'odka", "authors": "Jaros{\\l}aw Jankowski, Stanis{\\l}aw Saganowski and Piotr Br\\'odka", "title": "Evaluation of TRANSFoRm Mobile eHealth Solution for Remote Patient\n  Monitoring during Clinical Trials", "comments": "16 pages, 8 Figures, Results of EU FP7 TRANSFoRm project", "journal-ref": "Mobile Information Systems, Volume 2016, Article ID 1029368, 16\n  pages", "doi": "10.1155/2016/1029368", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, in the digital age, the mobile devices are more and more used to aid\npeople in the struggle to improve or maintain their health. In this paper, the\nmobile eHealth solution for remote patient monitoring during clinical trials is\npresented, together with the outcomes of quantitative and qualitative\nperformance evaluation. The evaluation is a third step to improve the quality\nof the application after earlier Good Clinical Practice certification and\nvalidation with the participation of 10 patients and three general\npractitioners. This time, the focus was on the usability which was evaluated by\nthe seventeen participants divided into three age groups (18-28, 29-50, and\n50+). The results, from recorded sessions and the eye tracking, show that there\nis no difference in performance between the first group and the second group,\nwhile for the third group the performance was worse, however, it was still good\nenough to complete task within reasonable time.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 06:48:07 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Jankowski", "Jaros\u0142aw", ""], ["Saganowski", "Stanis\u0142aw", ""], ["Br\u00f3dka", "Piotr", ""]]}, {"id": "1607.06979", "submitter": "Danial Esmaeili Aliabadi", "authors": "Bihter Avsar, Danial Esmaeili Aliabadi, Edris Esmaeili Aliabadi, Reza\n  Yousefnezhad", "title": "Academic Presenter: a New Storytelling Presentation Software for\n  Academic Purposes", "comments": "9 pages, 10 Figures, Research paper. ResearchGate Link:\n  https://www.researchgate.net/publication/305542281_Academic_Presenter_a_New_Storytelling_Presentation_Software_for_Academic_Purposes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From the dawn of civilization, people have used folktales and stories to\nshare information and knowledge. After the invention of printing in the 15th\ncentury, technology provided helpful yet complicated utilities to exchange\nideas. In the present computerized world, the art of storytelling is becoming\nmore influential through the unprecedented multimedia capabilities of\ncomputers. In this article, we introduce a state-of-the-art presentation\nsoftware by which academicians can present nonlinear topics efficiently and\nsharpen their storytelling skills. We show how the proposed software can\nimprove the scientific presentation style. We conducted a survey to measure the\nattractiveness of proposed utility among other alternatives. Results show that\nacademicians prefer the proposed platform to others.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 22:51:52 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Avsar", "Bihter", ""], ["Aliabadi", "Danial Esmaeili", ""], ["Aliabadi", "Edris Esmaeili", ""], ["Yousefnezhad", "Reza", ""]]}, {"id": "1607.07429", "submitter": "Gunnar Sigurdsson", "authors": "Gunnar A. Sigurdsson, Olga Russakovsky, Ali Farhadi, Ivan Laptev,\n  Abhinav Gupta", "title": "Much Ado About Time: Exhaustive Annotation of Temporal Data", "comments": "HCOMP 2016 Camera Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale annotated datasets allow AI systems to learn from and build upon\nthe knowledge of the crowd. Many crowdsourcing techniques have been developed\nfor collecting image annotations. These techniques often implicitly rely on the\nfact that a new input image takes a negligible amount of time to perceive. In\ncontrast, we investigate and determine the most cost-effective way of obtaining\nhigh-quality multi-label annotations for temporal data such as videos. Watching\neven a short 30-second video clip requires a significant time investment from a\ncrowd worker; thus, requesting multiple annotations following a single viewing\nis an important cost-saving strategy. But how many questions should we ask per\nvideo? We conclude that the optimal strategy is to ask as many questions as\npossible in a HIT (up to 52 binary questions after watching a 30-second video\nclip in our experiments). We demonstrate that while workers may not correctly\nanswer all questions, the cost-benefit analysis nevertheless favors consensus\nfrom multiple such cheap-yet-imperfect iterations over more complex\nalternatives. When compared with a one-question-per-video baseline, our method\nis able to achieve a 10% improvement in recall 76.7% ours versus 66.7%\nbaseline) at comparable precision (83.8% ours versus 83.0% baseline) in about\nhalf the annotation time (3.8 minutes ours compared to 7.1 minutes baseline).\nWe demonstrate the effectiveness of our method by collecting multi-label\nannotations of 157 human activities on 1,815 videos.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 19:51:42 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 01:20:32 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Sigurdsson", "Gunnar A.", ""], ["Russakovsky", "Olga", ""], ["Farhadi", "Ali", ""], ["Laptev", "Ivan", ""], ["Gupta", "Abhinav", ""]]}, {"id": "1607.07796", "submitter": "Igor Barahona Dr", "authors": "Igor Barahona, Alex Riba and James Freeman", "title": "Influence of personal values and the adoption of analytical tools using\n  laddering methodology", "comments": "laddering technique; personal values; business performance;\n  analytical tools in Int. J. Intercultural Information Management (2015)", "journal-ref": null, "doi": "10.1504/IJIIM.2015.072543", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analytical tools in business management are understood as a combination of\ninformation technologies and quantitative methods used to assist stakeholders\nto make better decisions. The contemporary business environment is dramatically\nchanging by the massive accumulation of data. Now, as never before, the use of\nanalytical tools must be expanded to take advantage of this growing digital\nuniverse. This article will apply the laddering technique to see how personal\nvalues (or managerial functions) influence a companys adoption of analytical\ntools. A set of ten in-depth interviews are conducted with CEOs, analytics\nconsultants, academics and businessmen in order to establish quantitative\nrelations among attributes, consequences and personal values. Two easy-to-read\noutputs are provided to interpret our results. The most important links are\nquantitatively associated through an implication matrix, and then visually\nrepresented on a hierarchical value map. Guidelines for improving the use of\nanalytical tools are provided in the last section\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 17:40:45 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Barahona", "Igor", ""], ["Riba", "Alex", ""], ["Freeman", "James", ""]]}, {"id": "1607.07980", "submitter": "James Hennessey", "authors": "James W. Hennessey, Han Liu, Holger Winnem\\\"oller, Mira Dontcheva,\n  Niloy J. Mitra", "title": "How2Sketch: Generating Easy-To-Follow Tutorials for Sketching 3D Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately drawing 3D objects is difficult for untrained individuals, as it\nrequires an understanding of perspective and its effects on geometry and\nproportions. Step-by-step tutorials break the complex task of sketching an\nentire object down into easy-to-follow steps that even a novice can follow.\nHowever, creating such tutorials requires expert knowledge and is a\ntime-consuming task. As a result, the availability of tutorials for a given\nobject or viewpoint is limited. How2Sketch addresses this problem by\nautomatically generating easy-to-follow tutorials for arbitrary 3D objects.\nGiven a segmented 3D model and a camera viewpoint,it computes a sequence of\nsteps for constructing a drawing scaffold comprised of geometric primitives,\nwhich helps the user draw the final contours in correct perspective and\nproportion. To make the drawing scaffold easy to construct, the algorithm\nsolves for an ordering among the scaffolding primitives and explicitly makes\nsmall geometric modifications to the size and location of the object parts to\nsimplify relative positioning. Technically, we formulate this scaffold\nconstruction as a single selection problem that simultaneously solves for the\nordering and geometric changes of the primitives. We demonstrate our algorithm\nfor generating tutorials on a variety of man-made objects and evaluate how\neasily the tutorials can be followed with a user study.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 06:55:22 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Hennessey", "James W.", ""], ["Liu", "Han", ""], ["Winnem\u00f6ller", "Holger", ""], ["Dontcheva", "Mira", ""], ["Mitra", "Niloy J.", ""]]}, {"id": "1607.08289", "submitter": "Gopal P. Sarma", "authors": "Gopal P. Sarma and Nick J. Hay", "title": "Mammalian Value Systems", "comments": "12 pages", "journal-ref": "Informatica Vol. 41 No. 3 (2017)", "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing human values is a topic deeply interwoven with the sciences,\nhumanities, art, and many other human endeavors. In recent years, a number of\nthinkers have argued that accelerating trends in computer science, cognitive\nscience, and related disciplines foreshadow the creation of intelligent\nmachines which meet and ultimately surpass the cognitive abilities of human\nbeings, thereby entangling an understanding of human values with future\ntechnological development. Contemporary research accomplishments suggest\nsophisticated AI systems becoming widespread and responsible for managing many\naspects of the modern world, from preemptively planning users' travel schedules\nand logistics, to fully autonomous vehicles, to domestic robots assisting in\ndaily living. The extrapolation of these trends has been most forcefully\ndescribed in the context of a hypothetical \"intelligence explosion,\" in which\nthe capabilities of an intelligent software agent would rapidly increase due to\nthe presence of feedback loops unavailable to biological organisms. The\npossibility of superintelligent agents, or simply the widespread deployment of\nsophisticated, autonomous AI systems, highlights an important theoretical\nproblem: the need to separate the cognitive and rational capacities of an agent\nfrom the fundamental goal structure, or value system, which constrains and\nguides the agent's actions. The \"value alignment problem\" is to specify a goal\nstructure for autonomous agents compatible with human values. In this brief\narticle, we suggest that recent ideas from affective neuroscience and related\ndisciplines aimed at characterizing neurological and behavioral universals in\nthe mammalian class provide important conceptual foundations relevant to\ndescribing human values. We argue that the notion of \"mammalian value systems\"\npoints to a potential avenue for fundamental research in AI safety and AI\nethics.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 01:22:26 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 18:17:46 GMT"}, {"version": "v3", "created": "Sun, 31 Dec 2017 18:47:10 GMT"}, {"version": "v4", "created": "Mon, 21 Jan 2019 19:29:30 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Sarma", "Gopal P.", ""], ["Hay", "Nick J.", ""]]}]