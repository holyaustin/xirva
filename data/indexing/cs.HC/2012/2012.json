[{"id": "2012.00012", "submitter": "Liye Fu", "authors": "Liye Fu, Susan R. Fussell and Cristian Danescu-Niculescu-Mizil", "title": "Facilitating the Communication of Politeness through Fine-Grained\n  Paraphrasing", "comments": "Proceedings of EMNLP 2020, 14 pages. Data and code at\n  https://convokit.cornell.edu/ and\n  https://github.com/CornellNLP/politeness-paraphrase", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aided by technology, people are increasingly able to communicate across\ngeographical, cultural, and language barriers. This ability also results in new\nchallenges, as interlocutors need to adapt their communication approaches to\nincreasingly diverse circumstances. In this work, we take the first steps\ntowards automatically assisting people in adjusting their language to a\nspecific communication circumstance.\n  As a case study, we focus on facilitating the accurate transmission of\npragmatic intentions and introduce a methodology for suggesting paraphrases\nthat achieve the intended level of politeness under a given communication\ncircumstance. We demonstrate the feasibility of this approach by evaluating our\nmethod in two realistic communication scenarios and show that it can reduce the\npotential for misalignment between the speaker's intentions and the listener's\nperceptions in both cases.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 19:00:00 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Fu", "Liye", ""], ["Fussell", "Susan R.", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "2012.00078", "submitter": "Zachary Taschdjian", "authors": "Zachary Taschdjian", "title": "Why Did the Robot Cross the Road? A User Study of Explanation in\n  Human-Robot Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work documents a pilot user study evaluating the effectiveness of\ncontrastive, causal and example explanations in supporting human understanding\nof AI in a hypothetical commonplace human robot interaction HRI scenario. In\ndoing so, this work situates explainable AI XAI in the context of the social\nsciences and suggests that HRI explanations are improved when informed by the\nsocial sciences.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:02:19 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Taschdjian", "Zachary", ""]]}, {"id": "2012.00215", "submitter": "Claudia Flores-Saviaga", "authors": "Claudia Flores-Saviaga, Jessica Hammer, Juan Pablo Flores, Joseph\n  Seering, Stuart Reeves, Saiph Savage", "title": "Audience and Streamer Participation at Scale on Twitch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale streaming platforms such as Twitch are becoming increasingly\npopular, but detailed audience-streamer interaction dynamics remain unexplored\nat scale. In this paper, we perform a mixed-methods study on a dataset with\nover 12 million audience chat messages and 45 hours of streaming video to\nunderstand audience participation and streamer performance on Twitch. We\nuncover five types of streams based on size and audience participation styles:\nClique Streams, small streams with close streamer-audience interactions; Rising\nStreamers, mid-range streams using custom technology and moderators to\nformalize their communities; Chatter-boxes, mid-range streams with established\nconversational dynamics; Spotlight Streamers, large streams that engage large\nnumbers of viewers while still retaining a sense of community; and\nProfessionals, massive streams with the stadium-style audiences. We discuss\nchallenges and opportunities emerging for streamers and audiences from each\nstyle and conclude by providing data-backed design implications that empower\nstreamers, audiences, live streaming platforms, and game designers\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 02:09:05 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Flores-Saviaga", "Claudia", ""], ["Hammer", "Jessica", ""], ["Flores", "Juan Pablo", ""], ["Seering", "Joseph", ""], ["Reeves", "Stuart", ""], ["Savage", "Saiph", ""]]}, {"id": "2012.00249", "submitter": "Charles Martin", "authors": "Charles Martin and Benjamin Forster and Hanna Cormick", "title": "Cross-artform performance using networked interfaces: Last Man to Die's\n  Vital LMTD", "comments": null, "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression (2010) pp. 204-207", "doi": "10.5281/zenodo.1177843", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In 2009 the cross artform group, Last Man to Die, presented a series of\nperformances using new interfaces and networked performance to integrate the\nthree artforms of its members (actor, Hanna Cormick, visual artist, Benjamin\nForster and percussionist, Charles Martin). This paper explains our artistic\nmotivations and design for a computer vision surface and networked heartbeat\nsensor as well as the experience of mounting our first major work, Vital LMTD.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 04:09:39 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Martin", "Charles", ""], ["Forster", "Benjamin", ""], ["Cormick", "Hanna", ""]]}, {"id": "2012.00250", "submitter": "Charles Martin", "authors": "Charles Martin and Chi-Hsia Lai", "title": "Strike on Stage: a percussion and media performance", "comments": null, "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression (2011) pp. 142-143", "doi": "10.5281/zenodo.1178103", "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes Strike on Stage, an interface and corresponding\naudio-visual performance work developed and performed in 2010 by percussionists\nand media artists Chi-Hsia Lai and Charles Martin. The concept of Strike on\nStage is to integrate computer visuals and sound into an improvised percussion\nperformance. A large projection surface is positioned directly behind the\nperformers, while a computer vision system tracks their movements. The setup\nallows computer visualisation and sonification to be directly responsive and\nunified with the performers' gestures.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 04:10:24 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Martin", "Charles", ""], ["Lai", "Chi-Hsia", ""]]}, {"id": "2012.00265", "submitter": "Charles Martin", "authors": "Charles Martin", "title": "Performing with a Mobile Computer System for Vibraphone", "comments": null, "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression (2013), pp. 377-380", "doi": "10.5281/zenodo.1178602", "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the development of an Apple iPhone based mobile computer\nsystem for vibraphone and its use in a series of the author's performance\nprojects in 2011 and 2012. This artistic research was motivated by a desire to\ndevelop an alternative to laptop computers for the author's existing percussion\nand computer performance practice. The aims were to develop a light, compact\nand flexible system using mobile devices that would allow computer music to\ninfiltrate solo and ensemble performance situations where it is difficult to\nuse a laptop computer. The project began with a system that brought computer\nelements to Nordlig Vinter, a suite of percussion duos, using an iPhone, RjDj,\nPure Data and a home-made pickup system. This process was documented with video\nrecordings and analysed using ethnographic methods. The mobile computer music\nsetup proved to be elegant and convenient in performance situations with very\nlittle time and space to set up, as well as in performance classes and\nworkshops. The simple mobile system encouraged experimentation and the\nplatforms used enabled sharing with a wider audience.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 04:57:25 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Martin", "Charles", ""]]}, {"id": "2012.00296", "submitter": "Charles Martin", "authors": "Charles Martin and Henry Gardner and Ben Swift", "title": "Tracking Ensemble Performance on Touch-Screens with Gesture\n  Classification and Transition Matrices", "comments": null, "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2015, pp. 359-364", "doi": "10.5281/zenodo.1179130", "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present and evaluate a novel interface for tracking ensemble performances\non touch-screens. The system uses a Random Forest classifier to extract\ntouch-screen gestures and transition matrix statistics. It analyses the\nresulting gesture-state sequences across an ensemble of performers. A series of\nspecially designed iPad apps respond to this real-time analysis of free-form\ngestural performances with calculated modifications to their musical\ninterfaces. We describe our system and evaluate it through cross-validation and\nprofiling as well as concert experience.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 06:36:26 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Martin", "Charles", ""], ["Gardner", "Henry", ""], ["Swift", "Ben", ""]]}, {"id": "2012.00323", "submitter": "Prithvi Kantan", "authors": "Prithvi Kantan, Erika G. Spaich, Sofia Dahl", "title": "A Technical Framework for Musical Biofeedback in Stroke Rehabilitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here present work a generalized low-level technical framework aimed to\nprovide musical biofeedback in post-stroke balance and gait rehabilitation,\nbuilt by an iterative user-centered process. The framework comprises wireless\nwearable inertial sensors and a software interface developed using inexpensive\nand open-source tools. The interface enables layered and adjustable music\nsynthesis, real-time control over biofeedback parameters in several training\nmodes, and extensive supplementary functionality. We evaluated the system in\nterms of technical performance, finding that the system has sufficiently low\nloop delay (~90 ms), good sensor range (>9 m) and low computational load even\nin its most demanding operation mode. In a series of expert interviews,\nselected training interactions using the system were deemed by clinicians to be\nmeaningful and relevant to clinical protocols with comprehensible feedback\n(albeit sometimes unpleasant or disturbing) for a wide patient demographic.\nFuture studies will focus on using this framework with real patients to both\ndevelop the interactions further and measure their effects during therapy.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 07:58:15 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Kantan", "Prithvi", ""], ["Spaich", "Erika G.", ""], ["Dahl", "Sofia", ""]]}, {"id": "2012.00337", "submitter": "Bidisha Sharma", "authors": "Bidisha Sharma, Xiaoxue Gao, Karthika Vijayan, Xiaohai Tian, Haizhou\n  Li", "title": "NHSS: A Speech and Singing Parallel Database", "comments": "Submitted to Speech Communication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.HC eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a database of parallel recordings of speech and singing, collected\nand released by the Human Language Technology (HLT) laboratory at the National\nUniversity of Singapore (NUS), that is called NUS-HLT Speak-Sing (NHSS)\ndatabase. We release this database to the public to support research\nactivities, that include, but not limited to comparative studies of acoustic\nattributes of speech and singing signals, cooperative synthesis of speech and\nsinging voices, and speech-to-singing conversion. This database consists of\nrecordings of sung vocals of English pop songs, the spoken counterpart of\nlyrics of the songs read by the singers in their natural reading manner, and\nmanually prepared utterance-level and word-level annotations. The audio\nrecordings in the NHSS database correspond to 100 songs sung and spoken by 10\nsingers, resulting in a total of 7 hours of audio data. There are 5 male and 5\nfemale singers, singing and reading the lyrics of 10 songs each. In this paper,\nwe discuss the design methodology of the database, analyze the similarities and\ndissimilarities in characteristics of speech and singing voices, and provide\nsome strategies to address relationships between these characteristics for\nconverting one to another. We develop benchmark systems for speech-to-singing\nalignment, spectral mapping and conversion using the NHSS database.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 08:44:37 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Sharma", "Bidisha", ""], ["Gao", "Xiaoxue", ""], ["Vijayan", "Karthika", ""], ["Tian", "Xiaohai", ""], ["Li", "Haizhou", ""]]}, {"id": "2012.00378", "submitter": "Jan Smeddinck", "authors": "Jan Smeddinck, Markus Krause, Kolja Lubitz", "title": "Mobile Game User Research: The World as Your Lab?", "comments": "CHI Conference on Human Factors in Computing Systems 2013, Workshop\n  Paper, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the advent of mobile games and the according growing and competitive\nmarket, game user research can provide valuable insights and a competitive edge\nif methods and procedures are employed that match the distinct challenges that\nmobile devices, games and usage scenarios induce. We present a summary of\nparameters that frame the research setup and procedure, focusing on the\ntrade-offs between lab and field studies and the related decision whether to\npursue large-scale and quantitative or small-scale focused research accompanied\nby qualitative methods. We then illustrate the implications of these\nconsiderations on real world projects along the lines of two evaluations of\ndifferent input methods for the action-puzzle mobile game Somyeol: a local\nstudy with 37 participants and a mixed design of qualitative and quantitative\nmethods, and the strictly quantitative analysis of game-play data from 117,118\nusers. The findings underline the importance of small-scale evaluations prior\nto release.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 10:11:40 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Smeddinck", "Jan", ""], ["Krause", "Markus", ""], ["Lubitz", "Kolja", ""]]}, {"id": "2012.00423", "submitter": "Tom S\\\"uhr", "authors": "Tom S\\\"uhr, Sophie Hilgard, Himabindu Lakkaraju", "title": "Does Fair Ranking Improve Minority Outcomes? Understanding the Interplay\n  of Human and Algorithmic Biases in Online Hiring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking algorithms are being widely employed in various online hiring\nplatforms including LinkedIn, TaskRabbit, and Fiverr. Prior research has\ndemonstrated that ranking algorithms employed by these platforms are prone to a\nvariety of undesirable biases, leading to the proposal of fair ranking\nalgorithms (e.g., Det-Greedy) which increase exposure of underrepresented\ncandidates. However, there is little to no work that explores whether fair\nranking algorithms actually improve real world outcomes (e.g., hiring\ndecisions) for underrepresented groups. Furthermore, there is no clear\nunderstanding as to how other factors (e.g., job context, inherent biases of\nthe employers) may impact the efficacy of fair ranking in practice. In this\nwork, we analyze various sources of gender biases in online hiring platforms,\nincluding the job context and inherent biases of employers and establish how\nthese factors interact with ranking algorithms to affect hiring decisions. To\nthe best of our knowledge, this work makes the first attempt at studying the\ninterplay between the aforementioned factors in the context of online hiring.\nWe carry out a largescale user study simulating online hiring scenarios with\ndata from TaskRabbit, a popular online freelancing site. Our results\ndemonstrate that while fair ranking algorithms generally improve the selection\nrates of underrepresented minorities, their effectiveness relies heavily on the\njob contexts and candidate profiles.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 11:45:27 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 09:31:51 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["S\u00fchr", "Tom", ""], ["Hilgard", "Sophie", ""], ["Lakkaraju", "Himabindu", ""]]}, {"id": "2012.00467", "submitter": "Qianwen Wang", "authors": "Qianwen Wang, Zhutian Chen, Yong Wang, Huamin Qu", "title": "A Survey on ML4VIS: Applying Machine Learning Advances to Data\n  Visualization", "comments": "19 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the great success of machine learning (ML), researchers have\napplied ML techniques to visualizations to achieve a better design,\ndevelopment, and evaluation of visualizations. This branch of studies, known as\nML4VIS, is gaining increasing research attention in recent years. To\nsuccessfully adapt ML techniques for visualizations, a structured understanding\nof the integration of ML4VISis needed. In this paper, we systematically survey\n88 ML4VIS studies, aiming to answer two motivating questions: \"what\nvisualization processes can be assisted by ML?\" and \"how ML techniques can be\nused to solve visualization problems?\" This survey reveals seven main processes\nwhere the employment of ML techniques can benefit visualizations:Data\nProcessing4VIS, Data-VIS Mapping, InsightCommunication, Style Imitation, VIS\nInteraction, VIS Reading, and User Profiling. The seven processes are related\nto existing visualization theoretical models in an ML4VIS pipeline, aiming to\nilluminate the role of ML-assisted visualization in general\nvisualizations.Meanwhile, the seven processes are mapped into main learning\ntasks in ML to align the capabilities of ML with the needs in visualization.\nCurrent practices and future opportunities of ML4VIS are discussed in the\ncontext of the ML4VIS pipeline and the ML-VIS mapping. While more studies are\nstill needed in the area of ML4VIS, we hope this paper can provide a\nstepping-stone for future exploration. A web-based interactive browser of this\nsurvey is available at https://ml4vis.github.io\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 13:19:59 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 00:04:28 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wang", "Qianwen", ""], ["Chen", "Zhutian", ""], ["Wang", "Yong", ""], ["Qu", "Huamin", ""]]}, {"id": "2012.00530", "submitter": "Maria Bada Dr", "authors": "Maria Bada and Richard Clayton", "title": "Online Suicide Games: A Form of Digital Self-harm or A Myth?", "comments": "7 pages", "journal-ref": "In Wiederhold, B, & Riva, G. & Debb, S. Annual Review of\n  Cybertherapy and Telemedicine (ARCTT) International Association of\n  CyberPsychology, Training, and Rehabilitation (iACToR), 2019", "doi": null, "report-no": "Conference Proceedings CYPSY24: 24th Annual CyberPsychology,\n  CyberTherapy & Social Networking Conference, 2019", "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online suicide games are claimed to involve a series of challenges, ending in\nsuicide. A whole succession of these such as the Blue Whale Challenge, Momo,\nthe Fire Fairy and Doki Doki have appeared in recent years. The challenge\nculture is a deeply rooted online phenomenon, whether the challenge is\ndangerous or not, while social media particularly motivates youngsters to take\npart because of their desire for attention. Although there is no evidence that\nthe suicide games are real, authorities around the world have reacted by\nreleasing warnings and creating information campaigns to warn youngsters and\nparents. We interviewed teachers, child protection experts and NGOs, conducted\na systematic review of historical news reports from 2015-2019 and searched\npolice and other authority websites to identify relevant warning releases. We\nthen synthesized the existing knowledge on the suicide games phenomenon. A key\nfinding of our work is that media, social media and warning releases by\nauthorities are mainly just serving to spread the challenge culture and\nexaggerate fears regarding online risk.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:45:47 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bada", "Maria", ""], ["Clayton", "Richard", ""]]}, {"id": "2012.00648", "submitter": "Prerit Datta", "authors": "Prerit Datta, Natalie Lodinger, Akbar Siami Namin, Keith S. Jones", "title": "Cyber-Attack Consequence Prediction", "comments": "9 pages. The pre-print of a paper to appear in the proceedings of the\n  3rd Workshop on Big Data Engineering and Analytics in Cyber-Physical Systems\n  (BigEACPS'20), IEEE BigData Conference 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cyber-physical systems posit a complex number of security challenges due to\ninterconnection of heterogeneous devices having limited processing,\ncommunication, and power capabilities. Additionally, the conglomeration of both\nphysical and cyber-space further makes it difficult to devise a single security\nplan spanning both these spaces. Cyber-security researchers are often\noverloaded with a variety of cyber-alerts on a daily basis many of which turn\nout to be false positives. In this paper, we use machine learning and natural\nlanguage processing techniques to predict the consequences of cyberattacks. The\nidea is to enable security researchers to have tools at their disposal that\nmakes it easier to communicate the attack consequences with various\nstakeholders who may have little to no cybersecurity expertise. Additionally,\nwith the proposed approach researchers' cognitive load can be reduced by\nautomatically predicting the consequences of attacks in case new attacks are\ndiscovered. We compare the performance through various machine learning models\nemploying word vectors obtained using both tf-idf and Doc2Vec models. In our\nexperiments, an accuracy of 60% was obtained using tf-idf features and 57%\nusing Doc2Vec method for models based on LinearSVC model.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:18:47 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 18:57:49 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Datta", "Prerit", ""], ["Lodinger", "Natalie", ""], ["Namin", "Akbar Siami", ""], ["Jones", "Keith S.", ""]]}, {"id": "2012.00697", "submitter": "\\c{C}a\\u{g}atay Demiralp", "authors": "James Gale and Max Seiden and Gretchen Atwood and Jason Frantz and Rob\n  Woollen and \\c{C}a\\u{g}atay Demiralp", "title": "Sigma Worksheet: Interactive Construction of OLAP Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new generation of cloud data warehouses (CDWs) brings large amounts of\ndata and compute power closer to users in enterprises. The ability to directly\naccess the warehouse data, interactively analyze and explore it at scale can\nempower users to improve their decision making cycles. However, existing tools\nfor analyzing data in CDWs are either limited in ad-hoc transformations or\ndifficult to use for business users, the largest user segment in enterprises.\nHere we introduce Sigma Worksheet, a new interactive system that enables users\nto easily perform ad-hoc visual analysis of data in CDWs at scale. For this,\nSigma Worksheet provides an accessible spreadsheet-like interface for data\nanalysis through direct manipulation. Sigma Worksheet dynamically constructs\nmatching SQL queries from user interactions on this familiar interface,\nbuilding on the versatility and expressivity of SQL. Sigma Worksheet executes\nconstructed queries directly on CDWs, leveraging the superior characteristics\nof the new generation CDWs, including scalability. To evaluate Sigma Worksheet,\nwe first demonstrate its expressivity through two real life use cases, cohort\nanalysis and sessionization. We then measure the performance of the Worksheet\ngenerated queries with a set of experiments using the TPC-H benchmark. Results\nshow the performance of our compiled SQL queries is comparable to that of the\nreference queries of the benchmark. Finally, to assess the usefulness of Sigma\nWorksheet in deployment, we elicit feedback through a 100-person survey\nfollowed by a semi-structured interview study with 70 participants. We find\nthat Sigma Worksheet is easier to use and learn, improving the productivity of\nusers. Our findings also suggest Sigma Worksheet can further improve user\nexperience by providing guidance to users at various steps of data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:56:59 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 05:42:36 GMT"}, {"version": "v3", "created": "Thu, 6 May 2021 00:42:53 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Gale", "James", ""], ["Seiden", "Max", ""], ["Atwood", "Gretchen", ""], ["Frantz", "Jason", ""], ["Woollen", "Rob", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "2012.00822", "submitter": "Haozheng Luo", "authors": "Haozheng Luo, Ruiyang Qin", "title": "Open-Ended Multi-Modal Relational Reason for Video Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People with visual impairments urgently need helps, not only on the basic\ntasks such as guiding and retrieving objects , but on the advanced tasks like\npicturing the new environments. More than a guiding dog, they might want some\ndevices which are able to provide linguistic interaction. Building on various\nresearch literature, we aim to conduct a research on the interaction between\nthe robot agent and visual impaired people. The robot agent, applied VQA\ntechniques, is able to analyze the environment, process and understand the\npronouncing questions, and provide feedback to the human user. In this paper,\nwe are going to discuss the related questions about this kind of interaction,\nthe techniques we used in this work, and how we conduct our research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 20:49:59 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 03:31:34 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Luo", "Haozheng", ""], ["Qin", "Ruiyang", ""]]}, {"id": "2012.00855", "submitter": "Caglar Yildirim", "authors": "Caglar Yildirim", "title": "A Review of Deep Learning Approaches to EEG-Based Classification of\n  Cybersickness in Virtual Reality", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Cybersickness is an unpleasant side effect of exposure to a virtual reality\n(VR) experience and refers to such physiological repercussions as nausea and\ndizziness triggered in response to VR exposure. Given the debilitating effect\nof cybersickness on the user experience in VR, academic interest in the\nautomatic detection of cybersickness from physiological measurements has\ncrested in recent years. Electroencephalography (EEG) has been extensively used\nto capture changes in electrical activity in the brain and to automatically\nclassify cybersickness from brainwaves using a variety of machine learning\nalgorithms. Recent advances in deep learning (DL) algorithms and increasing\navailability of computational resources for DL have paved the way for a new\narea of research into the application of DL frameworks to EEG-based detection\nof cybersickness. Accordingly, this review involved a systematic review of the\npeer-reviewed papers concerned with the application of DL frameworks to the\nclassification of cybersickness from EEG signals. The relevant literature was\nidentified through exhaustive database searches, and the papers were\nscrutinized with respect to experimental protocols for data collection, data\npreprocessing, and DL architectures. The review revealed a limited number of\nstudies in this nascent area of research and showed that the DL frameworks\nreported in these studies (i.e., DNN, CNN, and RNN) could classify\ncybersickness with an average accuracy rate of 93%. This review provides a\nsummary of the trends and issues in the application of DL frameworks to the\nEEG-based detection of cybersickness, with some guidelines for future research.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:50:53 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Yildirim", "Caglar", ""]]}, {"id": "2012.00922", "submitter": "Lauren Hayes", "authors": "Gabriella Isaac, Lauren Hayes and Todd Ingalls", "title": "Cross-Modal Terrains: Navigating Sonic Space through Haptic Feedback", "comments": null, "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2017", "doi": "10.5281/zenodo.1176163", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the idea of using virtual textural terrains as a means of\ngenerating haptic profiles for force-feedback controllers. This approach breaks\nfrom the para-digm established within audio-haptic research over the last few\ndecades where physical models within virtual environments are designed to\ntransduce gesture into sonic output. We outline a method for generating\nmultimodal terrains using basis functions, which are rendered into\nmonochromatic visual representations for inspection. This visual terrain is\ntraversed using a haptic controller, the NovInt Falcon, which in turn receives\nforce information based on the grayscale value of its location in this virtual\nspace. As the image is traversed by a performer the levels of resistance vary,\nand the image is realized as a physical terrain. We discuss the potential of\nthis approach to afford engaging musical experiences for both the performer and\nthe audience as iterated through numerous performances.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 01:44:45 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Isaac", "Gabriella", ""], ["Hayes", "Lauren", ""], ["Ingalls", "Todd", ""]]}, {"id": "2012.00923", "submitter": "Lauren Hayes", "authors": "Lauren Hayes and Adnan Marquez-Borbon", "title": "Nuanced and Interrelated Mediations and Exigencies (NIME): Addressing\n  the Prevailing Political and Epistemological Crises", "comments": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nearly two decades after its inception as a workshop at the ACM Conference on\nHuman Factors in Computing Systems, NIME exists as an established international\nconference significantly distinct from its precursor. While this origin story\nis often noted, the implications of NIME's history as emerging from a field\npredominantly dealing with human-computer interaction have rarely been\ndiscussed. In this paper we highlight many of the recent -- and some not so\nrecent -- challenges that have been brought upon the NIME community as it\nattempts to maintain and expand its identity as a platform for\nmultidisciplinary research into HCI, interface design, and electronic and\ncomputer music. We discuss the relationship between the market demands of the\nneoliberal university -- which have underpinned academia's drive for innovation\n-- and the quantification and economisation of research performance which have\nfacilitated certain disciplinary and social frictions to emerge within\nNIME-related research and practice. Drawing on work that engages with feminist\ntheory and cultural studies, we suggest that critical reflection and moreover\nmediation is necessary in order to address burgeoning concerns which have been\nraised within the NIME discourse in relation to methodological approaches,\n`diversity and inclusion', `accessibility', and the fostering of rigorous\ninterdisciplinary research.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 01:45:15 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Hayes", "Lauren", ""], ["Marquez-Borbon", "Adnan", ""]]}, {"id": "2012.00927", "submitter": "Lauren Hayes", "authors": "Lauren Hayes", "title": "Enacting Musical Worlds: Common Approaches to using NIMEs within\n  Performance and Person-Centred Arts Practices", "comments": null, "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2015", "doi": "10.5281/zenodo.1179082", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Live music making can be understood as an enactive process, whereby musical\nexperiences are created through human action. This suggests that musical worlds\ncoevolve with their agents through repeated sensorimotor interactions with the\nenvironment (where the music is being created), and at the same time cannot be\nseparated from their sociocultural contexts. This paper investigates this claim\nby exploring ways in which technology, physiology, and context are bound up\nwithin two different musical scenarios: live electronic musical performance;\nand person-centred arts applications of NIMEs.\n  In this paper I outline an ethnographic and phenomenological enquiry into my\nexperiences as both a performer of live electronic and electro-instrumental\nmusic, as well as my extensive background in working with new technologies in\nvarious therapeutic and person-centred artistic situations. This is in order to\nexplore the sociocultural and technological contexts in which these activities\ntake place. I propose that by understanding creative musical participation as a\nhighly contextualised practice, we may discover that the greatest impact of\nrapidly developing technological resources is their ability to afford richly\ndiverse, personalised, and embodied forms of music making. I argue that this is\napplicable over a wide range of musical communities.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 01:59:55 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Hayes", "Lauren", ""]]}, {"id": "2012.01007", "submitter": "Julie Gerlings", "authors": "Julie Gerlings, Arisa Shollo, Ioanna Constantiou", "title": "Reviewing the Need for Explainable Artificial Intelligence (xAI)", "comments": "Hawaii International Conference on System Sciences (HICSS) 54\n  Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The diffusion of artificial intelligence (AI) applications in organizations\nand society has fueled research on explaining AI decisions. The explainable AI\n(xAI) field is rapidly expanding with numerous ways of extracting information\nand visualizing the output of AI technologies (e.g. deep neural networks). Yet,\nwe have a limited understanding of how xAI research addresses the need for\nexplainable AI. We conduct a systematic review of xAI literature on the topic\nand identify four thematic debates central to how xAI addresses the black-box\nproblem. Based on this critical analysis of the xAI scholarship we synthesize\nthe findings into a future research agenda to further the xAI body of\nknowledge.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 07:52:41 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 18:04:08 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gerlings", "Julie", ""], ["Shollo", "Arisa", ""], ["Constantiou", "Ioanna", ""]]}, {"id": "2012.01162", "submitter": "Kenn Migan Vincent Gumonan", "authors": "Kenn Migan Vincent C. Gumonan, and Aleta C. Fabregas", "title": "ASIAVR: Asian Studies Virtual Reality Game a Learning Tool", "comments": "14 pages, 10 figures, presented in International Science, Technology\n  and Engineering Conference ISTEC 2019, and published in International Journal\n  of Computing Sciences Research", "journal-ref": null, "doi": "10.25147/ijcsr.2017.001.1.53", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study aims to develop an application that will serve as an alternative\nlearning tool for learning Asian Studies. The delivery of lessons into a\nvirtual reality game depends on the pace of students. The developed application\ncomprises several more features that enable users to get valuable information\nfrom an immersive environment. The researchers used Rapid Application\nDevelopment (RAD) in developing the application. It follows phases such as\nrequirement planning, user design, construction, and cutover. Two sets of\nquestionnaires were developed, one for the teachers and another for the\nstudents. Then, testing and evaluation were conducted through purposive\nsampling to select the respondents. The application was overall rated as 3.56\nwhich is verbally interpreted as very good. The result was based on the system\nevaluation using ISO 9126 in terms of functionality, usability, content,\nreliability, and performance. The developed application meets the objectives to\nprovide an alternative learning tool for learning Asian Studies. The\napplication is well commended and accepted by the end-users to provide an\ninteractive and immersive environment for students to learn at their own pace.\nFurther enhancement of the audio, gameplay, and graphics of the tool. Schools\nshould take into consideration the adoption of the Asian Studies Virtual\nReality is a good alternative tool for their teachers and students to teach and\nlearn Asian Studies. The use of more 3D objects relevant to the given\ninformation to enhance the game experience may be considered. A databank for\nthe quiz questions that will be loaded into the game should also be considered.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 04:24:03 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Gumonan", "Kenn Migan Vincent C.", ""], ["Fabregas", "Aleta C.", ""]]}, {"id": "2012.01165", "submitter": "Chinasa Okolo", "authors": "Chinasa T. Okolo", "title": "AI in the \"Real World\": Examining the Impact of AI Deployment in\n  Low-Resource Contexts", "comments": "Part of the Navigating the Broader Impacts of AI Research Workshop at\n  NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As AI becomes integrated throughout the world, its potential for impact\nwithin low-resource regions around the Global South have grown. AI research\nlabs from tech giants like Microsoft, Google, and IBM have a significant\npresence in countries such as India, Ghana, and South Africa. The work done by\nthese labs is often motivated by the potential impact it could have on local\npopulations, but the deployment of these tools has not always gone smoothly.\nThis paper presents a case study examining the deployment of AI by large\nindustry labs situated in low-resource contexts, highlights factors impacting\nunanticipated deployments, and reflects on the state of AI deployment within\nthe Global South, providing suggestions that embrace inclusive design\nmethodologies within AI development that prioritize the needs of marginalized\ncommunities and elevate their status not just as beneficiaries of AI systems\nbut as primary stakeholders.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 01:49:24 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Okolo", "Chinasa T.", ""]]}, {"id": "2012.01205", "submitter": "Angelos Chatzimparmpas", "authors": "Angelos Chatzimparmpas, Rafael M. Martins, Kostiantyn Kucher, Andreas\n  Kerren", "title": "VisEvol: Visual Analytics to Support Hyperparameter Search through\n  Evolutionary Optimization", "comments": "This manuscript is accepted for publication in a special issue of\n  Computer Graphics Forum (CGF)", "journal-ref": "Computer Graphics Forum 2021, 40(3), 201-214", "doi": "10.1111/cgf.14300", "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the training phase of machine learning (ML) models, it is usually\nnecessary to configure several hyperparameters. This process is computationally\nintensive and requires an extensive search to infer the best hyperparameter set\nfor the given problem. The challenge is exacerbated by the fact that most ML\nmodels are complex internally, and training involves trial-and-error processes\nthat could remarkably affect the predictive result. Moreover, each\nhyperparameter of an ML algorithm is potentially intertwined with the others,\nand changing it might result in unforeseeable impacts on the remaining\nhyperparameters. Evolutionary optimization is a promising method to try and\naddress those issues. According to this method, performant models are stored,\nwhile the remainder are improved through crossover and mutation processes\ninspired by genetic algorithms. We present VisEvol, a visual analytics tool\nthat supports interactive exploration of hyperparameters and intervention in\nthis evolutionary procedure. In summary, our proposed tool helps the user to\ngenerate new models through evolution and eventually explore powerful\nhyperparameter combinations in diverse regions of the extensive hyperparameter\nspace. The outcome is a voting ensemble (with equal rights) that boosts the\nfinal predictive performance. The utility and applicability of VisEvol are\ndemonstrated with two use cases and interviews with ML experts who evaluated\nthe effectiveness of the tool.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 13:43:37 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 18:42:07 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 04:37:57 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chatzimparmpas", "Angelos", ""], ["Martins", "Rafael M.", ""], ["Kucher", "Kostiantyn", ""], ["Kerren", "Andreas", ""]]}, {"id": "2012.01229", "submitter": "Roee Shraga PhD", "authors": "Roee Shraga, Ofra Amir, Avigdor Gal", "title": "Learning to Characterize Matching Experts", "comments": "Accepted by the 37th IEEE International Conference on Data\n  Engineering (ICDE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching is a task at the heart of any data integration process, aimed at\nidentifying correspondences among data elements. Matching problems were\ntraditionally solved in a semi-automatic manner, with correspondences being\ngenerated by matching algorithms and outcomes subsequently validated by human\nexperts. Human-in-the-loop data integration has been recently challenged by the\nintroduction of big data and recent studies have analyzed obstacles to\neffective human matching and validation. In this work we characterize human\nmatching experts, those humans whose proposed correspondences can mostly be\ntrusted to be valid. We provide a novel framework for characterizing matching\nexperts that, accompanied with a novel set of features, can be used to identify\nreliable and valuable human experts. We demonstrate the usefulness of our\napproach using an extensive empirical evaluation. In particular, we show that\nour approach can improve matching results by filtering out inexpert matchers.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:16:38 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Shraga", "Roee", ""], ["Amir", "Ofra", ""], ["Gal", "Avigdor", ""]]}, {"id": "2012.01311", "submitter": "Vladimir Iashin", "authors": "Vladimir Iashin, Francesca Palermo, G\\\"okhan Solak, Claudio Coppola", "title": "Top-1 CORSMAL Challenge 2020 Submission: Filling Mass Estimation Using\n  Multi-modal Observations of Human-robot Handovers", "comments": "Code: https://github.com/v-iashin/CORSMAL Docker:\n  https://hub.docker.com/r/iashin/corsmal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.RO cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human-robot object handover is a key skill for the future of human-robot\ncollaboration. CORSMAL 2020 Challenge focuses on the perception part of this\nproblem: the robot needs to estimate the filling mass of a container held by a\nhuman. Although there are powerful methods in image processing and audio\nprocessing individually, answering such a problem requires processing data from\nmultiple sensors together. The appearance of the container, the sound of the\nfilling, and the depth data provide essential information. We propose a\nmulti-modal method to predict three key indicators of the filling mass: filling\ntype, filling level, and container capacity. These indicators are then combined\nto estimate the filling mass of a container. Our method obtained Top-1 overall\nperformance among all submissions to CORSMAL 2020 Challenge on both public and\nprivate subsets while showing no evidence of overfitting. Our source code is\npublicly available: https://github.com/v-iashin/CORSMAL\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 16:31:03 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Iashin", "Vladimir", ""], ["Palermo", "Francesca", ""], ["Solak", "G\u00f6khan", ""], ["Coppola", "Claudio", ""]]}, {"id": "2012.01553", "submitter": "Lucy Simko", "authors": "Lucy Simko, Jack Lucas Chang, Maggie Jiang, Ryan Calo, Franziska\n  Roesner, Tadayoshi Kohno", "title": "COVID-19 Contact Tracing and Privacy: A Longitudinal Study of Public\n  Opinion", "comments": "37 pages, 11 figures. Supercedes arXiv:2005.06056", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.CR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is growing use of technology-enabled contact tracing, the process of\nidentifying potentially infected COVID-19 patients by notifying all recent\ncontacts of an infected person. Governments, technology companies, and research\ngroups alike have been working towards releasing smartphone apps, using IoT\ndevices, and distributing wearable technology to automatically track \"close\ncontacts\" and identify prior contacts in the event an individual tests\npositive. However, there has been significant public discussion about the\ntensions between effective technology-based contact tracing and the privacy of\nindividuals. To inform this discussion, we present the results of seven months\nof online surveys focused on contact tracing and privacy, each with 100\nparticipants. Our first surveys were on April 1 and 3, before the first peak of\nthe virus in the US, and we continued to conduct the surveys weekly for 10\nweeks (through June), and then fortnightly through November, adding topical\nquestions to reflect current discussions about contact tracing and COVID-19.\nOur results present the diversity of public opinion and can inform policy\nmakers, technologists, researchers, and public health experts on whether and\nhow to leverage technology to reduce the spread of COVID-19, while considering\npotential privacy concerns. We are continuing to conduct longitudinal\nmeasurements and will update this report over time; citations to this version\nof the report should reference Report Version 2.0, December 4, 2020.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 21:50:42 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 19:07:10 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Simko", "Lucy", ""], ["Chang", "Jack Lucas", ""], ["Jiang", "Maggie", ""], ["Calo", "Ryan", ""], ["Roesner", "Franziska", ""], ["Kohno", "Tadayoshi", ""]]}, {"id": "2012.01778", "submitter": "Konstantin Kobs", "authors": "Michael Fischer, Konstantin Kobs, Andreas Hotho", "title": "NICER: Aesthetic Image Enhancement with Humans in the Loop", "comments": "The code can be found at https://github.com/mr-Mojo/NICER", "journal-ref": "ACHI 2020, The Thirteenth International Conference on Advances in\n  Computer-Human Interactions; 2020; pages 357-362", "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully- or semi-automatic image enhancement software helps users to increase\nthe visual appeal of photos and does not require in-depth knowledge of manual\nimage editing. However, fully-automatic approaches usually enhance the image in\na black-box manner that does not give the user any control over the\noptimization process, possibly leading to edited images that do not\nsubjectively appeal to the user. Semi-automatic methods mostly allow for\ncontrolling which pre-defined editing step is taken, which restricts the users\nin their creativity and ability to make detailed adjustments, such as\nbrightness or contrast. We argue that incorporating user preferences by guiding\nan automated enhancement method simplifies image editing and increases the\nenhancement's focus on the user. This work thus proposes the Neural Image\nCorrection & Enhancement Routine (NICER), a neural network based approach to\nno-reference image enhancement in a fully-, semi-automatic or fully manual\nprocess that is interactive and user-centered. NICER iteratively adjusts image\nediting parameters in order to maximize an aesthetic score based on image style\nand content. Users can modify these parameters at any time and guide the\noptimization process towards a desired direction. This interactive workflow is\na novelty in the field of human-computer interaction for image enhancement\ntasks. In a user study, we show that NICER can improve image aesthetics without\nuser interaction and that allowing user interaction leads to diverse\nenhancement outcomes that are strongly preferred over the unedited image. We\nmake our code publicly available to facilitate further research in this\ndirection.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 09:14:10 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fischer", "Michael", ""], ["Kobs", "Konstantin", ""], ["Hotho", "Andreas", ""]]}, {"id": "2012.01792", "submitter": "Petros Ioannidis", "authors": "Petros Ioannidis, Lina Eklund, Anders Sundnes L{\\o}vlie", "title": "We Dare You: A Lifecycle Study of a Substitutional Reality Installation\n  in a Museum Space", "comments": "Accepted to be published in the Journal on Computing and Cultural\n  Heritage (JOCCH)", "journal-ref": null, "doi": "10.1145/3439862", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a lifecycle study of We Dare You, a\nSubstitutional Reality (SR) installation that combines visual and tactile\nstimuli. The installation is set up in a center for architecture, and invites\nvisitors to explore its facade while playing with vertigo, in a visual Virtual\nReality (VR) environment that replicates the surrounding physical space of the\ninstallation. Drawing on an ethnographic approach, including observations and\ninterviews, we researched the exhibit from its opening, through the initial\nmonths plagued by technical problems, its subsequent success as a social and\nplayful installation, on to its closure, due to COVID-19, and its subsequent\nreopening. Our findings explore the challenges caused by both the hybrid nature\nof the installation, as well as the visitor' playful use of the installation\nwhich made the experience social and performative - but also caused some\nproblems. We also discuss the problems We Dare You faced in light of hygiene\ndemands due to COVID-19. The analysis contrasts the design processes and\nexpectations of stakeholders with the audience's playful appropriation, which\nled the stakeholders to see the installation as both a success and a failure.\nEvaluating the design and redesign through use on behalf of visitors, we argue\nthat an approach that further opens up the post-production experience to a\nprocess of continuous redesign based on the user input - what has been termed\n\"design-after-design\" - could facilitate the design of similar experiences in\nthe museum and heritage sector, supporting a participatory agenda in the design\nprocess, and helping to resolve the tension between stakeholders' expectations\nand visitors' playful appropriations.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 09:49:18 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Ioannidis", "Petros", ""], ["Eklund", "Lina", ""], ["L\u00f8vlie", "Anders Sundnes", ""]]}, {"id": "2012.01934", "submitter": "Zohreh Raziei", "authors": "Zohreh Raziei, Mohsen Moghaddam", "title": "Adaptable Automation with Modular Deep Reinforcement Learning and Policy\n  Transfer", "comments": "32 pages, 13 Figures, Presented at 2020 INFORMS Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in deep Reinforcement Learning (RL) have created\nunprecedented opportunities for intelligent automation, where a machine can\nautonomously learn an optimal policy for performing a given task. However,\ncurrent deep RL algorithms predominantly specialize in a narrow range of tasks,\nare sample inefficient, and lack sufficient stability, which in turn hinder\ntheir industrial adoption. This article tackles this limitation by developing\nand testing a Hyper-Actor Soft Actor-Critic (HASAC) RL framework based on the\nnotions of task modularization and transfer learning. The goal of the proposed\nHASAC is to enhance the adaptability of an agent to new tasks by transferring\nthe learned policies of former tasks to the new task via a \"hyper-actor\". The\nHASAC framework is tested on a new virtual robotic manipulation benchmark,\nMeta-World. Numerical experiments show superior performance by HASAC over\nstate-of-the-art deep RL algorithms in terms of reward value, success rate, and\ntask completion time.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 03:09:05 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Raziei", "Zohreh", ""], ["Moghaddam", "Mohsen", ""]]}, {"id": "2012.01975", "submitter": "Silas {\\O}rting", "authors": "Silas Nyboe {\\O}rting", "title": "A small note on variation in segmentation annotations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We report on the results of a small crowdsourcing experiment conducted at a\nworkshop on machine learning for segmentation held at the Danish Bio Imaging\nnetwork meeting 2020. During the workshop we asked participants to manually\nsegment mitochondria in three 2D patches. The aim of the experiment was to\nillustrate that manual annotations should not be seen as the ground truth, but\nas a reference standard that is subject to substantial variation. In this note\nwe show how the large variation we observed in the segmentations can be reduced\nby removing the annotators with worst pair-wise agreement. Having removed the\nannotators with worst performance, we illustrate that the remaining variance is\nsemantically meaningful and can be exploited to obtain segmentations of cell\nboundary and cell interior.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:55:23 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["\u00d8rting", "Silas Nyboe", ""]]}, {"id": "2012.02087", "submitter": "Mohamed Sayed", "authors": "Mohamed Sayed, Robert Cinca, Enrico Costanza, Gabriel Brostow", "title": "LookOut! Interactive Camera Gimbal Controller for Filming Long Takes", "comments": "V2: - Fixed typos. - Cleaner supplemental. - New plot in control\n  section with same data from a supplemental video", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The job of a camera operator is more challenging, and potentially dangerous,\nwhen filming long moving camera shots. Broadly, the operator must keep the\nactors in-frame while safely navigating around obstacles, and while fulfilling\nan artistic vision. We propose a unified hardware and software system that\ndistributes some of the camera operator's burden, freeing them up to focus on\nsafety and aesthetics during a take. Our real-time system provides a solo\noperator with end-to-end control, so they can balance on-set responsiveness to\naction vs planned storyboards and framing, while looking where they're going.\nBy default, we film without a field monitor.\n  Our LookOut system is built around a lightweight commodity camera gimbal\nmechanism, with heavy modifications to the controller, which would normally\njust provide active stabilization. Our control algorithm reacts to speech\ncommands, video, and a pre-made script. Specifically, our automatic monitoring\nof the live video feed saves the operator from distractions. In pre-production,\nan artist uses our GUI to design a sequence of high-level camera \"behaviors.\"\nThose can be specific, based on a storyboard, or looser objectives, such as\n\"frame both actors.\" Then during filming, a machine-readable script, exported\nfrom the GUI, ties together with the sensor readings to drive the gimbal. To\nvalidate our algorithm, we compared tracking strategies, interfaces, and\nhardware protocols, and collected impressions from a) film-makers who used all\naspects of our system, and b) film-makers who watched footage filmed using\nLookOut.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:20:45 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2020 22:04:49 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sayed", "Mohamed", ""], ["Cinca", "Robert", ""], ["Costanza", "Enrico", ""], ["Brostow", "Gabriel", ""]]}, {"id": "2012.02299", "submitter": "Anton Smerdov", "authors": "Anton Smerdov, Andrey Somov, Evgeny Burnaev, Bo Zhou, Paul Lukowicz", "title": "Detecting Video Game Player Burnout with the Use of Sensor Data and\n  Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current research in eSports lacks the tools for proper game practising and\nperformance analytics. The majority of prior work relied only on in-game data\nfor advising the players on how to perform better. However, in-game mechanics\nand trends are frequently changed by new patches limiting the lifespan of the\nmodels trained exclusively on the in-game logs. In this article, we propose the\nmethods based on the sensor data analysis for predicting whether a player will\nwin the future encounter. The sensor data were collected from 10 participants\nin 22 matches in League of Legends video game. We have trained machine learning\nmodels including Transformer and Gated Recurrent Unit to predict whether the\nplayer wins the encounter taking place after some fixed time in the future. For\n10 seconds forecasting horizon Transformer neural network architecture achieves\nROC AUC score 0.706. This model is further developed into the detector capable\nof predicting that a player will lose the encounter occurring in 10 seconds in\n88.3% of cases with 73.5% accuracy. This might be used as a players' burnout or\nfatigue detector, advising players to retreat. We have also investigated which\nphysiological features affect the chance to win or lose the next in-game\nencounter.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 21:16:09 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Smerdov", "Anton", ""], ["Somov", "Andrey", ""], ["Burnaev", "Evgeny", ""], ["Zhou", "Bo", ""], ["Lukowicz", "Paul", ""]]}, {"id": "2012.02311", "submitter": "Charles Martin", "authors": "Charles Patrick Martin and Zeruo Liu and Yichen Wang and Wennan He and\n  Henry Gardner", "title": "Sonic Sculpture: Activating Engagement with Head-Mounted Augmented\n  Reality", "comments": null, "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2020, pp. 48-52", "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work examines how head-mounted AR can be used to build an interactive\nsonic landscape to engage with a public sculpture. We describe a sonic artwork,\n\"Listening To Listening\", that has been designed to accompany a real-world\nsculpture with two prototype interaction schemes. Our artwork is created for\nthe HoloLens platform so that users can have an individual experience in a\nmixed reality context. Personal head-mounted AR systems have recently become\navailable and practical for integration into public art projects, however\nresearch into sonic sculpture works has yet to account for the affordances of\ncurrent portable and mainstream AR systems. In this work, we take advantage of\nthe HoloLens' spatial awareness to build sonic spaces that have a precise\nspatial relationship to a given sculpture and where the sculpture itself is\nmodelled in the augmented scene as an \"invisible hologram\". We describe the\nartistic rationale for our artwork, the design of the two interaction schemes,\nand the technical and usability feedback that we have obtained from\ndemonstrations during iterative development.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 22:35:49 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Martin", "Charles Patrick", ""], ["Liu", "Zeruo", ""], ["Wang", "Yichen", ""], ["He", "Wennan", ""], ["Gardner", "Henry", ""]]}, {"id": "2012.02319", "submitter": "Ningcheng Li", "authors": "Ningcheng Li, Jonathan Wakim, Yilun Koethe, Timothy Huber, Terence\n  Gade, Stephen Hunt, Brian Park", "title": "Multicenter Assessment of Augmented Reality Registration Methods for\n  Image-guided Interventions", "comments": "16 pages, 5 figures (plus 1 supplemental figure), and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To evaluate manual and automatic registration times as well as\naccuracy with augmented reality during alignment of a holographic 3-dimensional\n(3D) model onto the real-world environment.\n  Method: 18 participants in various stages of clinical training across two\nacademic centers registered a 3D CT phantom model onto a CT grid using the\nHoloLens 2 augmented reality headset 3 consecutive times. Registration times\nand accuracy were compared among different registration methods (hand gesture,\nXbox controller, and automatic registration), levels of clinical experience,\nand consecutive attempts. Registration times were also compared with prior\nHoloLens 1 data.\n  Results: Mean aggregate manual registration times were 27.7, 24.3, and 72.8\nseconds for one-handed gesture, two-handed gesture, and Xbox controller,\nrespectively; mean automatic registration time was 5.3s (ANOVA p<0.0001). No\nsignificant difference in registration times was found among attendings,\nresidents and fellows, and medical students (p>0.05). Significant improvements\nin registration times were detected across consecutive attempts using hand\ngestures (p<0.01). Compared with previously reported HoloLens 1 experience,\nhand gesture registration times were 81.7% faster (p<0.05). Registration\naccuracies were not significantly different across manual registration methods,\nmeasuring at 5.9, 9.5, and 8.6 mm with one-handed gesture, two-handed gesture,\nand Xbox controller, respectively (p>0.05).\n  Conclusions: Manual registration times decreased significantly with updated\nhand gesture maneuvers on HoloLens 2 versus HoloLens 1, approaching the\nregistration times of automatic registration and outperforming Xbox controller\nmediated registration. These results will encourage wider clinical integration\nof HoloLens 2 in procedural medical care.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 23:05:48 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Li", "Ningcheng", ""], ["Wakim", "Jonathan", ""], ["Koethe", "Yilun", ""], ["Huber", "Timothy", ""], ["Gade", "Terence", ""], ["Hunt", "Stephen", ""], ["Park", "Brian", ""]]}, {"id": "2012.02322", "submitter": "Charles Martin", "authors": "Rohan Proctor and Charles Patrick Martin", "title": "A Laptop Ensemble Performance System using Recurrent Neural Networks", "comments": null, "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2020, pp. 43-48", "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The popularity of applying machine learning techniques in musical domains has\ncreated an inherent availability of freely accessible pre-trained neural\nnetwork (NN) models ready for use in creative applications. This work outlines\nthe implementation of one such application in the form of an assistance tool\ndesigned for live improvisational performances by laptop ensembles. The primary\nintention was to leverage off-the-shelf pre-trained NN models as a basis for\nassisting individual performers either as musical novices looking to engage\nwith more experienced performers or as a tool to expand musical possibilities\nthrough new forms of creative expression. The system expands upon a variety of\nideas found in different research areas including new interfaces for musical\nexpression, generative music and group performance to produce a networked\nperformance solution served via a web-browser interface. The final\nimplementation of the system offers performers a mixture of high and low-level\ncontrols to influence the shape of sequences of notes output by locally run NN\nmodels in real time, also allowing performers to define their level of\nengagement with the assisting generative models. Two test performances were\nplayed, with the system shown to feasibly support four performers over a four\nminute piece while producing musically cohesive and engaging music. Iterations\non the design of the system exposed technical constraints on the use of a\nJavaScript environment for generative models in a live music context, largely\nderived from inescapable processing overheads.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 23:11:16 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Proctor", "Rohan", ""], ["Martin", "Charles Patrick", ""]]}, {"id": "2012.02404", "submitter": "Charles Martin", "authors": "Charles Patrick Martin and Alexander Refsum Jensenius and Jim Torresen", "title": "Composing an Ensemble Standstill Work for Myo and Bela", "comments": null, "journal-ref": "Proceedings of the International Conference on New Interfaces for\n  Musical Expression, 2018, pp. 196-197", "doi": "10.5281/zenodo.1302543", "report-no": null, "categories": "cs.HC cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the process of developing a standstill performance work\nusing the Myo gesture control armband and the Bela embedded computing platform.\nThe combination of Myo and Bela allows a portable and extensible version of the\nstandstill performance concept while introducing muscle tension as an\nadditional control parameter. We describe the technical details of our setup\nand introduce Myo-to-Bela and Myo-to-OSC software bridges that assist with\nprototyping compositions using the Myo controller.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 05:03:58 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Martin", "Charles Patrick", ""], ["Jensenius", "Alexander Refsum", ""], ["Torresen", "Jim", ""]]}, {"id": "2012.02569", "submitter": "C G", "authors": "C Graves", "title": "Assessing the usability of the mobile application game Call of Duty", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The technological advances in the smartphone application market has expanded\nat great lengths in recent years (Kaya, Ozturk & Gumussoy 2019). One of the\nmain smartphone applications that often sees record breaking downloads are\ngaming apps. The gaming application that recently received the most downloads\nof all time, 100 million in one week, was the mobile game Call of Duty (CoD).\nWith the game generating revenues of over $8 million across app stores, it is\nimportant that the users are satisfied with the game and its experience\n(Cuthbertson, 2019). Unlike other applications, gamers are drawn to gaming\napplications for their experience more than their functionality. Ensuring that\nthe game is performing with efficiency and of a high level of satisfaction, is\nkey in enhancing the users experience and remaining popular in such a\ncompetitive market (Barnett, Harvey and Gatzidis 2018). There are significant\nadvances of mobile applications such as portability and accessibility but the\nchange in architecture has meant that some aspects of design and usability have\nhad to be constrained. Therefore, mobile games need to be more user focused\n(Nayebi, Desharnais and Abran 2012).\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 10:22:52 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Graves", "C", ""]]}, {"id": "2012.02570", "submitter": "Sebastian Stefan Feger", "authors": "Sebastian Stefan Feger", "title": "Interactive Tools for Reproducible Science -- Understanding, Supporting,\n  and Motivating Reproducible Science Practices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reproducibility should be a cornerstone of science as it enables validation\nand reuse. In recent years, the scientific community and the general public\nbecame increasingly aware of the reproducibility crisis, i.e. the wide-spread\ninability of researchers to reproduce published work, including their own.\nScientific research is increasingly focused on the creation, observation,\nprocessing, and analysis of large data volumes. On the one hand, this\ntransition towards computational and data-intensive science poses new\nchallenges for research reproducibility and reuse. On the other hand, increased\navailability and advances in computation and web technologies offer new\nopportunities to address the reproducibility crisis. This thesis reports on\nuser-centered design research conducted at CERN, a key laboratory in\ndata-intensive particle physics.\n  In this thesis, we build a wider understanding of researchers' interactions\nwith tools that support research documentation, preservation, and sharing. From\na Human-Computer Interaction (HCI) perspective the following aspects are\nfundamental: (1) Characterize and map requirements and practices around\nresearch preservation and reuse. (2) Understand the wider role and impact of\nresearch data management (RDM) tools in scientific workflows. (3) Design tools\nand interactions that promote, motivate, and acknowledge reproducible research\npractices. Research reported in this thesis represents the first systematic\napplication of HCI methods in the study and design of interactive tools for\nreproducible science. We advocate the unique role of HCI in supporting,\nmotivating, and transforming reproducible research practices through the design\nof tools that enable effective RDM. This thesis paves new ways for interaction\nwith RDM tools that support and motivate reproducible science.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 10:32:03 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Feger", "Sebastian Stefan", ""]]}, {"id": "2012.02571", "submitter": "Babis Magoutas", "authors": "Babis Magoutas, Gregoris Mentzas", "title": "Adaptivity and Personalization Application Scenarios in eParticipation", "comments": "eParticipation, 1st International Conference, ePart 2009, Linz,\n  Austria, September 1-3, 2009 Trauner Druck: Linz, Schriftenreihe Informatik #\n  31, pp. 205-215, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Adaptivity and personalization technologies appear not to be very much used\nin eparticipation projects to date. These technologies are commonly used to\novercome the overflow of information and service providers adopt them in order\nto acquire a better knowledge of their end-users and optimize their service\nofferings. In this paper we investigate the potential of adaptivity and\npersonalization principles and technologies when applied to the eParticipation\nfield and more specifically, to eParticipation websites. Potential application\nscenarios of these technologies in the context of policy engagement and active\nparticipation of citizens in democratic decision-making are defined and their\nimpact in eParticipation is examined.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:46:07 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Magoutas", "Babis", ""], ["Mentzas", "Gregoris", ""]]}, {"id": "2012.02596", "submitter": "Alex Deakyne", "authors": "Alex J. Deakyne, Erik N. Gaasedelen, Tinen L. Iles, and Paul A. Iaizzo", "title": "Immersive Anatomical Scenes that Enable Multiple Users to Occupy the\n  Same Virtual Space: A Tool for Surgical Planning and Education", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D modeling is becoming a well-developed field of medicine, but its\napplicability can be limited due to the lack of software allowing for easy\nutilizations of generated 3D visualizations. By leveraging recent advances in\nvirtual reality, we can rapidly create immersive anatomical scenes as well as\nallow multiple users to occupy the same virtual space: i.e., over a local or\ndistributed network. This setup is ideal for pre-surgical planning and\neducation, allowing users to identify and study structures of interest. I\ndemonstrate here such a pipeline on a broad spectrum of anatomical models and\ndiscuss its applicability to the medical field and its future prospects.3D\nmodeling is becoming a well-developed field of medicine, but its applicability\ncan be limited due to the lack of software allowing for easy utilizations of\ngenerated 3D visualizations. By leveraging recent advances in virtual reality,\nwe can rapidly create immersive anatomical scenes as well as allow multiple\nusers to occupy the same virtual space: i.e., over a local or distributed\nnetwork. This setup is ideal for pre-surgical planning and education, allowing\nusers to identify and study structures of interest. I demonstrate here such a\npipeline on a broad spectrum of anatomical models and discuss its applicability\nto the medical field and its future prospects.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 15:57:06 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Deakyne", "Alex J.", ""], ["Gaasedelen", "Erik N.", ""], ["Iles", "Tinen L.", ""], ["Iaizzo", "Paul A.", ""]]}, {"id": "2012.02748", "submitter": "Jonathan Dinu", "authors": "Jonathan Dinu (1), Jeffrey Bigham (2), J. Zico Kolter (2) ((1)\n  Unaffiliated, (2) Carnegie Mellon University)", "title": "Challenging common interpretability assumptions in feature attribution\n  explanations", "comments": "Presented at the NeurIPS 2020 ML-Retrospectives, Surveys &\n  Meta-Analyses Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As machine learning and algorithmic decision making systems are increasingly\nbeing leveraged in high-stakes human-in-the-loop settings, there is a pressing\nneed to understand the rationale of their predictions. Researchers have\nresponded to this need with explainable AI (XAI), but often proclaim\ninterpretability axiomatically without evaluation. When these systems are\nevaluated, they are often tested through offline simulations with proxy metrics\nof interpretability (such as model complexity). We empirically evaluate the\nveracity of three common interpretability assumptions through a large scale\nhuman-subjects experiment with a simple \"placebo explanation\" control. We find\nthat feature attribution explanations provide marginal utility in our task for\na human decision maker and in certain cases result in worse decisions due to\ncognitive and contextual confounders. This result challenges the assumed\nuniversal benefit of applying these methods and we hope this work will\nunderscore the importance of human evaluation in XAI research. Supplemental\nmaterials -- including anonymized data from the experiment, code to replicate\nthe study, an interactive demo of the experiment, and the models used in the\nanalysis -- can be found at: https://doi.pizza/challenging-xai.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 17:57:26 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Dinu", "Jonathan", ""], ["Bigham", "Jeffrey", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2012.02898", "submitter": "Isaac Lage", "authors": "Isaac Lage, Finale Doshi-Velez", "title": "Learning Interpretable Concept-Based Models with Human Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning models that first learn a representation of a domain in\nterms of human-understandable concepts, then use it to make predictions, have\nbeen proposed to facilitate interpretation and interaction with models trained\non high-dimensional data. However these methods have important limitations: the\nway they define concepts are not inherently interpretable, and they assume that\nconcept labels either exist for individual instances or can easily be acquired\nfrom users. These limitations are particularly acute for high-dimensional\ntabular features. We propose an approach for learning a set of transparent\nconcept definitions in high-dimensional tabular data that relies on users\nlabeling concept features instead of individual instances. Our method produces\nconcepts that both align with users' intuitive sense of what a concept means,\nand facilitate prediction of the downstream label by a transparent machine\nlearning model. This ensures that the full model is transparent and intuitive,\nand as predictive as possible given this constraint. We demonstrate with\nsimulated user feedback on real prediction problems, including one in a\nclinical domain, that this kind of direct feedback is much more efficient at\nlearning solutions that align with ground truth concept definitions than\nalternative transparent approaches that rely on labeling instances or other\nexisting interaction mechanisms, while maintaining similar predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:41:05 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lage", "Isaac", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "2012.02904", "submitter": "Jason Wilson", "authors": "Jason R. Wilson, Leilani Gilpin, Irina Rabkina", "title": "A Knowledge Driven Approach to Adaptive Assistance Using Preference\n  Reasoning and Explanation", "comments": "Accepted for presentation at the AAAI 2020 Fall Symposium Series, in\n  the symposium for Artificial Intelligence for Human-Robot Interaction: Trust\n  & Explainability in Artificial Intelligence for Human-Robot Interaction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a need for socially assistive robots (SARs) to provide transparency\nin their behavior by explaining their reasoning. Additionally, the reasoning\nand explanation should represent the user's preferences and goals. To work\ntowards satisfying this need for interpretable reasoning and representations,\nwe propose the robot uses Analogical Theory of Mind to infer what the user is\ntrying to do and uses the Hint Engine to find an appropriate assistance based\non what the user is trying to do. If the user is unsure or confused, the robot\nprovides the user with an explanation, generated by the Explanation\nSynthesizer. The explanation helps the user understand what the robot inferred\nabout the user's preferences and why the robot decided to provide the\nassistance it gave. A knowledge-driven approach provides transparency to\nreasoning about preferences, assistance, and explanations, thereby facilitating\nthe incorporation of user feedback and allowing the robot to learn and adapt to\nthe user.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 00:18:43 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Wilson", "Jason R.", ""], ["Gilpin", "Leilani", ""], ["Rabkina", "Irina", ""]]}, {"id": "2012.02927", "submitter": "Anam Ahmad Khan", "authors": "Anam Ahmad Khan, Sadia Nawaz, Joshua Newn, Jason M. Lodge, James\n  Bailey, Eduardo Velloso", "title": "Using voice note-taking to promote learners' conceptual understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though recent technological advances have enabled note-taking through\ndifferent modalities (e.g., keyboard, digital ink, voice), there is still a\nlack of understanding of the effect of the modality choice on learning. In this\npaper, we compared two note-taking input modalities -- keyboard and voice -- to\nstudy their effects on participants' learning. We conducted a study with 60\nparticipants in which they were asked to take notes using voice or keyboard on\ntwo independent digital text passages while also making a judgment about their\nperformance on an upcoming test. We built mixed-effects models to examine the\neffect of the note-taking modality on learners' text comprehension, the content\nof notes and their meta-comprehension judgement. Our findings suggest that\ntaking notes using voice leads to a higher conceptual understanding of the text\nwhen compared to typing the notes. We also found that using voice also triggers\ngenerative processes that result in learners taking more elaborate and\ncomprehensive notes. The findings of the study imply that note-taking tools\ndesigned for digital learning environments could incorporate voice as an input\nmodality to promote effective note-taking and conceptual understanding of the\ntext.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 02:32:59 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Khan", "Anam Ahmad", ""], ["Nawaz", "Sadia", ""], ["Newn", "Joshua", ""], ["Lodge", "Jason M.", ""], ["Bailey", "James", ""], ["Velloso", "Eduardo", ""]]}, {"id": "2012.02961", "submitter": "Yerbolat Khassanov", "authors": "Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat\n  Khassanov, Michael Lewis and Huseyin Atakan Varol", "title": "SpeakingFaces: A Large-Scale Multimodal Dataset of Voice Commands with\n  Visual and Thermal Video Streams", "comments": "20 pages, 9 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present SpeakingFaces as a publicly-available large-scale multimodal\ndataset developed to support machine learning research in contexts that utilize\na combination of thermal, visual, and audio data streams; examples include\nhuman-computer interaction, biometric authentication, recognition systems,\ndomain transfer, and speech recognition. SpeakingFaces is comprised of aligned\nhigh-resolution thermal and visual spectra image streams of fully-framed faces\nsynchronized with audio recordings of each subject speaking approximately 100\nimperative phrases. Data were collected from 142 subjects, yielding over 13,000\ninstances of synchronized data (~3.8 TB). For technical validation, we\ndemonstrate two baseline examples. The first baseline shows classification by\ngender, utilizing different combinations of the three data streams in both\nclean and noisy environments. The second example consists of thermal-to-visual\nfacial image translation, as an instance of domain transfer.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 06:49:42 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 05:00:44 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 05:27:42 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Abdrakhmanova", "Madina", ""], ["Kuzdeuov", "Askat", ""], ["Jarju", "Sheikh", ""], ["Khassanov", "Yerbolat", ""], ["Lewis", "Michael", ""], ["Varol", "Huseyin Atakan", ""]]}, {"id": "2012.02996", "submitter": "Mitchell van Zuijlen", "authors": "Mitchell J.P. van Zuijlen, Hubert Lin, Kavita Bala, Sylvia C. Pont,\n  Maarten W.A. Wijntjes", "title": "Materials In Paintings (MIP): An interdisciplinary dataset for\n  perception, art history, and computer vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A painter is free to modify how components of a natural scene are depicted,\nwhich can lead to a perceptually convincing image of the distal world. This\nsignals a major difference between photos and paintings: paintings are\nexplicitly created for human perception. Studying these painterly depictions\ncould be beneficial to a multidisciplinary audience. In this paper, we capture\nand explore the painterly depictions of materials to enable the study of\ndepiction and perception of materials through the artists' eye. We annotated a\ndataset of 19k paintings with 200k+ bounding boxes from which polygon segments\nwere automatically extracted. Each bounding box was assigned a coarse label\n(e.g., fabric) and a fine-grained label (e.g., velvety, silky). We demonstrate\nthe cross-disciplinary utility of our dataset by presenting novel findings\nacross art history, human perception, and computer vision. Our experiments\ninclude analyzing the distribution of materials depicted in paintings, showing\nhow painters create convincing depictions using a stylized approach, and\ndemonstrating how paintings can be used to build more robust computer vision\nmodels. We conclude that our dataset of painterly material depictions is a rich\nsource for gaining insights into the depiction and perception of materials\nacross multiple disciplines. The MIP dataset is freely accessible at\nhttps://materialsinpaintings.tudelft.nl\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 10:32:36 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 21:21:25 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["van Zuijlen", "Mitchell J. P.", ""], ["Lin", "Hubert", ""], ["Bala", "Kavita", ""], ["Pont", "Sylvia C.", ""], ["Wijntjes", "Maarten W. A.", ""]]}, {"id": "2012.03309", "submitter": "Jan Smeddinck", "authors": "Jan David Smeddinck", "title": "Human-Computer Interaction with Adaptable & Adaptive Motion-based Games\n  for Health", "comments": "200 pages. Dissertation ePrint. Original publication date: 2017\n  January 24", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Physical activity plays a major role both in prevention and in the treatment\nof afflictions linked to a modern sedentary lifestyle and improvements on life\nexpectancy, for example though the application area of physiotherapy.\nMotion-based games for health (MGH) are being discussed in research and\nindustry for their ability to play a supportive role in health, by offering\nmotivation to engage in treatments, objective insights on status and\ndevelopment, and guidance regarding treatment activities. Difficulty settings\nin games are typically limited to few discrete tiers. For most serious\napplications in health, more fine-grained and far-reaching adjustments are\nrequired. The need for applying adjustments on complex sets of parameters can\nbe overwhelming for patient-players and even trained professionals. Automatic\nadaptivity and efficient manual adaptability are thus major concerns for the\ndesign and development of MGH. Despite a growing amount of research on specific\nmethods for adaptivity, general considerations on human-computer interaction\nwith adaptable and adaptive MGH are rare. This thesis therefore focuses on\nestablishing and augmenting theory for adaptability and adaptivity in\nhuman-computer interaction in the context of MGH. Working with older adults and\npeople with Parkinson's disease as frequent target groups that can benefit from\ntailored activities, explorations and comparative studies that investigate the\ndesign, acceptance, and effectiveness of MGH are presented. The outcomes\nencourage the application of adaptivity for MGH following iterative\nhuman-centred design that considers the respective interests of stakeholders,\nprovided that the users receive adequate information and are empowered to exert\ncontrol over the automated system when desired or required, and if adaptivity\nis embedded in such a way that it does not interfere with the users' sense of\ncompetence or autonomy.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 16:25:06 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Smeddinck", "Jan David", ""]]}, {"id": "2012.03491", "submitter": "Anton Smerdov", "authors": "Anton Smerdov, Evgeny Burnaev, Andrey Somov", "title": "AI-enabled Prediction of eSports Player Performance Using the Data from\n  Heterogeneous Sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging progress of eSports lacks the tools for ensuring high-quality\nanalytics and training in Pro and amateur eSports teams. We report on an\nArtificial Intelligence (AI) enabled solution for predicting the eSports player\nin-game performance using exclusively the data from sensors. For this reason,\nwe collected the physiological, environmental, and the game chair data from Pro\nand amateur players. The player performance is assessed from the game logs in a\nmultiplayer game for each moment of time using a recurrent neural network. We\nhave investigated that attention mechanism improves the generalization of the\nnetwork and provides the straightforward feature importance as well. The best\nmodel achieves ROC AUC score 0.73. The prediction of the performance of\nparticular player is realized although his data are not utilized in the\ntraining set. The proposed solution has a number of promising applications for\nPro eSports teams as well as a learning tool for amateur players.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 07:31:53 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Smerdov", "Anton", ""], ["Burnaev", "Evgeny", ""], ["Somov", "Andrey", ""]]}, {"id": "2012.03507", "submitter": "Dae-Hyeok Lee", "authors": "Dae-Hyeok Lee, Hyung-Ju Ahn, Ji-Hoon Jeong, Seong-Whan Lee", "title": "Design of an EEG-based Drone Swarm Control System using Endogenous BCI\n  Paradigms", "comments": "Submitted IEEE The 9th International Winter Conference on\n  Brain-Computer Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive brain-computer interface (BCI) has been developed for\nunderstanding users' intentions by using electroencephalogram (EEG) signals.\nWith the recent development of artificial intelligence, there have been many\ndevelopments in the drone control system. BCI characteristic that can reflect\nthe users' intentions led to the BCI-based drone control system. When using\ndrone swarm, we can have more advantages, such as mission diversity, than using\na single drone. In particular, BCI-based drone swarm control could provide many\nadvantages to various industries such as military service or industry disaster.\nBCI Paradigms consist of the exogenous and endogenous paradigms. The endogenous\nparadigms can operate with the users' intentions independently of any stimulus.\nIn this study, we designed endogenous paradigms (i.e., motor imagery (MI),\nvisual imagery (VI), and speech imagery (SI)) specialized in drone swarm\ncontrol, and EEG-based various task classifications related to drone swarm\ncontrol were conducted. Five subjects participated in the experiment and the\nperformance was evaluated using the basic machine learning algorithm. The\ngrand-averaged accuracies were 51.1%, 53.2%, and 41.9% in MI, VI, and SI,\nrespectively. Hence, we confirmed the feasibility of increasing the degree of\nfreedom for drone swarm control using various endogenous paradigms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 07:58:18 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 01:18:45 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lee", "Dae-Hyeok", ""], ["Ahn", "Hyung-Ju", ""], ["Jeong", "Ji-Hoon", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2012.03520", "submitter": "Seo-Hyun Lee", "authors": "Seo-Hyun Lee, Minji Lee, and Seong-Whan Lee", "title": "Functional Connectivity of Imagined Speech and Visual Imagery based on\n  Spectral Dynamics", "comments": "Submitted to IEEE The 9th International Winter Conference on\n  Brain-Computer Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in brain-computer interface technology have shown the\npotential of imagined speech and visual imagery as a robust paradigm for\nintuitive brain-computer interface communication. However, the internal\ndynamics of the two paradigms along with their intrinsic features haven't been\nrevealed. In this paper, we investigated the functional connectivity of the two\nparadigms, considering various frequency ranges. The dataset of sixteen\nsubjects performing thirteen-class imagined speech and visual imagery were used\nfor the analysis. The phase-locking value of imagined speech and visual imagery\nwas analyzed in seven cortical regions with four frequency ranges. We compared\nthe functional connectivity of imagined speech and visual imagery with the\nresting state to investigate the brain alterations during the imagery. The\nphase-locking value in the whole brain region exhibited a significant decrease\nduring both imagined speech and visual imagery. Broca and Wernicke's area along\nwith the auditory cortex mainly exhibited a significant decrease in the\nimagined speech, and the prefrontal cortex and the auditory cortex have shown a\nsignificant decrease in the visual imagery paradigm. Further investigation on\nthe brain connectivity along with the decoding performance of the two paradigms\nmay play a crucial role as a performance predictor.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:17:20 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 15:19:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lee", "Seo-Hyun", ""], ["Lee", "Minji", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2012.03533", "submitter": "Dong-Kyun Han", "authors": "Dong-Kyun Han, Ji-Hoon Jeong", "title": "Domain Generalization for Session-Independent Brain-Computer Interface", "comments": "Submitted IEEE The 9th International Winter Conference on\n  Brain-Computer Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inter/intra-subject variability of electroencephalography (EEG) makes the\npractical use of the brain-computer interface (BCI) difficult. In general, the\nBCI system requires a calibration procedure to acquire subject/session-specific\ndata to tune the model every time the system is used. This problem is\nrecognized as a major obstacle to BCI, and to overcome it, an approach based on\ndomain generalization (DG) has recently emerged. The main purpose of this paper\nis to reconsider how the zero-calibration problem of BCI for a realistic\nsituation can be overcome from the perspective of DG tasks. In terms of the\nrealistic situation, we have focused on creating an EEG classification\nframework that can be applied directly in unseen sessions, using only\nmulti-subject/-session data acquired previously. Therefore, in this paper, we\ntested four deep learning models and four DG algorithms through\nleave-one-session-out validation. Our experiment showed that deeper and larger\nmodels were effective in cross-session generalization performance. Furthermore,\nwe found that none of the explicit DG algorithms outperformed empirical risk\nminimization. Finally, by comparing the results of fine-tuning using\nsubject-specific data, we found that subject-specific data may deteriorate\nunseen session classification performance due to inter-session variability.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 08:47:42 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Han", "Dong-Kyun", ""], ["Jeong", "Ji-Hoon", ""]]}, {"id": "2012.03598", "submitter": "Md. Rabiul Islam", "authors": "Md. Rabiul Islam, Shuji Sakamoto, Yoshihiro Yamada, Andrew Vargo,\n  Motoi Iwata, Masakazu Iwamura, Koichi Kise", "title": "Self-supervised Deep Learning for Reading Activity Classification", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reading analysis can give important information about a user's confidence and\nhabits and can be used to construct feedback to improve a user's reading\nbehavior. A lack of labeled data inhibits the effective application of\nfully-supervised Deep Learning (DL) for automatic reading analysis. In this\npaper, we propose a self-supervised DL method for reading analysis and evaluate\nit on two classification tasks. We first evaluate the proposed self-supervised\nDL method on a four-class classification task on reading detection using\nelectrooculography (EOG) glasses datasets, followed by an evaluation of a\ntwo-class classification task of confidence estimation on answers of\nmultiple-choice questions (MCQs) using eye-tracking datasets. Fully-supervised\nDL and support vector machines (SVMs) are used to compare the performance of\nthe proposed self-supervised DL method. The results show that the proposed\nself-supervised DL method is superior to the fully-supervised DL and SVM for\nboth tasks, especially when training data is scarce. This result indicates that\nthe proposed self-supervised DL method is the superior choice for reading\nanalysis tasks. The results of this study are important for informing the\ndesign and implementation of automatic reading analysis platforms.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:36:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Islam", "Md. Rabiul", ""], ["Sakamoto", "Shuji", ""], ["Yamada", "Yoshihiro", ""], ["Vargo", "Andrew", ""], ["Iwata", "Motoi", ""], ["Iwamura", "Masakazu", ""], ["Kise", "Koichi", ""]]}, {"id": "2012.03617", "submitter": "Byoung-Hee Kwon", "authors": "Byoung-Hee Kwon, Byeong-Hoo Lee, Ji-Hoon Jeong", "title": "Motor Imagery Classification Emphasizing Corresponding Frequency Domain\n  Method based on Deep Learning Framework", "comments": "Submitted IEEE The 9th International Winter Conference on\n  Brain-Computer Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electroencephalogram, a type of non-invasive-based brain signal that has\na user intention-related feature provides an efficient bidirectional pathway\nbetween user and computer. In this work, we proposed a deep learning framework\nbased on corresponding frequency empahsize method to decode the motor imagery\n(MI) data from 2020 International BCI competition dataset. The MI dataset\nconsists of 3-class, namely 'Cylindrical', 'Spherical', and 'Lumbrical'. We\nutilized power spectral density as an emphasize method and a convolutional\nneural network to classify the modified MI data. The results showed that\nMI-related frequency range was activated during MI task, and provide\nneurophysiological evidence to design the proposed method. When using the\nproposed method, the average classification performance in intra-session\ncondition was 69.68% and the average classification performance in\ninter-session condition was 52.76%. Our results provided the possibility of\ndeveloping a BCI-based device control system for practical applications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:09:30 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kwon", "Byoung-Hee", ""], ["Lee", "Byeong-Hoo", ""], ["Jeong", "Ji-Hoon", ""]]}, {"id": "2012.03632", "submitter": "Byeong-Hoo Lee", "authors": "Byeong-Hoo Lee, Byeong-Hee Kwon, Do-Yeun Lee, Ji-Hoon Jeong", "title": "Speech Imagery Classification using Length-Wise Training based on Deep\n  Learning", "comments": "Submitted IEEE The 9th International Winter Conference on\n  Brain-Computer Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interface uses brain signals to control external devices\nwithout actual control behavior. Recently, speech imagery has been studied for\ndirect communication using language. Speech imagery uses brain signals\ngenerated when the user imagines speech. Unlike motor imagery, speech imagery\nstill has unknown characteristics. Additionally, electroencephalography has\nintricate and non-stationary properties resulting in insufficient decoding\nperformance. In addition, speech imagery is difficult to utilize spatial\nfeatures. In this study, we designed length-wise training that allows the model\nto classify a series of a small number of words. In addition, we proposed\nhierarchical convolutional neural network structure and loss function to\nmaximize the training strategy. The proposed method showed competitive\nperformance in speech imagery classification. Hence, we demonstrated that the\nlength of the word is a clue at improving classification performance.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:25:33 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Lee", "Byeong-Hoo", ""], ["Kwon", "Byeong-Hee", ""], ["Lee", "Do-Yeun", ""], ["Jeong", "Ji-Hoon", ""]]}, {"id": "2012.03743", "submitter": "Marcos Baez", "authors": "Alessandro Pina, Marcos Baez, Florian Daniel", "title": "Bringing Cognitive Augmentation to Web Browsing Accessibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the opportunities brought by cognitive augmentation\nto provide a more natural and accessible web browsing experience. We explore\nthese opportunities through \\textit{conversational web browsing}, an emerging\ninteraction paradigm for the Web that enables blind and visually impaired users\n(BVIP), as well as regular users, to access the contents and features of\nwebsites through conversational agents. Informed by the literature, our\nprevious work and prototyping exercises, we derive a conceptual framework for\nsupporting BVIP conversational web browsing needs, to then focus on the\nchallenges of automatically providing this support, describing our early work\nand prototype that leverage heuristics that consider structural and content\nfeatures only.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:40:52 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pina", "Alessandro", ""], ["Baez", "Marcos", ""], ["Daniel", "Florian", ""]]}, {"id": "2012.04078", "submitter": "Alex Reneau", "authors": "Alex Reneau and Jason R. Wilson", "title": "Supporting User Autonomy with Multimodal Fusion to Detect when a User\n  Needs Assistance from a Social Robot", "comments": "9 pages, 5 figures, 3 tables, AI-HRI FSS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is crucial for any assistive robot to prioritize the autonomy of the user.\nFor a robot working in a task setting to effectively maintain a user's autonomy\nit must provide timely assistance and make accurate decisions. We use four\nindependent high-precision, low-recall models, a mutual gaze model, task model,\nconfirmatory gaze model, and a lexical model, that predict a user's need for\nassistance. Improving upon our four independent models, we used a sliding\nwindow method and a random forest classification algorithm to capture temporal\ndependencies and fuse the independent models with a late fusion approach. The\nlate fusion approach strongly outperforms all four of the independent models\nproviding a more wholesome approach with greater accuracy to better assist the\nuser while maintaining their autonomy. These results can provide insight into\nthe potential of including additional modalities and utilizing assistive robots\nin more task settings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:53:07 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Reneau", "Alex", ""], ["Wilson", "Jason R.", ""]]}, {"id": "2012.04142", "submitter": "Raiyan Abdul Baten", "authors": "Raiyan Abdul Baten and Ehsan Hoque", "title": "Technology-driven Alteration of Nonverbal Cues and its Effects on\n  Negotiation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A person's appearance, identity, and other nonverbal cues can substantially\ninfluence how one is perceived by a negotiation counterpart, potentially\nimpacting the outcome of the negotiation. With recent advances in technology,\nit is now possible to alter such cues through real-time video communication. In\nmany cases, a person's physical presence can explicitly be replaced by 2D/3D\nrepresentations in live interactive media. In other cases, technologies such as\ndeepfake can subtly and implicitly alter many nonverbal cues -- including a\nperson's appearance and identity -- in real-time. In this article, we look at\nsome state-of-the-art technological advances that can enable such explicit and\nimplicit alteration of nonverbal cues. We also discuss the implications of such\ntechnology for the negotiation landscape and highlight ethical considerations\nthat warrant deep, ongoing attention from stakeholders.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 01:01:38 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Baten", "Raiyan Abdul", ""], ["Hoque", "Ehsan", ""]]}, {"id": "2012.04143", "submitter": "Yuanxin Wu", "authors": "Maoran Zhu, Yuanxin Wu and Shitu Luo", "title": "f2IMU-R: Pedestrian Navigation by Low-cost Foot-Mounted Dual IMUs and\n  Inter-foot Ranging", "comments": "14 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Foot-mounted inertial sensors become popular in many indoor or GPS-denied\napplications, including but not limited to medical monitoring, gait analysis,\nsoldier and first responder positioning. However, the foot-mounted inertial\nnavigation relies largely on the aid of Zero Velocity Update (ZUPT) and has\nencountered inherent problems such as heading drift. This paper implements a\npedestrian navigation system based on dual foot-mounted low-cost inertial\nmeasurement units (IMU) and inter-foot ultrasonic ranging. The observability\nanalysis of the system is performed to investigate the roles of the ZUPT\nmeasurement and the foot-to-foot ranging measurement in improving the state\nestimability. A Kalman-based estimation algorithm is mechanized in the Earth\nframe, rather than in the common local-level frame, which is found to be\neffective in depressing the linearization error in Kalman filtering. An\nellipsoid constraint in the Earth frame is also proposed to further restrict\nthe height drift. Simulation and real field experiments show that the proposed\nmethod has better robustness and positioning accuracy (about 0.1-0.2% travelled\ndistance) than the traditional pedestrian navigation schemes do.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 01:02:31 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Zhu", "Maoran", ""], ["Wu", "Yuanxin", ""], ["Luo", "Shitu", ""]]}, {"id": "2012.04411", "submitter": "Michael Aupetit", "authors": "Ali Sheharyar, Talar Boghos Yacoubian, Dina Aljogol, Borbala Mifsud,\n  Dena Al Thani, Michael Aupetit", "title": "An Enhanced MA Plot with R-Shiny to Ease Exploratory Analysis of\n  Transcriptomic Data", "comments": "Presented at BioVis 2020 Redesign Challenge @ IEEE VIS.\n  http://biovis.net/2020/program_ieee/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  MA plots are used to analyze the genome-wide differences in gene expression\nbetween two distinct biological conditions. An MA plot is usually rendered as a\nstatic scatter plot. Our interview with 3 experts in genomics showed that we\ncould improve the usability of this plot by adding interactive analytic\nfeatures. In this work we present the design study of the enhanced MA plot.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 12:57:37 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Sheharyar", "Ali", ""], ["Yacoubian", "Talar Boghos", ""], ["Aljogol", "Dina", ""], ["Mifsud", "Borbala", ""], ["Thani", "Dena Al", ""], ["Aupetit", "Michael", ""]]}, {"id": "2012.04757", "submitter": "Min Chen", "authors": "M. Chen, A. Abdul-Rahman, D. Archambault, J. Dykes, A. Slingsby, P. D.\n  Ritsos, T. Torsney-Weir, C. Turkay, B. Bach, A. Brett, H. Fang, R. Jianu, S.\n  Khan, R. S. Laramee, P. H. Nguyen, R. Reeve, J. C. Roberts, F. Vidal, Q.\n  Wang, J. Wood, K. Xu", "title": "RAMPVIS: Towards a New Methodology for Developing Visualisation\n  Capabilities for Large-scale Emergency Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effort for combating the COVID-19 pandemic around the world has resulted\nin a huge amount of data, e.g., from testing, contact tracing, modelling,\ntreatment, vaccine trials, and more. In addition to numerous challenges in\nepidemiology, healthcare, biosciences, and social sciences, there has been an\nurgent need to develop and provide visualisation and visual analytics (VIS)\ncapacities to support emergency responses under difficult operational\nconditions. In this paper, we report the experience of a group of VIS\nvolunteers who have been working in a large research and development consortium\nand providing VIS support to various observational, analytical,\nmodel-developmental and disseminative tasks. In particular, we describe our\napproaches to the challenges that we have encountered in requirements analysis,\ndata acquisition, visual design, software design, system development, team\norganisation, and resource planning. By reflecting on our experience, we\npropose a set of recommendations as the first step towards a methodology for\ndeveloping and providing rapid VIS capacities to support emergency responses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 21:53:23 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Chen", "M.", ""], ["Abdul-Rahman", "A.", ""], ["Archambault", "D.", ""], ["Dykes", "J.", ""], ["Slingsby", "A.", ""], ["Ritsos", "P. D.", ""], ["Torsney-Weir", "T.", ""], ["Turkay", "C.", ""], ["Bach", "B.", ""], ["Brett", "A.", ""], ["Fang", "H.", ""], ["Jianu", "R.", ""], ["Khan", "S.", ""], ["Laramee", "R. S.", ""], ["Nguyen", "P. H.", ""], ["Reeve", "R.", ""], ["Roberts", "J. C.", ""], ["Vidal", "F.", ""], ["Wang", "Q.", ""], ["Wood", "J.", ""], ["Xu", "K.", ""]]}, {"id": "2012.04829", "submitter": "Jun Wang", "authors": "Jun Wang, Shaoguo Wen, Kaixing Chen, Jianghua Yu, Xin Zhou, Peng Gao,\n  Changsheng Li, Guotong Xie", "title": "Semi-supervised Active Learning for Instance Segmentation via Scoring\n  Predictions", "comments": "13 pages, 7 figures, accepted for presentation at BMVC2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Active learning generally involves querying the most representative samples\nfor human labeling, which has been widely studied in many fields such as image\nclassification and object detection. However, its potential has not been\nexplored in the more complex instance segmentation task that usually has\nrelatively higher annotation cost. In this paper, we propose a novel and\nprincipled semi-supervised active learning framework for instance segmentation.\nSpecifically, we present an uncertainty sampling strategy named Triplet Scoring\nPredictions (TSP) to explicitly incorporate samples ranking clues from classes,\nbounding boxes and masks. Moreover, we devise a progressive pseudo labeling\nregime using the above TSP in semi-supervised manner, it can leverage both the\nlabeled and unlabeled data to minimize labeling effort while maximize\nperformance of instance segmentation. Results on medical images datasets\ndemonstrate that the proposed method results in the embodiment of knowledge\nfrom available data in a meaningful way. The extensive quantitatively and\nqualitatively experiments show that, our method can yield the best-performing\nmodel with notable less annotation costs, compared with state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 02:36:52 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Wang", "Jun", ""], ["Wen", "Shaoguo", ""], ["Chen", "Kaixing", ""], ["Yu", "Jianghua", ""], ["Zhou", "Xin", ""], ["Gao", "Peng", ""], ["Li", "Changsheng", ""], ["Xie", "Guotong", ""]]}, {"id": "2012.05342", "submitter": "Pierre-Etienne Martin", "authors": "Pierre-Etienne Martin (LaBRI, UB), Jenny Benois-Pineau (LaBRI), Renaud\n  P\\'eteri, Julien Morlier", "title": "3D attention mechanism for fine-grained classification of table tennis\n  strokes using a Twin Spatio-Temporal Convolutional Neural Networks", "comments": null, "journal-ref": "25th International Conference on Pattern Recognition (ICPR2020),\n  Jan 2021, Milano, Italy", "doi": null, "report-no": null, "categories": "cs.CV cs.HC cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of recognition of actions in video with low\ninter-class variability such as Table Tennis strokes. Two stream, \"twin\"\nconvolutional neural networks are used with 3D convolutions both on RGB data\nand optical flow. Actions are recognized by classification of temporal windows.\nWe introduce 3D attention modules and examine their impact on classification\nefficiency. In the context of the study of sportsmen performances, a corpus of\nthe particular actions of table tennis strokes is considered. The use of\nattention blocks in the network speeds up the training step and improves the\nclassification scores up to 5% with our twin model. We visualize the impact on\nthe obtained features and notice correlation between attention and player\nmovements and position. Score comparison of state-of-the-art action\nclassification method and proposed approach with attentional blocks is\nperformed on the corpus. Proposed model with attention blocks outperforms\nprevious model without them and our baseline.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 09:55:12 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Martin", "Pierre-Etienne", "", "LaBRI, UB"], ["Benois-Pineau", "Jenny", "", "LaBRI"], ["P\u00e9teri", "Renaud", ""], ["Morlier", "Julien", ""]]}, {"id": "2012.05370", "submitter": "Ben Green", "authors": "Ben Green, Yiling Chen", "title": "Algorithmic risk assessments can alter human decision-making processes\n  in high-stakes government contexts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Governments are increasingly turning to algorithmic risk assessments when\nmaking important decisions, believing that these algorithms will improve public\nservants' ability to make policy-relevant predictions and thereby lead to more\ninformed decisions. Yet because many policy decisions require balancing\nrisk-minimization with competing social goals, evaluating the impacts of risk\nassessments requires considering how public servants are influenced by risk\nassessments when making policy decisions rather than just how accurately these\nalgorithms make predictions. Through an online experiment with 2,140 lay\nparticipants simulating two high-stakes government contexts, we provide the\nfirst large-scale evidence that risk assessments can systematically alter\ndecision-making processes by increasing the salience of risk as a factor in\ndecisions and that these shifts could exacerbate racial disparities. These\nresults demonstrate that improving human prediction accuracy with algorithms\ndoes not necessarily improve human decisions and highlight the need to\nexperimentally test how government algorithms are used by human\ndecision-makers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 23:44:45 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Green", "Ben", ""], ["Chen", "Yiling", ""]]}, {"id": "2012.05373", "submitter": "Mohammad Rafayet Ali", "authors": "Mohammad Rafayet Ali, Taylor Myers, Ellen Wagner, Harshil Ratnu, E.\n  Ray Dorsey, Ehsan Hoque", "title": "Facial expressions can detect Parkinson's disease: preliminary evidence\n  from videos collected online", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the symptoms of Parkinson's disease (PD) is hypomimia or reduced\nfacial expressions. In this paper, we present a digital biomarker for PD that\nutilizes the study of micro-expressions. We analyzed the facial action units\n(AU) from 1812 videos of 604 individuals (61 with PD and 543 without PD, mean\nage 63.9 yo, sd 7.8 ) collected online using a web-based tool\n(www.parktest.net). In these videos, participants were asked to make three\nfacial expressions (a smiling, disgusted, and surprised face) followed by a\nneutral face. Using techniques from computer vision and machine learning, we\nobjectively measured the variance of the facial muscle movements and used it to\ndistinguish between individuals with and without PD. The prediction accuracy\nusing the facial micro-expressions was comparable to those methodologies that\nutilize motor symptoms. Logistic regression analysis revealed that participants\nwith PD had less variance in AU6 (cheek raiser), AU12 (lip corner puller), and\nAU4 (brow lowerer) than non-PD individuals. An automated classifier using\nSupport Vector Machine was trained on the variances and achieved 95.6%\naccuracy. Using facial expressions as a biomarker for PD could be potentially\ntransformative for patients in need of physical separation (e.g., due to COVID)\nor are immobile.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 23:53:32 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ali", "Mohammad Rafayet", ""], ["Myers", "Taylor", ""], ["Wagner", "Ellen", ""], ["Ratnu", "Harshil", ""], ["Dorsey", "E. Ray", ""], ["Hoque", "Ehsan", ""]]}, {"id": "2012.05567", "submitter": "Brian Lim", "authors": "Wencan Zhang, Mariella Dimiccoli, Brian Y. Lim", "title": "Debiased-CAM for bias-agnostic faithful visual explanations of deep\n  convolutional networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class activation maps (CAMs) explain convolutional neural network predictions\nby identifying salient pixels, but they become misaligned and misleading when\nexplaining predictions on images under bias, such as images blurred\naccidentally or deliberately for privacy protection, or images with improper\nwhite balance. Despite model fine-tuning to improve prediction performance on\nthese biased images, we demonstrate that CAM explanations become more deviated\nand unfaithful with increased image bias. We present Debiased-CAM to recover\nexplanation faithfulness across various bias types and levels by training a\nmulti-input, multi-task model with auxiliary tasks for CAM and bias level\npredictions. With CAM as a prediction task, explanations are made tunable by\nretraining the main model layers and made faithful by self-supervised learning\nfrom CAMs of unbiased images. The model provides representative, bias-agnostic\nCAM explanations about the predictions on biased images as if generated from\ntheir unbiased form. In four simulation studies with different biases and\nprediction tasks, Debiased-CAM improved both CAM faithfulness and task\nperformance. We further conducted two controlled user studies to validate its\ntruthfulness and helpfulness, respectively. Quantitative and qualitative\nanalyses of participant responses confirmed Debiased-CAM as more truthful and\nhelpful. Debiased-CAM thus provides a basis to generate more faithful and\nrelevant explanations for a wide range of real-world applications with various\nsources of bias.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 10:28:47 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhang", "Wencan", ""], ["Dimiccoli", "Mariella", ""], ["Lim", "Brian Y.", ""]]}, {"id": "2012.05637", "submitter": "Enrico Bassetti", "authors": "Enrico Bassetti, Emanuele Panizzi, Edoardo Ottavianelli", "title": "Simplify Node-RED For End User Development in SeismoCloud", "comments": "4 pages, 2 figures, workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Networks of IoT devices often require configuration and definition of\nbehavior by the final user. Node-RED is a flow-based programming platform\ncommonly used for End User Development, but it requires networking and\nprotocols skills in order to be efficiently used. We add a level of abstraction\nto Node-RED nodes in order to allow non-skilled users to configure and control\nnetworks of IoT devices and online services. We applied such abstractions to\nthe SeismoCloud application for earthquake monitoring.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 12:43:10 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Bassetti", "Enrico", ""], ["Panizzi", "Emanuele", ""], ["Ottavianelli", "Edoardo", ""]]}, {"id": "2012.05710", "submitter": "Paul Hongsuck Seo", "authors": "Paul Hongsuck Seo, Arsha Nagrani, Cordelia Schmid", "title": "Look Before you Speak: Visually Contextualized Utterances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While most conversational AI systems focus on textual dialogue only,\nconditioning utterances on visual context (when it's available) can lead to\nmore realistic conversations. Unfortunately, a major challenge for\nincorporating visual context into conversational dialogue is the lack of\nlarge-scale labeled datasets. We provide a solution in the form of a new\nvisually conditioned Future Utterance Prediction task. Our task involves\npredicting the next utterance in a video, using both visual frames and\ntranscribed speech as context. By exploiting the large number of instructional\nvideos online, we train a model to solve this task at scale, without the need\nfor manual annotations. Leveraging recent advances in multimodal learning, our\nmodel consists of a novel co-attentional multimodal video transformer, and when\ntrained on both textual and visual context, outperforms baselines that use\ntextual inputs alone. Further, we demonstrate that our model trained for this\ntask on unlabelled videos achieves state-of-the-art performance on a number of\ndownstream VideoQA benchmarks such as MSRVTT-QA, MSVD-QA, ActivityNet-QA and\nHow2QA.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:47:02 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 01:54:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Seo", "Paul Hongsuck", ""], ["Nagrani", "Arsha", ""], ["Schmid", "Cordelia", ""]]}, {"id": "2012.05720", "submitter": "Wei Wang Dr.", "authors": "Xianan Zhang, Wei Wang, Xuedou Xiao, Hang Yang, Xinyu Zhang and Tao\n  Jiang", "title": "Peer-to-Peer Localization for Single-Antenna Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some important indoor localization applications, such as localizing a lost\nkid in a shopping mall, call for a new peer-to-peer localization technique that\ncan localize an individual's smartphone or wearables by directly using\nanother's on-body devices in unknown indoor environments. However, current\nlocalization solutions either require pre-deployed infrastructures or multiple\nantennas in both transceivers, impending their wide-scale application. In this\npaper, we present P2PLocate, a peer-to-peer localization system that enables a\nsingle-antenna device co-located with a batteryless backscatter tag to localize\nanother single-antenna device with decimeter-level accuracy. P2PLocate\nleverages the multipath variations intentionally created by an on-body\nbackscatter tag, coupled with spatial information offered by user movements, to\naccomplish this objective without relying on any pre-deployed infrastructures\nor pre-training. P2PLocate incorporates novel algorithms to address two major\nchallenges: (i) interference with strong direct-path signal while extracting\nmultipath variations, and (ii) lack of direction information while using\nsingle-antenna transceivers. We implement P2PLocate on commercial off-the-shelf\nGoogle Nexus 6p, Intel 5300 WiFi card, and Raspberry Pi B4. Real-world\nexperiments reveal that P2PLocate can localize both static and mobile targets\nwith a median accuracy of 0.88 m.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:53:24 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Zhang", "Xianan", ""], ["Wang", "Wei", ""], ["Xiao", "Xuedou", ""], ["Yang", "Hang", ""], ["Zhang", "Xinyu", ""], ["Jiang", "Tao", ""]]}, {"id": "2012.05854", "submitter": "David Puljiz", "authors": "David Puljiz, Bj\\\"orn Hein", "title": "Adapting the Human: Leveraging Wearable Technology in HRI", "comments": "As submitted to the Robot Metaphors Workshop, ICSR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adhering to current HRI paradigms, all of the sensors, visualisation and\nlegibility of actions and motions are borne by the robot or its working cell.\nThis necessarily makes robots more complex or confines them into specialised,\nstructured environments. We propose leveraging the state of the art of wearable\ntechnologies, such as augmented reality head mounted displays, smart watches,\nsensor tags and radio-frequency ranging, to \"adapt\" the human and reduce the\nrequirements and complexity of robots.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 18:01:46 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Puljiz", "David", ""], ["Hein", "Bj\u00f6rn", ""]]}, {"id": "2012.06161", "submitter": "Nikhil Prakash", "authors": "Nikhil Prakash and Kory W. Mathewson", "title": "Conceptualization and Framework of Hybrid Intelligence Systems", "comments": "8 pages, 1 figure, HAMLETS (Human And Machine in-the-Loop Evaluation\n  and Learning Strategies) workshop at Thirty-fourth Conference on Neural\n  Information Processing Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As artificial intelligence (AI) systems are getting ubiquitous within our\nsociety, issues related to its fairness, accountability, and transparency are\nincreasing rapidly. As a result, researchers are integrating humans with AI\nsystems to build robust and reliable hybrid intelligence systems. However, a\nproper conceptualization of these systems does not underpin this rapid growth.\nThis article provides a precise definition of hybrid intelligence systems as\nwell as explains its relation with other similar concepts through our proposed\nframework and examples from contemporary literature. The framework breakdowns\nthe relationship between a human and a machine in terms of the degree of\ncoupling and the directive authority of each party. Finally, we argue that all\nAI systems are hybrid intelligence systems, so human factors need to be\nexamined at every stage of such systems' lifecycle.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 06:42:06 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Prakash", "Nikhil", ""], ["Mathewson", "Kory W.", ""]]}, {"id": "2012.06206", "submitter": "Myoungki Kim", "authors": "Myoung-Ki Kim, Jeong-Hyun Cho, Ji-Hoon Jeong", "title": "Classification of Tactile Perception and Attention on Natural Textures\n  from EEG Signals", "comments": "Submitted IEEE The 9th International Winter Conference on\n  Brain-Computer Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain-computer interface allows people who have lost their motor skills to\ncontrol robot limbs based on electroencephalography. Most BCIs are guided only\nby visual feedback and do not have somatosensory feedback, which is an\nimportant component of normal motor behavior. The sense of touch is a very\ncrucial sensory modality, especially in object recognition and manipulation.\nWhen manipulating an object, the brain uses empirical information about the\ntactile properties of the object. In addition, the primary somatosensory cortex\nis not only involved in processing the sense of touch in our body but also\nresponds to visible contact with other people or inanimate objects. Based on\nthese findings, we conducted a preliminary experiment to confirm the\npossibility of a novel paradigm called touch imagery. A haptic imagery\nexperiment was conducted on four objects, and through neurophysiological\nanalysis, a comparison analysis was performed with the brain waves of the\nactual tactile sense. Also, high classification performance was confirmed\nthrough the basic machine learning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 09:32:49 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Kim", "Myoung-Ki", ""], ["Cho", "Jeong-Hyun", ""], ["Jeong", "Ji-Hoon", ""]]}, {"id": "2012.06326", "submitter": "Alex B\\\"auerle", "authors": "Alex B\\\"auerle, Raphael St\\\"ork, and Timo Ropinski", "title": "exploRNN: Understanding Recurrent Neural Networks through Visual\n  Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to the success of deep learning and its growing job market, students and\nresearchers from many areas are getting interested in learning about deep\nlearning technologies. Visualization has proven to be of great help during this\nlearning process, while most current educational visualizations are targeted\ntowards one specific architecture or use case. Unfortunately, recurrent neural\nnetworks (RNNs), which are capable of processing sequential data, are not\ncovered yet, despite the fact that tasks on sequential data, such as text and\nfunction analysis, are at the forefront of deep learning research. Therefore,\nwe propose exploRNN, the first interactively explorable, educational\nvisualization for RNNs. exploRNN allows for interactive experimentation with\nRNNs, and provides in-depth information on their functionality and behavior\nduring training. By defining educational objectives targeted towards\nunderstanding RNNs, and using these as guidelines throughout the visual design\nprocess, we have designed exploRNN to communicate the most important concepts\nof RNNs directly within a web browser. By means of exploRNN, we provide an\noverview of the training process of RNNs at a coarse level, while also allowing\ndetailed inspection of the data-flow within LSTM cells. Within this paper, we\nmotivate our design of exploRNN, detail its realization, and discuss the\nresults of a user study investigating the benefits of exploRNN.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 15:06:01 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["B\u00e4uerle", "Alex", ""], ["St\u00f6rk", "Raphael", ""], ["Ropinski", "Timo", ""]]}, {"id": "2012.06708", "submitter": "Ahmed Arif", "authors": "Anna-Maria Gueorguieva, Gulnar Rakhmetulla, Ahmed Sabbir Arif", "title": "Enabling Input on Tiny/Headless Systems Using Morse Code", "comments": "Poster at the 2nd Annual Center for Cellular and Biomolecular\n  Machines Open House, October 22, 2018, University of California, Merced, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents results of a pilot study that explored the potential of\nMorse code as a method for text entry on mobile devices. In the study,\nparticipants without prior experience with Morse code reached 6.7 wpm with a\nMorse code keyboard in three short sessions. Learning was observed both in\nterms of text entry speed and accuracy, which suggests that the overall\nperformance of the keyboard is likely to improve with practice.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 03:00:54 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gueorguieva", "Anna-Maria", ""], ["Rakhmetulla", "Gulnar", ""], ["Arif", "Ahmed Sabbir", ""]]}, {"id": "2012.06753", "submitter": "Jeong-Hyun Cho", "authors": "Jeong-Hyun Cho, Ji-Hoon Jeong, Myoung-Ki Kim, Seong-Whan Lee", "title": "Towards Neurohaptics: Brain-Computer Interfaces for Decoding Intuitive\n  Sense of Touch", "comments": "Submitted IEEE The 9th International Winter Conference on\n  Brain-Computer Interface", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noninvasive brain-computer interface (BCI) is widely used to recognize users'\nintentions. Especially, BCI related to tactile and sensation decoding could\nprovide various effects on many industrial fields such as manufacturing\nadvanced touch displays, controlling robotic devices, and more immersive\nvirtual reality or augmented reality. In this paper, we introduce haptic and\nsensory perception-based BCI systems called neurohaptics. It is a preliminary\nstudy for a variety of scenarios using actual touch and touch imagery\nparadigms. We designed a novel experimental environment and a device that could\nacquire brain signals under touching designated materials to generate natural\ntouch and texture sensations. Through the experiment, we collected the\nelectroencephalogram (EEG) signals with respect to four different texture\nobjects. Seven subjects were recruited for the experiment and evaluated\nclassification performances using machine learning and deep learning\napproaches. Hence, we could confirm the feasibility of decoding actual touch\nand touch imagery on EEG signals to develop practical neurohaptics.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 08:08:47 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 13:23:54 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cho", "Jeong-Hyun", ""], ["Jeong", "Ji-Hoon", ""], ["Kim", "Myoung-Ki", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2012.06965", "submitter": "Sabirat Rubya", "authors": "Sabirat Rubya, Joseph Numainville, Svetlana Yarosh", "title": "Comparing Generic and Community-Situated Crowdsourcing for Data\n  Validation in the Context of Recovery from Substance Use Disorders", "comments": "Accepted at ACM CHI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeting the right group of workers for crowdsourcing often achieves better\nquality results. One unique example of targeted crowdsourcing is seeking\ncommunity-situated workers whose familiarity with the background and the norms\nof a particular group can help produce better outcome or accuracy. These\ncommunity-situated crowd workers can be recruited in different ways from\ngeneric online crowdsourcing platforms or from online recovery communities. We\nevaluate three different approaches to recruit generic and community-situated\ncrowd in terms of the time and the cost of recruitment, and the accuracy of\ntask completion. We consider the context of Alcoholics Anonymous (AA), the\nlargest peer support group for recovering alcoholics, and the task of\nidentifying and validating AA meeting information. We discuss the benefits and\ntrade-offs of recruiting paid vs. unpaid community-situated workers and provide\nimplications for future research in the recovery context and relevant domains\nof HCI, and for design of crowdsourcing ICT systems.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 05:25:04 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Rubya", "Sabirat", ""], ["Numainville", "Joseph", ""], ["Yarosh", "Svetlana", ""]]}, {"id": "2012.06973", "submitter": "Chirag Kyal", "authors": "Chirag Kyal", "title": "Spontaneous Emotion Recognition from Facial Thermal Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the key research areas in computer vision addressed by a vast number\nof publications is the processing and understanding of images containing human\nfaces. The most often addressed tasks include face detection, facial landmark\nlocalization, face recognition and facial expression analysis. Other, more\nspecialized tasks such as affective computing, the extraction of vital signs\nfrom videos or analysis of social interaction usually require one or several of\nthe aforementioned tasks that have to be performed. In our work, we analyze\nthat a large number of tasks for facial image processing in thermal infrared\nimages that are currently solved using specialized rule-based methods or not\nsolved at all can be addressed with modern learning-based approaches. We have\nused USTC-NVIE database for training of a number of machine learning algorithms\nfor facial landmark localization.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 05:55:19 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kyal", "Chirag", ""]]}, {"id": "2012.06981", "submitter": "Stephen Macke", "authors": "Stephen Macke, Hongpu Gong, Doris Jung-Lin Lee, Andrew Head, Doris\n  Xin, Aditya Parameswaran", "title": "Fine-Grained Lineage for Safer Notebook Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.DB cs.HC cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computational notebooks have emerged as the platform of choice for data\nscience and analytical workflows, enabling rapid iteration and exploration. By\nkeeping intermediate program state in memory and segmenting units of execution\ninto so-called \"cells\", notebooks allow users to execute their workflows\ninteractively and enjoy particularly tight feedback. However, as cells are\nadded, removed, reordered, and rerun, this hidden intermediate state\naccumulates in a way that is not necessarily correlated with the notebook's\nvisible code, making execution behavior difficult to reason about, and leading\nto errors and lack of reproducibility. We present NBSafety, a custom Jupyter\nkernel that uses runtime tracing and static analysis to automatically manage\nlineage associated with cell execution and global notebook state. NBSafety\ndetects and prevents errors that users make during unaided notebook\ninteractions, all while preserving the flexibility of existing notebook\nsemantics. We evaluate NBSafety's ability to prevent erroneous interactions by\nreplaying and analyzing 666 real notebook sessions. Of these, NBSafety\nidentified 117 sessions with potential safety errors, and in the remaining 549\nsessions, the cells that NBSafety identified as resolving safety issues were\nmore than $7\\times$ more likely to be selected by users for re-execution\ncompared to a random baseline, even though the users were not using NBSafety\nand were therefore not influenced by its suggestions.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 06:50:31 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 20:20:44 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Macke", "Stephen", ""], ["Gong", "Hongpu", ""], ["Lee", "Doris Jung-Lin", ""], ["Head", "Andrew", ""], ["Xin", "Doris", ""], ["Parameswaran", "Aditya", ""]]}, {"id": "2012.07179", "submitter": "Harish Naik", "authors": "Harish Naik, Gy\\\"orgy Tur\\'an", "title": "Explanation from Specification", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable components in XAI algorithms often come from a familiar set of\nmodels, such as linear models or decision trees. We formulate an approach where\nthe type of explanation produced is guided by a specification. Specifications\nare elicited from the user, possibly using interaction with the user and\ncontributions from other areas. Areas where a specification could be obtained\ninclude forensic, medical, and scientific applications. Providing a menu of\npossible types of specifications in an area is an exploratory knowledge\nrepresentation and reasoning task for the algorithm designer, aiming at\nunderstanding the possibilities and limitations of efficiently computable modes\nof explanations. Two examples are discussed: explanations for Bayesian networks\nusing the theory of argumentation, and explanations for graph neural networks.\nThe latter case illustrates the possibility of having a representation\nformalism available to the user for specifying the type of explanation\nrequested, for example, a chemical query language for classifying molecules.\nThe approach is motivated by a theory of explanation in the philosophy of\nscience, and it is related to current questions in the philosophy of science on\nthe role of machine learning.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 23:27:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Naik", "Harish", ""], ["Tur\u00e1n", "Gy\u00f6rgy", ""]]}, {"id": "2012.07257", "submitter": "Sonia Castelo", "authors": "Sonia Castelo, Moacir Ponti, Rosane Minghim", "title": "A Visual Mining Approach to Improved Multiple-Instance Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-instance learning (MIL) is a paradigm of machine learning that aims\nto classify a set (bag) of objects (instances), assigning labels only to the\nbags. This problem is often addressed by selecting an instance to represent\neach bag, transforming a MIL problem into a standard supervised learning.\nVisualization can be a useful tool to assess learning scenarios by\nincorporating the users' knowledge into the classification process. Considering\nthat multiple-instance learning is a paradigm that cannot be handled by current\nvisualization techniques, we propose a multiscale tree-based visualization to\nsupport MIL. The first level of the tree represents the bags, and the second\nlevel represents the instances belonging to each bag, allowing the user to\nunderstand the data in an intuitive way. In addition, we propose two new\ninstance selection methods for MIL, which help the user to improve the model\neven further. Our methods are also able to handle both binary and multiclass\nscenarios. In our experiments, SVM was used to build the classifiers. With\nsupport of the MILTree layout, the initial classification model was updated by\nchanging the training set - composed by the prototype instances. Experimental\nresults validate the effectiveness of our approach, showing that visual mining\nby MILTree can help users in exploring and improving models in MIL scenarios,\nand that our instance selection methods over-perform current available\nalternatives in most cases.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 05:12:43 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Castelo", "Sonia", ""], ["Ponti", "Moacir", ""], ["Minghim", "Rosane", ""]]}, {"id": "2012.07609", "submitter": "Vivek Kaushal", "authors": "Vivek Kaushal and Kavita Vemuri", "title": "Clickbait in Hindi News Media : A Preliminary Study", "comments": "Accepted as a short paper in the 17th International Conference on\n  Natural Language Processing, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A corpus of Hindi news headlines shared on Twitter was created by collecting\ntweets of 5 mainstream Hindi news sources for a period of 4 months. 7\nindependent annotators were recruited to mark the 20 most retweeted news posts\nby each of the 5 news sources on its clickbait nature. The clickbait score\nhence generated was assessed for its correlation with interactions on the\nplatform (retweets, favorites, reader replies), tweet word count, and\nnormalized POS (part-of-speech) tag counts in tweets. A positive correlation\nwas observed between readers' interactions with tweets and tweets' clickbait\nscore. Significant correlations were also observed for POS tag counts and\nclickbait score. The prevalence of clickbait in mainstream Hindi news media was\nfound to be similar to its prevalence in English news media. We hope that our\nobservations would provide a platform for discussions on clickbait in\nmainstream Hindi news media.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 14:59:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Kaushal", "Vivek", ""], ["Vemuri", "Kavita", ""]]}, {"id": "2012.07610", "submitter": "Jiawei Liu", "authors": "Jiawei Liu, Zhe Gao, Yangyang Kang, Zhuoren Jiang, Guoxiu He,\n  Changlong Sun, Xiaozhong Liu, Wei Lu", "title": "Time to Transfer: Predicting and Evaluating Machine-Human Chatting\n  Handoff", "comments": "9pages, 4 figures, accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is chatbot able to completely replace the human agent? The short answer could\nbe - \"it depends...\". For some challenging cases, e.g., dialogue's topical\nspectrum spreads beyond the training corpus coverage, the chatbot may\nmalfunction and return unsatisfied utterances. This problem can be addressed by\nintroducing the Machine-Human Chatting Handoff (MHCH), which enables\nhuman-algorithm collaboration. To detect the normal/transferable utterances, we\npropose a Difficulty-Assisted Matching Inference (DAMI) network, utilizing\ndifficulty-assisted encoding to enhance the representations of utterances.\nMoreover, a matching inference mechanism is introduced to capture the\ncontextual matching features. A new evaluation metric, Golden Transfer within\nTolerance (GT-T), is proposed to assess the performance by considering the\ntolerance property of the MHCH. To provide insights into the task and validate\nthe proposed model, we collect two new datasets. Extensive experimental results\nare presented and contrasted against a series of baseline models to demonstrate\nthe efficacy of our model on MHCH.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 15:02:08 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Liu", "Jiawei", ""], ["Gao", "Zhe", ""], ["Kang", "Yangyang", ""], ["Jiang", "Zhuoren", ""], ["He", "Guoxiu", ""], ["Sun", "Changlong", ""], ["Liu", "Xiaozhong", ""], ["Lu", "Wei", ""]]}, {"id": "2012.07619", "submitter": "Maartje ter Hoeve", "authors": "Maartje ter Hoeve, Julia Kiseleva, Maarten de Rijke", "title": "What Makes a Good Summary? Investigating the Focus of Automatic\n  Summarization in an Educational Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic text summarization has enjoyed great progress over the last years.\nHowever, there is little research that investigates whether the current\nresearch focus adheres to users' needs. Importantly, these needs are dependent\non the envisioned target group of the generated summaries. One such important\ntarget group is formed by students, due to their usage of summaries in their\nstudy activities. For this reason, we investigate students' needs regarding\nautomatically generated summaries by means of a survey amongst university\nstudents and find that the current direction of the field does not fully align\nwith their needs. Motivated by our findings, we formulate three groups of\nimplications that together help us formulate a renewed perspective on future\nresearch on automatic summarization. First, the educational domain requires a\nbroader perspective on automatic summarization, beyond the approaches that are\ncurrently the standard. We illustrate how we can expand these approaches\nregarding the input material, the purpose of the summaries and their potential\nformat and we define requirements for datasets that can facilitate these\nresearch directions. Second, we propose a methodology to evaluate the\nusefulness of a summary based on the identified needs of a target group. Third,\nin more general terms, we hope that our survey will be reused to investigate\nthe needs of different user groups of automatically generated summaries to\nbroaden our perspective even further.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 15:12:35 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 19:08:43 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["ter Hoeve", "Maartje", ""], ["Kiseleva", "Julia", ""], ["de Rijke", "Maarten", ""]]}, {"id": "2012.07716", "submitter": "Jack Bandy", "authors": "Jack Bandy, Nicholas Diakopoulos", "title": "#TulsaFlop: A Case Study of Algorithmically-Influenced Collective Action\n  on TikTok", "comments": "Presented at the FAccTRec Workshop on Responsible Recommendation (at\n  RecSys 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a re-election rally for the U.S. president drew smaller crowds than\nexpected in Tulsa, Oklahoma, many people attributed the low turnout to\ncollective action organized by TikTok users. Motivated by TikTok's surge in\npopularity and its growing sociopolitical implications, this work explores the\nrole of TikTok's recommender algorithm in amplifying call-to-action videos that\npromoted collective action against the Tulsa rally. We analyze call-to-action\nvideos from more than 600 TikTok users and compare the visibility (i.e. play\ncount) of these videos with other videos published by the same users. Evidence\nsuggests that Tulsa-related videos generally received more plays, and in some\ncases the amplification was dramatic. For example, one user's call-to-action\nvideo was played over 2 million times, but no other video by the user exceeded\n100,000 plays, and the user had fewer than 20,000 followers. Statistical\nmodeling suggests that the increased play count is explained by increased\nengagement rather than any systematic amplification of call-to-action videos.\nWe conclude by discussing the implications of recommender algorithms amplifying\nsociopolitical messages, and motivate several promising areas for future work.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 17:09:25 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Bandy", "Jack", ""], ["Diakopoulos", "Nicholas", ""]]}, {"id": "2012.07750", "submitter": "Leon Abdillah", "authors": "Leon A. Abdillah", "title": "FinTech E-Commerce Payment Application User Experience Analysis during\n  COVID-19 Pandemic", "comments": "14 pages", "journal-ref": "Scientific Journal of Informatics (SJI), 7(2), 265-278 (2020)", "doi": "10.15294/sji.v7i2.26056", "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of information technology in the era of big data and cloud\ncomputing has led to the trend of electronic payments through financial\ntechnology, or FinTech. One of the most popular FinTech applications in\nIndonesia is Go-Pay in the Gojek start-up application. This research will\nanalyze how the FinTech Go-Pay user experience both for transactions on Gojek\nand at merchants that collaborate with Gojek. User Experience (UX) is analyzed\nusing the User Experience Questionnaire which consists of 6 (six) variables\n(Attractiveness, Perspicuity, Efficiency, Dependability, Stimulation, and\nNovelty). Total data collected amounted to 258. After analyzing the calculation\nresults, the mean scores are obtained in the following order: Efficiency,\nPerspicuity, Stimulation, Attractiveness, Dependability, and Novelty. Then when\ncompared with benchmark data the following sequence is obtained: Efficiency,\nPerspicuity, Stimulation, Attractiveness, Dependability, and Novelty. Overall\nthe Go-Pay service is efficient and perspicuity, but the Go-Pay service needs\nto improve its novelty. This article provides additional knowledge or novelty\ncontributions, especially for user experience analysis using FinTech\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 03:03:16 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Abdillah", "Leon A.", ""]]}, {"id": "2012.07816", "submitter": "Micah Smith", "authors": "Micah J. Smith, J\\\"urgen Cito, Kelvin Lu, Kalyan Veeramachaneni", "title": "Enabling collaborative data science development with the Ballet\n  framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the open-source software development model has led to successful\nlarge-scale collaborations in building software systems, data science projects\nare frequently developed by individuals or small teams. We describe challenges\nto scaling data science collaborations and present a conceptual framework and\nML programming model to address them. We instantiate these ideas in Ballet, a\nlightweight framework for collaborative, open-source data science through a\nfocus on feature engineering, and an accompanying cloud-based development\nenvironment. Using our framework, collaborators incrementally propose feature\ndefinitions to a repository which are each subjected to an ML performance\nevaluation and can be automatically merged into an executable feature\nengineering pipeline. We leverage Ballet to conduct a case study analysis of an\nincome prediction problem with 27 collaborators, and discuss implications for\nfuture designers of collaborative projects.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:51:23 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 20:15:07 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 22:20:36 GMT"}, {"version": "v4", "created": "Thu, 15 Jul 2021 18:57:44 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Smith", "Micah J.", ""], ["Cito", "J\u00fcrgen", ""], ["Lu", "Kelvin", ""], ["Veeramachaneni", "Kalyan", ""]]}, {"id": "2012.08174", "submitter": "Georgios Papadopoulos Th.", "authors": "Georgios Th. Papadopoulos, Margherita Antona, Constantine Stephanidis", "title": "Towards open and expandable cognitive AI architectures for large-scale\n  multi-agent human-robot collaborative learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from Demonstration (LfD) constitutes one of the most robust\nmethodologies for constructing efficient cognitive robotic systems. Despite the\nlarge body of research works already reported, current key technological\nchallenges include those of multi-agent learning and long-term autonomy.\nTowards this direction, a novel cognitive architecture for multi-agent LfD\nrobotic learning is introduced, targeting to enable the reliable deployment of\nopen, scalable and expandable robotic systems in large-scale and complex\nenvironments. In particular, the designed architecture capitalizes on the\nrecent advances in the Artificial Intelligence (AI) field, by establishing a\nFederated Learning (FL)-based framework for incarnating a multi-human\nmulti-robot collaborative learning environment. The fundamental\nconceptualization relies on employing multiple AI-empowered cognitive processes\n(implementing various robotic tasks) that operate at the edge nodes of a\nnetwork of robotic platforms, while global AI models (underpinning the\naforementioned robotic tasks) are collectively created and shared among the\nnetwork, by elegantly combining information from a large number of human-robot\ninteraction instances. Regarding pivotal novelties, the designed cognitive\narchitecture a) introduces a new FL-based formalism that extends the\nconventional LfD learning paradigm to support large-scale multi-agent\noperational settings, b) elaborates previous FL-based self-learning robotic\nschemes so as to incorporate the human in the learning loop and c) consolidates\nthe fundamental principles of FL with additional sophisticated AI-enabled\nlearning methodologies for modelling the multi-level inter-dependencies among\nthe robotic tasks. The applicability of the proposed framework is explained\nusing an example of a real-world industrial case study for agile\nproduction-based Critical Raw Materials (CRM) recovery.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:49:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:15:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Papadopoulos", "Georgios Th.", ""], ["Antona", "Margherita", ""], ["Stephanidis", "Constantine", ""]]}, {"id": "2012.08178", "submitter": "Marcos Baez", "authors": "Maisie Badami, Marcos Baez, Shayan Zamanirad, Wei Kang", "title": "On how Cognitive Computing will plan your next Systematic Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.HC cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic literature reviews (SLRs) are at the heart of evidence-based\nresearch, setting the foundation for future research and practice. However,\nproducing good quality timely contributions is a challenging and highly\ncognitive endeavor, which has lately motivated the exploration of automation\nand support in the SLR process. In this paper we address an often overlooked\nphase in this process, that of planning literature reviews, and explore under\nthe lenses of cognitive process augmentation how to overcome its most salient\nchallenges. In doing so, we report on the insights from 24 SLR authors on\nplanning practices, its challenges as well as feedback on support strategies\ninspired by recent advances in cognitive computing. We frame our findings under\nthe cognitive augmentation framework, and report on a prototype implementation\nand evaluation focusing on further informing the technical feasibility.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 09:56:09 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Badami", "Maisie", ""], ["Baez", "Marcos", ""], ["Zamanirad", "Shayan", ""], ["Kang", "Wei", ""]]}, {"id": "2012.08254", "submitter": "Tanja Schneeberger", "authors": "Patrick Gebhard, Tanja Schneeberger, Michael Dietz, Elisabeth Andr\\'e,\n  Nida ul Habib Bajwa", "title": "Designing a Mobile Social and Vocational Reintegration Assistant for\n  Burn-out Outpatient Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Using Social Agents as health-care assistants or trainers is one focus area\nof IVA research. While their use as physical health-care agents is well\nestablished, their employment in the field of psychotherapeutic care comes with\ndaunting challenges. This paper presents our mobile Social Agent EmmA in the\nrole of a vocational reintegration assistant for burn-out outpatient treatment.\nWe follow a typical participatory design approach including experts and\npatients in order to address requirements from both sides. Since the success of\nsuch treatments is related to a patients emotion regulation capabilities, we\nemploy a real-time social signal interpretation together with a computational\nsimulation of emotion regulation that influences the agent's social behavior as\nwell as the situational selection of verbal treatment strategies. Overall, our\ninterdisciplinary approach enables a novel integrative concept for Social\nAgents as assistants for burn-out patients.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:41:56 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Gebhard", "Patrick", ""], ["Schneeberger", "Tanja", ""], ["Dietz", "Michael", ""], ["Andr\u00e9", "Elisabeth", ""], ["Bajwa", "Nida ul Habib", ""]]}, {"id": "2012.08678", "submitter": "Peter Washington", "authors": "Peter Washington, Haik Kalantarian, Jack Kent, Arman Husic, Aaron\n  Kline, Emilie Leblanc, Cathy Hou, Cezmi Mutlu, Kaitlyn Dunlap, Yordan Penev,\n  Maya Varma, Nate Stockham, Brianna Chrisman, Kelley Paskov, Min Woo Sun,\n  Jae-Yoon Jung, Catalin Voss, Nick Haber, Dennis P. Wall", "title": "Training an Emotion Detection Classifier using Frames from a Mobile\n  Therapeutic Game for Children with Developmental Disorders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automated emotion classification could aid those who struggle to recognize\nemotion, including children with developmental behavioral conditions such as\nautism. However, most computer vision emotion models are trained on adult\naffect and therefore underperform on child faces. In this study, we designed a\nstrategy to gamify the collection and the labeling of child affect data in an\neffort to boost the performance of automatic child emotion detection to a level\ncloser to what will be needed for translational digital healthcare. We\nleveraged our therapeutic smartphone game, GuessWhat, which was designed in\nlarge part for children with developmental and behavioral conditions, to gamify\nthe secure collection of video data of children expressing a variety of\nemotions prompted by the game. Through a secure web interface gamifying the\nhuman labeling effort, we gathered and labeled 2,155 videos, 39,968 emotion\nframes, and 106,001 labels on all images. With this drastically expanded\npediatric emotion centric database (>30x larger than existing public pediatric\naffect datasets), we trained a pediatric emotion classification convolutional\nneural network (CNN) classifier of happy, sad, surprised, fearful, angry,\ndisgust, and neutral expressions in children. The classifier achieved 66.9%\nbalanced accuracy and 67.4% F1-score on the entirety of CAFE as well as 79.1%\nbalanced accuracy and 78.0% F1-score on CAFE Subset A, a subset containing at\nleast 60% human agreement on emotions labels. This performance is at least 10%\nhigher than all previously published classifiers, the best of which reached\n56.% balanced accuracy even when combining \"anger\" and \"disgust\" into a single\nclass. This work validates that mobile games designed for pediatric therapies\ncan generate high volumes of domain-relevant datasets to train state of the art\nclassifiers to perform tasks highly relevant to precision health efforts.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 00:08:51 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Washington", "Peter", ""], ["Kalantarian", "Haik", ""], ["Kent", "Jack", ""], ["Husic", "Arman", ""], ["Kline", "Aaron", ""], ["Leblanc", "Emilie", ""], ["Hou", "Cathy", ""], ["Mutlu", "Cezmi", ""], ["Dunlap", "Kaitlyn", ""], ["Penev", "Yordan", ""], ["Varma", "Maya", ""], ["Stockham", "Nate", ""], ["Chrisman", "Brianna", ""], ["Paskov", "Kelley", ""], ["Sun", "Min Woo", ""], ["Jung", "Jae-Yoon", ""], ["Voss", "Catalin", ""], ["Haber", "Nick", ""], ["Wall", "Dennis P.", ""]]}, {"id": "2012.08703", "submitter": "Bo Yang", "authors": "Bo Yang, Jian Huang, Xiaolong Li, Xinxing Chen, Caihua Xiong, Yasuhisa\n  Hasegawa", "title": "Natural grasp intention recognition based on gaze fixation in\n  human-robot interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movement is closely related to limb actions, so it can be used to infer\nmovement intentions. More importantly, in some cases, eye movement is the only\nway for paralyzed and impaired patients with severe movement disorders to\ncommunicate and interact with the environment. Despite this, eye-tracking\ntechnology still has very limited application scenarios as an intention\nrecognition method. The goal of this paper is to achieve a natural\nfixation-based grasping intention recognition method, with which a user with\nhand movement disorders can intuitively express what tasks he/she wants to do\nby directly looking at the object of interest. Toward this goal, we design\nexperiments to study the relationships of fixations in different tasks. We\npropose some quantitative features from these relationships and analyze them\nstatistically. Then we design a natural method for grasping intention\nrecognition. The experimental results prove that the accuracy of the proposed\nmethod for the grasping intention recognition exceeds 89\\% on the training\nobjects. When this method is extendedly applied to objects not included in the\ntraining set, the average accuracy exceeds 85\\%. The grasping experiment in the\nactual environment verifies the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 02:24:14 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Yang", "Bo", ""], ["Huang", "Jian", ""], ["Li", "Xiaolong", ""], ["Chen", "Xinxing", ""], ["Xiong", "Caihua", ""], ["Hasegawa", "Yasuhisa", ""]]}, {"id": "2012.08849", "submitter": "Andres Pinilla", "authors": "Andres Pinilla, Jaime Garcia, William Raffe, Jan-Niklas Voigt-Antons,\n  Robert Spang, Sebastian M\\\"oller", "title": "Affective visualization in Virtual Reality: An integrative review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A cluster of research in Affective Computing suggests that it is possible to\ninfer some characteristics of users' affective states by analyzing their\nelectrophysiological activity in real-time. However, it is not clear how to use\nthe information extracted from electrophysiological signals to create visual\nrepresentations of the affective states of Virtual Reality (VR) users.\nVisualization of users' affective states in VR can lead to biofeedback\ntherapies for mental health care. Understanding how to visualize affective\nstates in VR requires an interdisciplinary approach that integrates psychology,\nelectrophysiology, and audio-visual design. Therefore, this review aims to\nintegrate previous studies from these fields to understand how to develop\nvirtual environments that can automatically create visual representations of\nusers' affective states. The manuscript addresses this challenge in four\nsections: First, theories related to emotion and affect are summarized. Second,\nevidence suggesting that visual and sound cues tend to be associated with\naffective states are discussed. Third, some of the available methods for\nassessing affect are described. The fourth and final section contains five\npractical considerations for the development of virtual reality environments\nfor affect visualization.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 10:42:40 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 09:01:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Pinilla", "Andres", ""], ["Garcia", "Jaime", ""], ["Raffe", "William", ""], ["Voigt-Antons", "Jan-Niklas", ""], ["Spang", "Robert", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2012.09013", "submitter": "Sean Oesch", "authors": "Sean Oesch, Robert Bridges, Jared Smith, Justin Beaver, John Goodall,\n  Kelly Huffer, Craig Miles, Dan Scofield", "title": "An Assessment of the Usability of Machine Learning Based Tools for the\n  Security Operations Center", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gartner, a large research and advisory company, anticipates that by 2024 80%\nof security operation centers (SOCs) will use machine learning (ML) based\nsolutions to enhance their operations. In light of such widespread adoption, it\nis vital for the research community to identify and address usability concerns.\nThis work presents the results of the first in situ usability assessment of\nML-based tools. With the support of the US Navy, we leveraged the national\ncyber range, a large, air-gapped cyber testbed equipped with state-of-the-art\nnetwork and user emulation capabilities, to study six US Naval SOC analysts'\nusage of two tools. Our analysis identified several serious usability issues,\nincluding multiple violations of established usability heuristics form user\ninterface design. We also discovered that analysts lacked a clear mental model\nof how these tools generate scores, resulting in mistrust and/or misuse of the\ntools themselves. Surprisingly, we found no correlation between analysts' level\nof education or years of experience and their performance with either tool,\nsuggesting that other factors such as prior background knowledge or personality\nplay a significant role in ML-based tool usage. Our findings demonstrate that\nML-based security tool vendors must put a renewed focus on working with\nanalysts, both experienced and inexperienced, to ensure that their systems are\nusable and useful in real-world security operations settings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 15:17:18 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Oesch", "Sean", ""], ["Bridges", "Robert", ""], ["Smith", "Jared", ""], ["Beaver", "Justin", ""], ["Goodall", "John", ""], ["Huffer", "Kelly", ""], ["Miles", "Craig", ""], ["Scofield", "Dan", ""]]}, {"id": "2012.09015", "submitter": "Nuno Fachada", "authors": "Nuno Fachada", "title": "ColorShapeLinks: A board game AI competition for educators and students", "comments": "The peer-reviewed version of this paper is published in Computers and\n  Education: Artificial Intelligence at\n  https://doi.org/10.1016/j.caeai.2021.100014. This version is typeset by the\n  author and differs only in pagination and typographical detail", "journal-ref": "Computers and Education: Artificial Intelligence, 2, 100014, 2021", "doi": "10.1016/j.caeai.2021.100014", "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ColorShapeLinks is an AI board game competition framework specially designed\nfor students and educators in videogame development, with openness and\naccessibility in mind. The competition is based on an arbitrarily-sized version\nof the Simplexity board game, the motto of which, \"simple to learn, complex to\nmaster\", is curiously also applicable to AI agents. ColorShapeLinks offers\ngraphical and text-based frontends and a completely open and documented\ndevelopment framework built using industry standard tools and following\nsoftware engineering best practices. ColorShapeLinks is not only a competition,\nbut both a game and a framework which educators and students can extend and use\nto host their own competitions. It has been successfully used for running\ninternal competitions in AI classes, as well as for hosting an international AI\ncompetition at the IEEE Conference on Games.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 15:21:29 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 18:05:20 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Fachada", "Nuno", ""]]}, {"id": "2012.09130", "submitter": "Sarada Devaram", "authors": "Sarada Devaram", "title": "Empathic Chatbot: Emotional Intelligence for Empathic Chatbot: Emotional\n  Intelligence for Mental Health Well-being", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Conversational chatbots are Artificial Intelligence (AI)-powered applications\nthat assist users with various tasks by responding in natural language and are\nprevalent across different industries. Most of the chatbots that we encounter\non websites and digital assistants such as Alexa, Siri does not express empathy\ntowards the user, and their ability to empathise remains immature. Lack of\nempathy towards the user is not critical for a transactional or interactive\nchatbot, but the bots designed to support mental healthcare patients need to\nunderstand the emotional state of the user and tailor the conversations. This\nresearch explains the different types of emotional intelligence methodologies\nadopted in the development of an empathic chatbot and how far they have been\nadopted and succeeded.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:31:28 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Devaram", "Sarada", ""]]}, {"id": "2012.09131", "submitter": "Amir M. Rahmani", "authors": "Amir M. Rahmani, Jocelyn Lai, Salar Jafarlou, Asal Yunusova, Alex. P.\n  Rivera, Sina Labbaf, Sirui Hu, Arman Anzanpour, Nikil Dutt, Ramesh Jain,\n  Jessica L. Borelli", "title": "Personal Mental Health Navigator: Harnessing the Power of Data, Personal\n  Models, and Health Cybernetics to Promote Psychological Well-being", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.CY cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the regime of mental healthcare has followed an episodic\npsychotherapy model wherein patients seek care from a provider through a\nprescribed treatment plan developed over multiple provider visits. Recent\nadvances in wearable and mobile technology have generated increased interest in\ndigital mental healthcare that enables individuals to address episodic mental\nhealth symptoms. However, these efforts are typically reactive and\nsymptom-focused and do not provide comprehensive, wrap-around, customized\ntreatments that capture an individual's holistic mental health model as it\nunfolds over time. Recognizing that each individual is unique, we present the\nnotion of Personalized Mental Health Navigation (MHN): a therapist-in-the-loop,\ncybernetic goal-based system that deploys a continuous cyclic loop of\nmeasurement, estimation, guidance, to steer the individual's mental health\nstate towards a healthy zone. We outline the major components of MHN that is\npremised on the development of an individual's personal mental health state,\nholistically represented by a high-dimensional cover of multiple knowledge\nlayers such as emotion, biological patterns, sociology, behavior, and\ncognition. We demonstrate the feasibility of the personalized MHN approach via\na 12-month pilot case study for holistic stress management in college students\nand highlight an instance of a therapist-in-the-loop intervention using MHN for\nmonitoring, estimating, and proactively addressing moderately severe depression\nover a sustained period of time. We believe MHN paves the way to transform\nmental healthcare from the current passive, episodic, reactive process (where\nindividuals seek help to address symptoms that have already manifested) to a\ncontinuous and navigational paradigm that leverages a personalized model of the\nindividual, promising to deliver timely interventions to individuals in a\nholistic manner.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:34:09 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Rahmani", "Amir M.", ""], ["Lai", "Jocelyn", ""], ["Jafarlou", "Salar", ""], ["Yunusova", "Asal", ""], ["Rivera", "Alex. P.", ""], ["Labbaf", "Sina", ""], ["Hu", "Sirui", ""], ["Anzanpour", "Arman", ""], ["Dutt", "Nikil", ""], ["Jain", "Ramesh", ""], ["Borelli", "Jessica L.", ""]]}, {"id": "2012.09244", "submitter": "Sean Oesch", "authors": "Sean Oesch, Rob Gillen, Tom Karnowski", "title": "An Integrated Platform for Collaborative Data Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.SI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  While collaboration among data scientists is a key to organizational\nproductivity, data analysts face significant barriers to achieving this end,\nincluding data sharing, accessing and configuring the required computational\nenvironment, and a unified method of sharing knowledge. Each of these barriers\nto collaboration is related to the fundamental question of knowledge management\n\"how can organizations use knowledge more effectively?\". In this paper, we\nconsider the problem of knowledge management in collaborative data analytics\nand present ShareAL, an integrated knowledge management platform, as a solution\nto that problem. The ShareAL platform consists of three core components: a full\nstack web application, a dashboard for analyzing streaming data and a High\nPerformance Computing (HPC) cluster for performing real time analysis. Prior\nresearch has not applied knowledge management to collaborative analytics or\ndeveloped a platform with the same capabilities as ShareAL. ShareAL overcomes\nthe barriers data scientists face to collaboration by providing intuitive\nsharing of data and analytics via the web application, a shared computing\nenvironment via the HPC cluster and knowledge sharing and collaboration via a\nreal time messaging application.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:17:42 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Oesch", "Sean", ""], ["Gillen", "Rob", ""], ["Karnowski", "Tom", ""]]}, {"id": "2012.09559", "submitter": "Lakmal Meegahapola", "authors": "Lakmal Meegahapola and Daniel Gatica-Perez", "title": "Smartphone Sensing for the Well-being of Young Adults: A Review", "comments": "IEEE Access", "journal-ref": null, "doi": "10.1109/ACCESS.2020.3045935", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, mobile phones have become versatile devices with a multitude\nof capabilities due to the plethora of embedded sensors that enable them to\ncapture rich data unobtrusively. In a world where people are more conscious\nregarding their health and well-being, the pervasiveness of smartphones has\nenabled researchers to build apps that assist people to live healthier\nlifestyles, and to diagnose and monitor various health conditions. Motivated by\nthe high smartphone coverage among young adults and the unique issues they\nface, in this review paper, we focus on studies that have used smartphone\nsensing for the well-being of young adults. We analyze existing work in the\ndomain from two perspectives, namely Data Perspective and System Perspective.\nFor both these perspectives, we propose taxonomies motivated from human science\nliterature, which enable to identify important study areas. Furthermore, we\nemphasize the importance of diversity-awareness in smartphone sensing, and\nprovide insights and future directions for researchers in ubiquitous and mobile\ncomputing, and especially to new researchers who want to understand the basics\nof smartphone sensing research targeting the well-being of young adults.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 13:16:16 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 11:51:30 GMT"}, {"version": "v3", "created": "Tue, 22 Dec 2020 18:54:25 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2021 12:20:28 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Meegahapola", "Lakmal", ""], ["Gatica-Perez", "Daniel", ""]]}, {"id": "2012.09603", "submitter": "Shashishekar Ramakrishna", "authors": "Lukasz Gorski, Shashishekar Ramakrishna and Jedrzej M. Nowosielski", "title": "Towards Grad-CAM Based Explainability in a Legal Text Processing\n  Pipeline", "comments": "Workshop on EXplainable & Responsible AI in Law (XAILA) at 33rd\n  International Conference on Legal Knowledge and Information Systems (JURIX)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable AI(XAI)is a domain focused on providing interpretability and\nexplainability of a decision-making process. In the domain of law, in addition\nto system and data transparency, it also requires the (legal-) decision-model\ntransparency and the ability to understand the models inner working when\narriving at the decision. This paper provides the first approaches to using a\npopular image processing technique, Grad-CAM, to showcase the explainability\nconcept for legal texts. With the help of adapted Grad-CAM metrics, we show the\ninterplay between the choice of embeddings, its consideration of contextual\ninformation, and their effect on downstream processing.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:58:44 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Gorski", "Lukasz", ""], ["Ramakrishna", "Shashishekar", ""], ["Nowosielski", "Jedrzej M.", ""]]}, {"id": "2012.09633", "submitter": "Tim Krake", "authors": "Tim Krake, Stefan Reinhardt, Marcel Hlawatsch, Bernhard Eberhardt,\n  Daniel Weiskopf", "title": "Visualization and Selection of Dynamic Mode Decomposition Components for\n  Unsteady Flow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Mode Decomposition (DMD) is a data-driven and model-free\ndecomposition technique. It is suitable for revealing spatio-temporal features\nof both numerically and experimentally acquired data. Conceptually, DMD\nperforms a low-dimensional spectral decomposition of the data into the\nfollowing components: The modes, called DMD modes, encode the spatial\ncontribution of the decomposition, whereas the DMD amplitudes specify their\nimpact. Each associated eigenvalue, referred to as DMD eigenvalue,\ncharacterizes the frequency and growth rate of the DMD mode. In this paper, we\ndemonstrate how the components of DMD can be utilized to obtain temporal and\nspatial information from time-dependent flow fields. We begin with the\ntheoretical background of DMD and its application to unsteady flow. Next, we\nexamine the conventional process with DMD mathematically and put it in\nrelationship to the discrete Fourier transform. Our analysis shows that the\ncurrent use of DMD components has several drawbacks. To resolve these problems\nwe adjust the components and provide new and meaningful insights into the\ndecomposition: We show that our improved components describe the flow more\nadequately. Moreover, we remove redundancies in the decomposition and clarify\nthe interplay between components, allowing users to understand the impact of\ncomponents. These new representations ,which respect the spatio-temporal\ncharacter of DMD, enable two clustering methods that segment the flow into\nphysically relevant sections and can therefore be used for the selection of DMD\ncomponents. With a number of typical examples, we demonstrate that the\ncombination of these techniques allow new insights with DMD for unsteady flow.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 08:42:14 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Krake", "Tim", ""], ["Reinhardt", "Stefan", ""], ["Hlawatsch", "Marcel", ""], ["Eberhardt", "Bernhard", ""], ["Weiskopf", "Daniel", ""]]}, {"id": "2012.09950", "submitter": "Rajesh Kumar", "authors": "Rajesh Kumar and Can Isik and Vir V Phoha", "title": "Treadmill Assisted Gait Spoofing (TAGS): An Emerging Threat to wearable\n  Sensor-based Gait Authentication", "comments": "17 pages", "journal-ref": "ACM Journal of Digital Threats: Research and Practice, June 2021", "doi": "10.1145/3442151", "report-no": null, "categories": "cs.CR cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we examine the impact of Treadmill Assisted Gait Spoofing\n(TAGS) on Wearable Sensor-based Gait Authentication (WSGait). We consider more\nrealistic implementation and deployment scenarios than the previous study,\nwhich focused only on the accelerometer sensor and a fixed set of features.\nSpecifically, we consider the situations in which the implementation of WSGait\ncould be using one or more sensors embedded into modern smartphones. Besides,\nit could be using different sets of features or different classification\nalgorithms, or both. Despite the use of a variety of sensors, feature sets\n(ranked by mutual information), and six different classification algorithms,\nTAGS was able to increase the average False Accept Rate (FAR) from 4% to 26%.\nSuch a considerable increase in the average FAR, especially under the stringent\nimplementation and deployment scenarios considered in this study, calls for a\nfurther investigation into the design of evaluations of WSGait before its\ndeployment for public use.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 21:58:41 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kumar", "Rajesh", ""], ["Isik", "Can", ""], ["Phoha", "Vir V", ""]]}, {"id": "2012.09951", "submitter": "Yuriy Brun", "authors": "Brittany Johnson, Jesse Bartola, Rico Angell, Katherine Keith, Sam\n  Witty, Stephen J. Giguere, Yuriy Brun", "title": "Fairkit, Fairkit, on the Wall, Who's the Fairest of Them All? Supporting\n  Data Scientists in Training Fair Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Modern software relies heavily on data and machine learning, and affects\ndecisions that shape our world. Unfortunately, recent studies have shown that\nbecause of biases in data, software systems frequently inject bias into their\ndecisions, from producing better closed caption transcriptions of men's voices\nthan of women's voices to overcharging people of color for financial loans. To\naddress bias in machine learning, data scientists need tools that help them\nunderstand the trade-offs between model quality and fairness in their specific\ndata domains. Toward that end, we present fairkit-learn, a toolkit for helping\ndata scientists reason about and understand fairness. Fairkit-learn works with\nstate-of-the-art machine learning tools and uses the same interfaces to ease\nadoption. It can evaluate thousands of models produced by multiple machine\nlearning algorithms, hyperparameters, and data permutations, and compute and\nvisualize a small Pareto-optimal set of models that describe the optimal\ntrade-offs between fairness and quality. We evaluate fairkit-learn via a user\nstudy with 54 students, showing that students using fairkit-learn produce\nmodels that provide a better balance between fairness and quality than students\nusing scikit-learn and IBM AI Fairness 360 toolkits. With fairkit-learn, users\ncan select models that are up to 67% more fair and 10% more accurate than the\nmodels they are likely to train with scikit-learn.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 21:59:29 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Johnson", "Brittany", ""], ["Bartola", "Jesse", ""], ["Angell", "Rico", ""], ["Keith", "Katherine", ""], ["Witty", "Sam", ""], ["Giguere", "Stephen J.", ""], ["Brun", "Yuriy", ""]]}, {"id": "2012.10246", "submitter": "Bruno Cartaxo", "authors": "Antonio Sa Barreto Neto, Felipe Farias, Marco Aurelio Tomaz Mialaret,\n  Bruno Cartaxo, Priscila Alves Lima, Paulo Maciel", "title": "Building Energy Consumption Models Based On Smartphone User's Usage\n  Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing usage of smartphones in everyday tasks has been motivated many\nstudies on energy consumption characterization aiming to improve smartphone\ndevices' effectiveness and increase user usage time. In this scenario, it is\nessential to study mechanisms capable of characterizing user usage patterns, so\nsmartphones' components can be adapted to promote the best user experience with\nlower energy consumption. The goal of this study is to build an energy\nconsumption model based on user usage patterns aiming to provide the best\naccurate model to be used by application developers and automated optimization.\nTo develop the energy consumption models, we established a method to identify\nthe components with the most influence in the smartphone's energy consumption\nand identify the states of each influential device. Besides that, we\nestablished a method to prove the robustness of the models constructed using\ninaccurate hardware and a strategy to assess the accuracy of the model built.\nAfter training and testing each strategy to model the energy consumption based\non the user's usage and perform the Nemenyi test, we demonstrated that it is\npossible to get a Mean Absolute Error of 158.57mW when the smartphone's average\npower is 1970.1mW. Some studies show that the leading smartphone's workload is\nthe user. Based on this fact, we developed an automatic model building\nmethodology that is capable of analyzing the user's usage data and build smart\nmodels that can estimate the smartphone's energy consumption based on the\nuser's usage pattern. With the automatic model building methodology, we can\nadopt strategies to minimize the usage of components that drain the battery.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 00:53:40 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Neto", "Antonio Sa Barreto", ""], ["Farias", "Felipe", ""], ["Mialaret", "Marco Aurelio Tomaz", ""], ["Cartaxo", "Bruno", ""], ["Lima", "Priscila Alves", ""], ["Maciel", "Paulo", ""]]}, {"id": "2012.10422", "submitter": "Abhishek Das", "authors": "Abhishek Das, Vivek Dhuri, Ranjushree Pal", "title": "Smart Refrigerator using Internet of Things and Android", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The kitchen is regarded as the central unit of the traditional as well as\nmodern homes. It is where people cook meals and where our families sit together\nto eat food. The refrigerator is the pivotal of all that, and hence it plays an\nimportant part in our regular lives. The idea of this project is to improvise\nthe normal refrigerator into a smart one by making it to place order for food\nitems and to create an virtual interactive environment between it and the user.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 18:25:58 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Das", "Abhishek", ""], ["Dhuri", "Vivek", ""], ["Pal", "Ranjushree", ""]]}, {"id": "2012.10710", "submitter": "Mehul Bhatt", "authors": "Vasiliki Kondyli and Mehul Bhatt and Evgenia Spyridonos", "title": "Visuo-Locomotive Complexity as a Component of Parametric Systems for\n  Architecture Design", "comments": "This is a preprint of the contribution published as part of the\n  proceedings of ICoRD 2021: 8th International Conference on Research into\n  Design, IDC School of Design (IIT Mumbai, India). ICoRD 2021,\n  www.idc.iitb.ac.in/icord2021/ - The overall scientific agenda driving this\n  research may be consulted here: The DesignSpace Group / www.design-space.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A people-centred approach for designing large-scale built-up spaces\nnecessitates systematic anticipation of user's embodied visuo-locomotive\nexperience from the viewpoint of human-environment interaction factors\npertaining to aspects such as navigation, wayfinding, usability. In this\ncontext, we develop a behaviour-based visuo-locomotive complexity model that\nfunctions as a key correlate of cognitive performance vis-a-vis internal\nnavigation in built-up spaces. We also demonstrate the model's implementation\nand application as a parametric tool for the identification and manipulation of\nthe architectural morphology along a navigation path as per the parameters of\nthe proposed visuospatial complexity model. We present examples based on an\nempirical study in two healthcare buildings, and showcase the manner in which a\ndynamic and interactive parametric (complexity) model can promote\nbehaviour-based decision-making throughout the design process to maintain\ndesired levels of visuospatial complexity as part of a navigation or wayfinding\nexperience.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 15:11:32 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kondyli", "Vasiliki", ""], ["Bhatt", "Mehul", ""], ["Spyridonos", "Evgenia", ""]]}, {"id": "2012.10904", "submitter": "Thomas Kosch", "authors": "Thomas Kosch and Albrecht Schmidt", "title": "Enabling Tangible Interaction through Detection and Augmentation of\n  Everyday Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital interaction with everyday objects has become popular since the\nproliferation of camera-based systems that detect and augment objects\n\"just-in-time\". Common systems use a vision-based approach to detect objects\nand display their functionalities to the user. Sensors, such as color and depth\ncameras, have become inexpensive and allow seamless environmental tracking in\nmobile as well as stationary settings. However, object detection in different\ncontexts faces challenges as it highly depends on environmental parameters and\nthe conditions of the object itself. In this work, we present three tracking\nalgorithms which we have employed in past research projects to track and\nrecognize objects. We show, how mobile and stationary augmented reality can be\nused to extend the functionalities of objects. We conclude, how common items\ncan provide user-defined tangible interaction beyond their regular\nfunctionality.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 12:04:40 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kosch", "Thomas", ""], ["Schmidt", "Albrecht", ""]]}, {"id": "2012.10912", "submitter": "Carolin Wienrich Prof. Dr.", "authors": "Carolin Wienrich, Nina Ines D\\\"ollinger and Rebecca Hein", "title": "Mind the Gap: A Framework (BehaveFIT) Guiding The Use of Immersive\n  Technologies in Behavior Change Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The design and evaluation of assisting technologies to support behavior\nchange processes have become an essential topic within the field of\nhuman-computer interaction research in general and the field of immersive\nintervention technologies in particular. The mechanisms and success of behavior\nchange techniques and interventions are broadly investigated in the field of\npsychology. However, it is not always easy to adapt these psychological\nfindings to the context of immersive technologies. The lack of theoretical\nfoundation also leads to a lack of explanation as to why and how immersive\ninterventions support behavior change processes. The Behavioral Framework for\nimmersive Technologies (BehaveFIT) addresses this lack by (1) presenting an\nintelligible categorization and condensation of psychological barriers and\nimmersive features, by (2) suggesting a mapping that shows why and how\nimmersive technologies can help to overcome barriers, and finally by (3)\nproposing a generic prediction path that enables a structured, theory-based\napproach to the development and evaluation of immersive interventions. These\nthree steps explain how BehaveFIT can be used, and include guiding questions\nand one example for each step. Thus, the present paper contributes to guidance\nfor immersive intervention design and evaluation, showing that immersive\ninterventions support behavior change processes and explain and predict 'why'\nand 'how' immersive interventions can bridge the intention-behavior-gap.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 12:48:01 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wienrich", "Carolin", ""], ["D\u00f6llinger", "Nina Ines", ""], ["Hein", "Rebecca", ""]]}, {"id": "2012.10999", "submitter": "Morinaga Masaya", "authors": "Masaya Morinaga, Susumu Saito, Teppei Nakano, Tetsunori Kobayashi,\n  Tetsuji Ogawa", "title": "Exploring Effectiveness of Inter-Microtask Qualification Tests in\n  Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qualification tests in crowdsourcing are often used to pre-filter workers by\nmeasuring their ability in executing microtasks.While creating qualification\ntests for each task type is considered as a common and reasonable way, this\nstudy investigates into its worker-filtering performance when the same\nqualification test is used across multiple types of tasks.On Amazon Mechanical\nTurk, we tested the annotation accuracy in six different cases where tasks\nconsisted of two different difficulty levels, arising from the identical\nreal-world domain: four combinatory cases in which the qualification test and\nthe actual task were the same or different from each other, as well as two\nother cases where workers with Masters Qualification were asked to perform the\nactual task only.The experimental results demonstrated the two following\nfindings: i) Workers that were assigned to a difficult qualification test\nscored better annotation accuracy regardless of the difficulty of the actual\ntask; ii) Workers with Masters Qualification scored better annotation accuracy\non the low-difficulty task, but were not as accurate as those who passed a\nqualification test on the high-difficulty task.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 19:11:14 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Morinaga", "Masaya", ""], ["Saito", "Susumu", ""], ["Nakano", "Teppei", ""], ["Kobayashi", "Tetsunori", ""], ["Ogawa", "Tetsuji", ""]]}, {"id": "2012.11109", "submitter": "Annie Preston", "authors": "Annie Preston and Kwan-Liu Ma", "title": "Communicating Uncertainty and Risk in Air Quality Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air quality maps help users make decisions to mitigate the effects of\npollution on their health. Standard maps show readings from individual air\nquality sensors or colored contours indicating estimated pollution levels.\nHowever, showing a single estimate may conceal uncertainty and reduce the\nappearance of risk, while showing sensor data yields varied interpretations. We\npresent several visualizations of uncertainty in air quality maps, including a\nfrequency-framing \"dotmap\" and small multiples, and we compare them with\nstandard contour and sensor-based maps. In a user study, we find that including\nuncertainty in these maps has a significant effect on how much users would\nchoose to reduce their physical activity, and that people make more cautious\ndecisions when using uncertainty-aware maps. Additionally, we analyze\nthink-aloud transcriptions from the experiment to understand more about how the\nrepresentation of uncertainty influences people's decision-making. Our results\nsuggest ways to design maps that can encourage certain types of reasoning,\nyield more consistent responses, and convey risk better than standard maps.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 04:04:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Preston", "Annie", ""], ["Ma", "Kwan-Liu", ""]]}, {"id": "2012.11537", "submitter": "Vaishakh Kedambaimoole", "authors": "Vaishakh Kedambaimoole, Neelotpala Kumar, Vijay Shirhatti, Suresh\n  Nuthalapati, Saurabh Kumar, Mangalore Manjunatha Nayak, Prosenjit Sen, Deji\n  Akinwande and Konandur Rajanna", "title": "Reduced Graphene Oxide Tattoo as Wearable Proximity Sensor", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": "10.1002/aelm.202001214", "report-no": null, "categories": "cs.HC physics.app-ph physics.bio-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human body is punctuated with wide array of sensory systems that provide\na high evolutionary advantage by facilitating formation of a detailed picture\nof the immediate surroundings. The sensors range across a wide spectrum,\nacquiring input from non-contact audio-visual means to contact based input via\npressure and temperature. The ambit of sensing can be extended further by\nimparting the body with increased non-contact sensing capability through the\nphenomenon of electrostatics. Here we present graphene-based tattoo sensor for\nproximity sensing, employing the principle of electrostatic gating. The sensor\nshows a remarkable change in resistance upon exposure to objects surrounded\nwith static charge on them. Compared to prior work in this field, the sensor\nhas demonstrated the highest recorded proximity detection range of 20 cm. It is\nultra-thin, highly skin conformal and comes with a facile transfer process such\nthat it can be tattooed on highly curvilinear rough substrates like the human\nskin, unlike other graphene-based proximity sensors reported before. Present\nwork details the operation of wearable proximity sensor while exploring the\neffect of mounting body on the working mechanism. A possible role of the sensor\nas an alerting system against unwarranted contact with objects in public places\nespecially during the current SARS-CoV-2 pandemic has also been explored in the\nform of an LED bracelet whose color is controlled by the proximity sensor\nattached to it.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 10:06:01 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Kedambaimoole", "Vaishakh", ""], ["Kumar", "Neelotpala", ""], ["Shirhatti", "Vijay", ""], ["Nuthalapati", "Suresh", ""], ["Kumar", "Saurabh", ""], ["Nayak", "Mangalore Manjunatha", ""], ["Sen", "Prosenjit", ""], ["Akinwande", "Deji", ""], ["Rajanna", "Konandur", ""]]}, {"id": "2012.11878", "submitter": "Leonard Yoon", "authors": "Leonard Yoon, Dongseok Yang, Jaehyun Kim, Choongho Chung and Sung-Hee\n  Lee", "title": "Placement Retargeting of Virtual Avatars to Dissimilar Indoor\n  Environments", "comments": "IEEE Transactions on Visualization and Computer Graphics (Early\n  Access)", "journal-ref": null, "doi": "10.1109/TVCG.2020.3018458", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapidly developing technologies are realizing a 3D telepresence, in which\ngeographically separated users can interact with each other through their\nvirtual avatars. In this paper, we present novel methods to determine the\navatar's position in an indoor space to preserve the semantics of the user's\nposition in a dissimilar indoor space with different space configurations and\nfurniture layouts. To this end, we first perform a user survey on the preferred\navatar placements for various indoor configurations and user placements, and\nidentify a set of related attributes, including interpersonal relation, visual\nattention, pose, and spatial characteristics, and quantify these attributes\nwith a set of features. By using the obtained dataset and identified features,\nwe train a neural network that predicts the similarity between two placements.\nNext, we develop an avatar placement method that preserves the semantics of the\nplacement of the remote user in a different space as much as possible. We show\nthe effectiveness of our methods by implementing a prototype AR-based\ntelepresence system and user evaluations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 08:33:07 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Yoon", "Leonard", ""], ["Yang", "Dongseok", ""], ["Kim", "Jaehyun", ""], ["Chung", "Choongho", ""], ["Lee", "Sung-Hee", ""]]}, {"id": "2012.11905", "submitter": "Silvan Mertes", "authors": "Silvan Mertes, Tobias Huber, Katharina Weitz, Alexander Heimerl,\n  Elisabeth Andr\\'e", "title": "This is not the Texture you are looking for! Introducing Novel\n  Counterfactual Explanations for Non-Experts using Generative Adversarial\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the ongoing rise of machine learning, the need for methods for\nexplaining decisions made by artificial intelligence systems is becoming a more\nand more important topic. Especially for image classification tasks, many\nstate-of-the-art tools to explain such classifiers rely on visual highlighting\nof important areas of the input data. Contrary, counterfactual explanation\nsystems try to enable a counterfactual reasoning by modifying the input image\nin a way such that the classifier would have made a different prediction. By\ndoing so, the users of counterfactual explanation systems are equipped with a\ncompletely different kind of explanatory information. However, methods for\ngenerating realistic counterfactual explanations for image classifiers are\nstill rare. In this work, we present a novel approach to generate such\ncounterfactual image explanations based on adversarial image-to-image\ntranslation techniques. Additionally, we conduct a user study to evaluate our\napproach in a use case which was inspired by a healthcare scenario. Our results\nshow that our approach leads to significantly better results regarding mental\nmodels, explanation satisfaction, trust, emotions, and self-efficacy than two\nstate-of-the art systems that work with saliency maps, namely LIME and LRP.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 10:08:05 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Mertes", "Silvan", ""], ["Huber", "Tobias", ""], ["Weitz", "Katharina", ""], ["Heimerl", "Alexander", ""], ["Andr\u00e9", "Elisabeth", ""]]}, {"id": "2012.11976", "submitter": "Daniel Str\\\"uber", "authors": "Johan Aronsson, Philip Lu, Daniel Str\\\"uber, Thorsten Berger", "title": "A Maturity Assessment Framework for Conversational AI Development\n  Platforms", "comments": "10 pages, 10 figures. Accepted for publication at SAC 2021:\n  ACM/SIGAPP Symposium On Applied Computing", "journal-ref": null, "doi": "10.1145/3412841.3442046", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational Artificial Intelligence (AI) systems have recently\nsky-rocketed in popularity and are now used in many applications, from car\nassistants to customer support. The development of conversational AI systems is\nsupported by a large variety of software platforms, all with similar goals, but\ndifferent focus points and functionalities. A systematic foundation for\nclassifying conversational AI platforms is currently lacking. We propose a\nframework for assessing the maturity level of conversational AI development\nplatforms. Our framework is based on a systematic literature review, in which\nwe extracted common and distinguishing features of various open-source and\ncommercial (or in-house) platforms. Inspired by language reference frameworks,\nwe identify different maturity levels that a conversational AI development\nplatform may exhibit in understanding and responding to user inputs. Our\nframework can guide organizations in selecting a conversational AI development\nplatform according to their needs, as well as helping researchers and platform\ndevelopers improving the maturity of their platforms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 12:58:08 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Aronsson", "Johan", ""], ["Lu", "Philip", ""], ["Str\u00fcber", "Daniel", ""], ["Berger", "Thorsten", ""]]}, {"id": "2012.11981", "submitter": "George Fragulis", "authors": "Maria Papatsimouli, Lazaros Lazaridis, Konstantinos-Filippos Kollias,\n  Ioannis Skordas, and George F. Fragulis", "title": "Speak with signs: Active learning platform for Greek Sign Language,\n  English Sign Language, and their translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sign Language is used to facilitate the communication between Deaf and\nnon-Deaf people. It uses signs-words with basic structural elements such as\nhandshape, parts of face, body or space, and the orientation of the\nfingers-palm. Sign Languages vary from people to people and from country to\ncountry and evolve as spoken languages. In the current study, an application\nwhich aims at Greek Sign Language and English Sign Language learning by hard of\nhearing people and talking people, has been developed. The application includes\ngrouped signs in alphabetical order. The user can find Greek Sign Language\nsigns, English sign language signs and translate from Greek sign language to\nEnglish sign language. The written word of each sign, and the corresponding\nmeaning are displayed. In addition, the sound is activated in order to enable\nusers with partial hearing loss to hear the pronunciation of each word. The\nuser is also provided with various tasks in order to enable an interaction of\nthe knowledge acquired by the user. This interaction is offered mainly by\nmultiplechoice tasks, incorporating text or video. The current application is\nnot a simple sign language dictionary as it provides the interactive\nparticipation of users. It is a platform for Greek and English sign language\nactive learning.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 13:06:02 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Papatsimouli", "Maria", ""], ["Lazaridis", "Lazaros", ""], ["Kollias", "Konstantinos-Filippos", ""], ["Skordas", "Ioannis", ""], ["Fragulis", "George F.", ""]]}, {"id": "2012.12000", "submitter": "George Fragulis", "authors": "Heracles Michailidis, Eleni Michailidi, Stavroula Tavoultzidou, and\n  George F. Fragulis", "title": "Teaching young learners a foreign language via tangible and graphical\n  user interfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of tangible interfaces in teaching has been proved more effective,\nuser -friendly and helpful in collaborative learning departments, when compared\nto traditional teaching approaches. In particular, the tangible interface\n\"Makey Makey\"is a modern tool that enhances collaboration between pupils, with\npositive results in education, despite the limited research done on this\ninterface so far. \"Makey Makey\" succeeds in motivating and engaging young\nlearners in the learning process, showing better performance and scoring\nresults. In addition, its use in teaching has been shown to benefit the\nlearning process in every age learning group.The development and use of such an\ninnovative teaching/learning approach helps young learners perceive the\neducational process in a different way and assimilate new cognitive fields more\neffectively. Moreover, educators profit as well, as they can eliminate\ndifficulties and teach more efficiently using examples based on their teaching\napproach, while enhancing young learners parallel skills as well. This study\nwill confirm previous research results stating that assimilation of new\nconcepts is easier with tangible interfaces than with graphical ones, as well\nas that young learners participating in the survey have shown significant\nprogress in knowledge acquisition when compared to their prior knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 13:40:52 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Michailidis", "Heracles", ""], ["Michailidi", "Eleni", ""], ["Tavoultzidou", "Stavroula", ""], ["Fragulis", "George F.", ""]]}, {"id": "2012.12002", "submitter": "David Cameron", "authors": "David Cameron, Marina Sarda Gou, Laura Sbaffi", "title": "Trust in robot-mediated health information", "comments": "2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper outlines a social robot platform for providing health information.\nIn comparison with previous findings for accessing information online, the use\nof a social robot may affect which factors users consider important when\nevaluating the trustworthiness of health information provided.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 13:45:13 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Cameron", "David", ""], ["Gou", "Marina Sarda", ""], ["Sbaffi", "Laura", ""]]}, {"id": "2012.12041", "submitter": "Farbod Nosrat Nezami", "authors": "Farbod N. Nezami, Maximilian A. W\\\"achter, Nora Maleki, Philipp\n  Spaniol, Lea M. K\\\"uhne, Anke Haas, Johannes M. Pingel, Linus Tiemann,\n  Frederik Nienhaus, Lynn Keller, Sabine K\\\"onig, Peter K\\\"onig, Gordon Pipa", "title": "WestDrive X LoopAR: An open-access virtual reality project in Unity for\n  evaluating user interaction methods during TOR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With the further development of highly automated vehicles, drivers will\nengage in non-related tasks while being driven. Still, drivers have to take\nover control when requested by the car. Here the question arises, how\npotentially distracted drivers get back into the control-loop quickly and\nsafely when the car requests a takeover. To investigate effective human-machine\ninteractions in mobile, versatile, and cost-efficient setup is needed. We\ndeveloped a virtual reality toolkit for the Unity 3D game engine containing all\nnecessary code and assets to enable fast adaptations to various human-machine\ninteraction experiments, including close monitoring of the subject. The\npresented project contains all needed functionalities for realistic traffic\nbehavior, cars, and pedestrians, as well as a large, open-source, scriptable,\nand modular VR environment. It covers roughly 25 square km, a package of 125\nanimated pedestrians and numerous vehicles, including motorbikes, trucks, and\ncars. It also contains all needed nature assets to make it both highly dynamic\nand realistic. The presented repository contains a C++ library made for LoopAR\nthat enables force feedback for gaming steering wheels as a fully supported\ncomponent. It also includes All necessary scripts for eye-tracking in the used\ndevices. All main functions are integrated into the graphical user interface of\nthe Unity Editor or are available as prefab variants to ease the use of the\nembedded functionalities. The primary purpose of this project is to serve as\nopen access, cost-efficient toolkit that enables interested researchers to\nconduct realistic virtual reality research studies without costly and immobile\nsimulators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 14:27:53 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Nezami", "Farbod N.", ""], ["W\u00e4chter", "Maximilian A.", ""], ["Maleki", "Nora", ""], ["Spaniol", "Philipp", ""], ["K\u00fchne", "Lea M.", ""], ["Haas", "Anke", ""], ["Pingel", "Johannes M.", ""], ["Tiemann", "Linus", ""], ["Nienhaus", "Frederik", ""], ["Keller", "Lynn", ""], ["K\u00f6nig", "Sabine", ""], ["K\u00f6nig", "Peter", ""], ["Pipa", "Gordon", ""]]}, {"id": "2012.12042", "submitter": "Stefano Savazzi", "authors": "Stefano Savazzi, Vittorio Rampa, Leonardo Costa, Sanaz Kianoush, Denis\n  Tolochenko", "title": "Processing of body-induced thermal signatures for physical distancing\n  and temperature screening", "comments": "This work is in part funded by Regione Lombardia POR-FESR 2014-2020\n  Innodriver framework programme. The paper has been accepted for publication\n  in the IEEE Sensors Journal. The current arXiv contains an additional\n  Appendix that describes the Envisense system configuration", "journal-ref": null, "doi": "10.1109/JSEN.2020.3047143", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Massive and unobtrusive screening of people in public environments is\nbecoming a critical task to guarantee safety in congested shared spaces, as\nwell as to support early non-invasive diagnosis and response to disease\noutbreaks. Among various sensors and Internet of Things (IoT) technologies,\nthermal vision systems, based on low-cost infrared (IR) array sensors, allow to\ntrack thermal signatures induced by moving people. Unlike contact tracing\napplications that exploit short-range communications, IR-based sensing systems\nare passive, as they do not need the cooperation of the subject(s) and do not\npose a threat to user privacy. The paper develops a signal processing framework\nthat enables the joint analysis of subject mobility while automating the\ntemperature screening process. The system consists of IR-based sensors that\nmonitor both subject motions and health status through temperature\nmeasurements. Sensors are networked via wireless IoT tools and are deployed\naccording to different configurations (wall- or ceiling-mounted setups). The\nsystem targets the joint passive localization of subjects by tracking their\nmutual distance and direction of arrival, in addition to the detection of\nanomalous body temperatures for subjects close to the IR sensors. Focusing on\nBayesian methods, the paper also addresses best practices and relevant\nimplementation challenges using on field measurements. The proposed framework\nis privacy-neutral, it can be employed in public and private services for\nhealthcare, smart living and shared spaces scenarios without any privacy\nconcerns. Different configurations are also considered targeting both\nindustrial, smart space and living environments.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 14:27:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Savazzi", "Stefano", ""], ["Rampa", "Vittorio", ""], ["Costa", "Leonardo", ""], ["Kianoush", "Sanaz", ""], ["Tolochenko", "Denis", ""]]}, {"id": "2012.12181", "submitter": "Poorna Talkad Sukumar", "authors": "Poorna Talkad Sukumar, Thomas Breideband, Gonzalo Martinez, Megan\n  Caruso, Sierra Rose, Cooper Steputis, Sidney D'Mello, Gloria Mark, Aaron\n  Striegel", "title": "Designing an Interactive Visualization System for Monitoring Participant\n  Compliance in a Large-Scale, Longitudinal Study", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": "10.1145/3411763.3443436", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent monitoring of participant compliance is necessary when conducting\nlarge-scale, longitudinal studies to ensure that the collected data is of\nsufficiently high quality. While the need for achieving high compliance has\nbeen underscored and there are discussions on incentives and factors affecting\ncompliance, little is shared about the actual processes and tools used for\nmonitoring compliance in such studies. Monitoring participant compliance with\nrespect to multi-modal data can be a tedious process, especially if there are\nonly a few personnel involved. In this case study, we describe the iterative\ndesign of an interactive visualization system we developed for monitoring\ncompliance and refined based on changing requirements in an ongoing study. We\nfind that the visualization system, leveraging the digital medium, both\nfacilitates the exploratory tasks of monitoring participant compliance and\nsupports asynchronous collaboration among non-co-located researchers. Our\ndocumented requirements for checking participant compliance as well as the\ndesign of the visualization system can help inform the compliance-monitoring\nprocess in future studies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 17:20:48 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Sukumar", "Poorna Talkad", ""], ["Breideband", "Thomas", ""], ["Martinez", "Gonzalo", ""], ["Caruso", "Megan", ""], ["Rose", "Sierra", ""], ["Steputis", "Cooper", ""], ["D'Mello", "Sidney", ""], ["Mark", "Gloria", ""], ["Striegel", "Aaron", ""]]}, {"id": "2012.12291", "submitter": "Kapil Katyal", "authors": "Kapil Katyal, Yuxiang Gao, Jared Markowitz, I-Jeng Wang, Chien-Ming\n  Huang", "title": "Group-Aware Robot Navigation in Crowded Environments", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-aware robot navigation promises a range of applications in which mobile\nrobots bring versatile assistance to people in common human environments. While\nprior research has mostly focused on modeling pedestrians as independent,\nintentional individuals, people move in groups; consequently, it is imperative\nfor mobile robots to respect human groups when navigating around people. This\npaper explores learning group-aware navigation policies based on dynamic group\nformation using deep reinforcement learning. Through simulation experiments, we\nshow that group-aware policies, compared to baseline policies that neglect\nhuman groups, achieve greater robot navigation performance (e.g., fewer\ncollisions), minimize violation of social norms and discomfort, and reduce the\nrobot's movement impact on pedestrians. Our results contribute to the\ndevelopment of social navigation and the integration of mobile robots into\nhuman environments.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 19:04:40 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Katyal", "Kapil", ""], ["Gao", "Yuxiang", ""], ["Markowitz", "Jared", ""], ["Wang", "I-Jeng", ""], ["Huang", "Chien-Ming", ""]]}, {"id": "2012.12415", "submitter": "Tianshi Li", "authors": "Tianshi Li, Camille Cobb, Jackie (Junrui) Yang, Sagar Baviskar, Yuvraj\n  Agarwal, Beibei Li, Lujo Bauer, Jason I. Hong", "title": "What Makes People Install a COVID-19 Contact-Tracing App? Understanding\n  the Influence of App Design and Individual Difference on Contact-Tracing App\n  Adoption Intention", "comments": "44 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smartphone-based contact-tracing apps are a promising solution to help scale\nup the conventional contact-tracing process. However, low adoption rates have\nbecome a major issue that prevents these apps from achieving their full\npotential. In this paper, we present a national-scale survey experiment ($N =\n1963$) in the U.S. to investigate the effects of app design choices and\nindividual differences on COVID-19 contact-tracing app adoption intentions. We\nfound that individual differences such as prosocialness, COVID-19 risk\nperceptions, general privacy concerns, technology readiness, and demographic\nfactors played a more important role than app design choices such as\ndecentralized design vs. centralized design, location use, app providers, and\nthe presentation of security risks. Certain app designs could exacerbate the\ndifferent preferences in different sub-populations which may lead to an\ninequality of acceptance to certain app design choices (e.g., developed by\nstate health authorities vs. a large tech company) among different groups of\npeople (e.g., people living in rural areas vs. people living in urban areas).\nOur mediation analysis showed that one's perception of the public health\nbenefits offered by the app and the adoption willingness of other people had a\nlarger effect in explaining the observed effects of app design choices and\nindividual differences than one's perception of the app's security and privacy\nrisks. With these findings, we discuss practical implications on the design,\nmarketing, and deployment of COVID-19 contact-tracing apps in the U.S.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 23:46:47 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 06:57:11 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 21:59:50 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Tianshi", "", "Junrui"], ["Cobb", "Camille", "", "Junrui"], ["Jackie", "", "", "Junrui"], ["Yang", "", ""], ["Baviskar", "Sagar", ""], ["Agarwal", "Yuvraj", ""], ["Li", "Beibei", ""], ["Bauer", "Lujo", ""], ["Hong", "Jason I.", ""]]}, {"id": "2012.12554", "submitter": "Alina Kuznetsova", "authors": "A. Kuznetsova, A. Talati, Y. Luo, K. Simmons and V. Ferrari", "title": "Efficient video annotation with visual interpolation and frame selection\n  guidance", "comments": "accepted to WACV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a unified framework for generic video annotation with bounding\nboxes. Video annotation is a longstanding problem, as it is a tedious and\ntime-consuming process. We tackle two important challenges of video annotation:\n(1) automatic temporal interpolation and extrapolation of bounding boxes\nprovided by a human annotator on a subset of all frames, and (2) automatic\nselection of frames to annotate manually. Our contribution is two-fold: first,\nwe propose a model that has both interpolating and extrapolating capabilities;\nsecond, we propose a guiding mechanism that sequentially generates suggestions\nfor what frame to annotate next, based on the annotations made previously. We\nextensively evaluate our approach on several challenging datasets in simulation\nand demonstrate a reduction in terms of the number of manual bounding boxes\ndrawn by 60% over linear interpolation and by 35% over an off-the-shelf\ntracker. Moreover, we also show 10% annotation time improvement over a\nstate-of-the-art method for video annotation with bounding boxes [25]. Finally,\nwe run human annotation experiments and provide extensive analysis of the\nresults, showing that our approach reduces actual measured annotation time by\n50% compared to commonly used linear interpolation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 09:31:40 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Kuznetsova", "A.", ""], ["Talati", "A.", ""], ["Luo", "Y.", ""], ["Simmons", "K.", ""], ["Ferrari", "V.", ""]]}, {"id": "2012.12593", "submitter": "Zaid Amin", "authors": "Zaid Amin, Nazlena Mohamad Ali, Alan F. Smeaton", "title": "Attention and misinformation sharing on social media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The behaviour of sharing information on social media should be fulfilled only\nwhen a user is exhibiting attentive behaviour. So that the useful information\ncan be consumed constructively, and misinformation can be identified and\nignored. Attentive behaviour is related to users' cognitive abilities in their\nprocessing of set information. The work described in this paper examines the\nissue of attentive factors that affect users' behaviour when they share\nmisinformation on social media. The research aims to identify the significance\nof prevailing attention factors towards sharing misinformation on social media.\nWe used a closed-ended questionnaire which consisted of a psychometric scale to\nmeasure attention behaviour with participants (n = 112). The regression\nequation results are obtained as: y=(19,533-0,390+e) from a set of regression\nanalyses shows that attention factors have a significant negative correlation\neffect for users to share misinformation on social media. Along with the\nfindings of the analysis results, we propose that attentive factors are\nincorporated into a social media application's future design that could\nintervene in user attention and avoid potential harm caused by the spread of\nmisinformation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 10:50:30 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 07:19:01 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Amin", "Zaid", ""], ["Ali", "Nazlena Mohamad", ""], ["Smeaton", "Alan F.", ""]]}, {"id": "2012.12794", "submitter": "Athanasios Vourvopoulos", "authors": "Athanasios Vourvopoulos, Simon Legeay and Patricia Figueiredo", "title": "NeuXus: A Biosignal Processing and Classification Pipeline for Real-Time\n  Brain-Computer Interaction", "comments": "12 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the last few years,Brain-Computer Interfaces (BCIs) have progressed as an\nemerging research area in the fields of human-computer interaction and\ninteractive systems.This is primarily due to the introduction of low-cost\nelectroencephalographic (EEG) systems that render BCI technology accessible for\nnon-medical research but also due to the advancements of signal processing and\nmachine learning methods.Consequently,BCIs could provide a wide new range of\npossibilities in the way users interact with a computer system (e.g.,\nneuroadaptive interfaces).However,major challenges must still be addressed for\nBCI systems to mature into an established communication medium for effective\nhuman-computer interaction. One of the major challenges involves the easy\nintegration of real-time processing pipelines with portable EEG systems for an\nout-of-the-lab use. To date, despite the amount of options current open-source\ntools provide, most toolboxes focus mainly in extending the processing and\nclassification methods but lack on the ability to provide an easy-to-design yet\nextensible architecture for ubiquitous use.Here, we present NeuXus, a modular\ntoolbox in Python for real-time biosignal processing and pipeline design.NeuXus\nis open-source and platform independent,providing high-level implementation of\nprocessing pipelines for easy BCI design and deployment.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 16:56:28 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Vourvopoulos", "Athanasios", ""], ["Legeay", "Simon", ""], ["Figueiredo", "Patricia", ""]]}, {"id": "2012.13052", "submitter": "Zhendong Chu", "authors": "Zhendong Chu, Jing Ma, Hongning Wang", "title": "Learning from Crowds by Modeling Common Confusions", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing provides a practical way to obtain large amounts of labeled\ndata at a low cost. However, the annotation quality of annotators varies\nconsiderably, which imposes new challenges in learning a high-quality model\nfrom the crowdsourced annotations. In this work, we provide a new perspective\nto decompose annotation noise into common noise and individual noise and\ndifferentiate the source of confusion based on instance difficulty and\nannotator expertise on a per-instance-annotator basis. We realize this new\ncrowdsourcing model by an end-to-end learning solution with two types of noise\nadaptation layers: one is shared across annotators to capture their commonly\nshared confusions, and the other one is pertaining to each annotator to realize\nindividual confusion. To recognize the source of noise in each annotation, we\nuse an auxiliary network to choose the two noise adaptation layers with respect\nto both instances and annotators. Extensive experiments on both synthesized and\nreal-world benchmarks demonstrate the effectiveness of our proposed common\nnoise adaptation solution.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 01:13:23 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 05:32:29 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chu", "Zhendong", ""], ["Ma", "Jing", ""], ["Wang", "Hongning", ""]]}, {"id": "2012.13188", "submitter": "Yalda Foroutan", "authors": "Yalda Foroutan, Ahmad Kalhor, Saeid Mohammadi Nejati, Samad Sheikhaei", "title": "Control of computer pointer using hand gesture recognition in motion\n  pictures", "comments": "8 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A user interface is designed to control the computer cursor by hand detection\nand classification of its gesture. A hand dataset with 6720 image samples is\ncollected, including four classes: fist, palm, pointing to the left, and\npointing to the right. The images are captured from 15 persons in simple\nbackgrounds and different perspectives and light conditions. A CNN network is\ntrained on this dataset to predict a label for each captured image and measure\nthe similarity of them. Finally, commands are defined to click, right-click and\nmove the cursor. The algorithm has 91.88% accuracy and can be used in different\nbackgrounds.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 10:24:51 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Foroutan", "Yalda", ""], ["Kalhor", "Ahmad", ""], ["Nejati", "Saeid Mohammadi", ""], ["Sheikhaei", "Samad", ""]]}, {"id": "2012.13265", "submitter": "Youngjun Cho", "authors": "Anastasia Schmitz, Catherine Holloway, Youngjun Cho", "title": "Hearing through Vibrations: Perception of Musical Emotions by Profoundly\n  Deaf People", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in tactile-audio feedback technology have created new possibilities\nfor deaf people to feel music. However, little is known about deaf individuals'\nperception of musical emotions through vibrotactile feedback. In this paper, we\npresent the findings from a mixed-methods study with 16 profoundly deaf\nparticipants. The study protocol was designed to explore how users of a\nbackpack-style vibrotactile display perceive intended emotions in twenty music\nexcerpts. Quantitative analysis demonstrated that participants correctly\nidentified happy and angry excerpts and rated them as more arousing than sad\nand peaceful excerpts. More positive emotions were experienced during happy\ncompared to angry excerpts while peaceful and sad excerpts were hard to be\ndifferentiated. Based on qualitative data, we highlight the benefits and\nlimitations of using vibrations to convey musical emotions to profoundly deaf\nusers. Finally, we provide guidelines for designing accessible music\nexperiences for the deaf community.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 14:14:42 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Schmitz", "Anastasia", ""], ["Holloway", "Catherine", ""], ["Cho", "Youngjun", ""]]}, {"id": "2012.13341", "submitter": "Yuchi Zhang", "authors": "Yuchi Zhang, Willis Peng, Bastian Wandt and Helge Rhodin", "title": "AudioViewer: Learning to Visualize Sound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CV cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory substitution can help persons with perceptual deficits. In this work,\nwe attempt to visualize audio with video. Our long-term goal is to create sound\nperception for hearing impaired people, for instance, to facilitate feedback\nfor training deaf speech. Different from existing models that translate between\nspeech and text or text and images, we target an immediate and low-level\ntranslation that applies to generic environment sounds and human speech without\ndelay. No canonical mapping is known for this artificial translation task. Our\ndesign is to translate from audio to video by compressing both into a common\nlatent space with shared structure. Our core contribution is the development\nand evaluation of learned mappings that respect human perception limits and\nmaximize user comfort by enforcing priors and combining strategies from\nunpaired image translation and disentanglement. We demonstrate qualitatively\nand quantitatively that our AudioViewer model maintains important audio\nfeatures in the generated video and that generated videos of faces and numbers\nare well suited for visualizing high-dimensional audio features since they can\neasily be parsed by humans to match and distinguish between sounds, words, and\nspeakers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 21:52:45 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 21:35:09 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 19:51:23 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhang", "Yuchi", ""], ["Peng", "Willis", ""], ["Wandt", "Bastian", ""], ["Rhodin", "Helge", ""]]}, {"id": "2012.13449", "submitter": "Abdul Rafey Aftab", "authors": "Abdul Rafey Aftab, Michael von der Beeck, Michael Feld", "title": "You Have a Point There: Object Selection Inside an Automobile Using\n  Gaze, Head Pose and Finger Pointing", "comments": null, "journal-ref": "In Proceedings of the 2020 International Conference on Multimodal\n  Interaction, pp. 595-603. 2020", "doi": "10.1145/3382507.3418836", "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sophisticated user interaction in the automotive industry is a fast emerging\ntopic. Mid-air gestures and speech already have numerous applications for\ndriver-car interaction. Additionally, multimodal approaches are being developed\nto leverage the use of multiple sensors for added advantages. In this paper, we\npropose a fast and practical multimodal fusion method based on machine learning\nfor the selection of various control modules in an automotive vehicle. The\nmodalities taken into account are gaze, head pose and finger pointing gesture.\nSpeech is used only as a trigger for fusion. Single modality has previously\nbeen used numerous times for recognition of the user's pointing direction. We,\nhowever, demonstrate how multiple inputs can be fused together to enhance the\nrecognition performance. Furthermore, we compare different deep neural network\narchitectures against conventional Machine Learning methods, namely Support\nVector Regression and Random Forests, and show the enhancements in the pointing\ndirection accuracy using deep learning. The results suggest a great potential\nfor the use of multimodal inputs that can be applied to more use cases in the\nvehicle.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 21:31:21 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Aftab", "Abdul Rafey", ""], ["von der Beeck", "Michael", ""], ["Feld", "Michael", ""]]}, {"id": "2012.13546", "submitter": "Maxim Bakaev", "authors": "Maxim Bakaev, Sebastian Heil, Martin Gaedke", "title": "Distributional Ground Truth: Non-Redundant Crowdsourcing Data Quality\n  Control in UI Labeling Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  HCI increasingly employs Machine Learning and Image Recognition, in\nparticular for visual analysis of user interfaces (UIs). A popular way for\nobtaining human-labeled training data is Crowdsourcing, typically using the\nquality control methods ground truth and majority consensus, which necessitate\nredundancy in the outcome. In our paper we propose a non-redundant method for\nprediction of crowdworkers' output quality in web UI labeling tasks, based on\nhomogeneity of distributions assessed with two-sample Kolmogorov-Smirnov test.\nUsing a dataset of about 500 screenshots with over 74,000 UI elements located\nand classified by 11 trusted labelers and 298 Amazon Mechanical Turk\ncrowdworkers, we demonstrate the advantage of our approach over the baseline\nmodel based on mean Time-on-Task. Exploring different dataset partitions, we\nshow that with the trusted set size of 17-27% UIs our \"distributional ground\ntruth\" model can achieve R2s of over 0.8 and help to obviate the ancillary work\neffort and expenses.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 09:06:10 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Bakaev", "Maxim", ""], ["Heil", "Sebastian", ""], ["Gaedke", "Martin", ""]]}, {"id": "2012.13551", "submitter": "Jan Philip G\\\"opfert", "authors": "Jan Philip G\\\"opfert, Ulrike Kuhl, Lukas Hindemith, Heiko Wersing,\n  Barbara Hammer", "title": "Intuitiveness in Active Teaching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a double-edged sword: it gives rise to astonishing\nresults in automated systems, but at the cost of tremendously large data\nrequirements. This makes many successful algorithms from machine learning\nunsuitable for human-machine interaction, where the machine must learn from a\nsmall number of training samples that can be provided by a user within a\nreasonable time frame. Fortunately, the user can tailor the training data they\ncreate to be as useful as possible, severely limiting its necessary size -- as\nlong as they know about the machine's requirements and limitations. Of course,\nacquiring this knowledge can in turn be cumbersome and costly. This raises the\nquestion how easy machine learning algorithms are to interact with. In this\nwork we address this issue by analyzing the intuitiveness of certain algorithms\nwhen they are actively taught by users. After developing a theoretical\nframework of intuitiveness as a property of algorithms, we present and discuss\nthe results of a large-scale user study into the performance and teaching\nstrategies of 800 users interacting with prominent machine learning algorithms.\nVia this extensive examination we offer a systematic method to judge the\nefficacy of human-machine interactions and thus, to scrutinize how accessible,\nunderstandable, and fair, a system is.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 09:31:56 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["G\u00f6pfert", "Jan Philip", ""], ["Kuhl", "Ulrike", ""], ["Hindemith", "Lukas", ""], ["Wersing", "Heiko", ""], ["Hammer", "Barbara", ""]]}, {"id": "2012.13603", "submitter": "Feng Zhou", "authors": "Jackie Ayoub, X. Jessie Yang, Feng Zhou", "title": "Modeling Dispositional and Initial learned Trust in Automated Vehicles\n  with Predictability and Explainability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Technological advances in the automotive industry are bringing automated\ndriving closer to road use. However, one of the most important factors\naffecting public acceptance of automated vehicles (AVs) is the public's trust\nin AVs. Many factors can influence people's trust, including perception of\nrisks and benefits, feelings, and knowledge of AVs. This study aims to use\nthese factors to predict people's dispositional and initial learned trust in\nAVs using a survey study conducted with 1175 participants. For each\nparticipant, 23 features were extracted from the survey questions to capture\nhis or her knowledge, perception, experience, behavioral assessment, and\nfeelings about AVs. These features were then used as input to train an eXtreme\nGradient Boosting (XGBoost) model to predict trust in AVs. With the help of\nSHapley Additive exPlanations (SHAP), we were able to interpret the trust\npredictions of XGBoost to further improve the explainability of the XGBoost\nmodel. Compared to traditional regression models and black-box machine learning\nmodels, our findings show that this approach was powerful in providing a high\nlevel of explainability and predictability of trust in AVs, simultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 16:49:57 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ayoub", "Jackie", ""], ["Yang", "X. Jessie", ""], ["Zhou", "Feng", ""]]}, {"id": "2012.13906", "submitter": "Mark Colley", "authors": "Mark Colley and Marcel Walch and Enrico Rukzio", "title": "Towards Reducing Energy Waste through Usage of External Communication of\n  Autonomous Vehicles", "comments": "Presented at the workshop \"Should I Stay or Should I Go? Automated\n  Vehicles in the Age of Climate Change\", April 25, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated vehicles can implement strategies to drive with optimized fuel\nefficiency. Therefore, automated driving is seen as a major advancement in\ntackling climate change. However, with automated vehicles driving in cities and\nother areas rife with other road users such as human drivers, pedestrians, or\ncyclists, there is the potential for \"stop-and-go\" traffic. This would greatly\ndiminish the possibility of automated vehicles to drive fuel-efficient. We\nsuggest using external communication of automated vehicles to aid in ecological\ndriving by providing clues to other road users to show the intent and therefore\nultimately enable smoother traffic.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 10:25:33 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Colley", "Mark", ""], ["Walch", "Marcel", ""], ["Rukzio", "Enrico", ""]]}, {"id": "2012.13944", "submitter": "S\\'everin Lemaignan", "authors": "Youssef Mohamed and S\\'everin Lemaignan", "title": "ROS for Human-Robot Interaction", "comments": "8 pages + ref, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Integrating real-time, complex social signal processing into robotic systems\n-- especially in real-world, multi-party interaction situations -- is a\nchallenge faced by many in the Human-Robot Interaction (HRI) community. The\ndifficulty is compounded by the lack of any standard model for human\nrepresentation that would facilitate the development and interoperability of\nsocial perception components and pipelines. We introduce in this paper a set of\nconventions and standard interfaces for HRI scenarios, designed to be used with\nthe Robot Operating System (ROS). It directly aims at promoting\ninteroperability and re-usability of core functionality between the many\nHRI-related software tools, from skeleton tracking, to face recognition, to\nnatural language processing. Importantly, these interfaces are designed to be\nrelevant to a broad range of HRI applications, from high-level crowd\nsimulation, to group-level social interaction modelling, to detailed modelling\nof human kinematics. We demonstrate these interface by providing a reference\npipeline implementation, packaged to be easily downloaded and evaluated by the\ncommunity.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 14:09:51 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Mohamed", "Youssef", ""], ["Lemaignan", "S\u00e9verin", ""]]}, {"id": "2012.13961", "submitter": "Karola Marky", "authors": "Karola Marky, Andreas Wei{\\ss}, Julien Gedeon, Sebastian G\\\"unther", "title": "Mastering Music Instruments through Technology in Solo Learning Sessions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mastering a musical instrument requires time-consuming practice even if\nstudents are guided by an expert. In the overwhelming majority of the time, the\nstudents practice by themselves and traditional teaching materials, such as\nvideos or textbooks, lack interaction and guidance possibilities. Adequate\nfeedback, however, is highly important to prevent the acquirement of wrong\nmotions and to avoid potential health problems. In this paper, we envision\nmusical instruments as smart objects to enhance solo learning sessions. We give\nan overview of existing approaches and setups and discuss them. Finally, we\nconclude with recommendations for designing smart and augmented musical\ninstruments for learning purposes.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 15:23:06 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Marky", "Karola", ""], ["Wei\u00df", "Andreas", ""], ["Gedeon", "Julien", ""], ["G\u00fcnther", "Sebastian", ""]]}, {"id": "2012.14201", "submitter": "Stefan Konigorski", "authors": "Stefan Konigorski, Sarah Wernicke, Tamara Slosarek, Alexander M.\n  Zenner, Nils Strelow, Ferenc D. Ruether, Florian Henschel, Manisha Manaswini,\n  Fabian Pottb\\\"acker, Jonathan A. Edelman, Babajide Owoyele, Matteo\n  Danieletto, Eddye Golden, Micol Zweig, Girish Nadkarni, Erwin B\\\"ottinger", "title": "StudyU: a platform for designing and conducting innovative digital\n  N-of-1 trials", "comments": "Manuscript: 14 pages, 5 figures. Supplements: 23 pages, Supplementary\n  Text 1-5, Supplementary Figures 1-5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  N-of-1 trials are the gold standard study design to evaluate individual\ntreatment effects and derive personalized treatment strategies. Digital tools\nhave the potential to initiate a new era of N-of-1 trials in terms of scale and\nscope, but fully-functional platforms are not yet available. Here, we present\nthe open source StudyU platform which includes the StudyU designer and StudyU\napp. With the StudyU designer, scientists are given a collaborative web\napplication to digitally specify, publish, and conduct N-of-1 trials. The\nStudyU app is a smartphone application with innovative user-centric elements\nfor participants to partake in the published trials and assess the effects of\ndifferent interventions on their health. Thereby, the StudyU platform allows\nclinicians and researchers worldwide to easily design and conduct digital\nN-of-1 trials in a safe manner. We envision that StudyU can change the\nlandscape of personalized treatments both for patients and healthy individuals,\ndemocratize and personalize evidence generation for self-optimization and\nmedicine, and can be integrated in clinical practice.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:40:26 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 10:39:59 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Konigorski", "Stefan", ""], ["Wernicke", "Sarah", ""], ["Slosarek", "Tamara", ""], ["Zenner", "Alexander M.", ""], ["Strelow", "Nils", ""], ["Ruether", "Ferenc D.", ""], ["Henschel", "Florian", ""], ["Manaswini", "Manisha", ""], ["Pottb\u00e4cker", "Fabian", ""], ["Edelman", "Jonathan A.", ""], ["Owoyele", "Babajide", ""], ["Danieletto", "Matteo", ""], ["Golden", "Eddye", ""], ["Zweig", "Micol", ""], ["Nadkarni", "Girish", ""], ["B\u00f6ttinger", "Erwin", ""]]}, {"id": "2012.14406", "submitter": "Przemyslaw Biecek", "authors": "Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, Jakub\n  Wisniewski, Przemyslaw Biecek", "title": "dalex: Responsible Machine Learning with Interactive Explainability and\n  Fairness in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing amount of available data, computing power, and the constant\npursuit for higher performance results in the growing complexity of predictive\nmodels. Their black-box nature leads to opaqueness debt phenomenon inflicting\nincreased risks of discrimination, lack of reproducibility, and deflated\nperformance due to data drift. To manage these risks, good MLOps practices ask\nfor better validation of model performance and fairness, higher explainability,\nand continuous monitoring. The necessity of deeper model transparency appears\nnot only from scientific and social domains, but also emerging laws and\nregulations on artificial intelligence. To facilitate the development of\nresponsible machine learning models, we showcase dalex, a Python package which\nimplements the model-agnostic interface for interactive model exploration. It\nadopts the design crafted through the development of various tools for\nresponsible machine learning; thus, it aims at the unification of the existing\nsolutions. This library's source code and documentation are available under\nopen license at https://python.drwhy.ai/.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 18:39:59 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Baniecki", "Hubert", ""], ["Kretowicz", "Wojciech", ""], ["Piatyszek", "Piotr", ""], ["Wisniewski", "Jakub", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "2012.14477", "submitter": "Po-Ming Law", "authors": "Po-Ming Law, Leo Yu-Ho Lo, Alex Endert, John Stasko, Huamin Qu", "title": "Causal Perception in Question-Answering Systems", "comments": "ACM Conference on Human Factors in Computing Systems (CHI 2021)", "journal-ref": null, "doi": "10.1145/3411764.3445444", "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Root cause analysis is a common data analysis task. While question-answering\nsystems enable people to easily articulate a why question (e.g., why students\nin Massachusetts have high ACT Math scores on average) and obtain an answer,\nthese systems often produce questionable causal claims. To investigate how such\nclaims might mislead users, we conducted two crowdsourced experiments to study\nthe impact of showing different information on user perceptions of a\nquestion-answering system. We found that in a system that occasionally provided\nunreasonable responses, showing a scatterplot increased the plausibility of\nunreasonable causal claims. Also, simply warning participants that correlation\nis not causation seemed to lead participants to accept reasonable causal claims\nmore cautiously. We observed a strong tendency among participants to associate\ncorrelation with causation. Yet, the warning appeared to reduce the tendency.\nGrounded in the findings, we propose ways to reduce the illusion of causality\nwhen using question-answering systems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 20:34:36 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 07:54:53 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 12:08:51 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Law", "Po-Ming", ""], ["Lo", "Leo Yu-Ho", ""], ["Endert", "Alex", ""], ["Stasko", "John", ""], ["Qu", "Huamin", ""]]}, {"id": "2012.14544", "submitter": "Pramod Vadiraja", "authors": "Viny Saajan Victor, Pramod Vadiraja, Jan-Tobias Sohns, Heike Leitte", "title": "Visual Probing and Correction of Object Recognition Models with\n  Interactive user feedback", "comments": "2 Pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of state-of-the-art machine learning and deep learning\ntechnologies, several industries are moving towards the field. Applications of\nsuch technologies are highly diverse ranging from natural language processing\nto computer vision. Object recognition is one such area in the computer vision\ndomain. Although proven to perform with high accuracy, there are still areas\nwhere such models can be improved. This is in-fact highly important in\nreal-world use cases like autonomous driving or cancer detection, that are\nhighly sensitive and expect such technologies to have almost no uncertainties.\nIn this paper, we attempt to visualise the uncertainties in object recognition\nmodels and propose a correction process via user feedback. We further\ndemonstrate our approach on the data provided by the VAST 2020 Mini-Challenge\n2.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 00:36:12 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Victor", "Viny Saajan", ""], ["Vadiraja", "Pramod", ""], ["Sohns", "Jan-Tobias", ""], ["Leitte", "Heike", ""]]}, {"id": "2012.14653", "submitter": "Yi-Chia Wang", "authors": "Yi-Chia Wang, Alexandros Papangelis, Runze Wang, Zhaleh Feizollahi,\n  Gokhan Tur, Robert Kraut", "title": "Can You be More Social? Injecting Politeness and Positivity into\n  Task-Oriented Conversational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal-oriented conversational agents are becoming prevalent in our daily\nlives. For these systems to engage users and achieve their goals, they need to\nexhibit appropriate social behavior as well as provide informative replies that\nguide users through tasks. The first component of the research in this paper\napplies statistical modeling techniques to understand conversations between\nusers and human agents for customer service. Analyses show that social language\nused by human agents is associated with greater users' responsiveness and task\ncompletion. The second component of the research is the construction of a\nconversational agent model capable of injecting social language into an agent's\nresponses while still preserving content. The model uses a sequence-to-sequence\ndeep learning architecture, extended with a social language understanding\nelement. Evaluation in terms of content preservation and social language level\nusing both human judgment and automatic linguistic measures shows that the\nmodel can generate responses that enable agents to address users' issues in a\nmore socially appropriate way.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 08:22:48 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wang", "Yi-Chia", ""], ["Papangelis", "Alexandros", ""], ["Wang", "Runze", ""], ["Feizollahi", "Zhaleh", ""], ["Tur", "Gokhan", ""], ["Kraut", "Robert", ""]]}, {"id": "2012.14800", "submitter": "Anna Fariha", "authors": "Anna Fariha, Lucy Cousins, Narges Mahyar, Alexandra Meliou", "title": "Example-Driven User Intent Discovery: Empowering Users to Cross the SQL\n  Barrier Through Query by Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional data systems require specialized technical skills where users\nneed to understand the data organization and write precise queries to access\ndata. Therefore, novice users who lack technical expertise face hurdles in\nperusing and analyzing data. Existing tools assist in formulating queries\nthrough keyword search, query recommendation, and query auto-completion, but\nstill require some technical expertise. An alternative method for accessing\ndata is Query by Example (QBE), where users express their data exploration\nintent simply by providing examples of their intended data. We study a\nstate-of-the-art QBE system called SQuID, and contrast it with traditional SQL\nquerying. Our comparative user studies demonstrate that users with varying\nexpertise are significantly more effective and efficient with SQuID than SQL.\nWe find that SQuID eliminates the barriers in studying the database schema,\nformalizing task semantics, and writing syntactically correct SQL queries, and\nthus, substantially alleviates the need for technical expertise in data\nexploration.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:22:59 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 14:15:15 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Fariha", "Anna", ""], ["Cousins", "Lucy", ""], ["Mahyar", "Narges", ""], ["Meliou", "Alexandra", ""]]}, {"id": "2012.15035", "submitter": "Minkyu Shin", "authors": "Minkyu Shin, Jin Kim, Minkyung Kim", "title": "Measuring Human Adaptation to AI in Decision Making: Application to\n  Evaluate Changes after AlphaGo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Across a growing number of domains, human experts are expected to learn from\nand adapt to AI with superior decision making abilities. But how can we\nquantify such human adaptation to AI? We develop a simple measure of human\nadaptation to AI and test its usefulness in two case studies. In Study 1, we\nanalyze 1.3 million move decisions made by professional Go players and find\nthat a positive form of adaptation to AI (learning) occurred after the players\ncould observe the reasoning processes of AI, rather than mere actions of AI.\nThese findings based on our measure highlight the importance of explainability\nfor human learning from AI. In Study 2, we test whether our measure is\nsufficiently sensitive to capture a negative form of adaptation to AI (cheating\naided by AI), which occurred in a match between professional Go players. We\ndiscuss our measure's applications in domains other than Go, especially in\ndomains in which AI's decision making ability will likely surpass that of human\nexperts.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 04:34:46 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 18:57:08 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 01:33:29 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shin", "Minkyu", ""], ["Kim", "Jin", ""], ["Kim", "Minkyung", ""]]}, {"id": "2012.15097", "submitter": "Polina Ovsiannikova", "authors": "Polina Ovsiannikova, Igor Buzhinsky, Antti Pakonen, Valeriy Vyatkin", "title": "Visual counterexample explanation for model checking with Oeritte", "comments": "The 25th International Conference on Engineering of Complex Computer\n  Systems (ICECCS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.HC cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite being one of the most reliable approaches for ensuring system\ncorrectness, model checking requires auxiliary tools to fully avail. In this\nwork, we tackle the issue of its results being hard to interpret and present\nOeritte, a tool for automatic visual counterexample explanation for function\nblock diagrams. To learn what went wrong, the user can inspect a parse tree of\nthe violated LTL formula and a table view of a counterexample, where important\nvariables are highlighted. Then, on the function block diagram of the system\nunder verification, they can receive a visualization of causality relationships\nbetween the calculated values of interest and intermediate results or inputs of\nthe function block diagram. Thus, Oeritte serves to decrease formal model and\nspecification debugging efforts along with making model checking more\nutilizable for complex industrial systems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 09:54:35 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Ovsiannikova", "Polina", ""], ["Buzhinsky", "Igor", ""], ["Pakonen", "Antti", ""], ["Vyatkin", "Valeriy", ""]]}, {"id": "2012.15164", "submitter": "Zheng Wang", "authors": "Zheng Wang, Muhua Guan, Jin Lan, Bo Yang, Tsutomu Kaizuka, Junichi\n  Taki, and Kimihiko Nakano", "title": "Analysis of Truck Driver Behavior to Design Different Lane Change Styles\n  in Automated Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lane change is a very demanding driving task and number of traffic accidents\nare induced by mistaken maneuvers. An automated lane change system has the\npotential to reduce driver workload and to improve driving safety. One\nchallenge is how to improve driver acceptance on the automated system. From the\nviewpoint of human factors, an automated system with different styles would\nimprove user acceptance as the drivers can adapt the style to different driving\nsituations. This paper proposes a method to design different lane change styles\nin automated driving by analysis and modeling of truck driver behavior. A truck\ndriving simulator experiment with 12 participants was conducted to identify the\ndriver model parameters and three lane change styles were classified as the\naggressive, medium, and conservative ones. The proposed automated lane change\nsystem was evaluated by another truck driving simulator experiment with the\nsame 12 participants. Moreover, the effect of different driving styles on\ndriver experience and acceptance was evaluated. The evaluation results\ndemonstrate that the different lane change styles could be distinguished by the\ndrivers; meanwhile, the three styles were overall evaluated as acceptable on\nsafety issues and reliable by the human drivers. This study provides insight\ninto designing the automated driving system with different driving styles and\nthe findings can be applied to commercial automated trucks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 14:06:20 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Wang", "Zheng", ""], ["Guan", "Muhua", ""], ["Lan", "Jin", ""], ["Yang", "Bo", ""], ["Kaizuka", "Tsutomu", ""], ["Taki", "Junichi", ""], ["Nakano", "Kimihiko", ""]]}, {"id": "2012.15211", "submitter": "Claudia Flores-Saviaga", "authors": "Claudia Flores-Saviaga, Yuwen Li, Benjamin V. Hanrahan, Jeffrey\n  Bigham, Saiph Savage", "title": "The Challenges of Crowd Workers in Rural and Urban America", "comments": null, "journal-ref": "Proceedings of the AAAI Conference on Human Computation and\n  Crowdsourcing 2020", "doi": null, "report-no": null, "categories": "cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd work has the potential of helping the financial recovery of regions\ntraditionally plagued by a lack of economic opportunities, e.g., rural areas.\nHowever, we currently have limited information about the challenges facing\ncrowd work-ers from rural and super rural areas as they struggle to make a\nliving through crowd work sites. This paper examines the challenges and\nadvantages of rural and super rural AmazonMechanical Turk (MTurk) crowd workers\nand contrasts them with those of workers from urban areas. Based on a survey\nof421 crowd workers from differing geographic regions in theU.S., we identified\nhow across regions, people struggled with being onboarded into crowd work. We\nuncovered that despite the inequalities and barriers, rural workers tended to\nbe striving more in micro-tasking than their urban counterparts. We also\nidentified cultural traits, relating to time dimension and individualism, that\noffer us an insight into crowd workers and the necessary qualities for them to\nsucceed on gig platforms. We finish by providing design implications based on\nour findings to create more inclusive crowd work platforms and tools\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 16:08:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Flores-Saviaga", "Claudia", ""], ["Li", "Yuwen", ""], ["Hanrahan", "Benjamin V.", ""], ["Bigham", "Jeffrey", ""], ["Savage", "Saiph", ""]]}, {"id": "2012.15378", "submitter": "Emad Barsoum", "authors": "Emad Barsoum, John Kender, Zicheng Liu", "title": "3D Human motion anticipation and classification", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.09561", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human motion prediction and understanding is a challenging problem. Due to\nthe complex dynamic of human motion and the non-deterministic aspect of future\nprediction. We propose a novel sequence-to-sequence model for human motion\nprediction and feature learning, trained with a modified version of generative\nadversarial network, with a custom loss function that takes inspiration from\nhuman motion animation and can control the variation between multiple predicted\nmotion from the same input poses.\n  Our model learns to predict multiple future sequences of human poses from the\nsame input sequence. We show that the discriminator learns general presentation\nof human motion by using the learned feature in action recognition task.\nFurthermore, to quantify the quality of the non-deterministic predictions, we\nsimultaneously train a motion-quality-assessment network that learns the\nprobability that a given sequence of poses is a real human motion or not.\n  We test our model on two of the largest human pose datasets: NTURGB-D and\nHuman3.6M. We train on both single and multiple action types. Its predictive\npower for motion estimation is demonstrated by generating multiple plausible\nfutures from the same input and show the effect of each of the loss functions.\nFurthermore, we show that it takes less than half the number of epochs to train\nan activity recognition network by using the feature learned from the\ndiscriminator.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 00:19:39 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Barsoum", "Emad", ""], ["Kender", "John", ""], ["Liu", "Zicheng", ""]]}, {"id": "2012.15441", "submitter": "Erfan Pakdamanian", "authors": "Erfan Pakdamanian, Shili Sheng, Sonia Baee, Seongkook Heo, Sarit\n  Kraus, Lu Feng", "title": "DeepTake: Prediction of Driver Takeover Behavior using Multimodal Data", "comments": "Accepted to CHI 2021", "journal-ref": null, "doi": "10.1145/3411764.3445563", "report-no": null, "categories": "cs.LG cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated vehicles promise a future where drivers can engage in non-driving\ntasks without hands on the steering wheels for a prolonged period.\nNevertheless, automated vehicles may still need to occasionally hand the\ncontrol back to drivers due to technology limitations and legal requirements.\nWhile some systems determine the need for driver takeover using driver context\nand road condition to initiate a takeover request, studies show that the driver\nmay not react to it. We present DeepTake, a novel deep neural network-based\nframework that predicts multiple aspects of takeover behavior to ensure that\nthe driver is able to safely take over the control when engaged in non-driving\ntasks. Using features from vehicle data, driver biometrics, and subjective\nmeasurements, DeepTake predicts the driver's intention, time, and quality of\ntakeover. We evaluate DeepTake performance using multiple evaluation metrics.\nResults show that DeepTake reliably predicts the takeover intention, time, and\nquality, with an accuracy of 96%, 93%, and 83%, respectively. Results also\nindicate that DeepTake outperforms previous state-of-the-art methods on\npredicting driver takeover time and quality. Our findings have implications for\nthe algorithm development of driver monitoring and state detection.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 04:24:46 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 17:30:50 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Pakdamanian", "Erfan", ""], ["Sheng", "Shili", ""], ["Baee", "Sonia", ""], ["Heo", "Seongkook", ""], ["Kraus", "Sarit", ""], ["Feng", "Lu", ""]]}, {"id": "2012.15846", "submitter": "Amogh Gudi", "authors": "Amogh Gudi, Marian Bittner, Jan van Gemert", "title": "Real-time Webcam Heart-Rate and Variability Estimation with Clean Ground\n  Truth for Evaluation", "comments": "Published in the MDPI Applied Sciences journal special issue Video\n  Analysis for Health Monitoring on December 2, 2020. arXiv admin note: text\n  overlap with arXiv:1909.01206", "journal-ref": "Applied Sciences. 2020; 10(23):8630", "doi": "10.3390/app10238630", "report-no": null, "categories": "cs.CV cs.HC eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote photo-plethysmography (rPPG) uses a camera to estimate a person's\nheart rate (HR). Similar to how heart rate can provide useful information about\na person's vital signs, insights about the underlying physio/psychological\nconditions can be obtained from heart rate variability (HRV). HRV is a measure\nof the fine fluctuations in the intervals between heart beats. However, this\nmeasure requires temporally locating heart beats with a high degree of\nprecision. We introduce a refined and efficient real-time rPPG pipeline with\nnovel filtering and motion suppression that not only estimates heart rates, but\nalso extracts the pulse waveform to time heart beats and measure heart rate\nvariability. This unsupervised method requires no rPPG specific training and is\nable to operate in real-time. We also introduce a new multi-modal video\ndataset, VicarPPG 2, specifically designed to evaluate rPPG algorithms on HR\nand HRV estimation. We validate and study our method under various conditions\non a comprehensive range of public and self-recorded datasets, showing\nstate-of-the-art results and providing useful insights into some unique\naspects. Lastly, we make available CleanerPPG, a collection of human-verified\nground truth peak/heart-beat annotations for existing rPPG datasets. These\nverified annotations should make future evaluations and benchmarking of rPPG\nalgorithms more accurate, standardized and fair.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:57:05 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gudi", "Amogh", ""], ["Bittner", "Marian", ""], ["van Gemert", "Jan", ""]]}, {"id": "2012.15853", "submitter": "Wieslaw Kopec", "authors": "Anna Jaskulska, Kinga Skorupska, Barbara Karpowicz, Cezary Biele,\n  Jaros{\\l}aw Kowalski, Wies{\\l}aw Kope\\'c", "title": "Exploration of Voice User Interfaces for Older Adults - A Pilot Study to\n  Address Progressive Vision Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voice User Interfaces (VUIs) owing to recent developments in Artificial\nIntelligence (AI) and Natural Language Processing (NLP), are becoming\nincreasingly intuitive and functional. They are especially promising for older\nadults, also with special needs, as VUIs remove some barriers related to access\nto Information and Communications Technology (ICT) solutions. In this pilot\nstudy we examine interdisciplinary opportunities in the area of VUIs as\nassistive technologies, based on an exploratory study with older adults, and a\nfollow-up in-depth pilot study with two participants regarding the needs of\npeople who are gradually losing their sight at a later age.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 18:58:32 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jaskulska", "Anna", ""], ["Skorupska", "Kinga", ""], ["Karpowicz", "Barbara", ""], ["Biele", "Cezary", ""], ["Kowalski", "Jaros\u0142aw", ""], ["Kope\u0107", "Wies\u0142aw", ""]]}]